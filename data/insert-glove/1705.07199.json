{"id": "1705.07199", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "The High-Dimensional Geometry of Binary Neural Networks", "abstract": "Recent fesse research has tirtzu shown plover that lir one slick can train a neural 46.47 network with arisman binary weights mid-1960s and sabharwal activations gelasian at train riograndense time hicom by vallet augmenting 15,000-acre the weights with amour a 2,313 high - precision continuous gr\u00f6nefeld latent airbrush variable that mcdill accumulates small changes from misapplied stochastic gradient descent. http://www.ups.com However, there is elatior a rushlow dearth peirano of hupa theoretical analysis zelinski to adulteress explain why 4,000-word we can effectively capture the 1991 features 58.07 in chanthu our data with hanseong binary weights unclad and activations. instil Our enes main saudek result is that aldrick the neural iomega networks thro with melita binary bucktooth weights isogg and corsa activations trained using burnett the method minnie of deepesh Courbariaux, pattle Hubara kampi et tillmans al. (2016) work because of darbishire the leit\u00e3o high - towels dimensional shacking geometry racers of ukrainian binary amblecote vectors. kirkton In particular, the ideal badescu continuous pty vectors raghnall that clokey extract out hermeneutic features in trobriand the intermediate 7.742 representations of jeopardising these BNNs falco are dichagyris well - approximated 16:27 by ballymurphy binary unremarked vectors in wanta the 1.3178 sense apartheid-era that dot products are lors approximately preserved. Compared to bandura previous research that demonstrated dpl the viability of reichswehr such wakeful BNNs, 27.85 our work explains celebres why 1973-74 these BNNs work donata in wanxiang terms of the helix-loop-helix HD geometry. interfering Our theory gunturi serves as muellers a 1-martina foundation lesli for understanding not only BNNs but attapeu a variety ermey of methods that counterfeiter seek moine to carboniferous compress traditional santopadre neural mcgonigal networks. spikers Furthermore, samokhvalov a mass\u00e9 better understanding nevelson of multilayer binary samoothiri neural networks serves agglomeration as a campero starting point for generalizing BNNs prognoses to sugary other trapezohedron neural network juken architectures such as atwar recurrent estampa neural sintra networks.", "histories": [["v1", "Fri, 19 May 2017 21:33:00 GMT  (264kb,D)", "http://arxiv.org/abs/1705.07199v1", "12 pages, 4 Figures"]], "COMMENTS": "12 pages, 4 Figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alexander g", "erson", "cory p berg"], "accepted": false, "id": "1705.07199"}, "pdf": {"name": "1705.07199.pdf", "metadata": {"source": "CRF", "title": "The High-Dimensional Geometry of Binary Neural Networks", "authors": ["Alexander G. Anderson"], "emails": ["aga@berkeley.edu", "cberg500@berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "The rapidly decreasing cost of computation has driven many successes in the field of deep learning in recent years. Given these successes, researchers are now considering applications of deep learning in resource limited hardware such as neuromorphic chips, embedded devices and smart phones (Esser et al. (2016), Neftci et al. (2016), Andri et al. (2017)). A recent success for both theoretical researchers and industry practitioners is that traditional neural networks can be compressed because they are highly over-parameterized. While there has been a large amount of experimental work dedicated to compressing neural networks (Sec. 2), we focus on the particular approach that replaces costly 32-bit floating point multiplications with cheap binary operations. Our analysis reveals a simple geometric picture based on the geometry of high dimensional binary vectors that allows us to understand the successes of the recent efforts to compress neural networks.\nRecent work by Courbariaux et al. (2016) and Hubara et al. (2016) has shown that one can efficiently train neural networks with binary weights and activations that have similar performance to their continuous counterparts. They demonstrate that such BNNs execute 7 times faster using a dedicated GPU kernel at test time. Furthermore, they argue that such BNNs require at least a factor of 32 fewer memory accesses at test time that should result in an even larger energy savings. There are two key ideas in their papers (Fig. 1). First, they associate a continuous weight, wc, with each binary weight, wb, that accumulates small changes from stochastic gradient descent. Second, they replace the non-differentiable binarize function (\u03b8(x) = 1 if x > 0 and \u22121 otherwise) with a continuous one during backpropagation. These modifications allow them to train neural networks that have\nar X\niv :1\n70 5.\n07 19\n9v 1\n[ cs\n.L G\n] 1\n9 M\nay 2\nbinary weights and activations with stochastic gradient descent. While their work has shown how to train such networks, the existence of neural networks with binary weights and activations needs to be reconciled with previous work that has sought to understand weight matrices as extracting out continuous features in data (e.g. Zeiler & Fergus (2014)). Summary of contributions:\n1. Angle Preservation Property: We demonstrate that binarization approximately preserves the direction of high dimensional vectors. In particular, we show that the angle between a random vector (from a standard normal distribution) and its binarized version converges to arccos \u221a 2/\u03c0 \u2248 37\u25e6 as the dimension of the vector goes to infinity. This angle is an\nexceedingly small angle in high dimensions. Furthermore, we show that this property is present in the weight vectors of a network trained using the method of Courbariaux et al. (2016).\n2. Dot Product Preservation Property: We show that the batch normalized weight-activation dot products, an important intermediate quantity in these BNNs, are approximately preserved under the binarization of the weight vectors. Relatedly, we argue that the continuous weights in the Courbariaux et al. (2016) method aren\u2019t just a learning artifact - they correspond to continuous weights trained using an estimator of the true gradient. Finally, we argue that this dot product preservation property is a sufficient condition for the modified learning dynamics to approximate the true learning dynamics that would train the continuous weights.\n3. Generalized Binarization Transformation: We show that the computations done by the first layer of the network on CIFAR10 are fundamentally different than the computations being done in the rest of the network because the high variance principal components are not randomly oriented relative to the binarization. Thus we recommend an architecture that uses a continuous convolution for the first layer to embed the image in a high dimensional binary space, after which it can be manipulated with cheap binary operations. Furthermore, we hypothesize that a GBT (rotate, binarize, rotate back) will be useful for dealing with low dimensional data embedded in a HD space that is not randomly oriented relative to the axes of binarization."}, {"heading": "2 Related Work", "text": "Neural networks that achieve good performance on tasks such as IMAGENET object recognition are highly computationally intensive. For instance, AlexNet has 61 million parameters and executes 1.5 billion operations to classify one 224 by 224 image (30 thousand operations/pixel) (Rastegari et al. (2016)). There has been a substantial amount of work to reduce this computational cost for embedded applications.\nFirst, there are a variety of approaches that seek to compress pre-trained networks. Kim et al. (2015) uses a Tucker decomposition of the kernel tensor and fine tunes the network afterwards. Han et al. (2015b) train a network, prune low magnitude connections, and retrain. Han et al. (2015a) extend their previous work to additionally include a weight sharing quantization step and Huffman coding of the weights.\nSecond, researchers have sought to train networks using either low precision floating point numbers or fixed point numbers, which allow for cheaper multiplications (Courbariaux et al. (2014), Gupta et al. (2015), Judd et al. (2015), Gysel et al. (2016), Lin et al. (2016), Lai et al. (2017)).\nThird, there are numerous works on training networks that have quantized weights and or activations. Classically, Bengio et al. (2013) looked at a variety of estimators for the gradient through a stochastic binary unit. Courbariaux et al. (2015) trains networks with binary weights, and then later with binary weights and activations (Courbariaux et al. (2016)). Rastegari et al. (2016) replace a continuous weight matrix with a scalar times a binary matrix (and have a similar approximation for weight activation dot products). Kim & Smaragdis (2016) train a network with weights restricted in the range \u22121 to 1 and then use a noisy backpropagation scheme train a network with binary weights and activations. Alemdar et al. (2016), Li et al. (2016) and Zhu et al. (2016) focus on networks with ternary weights. Further work seeks to quantize the weights and activations in neural networks to an arbitrary number of bits (Zhou et al. (2016), Hubara et al. (2016)). Zhou et al. (2017) use weights and activations that are zero or powers of two. Lin et al. (2015) and Zhou et al. (2016) quantize backpropagation in addition to the forward propagation.\nBeyond merely seeking to compress neural networks, there is a variety of papers that seek to analyze the internal representations of neural networks. Agrawal et al. (2014) found that feature magnitudes in higher layers do not matter (e.g. binarizing features barely changes classification performance). Merolla et al. (2016) analyze the robustness of neural network representations to a collection of different distortions. Dosovitskiy & Brox (2016) observe that binarizing features in intermediate layers of a CNN and then using backpropagation to find an image with those features leads to relatively little distortion of the image compared to dropping out features. These works naturally lead into our work where we are seeking to better understand the representations in neural networks based on the geometry of HD binary vectors."}, {"heading": "3 Theory and Experiments", "text": "In this section, we outline two theoretical predictions and then verify them experimentally. We train a binary neural network on CIFAR-10 (same learning algorithm and architecture as in Courbariaux et al. (2016)). This convolutional neural network has six layers of convolutions, all of which have a 3 by 3 spatial kernel. The number of feature maps in each layer are 128, 128, 256, 256, 512, 512. After the second, fourth, and sixth convolutions, we do a 2 by 2 max pooling operation. Then we have two fully connected layers with 1024 units each. Each layer has a batch norm layer in between. We note that the dimension of the weight vector in consideration (i.e. convolution converted to a matrix multiply) is the patch size (= 3 \u2217 3 = 9) times the number of channels. We also carried out experiments using MNIST and got similar results."}, {"heading": "3.1 Preservation of Direction During Binarization", "text": "In the hyperdimensional computing theory of Kanerva (2009), one of the key ideas is that two random, high-dimensional vectors of dimension d whose entries are chosen uniformly from the set {\u22121, 1} are approximately orthogonal (by the central limit theorem, the cosine angle between two such random vectors is normally distributed with \u00b5 = 0 and \u03c3 \u223c 1/ \u221a d (cos \u03b8 \u2248 0 \u2192 \u03b8 \u2248 \u03c02 )). We apply this approach of analyzing the geometry (i.e. angle distributions) of high-dimensional vectors to binary\nvectors. As a null distribution, we use the standard normal distribution, which is rotationally invariant, to generate our vectors. In moderately high dimensions, binarizing a random vector changes its direction by a small amount relative to the angle between two random vectors. This is contrary to our low-dimensional intuition that is guided by the fact that the angle between two random 2D vectors is uniformly distributed (Fig. 2).\nIn order to test the applicability of our theory of Gaussian random vectors to real neural networks, we train a multilayer binary CNN on CIFAR10 (using the Courbariaux et al. (2016) method) and look at the weight vectors 1 in that trained network. We see a close correspondence between the experimental results and our theory for the angles between the binary and continuous weights (Fig. 2). We note that there is a small but systematic deviation from the theory towards larger angles for the higher layers of the network.\n1If we write each convolution as the matrix multiplication Wx where x is a column vector, then the weight vectors are the rows of W ."}, {"heading": "3.2 Dot Product Preservation as a Sufficient Condition for Sensible Learning Dynamics", "text": "One reasonable question to ask is: are these so-called \u2019continuous weights\u2019 just a learning artifact without a clear correspondence to the binary weights? While we know that wb = \u03b8(wc), there are many continuous weights that map onto a particular binary weight vector. Which one do we find when we apply the method of Courbariaux et al. (2016)? As we discuss below, we get the continuous weight that preserves the dot products with the activations. The key to our analysis is to focus on the transformers in our network whose forward and backward propagation functions are not related in the way that they would normally be related in typical gradient descent.\nWe show that the modified gradient that we are using can be viewed as an estimator of the true gradient that would be used to train the continuous weights in traditional backpropagation. Furthermore, we show that a sufficient property for this estimator to be a good one is that the dot products of the activations with the pre-binarized and post-binarized weights are proportional.\nSuppose we have a neural network where we allocate two tensors, u, and v (and the associated derivatives of the cost with respect to those tensors, \u03b4u and \u03b4v). Suppose that the loss as a function of v is L(x)|x=v . Further, there are two potential forward propagation functions, f , and g. If we trained our network under normal conditions using g as the forward propagation function, then we would compute\nv \u2190 g(u) \u03b4v \u2190 L\u2032(x = v = g(u)) \u03b4u\u2190 \u03b4v \u00b7 g\u2032(u)\nIn the modified backpropagation scheme, we compute\nv \u2190 f(u) \u03b4v = L\u2032(x = v = f(u)) \u03b4u\u2190 \u03b4v \u00b7 g\u2032(u) A sufficient condition for the updates \u03b4u to be the same is L\u2032(x = f(u)) \u223c L\u2032(x = g(u)) where a \u223c b means that the vector a is a scalar times the vector b. Now we specialize this argument to the binarize block that binarizes the weights in our networks. Here, u is the continuous weight, wc, f(u) is the pointwise binarize function, g(u) is the identity function 2, and L is the loss of the network as a function of the weights in a particular layer. Given our architecture, we can write L(x) = M(a \u00b7 x) where a are the activations corresponding to that layer (a is binary for all except the first layer) and M is the loss as a function of the weight-activation dot products. Then L\u2032(x) = M \u2032(a \u00b7 x) a where denotes a pointwise multiply. Thus the sufficient condition is M \u2032(a \u00b7wb) \u223cM \u2032(a \u00b7wc). Since the dot products are followed by a batch normalization, M(k~x) = M(~x)\u2192M \u2032(~x) = kM \u2032(k~x). Therefore, it is sufficient that\na \u00b7 wb \u223c a \u00b7 wc\nWe call this final relation the Dot Product Preservation (DPP) property. In summary, the learning dynamics where we use g for the forward and backward passes (i.e. training the network with continuous weights) is approximately equivalent to the modified learning dynamics (f on the forward pass, and g on the backward pass) when we have the DPP property.\nWe also come at this problem from another direction. In SI, Sec. 2 we work out the learning dynamics of the modified backprop scheme in the case of a one layer neural network that seeks to do regression (this ends up being linear regression with binary weights). In this case, the learning dynamics for the weights end up being \u2206wc \u223c Cyx \u2212 \u03b8(wc)Cxx where Cyx is the input-output correlation matrix and Cxx is the input covariance matrix. Since \u03b8 forces the weight matrix to be binary, this equation cannot be satisfied exactly in general conditions. Specializing to the case of an identity input covariance matrix, we show that E(\u03b8(wc)) = Cyx. Intuitively, the entries of the weight matrix oscillate between +1 and \u22121 in the correct proportion in order to get the weight matrix correct in expectation. In high dimensions, these are likely to be out of phase, leading to a low variance estimator.\nIndeed, in our numerical experiments on CIFAR10, we see that the dot products of the activations with the pre-binarization and post-binarization weights are highly correlated (Fig. 3). Likewise, we verify a second relation that corresponds to ablating the other instance of binarize transformer in\n2For the weights, g as in Fig. 1 is the identity function because the wc\u2019s are clipped to be in [\u22121, 1]\nthe network, the transformer that binarizes the activations : wb \u00b7 ac \u223c wb \u00b7 ab where ac denotes the pre-binarized (post-batch norm) activations (Fig. 4). For the practitioner, we recommend checking the DPP property in order to assess the areas in which the network\u2019s performance is being degraded by the compression of the weights or activations.\nImpact on Classification: As we\u2019ve argued, the quantity that the network cares about, the batch normalized weight-activation dot products, is preserved under binarization of the weights. It is also natural to ask to what extent the classification performance depends on the binarization of the weights. In our experiments on CIFAR10, if we remove the binarization of the weights on all of the convolutional layers, the classification performance drops by only 3 percent relative to the original network. Looking at each layer individually, we see that removing the weight binarization for the first layer accounts for this entire percentage, and removing the binarization of the weights for each other layer causes no degradation in performance. We note that removing the binarization of the activations unsurprisingly has a substantial impact on the classification performance because that removes the main non-linearity of the network."}, {"heading": "3.3 Permutation of Activations Reveals Fundamental Difference Between First Layer and Subsequent Layers", "text": "Looking at the correlations in Fig. 3, we see that the first layer has a much smaller dot product correlation than the other layers. In order to understand this observation better, we investigate the different factors that lead to the dot product correlation. For instance, it could be the case that the correlation between the two dot products is due to the two weight vectors being closely aligned. Another explanation is that the weight vectors are well-aligned with the informative directions in the data. To study this, we apply a random permutation to the activations in order to generate a distribution with the same marginal statistics as the original data but independent joint statistics. Such\na transformation gives us a distribution with a correlation equal to the normalized dot product of the weight vectors (SI Sec. 3). As we can see in Fig. 4, the correlations for the higher layers decrease substantially but the correlation in the first layer increases (for the first layer, the shuffling operation randomly permutes the pixels in the image). Thus we demonstrate that the binary weight vectors in the first layer are not well-aligned with the continuous weight vectors relative to the input data.\nWe hypothesize that the core issue at play is that the input data is not randomly oriented relative to the axes of binarization. In order to be clear on what we mean by the axes of binarization, first consider the Generalized Binarization Transformation (GBT):\n\u03b8R(x) = R T \u03b8(Rx)\nwhere x is a column vector, R is a rotation matrix, and \u03b8 is the pointwise binarization function from before. We call the rows of R the axes of binarization. If R is the identity matrix, then we reduce back to our original binarization function and the axes of binarization are just the canonical basis vectors (..., 0, 1, 0, ...). Consider the 27 dimensional input to the first set of convolutions in our network: 3 color channels of a 3 by 3 patch of an image from CIFAR10 with the mean removed. 3 PCs capture 90 percent of the variance of this data and 4 PCs capture 94.5 percent of the variance. Furthermore, these PCs aren\u2019t randomly oriented relative to the binarization axes. For instance, the first two PCs are spatially uniform colors. More generally, natural images (such as those in IMAGENET) will have the same issue. Translation invariance of the pixel covariance matrix implies that the principal components are the filters of the 2D fourier transform. Scale invariance implies a 1/f2 power spectrum, which results in the largest PCs corresponding to low frequencies.\nStepping back, this control gives us important insight: the first layer is fundamentally different from the other layers due to the non-random orientation of the data relative to the axes of binarization. Practically speaking, we have two recommendations. First, we recommend an architecture that uses a set of continuous convolutional weights to embed images in a high-dimensional binary space, after which it can be manipulated efficiently using binary vectors. While there isn\u2019t a large accuracy degradation on CIFAR10, these observations are going to be more important on datasets with larger images such as IMAGENET. We note that this theoretically grounded recommendation is consistent with previous empirical work. Han et al. (2015b) find that compressing the first set of convolutional weights of a particular layer by the same fraction has the highest impact on performance if done on the first layer. Zhou et al. (2016) find that accuracy degrades by about 0.5 to 1 percent on SHVN when quantizing the first layer weights. Second, we recommend experimenting with a GBT where the rotation is chosen so that it can be computed efficiently. This solves the problem of low-dimensional data embedded in a high dimensional space that is not randomly oriented relative to the binarization function."}, {"heading": "4 Conclusion", "text": "Neural networks with binary weights and activations have similar performance to their continuous counterparts with substantially reduced execution time and power usage. We provide an experimentally verified theory for understanding how one can get away with such a massive reduction in precision based on the geometry of HD vectors. First, we show that binarization of high-dimensional vectors preserves their direction in the sense that the angle between a random vector and its binarized version is much smaller than the angle between two random vectors (Angle Preservation Property). Second, we take the perspective of the network and show that binarization approximately preserves weight-activation dot products (Dot Product Preservation Property). More generally, when using a network compression technique, we recommend looking at the weight activation dot product histograms as a heuristic to help localize the layers that are most responsible for performance degradation. Third, we discuss the impacts of the low effective dimensionality on the first layer of the network and recommend either using continuous weights for the first layer or a Generalized Binarization Transformation. Such a transformation may be useful for architectures like LSTMs where the update for the hidden state declares a particular set of axes to be important (e.g. by taking the pointwise multiply of the forget gates with the cell state). More broadly speaking, our theory is useful for analyzing a variety of neural network compression techinques that transform the weights, activations or both to reduce the execution cost without degrading performance."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Bruno Olshausen, Urs K\u00f6ster, Spencer Kent, Eric Dodds, Dylan Paiton, and members of the Redwood Center for useful feedback on this work. This material is based upon work supported by the National Science Foundation under Grant No. DGE 1106400 (AGA). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. This work was supported in part by Systems on Nanoscale Information fabriCs (SONIC), one of the six SRC STARnet Centers, sponsored by MARCO and DARPA (AGA)."}, {"heading": "5 Supplementary Information", "text": ""}, {"heading": "5.1 Expected Angles", "text": "We draw random n dimensional vectors from a rotationally invariant distribution and compare the angles between two random vectors and the binarized version of that vector. We note that a rotationally invariant distribution can be factorized into a pdf for the magnitude of the vector times a distribution on angles. In the expectations that we are calculating, the magnitude cancels out and there is only one rotationally invariant distribution on angles. Thus it suffices to compute these expectations using a Gaussian.\nLemmas:\n1. Consider a vector, v, chosen from a standard normal distribution of dimension n. Let \u03c1 = v1\u221a\nv21+...+v 2 n\n. Then \u03c1 is distributed according to: g(\u03c1) =\n1\u221a \u03c0 \u0393(n/2) \u0393((n\u22121)/2) (1 \u2212 \u03c1 2) n\u22123 2 http://www-stat.wharton.upenn.edu/~tcai/paper/ Coherence-Phase-Transition.pdf where \u0393 is the Gamma function.\n2. \u0393(z+\u03b1)\u0393(z+\u03b2) = z \u03b1\u2212\u03b2 ( 1 + (\u03b1\u2212\u03b2)(\u03b1+\u03b2+1)2z ) +O(|z|\u22122) as z \u2192\u221e\n\u2022 Distribution of angles between two random vectors. Since a Gaussian is a rotationally invariant distribution, we can say without loss of generality that one of the vectors is (1, 0, 0, . . . 0). Then the cosine angle between those two vectors is \u03c1 as defined above. While we have the exact distribution, we note that\n\u2013 E(\u03c1) = 0 due to the symmetry of the distribution. \u2013 V ar(\u03c1) = E(\u03c12) = 1n because 1 = E (\u2211 i x 2 i\u2211\nj x 2 j\n) = \u2211 iE ( x2i\u2211 j x 2 j ) = n \u2217 E(\u03c12)\n\u2022 Angles between a vector and the binarized version of that vector, \u03b7 = v\u00b7\u03b8(v)||v||\u00b7||\u03b8(v)|| =\u2211 i |vi|\u221a\u2211 v2i \u2217 \u221a n\n\u2013\nE(\u03b7) = \u221a n\u221a \u03c0 \u2217 \u0393(n/2) \u0393((n+ 1)/2) lim n\u2192\u221e E(\u03b7) =\n\u221a 2\n\u03c0\nFirst, we noteE(\u03b7) = \u221a nE(|\u03c1|). ThenE(|\u03c1|) = \u222b 1 0 d\u03c1 \u03c1 2\u221a \u03c0 \u0393(n/2) \u0393((n\u22121)/2) (1\u2212\u03c1 2) n\u22123 2 = 2\u221a \u03c0 \u2217 1n\u22121 \u0393(n/2) \u0393((n\u22121)/2) (substitute u = \u03c1\n2 and use \u0393(x + 1) = x\u0393(x) ). Lemma two gives the n\u2192\u221e limit.\n\u2013 V ar(\u03b7) = 1\nn\n( 1\u2212 1\n\u03c0\n) +O(1/n2)\nThus we have the normal scaling as in the central limit theorem of the large n variance. We can calculate this explicitly following the approach of https://en.wikipedia. org/wiki/Volume_of_an_n-ball#Gaussian_integrals. As we\u2019ve calculated E(\u03b7), it suffices to calculate E(\u03b72). Expanding out \u03b72, we get E(\u03b72) = 1n + (n \u2212 1) \u2217 E( |v1v2| v21+...v 2 n ). Below we show that E( |v1v2| v21+...v 2 n\n) = 2\u03c0n . Thus the variance is:\n1 n \u2217 ( 1\u2212 2 \u03c0 ) + 2 \u03c0 \u2212 (\u221a n\u221a \u03c0 \u0393(n/2) \u0393((n+ 1)/2) )2 Using Lemma 2 to expand out the last term, we get [\n\u221a n\u221a \u03c0\n(n/2)\u22121/2(1 \u2212 1/(4n) + O(n\u22122))]2 = 2\u03c0 (1\u2212 1/(2n) +O(n\n\u22122)). Plugging this in gives the desired result. Going back to the calculation of that expectation, change variables to v1 = r cos \u03b8, v2 = r sin \u03b8, z2 = v23 + ...+ v 2 n. The integration over the volume element dv3 . . . dvn is rewritten as dzdAn\u22123 where dAn denotes the surface element of a n sphere. Since\nthe integrand only depends on the magnitude, z, \u222b dAn\u22123 = z\nn\u22123 \u2217 Sn\u22123 where Sn = 2\u03c0(n+1)/2\n\u0393( n+12 ) denotes the surface area of a unit n-sphere. Then\nE\n( |v1v2|\nv21 + . . . v 2 n\n) = (2\u03c0)\u2212n/2Sn\u22123 \u222b 2\u03c0 0 d\u03b8| cos \u03b8 sin \u03b8|\u2217 \u222b rdrzn\u22123dz\u2217 r 2 r2 + z2 \u2217e\u2212(z 2+r2)/2\nThen substitute r = p cos\u03c6, z = p sin\u03c6 where \u03c6 \u2208 [0, \u03c0/2]\n= (2\u03c0)\u2212n/2 \u2217 2Sn\u22123 \u222b \u03c0/2\n0\nd\u03c6 cos\u03c63 \u2217 sin\u03c6n\u22123 \u222b \u221e\n0\ndp \u2217 pn\u22121e\u2212p 2/2\nThe first integral is 2n(n\u22122) using u = sin 2 \u03c6. The second integral is 2(n\u22122)/2\u0393(n/2) using u = p2/2 and the definition of the gamma function. Simplifying, we get 2\u03c0\u2217n .\nRoughly speaking, we can see that the angle between a vector and a binarized version of that vector converges to arccos \u221a\n2 \u03c0 \u2248 37 \u25e6 which is a very small angle in high dimensions."}, {"heading": "5.2 An Explicit Example of Learning Dynamics", "text": "In this subsection, we look at the learning dynamics for the BNN training algorithm in a simple case and gain some insight about the learning algorithm. Consider the case of regression where we try and predict y with x with a binary linear predictor. Using a squared error loss, we have L = (y \u2212 y\u0302)2 = (y \u2212 wbx)2 = (y \u2212 \u03b8(wc)x)2. (In this notation, x is a column vector.) Taking the derivative of this loss with respect to the continuous weights and using the rule for back propagating through the binarize function, we get \u2206wc \u223c \u2212dL/dwc = \u2212dL/dwb \u00b7 dwb/dwc = (y \u2212 wbx)xT . Finally, averaging over the training data, we get\n\u2206wc \u223c Cyx \u2212 \u03b8(wc) \u00b7 Cxx Cyx = E[yxT ] Cxx = E(xxT ) (1) It is worthwhile to compare this equation the corresponding equation from typical linear regression: \u2206wc \u223c Cyx\u2212wc \u00b7Cxx For simplicity, lets consider the case where Cxx is the identity matrix. In this case, all of the components of w become independent and we get the equation \u03b4w = \u2217 (\u03b1\u2212 \u03b8(w)) where is the learning rate and \u03b1 is the entry of Cyx corresponding to a particular element, w. If we were doing regular linear regression, it is clear that the stable point of these equations is when w = \u03b1. Since we binarize the weight, that equation cannot be satisfied. However, it can be shown (?) that in this special case of binary weight linear regression, E(\u03b8(wc)) = \u03b1.\nIntuitively, if we consider a high dimensional vector and the fluctuations of each component are likely to be out of phase, then wb \u00b7 x \u2248 wc \u00b7 x is going to be correct in expectation with a variance that scales as 1n . During the actual learning process, we anneal the learning rate to a very small number, so the particular state of a fluctuating component of the vector is frozen in. Relatedly, the equation Cyx \u2248 wCxx is easier to satisfy in high dimensions, whereas in low dimensions, we only satisfy it in expectation.\nRough proof for (?): Suppose that |\u03b1| \u2264 1. The basic idea of these dynamics is that you are taking steps of size proportional to whose direction depends on whether w > 0 or w < 0. In particular, if w > 0, then we take a step \u2212 \u00b7 |1\u2212\u03b1| and if w < 0, we take a step \u00b7 (\u03b1+ 1). It is evident that after a sufficient burn-in period, |w| \u2264 \u2217max(|1\u2212\u03b1|, 1 +\u03b1) \u2264 2 . Suppose w > 0 occurs with fraction p and w < 0 occurs with fraction 1\u2212 p. In order for w to be in equilibrium, oscillating about zero, we must have that these steps balance out on average: p(1\u2212\u03b1) = (1\u2212 p)(1 + \u03b1)\u2192 p = (1 +\u03b1)/2. Then the expected value of \u03b8(w) is 1 \u2217 p+ (\u22121) \u2217 (1\u2212 p) = \u03b1. When |\u03b1| > 1, the dynamics diverge because \u03b1\u2212 \u03b8bin(w) will always have the same sign. This divergence demonstrates the importance of some normalization technique such as batch normalization or attempting to represent w with a constant times a binary matrix."}, {"heading": "5.3 Dot Product Correlations After Activation Permutation", "text": "Suppose that we look at A = w \u00b7 a and B = v \u00b7 a where a are now the randomly permuted activations. What does the distribution of A,B look like? To answer this, we look at the correlation between A\nand B and show that it is the correlation between w and v. First, let us assume that p(a) = \u220f i f(ai) E(ai) = 0, E(a2i ) = \u03c3 2. Then E(A) = E(B) = 0. Now we compute:\nE(AB) = \u2211 i,j wivjE(aiaj) = \u03c3\n2(w \u00b7 v) Likewise, E(A2) = \u03c32(w \u00b7 w) and E(B2) = \u03c32(v \u00b7 v). Thus the correlation coefficient w\u00b7v|w||v| , as desired"}], "references": [{"title": "Analyzing the performance of multilayer neural networks for object recognition", "author": ["Pulkit Agrawal", "Ross Girshick", "Jitendra Malik"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Agrawal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2014}, {"title": "Ternary neural networks for resource-efficient ai applications", "author": ["Hande Alemdar", "Nicholas Caldwell", "Vincent Leroy", "Adrien Prost-Boucle", "Fr\u00e9d\u00e9ric P\u00e9trot"], "venue": "arXiv preprint arXiv:1609.00222,", "citeRegEx": "Alemdar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alemdar et al\\.", "year": 2016}, {"title": "Yodann: An architecture for ultra-low power binary-weight cnn acceleration", "author": ["Renzo Andri", "Lukas Cavigelli", "Davide Rossi", "Luca Benini"], "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,", "citeRegEx": "Andri et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Andri et al\\.", "year": 2017}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Yoshua Bengio", "Nicholas L\u00e9onard", "Aaron Courville"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Training deep neural networks with low precision multiplications", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Binarized neural networks: Training neural networks with weights and activations constrained to +1 and -1", "author": ["Matthieu Courbariaux", "Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "Courbariaux et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2016}, {"title": "Inverting visual representations with convolutional networks", "author": ["Alexey Dosovitskiy", "Thomas Brox"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Dosovitskiy and Brox.,? \\Q2016\\E", "shortCiteRegEx": "Dosovitskiy and Brox.", "year": 2016}, {"title": "Convolutional networks for fast, energy-efficient neuromorphic computing", "author": ["Steven K Esser", "Paul A Merolla", "John V Arthur", "Andrew S Cassidy", "Rathinakumar Appuswamy", "Alexander Andreopoulos", "David J Berg", "Jeffrey L McKinstry", "Timothy Melano", "Davis R Barch"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Esser et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Esser et al\\.", "year": 2016}, {"title": "Deep learning with limited numerical precision", "author": ["Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan"], "venue": "In ICML, pp", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Hardware-oriented approximation of convolutional neural networks", "author": ["Philipp Gysel", "Mohammad Motamedi", "Soheil Ghiasi"], "venue": "arXiv preprint arXiv:1604.03168,", "citeRegEx": "Gysel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gysel et al\\.", "year": 2016}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Quantized neural networks: Training neural networks with low precision weights and activations", "author": ["Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.07061,", "citeRegEx": "Hubara et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hubara et al\\.", "year": 2016}, {"title": "Reduced-precision strategies for bounded memory in deep neural nets", "author": ["Patrick Judd", "Jorge Albericio", "Tayler Hetherington", "Tor Aamodt", "Natalie Enright Jerger", "Raquel Urtasun", "Andreas Moshovos"], "venue": "arXiv preprint arXiv:1511.05236,", "citeRegEx": "Judd et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Judd et al\\.", "year": 2015}, {"title": "Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors", "author": ["Pentti Kanerva"], "venue": "Cognitive Computation,", "citeRegEx": "Kanerva.,? \\Q2009\\E", "shortCiteRegEx": "Kanerva.", "year": 2009}, {"title": "Bitwise neural networks", "author": ["Minje Kim", "Paris Smaragdis"], "venue": "arXiv preprint arXiv:1601.06071,", "citeRegEx": "Kim and Smaragdis.,? \\Q2016\\E", "shortCiteRegEx": "Kim and Smaragdis.", "year": 2016}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Yong-Deok Kim", "Eunhyeok Park", "Sungjoo Yoo", "Taelim Choi", "Lu Yang", "Dongjun Shin"], "venue": "arXiv preprint arXiv:1511.06530,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Deep convolutional neural network inference with floating-point weights and fixed-point activations", "author": ["Liangzhen Lai", "Naveen Suda", "Vikas Chandra"], "venue": "arXiv preprint arXiv:1703.03073,", "citeRegEx": "Lai et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2017}, {"title": "Ternary weight networks", "author": ["Fengfu Li", "Bo Zhang", "Bin Liu"], "venue": "arXiv preprint arXiv:1605.04711,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Fixed point quantization of deep convolutional networks", "author": ["Darryl Lin", "Sachin Talathi", "Sreekanth Annapureddy"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Lin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Neural networks with few multiplications", "author": ["Zhouhan Lin", "Matthieu Courbariaux", "Roland Memisevic", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1510.03009,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Deep neural networks are robust to weight binarization and other non-linear distortions", "author": ["Paul Merolla", "Rathinakumar Appuswamy", "John Arthur", "Steve K Esser", "Dharmendra Modha"], "venue": null, "citeRegEx": "Merolla et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Merolla et al\\.", "year": 1981}, {"title": "Neuromorphic deep learning machines", "author": ["Emre Neftci", "Charles Augustine", "Somnath Paul", "Georgios Detorakis"], "venue": "arXiv preprint arXiv:1612.05596,", "citeRegEx": "Neftci et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neftci et al\\.", "year": 2016}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi"], "venue": "arXiv preprint arXiv:1603.05279,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "In European conference on computer vision,", "citeRegEx": "Zeiler and Fergus.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}, {"title": "Incremental network quantization: Towards lossless cnns with low-precision weights", "author": ["Aojun Zhou", "Anbang Yao", "Yiwen Guo", "Lin Xu", "Yurong Chen"], "venue": "arXiv preprint arXiv:1702.03044,", "citeRegEx": "Zhou et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2017}, {"title": "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "author": ["Shuchang Zhou", "Yuxin Wu", "Zekun Ni", "Xinyu Zhou", "He Wen", "Yuheng Zou"], "venue": "arXiv preprint arXiv:1606.06160,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "Trained ternary quantization", "author": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1612.01064,", "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al. (2016) work because of the highdimensional geometry of binary vectors.", "startOffset": 121, "endOffset": 142}, {"referenceID": 4, "context": "Given these successes, researchers are now considering applications of deep learning in resource limited hardware such as neuromorphic chips, embedded devices and smart phones (Esser et al. (2016), Neftci et al.", "startOffset": 177, "endOffset": 197}, {"referenceID": 4, "context": "Given these successes, researchers are now considering applications of deep learning in resource limited hardware such as neuromorphic chips, embedded devices and smart phones (Esser et al. (2016), Neftci et al. (2016), Andri et al.", "startOffset": 177, "endOffset": 219}, {"referenceID": 2, "context": "(2016), Andri et al. (2017)).", "startOffset": 8, "endOffset": 28}, {"referenceID": 2, "context": "(2016), Andri et al. (2017)). A recent success for both theoretical researchers and industry practitioners is that traditional neural networks can be compressed because they are highly over-parameterized. While there has been a large amount of experimental work dedicated to compressing neural networks (Sec. 2), we focus on the particular approach that replaces costly 32-bit floating point multiplications with cheap binary operations. Our analysis reveals a simple geometric picture based on the geometry of high dimensional binary vectors that allows us to understand the successes of the recent efforts to compress neural networks. Recent work by Courbariaux et al. (2016) and Hubara et al.", "startOffset": 8, "endOffset": 678}, {"referenceID": 2, "context": "(2016), Andri et al. (2017)). A recent success for both theoretical researchers and industry practitioners is that traditional neural networks can be compressed because they are highly over-parameterized. While there has been a large amount of experimental work dedicated to compressing neural networks (Sec. 2), we focus on the particular approach that replaces costly 32-bit floating point multiplications with cheap binary operations. Our analysis reveals a simple geometric picture based on the geometry of high dimensional binary vectors that allows us to understand the successes of the recent efforts to compress neural networks. Recent work by Courbariaux et al. (2016) and Hubara et al. (2016) has shown that one can efficiently train neural networks with binary weights and activations that have similar performance to their continuous counterparts.", "startOffset": 8, "endOffset": 703}, {"referenceID": 3, "context": "Figure 1: Review of the Courbariaux et al. (2016) BNN Training Algorithm: a.", "startOffset": 24, "endOffset": 50}, {"referenceID": 3, "context": "Since the binarize function is non-differentiable, they use a smoothed version of the forward function for the backward function (in particular, the straight-through estimator of Bengio et al. (2013)).", "startOffset": 179, "endOffset": 200}, {"referenceID": 4, "context": "Furthermore, we show that this property is present in the weight vectors of a network trained using the method of Courbariaux et al. (2016). 2.", "startOffset": 114, "endOffset": 140}, {"referenceID": 4, "context": "Furthermore, we show that this property is present in the weight vectors of a network trained using the method of Courbariaux et al. (2016). 2. Dot Product Preservation Property: We show that the batch normalized weight-activation dot products, an important intermediate quantity in these BNNs, are approximately preserved under the binarization of the weight vectors. Relatedly, we argue that the continuous weights in the Courbariaux et al. (2016) method aren\u2019t just a learning artifact - they correspond to continuous weights trained using an estimator of the true gradient.", "startOffset": 114, "endOffset": 450}, {"referenceID": 6, "context": "5 billion operations to classify one 224 by 224 image (30 thousand operations/pixel) (Rastegari et al. (2016)).", "startOffset": 86, "endOffset": 110}, {"referenceID": 5, "context": "Kim et al. (2015) uses a Tucker decomposition of the kernel tensor and fine tunes the network afterwards.", "startOffset": 0, "endOffset": 18}, {"referenceID": 3, "context": "Han et al. (2015b) train a network, prune low magnitude connections, and retrain.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Han et al. (2015b) train a network, prune low magnitude connections, and retrain. Han et al. (2015a) extend their previous work to additionally include a weight sharing quantization step and Huffman coding of the weights.", "startOffset": 0, "endOffset": 101}, {"referenceID": 1, "context": "Second, researchers have sought to train networks using either low precision floating point numbers or fixed point numbers, which allow for cheaper multiplications (Courbariaux et al. (2014), Gupta et al.", "startOffset": 165, "endOffset": 191}, {"referenceID": 1, "context": "Second, researchers have sought to train networks using either low precision floating point numbers or fixed point numbers, which allow for cheaper multiplications (Courbariaux et al. (2014), Gupta et al. (2015), Judd et al.", "startOffset": 165, "endOffset": 212}, {"referenceID": 1, "context": "Second, researchers have sought to train networks using either low precision floating point numbers or fixed point numbers, which allow for cheaper multiplications (Courbariaux et al. (2014), Gupta et al. (2015), Judd et al. (2015), Gysel et al.", "startOffset": 165, "endOffset": 232}, {"referenceID": 1, "context": "Second, researchers have sought to train networks using either low precision floating point numbers or fixed point numbers, which allow for cheaper multiplications (Courbariaux et al. (2014), Gupta et al. (2015), Judd et al. (2015), Gysel et al. (2016), Lin et al.", "startOffset": 165, "endOffset": 253}, {"referenceID": 1, "context": "Second, researchers have sought to train networks using either low precision floating point numbers or fixed point numbers, which allow for cheaper multiplications (Courbariaux et al. (2014), Gupta et al. (2015), Judd et al. (2015), Gysel et al. (2016), Lin et al. (2016), Lai et al.", "startOffset": 165, "endOffset": 272}, {"referenceID": 1, "context": "Second, researchers have sought to train networks using either low precision floating point numbers or fixed point numbers, which allow for cheaper multiplications (Courbariaux et al. (2014), Gupta et al. (2015), Judd et al. (2015), Gysel et al. (2016), Lin et al. (2016), Lai et al. (2017)).", "startOffset": 165, "endOffset": 291}, {"referenceID": 1, "context": "Classically, Bengio et al. (2013) looked at a variety of estimators for the gradient through a stochastic binary unit.", "startOffset": 13, "endOffset": 34}, {"referenceID": 1, "context": "Classically, Bengio et al. (2013) looked at a variety of estimators for the gradient through a stochastic binary unit. Courbariaux et al. (2015) trains networks with binary weights, and then later with binary weights and activations (Courbariaux et al.", "startOffset": 13, "endOffset": 145}, {"referenceID": 1, "context": "Classically, Bengio et al. (2013) looked at a variety of estimators for the gradient through a stochastic binary unit. Courbariaux et al. (2015) trains networks with binary weights, and then later with binary weights and activations (Courbariaux et al. (2016)).", "startOffset": 13, "endOffset": 260}, {"referenceID": 1, "context": "Classically, Bengio et al. (2013) looked at a variety of estimators for the gradient through a stochastic binary unit. Courbariaux et al. (2015) trains networks with binary weights, and then later with binary weights and activations (Courbariaux et al. (2016)). Rastegari et al. (2016) replace a continuous weight matrix with a scalar times a binary matrix (and have a similar approximation for weight activation dot products).", "startOffset": 13, "endOffset": 286}, {"referenceID": 1, "context": "Classically, Bengio et al. (2013) looked at a variety of estimators for the gradient through a stochastic binary unit. Courbariaux et al. (2015) trains networks with binary weights, and then later with binary weights and activations (Courbariaux et al. (2016)). Rastegari et al. (2016) replace a continuous weight matrix with a scalar times a binary matrix (and have a similar approximation for weight activation dot products). Kim & Smaragdis (2016) train a network with weights restricted in the range \u22121 to 1 and then use a noisy backpropagation scheme train a network with binary weights and activations.", "startOffset": 13, "endOffset": 451}, {"referenceID": 0, "context": "Alemdar et al. (2016), Li et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Alemdar et al. (2016), Li et al. (2016) and Zhu et al.", "startOffset": 0, "endOffset": 40}, {"referenceID": 0, "context": "Alemdar et al. (2016), Li et al. (2016) and Zhu et al. (2016) focus on networks with ternary weights.", "startOffset": 0, "endOffset": 62}, {"referenceID": 0, "context": "Alemdar et al. (2016), Li et al. (2016) and Zhu et al. (2016) focus on networks with ternary weights. Further work seeks to quantize the weights and activations in neural networks to an arbitrary number of bits (Zhou et al. (2016), Hubara et al.", "startOffset": 0, "endOffset": 231}, {"referenceID": 0, "context": "Alemdar et al. (2016), Li et al. (2016) and Zhu et al. (2016) focus on networks with ternary weights. Further work seeks to quantize the weights and activations in neural networks to an arbitrary number of bits (Zhou et al. (2016), Hubara et al. (2016)).", "startOffset": 0, "endOffset": 253}, {"referenceID": 0, "context": "Alemdar et al. (2016), Li et al. (2016) and Zhu et al. (2016) focus on networks with ternary weights. Further work seeks to quantize the weights and activations in neural networks to an arbitrary number of bits (Zhou et al. (2016), Hubara et al. (2016)). Zhou et al. (2017) use weights and activations that are zero or powers of two.", "startOffset": 0, "endOffset": 274}, {"referenceID": 0, "context": "Alemdar et al. (2016), Li et al. (2016) and Zhu et al. (2016) focus on networks with ternary weights. Further work seeks to quantize the weights and activations in neural networks to an arbitrary number of bits (Zhou et al. (2016), Hubara et al. (2016)). Zhou et al. (2017) use weights and activations that are zero or powers of two. Lin et al. (2015) and Zhou et al.", "startOffset": 0, "endOffset": 352}, {"referenceID": 0, "context": "Alemdar et al. (2016), Li et al. (2016) and Zhu et al. (2016) focus on networks with ternary weights. Further work seeks to quantize the weights and activations in neural networks to an arbitrary number of bits (Zhou et al. (2016), Hubara et al. (2016)). Zhou et al. (2017) use weights and activations that are zero or powers of two. Lin et al. (2015) and Zhou et al. (2016) quantize backpropagation in addition to the forward propagation.", "startOffset": 0, "endOffset": 375}, {"referenceID": 0, "context": "Agrawal et al. (2014) found that feature magnitudes in higher layers do not matter (e.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Agrawal et al. (2014) found that feature magnitudes in higher layers do not matter (e.g. binarizing features barely changes classification performance). Merolla et al. (2016) analyze the robustness of neural network representations to a collection of different distortions.", "startOffset": 0, "endOffset": 175}, {"referenceID": 0, "context": "Agrawal et al. (2014) found that feature magnitudes in higher layers do not matter (e.g. binarizing features barely changes classification performance). Merolla et al. (2016) analyze the robustness of neural network representations to a collection of different distortions. Dosovitskiy & Brox (2016) observe that binarizing features in intermediate layers of a CNN and then using backpropagation to find an image with those features leads to relatively little distortion of the image compared to dropping out features.", "startOffset": 0, "endOffset": 300}, {"referenceID": 4, "context": "We train a binary neural network on CIFAR-10 (same learning algorithm and architecture as in Courbariaux et al. (2016)).", "startOffset": 93, "endOffset": 119}, {"referenceID": 15, "context": "In the hyperdimensional computing theory of Kanerva (2009), one of the key ideas is that two random, high-dimensional vectors of dimension d whose entries are chosen uniformly from the set {\u22121, 1} are approximately orthogonal (by the central limit theorem, the cosine angle between two such random vectors is normally distributed with \u03bc = 0 and \u03c3 \u223c 1/ \u221a d (cos \u03b8 \u2248 0 \u2192 \u03b8 \u2248 \u03c02 )).", "startOffset": 44, "endOffset": 59}, {"referenceID": 4, "context": "In order to test the applicability of our theory of Gaussian random vectors to real neural networks, we train a multilayer binary CNN on CIFAR10 (using the Courbariaux et al. (2016) method) and look at the weight vectors 1 in that trained network.", "startOffset": 156, "endOffset": 182}, {"referenceID": 4, "context": "Which one do we find when we apply the method of Courbariaux et al. (2016)? As we discuss below, we get the continuous weight that preserves the dot products with the activations.", "startOffset": 49, "endOffset": 75}, {"referenceID": 11, "context": "Han et al. (2015b) find that compressing the first set of convolutional weights of a particular layer by the same fraction has the highest impact on performance if done on the first layer.", "startOffset": 0, "endOffset": 19}, {"referenceID": 11, "context": "Han et al. (2015b) find that compressing the first set of convolutional weights of a particular layer by the same fraction has the highest impact on performance if done on the first layer. Zhou et al. (2016) find that accuracy degrades by about 0.", "startOffset": 0, "endOffset": 208}], "year": 2017, "abstractText": "Recent research has shown that one can train a neural network with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of theoretical analysis to explain why we can effectively capture the features in our data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al. (2016) work because of the highdimensional geometry of binary vectors. In particular, the ideal continuous vectors that extract out features in the intermediate representations of these BNNs are wellapproximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated the viability of such BNNs, our work explains why these BNNs work in terms of the HD geometry. Our theory serves as a foundation for understanding not only BNNs but a variety of methods that seek to compress traditional neural networks. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.", "creator": "LaTeX with hyperref package"}}}