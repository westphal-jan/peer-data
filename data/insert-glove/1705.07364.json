{"id": "1705.07364", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2017", "title": "Stabilizing Adversarial Nets With Prediction Methods", "abstract": "Adversarial give neural talisker networks solve choquehuanca many important problems cartmill in incarnates data sinotrans science, cocker but are jordani notoriously difficult politeness to train. These ksar difficulties kragujevac come oul from serwotka the fact unaccountability that optimal weights p.e.n. for dergham adversarial nets 17-billion correspond 2,124 to confides saddle points, 2,284 and herein not minimizers, ivcher of the loss function. didacticism The agronomical alternating stochastic karankawa gradient methods qf1 typically gewehr used for counterweight such lehrte problems habel do not reliably gunshop converge (860) to saddle points, 127.4 and when syphax convergence highlands does problemas happen wktv it is often highly sensitive teabag to pleasurably learning rates. kalinda We waigel propose a paramount simple modification of aetius stochastic iu gradient opposing descent momoh that nominalist stabilizes dib\u00ebr adversarial networks. We cytogenetic show, both needlegrass in 76-65 theory 107.76 and practice, detainers that the proposed method reliably converges to martan saddle decamping points. v\u00f6lsung This risaldar makes adversarial raqqa networks less likely harenberg to \" pro-nazi collapse \", and asai enables period faster training with kishanganj larger learning reconsecrated rates.", "histories": [["v1", "Sat, 20 May 2017 22:27:19 GMT  (17818kb,D)", "https://arxiv.org/abs/1705.07364v1", null], ["v2", "Fri, 9 Jun 2017 04:22:35 GMT  (19105kb,D)", "http://arxiv.org/abs/1705.07364v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NA", "authors": ["abhay yadav", "sohil shah", "zheng xu", "david jacobs", "tom goldstein"], "accepted": false, "id": "1705.07364"}, "pdf": {"name": "1705.07364.pdf", "metadata": {"source": "CRF", "title": "Stabilizing Adversarial Nets With Prediction Methods", "authors": ["Abhay Yadav", "Sohil Shah", "Zheng Xu", "David Jacobs"], "emails": ["tomg}@cs.umd.edu,", "sohilas@umd.edu,", "djacobs@umiacs.umd.edu"], "sections": [{"heading": null, "text": "Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \u201ccollapse,\u201d and enables faster training with larger learning rates."}, {"heading": "1 Introduction", "text": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc. One particularly motivating application of adversarial nets is their ability to form generative models, as opposed to the classical discriminative models [14, 32, 8, 28].\nWhile adversarial networks have the power to attack a wide range of previously unsolved problems, they suffer from a major flaw: they are difficult to train. This is because adversarial nets try to accomplish two objectives simultaneously; weights are adjusted to maximize performance on one task while minimizing performance on another. Mathematically, this corresponds to finding a saddle point of a loss function - a point that is minimal with respect to one set of weights, and maximal with respect to another.\nConventional neural networks are trained by marching down a loss function until a minimizer is reached (Figure 1a). In contrast, adversarial training methods search for saddle points rather than a minimizer, which introduces the possibility that the training path \u201cslides off\u201d the objective functions and the loss goes to \u2212\u221e (Figure 1b), resulting in \u201ccollapse\u201d of the adversarial network. As a result, many authors suggest using early stopping, gradients/weight clipping [2], or specialized objective functions [14, 45, 2] to maintain stability.\nIn this paper, we present a simple \u201cprediction\u201d step that is easily added to many training algorithms for adversarial nets. We present theoretical analysis showing that the proposed prediction method is asymptotically stable for a class of saddle point problems. Finally, we use a wide range of experiments to show that prediction enables faster training of adversarial networks using large learning rates without the instability problems that plague conventional training schemes.\n\u2217Equal contribution\nar X\niv :1\n70 5.\n07 36\n4v 2\n[ cs\n.L G\n] 9\nJ un\n2 01\n7"}, {"heading": "2 Proposed Method", "text": "Saddle-point optimization problems have the general form\nmin u max v L(u, v) (1)\nfor some loss function L and variables u and v. Most authors use the alternating stochastic gradient method to solve saddle-point problems involving neural networks. This method alternates between updating u with a stochastic gradient descent step, and then updating v with a stochastic gradient ascent step. When simple/classical SGD updates are used, the steps of this method can be written\nuk+1 = uk \u2212 \u03b1kL\u2032u(uk, vk) | gradient descent in u, starting at (uk, vk) vk+1 = vk + \u03b2kL\u2032v(uk+1, vk) | gradient ascent in v, starting at (uk+1, vk) .\n(2)\nHere, {\u03b1k} and {\u03b2k} are learning rate schedules for the minimization and maximization steps, respectively. The vectors L\u2032u(u, v) and L\u2032v(u, v) denote (possibly stochastic) gradients of L with respect to u and v, respectively. In practice, the gradient updates are often performed by an automated solver, such as the Adam optimizer [19], and include momentum updates.\nWe propose to stabilize the training of adversarial networks by adding a prediction step. Rather than calculating vk+1 using uk+1, we first make a prediction, u\u0304k+1, about where the u iterates will be in the future, and use this predicted value to obtain vk+1.\nPrediction Method\nuk+1 = uk \u2212 \u03b1kL\u2032u(uk, vk) | gradient descent in u, starting at (uk, vk) u\u0304k+1 = uk+1 + (uk+1 \u2212 uk) | predict future value of u vk+1 = vk + \u03b2kL\u2032v(u\u0304k+1, vk) | gradient ascent in v, starting at (u\u0304k+1, vk) .\n(3)\nThe Prediction step (3) tries to estimate where u is going to be in the future by assuming its trajectory remains the same as in the current iteration."}, {"heading": "3 Background", "text": ""}, {"heading": "3.1 Adversarial Networks as a Saddle-Point Problem", "text": "We now discuss a few common adversarial network problems and their saddle-point formulations. Generative Adversarial Networks (GANs) fit a generative model to a dataset using a game in which a generative model competes against a discriminator [14]. The generator, G(z; \u03b8g), takes random noise vectors z as inputs, and maps them onto points in the target data distribution. The discriminator,\nD(x; \u03b8d), accepts a candidate point x and tries to determine whether it is really drawn from the empirical distribution (in which case it outputs 1), or fabricated by the generator (output 0). During a training iteration, noise vectors from a Gaussian distribution G are pushed through the generator network G to form a batch of generated data samples denoted by Dfake. A batch of empirical samples, Dreal, is also prepared. One then tries to adjust the weights of each network to solve a saddle point problem, which is popularly formulated as,\nmin \u03b8g max \u03b8d\nEx\u223cDreal f(D(x; \u03b8d)) + Ez\u223cG f(1\u2212D(G(z; \u03b8g); \u03b8d)). (4)\nHere f(.) is any monotonically increasing function. Initially, [14] proposed using f(x) = log(x).\nDomain Adversarial Networks (DANs) [25, 12, 10] take data collected from a \u201csource\u201d domain, and extract a feature representation that can be used to train models that generalize to another \u201ctarget\u201d domain. For example, in the domain adversarial neural network (DANN [12]), a set of feature layers maps data points into an embedded feature space, and a classifier is trained on these embedded features. Meanwhile, the adversarial discriminator tries to determine, using only the embedded features, whether the data points belong to the source or target domain. A good embedding yields better task-specific objective on target domain while fooling the discriminator, and is found by solving\nmin \u03b8f ,\u03b8yk max \u03b8d \u2211 k \u03b1kLyk ( xs; \u03b8f , \u03b8yk ) \u2212 \u03bbLd (xs,xt; \u03b8f , \u03b8d) . (5)\nHere Ld is any adversarial discriminator loss function and Lyk denotes the task specific loss. \u03b8f , \u03b8d, and \u03b8yk are network parameter of feature mapping, discriminator, and classification layers."}, {"heading": "3.2 Stabilizing saddle point solvers", "text": "It is well known that alternating stochastic gradient methods are unstable when using simple logarithmic losses. This led researchers to explore multiple directions for stabilizing GANs; either by adding regularization terms [2, 23, 5, 45], a myriad of training \u201chacks\u201d [34, 15], re-engineering network architectures [45], and designing different solvers [27]. Specifically, the Wasserstein GAN (WGAN) [2] approach modifies the original objective by replacing f(x) = log(x) with f(x) = x. This led to a training scheme in which the discriminator weights are \u201cclipped.\u201d However, as discussed in [2], the WGAN training is unstable at high learning rates, or when used with popular momentum based solvers such as Adam. Currently, it is known to work well only with RMSProp [2].\nThe unrolled GAN [27] is a new solver that can stabilize training at the cost of more expensive gradient computations. Each generator update requires the computation of multiple extra discriminator updates, which are then discarded when the generator update is complete. While avoiding GAN collapse, this method requires increased computation and memory.\nIn the convex optimization literature, saddle point problems are more well studied. One popular solver is the primal-dual hybrid gradient (PDHG) method [46, 11], which has been popularized by Chambolle and Pock [4], and has been successfully applied to a range of machine learning and statistical estimation problems [13]. PDHG relates closely to the method proposed here - it achieves stability using the same prediction step, although it uses a different type of gradient update and is only applicable to bi-linear problems.\nStochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31]. Similar optimization algorithms have been studied for reinforcement learning [40, 9]. Recently, a \u201cdoubly\u201d stochastic method that randomizes both primal and dual updates was proposed for strongly convex bilinear saddle point problems [42]. For general saddle point problems, \u201cdoubly\u201d stochastic gradient descent methods are discussed in [29, 30], in which primal and dual variables are updated simultaneously based on the previous iterates and the current gradients."}, {"heading": "4 Interpretations of the prediction step", "text": "We present three ways to explain the effect of prediction: an intuitive, non-mathematical perspective, a more analytical viewpoint involving dynamical systems, and finally a rigorous proof-based approach."}, {"heading": "4.1 An intuitive viewpoint", "text": "The standard alternating SGD switches between minimization and maximization steps. In this algorithm, there is a risk that the minimization step can overpower the maximization step, in which case the iterates will \u201cslide off\u201d the edge of saddle, leading to instability (Figure 1b). Conversely, an overpowering maximization step will dominate the minimization step, and drive the iterates to extreme values as well.\nThe effect of prediction is visualized in Figure 2. Suppose that a maximization step takes place starting at the red dot. Without prediction, the maximization step will be the same regardless of whether the previous minimization update was weak (Figure 2a) or strong (Figure 2b). Prediction allows the maximization step to exploit information about the minimization step. If the previous minimizations step was weak (Figure 2a), the prediction step (dotted black arrow) stays close to the red dot, resulting in a weak predictive maximization step (white arrow). But if we arrived at the red dot using a strong minimization step (Figure 2b), the prediction moves a long way down the loss surface, resulting in a stronger maximization step (white arrows) to compensate."}, {"heading": "4.2 A more mathematical perspective", "text": "To get stronger intuition about prediction methods, lets look at the behavior of Algorithm (3) on a simple bi-linear saddle of the form\nL(u, v) = vTKu (6) where K is a matrix. When exact (non-stochastic) gradient updates are used, the iterates follow the path of a simple dynamical system with closed-form solutions. We give here a sketch of this argument: a detailed derivation is provided in the Supplementary Material.\nWhen the (non-predictive) gradient method (2) is applied to the linear problem (6), the resulting iterations can be written\nuk+1 \u2212 uk \u03b1 = \u2212KT vk, v k+1 \u2212 vk \u03b1 = (\u03b2/\u03b1)Kuk+1.\nWhen the stepsize \u03b1 gets small, this behaves like a discretization of the system of differential equations\nu\u0307 = \u2212KT v, v\u0307 = \u03b2/\u03b1Ku where u\u0307 and v\u0307 denote the derivatives of u and v with respect to time. These equations describe a simple harmonic oscillator, and the closed form solution for u is\nu(t) = C cos(\u03a31/2t+ \u03c6)\nwhere \u03a3 is a diagonal matrix, and the matrix C and vector \u03c6 depend on the initialization. We can see that, for small values of \u03b1 and \u03b2, the non-predictive algorithm (2) approximates an undamped harmonic motion, and the solutions orbit around the saddle without converging.\nThe prediction step (3) improves convergence because it produces damped harmonic motion that sinks into the saddle point. When applied to the linearized problem (6), we get the dynamical system\nu\u0307 = \u2212KT v, v\u0307 = \u03b2/\u03b1K(u+ \u03b1u\u0307) (7)\nwhich has solution\nu(t) = UA exp(\u2212 t\u03b1 2\n\u221a \u03a3) sin(t \u221a (1\u2212 \u03b12/4)\u03a3 + \u03c6).\nFrom this analysis, we see that the damping caused by the prediction step causes the orbits to converge into the saddle point, and the error decays exponentially fast."}, {"heading": "4.3 A rigorous perspective", "text": "While the arguments above are intuitive, they are also informal and do not address issues like stochastic gradients, non-constant stepsize sequences, and more complex loss functions. We now provide a rigorous convergence analysis that handles these issues.\nWe assume that the function L(u, v) is convex in u and concave in v. We can then measure convergence using the \u201cprimal-dual\u201d gap, P (u, v) = L(u, v?)\u2212 L(u?, v) where (u?, v?) is a saddle. Note that P (u, v) > 0 for non-optimal (u, v), and P (u, v) = 0 if (u, v) is a saddle. Using these definitions, we formulate the following convergence result. The proof is in the supplementary material. Theorem 1. Suppose the function L(u, v) is convex in u, concave in v, and that the partial gradient L\u2032v is uniformly Lipschitz smooth in u (\u2016L\u2032v(u1, v)\u2212 L\u2032v(u2, v)\u2016 \u2264 Lv\u2016u1 \u2212 u2\u2016). Suppose further that the stochastic gradient approximations satisfy E\u2016L\u2032u(u, v)\u20162 \u2264 G2u, E\u2016L\u2032v(u, v)\u20162 \u2264 G2v for scalars Gu and Gv, and that E\u2016uk \u2212 u?\u20162 \u2264 D2u, and E\u2016vk \u2212 v?\u20162 \u2264 D2v for scalars Du and Dv. If we choose decreasing learning rate parameters of the form \u03b1k = C\u03b1\u221ak and \u03b2k = C\u03b2\u221a k , then the SGD method with prediction converges in expectation, and we have the error bound\nE[P (u\u0302l, v\u0302l)] \u2264 1 2 \u221a l ( D2u C\u03b1 + D2v C\u03b2 ) + \u221a l + 1 l ( C\u03b1G 2 u 2 + C\u03b1LvG 2 u + C\u03b1LvD 2 v + C\u03b2G 2 v 2 ) where u\u0302l = 1l \u2211l k=1 u k, v\u0302l = 1l \u2211l k=1 v k."}, {"heading": "5 Experiments", "text": "We present a wide range of experiments to demonstrate the benefits of the proposed prediction step for adversarial nets. We consider a saddle point problem on a toy dataset constructed using MNIST images, and then move on to consider state-of-the-art models for three tasks: GANs, domain adaptation, and learning of fair classifiers."}, {"heading": "5.1 MNIST Toy problem", "text": "We consider the task of classifying MNIST digits as being even or odd. To make the problem interesting, we corrupt 70% of odd digits with salt-and-pepper noise, while we corrupt only 30% of even digits. When we train a LeNet network [22] on this problem, we find that the network encodes and uses information about the noise; when a noise vs no-noise classifier is trained on the deep features generated by LeNet, it gets 100% accuracy. The goal of this task is to force LeNet to ignore the noise when making decisions. We create an adversarial model of the form (5) in which Ly is a softmax loss for the even vs odd classifier. We make Ld a softmax loss for the task of discriminating whether the input sample is noisy or not. The classifier and discriminator were both pre-trained using the default LeNet implementation in Caffe [18]. Then the combined adversarial net was jointly trained both with and without prediction. For implementation details, see the Supplementary Material.\nFigure 3 summarizes our findings. In this experiment, we considered applying prediction to both the classifier and discriminator. We note that our task is to retain good classification accuracy while preventing the discriminator from doing better than the trivial strategy of classifying odd digits as noisy and even digits as non-noisy. This means that the discriminator accuracy should ideally be \u223c 0.7. As shown in Figure 3a, the prediction step hardly makes any difference when evaluated at the small learning rate of 10\u22124. However, when evaluated at higher rates, Figures 3b and 3c show that the prediction solvers are very stable while one without prediction collapses (blue solid line is flat) very early. Figure 3c shows that the default learning rate (10\u22123) of the Adam solver is unstable unless prediction is used."}, {"heading": "5.2 Generative Adversarial Networks", "text": "Next, we test the efficacy and stability of our proposed predictive step on generative adversarial networks (GAN), which are formulated as saddle point problems (4) and are popularly solved using a heuristic approach [14]. We consider an image modeling task using CIFAR-10 [20] on the recently popular convolutional GAN architecture, DCGAN [32]. We compare our predictive method with that of DCGAN and the unrolled GAN [27] using the training protocol described in [32]. Note that we compared against the unrolled GAN with stop gradient switch2 and K = 5 unrolling steps. All the approaches were trained for five random seeds and 100 epochs each.\nWe start with comparing all three methods using the default solver for DCGAN (the Adam optimizer) with learning rate=0.0002 and \u03b21=0.5. Figure 4 compares the generated sample images (at the 100th epoch) and the training loss curve for all approaches. The discriminator and generator loss curves in Figure 4e show that without prediction, the DCGAN collapses at the 45th and 57th epochs. Similarly, Figure 4f shows that the training for unrolled GAN collapses in at least three instances. The training procedure using predictive steps never collapsed during any epochs. Qualitatively, the images generated using prediction are more diverse than the DCGAN and unrolled GAN images.\nFigure 5 compares all approaches when trained with 5\u00d7 higher learning rate (0.001) (the default for the Adam solver). As observed in [32], the standard and unrolled solvers are very unstable and collapse at this higher rate. However, as shown in Figure 5d, & 5a, training remains stable when a predictive step is used, and generates images of reasonable quality. The training procedure for both DCGAN and unrolled GAN collapsed on all five random seeds. The results on various additional intermediate learning rates are in the Supplementary Material.\nIn the Supplementary Material, we present one additional comparison showing results on a higher momentum of \u03b21=0.9 (learning rate=0.0002). We observe that all the training approaches are stable. However, the quality of images generated using DCGAN is inferior to that of the predictive and unrolled methods.\nOverall, of the 25 training settings we ran on (each of five learning rates for five random seeds), the DCGAN training procedure collapsed in 20 such instances while unrolled GAN collapsed in 14 experiments (not counting the multiple collapse in each training round). On the contrary, we find that our simple predictive step method collapsed only once.\nFinally note that prediction adds trivial cost to the training algorithm. Using a single TitanX Pascal, a training epoch of DCGAN takes 35 secs. With prediction, an epoch takes 38 secs. The unrolled GAN method, which requires extra gradient steps, takes 139 secs/epoch."}, {"heading": "5.3 Domain Adaptation", "text": "We consider the domain adaptation task [33, 12, 38] wherein the representation learned using the source domain samples is altered so that it can also generalize to samples from the target distribution. We use the problem setup and hyper-parameters as described in [12] using the OFFICE dataset [33] (experimental details are shared in the Supplementary Material). In Table 1, comparisons are drawn with respect to target domain accuracy on six pairs of source-target domain tasks. We observe that the prediction step has mild benefits on the \u201ceasy\u201d adaptation tasks with very similar source\n2We found the unrolled GAN without stop gradient switch as well as for smaller values of K collapsed when used on the DCGAN architecture.\nand target domain samples. However, on the transfer learning tasks of AMAZON-to-WEBCAM, WEBCAM-to-AMAZON, and DSLR-to-AMAZON which has noticeably distinct data samples, an extra prediction step gives an absolute improvement of 1.3\u2212 6.9% in predicting target domain labels."}, {"heading": "5.4 Fair Classifier", "text": "Finally, we consider a task of learning fair feature representations [26, 10, 24] such that the final learned classifier does not discriminate with respect to a sensitive variable. As proposed in [10] one way to measure fairness is using discrimination,\nydisc = \u2223\u2223\u2223\u2223\u2223 1N0 \u2211i:si=0 \u03b7(xi)\u2212 1 N1 \u2211 i:si=1 \u03b7(xi) \u2223\u2223\u2223\u2223\u2223 . (8) Here si is a binary sensitive variable for the ith data sample and Nk denotes the total number of samples belonging to the kth sensitive class. Similar to the domain adaptation task, the learning of each classifier can be formulated as a minimax problem in (5) [10, 26]. Unlike the previous example though, this task has a model selection component. From a pool of hundreds of randomly generated adversarial deep nets, for each value of t, one selects the model that maximizes the difference\nyt,Delta = yacc \u2212 t \u2217 ydisc. (9)\nThe \u201cAdult\u201d dataset from the UCI machine learning repository is used. The task (yacc) is to classify whether a person earns \u2265 $50k/year. The person\u2019s gender is chosen to be the sensitive variable. Details are in the supplementary. To demonstrate the advantage of using prediction for model selection, we follow the protocol developed in [10]. In this work, the search space is restricted to a class of models that consist of a fully connected autoencoder, one task specific discriminator, and one adversarial discriminator. The encoder output from the autoencoder acts as input to both the discriminators. In our experiment, 100 models are randomly selected. During the training of each adversarial model, Ld is a cross-entropy loss while Ly is a linear combination of reconstruction and cross-entropy loss. Once all the models are trained, the best model for each value of t is selected by evaluating (9) on the validation set.\nFigure 6a plots the results on the test set for the AFLR approach with and without prediction steps in their default Adam solver. For each value of t, Figure 6b, 6c also compares the number of layers in the selected encoder and discriminator networks. When using prediction for training, relatively stronger encoder models are produced and selected during validation, and hence the prediction results generalize better on the test set."}, {"heading": "6 Conclusion", "text": "We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks. We present theoretical results showing that the prediction step is asymptotically stable for solving saddle point problems. We show, using a variety of test problems, that prediction steps prevent network collapse and enable training with larger learning rates than plain SGD methods."}, {"heading": "A Detailed derivation of the harmonic oscillator equation", "text": "Here, we provide a detailed derivation of the harmonic oscillator behavior of Algorithm (3) on the simple bi-linear saddle of the form\nL(x, y) = yTKx where K is a matrix. Note that, within a small neighborhood of a saddle, all smooth weakly convex objective functions behave like (6).To see why, consider a smooth objective function L with a saddle point at x\u2217 = 0, y\u2217 = 0. Within a small neighborhood of the saddle, we can approximate the function L to high accuracy using its Taylor approximation\nL(x, y) \u2248 L(x\u2217, y\u2217) + yTL\u2032xyx+O(\u2016x\u20163 + \u2016y\u20163) where L\u2032xy denotes the matrix of mixed-partial derivatives with respect to x and y. Note that the first-order terms have vanished from this Taylor approximation because the gradients are zero at a saddle point. The O(\u2016x\u20162) and O(\u2016y\u20162) terms vanish as well because the problem is assumed to be weakly convex around the saddle. Up to third-order error (which vanishes quickly near the saddle), this Taylor expansion has the form (6). For this reason, stability on saddles of the form (6) is a necessary condition for convergence of (3), and the analysis here describes the asymptotic behavior of the prediction method on any smooth problem for which the method converges.\nWe will show that, as the learning rate gets small, the iterates of the non-prediction method (2) rotate in orbits around the saddle without converging. In contrast, the iterates of the prediction method fall into the saddle and converge.\nWhen the conventional gradient method (2) is applied to the linear problem (6), the resulting iterations can be written\nxk+1 \u2212 xk \u03b1 = \u2212KT yk, y k+1 \u2212 yk \u03b1 = (\u03b2/\u03b1)Kxk+1.\nWhen the stepsize \u03b1 gets small, this behaves like a discretization of the differential equation\nx\u0307 = \u2212KT y (10) y\u0307 = \u03b2/\u03b1Kx (11)\nwhere x\u0307 and y\u0307 denote the derivatives of x and y with respect to time.\nThe differential equations (10,11) describe a harmonic oscillator. To see why, differentiate (10) and plug (11) into the result to get a differential equation in x alone\nx\u0308 = \u2212KT y\u0307 = \u2212\u03b2/\u03b1KTKx. (12) We can decompose this into a system of independent single-variable problems by considering the eigenvalue decomposition \u03b2/\u03b1KTK = U\u03a3UT . We now multiply both sides of (12) by UT , and make the change of variables z \u2190 UTx to get\nz\u0308 = \u2212\u03a3z. where \u03a3 is diagonal. This is the standard equation for undamped harmonic motion, and its solution is z = A cos(\u03a31/2t + \u03c6), where cos acts entry-wise, and the diagonal matrix A and vector \u03c6 are constants that depend only on the initialization. Changing back into the variable x, we get the solution\nx = UA cos(\u03a31/2t+ \u03c6).\nWe can see that, for small values of \u03b1 and \u03b2, the non-predictive algorithm (2) approximates an undamped harmonic motion, and the solutions orbit around the saddle without converging.\nThe prediction step (3) improves convergence because it produces damped harmonic motion that sinks into the saddle point. When applied to the linearized problem (6), the iterates of the predictive method (3) satisfy\nxk+1 \u2212 xk \u03b1 = \u2212KT yk yk+1 \u2212 yk \u03b1 = \u03b2/\u03b1K(xk+1 + xk+1 \u2212 xk) = \u03b2/\u03b1Kxk+1 + \u03b2Kx k+1 \u2212 xk \u03b1 .\nFor small \u03b1, this approximates the dynamical system\nx\u0307 = \u2212KT y (13) y\u0307 = \u03b2/\u03b1K(x+ \u03b1x\u0307). (14)\nLike before, we differentiate (13) and use (14) to obtain\nx\u0308 = \u2212KT y\u0307 = \u2212\u03b2/\u03b1KT (Kx+ \u03b1Ax\u0307) = \u2212\u03b2/\u03b1KTKx\u2212 \u03b2/KTKx\u0307. (15)\nFinally, multiply both sides by UT and perform the change of variables z \u2190 UTx to get z\u0308 = \u2212\u03a3z \u2212 \u03b1\u03a3z\u0307.\nThis equation describes a damped harmonic motion. The solutions have the form z(t) = A exp(\u2212 t\u03b12 \u221a \u03a3) sin(t \u221a (1\u2212 \u03b12/4)\u03a3 + \u03c6). Changing back to the variable x, we see that the iterates of the original method satisfy\nx(t) = UA exp(\u2212 t\u03b1 2\n\u221a \u03a3) sin(t \u221a (1\u2212 \u03b12/4)\u03a3 + \u03c6).\nwhere A and \u03c6 depend on the initialization.\nFrom this analysis, we see that for small constant \u03b1 the orbits of the lookahead method converge into the saddle point, and the error decays exponentially fast."}, {"heading": "A Proof of Theorem 1", "text": "Assume the optimal solution (u?, v?) exists, then L\u2032u(u?, v) = L\u2032v(u, v?) = 0. In the following proofs, we use gu(u, v), gv(u, v) to represent the stochastic approximation of gradients, where E[gu(u, v)] = L\u2032u(u, v), E[gv(u, v)] = L\u2032v(u, v). We show the convergence of the proposed stochastic primal-dual gradients for the primal-dual gap P (uk, vk) = L(uk, v?) \u2212 L(u?, vk). We prove the O(1/ \u221a k) convergence rate in Theorem 1 by using Lemma 1 and Lemma 2, which present the contraction of primal and dual updates, respectively.\nLemma 1. Suppose L(u, v) is convex in u and E[\u2016gu(u, v)\u20162] \u2264 G2u, we have\nE[L(uk, vk)]\u2212 E[L(u?, vk)] \u2264 1 2\u03b1k\n( E[\u2016uk \u2212 u?\u20162]\u2212 E[\u2016uk+1 \u2212 u?\u20162] ) + \u03b1k 2 G2u (16)\nProof. Use primal update in (3), we have\n\u2016uk+1 \u2212 u?\u20162 = \u2016uk \u2212 \u03b1k gu(uk, vk)\u2212 u?\u20162 (17) = \u2016uk \u2212 u?\u20162 \u2212 2\u03b1k \u3008gu(uk, vk), uk \u2212 u?\u3009+ \u03b12k \u2016gu(uk, vk)\u20162. (18)\nTake expectation on both side of the equation, substitute with E[gu(u, v)] = L\u2032u(u, v) and apply E[\u2016g2u(u, v)\u2016] \u2264 G2u to get\nE[\u2016uk+1 \u2212 u?\u20162] \u2264 E[\u2016uk \u2212 u?\u20162]\u2212 2\u03b1k E[\u3008L\u2032u(uk, vk), uk \u2212 u?\u3009] + \u03b12kG2u. (19) Since L(u, v) is convex in u, we have\n\u3008L\u2032u(uk, vk), uk \u2212 u?\u3009 \u2265 L(uk, vk)\u2212 L(u?, vk). (20) (16) is proved by combining (19) and (20).\nLemma 2. Suppose L(u, v) is concave in v and has Lipschitz gradients, \u2016L\u2032v(u1, v)\u2212L\u2032v(u2, v)\u2016 \u2264 Lv\u2016u1 \u2212 u2\u2016; and bounded variance, E[\u2016gu(u, v)\u20162] \u2264 G2u, E[\u2016gv(u, v)\u20162] \u2264 G2v; and E[\u2016vk \u2212 v?\u20162] \u2264 D2v , we have\nE[L(uk, v?)]\u2212 E[L(uk, vk)] \u2264 1\n2\u03b2k\n( E[\u2016vk \u2212 v?\u20162]\u2212 E[\u2016vk+1 \u2212 v?\u20162] ) + \u03b2k 2 G2v + \u03b1kLv (G 2 u +D 2 v).\n(21)\nProof. From the dual update in (3), we have \u2016vk+1 \u2212 v?\u20162 = \u2016vk + \u03b2k gv(u\u0304k+1, vk)\u2212 v?\u20162 (22)\n= \u2016vk \u2212 v?\u20162 + 2\u03b2k \u3008gv(u\u0304k+1, vk), vk \u2212 v?\u3009+ \u03b22k \u2016gv(u\u0304k+1, vk)\u20162. (23) Take expectation on both sides of the equation, substitute E[gv(u, v)] = L\u2032v(u, v), and apply E[\u2016g2v(u, v)\u2016] \u2264 G2v to get E[\u2016vk+1 \u2212 v?\u20162] \u2264 E[\u2016vk \u2212 v?\u20162] + 2\u03b2k E[\u3008L\u2032v(u\u0304k+1, vk), vk \u2212 v?\u3009] + \u03b22k G2v. (24) Reorganize (24) to get\nE[\u2016vk+1 \u2212 v?\u20162]\u2212 E[\u2016vk \u2212 v?\u20162]\u2212 \u03b22k G2v \u2264 2\u03b2k E[\u3008L\u2032v(u\u0304k+1, vk), vk \u2212 v?\u3009]. (25) The right hand side of (25) can be represented as\nE[\u3008L\u2032v(u\u0304k+1, vk), uk \u2212 v?\u3009] (26) =E[\u3008L\u2032v(u\u0304k+1, vk)\u2212 L\u2032v(uk, vk) + L\u2032v(uk, vk), vk \u2212 v?\u3009] (27) =E[\u3008L\u2032v(u\u0304k+1, vk)\u2212 L\u2032v(uk, vk), vk \u2212 v?\u3009] + E[\u3008L\u2032v(uk, vk), vk \u2212 v?\u3009], (28)\nwhere E[\u3008L\u2032v(u\u0304k+1, vk)\u2212 L\u2032v(uk, vk), vk \u2212 v?\u3009] (29) \u2264E[\u2016L\u2032v(u\u0304k+1, vk)\u2212 L\u2032v(uk, vk)\u2016 \u2016vk \u2212 v?\u2016] (30) \u2264E[Lv \u2016u\u0304k+1 \u2212 uk\u2016 \u2016vk \u2212 v?\u2016] (31) =E[2Ly \u2016uk+1 \u2212 uk\u2016 \u2016vk \u2212 v?\u2016] (32) =E[2Ly \u2016\u03b1kgu(uk, vk)\u2016 \u2016vk \u2212 v?\u2016] (33) \u2264Ly\u03b1k E[ \u2016gu(uk, vk)\u20162 + \u2016vk \u2212 v?\u20162] (34) \u2264Ly\u03b1k (G2u +D2v). (35) Lipschitz smoothness is used for (31); the prediction step in (3) is used for (32); the primal update in (3) is used for (33); bounded assumptions are used for (35).\nSince L(u, v) is concave in v, we have \u3008L\u2032v(uk, vk), vk \u2212 v?\u3009 \u2264 L(uk, vk)\u2212 L(uk, v?). (36)\nCombine equations (25, 28, 35 to get36) 1\n2\u03b2k\n( E[\u2016vk+1 \u2212 v?\u20162]\u2212 E[\u2016vk \u2212 v?\u20162] ) \u2212 \u03b2k\n2 G2v\n\u2264 Lv\u03b1k (G2u +D2v) + E[L(uk, vk)]\u2212 E[L(uk, v?)]. (37)\nRearrange the order of (37) to achieve (21).\nWe now present the proof of Theorem 1.\nProof. Combining (16) and (21) in the Lemmas, the primal-dual gap P (uk, vk) = L(uk, v?) \u2212 L(u?, vk) satisfies, E[P (uk, vk)] \u2264 1\n2\u03b1k\n( E[\u2016uk \u2212 u?\u20162]\u2212 E[\u2016uk+1 \u2212 u?\u20162] ) + \u03b1k 2 G2u\n+ 1\n2\u03b2k\n( E[\u2016vk \u2212 v?\u20162]\u2212 E[\u2016vk+1 \u2212 v?\u20162] ) + \u03b2k 2 G2v + \u03b1kLv (G 2 u +D 2 v). (38)\nAccumulate (38) from k = 1, . . . , l to obtain l\u2211\nk=1\nE[P (uk, vk)] \u2264\n1\n2\u03b11 E[\u2016u1 \u2212 u?\u20162] + l\u2211 k=2 ( 1 2\u03b1k \u2212 1 2\u03b1k\u22121 )E[\u2016uk \u2212 u?\u20162] + l\u2211 k=1 \u03b1k( G2u 2 + LvG 2 u + LvD 2 v)\n+ 1\n2\u03b21 E[\u2016v1 \u2212 v?\u20162] + l\u2211 k=2 ( 1 2\u03b2k \u2212 1 2\u03b2k\u22121 )E[\u2016vk \u2212 v?\u20162] + l\u2211 k=1 \u03b2k G2v 2 .\n(39)\nAssume E[||uk \u2212 u?\u20162] \u2264 D2u, E[||vk \u2212 v?\u20162] \u2264 D2v are bounded, we have l\u2211\nk=1\nE[P (uk, vk)] \u2264 1 2\u03b11 D2u + l\u2211 k=2 ( 1 2\u03b1k \u2212 1 2\u03b1k\u22121 )D2u + l\u2211 k=1 \u03b1k( G2u 2 + LvG 2 u + LvD 2 v)\n+ 1\n2\u03b21 D2v + l\u2211 k=2 ( 1 2\u03b2k \u2212 1 2\u03b2k\u22121 )D2v + l\u2211 k=1 \u03b2k G2v 2 .\n(40)\nSince \u03b1k, \u03b2k are decreasing and \u2211l k=1 \u03b1k \u2264 C\u03b1 \u221a l + 1, \u2211l k=1 \u03b2k \u2264 C\u03b2 \u221a l + 1, we have\nl\u2211 k=1 E[P (uk, vk)] \u2264 \u221a l 2 ( D2u C\u03b1 + D2v C\u03b2 ) + \u221a l + 1 ( C\u03b1G 2 u 2 + C\u03b2LvG 2 u + C\u03b1LvD 2 v + C\u03b2G 2 v 2 ) (41)\nFor u\u0302l = 1l \u2211l k=1 u k, v\u0302l = 1l \u2211l k=1 v\nk, because L(u, v) is convex-concave, we have E[P (u\u0302l, v\u0302l)] = E[L(u\u0302l, v?)\u2212 L(u?, v\u0302l)] (42)\n\u2264 E[ 1 l l\u2211 k=1 (L(uk, v?)\u2212 L(u?, vk))] (43)\n= 1\nl l\u2211 k=1 E[L(uk, v?)\u2212 L(u?, vk)] (44)\n= 1\nl l\u2211 k=1 E[P (uk, vk)]. (45)\nCombine (41) and (45) to prove\nE[P (x\u0302l, y\u0302l)] \u2264 1 2 \u221a l ( D2u C\u03b1 + D2v C\u03b2 ) + \u221a l + 1 l ( C\u03b1G 2 u 2 + C\u03b1LvG 2 u + C\u03b1LvD 2 v + C\u03b2G 2 v 2 ) . (46)"}, {"heading": "B MNIST Toy example", "text": "Experimental details: We consider a classic MNIST digits dataset [22] consisting of 60,000 training images and 10,000 testing images each of size 28\u00d7 28. For simplicity, let us consider a task (T1) of classifying into odd and even numbered images. Let say, that\u223c 50% of data instances were corrupted using salt and pepper noise of probability 0.2 and this distortion process was biased. Specifically, only 30% of even numbered images were distorted as against the 70% of odd-numbered images. We have observed that any feature representation network \u03b8f trained using the binary classification loss function for task T1 has noise bias also encoded within it. This was verified by training an independent noise classifier on the learned features. This lead us to design of simple adversarial network to \u201cunlearn\u201d the noise bias from the feature learning pipeline. We formulate this using the minimax objective in (5).\nIn our model, Ld is a softmax loss for the task (T2) of classifying whether the input sample is noisy or not. Ly is a softmax loss for task T1 and \u03bb = 1. A LeNet network [22] is considered for training on task T1 while a two-layer MLP is used for training on task T2. LeNet consist of two convolutional (conv) layers followed by two fully connected (FC) layers at the top. The parameters of conv layers form \u03b8f while that of FC and MLP layers forms \u03b8y and \u03b8d respectively. We train the network in three stages. Following the training on task T1, \u03b8f were fixed and MLP is trained independently on task T2. The default learning schedule of LeNet implementation in Caffe [18] were followed for both the tasks. The total training iterations on each task were set to 10000. After pretraining, the whole network is jointly finetuned using the adversarial approach. (5) is alternatively minimized w.r.t. \u03b8f , \u03b8y and maximize w.r.t. \u03b8d. The predictive step were only used during the finetuning phase.\nOur finding is summarized in 3. In addition, Figure 7 provides neck-to-neck comparison of two popular solvers Adam and SGD using predictive step. Not surprisingly, the Adam solver shows\nrelatively better performance and convergence even with an additional predictive step. This also suggests that the default hyper-parameter for the Adam solver can be retained and utilized for training this networks without resorting to any further hyper-parameter tuning (as it is currently in practice)."}, {"heading": "C Domain Adaptation", "text": "Experimental details: To evaluate domain adaptation task, we consider the OFFICE dataset [33]. OFFICE is a small scale dataset consisting of images collected from three distinct domains: AMAZON, DSLR and WEBCAM. For such a small scale dataset, it is non-trivial to learn features from images of a single domain. For instance, consider the largest subset AMAZON, which contains only 2,817 labeled images spread across 31 different categories. However, one can leverage the power of domain adaptation to improve cross domain accuracy. We follow the protocol listed in [12] and the same network architecture is used. Caffe [18] is used for implementation. The training procedure from [12] is kept intact except for the additional prediction step. In table 1 comparisons are drawn with respect to target domain accuracy on three pairs of source-target domain tasks. The test accuracy is reported at the end of 50,000 training iterations."}, {"heading": "D Fair Classifier", "text": "Experimental details: The \u201cAdult\u201d dataset from the UCI machine learning repository is used, which consists of census data from \u223c 45, 000 people. The task is to classify whether a person earns \u2265 $50k/year. The person\u2019s gender is chosen to be the sensitive variable. We binarize all the category attributes, giving us a total of 102 input features per sample. We randomly split data into 35,000 samples for training, 5000 for validation and 5000 for testing. The result reported here is an average over five such random splits."}, {"heading": "E Generative Adversarial Networks", "text": "Architecture details: The DCGAN architecture utilized for our experiments on Cifar-10 is listed in Table 2, 3.\nToy Dataset: To illustrate the advantage of prediction method, we experiment on simple gan architecture with fully connected layers using the toy dataset. The constructed toy example and its architecture is inspired by the one presented in [27]. The two dimensional data is sampled from the mixture of eight gaussians with their means equally spaced around the unit circle centered at (0, 0). The standard deviation of each gaussian is set at 0.01. The two dimensional latent vector z is sampled from the multivariate gaussian. The generator and discriminator network consists of two fully connected hidden layers, each with 128 hidden units and tanh activations. The final layer of the generator has linear activation while that of discriminator has sigmoid activation. The solver optimizes both the discriminator and the generator network using the objective in (4). We use adam\nsolver with their default parameter and with input batch of size 512. The generated two dimensional samples using fixed seed is plotted in figure (8). The straightforward utilization of adam solver fails to construct all modes of the underlying dataset.\nAdditional Results:"}], "references": [{"title": "Learning to protect communications with adversarial neural cryptography", "author": ["Mart\u00edn Abadi", "David G Andersen"], "venue": "arXiv preprint arXiv:1610.06918,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Neural photo editing with introspective adversarial networks", "author": ["Andrew Brock", "Theodore Lim", "JM Ritchie", "Nick Weston"], "venue": "arXiv preprint arXiv:1609.07093,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "A first-order primal-dual algorithm for convex problems with applications to imaging", "author": ["Antonin Chambolle", "Thomas Pock"], "venue": "Journal of Mathematical Imaging and Vision,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Mode regularized generative adversarial networks", "author": ["Tong Che", "Yanran Li", "Athul Paul Jacob", "Yoshua Bengio", "Wenjie Li"], "venue": "arXiv preprint arXiv:1612.02136,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Optimal primal-dual methods for a class of saddle point problems", "author": ["Yunmei Chen", "Guanghui Lan", "Yuyuan Ouyang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Randomized first-order methods for saddle point optimization", "author": ["Cong Dang", "Guanghui Lan"], "venue": "arXiv preprint arXiv:1409.8625,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily Denton", "Soumith Chintala", "Arthur Szlam", "Rob Fergus"], "venue": "In Proceedings of the 28th International Conference on Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Stochastic variance reduction methods for policy evaluation", "author": ["Simon S Du", "Jianshu Chen", "Lihong Li", "Lin Xiao", "Dengyong Zhou"], "venue": "arXiv preprint arXiv:1702.07944,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Censoring representations with an adversary", "author": ["Harrison Edwards", "Amos Storkey"], "venue": "arXiv preprint arXiv:1511.05897,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A general framework for a class of first order primal-dual algorithms for tv minimization", "author": ["Ernie Esser", "Xiaoqun Zhang", "Tony Chan"], "venue": "UCLA CAM Report,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Yaroslav Ganin", "Victor Lempitsky"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Adaptive primal-dual splitting methods for statistical learning and image processing", "author": ["Tom Goldstein", "Min Li", "Xiaoming Yuan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Improved training of wasserstein gans", "author": ["Ishaan Gulrajani", "Faruk Ahmed", "Martin Arjovsky", "Vincent Dumoulin", "Aaron Courville"], "venue": "arXiv preprint arXiv:1704.00028,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "Model-free imitation learning with policy optimization", "author": ["Jonathan Ho", "Jayesh Gupta", "Stefano Ermon"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Image-to-image translation with conditional adversarial networks", "author": ["Phillip Isola", "Jun-Yan Zhu", "Tinghui Zhou", "Alexei A Efros"], "venue": "arXiv preprint arXiv:1611.07004,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": "Technical report,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "An optimal randomized incremental gradient method", "author": ["Guanghui Lan", "Yi Zhou"], "venue": "arXiv preprint arXiv:1507.02000,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Generative moment matching networks", "author": ["Yujia Li", "Kevin Swersky", "Richard S Zemel"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "The variational fair autoencoder", "author": ["Christos Louizos", "Kevin Swersky", "Yujia Li", "Max Welling", "Richard Zemel"], "venue": "arXiv preprint arXiv:1511.00830,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Disentangling factors of variation in deep representation using adversarial training", "author": ["Michael F Mathieu", "Junbo Jake Zhao", "Junbo Zhao", "Aditya Ramesh", "Pablo Sprechmann", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Unrolled generative adversarial networks", "author": ["Luke Metz", "Ben Poole", "David Pfau", "Jascha Sohl-Dickstein"], "venue": "arXiv preprint arXiv:1611.02163,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Conditional generative adversarial nets", "author": ["Mehdi Mirza", "Simon Osindero"], "venue": "arXiv preprint arXiv:1411.1784,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["Arkadi Nemirovski", "Anatoli Juditsky", "Guanghui Lan", "Alexander Shapiro"], "venue": "SIAM Journal on optimization,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Stochastic variance reduction methods for saddle-point problems", "author": ["Balamurugan Palaniappan", "Francis Bach"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "On stochastic primal-dual hybrid gradient approach for compositely regularized minimization", "author": ["Linbo Qiao", "Tianyi Lin", "Yu-Gang Jiang", "Fan Yang", "Wei Liu", "Xicheng Lu"], "venue": "In ECAI 2016: 22nd European Conference on Artificial Intelligence,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Adapting visual category models to new domains", "author": ["Kate Saenko", "Brian Kulis", "Mario Fritz", "Trevor Darrell"], "venue": "Computer Vision\u2013ECCV", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Stochastic primal dual coordinate method with non-uniform sampling based on optimality violations", "author": ["Atsushi Shibagaki", "Ichiro Takeuchi"], "venue": "arXiv preprint arXiv:1703.07056,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2017}, {"title": "Learning from simulated and unsupervised images through adversarial training", "author": ["Ashish Shrivastava", "Tomas Pfister", "Oncel Tuzel", "Josh Susskind", "Wenda Wang", "Russ Webb"], "venue": "arXiv preprint arXiv:1612.07828,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Unsupervised cross-domain image generation", "author": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "venue": "arXiv preprint arXiv:1611.02200,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Adversarial discriminative domain adaptation", "author": ["Eric Tzeng", "Judy Hoffman", "Kate Saenko", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1702.05464,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2017}, {"title": "Exploiting strong convexity from data with primal-dual first-order algorithms", "author": ["Jialei Wang", "Lin Xiao"], "venue": "arXiv preprint arXiv:1703.02624,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2017}, {"title": "An online primal-dual method for discounted markov decision processes", "author": ["Mengdi Wang", "Yichen Chen"], "venue": "In Decision and Control (CDC),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Generative image modeling using style and structure adversarial networks", "author": ["Xiaolong Wang", "Abhinav Gupta"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Doubly stochastic primal-dual coordinate method for empirical risk minimization and bilinear saddle-point problem", "author": ["Adams Wei Yu", "Qihang Lin", "Tianbao Yang"], "venue": "arXiv preprint arXiv:1508.03390,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks", "author": ["Han Zhang", "Tao Xu", "Hongsheng Li", "Shaoting Zhang", "Xiaolei Huang", "Xiaogang Wang", "Dimitris Metaxas"], "venue": "arXiv preprint arXiv:1612.03242,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "Stochastic primal-dual coordinate method for regularized empirical risk minimization", "author": ["Yuchen Zhang", "Xiao Lin"], "venue": "In ICML,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Energy-based generative adversarial network", "author": ["Junbo Zhao", "Michael Mathieu", "Yann LeCun"], "venue": "arXiv preprint arXiv:1609.03126,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}, {"title": "An efficient primal-dual hybrid gradient algorithm for total variation image restoration", "author": ["Mingqiang Zhu", "Tony Chan"], "venue": "UCLA CAM Report,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2008}, {"title": "Adaptive stochastic primal-dual coordinate descent for separable saddle point problems", "author": ["Zhanxing Zhu", "Amos J Storkey"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Stochastic parallel block coordinate descent for large-scale saddle point problems", "author": ["Zhanxing Zhu", "Amos J Storkey"], "venue": "In Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2016}], "referenceMentions": [{"referenceID": 40, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 101, "endOffset": 109}, {"referenceID": 38, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 101, "endOffset": 109}, {"referenceID": 1, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 126, "endOffset": 141}, {"referenceID": 34, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 126, "endOffset": 141}, {"referenceID": 38, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 126, "endOffset": 141}, {"referenceID": 15, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 126, "endOffset": 141}, {"referenceID": 34, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 161, "endOffset": 177}, {"referenceID": 35, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 161, "endOffset": 177}, {"referenceID": 10, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 161, "endOffset": 177}, {"referenceID": 33, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 161, "endOffset": 177}, {"referenceID": 14, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 198, "endOffset": 202}, {"referenceID": 8, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 212, "endOffset": 219}, {"referenceID": 0, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 212, "endOffset": 219}, {"referenceID": 23, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 241, "endOffset": 249}, {"referenceID": 8, "context": "Adversarial networks play an important role in a variety of applications, including image generation [43, 41], style transfer [3, 37, 41, 17], domain adaptation [37, 38, 12, 36], imitation learning [16], privacy [10, 1], fair representation [26, 10], etc.", "startOffset": 241, "endOffset": 249}, {"referenceID": 12, "context": "One particularly motivating application of adversarial nets is their ability to form generative models, as opposed to the classical discriminative models [14, 32, 8, 28].", "startOffset": 154, "endOffset": 169}, {"referenceID": 29, "context": "One particularly motivating application of adversarial nets is their ability to form generative models, as opposed to the classical discriminative models [14, 32, 8, 28].", "startOffset": 154, "endOffset": 169}, {"referenceID": 6, "context": "One particularly motivating application of adversarial nets is their ability to form generative models, as opposed to the classical discriminative models [14, 32, 8, 28].", "startOffset": 154, "endOffset": 169}, {"referenceID": 25, "context": "One particularly motivating application of adversarial nets is their ability to form generative models, as opposed to the classical discriminative models [14, 32, 8, 28].", "startOffset": 154, "endOffset": 169}, {"referenceID": 12, "context": "As a result, many authors suggest using early stopping, gradients/weight clipping [2], or specialized objective functions [14, 45, 2] to maintain stability.", "startOffset": 122, "endOffset": 133}, {"referenceID": 42, "context": "As a result, many authors suggest using early stopping, gradients/weight clipping [2], or specialized objective functions [14, 45, 2] to maintain stability.", "startOffset": 122, "endOffset": 133}, {"referenceID": 17, "context": "In practice, the gradient updates are often performed by an automated solver, such as the Adam optimizer [19], and include momentum updates.", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "Generative Adversarial Networks (GANs) fit a generative model to a dataset using a game in which a generative model competes against a discriminator [14].", "startOffset": 149, "endOffset": 153}, {"referenceID": 12, "context": "Initially, [14] proposed using f(x) = log(x).", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "Domain Adversarial Networks (DANs) [25, 12, 10] take data collected from a \u201csource\u201d domain, and extract a feature representation that can be used to train models that generalize to another \u201ctarget\u201d domain.", "startOffset": 35, "endOffset": 47}, {"referenceID": 8, "context": "Domain Adversarial Networks (DANs) [25, 12, 10] take data collected from a \u201csource\u201d domain, and extract a feature representation that can be used to train models that generalize to another \u201ctarget\u201d domain.", "startOffset": 35, "endOffset": 47}, {"referenceID": 10, "context": "For example, in the domain adversarial neural network (DANN [12]), a set of feature layers maps data points into an embedded feature space, and a classifier is trained on these embedded features.", "startOffset": 60, "endOffset": 64}, {"referenceID": 21, "context": "This led researchers to explore multiple directions for stabilizing GANs; either by adding regularization terms [2, 23, 5, 45], a myriad of training \u201chacks\u201d [34, 15], re-engineering network architectures [45], and designing different solvers [27].", "startOffset": 112, "endOffset": 126}, {"referenceID": 3, "context": "This led researchers to explore multiple directions for stabilizing GANs; either by adding regularization terms [2, 23, 5, 45], a myriad of training \u201chacks\u201d [34, 15], re-engineering network architectures [45], and designing different solvers [27].", "startOffset": 112, "endOffset": 126}, {"referenceID": 42, "context": "This led researchers to explore multiple directions for stabilizing GANs; either by adding regularization terms [2, 23, 5, 45], a myriad of training \u201chacks\u201d [34, 15], re-engineering network architectures [45], and designing different solvers [27].", "startOffset": 112, "endOffset": 126}, {"referenceID": 31, "context": "This led researchers to explore multiple directions for stabilizing GANs; either by adding regularization terms [2, 23, 5, 45], a myriad of training \u201chacks\u201d [34, 15], re-engineering network architectures [45], and designing different solvers [27].", "startOffset": 157, "endOffset": 165}, {"referenceID": 13, "context": "This led researchers to explore multiple directions for stabilizing GANs; either by adding regularization terms [2, 23, 5, 45], a myriad of training \u201chacks\u201d [34, 15], re-engineering network architectures [45], and designing different solvers [27].", "startOffset": 157, "endOffset": 165}, {"referenceID": 42, "context": "This led researchers to explore multiple directions for stabilizing GANs; either by adding regularization terms [2, 23, 5, 45], a myriad of training \u201chacks\u201d [34, 15], re-engineering network architectures [45], and designing different solvers [27].", "startOffset": 204, "endOffset": 208}, {"referenceID": 24, "context": "This led researchers to explore multiple directions for stabilizing GANs; either by adding regularization terms [2, 23, 5, 45], a myriad of training \u201chacks\u201d [34, 15], re-engineering network architectures [45], and designing different solvers [27].", "startOffset": 242, "endOffset": 246}, {"referenceID": 24, "context": "The unrolled GAN [27] is a new solver that can stabilize training at the cost of more expensive gradient computations.", "startOffset": 17, "endOffset": 21}, {"referenceID": 43, "context": "One popular solver is the primal-dual hybrid gradient (PDHG) method [46, 11], which has been popularized by Chambolle and Pock [4], and has been successfully applied to a range of machine learning and statistical estimation problems [13].", "startOffset": 68, "endOffset": 76}, {"referenceID": 9, "context": "One popular solver is the primal-dual hybrid gradient (PDHG) method [46, 11], which has been popularized by Chambolle and Pock [4], and has been successfully applied to a range of machine learning and statistical estimation problems [13].", "startOffset": 68, "endOffset": 76}, {"referenceID": 2, "context": "One popular solver is the primal-dual hybrid gradient (PDHG) method [46, 11], which has been popularized by Chambolle and Pock [4], and has been successfully applied to a range of machine learning and statistical estimation problems [13].", "startOffset": 127, "endOffset": 130}, {"referenceID": 11, "context": "One popular solver is the primal-dual hybrid gradient (PDHG) method [46, 11], which has been popularized by Chambolle and Pock [4], and has been successfully applied to a range of machine learning and statistical estimation problems [13].", "startOffset": 233, "endOffset": 237}, {"referenceID": 5, "context": "Stochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31].", "startOffset": 126, "endOffset": 153}, {"referenceID": 19, "context": "Stochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31].", "startOffset": 126, "endOffset": 153}, {"referenceID": 41, "context": "Stochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31].", "startOffset": 126, "endOffset": 153}, {"referenceID": 44, "context": "Stochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31].", "startOffset": 126, "endOffset": 153}, {"referenceID": 45, "context": "Stochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31].", "startOffset": 126, "endOffset": 153}, {"referenceID": 36, "context": "Stochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31].", "startOffset": 126, "endOffset": 153}, {"referenceID": 32, "context": "Stochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31].", "startOffset": 126, "endOffset": 153}, {"referenceID": 4, "context": "Stochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31].", "startOffset": 186, "endOffset": 193}, {"referenceID": 28, "context": "Stochastic methods for convex saddle-point problems can be roughly divided into two categories: stochastic coordinate descent [7, 21, 44, 47, 48, 39, 35] and stochastic gradient descent [6, 31].", "startOffset": 186, "endOffset": 193}, {"referenceID": 37, "context": "Similar optimization algorithms have been studied for reinforcement learning [40, 9].", "startOffset": 77, "endOffset": 84}, {"referenceID": 7, "context": "Similar optimization algorithms have been studied for reinforcement learning [40, 9].", "startOffset": 77, "endOffset": 84}, {"referenceID": 39, "context": "Recently, a \u201cdoubly\u201d stochastic method that randomizes both primal and dual updates was proposed for strongly convex bilinear saddle point problems [42].", "startOffset": 148, "endOffset": 152}, {"referenceID": 26, "context": "For general saddle point problems, \u201cdoubly\u201d stochastic gradient descent methods are discussed in [29, 30], in which primal and dual variables are updated simultaneously based on the previous iterates and the current gradients.", "startOffset": 97, "endOffset": 105}, {"referenceID": 27, "context": "For general saddle point problems, \u201cdoubly\u201d stochastic gradient descent methods are discussed in [29, 30], in which primal and dual variables are updated simultaneously based on the previous iterates and the current gradients.", "startOffset": 97, "endOffset": 105}, {"referenceID": 20, "context": "When we train a LeNet network [22] on this problem, we find that the network encodes and uses information about the noise; when a noise vs no-noise classifier is trained on the deep features generated by LeNet, it gets 100% accuracy.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "The classifier and discriminator were both pre-trained using the default LeNet implementation in Caffe [18].", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "2 Generative Adversarial Networks Next, we test the efficacy and stability of our proposed predictive step on generative adversarial networks (GAN), which are formulated as saddle point problems (4) and are popularly solved using a heuristic approach [14].", "startOffset": 251, "endOffset": 255}, {"referenceID": 18, "context": "We consider an image modeling task using CIFAR-10 [20] on the recently popular convolutional GAN architecture, DCGAN [32].", "startOffset": 50, "endOffset": 54}, {"referenceID": 29, "context": "We consider an image modeling task using CIFAR-10 [20] on the recently popular convolutional GAN architecture, DCGAN [32].", "startOffset": 117, "endOffset": 121}, {"referenceID": 24, "context": "We compare our predictive method with that of DCGAN and the unrolled GAN [27] using the training protocol described in [32].", "startOffset": 73, "endOffset": 77}, {"referenceID": 29, "context": "We compare our predictive method with that of DCGAN and the unrolled GAN [27] using the training protocol described in [32].", "startOffset": 119, "endOffset": 123}, {"referenceID": 29, "context": "As observed in [32], the standard and unrolled solvers are very unstable and collapse at this higher rate.", "startOffset": 15, "endOffset": 19}, {"referenceID": 30, "context": "3 Domain Adaptation We consider the domain adaptation task [33, 12, 38] wherein the representation learned using the source domain samples is altered so that it can also generalize to samples from the target distribution.", "startOffset": 59, "endOffset": 71}, {"referenceID": 10, "context": "3 Domain Adaptation We consider the domain adaptation task [33, 12, 38] wherein the representation learned using the source domain samples is altered so that it can also generalize to samples from the target distribution.", "startOffset": 59, "endOffset": 71}, {"referenceID": 35, "context": "3 Domain Adaptation We consider the domain adaptation task [33, 12, 38] wherein the representation learned using the source domain samples is altered so that it can also generalize to samples from the target distribution.", "startOffset": 59, "endOffset": 71}, {"referenceID": 10, "context": "We use the problem setup and hyper-parameters as described in [12] using the OFFICE dataset [33] (experimental details are shared in the Supplementary Material).", "startOffset": 62, "endOffset": 66}, {"referenceID": 30, "context": "We use the problem setup and hyper-parameters as described in [12] using the OFFICE dataset [33] (experimental details are shared in the Supplementary Material).", "startOffset": 92, "endOffset": 96}, {"referenceID": 10, "context": "Method Source AMAZON WEBCAM DSLR WEBCAM AMAZON DSLR Target WEBCAM AMAZON WEBCAM DSLR DSLR AMAZON DANN [12] 73.", "startOffset": 102, "endOffset": 106}, {"referenceID": 23, "context": "4 Fair Classifier Finally, we consider a task of learning fair feature representations [26, 10, 24] such that the final learned classifier does not discriminate with respect to a sensitive variable.", "startOffset": 87, "endOffset": 99}, {"referenceID": 8, "context": "4 Fair Classifier Finally, we consider a task of learning fair feature representations [26, 10, 24] such that the final learned classifier does not discriminate with respect to a sensitive variable.", "startOffset": 87, "endOffset": 99}, {"referenceID": 22, "context": "4 Fair Classifier Finally, we consider a task of learning fair feature representations [26, 10, 24] such that the final learned classifier does not discriminate with respect to a sensitive variable.", "startOffset": 87, "endOffset": 99}, {"referenceID": 8, "context": "As proposed in [10] one way to measure fairness is using discrimination,", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "Similar to the domain adaptation task, the learning of each classifier can be formulated as a minimax problem in (5) [10, 26].", "startOffset": 117, "endOffset": 125}, {"referenceID": 23, "context": "Similar to the domain adaptation task, the learning of each classifier can be formulated as a minimax problem in (5) [10, 26].", "startOffset": 117, "endOffset": 125}, {"referenceID": 8, "context": "To demonstrate the advantage of using prediction for model selection, we follow the protocol developed in [10].", "startOffset": 106, "endOffset": 110}], "year": 2017, "abstractText": "Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \u201ccollapse,\u201d and enables faster training with larger learning rates.", "creator": "TeX"}}}