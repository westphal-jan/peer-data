{"id": "1705.08168", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Look, Listen and Learn", "abstract": "armaments We comdirect consider trochophore the us-acan question: delgaudio what 43.18 can be learnt remulla by bam looking at afspc and socio-technical listening to behinds a large saujana amount 9-iron of muons unlabelled rasna videos? e23 There 40-ton is china a valuable, kutschera but so bommai far untapped, source of information 80-something contained balikesir in cheb the polygons video itself - - mcgonagall the ellimist correspondence suomen between traipse the visual uttarapatha and the proni audio l.s. streams, shlaes and bc4 we introduce linemate a fungi novel \" tanny Audio - morelli Visual contoured Correspondence \" learning hinglaj task that third-wave makes use of this. nanoscale Training visual stephenie and audio networks tepes from scratch, 2,098 without ganymede any additional supervision ellenbrook other than hadeed the raw unconstrained videos cozma themselves, gharti is breaststroker shown hardball to self-discovery successfully tamara solve reincarnations this shared-time task, and, normandin more interestingly, malec\u00f3n result in good vision engineer-in-chief and audio herpen representations. These features?! set the jirov new state - corti of - epicrocis the - www.hbo.com art pangle on two sound classification 23.98 benchmarks, lowers and perform converge on par with famously the state - outgo of - the - tigerlily art self - supervised approaches l'union on ImageNet classification. We soufli also foreland demonstrate revenue-earning that 992 the overseers network is nisour able understaffing to ramkrishna localize tuyserkan objects low-carb in both modalities, as highs well zagazig as xylem perform 95/98 fine - grained glaspell recognition conspiracy tasks.", "histories": [["v1", "Tue, 23 May 2017 10:37:54 GMT  (6032kb,D)", "http://arxiv.org/abs/1705.08168v1", null], ["v2", "Tue, 1 Aug 2017 12:04:50 GMT  (5737kb,D)", "http://arxiv.org/abs/1705.08168v2", "Appears in: IEEE International Conference on Computer Vision (ICCV) 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["relja arandjelovi\\'c", "rew zisserman"], "accepted": false, "id": "1705.08168"}, "pdf": {"name": "1705.08168.pdf", "metadata": {"source": "META", "title": "Look, Listen and Learn", "authors": ["Relja Arandjelovi\u0107", "Andrew Zisserman"], "emails": ["relja@google.com", "zisserman@google.com"], "sections": [{"heading": "1. Introduction", "text": "Visual and audio events tend to occur together; not always but often: the movement of fingers and sound of the instrument when a piano, guitar or drum is played; lips moving and speech when talking; cars moving and engine noise when observing a street. The visual and audio events are concurrent in these cases because there is a common cause. In this paper we investigate whether we can use this simple observation to learn about the world both visually and aurally by simply watching and listening to videos.\nWe ask the question: what can be learnt by training visual and audio networks simultaneously to predict whether visual information (a video frame) corresponds or not to audio information (a sound snippet)? This is a looser requirement than that the visual and audio events occur in sync. It only requires that there is something in the image that correlates with something in the audio clip \u2013 a car present in the video frame, for instance, correlating with engine noise.\nOur motivation for this work is three fold: first, as in many recent self-supervision tasks [1, 6, 8, 18, 19, 22, 34,\n35], it is interesting to learn from a virtually infinite source of free supervision (video with visual and audio modes in this case) rather than requiring strong supervision; second, this is a possible source of supervision that an infant could use as their visual and audio capabilities develop; third, we want to know what can be learnt, and how well the networks are trained, for example in the performance of the visual and audio networks for other tasks.\nOf course, we are not the first to make the observation that visual and audio events co-occur, and to use their concurrence or correlation as supervision for training a network. In a series of recent and inspiring papers [2, 10, 20, 21], the group at MIT has investigated precisely this. However, their goal is always to train a single network for one of the modes, for example, train a visual network to generate sounds in [20, 21]; or train an audio network to correlate with visual outputs in [2, 10], where the visual networks are pre-trained and fixed and act as a teacher. In earlier, pre deep-learning, approaches the observation was used to beautiful effect in [13] showing \u201cpixels that sound\u201d (e.g. for a guitar) learnt using CCA. In contrast, we train both visual and audio networks and, somewhat surprisingly, show that this is beneficial \u2013 in that our performance improves substantially over that of [2] when trained on the same data.\nIn summary: our goal is to design a system that is able to learn both visual and audio semantic information in a completely unsupervised manner by simply looking at and listening to a large amount of unlabelled videos. To achieve this we introduce a novel Audio-Visual Correspondence (AVC) learning task that is used to train the two (visual and audio) networks from scratch. This task is described in section 2, together with the network architecture and training procedure. In section 3 we describe what semantic information has been learnt, and assess the performance of the audio and visual networks. We find, which we had not anticipated, that this task leads to quite fine grained visual and audio discrimination, e.g. into different instruments. In terms of quantitative performance, the audio network exceed those recently trained for audio recognition using visual supervision, and the visual network has similar performance to\n1\nar X\niv :1\n70 5.\n08 16\n8v 1\n[ cs\n.C V\n] 2\n3 M\nay 2\n01 7\nAudio-visual correspondence detector network\nthose trained for other, purely visual, self-supervision tasks. Furthermore, we show, as an added benefit, that we are able to localize the source of the audio event in the video frame (and also localize the corresponding regions of the sound source) using activation visualization.\nIn terms of prior work, the most closely related deep learning approach that we know of is \u2018SyncNet\u2019 in [5]. However, [5] is aimed at learning to synchronize lip-regions and speech for lip-reading, rather than the more general video and audio material considered here for learning semantic representations. More generally, the AVC task is a form of co-training [4], where there are two \u2018views\u2019 of the data, and each view provides complementary information. In our case the two views are visual and audio (and each can determine semantic information independently) but the method is entirely unsupervised, rather than the more common semi-supervised scenario in co-training."}, {"heading": "2. Audio-visual correspondence learning", "text": "The core idea is to use a valuable but so far untapped source of information contained in the video itself \u2013 the correspondence between visual and audio streams available by virtue of them appearing together at the same time in the same video. By seeing and hearing many examples of a person playing a violin and a dog barking, and never, or at least very infrequently, seeing a violin being played while hearing a dog bark and vice versa, it should be possible to conclude what a violin and a dog look and sound like, without ever being explicitly taught what is a violin or a dog.\nWe leverage this for learning by an audio-visual correspondence (AVC) task, illustrated in Figure 1. The AVC task is a simple binary classification task: given an example video frame and a short audio clip \u2013 decide whether they correspond to each other or not. The corresponding (positive) pairs are the ones that are taken at the same time from the same video, while mismatched (negative) pairs are extracted from different videos. The only way for a system to solve this task is if it learns to detect various semantic concepts in both the visual and the audio domain. Indeed, we\ndemonstrate in Section 3.5 that our network automatically learns relevant semantic concepts in both modalities.\nIt should be noted that the task is very difficult. The network is made to learn visual and audio features and concepts from scratch without ever seeing a single label. Furthermore, the AVC task itself is quite hard when done on completely unconstrained videos \u2013 videos can be very noisy, the audio source is not necessarily visible in the video (e.g. camera operator speaking, person narrating the video, sound source out of view or occluded, etc.), and the audio and visual content can be completely unrelated (e.g. edited videos with added music, very low volume sound, ambient sound such as wind dominating the audio track despite of other audio events being present, etc.). Nevertheless, the results in Section 3 show that our network is able to fairly successfully solve the AVC task, and in the process learn very good visual and audio representations."}, {"heading": "2.1. Network architecture", "text": "To tackle the AVC task, we propose the network structure shown in Figure 2. It has three distinct parts: the vision and the audio subnetworks which extract visual and audio features, respectively, and the fusion network which takes these features into account to produce the final decision on whether the visual and audio signals correspond. Here we describe the three parts in more detail. Vision subnetwork. The input to the vision subnetwork is a 224\u00d7224 colour image where pixel values are shifted and scaled such that they occupy the [\u22121, 1] range. We follow the VGG-network [29] design style, with 3 \u00d7 3 convolutional filters, and 2\u00d72 max-pooling layers with stride 2 and no padding. The network can be segmented into four blocks of conv+conv+pool layers such that inside each block the two conv layers have the same number of filters, while consecutive blocks have doubling filter numbers: 64, 128, 256 and 512. At the very end, max-pooling is performed across all spatial locations to produce a single 512-D feature vector. Each conv layer is followed by batch normalization [11] and a ReLU nonlinearity. Audio subnetwork. The input to the audio subnetwork is a 1 second sound clip converted into a log-spectrogram (more details are provided later in this section), which is thereafter treated as a greyscale 257\u00d7 199 image. The architecture of the audio subnetwork is identical to the vision one with the exception that input pixels are 1-D intensities instead of 3- D colours and therefore the conv1 1 filter sizes are 3\u00d7 smaller compared to the vision subnetwork. The final audio feature is also 512-D. Fusion network. The two 512-D visual and audio features are concatenated into a 1024-D vector which is passed through the fusion network to produce a 2-way classification output, namely, whether the vision and audio correspond or not. It consists of two fully connected layers, with\nReLU in between them, and the intermediate feature size of 128-D."}, {"heading": "2.2. Implementation details", "text": "Training data sampling. A non-corresponding frameaudio pair is compiled by randomly sampling two different videos and picking a random frame from one and a random 1 second audio clip from the other. A corresponding frameaudio pair is created by sampling a random video, picking a random frame in that video, and then picking a random 1 second audio clip that overlaps in time with the sampled frame. This provides additional training samples compared to simply sampling the 1 second audio with the frame at its mid-point. We use standard data augmentation techniques for images: each training image is uniformly scaled such\nthat the smallest dimension is equal to 256, followed by random cropping into 224 \u00d7 224, random horizontal flipping, and brightness and saturation jittering. Audio is only augmented by changing the volume up to 10% randomly but consistently across the sample. Log-spectrogram computation. The 1 second audio is resampled to 48 kHz, and a spectrogram is computed with window length of 0.01 seconds and a half-window overlap; this produces 199 windows with 257 frequency bands. The response map is passed through a logarithm and global scaling is applied such that values fit into the [0, 1] range, before feeding it into the audio subnetwork. Training procedure. We use the Adam optimizer [14], weight decay 10\u22125, and perform a grid search on the learning rate, although 10\u22124 usually works well. The network was trained on 16 GPUs in parallel with synchronous training implemented in TensorFlow, where each worker processed a 16-element batch, thus making the effective batch size of 256. For a training set of 400k 10 second videos, the network is trained for a week, during which it has seen 60M frame-audio pairs."}, {"heading": "3. Results and discussion", "text": "Our \u201clook, listen and learn\u201d network (L3-Net) approach is evaluated and examined in multiple ways. First, the performance of the network on the audio-visual correspondence task itself is investigated, and compared to supervised baselines. Second, the quality of the learnt visual and audio features is tested in a transfer learning setting, on visual and audio classification tasks. Finally, we perform a qualitative analysis of what the network has learnt. We start by introducing the datasets used for training."}, {"heading": "3.1. Datasets", "text": "Two video datasets are used for training the networks: Flickr-SoundNet and Kinetics-Sounds. Flickr-SoundNet [2]. This is a large unlabelled dataset of completely unconstrained videos from Flickr, compiled by searching for popular tags, but no tags or any sort of additional information apart from the videos themselves are used. It contains over 2 million videos but for practical reasons we use a random subset of 500k videos (400k training, 50k validation and 50k test) and only use the first 10 seconds of each video. This is the dataset that is used for training the L3-Net for the transfer learning experiments in Sections 3.3 and 3.4. Kinetics-Sounds. While our goal is to learn from completely unconstrained videos, having a labelled dataset is useful for quantitative evaluation. For this purpose we took a subset (much smaller than Flickr-SoundNet) of the Kinetics dataset [12], which contains YouTube videos manually annotated for human actions using Mechanical Turk,\nand cropped to 10 seconds around the action. The subset contains 19k 10 second video clips (15k training, 1.9k validation, 1.9k test) formed by filtering the Kinetics dataset for 34 human action classes, which have been chosen to be potentially manifested visually and aurally, such as playing various instruments (guitar, violin, xylophone, etc.), using tools (lawn mowing, shovelling snow, etc.), as well as performing miscellaneous actions (tap dancing, bowling, laughing, singing, blowing nose, etc.). Although this dataset is fairly clean by construction, it still contains considerable noise, e.g. the bowling action is often accompanied by loud music at the bowling alley, human voices (camera operators or video narrations) often masks the sound of interest, and many videos contain sound tracks that are completely unrelated to the visual content (e.g. music montage for a snow shovelling video)."}, {"heading": "3.2. Audio-visual correspondence", "text": "First we evaluate the performance of our method on the task it was trained to solve \u2013 deciding whether a frame and a 1 second audio clip correspond (Section 2). For the Kinetics-Sounds dataset which contains labelled videos, we also evaluate two supervised baselines in order to gauge how well the AVC training compares to supervised training. Supervised baselines. For both baselines we first train vision and audio networks independently on the action classification task, and then combine them in two different ways. The vision network has an identical feature extraction trunk as our vision subnetwork (Section 2.1), on top of which two fully connected layers are attached (sizes: 512\u00d7128 and 128\u00d734) to perform classification into the 34 Kinetics-Sounds classes. The audio classification network is constructed analogously. The direct combination baseline computes the audio-video correspondence score as the similarity of class score distributions of the two networks, computed as the scalar product between the 34-D network softmax outputs, and decides that audio and video are in correspondence if the score is larger than a threshold. The motivation behind this baseline is that if the vision network believes the frame contains a dog while the audio network is confident it hears a violin, then the (frame, audio) pair is unlikely to be in correspondence. The supervised pretraining baseline takes the feature extraction trunks from the two trained networks, assembles them into our network architecture by concatenating the features and adding two fully connected layers (Section 2.1). The weights of the feature extractors are frozen and the fully connected layers are trained on the AVC task in the same manner as our network. This is the strongest baseline as it directly corresponds to our method, but with features learnt in a fully supervised manner. Results and discussion. Table 1 shows the results on the AVC task. The L3-net achieves 74% and 78% on the two\ndatasets, where chance is 50%. It should be noted that the task itself is quite hard due to the unconstrained nature of the videos (Section 2), as well as due to the very local input data which lacks context \u2013 even humans find it hard to judge whether an isolated frame and an isolated single second of audio correspond; informal human tests indicated that humans are only a few percent better than the L3-Net. Furthermore, the supervised baselines do not beat the L3-Net as \u201csupervised pretraining\u201d performs on par with it, while \u201csupervised direct combination\u201d works significantly worse as, unlike \u201csupervised pretraining\u201d, it has not been trained for the AVC task."}, {"heading": "3.3. Audio features", "text": "In this section we evaluate the power of the audio representation that emerges from the L3-Net approach. Namely, the L3-Net audio subnetwork trained on Flickr-SoundNet is used to extract features from 1 second audio clips, and the effectiveness of these features is evaluated on two standard sound classification benchmarks: ESC-50 and DCASE.\nEnvironmental sound classification (ESC-50) [24]. This dataset contains 2000 audio clips, 5 seconds each, equally balanced between 50 classes. These include animal sounds, natural soundscapes, human non-speech sounds, interior/domestic sounds, and exterior/urban noises. The data is split into 5 predefined folds and performance is measured in terms of mean accuracy over 5 leave-one-fold-out evaluations.\nDetection and classification of acoustic scenes and events (DCASE) [30]. We consider the scene classification task of the challenge which contains 10 classes (bus, busy street, office, open air market, park, quiet street, restaurant, supermarket, tube, tube station), with 10 training and 100 test clips per class, where each clip is 30 seconds long.\nExperimental procedure. To enable a fair direct comparison with the current state-of-the-art, Aytar et al. [2], we follow the same experimental setup. Multiple overlapping subclips are extracted from each recording and described using our features. For 5 second recordings from ESC-50 we extract 10 equally spaced 1 second subclips, while for the 6 times longer DCASE recordings, 60 subclips are extracted per clip. The audio features are obtained by maxpooling the last convolutional layer of the audio subnetwork\n(conv4 2), before the ReLU, into a 4 \u00d7 3 \u00d7 512 = 6144 dimensional representation (the conv4 2 outputs are originally 16 \u00d7 12 \u00d7 512). The features are preprocessed using z-score normalization, i.e. shifted and scaled to have a zero mean and unit variance. A multi-class one-vs-all linear SVM is trained, and at test time the class scores for a recording are computed as the mean over the class scores for its subclips.\nResults and discussion. Table 2 shows the results on ESC50 and DCASE. On both benchmarks we convincingly beat the previous state-of-the-art, SoundNet [2], by 5.1% and 5% absolute. For ESC-50 we reduce the gap between the previous best result and the human performance by 72% while for DCASE we reduce the error by 42%. The results are especially impressive as SoundNet uses two vision networks trained in a fully supervised manner on ImageNet and Places2 as teachers for the audio network, while we learn both the vision and the audio networks without any supervision whatsoever. Note that we train our networks with a random subset of the SoundNet videos for efficiency pur-\nposes, so it is possible that further gains can be achieved by using all the available training data."}, {"heading": "3.4. Visual features", "text": "In this section we evaluate the power of the visual representation that emerges from the L3-Net approach. Namely, the L3-Net vision subnetwork trained on Flickr-SoundNet is used to extract features from images, and the effectiveness of these features is evaluated on the ImageNet large scale visual recognition challenge 2012 [27]. Experimental procedure. We follow the experimental setup of Zhang et al. [35] where features are extracted from 256 \u00d7 256 images and used to perform linear classification on ImageNet. As in [35], we take conv4 2 features after ReLU and perform max-pooling with equal kernel and stride sizes until feature dimensionality is below 10k; in our case this results in 4\u00d74\u00d7512 = 8192-D features. A single fully connected layer is added to perform linear classification into the 1000 ImageNet classes. All the weights are frozen to their L3-Net-trained values, apart from the final classification layer which is trained with cross-entropy loss on the ImageNet training set. The training procedure (data augmentation, learning rate schedule, label smoothing) is identical to [31], the only differences being that we use the Adam optimizer instead of RMSprop, and a 256\u00d7256 input image instead of 299 \u00d7 299 as it fits our architecture better and to be consistent with [35]. Results and discussion. Classification accuracy on the ImageNet validation set is shown in Table 3 and contrasted with other unsupervised and self-supervised methods. We also test the performance of random features, i.e. our L3Net architecture without AVC training but with a trained classification layer.\nOur L3-Net-trained features achieve 32.3% accuracy which is on par with other state-of-the-art self-supervised methods of [6, 7, 19, 35], while convincingly beating random initialization, data-dependent initialization [15], and Context Encoders [22]. It should be noted that these methods use the AlexNet [16] architecture which is different to ours, so the results are not fully comparable. On the one hand, our architecture when trained from scratch in its entirety achieves a higher performance (59.2% vs AlexNet\u2019s 51.0%). On the other hand, it is deeper which makes it harder to train as can be seen from the fact that our random features perform worse than theirs (12.9% vs AlexNet\u2019s 18.3%), and that all competing methods hit peak performance when they use earlier layers (e.g. [7] drops from 31.0% to 27.1% when going from conv3 to pool5). In fact, when measuring the improvement achieved due to AVC or self-supervised training versus the performance of the network with random initialization, our AVC training beats all competitors.\nAnother important fact to consider is that all competing\nmethods actually use ImageNet images when training. Although they do not make use of the labels, the underlying image statistics are the same: objects are fairly central in the image, and the networks have seen, for example, abundant images of 120 breads of dogs and thus potentially learnt their distinguishing features. In contrast, we use a completely separate source of training data in the form of frames from Flickr videos \u2013 here the objects are in general not centred, it is likely that the network has never seen a \u201cTibetan terrier\u201d nor the majority of other fine-grained categories. Furthermore, video frames have vastly different low-level statistics to still images, with strong artefacts such as motion blur. With these factors hampering our network, it is impressive that our visual features L3-Net-trained on Flickr videos perform on par with self-supervised state-of-the-art trained on ImageNet."}, {"heading": "3.5. Qualitative analysis", "text": "In this section we analyse what is it that the network has learnt. We visualize the results on the test set of the Kinetics-Sounds and Flickr-SoundNet datasets, so the network has not seen the videos during training."}, {"heading": "3.5.1 Vision features", "text": "To probe what the vision subnetwork has learnt, we pick a particular \u2018unit\u2019 in pool4 (i.e. a component of the 512 dimensional pool4 vector) and rank the test images by its magnitude. Figure 3 shows the images from KineticsSounds that activate particular units in pool4 the most (i.e. are ranked highest by its magnitude). As can be seen, the vision subnetwork has automatically learnt, without any explicit supervision, to recognize semantic entities such as guitars, accordions, keyboards, clarinets, bowling alleys, lawns or lawnmowers, etc. Furthermore, it has learnt finergrained categories as well as it is able to distinguish between acoustic and bass guitars (\u201cfingerpicking\u201d is mostly associated with acoustic guitars).\nFigure 4 shows heatmaps for the Kinetics-Sounds images in Figure 3, obtained by simply displaying the spatial activations of the corresponding vision unit (i.e. if the k component of pool4 is chosen, then the k channel of conv4 2 is displayed \u2013 since the k component is just the spatial max over this channel (after ReLU)). Objects are successfully detected despite significant clutter and occlusions. It is interesting to observe the type of cues that the network decides to use, e.g. the \u201cplaying clarinet\u201d unit, instead of trying to detect the entire clarinet, seems to mostly activate on the interface between the player\u2019s face and the clarinet.\nFigures 5 and 6 show visual concepts learnt by the L3Net on the Flickr-SoundNet dataset. It can be seen that the network learns to recognize many scene categories (Figure\n5), such as outdoors, concert, water, sky, crowd, text, railway, etc. These are useful for the AVC task as, for example, crowds indicate a large event that is associated with a distinctive sound as well (e.g. a football game), text indicates narration, and outdoors scenes are likely to be accompanied with wind sounds. It should be noted that though at first sight some categories seem trivially detectable, it is not the case; for example, \u201csky\u201d detector is not equivalent to the \u201cblueness\u201d detector as it only fires on \u201csky\u201d and not on \u201cwater\u201d, and furthermore there are separate units sensitive to \u201cwater surface\u201d and to \u201cunderwater\u201d scenes. The network also learns to detect people as user uploaded content is substantially people-oriented \u2013 Figure 6 shows the network has learnt to distinguish between babies, adults and crowds."}, {"heading": "3.5.2 Audio features", "text": "Figure 7 shows what particular audio units are sensitive to in the Kinetics-Sounds dataset. For visualization purposes, instead of showing the sound form, we display the video frame that corresponds to the sound. It can be seen that the audio subnetwork, again without any supervision, manages to learn various semantic entities, as well as perform fine-grained classification (\u201cfingerpicking\u201d vs \u201cplaying bass guitar\u201d). Note that some units are naturally confused \u2013 the \u201ctap dancing\u201d unit also responds to \u201cpen tapping\u201d, while the \u201csaxophone\u201d unit is sometimes confused with a \u201ctrombone\u201d. These are reasonable mistakes, especially when taking into account that the sound input is only one second in length. The audio concepts learnt on the Flickr-SoundNet dataset (Figure 8) follow the same pattern as the visual ones \u2013 the network learns to distinguish various scene categories such as water, underwater, outdoors and windy scenes, as well as human-related concepts like baby and human voices, crowds, etc.\nFigure 9 shows spectrograms and their semantic heatmaps, illustrating that our L3-Net method learns to detect audio events. For example, it shows clear preference for low frequencies when detecting bass guitars, attention to wide frequency range when detecting lawnmowers, and temporal \u2018steps\u2019 when detecting fingerpicking and tap dancing."}, {"heading": "3.5.3 Versus random features", "text": "Could the results in Figures 3, 4, 5, 6, 7 8, and 9 simply be obtained by chance due to examining a large number of units, as colourfully illustrated by the dead salmon experiment [3]? It is unlikely as there are only 512 units in pool4 to choose from, and many of those were found to be highly correlated with a semantic concept. Nevertheless, we repeated the same experiment with a random network (i.e. a network that has not been trained), and have failed to find such correlation. In more detail, we examined how\nmany out of the action classes in Kinetics-Sounds have a unit in pool4 which shows high preference for the class. For the vision subnetwork the preference is determined by ranking all images by their unit activation, and retaining the top 5; if 4 out of these 5 images correspond to one class, then that class is deemed to have a high-preference for the unit (a similar procedure is carried out for the audio subnetwork using spectrograms). Our trained vision and audio networks have high-preference units for 10 and 11 out of a possible 34 action classes, respectively, compared to 1 and 1 for the random vision and audio networks. Furthermore, if the threshold for deeming a unit to be high-preference is reduced to 3, our trained vision and audio subnetworks cover 23 and 20 classes, respectively, compared to the 4 and 3 of a random network, respectively. These results confirm that our network has indeed learnt semantic features.\nFurthermore, Figure 10 shows the comparison between the trained and the non-trained (i.e. network with random weights) L3-Net representations for the visual and the audio modalities, on the Kinetics-Sounds dataset, using the t-SNE visualization [33]. It is clear that training for the audio-visual correspondence task produces representations that have a semantic meaning, as videos containing the same action classes often cluster together, while the random network\u2019s representations do not exhibit any clustering. There is still a fair amount of confusion in the representations, but this is expected as no class-level supervision is provided and classes can be very alike. For example, an organ and a piano are quite visually similar as they contain keyboards, and the visual difference between a bass guitar and an acoustic guitar is also quite fine; these similarities are reflected in the closeness or overlap of respective clusters in Figure 10(c) (e.g. as noted earlier, \u201cfingerpicking\u201d is mostly associated with acoustic guitars).\nThe t-SNE visualization also shows some interesting features, such as the \u201ctyping\u201d class being divided into two clusters in the visual domain. Further investigation reveals that all frames in one cluster show both a keyboard and hands, while the second cluster contains much fewer hands. Separating these two cases can be a good indication of whether the typing action is happening at the moment captured by the (frame, 1 second sound clip) pair, and thus whether the typing sound is expected to be heard. Furthermore, we found that the \u201ctyping\u201d audio samples appear in three clusters \u2013 the two fairly pure clusters (outlined in Figure 10(a)) correspond to strong typing sounds and talking while typing, respectively, and the remaining cluster, which is very impure and intermingled with other action classes, mostly corresponds to silence and background noise."}, {"heading": "4. Discussion", "text": "We have shown that the network trained for the AVC task achieves superior results on sound classification to re-\ncent methods that pre-train and fix the visual networks (one each for ImageNet and Scenes), and we conjecture that the reason for this is that the additional freedom of the visual network allows the learning to better take advantage of the opportunities offered by the variety of visual information in the video (rather than be restricted to seeing only through the eyes of the pre-trained network). Also, the visual features that emerge from the L3-Net are on par with the stateof-the-art among self-supervised approaches. Furthermore, it has been demonstrated that the network automatically learns, in both modalities, fine-grained distinctions such as bass versus acoustic guitar or saxophone versus clarinet.\nThe localization visualization results are reminiscent of the classic highlighted pixels in [13], except in our case we do not just learn the few pixels that move (concurrent with the sound) but instead are able to learn extended regions corresponding to the instrument.\nWe motivated this work by considering correlation of video and audio events. However, we believe there is additional information in concurrency of the two streams, as concurrency is stronger than correlation because the events need to be synchronised (of course, if events are concurrent then they will correlate, but not vice versa). Training for concurrency will require video (multiple frames) as input, rather than a single video frame, but it would be interesting to explore what more is gained from this stronger condition.\nAlso, we would like to transition to using raw waveforms\nover spectrograms [2, 28, 32]. Additionally, it would be interesting to learn from the recently released large dataset of videos curated according to audio, rather than visual, events [9] and see what subtle visual semantic categories are discovered."}], "references": [{"title": "Learning to see by moving", "author": ["P. Agrawal", "J. Carreira", "J. Malik"], "venue": "In Proc. ICCV,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "SoundNet: Learning sound representations from unlabeled video", "author": ["Y. Aytar", "C. Vondrick", "A. Torralba"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Neural correlates of interspecies perspective taking in the post-mortem Atlantic salmon: An argument for multiple comparisons correction", "author": ["C.M. Bennett", "M.B. Miller", "G.L. Wolford"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "In Computational learning theory,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Out of time: Automated lip sync in the wild", "author": ["J.S. Chung", "A. Zisserman"], "venue": "In Workshop on Multi-view Lip-reading,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "In Proc. CVPR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Adversarial feature learning", "author": ["J. Donahue", "P. Kr\u00e4henb\u00fchl", "T. Darrell"], "venue": "In Proc. ICLR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Discriminative unsupervised feature learning with convolutional neural networks", "author": ["A. Dosovitskiy", "J.T. Springenberg", "M. Riedmiller", "T. Brox"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Audio Set: An ontology and human-labeled dataset for audio", "author": ["J.F. Gemmeke", "D.P.W. Ellis", "D. Freedman", "A. Jansen", "W. Lawrence", "R.C. Moore", "M. Plakal", "M. Ritter"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Unsupervised learning of spoken language with visual context", "author": ["D. Harwath", "A. Torralba", "J.R. Glass"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In Proc. ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "The Kinetics human action video", "author": ["W. Kay", "J. Carreira", "K. Simonyan", "B. Zhang", "C. Hillier", "S. Vijayanarasimhan", "F. Viola", "T. Green", "T. Back", "P. Natsev", "M. Suleyman", "A. Zisserman"], "venue": "dataset. CoRR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Pixels that sound", "author": ["E. Kidron", "Y.Y. Schechner", "M. Elad"], "venue": "In Proc. CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "In Proc. ICLR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Datadependent initializations of convolutional neural networks", "author": ["P. Kr\u00e4henb\u00fchl", "C. Doersch", "J. Donahue", "T. Darrell"], "venue": "In Proc. ICLR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Auditory scene classification using machine learning techniques", "author": ["D. Li", "J. Tam", "D. Toub"], "venue": "IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Shuffle and learn:  Unsupervised learning using temporal order verification", "author": ["I. Misra", "C.L. Zitnick", "M. Herbert"], "venue": "In Proc. ECCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Unsupervised learning of visual representations by solving jigsaw puzzles", "author": ["M. Noroozi", "P. Favaro"], "venue": "In Proc. ECCV,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Visually indicated sounds", "author": ["A. Owens", "P. Isola", "J. McDermott", "A. Torralba", "E. Adelson", "W. Freeman"], "venue": "In Proc. CVPR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Ambient sound provides supervision for visual learning", "author": ["A. Owens", "W. Jiajun", "J. McDermott", "W. Freeman", "A. Torralba"], "venue": "In Proc. ECCV,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Context encoders: Feature learning by inpainting", "author": ["D. Pathak", "P. Kr\u00e4henb\u00fchl", "J. Donahue", "T. Darrell", "A.A. Efros"], "venue": "In Proc. CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Environmental sound classification with convolutional neural networks", "author": ["K.J. Piczak"], "venue": "In IEEE Workshop on Machine Learning for Signal processing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "ESC: Dataset for environmental sound classification", "author": ["K.J. Piczak"], "venue": "In Proc. ACMM,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Histogram of gradients of time-frequency representations for audio scene classification", "author": ["A. Rakotomamonjy", "G. Gasso"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Recurrence quantification analysis features for environmental sound recognition", "author": ["G. Roma", "W. Nogueira", "P. Herrera"], "venue": "In IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "S. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A. Berg", "F. Li"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Learning the speech front-end with raw waveform CLDNNs", "author": ["T.N. Sainath", "R.J. Weiss", "A.W. Senior", "K.W. Wilson", "O. Vinyals"], "venue": "In INTERSPEECH,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In Proc. ICLR,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Detection and classification of acoustic scenes and events", "author": ["D. Stowell", "D. Giannoulis", "E. Benetos", "M. Lagrange", "M.D. Plumbley"], "venue": "In IEEE Transactions on Multimedia,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Rethinking the Inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": "In Proc. CVPR,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "WaveNet: A generative model for raw audio", "author": ["A. van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Visualizing data using t-SNE", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "In Proc. ICCV,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Colorful image colorization", "author": ["R. Zhang", "P. Isola", "A.A. Efros"], "venue": "In Proc. ECCV,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Our motivation for this work is three fold: first, as in many recent self-supervision tasks [1, 6, 8, 18, 19, 22, 34, 35], it is interesting to learn from a virtually infinite source of free supervision (video with visual and audio modes in this case) rather than requiring strong supervision; second, this is a possible source of supervision that an infant could use as their visual and audio capabilities develop; third, we want to know what can be learnt, and how well the networks are trained, for example in the performance of the visual and audio networks for other tasks.", "startOffset": 92, "endOffset": 121}, {"referenceID": 5, "context": "Our motivation for this work is three fold: first, as in many recent self-supervision tasks [1, 6, 8, 18, 19, 22, 34, 35], it is interesting to learn from a virtually infinite source of free supervision (video with visual and audio modes in this case) rather than requiring strong supervision; second, this is a possible source of supervision that an infant could use as their visual and audio capabilities develop; third, we want to know what can be learnt, and how well the networks are trained, for example in the performance of the visual and audio networks for other tasks.", "startOffset": 92, "endOffset": 121}, {"referenceID": 7, "context": "Our motivation for this work is three fold: first, as in many recent self-supervision tasks [1, 6, 8, 18, 19, 22, 34, 35], it is interesting to learn from a virtually infinite source of free supervision (video with visual and audio modes in this case) rather than requiring strong supervision; second, this is a possible source of supervision that an infant could use as their visual and audio capabilities develop; third, we want to know what can be learnt, and how well the networks are trained, for example in the performance of the visual and audio networks for other tasks.", "startOffset": 92, "endOffset": 121}, {"referenceID": 17, "context": "Our motivation for this work is three fold: first, as in many recent self-supervision tasks [1, 6, 8, 18, 19, 22, 34, 35], it is interesting to learn from a virtually infinite source of free supervision (video with visual and audio modes in this case) rather than requiring strong supervision; second, this is a possible source of supervision that an infant could use as their visual and audio capabilities develop; third, we want to know what can be learnt, and how well the networks are trained, for example in the performance of the visual and audio networks for other tasks.", "startOffset": 92, "endOffset": 121}, {"referenceID": 18, "context": "Our motivation for this work is three fold: first, as in many recent self-supervision tasks [1, 6, 8, 18, 19, 22, 34, 35], it is interesting to learn from a virtually infinite source of free supervision (video with visual and audio modes in this case) rather than requiring strong supervision; second, this is a possible source of supervision that an infant could use as their visual and audio capabilities develop; third, we want to know what can be learnt, and how well the networks are trained, for example in the performance of the visual and audio networks for other tasks.", "startOffset": 92, "endOffset": 121}, {"referenceID": 21, "context": "Our motivation for this work is three fold: first, as in many recent self-supervision tasks [1, 6, 8, 18, 19, 22, 34, 35], it is interesting to learn from a virtually infinite source of free supervision (video with visual and audio modes in this case) rather than requiring strong supervision; second, this is a possible source of supervision that an infant could use as their visual and audio capabilities develop; third, we want to know what can be learnt, and how well the networks are trained, for example in the performance of the visual and audio networks for other tasks.", "startOffset": 92, "endOffset": 121}, {"referenceID": 33, "context": "Our motivation for this work is three fold: first, as in many recent self-supervision tasks [1, 6, 8, 18, 19, 22, 34, 35], it is interesting to learn from a virtually infinite source of free supervision (video with visual and audio modes in this case) rather than requiring strong supervision; second, this is a possible source of supervision that an infant could use as their visual and audio capabilities develop; third, we want to know what can be learnt, and how well the networks are trained, for example in the performance of the visual and audio networks for other tasks.", "startOffset": 92, "endOffset": 121}, {"referenceID": 34, "context": "Our motivation for this work is three fold: first, as in many recent self-supervision tasks [1, 6, 8, 18, 19, 22, 34, 35], it is interesting to learn from a virtually infinite source of free supervision (video with visual and audio modes in this case) rather than requiring strong supervision; second, this is a possible source of supervision that an infant could use as their visual and audio capabilities develop; third, we want to know what can be learnt, and how well the networks are trained, for example in the performance of the visual and audio networks for other tasks.", "startOffset": 92, "endOffset": 121}, {"referenceID": 1, "context": "In a series of recent and inspiring papers [2, 10, 20, 21], the group at MIT has investigated precisely this.", "startOffset": 43, "endOffset": 58}, {"referenceID": 9, "context": "In a series of recent and inspiring papers [2, 10, 20, 21], the group at MIT has investigated precisely this.", "startOffset": 43, "endOffset": 58}, {"referenceID": 19, "context": "In a series of recent and inspiring papers [2, 10, 20, 21], the group at MIT has investigated precisely this.", "startOffset": 43, "endOffset": 58}, {"referenceID": 20, "context": "In a series of recent and inspiring papers [2, 10, 20, 21], the group at MIT has investigated precisely this.", "startOffset": 43, "endOffset": 58}, {"referenceID": 19, "context": "However, their goal is always to train a single network for one of the modes, for example, train a visual network to generate sounds in [20, 21]; or train an audio network to correlate with visual outputs in [2, 10], where the visual networks are pre-trained and fixed and act as a teacher.", "startOffset": 136, "endOffset": 144}, {"referenceID": 20, "context": "However, their goal is always to train a single network for one of the modes, for example, train a visual network to generate sounds in [20, 21]; or train an audio network to correlate with visual outputs in [2, 10], where the visual networks are pre-trained and fixed and act as a teacher.", "startOffset": 136, "endOffset": 144}, {"referenceID": 1, "context": "However, their goal is always to train a single network for one of the modes, for example, train a visual network to generate sounds in [20, 21]; or train an audio network to correlate with visual outputs in [2, 10], where the visual networks are pre-trained and fixed and act as a teacher.", "startOffset": 208, "endOffset": 215}, {"referenceID": 9, "context": "However, their goal is always to train a single network for one of the modes, for example, train a visual network to generate sounds in [20, 21]; or train an audio network to correlate with visual outputs in [2, 10], where the visual networks are pre-trained and fixed and act as a teacher.", "startOffset": 208, "endOffset": 215}, {"referenceID": 12, "context": "In earlier, pre deep-learning, approaches the observation was used to beautiful effect in [13] showing \u201cpixels that sound\u201d (e.", "startOffset": 90, "endOffset": 94}, {"referenceID": 1, "context": "In contrast, we train both visual and audio networks and, somewhat surprisingly, show that this is beneficial \u2013 in that our performance improves substantially over that of [2] when trained on the same data.", "startOffset": 172, "endOffset": 175}, {"referenceID": 4, "context": "In terms of prior work, the most closely related deep learning approach that we know of is \u2018SyncNet\u2019 in [5].", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "However, [5] is aimed at learning to synchronize lip-regions and speech for lip-reading, rather than the more general video and audio material considered here for learning semantic representations.", "startOffset": 9, "endOffset": 12}, {"referenceID": 3, "context": "More generally, the AVC task is a form of co-training [4], where there are two \u2018views\u2019 of the data, and each view provides complementary information.", "startOffset": 54, "endOffset": 57}, {"referenceID": 28, "context": "We follow the VGG-network [29] design style, with 3 \u00d7 3 convolutional filters, and 2\u00d72 max-pooling layers with stride 2 and no padding.", "startOffset": 26, "endOffset": 30}, {"referenceID": 10, "context": "Each conv layer is followed by batch normalization [11] and a ReLU nonlinearity.", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "Each convolutional layer is followed by batch normalization [11] and a ReLU nonlinearity, and the first fully connected layer (fc1) is followed by ReLU.", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "The response map is passed through a logarithm and global scaling is applied such that values fit into the [0, 1] range, before feeding it into the audio subnetwork.", "startOffset": 107, "endOffset": 113}, {"referenceID": 13, "context": "We use the Adam optimizer [14], weight decay 10\u22125, and perform a grid search on the learning rate, although 10\u22124 usually works well.", "startOffset": 26, "endOffset": 30}, {"referenceID": 1, "context": "Flickr-SoundNet [2].", "startOffset": 16, "endOffset": 19}, {"referenceID": 11, "context": "For this purpose we took a subset (much smaller than Flickr-SoundNet) of the Kinetics dataset [12], which contains YouTube videos manually annotated for human actions using Mechanical Turk,", "startOffset": 94, "endOffset": 98}, {"referenceID": 23, "context": "Environmental sound classification (ESC-50) [24].", "startOffset": 44, "endOffset": 48}, {"referenceID": 29, "context": "Detection and classification of acoustic scenes and events (DCASE) [30].", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "[2], we follow the same experimental setup.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "SVM-MFCC [24] 39.", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "6% Autoencoder [2] 39.", "startOffset": 15, "endOffset": 18}, {"referenceID": 23, "context": "9% Random Forest [24] 44.", "startOffset": 17, "endOffset": 21}, {"referenceID": 22, "context": "3% Piczak ConvNet [23] 64.", "startOffset": 18, "endOffset": 22}, {"referenceID": 1, "context": "5% SoundNet [2] 74.", "startOffset": 12, "endOffset": 15}, {"referenceID": 23, "context": "[24] 81.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "RG [25] 69% LTT [17] 72% RNH [26] 77% Ensemble [30] 78% SoundNet [2] 88%", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "RG [25] 69% LTT [17] 72% RNH [26] 77% Ensemble [30] 78% SoundNet [2] 88%", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "RG [25] 69% LTT [17] 72% RNH [26] 77% Ensemble [30] 78% SoundNet [2] 88%", "startOffset": 29, "endOffset": 33}, {"referenceID": 29, "context": "RG [25] 69% LTT [17] 72% RNH [26] 77% Ensemble [30] 78% SoundNet [2] 88%", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "RG [25] 69% LTT [17] 72% RNH [26] 77% Ensemble [30] 78% SoundNet [2] 88%", "startOffset": 65, "endOffset": 68}, {"referenceID": 21, "context": "[22] 22.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] 24.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] 31.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] 31.", "startOffset": 0, "endOffset": 3}, {"referenceID": 34, "context": "[35] (init: [15]) 32.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[35] (init: [15]) 32.", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "6% Noroozi and Favaro [19] 34.", "startOffset": 22, "endOffset": 26}, {"referenceID": 34, "context": "Following [35], our features are evaluated by training a linear classifier on the ImageNet training set and measuring the classification accuracy on the validation set.", "startOffset": 10, "endOffset": 14}, {"referenceID": 34, "context": "All performance numbers apart from ours are provided by authors of [35], showing only the best performance for each method over all parameter choices (e.", "startOffset": 67, "endOffset": 71}, {"referenceID": 6, "context": "[7] achieve 27.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "On both benchmarks we convincingly beat the previous state-of-the-art, SoundNet [2], by 5.", "startOffset": 80, "endOffset": 83}, {"referenceID": 26, "context": "Namely, the L-Net vision subnetwork trained on Flickr-SoundNet is used to extract features from images, and the effectiveness of these features is evaluated on the ImageNet large scale visual recognition challenge 2012 [27].", "startOffset": 219, "endOffset": 223}, {"referenceID": 34, "context": "[35] where features are extracted from 256 \u00d7 256 images and used to perform linear classification on ImageNet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "As in [35], we take conv4 2 features after ReLU and perform max-pooling with equal kernel and stride sizes until feature dimensionality is below 10k; in our case this results in 4\u00d74\u00d7512 = 8192-D features.", "startOffset": 6, "endOffset": 10}, {"referenceID": 30, "context": "The training procedure (data augmentation, learning rate schedule, label smoothing) is identical to [31], the only differences being that we use the Adam optimizer instead of RMSprop, and a 256\u00d7256 input image instead of 299 \u00d7 299 as it fits our architecture better and to be consistent with [35].", "startOffset": 100, "endOffset": 104}, {"referenceID": 34, "context": "The training procedure (data augmentation, learning rate schedule, label smoothing) is identical to [31], the only differences being that we use the Adam optimizer instead of RMSprop, and a 256\u00d7256 input image instead of 299 \u00d7 299 as it fits our architecture better and to be consistent with [35].", "startOffset": 292, "endOffset": 296}, {"referenceID": 5, "context": "3% accuracy which is on par with other state-of-the-art self-supervised methods of [6, 7, 19, 35], while convincingly beating random initialization, data-dependent initialization [15], and Context Encoders [22].", "startOffset": 83, "endOffset": 97}, {"referenceID": 6, "context": "3% accuracy which is on par with other state-of-the-art self-supervised methods of [6, 7, 19, 35], while convincingly beating random initialization, data-dependent initialization [15], and Context Encoders [22].", "startOffset": 83, "endOffset": 97}, {"referenceID": 18, "context": "3% accuracy which is on par with other state-of-the-art self-supervised methods of [6, 7, 19, 35], while convincingly beating random initialization, data-dependent initialization [15], and Context Encoders [22].", "startOffset": 83, "endOffset": 97}, {"referenceID": 34, "context": "3% accuracy which is on par with other state-of-the-art self-supervised methods of [6, 7, 19, 35], while convincingly beating random initialization, data-dependent initialization [15], and Context Encoders [22].", "startOffset": 83, "endOffset": 97}, {"referenceID": 14, "context": "3% accuracy which is on par with other state-of-the-art self-supervised methods of [6, 7, 19, 35], while convincingly beating random initialization, data-dependent initialization [15], and Context Encoders [22].", "startOffset": 179, "endOffset": 183}, {"referenceID": 21, "context": "3% accuracy which is on par with other state-of-the-art self-supervised methods of [6, 7, 19, 35], while convincingly beating random initialization, data-dependent initialization [15], and Context Encoders [22].", "startOffset": 206, "endOffset": 210}, {"referenceID": 15, "context": "It should be noted that these methods use the AlexNet [16] architecture which is different to ours, so the results are not fully comparable.", "startOffset": 54, "endOffset": 58}, {"referenceID": 6, "context": "[7] drops from 31.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Could the results in Figures 3, 4, 5, 6, 7 8, and 9 simply be obtained by chance due to examining a large number of units, as colourfully illustrated by the dead salmon experiment [3]? It is unlikely as there are only 512 units in pool4 to choose from, and many of those were found to be highly correlated with a semantic concept.", "startOffset": 180, "endOffset": 183}, {"referenceID": 32, "context": "network with random weights) L-Net representations for the visual and the audio modalities, on the Kinetics-Sounds dataset, using the t-SNE visualization [33].", "startOffset": 154, "endOffset": 158}, {"referenceID": 12, "context": "The localization visualization results are reminiscent of the classic highlighted pixels in [13], except in our case we do not just learn the few pixels that move (concurrent with the sound) but instead are able to learn extended regions corresponding to the instrument.", "startOffset": 92, "endOffset": 96}, {"referenceID": 1, "context": "over spectrograms [2, 28, 32].", "startOffset": 18, "endOffset": 29}, {"referenceID": 27, "context": "over spectrograms [2, 28, 32].", "startOffset": 18, "endOffset": 29}, {"referenceID": 31, "context": "over spectrograms [2, 28, 32].", "startOffset": 18, "endOffset": 29}, {"referenceID": 8, "context": "Additionally, it would be interesting to learn from the recently released large dataset of videos curated according to audio, rather than visual, events [9] and see what subtle visual semantic categories are discovered.", "startOffset": 153, "endOffset": 156}], "year": 2017, "abstractText": "We consider the question: what can be learnt by looking at and listening to a large amount of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself \u2013 the correspondence between the visual and the audio streams, and we introduce a novel \u201cAudio-Visual Correspondence\u201d learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good vision and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art selfsupervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.", "creator": "LaTeX with hyperref package"}}}