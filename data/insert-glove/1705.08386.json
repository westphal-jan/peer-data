{"id": "1705.08386", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Better Text Understanding Through Image-To-Text Transfer", "abstract": "kalatrazan Generic text possibile embeddings dayn are depalma successfully used dept in gimbel a variety almo of tasks. However, they are simonides often vertonghen learnt anytime by bioregional capturing the grinning co - occurrence c\u00e1mpora structure super-sized from pure willows text corpora, 20,000-point resulting in heliozelidae limitations intermediates of their ability jobless to generalize. ashley-cooper In obelisk this taricco paper, we acyl-carrier-protein explore models czernich\u00f3w that indorama incorporate 180.6 visual information canaiolo into the 1.3673 text representation. ajinomoto Based sporturilor on comprehensive ablation studies, we propose brazen a manhoef conceptually 2,395 simple, stockaded yet gri well \u0440\u043e\u0441\u0441\u0438\u0438 performing architecture. It butel outperforms littleover previous multimodal 2555 approaches on motions a autographing set unrewarded of tyan well 6.41 established backbenches benchmarks. 29,035-foot We also 27-hour improve the jostein state - of - the - marketed art results loomba for flybe image - related fanconi text datasets, using orders maquinaria of magnitude webster less riparia data.", "histories": [["v1", "Tue, 23 May 2017 16:06:32 GMT  (2837kb,D)", "https://arxiv.org/abs/1705.08386v1", null], ["v2", "Fri, 26 May 2017 08:08:20 GMT  (2837kb,D)", "http://arxiv.org/abs/1705.08386v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.LG", "authors": ["karol kurach", "sylvain gelly", "michal jastrzebski", "philip haeusser", "olivier teytaud", "damien vincent", "olivier bousquet"], "accepted": false, "id": "1705.08386"}, "pdf": {"name": "1705.08386.pdf", "metadata": {"source": "CRF", "title": "Better Text Understanding Through Image-To-Text Transfer", "authors": ["Karol Kurach", "Sylvain Gelly", "Michal Jastrzebski", "Philip Haeusser", "Olivier Teytaud", "Damien Vincent", "Olivier Bousquet"], "emails": [], "sections": [{"heading": null, "text": "1 Introduction\nThe problem of finding good representations of text data is a very actively studied area of research. Many models are able to learn representations directly by optimizing the end-to-end task in a supervised manner. This, however, often requires an enormous amount of labeled data which is not available in many practical applications and gathering such data can be very costly. A common solution that requires an order of magnitude less labeled examples is to reuse pretrained embeddings.\nA large body of work in this space is focused on training embeddings from pure text data. However, there are many types of relations and cooccurrences that are hard to grasp from pure text. Instead, they appear naturally in other modalities, such as images. In particular, from a similarity measure in the image space and pairs of\nimages and sentences, a similarity measure for sentences can be induced, as illustrated in Figure 1.\nIn this work, we study how to build sentence embeddings from text-image pairs that are good in terms of sentence similary metrics. This extends previous works, for example [12] or [15].\nWe propose a conceptually simple, but well performing model that we call Visually Enhanced Text Embeddings (VETE). It takes advantage of visual information from images in order to improve the quality of sentence embeddings. This model uses simple ingredients that already exist and combines them properly. Using a pre-trained Convolutional Neural Network (CNN) for the image embedding, the sentence embeddings are obtained as the normalized sum of the word embeddings. Those are\nar X\niv :1\n70 5.\n08 38\n6v 2\n[ cs\n.C L\n] 2\n6 M\nay 2\ntrained end-to-end to be aligned with the corresponding image embeddings and not aligned with mismatching pairs, optimizing the Pearson correlation.\nDespite its simplicity, the model significantly outperforms pure-text models and the best multimodal model from [15] on a set of well established text similarity benchmarks from the SemEval competition [18]. In particular, for image-related datasets, our model matches state-of-the-art results with substantially less training data. These results indicate that exploring image data can significantly improve the quality of text embeddings and that incorporating images as a source of information can result in text representations which effectively captures visual knowledge. We also conduct a detailed ablation study to quantify the effect of different factors on the embedding quality in the image-to-text transfer setup.\nIn summary, the contributions of this work are:\n\u2022 We propose a simple multimodal model that outperforms previous image-text approaches on a wide variety of text similarity tasks. Furthermore, the proposed model matches state-ofthe-art results on image-related SemEval datasets, despite being trained with substantially less data.\n\u2022 We perform a comprehensive study of image-to-text transfer, comparing different model types, text encoders, loss types and datasets.\n\u2022 We provide evidence that the approach of learning sentence embeddings directly outperforms methods that learn word embeddings and then combine them."}, {"heading": "2 Related Work", "text": "Many works study the use of multimodal data, in particular pairs of images and text. Most of them explore how these pairs of data can be leveraged for tasks that require knowledge of both modalities, like captioning or image retrieval [1, 7, 11, 12, 29, 8, 26].\nWhile this line of work is very interesting as common embeddings can directly be applied to captioning or image retrieval tasks, the direct use of text embeddings for NLP tasks, using images only as auxiliary training data, is less explored.\n[12] propose to extend the skip-gram algorithm [16] to incorporate image data. [4] also took a similar approach before. In the original skip-gram algorithm, each word embedding is optimized to increase the likelihood of the neighboring words conditioned on the center word. In addition to predicting contextual words, [12]\u2019s models maximize the similarity between image and word embeddings. More precisely:\nLling(wt) = \u2211\n\u2212c\u2264j\u2264c,j 6=0\nlog p(wj |wt)\nwhere p(wj |wt) is given by a softmax formulation:\np(wj |wt) = ef(wj)\u00b7f(wt)\u2211 w e f(w)\u00b7f(wt)\nf(w) is the embedding vector of the word w and v \u00b7 v\u2032 is the dot product between the vectors v and v\u2032. To inject visual information, [12] add a max margin loss between the image embedding and the word embedding:\nLvision(wt) = Ew\u2032 max(0, \u03b3 \u2212 cos(f(wt), g(wt)) + cos(f(wt), g(w\u2032)))\nwith g(w) being the average embedding of the images paired with the word w.\nThey show that they can augment word embeddings learnt from large text sources with visual information and in addition to image labeling and retrieval, they show that those embeddings perform better on word similarity metrics.\n[11] also use a max margin loss to co-embed images and corresponding text. For the text embeddings, they use different models depending on the task. For the image captioning or image retrieval task,\nthey employ an LSTM encoder. To explore the resulting word embedding properties, in particular arithmetics, they use a simpler word embedding model. Some interesting arithmetical properties of the embeddings are demonstrated, like \u201cimage of a blue car\u201d - \u201cblue\u201d + \u201cred\u201d leading to an image of a red car. However, there is no quantitative evaluation of the quality of the text embeddings in terms of text similarity.\nSome more recent works investigate phrase embeddings trained with visual signals and their quality in terms of text similarity metrics. For example, [15] use an Recurrent Neural Network (RNN) as a language model in order to learn word embeddings which are then combined to create a phrase embedding. They propose three models. For their model A, they use a similar setup as the captioning model from [27], with an RNN decoder conditioned on a pre-trained CNN embedding. The RNN (GRU in that case) reads the text, trying to predict the next token. The initial state is set to a transformation of the last internal layer of a pre-trained VGGNet [24]. Let that vector be vimage. Model B tries to match the final RNN state with vimage. Finally, model C develops the multimodal skip-gram [12] by adding an additional loss measuring the distance between the word embeddings and vimage. The authors\u2019 experiments show that model A performs best and we use that model as a baseline in our experiments."}, {"heading": "3 VETE Models", "text": "Our setup aims at directly transferring knowledge between image and text representations and the main goal is to find reusable sentence embeddings. We make direct use of paired data, consisting of pictures and text describing them. We propose a model consisting of two separate encoders - one for images and another one for text. An overview of the archiecture is presented in Figure 2.\nFor the text encoder, we consider three families of models which combine words into text representations. For the bag-of-words (BOW) model, the sentence embedding is simply a normalized sum of vectors corresponding to the individual words. For the RNN model, we create a stacked recurrent neural network encoder (LSTM or GRU based). Finally, for the CNN model, the encoder includes a convolutional layer followed by a fully connected layer, as described in [9].\nFor encoding images, we use a pre-trained InceptionV3 network [25] which provides a 2048- dimensional feature vector for each image in the dataset (images are rescaled and cropped to 300 x 300 px).\nLet Eimg(I) denote the 2048-dimensional embedding vector for an image I and Etxt(S) the N - dimensional embedding for sentence S produced by txt \u2208 {BOW,RNN,CNN}. Throughout this paper, we will refer to the cosine similarity of two vectors v1, v2 as sim(v1, v2) = v1\u00b7v2\u2016v1\u2016\u2016v2\u2016 . Informally speaking, our training goal is to maximize sim(Eimg(I), Etxt(S)), when sentence S is paired with (i.e. describes) image I , and minimize this value otherwise.\nFormally, let V be the vocabulary, N the embedding dimensionality and txt \u2208 {BOW,RNN,CNN} the text encoding model. Let\u2019s define an affine transformation W \u2208 R2048\u00d7N that transforms the 2048-dimensional image embeddings to N dimensions. Our learnable parameters consist of a word embedding matrix \u2208 R|V |\u00d7N , the internal parameters of the txt model (when txt is an RNN or CNN encoder), as well as the transformation matrix W . For each batch of image-sentence pairs of the form (I1, S1), ..., (IB , SB), we randomly shuffle the sentences S1, ..., SB , and add the following \"incorrect\" pairs to the batch (I1, S\u03c3(1)), ..., (IB , S\u03c3(B)), with {\u03c3(1), .., \u03c3(B)} a random permutation of {1, ..., B}. If we denote\nsim(Ii, Sj) := sim(WEimg(Ii), Etxt(Sj))\nthen our training goal is to maximize the Pearson correlation \u03c1(x, y) := Cov(x,y)Std(x)Std(y) between the vectors\n[sim(I1, S1), ..., sim(IB , SB)\ufe38 \ufe37\ufe37 \ufe38 B correct pairs , sim(I1, S\u03c3(1)), ..., sim(IB , S\u03c3(B))\ufe38 \ufe37\ufe37 \ufe38 B wrong pairs ]\nand [ 1, 1, ..., 1\ufe38 \ufe37\ufe37 \ufe38 B positive labels , \u22121,\u22121, ...,\u22121\ufe38 \ufe37\ufe37 \ufe38 B negative labels ].\nWe will denote models trained this way VETE-BOW, VETE-RNN and VETE-CNN, respectively."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Training Datasets", "text": "In our experiments we consider three training datasets: MS COCO, SBU and Pinterest5M. They are described in detail below. One important note is that we modify datasets that contain multiple captions per image (MS COCO and Pinterest5M) to keep only one caption. This was done to prevent the network from \u201ccheating\u201d by using the image feature vector only as a way of joining text pairs. To the best of our knowledge, we are the first to notice this issue in the evaluation of the quality of multimodal image-text models. It is known that training similarity models directly on text-text pairs yields good results [30] but here we want to investigate only the effect of knowledge transfer from images.\nMS COCO The MS COCO dataset [13] contains 80 image categories. For each image five highquality captions are provided. We use the MS COCO 2014 dataset using the same train/validation/test split as the im2txt [23] Tensorflow implementation of [28]. Initially, our train/validation/test sets contain 586k/10k/20k examples, respectively. Then, we filter the sets to keep only one caption per image, so the \"text part\" of our final datasets is five times smaller.\nSBU The Stony Brook University dataset[19] consists of 1M image-caption pairs collected from Flickr, with only one caption per image. We randomly split this dataset into train/validation/test sets with sizes 900k/50k/50k, respectively.\nPinterest5M The original Pinterest40M dataset [15] contains 40M images. However, only 5M image urls were released at the time of this submission. Unfortunately, some images are no longer available so we were able to collect approx. 3.9M images from this dataset. For every image we keep only one caption. Then, we randomly split the data into 3.8M/50k/50k train/validation/test sets, respectively.\nThe training data in all datasets is lowercased and tokenized using the Stanford Tokenizer [14]. We also wrap all sentences with \"<S>\" and \"</S>\" marking the beginning and the end of the sentence."}, {"heading": "4.2 Hyperparameter selection and training", "text": "The performance of every machine learning model highly depends on the choice of its hyperparameters. In order to fairly compare our approach to previous works, we follow the same hyperparameter search protocol for all models. We choose the average score on the SemEval 2016 (c.f. Section 4.3) as the validation metric and refer to this as \u201cavg2016\u201d.\nAlgorithm 1: Protocol for hyperparameter search. for i=1,2,. . . ,100 do\nSample a set of hyperparameters in the allowed ranges; Run training and evaluate on \u201cavg2016\u201d;\nend Report the results on all benchmarks of the model that has the highest score on \u201cavg2016\u201d;\nIf a hyperparameter has a similar meaning in two models (e.g., learning rate, initialization scale, lr decay, etc.), the ranges searched were set to be the same. Additionally, we ensured that the parameters reported by the authors are included in the ranges.\nIn all models, we train using the Adam optimizer[10], for 10 (MS COCO, SBU) or 5 epochs (Pinterest5M). The final embeddings have size of 128. For all VETE models we use the Pearson loss (see ablation study in Section 5.2)."}, {"heading": "4.3 Evaluation", "text": "Our goal is to create good text embeddings that encode knowledge from corresponding images. To evaluate the effect of this knowledge transfer from images to text, we use a set of textual semantic similarity datasets from the SemEval 2014 and 2015 competitions[18]. Unfortunately, we could not compare our models directly on the Gold RP10K dataset introduced by [15] as it was not publicly released.\nWe also use two additional custom test sets: COCO-Test and Pin-Test. These were created from the MS COCO and Pinterest5M test datasets, respectively, by randomly sampling 1, 000 semantically related captions (from the same image) and 1, 000 non-related captions from different images. As opposed to the SemEval datasets, the similarity score is binary in this case. The goal was to check the performance on the same task as SemEval but with data from the same distribution of words as our training data.\nFor every model type we select the best model according to the average score on the SemEval 2016 datasets. Then, we report the results on all other test datasets."}, {"heading": "4.4 Results", "text": "Table 1 presents the scores obtained by models trained only on MS COCO datasets. This allows us to fairly compare only the algorithms, not the data used. In Section 5.3, we analyze the robustness of our methods on two additional datasets (SBU and Pinterest5M).\nAs a direct comparison, we implementModelA as described in [15], which we refer to as PinModelA. Our implementation uses a pre-trained InceptionV3 network for visual feature extraction, as in the VETE models. To understand the impact of adding information from images to text data, we also evaluate two models trained purely on text:\n\u2022 RNN-based language model This model learns sentence embeddings via an RNN based language model. It corresponds to the PureTextRnn baseline from [15].\n\u2022 Word2Vec We trained Word2Vec word embeddings [21] where the corpus consists of sentences from MS COCO.\nAll VETE models outperform pure-text baselines and PinModelA. Similarly to [30], we observed that RNN-based encoders are outperformed by a simpler BOW model. We also show that this holds for CNN-based encoders. It is worth noting that it is mostly a domain adaptation issue, as both RNN and CNN encoders perform better than BOW on COCO-Test, where the data has the same distribution as the training data. We analyze the effect of changing the text encoder in Section 5.1.\nTo put our results in context, Table 2 compares them to other methods trained with much larger corpora. We used word embeddings obtained from three methods:\n\u2022 Glove: embeddings proposed in [20], trained on a Common Crawl dataset with 840 billion tokens.\n\u2022 M-Skip-Gram: embeddings proposed in [12], trained on Wikipedia and a set of images from ImageNet.\n\u2022 PP-XXL: the best embeddings from the [30], trained on 9M phrase pairs from PPDB.\nFor all three approaches, we consider two versions that differ in the vocabulary allowed at inference time. One experiment was done with a vocabulary restricted to MS COCO (marked with \u201cR\u201d) and the non-restricted version (\u201cNR\u201d) where we use the whole vocabulary for given embeddings. The vocabulary size has a significant impact on the final score for the Pinterest-Test benchmark, where 16.5% of all tokens are not in the MS COCO vocabulary. That means that 97.6% of all sentences have at least one missing token.\nFinally, we also include the best results from the SemEval competition, where available. Note that those were obtained from heavily tuned and more complex models, trained without any data restrictions. Still, our VETE model is able to match their results."}, {"heading": "5 Ablation studies", "text": "To analyze the impact of different components of our architecture, we perform ablation studies on the employed text encoder, loss type and training dataset. We also investigate the effect of training on word or sentence level. In all cases, we follow a similar protocol as in Section 4.2:\nAlgorithm 2: Protocol for hyperparameter ablation study. Randomly generate 100 sets of combinations for all hyperparameters.; for Hyperparameter p (e.g, \u201closs type\u201d) do\nfor v in the allowed range of values for p do Run training using the 100 sets of hyperparameters, keeping p=v fixed.; end Choose the best one based on \u201cavg2016\u201d validation metric, and report the scores.;\nend"}, {"heading": "5.1 Encoder", "text": "We study the impact of the different text encoders on the VETE model. The results are summarized in Table 3. \u201cRNN-GRU\u201d and \u201cRNN-LSTM\u201d denote RNN encoders with GRU [2] and LSTM [5] cells, respectively. For BOW, we try two options: either we use the sum or the mean of word embeddings. Both bag-of-words encoders perform better than RNN encoders, although RNNs are slightly better on the test data which has the same distribution as the training data.\nEncoder images2014 images2015 COCO-Test Pin-Test RNN-GRU 0.834 0.821 0.906 0.507\nRNN-LSTM 0.838 0.835 0.901 0.549 BOW-SUM 0.860 0.853 0.898 0.573 BOW-MEAN 0.861 0.855 0.894 0.579\nTable 3: Results of applying different text encoders to the VETE model. The training data is MS COCO, and the RNN-based models learned to model this distribution better. However, BOW generalizes better to other datasets.\n.\nLoss type Avg score Covariance 0.594\nSKT.2 0.616 SKT1 0.730 Rank loss 0.788 SKT5 0.791\nPearson 0.797\nTable 4: Comparison of different loss types with the VETEBOW model."}, {"heading": "5.2 Loss type", "text": "In this section, we describe various loss types that we trained our model with. Consider two paired variables x (similarity score between two embeddings) and y \u2208 {\u22121, 1}. Then, the sample sets (x1, . . . , xn) and (y1, . . . , yn) stand for n corresponding realizations of x and y.\n\u2022 Covariance: Cov(x, y).\n\u2022 The Pearson correlation \u03c1: measures the linearity of the link between two variables, estimated on a sample; it is defined as \u03c1(x, y) = Cov(x,y)Std(x)Std(y) .\n\u2022 Surrogate Kendall \u03c4 : The Pearson correlation takes into account only linear dependencies. To mitigate this, we experimented with the Kendall correlation \u03c4 which is only rankdependent. Unfortunately, it is not differentiable. We therefore used its differentiable approximation: SKT\u03b1 [6] defined as SKT\u03b1(x, y) = \u2211 i,j tanh(\u03b1(xi\u2212xj)(yi\u2212yj))\nn(n\u22121)/2 for some \u03b1 > 0.\n\u2022 Rank loss: Another cost function is the pairwise ranking loss. We follow closely the definition in [11].\nTable 4 compares the effects of the various losses."}, {"heading": "5.3 Dataset", "text": "We study the effect of the training dataset. The results of training the model on MS COCO, SBU and Pinterest5M dataset are presented in Table 5. Each cell of the table contains the average score of 4 evaluation datasets (images2014, images2014, COCO-Test, Pin-Test). The quality of image captions varies significantly between the datasets, as can be seen in Figure 3. However, we conclude that the relation between the models is preserved, that is: regardless of the dataset used for training, PinModelA is always worse than VETE-RNN, which in turn is worse than VETE-BOW."}, {"heading": "5.4 Sentence-level vs word-level embedding", "text": "Previous methods for transferring knowledge from images to text focused on improving the wordlevel embeddings. A sentence representation could then be created by combining them. In our work, we learn sentence embeddings as a whole, but the best performing text encoder turned out to be BOW. This raises the following question: could the model perform equally well if we train it on word-level, and then only combine word embeddings during inference? The comparison of these two approaches is presented in Table 6 which clearly shows the benefit of sentence-level training. This effect should be studied further, but while separately training word embeddings forces each of them to be close to the corresponding images, training at the sentence level gives the opportunity to have the word embeddings become complementary, each of them explaining a part of the image, and capturing co-occurences."}, {"heading": "6 Conclusion", "text": "We studied how to improve text embeddings by leveraging multimodal datasets, using a pre-trained image model and paired text-image datasets. We showed that VETE, a simple approach which directly optimizes phrase embeddings to match corresponding image representations, outperforms previous multimodal approaches which are sometimes more complex and optimize word embeddings as opposed to sentence embeddings. We also showed that even for relatively complex similarity tasks at sentence levels, our proposed models can create very competitive embeddings, even compared to more sophisticated models trained on orders of magnitude more text data, especially when the vocabulary is related to visual concepts.\nTo our initial surprise, state-of-the-art encoder models, like LSTMs, performed significantly worse than much simpler encoders like bag-of-word models. While they achieve better results when evaluated on the same data distribution, their embeddings do not transfer well to other text distributions. General embeddings need to be robust to distribution shifts and applying such techniques can probably further improve the results.\nUsing a multimodal approach in order to improve general text embeddings is under-explored and we hope that our results motivate further developments. For example, the fact that the best models are very simple suggests that there is a large headroom in that direction."}], "references": [{"title": "Matching words and pictures", "author": ["K. Barnard", "P. Duygulu", "D. Forsyth", "N. d. Freitas", "D.M. Blei", "M.I. Jordan"], "venue": "Journal of machine learning research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Flickr", "author": ["D. Defreyne"], "venue": "https://www.flickr.com/photos/denisdefreyne/1091487059,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning abstract concept embeddings from multi-modal data: Since you probably can\u2019t see what i mean", "author": ["F. Hill", "A. Korhonen"], "venue": "EMNLP, pages 255\u2013265,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u2013 1780,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "Content-based medical image retrieval with metric learning via rank correlation", "author": ["W. Huang", "K.L. Chan", "H. Li", "J. Lim", "J. Liu", "T.Y. Wong"], "venue": "F. Wang, P. Yan, K. Suzuki, and D. Shen, editors, Machine Learning in Medical Imaging, First International Workshop, MLMI 2010, Held in Conjunction with MICCAI 2010, Beijing, China, September 20, 2010. Proceedings, volume 6357 of Lecture Notes in Computer Science, pages 18\u201325. Springer,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning cross-modality similarity for multinomial data", "author": ["Y. Jia", "M. Salzmann", "T. Darrell"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, pages 2407\u20132414. IEEE,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u20133137,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "Empirical Methods in Natural Language Processing, pages 1746\u20131751,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "CoRR, abs/1411.2539,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["A. Lazaridou", "N.T. Pham", "M. Baroni"], "venue": "CoRR, abs/1501.02598,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "European Conference on Computer Vision, pages 740\u2013755. Springer,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J.R. Finkel", "S. Bethard", "D. McClosky"], "venue": "ACL (System Demonstrations), pages 55\u201360,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Training and evaluating multimodal word embeddings with large-scale web annotated images", "author": ["J. Mao", "J. Xu", "K. Jing", "A.L. Yuille"], "venue": "D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 442\u2013450. Curran Associates, Inc.,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W.-t. Yih", "G. Zweig"], "venue": "In Hlt-naacl,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Flickr", "author": ["J. Moes"], "venue": "https://www.flickr.com/photos/jeroenmoes/4265223393,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, pages 1532\u20131543,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["R. \u0158eh\u016f\u0159ek", "P. Sojka"], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta, Malta, May", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Flickr", "author": ["F. Rosa"], "venue": "https://www.flickr.com/photos/kairos_of_tyre/6318245758,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Show and Tell: A Neural Image Caption Generator", "author": ["C. Shallue"], "venue": "https://github.com/ tensorflow/models/tree/master/im2txt,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Rethinking the inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2818\u20132826,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Order-embeddings of images and language", "author": ["I. Vendrov", "R. Kiros", "S. Fidler", "R. Urtasun"], "venue": "arXiv preprint arXiv:1511.06361,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: Lessons learned from the 2015 MSCOCO image captioning challenge", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CoRR, abs/1609.06647,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deep structure-preserving image-text embeddings", "author": ["L. Wang", "Y. Li", "S. Lazebnik"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5005\u20135013,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards universal paraphrastic sentence embeddings", "author": ["J. Wieting", "M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "arXiv preprint arXiv:1511.08198,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "Photo credit: [17, 22] The problem of finding good representations of text data is a very actively studied area of research.", "startOffset": 14, "endOffset": 22}, {"referenceID": 20, "context": "Photo credit: [17, 22] The problem of finding good representations of text data is a very actively studied area of research.", "startOffset": 14, "endOffset": 22}, {"referenceID": 11, "context": "This extends previous works, for example [12] or [15].", "startOffset": 41, "endOffset": 45}, {"referenceID": 14, "context": "This extends previous works, for example [12] or [15].", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "Despite its simplicity, the model significantly outperforms pure-text models and the best multimodal model from [15] on a set of well established text similarity benchmarks from the SemEval competition [18].", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "Most of them explore how these pairs of data can be leveraged for tasks that require knowledge of both modalities, like captioning or image retrieval [1, 7, 11, 12, 29, 8, 26].", "startOffset": 150, "endOffset": 175}, {"referenceID": 6, "context": "Most of them explore how these pairs of data can be leveraged for tasks that require knowledge of both modalities, like captioning or image retrieval [1, 7, 11, 12, 29, 8, 26].", "startOffset": 150, "endOffset": 175}, {"referenceID": 10, "context": "Most of them explore how these pairs of data can be leveraged for tasks that require knowledge of both modalities, like captioning or image retrieval [1, 7, 11, 12, 29, 8, 26].", "startOffset": 150, "endOffset": 175}, {"referenceID": 11, "context": "Most of them explore how these pairs of data can be leveraged for tasks that require knowledge of both modalities, like captioning or image retrieval [1, 7, 11, 12, 29, 8, 26].", "startOffset": 150, "endOffset": 175}, {"referenceID": 27, "context": "Most of them explore how these pairs of data can be leveraged for tasks that require knowledge of both modalities, like captioning or image retrieval [1, 7, 11, 12, 29, 8, 26].", "startOffset": 150, "endOffset": 175}, {"referenceID": 7, "context": "Most of them explore how these pairs of data can be leveraged for tasks that require knowledge of both modalities, like captioning or image retrieval [1, 7, 11, 12, 29, 8, 26].", "startOffset": 150, "endOffset": 175}, {"referenceID": 24, "context": "Most of them explore how these pairs of data can be leveraged for tasks that require knowledge of both modalities, like captioning or image retrieval [1, 7, 11, 12, 29, 8, 26].", "startOffset": 150, "endOffset": 175}, {"referenceID": 11, "context": "[12] propose to extend the skip-gram algorithm [16] to incorporate image data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[12] propose to extend the skip-gram algorithm [16] to incorporate image data.", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "[4] also took a similar approach before.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "In addition to predicting contextual words, [12]\u2019s models maximize the similarity between image and word embeddings.", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "To inject visual information, [12] add a max margin loss between the image embedding and the word embedding:", "startOffset": 30, "endOffset": 34}, {"referenceID": 10, "context": "[11] also use a max margin loss to co-embed images and corresponding text.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Photo credit: [3, 17, 22]", "startOffset": 14, "endOffset": 25}, {"referenceID": 16, "context": "Photo credit: [3, 17, 22]", "startOffset": 14, "endOffset": 25}, {"referenceID": 20, "context": "Photo credit: [3, 17, 22]", "startOffset": 14, "endOffset": 25}, {"referenceID": 14, "context": "For example, [15] use an Recurrent Neural Network (RNN) as a language model in order to learn word embeddings which are then combined to create a phrase embedding.", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "For their model A, they use a similar setup as the captioning model from [27], with an RNN decoder conditioned on a pre-trained CNN embedding.", "startOffset": 73, "endOffset": 77}, {"referenceID": 22, "context": "The initial state is set to a transformation of the last internal layer of a pre-trained VGGNet [24].", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "Finally, model C develops the multimodal skip-gram [12] by adding an additional loss measuring the distance between the word embeddings and vimage.", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "Finally, for the CNN model, the encoder includes a convolutional layer followed by a fully connected layer, as described in [9].", "startOffset": 124, "endOffset": 127}, {"referenceID": 23, "context": "For encoding images, we use a pre-trained InceptionV3 network [25] which provides a 2048dimensional feature vector for each image in the dataset (images are rescaled and cropped to 300 x 300 px).", "startOffset": 62, "endOffset": 66}, {"referenceID": 28, "context": "It is known that training similarity models directly on text-text pairs yields good results [30] but here we want to investigate only the effect of knowledge transfer from images.", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "MS COCO The MS COCO dataset [13] contains 80 image categories.", "startOffset": 28, "endOffset": 32}, {"referenceID": 21, "context": "We use the MS COCO 2014 dataset using the same train/validation/test split as the im2txt [23] Tensorflow implementation of [28].", "startOffset": 89, "endOffset": 93}, {"referenceID": 26, "context": "We use the MS COCO 2014 dataset using the same train/validation/test split as the im2txt [23] Tensorflow implementation of [28].", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "SBU The Stony Brook University dataset[19] consists of 1M image-caption pairs collected from Flickr, with only one caption per image.", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": "Pinterest5M The original Pinterest40M dataset [15] contains 40M images.", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "The training data in all datasets is lowercased and tokenized using the Stanford Tokenizer [14].", "startOffset": 91, "endOffset": 95}, {"referenceID": 9, "context": "In all models, we train using the Adam optimizer[10], for 10 (MS COCO, SBU) or 5 epochs (Pinterest5M).", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "Unfortunately, we could not compare our models directly on the Gold RP10K dataset introduced by [15] as it was not publicly released.", "startOffset": 96, "endOffset": 100}, {"referenceID": 14, "context": "As a direct comparison, we implementModelA as described in [15], which we refer to as PinModelA.", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "It corresponds to the PureTextRnn baseline from [15].", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": "\u2022 Word2Vec We trained Word2Vec word embeddings [21] where the corpus consists of sentences from MS COCO.", "startOffset": 47, "endOffset": 51}, {"referenceID": 28, "context": "Similarly to [30], we observed that RNN-based encoders are outperformed by a simpler BOW model.", "startOffset": 13, "endOffset": 17}, {"referenceID": 18, "context": "\u2022 Glove: embeddings proposed in [20], trained on a Common Crawl dataset with 840 billion tokens.", "startOffset": 32, "endOffset": 36}, {"referenceID": 11, "context": "\u2022 M-Skip-Gram: embeddings proposed in [12], trained on Wikipedia and a set of images from ImageNet.", "startOffset": 38, "endOffset": 42}, {"referenceID": 28, "context": "\u2022 PP-XXL: the best embeddings from the [30], trained on 9M phrase pairs from PPDB.", "startOffset": 39, "endOffset": 43}, {"referenceID": 1, "context": "\u201cRNN-GRU\u201d and \u201cRNN-LSTM\u201d denote RNN encoders with GRU [2] and LSTM [5] cells, respectively.", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "\u201cRNN-GRU\u201d and \u201cRNN-LSTM\u201d denote RNN encoders with GRU [2] and LSTM [5] cells, respectively.", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "We therefore used its differentiable approximation: SKT\u03b1 [6] defined as SKT\u03b1(x, y) = \u2211 i,j tanh(\u03b1(xi\u2212xj)(yi\u2212yj)) n(n\u22121)/2 for some \u03b1 > 0.", "startOffset": 57, "endOffset": 60}, {"referenceID": 10, "context": "We follow closely the definition in [11].", "startOffset": 36, "endOffset": 40}], "year": 2017, "abstractText": "Generic text embeddings are successfully used in a variety of tasks. However, they are often learnt by capturing the co-occurrence structure from pure text corpora, resulting in limitations of their ability to generalize. In this paper, we explore models that incorporate visual information into the text representation. Based on comprehensive ablation studies, we propose a conceptually simple, yet well performing architecture. It outperforms previous multimodal approaches on a set of well established benchmarks. We also improve the state-of-the-art results for image-related text datasets, using orders of magnitude less data.", "creator": "LaTeX with hyperref package"}}}