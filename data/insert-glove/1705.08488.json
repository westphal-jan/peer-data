{"id": "1705.08488", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Second-Order Word Embeddings from Nearest Neighbor Topological Features", "abstract": "coriolan We lysaght introduce second - 1.7000 order paetsch vector representations zhelyu of words, induced ba873 from nearest 126.35 neighborhood tomm topological hanvdir features lydden in benghalensis pre - trained contextual taskmasters word 89.23 embeddings. multi-state We then eku analyze gregers the effects meuse-argonne of using mackays second - bowerbirds order cosipa embeddings as input picaresque features hardboard in reidemeister two deep natural language processing tamayo models, for thespian named globulus entity recognition thorvaldsen and recognizing dabel@globe.com textual brittania entailment, 3220 as mourier well as a versions linear model sisak for paraphrase antiviral recognition. Surprisingly, we veblen find that 3-58 nearest 49-foot neighbor information ifj alone ram\u00f3n is sufficient aurantium to karelo-finnish capture barz most of unspoiled the performance benefits derived bitran from linschoten using mndot pre - puiseux trained word embeddings. color Furthermore, henham second - kharyaga order energi embeddings yepes are able 19:27 to handle heavyhanded highly heterogeneous 222 data unannounced better than 1,387 first - order tahmasebi representations, hanji though at the r.s.v.p. cost referees of some specificity. garam Additionally, augmenting contextual embeddings anfani with second - oxberry order 600-man information further 70-70 improves ptolemy model performance a-days in coalesces some theoktistos cases. corium Due to barasat variance kingsway in the 108.67 random initializations of elantra word 820,000 embeddings, krzy\u017can\u00f3w utilizing nearest igloos neighbor features moonrise from reedys multiple <first - repurposing order aguila embedding samples can mobsters also coolant contribute to downstream shanshan performance ouverte gains. Finally, sundaravej we identify intriguing puce characteristics .433 of second - sahir order embedding spaces spiral-shaped for further research, including much monch higher density alstead and different tornio semantic marra interpretations larrazabal of wheaties cosine similarity.", "histories": [["v1", "Tue, 23 May 2017 19:12:05 GMT  (36kb,D)", "http://arxiv.org/abs/1705.08488v1", "Submitted to NIPS 2017. (8 pages + 4 reference)"]], "COMMENTS": "Submitted to NIPS 2017. (8 pages + 4 reference)", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["denis newman-griffis", "eric fosler-lussier"], "accepted": false, "id": "1705.08488"}, "pdf": {"name": "1705.08488.pdf", "metadata": {"source": "CRF", "title": "Second-Order Word Embeddings from Nearest Neighbor Topological Features", "authors": ["Denis R. Newman-Griffis", "Eric Fosler-Lussier"], "emails": ["newman-griffis.1@osu.edu,", "fosler@cse.ohio-state.edu"], "sections": [{"heading": "1 Introduction", "text": "Word embeddings are dense, low-dimensional vector representations of words that are commonly used as input features in a variety of natural language processing (NLP) tasks [1]. In contrast to symbolic one-hot or hierarchical clustering\u2013based representations, real-valued embedding vectors easily reflect varying degrees of similarity between words, and significantly reduce sparsity in linear algebra operations. The most common methods for learning word embeddings take an unsupervised approach based on word cooccurrence, using fixed-width context windows within a large text corpus [2, 3, 4, 5]. Compressing this contextual information via neural language modeling gives representations that retain some semantic and syntactic properties of the target words [6, 7, 8], and often lead to large performance gains when used as input features in downstream NLP systems [9, 10].\nThere has been significant recent research on improving the utility of word embeddings as downstream features, by modifying the contextual information used in training [11, 12], by augmenting the embeddings with additional data relevant to the target task [13, 14], or both [15]. A number of other recent studies have approached embeddings from another angle, by trying to analyze exactly what is encoded in the space characterized by the embedded representations [16, 17, 18]. Since embeddings are usually initialized to random locations in the target d-dimensional space, and then trained based on position relative to observed context words, values of individual features are notoriously difficult to interpret. However, a recent study by Linzen [19] illustrated that in some semantic tasks, what matters most is neighborhood structure in the embedding space. Thus, while absolute position is clearly informative, we hypothesize that the relative position of words encodes the most critical information for downstream tasks.\nar X\niv :1\n70 5.\n08 48\n8v 1\n[ cs\n.C L\n] 2\n3 M\nay 2\nIn this work, we present a novel method for deriving second-order word representations from the nearest neighborhood topology of pre-trained word embeddings, and analyze the results of using these representations in downstream NLP applications. In particular, we propose a two-step process of using inducing a k\u2013nearest neighbor graph from pre-trained embeddings,1 where each node is a word, and then using recent methods for unsupervised graph embeddings to re-learn word representations from this graph. We explore using these second-order embeddings in proven models for three downstream tasks: named entity recognition, textual entailment, and paraphrase recognition. We find that replacing first-order contextual embeddings with second-order embeddings as input features yields almost equivalent performance to the original word embeddings, and actually increases recall in some cases. Furthermore, we show that augmenting first-order embeddings with second-order information can improve performance when used in non-linear models, especially on heterogeneous data, although the additional information confuses linear models. Finally, we analyze the changes in the nearest neighborhoods of selected terms between first-order and second-order embeddings, and find that the second-order space is significantly denser than the original contextual embedding space."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Alternate features for training embeddings", "text": "Recent research on improving word embedding performance in downstream tasks has explored a number of different directions. Levy and Goldberg [11] utilized syntactic dependencies as context, and found improved functional similarity and decreased topical sensitivity. Morphological information about target and context words has also been incorporated to improve language model robustness in a number of studies, both at the word level [20, 21, 22] and implicitly at the character level [23]. Additionally, multilingual data has been extensively investigated for improving language models: Upadhyay et al. [24] review several recent methods with multilingual corpora, while Faruqui and Dyer [25] and Lu et al. [26] used canonical correlation analysis to learn cross-lingual embeddings. Recently, Vulic\u0301 [15] combined multilingual corpora with syntactic dependencies in embedding training, and observed improvements in both monolingual and cross-lingual tasks.\nThere has also been significant interest in augmenting contextual embedding learning with information critical to specific downstream tasks. Faruqui et al. [13], Mrks\u0306ic\u0301 et al. [27], and Kim et al. [14] enrich pre-trained embeddings with semantic knowledge via lexical constraints. Tsvetkov et al. [28] find benefits from tailoring the learning curriculum or embedding training to specific downstream tasks, and Rothe et al. [29] project pre-trained embeddings into task-specific subspaces. Finally, Yatbaz et al. [30] use second-order contexts in the form of possible lexical substitutions for word representations; Melamud et al. [31, 32, 10] adapt this approach for embedding learning, by using lexical substitutions to incorporate the joint contexts of two words and extend contextual information in training."}, {"heading": "2.2 Absolute positioning in the embedding space", "text": "There has also been significant research investment in analyzing and interpreting neural models for NLP, and word embeddings in particular. Several studies have found correlations between individual dimensions in a sample embedding set and semantic groupings [16, 33], as well as with predictive outcomes [17, 18]. However, in line with the random initializations used in most embedding training, correlations of specific features vary between studies, and direct interpretability remains elusive.\nAdditionally, Li et al. [34] demonstrated sensitivity of neural NLP models to noise in the input space, and present regularization methods for compositional models to more robustly handle perturbations in input. Several studies have also shown wide variability in the reliability of semantic and syntactic information as encoded linearly in the vector space [35, 36, 37]. Linzen [19] illustrated that many of the successes on similar tasks have relied more on nearest neighborhood structure than consistent affine transformations.\n1 We use cosine similarity to calculate nearest neighbors; all further references to distance or similarity in this paper refer to cosine distance or similarity."}, {"heading": "3 Second-order embeddings", "text": "We present a method to generate second-order word embeddings from the nearest neighborhood structure calculated over a pre-trained embedding vocabulary. Given a set of pre-trained word embeddings V , let NNkv denote the k nearest neighbors of each word v \u2208 V , as calculated by cosine similarity. We then use these k nearest neighbor sets to induce a graph GV over the vocabulary, where each word v is a vertex, and the directed edge (v, w) is added for each w \u2208 NNkv .2 An example induction is shown in Figure 1. Finally, we use node2vec [38], a recent method for learning unsupervised embeddings of graphs nodes based on weighted random walks, to learn second-order embeddings for each word in the vocabulary. This yields a new embedding for each word, based solely on nearest neighbor topological features from the first-order embedding space.\nAs most unsupervised embedding methods use a random initialization, the nearest neighborhood structures may vary between multiple embedding sets; Table 1 shows empirical observations from three embedding samples trained with skip-gram on English Gigaword 5 [39]. However, our graph induction step can be adjusted to accommodate multiple samples of nearest neighborhoods for each word, to make it more robust to random initializations. With sample embedding sets V , we can calculate the nearest neighborhood NNkv,i for each word v in each sample i. The weighted edges in G originating at v are then defined as\nweight(v, w) = 1 |V| \u2211 i f(v, w; i)\nwhere w is an element of \u222aiNNkv,i, and f(v, w; i) is an indicator function that returns 1 if w \u2208 NNkv,i and 0 otherwise. While both the graph induction and second embedding steps increase the hyperparameters to consider in the model, the ability to generate a weighted multi-sample nearest neighborhood graph allows for including first-order embeddings trained with a variety of hyperparameter settings.\n2Since we consider only the k nearest neighbors for each vertex, this may result in graph components that are only connected in one direction."}, {"heading": "4 Experiments", "text": "In order to explore the properties of second-order embeddings, we apply them as input features to proven algorithms for three tasks: named entity recognition (NER), recognizing textual entailment, and paraphrase recognition. For all three tasks, we use existing methods that use word embeddings as input features; for the first two tasks, we use deep and highly non-linear neural models, while the model for paraphrase recognition is a simple logistic regression. Finally, we compare the nearest neighborhood structure of first-order and second-order embeddings.\nTo control for corpus and hyperparameter effects in the different tasks, we use the same sets of embeddings in all applications. Our initial word embeddings are trained on Gigaword; following Lample et al. [40], we remove the New York Times and LA Times portions of the corpus, and train skip-gram embeddings with word2vec [4] for 10 iterations, with vector dimensionality of 100, window size of 8, and minimum word frequency of 4. To induce the nearest neighbor graph, we choose k = 10, and then train second-order embeddings with node2vec, again using an embedding size of 100; all other settings are node2vec defaults. We also experiment with using three samples in the graph induction step, as well as varying the neighborhood size k to k \u2208 {5, 25}."}, {"heading": "4.1 Named entity recognition", "text": "We first evaluate our embeddings on the English NER data from the well-studied CoNLL 2003 shared task [41]. The goal of the task is to take as input unannotated documents and tag within them mentions of persons (PER), locations (LOC), organizations (ORG), or other entities that do not fit in any of these three categories (MISC).\nWe adopt the NER system of Lample et al. [40], which is based on a bidirectional long short-term memory (LSTM) network with a conditional random field (CRF) over the output layer.3 They use character embeddings, learned during training, in addition to pre-initialized word embeddings; for our experiments, we vary the input word embeddings, but do not change character-level behavior.\nTable 2 gives precision, recall, and F-score results for the full test set. The second-order embeddings alone perform nearly as well as word2vec embeddings, achieving 1.5% absolute lower precision, and only 0.5% absolute lower recall. Interestingly, using multiple embedding samples to generate the graph increases precision to nearly 87% at the expense of recall (a drop of over 1% absolute compared to the single sample). Concatenating skip-gram and second-order embeddings gives overall performance similar to skip-gram alone, with slightly lower precision and identical recall (when using a single sample in graph induction) or decreased (with multiple samples or smaller neighborhoods). Increasing the neighborhood size to k = 25 appears to introduce additional noise in the neighborhood graph, and decreases performance by a point across the board.\nThe picture gets more interesting when the results are broken down by named entity type, shown in Table 3. Most strikingly, nearest neighbor information is critical for recognizing MISC entities, leading to 8.4% absolute increase in precision when using multi-sample second order embeddings alone, and similar increases with concatenation. Recall on MISC falls slightly when incorporating\n3 We use their publicly-available implementation: https://github.com/glample/tagger\nsecond-order information, though the noisy k = 25 graph does increase recall by 0.2% absolute. The converse pattern emerges for PER entities, where precision falls by over a point when including any second-order information, but recall increases slightly with single-sample nearest neighbor information. ORG and LOC are less clear, with small variations in precision and recall."}, {"heading": "4.2 Recognizing textual entailment", "text": "For textual entailment, we use the Stanford Natural Language Inference (SNLI) dataset [42]. The dataset consists of 570,152 sentence pairs (550,152 for training, 10k for development, and 10k for testing), each of which is annotated with the label entailment (sentence 1 entails sentence 2), contradiction (sentence 2 contradicts sentence 1), or neutral (sentence 1 does not inform sentence 2). The three labels are roughly equally distributed.\nTo evaluate our embeddings on this task, we use the Long Short-Term Memory Network model proposed by Cheng et al. [43], which they evaluated on several machine reading tasks, including SNLI. We use the publicly-available implementation of their model,4 and compare second-order embeddings with skip-gram embeddings as pre-initialization. We use their implementation\u2019s default settings of a batch size of 40, sentence embedding dimensionality of 450, dropout of 0.5, and learning rate of 0.001, and vary the dimensionality of the input word representations to match our embedding settings. We halt training after 3 iterations.\nTable 4 presents accuracy of the various models over the development and test sets. In contrast to the NER results, here we see an increase in overall performance when augmenting skip-gram embeddings with second-order information. The second-order embeddings alone give only slightly worse results than the skip-gram embeddings, with around a 1% drop in test accuracy. Interestingly, concatenating skip-gram with a single k = 10 second-order embedding decreases performance, but using multiple samples or different settings for k show equivalent or superior performance."}, {"heading": "4.3 Paraphrase recognition", "text": "We also evaluate our embeddings on the task of paraphrase recognition: given two sentences, the task is to decide if sentence 2 is a paraphrase of sentence 1 or not. We use the well-studied Microsoft\n4 https://github.com/cheng6076/SNLI-attention\nResearch Paraphrase Corpus (MSRPC) [44], consisting of 5,801 sentence pairs (4,076 for training, 1,725 for test), each of which is labeled as \u201cequivalent\u201d (3,900 pairs, 67%) or \u201cnot equivalent.\u201d\nWe follow the methodology of Blacoe and Lapata [45] for this task. Specifically, we represent each sentence as the sum5 of the embeddings of its in-vocabulary words; the feature vector for a sentence pair is then either the concatenation or difference of the two sentence embeddings. For classification, we use logistic regression as implemented in LIBLINEAR [46], with a cost parameter of 0.001.6\nTable 5 shows precision, recall, and F1 for each embedding set on the MSRPC test set, under the concatenation and subtraction sentence combination schema. With concatenated feature vectors, precision varies slightly from the first-order baseline in the various concatenated settings, but decreases by 1% absolute when using second-order embeddings alone. Interestingly, recall behaves inversely: it decreases significantly in the concatenated settings, but increases when using second-order embeddings alone. The impact of second-order information in the subtractive combination scheme is less clear; as with concatenation, precision drops with second-order embeddings alone, but recall is most strongly affected by using multiple samples in the graph induction step."}, {"heading": "4.4 Neighborhood analysis", "text": "Table 1 illustrated the variance in nearest neighborhoods between multiple contextual embedding samples. Since nearest neighborhoods remain important in second-order embeddings, we analyzed how these neighborhoods changed from first-order to second-order representations. Table 6 shows the 10 nearest neighbors for zucchini in second-order embedding spaces induced from the samples used in Table 1, along with a space induced from combining all three samples. While the theme of cooking remains the same between the two, and both include a number of ingredients appropriate to combine with zucchini, the second-order samples put more broadly related words near to zucchini, as opposed to words with highly similar usage patterns.\nZucchini is a reasonably frequent word, occurring 245 times in our Gigaword subset. However, the nearest neighborhood graph induction step can be considered as a normalization of distances in the embedding space, in that outliers that had universally low similarity in the first-order space and words with extremely high similarity to their neighbors both end up connected to k neighbor words in the nearest neighbor graph, at an edge distance of 1. This raises the question: what happens to these outliers and dense points in the second-order space? To answer this question, for each word in the original embedding space, we calculated the average of its similarity to its 10 nearest neighbors. Table 7 shows the nearest neighbors in the first and second-order spaces of two words: cibber (frequency 4; the name of a vocal soloist mentioned in two articles on a musical performance), which had one of the lowest such average similarities, and 1976_ferrari7 (frequency 10; discussed in\n5 We also experimented with the alternative point-wise multiplication method they discuss, but the low magnitude of our embeddings made it impractical for all but the shortest sentences. For additive composition, our results mirror their findings with neural language models.\n6 This cost was empirically determined by experimenting on a validation set created by holding out a randomly-selected 20% of the training data. We experimented with c from 1 to 10\u22125, and found the best performance for first-order, second-order, and concatenated embeddings with 0.001.\n7 Some multi-word expressions in the Gigaword corpus were conjoined with underscores, and these were kept in the plaintext version we used for training our embeddings.\nthe context of Formula 1 racing), which had the highest average similarity. Qualitatively, unrelated nearest neighbors in the first-order space lead to unrelated nearest neighbors in the second-order space, while highly related neighbors stay nearby.\nIn a quantitative analysis, however, the distance normalization has a much clearer effect. We took the 10 words in each of the first-order and second-order embedding spaces with maximal or minimal average similarity to its nearest neighbors, and compiled the similarities of each word to its 10 nearest neighbors (yielding 100 maximal similarity sample points, and 100 minimal samples). Figure 2 shows the distribution of these minimal and maximal similarities in each space. We see that the second-order space is strikingly denser than the first-order space: the lowest pairwise similarity of a word to one of its nearest neighbors in the second-order space is 0.75, in contrast to 0.24 in the first order space. Moreover, already dense areas of the first-order space become appreciably denser, with all 100 of the top pairwise similarities clustering around 0.999."}, {"heading": "5 Discussion", "text": "Second-order embeddings retain a surprisingly high degree of the discriminative signal encoded by first-order embeddings, as measured by their performance on our semantic tasks. The consistency of second-order embedding performance relative to first-order embeddings, typically only differing by 1 to 2 points absolute in both the deep and linear models, suggests that the nearest neighborhood topology of an embedding space contains the lion\u2019s share of the important information for these tasks, independent of the values of individual features. This holds true in spite of the greater density of second-order embeddings, and the different semantic correlations we observe between nearby points.\nHowever, a couple of specific performance differences stand out when comparing non-concatenated first-order and second-order embeddings. On the NER task, the distinct increase in precision for MISC entities (which are highly heterogeneous in their textual realizations), and a large drop in precision for the much more consistent PER category, suggest that the second-order embeddings handle variance in textual patterns more effectively, but with a corresponding loss of specificity. This is reflected as well in the paraphrase recognition results, where second-order representations trade lower precision for higher recall.\nFurthermore, concatenating the different embeddings poses an interesting challenge for use in downstream tasks. The large drop in performance when using concatenated vectors in the linear model for paraphrase recognition indicates that not only do the second-order embeddings contain different information from the first-order vectors, but that aligning the two sets of signals with a linear transformation is a challenge. The highly non-linear models for NER and textual entailment, on the other hand, can adjust to the combined space, and in the latter case even benefit from the dual signals."}, {"heading": "6 Conclusion", "text": "We introduced second-order word embeddings, derived from the nearest neighborhood topology of context-based word representations. We analyzed the effects of using these embeddings in existing models for named entity recognition, recognizing textual entailment, and paraphrase recognition, both as the second-order information alone and concatenated with the first-order contextual embeddings. Our analysis demonstrated that second-order embeddings yield similar performance to their first-order counterparts, often trading some specificity (reflected in decreased precision values) for improved handling of heterogeneous data (reflected in increased recall). Furthermore, we illustrated that the second-order embedding space is much denser than its first-order version, and we found that high second-order similarity is more indicative of broad relatedness than contextual similarity.\nOur findings suggest that second-order embeddings are an intriguing area for further research. In particular, the higher recall we observe on all three tasks indicates that second-order embeddings contain valuable information for reliably dealing with heterogeneous data. It is clear that non-linear transformations help in combining this information with the direct contextual signals from first-order embeddings, but how best to find that combination remains an open question. Additionally, the ability to derive a single second-order representation from multiple samples, and the often superior performance achieved by doing so, suggests that this method could be used to reduce some of the variance we observe between different contextual embedding samples trained on the same data."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Adam Stiff and Chunxiao Zhou for helpful discussions, as well as the Ohio Supercomputer Center [47] for use of experimental resources.\nDenis is a pre-doctoral fellow at the National Institutes of Health, Clinical Center."}], "references": [{"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["Peter D Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "A Neural Probabilistic Language Model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change", "author": ["William L. Hamilton", "Jure Leskovec", "Dan Jurafsky"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "The Interplay of Semantics and Morphology in Word Embeddings", "author": ["Oded Avraham", "Yoav Goldberg"], "venue": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2017}, {"title": "Word Representations: A Simple and General Method for Semi-Supervised Learning", "author": ["Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "The Role of Context Types and Dimensionality in Learning Word Embeddings. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT \u201916", "author": ["Oren Melamud", "David McClosky", "Siddharth Patwardhan", "Mohit Bansal"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Dependency-Based Word Embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 302\u2013308", "author": ["Omer Levy", "Yoav Goldberg"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Symmetric Patterns and Coordinations: Fast and Enhanced Representations of Verbs and Adjectives", "author": ["Roy Schwartz", "Roi Reichart", "Ari Rappoport"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Retrofitting Word Vectors to Semantic Lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Intent detection using semantically enriched word embeddings", "author": ["Joo-Kyung Kim", "Gokhan Tur", "Asli Celikyilmaz", "Bin Cao", "Ye-Yi Wang"], "venue": "IEEE Spoken Language Technology Workshop (SLT),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Cross-Lingual Syntactically Informed Distributed Word Representations. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 408\u2013414", "author": ["Ivan Vuli\u0107"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "A Compositional and Interpretable Semantic Space", "author": ["Alona Fyshe", "Leila Wehbe", "Partha P Talukdar", "Brian Murphy", "Tom M Mitchell"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Visualizing and Understanding Neural Models in NLP", "author": ["Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Understanding Neural Networks through Representation Erasure", "author": ["Jiwei Li", "Will Monroe", "Dan Jurafsky"], "venue": "arXiv preprint arXiv:1612.08220,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Issues in evaluating semantic spaces using word analogies", "author": ["Tal Linzen"], "venue": "In Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Better Word Representations with Recursive Neural Networks for Morphology", "author": ["Thang Luong", "Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Morphological Word-Embeddings", "author": ["Ryan Cotterell", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Implicitly Incorporating Morphological Information into Word Embedding", "author": ["Yang Xu", "Jiawei Liu"], "venue": "arXiv preprint arXiv:1701.02481,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2017}, {"title": "Character-aware Neural Language Models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Cross-lingual Models of Word Embeddings: An Empirical Comparison", "author": ["Shyam Upadhyay", "Manaal Faruqui", "Chris Dyer", "Dan Roth"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Improving Vector Space Word Representations Using Multilingual Correlation", "author": ["Manaal Faruqui", "Chris Dyer"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Deep Multilingual Correlation for Improved Word Embeddings", "author": ["Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Counter-fitting Word Vectors to Linguistic Constraints", "author": ["Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Lina Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Yulia Tsvetkov", "Manaal Faruqui", "Wang Ling", "Brian MacWhinney", "Chris Dyer"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Ultradense Word Embeddings by Orthogonal Transformation", "author": ["Sascha Rothe", "Sebastian Ebert", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Learning Syntactic Categories Using Paradigmatic Representations of Word Context", "author": ["Mehmet Ali Yatbaz", "Enis Sert", "Deniz Yuret"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Probabilistic Modeling of Joint-context in Distributional Similarity", "author": ["Oren Melamud", "Ido Dagan", "Jacob Goldberger", "Idan Szpektor", "Deniz Yuret"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Modeling Word Meaning in Context with Substitute Vectors", "author": ["Oren Melamud", "Ido Dagan", "Jacob Goldberger"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Sparse Overcomplete Word Vector Representations", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Dani Yogatama", "Chris Dyer", "Noah A Smith"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Learning Robust Representations of Text", "author": ["Yitong Li", "Trevor Cohn", "Timothy Baldwin"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Intrinsic Evaluation of Word Vectors Fails to Predict Extrinsic Performance", "author": ["Billy Chiu", "Anna Korhonen", "Sampo Pyysalo"], "venue": "Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "Analogy-based Detection of Morphological and Semantic Relations With Word Embeddings: What Works and What Doesn\u2019t", "author": ["Anna Gladkova", "Aleksandr Drozd", "Satoshi Matsuoka"], "venue": "Proceedings of the NAACL Student Research Workshop,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Word Embeddings, Analogies, and Machine Learning: Beyond king - man + woman = queen", "author": ["Aleksandr Drozd", "Anna Gladkova", "Satoshi Matsuoka"], "venue": "In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Node2Vec: Scalable Feature Learning for Networks", "author": ["Aditya Grover", "Jure Leskovec"], "venue": "In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Neural Architectures for Named Entity Recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition", "author": ["Erik F Tjong Kim Sang", "Fien De Meulder"], "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2003}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Long Short-Term Memory-Networks for Machine Reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources", "author": ["Bill Dolan", "Chris Quirk", "Chris Brockett"], "venue": "In Proceedings of Coling", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2004}, {"title": "A Comparison of Vector-based Representations for Semantic Composition", "author": ["William Blacoe", "Mirella Lapata"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP- CoNLL \u201912),", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "LIB- LINEAR: A Library for Large Linear Classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Word embeddings are dense, low-dimensional vector representations of words that are commonly used as input features in a variety of natural language processing (NLP) tasks [1].", "startOffset": 172, "endOffset": 175}, {"referenceID": 1, "context": "The most common methods for learning word embeddings take an unsupervised approach based on word cooccurrence, using fixed-width context windows within a large text corpus [2, 3, 4, 5].", "startOffset": 172, "endOffset": 184}, {"referenceID": 2, "context": "The most common methods for learning word embeddings take an unsupervised approach based on word cooccurrence, using fixed-width context windows within a large text corpus [2, 3, 4, 5].", "startOffset": 172, "endOffset": 184}, {"referenceID": 3, "context": "The most common methods for learning word embeddings take an unsupervised approach based on word cooccurrence, using fixed-width context windows within a large text corpus [2, 3, 4, 5].", "startOffset": 172, "endOffset": 184}, {"referenceID": 4, "context": "The most common methods for learning word embeddings take an unsupervised approach based on word cooccurrence, using fixed-width context windows within a large text corpus [2, 3, 4, 5].", "startOffset": 172, "endOffset": 184}, {"referenceID": 5, "context": "Compressing this contextual information via neural language modeling gives representations that retain some semantic and syntactic properties of the target words [6, 7, 8], and often lead to large performance gains when used as input features in downstream NLP systems [9, 10].", "startOffset": 162, "endOffset": 171}, {"referenceID": 6, "context": "Compressing this contextual information via neural language modeling gives representations that retain some semantic and syntactic properties of the target words [6, 7, 8], and often lead to large performance gains when used as input features in downstream NLP systems [9, 10].", "startOffset": 162, "endOffset": 171}, {"referenceID": 7, "context": "Compressing this contextual information via neural language modeling gives representations that retain some semantic and syntactic properties of the target words [6, 7, 8], and often lead to large performance gains when used as input features in downstream NLP systems [9, 10].", "startOffset": 162, "endOffset": 171}, {"referenceID": 8, "context": "Compressing this contextual information via neural language modeling gives representations that retain some semantic and syntactic properties of the target words [6, 7, 8], and often lead to large performance gains when used as input features in downstream NLP systems [9, 10].", "startOffset": 269, "endOffset": 276}, {"referenceID": 9, "context": "Compressing this contextual information via neural language modeling gives representations that retain some semantic and syntactic properties of the target words [6, 7, 8], and often lead to large performance gains when used as input features in downstream NLP systems [9, 10].", "startOffset": 269, "endOffset": 276}, {"referenceID": 10, "context": "There has been significant recent research on improving the utility of word embeddings as downstream features, by modifying the contextual information used in training [11, 12], by augmenting the embeddings with additional data relevant to the target task [13, 14], or both [15].", "startOffset": 168, "endOffset": 176}, {"referenceID": 11, "context": "There has been significant recent research on improving the utility of word embeddings as downstream features, by modifying the contextual information used in training [11, 12], by augmenting the embeddings with additional data relevant to the target task [13, 14], or both [15].", "startOffset": 168, "endOffset": 176}, {"referenceID": 12, "context": "There has been significant recent research on improving the utility of word embeddings as downstream features, by modifying the contextual information used in training [11, 12], by augmenting the embeddings with additional data relevant to the target task [13, 14], or both [15].", "startOffset": 256, "endOffset": 264}, {"referenceID": 13, "context": "There has been significant recent research on improving the utility of word embeddings as downstream features, by modifying the contextual information used in training [11, 12], by augmenting the embeddings with additional data relevant to the target task [13, 14], or both [15].", "startOffset": 256, "endOffset": 264}, {"referenceID": 14, "context": "There has been significant recent research on improving the utility of word embeddings as downstream features, by modifying the contextual information used in training [11, 12], by augmenting the embeddings with additional data relevant to the target task [13, 14], or both [15].", "startOffset": 274, "endOffset": 278}, {"referenceID": 15, "context": "A number of other recent studies have approached embeddings from another angle, by trying to analyze exactly what is encoded in the space characterized by the embedded representations [16, 17, 18].", "startOffset": 184, "endOffset": 196}, {"referenceID": 16, "context": "A number of other recent studies have approached embeddings from another angle, by trying to analyze exactly what is encoded in the space characterized by the embedded representations [16, 17, 18].", "startOffset": 184, "endOffset": 196}, {"referenceID": 17, "context": "A number of other recent studies have approached embeddings from another angle, by trying to analyze exactly what is encoded in the space characterized by the embedded representations [16, 17, 18].", "startOffset": 184, "endOffset": 196}, {"referenceID": 18, "context": "However, a recent study by Linzen [19] illustrated that in some semantic tasks, what matters most is neighborhood structure in the embedding space.", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "Levy and Goldberg [11] utilized syntactic dependencies as context, and found improved functional similarity and decreased topical sensitivity.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "Morphological information about target and context words has also been incorporated to improve language model robustness in a number of studies, both at the word level [20, 21, 22] and implicitly at the character level [23].", "startOffset": 168, "endOffset": 180}, {"referenceID": 20, "context": "Morphological information about target and context words has also been incorporated to improve language model robustness in a number of studies, both at the word level [20, 21, 22] and implicitly at the character level [23].", "startOffset": 168, "endOffset": 180}, {"referenceID": 21, "context": "Morphological information about target and context words has also been incorporated to improve language model robustness in a number of studies, both at the word level [20, 21, 22] and implicitly at the character level [23].", "startOffset": 168, "endOffset": 180}, {"referenceID": 22, "context": "Morphological information about target and context words has also been incorporated to improve language model robustness in a number of studies, both at the word level [20, 21, 22] and implicitly at the character level [23].", "startOffset": 219, "endOffset": 223}, {"referenceID": 23, "context": "[24] review several recent methods with multilingual corpora, while Faruqui and Dyer [25] and Lu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[24] review several recent methods with multilingual corpora, while Faruqui and Dyer [25] and Lu et al.", "startOffset": 85, "endOffset": 89}, {"referenceID": 25, "context": "[26] used canonical correlation analysis to learn cross-lingual embeddings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Recently, Vuli\u0107 [15] combined multilingual corpora with syntactic dependencies in embedding training, and observed improvements in both monolingual and cross-lingual tasks.", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "[13], Mrks\u0306i\u0107 et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27], and Kim et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] enrich pre-trained embeddings with semantic knowledge via lexical constraints.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] find benefits from tailoring the learning curriculum or embedding training to specific downstream tasks, and Rothe et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] project pre-trained embeddings into task-specific subspaces.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] use second-order contexts in the form of possible lexical substitutions for word representations; Melamud et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31, 32, 10] adapt this approach for embedding learning, by using lexical substitutions to incorporate the joint contexts of two words and extend contextual information in training.", "startOffset": 0, "endOffset": 12}, {"referenceID": 31, "context": "[31, 32, 10] adapt this approach for embedding learning, by using lexical substitutions to incorporate the joint contexts of two words and extend contextual information in training.", "startOffset": 0, "endOffset": 12}, {"referenceID": 9, "context": "[31, 32, 10] adapt this approach for embedding learning, by using lexical substitutions to incorporate the joint contexts of two words and extend contextual information in training.", "startOffset": 0, "endOffset": 12}, {"referenceID": 15, "context": "Several studies have found correlations between individual dimensions in a sample embedding set and semantic groupings [16, 33], as well as with predictive outcomes [17, 18].", "startOffset": 119, "endOffset": 127}, {"referenceID": 32, "context": "Several studies have found correlations between individual dimensions in a sample embedding set and semantic groupings [16, 33], as well as with predictive outcomes [17, 18].", "startOffset": 119, "endOffset": 127}, {"referenceID": 16, "context": "Several studies have found correlations between individual dimensions in a sample embedding set and semantic groupings [16, 33], as well as with predictive outcomes [17, 18].", "startOffset": 165, "endOffset": 173}, {"referenceID": 17, "context": "Several studies have found correlations between individual dimensions in a sample embedding set and semantic groupings [16, 33], as well as with predictive outcomes [17, 18].", "startOffset": 165, "endOffset": 173}, {"referenceID": 33, "context": "[34] demonstrated sensitivity of neural NLP models to noise in the input space, and present regularization methods for compositional models to more robustly handle perturbations in input.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Several studies have also shown wide variability in the reliability of semantic and syntactic information as encoded linearly in the vector space [35, 36, 37].", "startOffset": 146, "endOffset": 158}, {"referenceID": 35, "context": "Several studies have also shown wide variability in the reliability of semantic and syntactic information as encoded linearly in the vector space [35, 36, 37].", "startOffset": 146, "endOffset": 158}, {"referenceID": 36, "context": "Several studies have also shown wide variability in the reliability of semantic and syntactic information as encoded linearly in the vector space [35, 36, 37].", "startOffset": 146, "endOffset": 158}, {"referenceID": 18, "context": "Linzen [19] illustrated that many of the successes on similar tasks have relied more on nearest neighborhood structure than consistent affine transformations.", "startOffset": 7, "endOffset": 11}, {"referenceID": 37, "context": "Finally, we use node2vec [38], a recent method for learning unsupervised embeddings of graphs nodes based on weighted random walks, to learn second-order embeddings for each word in the vocabulary.", "startOffset": 25, "endOffset": 29}, {"referenceID": 38, "context": "[40], we remove the New York Times and LA Times portions of the corpus, and train skip-gram embeddings with word2vec [4] for 10 iterations, with vector dimensionality of 100, window size of 8, and minimum word frequency of 4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[40], we remove the New York Times and LA Times portions of the corpus, and train skip-gram embeddings with word2vec [4] for 10 iterations, with vector dimensionality of 100, window size of 8, and minimum word frequency of 4.", "startOffset": 117, "endOffset": 120}, {"referenceID": 39, "context": "We first evaluate our embeddings on the English NER data from the well-studied CoNLL 2003 shared task [41].", "startOffset": 102, "endOffset": 106}, {"referenceID": 38, "context": "[40], which is based on a bidirectional long short-term memory (LSTM) network with a conditional random field (CRF) over the output layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "For textual entailment, we use the Stanford Natural Language Inference (SNLI) dataset [42].", "startOffset": 86, "endOffset": 90}, {"referenceID": 41, "context": "[43], which they evaluated on several machine reading tasks, including SNLI.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "Research Paraphrase Corpus (MSRPC) [44], consisting of 5,801 sentence pairs (4,076 for training, 1,725 for test), each of which is labeled as \u201cequivalent\u201d (3,900 pairs, 67%) or \u201cnot equivalent.", "startOffset": 35, "endOffset": 39}, {"referenceID": 43, "context": "We follow the methodology of Blacoe and Lapata [45] for this task.", "startOffset": 47, "endOffset": 51}, {"referenceID": 44, "context": "For classification, we use logistic regression as implemented in LIBLINEAR [46], with a cost parameter of 0.", "startOffset": 75, "endOffset": 79}], "year": 2017, "abstractText": "We introduce second-order vector representations of words, induced from nearest neighborhood topological features in pre-trained contextual word embeddings. We then analyze the effects of using second-order embeddings as input features in two deep natural language processing models, for named entity recognition and recognizing textual entailment, as well as a linear model for paraphrase recognition. Surprisingly, we find that nearest neighbor information alone is sufficient to capture most of the performance benefits derived from using pre-trained word embeddings. Furthermore, second-order embeddings are able to handle highly heterogeneous data better than first-order representations, though at the cost of some specificity. Additionally, augmenting contextual embeddings with second-order information further improves model performance in some cases. Due to variance in the random initializations of word embeddings, utilizing nearest neighbor features from multiple first-order embedding samples can also contribute to downstream performance gains. Finally, we identify intriguing characteristics of second-order embedding spaces for further research, including much higher density and different semantic interpretations of cosine similarity.", "creator": "LaTeX with hyperref package"}}}