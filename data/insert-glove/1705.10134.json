{"id": "1705.10134", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "On Residual CNN in text-dependent speaker verification task", "abstract": "waugh Deep learning 47.30 approaches rosscarbery are still opals not hastert very citizenship common oysters in the speaker syf verification m\u00fcnchausen field. shantiniketan We investigate the possibility panpipes of 20million using tacis deep kubenka residual convolutional 2,578 neural o'berry network leftarm with re-invention spectrograms eois as darnestown an cr\u00e9puscule input features in the tasered text - dependent josaia speaker veto verification suluk task. chokes Despite east-north-east the norn fact tamandar\u00e9 that we reginae were not lupberger able to surpass the goldrich baseline system loggins in quality, mfdc we ljubic achieved micrometre a decoupled quite autogyros good infonie results cuffe for such a reisz new approach getting an 5. 23 \\% ERR on the haoved RSR2015 evaluation part. deblieux Fusion of neb the baseline and dharti proposed unpersuasive systems outperformed the mokanji best individual sharptooth system by 18 \\% rcds relatively.", "histories": [["v1", "Mon, 29 May 2017 11:50:57 GMT  (1699kb,D)", "https://arxiv.org/abs/1705.10134v1", "Accepted for Specom 2017"], ["v2", "Tue, 30 May 2017 13:17:47 GMT  (1708kb,D)", "http://arxiv.org/abs/1705.10134v2", "Accepted for Specom 2017"]], "COMMENTS": "Accepted for Specom 2017", "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["egor malykh", "sergey novoselov", "oleg kudashev"], "accepted": false, "id": "1705.10134"}, "pdf": {"name": "1705.10134.pdf", "metadata": {"source": "CRF", "title": "On Residual CNN in text-dependent speaker verification task", "authors": ["Egor Malykh", "Sergey Novoselov", "Oleg Kudashev"], "emails": ["kudashev}@speechpro.com"], "sections": [{"heading": null, "text": "Keywords: speaker verification, residual learning, CNN, FFT"}, {"heading": "1 Introduction", "text": "I-vector systems are well-known for being state-of-the-art solutions to the textindependent speaker verification task [1\u20133, 21]. Recently, the solution of this task has increasingly been considered from the perspective of deep learning approaches. For instance, ASR deep neural network (DNN) model [3,22] divides the acoustic space into senone classes and discriminates the speakers in this space using the classic total variability (TV) model [1]. In such phonetic discriminative DNN based systems two main approaches can be distinguished. The first is to use DNN posteriors to calculate Baum-Welch statistics, and the second is to use the bottleneck features in combination with speaker specific features (MFCC) for training the full TV-UBM system. The second approach is considered the most robust to varying conditions [4].\nAs demonstrated by recent publications [6,8\u201310,23], substantial success of the state-of-the-art text-dependent verification systems is mainly due to the progress in text-independent speaker recognition task. Thus, the success of the phonetic discriminative DNN in such a task leads to attempts to use similar approach in text-dependent systems [5, 11,16].\nIn parallel, there are several studies on the use of Deep-Learning approaches aiming to create an end-to-end solutions for discriminating speakers directly in a text-dependent task [13, 14]. Such approaches are easily applicable when the duration of the considered utterances is small, since they can be fed as an input of a deep architecture entirely, for example as a spectrogram.\nar X\niv :1\n70 5.\n10 13\n4v 2\n[ cs\n.S D\n] 3\n0 M\nay 2\n2 A speaker discriminative approach is the most natural way for speaker verification. [12] describes a DNN for extracting a small speaker footprint which can be used to discriminate between speakers.\nIn this paper we investigate the deep residual CNN [15] for direct speaker discrimination. Unlike [14] we focus on the use of spectrograms instead of MFCC as the input features and deep but light residual architecture instead of VGG-like network as the mapping."}, {"heading": "2 Baseline", "text": "A standard i-vector system is used as the baseline in our experiments. The ivector system models a speech utterance as a low dimensional vector of channeland speaker-dependent factors using total variability approach, as follows:\ns = \u00b5+ Tw,\nwhere s is the mean supervector, \u00b5 is the mean supervector of an Universal Background Model (UBM), T is a low rank matrix and w is the i-vector estimated using the Factor Analysis method [1].\nWe used implementation of the back-end from [16]. All i-vectors are length normalized and further regularized using the phrase-dependent Within-class Covariance Normalization (WCCN). A simple cosine distance scoring is used followed by phrase-dependent s-norm score normalization [10].\n19 Mel-Frequency Cepstral Coefficients (MFCC) + log energy is used as the baseline features. They are normalized by mean and variance and augmented with \u2206 and \u2206\u2206. For this system we did not apply voice activity detection."}, {"heading": "3 CNN", "text": ""}, {"heading": "3.1 Features", "text": "We use the normalized log power magnitude spectrum obtained via Fast Fourier Transform (FFT) as the input acoustic features for this system. Spectrograms are extracted with the following parameters: window size is 256, step size is 64 and Blackman window function is used. Example of such spectrogram is shown in Figure 1.\nThe length of the spectrogram along the frequency axis is fixed, but the length along the time axis varies depending on the utterance. However, CNN requires a constant-size image as the input. In order to satisfy this requirement we use the following technique. Images longer than 800 pixels wide are cropped. Images shorter than 800 pixels wide are complimented to the right by their own copy. Such cropping and padding technique is illustrated in Figure 2.\n3"}, {"heading": "3.2 Residual architecture", "text": "Spectrograms, being two-dimensional tensors, can be considered as images and can be processed by methods used for image processing. Currently, the best convolutional architecture for solving image processing tasks is a Residual CNN [15]. Residual architecture is described in [15, 20] as a stack of several residual units. Residual unit is a mapping\nxl+1 = xl + F(xl,Wl),\nwhere xl and xl+1 are the unit\u2019s input and output. F consists of two 3 \u00d7 3 convolutions with weightsWl. Additive \"shortcut connection\" allows the network to satisfy the basic property: adding more layers does not lead to a degradation of the network. Thus, it becomes possible to train very deep networks with a size of 152 or more layers, as shown in the [15]. For this study, a network with 18 layers from [15] with modifications from [20] was used. Network architecture is shown in table 1. The structure of basic residual block is presented in figure 3.\n4\n5"}, {"heading": "4 Experimental setup", "text": ""}, {"heading": "4.1 RSR2015 corpus", "text": "In our experiments we use the RSR2015 database [7]. The RSR2015 provides data for three main use-case verification scenarios:\n\u2013 unique pass-phrase: each client pronounces the same pass-phrase, \u2013 user-dependent pass-phrase: each client pronounces his or her own pass-\nphrase, \u2013 prompted text: each client pronounces a sentence prompted by the system.\nIn this paper, our focus is on the first use-case where each speaker pronounces a particular sentence. The RSR2015 database contains audio recordings from 300 speakers (143 female and 157 male). There are 9 sessions for each of the participants. Each session consists of 30 short sentences. The database is collected in the office environment using six different portable recording devices (four smartphones and two tablets). Each speaker was recorded using three random different devices out of the six.\nThe database is randomly split into three non-overlapping groups of speakers, one for background training, one for development stage and one for evaluation\n6 stage. The number of male/female speakers is balanced for each group: 50/47 in the background set, 50/47 in the development set and 57/49 in the evaluation set.\nWe use the background set only for training our speaker verification systems. The development set is used to estimate calibration and fusion parameters. All test trials are performed on the evaluation set.\nWe focuse only on the scenario where the speaker pronounces correct passphrase. All experiments are conducted according to the part 1 protocols of the RSR2015 database. We consider pooled male and female trials for system performance measure.\nExtended training set which contains the background and development sets is used in additional experiment."}, {"heading": "4.2 Baseline", "text": "Parameters of WCCN matrix and i-vector extractor are estimated using the background subset of the RSR2015 corpus only. As described in [16], we use the following representation of the WCCN matrix:\nW =W + 1\n2 E,\nwhere E is the unit matrix of appropriate dimensionality. This trick helps to prevent an overfitting despite the small number of speakers in the background subset."}, {"heading": "4.3 CNN", "text": "CNN is implemented using the Keras framework [17] on top of the TensorFlow [18] backend. ADAM optimizer [19] with learning rate set at 10\u22124 is used for training\nNetwork is trained to discriminate between all speakers in training set using the softmax layer and categorical cross-entropy loss function. In the evaluation phase an output from the 512-dimensional (same as i-vector) penultimate layer is used as the embedding corresponding to the input utterance."}, {"heading": "5 Results and discussion", "text": "The result of our research is presented in Table 2 in terms of the Equal Error Rate (EER) and the minimum detection cost function (minDCF) with Ptar = 10\u22123. Baseline system demonstrated a very good result with an EER of less than 1% which is comparable with the result from [16]. Deep CNN system achieved an EER of 6.02%. Fusion of this two systems shows 18% relative improvement over the baseline system which is the evidence of the fact that classic i-vector systems and deep learning systems results in decorrelated embeddings and thus can be used together.\nRelatively poor performance of the system under investigation can be explained by the small size of the training set (97 speakers). Such conditions leads to overfitting of discriminative model. The hypothesis is that the deep residual CNN requires much more data for training and expanding training set will lead to a significant increase in accuracy. Experiments on the extended training set (194 speakers) sustains it resulting in an 5.23% EER. We hope that deep learning approaches will be able to outperform the i-vector based systems in the future.\nFigure 5 illustrates the projection of CNN embeddings of the 9 randomly chosen speakers on two principal axis using the Principal Component Analysis. DET-curves of the all considered methods are shown in Figure 4."}, {"heading": "6 Conclusion", "text": "In this paper, we presented studies of deep residual CNN architecture in the task of text-dependent verification. Raw normalized spectrograms of speech signals is used as the input features. Experiments conducted on Part 1 of the RSR2015 database showed that despite the small amount of training data, it is possible to train a deep speaker embeddings extractor, which makes it possible to separate the speaker classes fairly well. Best achieved result of the individual system is an 5.23% EER.\n8\nWe also showed that increasing the amount of training data leads to the expected strengthening of the extractor and improvement of the results. Our future work will be focused on the improving the quality of deep CNN based systems and bringing them to the level of baseline i-vector systems. It can be noted already that fusion of the deep CNN and i-vector extractors gives a good performance gain of 18% relative improvement."}, {"heading": "7 Acknowledgements", "text": "This work was financially supported by the Ministry of Education and Science of the Russian Federation, contract 14.578.21.0126 (ID RFMEFI57815X0126)."}], "references": [{"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P.J. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "A study of interspeaker variability in speaker verification", "author": ["P. Kenny", "P. Ouellet", "N. Dehak", "V. Gupta", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "A novel scheme for speaker recognition using a phonetically-aware deep neural network", "author": ["Y. Lei", "N. Scheffer", "L. Ferrer", "McLaren", "May"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Advances in deep neural network approaches to speaker recognition", "author": ["M. McLaren", "Y. Lei", "Ferrer", "April"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Text-dependent speaker recognition using PLDA with uncertainty propagation", "author": ["T. Stafylakis", "P. Kenny", "P. Ouellet", "J. Perez", "M. Kockmann", "P. Dumouchel"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Database for Text-Dependent Speaker Verification using Multiple Pass-Phrases", "author": ["A. Larcher", "K.A. Lee", "B. Ma", "Li", "H. (2012", "September). RSR2015"], "venue": "In INTERSPEECH", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1583}, {"title": "Text-dependent speaker verification: Classifiers, databases and RSR2015", "author": ["A. Larcher", "K.A. Lee", "B. Ma", "H. Li"], "venue": "Speech Communication,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Text dependent speaker verification using a small development", "author": ["H. Aronowitz"], "venue": "set. In Odyssey 2012-The Speaker and Language Recognition Workshop", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Textdependent GMM-JFA system for password based speaker verification", "author": ["S. Novoselov", "T. Pekhovsky", "A. Shulipa", "Sholokhov", "May"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Analysis of DNN approaches to speaker identification", "author": ["P. Mat\u011bjka", "O. Glembek", "O. Novotn\u00fd", "O. Plchot", "F. Gr\u00e9zl", "L. Burget", "Cernock\u00fd", "J. H", "March"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Deep neural networks for small footprint text-dependent speaker verification", "author": ["E. Variani", "X. Lei", "E. McDermott", "I.L. Moreno", "Gonzalez-Dominguez", "May"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "End-to-end text-dependent speaker verification", "author": ["G. Heigold", "I. Moreno", "S. Bengio", "Shazeer", "March"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "2016, December). Endto-End attention based text-dependent speaker verification", "author": ["S.X. Zhang", "Z. Chen", "Y. Zhao", "J. Li", "Y. Gong"], "venue": "In Spoken Language Technology Workshop (SLT),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Deep Neural Networks and Hidden Markov Models in i-vector-based Text-Dependent Speaker Verification", "author": ["H. Zeinali", "L. Burget", "H. Sameti", "O. Glembek", "Plchot", "June"], "venue": "In Odyssey-The Speaker and Language Recognition Workshop", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "S. Ghemawat"], "venue": "arXiv preprint arXiv:1603.04467", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "Sun", "October"], "venue": "In European Conference on Computer Vision (pp", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Non-linear PLDA for i-vector speaker verification", "author": ["S. Novoselov", "T. Pekhovsky", "O. Kudashev", "V. Mendelev", "A. Prudnikov"], "venue": "Proc. of the Annual Conference of the International Speech Communication", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Usage of DNN in speaker recognition: advantages and problems", "author": ["O. Kudashev", "S. Novoselov", "T. Pekhovsky", "K. Simonchik", "Lavrentyeva", "July"], "venue": "In International Symposium on Neural Networks (pp", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Pldabased system for text-prompted password speaker verification", "author": ["S. Novoselov", "T. Pekhovsky", "A. Shulipa", "Kudashev", "August"], "venue": "In Advanced Video and Signal Based Surveillance (AVSS),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "I-vector systems are well-known for being state-of-the-art solutions to the textindependent speaker verification task [1\u20133, 21].", "startOffset": 118, "endOffset": 127}, {"referenceID": 1, "context": "I-vector systems are well-known for being state-of-the-art solutions to the textindependent speaker verification task [1\u20133, 21].", "startOffset": 118, "endOffset": 127}, {"referenceID": 2, "context": "I-vector systems are well-known for being state-of-the-art solutions to the textindependent speaker verification task [1\u20133, 21].", "startOffset": 118, "endOffset": 127}, {"referenceID": 18, "context": "I-vector systems are well-known for being state-of-the-art solutions to the textindependent speaker verification task [1\u20133, 21].", "startOffset": 118, "endOffset": 127}, {"referenceID": 2, "context": "For instance, ASR deep neural network (DNN) model [3,22] divides the acoustic space into senone classes and discriminates the speakers in this space using the classic total variability (TV) model [1].", "startOffset": 50, "endOffset": 56}, {"referenceID": 19, "context": "For instance, ASR deep neural network (DNN) model [3,22] divides the acoustic space into senone classes and discriminates the speakers in this space using the classic total variability (TV) model [1].", "startOffset": 50, "endOffset": 56}, {"referenceID": 0, "context": "For instance, ASR deep neural network (DNN) model [3,22] divides the acoustic space into senone classes and discriminates the speakers in this space using the classic total variability (TV) model [1].", "startOffset": 196, "endOffset": 199}, {"referenceID": 3, "context": "The second approach is considered the most robust to varying conditions [4].", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "As demonstrated by recent publications [6,8\u201310,23], substantial success of the state-of-the-art text-dependent verification systems is mainly due to the progress in text-independent speaker recognition task.", "startOffset": 39, "endOffset": 50}, {"referenceID": 6, "context": "As demonstrated by recent publications [6,8\u201310,23], substantial success of the state-of-the-art text-dependent verification systems is mainly due to the progress in text-independent speaker recognition task.", "startOffset": 39, "endOffset": 50}, {"referenceID": 7, "context": "As demonstrated by recent publications [6,8\u201310,23], substantial success of the state-of-the-art text-dependent verification systems is mainly due to the progress in text-independent speaker recognition task.", "startOffset": 39, "endOffset": 50}, {"referenceID": 8, "context": "As demonstrated by recent publications [6,8\u201310,23], substantial success of the state-of-the-art text-dependent verification systems is mainly due to the progress in text-independent speaker recognition task.", "startOffset": 39, "endOffset": 50}, {"referenceID": 20, "context": "As demonstrated by recent publications [6,8\u201310,23], substantial success of the state-of-the-art text-dependent verification systems is mainly due to the progress in text-independent speaker recognition task.", "startOffset": 39, "endOffset": 50}, {"referenceID": 9, "context": "Thus, the success of the phonetic discriminative DNN in such a task leads to attempts to use similar approach in text-dependent systems [5, 11,16].", "startOffset": 136, "endOffset": 146}, {"referenceID": 14, "context": "Thus, the success of the phonetic discriminative DNN in such a task leads to attempts to use similar approach in text-dependent systems [5, 11,16].", "startOffset": 136, "endOffset": 146}, {"referenceID": 11, "context": "In parallel, there are several studies on the use of Deep-Learning approaches aiming to create an end-to-end solutions for discriminating speakers directly in a text-dependent task [13, 14].", "startOffset": 181, "endOffset": 189}, {"referenceID": 12, "context": "In parallel, there are several studies on the use of Deep-Learning approaches aiming to create an end-to-end solutions for discriminating speakers directly in a text-dependent task [13, 14].", "startOffset": 181, "endOffset": 189}, {"referenceID": 10, "context": "[12] describes a DNN for extracting a small speaker footprint which can be used to discriminate between speakers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "In this paper we investigate the deep residual CNN [15] for direct speaker discrimination.", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "Unlike [14] we focus on the use of spectrograms instead of MFCC as the input features and deep but light residual architecture instead of VGG-like network as the mapping.", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "where s is the mean supervector, \u03bc is the mean supervector of an Universal Background Model (UBM), T is a low rank matrix and w is the i-vector estimated using the Factor Analysis method [1].", "startOffset": 187, "endOffset": 190}, {"referenceID": 14, "context": "We used implementation of the back-end from [16].", "startOffset": 44, "endOffset": 48}, {"referenceID": 8, "context": "A simple cosine distance scoring is used followed by phrase-dependent s-norm score normalization [10].", "startOffset": 97, "endOffset": 101}, {"referenceID": 13, "context": "Currently, the best convolutional architecture for solving image processing tasks is a Residual CNN [15].", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "Residual architecture is described in [15, 20] as a stack of several residual units.", "startOffset": 38, "endOffset": 46}, {"referenceID": 17, "context": "Residual architecture is described in [15, 20] as a stack of several residual units.", "startOffset": 38, "endOffset": 46}, {"referenceID": 13, "context": "Thus, it becomes possible to train very deep networks with a size of 152 or more layers, as shown in the [15].", "startOffset": 105, "endOffset": 109}, {"referenceID": 13, "context": "For this study, a network with 18 layers from [15] with modifications from [20] was used.", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "For this study, a network with 18 layers from [15] with modifications from [20] was used.", "startOffset": 75, "endOffset": 79}, {"referenceID": 5, "context": "In our experiments we use the RSR2015 database [7].", "startOffset": 47, "endOffset": 50}, {"referenceID": 14, "context": "As described in [16], we use the following representation of the WCCN matrix:", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "CNN is implemented using the Keras framework [17] on top of the TensorFlow [18] backend.", "startOffset": 75, "endOffset": 79}, {"referenceID": 16, "context": "ADAM optimizer [19] with learning rate set at 10\u22124 is used for training Network is trained to discriminate between all speakers in training set using the softmax layer and categorical cross-entropy loss function.", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "Baseline system demonstrated a very good result with an EER of less than 1% which is comparable with the result from [16].", "startOffset": 117, "endOffset": 121}], "year": 2017, "abstractText": "Deep learning approaches are still not very common in the speaker verification field. We investigate the possibility of using deep residual convolutional neural network with spectrograms as an input features in the text-dependent speaker verification task. Despite the fact that we were not able to surpass the baseline system in quality, we achieved a quite good results for such a new approach getting an 5.23% ERR on the RSR2015 evaluation part. Fusion of the baseline and proposed systems outperformed the best individual system by 18% relatively.", "creator": "LaTeX with hyperref package"}}}