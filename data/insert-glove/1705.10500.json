{"id": "1705.10500", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Exploiting Restricted Boltzmann Machines and Deep Belief Networks in Compressed Sensing", "abstract": "This flagellation paper punchbowl proposes a niklot CS sheren scheme generalship that exploits saguen\u00e9ens the representational power rd-180 of restricted maranghi Boltzmann machines and deep learning architectures to model the prior distribution sportfishing of the sparsity pattern ghaznavid of sahl signals belonging lightnin to hazard the plane same class. aceralia The non-magnetic determined peered probability kilrathi distribution is preemptions then used feitosa in 20.61 a maximum destructionists a posteriori (michaelides MAP) approach comr for the reconstruction. salehi The parameters of unidentifiable the gavlak prior iyengars distribution trouv\u00e8res are baking learned quincunx from training novatel data. The 33,200 motivation five-seat behind inflecting this approach voestalpine is furedi to model the tamanna higher - order tvp1 statistical dependencies murom between the hasti coefficients att of hogi the alejandrino sparse courseware representation, with the continent final wuerffel goal 123.87 of 3-116 improving the reconstruction. The \u0161piranovi\u0107 performance of eleutherius the proposed method is 5p-10p validated .000105 on 1640.4 the out-of-competition Berkeley Segmentation saddle-shaped Dataset and maccabee the hatemongers MNIST Database of recomposed handwritten 290.2 digits.", "histories": [["v1", "Tue, 30 May 2017 08:11:05 GMT  (5060kb)", "http://arxiv.org/abs/1705.10500v1", "Accepted for publication at IEEE Transactions on Signal Processing"]], "COMMENTS": "Accepted for publication at IEEE Transactions on Signal Processing", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["luisa f polania", "kenneth e barner"], "accepted": false, "id": "1705.10500"}, "pdf": {"name": "1705.10500.pdf", "metadata": {"source": "CRF", "title": "Exploiting Restricted Boltzmann Machines and Deep Belief Networks in Compressed Sensing", "authors": ["Luisa F. Polan\u0131\u0301a"], "emails": ["lpolania@amfam.com).", "barner@udel.edu)."], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n10 50\n0v 1\n[ cs\n.L G\n] 3\n0 M\nay 2\n01 7\nIndex Terms\u2014Compressed sensing (CS), restricted Boltzmann machine (RBM), deep learning, deep belief network (DBN), wavelets, dictionary learning.\nI. INTRODUCTION\nCOMPRESSED sensing has become an extensive researcharea due to its potential to perfectly reconstruct sparse signals from a small set of nonadaptive linear measurements in the form of random projections [1], [2]. In essence, CS states that data acquisition with far fewer measurements than that dictated by the Shannon-Nyquist theorem is possible, under certain conditions. In last decade, the area of CS has extended to new applications that require structured signal models that go beyond the simplistic sparsity model [3]\u2013[7]. Examples of deterministic models include the wavelet tree model, which assumes that the non-zero signal coefficients lie in a rooted and connected tree structure, and the block-sparsity model, which assumes that the non-zero signal coefficients form clusters [3], [4]. Instead of imposing an explicit structure of the coefficients, statistical approaches usually impose a prior belief about the signal of interest in terms of a sparseness prior [5], [6].\nEven though the bulk of CS theory has been developed for signals that have a sparse representation in an orthonormal basis, efforts have been made to extend CS theory to signals that are sparse with respect to an overcomplete dictionary [8]\u2013 [10]. This extension adds more flexibility to CS as many signals of interest are not sparse in an orthonormal basis, but are in an overcomplete dictionary. For example, reflected radar and sonar signals have a sparse representation in Gabor frames. The coherence between the columns of an overcomplete dictionary poses some limitations in extending the CS theory to sparse overcomplete representations [1], [8]. However, Raught\nL.F. Polan\u0131\u0301a is with American Family Mutual Insurance Company, Madison, WI, 53783 USA (e-mail: lpolania@amfam.com). K.E. Barner is with the Department of Electrical and Computer Engineering, University of Delaware, Newark, DE, 19711 USA (e-mail: barner@udel.edu).\net al. [8] showed that CS is viable in the context of signals that are sparse in an overcomplete dictionary. They studied the conditions on the overcomplete dictionary that, in combination with a random sampling matrix, results in small restricted isometry constants.\nIn this paper, a statistical approach is proposed that uses restricted Boltzmann machines (RBMs) and deep belief networks (DBNs) to model the prior distribution of the sparsity pattern of the signal to be recovered. The proposed method requires a priori training data of the same class as the signal of interest. Either orthonormal bases, such as the wavelet transform, or overcomplete learned dictionaries can be employed as sparsifying transforms in the proposed approach. In the case of overcomplete dictionaries, a training stage is employed with dual purpose. First, it learns an overcomplete dictionary to sparsely represent the signal of interest. Second, it estimates the parameters of the prior distribution from the sparse codes of the training data. Therefore, unlike most of the works that use overcomplete learned dictionaries in CS problems [11]\u2013[13], which only use the training stage to learn the dictionary and disregard the sparse codes associated with such a dictionary, the proposed approach exploits both dictionary and sparse codes from the training stage to improve CS reconstruction algorithm performance.\nIn addition to the training stage, another contribution of the proposed approach is related to the reconstruction algorithm. After either RBMs or DBNs are employed to model the prior distribution of the sparsity pattern of the signal to be recovered, the determined prior is then employed in a maximum a posteriori approach for reconstruction. Obtaining the exact MAP estimator solution can become computational unfeasible since complexity increases exponentially with the signal length. To overcome this limitation, we propose a greedy approach realized by modifying the orthogonal matching pursuit\u2013based algorithm proposed in [14] to maximize the posterior distribution of the sparsity pattern.\nThe motivation for using RBMs and DBNs is twofold. First, they possess tremendous representational power; second, inference and parameter learning can be efficiently achieved using contrastive divergence and greedy layer\u2013wise training [15]\u2013 [17]. Indeed, Le Roux et al. [15] showed that an RBM can model any discrete distribution. Moreover, adding hidden units yields strictly enhanced modeling performance, unless the RBM already perfectly models the data. Similarly, Sutskever et al. [16] showed that deep belief networks can approximate any distribution over binary vectors to an arbitrary level of accuracy, even when the width of each layer is limited to the dimensionality of the data. Deep belief networks is one of the architectures of deep learning, a powerful and fast\u2013\n2 growing field in artificial intelligence [17]. Therefore, this manuscript links deep learning with CS by exploring the capabilities of deep learning architectures in modeling the statistical dependencies in the sparsity pattern of signals. To the best of our knowledge, this is the first paper that uses deep learning\u2013based priors to model the sparsity pattern of signals in a compressed sensing framework.\nPrevious works have employed fully visible Boltzmann machines to model the signal support in the context of compressed sensing [14], [18] and sparse coding [19], [20]. Restricted Boltzmann machines have been employed to model the dependencies between low resolution and high resolution patches in the image super\u2013resolution problem [21]. The work of Tramel et al. [22] also uses RBMs to model the sparsity pattern of signals. Their work is based on the approximate message passing (AMP) framework, which is very powerful at reconstructing sparse signals by exploiting the statistical properties of the problem. However, it has been shown that AMP algorithms are very sensitive to parameter tuning [23]. Regardless of their simplicity and ease of implementation, OMP-based algorithms outperform AMP algorithms in some cases [24], [25], specially when the non-zero coefficients of sparse signals differ in magnitude [25].\nThe closest related work to ours was proposed by Peleg et al. [14]. They used fully visible Boltzmann machines to model the distribution of the sparsity pattern of sparse signals. We follow their method in trying to reconstruct the signal support using a MAP approach. Our work differs from that of Peleg et al. in several aspects. First, the aim of their work is to learn sparse representations for signal modeling. The aim of our work is different, namely to reconstruct sparse signals from undersampled measurements. Second, they use fully visible Boltzmann machines that can only model pair\u2013 wise dependencies between elements in the sparsity pattern. Instead, RBMs and DBNs can model higher\u2013order dependencies and, therefore, they offer superior representational power. Third, we employ contrastive divergence for parameter learning instead of the maximum pseudo\u2013likelihood approach in order to realize computational complexity and performance improvements. In practice, pseudo\u2013likelihood learning has a high computational overhead compared to contrastive divergence [26]. Pseudo\u2013likelihood learning does not approximate the maximum likelihood estimator well, except in the limit of zero dependence [27]. It was shown that contrastive divergence is equivalent to pseudo\u2013likelihood for fully visible Boltzmann machines if single\u2013step Gibbs sampling is employed and outperforms pseudo\u2013likelihood when the number of sampling steps is larger than one [28].\nThe organization of the paper is as follows. Section II presents a brief review of CS and deep learning architectures. In Section III, the proposed method is presented. Numerical results for the proposed method and comparisons with CS reconstruction algorithms are presented in Section IV. Finally, Section V concludes the work with closing remarks."}, {"heading": "II. BACKGROUND", "text": ""}, {"heading": "A. Compressed sensing", "text": "Let x \u2208 RN be a signal that is approximately K-sparse in a dictionary D \u2208 RN\u00d7Q. Thus, the signal x can be approximated by a linear combination of a small number of column vectors from D, i.e. x = Ds+r, where s is the sparse vector of weighting coefficients, r is the representation error, andK \u226a N . The support of s is denoted as \u03b8 and is associated with the sparsity pattern S, which is defined as Si = 1\u03b8(i) for i = 1, . . . , N , where 1\u03b8(i) takes the value of one if i \u2208 \u03b8 and zero elsewhere.\nLet \u03a6 be an M \u00d7N sensing matrix, M < N . Compressed sensing [1], [2] addresses the recovery of x from linear measurements of the form y = \u03a6x \u2248 \u03a6Ds. Compressed sensing results show that the signal x can be reconstructed from y if the matrix \u039e = \u03a6D satisfies a condition, known as the restricted isometry property (RIP) [29], with a sufficiently small restricted isometry constant. If the sampling matrix \u03a6 is a sub\u2013Gaussian matrix and the dictionary D is unitary, then the matrix \u039e satisfies the RIP with high probability. However, if the matrix D is overcomplete, the coherence between its columns makes it difficult for the matrix \u039e to satisfy the RIP with a sufficiently small restricted isometry constant [8]. Several works have addressed this limitation by designing the sampling matrix \u039e so as to minimize the mutual coherence of the effective dictionary \u039e [9], [10]."}, {"heading": "B. Deep learning architectures", "text": "Deep learning aims at learning hierarchical feature representations with higher level features formed by the composition of lower level features [17]. Deep learning is inspired by biological structures in human brain mechanisms for processing of natural signals. A deep learning architecture, the DBN [30], is presented in this section.\nRestricted Boltzmann machines are the building blocks of DBNs. They are probabilistic generative models that learn a joint probability distribution of training data. An RBM is composed of a single visible layer and a single hidden layer. The visible units, v = [v1v2 . . . vJ ] T , represent the input data whose probability distribution needs to be modeled. The hidden units, h = [h1h2 . . . hP ] T , are trained to capture higherorder data correlations that are observed at the visible units. Symmetric connections between the layers are represented by a weight matrixW. The structure of an RBM forms a bipartite graph, as shown in Fig. 1(a).\nIn an RBM, the joint distribution p(v,h), over the visible units v and the hidden units h, is defined as\np(v,h) = exp(\u2212E(v,h))\nZ , (1)\nwhere E(v,h) is the energy function and Z = \u2211\nv\n\u2211\nh exp(\u2212E(v,h)) is the normalization term. For a\nBernoulli (visible)\u2013Bernoulli (hidden) RBM, the energy function takes the form\nE(v,h) = \u2212vTWh\u2212 bTv v \u2212 bThh, (2)\n3\nwhere W denotes the weights between visible and hidden units, and bv and bh are the bias terms. The RBM parameters, i.e.,W, bv and bh, can be optimized by performing stochastic gradient ascent on the log\u2013likelihood of training data. Given that computing the exact gradient of the log\u2013likelihood is intractable, the contrastive divergence approximation [31] is typically employed.\nIn an RBM, units within the same layer are not connected. Therefore, the posterior distribution over hidden vectors factorizes into a product of independent distributions for each hidden unit. The conditional distributions over hidden and visible units take the form\np(hj = 1|v) = \u03c3((bh)j +WT\u00b7jv), (3)\np(vi = 1|h) = \u03c3((bv)i +WTi\u00b7h), (4)\nwhere \u03c3(x) = (1 + e\u2212x)\u22121, and W\u00b7j and Wi\u00b7 correspond to the jth column and ith row of matrix W, respectively.\nA DBN architecture is composed of a stack of RBMs. The lowest\u2013level RBM learns a shallow model of the data. The RBM at the next level captures high\u2013order correlations between the hidden units of the first, and so on. A DBN with L layers models the joint distribution between the visible layer v and the hidden layers hl, l = 1, . . . , L as follows\np(v,h1, . . . ,hL) = p(v|h1) ( L\u22122 \u220f\nl=1\np(hl|hl+1) ) p(hL\u22121,hL).\n(5)\nThe log-probability of the training data can be improved by adding layers to the network, which, in turn, increases the true representational power of the network [30].\nLet v = h0. The bias vector of layer l and the weight matrix that represents the connections between layer l\u22121 and layer l are denoted by bhl and W l, respectively. A schematic representation of a DBN with one visible and three hidden layers is shown in Fig. 1(b). The top two layers form a restricted Boltzmann machine, which is an undirected graphical model, and the lower layers form a directed generative model.\nThe main breakthrough introduced by Hinton et al. [30] was a greedy, layer\u2013wise unsupervised learning algorithm that allows efficient training of DBNs. This algorithm trains each RBM separately, making the time complexity of the DBN learning linear in the size and depth of the networks."}, {"heading": "III. PROPOSED COMPRESSED SENSING SCHEME", "text": "The proposed scheme requires training data of the same class as the signal to be reconstructed. A training stage is employed to learn a prior model for the sparsity pattern of the signal class. The proposed CS reconstruction algorithm exploits the determined prior in a MAP approach. The CS and training stages are described thoroughly in this section. The training stage varies depending on the employed sparsifying transform, either orthonormal bases or overcomplete learned dictionaries. The block diagrams of the proposed CS schemes for overcomplete dictionaries and orthonormal bases are presented in Figs. 2 and 3, respectively."}, {"heading": "A. Compressed sensing stage", "text": "1) Problem formulation: Let D \u2208 RN\u00d7Q denote the sparsifying transform employed to represent a signal x \u2208 RN , i.e., x = Ds+r, where s and r are the sparse representation and the representation error, respectively. A Gaussian distribution with zero mean and covariance \u03a3r is assumed for r. In this paper, we consider the traditional synthesis\u2013based CS approach that aims at reconstructing the sparse representation s of a signal x from undersampled and noisy measurements of the form y = \u03a6x + n, where \u03a6 \u2208 RM\u00d7N is the sampling matrix and n accounts for the additive Gaussian sampling noise of zero mean and variance \u03c32n. Vector y can also be written as\ny = \u03a6Ds +\u03a6r+ n. (6)\nLet \u03b7 = \u03a6r + n and \u039e = \u03a6D, then vector y takes the form\ny = \u039es+ \u03b7. (7)\nAs both r and n are Gaussian distributed, vector \u03b7 is also Gaussian distributed with zero mean and covariance \u03a3\u03b7 = \u03a6\u03a3r\u03a6\nT +\u03c32nI. We adopt the commonly used assumption that the sampling noise variance \u03c32n is known [14], [33], [34]. The approach proposed in [14] is adopted in this paper, namely first calculating the MAP estimator of the sparsity pattern and then calculating the MAP estimator of the sparse vector. The support of s, of cardinality K , is denoted as \u03b8. Let s\u03b8 denote the nonzero coefficients of s. A Gaussian distribution with zero mean and variance \u03c32si is assumed for each nonzero coefficient si, i \u2208 \u03b8. The same probability distribution is\n4 Training\ndata Prior distribution modeling\nSampling Reconstruction algorithm\nCOMPRESSED SENSING\nTRAINING STAGE Prior distribution parameters\nx x\u0302 y\nF\nFig. 3. Block diagram of the proposed CS scheme when using orthonormal bases as the sparsifying transform.\nemployed in [14], [35] for nonzero sparse coefficients. Then, the conditional distribution of s\u03b8 given \u03b8 is given by\ns\u03b8|\u03b8 \u223c N (0,\u03a3\u03b8), (8) where \u03a3\u03b8 \u2208 RK\u00d7K is a diagonal matrix, whose diagonal is formed by the variances of the nonzero coefficients \u03c32si , i \u2208 \u03b8. The Gaussian distribution of \u03b7 leads to the following distribution for the likelihood p(y|s\u03b8, \u03b8): y|s\u03b8, \u03b8 \u223c N (\u039e\u03b8s\u03b8,\u03a3\u03b7), (9)\nwhere \u039e\u03b8 is the submatrix obtained by extracting the columns of matrix \u039e corresponding to the indexes in \u03b8. By integrating the product of p(s\u03b8|\u03b8) and p(y|s\u03b8, \u03b8) over all possible s\u03b8 , an expression for the probability distribution p(y|\u03b8) is obtained,\np(y|\u03b8) =C \u00d7 det ( \u039eT\u03b8 \u03a3 \u22121 \u03b7 \u039e\u03b8\u03a3\u03b8 + I\n)\u22121/2\n\u00d7 exp { 1\n2 yT\u03a3\u22121\u03b7 \u039e\u03b8P \u22121\u039eT\u03b8 \u03a3 \u22121 \u03b7 y\n}\n, (10)\nwhere C = det(2\u03c0\u03a3\u03b7) \u22121/2 exp { \u2212 12yT\u03a3\u22121\u03b7 y } and P = \u039eT\u03b8 \u03a3 \u22121 \u03b7 \u039e\u03b8 +\u03a3 \u22121 \u03b8 .\nThe MAP estimator of \u03b8, denoted by \u03b8\u0302, can be calculated as\n\u03b8\u0302 = argmax \u03b8 p(\u03b8|y) = argmax \u03b8 p(y|\u03b8)p(\u03b8). (11)\nThe posterior distribution p(s\u03b8\u0302|y, \u03b8\u0302) has a Gaussian distribution with mean \u00b5s and covariance \u03a3s, such that\n\u00b5s = \u03a3\u03b8\u0302\u039e T \u03b8\u0302 (\u039e\u03b8\u0302\u03a3\u03b8\u0302\u039e T \u03b8\u0302 +\u03a3\u03b7) \u22121y (12) \u03a3s = \u03a3\u03b8\u0302 \u2212\u03a3\u03b8\u0302\u039eT\u03b8\u0302 (\u039e\u03b8\u0302\u03a3\u03b8\u0302\u039e T \u03b8\u0302 +\u03a3\u03b7) \u22121\u039e\u03b8\u0302\u03a3\u03b8\u0302. (13)\nTherefore, the MAP estimate of s, denoted as s\u0302\u03b8\u0302, is directly obtained from the mean of the posterior, i.e.,\ns\u0302\u03b8\u0302 = argmax s \u03b8\u0302\np(s\u03b8\u0302|y, \u03b8\u0302), (14)\n= \u03a3\u03b8\u0302\u039e T \u03b8\u0302 (\u039e\u03b8\u0302\u03a3\u03b8\u0302\u039e T \u03b8\u0302 +\u03a3\u03b7) \u22121y.\nAn expression for p(\u03b8) needs to be calculated to solve (11). Note that p(\u03b8) = p (S). We propose to use the probability distribution over visible units p(v) of RBMs and DBNs to model the prior distribution p (S), or equivalently, p(\u03b8).\nUsing RBMs to model the sparsity pattern probability distribution is justified by results that show that an RBM can model any discrete distribution and that adding hidden units yields strictly enhanced modeling performance, unless\nthe RBM already perfectly models the data [15]. Similarly, the use of deep belief networks is justified by their capabilities to approximate any distribution over binary vectors to an arbitrary level of accuracy, even when the width of each layer is limited to the dimensionality of the data [16]. However, DBNs offer an additional advantage over RBMs: they yield more efficient and compact representations in terms of the number of parameters [15].\n2) Prior distribution: In an RBM, the probability distribution over visible units is obtained by marginalizing (1) over the hidden units\np(v) = \u2211\nh\np(v,h) = \u2212 1 Z exp (\u2212E(v)) , (15)\nwhere\nE(v) = \u2212 \u2211\nj\nlog ( 1 + eW T \u00b7jv+bhj )\n\u2212 bT v v. (16)\nA DBN can be seen as a probabilistic generative model. To calculate the probability distribution of the visible units of a DBN, we start with a random configuration at the top hidden layer, hL, and let the top\u2013level RBM converge to a stationary distribution using alternating Gibbs sampling. Alternating Gibbs sampling iterates between updating the hidden units in parallel using (3) and updating the visible units in parallel using (4). Next, it performs a top\u2013down pass in which the state of each variable in a layer is chosen from a Bernoulli distribution with the probability that a variable has a value of one depending on the states of the layer above. That is, p(hli = 1|hl+1) = \u03c3((bhl)i +Wl+1i\u00b7 hl+1), (17) where, as before, v = h0.\nRepeated top\u2013down passes generates a full set of data vectors at each layer of the DBN. Let H be the total number of top\u2013down passes. The sequence of data vectors for the hidden layers is denoted as hl (1) , . . . ,hl (H)\n, l = 1, ..., L, and v(1), . . . ,v(H), for the visible layer. Such sequence of data vectors assigned to the visible layer can be employed to give a rough approximation of the marginal distribution p(v). However, the conditional density function p(v|h1) contains more information about the shape of the distribution p(v) than the sequence of the individual realizations since p(v) = Eh1 [p(v|h1)], where E[\u00b7] denotes the expected value of a random variable. Therefore, the marginal density is approximated by p\u0302(v) = 1H \u2211H k=1 p ( v|h1(k) ) . Since the visible units are conditionally independent given the hidden states h1, the approximation of p(v) takes the form\np\u0302(v) = 1\nH\nH \u2211\nk=1\nN \u220f\ni=1\np ( vi|h1(k) ) . (18)\nUsing the probability distribution (15) to model p(\u03b8) in (11) leads to the following MAP estimator of \u03b8:\n5 \u03b8\u0302 = argmax \u03b8 ( 1 2 yT\u03a3\u22121\u03b7 \u039e\u03b8P \u22121\u039eT\u03b8 \u03a3 \u22121 \u03b7 y (19)\n\u22121 2 log (det(P\u03a3\u03b8)) + \u2211\nj\nlog ( 1 + eW T \u00b7jS+bhj )\n+bTvS ) .\nSimilarly, when using (18) to model p(\u03b8) in (11), the MAP estimator of \u03b8 becomes\n\u03b8\u0302 = argmax \u03b8\n(\n1 2 yT\u03a3\u22121\u03b7 \u039e\u03b8P \u22121\u039eT\u03b8 \u03a3 \u22121 \u03b7 y (20)\n\u22121 2 log (det(P\u03a3\u03b8)) +log\n(\nH \u2211\nk=1\nN \u220f\ni=1\np ( Si|h1(k) )\n))\nExpressions (19) and (20) are combinatorial optimization problems that require an exhaustive search over all possible sparsity patterns of dimension K . Therefore, there is a need to use algorithms that approximate the MAP solution to overcome the intractability of the combinatorial search.\n3) MAP estimator via a greedy approach: Peleg et al. [14] proposed a greedy pursuit algorithm based on Orthogonal Matching Pursuit (OMP) to approximate the MAP estimator of a sparse representation. The same algorithm is used here for reconstructing the sparse representation s, although using a different posterior distribution than that in [14]. The support is initialized to the empty set. At each iteration, the algorithm searches for the element i\u0304 that can be added to the support in order to maximize p(\u03b8|y). The algorithm stops when any additional element in the support decreases either the objective function in (19), in the case of RBMs, or (20), in the case of DBNs. The algorithm, at each iteration, makes locally optimal choices with the hope that this will lead to the optimal global solution. It is noted that the algorithm achieves the optimal solution for supports of cardinality 1 since it goes through the same computational stages as exhaustive search in this special case.\nOnce the support is recovered, the sparse representation s is calculated using (14). A summary of the algorithm is presented in Algorithm 1. The functions gRBM(\u00b7) and gDBN(\u00b7) refer to the objective functions in (19) and (20), respectively. If an RBM is employed to model the prior distribution p(\u03b8), the function gRBM(\u00b7) is used and the algorithm is referred to as the RBM\u2013OMP\u2013like algorithm. If a DBN is employed instead, the function gDBN(\u00b7) is used and the algorithm is referred to as the DBN\u2013OMP\u2013like algorithm.\n4) Computational Complexity Analysis: The computational complexity in Algorithm 1 is dominated by the calculation of the functions in lines 6, gRBM(\u03b8\u0304 (t)) and gDBN(\u03b8\u0304\n(t)). It is assumed that all the operations that do not depend on \u03b8\u0304(t) are precomputed and do not contribute to the cost. Evaluation of the first two terms of gRBM(\u03b8\u0304 (t)) is dominated by the calculation of P, which costs O(KM2) flops. The third and fourth terms cost O(NP ) and O(N) flops, respectively. Therefore, calculating gRBM(\u00b7) costs O(KM2 + NP ) flops. Let D denote the total number of iterations of the while loop in Algorithm 1. Since gRBM(\u00b7) needs to be calculated\nAlgorithm 1 RBM\u2013OMP\u2013like/DBN\u2013OMP\u2013like algorithm\nRequire: Matrix \u039e = \u03a6D, measurements y, model parameters defining the probability distribution p(\u03b8|y).\n1: Initialize t = 0, \u03b8(0) = \u2205. 2: while halting criterion false do 3: t \u2190 t+ 1 4: for i /\u2208 \u03b8(t\u22121) do 5: \u03b8\u0304(t) \u2190 \u03b8(t\u22121) \u222a i 6: f(i) \u2190 gRBM(\u03b8\u0304(t)) or f(i) \u2190 gDBN(\u03b8\u0304(t)) 7: end for\n8: i\u0304 \u2190 argmaxi f(i) 9: \u03b8(t) \u2190 \u03b8(t\u22121) \u222a i\u0304 10: end while 11: \u03b8\u0302 \u2190 \u03b8(t) 12: return s\u0302\u03b8\u0302 \u2190 \u03a3\u03b8\u0302\u039eT\u03b8\u0302 (\u039e\u03b8\u0302\u03a3\u03b8\u0302\u039e T \u03b8\u0302 +\u03a3\u03b7) \u22121y.\nD(N\u22121) times, the computational complexity of Algorithm 1 is O(DN(KM2 +NP )), when RBMs are employed. The first two terms of gDBN(\u03b8\u0304\n(t)) are the same as those of gRBM(\u03b8\u0304\n(t)), which means that their cost is O(KM2) flops. Let C denote the number of units of the first hidden layer of the DBN. Then, calculating the third term of gDBN(\u03b8\u0304\n(t)) costs O(CHN) flops and, therefore, the computational complexity of Algorithm 1 becomes O(DN(KM2 + CNH)), when DBNs are employed.\n5) Selection of the probabilistic generative model: Even though DBNs and RBMs have the same representational power [15], they differ in the number of parameters needed to model the probability distribution of the data. Deep Belief Networks offer more compact representations. Indeed, in [17], it was shown that deep architectures can sometimes be exponentially more efficient than shallow ones in terms of number of parameters needed to represent a function. More precisely, there are functions that can be compactly represented with an architecture of depth L but that would require exponential size (with respect to input size) architectures of depth L\u2212 1. The number of parameters have computational and statistical consequences, and, therefore, can be used as a factor to decide which generative model to use. Since the number of parameters one can afford depends on the number of training samples available to learn them, the selection of RBMs, which typically require more parameters than DBNs, may lead to poor generalization. Also, the computational cost of the training stage and reconstruction algorithms increases with the number of parameters.\nThis result may initially suggest that DBNs should always be selected. However, as shown in Section III-A4, the computational complexity of the DBN-OMP-like algorithm also depends on the number of top-down passes used for the approximation of the probability distribution of the visible layer. Therefore, the decision of which generative model to use is data dependent as the exact number of required parameters depends on the complexity of the data and the amount of training data. We suggest to estimate the number of parameters of the generative models using cross-validation, as described in Section III-B4, and then choose the generative\n6 model that would lead to the lowest computational cost of the reconstruction algorithm using the results in Section III-A4."}, {"heading": "B. Training stage using Overcomplete Learned Dictionaries", "text": "In this section, the sparsifying transform D from (6) is assumed to be an overcomplete dictionary. We learn D from a set of training data belonging to the same class as signal x. The resulting sparse codes and the representation error are employed to estimate the model parameters defining p(\u03b8|y). 1) Traditional dictionary learning: First, the dictionary learning problem is described. Let G = [G\u00b71 . . .G\u00b7B] \u2208 R\nN\u00d7B denotes the set of N -dimensional training samples, which is referred to as training dataset I. One methodology for building the overcomplete dictionary D = [D\u00b71 . . .D\u00b7J ] \u2208 R N\u00d7J (J > N) is to solve the following optimization problem\n{D\u0302, A\u0302} = argmin D,A \u2016G\u2212DA\u20162F s. t. \u2016A\u00b7j\u20160 < K, \u2200j (21) with A = [A\u00b71 . . .A\u00b7B] \u2208 RJ\u00d7B denoting the sparse codes of G, and K the pre-specified sparsity level. The representation error is defined as E = G\u2212D\u0302A\u0302. Several algorithms have been proposed to solve (21). Here, the K\u2013SVD algorithm proposed by Aharon et al. [36] is employed. Rubinstein et al. [37] show that the dominant operations in K-SVD are sparse-coding, atom updates and coefficients updates, wich leads to a total computational complexity of O(B(K3 + 2KNJ + 4NK + 4KJ) + 5NJ2). 2) Sampling matrix design: Even though an overcomplete dictionary offers more flexibility and leads to sparser representations compared to an orthonormal basis, the coherence between its columns limits the performance of CS recovery algorithms when using the traditional sub\u2013Gaussian sensing matrix [8]\u2013[10]. That is why several works have focused on how to design the sampling matrix to guarantee accurate recoverability when using an overcomplete dictionary as the sparsifying transform. For example, Elad [9] optimizes the sampling matrix to minimize the coherence of the effective dictionary. Similarly, Duarte et al. [10] also aims at minimizing the coherence of the effective dictionary, with their approach designed to make the columns of the effective dictionary as orthogonal as possible. In this paper, the method of Duarte et al. is employed to build the sensing matrix as it has superior performance in terms of running time and accuracy of the reconstructed signal. That is, the sensing matrix\u03a6 is optimized as to minimize the mutual coherence of the effective dictionary \u03a6D.\n3) Estimation of model parameters: This section addresses the parameter estimation of p(\u03b8|y), which comprises the parameter estimation of p(\u03b8), the estimation of the variances \u03c32si , \u2200i, and the estimation of the covariance \u03a3\u03b7 . Let U\u00b7j denote the sparsity pattern of the sparse code A\u00b7j , j = 1, . . . , B. The ith element of U\u00b7j is defined as Uij = 1supp(A\n\u00b7j)(i), where supp(A\u00b7j) denotes the support of A\u00b7j . The set of vectors U = [U\u00b71 . . .U\u00b7B] can be employed to model a prior distribution for the sparsity pattern of signals belonging to the same class as the training data. Since the signal to be reconstructed, x, belongs to this class, the set of column\nvectors of U, which are referred to as training dataset II, are used to learn the parameters of p(\u03b8). As mentioned in Section III-A1, RBMs and DBNs are used for modeling such a prior distribution.\nIn the case of the RBM model, the probability distribution parameters that need to be estimated are the weight matrix W and the bias terms bv and bh. They can be optimized by performing stochastic gradient ascent on the log\u2013likelihood of the training dataset II. However, computing the exact gradient of the log\u2013likelihood is intractable. Here, we use contrastive divergence, which approximates the gradient of the log\u2013likelihood by using a Markov chain. For more details on contrastive divergence, the reader is referred to [31].\nWhen the prior distribution of the sparsity pattern is modeled with a DBN, the parameters that need to be estimated are the weights and bias terms of each layer. Hinton presented a powerful greedy layer\u2013wise method to learn these parameters [30]. The weight matrix W1, the bias terms of the visible layer bv, and the bias terms of the lowest hidden layer bh1 are learned by training an RBM with the training dataset II as input. Then, the inferred hidden values of h1 can be used as input for training another RBM that learns the parameters of the layer above. This bottom\u2013up process is repeated at the next layers until all the parameters of the network are learned.\nAdditionally, the set of sparse codes A can be employed to estimate the variance \u03c32si of each ith element of the sparse representation s of signal x (see [14]):\n\u03c3\u03022si =\n\u2211B j=1 A 2 ij\n\u2211B j=1 1[i \u2208 supp (A\u00b7j)]\n. (22)\nFor the estimation of the covariance matrix \u03a3r, independence is assumed between the representation error coefficients ri and rj for i 6= j. Therefore, the covariance matrix \u03a3r is a diagonal matrix, whose diagonal is formed by the variances of the representation error coefficients \u03c32ri , for i = 1, . . . , N . For the estimate of \u03a3r, denoted as \u03a3\u0302r, the representation error of the learned dictionary E = [E\u00b71 . . .E\u00b7B] is employed. More precisely, each ith diagonal element of \u03a3r is estimated as\n\u03c3\u03022ri = 1\nB\nB \u2211\nj=1\nE2ij . (23)\nThe estimate of \u03a3\u03b7 is directly calculated as \u03a3\u0302\u03b7 = \u03a6\u03a3\u0302r\u03a6 T + \u03c32nI.\n4) Selection of number of hidden layers and units: For the selection of the number of hidden layers and units of a DBN, hereafter referred to as hyperparameters, a validation dataset is employed. The selection of the hyperparameters is performed by grid search on the parameter space using holdout cross-validation. For every combination of hyperparameter values in the grid, the weights and biases of the DBN are learned using the training dataset I, which results in different DBN configurations. Those configurations are then used with the validation dataset. That is, the validation dataset is sampled with the sampling matrix \u03a6 and recovered with the DBN-OMP-like reconstruction algorithm using the different DBN configurations. The reconstruction SNR of the validation\n7 dataset, across the different architecture configurations, is used as a metric to determine the number of hidden layers and units. The same approach is employed for the selection of the number of hidden units in RBMs."}, {"heading": "C. Training stage using Orthonormal bases", "text": "As in Section III-B1, let G = [G\u00b71 . . .G\u00b7B] \u2208 RN\u00d7B denote the set of N -dimensional training samples belonging to the same class as signal x. In this section, D does not denote an overcomplete dictionary, but instead, it denotes an orthonormal basis. Each vector G\u00b7j can be expressed as G\u00b7j = DA\u0304\u00b7j , where A\u0304\u00b7j is the representation of the signal G\u00b7j in the D domain. Let A\u00b7j denote the best K-term approximation of A\u0304\u00b7j , which is obtained by keeping only the K largest (in magnitude) coefficients in A\u0304\u00b7j and setting the others to zero. Therefore, the signal G can be modeled as G = DA+E, where E is the representation error matrix. Let U\u00b7j denote the sparsity pattern of the sparse code A\u00b7j , Uij = 1supp(A \u00b7j)(i). As in the case of overcomplete dictionaries, the sparse codes A = [A\u00b71 . . .A\u00b7B], U = [U\u00b71 . . .U\u00b7B], and the representation error E = [E\u00b71 . . .E\u00b7B] are used to learn the model parameters defining p(\u03b8|y). That is, the set of vectors U is used to train either the RBM or the DBN that models p(\u03b8), the sparse codes A are used to estimate the variances \u03c32si , \u2200i, using (22), and E is used to estimate \u03c32si , \u2200i, using (23). For the selection of the generative model configuration, the same procedure described for overcomplete dictionaries in Section III-B4 is employed.\nUnlike the case of overcomplete dictionaries, the training stage does not require optimization of the sensing matrix. For the case of orthonormal bases, the entries of the sampling matrix \u03a6 \u2208 RM\u00d7N are independently sampled from a normal distribution with mean zero and variance 1/M ."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "To validate the proposed compressed sensing schemes, a set of experiments are conducted with synthetic and real signals. Results are presented for averages of 50 repetitions of each experiment, with a different realization of the measurement matrix at each iteration. The reconstruction SNR (R\u2013SNR) and the peak SNR (PSNR) are employed as performance measures for one-dimensional signals and images, respectively. The reconstruction SNR is defined as\nR\u2013SNR (dB) = 10log10 \u2016x\u201622\n\u2016x\u2212 x\u0302\u201622 , (24)\nwhere x and x\u0302 denote the N -dimensional original and reconstructed signals, respectively. The PSNR is defined as\nPSNR (dB) = 10log10 R2\nMSE , (25)\nwhere R is the maximum possible pixel value of the image (255 for 8\u2013bit images) and MSE denotes the mean\u2013square error defined as MSE = 1N \u2016I \u2212 I\u0302\u20162F , where I and I\u0302 denote the original and reconstructed images of size \u221a N \u00d7 \u221a N , respectively.\nIn the experiments presented in this section, the proposed methods are compared with the oracle estimate in order to\nillustrate the best achievable reconstruction. An oracle reveals the true support set \u03b8. Then, such support is used to obtain the estimate of the sparse representation using (14).\nIn [38]\u2013[40], a number of samples equal to 500 leads to a good approximation of the empirical average using samplingbased methods. Following the same choice, the number of samples H for the estimation of the probability distribution of the DBN visible layer is set to 500 in our experiments."}, {"heading": "A. Experiments with synthetic signals", "text": "The first set of experiments assume that the parameters of the RBM and DBN are known. The motivation of these experiments is to prevent errors in the parameter estimation, which takes place during training, to propagate into the reconstruction algorithm and, therefore, attain a more faithful evaluation of the algorithm.\nFor the first experiment, a set of 50000 sparsity patterns from an RBM, whose parameters are known, are generated via Gibbs sampling. From this set, 45000 and 5000 sparsity patterns are selected at random for training and testing, respectively. The number of hidden units is set equal to the number of visible units. Synthetic signals of dimension N = 256 are formed with the sparsity patterns and the magnitude of each nonzero coefficient is drawn from a Gaussian distribution with zero mean and known variance \u03c32si \u2208 [10, 50]. The weights and hidden bias terms are drawn from a uniform distribution, {bhj ,Wij} \u223c U(\u22121, 1), \u2200i, j. The visible bias terms are set to \u221214 to enforce sparsity. A Gaussian sensing matrix \u03a6 is employed to sample the testing dataset. Gaussian noise of variance \u03c32n = 1 is added to the measurements. Note that as the synthetic signals are already sparse, a sparsifying transform is not necessary, or equivalently, D = I. The proposed algorithms, RBM\u2013OMP\u2013like and DBN\u2013 OMP\u2013like, are executed on the random measurements of the testing dataset and the results are compared with the traditional OMP and with the oracle estimate (14). The RBM-OMP-like algorithm uses the same parameters of the RBM employed to generate the sparsity patterns. The DBN\u2013OMP\u2013like algorithm uses a 2-layer DBN model whose parameters are estimated using the training dataset, as described in Section III-B3. The number of hidden units per layer of the DBN is set the same as the number of visible units.\nThe mean R-SNR across samples of the testing dataset are shown in Fig. 4(a). Performance gains are observed when using the proposed algorithms. Indeed, their performance is very close to that of the oracle estimate for M > 0.35N . The proposed algorithms require fewer measurements than the traditional OMP to achieve successful reconstruction. The RBMOMP-like algorithm slightly outperforms the DBN-OMP-like algorithm since the data is generated from a probabilistic model that exactly matches the one used by the RBM-OMPlike algorithm. Figure 4(b) illustrates the standard deviation of the R-SNR across samples of the testing dataset. The proposed algorithms have lower R-SNR standard deviation than OMP for M/N > 0.25, which corresponds to their measurement range of successful reconstruction.\nA similar experiment is conducted with 50000 signals generated from a 2\u2013layer DBN, whose parameters are known a\n8 0.2 0.3 0.4 0.5 0.6 0.7 5 10 15 20 25 30\nM/N (a)\nM ea\nn R\n\u2212 S\nN R\nOracle OMP RBM\u2212OMP\u2212like DBN\u2212OMP\u2212like\n0.2 0.3 0.4 0.5 0.6 0.7 0\n1\n2\n3\n4\nM/N (b)\nS ta\nnd ar\nd de\nvi at\nio n\nof R\n\u2212 S\nN R\nOracle OMP RBM\u2212OMP\u2212like DBN\u2212OMP\u2212like\nFig. 4. Comparison of the reconstruction of synthetic signals generated with the RBM model using OMP, the RBM-OMP-like algorithm, the DBN-OMPlike algorithm, and the oracle estimator.\npriori. The number of hidden units per layer is set the same as the number of visible units. The weights and hidden bias terms are drawn from a uniform distribution, {bh1 j ,W 1ij , bh2j ,W 2 jk} \u223c U(\u22121, 1), \u2200i, j, k. The visible bias terms are set to \u22126.5 to enforce sparsity. As in the previous experiment, 45000 and 5000 sparsity patterns are selected at random for training and testing, respectively. The sampling noise variance and the variance of the nonzero coefficients are kept the same as in the previous experiment, \u03c32n = 1 and \u03c3 2 si \u2208 [10, 50], \u2200i, respectively.\nIn this experiment, the proposed algorithms, traditional OMP, and the oracle are employed for reconstruction. The DBN-OMP-like algorithm uses the same parameters of the DBN employed to generate the sparsity patterns. The RBM\u2013 OMP\u2013like algorithm uses an RBM model whose parameters are estimated using the training dataset, as described in Section III-B3. The number of hidden units of the RBM is set the same as the number of visible units. Figure 5(a) indicates that the proposed algorithms outperform OMP, particularly for low\u2013measurement regimes, which demonstrates that exploiting the structural information of the signal\u2019s sparsity pattern has the ability to boost the reconstruction performance.\nFor example, at a measurement rate of M = 0.3N , the DBN\u2013OMP\u2013like algorithm provides a 4.5 dB mean PSNR improvement in performance over OMP. The results are favorably biased towards the DBN\u2013OMP\u2013like algorithm as it uses a distribution that exactly matches the one used to generate the data. The results in Fig. 5(b) indicate that the proposed algorithms only have higher R-SNR standard deviation than OMP for very low\u2013measurement regimes, M/N < 0.23.\n0.2 0.3 0.4 0.5 0.6 0.7 5\n10\n15\n20\n25\n30\nM/N (a)\nM ea\nn R\n\u2212 S\nN R\nOracle OMP RBM\u2212OMP\u2212like DBN\u2212OMP\u2212like\n0.2 0.3 0.4 0.5 0.6 0.7 0\n1\n2\n3\n4\nM/N (b)\nS ta\nnd ar\nd de\nvi at\nio n\nof R\n\u2212 S\nN R\nOracle OMP RBM\u2212OMP\u2212like DBN\u2212OMP\u2212like\nFig. 5. Comparison of the reconstruction of synthetic signals generated with the DBN model using OMP, the RBM-OMP-like algorithm, the DBN-OMPlike algorithm, and the oracle estimator.\nNext, we demonstrate that the proposed algorithms are stable in the presence of measurement noise using the same datasets as those of the previous experiments. For this experiment, the number of measurements is set to M = 0.4N , as Figs. 4 and 5 indicate that the reconstruction is successful when such a number of measurements is used. The Gaussian sensing matrix is kept fixed while the sampling noise variance is varied in the range [0.5, 2.5]. Table I indicates that both RBM\u2013OMP\u2013like and DBN\u2013OMP\u2013like algorithms outperform the traditional OMP algorithm for the specified range of variances. Below \u03c32n = 2, the proposed algorithms produce faithful reconstructions with an SNR greater than 26 dB."}, {"heading": "B. Experiments with the MNIST Database", "text": "In this section, the performance of the proposed reconstruction algorithms as a function of the number of hidden units and hidden layers is studied via numerical experiments. The MNIST dataset [41], which contains 70000 grayscale images of handwritten digits of size N = 28 \u00d7 28, is employed for the experiments. The dataset is divided into 40000 samples for training, 20000 samples for validation and 10000 samples for testing. Since the images are already sparse in the spatial domain, a sparsifying transform is not necessary, or equivalently, D = I. The validation and testing datasets are sampled with a matrix\u03a6 whose entries are drawn from a zero\u2013 mean Gaussian distribution with variance 1/M . The resulting compressed measurements are artificially contaminated with Gaussian noise of variance \u03c32n = 1.2. As described in Section III-B4, the optimal model configurations are selected via cross-validation. RBM and DBN models\n9\nwith different number of hyperparameters are learned using the training dataset and evaluated using the validation dataset. Figure 6(a) illustrates the mean PSNR across samples of the validation dataset as a function of the number of measurements for different RBM models, which are employed by the RBM\u2013OMP\u2013like algorithm. The reconstruction performance improves as the number of hidden units of the RBM model increases from 0.5N to 8N . The standard deviation across samples of the validation dataset also increases as the number of hidden units increases (Fig. 6(b)). The reconstruction curves in Fig. 6(a) corresponding to RBMs with 8N and 16N hidden units are very similar, which may indicate that their representational power is almost the same. Therefore, setting the number of hidden units above 8N does not improve the reconstruction performance and may lead to overfitting. Consequently, we select an RBM with 8N hidden units for testing.\nTramel et al. also evaluated their RBM-based reconstruction method using the MNIST database [22]. They employed the percent of successfully recovered digit images as performance metric instead of the mean R-SNR. They showed that they could successfully recover 90% digit images from their testing dataset with only 0.25N measurements, while we attain poor reconstruction performance when using only 0.25N measurements as it is shown in Fig. 6(a). However, this is not a fair comparison since the authors of [22] selected different values for the model and experiment parameters. For example, they selected a noise variance that was much smaller than ours, a larger training dataset and a smaller testing dataset. The RBM\u2013 OMP\u2013like algorithm is expected to improve the reconstruction performance of the MNIST dataset when noise of smaller variance is added to the signals.\nA similar experiment is performed to select the number of hidden layers of the DBN employed by the DBN\u2013OMP\u2013 like algorithm. Deep and narrow belief networks, with hidden layers of the same dimension as that of the visible layer, are considered. Figures 7(a) and 7(b) illustrate that the mean PSNR and standard deviation across samples of the validation dataset increase as the number of hidden layers changes from L = 2 to L = 3, respectively. Setting the number of hidden layers above 3 does not improve the reconstruction performance, which suggests that the representational power of the DBN does not improve for L > 3. Therefore, we select a DBN with 3 hidden layers, whose number of hidden units per layer is the same as the number of visible units, for\ntesting. By comparing Figs. 6 and 7, it is noted that using an RBM with 8 hidden units leads to a similar performance as using a DBN with three hidden layers. However, the DBN architecture requires fewer parameters to be trained. More precisely, the RBM model requires 8N2 + 9N = 4924304 parameters (8N2 weights and 9N bias terms) while the DBN model only requires 3N2 + 4N = 1847104 parameters (3N2 weights and 4N bias terms). This comparison illustrates that RBMs and DBNs have the same representational power, but DBNs may lead to more compact representations.\nFigures 8 compares the performance of the proposed algorithms on the testing dataset, using the model configurations chosen via cross-validation, with CS reconstruction algorithms, such as OMP and basis pursuit denoising (BPDN) [42]. We also compare with the MAP\u2013OMP\u2013like algorithm pre-\n10\nsented in [14] (henceforth referred to as FV\u2013OMP-like algorithm), which has the same structure as the RBM\u2013OMP\u2013 like algorithm, with the difference being that it employs a fully visible Boltzmann machine to model the probability distribution of the sparsity pattern. The proposed algorithms attain the best reconstruction performance, followed closely by the FV\u2013OMP\u2013like algorithm, which also achieves successful reconstruction given that it also exploits statistical dependencies in the sparsity pattern. Algorithms that do not exploit structural information beyond sparsity require more measurements to achieve successful reconstruction than the proposed algorithms. For example, OMP and BPDN require M > 0.41N to attain PSNR > 25 dB. Contrarily, the proposed algorithms only require M > 0.35N measurements to attain PSNR > 25 dB. On the downside, the proposed algorithms have higher PSNR standard deviation than BPDN as shown by Fig. 8(b)."}, {"heading": "C. Experiments with the Berkeley segmentation dataset", "text": "This section presents experimental validation of the proposed algorithms using real data from the Berkeley segmentation dataset [43]. This dataset contains 400 images of real\u2013 life scenes. Each image is comprised of 321 \u00d7 481 pixels. Experiments are conducted with the proposed compressed sensing schemes using orthonormal bases and overcomplete learned dictionaries as sparsifying transforms. The training dataset for these experiments consists of 50400 patches of size\nN = 8 \u00d7 8, extracted at random from a set of 350 images. Similarly, the testing dataset consist of 2000 8 \u00d7 8 patches, extracted at random from a different set of 50 images. The same number of patches is extracted from each image.\nAn RBM with the same number of hidden units as visible units and a 2\u2013layer DBN are employed to model the probability distribution of the sparsity pattern. The number of hidden units per layer of the DBN is set to half the number of visible units. Compressed measurements are artificially contaminated with Gaussian noise of variance \u03c32n = 1. In the case of orthonormal bases, the entries of the sensing matrix \u03a6 are drawn from a zero\u2013mean Gaussian distribution with variance 1/M . In the case of overcomplete learned dictionaries, the sensing matrix is optimized as described in [10] and the number of dictionary atoms is set to 2N . The sparsity level for overcomplete dictionaries is set to K = 0.15N .\nThe scheme illustrated in Fig. 3 is considered first. Since the wavelet transform has proven useful for compressing natural images, the Symlets\u20138 wavelet transform, using a decomposition level L = 4, is chosen as the orthonormal sparsifying transform for our experiments. The proposed algorithms, RBM\u2013OMP\u2013like and DBN\u2013OMP\u2013like, are compared with the oracle estimator, OMP, and BPDN. We also compare with other algorithms that exploit structural information beyond sparsity, such as the Bayesian compressed sensing algorithm described in [44] (henceforth referred to as TS-BCS) and\n11\nthe FV\u2013OMP-like algorithm, which has the same structure as the RBM\u2013OMP\u2013like algorithm, with the difference being that it employs a fully visible Boltzmann machine to model the probability distribution of the sparsity pattern.\nIn this experiment, the true support revealed by the oracle corresponds to the best K\u2013term approximation of the signal to be recovered. As in our proposed algorithms, the FV\u2013OMP\u2013 like algorithm requires the parameters of the support prior distribution. Such parameters are learned from the training dataset using the maximum likelihood approach described in [14].\nThe results of the comparison in terms of mean PSNR across the testing dataset are reported in Fig. 9(a). It is noted that both RBM\u2013OMP\u2013like and DBN\u2013OMP\u2013like algorithms outperform OMP, BPDN, TS-BCS, and the FV\u2013OMP\u2013like algorithm. The number of parameters of the RBM and DBN models are N2+ 2N (N2 weights and 2N bias terms) and 3N2/4+2N (3N2/4 weights and 2N bias terms), respectively. Even though the DBN requires fewer parameters to be learned, the performance of the DBN\u2013OMP\u2013like algorithm is slightly superior to that of the RBM\u2013OMP\u2013like algorithm due to the multi\u2013layer structure of the DBN. As the reconstruction performance is related to the representational power of the model, the results in Fig. 9(a) are consistent with the work by Le Roux et al. [15], which shows that, even though a DBN and an RBM can have the\nsame representational power, a DBN offers a more compact and efficient representation in terms of number of parameters.\nFigure 9(b) illustrates the standard deviation of PSNR across samples of the testing dataset for the different reconstruction algorithms. The FV\u2013OMP\u2013like algorithm exhibits the largest standard deviation for almost the entire measurement range. Compared to the other algorithms, the proposed algorithms exhibit low standard deviation for M/N > 0.3.\nConsider next the implementation of the scheme illustrated in Fig. 2. In this case, the overcomplete dictionary is learned using the K\u2013SVD algorithm. An experiment is conducted to evaluate the performance of the proposed algorithms and the results of the mean PSNR across the testing dataset are shown in Fig. 10(a). The RBM\u2013OMP\u2013like and DBN\u2013OMP\u2013 like algorithms do not only require fewer measurements than conventional OMP to achieve stable recovery but also attain higher mean PSNR values for the entire range of measurements. There is a small performance gap in favor of the DBN\u2013 OMP\u2013like algorithm when compared to the RBM\u2013OMP\u2013like algorithm. The proposed algorithms also outperform the FV\u2013 OMP\u2013like algorithm, which suggests that exploiting higher\u2013 order dependencies between the sparse representation coefficients leads to superior reconstruction performance. As in the previous experiment, the proposed algorithms exhibit\n12\nlower standard deviation than the FV\u2013OMP\u2013like algorithm (Fig. 10(b)).\nFinally, visual evaluation of a reconstructed test image using the proposed CS schemes is presented in Fig. 11. Each non\u2013 overlapping 8 \u00d7 8 patch from the image was reconstructed from their noisy projections using M = 2\u00d7N measurements and sampling noise of variance \u03c32n = 1. The first row of Fig. 11 shows the original image (Fig. 11(a)). The second and third rows show the results when using wavelets and overcomplete dictionaries as sparsifying transforms, respectively. In the second and third rows, the first, second, third and fourth columns correspond to the reconstruction via the OMP, FV\u2013OMP\u2013like, RBM\u2013OMP\u2013like and DBN\u2013OMP\u2013like algorithms, respectively. For both overcomplete dictionaries and orthonormal bases, the RBM\u2013OMP\u2013like and DBN\u2013OMP\u2013 like algorithms generate higher quality images than the OMP and FV\u2013OMP\u2013like algorithms, as can be noticed by the reduction of artifacts, the sharper edges, and the preservation of details. In contrast, images reconstructed with the OMP algorithm have poor quality as OMP does not exploit any structure beyond sparsity."}, {"heading": "V. CONCLUSIONS", "text": "Deep learning is one of the most powerful representation learning techniques. In this paper, it was shown how the ability of one deep learning architecture, the deep belief network, and\nrestricted Boltzmann machines to capture the complex statistical structure of the input data can be leveraged by CS systems. Statistical dependencies are informative and exploiting them leads to improvements in reconstruction performance.\nThe proposed scheme operates over signals belonging to a certain signal class. In this paper, the signal classes of natural images from the Berkeley Segmentation Dataset and of handwritten digits from the MNIST Database were selected, but the scheme can also be applied to other signal classes; e.g. radar, ECG, EEG, medical imaging, speech signals, etc. Restricted Boltzmann machines and DBNs were employed to model a prior distribution for the sparsity pattern of the signal class. Such a prior was employed by a MAP estimator for the reconstruction. It was shown through simulations that the proposed scheme leads to significantly superior reconstruction results when compared with CS methods that do not exploit any statistical dependencies between dictionary atoms. The proposed approach also outperforms CS methods that only exploit pair\u2013wise correlations between dictionary atoms."}], "references": [{"title": "Compressed sensing", "author": ["D. Donoho"], "venue": "IEEE Transactions on Information Theory, vol. 52, no. 4, pp. 1289\u20131306, Sept. 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "An introduction to compressive sampling", "author": ["E.J. Cand\u00e8s", "M.B. Wakin"], "venue": "IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 21\u201330, Mar. 2008.  13", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Structured compressed sensing: From theory to applications", "author": ["M.F. Duarte", "Y.C. Eldar"], "venue": "IEEE Transactions on Signal Processing, vol. 59, no. 9, pp. 4053\u20134085, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Block-sparse signals: Uncertainty relations and efficient recovery", "author": ["Y.C. Eldar", "P. Kuppinger", "H. Bolcskei"], "venue": "IEEE Transactions on Signal Processing, vol. 58, no. 6, pp. 3042\u20133054, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian compressive sensing", "author": ["S. Ji", "Y. Xue", "L. Carin"], "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 6, pp. 2346\u20132356, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Compressed sensing and Bayesian experimental design", "author": ["M.W. Seeger", "H. Nickisch"], "venue": "Proceedings of the 25th international conference on Machine learning. ACM, 2008, pp. 912\u2013919.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Iterative algorithms for compressed sensing with partially known support", "author": ["R.E. Carrillo", "L.F. Polania", "K.E. Barner"], "venue": "Proc., IEEE ICASSP, Dallas, TX, Mar. 2010, pp. 3654 \u20133657.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Compressed sensing and redundant dictionaries", "author": ["H. Rauhut", "K. Schnass", "P. Vandergheynst"], "venue": "IEEE Transactions on Information Theory, vol. 54, no. 5, pp. 2210\u20132219, May 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Optimized projections for compressed sensing", "author": ["M. Elad"], "venue": "IEEE Transactions on Signal Processing, vol. 55, no. 12, pp. 5695\u20135702, Dec 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning to sense sparse signals: Simultaneous sensing matrix and sparsifying dictionary optimization", "author": ["J.M. Duarte-Carvajalino", "G. Sapiro"], "venue": "IEEE Transactions on Image Processing, vol. 18, no. 7, pp. 1395\u20131408, July 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Accelerated diffusion spectrum imaging with compressed sensing using adaptive dictionaries", "author": ["B. Bilgic", "K. Setsompop", "J. Cohen-Adad", "A. Yendiki", "L.L. Wald", "E. Adalsteinsson"], "venue": "Magnetic Resonance in Medicine, vol. 68, no. 6, pp. 1747\u20131754, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Task-driven dictionary learning", "author": ["J. Mairal", "F. Bach", "J. Ponce"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 4, pp. 791\u2013804, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Nonparametric Bayesian dictionary learning for analysis of noisy and incomplete images", "author": ["M. Zhou", "H. Chen", "J. Paisley", "L. Ren", "L. Li", "Z. Xing", "D. Dunson", "G. Sapiro", "L. Carin"], "venue": "IEEE Transactions on Image Processing, vol. 21, no. 1, pp. 130\u2013144, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploiting statistical dependencies in sparse representations for signal recovery", "author": ["T. Peleg", "Y.C. Eldar", "M. Elad"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 5, pp. 2286\u20132303, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Representational power of restricted Boltzmann machines and deep belief networks", "author": ["N. Le Roux", "Y. Bengio"], "venue": "Neural Computation, vol. 20, no. 6, pp. 1631\u20131649, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep, narrow sigmoid belief networks are universal approximators", "author": ["I. Sutskever", "G.E. Hinton"], "venue": "Neural Computation, vol. 20, no. 11, pp. 2629\u20132636, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Found. Trends Mach. Learn., vol. 2, no. 1, pp. 1\u2013127, Jan. 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse signal recovery using Markov random fields", "author": ["V. Cevher", "M.F. Duarte", "C. Hegde", "R. Baraniuk"], "venue": "Advances in Neural Information Processing Systems, 2009, pp. 257\u2013264.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning horizontal connections in a sparse coding model of natural images", "author": ["P. Garrigues", "B.A. Olshausen"], "venue": "Advances in Neural Information Processing Systems, 2008, pp. 505\u2013512.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Boltzmann machine and mean-field approximation for structured sparse decompositions", "author": ["A. Dremeau", "C. Herzet", "L. Daudet"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 7, pp. 3425\u20133438, July 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "A statistical prediction model based on sparse representations for single image super-resolution", "author": ["T. Peleg", "M. Elad"], "venue": "IEEE transactions on image processing, vol. 23, no. 6, pp. 2569\u20132582, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Approximate message passing with restricted Boltzmann machine priors", "author": ["E.W. Tramel", "A. Dr\u00e9meau", "F. Krzakala"], "venue": "Journal of Statistical Mechanics: Theory and Experiment, vol. 2016, no. 7, pp. 073401, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Approximate message passing algorithms for compressed sensing", "author": ["M.A. Maleki", "D. Donoho", "R. Gray", "A. Montanari"], "venue": "Stanford University,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Sparse approximation based on a random overcomplete basis", "author": ["Y. Nakanishi-Ohno", "T. Obuchi", "M. Okada", "Y. Kabashima"], "venue": "Journal of Statistical Mechanics: Theory and Experiment, vol. 2016, no. 6, pp. 063302, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "An lp-based reconstruction algorithm for compressed sensing radar imaging", "author": ["L. Zheng", "A. Maleki", "Q. Liu", "X. Wang", "X. Yang"], "venue": "IEEE Radar Conference, 2016, pp. 1\u20135.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning algorithms for the classification restricted boltzmann machine", "author": ["H. Larochelle", "M. Mandel", "R. Pascanu", "Y. Bengio"], "venue": "The Journal of Machine Learning Research, vol. 13, no. 1, pp. 643\u2013669, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Markov chain Monte Carlo maximum likelihood", "author": ["C.J. Geyer"], "venue": "1991.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning with blocks: Composite likelihood and contrastive divergence", "author": ["A.U. Asuncion", "Q. Liu", "A.T. Ihler", "P. Smyth"], "venue": "International Conference on Artificial Intelligence and Statistics, 2010, pp. 33\u201340.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Near-optimal signal recovery from random projections: Universal encoding strategies", "author": ["E.J. Cand\u00e8s", "T. Tao"], "venue": "IEEE Transactions on Information Theory, vol. 52, no. 12, pp. 5406\u20135425, Dec 2006.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation, vol. 18, no. 7, pp. 1527\u20131554, July 2006.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Comput., vol. 14, no. 8, pp. 1771\u20131800, Aug. 2002.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2002}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "AISTATS, 2009, vol. 1, p. 3.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Variational bayesian algorithm for quantized compressed sensing", "author": ["Z. Yang", "L. Xie", "C. Zhang"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 11, pp. 2815\u20132824, June 2013.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Cram\u00e9r-Rao-type bounds for sparse Bayesian learning", "author": ["R. Prasad", "C.R. Murthy"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 3, pp. 622\u2013632, Feb 2013.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning horizontal connections in a sparse coding model of natural images", "author": ["P. Garrigues", "B.A. Olshausen"], "venue": "Advances in Neural Information Processing Systems, 2008, pp. 505\u2013512.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A Bruckstein"], "venue": "IEEE Transactions on Signal Processing, vol. 54, no. 11, pp. 4311\u20134322, Nov 2006.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "CS Technion, vol. 40, no. 8, pp. 1\u201315, 2008.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "Joint segmentation of piecewise constant autoregressive processes by using a hierarchical model and a Bayesian sampling approach", "author": ["N. Dobigeon", "J. Tourneret", "M. Davy"], "venue": "IEEE Transactions on Signal Processing, vol. 55, no. 4, pp. 1251\u20131263, 2007.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2007}, {"title": "Semi-supervised linear spectral unmixing using a hierarchical Bayesian model for hyperspectral imagery", "author": ["N. Dobigeon", "J. Tourneret", "C. Chang"], "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 7, pp. 2684\u20132695, 2008.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Bayesian fusion of multi-band images", "author": ["Q. Wei", "N. Dobigeon", "J. Tourneret"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 9, no. 6, pp. 1117\u20131127, 2015.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1998}, {"title": "Atomic decomposition by basis pursuit", "author": ["S.S. Chen", "D.L. Donoho", "M.A. Saunders"], "venue": "SIAM Journal on Scientific Computing, vol. 20, no. 1, pp. 33\u201361, Dec. 1998.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1998}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["D. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "Eighth IEEE International Conference on Computer Vision. IEEE, 2001, vol. 2, pp. 416\u2013423.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "COMPRESSED sensing has become an extensive research area due to its potential to perfectly reconstruct sparse signals from a small set of nonadaptive linear measurements in the form of random projections [1], [2].", "startOffset": 204, "endOffset": 207}, {"referenceID": 1, "context": "COMPRESSED sensing has become an extensive research area due to its potential to perfectly reconstruct sparse signals from a small set of nonadaptive linear measurements in the form of random projections [1], [2].", "startOffset": 209, "endOffset": 212}, {"referenceID": 2, "context": "In last decade, the area of CS has extended to new applications that require structured signal models that go beyond the simplistic sparsity model [3]\u2013[7].", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "In last decade, the area of CS has extended to new applications that require structured signal models that go beyond the simplistic sparsity model [3]\u2013[7].", "startOffset": 151, "endOffset": 154}, {"referenceID": 2, "context": "Examples of deterministic models include the wavelet tree model, which assumes that the non-zero signal coefficients lie in a rooted and connected tree structure, and the block-sparsity model, which assumes that the non-zero signal coefficients form clusters [3], [4].", "startOffset": 259, "endOffset": 262}, {"referenceID": 3, "context": "Examples of deterministic models include the wavelet tree model, which assumes that the non-zero signal coefficients lie in a rooted and connected tree structure, and the block-sparsity model, which assumes that the non-zero signal coefficients form clusters [3], [4].", "startOffset": 264, "endOffset": 267}, {"referenceID": 4, "context": "Instead of imposing an explicit structure of the coefficients, statistical approaches usually impose a prior belief about the signal of interest in terms of a sparseness prior [5], [6].", "startOffset": 176, "endOffset": 179}, {"referenceID": 5, "context": "Instead of imposing an explicit structure of the coefficients, statistical approaches usually impose a prior belief about the signal of interest in terms of a sparseness prior [5], [6].", "startOffset": 181, "endOffset": 184}, {"referenceID": 7, "context": "Even though the bulk of CS theory has been developed for signals that have a sparse representation in an orthonormal basis, efforts have been made to extend CS theory to signals that are sparse with respect to an overcomplete dictionary [8]\u2013 [10].", "startOffset": 237, "endOffset": 240}, {"referenceID": 9, "context": "Even though the bulk of CS theory has been developed for signals that have a sparse representation in an orthonormal basis, efforts have been made to extend CS theory to signals that are sparse with respect to an overcomplete dictionary [8]\u2013 [10].", "startOffset": 242, "endOffset": 246}, {"referenceID": 0, "context": "The coherence between the columns of an overcomplete dictionary poses some limitations in extending the CS theory to sparse overcomplete representations [1], [8].", "startOffset": 153, "endOffset": 156}, {"referenceID": 7, "context": "The coherence between the columns of an overcomplete dictionary poses some limitations in extending the CS theory to sparse overcomplete representations [1], [8].", "startOffset": 158, "endOffset": 161}, {"referenceID": 7, "context": "[8] showed that CS is viable in the context of signals that are sparse in an overcomplete dictionary.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Therefore, unlike most of the works that use overcomplete learned dictionaries in CS problems [11]\u2013[13], which only use the training stage to learn the dictionary and disregard the sparse codes associated with such a dictionary, the proposed approach exploits both dictionary and sparse codes from the training stage to improve CS reconstruction algorithm performance.", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": "Therefore, unlike most of the works that use overcomplete learned dictionaries in CS problems [11]\u2013[13], which only use the training stage to learn the dictionary and disregard the sparse codes associated with such a dictionary, the proposed approach exploits both dictionary and sparse codes from the training stage to improve CS reconstruction algorithm performance.", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "To overcome this limitation, we propose a greedy approach realized by modifying the orthogonal matching pursuit\u2013based algorithm proposed in [14] to maximize the posterior distribution of the sparsity pattern.", "startOffset": 140, "endOffset": 144}, {"referenceID": 14, "context": "First, they possess tremendous representational power; second, inference and parameter learning can be efficiently achieved using contrastive divergence and greedy layer\u2013wise training [15]\u2013 [17].", "startOffset": 184, "endOffset": 188}, {"referenceID": 16, "context": "First, they possess tremendous representational power; second, inference and parameter learning can be efficiently achieved using contrastive divergence and greedy layer\u2013wise training [15]\u2013 [17].", "startOffset": 190, "endOffset": 194}, {"referenceID": 14, "context": "[15] showed that an RBM can model any discrete distribution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] showed that deep belief networks can approximate any distribution over binary vectors to an arbitrary level of accuracy, even when the width of each layer is limited to the dimensionality of the data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "growing field in artificial intelligence [17].", "startOffset": 41, "endOffset": 45}, {"referenceID": 13, "context": "Previous works have employed fully visible Boltzmann machines to model the signal support in the context of compressed sensing [14], [18] and sparse coding [19], [20].", "startOffset": 127, "endOffset": 131}, {"referenceID": 17, "context": "Previous works have employed fully visible Boltzmann machines to model the signal support in the context of compressed sensing [14], [18] and sparse coding [19], [20].", "startOffset": 133, "endOffset": 137}, {"referenceID": 18, "context": "Previous works have employed fully visible Boltzmann machines to model the signal support in the context of compressed sensing [14], [18] and sparse coding [19], [20].", "startOffset": 156, "endOffset": 160}, {"referenceID": 19, "context": "Previous works have employed fully visible Boltzmann machines to model the signal support in the context of compressed sensing [14], [18] and sparse coding [19], [20].", "startOffset": 162, "endOffset": 166}, {"referenceID": 20, "context": "Restricted Boltzmann machines have been employed to model the dependencies between low resolution and high resolution patches in the image super\u2013resolution problem [21].", "startOffset": 164, "endOffset": 168}, {"referenceID": 21, "context": "[22] also uses RBMs to model the sparsity pattern of signals.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "However, it has been shown that AMP algorithms are very sensitive to parameter tuning [23].", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "Regardless of their simplicity and ease of implementation, OMP-based algorithms outperform AMP algorithms in some cases [24], [25], specially when the non-zero coefficients of sparse signals differ in magnitude [25].", "startOffset": 120, "endOffset": 124}, {"referenceID": 24, "context": "Regardless of their simplicity and ease of implementation, OMP-based algorithms outperform AMP algorithms in some cases [24], [25], specially when the non-zero coefficients of sparse signals differ in magnitude [25].", "startOffset": 126, "endOffset": 130}, {"referenceID": 24, "context": "Regardless of their simplicity and ease of implementation, OMP-based algorithms outperform AMP algorithms in some cases [24], [25], specially when the non-zero coefficients of sparse signals differ in magnitude [25].", "startOffset": 211, "endOffset": 215}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "In practice, pseudo\u2013likelihood learning has a high computational overhead compared to contrastive divergence [26].", "startOffset": 109, "endOffset": 113}, {"referenceID": 26, "context": "Pseudo\u2013likelihood learning does not approximate the maximum likelihood estimator well, except in the limit of zero dependence [27].", "startOffset": 126, "endOffset": 130}, {"referenceID": 27, "context": "It was shown that contrastive divergence is equivalent to pseudo\u2013likelihood for fully visible Boltzmann machines if single\u2013step Gibbs sampling is employed and outperforms pseudo\u2013likelihood when the number of sampling steps is larger than one [28].", "startOffset": 242, "endOffset": 246}, {"referenceID": 0, "context": "Compressed sensing [1], [2] addresses the recovery of x from linear measurements of the form y = \u03a6x \u2248 \u03a6Ds.", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "Compressed sensing [1], [2] addresses the recovery of x from linear measurements of the form y = \u03a6x \u2248 \u03a6Ds.", "startOffset": 24, "endOffset": 27}, {"referenceID": 28, "context": "Compressed sensing results show that the signal x can be reconstructed from y if the matrix \u039e = \u03a6D satisfies a condition, known as the restricted isometry property (RIP) [29], with a sufficiently small restricted isometry constant.", "startOffset": 170, "endOffset": 174}, {"referenceID": 7, "context": "However, if the matrix D is overcomplete, the coherence between its columns makes it difficult for the matrix \u039e to satisfy the RIP with a sufficiently small restricted isometry constant [8].", "startOffset": 186, "endOffset": 189}, {"referenceID": 8, "context": "Several works have addressed this limitation by designing the sampling matrix \u039e so as to minimize the mutual coherence of the effective dictionary \u039e [9], [10].", "startOffset": 149, "endOffset": 152}, {"referenceID": 9, "context": "Several works have addressed this limitation by designing the sampling matrix \u039e so as to minimize the mutual coherence of the effective dictionary \u039e [9], [10].", "startOffset": 154, "endOffset": 158}, {"referenceID": 16, "context": "Deep learning aims at learning hierarchical feature representations with higher level features formed by the composition of lower level features [17].", "startOffset": 145, "endOffset": 149}, {"referenceID": 29, "context": "A deep learning architecture, the DBN [30], is presented in this section.", "startOffset": 38, "endOffset": 42}, {"referenceID": 31, "context": "(b) Schematic of a deep belief network of one visible and three hidden layers (adapted from [32]).", "startOffset": 92, "endOffset": 96}, {"referenceID": 30, "context": "Given that computing the exact gradient of the log\u2013likelihood is intractable, the contrastive divergence approximation [31] is typically employed.", "startOffset": 119, "endOffset": 123}, {"referenceID": 29, "context": "(5) The log-probability of the training data can be improved by adding layers to the network, which, in turn, increases the true representational power of the network [30].", "startOffset": 167, "endOffset": 171}, {"referenceID": 29, "context": "[30] was a greedy, layer\u2013wise unsupervised learning algorithm that allows efficient training of DBNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "We adopt the commonly used assumption that the sampling noise variance \u03c3 n is known [14], [33], [34].", "startOffset": 84, "endOffset": 88}, {"referenceID": 32, "context": "We adopt the commonly used assumption that the sampling noise variance \u03c3 n is known [14], [33], [34].", "startOffset": 90, "endOffset": 94}, {"referenceID": 33, "context": "We adopt the commonly used assumption that the sampling noise variance \u03c3 n is known [14], [33], [34].", "startOffset": 96, "endOffset": 100}, {"referenceID": 13, "context": "The approach proposed in [14] is adopted in this paper, namely first calculating the MAP estimator of the sparsity pattern and then calculating the MAP estimator of the sparse vector.", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "employed in [14], [35] for nonzero sparse coefficients.", "startOffset": 12, "endOffset": 16}, {"referenceID": 34, "context": "employed in [14], [35] for nonzero sparse coefficients.", "startOffset": 18, "endOffset": 22}, {"referenceID": 14, "context": "Using RBMs to model the sparsity pattern probability distribution is justified by results that show that an RBM can model any discrete distribution and that adding hidden units yields strictly enhanced modeling performance, unless the RBM already perfectly models the data [15].", "startOffset": 273, "endOffset": 277}, {"referenceID": 15, "context": "Similarly, the use of deep belief networks is justified by their capabilities to approximate any distribution over binary vectors to an arbitrary level of accuracy, even when the width of each layer is limited to the dimensionality of the data [16].", "startOffset": 244, "endOffset": 248}, {"referenceID": 14, "context": "However, DBNs offer an additional advantage over RBMs: they yield more efficient and compact representations in terms of the number of parameters [15].", "startOffset": 146, "endOffset": 150}, {"referenceID": 13, "context": "[14] proposed a greedy pursuit algorithm based on Orthogonal Matching Pursuit (OMP) to approximate the MAP estimator of a sparse representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "The same algorithm is used here for reconstructing the sparse representation s, although using a different posterior distribution than that in [14].", "startOffset": 143, "endOffset": 147}, {"referenceID": 14, "context": "5) Selection of the probabilistic generative model: Even though DBNs and RBMs have the same representational power [15], they differ in the number of parameters needed to model the probability distribution of the data.", "startOffset": 115, "endOffset": 119}, {"referenceID": 16, "context": "Indeed, in [17], it was shown that deep architectures can sometimes be exponentially more efficient than shallow ones in terms of number of parameters needed to represent a function.", "startOffset": 11, "endOffset": 15}, {"referenceID": 35, "context": "[36] is employed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] show that the dominant operations in K-SVD are sparse-coding, atom updates and coefficients updates, wich leads to a total computational complexity of O(B(K3 + 2KNJ + 4NK + 4KJ) + 5NJ).", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "2) Sampling matrix design: Even though an overcomplete dictionary offers more flexibility and leads to sparser representations compared to an orthonormal basis, the coherence between its columns limits the performance of CS recovery algorithms when using the traditional sub\u2013Gaussian sensing matrix [8]\u2013[10].", "startOffset": 299, "endOffset": 302}, {"referenceID": 9, "context": "2) Sampling matrix design: Even though an overcomplete dictionary offers more flexibility and leads to sparser representations compared to an orthonormal basis, the coherence between its columns limits the performance of CS recovery algorithms when using the traditional sub\u2013Gaussian sensing matrix [8]\u2013[10].", "startOffset": 303, "endOffset": 307}, {"referenceID": 8, "context": "For example, Elad [9] optimizes the sampling matrix to minimize the coherence of the effective dictionary.", "startOffset": 18, "endOffset": 21}, {"referenceID": 9, "context": "[10] also aims at minimizing the coherence of the effective dictionary, with their approach designed to make the columns of the effective dictionary as orthogonal as possible.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "For more details on contrastive divergence, the reader is referred to [31].", "startOffset": 70, "endOffset": 74}, {"referenceID": 29, "context": "Hinton presented a powerful greedy layer\u2013wise method to learn these parameters [30].", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "Additionally, the set of sparse codes A can be employed to estimate the variance \u03c3 si of each ith element of the sparse representation s of signal x (see [14]):", "startOffset": 154, "endOffset": 158}, {"referenceID": 37, "context": "In [38]\u2013[40], a number of samples equal to 500 leads to a good approximation of the empirical average using samplingbased methods.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "In [38]\u2013[40], a number of samples equal to 500 leads to a good approximation of the empirical average using samplingbased methods.", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "Synthetic signals of dimension N = 256 are formed with the sparsity patterns and the magnitude of each nonzero coefficient is drawn from a Gaussian distribution with zero mean and known variance \u03c3 si \u2208 [10, 50].", "startOffset": 202, "endOffset": 210}, {"referenceID": 9, "context": "The sampling noise variance and the variance of the nonzero coefficients are kept the same as in the previous experiment, \u03c3 n = 1 and \u03c3 2 si \u2208 [10, 50], \u2200i, respectively.", "startOffset": 143, "endOffset": 151}, {"referenceID": 40, "context": "The MNIST dataset [41], which contains 70000 grayscale images of handwritten digits of size N = 28 \u00d7 28, is employed for the experiments.", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "also evaluated their RBM-based reconstruction method using the MNIST database [22].", "startOffset": 78, "endOffset": 82}, {"referenceID": 21, "context": "However, this is not a fair comparison since the authors of [22] selected different values for the model and experiment parameters.", "startOffset": 60, "endOffset": 64}, {"referenceID": 41, "context": "Figures 8 compares the performance of the proposed algorithms on the testing dataset, using the model configurations chosen via cross-validation, with CS reconstruction algorithms, such as OMP and basis pursuit denoising (BPDN) [42].", "startOffset": 228, "endOffset": 232}, {"referenceID": 13, "context": "sented in [14] (henceforth referred to as FV\u2013OMP-like algorithm), which has the same structure as the RBM\u2013OMP\u2013 like algorithm, with the difference being that it employs a fully visible Boltzmann machine to model the probability distribution of the sparsity pattern.", "startOffset": 10, "endOffset": 14}, {"referenceID": 42, "context": "This section presents experimental validation of the proposed algorithms using real data from the Berkeley segmentation dataset [43].", "startOffset": 128, "endOffset": 132}, {"referenceID": 9, "context": "In the case of overcomplete learned dictionaries, the sensing matrix is optimized as described in [10] and the number of dictionary atoms is set to 2N .", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "Such parameters are learned from the training dataset using the maximum likelihood approach described in [14].", "startOffset": 105, "endOffset": 109}, {"referenceID": 14, "context": "[15], which shows that, even though a DBN and an RBM can have the 0.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "This paper proposes a CS scheme that exploits the representational power of restricted Boltzmann machines and deep learning architectures to model the prior distribution of the sparsity pattern of signals belonging to the same class. The determined probability distribution is then used in a maximum a posteriori (MAP) approach for the reconstruction. The parameters of the prior distribution are learned from training data. The motivation behind this approach is to model the higher\u2013order statistical dependencies between the coefficients of the sparse representation, with the final goal of improving the reconstruction. The performance of the proposed method is validated on the Berkeley Segmentation Dataset and the MNIST Database of handwritten digits.", "creator": "LaTeX with hyperref package"}}}