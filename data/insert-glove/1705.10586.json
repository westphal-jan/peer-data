{"id": "1705.10586", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "Character-Based Text Classification using Top Down Semantic Model for Sentence Representation", "abstract": "tenders Despite the success a340-300 of deep french-algerian learning chz on shanqin many marah fronts especially kuribayashi image multilinear and chieftan speech, its 50-kilometre application in indogate text osvald classification often is still not zalmoxis as good 576i as costar a kovick simple linear backslash SVM on 14-seat n - regality gram TF - quizbowl IDF representation najd especially williamsburg for artt smaller meditative datasets. denaby Deep learning tends bsdi to emphasize on sentence 672 level semantics when blowouts learning a gag representation with models like recurrent neural kizirian network or recursive anti-revolutionary neural network, however] from crevecoeur the success of TF - IDF zeehan representation, it private-owned seems a thit bag - of - copeia words type of representation has its alnajjar strength. Taking advantage 65.72 of bouverie both representions, we pseudouridines present dreadfuls a model lescano known as TDSM (mionica Top sitchensis Down philostratus Semantic harmattan Model) idziak for extracting a boxley sentence representation laraque that hampson considers 15-67 both acupressure the word - tsewang level ayam semantics by bhasin linearly combining tambi the words lowculture with mazeroski attention fernald weights berchtesgadener and 97.83 the hipkiss sentence - maleeha level semantics kyivan with skarsgard BiLSTM berab and use it on text attash classification. We apply garb the model sutherlands on sublets characters donaghey and tim our results disprove show that 16,350 our model mastoid is better ladywood than all the laryngitis other donder character - ritcher based gasconade and an-24 word - based convolutional leske neural kalibata network camaret models somasundaram by \\ radiochemistry cite {zhang15} across seven direito different rabie datasets posters with only 2,328 1 \\% of sesh their parameters. hormizd We also demonstrate mirer that damiani this 4-percent model pokeweed beats alpha-helices traditional ohthere linear whitby models on openbsd TF - tyrosine IDF vectors on ferociously small blanes and dga polished severns datasets viharas like news article chibnall in stockpiled which 54.79 typically deep neelum learning barritt models kunani surrender.", "histories": [["v1", "Mon, 29 May 2017 15:53:00 GMT  (1514kb,D)", "http://arxiv.org/abs/1705.10586v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["zhenzhou wu", "xin zheng", "daniel dahlmeier"], "accepted": false, "id": "1705.10586"}, "pdf": {"name": "1705.10586.pdf", "metadata": {"source": "CRF", "title": "Character-Based Text Classification using Top Down Semantic Model for Sentence Representation", "authors": ["Zhenzhou Wu", "Xin Zheng", "Daniel Dahlmeier"], "emails": ["hyciswu@gmail.com", "xin.zheng@sap.com", "d.dahlmeier@sap.com"], "sections": [{"heading": "1 Introduction", "text": "Recently, deep learning has been particularly successful in speech and image as an automatic feature extractor (Graves et al., 2013b; Russakovsky et al., 2014; He et al., 2015), however deep learn-\ning\u2019s application to text as an automatic feature extractor has not been always successful (Zhang et al., 2015) even compared to simple linear models with BoW or TF-IDF feature representation. In many experiments when the text is polished like news articles or when the dataset is small, BoW or TF-IDF is still the state-of-art representation compared to sent2vec or paragraph2vec (Le & Mikolov, 2014) representation using deep learning models like RNN (Recurrent Neural Network) or CNN (Convolution Neural Network) (Zhang et al., 2015). It is only when the dataset becomes large or when the words are noisy and non-standardized with misspellings, text emoticons and short-forms that deep learning models which learns the sentence-level semantics start to outperform BoW representation, because under such circumstances, BoW representation can become extremely sparse and the vocabulary size can become huge. It becomes clear that for large, complex data, a large deep learning model with a large capacity can extract a better sentence-level representation than BoW sentence representation. However, for small and standardized news-like dataset, a direct word counting TF-IDF sentence representation is superior. Then the question is can we design a deep learning model that performs well for both simple and complex, small and large datasets? And when the dataset is small and standardized, the deep learning model should perform comparatively well as BoW? With that problem in mind, we designed TDSM (Top-Down-SemanticModel) which learns a sentence representation that carries the information of both the BoW-like representation and RNN style of sentence-level semantic which performs well for both simple and complex, small and large datasets.\nGetting inspiration from the success of TFIDF representation, our model intends to learn a word topic-vector which is similar to TF-IDF vec-\nar X\niv :1\n70 5.\n10 58\n6v 1\n[ cs\n.C L\n] 2\n9 M\nay 2\n01 7\ntor of a word but is different from word embedding, whereby the values in the topic-vector are all positives, and each dimension of the topicvector represents a topic aspect of the word. Imagine a topic-vector of representation meaning [animal, temperature, speed], so a rat maybe represented as [0.9, 0.7, 0.2] since rat is an animal with high body temperature but slow running speed compared to a car which maybe represented as [0.1, 0.8, 0.9] for being a non-animal, but high engine temperature and fast speed. A topic-vector will have a much richer semantic meaning than one-hot TF-IDF representation and also, it does not have the cancellation effect of summing wordembeddings positional vectors ([\u22121, 1]+[1,\u22121] = [0, 0]). The results from (Johnson & Zhang, 2015) show that by summing word-embedding vectors as sentence representation will have a catastrophic result for text classification.\nKnowing the topic-vector of each word, we can combine the words into a sentence representation s\u0303 by learning a weight wi for each word vi and do a linear sum of the words, s\u0303 = \u2211 iwiv\u0303i. The weights wi for each word in the sentence summation is learnt by recurrent neural network (RNN) (Pascanu et al., 2013) with attention over the words (Yang et al., 2016). The weights corresponds to the IDF (inverse document frequency) in TF-IDF representation, but with more flexibility and power. IDF is fixed for each word and calculated from all the documents (entire dataset), however attention weights learned from RNN is conditioned on both the document-level and datasetlevel semantics. This sentence representation from topic-vector of each word is then concatenated with the sentence-level semantic vector from RNN to give a top-down sentence representation as illustrated in Figure 2."}, {"heading": "1.1 TDSM on Characters", "text": "TDSM is a framework that can be applied to both word-level or character-level inputs. Here in this paper, we choose character-level over word-level inputs for practical industry reasons.\n1. In industry applications, often the model is required to have continuous learning on datasets that morph over time. Which means the vocabulary may change over time, therefore feeding the dataset by characters dispel the need for rebuilding a new vocabulary every time when there are new words.\n2. Industry datasets are usually very complex and noisy with a large vocabulary, therefore the memory foot-print of storing word embeddings is much larger than character embeddings.\nTherefore improving the performance of a character-based model has a much larger practical value compared to word-based model."}, {"heading": "2 Related Work", "text": "There are many traditional machine learning methods for text classification and most of them could achieve quite good results on formal text datasets. Recently, many deep learning methods have been proposed to solve the text classification task (Zhang et al., 2015; dos Santos & Gatti, 2014; Kim, 2014).\nDeep convolutional neural network has been extremely successful for image classification (Krizhevsky et al., 2012; Sermanet et al., 2013). Recently, many research also tries to apply it on text classification problem. Kim 2014 proposed a model similar to Collobert\u2019s et al. 2011 architecture. However, they employ two channels of word vectors. One is static throughout training and the other is fine-tuned via back-propagation. Various size of filters are applied on both channels, and the outputs are concatenated together. Then max-pooling over time is taken to select the most significant feature among each filter. The selected features are concatenated as the sentence vector.\nSimilarly, Zhang et al. 2015 also employs the convolutional networks but on characters instead of words for text classification. They design two networks for the task, one large and one small. Both of them have nine layers including six convolutional layers and three fully-connected layers. Between the three fully connected layers they insert two dropout layers for regularization. For both convolution and max-pooling layers, they employ 1-D filters (Boureau et al., 2010). After each convolution, they apply 1-D max-pooling. Specially, they claim that 1-D max-pooling enable them to train a relatively deep network.\nBesides applying models directly on testing datasets, more aspects are considered when extracting features. Character-level feature is adopted in many tasks besides Zhang et al. 2015 and most of them achieve quite good performance.\nSantos and Zadrozny 2014 take word morphology and shape into consideration which have been ignored for part-of-speech tagging task. They suggest the intra-word information is extremely useful when dealing with morphologically rich languages. They adopt neural network model to learn the character-level representation which is further delivered to help word embedding learning.\nKim et al. 2015 constructs neural language model by analysis of word representation obtained from character composition. Results suggest that the model could encode semantic and orthographic information from character level.\n(Tang et al., 2016; Yang et al., 2016) uses two hierarchies of recurrent neural network to extract\nthe document representation. The lower hierarchical recurrent neural network summarizes a sentence representation from the words in the sentence. The upper hierarchical neural network then summarizes a document representation from the sentences in the document. The major difference between (Tang et al., 2016) and (Yang et al., 2016) is that Yang applies attention over outputs from the recurrent when learning a summarizing representation.\nAttention model is also utilized in our model, which is used to assign weights for each word. Usually, attention is used in sequential model (Rockta\u0308schel et al., 2015; Mnih et al., 2014; Bahdanau et al., 2016; Kadlec et al., 2016). The at-\ntention mechanism includes sensor, internal state, actions and reward. At each time-step, the sensor will capture a glimpse of the input which is a small part of the entire input. Internal state will summarize the extracted information. Actions will decide the next glimpse location for the next step and reward suggests the benefit when taking the action. In our network, we adopt a simplified attention network as (Raffel & Ellis, 2015, 2016). We learn the weights over the words directly instead of through a sequence of actions and rewards.\nResidual network (He et al., 2015, 2016; Chen et al., 2016) is known to be able to make very deep neural networks by having skip-connections that allows gradient to back-propagate through the skip-connections. Residual network in (He et al., 2015) outperforms the state-of-the-art models on image recognition. He 2016 introduces residual block as similar to feature refinement for image classification. Similarly, for text classification problem, the quality of sentence representation is also quite important for the final result. Thus, we try to adopt the residual block as in (He et al., 2015, 2016) to refine the sentence vector."}, {"heading": "3 Model", "text": ""}, {"heading": "3.1 Characters to Topic-Vector", "text": "Unlike word-embedding (Mikolov et al., 2013), topic-vector tries to learn a distributed topic representation at each dimension of the representation vector, which thus allows the simple addition of the word-level topic-vectors to form a sentence representation. Figure 1 illustrates how topicvector is extracted from characters in words using FCN (Fully Convolutional Network). In order to force word level representation with topic meanings, we apply a sigmoid function over the output from FCN. Doing so, restrain the values at each dimension to be between 0 and 1, thus forcing the model to learn a distributed topic representation of the word."}, {"heading": "3.2 Sentence Representation Vector Construction", "text": "Forming a sentence representation from words can be done simply by summing of the wordembeddings which produce catastrophic results (Johnson & Zhang, 2015) due to the cancellation effect of adding embedding vectors (negative plus positive gives zero). Or in our model, the summing of word-level topic-vectors which give a much bet-\nter sentence representation as shown in Table 3 than summing word-embeddings.\nBoW Features: Sentence vector derived from summing of the word topic-vectors is equivalent to the BoW vectors in word counting, whereby we treat the prior contribution of each word to the final sentence vector equally. Traditionally, a better sentence representation over BoW will be TF-IDF, which gives a weight to each word in a document in terms of IDF (inverse document frequency). Drawing inspiration from TF-IDF representation, we can have a recurrent neural network that outputs the attention (Yang et al., 2016) over the words. And the attention weights serve similar function as the IDF except that it is local to the context of the document, since the attention weight for a word maybe different for different documents while the IDF of a word is the same throughout all documents. With the attention weights wi and word topic-vector v\u0303i, we can form a sentence vector s\u0303bow by linear sum\ns\u0303bow = \u2211 i wiv\u0303i (1)\nPositional Features: With the neural sentence vector that derived from BoW which captures the information of individual words. We can also concatenate it with the output state from the RNN which captures the document level information and whose representation is conditioned on the positioning of the words in the document.\ns\u0303t = RNN(v\u0303t, s\u0303t\u22121) (2)\ns\u0303pos = s\u0303T (3) s\u0303 = s\u0303bow \u2295 s\u0303pos (4)\nwhere T is the length of the document, and \u2295 represents concatenation such that |s\u0303| = |s\u0303bow| + |s\u0303pos|. The overall sentence vector s\u0303 will then capture the information of both the word-level and document-level semantics of the document. And thus it has a very rich representation."}, {"heading": "3.2.1 Bi-Directional LSTM", "text": "We used Bi-directional LSTM (BiLSTM) (Graves et al., 2013a) as the recurrent unit. BiLSTM consist of a forward LSTM (FLSTM) and a backward LSTM (BLSTM), both LSTMs are of the same design, except that FLSTM reads the sentence in a forward manner and BLSTM reads the sentence in a backward manner. One recurrent step in LSTM\nof Equation 2 consists of the following steps f\u0303t = \u03c3 ( Wf (s\u0303t\u22121 \u2295 v\u0303t) + b\u0303f ) (5)\ni\u0303t = \u03c3 ( Wi(s\u0303t\u22121 \u2295 v\u0303t) + b\u0303i ) (6)\nC\u0303t = tanh ( WC(s\u0303t\u22121, v\u0303t) + b\u0303C ) (7)\nC\u0303t = f\u0303t \u2297 C\u0303t\u22121 + i\u0303t \u2297 C\u0303t (8) o\u0303t = \u03c3 ( Wo(s\u0303t\u22121 \u2295 v\u0303t) + b\u0303o ) (9)\ns\u0303t = o\u0303t \u2297 tanh(C\u0303t) (10)\nwhere \u2297 is the element-wise vector multiplication, \u2295 is vector concatenation similarly defined in Equation 4. f\u0303t is forget state, i\u0303t is input state, o\u0303t is output state, C\u0303t is the internal context which contains the long-short term memory of historical semantics that LSTM reads. Finally, the output from the BiLSTM will be a concatenation of the output from FLSTM and BLSTM\nf\u0303t = FLSTM(v\u0303t, s\u0303t\u22121) (11)\nb\u0303t = BLSTM(v\u0303t, s\u0303t+1) (12)\nh\u0303t = f\u0303t \u2295 b\u0303t (13)\nHere, the concatenated output state of BiLSTM has visibility of the entire sequence at any timestep compared to single-directional LSTM which only has visibility of the sequence in the past. This property of BiLSTM is very useful for learning attention weights for each word in a document because then the weights are decided based on the information of the entire document instead of just words before it as in LSTM."}, {"heading": "4 Experiment", "text": ""}, {"heading": "4.1 Model", "text": "The entire model has only 780,000 parameters which is only 1% of the parameters in (Zhang et al., 2015) large CNN model. We used BiLSTM\n(Graves et al., 2013a) with 100 units in both forward and backward LSTM cell. The output from the BiLSTM is 200 dimensions after we concatenate the outputs from the forward and backward cells. We then use attention over the words by linearly transform 200 output dimensions to 1 follow by a softmax over the 1 dimension outputs from all the words. After the characters to topic-vector transformation by FCN, each topic vector will be 180 dimensions. The topic-vectors are then linearly sum with attention weights to form a 180 dimensions BoW-like sentence vector. This vector is further concatenate with 200 dimensions BiLSTM outputs. The 380 dimensions undergo 10 blocks of ResNet (Chen et al., 2016) plus one fully connected layer. We use RELU (Nair & Hinton, 2010) for all the intra-layer activation functions. The source code will be released after some code refactoring and we built the models with tensorflow (Abadi et al., 2016) and tensorgraph 1."}, {"heading": "4.2 Datasets", "text": "We use the standard benchmark datasets prepare by (Zhang et al., 2015). The datasets have different number of training samples and test samples ranging from 28,000 to 3,600,000 training sam-\n1https://github.com/hycis/TensorGraph\nples, and of different text length ranging from average of 38 words for Ag News to 566 words in Sogou news as illustrated in Table 1. The datasets are a good mix of polished (AG) and noisy (Yelp and Amazon reviews), long (Sogou) and short (DBP and AG), large (Amazon reviews) and small (AG) datasets. And thus the results over these datasets serve as good evaluation on the quality of the model."}, {"heading": "4.3 Word Setting", "text": "In this paper, we take 128 ASCII characters as character set, by which most of the testing documents are composite. We define word length as 20 and character embedding length as 100. If a word with characters less than 20, we will pad it with ze-\nros. If the length is larger than 20, we just take the first 20 characters. We set the maximum length of words as the average number of words of the documents in the dataset plus two standard deviation, which is long enough to cover more than 97.5% of the documents. For documents with number of words more than the preset maximum number of words, we will discard the exceeding words."}, {"heading": "4.4 Baseline Models", "text": "We select both traditional models and the convolutional models from (Zhang et al., 2015), the recurrent models from (Yang et al., 2016; Tang et al., 2016) as baselines. Also in order to ensure a fair comparison of models, such that any variation in the result is purely due to the model difference, we compare TDSM only with models that are trained in the same way of data preparation, that is the words are lowered and there are no additional data alteration or augmentation with thesaurus. Unfortunately, (Yang et al., 2016; Tang et al., 2016) recurrent models are trained on full text instead of lowered text, so their models may not be objectively compared to our models, since it is well known from (Uysal & Gunal, 2014) that differ-\nent text preprocessing will have significant impact on the final results, Zhang\u2019s result shows that a simple case lowering can result up to 4% difference in classification accuracy. Despite this, we still include the recurrent models for comparison, because they provide a good reference for understanding time-based models on large datasets of long sentences."}, {"heading": "4.4.1 Traditional Models", "text": "BoW is the standard word counting method whereby the feature vector represents the term frequency of the words in a sentence.\nTF-IDF is similar to BoW, except that it is derived by the counting of the words in the sentence weighted by individual word\u2019s term-frequency and inverse-document-frequency (Joachims, 1998). This is a very competitive model especially on clean and small dataset."}, {"heading": "4.4.2 Word-Based Models", "text": "Bag-of-Means is derived from clustering of words-embeddings with k-means into 5000 clusters, and follow by BoW representation of the words in 5000 clusters.\nLg. w2v Conv and Sm w2v Conv is CNN model on word embeddings following (Zhang et al., 2015), to ensure fair comparison with character-based models, the CNN architecture is the same as Lg. Conv and Sm. Conv with the same number of parameters.\nLSTM-GRNN from (Tang et al., 2016) is basically a recurrent neural network based on LSTM and GRU (Chung et al., 2014) over the words in a sentence, and over the sentences in a document. It tries to learn a hierarchical representation of the text from multi-levels of recurrent layers.\nHN-ATT from (Yang et al., 2016) is basically similar to LSTM-GRNN except that instead of just learning the hierarchical representation of the text directly with RNN, it also learns attention weights over the words during the summarization of the words and over the sentences during the summarization of the sentences."}, {"heading": "4.4.3 Character-Based Models", "text": "Lg. Conv and Sm. Conv are proposed in (Zhang et al., 2015), which is a CNN model on character encoding and is the primary characterbased baseline model that we are comparing with."}, {"heading": "5 Results", "text": "Table 3 shows the comparison results of different datasets with different size, different sentence length, and different quality (polished AG news vs messy Yelp and Amazon reviews).\nFrom the results, we see that TDSM outperforms all the other CNN models across all the datasets with only 1% of the parameters of Zhang\u2019s large conv model and 7.8% of his small conv model. Since these results are based on the same text preprocessing and across all kinds of dataset (long, short, large, small, polished, messy), we can confidently say that TDSM generalizes better than the other CNN models over text classification. These results show that a good architecture design can achieve a better accuracy with significantly less parameters.\nCharacter-based models are the most significant and practical model for real large scale industry deployment because of its smaller memory footprint, agnostic to changes in vocabulary and robust to misspellings (Kim et al., 2015). For a very long time, TF-IDF has been state-of-art models especially in small and standardized datasets. However because of its large memory footprint and non-suitability for continuous learning (because a new vocabulary has to be rebuilt every once in awhile when there are new words especially for data source like Tweeter), it was not an ideal model until character-based models came out. From the results, previous character-based models are generally better than TF-IDF for large datasets but falls short for smaller dataset like AG news. TDSM successfully close the gap between character-based models and TF-IDF by beating TF-IDF with 1% better performance. The results also confirm the hypothesis that TDSM as illustrated in Figure 2 which contains both the BoWlike and sentence-level features, has the best of the traditional TF-IDF and the recent deep learning model, is able to perform well for both small and large datasets.\nFrom the results, we also observe that TDSM improves over other character-based models by a big margin of 3% for Lg. Conv and 5.7% for Sm. Conv on the AG dataset. But the improvement tails off to only 0.5% for Amazon reviews when the dataset size increases from 120,000 to 3.6 million. This is probably because TDSM has reached its maximum capacity when the dataset gets very large compared to other character-based models\nwhich have 100 times the capacity of TDSM. For Yelp Full, we observe that the hierarchical recurrent models LSTM-GRNN and HN-ATT performs about 10% points better than TDSM but drops to only 3% for Amazon Full. This may be partly due to their data being prepared differently from our models. This can also be due to the structure of these hierarchical recurrent models which has two levels of recurrent neural networks for summarizing a document, whereby the first level summarizes a sentence vector from the words and the second level summarizes a document vector from the sentences. So these models will start to perform much better when there are alot of sentences and words in a document. For Yelp Full, there are of average 134 words in one document and Amazon Full has about 80 words per document. That\u2019s why the performance is much better for these recurrent models on Yelp than on Amazon. However, these hierarchical recurrent models will be reduce to a purely vanilla RNN for short text like AG News or Tweets with a few sentences, and under such circumstances its result will not be much different from a standard RNN. Nevertheless, LSTM-GRNN or HN-ATT does indicate the strength of RNN models in summarizing the sentences and documents and deriving a coherent sentence-level and document-level representation."}, {"heading": "6 Conclusion", "text": "From the results, we see a strong promise for TDSM as a competitive model for text classification because of its hybrid architecture that looks at the sentence from both the traditional TF-IDF point of view and the recent deep learning point of view. The results show that this type of view can derive a rich text representation for both small and large datasets."}], "references": [{"title": "Learning mid-level features for recognition", "author": ["Y-Lan Boureau", "Francis R. Bach", "Yann LeCun", "Jean Ponce"], "venue": "In The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Boureau et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2010}, {"title": "Semantic segmentation with modified deep residual networks", "author": ["Xinze Chen", "Guangliang Cheng", "Yinghao Cai", "Dayong Wen", "Heping Li"], "venue": "In Pattern Recognition - 7th Chinese Conference, CCPR,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Learning character-level representations for part-ofspeech tagging", "author": ["C\u0131\u0301cero Nogueira dos Santos", "Bianca Zadrozny"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Santos and Zadrozny.,? \\Q2014\\E", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Alex Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Computer Vision - ECCV - 14th European Conference,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["Thorsten Joachims"], "venue": "In European conference on machine learning,", "citeRegEx": "Joachims.,? \\Q1998\\E", "shortCiteRegEx": "Joachims.", "year": 1998}, {"title": "Semi-supervised convolutional neural networks for text categorization via region embedding", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Johnson and Zhang.,? \\Q2015\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2015}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP, A meeting of SIGDAT, a Special Interest Group of the ACL,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V. Le", "Tomas Mikolov"], "venue": "CoRR, abs/1405.4053,", "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "\u00c7aglar G\u00fcl\u00e7ehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1312.6026,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Feed-forward networks with attention can solve some long-term memory problems", "author": ["Colin Raffel", "Daniel P.W. Ellis"], "venue": "CoRR, abs/1512.08756,", "citeRegEx": "Raffel and Ellis.,? \\Q2015\\E", "shortCiteRegEx": "Raffel and Ellis.", "year": 2015}, {"title": "Pruning subsequence search with attention-based embedding", "author": ["Colin Raffel", "Daniel P.W. Ellis"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Raffel and Ellis.,? \\Q2016\\E", "shortCiteRegEx": "Raffel and Ellis.", "year": 2016}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Phil Blunsom"], "venue": "CoRR, abs/1509.06664,", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael S. Bernstein", "Alexander C. Berg", "Fei-Fei Li"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Micha\u00ebl Mathieu", "Rob Fergus", "Yann LeCun"], "venue": "CoRR, abs/1312.6229,", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Tang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "The impact of preprocessing on text classification", "author": ["Alper Kursat Uysal", "Serkan Gunal"], "venue": "Information Processing & Management,", "citeRegEx": "Uysal and Gunal.,? \\Q2014\\E", "shortCiteRegEx": "Uysal and Gunal.", "year": 2014}, {"title": "Hierarchical attention networks for document classification", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 28, "context": "We apply the model on characters and our results show that our model is better than all the other characterbased and word-based convolutional neural network models by (Zhang et al., 2015) across seven different datasets with only 1% of their parameters.", "startOffset": 167, "endOffset": 187}, {"referenceID": 23, "context": "Recently, deep learning has been particularly successful in speech and image as an automatic feature extractor (Graves et al., 2013b; Russakovsky et al., 2014; He et al., 2015), however deep learning\u2019s application to text as an automatic feature extractor has not been always successful (Zhang et al.", "startOffset": 111, "endOffset": 176}, {"referenceID": 7, "context": "Recently, deep learning has been particularly successful in speech and image as an automatic feature extractor (Graves et al., 2013b; Russakovsky et al., 2014; He et al., 2015), however deep learning\u2019s application to text as an automatic feature extractor has not been always successful (Zhang et al.", "startOffset": 111, "endOffset": 176}, {"referenceID": 28, "context": ", 2015), however deep learning\u2019s application to text as an automatic feature extractor has not been always successful (Zhang et al., 2015) even compared to simple linear models with BoW or TF-IDF feature representation.", "startOffset": 118, "endOffset": 138}, {"referenceID": 28, "context": "In many experiments when the text is polished like news articles or when the dataset is small, BoW or TF-IDF is still the state-of-art representation compared to sent2vec or paragraph2vec (Le & Mikolov, 2014) representation using deep learning models like RNN (Recurrent Neural Network) or CNN (Convolution Neural Network) (Zhang et al., 2015).", "startOffset": 323, "endOffset": 343}, {"referenceID": 19, "context": "The weights wi for each word in the sentence summation is learnt by recurrent neural network (RNN) (Pascanu et al., 2013) with attention over the words (Yang et al.", "startOffset": 99, "endOffset": 121}, {"referenceID": 27, "context": ", 2013) with attention over the words (Yang et al., 2016).", "startOffset": 38, "endOffset": 57}, {"referenceID": 28, "context": "Recently, many deep learning methods have been proposed to solve the text classification task (Zhang et al., 2015; dos Santos & Gatti, 2014; Kim, 2014).", "startOffset": 94, "endOffset": 151}, {"referenceID": 12, "context": "Recently, many deep learning methods have been proposed to solve the text classification task (Zhang et al., 2015; dos Santos & Gatti, 2014; Kim, 2014).", "startOffset": 94, "endOffset": 151}, {"referenceID": 24, "context": "Deep convolutional neural network has been extremely successful for image classification (Krizhevsky et al., 2012; Sermanet et al., 2013).", "startOffset": 89, "endOffset": 137}, {"referenceID": 0, "context": "For both convolution and max-pooling layers, they employ 1-D filters (Boureau et al., 2010).", "startOffset": 69, "endOffset": 91}, {"referenceID": 15, "context": "The transformation is done with fully convolutional network (FCN) similar to (Long et al., 2015), each hierarchical level of the FCN will extract an n-gram character feature of the word until the word-level topic-vector.", "startOffset": 77, "endOffset": 96}, {"referenceID": 25, "context": "(Tang et al., 2016; Yang et al., 2016) uses two hierarchies of recurrent neural network to extract the document representation.", "startOffset": 0, "endOffset": 38}, {"referenceID": 27, "context": "(Tang et al., 2016; Yang et al., 2016) uses two hierarchies of recurrent neural network to extract the document representation.", "startOffset": 0, "endOffset": 38}, {"referenceID": 25, "context": "The major difference between (Tang et al., 2016) and (Yang et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 27, "context": ", 2016) and (Yang et al., 2016) is that Yang applies attention over outputs from the recurrent when learning a summarizing representation.", "startOffset": 12, "endOffset": 31}, {"referenceID": 22, "context": "Usually, attention is used in sequential model (Rockt\u00e4schel et al., 2015; Mnih et al., 2014; Bahdanau et al., 2016; Kadlec et al., 2016).", "startOffset": 47, "endOffset": 136}, {"referenceID": 17, "context": "Usually, attention is used in sequential model (Rockt\u00e4schel et al., 2015; Mnih et al., 2014; Bahdanau et al., 2016; Kadlec et al., 2016).", "startOffset": 47, "endOffset": 136}, {"referenceID": 11, "context": "Usually, attention is used in sequential model (Rockt\u00e4schel et al., 2015; Mnih et al., 2014; Bahdanau et al., 2016; Kadlec et al., 2016).", "startOffset": 47, "endOffset": 136}, {"referenceID": 1, "context": "Residual network (He et al., 2015, 2016; Chen et al., 2016) is known to be able to make very deep neural networks by having skip-connections that allows gradient to back-propagate through the skip-connections.", "startOffset": 17, "endOffset": 59}, {"referenceID": 7, "context": "Residual network in (He et al., 2015) outperforms the state-of-the-art models on image recognition.", "startOffset": 20, "endOffset": 37}, {"referenceID": 16, "context": "Unlike word-embedding (Mikolov et al., 2013), topic-vector tries to learn a distributed topic representation at each dimension of the representation vector, which thus allows the simple addition of the word-level topic-vectors to form a sentence representation.", "startOffset": 22, "endOffset": 44}, {"referenceID": 27, "context": "Drawing inspiration from TF-IDF representation, we can have a recurrent neural network that outputs the attention (Yang et al., 2016) over the words.", "startOffset": 114, "endOffset": 133}, {"referenceID": 28, "context": "The entire model has only 780,000 parameters which is only 1% of the parameters in (Zhang et al., 2015) large CNN model.", "startOffset": 83, "endOffset": 103}, {"referenceID": 1, "context": "The 380 dimensions undergo 10 blocks of ResNet (Chen et al., 2016) plus one fully connected layer.", "startOffset": 47, "endOffset": 66}, {"referenceID": 28, "context": "We use the standard benchmark datasets prepare by (Zhang et al., 2015).", "startOffset": 50, "endOffset": 70}, {"referenceID": 28, "context": "Unfortunately, these two RNN models did not use the same text preprocessing technique as other models, so their models may not be objectively comparable to Zhang\u2019s or our model, because it is well known that (Zhang et al., 2015; Uysal & Gunal, 2014), the difference in text preprocessing will have a significant impact on the final accuracy.", "startOffset": 208, "endOffset": 249}, {"referenceID": 25, "context": "00 LSTM-GRNN (Tang et al., 2016) - - - 67.", "startOffset": 13, "endOffset": 32}, {"referenceID": 27, "context": "6 HN-ATT (Yang et al., 2016) - - - 71.", "startOffset": 9, "endOffset": 28}, {"referenceID": 28, "context": "Conv (Zhang et al., 2015) 87.", "startOffset": 5, "endOffset": 25}, {"referenceID": 28, "context": "Conv (Zhang et al., 2015) 84.", "startOffset": 5, "endOffset": 25}, {"referenceID": 28, "context": "We select both traditional models and the convolutional models from (Zhang et al., 2015), the recurrent models from (Yang et al.", "startOffset": 68, "endOffset": 88}, {"referenceID": 27, "context": ", 2015), the recurrent models from (Yang et al., 2016; Tang et al., 2016) as baselines.", "startOffset": 35, "endOffset": 73}, {"referenceID": 25, "context": ", 2015), the recurrent models from (Yang et al., 2016; Tang et al., 2016) as baselines.", "startOffset": 35, "endOffset": 73}, {"referenceID": 27, "context": "Unfortunately, (Yang et al., 2016; Tang et al., 2016) recurrent models are trained on full text instead of lowered text, so their models may not be objectively compared to our models, since it is well known from (Uysal & Gunal, 2014) that differ-", "startOffset": 15, "endOffset": 53}, {"referenceID": 25, "context": "Unfortunately, (Yang et al., 2016; Tang et al., 2016) recurrent models are trained on full text instead of lowered text, so their models may not be objectively compared to our models, since it is well known from (Uysal & Gunal, 2014) that differ-", "startOffset": 15, "endOffset": 53}, {"referenceID": 9, "context": "TF-IDF is similar to BoW, except that it is derived by the counting of the words in the sentence weighted by individual word\u2019s term-frequency and inverse-document-frequency (Joachims, 1998).", "startOffset": 173, "endOffset": 189}, {"referenceID": 28, "context": "w2v Conv and Sm w2v Conv is CNN model on word embeddings following (Zhang et al., 2015), to ensure fair comparison with character-based models, the CNN architecture is the same as Lg.", "startOffset": 67, "endOffset": 87}, {"referenceID": 25, "context": "LSTM-GRNN from (Tang et al., 2016) is basically a recurrent neural network based on LSTM and GRU (Chung et al.", "startOffset": 15, "endOffset": 34}, {"referenceID": 2, "context": ", 2016) is basically a recurrent neural network based on LSTM and GRU (Chung et al., 2014) over the words in a sentence, and over the sentences in a document.", "startOffset": 70, "endOffset": 90}, {"referenceID": 27, "context": "HN-ATT from (Yang et al., 2016) is basically similar to LSTM-GRNN except that instead of just learning the hierarchical representation of the text directly with RNN, it also learns attention weights over the words during the summarization of the words and over the sentences during the summarization of the sentences.", "startOffset": 12, "endOffset": 31}, {"referenceID": 28, "context": "Conv are proposed in (Zhang et al., 2015), which is a CNN model on character encoding and is the primary characterbased baseline model that we are comparing with.", "startOffset": 21, "endOffset": 41}, {"referenceID": 13, "context": "Character-based models are the most significant and practical model for real large scale industry deployment because of its smaller memory footprint, agnostic to changes in vocabulary and robust to misspellings (Kim et al., 2015).", "startOffset": 211, "endOffset": 229}], "year": 2017, "abstractText": "Despite the success of deep learning on many fronts especially image and speech, its application in text classification often is still not as good as a simple linear SVM on n-gram TF-IDF representation especially for smaller datasets. Deep learning tends to emphasize on sentence level semantics when learning a representation with models like recurrent neural network or recursive neural network, however from the success of TF-IDF representation, it seems a bag-of-words type of representation has its strength. Taking advantage of both representions, we present a model known as TDSM (Top Down Semantic Model) for extracting a sentence representation that considers both the word-level semantics by linearly combining the words with attention weights and the sentence-level semantics with BiLSTM and use it on text classification. We apply the model on characters and our results show that our model is better than all the other characterbased and word-based convolutional neural network models by (Zhang et al., 2015) across seven different datasets with only 1% of their parameters. We also demonstrate that this model beats traditional linear models on TF-IDF vectors on small and polished datasets like news article in which typically deep learning models surrender.", "creator": "LaTeX with hyperref package"}}}