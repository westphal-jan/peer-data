{"id": "1705.10742", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Generating Steganographic Text with LSTMs", "abstract": "Motivated by kusti concerns for ev-1 user mistake privacy, we hannover design a 134.95 steganographic system (\" shadrick stegosystem \") that doubters enables friedle two strong users to exchange summerton encrypted messages without an 900-year adversary detecting that m\u00e4rz such opcode an vining exchange is taking place. 54.13 We evgueni propose 33.28 a hakham new sompo linguistic eaglesmith stegosystem rouen based on a Long gyros Short - Term jafarzadeh Memory (rakers LSTM) biopsy neural gomberg network. groenendaal We kokia demonstrate our alfriston approach on the Twitter politifact.com and limnos Enron awg email cemento datasets decentralising and dkim show tezpur that bellange it memorializing yields high - quality steganographic ihrc text while 712.5 significantly segeberg improving toca capacity (encrypted isonomy bits per zamora word) pelota relative merenda to francona the state - of - iliopoulos the - mosser art.", "histories": [["v1", "Tue, 30 May 2017 16:52:48 GMT  (75kb,D)", "http://arxiv.org/abs/1705.10742v1", "ACL 2017 Student Research Workshop"]], "COMMENTS": "ACL 2017 Student Research Workshop", "reviews": [], "SUBJECTS": "cs.AI cs.CR cs.MM", "authors": ["tina fang", "martin jaggi", "katerina argyraki"], "accepted": true, "id": "1705.10742"}, "pdf": {"name": "1705.10742.pdf", "metadata": {"source": "CRF", "title": "Generating Steganographic Text with LSTMs", "authors": ["Tina Fang", "Martin Jaggi", "Katerina Argyraki"], "emails": ["tbfang@edu.uwaterloo.ca", "martin.jaggi@epfl.ch", "katerina.argyraki@epfl.ch"], "sections": [{"heading": "1 Introduction", "text": "The business model behind modern communication systems (email services or messaging services provided by social networks) is incompatible with end-to-end message encryption. The providers of these services can afford to offer them free of charge because most of their users agree to receive \u201ctargeted ads\u201d (ads that are especially chosen to appeal to each user, based on the needs the user has implied through their messages). This model works as long as users communicate mostly in the clear, which enables service providers to make informed guesses about user needs.\nThis situation does not prevent users from encrypting a few sensitive messages, but it does take away some of the benefits of confidentiality. For instance, imagine a scenario where two users want to exchange forbidden ideas or organize forbidden events under an authoritarian regime; in a world where most communication happens in the clear, encrypting a small fraction of messages automatically makes these messages\u2014and the users who exchange them\u2014suspicious.\nWith this motivation in mind, we want to design a system that enables two users to exchange encrypted messages, such that a passive adversary that reads the messages can determine neither the original content of the messages nor the fact that the messages are encrypted.\nWe build on linguistic steganography, i.e., the science of encoding a secret piece of information (\u201cpayload\u201d) into a piece of text that looks like natural language (\u201cstegotext\u201d). We propose a novel stegosystem, based on a neural network, and demonstrate that it combines high quality of output (i.e., the stegotext indeed looks like natural language) with the highest capacity (number of bits encrypted per word) published in literature.\nIn the rest of the paper, we describe existing linguistic stegosystems along with ours (\u00a72), provide details on our system (\u00a73), present preliminary experimental results on Twitter and email messages (\u00a74), and conclude with future directions (\u00a75)."}, {"heading": "2 Linguistic Steganography", "text": "In this section, we summarize related work (\u00a72.1), then present out proposal (\u00a72.2)."}, {"heading": "2.1 Related Work", "text": "Traditional linguistic stegosystems are based on modification of an existing cover text, e.g., using synonym substitution (Topkara et al., 2006; Chang and Clark, 2014) and/or paraphrase substitution (Chang and Clark, 2010). The idea is to encode the secret information in the transformation of the cover text, ideally without affecting its meaning or grammatical correctness. Of these systems, the most closely related to ours is CoverTweet (Wilson et al., 2014), a state-of-theart cover modification stegosystem that uses Twitter as the medium of cover; we compare to it in our preliminary evaluation (\u00a74).\nar X\niv :1\n70 5.\n10 74\n2v 1\n[ cs\n.A I]\n3 0\nM ay\n2 01\n7\nCover modification can introduce syntactic and semantic unnaturalness (Grosvald and Orgun, 2011); to address this, Grovsald and Orgun proposed an alternative stegosystem where a human generates the stegotext manually, thus improving linguistic naturalness at the cost of human effort (Grosvald and Orgun, 2011).\nMatryoshka (Safaka et al., 2016) takes this further: in step 1, it generates candidate stegotext automatically based on an n-gram model of the English language; in step 2, it presents the candidate stegotext to the human user for polishing, i.e., ideally small edits that improve linguistic naturalness. However, the cost of human effort is still high, because the (automatically generated) candidate stegotext is far from natural language, and, as a result, the human user has to spend significant time and effort manually editing and augmenting it.\nVolkhonskiy et al. have applied Generative Adversarial Networks (Goodfellow et al., 2014) to image steganography (Volkhonskiy et al., 2017), but we are not aware of any text stegosystem based on neural networks."}, {"heading": "2.2 Our Proposal: Steganographic LSTM", "text": "Motivated by the fact that LSTMs (Hochreiter and Schmidhuber, 1997) constitute the state of the art in text generation (Jozefowicz et al., 2016), we propose to automatically generate the stegotext from an LSTM (as opposed to an n-gram model). The output of the LSTM can then be used either directly as the stegotext, or Matryoshka-style, i.e., as a candidate stegotext to be polished by a human user; in this paper, we explore only the former option, i.e., we do not do any manual polishing. We describe the main components of our system in the paragraphs below; for reference, Fig. 1 outlines the building blocks of a stegosystem (Salomon, 2003).\nSecret data. The secret data is the information we want to hide. First, we compress and/or encrypt the secret data (e.g., in the simplest set-\nting using the ASCII coding map) into a secretcontaining bit string S. Second, we divide S into smaller bit blocks of length |B|, resulting in a total of |S|/|B|1 bit blocks. For example, if S = 100001 and |B| = 2, our bit-block sequence is 10, 00, 01. Based on this bit-block sequence, our steganographic LSTM generates words.\nKey. The sender and receiver share a key that maps bit blocks to token sets and is constructed as follows: We start from the vocabulary, which is the set of all possible tokens that may appear in the stegotext; the tokens are typically words, but may also be punctuation marks. We partition the vocabulary into 2|B| bins, i.e., disjoint token sets, randomly selected from the vocabulary without replacement; each token appears in exactly one bin, and each bin contains |V |/2|B| tokens. We map each bit block B to a bin, denoted by WB . This mapping constitutes the shared key.\nEmbedding algorithm. The embedding algorithm uses a modified word-level LSTM for language modeling (Mikolov et al., 2010). To encode the secret-containing bit string S, we consider one bit block B at a time and have our LSTM select one token from bin WB; hence, the candidate stegotext has as many tokens as the number of bit blocks in S. Even though we restrict the LSTM to select a token from a particular bin, each bin should offer sufficient variety of tokens, allowing the LSTM to generate text that looks natural. For example, given the bit string \u201c1000011011\u201d and the key in Table 1, the LSTM can form the partial sentence in Table 2. We describe our LSTM model in more detail in the next section.\n1If |B|6 | |S|, then we leave the remainder bit string out of encryption.\nDecoder. The decoder recovers the original data deterministically and in a straightforward manner: it takes as input the generated stegotext, considers one token at a time, finds the token\u2019s bin in the shared key, and recovers the original bit block.\nCommon-token variant. We also explore a variant where we add a set of common tokens, C, to all bins. These common tokens do not carry any secret information; they serve only to enhance stegotext naturalness. When the LSTM selects a common token from a bin, we have it select an extra token from the same bin, until it selects a noncommon token. The decoder removes all common tokens before decoding. We discuss the choice of common tokens and its implication on our system\u2019s performance in Section 4."}, {"heading": "3 Steganographic LSTM Model", "text": "In this section, we provide more details on our system: how we modify the LSTM (\u00a73.1) and how we evaluate its output (\u00a73.2)."}, {"heading": "3.1 LSTM Modification", "text": "Text generation in classic LSTM. Classic LSTMs generate words as follows (Sutskever et al., 2011): Given a word sequence (x1, x2, . . . , xT ), the model has hidden states (h1, . . . , hT ), and resulting output vectors (o1, . . . , oT ). Each output vector ot has length |V |, and each output-vector element o(j)t is the unnormalized probability of word j in the vocabulary. Normalized probabilities for each candidate word are obtained by the following softmax activation function:\nsoftmax(ot)j := exp(o (j) t ) /\u2211 k exp(o (k) t ).\nThe LSTM then selects the word with the highest probability P [xt+1 |x\u2264t] as its next word.\nText generation in our LSTM. In our steganographic LSTM, word selection is restricted by the shared key. That is, given bit block B, the LSTM has to select its next word from bin WB . We set P [x = wj ] = 0 for j /\u2208 WB , so that the multinomial softmax function selects the word with the highest probability within WB .\nCommon tokens. In the common-token variant, we restrict P [x = wj ] = 0 only for j /\u2208 (WB\u222aC), where C is the set of common tokens added to all bins."}, {"heading": "3.2 Evaluation Metrics", "text": "We use perplexity to quantify stegotext quality; and capacity (i.e., encrypted bits per output word) to quantify its efficiency in carrying secret information. In Section 4, we also discuss our stegotext quality as empirically perceived by us as human readers.\nPerplexity. Perplexity is a standard metric for the quality of language models (Martin and Jurafsky, 2000), and it is defined as the average per-word log-probability on the valid data set: exp(\u22121/N \u2211 i ln p[wi]) (Jozefowicz et al., 2016). Lower perplexity indicates a better model. In our steganographic LSTM, we cannot use this metric as is: since we enforce p[wi] = 0 for wi /\u2208WB , the corresponding ln p[wi] becomes undefined under this vocabulary.\nInstead, we measure the probability of wi by taking the average of p[wi] over all possible secret bit blocks B, under the assumption that bit blocks are distributed uniformly. By the Law of Large Numbers (Re\u0301ve\u0301sz, 2014), if we perform many stegotext-generating trials using different random secret data as input, the probability of each word will tend to the expected value, \u2211 p[wi, B]/2\n|B|, Hence, we set p[wi] := \u2211 p[wi, B]/2\n|B| instead of p[wi] = 0 for wi /\u2208WB .\nCapacity. Our system\u2019s capacity is the number of encrypted bits per output word. Without common tokens, capacity is always |B| bits/word (since each bit block of size |B| is always mapped to one output word). In the common-token variant, capacity decreases because the output includes common tokens that do not carry any secret information; in particular, if the fraction of common tokens is p, then capacity is (1\u2212 p) \u00b7 |B|."}, {"heading": "4 Experiments", "text": "In this section, we present our preliminary experimental evaluation: our Twitter and email datasets (\u00a74.1), details about the LSTMs used to produce our results (\u00a74.2), and finally a discussion of our results (\u00a74.3)."}, {"heading": "4.1 Datasets", "text": "Tweets and emails are among the most popular media of open communication and therefore provide very realistic environments for hiding information. We thus trained our LSTMs on those two domains, Twitter messages and Enron emails\n(Klimt and Yang, 2004), which vary greatly in message length and vocabulary size.\nFor Twitter, we used the NLTK tokenizer to tokenize tweets (Bird, 2006) into words and punctuation marks. We normalized the content by replacing usernames and URLs with a username token (<user>) and a URL token (<url>), respectively. We used 600 thousand tweets with a total of 45 million words and a vocabulary of size 225 thousand.\nFor Enron, we cleaned and extracted email message bodies (Zhou et al., 2007) from the Enron dataset, and we tokenized the messages into words and punctuation marks. We took the first 100MB of the resulting messages, with 16.8 million tokens and a vocabulary size of 406 thousand."}, {"heading": "4.2 Implementation Details", "text": "We implemented multi-layered LSTMs based on PyTorch2 in both experiments. We did not use pretrained word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and instead trained word embeddings of dimension 200 from scratch.\nWe optimized with Stochastic Gradient Descent and used a batch size of 20. The initial learning rate was 20 and the decay factor per epoch was 4. The learning rate decay occurred only when the validation loss did not improve. Model training was done on an NVIDIA GeForce GTX TITAN X.\nFor Twitter, we used a 2-layer LSTM with 600 units, unrolled for 25 steps for back propagation. We clipped the norm of the gradients (Pascanu et al., 2013) at 0.25 and applied 20% dropout (Srivastava et al., 2014). We stopped the training after 12 epochs (10 hours) based on validation loss convergence.\nFor Enron, we used a 3-layer LSTM with 600 units and no regularization. We unrolled the network for 20 steps for back propagation. We stopped the training after 6 epochs (2 days)."}, {"heading": "4.3 Results and Discussion", "text": ""}, {"heading": "4.3.1 Tweets", "text": "We evaluate resulting tweets generated by LSTMs of 1 (non-steganographic), 2, 4, 8 bins. Furthermore, we found empirically that adding 10 most frequent tokens from the Twitter corpus was enough to significantly improve the grammatical correctness and semantic reasonableness of\n2https://github.com/pytorch\ntweets. Table 3 shows the relationship between capacity (bits per word), and quantitative text quality (perplexity). It also compares models with and without adding common tokens using perplexity and bits per word.\nTable 4 shows example output texts of LSTMs with and without common tokens added. To reflect the variation in the quality of the tweets, we represent tweets that are good and poor in quality3.\nWe replaced <user> generated by the LSTM with mock usernames for a more realistic presentation in Table 4. In practice, we can replace the <user> tokens systematically, randomly selecting followers or followees of that tweet sender, for example.\nRe-tweet messages starting with \u201cRT\u201d can also be problematic, because it will be easy to check whether the original message of the retweeted message exists. A simple approach to deal with this is to eliminate \u201cRT\u201d messages from training (or at generation). Finally, since we made all tweets lower case in the pre-processing step, we can also post-process tweets to adhere to proper English capitalization rules."}, {"heading": "4.3.2 Emails", "text": "We also tested email generation, and Table 5 shows sample email passages4 from each bin. We post-processed the emails with untokenization of punctuations.\nThe biggest difference between emails and tweets is that emails have a much longer range for\n3For each category, we manually evaluate 60 randomly generated tweets based grammatical correctness, semantic coherence, and resemblance to real tweets. We select tweets from the 25th, 50th, and 75th percentile, and call them \u201cgood\u201d, \u201caverage\u201d, and \u201cpoor\u201d respectively. We limit to tweets that are not offensive in language.\n4We only present passages \u201daverage\u201d in quality to conserve space.\ncontext dependency, with context spanning sentences and paragraphs. This is challenging to model even for the non-steganographic LSTM. Once the long-range context dependency of the non-steganographic LSTM improves, the context dependency of the steganographic LSTMs should also improve."}, {"heading": "4.4 Comparison with Other Stegosystems", "text": "For all comparisons, we use our 4-bin model with no common tokens added.\nOur model significantly improves the stateof-the-art capacity. Cover modification based stegosystems hide 1-2 bits per sentence (Chang and Clark, 2012). The state-of-the-art Twitter stegosystem hides 2.8 bits of per tweet (Wil-\nson and Ker, 2016). Assuming 16.04 words per tweet5, our 4-bin system hides 32 bits per tweet, over 11 times higher than (Wilson and Ker, 2016).\nWe hypothesize that the subjective quality of our generated tweets will be comparable to tweets produced by CoverTweet (2014). We present some examples6 in Table 6 to show there is potential for a comparison. This contrasts the previous conception that cover generation methods are fatally weak against human judges (Wilson et al., 2014). CoverTweet was tested to be secure against human judges. Formal experiments will be necessary to establish that our system is also secure against human judges.\nOur system also offers flexibility for the user to freely trade-off capacity and text quality. Though we chose the 4-bin model with no common tokens for comparison, user can choose to use more bins\n5Based a random sample of 2 million tweets. 6Tweets selected for comparison are \u201caverage\u201d in quality.\nto achieve an even higher capacity, or use less bins and add common tokens to increase text quality. This is not the case with existing cover modification systems, where capacity is bounded above by the number of transformation options (Wilson et al., 2014)."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we opened a new application of LSTMs, namely, steganographic text generation. We presented our steganographic model based on existing language modeling LSTMs, and demonstrated that our model produces realistic tweets and emails while hiding information.\nIn comparison to the state-of-the-art steganographic systems, our system has the advantage of encoding much more information (around 2 bits per word). This advantage makes the system more usable and scalable in practice.\nIn future work, we will formally evaluate our system\u2019s security against human judges and other steganography detection (steganalysis) methods (Wilson et al., 2015; Kodovsky et al., 2012). When evaluated against an automated classifier, the setup becomes that of a Generative Adversarial Network (Goodfellow et al., 2014), though with additional conditions for the generator (the secret bits) which are unknown to the discriminator, and not necessarily employing joint training. Another line of future research is to generate tweets which are personalized to a user type or interest group, instead of reflecting all twitter users. Furthermore, we plan to explore if capacity can be improved even more by using probabilistic encoders/decoders, as e.g. in Matryoshka (Safaka et al., 2016, Section 4).\nUltimately, we aim to open-source our stegosystem so that users of open communication systems (e.g. Twitter, emails) can use our stegosystem to communicate private and sensitive information."}], "references": [{"title": "Nltk: the natural language toolkit", "author": ["Steven Bird."], "venue": "Proceedings of the COLING/ACL on Interactive presentation sessions. Association for Computational Linguistics, pages 69\u201372.", "citeRegEx": "Bird.,? 2006", "shortCiteRegEx": "Bird.", "year": 2006}, {"title": "Linguistic steganography using automatically generated paraphrases", "author": ["Ching-Yun Chang", "Stephen Clark."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Chang and Clark.,? 2010", "shortCiteRegEx": "Chang and Clark.", "year": 2010}, {"title": "Adjective deletion for linguistic steganography and secret sharing", "author": ["Ching-Yun Chang", "Stephen Clark."], "venue": "COLING. pages 493\u2013510.", "citeRegEx": "Chang and Clark.,? 2012", "shortCiteRegEx": "Chang and Clark.", "year": 2012}, {"title": "Practical linguistic steganography using contextual synonym substitution and a novel vertex coding method", "author": ["Ching-Yun Chang", "Stephen Clark."], "venue": "Computational Linguistics 40(2):403\u2013448.", "citeRegEx": "Chang and Clark.,? 2014", "shortCiteRegEx": "Chang and Clark.", "year": 2014}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."], "venue": "Advances in neural information processing systems. pages 2672\u20132680.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Free from the cover text: a human-generated natural language approach to text-based steganography", "author": ["Michael Grosvald", "C Orhan Orgun."], "venue": "Journal of Information Hiding and Multimedia Signal Processing 2(2):133\u2013141.", "citeRegEx": "Grosvald and Orgun.,? 2011", "shortCiteRegEx": "Grosvald and Orgun.", "year": 2011}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "arXiv preprint arXiv:1602.02410 .", "citeRegEx": "Jozefowicz et al\\.,? 2016", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "The enron corpus: A new dataset for email classification research", "author": ["Bryan Klimt", "Yiming Yang."], "venue": "European Conference on Machine Learning. Springer, pages 217\u2013226.", "citeRegEx": "Klimt and Yang.,? 2004", "shortCiteRegEx": "Klimt and Yang.", "year": 2004}, {"title": "Ensemble classifiers for steganalysis of digital media", "author": ["Jan Kodovsky", "Jessica Fridrich", "Vojt\u011bch Holub."], "venue": "IEEE Transactions on Information Forensics and Security 7(2):432\u2013444.", "citeRegEx": "Kodovsky et al\\.,? 2012", "shortCiteRegEx": "Kodovsky et al\\.", "year": 2012}, {"title": "Speech and language processing", "author": ["James H Martin", "Daniel Jurafsky."], "venue": "International Edition 710.", "citeRegEx": "Martin and Jurafsky.,? 2000", "shortCiteRegEx": "Martin and Jurafsky.", "year": 2000}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Interspeech. volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "ICML (3) 28:1310\u20131318.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP. volume 14, pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "The laws of large numbers, volume 4", "author": ["P\u00e1l R\u00e9v\u00e9sz."], "venue": "Academic Press.", "citeRegEx": "R\u00e9v\u00e9sz.,? 2014", "shortCiteRegEx": "R\u00e9v\u00e9sz.", "year": 2014}, {"title": "Matryoshka: Hiding secret communication in plain sight", "author": ["Iris Safaka", "Christina Fragouli", "Katerina Argyraki."], "venue": "6th USENIX Workshop on Free and Open Communications on the Internet (FOCI 16). USENIX Association.", "citeRegEx": "Safaka et al\\.,? 2016", "shortCiteRegEx": "Safaka et al\\.", "year": 2016}, {"title": "Data privacy and security: encryption and information hiding", "author": ["David Salomon."], "venue": "Springer Science & Business Media.", "citeRegEx": "Salomon.,? 2003", "shortCiteRegEx": "Salomon.", "year": 2003}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton."], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11). pages 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions", "author": ["Umut Topkara", "Mercan Topkara", "Mikhail J Atallah."], "venue": "Proceedings of the 8th workshop on Multimedia and security. ACM,", "citeRegEx": "Topkara et al\\.,? 2006", "shortCiteRegEx": "Topkara et al\\.", "year": 2006}, {"title": "Steganographic generative adversarial networks", "author": ["Denis Volkhonskiy", "Ivan Nazarov", "Boris Borisenko", "Evgeny Burnaev."], "venue": "arXiv preprint arXiv:1703.05502 .", "citeRegEx": "Volkhonskiy et al\\.,? 2017", "shortCiteRegEx": "Volkhonskiy et al\\.", "year": 2017}, {"title": "Detection of steganographic techniques on twitter", "author": ["Alex Wilson", "Phil Blunsom", "Andrew Ker."], "venue": "EMNLP. pages 2564\u20132569.", "citeRegEx": "Wilson et al\\.,? 2015", "shortCiteRegEx": "Wilson et al\\.", "year": 2015}, {"title": "Linguistic steganography on twitter: hierarchical language modeling with manual interaction", "author": ["Alex Wilson", "Phil Blunsom", "Andrew D Ker."], "venue": "IS&T/SPIE Electronic Imaging. International Society for Optics and Photonics, pages", "citeRegEx": "Wilson et al\\.,? 2014", "shortCiteRegEx": "Wilson et al\\.", "year": 2014}, {"title": "Avoiding detection on twitter: embedding strategies for linguistic steganography", "author": ["Alex Wilson", "Andrew D Ker."], "venue": "Electronic Imaging 2016(8):1\u20139.", "citeRegEx": "Wilson and Ker.,? 2016", "shortCiteRegEx": "Wilson and Ker.", "year": 2016}, {"title": "Strategies for cleaning organizational emails with an application to enron email dataset", "author": ["Yingjie Zhou", "Mark Goldberg", "Malik Magdon-Ismail", "Al Wallace."], "venue": "5th Conf. of North American Association for Computational Social and Organizational", "citeRegEx": "Zhou et al\\.,? 2007", "shortCiteRegEx": "Zhou et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 20, "context": ", using synonym substitution (Topkara et al., 2006; Chang and Clark, 2014) and/or paraphrase substitution (Chang and Clark, 2010).", "startOffset": 29, "endOffset": 74}, {"referenceID": 3, "context": ", using synonym substitution (Topkara et al., 2006; Chang and Clark, 2014) and/or paraphrase substitution (Chang and Clark, 2010).", "startOffset": 29, "endOffset": 74}, {"referenceID": 1, "context": ", 2006; Chang and Clark, 2014) and/or paraphrase substitution (Chang and Clark, 2010).", "startOffset": 62, "endOffset": 85}, {"referenceID": 23, "context": "Of these systems, the most closely related to ours is CoverTweet (Wilson et al., 2014), a state-of-theart cover modification stegosystem that uses Twitter as the medium of cover; we compare to it in our preliminary evaluation (\u00a74).", "startOffset": 65, "endOffset": 86}, {"referenceID": 5, "context": "Cover modification can introduce syntactic and semantic unnaturalness (Grosvald and Orgun, 2011); to address this, Grovsald and Orgun proposed an alternative stegosystem where a human generates the stegotext manually, thus improving linguistic naturalness at the cost of human effort (Grosvald and Orgun, 2011).", "startOffset": 70, "endOffset": 96}, {"referenceID": 5, "context": "Cover modification can introduce syntactic and semantic unnaturalness (Grosvald and Orgun, 2011); to address this, Grovsald and Orgun proposed an alternative stegosystem where a human generates the stegotext manually, thus improving linguistic naturalness at the cost of human effort (Grosvald and Orgun, 2011).", "startOffset": 284, "endOffset": 310}, {"referenceID": 16, "context": "Matryoshka (Safaka et al., 2016) takes this further: in step 1, it generates candidate stegotext automatically based on an n-gram model of the English language; in step 2, it presents the candidate stegotext to the human user for polishing, i.", "startOffset": 11, "endOffset": 32}, {"referenceID": 4, "context": "have applied Generative Adversarial Networks (Goodfellow et al., 2014) to image steganography (Volkhonskiy et al.", "startOffset": 45, "endOffset": 70}, {"referenceID": 21, "context": ", 2014) to image steganography (Volkhonskiy et al., 2017), but we are not aware of any text stegosystem based on neural networks.", "startOffset": 31, "endOffset": 57}, {"referenceID": 6, "context": "Motivated by the fact that LSTMs (Hochreiter and Schmidhuber, 1997) constitute the state of the art in text generation (Jozefowicz et al.", "startOffset": 33, "endOffset": 67}, {"referenceID": 7, "context": "Motivated by the fact that LSTMs (Hochreiter and Schmidhuber, 1997) constitute the state of the art in text generation (Jozefowicz et al., 2016), we propose to automatically generate the stegotext from an LSTM (as opposed to an n-gram model).", "startOffset": 119, "endOffset": 144}, {"referenceID": 17, "context": "1 outlines the building blocks of a stegosystem (Salomon, 2003).", "startOffset": 48, "endOffset": 63}, {"referenceID": 11, "context": "The embedding algorithm uses a modified word-level LSTM for language modeling (Mikolov et al., 2010).", "startOffset": 78, "endOffset": 100}, {"referenceID": 19, "context": "Classic LSTMs generate words as follows (Sutskever et al., 2011): Given a word sequence (x1, x2, .", "startOffset": 40, "endOffset": 64}, {"referenceID": 10, "context": "Perplexity is a standard metric for the quality of language models (Martin and Jurafsky, 2000), and it is defined as the average per-word log-probability on the valid data set: exp(\u22121/N \u2211 i ln p[wi]) (Jozefowicz et al.", "startOffset": 67, "endOffset": 94}, {"referenceID": 7, "context": "Perplexity is a standard metric for the quality of language models (Martin and Jurafsky, 2000), and it is defined as the average per-word log-probability on the valid data set: exp(\u22121/N \u2211 i ln p[wi]) (Jozefowicz et al., 2016).", "startOffset": 200, "endOffset": 225}, {"referenceID": 15, "context": "By the Law of Large Numbers (R\u00e9v\u00e9sz, 2014), if we perform many stegotext-generating trials using different random secret data as input, the probability of each word will tend to the expected value, \u2211 p[wi, B]/2 |B|, Hence, we set p[wi] := \u2211 p[wi, B]/2 |B| instead of p[wi] = 0 for wi / \u2208WB .", "startOffset": 28, "endOffset": 42}, {"referenceID": 8, "context": "(Klimt and Yang, 2004), which vary greatly in message length and vocabulary size.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "For Twitter, we used the NLTK tokenizer to tokenize tweets (Bird, 2006) into words and punctuation marks.", "startOffset": 59, "endOffset": 71}, {"referenceID": 25, "context": "For Enron, we cleaned and extracted email message bodies (Zhou et al., 2007) from the Enron dataset, and we tokenized the messages into words and punctuation marks.", "startOffset": 57, "endOffset": 76}, {"referenceID": 12, "context": "We did not use pretrained word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and instead trained word embeddings of dimension 200 from scratch.", "startOffset": 42, "endOffset": 89}, {"referenceID": 14, "context": "We did not use pretrained word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and instead trained word embeddings of dimension 200 from scratch.", "startOffset": 42, "endOffset": 89}, {"referenceID": 13, "context": "We clipped the norm of the gradients (Pascanu et al., 2013) at 0.", "startOffset": 37, "endOffset": 59}, {"referenceID": 18, "context": "25 and applied 20% dropout (Srivastava et al., 2014).", "startOffset": 27, "endOffset": 52}, {"referenceID": 2, "context": "Cover modification based stegosystems hide 1-2 bits per sentence (Chang and Clark, 2012).", "startOffset": 65, "endOffset": 88}, {"referenceID": 24, "context": "8 bits of per tweet (Wilson and Ker, 2016).", "startOffset": 20, "endOffset": 42}, {"referenceID": 24, "context": "04 words per tweet5, our 4-bin system hides 32 bits per tweet, over 11 times higher than (Wilson and Ker, 2016).", "startOffset": 89, "endOffset": 111}, {"referenceID": 23, "context": "This contrasts the previous conception that cover generation methods are fatally weak against human judges (Wilson et al., 2014).", "startOffset": 107, "endOffset": 128}, {"referenceID": 1, "context": "Cover modification based stegosystems hide 1-2 bits per sentence (Chang and Clark, 2012). The state-of-the-art Twitter stegosystem hides 2.8 bits of per tweet (Wilson and Ker, 2016). Assuming 16.04 words per tweet5, our 4-bin system hides 32 bits per tweet, over 11 times higher than (Wilson and Ker, 2016). We hypothesize that the subjective quality of our generated tweets will be comparable to tweets produced by CoverTweet (2014). We present some examples6 in Table 6 to show there is potential for a comparison.", "startOffset": 66, "endOffset": 434}, {"referenceID": 23, "context": "This is not the case with existing cover modification systems, where capacity is bounded above by the number of transformation options (Wilson et al., 2014).", "startOffset": 135, "endOffset": 156}, {"referenceID": 22, "context": "In future work, we will formally evaluate our system\u2019s security against human judges and other steganography detection (steganalysis) methods (Wilson et al., 2015; Kodovsky et al., 2012).", "startOffset": 142, "endOffset": 186}, {"referenceID": 9, "context": "In future work, we will formally evaluate our system\u2019s security against human judges and other steganography detection (steganalysis) methods (Wilson et al., 2015; Kodovsky et al., 2012).", "startOffset": 142, "endOffset": 186}, {"referenceID": 4, "context": "When evaluated against an automated classifier, the setup becomes that of a Generative Adversarial Network (Goodfellow et al., 2014), though with additional conditions for the generator (the secret bits) which are unknown to the discriminator, and not necessarily employing joint training.", "startOffset": 107, "endOffset": 132}], "year": 2017, "abstractText": "Motivated by concerns for user privacy, we design a steganographic system (\u201cstegosystem\u201d) that enables two users to exchange encrypted messages without an adversary detecting that such an exchange is taking place. We propose a new linguistic stegosystem based on a Long ShortTerm Memory (LSTM) neural network. We demonstrate our approach on the Twitter and Enron email datasets and show that it yields high-quality steganographic text while significantly improving capacity (encrypted bits per word) relative to the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}