{"id": "1705.10834", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Experience Replay Using Transition Sequences", "abstract": "Experience sakalava replay is netchannel one petticoats of qassab the most zeez commonly akkar used untoasted approaches opportunties to groene improve the ottawa sample efficiency 60e of mwintot reinforcement learning 2,238 algorithms. birstein In schuch this 2200 work, nucleophile we 10.68 propose ground-breaking an approach to big-game select asterisms and replay sequences avmark of shorenstein transitions in yuanpei order to alexx accelerate the oschatz learning torreblanca of freely a reinforcement stabbing learning agent wente in garsson an mazagon off - redundantly policy kemmons setting. wlaj In caecum addition ccx to 316th selecting 1.0-1 appropriate sequences, we 380th also trenchant artificially connecting construct transition eoe sequences using 1407 information gathered ludewig from previous agent - ahaz environment tolga interactions. ryan@globe.com These sequences, pampered when replayed, ogoniland allow value function information r.m.s. to trickle down to larger sections of the pharoahs state / igoeti state - action hedline space, thereby schloendorff making the flask-shaped most of kenoly the vyse agent ' s experience. winelands We karta demonstrate knip our inskip approach on skedsmo modified convincingly versions of standard squadrone reinforcement learning principe tasks such as objectified the ktvx mountain car borgenicht and puddle world problems and empirically genyen show that katich it vicioso enables better gup learning sidhom of oughtibridge value functions miscik as compared to dzon other forms of experience replay. Further, sandwich we corneas briefly discuss clervi some 107.65 of allophonic the possible extensions redevelop to gildeyev this work, as nolidae well as carola applications congealing and situations haradh where rokkaku this approach could be http://www.navy.mil particularly useful.", "histories": [["v1", "Tue, 30 May 2017 19:24:09 GMT  (904kb)", "http://arxiv.org/abs/1705.10834v1", "23 pages, 6 figures, Submitted to the journal Artificial Intelligence"]], "COMMENTS": "23 pages, 6 figures, Submitted to the journal Artificial Intelligence", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["thommen george karimpanal", "roland bouffanais"], "accepted": false, "id": "1705.10834"}, "pdf": {"name": "1705.10834.pdf", "metadata": {"source": "CRF", "title": "Experience Replay Using Transition Sequences", "authors": ["Thommen George Karimpanal", "Roland Bouffanais"], "emails": ["thommen_george@mymail.sutd.edu.sg,", "bouffanais@sutd.edu.sg"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n10 83\n4v 1\n[ cs\n.A I]\n3 0\nM ay\nExperience replay is one of the most commonly used approaches to improve the sample efficiency of reinforcement learning algorithms. In this work, we propose an approach to select and replay sequences of transitions in order to accelerate the learning of a reinforcement learning agent in an off-policy setting. In addition to selecting appropriate sequences, we also artificially construct transition sequences using information gathered from previous agentenvironment interactions. These sequences, when replayed, allow value function information to trickle down to larger sections of the state/state-action space, thereby making the most of the agent\u2019s experience. We demonstrate our approach on modified versions of standard reinforcement learning tasks such as the mountain car and puddle world problems and empirically show that it enables better learning of value functions as compared to other forms of experience replay. Further, we briefly discuss some of the possible extensions to this work, as well as applications and situations where this approach could be particularly useful.\nKeywords: Experience Replay, Q-learning, Off-Policy, Multi-task Reinforcement Learning, Probabilistic Policy Reuse"}, {"heading": "1. Introduction", "text": "Real-world artificial agents ideally need to be able to learn as much as possible from their interactions with the environment. This is especially true for mobile robots operating within the reinforcement learning (RL) framework, where the cost of acquiring information from the environment through\nEmail address: thommen_george@mymail.sutd.edu.sg, bouffanais@sutd.edu.sg\n(Thommen George Karimpanal, Roland Bouffanais)\nPreprint submitted to Artificial Intelligence June 1, 2017\nexploration generally exceeds the computational cost of learning [1\u20134]. Experience replay [5] is a technique that reuses information gathered from past experiences to improve the efficiency of learning. In order to replay stored experiences using this approach, an off-policy [6, 7] setting is a prerequisite. In off-policy learning, the policy that dictates the agent\u2019s current actions is referred to as the behavior policy. Other policies\u2014possibly corresponding to other tasks\u2014that the agent aims to learn using actions arising from the behavior policy are referred to as target policies. Off-policy algorithms utilize the agent\u2019s behavior policy to interact with the environment, while simultaneously updating the value functions associated with the target policies. These algorithms can hence be used to parallelize learning, and, thus gather as much knowledge as possible using real experiences [8\u201310]. However, when the behavior and target policies differ considerably from each other, the actions executed by the behavior policy may only seldom correspond to those of the target policy. This could lead to poor estimates of the corresponding value function. In such cases, and generally, in environments where desirable experiences are rare occurrences, experience replay could be employed to improve the estimates by storing and replaying transitions (state, actions and rewards) from time to time.\nAlthough most experience replay approaches store and reuse individual transitions, replaying sequences of transitions may have certain advantages. For instance, if a value function update following a particular transition results in a relatively large change in the value of the corresponding state or state-action pair, this change will have a considerable influence on the bootstrapping targets of states or state-action pairs that led to this transition. Hence, the effects of this change should ideally be propagated to these states or state-action pairs. If instead of individual transitions, sequences of transitions are replayed, this propagation can be achieved in a straightforward manner. Our approach aims to improve the efficiency of learning by replaying transition sequences in this manner. The sequences are selected on the basis of the magnitudes of the temporal difference (TD) errors associated with them. We hypothesize that selecting sequences that contain transitions associated with higher magnitudes of TD errors allow considerable learning progress to take place. This is enabled by the propagation of the effects of these errors to the values associated with other states or state-action pairs in the transition sequence.\nReplaying a greater variety of such sequences would result in better propagation of the mentioned effects to other regions in the state/state-action\nspace. Hence, in order to aid the propagation in this manner, other sequences that could have occurred are artificially constructed by comparing the state trajectories of previously observed sequences. These virtual transition sequences are appended to the replay memory, and they help bring about learning progress in other regions of the state/state-action space when replayed. The generated transition sequences are virtual in the sense that\nthey may have never occurred in reality, but are constructed from sequences that have actually occurred in the past. The additional replay updates corresponding to the mentioned transition sequences supplement the regular offpolicy value function updates that follow the real-world execution of actions, thereby making the most out of the agent\u2019s interactions with the environment."}, {"heading": "2. Background", "text": "The problem of learning from limited experience is not new in the field of RL [11, 12]. Generally, learning speed and sample efficiency are critical\nfactors that determine the feasibility of deploying learning algorithms in the real world. Particularly for robotics applications, these factors are even more important, as exploration of the environment is typically time and energy expensive [13, 14]. It is thus important for a learning agent to be able to gather as much relevant knowledge as possible from whatever exploratory actions occur.\nOff-policy algorithms are well suited to this need as it enables multiple value functions to be learned together in parallel. When the behavior and target policies vary considerably from each other, importance sampling [6, 15] is commonly used in order to obtain better estimates of the value functions. Importance sampling reduces the variance of the estimate by taking into account the distributions associated with the behavior and target policies, and making modifications to the off-policy update equations accordingly. However, the estimates are still unlikely to be close to their optimal values if the agent receives very little experience relevant to a particular task.\nExperience replay can be a useful approach to deal with such situations where favorable experiences occur rarely. Here, a replay memory is maintained, which consists of previously experienced states and actions, and the corresponding subsequent states and rewards observed following the execution of those actions. The information contained in the replay memory is used from time to time in order to update the value functions, thereby making better use of the agent-environment interactions. This approach has received a lot of attention in recent years due to its utility in deep RL applications [2, 16\u201319].\nIt is known that for experience replay, certain transitions may be more useful than others. Recent works [3, 20] have explored different ways in which transitions may be prioritized. Schaul et al. [3] prioritized transitions on the basis of their associated TD errors. They also briefly mentioned the possibility of replaying transitions in a sequential manner. The experience replay framework developed by Adam et al. [2] involved some variants that replayed sequences of experiences, but these sequences were drawn randomly from the replay memory.\nAs in Schaul et al. [3], TD errors have been frequently used as a basis for prioritization in other RL problems [3, 21, 22]. In particular, the modelbased approach of prioritized sweeping [23, 24] prioritizes backups that are expected to result in a significant change in the value function.\nThe algorithm we propose here is model-free, and it is based on the idea of selectively reusing previous experience. However, we describe the reuse of se-\nquences of transitions based on the TD errors observed when these transitions take place. Replaying sequences of experiences also seems to be biologically plausible [25, 26]. In addition, it is known that animals tend to remember experiences that lead to high rewards [27]. This is an idea reflected in our work, as only those transition sequences that lead to high rewards are considered for being stored in the replay memory. In filtering transition sequences in this manner, we simultaneously address the issue of determining which experiences are to be stored.\nIn addition to selecting transition sequences, we also generate virtual sequences of transitions which the agent could have possibly experienced, but in reality, did not. This virtual experience is then replayed to improve the agent\u2019s learning. Some early approaches in RL, such as the dyna architecture [28] also made use of simulated experience to improve the value function estimates. However, unlike the approach proposed here, the simulated experience was generated based on models of the reward function and transition probabilities which were continuously updated based on the agent\u2019s interactions with the environment. In this sense, the virtual experience generated in our approach is more grounded in reality, as it is based directly on the data collected through the agent-environment interaction. Our approach also recognizes the real-world limitations of replay memory [19], and stores only a certain amount of information at a time, specified by memory parameters. The selected and generated sequences are stored in the replay memory in the form of libraries which are continuously updated so that the agent is equipped with transition sequences that are most relevant to the task at hand."}, {"heading": "3. Methodology", "text": "RL agents are designed to learn from experiences gathered by interacting with their environment. However, when certain critical experiences occur rarely, their effect on the agent\u2019s learning is limited, and this adversely affects the agent\u2019s value function estimates. For example, in the context of off-policy learning, value functions may converge poorly if the target and behavior policies are vastly different from one another. The approach proposed here improves the learning of value functions in such cases by selectively replaying sequences of experiences. Similar to ordinary experience replay, this approach requires additional memory and computation. However, this can be justified for real-world robotics applications where the time and energy costs of exploration typically outweigh the cost of memory storage and the\naccompanying increase in computational requirements (e.g. [29]). We describe our approach in the context of learning target policies as mentioned above, in a non-ideal situation where the behavior and target policies are substantially different.\nThe idea of selecting appropriate transition sequences for replay is relatively straightforward. In order to improve the agent\u2019s learning, first, we simply keep track of the state, actions, rewards and TD errors associated with each transition. Generally, high rewards occur rarely, so when such an event is observed, we consider storing the corresponding sequence of transitions into a replay library L. In this manner, we use the reward information as a means to filter transition sequences. The approach is similar to that used by Narasimhan et al. [20], where transitions associated with positive rewards are prioritized for replay.\nAmong the transition sequences considered for inclusion in the library L, those containing transitions with high absolute TD error values are considered to be the ones with high potential for learning progress. Hence, they are accordingly prioritized for replay. The key idea is that when the TD error associated with a particular transition is large in magnitude, it generally implies a proportionately greater change in the value of the corresponding state/state-action pair. Such large changes have the potential to influence the values of the states/state-action pairs leading to it, which implies a high potential for learning. Hence, prioritizing such sequences of transitions for replay is likely to bring about greater learning progress. Transition sequences associated with large magnitudes of TD error are retained in the library, while those with lower magnitudes are removed and replaced with better alternatives. In reality, such transition sequences may be very long and hence, impractical to store. Due to such practical considerations, we store only a portion of the sequence, based on a predetermined memory parameter. The library is continuously updated as and when the agent-environment interaction takes place, such that it will eventually contain sequences associated with the highest absolute TD errors.\nAs described earlier, replaying suitable sequences allows the effects of large changes in value functions to be propagated throughout the sequence. In order to propagate this information even further to other regions of the state/state-action space, we use the sequences in L to construct additional transition sequences which could have possibly occurred. These virtual sequences are stored in another library Lv, and later used for experience replay.\nIn order to intuitively describe our approach of artificially constructing sequences, we consider the hypothetical example shown in Figure 2a, where an agent executes behavior policies that help it learn to navigate towards location B from the start location. However, using off-policy learning, we aim to learn value functions corresponding to the policy that helps the agent navigate towards location T .\nThe trajectories shown in Figure 2a correspond to hypothetical actions dictated by the behavior policy midway through the learning process, during two separate episodes. The trajectories begin at the start location and terminate at location B. However, the trajectory corresponding to behavior policy 1 also happens to pass through location T , at which point the agent receives a high reward. This triggers the transition sequence storage mechanism described earlier, and we assume that some portion of the sequence (shown by the highlighted portion of the trajectory in Figure 2a) is stored in library L. Behavior policy 2 takes the agent directly from the start location towards the location B, where it terminates. As the agent moves along its trajectory, it intersects with the state trajectory corresponding to the sequence stored in L. Using this intersection, it is possible to artificially construct additional trajectories (and their associated transition sequences) that are successful with respect to the task of navigating to location T . The highlighted portions of the trajectories corresponding to the two behavior\npolicies in Figure 2b show such a state trajectory, constructed using information related to the intersection of portions of the two previously observed trajectories. The state, action and reward sequences associated with this highlighted trajectory form a virtual transition sequence.\nSuch artificially constructed transition sequences present the possibility of considerable learning progress. This is because, when replayed, they help propagate the large learning potential (characterized by large magnitudes of TD errors) associated with sequences in L to other regions of the state/stateaction space. These replay updates supplement the off-policy value function updates that are carried out in parallel, thus accelerating the learning of the task in question. This outlines the basic idea behind our approach.\nFundamentally, our approach can be decomposed into three steps:\n1. Tracking and storage of relevant transition sequences 2. Construction of virtual transition sequences using the stored transition\nsequences 3. Replaying the transition sequences\nThese steps are explained in detail in Sections 3.1, 3.2 and 3.3."}, {"heading": "3.1. Tracking and Storage of Relevant Transition Sequences", "text": "As described, virtual transition sequences are constructed by joining together two transition sequences. One of them, say \u0398t, composed of mt transitions, is historically successful\u2014it has experienced high rewards\u2014with respect to the task, and is part of the library L. The other sequence, \u0398b, is simply a sequence of the latest mb transitions executed by the agent.\nIf the agent starts at state s0 and moves through intermediate states si and eventually to sj (most recent state) by executing a series of actions a0...ai...aj , it receives rewards rt0...rti...rtj from the environment. These transitions comprise the transition sequence \u0398b.\n\u0398b =\n{\n{Sj0 \u03c0 j 0 Rt j 0} if j \u2264 mb {Sjj\u2212mb \u03c0 j j\u2212mb Rt j j\u2212mb } otherwise\n(1)\nwhere:\nSyx = {sx...si...sy} T , \u03c0yx = {ax...ai...ay} T , Rt y x = {rtx...rti...rty} T .\nWe respectively refer to Syx , \u03c0 y x and Rt y x as the state, action and reward transition sequences between states x and y. For the case of the transition sequence \u0398t, we keep track of the sequence of TD errors \u03b40...\u03b4i...\u03b4k observed as well. If a high reward is observed in transition k, then:\n\u0398t =\n{\n{Sk0 \u03c0 k 0 Rt k 0 \u2206 k 0} if k \u2264 mt {Skk\u2212mt \u03c0 k k\u2212mt Rt k k\u2212mt \u2206kk\u2212mt} otherwise (2)\nwhere \u2206yx = {\u03b4x...\u03b4i...\u03b4y} T .\nThe memory parameters mb and mt are chosen based on the memory constraints of the agent. They determine how much of the recent agentenvironment interaction history is to be stored in memory.\nIt is possible that the agent encounters a number of transitions associated with high rewards while executing the behavior policy. Accordingly, a number of such successful transition sequences \u0398t may also exist. These sequences are maintained in the library L in a manner similar to the Policy Library through Policy Reuse (PLPR) algorithm [30]. To decide whether to include a new transition sequence \u0398tnew into the library L, we determine the maximum absolute value of the TD error sequence \u2206 corresponding to \u0398tnew and check whether it is \u03c4 -close\u2014the parameter \u03c4 determines the exclusivity of the library\u2014to the maximum of the corresponding values associated with the transition sequences in L. If this is the case, then \u0398tnew is included in L. Using the absolute TD error as a basis for selection, we maintain a fixed number (l) of transition sequences in the library L. This ensures that the library is continuously updated with the latest transition sequences associated with the highest absolute TD errors. The complete algorithm is illustrated in Algorithm 1."}, {"heading": "3.2. Virtual Transition Sequences", "text": "Once the transition sequence \u0398b is available and a library L of successful transition sequences \u0398t is obtained, we use this information to construct a library Lv of virtual transition sequences \u0398v. The virtual transition sequences are constructed by first finding points of intersection sc in the state transition sequences of \u0398b and the \u0398t\u2019s in L.\nLet us consider the transition sequence \u0398b:\n\u0398b = {S y x \u03c0 y x Rt y x},\nAlgorithm 1 Maintaining a replay library of transition sequences\n1: Inputs: \u03c4 : Parameter that determines the exclusivity of the library l : Parameter that determines the number of transition sequences allowed in the library \u2206k : Sequence of TD errors corresponding to a transition sequence \u0398k L = {\u0398t0...\u0398ti...\u0398tm} : A library of transition sequences (m \u2264 l) \u0398tnew : New transition sequence to be evaluated 2: Wnew = max(|\u2206tnew|) 3: for i = 1 : m do 4: Wi = max(|\u2206ti|) 5: end for 6: if Wnew \u2217 \u03c4 > max(W ) then 7: L = L \u222a {\u0398tnew} 8: nt =Number of transition sequences in L 9: if nt > l then\n10: L = {\u0398tnt\u2212l...\u0398ti...\u0398tnt} 11: end if\n12: end if\nand a transition sequence \u0398t:\n\u0398t = {S y\u2032 x\u2032 \u03c0 y\u2032 x\u2032 Rt y\u2032 x\u2032 \u2206 y\u2032 x\u2032},\nLet \u0398s be a subset of \u0398t such that:\n\u0398s = {S y\u2032 x\u2032 \u03c0 y\u2032 x\u2032 Rt y\u2032 x\u2032},\nIf \u2203sc \u2208 {S y x \u2229 S\ny\u2032 x\u2032}, then:\nSyx T = {sx, sx+1...sc} \u222a {sc+1, sc+2...sy},\nand\nS y\u2032\nx\u2032\nT = {sx\u2032, sx\u2032+1...sc} \u222a {sc+1, sc+2...sy\u2032}.\nOnce points of intersection have been obtained as described above, each of the two sequences \u0398b and \u0398t are decomposed into two subsequences at the\npoint of intersection such that:\n\u0398b = \u0398 1 b \u222a\u0398 2 b , (3)\nwhere \u03981b = {S c x \u03c0 c x Rt c x} and \u0398 2 b = {S y c+1 \u03c0 y c+1 Rt y c+1}, and \u0398s = \u0398 1 s \u222a\u0398 2 s, (4)\nwhere \u03981s = {S c x\u2032 \u03c0 c x\u2032 Rt c x\u2032} and \u0398 2 s = {S\ny\u2032 c+1 \u03c0 y\u2032 c+1 Rt y\u2032\nc+1}. The virtual transition sequence is then simply:\n\u0398v = \u0398 1 b \u222a\u0398 2 s. (5)\nWe perform the above procedure for each transition sequence in L to obtain the corresponding virtual transition sequences \u0398v. These virtual transition sequences are stored in a library Lv:\nLv = {\u0398v1...\u0398vi...\u0398vnv},\nwhere nv denotes the number of virtual transition sequences in Lv, subjected to the constraint nv \u2264 l.\nOnce the library Lv has been constructed, we replay the sequences contained in it to improve the estimates of the value function. The details of this are discussed in Section 3.3."}, {"heading": "3.3. Replaying the Transition Sequences", "text": "In order to make use of the transition sequences described, each of the state-action-reward triads {s a r} in the transition sequence Lv is replayed as if the agent had actually experienced them. To achieve a quicker propagation of information, the sequences are replayed in the reverse temporal order in which they were observed, as shown in Algorithm 2. Such an idea has been briefly mentioned in previous studies on experience replay [2, 3]. In addition, a similar mechanism of reverse replay has been observed in animals [31].\nSimilarly, sequences in L may also be replayed from time to time. Replaying sequences from L and Lv in this manner causes the effects of large absolute TD errors originating from further up in the sequence to propagate through the respective transitions, ultimately leading to better estimates of the value\nfunction. The transitions are replayed as per the standard Q-learning update equation shown below:\nQt(sj, aj)\u2190 Qt(sj , aj) + \u03b1[Rt(sj, aj) + \u03b3max a\u2032 Qt(sj+1, a \u2032)\u2212Qt(sj, aj)]. (6)\nWhere sj and aj refer to the state and action at transition j, and Qt and Rt represent the action-value function and reward corresponding to the task. The variable a\u2032 is a bound variable that represents any action in the action set A. The learning rate and discount parameters are represented by \u03b1 and \u03b3 respectively.\nThe sequence \u0398s in Equation (5) is a subset of \u0398t, which is in turn part of the library L and thus associated with a high absolute TD error. When replaying \u0398v, the effects of the high absolute TD errors propagate from the values of state/state-action pairs in \u03982s to those in \u0398 1 b . Hence, in case of multiple points of intersection, we consider points that are furthest down \u0398b. In other words, the intersection point is chosen to maximize the length of \u03981b . In this manner, a larger number of state-action values experience improvements brought about by replaying the transition sequences.\nAlgorithm 2 Replay of virtual transition sequences from library Lv\n1: Inputs: \u03b1 : learning rate \u03b3 : discount factor Lv = {\u0398v0...\u0398vi...\u0398vnv} : A library of virtual transition sequences with nv sequences 2: for i = 1 : nv do 3: nsar =number of {s a r} triads in \u0398vi 4: j = nsar 5: while j > 0 do 6: Qt(sj, aj) \u2190 Qt(sj, aj) + \u03b1[Rt(sj, aj) + \u03b3maxa\u2032 Qt(sj+1, a\n\u2032) \u2212 Qt(sj, aj)]\n7: j \u2190 j \u2212 1 8: end while\n9: end for"}, {"heading": "4. Results and Discussion", "text": "We demonstrate our approach on modified versions of two standard reinforcement learning tasks. The first is a navigation/puddle-world problem (Figure 3), and the second is a mountain car problem (Figure 5). In both these problems, behavior policies are generated to solve a given task (which we refer to as the primary task) relatively greedily, while the value function for another task of interest (which we refer to as the secondary task) is simultaneously learned in an off-policy manner. The secondary task is intentionally made more difficult by making appropriate modifications to the environment. Such a setting best demonstrates the effectiveness of our approach and emphasizes its advantages over other experience replay approaches. We characterize the difficulty of the secondary task with a difficulty ratio \u03c1, which is the fraction of the executed behavior policies that experience a high reward with respect to the secondary task. A low value of \u03c1 indicates that achieving the secondary task under the given behavior policy is difficult."}, {"heading": "4.1. Navigation/Puddle-World Task", "text": "In the navigation environment, the simulated agent is assigned tasks of navigating to certain locations in its environment. We consider two locations, P1 and P2, which represent the primary and secondary task locations respectively. The environment is set up such that the location corresponding to high rewards with respect to the secondary task lies far away from that of the primary task (see Figure 3). In addition to this, the accessibility to the secondary task location is deliberately limited by surrounding it with obstacles on all but one side. These modifications contribute towards a low value of \u03c1, especially when the agent operates with a greedy behavior policy with respect to the primary task.\nThe agent is assumed to be able to sense its location in the environment accurately and is capable of detecting obstacles directly in front of it, up to 1 unit away. It can move around in the environment at a maximum speed of 1 unit per time step by executing actions to take it forwards, backwards, sideways and diagonally forwards or backwards to either side. In addition to these actions, the agent can choose to hold its current position. However, the transitions resulting from these actions are probabilistic in nature. The intended movements occur only 80 % of the time, and for the remaining 20 %, the x- and y-coordinates may deviate from their intended values by 1 unit.\nAlso, the agent\u2019s location does not change if the chosen action forces it to run into an obstacle.\nThe agent employs tabular Q-learning with a relatively greedy policy (\u01eb = 0.1) that attempts to maximize the expected sum of primary rewards. The reward structure for both tasks is such that the agent receives a high reward (100) for visiting the respective goal locations, and a high penalty (\u2212100) for bumping into an obstacle in the environment. In addition to this, the agent is assigned a living penalty (\u221210) for each action that fails to result in the goal state. In all simulations, the discount factor \u03b3 is set to be 0.9 and the learning rate \u03b1 is set to a value of 0.3. These values were found to work well in both the navigation as well as the mountain-car environments.\nIn the environment described, the agent executes actions to learn the primary task. Simultaneously, the approach described in Section 3 is employed to learn the value functions associated with the secondary task. At each episode of the learning process, the agent\u2019s performance with respect to the secondary task is evaluated. This is done by allowing the agent to execute 100 actions from randomly generated starting points in the state space and\nmeasuring the average sum of rewards (return) accumulated per episode. Figure 4 shows the average return for the secondary task plotted for 50 runs of 1000 learning episodes using different learning approaches. The low average value of \u03c1 (= 0.0065 as indicated in Figure 4) indicates the relatively high difficulty of the secondary task under the behavior policy being executed. As observed in Figure 4, an agent that replays transition sequences manages to accumulate high average returns at a much faster rate as compared to regular Q-learning. The approach also performs better than other experience replay approaches for the same number of replay updates.\nTable 1 shows the average return for the secondary task accumulated per episode (Ge) during 50 runs of the navigation task for different values of memory parameters mb, mt and nv used in our approach. Each of the parameters are varied separately while keeping the other parameters fixed to their default values. The default values used for mb, mt and nv are 1000, 1000 and 50 respectively."}, {"heading": "4.2. Mountain Car Task", "text": "In the mountain car task, the agent, an under-powered vehicle represented by the circle in Figure 5 is assigned a primary task of getting out of the trough\nand visiting point P1. The act of visiting point P2 is treated as the secondary task. The agent is assigned a high reward (100) for for fulfilling the respective objectives, and a living penalty (\u22121) is assigned for all other situations. At each time step, the agent can choose from three possible actions: (1) accelerating in the positive x direction, (2) accelerating in the negative x direction, and (3) applying no control. The environment is discretized such that 120 unique positions and 100 unique velocity values are possible.\nThe mountain is described by the equation y = e\u22120.5x sin(4x) such that point P2 is higher than P1. Also, the average slope leading to P2 is steeper than that leading to P1. In addition to this, the agent is set to be relatively greedy with respect to the primary task, with an exploration parameter \u01eb = 0.1. These factors make the secondary task more difficult, resulting in a low value of \u03c1 (= 0.0354) under the policy executed.\nFigure 6 shows the average secondary task returns for 50 runs of 5000 learning episodes. It is seen that especially during the initial phase of learning, the agent accumulates rewards at a higher rate as compared to other learning approaches. As in the navigation task, the number of replay updates are restricted to be the same while comparing the different experience replay approaches in Figure 6. Analogous to Table 1, Table 2 shows the average secondary returns accumulated per episode (Ge) over 50 runs in the mountain-car environment, for different values of the memory parameters. The default values for mb, mt and nv are the same as those mentioned in the navigation environment, that is, 1000, 1000 and 50 respectively.\nFrom Figures 4 and 6, the agent is seen to be able to accumulate significantly higher average secondary returns per episode when experiences are replayed. Among the experience replay approaches, the approach of replaying transition sequences is superior for the same number of replay updates. This is especially true in the navigation environment, where visits to regions associated with high secondary task rewards are much rarer, as indicated by the low value of \u03c1. In the mountain car problem, the visits are more frequent, and the differences between the different experience replay approaches are less significant. In both environments, the performances of the approaches\nthat replay individual transitions\u2014experience replay with uniform random sampling and prioritized experience replay\u2014are found to be nearly equivalent.\nThe approach of replaying transition sequences seems to be particularly sensitive to the memory parameter mt, with better performances being achieved for larger values of mt. A possible explanation for this could simply be that larger values of mt correspond to longer \u0398t sequences, which allow a larger number of replay updates to occur in more regions of the state/stateaction space. The influence of the length of the \u0398b sequence, specified by the parameter mb is also similar in nature, but its impact on the performance is less emphatic. This could be because longer \u0398b sequences allow a greater chance for their state trajectories to intersect with those of \u0398t, thus improving the chances of virtual transition sequences being discovered, and of the agent\u2019s value functions being updated using virtual experiences. However, the parameter nv, associated with the size of the library Lv does not seem to have a noticeable influence on the performance of this approach. This is probably due to the fact that the library L (and consequently Lv) is continuously updated with new, suitable transition sequences (successful\nsequences associated with higher magnitudes of TD errors) as and when they are observed. Hence, the storage of a number of transition sequences in the libraries becomes largely redundant.\nAlthough the method of constructing virtual transition sequences is more naturally applicable to the tabular case, it could also possibly be extended to approaches with function approximation. However, soft intersections between state trajectories would have to be considered instead of absolute intersections. That is, while comparing the state trajectories Syx and S y\u2032 x\u2032 , the existence of sc could be considered if it is close to elements in both S y x and S y\u2032 x\u2032 within some specified tolerance limit. One of the limitations of constructing virtual transition sequences is that in higher dimensional spaces, intersections in the state trajectories may become less frequent. However, other sequences in the library L may still be replayed. If appropriate sequences have not yet been discovered or constructed, and are thus not available for replay, other experience replay approaches that replay individual transitions can be used to accelerate learning in the meanwhile.\nPerhaps another limitation of the approach described here is that constructing the library L requires some notion of a goal state associated with high rewards. By tracking the statistical properties such as the mean and variance of the rewards experienced by an agent in its environment in an online manner, the notion of what qualifies as a high reward could be automated using suitable thresholds. In addition to this, other criteria such as the returns or average absolute TD errors of a sequence could also be used to maintain the library.\nIt is worth adding that the memory parameters mb, mt and nv have been set arbitrarily in the examples described here. Selecting appropriate values for these parameters as the agent interacts with its environment could be a topic for further research.\nThe approach of replaying transition sequences has direct applications in multi-task RL, where agents are required to learn multiple tasks in parallel. In addition, this approach could also be significantly useful in deep RL applications. Certain tasks could be associated with the occurrence of relatively rare events during the agent\u2019s interaction with the environment. The replay of virtual transition sequences could further improve the learning in such tasks. This approach could also prove to be particularly valuable in fields such as robotics, where exploration of the state/state-action space is typically expensive in terms of time and energy. By reusing the agent-environment in-\nteractions in the manner described here, reasonably good estimates of the value functions corresponding to multiple tasks can be maintained, thereby improving the efficiency of exploration."}, {"heading": "5. Conclusion", "text": "In this work, we described an approach to replay sequences of transitions to accelerate the learning of tasks in an off-policy setting. Suitable transition sequences are selected and stored in a replay library based on the magnitudes of the TD errors associated with them. Using these sequences, we showed that it is possible to construct virtual experiences in the form of virtual transition sequences, which could be replayed to improve an agent\u2019s learning, especially in environments where desirable events occur rarely. We demonstrated the benefits of this approach by applying it to versions of standard reinforcement learning tasks such as the puddle-world and mountaincar tasks. In both tasks, a significant improvement in learning speed was observed compared to regular Q-learning as well as other forms of experience replay. Further, the influence of the different memory parameters used was described and evaluated empirically, and possible extensions to this work were briefly discussed. Characterized by controllable memory parameters and the potential to significantly improve the efficiency of exploration at the expense of some increase in computation, the approach of using replaying transition sequences could be especially useful in fields such as robotics, where these factors are of prime importance."}, {"heading": "Acknowledgements", "text": "The authors thank Richard S. Sutton from the University of Alberta for his feedback and many helpful discussions during the development of this work."}], "references": [{"title": "Experience replay for real-time reinforcement learning control", "author": ["S. Adam", "L. Busoniu", "R. Babuska"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 42 (2) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "in: International Conference on Learning Representations, Puerto Rico", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Identification and off-policy learning of multiple objectives using adaptive clustering", "author": ["T.G. Karimpanal", "E. Wilhelm"], "venue": "Neurocomputing (In Press) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2017}, {"title": "Self-improving reactive agents based on reinforcement learning", "author": ["L.-J. Lin"], "venue": "planning and teaching, Machine Learning 8 (3-4) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1992}, {"title": "A", "author": ["R.S. Sutton"], "venue": "G. Barto, Reinforcement learning: An introduction ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "B", "author": ["M. Geist"], "venue": "Scherrer, et al., Off-policy learning with eligibility traces: a survey., Journal of Machine Learning Research 15 (1) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup"], "venue": "in: The 10th International Conference on Autonomous Agents and Multiagent Systems- Volume 2, International Foundation for Autonomous Agents and Multiagent Systems", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Scaling life-long off-policy learning", "author": ["A. White", "J. Modayil", "R.S. Sutton"], "venue": "in: Development and Learning and Epigenetic Robotics (ICDL), 2012 IEEE International Conference on, IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-timescale nexting in a reinforcement learning robot", "author": ["J. Modayil", "A. White", "R.S. Sutton"], "venue": "Adaptive Behavior 22 (2) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient exploration in reinforcement learning", "author": ["S.B. Thrun"], "venue": "Tech. rep., Pittsburgh, PA, USA ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1992}, {"title": "Data-efficient off-policy policy evaluation for reinforcement learning", "author": ["P.S. Thomas", "E. Brunskill"], "venue": "in: International Conference on Machine Learning", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Quasi-online reinforcement learning for robots", "author": ["B. Bakker", "V. Zhumatiy", "G. Gruener", "J. Schmidhuber"], "venue": "in: Robotics and Automation, 2006. ICRA 2006. Proceedings 2006 IEEE International Conference on, IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Reinforcement learning in robotics: A survey", "author": ["J. Kober", "J.A. Bagnell", "J. Peters"], "venue": "The International Journal of Robotics Research 32 (11) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Simulation and the Monte Carlo method", "author": ["R.Y. Rubinstein", "D.P. Kroese"], "venue": "John Wiley & Sons", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "G", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland"], "venue": "Ostrovski, et al., Human-level control through deep reinforcement learning, Nature 518 (7540) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "in: International Conference on Machine Learning", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Babu\u0161ka, The importance of experience replay database composition in deep reinforcement learning, in: Deep Reinforcement Learning", "author": ["T. de Bruin", "J. Kober", "R.K. Tuyls"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["K. Narasimhan", "T.D. Kulkarni", "R. Barzilay"], "venue": "in: L. M\u00e0rquez, C. Callison-Burch, J. Su, D. Pighin, Y. Marton (Eds.), Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, The Association for Computational Linguistics", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Surprise and curiosity for big data robotics", "author": ["A. White", "J. Modayil", "R.S. Sutton"], "venue": "in: AAAI-14 Workshop on Sequential Decision-Making with Big Data, Quebec City, Quebec, Canada", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient exploration in reinforcement learning", "author": ["S.B. Thrun"], "venue": "Tech. rep. ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1992}, {"title": "Prioritized sweeping: Reinforcement learning with less data and less time", "author": ["A.W. Moore", "C.G. Atkeson"], "venue": "Machine learning 13 (1) ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1993}, {"title": "Planning by Prioritized Sweeping with Small Backups", "author": ["H. van Seijen", "R.S. Sutton"], "venue": "in: Proceedings of the 30th International Conference on Machine Learning, Cycle 3,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Hippocampal place cells construct reward related sequences through unexplored space", "author": ["H.F. \u00d3lafsd\u00f3ttir", "C. Barry", "A.B. Saleem", "D. Hassabis", "H.J. Spiers"], "venue": "Elife 4 ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Reactivation", "author": ["L. Buhry", "A.H. Azizi", "S. Cheng"], "venue": "replay, and preplay: how it might all fit together, Neural plasticity 2011 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Rewarded outcomes enhance reactivation of experience in the hippocampus", "author": ["A.C. Singer", "L.M. Frank"], "venue": "Neuron 64 (6) ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Integrated architectures for learning", "author": ["R.S. Sutton"], "venue": "planning, and reacting based on approximating dynamic programming, in: Proceedings of the Seventh Int. Conf. on Machine Learning", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1990}, {"title": "Swarm-enabling technology for multi-robot systems", "author": ["M. Chamanbaz", "D. Mateo", "B.M. Zoss", "G. Toki\u0107", "E. Wilhelm", "R. Bouffanais", "D.K.P. Yue"], "venue": "Front. Robot. AI 4 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2017}, {"title": "Building a library of policies through policy reuse", "author": ["F. Fern\u00e1ndez", "M. Veloso"], "venue": "Tech. Rep. CMU-CS-05-174, Computer Science Department, Carnegie Mellon University, Pittsburgh, PA ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "Reverse replay of behavioural sequences in hippocampal place cells during the awake state", "author": ["D.J. Foster", "M.A. Wilson"], "venue": "Nature 440 (7084) ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "exploration generally exceeds the computational cost of learning [1\u20134].", "startOffset": 65, "endOffset": 70}, {"referenceID": 1, "context": "exploration generally exceeds the computational cost of learning [1\u20134].", "startOffset": 65, "endOffset": 70}, {"referenceID": 2, "context": "exploration generally exceeds the computational cost of learning [1\u20134].", "startOffset": 65, "endOffset": 70}, {"referenceID": 3, "context": "Experience replay [5] is a technique that reuses information gathered from past experiences to improve the efficiency of learning.", "startOffset": 18, "endOffset": 21}, {"referenceID": 4, "context": "In order to replay stored experiences using this approach, an off-policy [6, 7] setting is a prerequisite.", "startOffset": 73, "endOffset": 79}, {"referenceID": 5, "context": "In order to replay stored experiences using this approach, an off-policy [6, 7] setting is a prerequisite.", "startOffset": 73, "endOffset": 79}, {"referenceID": 6, "context": "These algorithms can hence be used to parallelize learning, and, thus gather as much knowledge as possible using real experiences [8\u201310].", "startOffset": 130, "endOffset": 136}, {"referenceID": 7, "context": "These algorithms can hence be used to parallelize learning, and, thus gather as much knowledge as possible using real experiences [8\u201310].", "startOffset": 130, "endOffset": 136}, {"referenceID": 8, "context": "These algorithms can hence be used to parallelize learning, and, thus gather as much knowledge as possible using real experiences [8\u201310].", "startOffset": 130, "endOffset": 136}, {"referenceID": 9, "context": "The problem of learning from limited experience is not new in the field of RL [11, 12].", "startOffset": 78, "endOffset": 86}, {"referenceID": 10, "context": "The problem of learning from limited experience is not new in the field of RL [11, 12].", "startOffset": 78, "endOffset": 86}, {"referenceID": 11, "context": "Particularly for robotics applications, these factors are even more important, as exploration of the environment is typically time and energy expensive [13, 14].", "startOffset": 152, "endOffset": 160}, {"referenceID": 12, "context": "Particularly for robotics applications, these factors are even more important, as exploration of the environment is typically time and energy expensive [13, 14].", "startOffset": 152, "endOffset": 160}, {"referenceID": 4, "context": "When the behavior and target policies vary considerably from each other, importance sampling [6, 15] is commonly used in order to obtain better estimates of the value functions.", "startOffset": 93, "endOffset": 100}, {"referenceID": 13, "context": "When the behavior and target policies vary considerably from each other, importance sampling [6, 15] is commonly used in order to obtain better estimates of the value functions.", "startOffset": 93, "endOffset": 100}, {"referenceID": 0, "context": "This approach has received a lot of attention in recent years due to its utility in deep RL applications [2, 16\u201319].", "startOffset": 105, "endOffset": 115}, {"referenceID": 14, "context": "This approach has received a lot of attention in recent years due to its utility in deep RL applications [2, 16\u201319].", "startOffset": 105, "endOffset": 115}, {"referenceID": 15, "context": "This approach has received a lot of attention in recent years due to its utility in deep RL applications [2, 16\u201319].", "startOffset": 105, "endOffset": 115}, {"referenceID": 16, "context": "This approach has received a lot of attention in recent years due to its utility in deep RL applications [2, 16\u201319].", "startOffset": 105, "endOffset": 115}, {"referenceID": 1, "context": "Recent works [3, 20] have explored different ways in which transitions may be prioritized.", "startOffset": 13, "endOffset": 20}, {"referenceID": 17, "context": "Recent works [3, 20] have explored different ways in which transitions may be prioritized.", "startOffset": 13, "endOffset": 20}, {"referenceID": 1, "context": "[3] prioritized transitions on the basis of their associated TD errors.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[2] involved some variants that replayed sequences of experiences, but these sequences were drawn randomly from the replay memory.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3], TD errors have been frequently used as a basis for prioritization in other RL problems [3, 21, 22].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3], TD errors have been frequently used as a basis for prioritization in other RL problems [3, 21, 22].", "startOffset": 92, "endOffset": 103}, {"referenceID": 18, "context": "[3], TD errors have been frequently used as a basis for prioritization in other RL problems [3, 21, 22].", "startOffset": 92, "endOffset": 103}, {"referenceID": 19, "context": "[3], TD errors have been frequently used as a basis for prioritization in other RL problems [3, 21, 22].", "startOffset": 92, "endOffset": 103}, {"referenceID": 20, "context": "In particular, the modelbased approach of prioritized sweeping [23, 24] prioritizes backups that are expected to result in a significant change in the value function.", "startOffset": 63, "endOffset": 71}, {"referenceID": 21, "context": "In particular, the modelbased approach of prioritized sweeping [23, 24] prioritizes backups that are expected to result in a significant change in the value function.", "startOffset": 63, "endOffset": 71}, {"referenceID": 22, "context": "Replaying sequences of experiences also seems to be biologically plausible [25, 26].", "startOffset": 75, "endOffset": 83}, {"referenceID": 23, "context": "Replaying sequences of experiences also seems to be biologically plausible [25, 26].", "startOffset": 75, "endOffset": 83}, {"referenceID": 24, "context": "In addition, it is known that animals tend to remember experiences that lead to high rewards [27].", "startOffset": 93, "endOffset": 97}, {"referenceID": 25, "context": "Some early approaches in RL, such as the dyna architecture [28] also made use of simulated experience to improve the value function estimates.", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "Our approach also recognizes the real-world limitations of replay memory [19], and stores only a certain amount of information at a time, specified by memory parameters.", "startOffset": 73, "endOffset": 77}, {"referenceID": 26, "context": "[29]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20], where transitions associated with positive rewards are prioritized for replay.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "These sequences are maintained in the library L in a manner similar to the Policy Library through Policy Reuse (PLPR) algorithm [30].", "startOffset": 128, "endOffset": 132}, {"referenceID": 0, "context": "Such an idea has been briefly mentioned in previous studies on experience replay [2, 3].", "startOffset": 81, "endOffset": 87}, {"referenceID": 1, "context": "Such an idea has been briefly mentioned in previous studies on experience replay [2, 3].", "startOffset": 81, "endOffset": 87}, {"referenceID": 28, "context": "In addition, a similar mechanism of reverse replay has been observed in animals [31].", "startOffset": 80, "endOffset": 84}], "year": 2017, "abstractText": "Experience replay is one of the most commonly used approaches to improve the sample efficiency of reinforcement learning algorithms. In this work, we propose an approach to select and replay sequences of transitions in order to accelerate the learning of a reinforcement learning agent in an off-policy setting. In addition to selecting appropriate sequences, we also artificially construct transition sequences using information gathered from previous agentenvironment interactions. These sequences, when replayed, allow value function information to trickle down to larger sections of the state/state-action space, thereby making the most of the agent\u2019s experience. We demonstrate our approach on modified versions of standard reinforcement learning tasks such as the mountain car and puddle world problems and empirically show that it enables better learning of value functions as compared to other forms of experience replay. Further, we briefly discuss some of the possible extensions to this work, as well as applications and situations where this approach could be particularly useful.", "creator": "LaTeX with hyperref package"}}}