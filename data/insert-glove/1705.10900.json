{"id": "1705.10900", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Does the Geometry of Word Embeddings Help Document Classification? A Case Study on Persistent Homology Based Representations", "abstract": "paperwork We investigate joanette the 13-team pertinence of pasteles methods from indi algebraic topology for text harras data third-wave analysis. h\u00e4ssleholm These harve methods rigmor enable stobie the development of mathematically - diameters principled isometric - kmart invariant serot mappings from benke a set deeg of vectors to a parrish document embedding, deepwater which modzelewski is stable harring with jeppe respect ulcinj to shaykhs the marvic geometry of the document suicidology in aelred the generalisations selected metric gunner space. In this us-50 work, klimova we evaluate otoliths the epogen utility mihails of folly these topology - wwu based hornby document representations in traditional NLP culti tasks, ogerman specifically document clustering and sentiment gudjonsson classification. 38-year We torreya find ventura that the qudah embeddings 163-page do 66.42 not firuzeh benefit text analysis. jahch In virac fact, 223.7 performance is hemachandra worse than blundell simple chivilcoy techniques rosentraub like $ \\ textit {veselica tf - birol idf} $, try indicating stape that the geometry of masta the document cotinis does situationally not provide chikungunya enough variability roshni for classification relegates on villeneuve the mulvaneys basis of topic plenipotentiary or awale sentiment um in the boiling chosen datasets.", "histories": [["v1", "Wed, 31 May 2017 00:43:04 GMT  (474kb,D)", "http://arxiv.org/abs/1705.10900v1", "5 pages, 3 figures. Rep4NLP workshop at ACL 2017"]], "COMMENTS": "5 pages, 3 figures. Rep4NLP workshop at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["paul michel", "abhilasha ravichander", "shruti rijhwani"], "accepted": false, "id": "1705.10900"}, "pdf": {"name": "1705.10900.pdf", "metadata": {"source": "CRF", "title": "Does the Geometry of Word Embeddings Help Document Classification? A Case Study on Persistent Homology Based Representations", "authors": ["Paul Michel", "Abhilasha Ravichander", "Shruti Rijhwani"], "emails": ["pmichel1@cs.cmu.edu", "aravicha@cs.cmu.edu", "srijhwan@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Given a embedding model mapping words to n dimensional vectors, every document can be represented as a finite subset of Rn. Comparing documents then amounts to comparing such subsets. While previous work shows that the Earth Mover\u2019s Distance (Kusner et al., 2015) or distance between the weighted average of word vectors (Arora et al., 2017) provides information that is useful for classification tasks, we wish to go a step further and investigate whether useful information can also be found in the \u2018shape\u2019 of a document in word embedding space.\nPersistent homology is a tool from algebraic topology used to compute topological signatures (called persistence diagrams) on compact metric\n\u2217*The indicated authors contributed equally to this work.\nspaces. These have the property of being stable with respect to the Gromov-Haussdorff distance (Gromov et al., 1981). In other words, compact metric spaces that are close, up to an isometry, will have similar embeddings. In this work, we examine the utility of such embeddings in text classification tasks. To the best of our knowledge, no previous work has been performed on using topological representations for traditional NLP tasks, nor has any comparison been made with state-ofthe-art approaches.\nWe begin by considering a document as the set of its word vectors, generated with a pretrained word embedding model. These form the metric space on which we build persistence diagrams, using Euclidean distance as the distance measure. The diagrams are a representation of the document\u2019s geometry in the metric space. We then perform clustering on the Twenty Newsgroups dataset with the features extracted from the persistence diagram. We also evaluate the method on sentiment classification tasks, using the Cornell Sentence Polarity (CSP) (Pang and Lee, 2005) and IMDb movie review datasets (Maas et al., 2011).\nAs suggested by Zhu (2013), we posit that the information about the intrinsic geometry of documents, found in the persistence diagrams, might yield information that our classifier can leverage, either on its own or in combination with other representations. The primary objective of our work is to empirically evaluate these representations in the case of sentiment and topic classification, and assess their usefulness for real-world tasks."}, {"heading": "2 Method", "text": ""}, {"heading": "2.1 Word embeddings", "text": "As a first step we compute word vectors for each document in our corpus using a word2vec (Mikolov et al., 2013) model trained on the Google\nar X\niv :1\n70 5.\n10 90\n0v 1\n[ cs\n.C L\n] 3\n1 M\nay 2\n01 7\nNews dataset1. In addition to being a widely used word embedding technique, word2vec has been known to exhibit interesting linear properties with respect to analogies (Mikolov et al., 2013), which hints at rich semantic structure."}, {"heading": "2.2 Gromov-Haussdorff Distance", "text": "Given a dictionary of word vectors of dimension n, we can represent any document as a finite subset of Rn. The Haussdorff distance gives us a way to evaluate the distance between two such sets. More precisely, the Haussdorff distance dH between two finite subsets A,B of Rn is defined as:\ndH(A,B) = max(sup a\u2208A d(a,B), sup b\u2208B d(b, A))\nwhere d(x, Y ) = infy\u2208Y \u2016x \u2212 y\u20162 is the distance of point x from set Y .\nHowever, this distance is sensitive to translations and other isometric2 transformations. Hence, a more natural metric is the Gromov-Haussdorff distance (Gromov et al., 1981), simply defined as\ndGH(A,B) = inf f\u2208En dH(A, f(B))\nwhere En is the set of all isometries of Rn. Figure 1 provides an example of practical Gromov-Haussdorff (GH) distance computation between two sets of three points each. Both sets are embedded in R2 (middle panel) using isometries i.e the distance between points in each set is conserved. The Haussdorff distance between the two embedded sets corresponds to the length of the black segment. The GH distance is the minimum Haussdorff distance under all possible isometric embeddings.\nWe want to compare documents based on their intrinsic geometric properties. Intuitively, the GH distance measures how far two sets are from being isometric. This allows us to define the geometry of a document more precisely:\nDefinition 1 (Document Geometry) We say that two documents A, B have the same geometry if dGH(A,B) = 0, ie if they are the same up to an isometry.\nMathematically speaking, this amounts to defining the geometry of a document as its equivalence class under the equivalence relation induced by the GH distance on the set of all documents.\n1https://code.google.com/archive/p/word2vec/ 2f : Rn \u2212\u2192 Rn is isometric if it is distance preserving, ie \u2200x, y \u2208 Rn, \u2016f(x)\u2212f(y)\u20162 = \u2016x\u2212y\u20162. Rotations, translations and reflections are examples of (linear) isometries.\nComparison to the Earth Mover Distance : Kusner et al. (2015) proposed a new method for computing a distance between documents based on an instance of the Earth Mover Distance (Rubner et al., 1998) called Word Mover Distance (WMD). While WMD quantifies the total cost of matching all words of one document to another, the GH distance is the cost, up to an isometry, of the worst-case matching."}, {"heading": "2.3 Persistence diagrams", "text": "Efficiently computing the GH distance is still an open problem despite a lot of recent work in this area (Me\u0301moli and Sapiro, 2005; Bronstein et al., 2006; Me\u0301moli, 2007; Agarwal et al., 2015).\nFortunately, Carrie\u0300re et al. (2015) provides us with a way to derive a signature which is stable with respect to the GH distance. More specifically, given a finite point cloud A \u2282 Rn, the persistence diagram of the Vietori-Rips filtration on A, Dg(A), can be computed. This approach is inspired by persistent homology, a subfield of algebraic topology.\nThe rigorous definition of these notions is not the crux of this paper and we will only present them informally. The curious reader is invited to refer to Zhu (2013) for a short introduction. More details are in Delfinado and Edelsbrunner (1995); Edelsbrunner et al. (2002); Robins (1999).\nA persistence diagram is a scatter plot of 2-D points representing the appearance and disappearance of geometric features3 under varying resolutions. This can be imagined as replacing each point by a sphere of increasing radius.\nWe use the procedure described in Carrie\u0300re et al.\n3such as connected components, holes or empty hulls\n(2015) to derive fixed-sized vectors from persistence diagrams. These vectors have the following property: if A and B are two finite subsets of Rn, Dg(A) and Dg(B) are their persistence diagrams, N = max(|Dg(A)|, |Dg(B)|) and VA, VB \u2208 R N(N\u22121) 2 , then\n\u2016VA \u2212 VB\u20162 6 \u221a 2N(N \u2212 1)dGH(A,B)\nIn other words, the resulting signatures VA and VB are stable with respect to the GH distance. The size of the vectors are dependent on the underlying sets A and B. However, as is argued in Carrie\u0300re et al. (2015), we can truncate the vectors to a dimension fixed across our dataset while preserving the stability property (albeit losing some of the representative ability of the signatures)."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Experiments", "text": "The pipeline for our experiments is shown in Figure 2. In order to build a persistence diagram, we convert each document to the set of its word vectors. We then use Dionysus (Morozov, 2008\u2013 2016), a C++ library for computing persistence diagrams, and form the signatures described in 2.3. We will subsequently refer to these diagrams as Persistent Homology (PH) embeddings. Once we have the embeddings for each document, they can be used as input to standard clustering or classification algorithms.\nAs a baseline document representation, we use the average of the word vectors for that document (subsequently called AW2V embeddings).\nFor clustering, we experiment with K-means and Gaussian Mixture Models (GMM) on a subset4 of the Twenty Newsgroups dataset. The subset was selected to ensure that most documents are from related topics, making clustering nontrivial, and the documents are of reasonable length to compute the representation.\n4alt.atheism, sci.space and talk.religion.misc categories\nFor classification, we perform both sentencelevel and document-level binary sentiment classification using logistic regression on the CSP and IMDb corpora respectively."}, {"heading": "4 Results", "text": ""}, {"heading": "4.1 Hyper-parameters", "text": "Our method depends on very few hyperparameters. Our main choices are listed below.\nChoice of distance We experimented with both euclidean distance and cosine similarity (angular distance). After preliminary experiments, we determined that both performed equally and hence, we only report results with the euclidean distance.\nPersistence diagram computation The hyperparameters of the diagram computation are monotonic and mostly control the degree of approximation. We set them to the highest values that allowed our experiment to run in reasonable time5."}, {"heading": "4.2 Document Clustering", "text": "We perform clustering experiments with the baseline document features (AW2V), tf-idf and our PH signatures. Figure 3 shows the B-Cubed precision, recall and F1-Score of each method (metrics as defined in Amigo\u0301 et al. (2009)). To further assess the utility of PH embeddings, we concatenate them with AW2V to obtain a third representation, AW2V+PH.\nWith GMM and AW2V+PH, the F1-Score of clustering is 0.499. In terms of F1 and precision, we see that tf-idf representations perform better than PH, for reasons that we will discuss in later sections. In terms of recall, PH as well as AW2V perform fairly well. Importantly, we see that all the metrics for PH are significantly above the random baseline, indicating that some valuable information is contained in them."}, {"heading": "4.3 Sentiment Classification", "text": ""}, {"heading": "4.3.1 Sentence-Level Sentiment Analysis", "text": "We evaluate our method on the CSP dataset6. The results are presented in Table 1. For comparison, we provide results for one of the state of the art models, a CNN-based sentence classifier (Kim,\n5Selected such that the computation of the diagram of the longest file in the training data took less than 10 minutes.\n6For lack of a canonical split, we use a random 10% of the dataset as a test set\n2014). We observe that by themselves, PH embeddings are not useful at predicting the sentiment of each sentence. AW2V gives reasonable performance in this task, but combining the two representations does not impact the accuracy at all."}, {"heading": "4.3.2 Document-Level Sentiment Analysis", "text": "We perform document-level binary sentiment classification on the IMDb Movie Reviews Dataset (Maas et al., 2011). We use sentence vectors in this experiment, each of which is the average of the word vectors in that sentence. The results are presented in Table 2. We compare our results with the paragraph-vector approach (Le and Mikolov, 2014). We observe that PH embeddings perform poorly on this dataset. Similar to the CSP dataset, AW2V embeddings give acceptable results. The combined representation performs slightly better, but not by a margin of significance."}, {"heading": "5 Discussion and Analysis", "text": "As seen in Figure 3, the PH representation does not outperform tf-idf or AW2V, and in fact often doesn\u2019t perform much better than chance.\nOne possible reason is linked to the nature of our datasets: the computation of the persistence diagram is very sensitive to the size of the documents. The geometry of small documents, where the number of words is negligible with respect to the dimensionality of the word vectors, is not very rich. The resulting topological signatures are very sparse, which is a problem for CSP as well as documents in IMDb and Twenty Newsgroups that contain only one line. On the opposite side of the spectrum, persistence diagrams are intractable to compute without down-sampling for very long documents (which in turn negatively impacts the representation of smaller documents).\nWe performed an additional experiment on a subset of the IMDb corpus that only contained documents of reasonable length, but obtained similar results. This indicates that the poor performance of PH representations, even when combined with other features (AW2V), cannot be explained only by limitations of the data.\nThese observations lead to the conclusion that, for these datasets, the intrinsic geometry of documents in the word2vec semantic space does not help text classification tasks."}, {"heading": "6 Related Work", "text": "Learning distributed representations of sentences or documents for downstream classification and information retrieval tasks has received recent attention owing to their utility in several applications, be it representations trained on the sen-\ntence/paragraph level Le and Mikolov (2014); Kiros et al. (2015) or purely word vector based methods Arora et al. (2017).\nDocument classification and clustering (Willett, 1988; Hotho et al., 2005; Steinbach et al., 2000; Huang, 2008; Xu and Gong, 2004; Kuang et al., 2015; Miller et al., 2016) and sentiment classification (Nakagawa et al., 2010; Kim, 2014; Wang and Manning, 2012) are relatively well studied.\nTopological data analysis has been used for various tasks such as 3D shapes classification (Chazal et al., 2009) or protein structure analysis (Xia and Wei, 2014). However, such techniques have not been used in NLP, primarily because the theory is inaccessible and suitable applications are scarce. Zhu (2013) offers an introduction to using persistent homology in NLP, by creating representations of nursery-rhymes and novels, as well as highlights structural differences between child and adolescent writing. However, these techniques have not been applied to core NLP tasks."}, {"heading": "7 Conclusion", "text": "Based on our experiments, using persistence diagrams for text representation does not seem to positively contribute to document clustering and sentiment classification tasks. There are certainly merits to the method, specifically its strong mathematical foundation and its domain-independent, unsupervised nature. Theoretically, algebraic topology has the ability to capture structural context, and this could potentially benefit syntaxbased NLP tasks such as parsing. We plan to investigate this connection in the future."}, {"heading": "Acknowledgments", "text": "This work was supported in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under Contract No. HR0011-15-C-0114. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.\nWe are grateful to Matt Gormley, Hyun Ah Song, Shivani Poddar and Hai Pham for their suggestions on the writing of this paper as well as to Steve Oudot for pointing us to helpful references. We would also like to thank the anonymous ACL reviewers for their valuable suggestions."}], "references": [{"title": "Computing the gromov-hausdorff distance for metric trees", "author": ["Pankaj K Agarwal", "Kyle Fox", "Abhinandan Nath", "Anastasios Sidiropoulos", "Yusu Wang."], "venue": "International Symposium on Algorithms and Computation. Springer, pages 529\u2013540.", "citeRegEx": "Agarwal et al\\.,? 2015", "shortCiteRegEx": "Agarwal et al\\.", "year": 2015}, {"title": "A comparison of extrinsic clustering evaluation metrics based on formal constraints", "author": ["Enrique Amig\u00f3", "Julio Gonzalo", "Javier Artiles", "Felisa Verdejo."], "venue": "Information retrieval 12(4):461\u2013486.", "citeRegEx": "Amig\u00f3 et al\\.,? 2009", "shortCiteRegEx": "Amig\u00f3 et al\\.", "year": 2009}, {"title": "A simple but tough-to-beat baseline for sentence embeddings", "author": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma."], "venue": "International Conference on Learning Representations. To Appear.", "citeRegEx": "Arora et al\\.,? 2017", "shortCiteRegEx": "Arora et al\\.", "year": 2017}, {"title": "Efficient computation of isometry-invariant distances between surfaces", "author": ["Alexander M Bronstein", "Michael M Bronstein", "Ron Kimmel."], "venue": "SIAM Journal on Scientific Computing 28(5):1812\u2013 1836.", "citeRegEx": "Bronstein et al\\.,? 2006", "shortCiteRegEx": "Bronstein et al\\.", "year": 2006}, {"title": "Stable topological signatures for points on 3d shapes", "author": ["Mathieu Carri\u00e8re", "Steve Y Oudot", "Maks Ovsjanikov."], "venue": "Computer Graphics Forum. Wiley Online Library, volume 34, pages 1\u201312.", "citeRegEx": "Carri\u00e8re et al\\.,? 2015", "shortCiteRegEx": "Carri\u00e8re et al\\.", "year": 2015}, {"title": "Gromov-hausdorff stable signatures for shapes using persistence", "author": ["Fr\u00e9d\u00e9ric Chazal", "David Cohen-Steiner", "Leonidas J Guibas", "Facundo M\u00e9moli", "Steve Y Oudot."], "venue": "Computer Graphics Forum. Wiley Online Library, volume 28, pages 1393\u20131403.", "citeRegEx": "Chazal et al\\.,? 2009", "shortCiteRegEx": "Chazal et al\\.", "year": 2009}, {"title": "An incremental algorithm for betti numbers of simplicial complexes on the 3-sphere", "author": ["Cecil Jose A Delfinado", "Herbert Edelsbrunner."], "venue": "Computer Aided Geometric Design 12(7):771\u2013784.", "citeRegEx": "Delfinado and Edelsbrunner.,? 1995", "shortCiteRegEx": "Delfinado and Edelsbrunner.", "year": 1995}, {"title": "Topological persistence and simplification", "author": ["Herbert Edelsbrunner", "David Letscher", "Afra Zomorodian."], "venue": "Discrete and Computational Geometry 28(4):511\u2013533.", "citeRegEx": "Edelsbrunner et al\\.,? 2002", "shortCiteRegEx": "Edelsbrunner et al\\.", "year": 2002}, {"title": "Structures m\u00e9triques pour les vari\u00e9t\u00e9s riemanniennes", "author": ["Mikhael Gromov", "Jacques Lafontaine", "Pierre Pansu"], "venue": null, "citeRegEx": "Gromov et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Gromov et al\\.", "year": 1981}, {"title": "A brief survey of text mining", "author": ["Andreas Hotho", "Andreas N\u00fcrnberger", "Gerhard Paa\u00df."], "venue": "Ldv Forum.", "citeRegEx": "Hotho et al\\.,? 2005", "shortCiteRegEx": "Hotho et al\\.", "year": 2005}, {"title": "Similarity measures for text document clustering", "author": ["Anna Huang."], "venue": "Proceedings of the sixth new zealand computer science research student conference (NZCSRSC2008), Christchurch, New Zealand. pages 49\u201356.", "citeRegEx": "Huang.,? 2008", "shortCiteRegEx": "Huang.", "year": 2008}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "In EMNLP.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in neural information processing systems. pages 3294\u20133302.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Nonnegative matrix factorization for interactive topic modeling and document clustering", "author": ["Da Kuang", "Jaegul Choo", "Haesun Park."], "venue": "Partitional Clustering Algorithms, Springer, pages 215\u2013243.", "citeRegEx": "Kuang et al\\.,? 2015", "shortCiteRegEx": "Kuang et al\\.", "year": 2015}, {"title": "From word embeddings to document distances", "author": ["Matt J Kusner", "Yu Sun", "Nicholas I Kolkin", "Kilian Q Weinberger"], "venue": "In ICML", "citeRegEx": "Kusner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kusner et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V. Le", "Tomas Mikolov."], "venue": "Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 2126 June 2014. pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human", "citeRegEx": "Maas et al\\.,? 2011", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "On the use of gromovhausdorff distances for shape comparison", "author": ["Facundo M\u00e9moli"], "venue": null, "citeRegEx": "M\u00e9moli.,? \\Q2007\\E", "shortCiteRegEx": "M\u00e9moli.", "year": 2007}, {"title": "A theoretical and computational framework for isometry invariant recognition of point cloud data", "author": ["Facundo M\u00e9moli", "Guillermo Sapiro."], "venue": "Foundations of Computational Mathematics 5(3):313\u2013347.", "citeRegEx": "M\u00e9moli and Sapiro.,? 2005", "shortCiteRegEx": "M\u00e9moli and Sapiro.", "year": 2005}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Unsupervised document classification with informed topic models", "author": ["Timothy A Miller", "Dmitriy Dligach", "Guergana K Savova."], "venue": "ACL .", "citeRegEx": "Miller et al\\.,? 2016", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Dyonisus : a c++ library for computing persistent homology. http: //mrzv.org/software/dionysus", "author": ["Dmitriy Morozov"], "venue": null, "citeRegEx": "Morozov.,? \\Q2008\\E", "shortCiteRegEx": "Morozov.", "year": 2008}, {"title": "Dependency tree-based sentiment classification using crfs with hidden variables", "author": ["Tetsuji Nakagawa", "Kentaro Inui", "Sadao Kurohashi."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter", "citeRegEx": "Nakagawa et al\\.,? 2010", "shortCiteRegEx": "Nakagawa et al\\.", "year": 2010}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Bo Pang", "Lillian Lee."], "venue": "Proceedings of the ACL.", "citeRegEx": "Pang and Lee.,? 2005", "shortCiteRegEx": "Pang and Lee.", "year": 2005}, {"title": "Towards computing homology from finite approximations", "author": ["Vanessa Robins."], "venue": "Topology proceedings. volume 24, pages 503\u2013532.", "citeRegEx": "Robins.,? 1999", "shortCiteRegEx": "Robins.", "year": 1999}, {"title": "A metric for distributions with applications to image databases", "author": ["Yossi Rubner", "Carlo Tomasi", "Leonidas J Guibas."], "venue": "Computer Vision, 1998. Sixth International Conference on. IEEE, pages 59\u201366.", "citeRegEx": "Rubner et al\\.,? 1998", "shortCiteRegEx": "Rubner et al\\.", "year": 1998}, {"title": "A comparison of document clustering techniques", "author": ["Michael Steinbach", "George Karypis", "Vipin Kumar"], "venue": "In KDD workshop on text mining", "citeRegEx": "Steinbach et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Steinbach et al\\.", "year": 2000}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Sida Wang", "Christopher D. Manning."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume", "citeRegEx": "Wang and Manning.,? 2012", "shortCiteRegEx": "Wang and Manning.", "year": 2012}, {"title": "Recent trends in hierarchic document clustering: a critical review", "author": ["Peter Willett."], "venue": "Information Processing & Management 24(5):577\u2013597.", "citeRegEx": "Willett.,? 1988", "shortCiteRegEx": "Willett.", "year": 1988}, {"title": "Persistent homology analysis of protein structure, flexibility, and folding", "author": ["Kelin Xia", "Guo-Wei Wei."], "venue": "International journal for numerical methods in biomedical engineering 30(8):814\u2013844.", "citeRegEx": "Xia and Wei.,? 2014", "shortCiteRegEx": "Xia and Wei.", "year": 2014}, {"title": "Document clustering by concept factorization", "author": ["Wei Xu", "Yihong Gong."], "venue": "Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, New", "citeRegEx": "Xu and Gong.,? 2004", "shortCiteRegEx": "Xu and Gong.", "year": 2004}, {"title": "Persistent homology: An introduction and a new text representation for natural language processing", "author": ["Xiaojin Zhu."], "venue": "Proceedings of the 23rd International Joint Conference on Artificial Intelligence.", "citeRegEx": "Zhu.,? 2013", "shortCiteRegEx": "Zhu.", "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "While previous work shows that the Earth Mover\u2019s Distance (Kusner et al., 2015) or distance between the weighted average of word vectors (Arora et al.", "startOffset": 58, "endOffset": 79}, {"referenceID": 2, "context": ", 2015) or distance between the weighted average of word vectors (Arora et al., 2017) provides information that is useful for classification tasks, we wish to go a step further and investigate whether useful information can also be found in the \u2018shape\u2019 of a document in word embedding space.", "startOffset": 65, "endOffset": 85}, {"referenceID": 8, "context": "These have the property of being stable with respect to the Gromov-Haussdorff distance (Gromov et al., 1981).", "startOffset": 87, "endOffset": 108}, {"referenceID": 23, "context": "We also evaluate the method on sentiment classification tasks, using the Cornell Sentence Polarity (CSP) (Pang and Lee, 2005) and IMDb movie review datasets (Maas et al.", "startOffset": 105, "endOffset": 125}, {"referenceID": 16, "context": "We also evaluate the method on sentiment classification tasks, using the Cornell Sentence Polarity (CSP) (Pang and Lee, 2005) and IMDb movie review datasets (Maas et al., 2011).", "startOffset": 157, "endOffset": 176}, {"referenceID": 31, "context": "As suggested by Zhu (2013), we posit that the information about the intrinsic geometry of documents, found in the persistence diagrams, might yield information that our classifier can leverage, either on its own or in combination with other representations.", "startOffset": 16, "endOffset": 27}, {"referenceID": 19, "context": "As a first step we compute word vectors for each document in our corpus using a word2vec (Mikolov et al., 2013) model trained on the Google ar X iv :1 70 5.", "startOffset": 89, "endOffset": 111}, {"referenceID": 19, "context": "In addition to being a widely used word embedding technique, word2vec has been known to exhibit interesting linear properties with respect to analogies (Mikolov et al., 2013), which hints at rich semantic structure.", "startOffset": 152, "endOffset": 174}, {"referenceID": 8, "context": "Hence, a more natural metric is the Gromov-Haussdorff distance (Gromov et al., 1981), simply defined as", "startOffset": 63, "endOffset": 84}, {"referenceID": 25, "context": "(2015) proposed a new method for computing a distance between documents based on an instance of the Earth Mover Distance (Rubner et al., 1998) called Word Mover Distance (WMD).", "startOffset": 121, "endOffset": 142}, {"referenceID": 14, "context": "Comparison to the Earth Mover Distance : Kusner et al. (2015) proposed a new method for computing a distance between documents based on an instance of the Earth Mover Distance (Rubner et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 18, "context": "Efficiently computing the GH distance is still an open problem despite a lot of recent work in this area (M\u00e9moli and Sapiro, 2005; Bronstein et al., 2006; M\u00e9moli, 2007; Agarwal et al., 2015).", "startOffset": 105, "endOffset": 190}, {"referenceID": 3, "context": "Efficiently computing the GH distance is still an open problem despite a lot of recent work in this area (M\u00e9moli and Sapiro, 2005; Bronstein et al., 2006; M\u00e9moli, 2007; Agarwal et al., 2015).", "startOffset": 105, "endOffset": 190}, {"referenceID": 17, "context": "Efficiently computing the GH distance is still an open problem despite a lot of recent work in this area (M\u00e9moli and Sapiro, 2005; Bronstein et al., 2006; M\u00e9moli, 2007; Agarwal et al., 2015).", "startOffset": 105, "endOffset": 190}, {"referenceID": 0, "context": "Efficiently computing the GH distance is still an open problem despite a lot of recent work in this area (M\u00e9moli and Sapiro, 2005; Bronstein et al., 2006; M\u00e9moli, 2007; Agarwal et al., 2015).", "startOffset": 105, "endOffset": 190}, {"referenceID": 4, "context": "Fortunately, Carri\u00e8re et al. (2015) provides us with a way to derive a signature which is stable with respect to the GH distance.", "startOffset": 13, "endOffset": 36}, {"referenceID": 28, "context": "The curious reader is invited to refer to Zhu (2013) for a short introduction.", "startOffset": 42, "endOffset": 53}, {"referenceID": 6, "context": "More details are in Delfinado and Edelsbrunner (1995); Edelsbrunner et al.", "startOffset": 20, "endOffset": 54}, {"referenceID": 6, "context": "More details are in Delfinado and Edelsbrunner (1995); Edelsbrunner et al. (2002); Robins (1999).", "startOffset": 20, "endOffset": 82}, {"referenceID": 6, "context": "More details are in Delfinado and Edelsbrunner (1995); Edelsbrunner et al. (2002); Robins (1999).", "startOffset": 20, "endOffset": 97}, {"referenceID": 4, "context": "However, as is argued in Carri\u00e8re et al. (2015), we can truncate the vectors to a dimension fixed across our dataset while preserving the stability property (albeit losing some of the representative ability of the signatures).", "startOffset": 25, "endOffset": 48}, {"referenceID": 1, "context": "Figure 3 shows the B-Cubed precision, recall and F1-Score of each method (metrics as defined in Amig\u00f3 et al. (2009)).", "startOffset": 96, "endOffset": 116}, {"referenceID": 16, "context": "We perform document-level binary sentiment classification on the IMDb Movie Reviews Dataset (Maas et al., 2011).", "startOffset": 92, "endOffset": 111}, {"referenceID": 15, "context": "We compare our results with the paragraph-vector approach (Le and Mikolov, 2014).", "startOffset": 58, "endOffset": 80}, {"referenceID": 13, "context": "tence/paragraph level Le and Mikolov (2014); Kiros et al.", "startOffset": 22, "endOffset": 44}, {"referenceID": 11, "context": "tence/paragraph level Le and Mikolov (2014); Kiros et al. (2015) or purely word vector based methods Arora et al.", "startOffset": 45, "endOffset": 65}, {"referenceID": 2, "context": "(2015) or purely word vector based methods Arora et al. (2017).", "startOffset": 43, "endOffset": 63}, {"referenceID": 28, "context": "Document classification and clustering (Willett, 1988; Hotho et al., 2005; Steinbach et al., 2000; Huang, 2008; Xu and Gong, 2004; Kuang et al., 2015; Miller et al., 2016) and sentiment classification (Nakagawa et al.", "startOffset": 39, "endOffset": 171}, {"referenceID": 9, "context": "Document classification and clustering (Willett, 1988; Hotho et al., 2005; Steinbach et al., 2000; Huang, 2008; Xu and Gong, 2004; Kuang et al., 2015; Miller et al., 2016) and sentiment classification (Nakagawa et al.", "startOffset": 39, "endOffset": 171}, {"referenceID": 26, "context": "Document classification and clustering (Willett, 1988; Hotho et al., 2005; Steinbach et al., 2000; Huang, 2008; Xu and Gong, 2004; Kuang et al., 2015; Miller et al., 2016) and sentiment classification (Nakagawa et al.", "startOffset": 39, "endOffset": 171}, {"referenceID": 10, "context": "Document classification and clustering (Willett, 1988; Hotho et al., 2005; Steinbach et al., 2000; Huang, 2008; Xu and Gong, 2004; Kuang et al., 2015; Miller et al., 2016) and sentiment classification (Nakagawa et al.", "startOffset": 39, "endOffset": 171}, {"referenceID": 30, "context": "Document classification and clustering (Willett, 1988; Hotho et al., 2005; Steinbach et al., 2000; Huang, 2008; Xu and Gong, 2004; Kuang et al., 2015; Miller et al., 2016) and sentiment classification (Nakagawa et al.", "startOffset": 39, "endOffset": 171}, {"referenceID": 13, "context": "Document classification and clustering (Willett, 1988; Hotho et al., 2005; Steinbach et al., 2000; Huang, 2008; Xu and Gong, 2004; Kuang et al., 2015; Miller et al., 2016) and sentiment classification (Nakagawa et al.", "startOffset": 39, "endOffset": 171}, {"referenceID": 20, "context": "Document classification and clustering (Willett, 1988; Hotho et al., 2005; Steinbach et al., 2000; Huang, 2008; Xu and Gong, 2004; Kuang et al., 2015; Miller et al., 2016) and sentiment classification (Nakagawa et al.", "startOffset": 39, "endOffset": 171}, {"referenceID": 22, "context": ", 2016) and sentiment classification (Nakagawa et al., 2010; Kim, 2014; Wang and Manning, 2012) are relatively well studied.", "startOffset": 37, "endOffset": 95}, {"referenceID": 11, "context": ", 2016) and sentiment classification (Nakagawa et al., 2010; Kim, 2014; Wang and Manning, 2012) are relatively well studied.", "startOffset": 37, "endOffset": 95}, {"referenceID": 27, "context": ", 2016) and sentiment classification (Nakagawa et al., 2010; Kim, 2014; Wang and Manning, 2012) are relatively well studied.", "startOffset": 37, "endOffset": 95}, {"referenceID": 5, "context": "Topological data analysis has been used for various tasks such as 3D shapes classification (Chazal et al., 2009) or protein structure analysis (Xia and Wei, 2014).", "startOffset": 91, "endOffset": 112}, {"referenceID": 29, "context": ", 2009) or protein structure analysis (Xia and Wei, 2014).", "startOffset": 38, "endOffset": 57}, {"referenceID": 5, "context": "Topological data analysis has been used for various tasks such as 3D shapes classification (Chazal et al., 2009) or protein structure analysis (Xia and Wei, 2014). However, such techniques have not been used in NLP, primarily because the theory is inaccessible and suitable applications are scarce. Zhu (2013) offers an introduction to using persistent homology in NLP, by creating representations of nursery-rhymes and novels, as well as highlights structural differences between child and adolescent writing.", "startOffset": 92, "endOffset": 310}], "year": 2017, "abstractText": "We investigate the pertinence of methods from algebraic topology for text data analysis. These methods enable the development of mathematically-principled isometric-invariant mappings from a set of vectors to a document embedding, which is stable with respect to the geometry of the document in the selected metric space. In this work, we evaluate the utility of these topology-based document representations in traditional NLP tasks, specifically document clustering and sentiment classification. We find that the embeddings do not benefit text analysis. In fact, performance is worse than simple techniques like tf-idf, indicating that the geometry of the document does not provide enough variability for classification on the basis of topic or sentiment in the chosen datasets.", "creator": "LaTeX with hyperref package"}}}