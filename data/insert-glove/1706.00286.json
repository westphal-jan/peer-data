{"id": "1706.00286", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "Learning to Compute Word Embeddings On the Fly", "abstract": "netterville Words in natural vizille language 137.0 follow a melodious Zipfian poage distribution whereby some n\u00f8rgaard words are gee-gees frequent but most are rare. Learning cotterets representations for words pangani in the \" gca long seabrooks tail \" olawale of dji-30 this northesk distribution requires kppu enormous rauschenbach amounts of data. Representations urokinase of rare words gasse trained kutiman directly on kilda end - tasks curcic are positronic usually poor, overnight requiring us to bodybuilding pre - train embeddings on abrons external prerogatives data, crocs or treat schul all rare hoogervorst words as out - 433,000 of - vocabulary naustdal words star with rosborough a unique heve representation. We slipped provide meloy a method sargasso for predicting burrup embeddings dafovska of rare icasa words gearloose on the zelin fly from ninny small wellhouse amounts complementary of speght auxiliary blushing data whisler with interpleader a network cetkovsk\u00e1 trained against wolfratshausen the yeylaq end cydf task. We 1989-1999 show lotta that this melech improves results suya against 3dnow baselines where ternate embeddings are dehsh trained on mattersdorf the end task in 21min a reading cataplexy comprehension task, immunological a astp recognizing textual emmaboda entailment pausch task, thirteen and dappy in bissix language modelling.", "histories": [["v1", "Thu, 1 Jun 2017 13:12:15 GMT  (198kb,D)", "http://arxiv.org/abs/1706.00286v1", null], ["v2", "Mon, 5 Jun 2017 20:18:27 GMT  (254kb,D)", "http://arxiv.org/abs/1706.00286v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["dzmitry bahdanau", "tom bosc", "stanis{\\l}aw jastrz\\k{e}bski", "edward grefenstette", "pascal vincent", "yoshua bengio"], "accepted": false, "id": "1706.00286"}, "pdf": {"name": "1706.00286.pdf", "metadata": {"source": "CRF", "title": "Learning to Compute Word Embeddings On the Fly", "authors": ["Dzmitry Bahdanau", "Tom Bosc"], "emails": ["bahdanau@iro.umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "Natural Language Understanding is a particularly difficult branch of artificial intelligence research. This is due in part to the fact that being able to comprehend, reason, and generate fluent natural language is the hallmark of human-level intelligence, but there are practical challenges as well. Although we can operate at finer levels of granularity, such as characters or phonemes, the base unit of language is frequently taken to be the word. This medium presents particular difficulties for machine learning because words are sparse features, for which there may be a paucity of data from which to learn good representations. Indeed, Natural Language yields a Zipfian distribution (Zipf, 1949) which tells us that a core set of words (at the head of the distribution) are frequent and ubiquitous, while a significantly larger number (in the long tail) are rare.\nThe typical solution to deal with this problem is to learn embeddings for some proportion of the head of the distribution, possibly shifted towards the domain-specific vocabulary of the dataset or task at hand, and to treat all other words as out-of-vocabulary (OOV), replacing them with an unknown word \u201cUNK\u201d token with a shared embedding. This essentially heuristic solution is inelegant, as words from technical domains, names of people, places, institutions, and so on will lack a specific representation unless sufficient data are available to justify inclusion in the vocabulary. This forces model designers to rely on overly large vocabularies, as observed by (Mi et al., 2016; Sennrich et al., 2015), which are parametrically expensive, or to employ vocabulary selection strategies (L\u2019Hostis et al., 2016). In both cases, we face the issue that words in the tail of the Zipfian distribution will typically still be too rare to learn good representations for through standard embedding methods. Some models, such as in the work of Ling et al. (2015), have sought to deal with the open vocabulary\nar X\niv :1\n70 6.\n00 28\n6v 1\n[ cs\n.L G\nproblem by obtaining representations of words from characters. This is successful at capturing the semantics of morphological derivations (e.g. \u201crunning\u201d from \u201crun\u201d) but puts significant pressure on the encoder to capture semantic distinctions amongst syntactically similar but semantically unrelated words (e.g. \u201crun\u201d vs. \u201crung\u201d). Additionally, nothing about the spelling of named entities, e.g. \u201cThe Beatles\u201d, tells you anything about their semantics (namely that they are a rock band).\nIn this paper, after reviewing related work in Section 2, we propose a new method for computing embeddings \u201con the fly\u201d, which jointly addresses the large vocabulary problem and the paucity of data for learning representations in the long tail of the Zipfian distribution. This method, which we illustrate in Figure 1 and explain in Section 3 can be summarized as follows: at a high level, instead of directly learning representations for all words in a potentially unbounded vocabulary, we learn a network which predicts the representations of words based on auxiliary data. Such auxiliary data need only satisfy the general requirement that it describe some aspect of the semantics of the word for which a representation is needed. Examples of such data could be dictionary definitions, Wikipedia infoboxes, linguistic descriptions of named entities obtained from Wikipedia articles, or something as simple as the spelling of a word. We will refer to the content of auxiliary data as \u201cdefinitions\u201d throughout the paper, regardless of the source. Several sources of auxiliary data can be used simultaneously as input to a neural network that will compute an associated representation. These representations can then be used as the alternative representation for out-of-vocabulary words, or combined with within-vocabulary word embeddings directly trained on the task of interest or obtained from an external data source (Mikolov et al., 2013; Pennington et al., 2014). In the present paper, we will focus on a subset of these approaches and auxiliary data sources, restricting ourselves to producing out-of-vocabulary words embeddings from dictionary data, spelling, or both. Crucially, the auxiliary data encoders are trained jointly with the objective, ensuring the preservation of semantic alignment with representations which are directly learned from the data.\nIn Section 4, we aim to demonstrate that this \u201con the fly\u201d embedding method provides improvements over models which learn embeddings directly from data. The obvious use case for this method would be datasets and tasks where there are many rare terms such as technical writing or bio/medical text (Del\u00e9ger et al., 2016). On such datasets, attempting to learn global vectors\u2014for example GloVe embeddings (Pennington et al., 2014)\u2014from external data, would only provide coverage for common words and would be unlikely to be exposed to sufficient (or any) examples of domain-specific technical terms to learn good enough representations. However, there are no (or significantly fewer) established neural network-based baselines on these tasks, which makes it harder to validate baseline results. Instead, we present results on a trio of tasks, namely reading comprehension, recognizing textual entailment, and a variant on language modelling. For each task, we compare baseline models with embeddings trained against the objective to those same models with our on the fly embedding method. Additionally, we report results for those models with pretrained GLoVe vectors as input, which we do not update. We aim to show how the gap in results between the baseline and the datarich GLoVe-based models can be partially but substantially closed merely through the introduction of relatively small amounts of auxiliary definitions. Quantitative results show that auxiliary data improves performance. Qualitative evaluation indicates our method allows models to draw and exploit connections defined in auxiliary data, along the lines of synonymy and semantic relatedness. We complete the paper in Section 5 by a discussion of the results across tasks, and suggesting further research directions using this model enhancement technique."}, {"heading": "2 Related Work", "text": "Arguably, the most popular approach for representing rare words is by using word embeddings trained on very large corpora of raw text. (Mikolov et al., 2013; Pennington et al., 2014). Such embeddings are typically explicitly or implicitly based on word co-occurence statistics. Being a big step forward from the models that are trained from scratch only on the task at hand, the approach can be criticized for being extremely data-hungry1. Obtaining the necessary amounts of data may be difficult, e.g. in technical domains. Besides, auxiliary training criteria used in the pretraining approaches are not guaranteed to yield representations that are useful for the task at hand.\nThere have been a number of attempts to achieve out-of-vocabulary generalization by relying on the spelling. (Ling et al., 2015) used a bidirectional LSTM to read the spelling of rare words and showed\n1The GLoVe embeddings used are computed using 840 billion words of English text.\nthat this can be helpful for language modeling and POS tagging. We too will investigate spelling as a source of auxiliary data. In this respect, the approach presented here subsumes theirs, and can be seen as a generalization to other types of definitions.\nThe closest to our work is the study by Hill et al. (2016), in which a recurrent network is trained to produce an embedding of a dictionary definition that is close to the embedding of the headword. The network is shown to be an effective reverse dictionary and a crossword solver. Our approach is different in that we train a dictionary reader in an end-to-end fashion for a specific task, side-stepping the potentially suboptimal auxiliary ranking cost that was used in that earlier work. Their method also relies on the availability of high-quality pretrained embeddings which might not always be the case. Another related work by Long et al. (2016) uses dictionary definitions to provide initialization to a database embedding method, which is different from directly learning to use the definitions like we do.\nAt a higher level our approach belongs to the a broad family of methods for conditioning neural networks on external knowledge. For example, Larochelle et al. (2008) propose to add classes to a classifier by representing them using their \u201cdescriptions\u201d. By description they meant, for example, a canonical picture of a printed character, that would represent all its possible handwritten versions. Their idea to rely on descriptions is similar to our idea to rely on definitions, however we focus on understanding complex inputs instead of adding new output classes.\nEnhancing word embeddings with auxiliary data from knowledge bases (including wordnet) has a long tradition (Xu et al., 2014; Faruqui et al., 2015). Our work differs from previous approaches in essential ways. First, we use a textual form and are not restricted to knowledge represented as a graph. Second, we learn in an end to end fashion, allowing the model to pick useful information for the task of interest.\nFinally, this work bears some relation, albeit distant, to the fast weights literature in the subfield of metalearning. In particular, Schmidhuber (1992) predicts updates to the weights of a \u201cfast\u201d network from the output of a \u201cslow\u201d network (which is trained by backpropagation) instead of updating them through gradient descent. Relatedly, Ha et al. (2016) explore architectures wherein an auxiliary network predicts the weights of a \u201cslow\u201d network directly. There are two key differences with the work presented here: first, the fast and slow networks observe the same input, whereas we explore scenarios whereby embeddings for one network are obtained from the output of another which observes auxiliary data; second, the auxiliary network predicts only embeddings, which can be seen as a subset of model parameters of the primary network, but leave the other parameters of the network to be learned as usual by gradient descent."}, {"heading": "3 On the Fly Embeddings", "text": "In general, a neural network processes a language input by replacing its elements xi, most often words, with the respective vectors e(xi), often called embeddings Bengio et al. (2003). Embeddings are typically either trained from scratch or pretrained. When embeddings are trained from scratch, a restricted vocabulary Vtrain = {w1, . . . , wn} is defined, usually based on training set frequency. Words not in Vtrain are replaced by a special token UNK with a trainable embedding e(UNK). Unseen test-time words w /\u2208 Vtrain are then represented by e(UNK), which effectively means the specific meaning of this word is lost. Even if w had been included in Vtrainbut was very rare, its learned embedding e(w) would likely not be very informative.\nThe approach proposed in this work, described in Figure 1, is to use definitions from auxiliary data, such as dictionaries, to compute embeddings of rare words. More specifically, this involves fetching a definition d(w) = (x\u20321, . . . , x \u2032 k) and feeding it into a network f that produces an embedding ed(w) = f(e \u2032(x\u20321), . . . , e\n\u2032(x\u2032k)). We will refer to ed(w) a definition embedding produced by a definition reader f . One can either use the same embeddings e\u2032 = e when reading the dictionary or train different ones. Likewise, one can either stick to a shared vocabulary Vdict = Vtrain, or consider two different ones. When a word x\u2032i 6\u2208 Vdict is encountered, it is replaced by UNK and the respective trainable embedding e\u2032(UNK) is used. For the function f we can consider two choices: a simple mean pooling ed(w) = \u2211k i=1 e\n\u2032(xi)/k or using the last state of an LSTM (Hochreiter and Schmidhuber, 1997), ed(w) = LSTM(e\u2032(x\u20321), . . . , e\n\u2032(x\u2032k)). Many words have multiple dictionary definitions. We combine embeddings for multiple definitions using mean pooling. We include all\ndefinitions whose headwords match w or any possible lemma of a lower-cased w2. To simplify the notation, the rest of the paper assumes that there is only one definition for each word.\nWhile the primary purpose of definition embeddings ed(w) is to inform the network about the rare words, they might also contain useful information for the words are in Vtrain. When we use both, we combine the information coming from the embeddings e(w) and the definition embeddings ed(w) by computing ec(w) = e(w) +Wed(w), where W is a trainable matrix, or just by simply summing the two, ec(w) = e(w) + ed(w). Alternatively it is possible to just use ec(w) = e(w) for w from Vtrain and ec(w) = ed(w) otherwise. When no definition is available for a word w, we posit that ed(w) is a zero vector. A crucial observation that makes an implementation of the proposed approach feasible is that the definitions d(xi) of all words xi from the input can be processed in parallel3.\nWe note, that there are least two aspects of our approach in which it can be considered very simplistic. First, we do not consider definition of word combinations, such as phrasal verbs like \"give up\" and geographical entities like \"San Francisco\". Second, our definition reader could better handle the unknown words w /\u2208 Vdict by using their definition embeddings ed(w) instead e\u2032(UNK), thereby implementing a form of recursion. We will investigate both in our future work."}, {"heading": "4 Experiments", "text": "We worked on extractive question answering, semantic entailment classification and language modelling. For each task, we picked as baseline model and architecture from the literature we knew would provide sensible results, to explore how augmenting it with on the fly embeddings would affect performance. We explored two complementary sources of auxiliary data. First, we used word definitions from WordNet (Miller, 1995). While WordNet is mostly known for its structured information about synonyms, it does contain natural language definitions for all its 147306 lemmas (this also includes multi-word headwords which we do consider in this work)4. Second, we experimented with the character-level spelling of words as auxiliary data. To this end, in order to fit in with our use of dictionaries, we added fake definitions of the form \u201cWord\"\u2192 \u201cW\", \u201co\", \u201cr\", \u201cd\". In order to measure the performance of models in \u201cdata-rich\u201d scenarios where large amount of unlabelled language data is available for the training of word representations, we used as pretrained word embeddings 300-dimensional GLoVe vectors trained on 840 billion words (Pennington et al., 2014). We compared our auxiliary data-augmented on the fly embedding technique to baselines and models with fixed GLoVe embeddings to measure how well our technique closes the gap between a data-poor and data-rich scenario.\n2We used the wordnet-based lemmatizer from NLTK. 3The same applies for all the words from a mini-batch of examples. By composing huge batches of up 10000 definitions from a mini-batch of examples, we were able to process them all in a reasonable time on GPUs. 4Advantages of using WordNet include its free availability and the ease of parsing, e.g., we used the NLTK (Bird, 2006) interface to extract the definitions."}, {"heading": "4.1 Question Answering", "text": "We used the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) that consists of approximately 100000 human-generated question-answer pairs. For each pair, a paragraph from Wikipedia is provided that contains the answer as a continuous span of words.\nOur basic model is a simplified version of a coattention proposed in (Xiong et al., 2016). First, we represent the context of length n and the question of length m as matrices C \u2208 Rn,d and Q \u2208 Rm,d by running them through an LSTM and a linear transform. Next, we compute the affinity scores L = CQT . By normalizing L with row-wise and column-wise softmaxes we construct a contextto-question and question-to-context attention maps AC and AQ. These are used to construct a joint question-document representation U0 as a concatenation along the feature axis of the matrices C, ACQ and ACATQC. We transform U0 with a bidirectional LSTM and another ReLU(Glorot et al., 2011) layer to obtain the final context-document representation U2. Finally, two linear layers followed by a softmax assign to each position of the document probabilities of it being the beginning and the end of the answer span. We refer the reader to the work of Xiong et al. (2016) for more details. Compared to their model, the two main simplifications that we applied is skipping the iterative inference procedure and using usual ReLU instead of the highway-maxout units.\nOur baseline is a model with the embeddings trained purely for scratch. We found that it performs best with a very small vocabulary of 3k most common words from the training set. This can be explained by a rather moderate size of the dataset: all the models we tried tended to overfit severely. In addition to using the spelling and the dictionary definitions we tried mixing the dictionary definitions with the spelling. The last model in our comparison is trained with the GLoVe embeddings. In this round of experiments we only used definitions for the out-of-vocabulary words.\nThe results are reported in Table 1. We report the exact match ratio as computed by the evaluation tools provided with the dataset, which is basically the accuracy of selecting the right answer. For the development set we report the average over four runs, for the test set we could evaluate only one model because the test was not released and evaluation was done manually by the dataset authors. Looking at the development set results one can see that the basic model (B) performs quite poorly, and adding any external information boosts the accuracy significantly (7.5% - 13.5%). Adding the spelling (S) helps more than adding a dictionary (D), but the model that has both (SD) has a 1.3% advantage over the model that uses just the spelling (S), demonstrating that combining several forms of auxiliary data allows the model to exploit the complementary information they provide. The model with GLoVe embeddings (G) is still ahead with a 3.3% margin, but the gap has been more than halved. Finally, the test set results confirm the main takeaway that SD performs the best, even though the relative order of S and D is different.\nFurthermore, we compared the models G and SD in a similar way. We found that often SD simply missed a definition. For a example, it was not able to match \u201cXBox\u201d and \u201cconsole\u201d, \u201cCoronation\u201d5 and \u201cshow\u201d, \u201cmost-watched\u201d and \u201cmost watched\u201d. We also saw cases where the definition was available but was not used, seemingly because the key word in the definition was outside Vtrain = Vdict. For example, \u201carrow\u201d was defined \u201ca projectile with a straight thin shaft\u201d, and the word \u201cprojectile\u201d was quite rare in the training corpus. As a consequence, the model had no chances to understand that an arrow is a weapon and match the word \u201carrow\u201d in the context with the word \u201cweapon\u201d in the question. Finally, we saw cases where inferring important aspects of meaning from the dictionary would be non-trivial, for example, guessing that \u201chistorian\u201d is a \u201cprofession\u201d from the definition \u201ca person who is an authority on history and who studies it and writes about it\u201d would involve serious common sense reasoning."}, {"heading": "4.2 Entailment prediction", "text": "We used Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), which consists of around 500k pairs of sentences (hypothesis and premise) and the task is to predict the logical relation (contradiction, neutral or entailment) between them. Our first model (denoted as \"simple\" in Table 2) is a slightly improved baseline from (Bowman et al., 2015). This model first computes an embedding of the two sentences by summing word vectors and concatenates them forming h \u2208 R2D embedding. The resulting representation is fed to a 2 layer ReLU MLP normalized by Batch Normalization (Ioffe and Szegedy, 2015).\nIn order to validate that improvements translate to state of the art models we implemented a variant (replacing TreeLSTM by biLSTM) of Enhanced Sequential Inference Model (ESIM) (Chen et al., 2016) that achieves close to SOTA accuracy. Similarly as in the model used in SQuAD experiments, this model represents hypothesis and premise as H \u2208 Rn,d and P \u2208 Rm,d matrices by encoding them using bidirectional LSTM. Analogously, alignment matrices AH and AP are computed by normalizing affinity scores. The alignment matrices are used to form joint hypothesis-premise representation. For hypothesis we compute and concatenate H , AHP , H \u2212 AHP and H AHP and repeat the same operations for premise forming h \u2208 Rn,4D sentence embedding for each. These sentence representations are processed in parallel by bidirectional LSTM and the final states of the LSTMs are concatenated and used to predict entailment after processing by single layer Tanh MLP. Dropout is applied on word embeddings in the case of all models (with tuned rate), similarly as in SQuAD experiments we use small vocabulary of size 3000 or 5000. We never compute definition embeddings for top 1k words. Definition and word embeddings are combined using simple sum. All runs were repeated 3 times and scores are averaged.\nResults of the simple and ESIM models are presented in Table 2. We include as baseline model using only embeddings trained from scratch and dictionary composed of word spellings. Results are directionally similar to results on SQuAD, auxiliary definitions bring 0.19% and 1.2% margin\n5\u201cCoronation Street\u201d is a popular British TV show.\nwhen used with ESIM and baseline models respectively6. One difference is that spelling was not as important for good performance on SNLI (spelling showed a smaller margin, and adding it to the dictionary definitions was also less helpful).\nIn order to aid understanding of the SNLI models performance we first plot t-SNE (van der Maaten and Hinton, 2008) of word embeddings computed using definitions, see Figure 3. We can see, for instance, that \u201cbomber\u201d 7 is close both to words like \u201cjet\u201d and \u201cpeach\u201d, which would be challenging to learn using standard word embeddings. We also note that, as expected, dictionary-enabled models significantly outperform baseline models at sentences containing rare words, see Figure 3."}, {"heading": "4.3 Language Modelling", "text": "In our first two experiments we used datasets of moderate size. To get an idea of how useful our auxiliary data sources will be on larger datasets, we conduct an experiment on the One Billion Words language modeling task (Chelba et al., 2014). Similarly to prior work on using the spelling (Ling et al., 2015) we restrict the softmax output layer to only predict probabilities of the 10k most common words, however, we do not impose such a constraint when the model reads the words. While such language models are not, strictly speaking, generative models, one can still study if having a definition of an observed word helps the model to predict the following ones from a restricted vocabulary.\nOur baseline model is an LSTM with 500 units and it has trainable input embeddings for 10k input words. This covers around 90.24% of all the word occurrences. We consider predicting the embeddings of the other input words using GLoVe vector (G), spelling (S) and dictionary definitions (D-*). Dictionary definitions were only available for 63.35% of the rest of occurences whereas GloVe\n6If the model selection is done using the dev set performance. 7A bomber is both an aircraft and a type of sandwich.\ncovers 97.43% of the rest and spelling is available everywhere. Thus, in order to compare how helpful the different auxiliary sources of information are, we tried restricted variants of GLoVe and spelling experiments (S-R, G-R, T-R), in which the embeddings or the spelling were only provided where a definition was available. We tried three varieties of models with the dictionary. In first one (D-shared), the same LSTM was used for reading the text and the dictionaries. In the second one (D-separate), separate LSTMs were used for these purposes, but they still shared the word embeddings. We also tried building a separate vocabulary to better cover the words from the dictionary definitions (D-separate vocab.). Lastly, we trained a model that had trainable embeddings for all the words with definitions as a reference.\nWe report the validation perplexities measured at iteration 200k in Table 3. Unsurprisingly, using external information to compute embeddings of unknown words helps in all cases. We observe a significant gain (3.6 units) even when for D-shared, which is remarkable as this model has the same parameters as the baseline. The perplexities of the best dictionary-equipped (D-shared) model and the restricted variants of other models (S-R, G-R) ended up being close (from 48.24 to 48.51) and not far from the reference (T-R). Thus, for the same coverage, the 3 models with additional information are comparable in performance. The regular perplexity is heavily influenced by how well the models handle the frequent words. To zoom in on how the models deal with rare words, we look at the perplexities of the words that (a) follow right after a word outside of the vocabulary (b) follow right after a word for which a definition is available. Interestingly, GLoVe embeddings were less helpful in situation (b) (42.53 vs 35.92). We note the strong performance of the models using spelling according to all our metrics. In our nearest future work we will try combine spelling with dictionary similarly to the first two experiments."}, {"heading": "5 Discussion", "text": "We showed how different sources of auxiliary information, such as the spelling and a dictionary of definitions can be used to produce on the fly useful embeddings for rare words. While it was known before that adding the spelling information to the model is helpful, it is often hard or not possible to infer the meaning directly from the characters. Our more general approach offers endless possibilities of adding other data sources and learning end-to-end to extract the relevant bits information from them. Our experiments with a dictionary of definitions show the feasibility of the approach, as we report improvements over using just the spelling on question answering and semantic entailment classification tasks. Our qualitative investigations on the question answering data confirms our intuition on where the improvement comes from. It is also clear from them that adding more auxiliary data would help, and that it would probably be also useful to add definitions not just for words, but also for phrases (see \u201cCoronation Street\u201d from Section 4.1. We are planning to add more data sources (e.g. first sentences from Wikipedia articles) and better use the available ones (WordNet has definitions of phrasal verbs like \"come across\") in our future work.\nAn important question that we did not touch in this paper is how to deal with rare words in the auxiliary information, such as dictionary definitions. Based on our qualitative investigations (see the example with \u201carrow\u201d and \u201cweapon\u201d in Section 4.1), we believe that better handling rare words in the auxiliary information could substantially improve the proposed method. It would be natural to use on the fly embeddings similarly to the ones that we produce for words from the input, but a\nstraight-forward approach of computing them on request would be very computation and memory hungry. One would furthermore have to resolve cyclical dependencies, which are unfortunately common in dictionary data (when e.g. \u201centertainment\u201d is defined using \u201cdiverting\u201d and \u201cdiverting\u201d is defined using \u201centertainment\u201d). In our future work we want to investigate asynchronous training of on the fly embeddings and the main model."}, {"heading": "6 Conclusion", "text": "In this paper, we have shown that introducing relatively small amounts of auxiliary data and a method for computing embeddings on the fly from these data bridges the gap between data-poor setups, where embeddings need to be learned directly from the end task, and data-rich setups, where embeddings can be pretrained and sufficient external data exists to ensure in-domain lexical coverage. A large representative corpus to pretrain word embeddings is not always available and our method is applicable when one has access only to limited auxiliary data. Learning end-to-end from auxiliary sources can be extremely data efficient when these sources represent compressed relevant information about the word, as dictionary definitions do. A related desirable aspect of our approach is that it may partially return the control over what a language processing system does into the hands of engineers or even users: when dissatisfied with the output, they may edit or add auxiliary information to the system to make it perform as desired. Furthermore, domain adaptation with our method could be carried out simply by using other sources of auxiliary knowledge, for example definitions of domain-specific technical terms in order to understand medical text. Overall, the aforementioned properties of our method make it a promising alternative to the existing approaches to handling rare words."}, {"heading": "Acknowledgements", "text": "We thank the developers of Theano (Theano Development Team, 2016) and Blocks (van Merri\u00ebnboer et al., 2015) for their great work. We thank NVIDIA for giving access to their DGX-1 computers used in this work. Stanis\u0142aw Jastrze\u0328bski was supported by Grant No. DI 2014/016644 from Ministry of Science and Higher Education, Poland. We thank \u00c7ag\u0306lar G\u00fcl\u00e7ehre and Alexandre Lacoste for useful discussions."}], "references": [{"title": "How we blessed distributional semantic evaluation", "author": ["M. References Baroni", "A. Lenci"], "venue": "Proceedings of the", "citeRegEx": "Baroni and Lenci,? 2011", "shortCiteRegEx": "Baroni and Lenci", "year": 2011}, {"title": "A neural probabilistic language model", "author": ["PA Stroudsburg", "Y. USA. Association for Computational Linguistics. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "Journal of", "citeRegEx": "Stroudsburg et al\\.,? 2003", "shortCiteRegEx": "Stroudsburg et al\\.", "year": 2003}, {"title": "A large annotated corpus for learning natural", "author": ["S.R. Bowman", "G. Angeli", "C. Potts", "C.D. Manning"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Enhancing and combining sequential and tree LSTM", "author": ["Q. Chen", "X. Zhu", "Z. Ling", "S. Wei", "H. Jiang"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Retrofitting word vectors", "author": ["M. Linguistics. Faruqui", "J. Dodge", "S.K. Jauhar", "C. Dyer", "E.H. Hovy", "N.A. Smith"], "venue": null, "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Aistats, volume 15, page 275.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Hypernetworks", "author": ["D. Ha", "A. Dai", "Q.V. Le"], "venue": "arXiv preprint arXiv:1609.09106.", "citeRegEx": "Ha et al\\.,? 2016", "shortCiteRegEx": "Ha et al\\.", "year": 2016}, {"title": "Learning to understand phrases by embedding the dictionary", "author": ["F. Hill", "K. Cho", "A. Korhonen", "Y. Bengio"], "venue": "Transactions of the Association for Computational Linguistics, 4:17\u201330.", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 448\u2013456.", "citeRegEx": "Ioffe and Szegedy,? 2015", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Zero-data learning of new tasks", "author": ["H. Larochelle", "D. Erhan", "Y. Bengio"], "venue": "Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence, AAAI 2008, Chicago, Illinois, USA, July 13-17, 2008, pages 646\u2013651.", "citeRegEx": "Larochelle et al\\.,? 2008", "shortCiteRegEx": "Larochelle et al\\.", "year": 2008}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["W. Ling", "T. Lu\u00eds", "L. Marujo", "R.F. Astudillo", "S. Amir", "C. Dyer", "A.W. Black", "I. Trancoso"], "venue": "arXiv preprint arXiv:1508.02096.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Leveraging lexical resources for learning entity embeddings in multi-relational data", "author": ["T. Long", "R. Lowe", "J.C.K. Cheung", "D. Precup"], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short Papers.", "citeRegEx": "Long et al\\.,? 2016", "shortCiteRegEx": "Long et al\\.", "year": 2016}, {"title": "Vocabulary selection strategies for neural machine translation", "author": ["G. L\u2019Hostis", "D. Grangier", "M. Auli"], "venue": null, "citeRegEx": "L.Hostis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "L.Hostis et al\\.", "year": 2016}, {"title": "Vocabulary manipulation for neural machine translation", "author": ["H. Mi", "Z. Wang", "A. Ittycheriah"], "venue": "arXiv preprint arXiv:1605.03209.", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: A lexical database for english", "author": ["G.A. Miller"], "venue": "Communications of ACM, 38(11):39\u201341.", "citeRegEx": "Miller,? 1995", "shortCiteRegEx": "Miller", "year": 1995}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100, 000+ questions for machine comprehension of text", "author": ["P. Rajpurkar", "J. Zhang", "K. Lopyrev", "P. Liang"], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 2383\u20132392.", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Learning to control fast-weight memories: An alternative to dynamic recurrent networks", "author": ["J. Schmidhuber"], "venue": "Neural Computation, 4(1):131\u2013139.", "citeRegEx": "Schmidhuber,? 1992", "shortCiteRegEx": "Schmidhuber", "year": 1992}, {"title": "Neural machine translation of rare words with subword units", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "arXiv preprint arXiv:1508.07909.", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv e-prints, abs/1605.02688.", "citeRegEx": "Team,? 2016", "shortCiteRegEx": "Team", "year": 2016}, {"title": "Visualizing high-dimensional data using t-sne", "author": ["L. van der Maaten", "G. Hinton"], "venue": null, "citeRegEx": "Maaten and Hinton,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton", "year": 2008}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["B. van Merri\u00ebnboer", "D. Bahdanau", "V. Dumoulin", "D. Serdyuk", "D. Warde-Farley", "J. Chorowski", "Y. Bengio"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "Dynamic coattention networks for question answering", "author": ["C. Xiong", "V. Zhong", "R. Socher"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Rc-net: A general framework for incorporating knowledge into word representations", "author": ["C. Xu", "Y. Bai", "J. Bian", "B. Gao", "G. Wang", "X. Liu", "Liu", "T.-Y."], "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM \u201914, pages 1219\u20131228, New York, NY, USA. ACM.", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Human behavior and the principle of least effort: An introduction to human ecology", "author": ["G.K. Zipf"], "venue": null, "citeRegEx": "Zipf,? \\Q1949\\E", "shortCiteRegEx": "Zipf", "year": 1949}], "referenceMentions": [{"referenceID": 26, "context": "Indeed, Natural Language yields a Zipfian distribution (Zipf, 1949) which tells us that a core set of words (at the head of the distribution) are frequent and ubiquitous, while a significantly larger number (in the long tail) are rare.", "startOffset": 55, "endOffset": 67}, {"referenceID": 14, "context": "This forces model designers to rely on overly large vocabularies, as observed by (Mi et al., 2016; Sennrich et al., 2015), which are parametrically expensive, or to employ vocabulary selection strategies (L\u2019Hostis et al.", "startOffset": 81, "endOffset": 121}, {"referenceID": 20, "context": "This forces model designers to rely on overly large vocabularies, as observed by (Mi et al., 2016; Sennrich et al., 2015), which are parametrically expensive, or to employ vocabulary selection strategies (L\u2019Hostis et al.", "startOffset": 81, "endOffset": 121}, {"referenceID": 13, "context": ", 2015), which are parametrically expensive, or to employ vocabulary selection strategies (L\u2019Hostis et al., 2016).", "startOffset": 90, "endOffset": 113}, {"referenceID": 11, "context": "Some models, such as in the work of Ling et al. (2015), have sought to deal with the open vocabulary ar X iv :1 70 6.", "startOffset": 36, "endOffset": 55}, {"referenceID": 15, "context": "These representations can then be used as the alternative representation for out-of-vocabulary words, or combined with within-vocabulary word embeddings directly trained on the task of interest or obtained from an external data source (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 235, "endOffset": 282}, {"referenceID": 17, "context": "These representations can then be used as the alternative representation for out-of-vocabulary words, or combined with within-vocabulary word embeddings directly trained on the task of interest or obtained from an external data source (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 235, "endOffset": 282}, {"referenceID": 17, "context": "On such datasets, attempting to learn global vectors\u2014for example GloVe embeddings (Pennington et al., 2014)\u2014from external data, would only provide coverage for common words and would be unlikely to be exposed to sufficient (or any) examples of domain-specific technical terms to learn good enough representations.", "startOffset": 82, "endOffset": 107}, {"referenceID": 15, "context": "(Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 0, "endOffset": 47}, {"referenceID": 17, "context": "(Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 0, "endOffset": 47}, {"referenceID": 11, "context": "(Ling et al., 2015) used a bidirectional LSTM to read the spelling of rare words and showed", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "The closest to our work is the study by Hill et al. (2016), in which a recurrent network is trained to produce an embedding of a dictionary definition that is close to the embedding of the headword.", "startOffset": 40, "endOffset": 59}, {"referenceID": 7, "context": "The closest to our work is the study by Hill et al. (2016), in which a recurrent network is trained to produce an embedding of a dictionary definition that is close to the embedding of the headword. The network is shown to be an effective reverse dictionary and a crossword solver. Our approach is different in that we train a dictionary reader in an end-to-end fashion for a specific task, side-stepping the potentially suboptimal auxiliary ranking cost that was used in that earlier work. Their method also relies on the availability of high-quality pretrained embeddings which might not always be the case. Another related work by Long et al. (2016) uses dictionary definitions to provide initialization to a database embedding method, which is different from directly learning to use the definitions like we do.", "startOffset": 40, "endOffset": 653}, {"referenceID": 10, "context": "For example, Larochelle et al. (2008) propose to add classes to a classifier by representing them using their \u201cdescriptions\u201d.", "startOffset": 13, "endOffset": 38}, {"referenceID": 25, "context": "Enhancing word embeddings with auxiliary data from knowledge bases (including wordnet) has a long tradition (Xu et al., 2014; Faruqui et al., 2015).", "startOffset": 108, "endOffset": 147}, {"referenceID": 4, "context": "Enhancing word embeddings with auxiliary data from knowledge bases (including wordnet) has a long tradition (Xu et al., 2014; Faruqui et al., 2015).", "startOffset": 108, "endOffset": 147}, {"referenceID": 18, "context": "In particular, Schmidhuber (1992) predicts updates to the weights of a \u201cfast\u201d network from the output of a \u201cslow\u201d network (which is trained by backpropagation) instead of updating them through gradient descent.", "startOffset": 15, "endOffset": 34}, {"referenceID": 6, "context": "Relatedly, Ha et al. (2016) explore architectures wherein an auxiliary network predicts the weights of a \u201cslow\u201d network directly.", "startOffset": 11, "endOffset": 28}, {"referenceID": 8, "context": "For the function f we can consider two choices: a simple mean pooling ed(w) = \u2211k i=1 e (xi)/k or using the last state of an LSTM (Hochreiter and Schmidhuber, 1997), ed(w) = LSTM(e(x1), .", "startOffset": 129, "endOffset": 163}, {"referenceID": 16, "context": "First, we used word definitions from WordNet (Miller, 1995).", "startOffset": 45, "endOffset": 59}, {"referenceID": 17, "context": "In order to measure the performance of models in \u201cdata-rich\u201d scenarios where large amount of unlabelled language data is available for the training of word representations, we used as pretrained word embeddings 300-dimensional GLoVe vectors trained on 840 billion words (Pennington et al., 2014).", "startOffset": 270, "endOffset": 295}, {"referenceID": 18, "context": "We used the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) that consists of approximately 100000 human-generated question-answer pairs.", "startOffset": 56, "endOffset": 80}, {"referenceID": 24, "context": "Our basic model is a simplified version of a coattention proposed in (Xiong et al., 2016).", "startOffset": 69, "endOffset": 89}, {"referenceID": 5, "context": "We transform U0 with a bidirectional LSTM and another ReLU(Glorot et al., 2011) layer to obtain the final context-document representation U2.", "startOffset": 58, "endOffset": 79}, {"referenceID": 5, "context": "We transform U0 with a bidirectional LSTM and another ReLU(Glorot et al., 2011) layer to obtain the final context-document representation U2. Finally, two linear layers followed by a softmax assign to each position of the document probabilities of it being the beginning and the end of the answer span. We refer the reader to the work of Xiong et al. (2016) for more details.", "startOffset": 59, "endOffset": 358}, {"referenceID": 2, "context": "We used Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), which consists of around 500k pairs of sentences (hypothesis and premise) and the task is to predict the logical relation (contradiction, neutral or entailment) between them.", "startOffset": 58, "endOffset": 79}, {"referenceID": 2, "context": "Our first model (denoted as \"simple\" in Table 2) is a slightly improved baseline from (Bowman et al., 2015).", "startOffset": 86, "endOffset": 107}, {"referenceID": 9, "context": "The resulting representation is fed to a 2 layer ReLU MLP normalized by Batch Normalization (Ioffe and Szegedy, 2015).", "startOffset": 92, "endOffset": 117}, {"referenceID": 3, "context": "In order to validate that improvements translate to state of the art models we implemented a variant (replacing TreeLSTM by biLSTM) of Enhanced Sequential Inference Model (ESIM) (Chen et al., 2016) that achieves close to SOTA accuracy.", "startOffset": 178, "endOffset": 197}, {"referenceID": 0, "context": "Right plot shows tSNE of word embeddings for 3 random categories from BLESS dataset (Baroni and Lenci, 2011) (denoted by color).", "startOffset": 84, "endOffset": 108}, {"referenceID": 11, "context": "Similarly to prior work on using the spelling (Ling et al., 2015) we restrict the softmax output layer to only predict probabilities of the 10k most common words, however, we do not impose such a constraint when the model reads the words.", "startOffset": 46, "endOffset": 65}], "year": 2017, "abstractText": "Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare. Learning representations for words in the \u201clong tail\u201d of this distribution requires enormous amounts of data. Representations of rare words trained directly on end-tasks are usually poor, requiring us to pre-train embeddings on external data, or treat all rare words as out-of-vocabulary words with a unique representation. We provide a method for predicting embeddings of rare words on the fly from small amounts of auxiliary data with a network trained against the end task. We show that this improves results against baselines where embeddings are trained on the end task in a reading comprehension task, a recognizing textual entailment task, and in language modelling.", "creator": "LaTeX with hyperref package"}}}