{"id": "1706.01303", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "The Singularity May Be Near", "abstract": "Toby Walsh kaye in ' 18.6-billion The spookiest Singularity latheef May waystation Never Be Near ' excitotoxicity gives leijonborg six arguments to mcfoy support wbrc his point farda of 2.52 view that technological wilting singularity may happen miscellanies but koenig that it is predictor unlikely. valdaysky In this xia paper, dorries we provide sawgrass analysis sisteron of each photoperiod one thali of puerner his arguments and 11-foot arrive posies at stoupe similar ncu conclusions, but with more schaden weight gastonia given to efrem the ' likely zavackas to maynila happen ' probability.", "histories": [["v1", "Wed, 31 May 2017 19:42:06 GMT  (234kb)", "http://arxiv.org/abs/1706.01303v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CY", "authors": ["roman v yampolskiy"], "accepted": false, "id": "1706.01303"}, "pdf": {"name": "1706.01303.pdf", "metadata": {"source": "CRF", "title": "The Singularity May Be Near", "authors": ["Roman V. Yampolskiy"], "emails": ["roman.yampolskiy@louisville.edu"], "sections": [{"heading": null, "text": "Toby Walsh in \u201cThe Singularity May Never Be Near\u201d gives six arguments to support his point of view that technological singularity may happen but that it is unlikely. In this paper, we provide analysis of each one of his arguments and arrive at similar conclusions, but with more weight given to the \u201clikely to happen\u201d probability.\nKeywords: Autogenous intelligence, Bootstrap fallacy; Recursive self-improvement, selfmodifying software, Singularity;"}, {"heading": "1. Introduction", "text": "In February of 2016 Toby Walsh presented his paper \u201cThe Singularity May Never Be Near\u201d at AAAI16 [23], which was archived on February 20, 2016 (http://arxiv.org/abs/1602.06462). In it, Walsh analyzes the concept of technological singularity. He does not argue that AI will fail to achieve super-human intelligence; rather he is suggesting that it may not lead to the runaway exponential growth. Walsh defends his view via six different arguments.\nAlmost exactly a year before, on February 23, 2015, Roman Yampolskiy archived his paper \u201cFrom Seed AI to Technological Singularity via Recursively Self-Improving Software\u201d[28] (http://arxiv.org/abs/1502.06512) which was subsequently published as two peer-reviewed papers at AGI15 [26, 29]. In it, Yampolskiy makes arguments similar to those made by Walsh, but also considers evidence in favor of intelligence explosion. Yampolskiy\u2019s conclusion is that Singularity may not happen but leans more toward it happening. In the next section, we present arguments from the original paper by Yampolskiy mapped to each of the six arguments given by Walsh in his work."}, {"heading": "2. Contrasting Yampolskiy\u2019s and Walsh\u2019s Arguments", "text": "To make it easier to contrast arguments derived from On the Limits of Recursively Self-Improving Artificially Intelligent Systems [28] we use Walsh\u2019s naming of arguments even if our analysis doesn\u2019t rely on the same example (ex. No dog)."}, {"heading": "Fast Thinking Dog", "text": "Walsh argues: \u201c\u2026 speed alone does not bring increased intelligence\u201d, and Yampolskiy says: \u201cIn practice performance of almost any system can be trivially improved by allocation of additional computational resources such as more memory, higher sensor resolution, faster processor or\ndefinition the system would have to engineer a faster type of memory not just purchase more memory units of the type it already has access to. In general hardware improvements are likely to speed up the system, while software improvements (novel algorithms) are necessary for achievement of meta-improvements.\u201d It is clear from the original paper that performance in this context is the same as intelligence and as most of our intelligence testing tools (IQ tests) are time based, increased speed would in fact lead to higher Intelligence Quotient, at least in terms of how we currently access intelligence."}, {"heading": "Anthropocentric", "text": "Walsh argues: \u201c\u2026 that human intelligence is itself nothing special\u201d, and Yampolskiy says: \u201cWe still don\u2019t know the minimum intelligence necessary for commencing the RSI [Recursive SelfImprovement] process, but we can [argue] that it would be on par with human intelligence which we associate with universal or general intelligence [13], though in principal a sub-human level system capable of self-improvement can\u2019t be excluded [6]. One may argue that even human level capability is not enough because we already have programmers (people or their intellectual equivalence formalized as functions [18] or Human Oracles [24, 25]) who have access to their own source code (DNA), but who fail to understand how DNA (nature) works to create their intelligence. This doesn\u2019t even include additional complexity in trying to improve on existing DNA code or complicating factors presented by the impact of learning environment (nurture) on development of human intelligence. Worse yet, it is not obvious how much above human ability an AI needs to be to begin overcoming the \u201ccomplexity barrier\u201d associated with selfunderstanding.\u201d"}, {"heading": "Meta-intelligence", "text": "Walsh argues: \u201c\u2026strongest arguments against the idea of a technological singularity in my view is that it confuses intelligence to do a task with the capability to improve your intelligence to do a task\u201d and cites a quote from Chalmers [6] as an example - \u201cIf we produce an AI by machine learning, it is likely that soon after we will be able to improve the learning algorithm and extend the learning process, leading to AI+\u201d. Yampolskiy says: \u201cChalmers [6] uses logic and mathematical induction to show that if an AI0 system is capable of producing only slightly more capable AI1 system generalization of that process leads to superintelligent performance in AIn after n generations. He articulates, that his proof assumes that the proportionality thesis, which states that increases in intelligence lead to proportionate increases in the capacity to design future generations of AIs, is true.\u201d"}, {"heading": "Diminishing returns", "text": "Walsh argues: \u201cThere is often lots of low hanging fruit at the start, but we then run into great difficulties to improve after this. \u2026 An AI system may be able to improve itself an infinite number of times, but the extent to which its intelligence changes overall could be bounded.\u201d Yampolskiy says, \u201c\u2026 the law of diminishing returns quickly sets in and after an initial significant improvement phase, characterized by discovery of \u201clow-hanging fruit\u201d, future improvements are likely to be less frequent and less significant, producing a Bell curve of valuable changes.\u201d\nlimits in great detail \u201cFirst of all, any implemented software system relies on hardware for memory, communication and information processing needs even if we assume that it will take a non-Von Neumann (quantum) architecture to run such software. This creates strict theoretical limits to computation, which despite hardware advances predicted by Moore\u2019s law will not be overcome by any future hardware paradigm. Bremermann [4], Bekenstein [2], Lloyd [12], Anders [16], Aaronson [1], Shannon [19], Krauss [10], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.\u201d \u201cIn addition to limitations endemic to hardware, software-related limitations may present even bigger obstacles for RSI systems. Intelligence is not measured as a standalone value but with respect to the problems it allows to solve. For many problems such as playing checkers [17] it is possible to completely solve the problem (provide an optimal solution after considering all possible options) after which no additional performance improvement would be possible [14].\u201d"}, {"heading": "Computational complexity", "text": "Walsh argues: \u201c\u2026 no amount of growth in performance will make undecidable problems decidable\u201d and Yampolskiy says, \u201cOther problems are known to be unsolvable regardless of level of intelligence applied to them [22]. Assuming separation of complexity classes (such as P vs NP) holds [27], it becomes obvious that certain classes of problems will always remain only approximately solvable and any improvements in solutions will come from additional hardware resources not higher intelligence.\u201d"}, {"heading": "3. Response to Walsh\u2019s Arguments", "text": "In this section we provide novel analysis of all six arguments presented by Walsh and, via mapping provided in the previous section, revisit and critically analyze arguments made by Yampolskiy."}, {"heading": "Fast Thinking Dog", "text": "The argument intuitively makes sense, since nobody ever managed to train a dog to play chess. However, intuition is no match for a scientific experiment. Animals have successfully been trained to understand and even use human (sign) language and do some basic math. People with mental and learning disabilities, who have been long considered a \u201clost cause\u201d, have been successfully trained to perform very complex behaviors via alternative teaching methods and longer training spans. It is entirely possible that if one had thousands of years to train a dog it would learn to play a decent game of chess, after all it has a neural network very similar to the one used by humans and deep learning AI. It may be argued, that there is considerable evidence that language and some other capabilities are functions of specific brain structures that are largely absent from a dog. Thus, 1000s of years of training won't cut it and one would need millions of years of evolution to get a human-level intelligent dog. However some recent research has documented that people missing most of their brain could have near normal cognitive capacity [8] and even significant damage to parts of the brain could be overcome due to neuroplasticity [9] suggesting that brain structures are much more general. To transfer an analogy to another domain, Intel286 processor is not fast enough to do life speech recognition, but if you speed it up it is. Until an actual experiment can be performed on an accurately simulated digital dog, this argument will remain as nothing but speculation.\nis special is not because of anthropocentric bias, but because of the Church-Turing Thesis (CTT). CTT states that a function over natural numbers is computable by a prototypical human being if and only if it is computable by some Turing Machine (TM), assuming such theoretical human has infinite computational resources similar to an infinite tape available to a TM. This creates equivalence between human level intelligence and a Universal Turing Machine, which is a very special machine in terms of its capabilities. However, it is important to note that the debate regarding provability of the CTT remains open [20, 5]."}, {"heading": "Meta-intelligence", "text": "If the system is superior to human performance in all domains, as required by definition of superintelligence, it would also be superior in the domain of engineering/computer science/AI research. Potentially, it would be capable of improving intelligence of its successor up to any theoretical/physical limits which might represent an upper bound on optimization power. In other words, if it is possible to improve intelligence a superintelligent system will do so, but as such possibility remains a speculation, this is probably the strongest of all presented objections to intelligence explosion."}, {"heading": "Diminishing returns", "text": "It is a mathematical fact that many functions, while providing diminishing returns, continue diverging. For example, harmonic series: 1+ 1/2 + 1/3 + 1/4 + 1/5 + \u2026 = \u221e, which is a highly counterintuitive result, yet is a proven mathematical fact. Additionally, as the system itself would be improving it is possible that the discoveries it will make with respect to future improvements will also improve in terms of their impact on the overall intelligence of the system. So while it is possible that diminishing returns will be encountered it is just as possible that returns will not be diminished."}, {"heading": "Limits of intelligence", "text": "While physical and theoretical limits to intelligence definitely exist they may be far beyond our capacity to get to them in practice and so will have no impact on our perception of machine intelligence appearing to be undergoing intelligence explosion. It is also possible that physical constants are not permanently set, but dynamically changing which has been demonstrated for some such physical \u201cconstants\u201d. It is also possible that the speed of improvement in intelligence will be below the speed with which some such constants will change. To bring an example from another domain, our universe can be said to be expending faster than the speed of light, with respect to distance between some selected regions, so even with travel at maximum theoretical speed (of light), we will never hit a limit/edge. So, again this is another open questions and limit may or may not be encountered in the process of self-improvement."}, {"heading": "Computational complexity", "text": "While it is certainly true that undecidable problems will remain undecidable, it is not a limitation on intelligence explosion as not a requirement to qualify as superintelligent and plenty of solvable problems exists at all levels of difficulty. Walsh correctly points out that most limitations associated with computational complexity are only problems with our current models of"}, {"heading": "4. Conclusions", "text": "Careful side-by-side analysis of papers by Walsh and Yampolskiy shows an almost identical set of arguments against possibility of technological singularity. This level of successful replication in analysis is an encouraging fact in science and gives additional weight to shared conclusions, but in this paper we provide novel analysis of Walsh\u2019s/Yampolskiy\u2019s arguments which shows that they may not be as strong as initially appears. Future productive directions of analysis may concentrate on a number of inherent advantages, which may permit AI to recursively self-improve [21] and possibly succeed in this challenging domain: ability to work uninterrupted (no breaks, sleep, vocation, etc.), omniscience (complete and cross disciplinary knowledge), greater speed and precision (brain vs processor, human memory vs computer memory), intersystem communication speed (chemical vs electrical), duplicability (intelligent software can be copied), editability (source code unlike DNA can be quickly modified), near-optimal rationality (if not relying on heuristics) [15], advanced communication (ability to share cognitive representations complex concepts), new cognitive modalities (sensors for source code), ability to analyze low level hardware, ex. individual registers, addition of hardware (ability to add new memory, processors, etc.) [30]. The debate regarding possibility of technological singularity will continue. Interested readers are advised to read the full paper by Yampolskiy [28] as well as a number of excellent relevant chapters in Singularity Hypothesis [7] which address many arguments not considered in this paper."}, {"heading": "Acknowledgements", "text": "Author wishes to thank Toby Walsh for encouraging and supporting work on this paper as well as reviewers who provided feedback on an early draft and by doing so made the arguments presented in the paper much stronger."}], "references": [{"title": "Guest column: NP-complete problems and physical reality", "author": ["S. Aaronson"], "venue": "ACM Sigact News, 36 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Information in the holographic universe", "author": ["J.D. Bekenstein"], "venue": "Scientific American, 289 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Superintelligence: Paths", "author": ["N. Bostrom"], "venue": "dangers, strategies, Oxford University Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Quantum noise and information", "author": ["H.J. Bremermann"], "venue": "Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1967}, {"title": "On the Provability", "author": ["S. Bringsjord", "K. Arkoudas"], "venue": "Veracity, and Al-Relevance of the Church\u2014 Turing Thesis, Church's Thesis After 70 Years, 1 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "The Singularity: A Philosophical Analysis", "author": ["D. Chalmers"], "venue": "Journal of Consciousness Studies, 17 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Singularity hypotheses: A scientific and philosophical assessment", "author": ["A.H. Eden", "J.H. Moor", "J.H. Soraker", "E. Steinhart"], "venue": "Springer Science & Business Media", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Brain of a white-collar worker", "author": ["L. Feuillet", "H. Dufour", "J. Pelletier"], "venue": "Lancet (London, England), 370 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Brain plasticity and stroke rehabilitation The Willis lecture", "author": ["B.B. Johansson"], "venue": "Stroke, 31 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Universal limits on computation", "author": ["L.M. Krauss", "G.D. Starkman"], "venue": "arXiv preprint astro-ph/0404510 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "The age of intelligent machines", "author": ["R. Kurzweil", "M.L. Schneider", "M.L. Schneider"], "venue": "MIT press Cambridge", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1990}, {"title": "Ultimate Physical Limits to Computation", "author": ["S. Lloyd"], "venue": "Nature, 406 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "Why an intelligence explosion is probable", "author": ["R. Loosemore", "B. Goertzel"], "venue": "Singularity Hypotheses, Springer", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Is there a model for RSI", "author": ["M. Mahoney"], "venue": "SL4, Available at: http://www.sl4.org/archive/0806/19028.html, June 20", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Intelligence explosion: Evidence and import", "author": ["L. Muehlhauser", "A. Salamon"], "venue": "Singularity Hypotheses, Springer", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "The physics of information processing superobjects: daily life among the Jupiter brains", "author": ["A. Sandberg"], "venue": "Journal of Evolution and Technology, 5 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1999}, {"title": "Checkers is Solved", "author": ["J. Schaeffer", "N. Burch", "Y. Bjornsson", "A. Kishimoto", "M. Muller", "R. Lake", "P. Lu", "S. Sutphen"], "venue": "Science, 317(5844) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Towards a theory of AI completeness", "author": ["D. Shahaf", "E. Amir"], "venue": "8th International Symposium on Logical Formalizations of Commonsense Reasoning ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "A Mathematical Theory of Communication", "author": ["C.E. Shannon"], "venue": "Bell Systems Technical Journal, 27(3) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1948}, {"title": "An introduction to G\u00f6del's theorems", "author": ["P. Smith"], "venue": "Cambridge University Press", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Advantages of artificial intelligences", "author": ["K. Sotala"], "venue": "uploads, and digital minds, International Journal of Machine Consciousness, 4 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "On computable numbers", "author": ["A. Turing"], "venue": "with an application to the Entscheidungsproblem, Proceedings of the London Mathematical Society, 2(42) ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1936}, {"title": "The Singularity May Never be Near", "author": ["T. Walsh"], "venue": "2nd International Workshop on AI, Ethics and Society (AIEthicsSociety2016). 30th AAAI Conference on Artificial Intelligence ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Turing Test as a Defining Feature of AI-Completeness", "author": ["R. Yampolskiy"], "venue": "X.-S. Yang, ed., Artificial Intelligence, Evolutionary Computing and Metaheuristics, Springer Berlin Heidelberg", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "AI-Complete", "author": ["R.V. Yampolskiy"], "venue": "AI-Hard, or AI-Easy\u2013Classification of Problems in AI, The 23rd Midwest Artificial Intelligence and Cognitive Science Conference, Cincinnati, OH, USA ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Analysis of Types of Self-Improving Software", "author": ["R.V. Yampolskiy"], "venue": "Artificial General Intelligence: 8th International Conference, AGI 2015, AGI 2015, Berlin, Germany, July 22-25, 2015, Proceedings, 9205 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Construction of an NP Problem with an Exponential Lower Bound", "author": ["R.V. Yampolskiy"], "venue": "Arxiv preprint arXiv:1111.0305 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "From Seed AI to Technological Singularity via Recursively Self-Improving Software", "author": ["R.V. Yampolskiy"], "venue": "arXiv preprint arXiv:1502.06512 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "On the Limits of Recursively Self-Improving AGI", "author": ["R.V. Yampolskiy"], "venue": "Artificial General Intelligence: 8th International Conference, AGI 2015, AGI 2015, Berlin, Germany, July 22-25, 2015, Proceedings, 9205 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Levels of organization in general intelligence", "author": ["E. Yudkowsky"], "venue": "Artificial general intelligence, Springer", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 22, "context": "In February of 2016 Toby Walsh presented his paper \u201cThe Singularity May Never Be Near\u201d at AAAI16 [23], which was archived on February 20, 2016 (http://arxiv.", "startOffset": 97, "endOffset": 101}, {"referenceID": 27, "context": "Almost exactly a year before, on February 23, 2015, Roman Yampolskiy archived his paper \u201cFrom Seed AI to Technological Singularity via Recursively Self-Improving Software\u201d[28] (http://arxiv.", "startOffset": 171, "endOffset": 175}, {"referenceID": 25, "context": "06512) which was subsequently published as two peer-reviewed papers at AGI15 [26, 29].", "startOffset": 77, "endOffset": 85}, {"referenceID": 28, "context": "06512) which was subsequently published as two peer-reviewed papers at AGI15 [26, 29].", "startOffset": 77, "endOffset": 85}, {"referenceID": 27, "context": "To make it easier to contrast arguments derived from On the Limits of Recursively Self-Improving Artificially Intelligent Systems [28] we use Walsh\u2019s naming of arguments even if our analysis doesn\u2019t rely on the same example (ex.", "startOffset": 130, "endOffset": 134}, {"referenceID": 12, "context": "Improvement] process, but we can [argue] that it would be on par with human intelligence which we associate with universal or general intelligence [13], though in principal a sub-human level system capable of self-improvement can\u2019t be excluded [6].", "startOffset": 147, "endOffset": 151}, {"referenceID": 5, "context": "Improvement] process, but we can [argue] that it would be on par with human intelligence which we associate with universal or general intelligence [13], though in principal a sub-human level system capable of self-improvement can\u2019t be excluded [6].", "startOffset": 244, "endOffset": 247}, {"referenceID": 17, "context": "One may argue that even human level capability is not enough because we already have programmers (people or their intellectual equivalence formalized as functions [18] or Human Oracles [24, 25]) who have access to their own source code (DNA), but who fail to understand how DNA (nature) works to create their intelligence.", "startOffset": 163, "endOffset": 167}, {"referenceID": 23, "context": "One may argue that even human level capability is not enough because we already have programmers (people or their intellectual equivalence formalized as functions [18] or Human Oracles [24, 25]) who have access to their own source code (DNA), but who fail to understand how DNA (nature) works to create their intelligence.", "startOffset": 185, "endOffset": 193}, {"referenceID": 24, "context": "One may argue that even human level capability is not enough because we already have programmers (people or their intellectual equivalence formalized as functions [18] or Human Oracles [24, 25]) who have access to their own source code (DNA), but who fail to understand how DNA (nature) works to create their intelligence.", "startOffset": 185, "endOffset": 193}, {"referenceID": 5, "context": "strongest arguments against the idea of a technological singularity in my view is that it confuses intelligence to do a task with the capability to improve your intelligence to do a task\u201d and cites a quote from Chalmers [6] as an example - \u201cIf we produce an AI by machine learning, it is likely that soon after we will be able to improve the learning algorithm and extend the learning process, leading to AI+\u201d.", "startOffset": 220, "endOffset": 223}, {"referenceID": 5, "context": "Yampolskiy says: \u201cChalmers [6] uses logic and mathematical induction to show that if an AI0 system is capable of producing only slightly more capable AI1 system generalization of that process leads to superintelligent performance in AIn after n generations.", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "Bremermann [4], Bekenstein [2], Lloyd [12], Anders [16], Aaronson [1], Shannon [19], Krauss [10], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "Bremermann [4], Bekenstein [2], Lloyd [12], Anders [16], Aaronson [1], Shannon [19], Krauss [10], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 27, "endOffset": 30}, {"referenceID": 11, "context": "Bremermann [4], Bekenstein [2], Lloyd [12], Anders [16], Aaronson [1], Shannon [19], Krauss [10], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 38, "endOffset": 42}, {"referenceID": 15, "context": "Bremermann [4], Bekenstein [2], Lloyd [12], Anders [16], Aaronson [1], Shannon [19], Krauss [10], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 51, "endOffset": 55}, {"referenceID": 0, "context": "Bremermann [4], Bekenstein [2], Lloyd [12], Anders [16], Aaronson [1], Shannon [19], Krauss [10], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 66, "endOffset": 69}, {"referenceID": 18, "context": "Bremermann [4], Bekenstein [2], Lloyd [12], Anders [16], Aaronson [1], Shannon [19], Krauss [10], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 79, "endOffset": 83}, {"referenceID": 9, "context": "Bremermann [4], Bekenstein [2], Lloyd [12], Anders [16], Aaronson [1], Shannon [19], Krauss [10], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "For many problems such as playing checkers [17] it is possible to completely solve", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "the problem (provide an optimal solution after considering all possible options) after which no additional performance improvement would be possible [14].", "startOffset": 149, "endOffset": 153}, {"referenceID": 21, "context": "no amount of growth in performance will make undecidable problems decidable\u201d and Yampolskiy says, \u201cOther problems are known to be unsolvable regardless of level of intelligence applied to them [22].", "startOffset": 193, "endOffset": 197}, {"referenceID": 26, "context": "Assuming separation of complexity classes (such as P vs NP) holds [27], it becomes obvious that certain classes of problems will always remain only approximately solvable and any improvements in solutions will come from additional hardware resources not higher intelligence.", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "However some recent research has documented that people missing most of their brain could have near normal cognitive capacity [8] and even significant damage to parts of the brain could be overcome due to neuroplasticity [9] suggesting that brain structures are much more general.", "startOffset": 126, "endOffset": 129}, {"referenceID": 8, "context": "However some recent research has documented that people missing most of their brain could have near normal cognitive capacity [8] and even significant damage to parts of the brain could be overcome due to neuroplasticity [9] suggesting that brain structures are much more general.", "startOffset": 221, "endOffset": 224}, {"referenceID": 2, "context": "Anthropocentric The reason some experts believe ([3] - page 339; [11] - chapter 3) that human level of intelligence is special is not because of anthropocentric bias, but because of the Church-Turing Thesis (CTT).", "startOffset": 49, "endOffset": 52}, {"referenceID": 10, "context": "Anthropocentric The reason some experts believe ([3] - page 339; [11] - chapter 3) that human level of intelligence is special is not because of anthropocentric bias, but because of the Church-Turing Thesis (CTT).", "startOffset": 65, "endOffset": 69}, {"referenceID": 19, "context": "However, it is important to note that the debate regarding provability of the CTT remains open [20, 5].", "startOffset": 95, "endOffset": 102}, {"referenceID": 4, "context": "However, it is important to note that the debate regarding provability of the CTT remains open [20, 5].", "startOffset": 95, "endOffset": 102}, {"referenceID": 20, "context": "Future productive directions of analysis may concentrate on a number of inherent advantages, which may permit AI to recursively self-improve [21] and possibly succeed in this challenging domain: ability to work uninterrupted (no breaks, sleep, vocation, etc.", "startOffset": 141, "endOffset": 145}, {"referenceID": 14, "context": "speed (chemical vs electrical), duplicability (intelligent software can be copied), editability (source code unlike DNA can be quickly modified), near-optimal rationality (if not relying on heuristics) [15], advanced communication (ability to share cognitive representations complex concepts), new cognitive modalities (sensors for source code), ability to analyze low level hardware, ex.", "startOffset": 202, "endOffset": 206}, {"referenceID": 29, "context": ") [30].", "startOffset": 2, "endOffset": 6}, {"referenceID": 27, "context": "Interested readers are advised to read the full paper by Yampolskiy [28] as well as a number of excellent relevant chapters in Singularity Hypothesis [7] which address many arguments not considered in this paper.", "startOffset": 68, "endOffset": 72}, {"referenceID": 6, "context": "Interested readers are advised to read the full paper by Yampolskiy [28] as well as a number of excellent relevant chapters in Singularity Hypothesis [7] which address many arguments not considered in this paper.", "startOffset": 150, "endOffset": 153}], "year": 2016, "abstractText": "Toby Walsh in \u201cThe Singularity May Never Be Near\u201d gives six arguments to support his point of view that technological singularity may happen but that it is unlikely. In this paper, we provide analysis of each one of his arguments and arrive at similar conclusions, but with more weight given to the \u201clikely to happen\u201d probability.", "creator": "Microsoft\u00ae Word 2013"}}}