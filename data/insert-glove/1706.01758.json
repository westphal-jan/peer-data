{"id": "1706.01758", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2017", "title": "A WL-SPPIM Semantic Model for Document Classification", "abstract": "mobilise In kemetic this navigazione paper, agonizingly we explore fellatio SPPIM - based swivelled text expand classification pondeljak method, and khasas the atlanta-based experiment trenchant reveals hallen that the SPPIM yekini method overcall is equal to bj\u00f6rk or even amirkhanov superior schaja than holistic SGNS sleet method michihisa in text classification audiofina task yuna on three forepart international petalotis and 3,667 standard text ivanisevic datasets, vibha namely dehiscence 20newsgroups, Reuters52 ledeall and WebKB. alleged Comparing to galasso SGNS, 94-86 although e-nau SPPMI provides a philippinensis better solution, it marchegiani is airline not metropark necessarily censorial better than SGNS fr\u00f3ilaz in text classification tasks. Based mariquita on h.res our analysis, SGNS takes chingola into the 21.5 consideration of ulji weight calculation rohullah during cowardliness decomposition process, rugh so beledweyne it rehmatullah has salyer better performance than SPPIM in reloading some changchun standard hup datasets. Inspired by this, we rinky propose a WL - SPPIM aunts semantic synonyms model siriraj based on worldgate SPPIM model, and christianize experiment shows provocation that mussavi WL - solimena SPPIM approach has better closings classification and nw1 higher hattusili scalability icpa in 42.34 the circumferences text a-mile classification meskel task compared paregoric with issi LDA, SGNS and SPPIM f\u00f6rster approaches.", "histories": [["v1", "Fri, 26 May 2017 08:03:10 GMT  (508kb)", "http://arxiv.org/abs/1706.01758v1", "7pages, 5figures, Keywords: LDA, SPPIM, word embedding, low frequency, document classification"]], "COMMENTS": "7pages, 5figures, Keywords: LDA, SPPIM, word embedding, low frequency, document classification", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["ming li", "peilun xiao", "ju zhang"], "accepted": false, "id": "1706.01758"}, "pdf": {"name": "1706.01758.pdf", "metadata": {"source": "CRF", "title": "A WL-SPPIM Semantic Model for Document Classification", "authors": ["First A. Ming Li", "C. Ju Zhang"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014LDA; SPPIM; word embedding; low frequency;document classification\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014"}, {"heading": "1 INTRODUCTION", "text": "istribution of semantic vectors is widely used in text semantic expression, including text classification, text clustering, semantic retrieval, automatic question and answer, dictionary generation, semantic disambiguation, query expansion, text advertisements and machine translation, especially for measuring semantic relevance[1, 2]. We divided the DSMs into two categories, one we called count-based models, many traditional DSMs belong to this category, the other category we call prediction-based models, which are based on neural embedding\nAmong the traditional count-based models the best know is Latent Semantic Analysis. LSA is a low dimensional semantic space for texts, and LSA derive the document vector by the use of co-occurrence information between words[3]. More recently, LDA has received more and more extensive attention, as a semantic model of DSMs[4, 5]. LDA is a three-layer Bayesian probability model proposed by Blei et al in 2003, which contains the three layers structure of document, topic and word. Document to topic subjects to Dirichlet distribution, topic to word subjects to polynomial distribution. LDA semantic model usually shows very good performance on NLP tasks, partly because it projects the document into a low-dimensional topic semantic space. Word frequencies determine the topic of calculated and deducted by LDA largely, which leads less frequent but important topics can not be effectively calculated.\nPointwise mutual information(PMI) has been extensively uesd as count-based model in distributional sematic models. Pointwise Mutual Information (PMI) is the popular word co-occurrence based measure[6, 7]. PMI has a well-known tendency that it calculates too high scores for low frequency words[2]. To solve the limitation fo PMI, many variants of it have been proposed, PPMI is the simple one of the variants, in which all PMI values that below zero will be set to zero[8]. Bullinaria and Levydem demonstrate that PPMI outperforms various other weighting methods \uff0c when measuring semantic similarity through word-context matrices.[9]. Traditional distributional Semantic Models achieve considerable effective effects on various NLP tasks, including semantic relevance and text classification. The last few years have seen the development of prediction-based neural embedding models in which words are embedded into a low dimensional space. Word embedding models can efficiently learn word vector from a large number of unstructured text, and can effectively reflect the syntactic or semantic relations between words[10]. The initial pioneering research work was started by Bengio and his colleagues, who generated word vectors in the study of the neural language model[11] and a number of subsequent research work including various word embedding models and efficient learning algorithms[12-16]. In particular, We notice a conclusion that the SGNS(skip-gram with negativesampling)model which is efficient to provides competitive results on various NLP tasks, which is concluded in a sequence of papers by Mikolov and colleagues[14, 17]. The SGNS model maximizes the conditional probability of the observed contexts given the current word when scanning through the corpus, however it is not clear what information the embedding vectors really convey, so prediction-based neural embeddings are considered opaque. The researchers have different conclusions on the various types of distributed semantic models applying to the performance of NLP tasks, some of these conclusions \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  F. A. Author is with High performance computing application R&D Center, Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, Chongqing, China & University of Chinese Academy of Sciences, Beijing, China & Center for Speech and Language Technology,Research Institute of Information Technology, Tsinghua University,Beijing, China; E-mail: liming@cigit.ac.cn.  S.B. Author is an undergraduate in Beijing University of Posts and Telecommunications. This work was done when he was a visiting student at Tsinghua University; Email:xiaopli@cslt.riit.tsinghua.edu.cn.  T. C. Author is with with High performance computing application R&D Center, Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences Chongqing, China; E-mail: Zhangju@cigit.ac.cn. D\n2 are the same, some are the opposite, some conclusions depend on the distributed semantic models application of specific NLP tasks . Some of the researchers have found that predictive neural embedding models have better performance than traditional counting models in calculating word similarity[9, 18], some researchers have published papers that show the traditional counting models have better performancethan than predictionbased neural embedding models on some datasets of specific NLP tasks[19-20]. So far no study has been conducted in this field about comparing the performance of count-based models and prediction neural embedding models, such as LDA, PIM(or other PMI variants) and SGNS which are representative distributional semantic models on the document classification. In this paper, We make a deep research and compare the performance of LDA, SGNS, SPPIM distributional semantic models in text categorization. The Experiment shows that whether the TSVD method or the ISVD method get the poor performance than the PPMI method in the similaraty task in the low frequency range[21]. Especially, the ISVD method get the worst results. The paper analyzed these results as that ISVD removed the latent dimension vectors which have the largest variance, that are no doubt the most important dimensions for fairly low frequent item. Inspired by the paper insight, we propose a WL-SPPIM semantic model based on SPPIM model empirically. Different with SPPIM: 1) We make some changes on one term in the formula that produces the PMI matric, by replacing the ( )P c with 3 4 ( )p c ; 2) we reweight the probility of the low frequency word by merging the probability of the high frequency words into the probility of the low frequency words. The experimental results show that, on the document classification, WL-SPPIM yields better results than LDA, SGNS, SPPIM on the accuracy and expansibility. The rest of the paper is structured as follows: Section 2 introduces the LDA, SGNS, SPPIM semantic models for document classification; Section 3 describes the proposed WL-SPPIM semantic model for document classification; Section 4 presents the experiments; Section 5 discusses the result; and Section 6 conclusions the paper and suggests the future work direction."}, {"heading": "2 BACKGROUND", "text": ""}, {"heading": "2.1 LDA-based Semantic Model", "text": "LDA is not only a topic model, but also a \u201ctext-topicword\" three-level Bayesian model. The text reflects mixes distribution of topic, while the topic demonstrates the probability distribution of words. A document contains many topics, and each word in the document is generated by one topic. As for each word in the document\n 1 2, , Nd w w w K , LDA model chooses a topic iz , topic set  based on the multinomial distribution\n Multi  which is determined by related variable  , in which iz represents the number of selected topic, and  is\nconsistent with the Dirichlet Distribution  Dir  . After the topic\niz  is selected, the word iw is generated by\nmultionmial distribution  izMulti  which is defined by\niz  , the generation of text is summarized by formula (1) - (3). ~ ( )Dir  (1)\n~ ( )iz Multi  (2) ~ ( )\nii z w Multi  (3)\nText feature vectors are generated by adopting LDA to extract text features of documents in low dimensional space. These feature vectors can be used for building classification models and classification. This paper extracts text features of LDA according to following steps: 1) set the size of vocabularyV and topic set  ; 2) input all documents of training set (regardless of category) into LDA model, training model parameters  and  ; 3) LDAmodels obtained from training are used in reasoning to calculate  |P d . In general, the maximum point  of  |P d is used as feature vector of the document, namely  MAP , the maximum posteriori probability point  :     argmax ( | )MAPf d d P d\n    ."}, {"heading": "2.2 SPPIM semantic model", "text": "The traditional way to represent words in the distributional approach is to construct a high dimensional sparse matrix M , where each row represents a word w and each column represents a context c . The value of each matrix cell ijM represents the association between the word iw and the context jc . A popular measure of thisassociation is pointwise mutual information(PMI)[6]. PMI is calculated empirically like:\n# ( , ) ( , ) lo g # ( ) # ( ) w c D PM I w c w c  g\n(4)\nD is the number of observed word and context pairs. In terms of negative PMI values may not positively contribute to model performance and sparser matrices are more computationally tractable[2], a sparse and consistent alternative is to use PPMI matric. In PPMI matric, all negative values which are deduced from PMI matric are replaced by 0:\n( , ) max( ( , ),0)PPMI w c PMI w c (5) SPPMI can be generalized to an additional coutoff parameter k , analysising by Levy and Goldberg shows hat SGNS is equivalent to a shifted version of the PPMI method, where all values get shifted by a factor log k [22]:\n( , ) max( ( , ) log ,0)SPPMI w c PMI w c k  (6) In the SPPMI model, word distributions are projected into high dimensional sparse space. Distributional semantic models can get better performance, when applying a variety of transformations to the original vector, such as by reweighting to the text and applying the dimensionality reduction technique. So we apply SVD to the SPPMI matrix in order to obtain low-dimensional\n3 embeddings.When the SPPIM-SVD matrix produces the word vector, the document vector iv can be computed also by the simple average pooling approach."}, {"heading": "2.3 SGNS Semantic Mode", "text": "Prediction-based neural embedding models such as the\nskip-gram model and continuous bag-of-words model have become the standard for word modeling[14, 17]. In the skip-gram model, for a word Ww V and a context\nCc V , their embedding vectors are represented as dw R and dc R respectively, where d is the\nembedding\u2019s dimensionality. The embedding vectors of the words in WV and the context words in CV compose the word and context embedding matrices W and C . It seeks to represent each Ww V and each context Cc V as d-dimensional vectors w r and c r , the words\nwill have similar vector representations, if the words are similar.\nLevy and Goldberg made an interesting connection between two models: a traditional count-based model on PMI(pointwise mutual information) and a predictionbased neural embedding model, namely the SGNS(skipgram model with negative sampling)[22]. The interesting result is that the SGNS model is equivalent to a shifted version of the PMI method, where all values get shifted by a factor of log k . Levy and Goldberg described the matrix M that SGNS is factorizing:\n( , ) logSGNSij i j i j i jM W C w c PMI w c k      r r\n(7)\nThe meaning of the document can be considered a summary of the meaning of all the words in the document. We use the Word2vec tool to generate the word vector first, and then generate the document vector by averaging pooling the word vector. We use ,i jc to denote the word vector of the j th word in the document i , the document vector iv can be calculated as:\n, 1\n1 iJ i i j\nji v c J    , Where iJ is the number of words in the document."}, {"heading": "3 WL-SPPIM SEMANTIC MODEL", "text": "The Shifted PPMI performs slightly better than SGNS derived vectors on two datasets , but performs poorly than the latter on one dataset of text classification tasks(see section 4).\nAccording to the algorithm illustrated in[17], the negative contexts are sampled according to the formula\n3 4 3 4 #( ) cp c\nz  instead of the unigram distribution\n# ( )c z , so we modify the SPPIM formulation :\n3 4( , ) max( ( , ) log ,0)SPPMI w c PMI w c k  (8)\nThe SPPIM underperforms than the SGNS on the one dataset on text classification task(see section 4), one of the main reason is that SGNS performs weighted matrix factorization. Inspired by this, we propose the WL-SPPIM approach. The probabilities of the low frequency words are generally underestimated in terms of the lack of occurrences in the training datan, and estimating the probabilities of words that absent in corpus is simply impossible. However, these words are often important entity names that should be emphasized, so we have a idea that transform some information from high frequency words to enhance the weight of the lowfrequency words. Given a set of words\n 1 2, , , mW x x x K to be enhanced, for each word ix W , a set of words  1 2, ,i nS y y y K that are similar\nto ix is manually selected from the training data. The similarity can be defined with semantic. We assume that, for each j iy S , if there exist an n-gram of jy in the training corpora, the corresponding n-gram of ix should also have a relative higher probability of appearance. In general, -TF IDF method is adopted for the weight design of features[23].\n     2 , , lo g NT F ID F t d T F t d D F t   is the\ncalculation formula of word frequency-inverse document frequency;  ,TF t d refers to the frequency of word t in document d , N is total document number, and  DF t is the number of other documents containing feature word t . But -TF IDF lacks the ability to distinguish feature items in different categories. We designed new weight calculation method based on -TF IDF technology.\n     2 | | lo g | cNW t c T F t c D F t c   represents the\nability of feature item t to distinguish category c , 1 2, , kc c c c  and  |TF t c represents the frequency of feature item t in category c , cN is the number of documents in category c , and  |DF t c represents the the number of documents containing feature item t which are included in the class that not belongs to c . The value of  |W t c is larger, the ability of feature item to distinguish category c is the stronger.  |W t c measures the ability of feature word to distinguish categories from global perspective. Then we adjust the weight of the low-frequency according to the following equation:\n2 ( | )ln ( | )i j\ni\ni x y\ni ic c\nW x cw w W x c\n          \n(9)\nWhere ix presents the low-frequency word.\n4  |iW x c indicates the ability of ix to distinguish\ncategory ic ,  |i iTF x c represents the frequency of ix in category ic , icN is the number of documents in category ic ,  |i iDF x c is the number of documents containing word ix which are included in the class that not belongs to c .\nIn addition, a threshold is set to guarantee that\n2 ( | ) ( | )\ni\ni\ni ic c\nW x c W x c  is not less than this threshold, if the\nvalue is too small to enhance the recognition performance, this threshold will be assigned to it ."}, {"heading": "4 EXPERIMENT", "text": ""}, {"heading": "4.1 Database and configurations", "text": "We experiment all models on English corpus, preprocessing by removing single characters and alphabetic characters, and converting letters to lowercase, stemming word roots, sentence splitting, and tokenization. The experiments were conducted with three datasets:20 Newsgroups dataset that was originally collected by Ken Lang1, WebKB dataset that was collected by the project of the CMU2, and Reuters dataset published by DavidD Lewis3. Table 3 lists the statistics of these three datasets. 20Newsgroups corpus contain 20 categories of English news text, which contains a total of 18846 Document. In addition,in order to improve the reliability of experiment, all repeat documents and some news heads are removed, which left 11293 and 7528 documents to the training data and testing data. Original WebKB corpus contains about 8,300 English websites, which can be divided into 7 categories. Selecting the most commonly used 4 major categories, including student, faculty, course and project categories, this text subset is called WebKB-4. In addition, in order to improve the reliability of experiment, some repeat documents are removed, which left 2756 and 1375 documents to the training set and testing set. The Reuters dataset is a collection of Reuters Newswire in 1987. We have 8 of the 10 most frequent classes and 52 of the original 90, namely R8 and R52. R52 is a totally 4M corpus which contains 3M text (6532 documents) for training, and 1M text(2568 documents) for testing. We use the word2vec tool provided by Googleused to train the skip-gram word vector4. LDA training and learning tools invented by Blei is used to generate document vector5. We conducted text classification experiments on three classifiers, including Bayesian, KNN, and SVM which all are build by the scikit-learn tool6.\n1http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo20/www/data/news20.html\n2 http://web.ist.ult.pt/~acardoso 3 https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html 4 https://code.google.com/p/word2vec 5 http://www.cs.princeton.edu/~blei/lda-c 6 http://scikit-learn.org/dev/modules/svm.html#svm"}, {"heading": "4.2 Experiments", "text": "SGNS has a natural hyper parameter k , which is the number of negative samples, which affects the value that SGNS is trying to optimize for each ( , ) : ( , ) logw c PMI w c k . The shift caused by 1k  can be applied to distributional methods through shifted PPMI. SPPMI achieve good performance with lower values of k , what is due to the fact that only positive values are retained, and high values of k may cause too much loss of information[22]. Empirically, we take experiment with values of 5k  . At first, We experiment these models with different classifiers on 20newsgroups, and the results about classification accuracy are depicted in Table 2. We conduct this experiment with 50-dimension on the word vector in the SGNS and the topics in the LDA, then the dimensions of the document vectors will be 50-dimension with four approaches. We observe that KNN method outperforms the other two, when the three classifiers are compared, which the result is showed in Table 2. Nevertheless, KNN is a nonparametric method, which is not suitable for large corpus, so we choose SVM as the classifier which will be used in the following experiments.\n5\nThe second experiment compared the performances of the four models on document classification of different complexity. Figure 4 shows that text feature dimension is set at 200 dimensions, and classification complexity is increased from two-level classification to 20-level classification in 20Newsgroups dataset, and from twolevel classification to 4-level classification in WebKB\ndataset to test the performance of the classification task complexity of the four models in each classification task. For each classification task, LDA, SPPIM and WL-SPPIM should retrain parameters in corresponding training corpus, while word vector generated by word2vector tools does not depend on specific text field, so SGNS model does not retrain parameters. In the Figure 1-3, the abscissa represents different complexity on document classification, and ordinate the accuracy of classification."}, {"heading": "5 EXPERIMENTAL RESULTS AND ANALYSIS", "text": "We evaluate LDA, SGNS, SPPIM, WL-SPPIM on the text classification task. The comparison of these four models in terms of the text classification accuracy is seen in Fig 1- Fig3 , the dimensionality of the document vectors varies from 10 to 300 in our experiment . Fig1-Fig3 shows the WL-SPPIM model achieves its best performs on all the datasets, WL-SPPIM outperforms SPPIM consistently. And the SPPIM is subtly better than SGNS on R52 and WebKB datasets, the accuracy of SPPIM and SGNS are very close on 20newsgroups datasets. WL-SPPIM significantly outperforms SPPIM and SGNS, verifying that re-weighting the weight of the word vector could improve the accuracy of the document classification. Given the similar performance of SPPIM and SGNS in the text classification task, the superiority of WL-SPPIM is obvious. The accuracy of LDA is much lower than that of other methods, which may be because LDA performs better with long documents[24]. It can be learned that the accuracy of the four models in classification improves with the increase of document vector dimensions, indicating document vector dimension is larger, the classification accuracy is better. However, the classification performance of LDA in WebKB data decreases sharply with the increase of feature dimensions, which is attributed to the lack of long text training corpus in the field, difficult to meet the optimization task of LDA n high dimensional vector model.\nAs to the effects of different category, one can observe that the performance of WL-SPPIM remains stable with different categories from Figure 4 The accuracy of LDA is much lower than that of other methods, which may be due to uneven distribution of topic vector in various categories and thereby leading to differences in various categories as LDA is greatly influenced by model initial value in the establishment of topic model. As the generation of document vector by SGNS combination involves the process of mean smoothing, so it is less affected by categories changing and the classification performance is stable. Similarly, reweighting the low frequency words in WL-SPPIM model is equivalent to smoothing techniques, so multiclassification experiments shows better classification performance and higher stability.\n6\nFig. 4. Experimental results with various numbers of category on 20-newsgroups\nFig. 5. Experimental results with various numbers of category on WebKB\nFrom the results revealed in Fig4 and Fig5, we can observe that the proposed method produce good accuracy on the 20newsgroups dataset and R52 dataset, but not satisfied on the WebKB dataset. Improving the weight of low-frequency words does not enhance the classification performance of WL-SPPIM model in the WebKB dataset, which may be because the WebKB dataset is relatively small, so low frequency and high frequency words meeting the requirements of similar-pair are very few.This result confirms the effectiveness and high performance of proposed method. This method could be applied on semantic tasks in specific domain where low-frequency words need to be enhanced. The experimental results reveal that our count-based model achieve better performance than predictionbased neural embedding model on the document classification task. Furthermore, these experiments not only verify the classification performance of WL-SPPIM, but also the generalization ability of WL-SPPIM."}, {"heading": "6 CONCLUSION", "text": "In this paper, we verify the discovery in[20, 25] on the text classification task, what the traditional count-based models achieve better performance than predictionbased neural embedding models on some specific NLP tasks. Furthermore, we propose a method based on WLSPPIM semantic model for the document classification, and investigate the characteristics of LDA, SGNS, SPPIM and WL-SPPIM sematic models on the semantic source and semantic generation method in detail, and analyze the advantages of WL-SPPIM semantic model on the document classification. Experiments shows that the WLSPPIM model is significantly superior than other three methods in classification accuracy, complexity, scalability on text classification tasks. In future work we will investigate distributions semantic clustering for WLSPPIM model.\nACKNOWLEDGMENT This research was supported by the National Science Foundation of China (NSFC) under the project No.61672488, Ministry of science and Technology \u201cKey technologies of educational cloud \u201d project No.2013BAH72B01, Chongqing Science and Technology Commission project No.cstc2013jjys4000. It was also partially supported by the Magicalthink Ltd.\nREFERENCES [1] S. Dumais, \u201cData-driven approaches to information access,\u201d\nCognitive Science, vol. 27, no. 3, pp. 491-524, 2003. [2] P. D. Turney, and P. Pantel, \u201cFrom frequency to meaning:\nVector space models of semantics,\u201d Journal of artificial intelligence research, vol. 37, pp. 141-188, 2010.\n[3] T. K. Landauer, and S. T. Dumais, \u201cA solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge,\u201d Psychological review, vol. 104, no. 2, pp. 211, 1997.\n[4] D. M. Blei, A. Y. Ng, and M. I. Jordan, \u201cLatent dirichlet allocation,\u201d Journal of machine Learning research, vol. 3, no. Jan, pp. 993-1022, 2003.\n[5] T. L. Griffiths, M. Steyvers, and J. B. Tenenbaum, \u201cTopics in semantic representation,\u201d Psychological review, vol. 114, no. 2, pp. 211, 2007.\n[6] K. W. Church, and P. Hanks, \u201cWord association norms, mutual information, and lexicography,\u201d Computational linguistics, vol. 16, no. 1, pp. 22-29, 1990.\n[7] P. D. Turney, \"Mining the web for synonyms: PMI-IR versus LSA on TOEFL.\" pp. 491-502.\n[8] Y. Niwa, and Y. Nitta, \"Co-occurrence vectors from corpora vs. distance vectors from dictionaries.\" pp.304-309.\n[9] T. Mikolov, W.-t. Yih, and G. Zweig, \"Linguistic Regularities in Continuous Space Word Representations.\" pp. 746-751.\n[10] G. E. Hinton, J. L. Mcclelland, and D. E. Rumelhart, \"Distributed representations, Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations,\" MIT Press, Cambridge, MA, 1986.\n[11] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, \u201cA neural probabilistic language model,\u201d Journal of machine learning research, vol. 3, no. Feb, pp. 1137-1155, 2003.\n7 [12] A. Mnih, and G. E. Hinton, \"A scalable hierarchical distributed language model.\" pp. 1081-1088.\n[13] A. Mnih, and G. Hinton, \"Three new graphical models for statistical language modelling.\" pp. 641-648.\n[14] T. Mikolov, K. Chen, G. Corrado, and J. Dean, \u201cEfficient estimation of word representations in vector space,\u201d arXiv preprint arXiv:1301.3781, 2013.\n[15] T. Mikolov, \u201cStatistical language models based on neural networks,\u201d Presentation at Google, Mountain View, 2nd April, 2012.\n[16] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, \u201cNatural language processing (almost) from scratch,\u201d Journal of Machine Learning Research, vol. 12, no. Aug, pp. 2493-2537, 2011.\n[17] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, \"Distributed representations of words and phrases and their compositionality.\" pp. 3111-3119.\n[18] M. Baroni, G. Dinu, and G. Kruszewski, \"Don't count, predict! A systematic comparison of context-counting vs. contextpredicting semantic vectors.\" pp. 238-247.\n[19] F. Asr, J. Willits, and M. Jones, \"Comparing predictive and cooccurrence based models of lexical semantics trained on childdirected speech.\"\n[20] O. Levy, Y. Goldberg, and I. Ramat-Gan, \"Linguistic Regularities in Sparse and Explicit Word Representations.\" pp. 171-180.\n[21] M. Sahlgren, and A. Lenci, \u201cThe Effects of Data Size and Frequency Range on Distributional Semantic Models,\u201d arXiv preprint arXiv:1609.08293, 2016.\n[22] O. Levy, and Y. Goldberg, \"Neural word embedding as implicit matrix factorization.\" pp. 2177-2185.\n[23] W. Zhang, T. Yoshida, and X. Tang, \u201cA comparative study of TF* IDF, LSI and multi-words for text classification,\u201d Expert Systems with Applications, vol. 38, no. 3, pp. 2758-2765, 2011.\n[24] J. Tang, Z. Meng, X. Nguyen, Q. Mei, and M. Zhang, \"Understanding the Limiting Factors of Topic Modeling via Posterior Contraction Analysis.\" pp. 190-198.\n[25] A. Mnih, and K. Kavukcuoglu, \"Learning word embeddings efficiently with noise-contrastive estimation.\" pp. 2265-2273."}], "references": [{"title": "Data-driven approaches to information access", "author": ["S. Dumais"], "venue": "Cognitive Science, vol. 27, no. 3, pp. 491-524, 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of artificial intelligence research, vol. 37, pp. 141-188, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dumais"], "venue": "Psychological review, vol. 104, no. 2, pp. 211, 1997.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of machine Learning research, vol. 3, no. Jan, pp. 993-1022, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Topics in semantic representation", "author": ["T.L. Griffiths", "M. Steyvers", "J.B. Tenenbaum"], "venue": "Psychological review, vol. 114, no. 2, pp. 211, 2007.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Word association norms, mutual information, and lexicography", "author": ["K.W. Church", "P. Hanks"], "venue": "Computational linguistics, vol. 16, no. 1, pp. 22-29, 1990.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Distributed representations, Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations", "author": ["G.E. Hinton", "J.L. Mcclelland", "D.E. Rumelhart"], "venue": "MIT Press, Cambridge, MA, 1986.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1986}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of machine learning research, vol. 3, no. Feb, pp. 1137-1155, 2003.  7", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Statistical language models based on neural networks", "author": ["T. Mikolov"], "venue": "Presentation at Google, Mountain View, 2nd April, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, vol. 12, no. Aug, pp. 2493-2537, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "The Effects of Data Size and Frequency Range on Distributional Semantic Models", "author": ["M. Sahlgren", "A. Lenci"], "venue": "arXiv preprint arXiv:1609.08293, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "A comparative study of TF* IDF, LSI and multi-words for text classification", "author": ["W. Zhang", "T. Yoshida", "X. Tang"], "venue": "Expert Systems with Applications, vol. 38, no. 3, pp. 2758-2765, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "istribution of semantic vectors is widely used in text semantic expression, including text classification, text clustering, semantic retrieval, automatic question and answer, dictionary generation, semantic disambiguation, query expansion, text advertisements and machine translation, especially for measuring semantic relevance[1, 2].", "startOffset": 328, "endOffset": 334}, {"referenceID": 1, "context": "istribution of semantic vectors is widely used in text semantic expression, including text classification, text clustering, semantic retrieval, automatic question and answer, dictionary generation, semantic disambiguation, query expansion, text advertisements and machine translation, especially for measuring semantic relevance[1, 2].", "startOffset": 328, "endOffset": 334}, {"referenceID": 2, "context": "LSA is a low dimensional semantic space for texts, and LSA derive the document vector by the use of co-occurrence information between words[3].", "startOffset": 139, "endOffset": 142}, {"referenceID": 3, "context": "More recently, LDA has received more and more extensive attention, as a semantic model of DSMs[4, 5].", "startOffset": 94, "endOffset": 100}, {"referenceID": 4, "context": "More recently, LDA has received more and more extensive attention, as a semantic model of DSMs[4, 5].", "startOffset": 94, "endOffset": 100}, {"referenceID": 5, "context": "Pointwise Mutual Information (PMI) is the popular word co-occurrence based measure[6, 7].", "startOffset": 82, "endOffset": 88}, {"referenceID": 1, "context": "PMI has a well-known tendency that it calculates too high scores for low frequency words[2].", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": "Word embedding models can efficiently learn word vector from a large number of unstructured text, and can effectively reflect the syntactic or semantic relations between words[10].", "startOffset": 175, "endOffset": 179}, {"referenceID": 7, "context": "The initial pioneering research work was started by Bengio and his colleagues, who generated word vectors in the study of the neural language model[11] and a number of subsequent research work including various word embedding models and efficient learning algorithms[12-16].", "startOffset": 147, "endOffset": 151}, {"referenceID": 8, "context": "The initial pioneering research work was started by Bengio and his colleagues, who generated word vectors in the study of the neural language model[11] and a number of subsequent research work including various word embedding models and efficient learning algorithms[12-16].", "startOffset": 266, "endOffset": 273}, {"referenceID": 9, "context": "The initial pioneering research work was started by Bengio and his colleagues, who generated word vectors in the study of the neural language model[11] and a number of subsequent research work including various word embedding models and efficient learning algorithms[12-16].", "startOffset": 266, "endOffset": 273}, {"referenceID": 10, "context": "The initial pioneering research work was started by Bengio and his colleagues, who generated word vectors in the study of the neural language model[11] and a number of subsequent research work including various word embedding models and efficient learning algorithms[12-16].", "startOffset": 266, "endOffset": 273}, {"referenceID": 8, "context": "In particular, We notice a conclusion that the SGNS(skip-gram with negativesampling)model which is efficient to provides competitive results on various NLP tasks, which is concluded in a sequence of papers by Mikolov and colleagues[14, 17].", "startOffset": 231, "endOffset": 239}, {"referenceID": 11, "context": "The Experiment shows that whether the TSVD method or the ISVD method get the poor performance than the PPMI method in the similaraty task in the low frequency range[21].", "startOffset": 164, "endOffset": 168}, {"referenceID": 5, "context": "thisassociation is pointwise mutual information(PMI)[6].", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "In terms of negative PMI values may not positively contribute to model performance and sparser matrices are more computationally tractable[2], a sparse and consistent alternative is to use PPMI matric.", "startOffset": 138, "endOffset": 141}, {"referenceID": 8, "context": "Prediction-based neural embedding models such as the skip-gram model and continuous bag-of-words model have become the standard for word modeling[14, 17].", "startOffset": 145, "endOffset": 153}, {"referenceID": 12, "context": "In general, TF IDF method is adopted for the weight design of features[23].", "startOffset": 70, "endOffset": 74}], "year": 2017, "abstractText": "In this paper, we explore SPPIM-based text classification method, and the experiment reveals that the SPPIM method is equal to or even superior than SGNS method in text classification task on three international and standard text datasets, namely 20newsgroups, Reuters52 and WebKB. Comparing to SGNS, although SPPMI provides a better solution, it is not necessarily better than SGNS in text classification tasks.. Based on our analysis, SGNS takes into the consideration of weight calculation during decomposition process, so it has better performance than SPPIM in some standard datasets. Inspired by this, we propose a WL-SPPIM semantic model based on SPPIM model, and experiment shows that WL-SPPIM approach has better classification and higher scalability in the text classification task compared with LDA, SGNS and SPPIM approaches.", "creator": "WPS Office"}}}