{"id": "1706.02293", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "Sound Event Detection in Multichannel Audio Using Spatial and Harmonic Features", "abstract": "In this gush paper, 3.76 we propose the use 1-888-nyt of collegues spatial linesmen and freind harmonic andrie features merrow in kartchner combination with long 1392 short term bukharin memory (gaetan LSTM) recurrent owyang neural srbije network (RNN) for automatic sound http://www.nationalgeographic.com event falseness detection (SED) parkas task. Real life hawera sound zundert recordings vfl/afl typically have lcp many complement overlapping iencsi sound events, wavefunction making it crossair hard to recognize kwc with just celes mono enernoc channel villarreal audio. rm50 Human stradbally listeners comedown have proops been successfully recognizing linera the mixture of counterstrikes overlapping sound ardath events using potsie pitch cues watabe and exploiting pipistrelle the stereo (multichannel) audio euro840 signal krue available at their ears mont. to redner spatially kontinent localize these events. biv Traditionally supplant SED systems balasingam have qurra only been delhagen using 3225 mono channel felcor audio, batsmen motivated song-writing by mongan the human listener suginami we zutons propose to elst extend them ronen to condones use herrenberg multichannel magliore audio. voila The proposed loeb SED system is infarcts compared against roundels the state mehu of the confounded art mono channel method on oral the development subset ghairat of reflectiveness TUT habomai sound events detection nimmo 2016 ratos database. lecompton The usage cellini of spatial and 10-million harmonic features are ezio shown to komunyakaa improve clear-eyed the kwouk performance 8:52 of rm600 SED.", "histories": [["v1", "Wed, 7 Jun 2017 06:11:32 GMT  (95kb,D)", "http://arxiv.org/abs/1706.02293v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["sharath adavanne", "giambattista parascandolo", "pasi pertil\\\"a", "toni heittola", "tuomas virtanen"], "accepted": false, "id": "1706.02293"}, "pdf": {"name": "1706.02293.pdf", "metadata": {"source": "CRF", "title": "SOUND EVENT DETECTION IN MULTICHANNEL AUDIO USING SPATIAL AND HARMONIC FEATURES", "authors": ["Sharath Adavanne", "Giambattista Parascandolo", "Pasi Pertil\u00e4", "Toni Heittola", "Tuomas Virtanen"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Sound event detection, multichannel, time difference of arrival, pitch, recurrent neural networks, long short term memory"}, {"heading": "1. INTRODUCTION", "text": "A sound event is a segment of audio that a human listener can consistently label and distinguish in an acoustic environment. The applications of such automatic sound event detection (SED) are numerous; embedded systems with listening capability can become more aware of its environment [2][3]. Industrial and environmental surveillance systems and smart homes can start automatically detecting events of interest [4]. Automatic annotation of multimedia can enable better retrieval for content based query methods [5].\nThe task of automatic SED is to recognize the sound events in a continuous audio signal. Sound event detection systems built so far can be broadly classified to monophonic and polyphonic. Monophonic systems are trained to recognize the most dominant of the sound events in the audio signal [6]. While polyphonic systems go beyond the most dominant sound event and recognize all the overlapping sound events in a segment [6][7][8][9]. We propose to tackle such polyphonic soundscape which replicates real life scenario in this paper.\nSome SED systems have tackled polyphonic detection using mel-frequency cepstral coefficients (MFCC) and hidden Markov models (HMMs) as classifiers with consecutive passes of the Viterbi\nThe research leading to these results has received funding from the European Research Council under the European Unions H2020 Framework Programme through ERC Grant Agreement 637422 EVERYSOUND, and Google Faculty Research Award project \u201cAcoustic Event Detection and Classification Using Deep Recurrent Neural Networks\u201d. The authors also wish to acknowledge CSC-IT Center for Science, Finland, for computational resources.\nalgorithm [10]. In [11], a non-negative matrix factorization was used as a pre-processing step, and the most prominent event in each of the stream was detected. However, it still had a hard constraint of estimating the number of overlapping events. This was overcome by using coupled NMF in [12]. Dennis et al [7] took an entirely different path from the traditional frame-based features by combining generalized Hough transform (GHT) with local spectral features.\nMore recently, the state of the art SED systems have used log mel-band energy features in DNN [8], and RNN-LSTM [9] networks trained for multi-label classification. Motivated by the good performance of RNN-LSTM over DNN as shown in [9], we continue to use the multi-label RNN-LSTM network.\nThe present state of the art polyphonic SED systems have been using a single channel of audio for sound event detection. Polyphonic events can potentially be tackled better if we had multichannel data. Just like humans use their two ears (two channels) to recognize and localize the sound events around them [13], we can also potentially train machines to learn sound events from multichannel of audio. Recently, Xiao et al [14] have successfully used spatial features from multichannel audio for far field automatic speech recognition (ASR) and shown considerable improvements over just using mono channel audio. This further motivates us to use spatial features for SED tasks. In this paper, we propose a spatial feature along with harmonic feature and prove its superiority over mono channel feature even with a small dataset of around 60 minutes.\nThe remaining of the paper is structured as follows. We describe in Section 2 the features used and the proposed approach. Section 3 presents a short introduction to RNNs and long shortterm memory (LSTM) blocks. Section 4 presents the experimental set-up and results on a database of real life recordings. Finally, we present our conclusions in Section 5."}, {"heading": "2. SOUND EVENT DETECTION", "text": "The sound event detection task involves identifying temporally the locations of sound event and assigning them to one among the known set of labels. Sound events in real life have no fixed pattern. Different contexts, for example, forest, city, and home have a different variety of sound events. They can be of different sparsity based on the context, and can occur in isolation or be completely overlapped with other sound events. While recognizing isolated sounds have been done with an appreciable accuracy [15], detecting the mixture of labels in an overlapped sound event is a challenging task, where still a considerable amount of improvements can be made. Figure 2 shows a snippet of sound event annotation, where three sound events - speech, car, and dog bark happen to occur. At time frame t, two events - speech and car are overlapping. An ideal SED system should be able to handle such overlapping events.\nThe human auditory system has been successfully exploiting the stereo (multichannel) audio information it receives at its ears to\nar X\niv :1\n70 6.\n02 29\n3v 1\n[ cs\n.S D\n] 7\nJ un\n2 01\n7\nisolate, localize and classify the sound events. A similar set up is envisioned and implemented, where the sound event detection system gets a stereo input and suitable spatial features are implemented to localize and classify sound events.\nThe proposed sound event detection system, shown in Figure 1, works on real life multichannel audio recordings and aims at detecting and classifying isolated and overlapping sound events.\nThree sets of features -log mel-band energies, pitch frequency, and its periodicity, and time difference of arrival (TDOA) in subbands, are extracted from the stereo audio. All features are extracted at a hop length of 20 ms to have consistency across features."}, {"heading": "2.1. Log mel-band energy", "text": "Log mel-band energies have been used for mono channel sound event detection extensively [8][9][16] and have proven to be good features. In the proposed system we continue to use log mel-band energies, and extract it for both the stereo channels. This is motivated from the idea that human auditory system exploits the interaural intensity difference (IID) for spatial localization of sound source [13]. Neural networks are capable of performing linear operations, which includes the difference. Therefore, when trained on the stereo log mel-band energy data, it will learn to obtain information similar to IID.\nEach channel of the audio is divided into 40 ms frames with 50% overlap using hamming window. Log mel-band energies are then extracted for each of the frames (mel in Table 1). We use 40 mel-bands spread across the entire spectrum."}, {"heading": "2.2. Harmonic features", "text": "The pitch is an important perceptual feature of sound. Human listeners have evolved to identify different sounds using the pitch cues, and can make efficient use of pitch to acoustically separate each of the mixture in an overlapping sound event [17]. Uzkent et al [18] have shown improvement in accuracy of non speech environmental sound detection used pitch range along with MFCC\u2019s. Here we propose using the absolute pitch and its periodicity as the features (pitch in Table 1).\nThe librosa implementation of pitch tracking [19] on thresholded parabolically-interpolated STFT [20] was used to estimate the pitch and periodicity.\nSince we are handling multi-label classification it is intuitive to identify as many dominant fundamental frequencies as possible and use them to identify the sound events. The periodicity feature gives the confidence measure for the extracted pitch value and helps the classifier to make better decisions based on pitch.\nThe overlapping sound events in the training data (Section 4.1) did not have more than three events overlapping at a time, hence we have limited ourselves to using the top three dominant pitch values\nper frame. So, for each of the channels, top three pitch values, and its respective periodicity values are extracted at every frame in 100- 4000 Hz frequency range (pitch3 in Table 1)."}, {"heading": "2.3. Time difference of arrival (TDOA) features", "text": "Overlapping sound events have forever troubled classification systems. This is mainly because the feature vector for the overlapped frame is a combination of different sound events. But, human listeners have been able to successfully identify each of the overlapping sound events by isolating and localizing the source spatially. This has been possible due to the interaural time delay (ITD) [13]\nEach sound event has its own frequency band, some occur in low frequencies, some in high, and some occur all across the frequency band. If we can divide the frequency spectrum into different bands, and identify the spatial location of the sound source in each of these bands, then this is an extra dimension of the feature, which the classifier can learn to estimate the number of possible sources in each frame, and their orientation in the space. We implement this by dividing the spectral frame into five mel-bands and calculating the time difference of arrival (TDOA) at each of these bands.\nFor example, if a non-overlapping isolated sound event is spread across the entire frequency range, and we are calculating the TDOA in five mel-bands. We should have the same TDOA values for each of the bands. However, if we have two overlapping sounds S1 and S2, where S1 is spread in the first two bands and S2 is spread in the last two bands. The feature vector will have different TDOA values for each of the sounds, which the classifier can learn to isolate and identify them as separate sound events.\nThe TDOA can be estimated using the generalized crosscorrelation with phase-based weighting (GCC-PHAT) [21]. Here, we extract the correlation for each mel-band separately:\nRb(\u220612, t) = N\u22121\u2211 k=0 Hb(k) X1(k, t) \u00b7X\u22172 (k, t) |X1(k, t)||X2(k, t)| ei2\u03c0k\u220612/N , (1)\nwhere N is the number of frequency bands, X(k, t) is the FFT coefficient of the kth frequency band at time frame t and the subscript specifies the channel number, Hb(k) is the magnitude response of the bth mel-band of total of B bands and \u220612 is the sample delay value between channels. The TDOA is extracted as the location of correlation peak magnitude for each mel-band and time frame.\n\u03c4(b, t) = argmax \u220612 {Rb(\u220612, t)} (2)\nThe maximum and minimum TDOA values are truncated between values \u22122\u03c4max, 2\u03c4max, where \u03c4max is the maximum sample delay between a sound wave traveling between microphones.\nThe sound events in the training set were seen to be varying from 50 ms to a few seconds. In order to accommodate such variable length sound events, TDOA was calculated in three different window lengths \u2014 120, 240 and 480 ms, with a constant hop length of 20 ms. The TDOA values of these three windows were concatenated for each mel-band to form one set of TDOA features. So, TDOA values extracted in five mel-band, and for three window lengths, on concatenation gives 15 TDOA values per frame (tdoa3 in Table 1).\nTDOA values in small windows are generally very noisy and unreliable. To overcome this, the median of the TDOA values from the above three different window lengths for each sub-band of the frame was used as the second set of TDOA features (tdoa in Table 1). Post filtering across window lengths, the TDOA values in each mel-band were also median filtered temporally using a kernel of length three to remove outliers."}, {"heading": "3. MULTI-LABEL RECURRENT NEURAL NETWORK BASED SOUND EVENT DETECTION", "text": "Deep neural networks have shown to perform well on complex pattern recognition tasks, such as speech recognition [22], image recognition [23] and machine translation [24]. A deep neural network typically computes a map from an input to an output space through several subsequent matrix multiplications and non-linear activation functions. The parameters of the model, i.e. its weights and biases, are iteratively adjusted using a form of optimization such as gradient descent.\nWhen the network is a directed acyclic graph, i.e. information is only propagated forward, it is known as a feedforward neural network (FNN). When there are feedback connections the model is called a recurrent neural network (RNN). An RNN can incorporate information from previous timesteps in its hidden layers, thus providing context information for tasks based on sequential data, such as temporal context in audio tasks. Complex RNN architectures \u2014 such as long short-term memory (LSTM) [25] \u2014 have been proposed in recent years in order to attenuate the vanishing gradient problem [26]. LSTM is currently the most widely used form of RNN, and the one used in this work as well.\nIn SED, RNNs can be used to predict probabilities for each class to be active in a given frame at timestep t. The input to the network is a sequence of feature vectors x(t); the network computes hidden activations for each hidden layer, and at the output layer a vector of predictions for each class y(t). A sigmoid activation function is used at the output layer in order to allow several classes to be predicted as active simultaneously. By thresholding the predictions at the output layer it is possible to obtain a binary activity matrix."}, {"heading": "3.1. Neural network configurations", "text": "For each recording, we obtain a sequence of feature vectors, which is normalized to zero mean and unit variance, and the scaling parameters are saved for normalizing the test feature vectors. The se-\nquences are further split into non-overlapping sequences of length 25 frames. Each of these frames has a target binary vector, indicating which classes are present in the feature vector.\nWe use a multi-label RNN-LSTM with two hidden layers each having 32 LSTM units. The number of units in the input layer depends on the length of the feature being used. The output layer has one neuron for each class. The network is trained by back propagation through time (BPTT) [27] using binary cross-entropy as loss function, Adam optimizer [28] and block mixing [9] data augmentation. Early stopping is used to reduce over-fitting, the training is halted if the segment based error rate (ER) (see Section 4.2) on the validation set does not decrease for 100 epochs.\nAt test time we use scaling parameters estimated on training data to scale the feature vectors and present them in nonoverlapping sequences of 25 frames, and threshold the outputs with a fixed threshold of 0.5, i.e., we mark an event is active if the posterior in the output layer of network is greater than 0.5 and otherwise inactive."}, {"heading": "4. EVALUATION AND RESULTS", "text": ""}, {"heading": "4.1. Dataset", "text": "We evaluate the proposed SED system on the development subset of TUT sound events detection 2016 database [1]. This database has stereo recordings which were collected using binaural Soundman OKM II Klassik/studio A3 electret in-ear microphones and Roland Edirol R09 wave recorder using 44.1 kHz sampling rate and 24-bit resolution. It contains two contexts - home and residential area. Home context has 10 recordings with 11 sound event classes and the residential area context has 12 recordings with 7 classes. The length of these recordings is between 3-5 minutes.\nIn the development subset provided, each of the context data is already partitioned into four folds of training and test data. The test data was collected such that each recording is used exactly once as the test, and the classes in it are always a subset of the classes in the training data. Also, 20% of the training data recordings in each fold were selected randomly to be used as validation data. The same validation data was used across all our evaluations."}, {"heading": "4.2. Metrics", "text": "We perform the evaluation of our system in a similar fashion as [1] which uses the established metrics for sound event detection defined in [30]. The error rate (ER) and F-scores are calculated on one second long segments. The results from all the folds are combined to produce a single evaluation. This is done to avoid biases caused due to data imbalance between folds as discussed in [31]."}, {"heading": "4.3. Results", "text": "The baseline system for the dataset [1] uses 20 static (excluding the 0th coefficient), 20 delta and 20 acceleration MFCC coefficients\nextracted on mono audio with 40 ms frames and 20 ms hop length. A Gaussian mixture model (GMM) consisting of 16 Gaussians is then trained for each of the positive and negative values of the class. This baseline system gives a context average ER of 0.91 and F-score of 23.%. An ideal system should have an ER of 0 and an F-score of 100%.\nIn Table 2 we compare the segment based ER and F-score for different combinations of proposed spatial and harmonic features. In all these evaluations, only the size of the input layer changes based on the feature set, with the rest of the configurations in the RNN-LSTM network remaining unchanged.\nMono channel audio was created by averaging the stereo channels in order to compare the performance of the proposed spatial and harmonic features for multichannel audio. One of the present state of the art SED system for mono channel is proposed in [9]. An RNN-LSTM network is trained in a similar fashion with log melband energy feature (Section 2.1) and evaluated. Across contexts, the F-score was seen to be better than the GMM baseline system with comparable ER. Here onwards we use this mono-channel log mel-band feature and RNN-LSTM network configuration result as a baseline for comparisons.\nA set of hybrid combinations were tried as shown in Table 2. All combinations other than mel1; tdoa performed better than the baseline across contexts in F-score.\nFinally, the full spectrum of proposed spatial and harmonic features were evaluated in different combinations with RNNLSTM network. With a couple of exceptions - mel2; pitch2 and mel2; tdoa3; pitch32, all the combinations of features performed equal to or better than the baseline in average F-scores, with marginally similar average ER as baseline. Given the dataset size of around 60 minutes, it is difficult to conclusively say that the binaural features are far superior to monaural features; but they surely look promising.\nBinaural features - mel2 and mel2; tdoa; pitch2 in Table 3 were submitted to the DCASE 2016 challenge [29], where they\nwere evaluated as the top performing systems. Monaural feature mel1 was submitted unofficially to compare the performance with binaural features. The hyper-parameters of the network were tuned before the submission, and hence the development set results in Table 3 are different from Table 2. Three hidden layers with 16 LSTM units each were used for mel2, while mel1 and mel2; tdoa; pitch2 were trained with two layers each having 16 LSTM units."}, {"heading": "5. CONCLUSION", "text": "In this paper, we proposed to use spatial and harmonic features for multi-label sound event detection along with RNN-LSTM networks. The evaluation was done on a limited dataset size of 60 mins, which included four cross validation data for two contexts \u2014 home and residential area. The proposed multi-channel features were seen to be performing substantially better than the baseline system using mono-channel features.\nFuture work will concentrate on finding novel data augmentation techniques. Augmenting spatial features is an unexplored space, and will be a challenge worth looking into. Concerning the model, further studies can be done on different configurations of RNN like extending them to bidirectional RNN\u2019s and coupling with convolutional neural networks."}, {"heading": "6. REFERENCES", "text": "[1] A. Mesaros, T. Heittola, and T. Virtanen, \u201cTUT database for acoustic scene classification and sound event detection,\u201d in In 24rd European Signal Processing Conference 2016 (EUSIPCO 2016), 2016.\n[2] S. Chu, S. Narayanan, and C. J. Kuo, \u201cEnvironmental sound recognition with timefrequency audio features,\u201d in IEEE Transactions on Audio, Speech, and Language Processing, vol. 17, no. 6, 2009, p. 1142.\n[3] S. Chu, S. Narayanan, C. C. J. Kuo, and M. J. Mataric, \u201cWhere am I? Scene recognition for mobile robots using audio features,\u201d in IEEE Int. Conf. Multimedia and Expo (ICME), 2006, p. 885.\n[4] A. Harma, M. F. McKinney, and J. Skowronek, \u201cAutomatic surveillance of the acoustic activity in our living environment,\u201d in IEEE International Conference on Multimedia and Expo (ICME), 2005.\n[5] M. Xu, C. Xu, L. Duan, J. S. Jin, and S. Luo, \u201cAudio keywords generation for sports video analysis,\u201d in ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 4, no. 2, 2008, p. 11.\n[6] T. Heittola, A. Mesaros, A. Eronen, and T. Virtanen, \u201cContextdependent sound event detection,\u201d in EURASIP Journal on Audio, Speech, and Music Processing, vol. 2013, no. 1, 2013, p. 1.\n[7] J. Dennis, H. D. Tran, and E. S. Chng, \u201cOverlapping sound event recognition using local spectrogram features and the generalised hough transform,\u201d in Pattern Recognition Letters, vol. 34, no. 9, 2013, p. 1085.\n[8] E. Cakir, T. Heittola, H. Huttunen, and T. Virtanen, \u201cPolyphonic sound event detection using multi-label deep neural networks,\u201d in IEEE International Joint Conference on Neural Networks (IJCNN), 2015.\n[9] G. Parascandolo, H. Huttunen, and T. Virtanen, \u201cRecurrent neural networks for polyphonic sound event detection in real life recordings,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016.\n[10] A. Mesaros, T. Heittola, A. Eronen, and T. Virtanen, \u201cAcoustic event detection in real-life recordings,\u201d in 18th European Signal Processing Conference (EUSIPCO 2010), Aalborg, Denmark, 2010, pp. 1267\u20131271.\n[11] T. Heittola, A. Mesaros, T. Virtanen, and M. Gabbouj, \u201cSupervised model training for overlapping sound events based on unsupervised source separation,\u201d in Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP), Vancouver, Canada, 2013,, p. 8677.\n[12] O. Dikmen and A. Mesaros, \u201cSound event detection using non-negative dictionaries learned from annotated overlapping events,\u201d in IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2013,, p. 1.\n[13] J. W. Strutt, \u201cOn our perception of sound direction,\u201d in Philosophical Magazine, vol. 13, 1907,, p. 214.\n[14] X. Xiao, S. Watanabe, H. Erdogan, L. Lu, J. Hershey, M. L. Seltzer, G. Chen, Y. Zhang, M. Mandel, and DongYu, \u201cDeep beamforming networks for multi-channel speech recognition,\u201d in ICASSP, 2016,.\n[15] O. Gencoglu, T. Virtanen, and H. Huttunen, \u201cRecognition of acoustic events using deep neural networks,\u201d in European Signal Processing Conference (EUSIPCO 2014), 2014,.\n[16] T. N. Sainath, O. Vinyals, A. Senior, and H. Sak, \u201cConvolutional, long short-term memory, fully connected deep neural networks,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.\n[17] A. S. Bregman, \u201cAuditory scene analysis: The perceptual organization of sound,\u201d in MIT Press, Cambridge, MA, 1990.\n[18] B. Uzkent, B. D. Barkana, and H. Cevikalp, \u201cNon-speech environmental sound classification using SVM\u2019s with a new set of features,\u201d in International Journal of Innovative Computing, Information and Control, 2012, p. 3511.\n[19] B. McFee, M. McVicar, C. Raffel, D. Liang, O. Nieto, E. Battenberg, J. Moore, D. Ellis, R. YAMAMOTO, R. Bittner, D. Repetto, P. Viktorin, J. F. Santos, and A. Holovaty, \u201clibrosa: 0.4.1,\u201d Oct. 2015. [Online]. Available: http://dx.doi.org/10.5281/zenodo.32193\n[20] J. O. Smith, Sinusoidal Peak Interpolation, in Spectral Audio Signal Processing, accessed 23.06.2016, online book, 2011 edition. [Online]. Available: https://ccrma.stanford.edu/\u223cjos/ sasp/Sinusoidal Peak Interpolation.htm\n[21] C. Knapp and G. Carter, \u201cThe generalized correlation method for estimation of time delay,\u201d IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 24, no. 4, pp. 320\u2013327, Aug 1976.\n[22] A. Graves, A. Mohamed, and G. Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2013, pp. 6645\u20136649.\n[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImagenet classification with deep convolutional neural networks,\u201d in Advances in neural information processing systems, 2012, pp. 1097\u20131105.\n[24] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine translation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014.\n[25] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[26] Y. Bengio, P. Simard, and P. Frasconi, \u201cLearning long-term dependencies with gradient descent is difficult,\u201d IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157\u2013166, 1994.\n[27] P. J. Werbos, \u201cBackpropagation through time: what it does and how to do it,\u201d in Proceedings of the IEEE,, vol. 78 no. 10, 1990, p. 15501560.\n[28] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in arXiv:1412.6980 [cs.LG], December, 2014.\n[29] \u201cDetection and classification of acoustic scenes and events,\u201d 2016. [Online]. Available: http://www.cs.tut.fi/sgn/ arg/dcase2016/task-sound-event-detection-in-real-life-audio\n[30] A. Mesaros, T. Heittola, and T. Virtanen, \u201cMetrics for polyphonic sound event detection,\u201d in Applied Sciences, vol. 6(6):162, 2016.\n[31] G. Forman and M. Scholz, \u201cApples-to-apples in cross validation studies: Pitfalls in classifier performance measurement,\u201d in SIGKDD Explor. Newsl., vol. 12, no. 1, Nov. 2010, p. 49."}], "references": [{"title": "TUT database for acoustic scene classification and sound event detection", "author": ["A. Mesaros", "T. Heittola", "T. Virtanen"], "venue": "In 24rd European Signal Processing Conference 2016 (EU- SIPCO 2016), 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Environmental sound recognition with timefrequency audio features", "author": ["S. Chu", "S. Narayanan", "C.J. Kuo"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 17, no. 6, 2009, p. 1142.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Where am I? Scene recognition for mobile robots using audio features", "author": ["S. Chu", "S. Narayanan", "C.C.J. Kuo", "M.J. Mataric"], "venue": "IEEE Int. Conf. Multimedia and Expo (ICME), 2006, p. 885.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Automatic surveillance of the acoustic activity in our living environment", "author": ["A. Harma", "M.F. McKinney", "J. Skowronek"], "venue": "IEEE International Conference on Multimedia and Expo (ICME), 2005.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Audio keywords generation for sports video analysis", "author": ["M. Xu", "C. Xu", "L. Duan", "J.S. Jin", "S. Luo"], "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 4, no. 2, 2008, p. 11.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Contextdependent sound event detection", "author": ["T. Heittola", "A. Mesaros", "A. Eronen", "T. Virtanen"], "venue": "EURASIP Journal on Audio, Speech, and Music Processing, vol. 2013, no. 1, 2013, p. 1.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Overlapping sound event recognition using local spectrogram features and the generalised hough transform", "author": ["J. Dennis", "H.D. Tran", "E.S. Chng"], "venue": "Pattern Recognition Letters, vol. 34, no. 9, 2013, p. 1085.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Polyphonic sound event detection using multi-label deep neural networks", "author": ["E. Cakir", "T. Heittola", "H. Huttunen", "T. Virtanen"], "venue": "IEEE International Joint Conference on Neural Networks (IJCNN), 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural networks for polyphonic sound event detection in real life recordings", "author": ["G. Parascandolo", "H. Huttunen", "T. Virtanen"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Acoustic event detection in real-life recordings", "author": ["A. Mesaros", "T. Heittola", "A. Eronen", "T. Virtanen"], "venue": "18th European Signal Processing Conference (EUSIPCO 2010), Aalborg, Denmark, 2010, pp. 1267\u20131271.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Supervised model training for overlapping sound events based on unsupervised source separation", "author": ["T. Heittola", "A. Mesaros", "T. Virtanen", "M. Gabbouj"], "venue": "Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP), Vancouver, Canada, 2013,, p. 8677.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Sound event detection using non-negative dictionaries learned from annotated overlapping events", "author": ["O. Dikmen", "A. Mesaros"], "venue": "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2013,, p. 1.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "On our perception of sound direction", "author": ["J.W. Strutt"], "venue": "Philosophical Magazine, vol. 13, 1907,, p. 214.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1907}, {"title": "Deep beamforming networks for multi-channel speech recognition", "author": ["X. Xiao", "S. Watanabe", "H. Erdogan", "L. Lu", "J. Hershey", "M.L. Seltzer", "G. Chen", "Y. Zhang", "M. Mandel", "DongYu"], "venue": "ICASSP, 2016,.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Recognition of acoustic events using deep neural networks", "author": ["O. Gencoglu", "T. Virtanen", "H. Huttunen"], "venue": "European Signal Processing Conference (EUSIPCO 2014), 2014,.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Auditory scene analysis: The perceptual organization of sound", "author": ["A.S. Bregman"], "venue": "MIT Press, Cambridge, MA, 1990.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1990}, {"title": "Non-speech environmental sound classification using SVM\u2019s with a new set of features", "author": ["B. Uzkent", "B.D. Barkana", "H. Cevikalp"], "venue": "International Journal of Innovative Computing, Information and Control, 2012, p. 3511.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "librosa: 0.4.1", "author": ["B. McFee", "M. McVicar", "C. Raffel", "D. Liang", "O. Nieto", "E. Battenberg", "J. Moore", "D. Ellis", "R. YAMAMOTO", "R. Bittner", "D. Repetto", "P. Viktorin", "J.F. Santos", "A. Holovaty"], "venue": "Oct. 2015. [Online]. Available: http://dx.doi.org/10.5281/zenodo.32193", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Sinusoidal Peak Interpolation, in Spectral Audio Signal Processing, accessed 23.06.2016, online book, 2011 edition. [Online]. Available: https://ccrma.stanford.edu/\u223cjos/ sasp/Sinusoidal Peak Interpolation.htm", "author": ["J.O. Smith"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "The generalized correlation method for estimation of time delay", "author": ["C. Knapp", "G. Carter"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 24, no. 4, pp. 320\u2013327, Aug 1976.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1976}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157\u2013166, 1994.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1994}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proceedings of the IEEE,, vol. 78 no. 10, 1990, p. 15501560.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1990}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv:1412.6980 [cs.LG], December, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Metrics for polyphonic sound event detection", "author": ["A. Mesaros", "T. Heittola", "T. Virtanen"], "venue": "Applied Sciences, vol. 6(6):162, 2016.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Apples-to-apples in cross validation studies: Pitfalls in classifier performance measurement", "author": ["G. Forman", "M. Scholz"], "venue": "SIGKDD Explor. Newsl., vol. 12, no. 1, Nov. 2010, p. 49.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "The proposed SED system is compared against the state of the art mono channel method on the development subset of TUT sound events detection 2016 database [1].", "startOffset": 155, "endOffset": 158}, {"referenceID": 1, "context": "The applications of such automatic sound event detection (SED) are numerous; embedded systems with listening capability can become more aware of its environment [2][3].", "startOffset": 161, "endOffset": 164}, {"referenceID": 2, "context": "The applications of such automatic sound event detection (SED) are numerous; embedded systems with listening capability can become more aware of its environment [2][3].", "startOffset": 164, "endOffset": 167}, {"referenceID": 3, "context": "Industrial and environmental surveillance systems and smart homes can start automatically detecting events of interest [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 4, "context": "Automatic annotation of multimedia can enable better retrieval for content based query methods [5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 5, "context": "Monophonic systems are trained to recognize the most dominant of the sound events in the audio signal [6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 5, "context": "While polyphonic systems go beyond the most dominant sound event and recognize all the overlapping sound events in a segment [6][7][8][9].", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "While polyphonic systems go beyond the most dominant sound event and recognize all the overlapping sound events in a segment [6][7][8][9].", "startOffset": 128, "endOffset": 131}, {"referenceID": 7, "context": "While polyphonic systems go beyond the most dominant sound event and recognize all the overlapping sound events in a segment [6][7][8][9].", "startOffset": 131, "endOffset": 134}, {"referenceID": 8, "context": "While polyphonic systems go beyond the most dominant sound event and recognize all the overlapping sound events in a segment [6][7][8][9].", "startOffset": 134, "endOffset": 137}, {"referenceID": 9, "context": "algorithm [10].", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "In [11], a non-negative matrix factorization was used as a pre-processing step, and the most prominent event in each of the stream was detected.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "This was overcome by using coupled NMF in [12].", "startOffset": 42, "endOffset": 46}, {"referenceID": 6, "context": "Dennis et al [7] took an entirely different path from the traditional frame-based features by combining generalized Hough transform (GHT) with local spectral features.", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "More recently, the state of the art SED systems have used log mel-band energy features in DNN [8], and RNN-LSTM [9] networks trained for multi-label classification.", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "More recently, the state of the art SED systems have used log mel-band energy features in DNN [8], and RNN-LSTM [9] networks trained for multi-label classification.", "startOffset": 112, "endOffset": 115}, {"referenceID": 8, "context": "Motivated by the good performance of RNN-LSTM over DNN as shown in [9], we continue to use the multi-label RNN-LSTM network.", "startOffset": 67, "endOffset": 70}, {"referenceID": 12, "context": "Just like humans use their two ears (two channels) to recognize and localize the sound events around them [13], we can also potentially train machines to learn sound events from multichannel of audio.", "startOffset": 106, "endOffset": 110}, {"referenceID": 13, "context": "Recently, Xiao et al [14] have successfully used spatial features from multichannel audio for far field automatic speech recognition (ASR) and shown considerable improvements over just using mono channel audio.", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "While recognizing isolated sounds have been done with an appreciable accuracy [15], detecting the mixture of labels in an overlapped sound event is a challenging task, where still a considerable amount of improvements can be made.", "startOffset": 78, "endOffset": 82}, {"referenceID": 7, "context": "Log mel-band energies have been used for mono channel sound event detection extensively [8][9][16] and have proven to be good features.", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "Log mel-band energies have been used for mono channel sound event detection extensively [8][9][16] and have proven to be good features.", "startOffset": 91, "endOffset": 94}, {"referenceID": 15, "context": "Log mel-band energies have been used for mono channel sound event detection extensively [8][9][16] and have proven to be good features.", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": "This is motivated from the idea that human auditory system exploits the interaural intensity difference (IID) for spatial localization of sound source [13].", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "Human listeners have evolved to identify different sounds using the pitch cues, and can make efficient use of pitch to acoustically separate each of the mixture in an overlapping sound event [17].", "startOffset": 191, "endOffset": 195}, {"referenceID": 17, "context": "Uzkent et al [18] have shown improvement in accuracy of non speech environmental sound detection used pitch range along with MFCC\u2019s.", "startOffset": 13, "endOffset": 17}, {"referenceID": 18, "context": "The librosa implementation of pitch tracking [19] on thresholded parabolically-interpolated STFT [20] was used to estimate the pitch and periodicity.", "startOffset": 45, "endOffset": 49}, {"referenceID": 19, "context": "The librosa implementation of pitch tracking [19] on thresholded parabolically-interpolated STFT [20] was used to estimate the pitch and periodicity.", "startOffset": 97, "endOffset": 101}, {"referenceID": 12, "context": "This has been possible due to the interaural time delay (ITD) [13] Each sound event has its own frequency band, some occur in low frequencies, some in high, and some occur all across the frequency band.", "startOffset": 62, "endOffset": 66}, {"referenceID": 20, "context": "The TDOA can be estimated using the generalized crosscorrelation with phase-based weighting (GCC-PHAT) [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 21, "context": "Deep neural networks have shown to perform well on complex pattern recognition tasks, such as speech recognition [22], image recognition [23] and machine translation [24].", "startOffset": 113, "endOffset": 117}, {"referenceID": 22, "context": "Deep neural networks have shown to perform well on complex pattern recognition tasks, such as speech recognition [22], image recognition [23] and machine translation [24].", "startOffset": 137, "endOffset": 141}, {"referenceID": 23, "context": "Deep neural networks have shown to perform well on complex pattern recognition tasks, such as speech recognition [22], image recognition [23] and machine translation [24].", "startOffset": 166, "endOffset": 170}, {"referenceID": 24, "context": "Complex RNN architectures \u2014 such as long short-term memory (LSTM) [25] \u2014 have been proposed in recent years in order to attenuate the vanishing gradient problem [26].", "startOffset": 66, "endOffset": 70}, {"referenceID": 25, "context": "Complex RNN architectures \u2014 such as long short-term memory (LSTM) [25] \u2014 have been proposed in recent years in order to attenuate the vanishing gradient problem [26].", "startOffset": 161, "endOffset": 165}, {"referenceID": 26, "context": "The network is trained by back propagation through time (BPTT) [27] using binary cross-entropy as loss function, Adam optimizer [28] and block mixing [9] data augmentation.", "startOffset": 63, "endOffset": 67}, {"referenceID": 27, "context": "The network is trained by back propagation through time (BPTT) [27] using binary cross-entropy as loss function, Adam optimizer [28] and block mixing [9] data augmentation.", "startOffset": 128, "endOffset": 132}, {"referenceID": 8, "context": "The network is trained by back propagation through time (BPTT) [27] using binary cross-entropy as loss function, Adam optimizer [28] and block mixing [9] data augmentation.", "startOffset": 150, "endOffset": 153}, {"referenceID": 0, "context": "We evaluate the proposed SED system on the development subset of TUT sound events detection 2016 database [1].", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "We perform the evaluation of our system in a similar fashion as [1] which uses the established metrics for sound event detection defined in [30].", "startOffset": 64, "endOffset": 67}, {"referenceID": 28, "context": "We perform the evaluation of our system in a similar fashion as [1] which uses the established metrics for sound event detection defined in [30].", "startOffset": 140, "endOffset": 144}, {"referenceID": 29, "context": "This is done to avoid biases caused due to data imbalance between folds as discussed in [31].", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "The baseline system for the dataset [1] uses 20 static (excluding the 0th coefficient), 20 delta and 20 acceleration MFCC coefficients", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "Baseline system using GMM classifier in DCASE 2016 [1][29] mfcc; delta; acc 0.", "startOffset": 51, "endOffset": 54}, {"referenceID": 8, "context": "One of the present state of the art SED system for mono channel is proposed in [9].", "startOffset": 79, "endOffset": 82}], "year": 2017, "abstractText": "In this paper, we propose the use of spatial and harmonic features in combination with long short term memory (LSTM) recurrent neural network (RNN) for automatic sound event detection (SED) task. Real life sound recordings typically have many overlapping sound events, making it hard to recognize with just mono channel audio. Human listeners have been successfully recognizing the mixture of overlapping sound events using pitch cues and exploiting the stereo (multichannel) audio signal available at their ears to spatially localize these events. Traditionally SED systems have only been using mono channel audio, motivated by the human listener we propose to extend them to use multichannel audio. The proposed SED system is compared against the state of the art mono channel method on the development subset of TUT sound events detection 2016 database [1]. The usage of spatial and harmonic features are shown to improve the performance of SED.", "creator": "LaTeX with hyperref package"}}}