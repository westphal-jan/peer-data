{"id": "1706.02949", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "K+ Means : An Enhancement Over K-Means Clustering Algorithm", "abstract": "gomhuria K - means (MacQueen, 1967) [colaba 1] mcclymonds is kannauj one of the simplest stro unsupervised learning algorithms that kanagal solve ncvs the darrowby well - co-prosperity known clustering problem. The procedure follows a simple c02 and easy oppo way to admission classify farruquito a 7.13 given data set eweek to a obscure predefined, say gundappa K secura number oxymoronic of mianchi clusters. overestimated Determination liber of erciyes K moiraine is 4-2-0 a difficult iravan job and 46.22 it barrack is not zabirova known that which value codice of K can vassilev partition ogbonnaya the salon-de-provence objects as 70-plus per three-way our rhein intuition. schmuhl To overcome cataluna this patter problem we anemic proposed cockerell K + maschio Means eho algorithm. turd This r-ks algorithm zasiadko is secretar\u00eda an enhancement khyber-pakhtunkhwa over K - ladling Means 1,282 algorithm.", "histories": [["v1", "Thu, 8 Jun 2017 10:06:25 GMT  (109kb)", "http://arxiv.org/abs/1706.02949v1", null], ["v2", "Thu, 22 Jun 2017 13:25:51 GMT  (87kb)", "http://arxiv.org/abs/1706.02949v2", "Authors: Co-author's name added Section 3: Step (a) and (b) of K+Means algorithm are merged for simplicity. Section 3.1: K+ Means algorithm complexity rectified. Section 4.3: Figure-7 and Figure-8 modified for clarity"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["srikanta kolay", "kumar sankar ray", "abhoy chand mondal"], "accepted": false, "id": "1706.02949"}, "pdf": {"name": "1706.02949.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Srikanta Kolay"], "emails": ["kolaysrikanta@gmail.com", "ksray@isical.ac.in"], "sections": [{"heading": null, "text": "that solve the well-known clustering problem. The procedure follows a simple and easy way to classify a given data set to a predefined, say K number of clusters. Determination of K is a difficult job and it is not known that which value of K can partition the objects as per our intuition. To overcome this problem we proposed K+ Means algorithm. This algorithm is an enhancement over K-Means algorithm.\nKeywords: K-Means, K+ Means, Clustering."}, {"heading": "1. Introduction", "text": "Hierarchical clustering[2][3], Partitional clustering[4], Bayesian clustering[5] are the different kind of clustering techniques. K-Means clustering algorithm is considered as the most popular clustering algorithm due to its simplicity and efficiency. This is the reason why we take K-Means algorithm for enhancement. We enhance the algorithm to K+ Means by taking the strengths of the algorithm and eliminated the weaknesses of the algorithm."}, {"heading": "2. K-Means Algorithm", "text": "(a) Place K points into the space represented by the objects that are being clustered. These points represent initial group centroids. (b) Assign each object to the group that has the closest centroid. (c) When all objects have been assigned, recalculate the positions of the K centroids. (d) Repeat Steps (b) and (c) until the centroids no longer move. This produces a\nseparation of the objects into groups from which the metric to be minimized can be calculated.\nThe distance between objects are taken as Euclidian distance."}, {"heading": "2.1. Strengths of K-Means Algorithm", "text": "The strengths of the algorithm is as follows:\n(a) Simple: easy to understand and to implement. (b) Efficient: Time complexity: O(tkn), where n is the number of data points, k is the\nnumber of clusters, and t is the number of iterations. Since both k and t are small K-Means is considered a linear algorithm."}, {"heading": "2.2. Weaknesses of K-Means Algorithm", "text": "The weaknesses of the algorithm is as follows:\n(a) The user needs to specify K. (b) The algorithm is sensitive to outliers. Outliers are data points that are very far\naway from other data points."}, {"heading": "3. K+ Means Algorithm", "text": "(a) Place K points into the space represented by the objects that are being clustered. These points represent initial group centroids. (b) Assign each object to the group that has the closest centroid. (c) Min, Max and Average intra cluster dissimilarity for each of the K clusters is\ncomputed.\n(d) The average intra cluster distance is expected to be almost similar and preferably small for each of the K clusters. (e) If for any cluster, the average value is more, its Max and Min values are checked. If the Max value is high, then outlier object is detected, as it has the maximum\ndistance from its cluster representative.\n(f) Now, taking this outlier as another new representative, the algorithm is repeated to assign objects to K+1 cluster. (g) The algorithm is repeated until no more new representatives are formed and existing representatives do not change.\nThe distance between objects are taken as Euclidian distance."}, {"heading": "3.1. Strengths of K+ Means Algorithm", "text": "The strengths of the algorithm is as follows:\n(a) Simple: easy to understand and to implement. (b) Efficient: Time complexity: O(tkn), where n is the number of data points, k is the\nnumber of clusters, and t is the number of iterations. Here the value of t is higher than K-Means algorithm.\n(c) The user only need to specify initial k value. Actual number of clusters will be obtained based on the data.\n(d) The algorithm is not sensitive to outliers. In case of outliers, it will define a new cluster."}, {"heading": "4. An Worked Out Example", "text": "Consider an example dataset shown in table-1 below:\nObject/Point x y\np1 1 4 p2 1 3 p3 2 2 p4 7 2\np5 8 3 p6 9 2 p7 5 6 p8 6 7 p9 7 6\np10 8 7\nBased on the above table the objects are plotted in 2D space as shown in figure-1 below:\nFigure-1: Example Data"}, {"heading": "4. 1. Application of K-Means Algorithm", "text": "Let us assume K=2 and select p1 and p5 as initial centroid. Now if we run K-means algorithm, we get Cluster 1 with {p1,p2,p3} and Cluster 2 with {P4,p5,p6,p7,p8,p9,p10}. The points (objects) and cluster centroids after running the K-means algorithm is shown in the figure-2 below:\nFigure-2: Clusters after running K-means algorithm"}, {"heading": "4. 2. Application of K+ Means Algorithm", "text": "Let us assume K=2 and select p1 and p5 as initial centroid. Based on that we get the initial clusters as shown in the figure-3 below:\nFigure-3: Initial clusters\nNow we calculate the new centroids and get the clusters as shown in figure-4 below:\nFigure-4: New cluster centroids\nNow we calculate the intra-cluster distances of c1 and c2. Consider c1: Min=0.33, Max=1.20, Avg=0.86 Consider c2: Min=1.30, Max=3.29 Avg=2.39\nHere the avg value is comparatively high for c2. Max value is also high. So, we get the outlier object p6 as d(c2,p6)=3.29.\nTaking c3(9,2) as new cluster centroid, we reassign the objects in c1, c2 and c3 as shown in figure-5 below:\nFigure-5: Initials centroids c1,c2,c3\nNow we recalculate the centroids as shown in the figure-6 below:\nFigure-6: Modified centroids c1,c2,c3\nNow we calculate the intra-cluster distances for c1, c2 and c3. Consider c1: Min=0.33, Max=1.20, Avg=0.86 Consider c2: Min=0.71, Max=1.58, Avg=1.14 Consider c3: Min=0.67, Max=1.05, Avg=0.92 Here avg intra-cluster distances are similar for all three clusters, so we stop here. So finally we get three clusters cluster 1={p1,p2,p3}, cluster2={p7,p8,p9,p10} and cluster3={p4,p5,p6}"}, {"heading": "4.3. Comparison:", "text": "K+ Means algorithm gives better clusters than K-Means algorithm. As the number of iteration in K+ Means algorithm is higher than the number of iteration of K-Means algorithm, K+ Means algorithm is little costly than K-Means. The below example shows how for outlier object two algorithms perform. Figure-7 shows how K-Means algorithm group objects when we run the algorithm taking K=2.\nFigure-7: Grouping through K-Means algorithm\nFigure-8 shows how K+ Means algorithm group objects when we run the algorithm taking K=2.\nFigure-8: Grouping through K+ Means algorithm"}, {"heading": "9. Conclusion", "text": "In this paper we enhanced the K-Means algorithm to develop a new K+ Means algorithm. In this K+ Means algorithm all the strengths of K-Means algorithm is taken and the weaknesses of K-Means algorithms are removed."}], "references": [{"title": "Some methods for classification and analysis of multivariate observations", "author": ["J. MCQUEEN"], "venue": "In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1967}, {"title": "Bayesian clustering using hidden Markov random fields in spatial population", "author": ["O Fran\u00e7ois", "S Ancelet", "G Guillot"], "venue": "genetics. Genetics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Abstract K-means (MacQueen, 1967) [1] is one of the simplest unsupervised learning algorithms that solve the well-known clustering problem.", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "Introduction Hierarchical clustering[2][3], Partitional clustering[4], Bayesian clustering[5] are the different kind of clustering techniques.", "startOffset": 90, "endOffset": 93}], "year": 2017, "abstractText": "K-means (MacQueen, 1967) [1] is one of the simplest unsupervised learning algorithms that solve the well-known clustering problem. The procedure follows a simple and easy way to classify a given data set to a predefined, say K number of clusters. Determination of K is a difficult job and it is not known that which value of K can partition the objects as per our intuition. To overcome this problem we proposed K+ Means algorithm. This algorithm is an enhancement over K-Means algorithm.", "creator": "PScript5.dll Version 5.2.2"}}}