{"id": "1706.03041", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2017", "title": "Learning optimal wavelet bases using a neural network approach", "abstract": "213.3 A piccard novel method qmc for alro learning t-bane optimal, temmerman orthonormal wavelet bases nessi for toyad representing heliborne 1 - misr and 2D sextante signals, based on lubinski parallels between the mirer wavelet patek transform and expedia.com fully thakin connected artificial dikhil neural networks, ece is 1.063 described. The structural shibergan similarities between vanger these sreerema two mastertronic concepts are khamzat reviewed snooping and combined shiong to falletti a \" wavenet \", astronomy allowing jemini for the direct learning spiliotopoulos of aljunied optimal uea wavelet filter coefficient through stochastic pgf gradient descent unrwa with worthier back - propagation over ensembles sound-on-film of 1728 training 80.9 inputs, umlaut where conditions on invasives the filter coefficients rebooting for vbe constituting mireia orthonormal wavelet kharqan bases two-book are cast as kunduz quadratic 218 regularisations imanishi terms. merrymeeting We 44.45 describe the promo-only practical tubay implementation ramli of supergroups this piratical method, and study 2,546 its eighteenth-century performance for high - energy physics collision events for radlett QCD $ neurolinguistics 2 \\ jayaram to yiwen 2 $ yuehua processes. It is ndeti shown that 119.60 an 1,273 optimal nasgovitz solution hewes is golddigger found, even fieldy in a high - mourlot dimensional glenshaw search space, askin and the simoom implications of epithets the lhsaa result are break-away discussed.", "histories": [["v1", "Sat, 25 Mar 2017 15:46:01 GMT  (1077kb,D)", "http://arxiv.org/abs/1706.03041v1", "11 pages, 9 figures"]], "COMMENTS": "11 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["andreas s{\\o}gaard"], "accepted": false, "id": "1706.03041"}, "pdf": {"name": "1706.03041.pdf", "metadata": {"source": "CRF", "title": "Learning optimal wavelet bases using a neural network approach", "authors": ["Andreas S\u00f8gaard"], "emails": ["andreas.sogaard@ed.ac.uk"], "sections": [{"heading": null, "text": "Learning optimal wavelet bases using a neural network approach\nAndreas S\u00f8gaard\u2217\nSchool of Physics and Astronomy, University of Edinburgh\nA novel method for learning optimal, orthonormal wavelet bases for representing 1- and 2D signals, based on parallels between the wavelet transform and fully connected artificial neural networks, is described. The structural similarities between these two concepts are reviewed and combined to a \u201cwavenet\u201d, allowing for the direct learning of optimal wavelet filter coefficient through stochastic gradient descent with back-propagation over ensembles of training inputs, where conditions on the filter coefficients for constituting orthonormal wavelet bases are cast as quadratic regularisations terms. We describe the practical implementation of this method [1], and study its performance for high-energy physics collision events for QCD 2 \u2192 2 processes. It is shown that an optimal solution is found, even in a high-dimensional search space, and the implications of the result are discussed.\nI. INTRODUCTION\n1The Fourier transform has proved an indispensable tool within the natural sciences, allowing for the study of frequency information of functions and for the efficient representation of signals exhibiting angular structure. However, the Fourier transform is limited by being global: each frequency component carries no information about its spatial localisation; information which might be valuable. Multiresolution, and in particular wavelet, analysis has been developed, in part, to address this limitation, representing a function at various levels of resolution, or at different frequency scales, while retaining information about position-space localisation. This encoding uses the fact that due to their smaller wavelengths, highfrequency components may be localised more precisely than their low-frequency counterparts.\nThe wavelet decomposition expresses any given signal in terms of a \u201cfamily\u201d of orthonormal basis functions [3, 4], efficiently encoding frequencyposition information. Several different such wavelet families exist, both for continuous and discrete input, but these are generally quite difficult to construct exactly as they don\u2019t possess closed-form representations. Furthermore, the best basis function for any given problem depends on the class of signal, choosing the best among existing functional families is hard and likely sub-optimal, and constructing new bases is non-trivial, as mentioned above. Therefore, we present a practical, efficient method for directly learning the\n\u2217Electronic address: andreas.sogaard@ed.ac.uk 1 Sections I and III contain overlaps with [2].\nbest wavelet bases, according to some optimality criterion, by exploiting the intimate relationship between neural networks and the wavelet transform.\nSuch a method could have potential uses e.g. in areas utilising time-series data and imaging, for instance \u2014 but not limited to \u2014 EEG, speech recognition, seismographic studies, financial markets as well as image compression, feature extraction, and de-noising. However, as is shown in Section VII, the areas to which such an approach can be applied are quite varied.\nIn Section II we review some of the work previously done along these lines. In Section III we briefly describe wavelet analyses, neural networks, as well as their structural similarity and how they can be combined. In Section IV we discuss metrics appropriate for measuring the quality of a certain wavelet basis. In Section V we describe the actual algorithm for learning optimal wavelet bases. Section VI describes the practical implementation and, finally, Section VII provides an example use case from high-energy physics."}, {"heading": "II. PREVIOUS WORK", "text": "A typical approach [5\u20137] when faced with the task of choosing a wavelet basis in which to represent some class of signals, is to select one among an existing set wavelet families, which is deemed suitable to the particular use case based on some measure of fitness. This might lead to sub-optimal results, as mentioned above, since limiting the search to a few dozen pre-exiting wavelet families will likely result in inefficient encoding or representation of (possibly subtle) structure particular, or unique, to the problem at hand. To address this shortcoming, considerable effort has already gone\nar X\niv :1\n70 6.\n03 04\n1v 1\n[ cs\n.N E\n] 2\n5 M\nar 2\n01 7\n2 into the question of the existence and construction of optimal wavelet bases.\nRef. [8] describes a method for constructing optimally matched wavelets, i.e. wavelet bases matching a prescribed pattern as closely as possible, through lifting [9]. However, the proposed method is somewhat arduous and relies on the specification of a pattern to which to match, requiring considerable and somewhat artificial preprocessing2. This is not necessarily possible, let alone easy, for many use cases as well as for the study of more general classes of inputs rather than single examples. In a similar vein, Ref. [10] provides a method for unconstrained optimisation of a wavelet basis with respect to a sparsity measure using lifting, but has the same limitations as Ref. [8].\nRefs. [11, 12] provide theoretical arguments for the existence of optimal wavelet bases as well as an algorithm for constructing such a basis for single 1- or 2D inputs, based on gradient descent. However, results are only presented for low-order wavelet bases, the implementation of orthonormality constraints is not discussed, and the question of generalisation from single inputs to classes of inputs is not addressed. In addition, the optimal filter coefficients referenced in [12,\nTable 1] do not satisfy the explicit conditions (C2), (C3), and (C4) for orthonormality in Section III A below. These constraints are violated at the 1%-level, which also corresponds roughly to the relative angular deviation of the reported optimal basis from the Daubechies [13] basis of similar order.\nFinally, Refs. [14, 15] provide a comprehensive prescription for designing wavelets that optimally represent signals, or classes of signals, at some fixed scale J. However, the results are quite cumbersome, are based on a number of assumptions regarding the characteristics of the input signal(s), and relate only to the question of optimal representation at fixed scales.\nThis indicates that, although the question of constructing optimal wavelet bases has been given substantial consideration, and clear developments have been made already, a general approach to easily learning discrete, demonstrably orthonormal wavelet bases of arbitrary structure and complexity, optimised over classes of input has yet to be developed and implemented for practically arbitrary choice of optimality metric. This is what is done below.\n2 \u201cIt is difficult to find a problem our method can be applied to without major modifications.\u201d [8, p. 125]."}, {"heading": "III. THEORICAL CONCEPTS", "text": "In this section, we briefly review some of the underlying aspects of wavelet analysis, Section III A, and neural networks, Section III B, upon which the learning algorithm is based. In Section III C we discuss the parallels between the two concepts, and how these can be used to directly learn optimal wavelet bases."}, {"heading": "A. Wavelet", "text": "Numerous excellent references explain multiresolution analysis and the wavelet transform in depth, so the present text will focus on the discrete class of wavelet transforms, formulated in the language of matrix algebra as it relates directly to the task at hand. For a more complete review, see e.g. [2] or [13, 16\u201319].\nIn the parlance of matrix algebra, the simplest possible input signal f \u2208 RN is a column vector3\nf =  f[0] f[1] ...\nf[2M \u2212 2] f[2M \u2212 1]\n (1)\nand the dyadic structure of the wavelet transform means that N must be radix 2, i.e. N = 2M for some M \u2208 N0. The forward wavelet transform is then performed by the iterative application of low- and highpass filters. Let L(f) denote the low-pass filtering of input f, the i\u2019th entry of which is then given by the convolution\nL(f)[i] = 2M\u22121\u2211 k=0 a[k]f[i + N/2 \u2212 k], i \u2208 [0, 2M\u22121 \u2212 1]\n(2) assuming periodicity, such that f[\u22121] = f[N \u2212 1], etc. The low-pass filter, a, is represented as a row vector of length Nfilt, with Nfilt even, and its entries are called the filter coefficients, {a}.\nThe convolution yielding each entry i in L(f) can be seen as a matrix inner product of f with a row matrix of the form[\n\u00b7 \u00b7 \u00b7 0 a[N \u2212 1] \u00b7 \u00b7 \u00b7 a[1] a[0] 0 \u00b7 \u00b7 \u00b7 ]\n(3)\n3 Although the results below are also applicable to 2D, i.e. matrix, input, cf. Section VII.\n3 Since this is true for each entry, the full low-pass filter may be represented as a (2M\u22121 \u00d7 2M) \u00b7 (2M \u00d7 1) matrix inner product:\nL(f) = LM\u22121 f (4)\nwhere, for each low-pass operation, the matrix operator is written as\nLm =  . . . . . .\n. . . . . .\n\u00b7 \u00b7 \u00b7 a[N \u2212 1] \u00b7 \u00b7 \u00b7 a[1] a[0] 0 0 0 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 0 a[N \u2212 1] \u00b7 \u00b7 \u00b7 a[1] a[0] 0 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 0 0 0 a[N \u2212 1] \u00b7 \u00b7 \u00b7 a[1] a[0] \u00b7 \u00b7 \u00b7\n. . . . . . . . . . . . \ufe38 \ufe37\ufe37 \ufe38 2m+1  2m (5)\nIn complete analogy to Eq. (5), a high-pass filter matrix Hm can be expressed as a 2m \u00d7 2m+1 matrix parametrised in the same way by coefficients {b}, which we choose [13] to relate to {a} by\nbk = (\u22121)k aNfilt\u22121\u2212k for k \u2208 [0,Nfilt \u2212 1] (6)\nThe means that, given wavelet coefficients {a}, we have specified the full wavelet transform in terms of repeated application of matrix operators Lm and Hm. The filter coefficients will therefore serve as our parametrisation of any given wavelet basis.\nAt each step in the transform, the power of 2 that gives the current length of the (partially transformed) input, n = 2m, is referred to as the frequency scale, m. Large frequency scales m correspond to large input arrays, which are able encode more granular, and therefore more high-frequency, information than for small m.\nAs the name implies, the low-pass filter acts as a spatial sub-sampling of the input from frequency scale m to m \u2212 1, averaging out the frequency information at scale m in the process. Similarly, the high-pass filter encodes the frequency information at scale m; the information which is lost in the low-pass filtering. After each step, another pass of high- and low-pass filters are applied to the sub-sampled, low-pass filtered input. This procedure is repeated from frequency scale M to 0. At each step, the high-pass filter encodes the frequency information specific to the current frequency scale. This is illustrated in Figure 1a.\nThe coefficients obtained through successive convolution of the signal with the high- and low-pass fil-\nters, i.e. the right-most layers in Figure 1a, collectively encode the same information as the position-space input f, but in the basis of wavelet functions. These are called the wavelet coefficients {c}. Given such a set of wavelet coefficients, the inverse transform can be perform by retracing the steps of the forward transform. Letting fm denote the input signal low-pass filtered down to scale m, with fM \u2261 f the inverse transform proceeds as\nf0 = [c0] (7a) f1 = LT0 f0 + H T 0 [c1] (7b) f2 = LT1 f1 + H T 1 [c2 c3] (7c)\n...\nf \u2261 fM = LTM\u22121fM\u22121 + HTM\u22121[ c2M\u22121 \u00b7 \u00b7 \u00b7 c2M\u22121 ] (7d)\nIn this way it is seen that c0 encodes the average information content in the input signal f, and that ci>0 dyadically encode the frequency information at larger and larger scales m. The explicit wavelet basis function corresponding to each wavelet coefficient can be found by setting c = [ \u00b7 \u00b7 \u00b7 0 1 0 \u00b7 \u00b7 \u00b7 ] and studying the resulting, reconstructed position-space signal f\u0302 at some suitable largest scale M.\nThe filter coefficients {a} completely specify the wavelet transform and -basis, but they are not completely free parameters, however. Instead, they must satisfy a number of explicit conditions in order to corresponds to an orthonormal wavelet basis. These conditions [20] are as follows:\nIn order to satisfy the dilation equation, the filter\ncoefficients {a} must satisfy\u2211 k ak = \u221a 2 (C1)\nIn order to ensure orthonormality of the scalingand wavelet functions, the coefficients {a} and {b}must satisfy \u2211\nk\nakak+2m = \u03b4m,0 \u2200 m \u2208 Z (C2)\nand \u2211 k bkbk+2m = \u03b4m,0 \u2200 m \u2208 Z (C3)\nwhere the condition for m = 0 is trivially fulfilled from (C2) through Eq. (6).\nTo ensure that the corresponding wavelets have zero area, i.e. encode only frequency information, we require \u2211\nk\nbk = 0 (C4)\nFinally, to ensure orthogonality of scaling and wavelet functions, we must have\u2211\nk\nakbk+2m = 0 \u2200 m \u2208 Z (C5)\nwhere condition (C5) is automatically satisfied through Eq. (6).\nConditions (C1\u20135) then collectively ensure that the filter coefficients {a} (and {b}) yield a wavelet analysis in terms of orthonormal basis functions. As we parametrise our basis uniquely in terms of filter coefficients {a}, since {b} are fixed through Eq. (6), we will need to explicitly ensure that these conditions are met. The method for doing this is described in Section III C."}, {"heading": "B. Neural network", "text": "Since (artificial) neural networks have become ubiquitous within most areas of the physical sciences, we will only briefly review the central concepts as they relate to the rest of this discussion. A comprehensive introduction can be found e.g. in Ref. [21].\nNeural networks can be seen general mappings f : Rn \u2192 Rm, which can approximate any function, provided sufficient capacity. In the simplest case, such networks are constructed sequentially, where the input vector f = h0 \u2208 RN0 is transformed through the inner product with a weight matrix \u03b81, the output of which is a hidden layer h1 \u2208 RN1 , and so forth, until the output layer hl \u2208 RNl is reached. The configuration of a given neural network, in terms of number of layers and their respective sizes, is called the network architecture. In addition to the transfer matrices \u03b8i, the layers may be equipped with bias nodes, providing the opportunity for an offset, as well as non-linear activation functions. A schematic representation of one such network, without bias nodes and non-linearities, is shown in Figure 1b.\nThe neural network can then be trained on a set of training examples, {(fi, yi)}, where the task of network usually is to output a vector y\u0302i trying to predict yi given fi. The quality of the prediction is quantified by the cost or objective function J(y, y\u0302). The central idea is then to take the error of any given preduction y\u0302i, given by the derivative of the cost function with respect to the prediction at the current value, and backpropagate it through the network, performing the inverse operation of the forward pass at each layer. In this way, the gradient of the cost function J with respect to each entry in the network\u2019s weight matrices\n5 (\u03b8i) jk is computed. Using stochastic gradient descent, for each training example one performs small update steps of the weight matrix entries along these error gradients, which is then expected to produce slightly better performance of the network with respect to the task specified by the cost function.\nOne challenge posed by such a fully connected network is the shear multiplicity of weights for just a few layers of moderate sizes. Such a large number of free parameters can make the network prone to over-fitting, which can be mitigated e.g. by L2 weight regularisation, where a regularisation term R({\u03b8}) is added to the cost function, with a multiplier \u03bb controlling the tradeoff between the two contributions."}, {"heading": "C. Combining concepts", "text": "The crucial step is then to recognise the deep parallels between these two constructs. We can cast the discrete wavelet transform as an RN \u2192 RN neural network with a fully-connected, deep, non-sequential, dyadic architecture without bias-units and with linear (i.e. no) activations. A schematic representation of this setup, here called a \u201cwavenet\u201d, is shown in Figure 1c. This is done by identifying the neural network transfer matrices with the low- and high-pass filter operators in the matrix formulation of the wavelet transform, cf. Eq. (5). The forward wavelet transform then corresponds to the neural network mapping, and the output vector of the neural network is exactly the wavelet coefficients of the input with respect to the basis prescribed by {a}.\nIf we can formulate an objective functionJ for the wavelet coefficients, i.e. the output of the \u201cwavenet\u201d, this means that we can utilise the parallel with neural networks and employ back-propagation to gradually update the weight matrix entries, i.e. the filter coefficients {a}, in order to improve our wavelet basis with respect to this metric. Therefore, choosing a fixed filter length |{a}| = Nfilt, and parametrising the \u201cwavenet\u201d in terms of {a}, we are able to directly learn the wavelet basis which is optimal according to some task J .\nInterestingly, and unlike some of the approaches mentioned in Section II, a neural network approach naturally accommodates classes of inputs, in addition to single examples. That is, one can train repeatedly on a single example and learn a basis which optimally represents this particular signal in some way, cf. e.g. [8]. However, the use of stochastic gradient\ndescent is naturally suited for fitting the weight matrices to ensembles of training examples, which in many cases is much more meaningful and useful, cf. Section VII.\nAnother key observation is that while the entries in a standard neural network wight matrix are free parameters, the weights in the \u201cwavenet\u201d are highly constrained, since they must correspond to the lowand high-pass filters of the wavelet transform. For instance, a neural network like the one in Figure 1c, mapping R8 \u2192 R8 will have 84 free parameters in the standard treatment. However, identifying each of the 6 weight matrices with the wavelet filter operators, this number is reduced to Nfilt, which can be as low as 2. This is schematically shown in Figure 2. For inputs of \u201crealistic\u201d sizes, i.e. |f| = N & 64 this reduction is exponentially greater, leading to a significant reduction of complexity.\nFinally, we note that the filter coefficients need to conform with conditions (C1\u20135), cf. Section III A above, in order to correspond to an orthonormal wavelet basis. This can be solved by noting that all conditions (C1\u20135) are differentiable with respect to {a}, which means that we can cast these conditions in the form of quadratic regularisation terms, Ri, which can then be added to the cost function with some multiplier \u03bb, in analogy to standard L2 weight regularisation. The multiplier \u03bb then controls the trade-off between the possibly competing objectives of optimising J and ensuring fulfillment of conditions (C1\u20135). In principle, this means that for finite \u03bb any learned filter configuration {a} might violate these conditions to order 1/\u03bb, and might therefore strictly be taken to constitute a \u201cpseudo-orthonormal\u201d basis. This will, however, have little impact in practical application, where one can simply choose a value of \u03bb sufficiently high that O(1/\u03bb) is within the tolerances of the use case at hand."}, {"heading": "IV. MEASURING OPTIMALITY", "text": "The choice of objective function defines the sense in which the basis learned through the method outlined in Section III C will be optimal. This also affords the user a certain degree of freedom in defining the measure of optimality, the only condition being that the\nobjective function be differentiable4 with respect to the wavelet coefficients {c}.\nIn this example we choose sparsity, i.e. the ability of a certain basis to efficiently encode the information contained in a given signal, as our measure of optimality. From the point of view of compression, sparsity is clearly a useful metric, in that it measures the amount of information that can be stored with a within certain amount of space/memory. From the point of view of representation, sparsity is likely also a meaningful objective, since a basis which efficiently represents the defining features of a (class of) signal(s) will also lead the signal(s) to be sparse in this basis.\nBased on [22], we choose the Gini coefficient G( \u00b7 ) as our metric for the sparsity of a set of wavelets coefficients {c},\nG({c}) = \u2211Nc\u22121\ni=0 (2i \u2212 Nc \u2212 1)|ci| Nc \u2211Nc\u22121 i=0 |ci| \u2261 f ({c}) g({c}) (8)\nfor wavelet coefficients {c} sorted by ascending absolute value, i.e. |ci| \u2264 |ci+1| for all i. Here Nc \u2261 |{c}| is the number of wavelet coefficients.\nA Gini coefficient of 1 indicates a completely unequal, and therefore maximally sparse, distribution, i.e. the case in which only one coefficient has non-zero value, and therefore carries all of the information content in the signal. Conversely, a Gini coefficient of 0 indicates a completely equal distribution, i.e. each coefficient has exactly the same (absolute) value, and therefore all carry exactly the same amount of information content.\n4 Possibly except for a finite number of points.\nHaving settled on a choice of objective function, we now proceed to describing the details of the learning procedure itself. We stress that the results of the following sections should generalise to other reasonable choices of objectives, which may be chosen based on the particular use case at hand."}, {"heading": "V. LEARNING PROCEDURE", "text": "As noted above, the full objective function for the optimisation problem is given as the sum of a sparsity term S({c}) and a regularisation term R({a}), the relative contribution of the latter controlled by the regularisation constant \u03bb, i.e.\nJ({c}, {a}) = S({c}) + \u03bbR({a}) (9)\nwhere {c} is the set of wavelet coefficients for a given training example and {a} is the current set of filter coefficients. The R-term ensures that the filter coefficient configuration {a} does indeed correspond to a wavelet basis as defined by conditions (C1\u20135) above; the S-term measures the quality of a given wavelet basis according to the chosen fitness measure. The learning task then consists of optimising the filter coefficients according to this combined objective function, i.e. finding a filter coefficient configuration, in an Nfilt-dimensional parameter space, which minimises J . The procedure for computing a filter coefficient gradient for each of the two terms is outlined below.\n7"}, {"heading": "A. Sparsity term", "text": "Based on the discussion in Section IV, we have chosen the Gini coefficient G( \u00b7 ) as defined in Eq. (8) as our measure of the sparsity of any given set of wavelet coefficients {c}. The sparsity term in the objective function is chosen to be\nS({c}) = 1 \u2212 G({c}) (10)\nThis definition means that low values of S({c}) correspond to greater degree of sparsity, such that that minimising this objective function term increases the degree of sparsity.\nIn order to utilise stochastic gradient descent with back-propagation, the objective function needs to be differentiable in the values of the output nodes, i.e. the wavelet coefficients. Since the sparsity term is the only term which depends on the wavelet coefficients, particular care needs to be afforded here. The sparsity term is seen to be differentiable everywhere except for a finite number of points where ci = 0. In these cases the derivative is taken to be zero, which is meaningful considering the chosen optimisation objective: coefficients of value zero will, assuming at least one non-zero coefficient exists, contribute maximally to the sparsity of the set as a whole. Therefore we don\u2019t want these coefficients to change, and the corresponding gradient should be zero5.\nTherefore, assuming ci , 0, the derivative of the sparsity term is given by (suppressing the arguments of the objective function terms for brevity)\n\u2207|c| S \u2261 e\u0302i dS d|ci| = e\u0302i d d|ci| (1 \u2212 G)\n= \u2212\u2207|c| G = \u2212 \u2207|c| f \u00b7 g \u2212 f \u00b7 \u2207|c| g\ng2 (11)\nwhere\n\u2207|c| f = e\u0302i d\nd|ci| Nc\u22121\u2211 k=0 (2k \u2212 Nc \u2212 1)|ck| \n= (2i \u2212 Nc \u2212 1) e\u0302i (12)\nand\n\u2207|c| g = e\u0302i d\nd|ci| Nc Nc\u22121\u2211 k=0 |ck|  = Nc e\u0302i (13)\n5 Cases with all zero-valued coefficients are ill-defined but also practically irrelevant.\nfor f and g defined in Eq. (8), where summation of vector indices is implied.\nTo get the gradient with respect the the signed coefficient values, the gradients of f and g are multiplied by the corresponding coefficient sign, i.e.\n\u2207c f = sign(c) \u00d7 \u2207|c| f (14)\nand\n\u2207c g = sign(c) \u00d7 \u2207|c| g (15)\nwhere \u00d7 indicates element-wise multiplication. The gradients with respect to the base, non-sorted set of wavelet coefficients {c},\u2207c f and\u2207c g respectively, are found by performing the inverse sorting with respect to the absolute wavelet coefficient values. In this way \u2207c S can be computed from \u2207c f and \u2207c g through Eq. (11).\nHaving computed the gradient of the sparsity cost with respect to the output nodes (wavelet coefficients) we can now use standard back-propagation on the full network to compute the associated gradient on each entry in the low- and high-pass filter matrices. For a given, fixed filter length Nfilt, entries in the filter matrices which are identically zero are not modified by a gradient. Conversely, the gradient on every filter matrix entry to which a particular filter coefficient is contributing is added to the corresponding sparsity gradient in filter coefficient space, possibly with a sign change in the case of high-pass filter matrices, cf. Eq. (6). In this way, the gradient on the wavelet coefficients is translated into a gradient in filter coefficient space, which we can then use in stochastic gradient descent, along with a similar regularisation gradient, to gradually improve our wavelet basis as parametrised by {a}."}, {"heading": "B. Regularisation term", "text": "The regularisation terms are included to ensure that the optimal filter coefficient configuration does indeed correspond to an orthonormal wavelet basis as defined through conditions (C1\u20135). As noted in Section III C, we choose to cast cast these conditions in the form of quadratic regularisation conditions on the filter coefficients {a}. Each of the conditions (C1\u20135) is of the form\nhk({a}) = dk (16)\n8 which can be written as a quadratic regularisation term, i.e.\nRk({a}) = (hk({a}) \u2212 dk)2 (17)\nand the combined regularisation term is then given by\nR({a}) = 5\u2211\nk=1\nRk({a}) (18)\nThis formulation allows for the search to proceed in the full Nfilt-dimensional search space, and the regularisation constant \u03bb regulates the degree of precision to which the optimal filter coefficient configuration will fulfill conditions (C1\u20135).\nIn order to translate deviations from conditions (C1\u20135) into gradients in filter coefficient space, we take the derivative of each of the terms Rk with respect to the filter coefficients ai. The gradients are found to be:\n\u2207a R1 = e\u0302i 2 [\u2211\nk\nak ] \u2212 \u221a 2  (D1) \u2207a R2 = e\u0302i\n\u2211 m 2 \u2211 k [ akak+2m ] \u2212 \u03b4m,0  \u00d7 (ai+2m + ai\u22122m) (D2)\n\u2207a R3 = e\u0302i \u2211\nm\n2 \u2211 k [ bkbk+2m ] \u2212 \u03b4m,0  \u00d7 (ai+2m + ai\u22122m) (D3)\n\u2207a R4 = e\u0302i 2 \u2211\nk\nbk  \u00d7 (\u22121)N\u2212i\u22121 (D4) \u2207a R5 = 0 (D5)\nSince condition (C5) is satisfied exactly by the definition in Eq. (6), the corresponding gradient is identically equal to zero.\nThe combined gradient from the regularisation term is then the sum of the above five (four) contributions.\nVI. IMPLEMENTATION\nThe learning procedure based on the objective function and associated gradients presented in Section V is implemented [1] as a publicly available C++ [23] package. The matrix algebra operations are implemented using armadillo [24], with optional interface to the high-energy physics root library [25].\nThis package allows for the processing of 1- and 2D dimensional training examples of arbitrary size, provides data generator for a few toy examples and reads CSV input as well as high-energy physics collision events in the HepMC [26] format. The 2D wavelet transform is perform by performing the 1D transform on each row in the signal, concatenating the output rows, and then performing the 1D transform on each of the resulting columns. Their matrix concatenation then corresponds to the 2D set of wavelet coefficients.\nIn addition to standard (batch) gradient descent, the library allows for the use of gradient momentum and simulated annealing of the regularisation term in order to ensure faster and more robust convergence to the global minimum even in the presence of local minima and steep regularisation contours."}, {"heading": "VII. EXAMPLE: QCD 2\u2192 2 PROCESSES IN HIGH-ENERGY PHYSICS", "text": "As an example of the procedure for learning optimal wavelet bases according to the metric presented in Section IV, using the implementation in Sections V and VI, we choose that of hadronic jets produced at proton colliders. In particular, the input to the training is taken to be simulated quantum chromodynamics (QCD) 2 \u2192 2 processes, generated in Pythia8 [27, 28], segmented into a 2D array of size 64 \u00d7 64 in the \u03b7 \u2212 \u03c6 plane, roughly corresponding to the angular granularity of present-day general purpose particle detectors. The collision events are generated at a center of mass energy of \u221a s = 13 TeV with a generator-level p\u22a5 cut of 280 GeV imposed on the leading parton. QCD radiation patterns are governed by scaleindependent splitting kernels [29], which could make them suitable candidates for wavelet representation, since these naturally exhibit self-similar, scaleindependent behaviour. In that case, the optimal (in the sense of Section IV) representation is one which efficiently encodes the localised angular structure of this type of process, and could be used to study, or even learn, such radiation patterns. In addition, differences in representation might help distinguish between such non-resonant, one-prong \u201cQCD jets\u201d and resonant, two-prong jets e.g. from the hadronic decay of the W and Z eletroweak bosons.\nWe also note that, as alluded to in Section III C, for signals of interest in collider physics, a standard neural network with \u201cwavenet\u201d architecture contains\n9 1Filter coeff. a -1 -0.5 0 0.5 1 2 Fi lte r c oe ff. a -1 -0.5 0 0.5 1 C os t ( sp ar si ty + re gu la ris at io n) [a .u .] -110 1 10 210 > 280 GeV T p qq) + jets, \u2192W ( = 13 TeVsQCD 2 \u2192 2\nFigure 3: Map of the average total cost (regularisation and sparsity) for QCD 2\u2192 2 events with p\u0302\u22a5 > 280 GeV, for only two filter coefficients a1,2. Initial configurations are generated on the unit circle in the a1 \u2212 a2 plane (red dots on dashed red line), to initially satisfy condition (C2), and better configurations are then learned iteratively (solid black lines) by using back-propagation with gradient descent, until a minimum (blue dot(s)) is found.\nan enormous number of free parameters, e.g. N2Dc \u2248 4.4 \u00d7 107 for N \u00d7 N = 64 \u00d7 64 input, which is reduced to Nfilt, i.e. as few as two, by the parametrisation in terms of the filter coefficients {a}.\nWe apply the learning procedure using Ref. [1], iterating over such \u201cdijet\u201d events pixelised in the \u03b7 \u2212 \u03c6 plane, and use back-propagation with gradient descent to learn the configuration of {a} which, for fixed Nfilt, minimises the combined sparsity and regularisation in Eq. (9). This is shown in Fig. 3 for Nfilt = 2.\nIt is seen that, for Nfilt = 2, only one minimum exists, due to only one point in a1 \u2212 a2 space fulfilling all five conditions (C1\u20135). This configuration has a1 = a2 = 1/ \u221a 2 and is exactly the Haar wavelet [30]. Although this is an instructive example allowing for clean visualisation, showing the clear effect of the gradient descent algorithm and the efficacy of the interpretation of conditions (C1\u20135) as quadratic regularisation terms, it also doesn\u2019t tell us much since the global minimum will be the same for all classes of inputs. For Nfilt > 2 the regularisation allows for minima in an effective hyperspace with dimension D > 0.\nInstead choosing Nfilt = 16 we can perform the same optimisation, but now with sufficient capacity of the wavelet basis to encode the defining features of this class of signals. The effect of the learning procedure is presented in Figure 4, showing a selection of\nthe lowest-scale wavelet basis functions corresponding to particular filter coefficient configurations at the beginning of, during, and at convergence of the learning procedure in this higher-dimensional search space.\nThe random initialisation on the unit hyper-sphere is shown to produce random noise (Figure 4a), which does not correspond to a wavelet basis, since the algorithm has not yet been afforded time to update the filter coefficients to conform with the regularisation requirements. At some point roughly half way through the training, the filter coefficient configuration does indeed yield an orthonormal wavelet basis (Figure 4b), and the learning procedure now follows the gradients towards greater sparsity along a high-dimensional, quadratic regularisation \u201cvalley\u201d. Finally, at convergence, the optimal wavelet found is again seen to be exactly the Haar wavelet (Figure 4c), despite the vast amount of freedom provided the algorithm by virtue of 16 filter coefficients. That is, the learning procedure arrives at the optimal configuration by setting 14 filter coefficients to exactly zero without any manual tuning.\nThis result shows that limiting the support of the basis functions provides for more efficient representation than any deviations due to radiation patterns could compensate for. Indeed, it can be show that removing some of the conditions (C1\u20135) so as to ensure that {a} simply corresponds to an orthonormal basis (i.e. not necessarily an orthonormal wavelet basis) the learning procedure results in the pixel basis, i.e. the one in which each basis function corresponds to a single entry in the input array. This shows that, due to the fact that QCD showers are fundamentally point-like (due to the constituent particles) and since they, to leading order, are dominated by a few particles carrying the majority of the energy in the jet, the representation which best allows for representation of single particles will prove optimal according to our chosen measure Eq. (8). However, since this example studies the optimal representation of entire event, its conclusions may change for inputs restricted to a certain region in \u03b7\u2212 \u03c6 space around a particular jet, i.e. for the study of optimal representation of jets themselves."}, {"heading": "Acknowledgments", "text": "The author would like to thank Troels C. Petersen for insightful discussions on the subject matter, and James W. Monk for providing Monte Carlo samples.\n10\n[1] Andreas S\u00f8gaard. \u201cWavenet\u201d package, 2017. Retrieved from www.github.com/asogaard/Wavenet on June 12, 2017. [2] A. S\u00f8gaard. Boosted bosons and wavelets. MSc thesis, University of Copenhagen, 2015. [3] J. Morlet et al. Wave propagation and sampling theory, Part I: Complex signal land scattering in multilayer media. J. Geophys., 47:203\u2013221, 1982. [4] J. Morlet et al. Wave propagation and sampling theory, Part II: Sampling theory and complex waves. J. Geophys., 47:222\u2013236, 1982. [5] A. Mojsilovic\u0300, M. V. Popovic\u0300, and D. M. Rackov. On the Selection of an Optimal Wavelet Basis for Texture Characterization. In IEEE Transactions on Image Processing, volume 4, 2000. [6] H. Qureshi, R. Wilson, and N. Rajpoot. Optimal Wavelet Basis for Wavelet Packets based Meningioma Subtype Classification. In 12th Medical Image Understanding and Analysis (MIUA 2008), 2008. [7] O. Pont, A. Turiel, and C. J. P\u00e9rez-Vicente. On optimal wavelet bases for the realization of microcanonical cascade processes. International Journal of Wavelets Multiresolution and Information Processing, 9(1):35\u201361, 2011. [8] H. Thielemann. Optimally matched wavelets. PhD thesis, Universit\u00e4t Bremen, March 2006. [9] W. Sweldens. The Lifting Scheme: A Construction of Second Generation Wavelets. Journal on Mathematical Analysis, 29(2):511\u2013546, 1997. [10] N. P. Hurley et al. Maximizing sparsity of wavelet representations via parameterized lifting. In 15th International Conference on Digital Signal Processing, pages 631\u2013634, 2007. [11] Y. Zhuang and J. S. Barras. Optimal wavelet basis selection for signal representation. In Proc. SPIE, vol-\nume 2242 of Wavelet Applications, pages 200\u2013211, 1994.\n[12] Y. Zhuang and J. S. Barras. Constructing optimal wavelet basis for image compression. In IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 4, pages 2351\u20132354, 1996. [13] I. Daubechies. Ten Lectures on Wavelets. CBMSNDF Regional Conference Series in Applied Mathematics. Society for Industrial and Applied Mathematics (SIAM), 1992. [14] A. H. Tewfik, D. Sinha, and P. On the Optimal Choice of a Wavelet for Signal Representation. In IEEE Transactions on Information Theory, volume 38, 1992. [15] R. A. Gopinath, J. E. Odegard, and C. S. Burrus. Optimal wavelet representation of signals and the wavelet sampling theorem. In IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing, volume 41, 1994. [16] S. G. Mallat. A Theory for Multiresolution Signal Decomposition: The Wavelet Representation. IEEE Transactions on Pattern Recognition and Machine Intelligence, 11(7):674\u2013693, 1989. [17] Y. Meyer. Wavelets and Operators. Cambridge University Press, 1992. [18] A. Jensen and A. la Cour-Harbo. Ripples in Mathematics: the Discrete Wavelet Transform. Springer, 2001. [19] J. Williams and K. Amaratunga. Introduction to Wavelets in Engineering. Int. Journ. Num. Meth. Eng., 37(14):2365\u20132388, 1994. [20] D. S. G. Pollock. The Framework of a Dyadic Wavelets Analysis. Retrieved from http://www.le. ac.uk/users/dsgp1/SIGNALS/MoreWave.pdf on June 12, 2017.\n11\n[21] C. M. Bishop. Neural networks for pattern recognition. Clarendon Press, 1995. [22] N. P. Hurley and S. T. Rickard. Comparing Measures of Sparsity. IEEE Transactions on Information Theory, 55(10):4723\u20134741, 2009. [23] B. Stoustrup. The C++ Programming Language. Pearson Education India, 1995. [24] C. Sanderson and R. Curtin. Armadillo: a templatebased C++ library for linear algebra. Journal of Open Source Software, 1(26), 2016. [25] R. Brun and F. Rademakers. ROOT - An Object Oriented Data Analysis Framework. Nucl. Inst. & Meth. in Phys. Res. A, 389:81\u201386, 1997. See also http://root.cern.ch/. [26] M. Dobbs and J. B. Hansen. The HepMC C++ Monte Carlo Event Record for High Energy Physics. Comput. Phys. Commun., 134(41), 2001. [27] T. Sj\u00f6strand, S. Mrenna, and P. Skands. A Brief Introduction to PYTHIA 8.1. JHEP, 05(026), 2006. [28] T. Sj\u00f6strand, S. Mrenna, and P. Skands. A Brief Introduction to PYTHIA 8.1. Comput. Phys. Comm., 178(852), 2008. [arxiv:0710.3820]. [29] A. Buckley et al. General-purpose event generators for LHC physics. Phys. Rept., 504:145\u2013233. [30] A. Haar. Zur Theorie der orthogonalen Funktionensysteme. Mathematische Annalen, 69(3):331\u2013371, 1919."}], "references": [{"title": "Wavenet\u201d package, 2017", "author": ["Andreas S\u00f8gaard"], "venue": "Retrieved from www.github.com/asogaard/Wavenet on June", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "Boosted bosons and wavelets", "author": ["A. S\u00f8gaard"], "venue": "MSc thesis, University of Copenhagen,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Wave propagation and sampling theory, Part I: Complex signal land scattering in multilayer media", "author": ["J. Morlet"], "venue": "J. Geophys.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1982}, {"title": "Wave propagation and sampling theory, Part II: Sampling theory and complex waves", "author": ["J. Morlet"], "venue": "J. Geophys.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1982}, {"title": "On the Selection of an Optimal Wavelet Basis for Texture Characterization", "author": ["A. Mojsilovic", "M.V. Popovic", "D.M. Rackov"], "venue": "In IEEE Transactions on Image Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Optimal Wavelet Basis for Wavelet Packets based Meningioma Subtype Classification", "author": ["H. Qureshi", "R. Wilson", "N. Rajpoot"], "venue": "12th Medical Image Understanding and Analysis (MIUA 2008),", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "On optimal wavelet bases for the realization of microcanonical cascade processes", "author": ["O. Pont", "A. Turiel", "C.J. P\u00e9rez-Vicente"], "venue": "International Journal of Wavelets Multiresolution and Information Processing, 9(1):35\u201361,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimally matched wavelets", "author": ["H. Thielemann"], "venue": "PhD thesis, Universit\u00e4t Bremen, March", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "The Lifting Scheme: A Construction of Second Generation Wavelets", "author": ["W. Sweldens"], "venue": "Journal on Mathematical Analysis, 29(2):511\u2013546,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Maximizing sparsity of wavelet representations via parameterized lifting", "author": ["N.P. Hurley"], "venue": "In 15th International Conference on Digital Signal Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Optimal wavelet basis selection for signal representation", "author": ["Y. Zhuang", "J.S. Barras"], "venue": "Proc. SPIE, vol-  ume 2242 of Wavelet Applications, pages 200\u2013211,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1994}, {"title": "Constructing optimal wavelet basis for image compression", "author": ["Y. Zhuang", "J.S. Barras"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 4, pages 2351\u20132354,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1996}, {"title": "Ten Lectures on Wavelets", "author": ["I. Daubechies"], "venue": "CBMS- NDF Regional Conference Series in Applied Mathematics. Society for Industrial and Applied Mathematics (SIAM),", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1992}, {"title": "On the Optimal Choice of a Wavelet for Signal Representation", "author": ["A.H. Tewfik", "D. Sinha"], "venue": "In IEEE Transactions on Information Theory,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1992}, {"title": "Optimal wavelet representation of signals and the wavelet sampling theorem", "author": ["R.A. Gopinath", "J.E. Odegard", "C.S. Burrus"], "venue": "IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing, volume 41,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1994}, {"title": "A Theory for Multiresolution Signal Decomposition: The Wavelet Representation", "author": ["S.G. Mallat"], "venue": "IEEE Transactions on Pattern Recognition and Machine Intelligence, 11(7):674\u2013693,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1989}, {"title": "Wavelets and Operators", "author": ["Y. Meyer"], "venue": "Cambridge University Press,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1992}, {"title": "Ripples in Mathematics: the Discrete Wavelet Transform", "author": ["A. Jensen", "A. la Cour-Harbo"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Introduction to Wavelets in Engineering", "author": ["J. Williams", "K. Amaratunga"], "venue": "Int. Journ. Num. Meth. Eng., 37(14):2365\u20132388,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "The Framework of a Dyadic Wavelets Analysis", "author": ["D.S.G. Pollock"], "venue": "Retrieved from http://www.le. ac.uk/users/dsgp1/SIGNALS/MoreWave.pdf on June 12,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2017}, {"title": "Neural networks for pattern recognition", "author": ["C.M. Bishop"], "venue": "Clarendon Press,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Comparing Measures of Sparsity", "author": ["N.P. Hurley", "S.T. Rickard"], "venue": "IEEE Transactions on Information Theory, 55(10):4723\u20134741,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "The C++ Programming Language", "author": ["B. Stoustrup"], "venue": "Pearson Education India,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1995}, {"title": "Armadillo: a templatebased C++ library for linear algebra", "author": ["C. Sanderson", "R. Curtin"], "venue": "Journal of Open Source Software, 1(26),", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "ROOT - An Object Oriented Data Analysis Framework", "author": ["R. Brun", "F. Rademakers"], "venue": "Nucl. Inst. & Meth. in Phys. Res. A, 389:81\u201386,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "The HepMC C++ Monte Carlo Event Record for High Energy Physics", "author": ["M. Dobbs", "J.B. Hansen"], "venue": "Comput. Phys. Commun., 134(41),", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "A Brief Introduction to PYTHIA 8.1. JHEP", "author": ["T. Sj\u00f6strand", "S. Mrenna", "P. Skands"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "A Brief Introduction to PYTHIA 8.1", "author": ["T. Sj\u00f6strand", "S. Mrenna", "P. Skands"], "venue": "Comput. Phys. Comm.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Zur Theorie der orthogonalen Funktionensysteme", "author": ["A. Haar"], "venue": "Mathematische Annalen, 69(3):331\u2013371,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1919}], "referenceMentions": [{"referenceID": 0, "context": "We describe the practical implementation of this method [1], and study its performance for high-energy physics collision events for QCD 2 \u2192 2 processes.", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": "The wavelet decomposition expresses any given signal in terms of a \u201cfamily\u201d of orthonormal basis functions [3, 4], efficiently encoding frequencyposition information.", "startOffset": 107, "endOffset": 113}, {"referenceID": 3, "context": "The wavelet decomposition expresses any given signal in terms of a \u201cfamily\u201d of orthonormal basis functions [3, 4], efficiently encoding frequencyposition information.", "startOffset": 107, "endOffset": 113}, {"referenceID": 1, "context": "uk 1 Sections I and III contain overlaps with [2].", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "A typical approach [5\u20137] when faced with the task of choosing a wavelet basis in which to represent some class of signals, is to select one among an existing set wavelet families, which is deemed suitable to the particular use case based on some measure of fitness.", "startOffset": 19, "endOffset": 24}, {"referenceID": 5, "context": "A typical approach [5\u20137] when faced with the task of choosing a wavelet basis in which to represent some class of signals, is to select one among an existing set wavelet families, which is deemed suitable to the particular use case based on some measure of fitness.", "startOffset": 19, "endOffset": 24}, {"referenceID": 6, "context": "A typical approach [5\u20137] when faced with the task of choosing a wavelet basis in which to represent some class of signals, is to select one among an existing set wavelet families, which is deemed suitable to the particular use case based on some measure of fitness.", "startOffset": 19, "endOffset": 24}, {"referenceID": 7, "context": "[8] describes a method for constructing optimally matched wavelets, i.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "wavelet bases matching a prescribed pattern as closely as possible, through lifting [9].", "startOffset": 84, "endOffset": 87}, {"referenceID": 9, "context": "[10] provides a method for unconstrained optimisation of a wavelet basis with respect to a sparsity measure using lifting, but has the same limitations as Ref.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11, 12] provide theoretical arguments for the existence of optimal wavelet bases as well as an algorithm for constructing such a basis for single 1- or 2D inputs, based on gradient descent.", "startOffset": 0, "endOffset": 8}, {"referenceID": 11, "context": "[11, 12] provide theoretical arguments for the existence of optimal wavelet bases as well as an algorithm for constructing such a basis for single 1- or 2D inputs, based on gradient descent.", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "These constraints are violated at the 1%-level, which also corresponds roughly to the relative angular deviation of the reported optimal basis from the Daubechies [13] basis of similar order.", "startOffset": 163, "endOffset": 167}, {"referenceID": 13, "context": "[14, 15] provide a comprehensive prescription for designing wavelets that optimally represent signals, or classes of signals, at some fixed scale J.", "startOffset": 0, "endOffset": 8}, {"referenceID": 14, "context": "[14, 15] provide a comprehensive prescription for designing wavelets that optimally represent signals, or classes of signals, at some fixed scale J.", "startOffset": 0, "endOffset": 8}, {"referenceID": 1, "context": "[2] or [13, 16\u201319].", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[2] or [13, 16\u201319].", "startOffset": 7, "endOffset": 18}, {"referenceID": 15, "context": "[2] or [13, 16\u201319].", "startOffset": 7, "endOffset": 18}, {"referenceID": 16, "context": "[2] or [13, 16\u201319].", "startOffset": 7, "endOffset": 18}, {"referenceID": 17, "context": "[2] or [13, 16\u201319].", "startOffset": 7, "endOffset": 18}, {"referenceID": 18, "context": "[2] or [13, 16\u201319].", "startOffset": 7, "endOffset": 18}, {"referenceID": 0, "context": "f = \uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 f[0] f[1] .", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "The convolution yielding each entry i in L(f) can be seen as a matrix inner product of f with a row matrix of the form [ \u00b7 \u00b7 \u00b7 0 a[N \u2212 1] \u00b7 \u00b7 \u00b7 a[1] a[0] 0 \u00b7 \u00b7 \u00b7 ] (3)", "startOffset": 145, "endOffset": 148}, {"referenceID": 0, "context": "\u00b7 \u00b7 \u00b7 a[N \u2212 1] \u00b7 \u00b7 \u00b7 a[1] a[0] 0 0 0 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 0 a[N \u2212 1] \u00b7 \u00b7 \u00b7 a[1] a[0] 0 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 0 0 0 a[N \u2212 1] \u00b7 \u00b7 \u00b7 a[1] a[0] \u00b7 \u00b7 \u00b7 .", "startOffset": 22, "endOffset": 25}, {"referenceID": 0, "context": "\u00b7 \u00b7 \u00b7 a[N \u2212 1] \u00b7 \u00b7 \u00b7 a[1] a[0] 0 0 0 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 0 a[N \u2212 1] \u00b7 \u00b7 \u00b7 a[1] a[0] 0 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 0 0 0 a[N \u2212 1] \u00b7 \u00b7 \u00b7 a[1] a[0] \u00b7 \u00b7 \u00b7 .", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "\u00b7 \u00b7 \u00b7 a[N \u2212 1] \u00b7 \u00b7 \u00b7 a[1] a[0] 0 0 0 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 0 a[N \u2212 1] \u00b7 \u00b7 \u00b7 a[1] a[0] 0 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 0 0 0 a[N \u2212 1] \u00b7 \u00b7 \u00b7 a[1] a[0] \u00b7 \u00b7 \u00b7 .", "startOffset": 120, "endOffset": 123}, {"referenceID": 12, "context": "(5), a high-pass filter matrix Hm can be expressed as a 2m \u00d7 2m+1 matrix parametrised in the same way by coefficients {b}, which we choose [13] to relate to {a} by", "startOffset": 139, "endOffset": 143}, {"referenceID": 19, "context": "These conditions [20] are as follows:", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "Based on [22], we choose the Gini coefficient G( \u00b7 ) as our metric for the sparsity of a set of wavelets coefficients {c},", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "The learning procedure based on the objective function and associated gradients presented in Section V is implemented [1] as a publicly available C++ [23] package.", "startOffset": 118, "endOffset": 121}, {"referenceID": 22, "context": "The learning procedure based on the objective function and associated gradients presented in Section V is implemented [1] as a publicly available C++ [23] package.", "startOffset": 150, "endOffset": 154}, {"referenceID": 23, "context": "The matrix algebra operations are implemented using armadillo [24], with optional interface to the high-energy physics root library [25].", "startOffset": 62, "endOffset": 66}, {"referenceID": 24, "context": "The matrix algebra operations are implemented using armadillo [24], with optional interface to the high-energy physics root library [25].", "startOffset": 132, "endOffset": 136}, {"referenceID": 25, "context": "This package allows for the processing of 1- and 2D dimensional training examples of arbitrary size, provides data generator for a few toy examples and reads CSV input as well as high-energy physics collision events in the HepMC [26] format.", "startOffset": 229, "endOffset": 233}, {"referenceID": 26, "context": "In particular, the input to the training is taken to be simulated quantum chromodynamics (QCD) 2 \u2192 2 processes, generated in Pythia8 [27, 28], segmented into a 2D array of size 64 \u00d7 64 in the \u03b7 \u2212 \u03c6 plane, roughly corresponding to the angular granularity of present-day general purpose particle detectors.", "startOffset": 133, "endOffset": 141}, {"referenceID": 27, "context": "In particular, the input to the training is taken to be simulated quantum chromodynamics (QCD) 2 \u2192 2 processes, generated in Pythia8 [27, 28], segmented into a 2D array of size 64 \u00d7 64 in the \u03b7 \u2212 \u03c6 plane, roughly corresponding to the angular granularity of present-day general purpose particle detectors.", "startOffset": 133, "endOffset": 141}, {"referenceID": 0, "context": "[1], iterating over such \u201cdijet\u201d events pixelised in the \u03b7 \u2212 \u03c6 plane, and use back-propagation with gradient descent to learn the configuration of {a} which, for fixed Nfilt, minimises the combined sparsity and regularisation in Eq.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "a1 = a2 = 1/ \u221a 2 and is exactly the Haar wavelet [30].", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "[1] Andreas S\u00f8gaard.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] H.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] O.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] H.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] W.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] I.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[30] A.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "A novel method for learning optimal, orthonormal wavelet bases for representing 1and 2D signals, based on parallels between the wavelet transform and fully connected artificial neural networks, is described. The structural similarities between these two concepts are reviewed and combined to a \u201cwavenet\u201d, allowing for the direct learning of optimal wavelet filter coefficient through stochastic gradient descent with back-propagation over ensembles of training inputs, where conditions on the filter coefficients for constituting orthonormal wavelet bases are cast as quadratic regularisations terms. We describe the practical implementation of this method [1], and study its performance for high-energy physics collision events for QCD 2 \u2192 2 processes. It is shown that an optimal solution is found, even in a high-dimensional search space, and the implications of the result are discussed.", "creator": "LaTeX with hyperref package"}}}