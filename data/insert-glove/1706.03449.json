{"id": "1706.03449", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Scientific document summarization via citation contextualization and scientific discourse", "abstract": "The rapid growth campestre of scientific literature has made byrns it duette difficult qibya for unbiased the researchers shashthi to quickly +.07 learn about the takei developments in their drovers respective fields. Scientific document summarization potluck addresses this mohannad challenge roid by pronominal providing graffeo summaries kigezi of the andjelic important e5 contributions of garastas scientific papers. We present a nytnews framework zhongyi for scientific corning summarization which takes advantage 43.44 of ehara the citations colonel and .697 the scientific phosphorus discourse saltdal structure. Citation silkie texts often inhalational lack the evidence and dasu context to vaanam support the trasimeno content drut of the cited zoa paper bit.ly and kiem are even eibl sometimes inaccurate. habuba We yuzon first enchant address the lampo problem of telltale inaccuracy trendies of the nsd citation texts unbiased by filmgoing finding 6,720 the remote-sensing relevant meksi context coxless from 513th the cited freakishness paper. switchboard We propose three clockmaking approaches extricates for contextualizing lipe citations ciszuk which lpa are ficarrotta based tirumalai on query reformulation, floristry word embeddings, sabhai and supervised 2494 learning. euro234 We then train sherawat a model to identify schmerling the discourse heerhugowaard facets for hadrosaur each citation. koteas We homestand finally higashi-ku propose sainte-chapelle a method for fronteira summarizing videoclips scientific papers 124.46 by leveraging tramline the faceted telopea citations and xylitol their corresponding contexts. We ishmaelites evaluate our a27 proposed method supercontinents on two fefe scientific summarization stagliano datasets ppu in the 27.09 biomedical and computational linguistics mattoo domains. Extensive evaluation kittens results wade show docetaxel that our louvet methods nishantha can urlacher improve evernight over orti the state milverton of the f-22 art by first-person large margins.", "histories": [["v1", "Mon, 12 Jun 2017 03:21:38 GMT  (207kb,D)", "http://arxiv.org/abs/1706.03449v1", "Preprint. The final publication is available at Springer viathis http URL, International Journal on Digital Libraries (IJDL) 2017"]], "COMMENTS": "Preprint. The final publication is available at Springer viathis http URL, International Journal on Digital Libraries (IJDL) 2017", "reviews": [], "SUBJECTS": "cs.CL cs.DL", "authors": ["arman cohan", "nazli goharian"], "accepted": false, "id": "1706.03449"}, "pdf": {"name": "1706.03449.pdf", "metadata": {"source": "CRF", "title": "Scientific document summarization via citation contextualization and scientific discourse", "authors": ["Arman Cohan", "Nazli Goharian"], "emails": ["arman@ir.cs.georgetown.edu", "nazli@ir.cs.georgetown.edu"], "sections": [{"heading": null, "text": "made it difficult for the researchers to quickly learn about the developments in their respective fields. Scientific document summarization addresses this challenge by providing summaries of the important contributions of scientific papers. We present a framework for scientific summarization which takes advantage of the citations and the scientific discourse structure. Citation texts often lack the evidence and context to support the content of the cited paper and are even sometimes inaccurate. We first address the problem of inaccuracy of the citation texts by finding the relevant context from the cited paper. We propose three approaches for contextualizing citations which are based on query reformulation, word embeddings, and supervised learning. We then train a model to identify the discourse facets for each citation. We finally propose a method for summarizing scientific papers by leveraging the faceted citations and their corresponding contexts. We evaluate our proposed method on two scientific summarization datasets in the biomedical and computational linguistics domains. Extensive evaluation results show that our methods can improve over the state of the art by large margins.\n\u2217 This is a pre-print of an article published on IJDL. The final publication is available at Springer via http://dx.doi.org/10.1007/s00799-017-0216-8\nArman Cohan E-mail: arman@ir.cs.georgetown.edu\nNazli Goharian E-mail: nazli@ir.cs.georgetown.edu\n1 Information Retrieval Lab, Department of Computer Science, Georgetown University, Washington DC, USA"}, {"heading": "1 Introduction", "text": "The rapid growth of scientific literature in recent decades has created a challenge for researchers in various fields to keep up with the newest developments. According to a recent study by bibliometric analysts, the global scientific output doubles approximately every nine years [11], further signifying this challenge. Existence of surveys in different fields shows that finding an overview of key developments in scientific areas is desirable, however procuring such surveys requires extensive human efforts. Scientific summarization aims at addressing this problem by providing a concise representation of important findings and contributions of scientific papers, reducing the time required to overview the entire paper to understand important contributions. Article abstracts are a basic form of scientific summaries. While abstracts provide an overview of the paper, they do not necessarily convey all the important contributions and impacts of the paper [28]: (i) The authors might ascribe contributions to their papers that are not existent. (ii) some important contributions might not be included in the abstract. (iii) the contributions stated in the abstract do not convey the article\u2019s impact over time. (iv) abstracts usually provide a very broad view of the papers and they may not be detailed enough for people seeking detailed contributions. (v) The content distribution in the abstracts are not evenly drawn from different sections of the papers [4]. These problems have inspired another type of scientific summaries which are obtained by utilizing a set of citations referencing the original paper [66,68]. Each citation, is often accompanied by a short description, explaining the ideas, methods, results, or findings of the cited work. This short description is called citation text or citance [58]. Therefore, a set of citation texts by different paar X iv :1 70 6. 03 44 9v 1 [ cs .C L ] 1\n2 Ju\nn 20\n2 Arman Cohan 1, Nazli Goharian 1\npers can provide an overview of the main ideas, methods and contributions of the cited paper, and thus, can form a summary of the referenced paper. These community based summaries capture the important contributions of the paper, view the article from multiple aspects, and reflect the impact of the article to the community.\nAt the same time, there are multiple problems associated with citation texts. They are written by different authors so they may be biased toward another work. The citation texts lack the context in terms of the details of the methods, the data, assumptions, and results. More importantly, the points and claims by the original paper might be misunderstood by the citing authors; certain contributions might be ascribed to the cited work that are not on par with the original author\u2019s intent. Another serious problem is the modification of the epistemic value of claims, which states that many claims by the original author might be stated as facts in the future citations [26]. An example of this is shown in Figure 1. As illustrated, while the original authors write on some possibilities, later the citing authors state them as known facts. These problems are even more serious in biomedical domain where slight misrepresentations of the specific findings about treatments, diagnosis, and medications, could directly affect human lives.\nOne way to address such problems is to consider the citations in their context from the reference article. Therefore, citation texts should be linked to the specific parts in the reference paper that correctly reflect them. We call this \u201ccitation contextualization\u201d. Citation contextualization is a challenging task due to the terminology variations between the citing and cited author\u2019s language usage.\nScientific papers have the unique characteristic of following a specific discourse structure. For example, a typical scientific discourse structure follows this form: problem and motivation, methods, experiments, results, and implications. The rhetorical status of a citation provides additional useful information that can be used in applications such as information extraction, retrieval, and summarization [77]. Each citation text could refer to specific discourse facets of the referenced paper. For example one citation could be about the main method of the referenced paper while the other one could mention their results. Identifying these discourse facets has distinct values for scientific summarization; it allows creating more coherent summaries and diversifying the points included in the generated scientific summaries.\nScientific summarization is recently further motivated by TAC1 2014 summarization track, and the 2016 computation linguistics summarization shared task [41]. Following these works and motivated by the challenges mentioned above, we propose a framework for scientific summarization based on citations. Our approach consists of the following steps:\n\u2013 Contextualizing citation texts: We propose several\napproaches for contextualizing citations. Finding the exact reference context for the citations is challenging due to discourse variation and terminology differences between the citing and the referenced authors. Therefore, traditional Information Retrieval (IR) methods are inadequate for finding the relevant contexts. We propose to address this challenge by query reformulations, utilizing word embeddings [7], and domain-specific knowledge. Our main approach is a retrieval model for finding the appropriate context of the citations and is designed to handle terminology variations between the citing and cited authors. \u2013 Discourse structure: After extracting the context of\nthe citation texts, we classify them into different discourse facets. We use a linear classifier with variety of features for classifying the citations. \u2013 Summarization: We propose two approaches for\nsummarizing the papers. Both approaches are based on summarization through the scientific community where the main points of a paper are captured by a set of given citations. Our approach extends the previous works on citation-based summarization [65, 66,67] by including the reference context to address the inaccuracy problem associated with the citation texts. After extracting the citation contexts from the reference paper, we group them into different\n1 Text Analysis Conference, http://tac.nist.gov/2014/BiomedSumm/\nScientific document summarization via citation contextualization and scientific discourse 3\ndiscourse facets. Then using the most central sentences in each group, we generate the final summary.\nIn particular our contributions are summarized as follows: (i) An approach for extracting the context of the citation texts from the reference article. (ii) Identifying the discourse facets of the citation contexts. (iii) A scientific summarization framework utilizing citation contexts and the scientific discourse structure. (iv) Extensive evaluation on two scientific domains."}, {"heading": "2 Related work", "text": "2.1 Citation text analysis\nCitations play an integral role in the scientific development. They help disseminate the new findings and they allow new works to be grounded on previous efforts [36]. While there is a large body of related work on analysis of citation networks, instead of link analysis, we focus on textual aspects of the citations. To better utilize the citations, researchers have explored ways to extract citation texts, which are short textual parts describing some aspects of the cited work. Examples of the proposed approaches for extracting the citation texts include jointly modeling the link information and the citation texts [46], supervised Markov Random Fields classifiers [67], and sequence labeling with segment classification [3]. These approaches focus on finding the sentences or textual spans in the citing article that explain some aspects of the cited work. In this work, we assume that citation texts are already obtained either manually or by using one of these works. Given the citation texts, we instead focus on contextualizing these citation texts using the reference; we find the text spans in the reference article that most closely reflect the citation text.\nThere exists some related work on further analyzing the citations for finding their function or rhetorical status [77,32,1,36]. In these works, the authors tried to identify the reasons behind citations which can be a statement of weakness, contrast or comparison, usage or compatibility, or a neutral category. They proposed a classification framework based on lexically and linguistically inspired features for classifying citation functions. The distribution of citations within the structure of scientific papers have been also studied [9]. The authors of [17] have investigated the problem of measuring the intensity of the citations in scientific papers and in [16], the authors proposed using the discourse facets for scientific article recommendation. Recently, a framework for understanding citation function has\nbeen proposed [45] which unifies all the previous efforts in terms of definition of citation functions. While citation function can provide additional information for summarization, in this work we do not utilize these information. Instead, we utilize the discourse facet of the citation contexts in a reference paper.\n2.2 Citation contextualization\nMore recently, there has been some efforts in contextualizing citations from the reference. In particular, TAC 2014 summarization track,2 and the CL-SciSumm 2016 shared task on computational linguistic summarization [41] have released datasets to promote research for citation contextualization. The former is more domain specific, focusing on biomedical scientific literature, while the latter is in a more general domain consisting of publications in computational linguistics. To our knowledge, there is no overview paper on TAC. We briefly discuss the successful approaches in CL-SciSumm 2016. The authors of [13] used an SVM-rank approach with features such as tf-idf3 cosine similarity, position of the reference sentence, section position, and named entity features. In another approach [49], the authors used an SVM classifier with sentence similarity and lexicon based features. The authors of [59] proposed a hybrid model based on tf-idf similarity and a single layer neural network that scores the relevant reference texts above the irrelevant ones. Finally, in the work by [47], the authors proposed the use of TextSentenceRank algorithm which is an enhanced version of the TextRank algorithm for ranking keywords in the documents. Here, we specifically focus on the problem of terminology variation between the citing and cited authors. We propose approaches that address this problem. Our proposed approaches are based on query reformulations, word embeddings, and domain-specific knowledge.\n2.3 Text summarization\nDocument summarization has been an active research area in NLP in recent decades; there is a rich literature on text summarization. Approaches towards summarization can be divided into the following categories: (i) topic modeling based [33,74,78,15]: In these approaches, the content or topical distribution of the final summary is estimated using a probabilistic framework. (ii) solving an optimization problem [20,8,27]: these approaches cast the summarization problem as\n2 http://tac.nist.gov/2014/BiomedSumm/ 3 Term Frequency - Inverted Document Frequency.\n4 Arman Cohan 1, Nazli Goharian 1\nan optimization problem where an objective function needs to be optimized with respect to some constraints. (iii) supervised models [60,25,18], where selection of sentences in the summary are learned using a supervised framework. (iv) graph based [29,53,62]: these approaches seek to find the most central sentences in a document\u2019s graph where sentences are nodes and edges are similarities. (v) Heuristic based [14,34,51]: these works approach the summarization problem by greedy selection of the content. (vi) Neural networks: More recently, there has been some efforts on utilizing neural networks and sequence-to-sequence models [75] for generating summaries of short texts and sentences [70,19]. Most of these works have focused on general domain summarization and news articles. Scientific articles are much different than news articles in elements such as length, language, complexity and structure [76].\nOne of the first works in scientific article summarization is done by [76] where the authors trained a supervised Naive Bayes classifier to select informative content for the summary. Later, the impact of citations to generate scientific summaries was realized [28]. In the work by [65], the authors proposed an approach for citation-based summarization based on a clustering approach, while in [2] and [42], the focused on producing coherent scientific summaries. We argue that citation texts by themselves are not always accurate and they lack the context of the cited paper. Therefore, if we only use the citation texts for scientific summarization, the resulting summary would potentially suffer from the same problems, and it might not accurately reflect the claims made in the original paper. We address this problem by leveraging the citation contexts from the reference paper. We also utilize the inherent discourse structure of the scientific documents to capture the important content from all sections of the paper.\nWe present a comprehensive framework for scientific summarization which utilizes and builds upon our earlier efforts [23,21,22]. We propose new approaches for citation contextualization. We further extend our experiments on an additional dataset (CL-SciSum 2016) and evaluate our approaches on both TAC and CLSciSum datasets, providing detailed analysis."}, {"heading": "3 Methodology", "text": "Our proposed method is a pipeline for summarizing scientific papers. It consists of the following steps:\n1. citation contextualization (extracting the relevant\ncontext from the reference paper)\n2. identifying the discourse facet of the extracted con-\ntext\n3. summarization\nWe first explain our proposed methods for contextualization, we then describe our approach for identifying discourse facets of the citation contexts, and finally we outline our summarization approach.\n3.1 Citation contextualization\nCitation contextualization refers to extracting the relevant context from the reference article for a given citation text. We propose the following three approaches for this problem: (i) Query reformulation, (ii) Word embeddings and domain knowledge, and (iii) Supervised classification."}, {"heading": "3.1.1 Query reformulation (QR)", "text": "We cast the contextualization problem as an Information Retrieval (IR) task. We first extract textual spans from the reference article and index them using an IR model. The textual spans are of granularity of sentences. In order to capture longer contexts (those consisting of multiple consecutive sentences), we also index sentence n-grams. That is, we index each n consecutive sentences as a separate text span.4 After constructing the index, we consider the citation text as the query, and we seek to find the relevant context from the indexed spans. Since the citation texts are often longer than usual queries in standard IR tasks, we apply query reformulation methods on the citation to better retrieve the related context. We utilize both general and domain-specific query reformulations for this purpose. We first remove the citation markers (author names and year, and numbered citations) from the citations, as they do not appear in the reference text and hence are not helpful. We design several regular expressions to capture these names. The proposed query reformulation (QR) methods are described below:\nQuery reduction Since the citation texts are usually more verbose than standard queries, there might be many uninformative terms in them that do not contribute in finding the correct context. Hence, we apply query reduction methods to only retain the important concepts in the citation. After removing the stop words from the citation, we further experiment with the following three query reduction methods:\n1. Noun phrases (QR-NP). Citation texts are usu-\nally linguistically well-formed, as they are extracted\n4 we indexed up to 3 consecutive sentences in our experiments.\nScientific document summarization via citation contextualization and scientific discourse 5\nfrom scientific papers. This allows us to apply a variety of linguistic tagging and chunking methods to the query to capture the informative phrases. Previous works have shown that noun phrases are good representation of informative concepts in the query [5,40,39]. We thus extract noun phrases from the citation text and omit all other terms. 2. Key concepts (QR-KW). Key concepts or keywords\nare single or multi-word expressions that are informative in finding the relevant context. We use the Inverted Document Frequency (IDF) [73] measure to find the key concepts. The terms that are prevalent throughout all the text spans do not provide much information in retrieval. IDF values help capturing the terms and concepts that are more specific. For key concept extraction, we limit the IDF values between some threshold that can be tuned according to the dataset.5 We consider phrases of up to three terms. 3. Ontology (QR-Domain). Domain-specific ontologies\nare expert curated lexicons that contain domainspecific concepts. In this reformulation method, we use an ontology to only keep important (domainspecific) concepts in the query. Since the TAC dataset is in the biomedical domain, we use the UMLS [10] thesaurus which is a comprehensive ontology of biomedical concepts. We specifically use the SNOMED CT [72] subset of UMLS.\nAs explained in Section 3.1.1, the indexing approach also contains consecutive sentences. Therefore, our retrieval approach can find text spans that have overlaps with each other. Furthermore, retrieving multiple spans from around the same location in the text signals the importance of that specific location. We apply a reranking and merging method to the retrieved spans to remove shared spans and better rank the more relevant context. We merge the two overlapping spans if the retrieval score of the larger span is higher than the smaller span. We also evaluated other query reformulation methods such as Pseudo Relevance Feedback [12]; however, they performed worse than the baseline and thus we do not discuss them further."}, {"heading": "3.1.2 Contextualization using word embeddings and domain knowledge", "text": "To explicitly account for terminology variations and paraphrasing between the citing and the cited authors, we propose another model for citation contextualization utilizing word embeddings and domain-specific knowledge.\n5 We empirically set this threshold to 1.9 and 2.2 for the TAC and CL-SciSum datasets, respectively.\nEmbeddings. Word embeddings or distributed representations of words are mapping of words to dense vectors according to a distributional space, with the goal that similar words will be located close to each other [6]. We extend the Language Modeling (LM) for information retrieval model [64] by utilizing word embeddings to account for terminology variations. Given a citation text (query) q, and a reference span (document) d, the LM scores d based on the probability that d has generated q (p(d|q)). Using standard simplifying assumptions of term independence and uniform document prior, we have:\np(d|q) \u221d p(q|d) = n\u220f\ni=1\np(qi|d) (1)\nwhere qi (i = 1, ..., n) are the terms in the query. In LM with Dirichlet Smoothing [80], p(qi|d) is calculated using a smoothed maximum likelihood estimate:\np(qi|d) = f(qi, d) + \u00b5 p(qi|C)\u2211\nw\u2208V f(w, d) + \u00b5 (2)\nwhere f is the frequency function, p(qi|C) shows the background probability of term qi in collection C, V is the entire vocabulary, and \u00b5 is the Dirichlet parameter.\nOur model extends the above formulation (Eq. 2) by using word embeddings. In particular we estimate the probability p(qi|d) according to the following equation: p(qi|d) = \u2211\ndj\u2208d s(qi, dj) + \u00b5 p(qi|C)\u2211 w\u2208V \u2211 dj\u2208d s(w, dj) + \u00b5\n(3)\nwhere dj are terms in the document d, and s is a function that captures the similarity between the terms and is defined as:\ns(qi, dj) =\n{ \u03c6 ( e(qi), e(dj) ) , if e(qi).e(dj) > \u03c4\n0, otherwise (4)\nwhere e(qi) shows the unit vector corresponding to the embedding of word qi, \u03c4 is a threshold, and \u03c6 is a transformation function. Below we explain the role of parameter \u03c4 and the transformation function \u03c6.\nWord embeddings can capture the similarity values of words according to some distance function. Most embedding methods represent the distance in the distributional semantics space. Therefore, similarities between two words qi and dj can be captured using the dot product of their corresponding embeddings (i.e. e(qi).e(dj)). While high values of this product suggest syntactic and semantic relatedness between the two terms [54,63,38],\n6 Arman Cohan 1, Nazli Goharian 1\nword 1 word 2 Similarity\nmarker mint 0.11 notebook sky 0.07 capture promotion 0.12\nblue sky 0.31 produce make 0.43\nTable 1 Example of similarity values between terms according to the dot product of their corresponding embeddings. Using the pre-trained Word2Vec model on Google News corpus. The top part of the table shows pairs of random words, while the bottom part shows similarity values for pairs of related words.\nmany unrelated words have non-zero dot products (an example is shown in Table 1). Therefore, considering them in the retrieval model introduces noise and hurts the performance. We address this issue by first considering a threshold \u03c4 below which all similarity values are squashed to zero. This ensures that only highly relevant terms contribute to the retrieval model. To identify an appropriate value for \u03c4 , we select a random set of words from the embedding model and calculate the average and standard deviation of point-wise absolute values of similarities between the pairs of terms from these samples. We then set \u03c4 to be two standard deviations larger than the average similarities, to only consider very high similarity values. We also observe that for high similarity values between the terms, the values are not discriminative enough between more or less related words. This is illustrated in Figure 2 where we can see that the most similar terms to the given term are not too discriminative. In other words, the similarity values decline slowly as moving away from top similar words. We instead want only very top similar words to contribute to the retrieval score. Therefore, we transform the similarity values according to a logit function (equation 5) to dampen the effect of less similar words (see Figure 2):\n\u03c6(x) = log( x\n1\u2212 x ) (5)\nWhile any approach for training the word embeddings could be used, we use the Word2Vec [48] method, which has proven effective in several word similarity tasks. We train Word2Vec on the recent dump of Wikipedia.6 Since the TAC dataset is in biomedical domain, we also train embeddings on a domain-specific collection; we use the TREC Genomics collections, 2004 and 2006 [37] which together consist of 1.45 billion tokens.\n6 https://dumps.wikimedia.org/enwiki/\nIncorporating domain knowledge Word embedding models learn the relationship between terms by being trained on a large corpus. They are based on the distributional hypothesis [35] which states that similar words appear in similar contexts. While these models have been very successful in capturing semantic relatedness, recent related works have shown that domain ontologies and expert curated lexicons may contain information that are not captured by embeddings [57,38,31]; hence, we account for the domain knowledge according to the following.\n\u2013 Retrofitting embeddings: In this method, we apply\na post-processing step called retrofitting [31] to the word embeddings used in the model. Retrofitting optimizes an objective function that is based on relationships between words in a lexicon; it intuitively pulls closer the words that are related to each other and pushes farther the words that are not related to each other according to a given ontology. For the ontology, since TAC data is in biomedical domain we use two domain-specific ontologies, Mesh7 [52] and Protein Ontology (PRO).8 For the CL-SciSum data, since it is less domain-specific, we use the WordNet lexicon [55]. \u2013 Interpolating in the LM: In this method, instead of\nmodifying the word vectors, we incorporate the domain knowledge directly in the retrieval model. We do so by interpolation of two following probability estimates:\np(qi|d) = \u03bbp1(qi|d) + (1\u2212 \u03bb)p2(qi|d) (6)\n7 MEdical Subject Headings 8 http://pir.georgetown.edu/pro/\nScientific document summarization via citation contextualization and scientific discourse 7\nwhere p1 is estimated using Eq. 3 and p2 is a similar model that counts in the is-synonym relations (is-syn) in calculating similarities. Its formulation is exactly like Eq. 3 except it replaces the function s with the following function:\ns2(qi, dj)=  1, if qi=dj \u03b3, if qi is-syn dj\n0, o.w.\n(7)\nThis function is essentially partially counting the synonyms in calculation of the probability estimate p(qi|d) by the amount of \u03b3. We empirically set the value of \u03b3. Word embedding based methods are shown by WE in short in the results."}, {"heading": "3.1.3 Supervised classification", "text": "The two previous context retrieval models are unsupervised and as such do not take advantage of the already labeled data. CL-SciSum dataset includes separate training and testing sets which allow us to also investigate supervised approaches. We propose a featurerich classifier to find the correct context for each given citation. Our approach aims to capture the semantic relatedness between a given citation text and a candidate context sentence. We specifically utilize the following features to capture this relatedness:\n\u2013 Word match: counts the number of identical words\nbetween the source citation text and the candidate reference context normalized by length. \u2013 Fuzzy word match: same as above, with the differ-\nence that we use character n-grams to capture partial matches between the words. \u2013 Embedding-based alignment: measures the similar-\nity between the source and target sentences using word embedding alignment. Specifically for the two sentences S1 and S2, the following function f scores the sentences based on their similarity:\nf(S1, S2) =\n\u2211 w\u2208S1 maxv\u2208S2 s(w, v)\n|S1| (8)\nwhere s is a similarity function according to the equation 4. Intuitively, f captures the similarity between the two sentences without only relying on lexical overlaps; it takes into account the similarity values between the terms. \u2013 Distance between average of embeddings: measure\nthe similarity between the two sentences by dot product of the average of their constituent word vectors.\nFeature Name\nCitation Text Extracted Reference Context Verb Features Ralative Section Position\nWe train a standard linear classifier (e.g. Logistic Regression) using these features to identify the correct context for a given citation text.\n3.2 Identifying discourse facets\nThe organization of scientific papers usually follows a standardized discourse pattern, where the authors first describe the problem or motivation, then they talk about their methods, then the results, and finally discussion and implications. Our goal is to capture the important content from all sections of the paper; therefore, after extracting the citation contexts, we identify the associated discourse facet for each of the citation contexts retrieved from the previous step. Each citation context refers to some specific discourse facets of the reference document. To identify the correct discourse facets, we train a simple supervised model with features listed in Table 2. Essentially, we use the citation text and the extracted reference context represented by character n-grams, the verbs in the context sentence, and the relative position of the retrieved context in the paper as features for the classifier. While the textual features (citation and it\u2019s context) were the most helpful, we empirically observed slight improvements by incorporating the verb and section position features. We train the model using an SVM classifier [79]. For the textual features, we transform them using character ngrams to allow fuzzy matching between the terms.\n3.3 Generating the summary\nAfter extracting reference contexts for the citations as described in Section 3.1, and identifying their discourse\n8 Arman Cohan 1, Nazli Goharian 1\nfacet (Section 3.2), we generate a summary of the reference paper. Our goal is to create a summary that contains information from different discourse facets of the paper. This helps not only in diversifying the content in the summary, but also in creating a more coherent summary. To generate a summary, we first identify the most representative sentences in each group. Intuitively, we only need a few top representative sentences from each discourse facet to include in the summary. In order to find the most representative sentences, we consider sentences in each facet as nodes and their similarities as weighted edges in a graph. We then apply the \u201cpower method\u201d [30] which is an algorithm similar to the PageRank random walk ranking model [61], that finds the most central nodes in a graph. It works by iteratively updating the score of each sentence according to its centrality (total weight of incoming edges) and the centrality of its neighbors. After ranking the sentences in each group according to their centrality score, we select sentences for the final summary. We use the following methods for creating the final summary:\n\u2013 Iterative. This method simply iterates over the dis-\ncourse facets and selects the top representative sentence from each group until the summary length threshold is met. \u2013 Greedy. The iterative approach could result in simi-\nlar sentences ending up in the summary; this results in redundant information and potential exclusion of other important aspects of the paper from the summary. To address this potential problem, we use a heuristic that accounts for both the informativeness of candidate sentence and their novelty with respect to what is already included in the summary. Maximal Marginal Relevance [14] is one such heuristic that has these properties. It is based on the linear interpolation of the informativeness and the novelty of the sentences.\n4 Experiments\n4.1 Data\nWe conducted our experiments on two scientific summarization datasets. The first dataset is the TAC 2014 scientific summarization dataset.9 The TAC benchmark is in biomedical domain and is publicly available upon request from NIST.10 The second dataset is the 2016 CL-SciSumm dataset [41] which is available on a public repository11 and contains scientific articles from the\n9 http://tac.nist.gov/2014/BiomedSumm/ 10 National Institute of Standards and Technology 11 https://github.com/WING-NUS/scisumm-corpus\nCharacteristic TAC CL-SciSum\n# Documents 220 506 # Reference Documents 20 30 Avg. # Citing Docs for each Ref 15.5 15.9 Total # Citation Texts 313 702 Avg. Gold summary length (words) 235.6 134.2 Stdev. Gold summary length (words) 31.2 27.9 Separate train test sets No Yes\nTable 3 Characteristics of the datasets. #: number of, Avg: average, and Stdev: standard deviation.\ncomputational linguistics domain. To our knowledge, these two are the only datasets on scientific summarization.\nThe TAC dataset only has one training set consisting of 20 topics. There is one reference article in each topic and another set of articles citing the reference. For each topic, 4 annotators have identified the relevant contexts, the correct discourse facet, and they have written a summary. The documents are provided as plain text files and there is no predefined sentence boundaries and sections. On the other hand, the CL-SciSumm data contain separate train, development, and test sets with 30 topics in total. Similar to TAC, each topic consists of reference and a set of citing articles but in the computational linguistics domain. The articles are in xml format with known sentence boundaries and sections. Another distinction is that topics in the CL-SciSumm data are annotated by one annotator at a time. The full statistics of the datasets is illustrated in Table 3. The distribution of the discourse facets in the two datasets is also shown in Figure 3. Since the two datasets are in different domains, the difference between the distribution of the facets is expected.\n4.2 Citation contextualization\nEvaluation Evaluation of the retrieved contexts is based on the overlap of the position of the retrieved contexts and the gold standard contexts. Per TAC guidelines12, evaluation of the TAC benchmark was performed using character offset overlaps weighted by human annotators. More formally, for a set of system retrieved contexts S, and gold standard context R = {R1\u222aR2\u222a....\u222aRm} by m annotators, the weighted character based precision (Pchar) and recall (Rchar) are defined as follows:\nPchar = \u2211m i |S \u2229Ri| m\u00d7 |S| (9) Rchar = \u2211m i |S \u2229Ri|\u2211m i |Ri| (10)\n12 http://tac.nist.gov/2014/BiomedSumm/guidelines.html\nThe official metric for the CL-SciSum challenge was sentence level overlaps of the retrieved contexts with the gold standard. This was possible because unlike the articles in TAC which were in plaintext format, the sentence boundaries in CL-SciSum were pre-specified. We also report character level metrics for the CL-SciSum corpus; as we will see, the character level and sentence level metrics are more or less comparable.\nOne problem with position based evaluation metrics (character, or sentence) is that a system might retrieve a context that is in a different position than gold standard, but similar to the content of the gold standard. In such cases, the system is not rewarded at all. This is possible because authors might talk about a similar concept in different sections of the paper. To consider textual similarities of the retrieved context with the gold standard, we also compute Rouge-N scores [50].\nComparison To our knowledge, no review paper about the TAC challenge was released. Hence, for the TAC dataset, we compare our method against the following baselines:\n\u2013 VSM. Ranking by Vector Space Model (VSM) with\ntf-idf weighting of the citations and the target reference contexts. \u2013 BM25. BM25 scoring model [44] which is a prob-\nabilistic framework for ranking the relevant documents based on the query terms appearing in each document, regardless of their relative proximity. \u2013 LMD. Language modeling with Dirichlet smoothing\n(LMD) [80] is a probabilistic framework that models the probability of documents generating the given query. \u2013 LMD-LDA. An extension of the LMD retrieval\nmodel using Latent Dirichlet Allocation (LDA) which is recently proposed [43]. This model considers latent topics in ranking the relevant documents\nFor the CL-SciSum data, we also compare against the top 5 best performing system. For brief description about these approaches refer to section 2.\nResults. The results on the TAC dataset are presented in Table 4. We observe that our proposed methods im-\nprove over all the baselines. Query Reformulation methods (NP and KW, respectively,) obtain character offset F1-scores of 23.8 and 24.1, which improve the best baseline by 7% and 8%. They also obtain higher Rouge scores. This shows that noun phrases and key words can capture informative concepts in the citation that help better retrieving the related reference context. Our models based on word embeddings are also outperforming the baselines in virtually all metrics. General domain embeddings trained on Wikipedia (WEwiki) and domain-specific embeddings trained on Genomics data (WEBio), achieve F1-scores of 23.2 and 25.5 with 4% and 14% improvement over the best baseline, respectively. Higher performance of the biomedical embeddings in comparison with general embeddings is expected because the words are captured in their correct context. An example is shown in Table 6, where the top similar words to the word \u201cexpression\u201d are shown. The word \u201cexpression\u201d in the biomedical context is defined as \u201cthe process by which genetic instructions are used to synthesize gene products\u201d. As we can see, using general domain embeddings, we might fail to capture this notion. Incorporating domain knowledge in the model results in further improvement as shown in last two rows of Table 4. The model using retrofitting WEBio+Retrofit improves the best baseline by 18% while the interpolated model (WEBio+Domain) achieves the highest improvement by 21%. These results show the effectiveness of domain knowledge in the model.\nTable 5 shows the results for the CL-SciSum dataset. The first 3 rows are baselines that also are reported in TAC evaluation; in addition to those baselines, we also consider top performing state-of-the-art systems of 2016 CL-SciSum (lines 4-8) as additional baselines to compare with. For the CL-SciSum participating systems, we report the official sentence based evaluation metrics; the Rouge scores and character based metrics were not reported in the official evaluation of the task. Some of our methods are specific to the biomedical domain such as WEBio; therefore, we do not evaluate those on the CL-SciSum dataset which is in a completely different domain.\nAs shown in Table 5, our methods outperform the state-of-the-art on this dataset as well. The embedding-based model with Wikipedia trained em-\nbeddings (WEwiki) achieves the best results with 13.9% F-1 score of sentence overlaps which is slightly higher than the F-1 score of 13.4 achieved by the best previous work (Tf-idf+stem in the Table) [56]. Interestingly, we observe that retrofitting (WEwiki+Retrofit) does not improve over the standard embedding-based approach. This is likely due to the choice of the Wordnet lexicon for retrofitting. While Wordnet contains general domain terms, it does not necessarily capture relationships of words in the context of computational linguistics. In contrast to TAC where we had a domain specific lexicon suitable for the dataset, for the CL-SciSum data we did not find any lexicon capturing the term relationships in the computational linguistics domain. We believe that retrofitting with such lexicon, could result in further improvements. While query reformulationbased approaches improve over most of the baselines, their performance fall below the best baseline system. On the other hand, our supervised method also improves the best baseline, achieving the highest overall prevision (11.3%) and Rouge-2 (17.5%) and Rouge-3\nNumber of Citations Number of Annotators with at\nleast partial agreement\n68 4 66 3\n121 2\n11 No agreement\nTable 8 The table shows the number of citations grouped by the number of annotators that agree at least partially on the context.\nscores (15.0%).13 It is encouraging that our embeddingbased models (method names starting with \u201cWE\u201d in the Table 5), which are unsupervised models achieve the best results on this task and surpass the performance of the feature-rich supervised models. Table 7 shows the importance of each feature for our supervised method (explained in \u00a7 3.1.3). While the most important features are n-gram and character n-gram based tf-idf similarity, embedding based alignment and distance of average embeddings are also important in finding the correct context.\nAs evident from tables 4 and 5, the absolute system performances are not high, which further shows that this task is challenging. Since the TAC data are annotated by 4 people, we investigate the difficulty of this task for the human annotators. To do so, we calculate the agreement of the annotators with respect to the relevant context for the citations. Table 8 shows the number of citations grouped by the number of annotators that agree at least partially on the correct context. As illustrated, there are 68 citations out of 313 that all 4 annotators have partial agreement on the context span. This shows that the contextualization task is not trivial even for the human expert annotators.\n13 We do not report results of supervised model on TAC dataset because the TAC data do not have separate train and test sets.\nParameters Our interpolated model of embeddings and domain knowledge (WEBio+Domain) has two main parameters \u03b3 and \u03bb. Figure 4 shows the sensitivity of our model to different parameters. We observe that the best performance is achieved when \u03b3 = 0.8 and \u03bb = 0.5. Our models retrieve a ranked list of contexts for the citations; we choose a cut-off point for returning the final results. Figure 4(c) shows the effect of the cut-off point on one of our models.14 We observe that the optimal cut-off point for best sentence F1-score is 3.\n4.3 Identifying discourse facets\nEvaluation The official metric for evaluation of discourse facet identification is the Precision, Recall and F1-scores of the discourse facets, conditioned on the correctness of the retrieved reference context [41]. Therefore, we report the results for the CL-SciSum data based on this metric. For the TAC dataset, the official metric is the classification accuracy weighted by the annotator agreements.15 The accuracy for a system returned discourse facet is the number of annotators agreeing with that discourse facet divided by total number of annotators.\nResults Table 9 shows the results of our methods compared with the top performing official submitted runs to the CL-SciSum 2016. We do not report the results of low performing systems. The classification algorithm for identifying the discourse facets is the method described in Section 3.2 across all our methods. However, since only the correct retrieved contexts are rewarded,\n14 The cut-off point has similar effect on all the models. 15 http://tac.nist.gov/2014/BiomedSumm/guidelines/\nthe performance of each model differs based on the accuracy of retrieving the correct contexts. We observe that most of our methods (except for the QR-NP) improve over all the baselines in terms of all metrics. We obtain substantial improvements especially in terms of precision. The best method for identifying the discourse facets is the supervised method (indicated with \u201csupervised\u201d in the Table) which obtains 36.1% F-1 score, improving the best baseline (\u201cJaccard Focused Method\u201d) by 16%. Embedding methods also perform well by obtaining F-1 scores of 33.1% for the Wikipedia embeddings, and 34.8% for the retrofitted embeddings. These results further show the effectiveness of our contextualization methods along with the proposed classifier for identifying the facets.\nWe also demonstrate the intrinsic performance of our classifier for identifying the discourse facets in Table 10. As illustrated, the weighed average F1 performance over all discourse facets is 0.73. One challenge in identifying the discourse facets is the unbalanced dataset and the limited number of training examples for some specific facets. As also reflected in the table, we observe that for categories with smaller number of\nScientific document summarization via citation contextualization and scientific discourse 13\nSVM RF LR Oracle\nTAC 0.53 0.49 0.51 0.67 CL-SciSum 0.67 0.64 0.66 -\nTable 11 Effect of learning algorithms in identifying the discourse facets. SVM: Support Vector Machine with Linear Kernel, RF: Random Forest, LR: Logistic Regression, Oracle: Highest achievable score. Numbers are weighted accuracy scores by annotators.\ninstances, the performance is generally lower. We therefore believe that having more training samples in the rare categories could further increase the performance.\nTable 11 shows the results of facet identification in the TAC dataset as well as the effect of learning algorithms. Since for the TAC dataset there are 4 annotators, and the official metric is weighted accuracy scores, we also calculate the oracle score by always predicting what the majority of the annotators agree on. The oracle achieves 0.67 percent, suggesting that identifying discourse facets is not trivial for humans. We can see that the SVM classifier achieves the highest results with 81% relative accuracy to the oracle. For the CL-SciSum dataset, there is only one annotator per discourse facet and therefore, the weighted accuracy metrics translates to simple accuracy scores.\n4.4 Summarization\nWe evaluate our summarization approach against the gold standard summaries written by human annotators. We set the summary length threshold to the average length of summary by words in each dataset (see Table 3). Table 12 shows the results for the summarization task. The first lines show the baselines which are existing summarization approaches including the SumBasic [78] algorithm and the original citation-based summarization approach [66]. The next four lines are the top state-of-the-art systems on the CL-SciSum dataset. For the CL-SciSum systems, the official reported results only included Rouge-2 and Rouge-SU4 scores. As illustrated in the table, virtually all our methods improve over the state-of-the-art, showing the effectiveness of our proposed summarization approach. Our best method (QR-NP-greedy) is based on the noun phrases query reformulation using the greedy strategy of sentence selection . It achieves Rouge-2 score of 30.2, which improves over the best baseline by 37.4%. In general, we can see that the greedy sentence selection strategy works better than the iterative approach. This is because the greedy strategy takes into account both the informativeness and the redundancy of the selected sentences.\nTable 13 shows the results of summarization using on the TAC dataset. The reported approaches all use the greedy sentence selection strategy as it consistently outperforms the iterative approach. In general, while all our approaches outperform the baseline, query reformulation based approaches achieve the highest Rouge scores; query reformulation method using noun phrases (QR-NP) achieves 15.8 and 6.9 Rouge-2 and Rouge-3 scores, respectively which is the highest scores. The interpolated word embedding based model (WEBio+Domain) achieves the highest Rougesu4 score (20.7). Comparing Tables 12 and 13 we notice that the scores for the TAC dataset are lower than that of CL-SciSum. This is due to the length of the generated summaries. As shown in Table 3, the average human summary length in the TAC data is almost 100 words more than the CL-SciSum summaries. An interesting observation in these two tables is regarding the relative poor performance of the citation-based summarization baseline (CLexRank) that only uses citation texts in comparison with our methods that also take advantage of the citation context and the discourse structure of the articles. This observation further confirms our initial hypothesis that relying only on the citation texts could result in summaries that do not accurately reflect the content of the original paper, and that adding citation contexts can help produce better summaries.\nTo better analyze the effect of identifying discourse facets on the overall quality of the summary, we compare the Rouge scores of the summary generated by our approach with and without this step. Table 14 shows the overall summarization results based on our QR-NP approach when we only use contextualized citations compared with when we use faceted contextualized citations. We observe that grouping citation contexts by their corresponding discourse facet has a positive effect on the quality of the summary on both datasets (17% and 55% improvements over TAC and CL-SciSum datasets in terms of Rouge-2, respectively). This is because identifying facets and grouping the contextualized citations by facets, results in a summary that captures the content from all sections of the paper. We observe similar trends for other variants of our approaches; for brevity we only show the results for QR-NP as an illustrative analysis on the effect of identifying discourse facets on the quality of the generated summary.\nFinally, an example of the generated summaries by our system (QR-NP-greedy) that uses citation contexts and discourse facets is illustrated in Figure 5. We observe that compared with the human summary, the summary generated by our system can capture the significant points of the paper."}, {"heading": "5 Discussion", "text": "Citations are a significant part of scientific papers and analysis of citation texts can provide valuable information for various scholary applications. Our work provides new approaches for contextualizing citations which is a sub-task for enriching citation texts and thus can benefit various bibliometric enhanced NLP applications such as information extraction, information retrieval, article recommendation, and article summarization. Our work provides a comprehensive new framework for summarizing scientific papers that helps generating better scientific summaries.\nWe note that our evaluation was based on the Rouge automatic summarization evaluation frame-\nwork. Automatic evaluation metrics have their own limitations and cannot fully characterize the effectiveness of the systems. Manual or semi-manual evaluation of summarization (e.g. through Pyramid framework) are alternative evaluation approaches that can provide additional insights into the performance of the systems. Yet, due to expense and reproduction issues, most of the standard evaluation benchmarks including TAC and CL-SciSum have been evaluated through Rouge. As it is standard in the field and to be able to compare our results with the related work, we used the Rouge framework for evaluation. We also note that our focus has been on the content quality of the summaries and other criteria such as coherence and linguistic cohesion have not been the focus of our approach. Future work can investigate approaches for improving coherence and linguistic properties of the generated summaries."}, {"heading": "6 Conclusions", "text": "We presented a unified framework for scientific summarization; our framework consists of three main parts: finding the context for the citations in the reference paper, identifying the discourse facet of each citation context, and generating the summary from the faceted citation contexts. We utilized query reformulation methods,\nword embeddings, and domain knowledge in our methods to capture the terminology variations between the citing and cited authors. We furthermore took advantage of the scientific discourse structure of the articles. We demonstrated the effectiveness of our approach on two scientific summarization benchmarks each in a different domain. We improved over the state-of-the-art by large margins in most of the tasks. While the results are encouraging, the absolute values of some metrics especially in the contextualization task suggest that this problem is worth further exploration. Contextualizing citations is a new task and not only it helps improving scientific summarization, but also it can benefit other bibliometric enhanced end-to-end applications such as keyword extraction, information retrieval, and article recommendation."}], "references": [{"title": "Purpose and polarity of citation: Towards nlp-based bibliometrics", "author": ["A. Abu-Jbara", "J. Ezra", "D.R. Radev"], "venue": "NAACL-HLT, pp. 596\u2013606", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Coherent citation-based summarization of scientific papers", "author": ["A. Abu-Jbara", "D. Radev"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesVolume 1, pp. 500\u2013509. Association for Computational Linguistics", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Reference scope identification in citing sentences", "author": ["A. Abu-Jbara", "D. Radev"], "venue": "NAACL-HLT, pp. 80\u201390. ACL", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "On the composition of scientific abstracts", "author": ["I. Atanassova", "M. Bertin", "V. Larivi\u00e8re", "D. Bawden"], "venue": "Journal of Documentation 72(4)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Discovering key concepts in verbose queries", "author": ["M. Bendersky", "W.B. Croft"], "venue": "Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pp. 491\u2013498. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE transactions on pattern analysis and machine intelligence 35(8), 1798\u20131828", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "The Journal of Machine Learning Research 3, 1137\u20131155", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Jointly learning to extract and compress", "author": ["T. Berg-Kirkpatrick", "D. Gillick", "D. Klein"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pp. 481\u2013490. Association for Computational Linguistics", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "The invariant distribution of references in scientific articles", "author": ["M. Bertin", "I. Atanassova", "Y. Gingras", "V. Larivi\u00e8re"], "venue": "Journal of the Association for Information Science and Technology 67(1), 164\u2013177", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "The unified medical language system (umls): integrating biomedical terminology", "author": ["O. Bodenreider"], "venue": "Nucleic acids research 32(suppl 1), D267\u2013D270", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references", "author": ["L. Bornmann", "R. Mutz"], "venue": "Journal of the Association for Information Science and Technology 66(11), 2215\u20132222", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Selecting good expansion terms for pseudo-relevance feedback", "author": ["G. Cao", "J.Y. Nie", "J. Gao", "S. Robertson"], "venue": "Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pp. 243\u2013250. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Polyu at cl-scisumm 2016", "author": ["Z. Cao", "W. Li", "D. Wu"], "venue": "BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "The use of mmr, diversitybased reranking for reordering documents and producing summaries", "author": ["J. Carbonell", "J. Goldstein"], "venue": "SIGIR, pp. 335\u2013336. ACM", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "A hybrid hierarchical model for multi-document summarization", "author": ["A. Celikyilmaz", "D. Hakkani-Tur"], "venue": "ACL, pp. 815\u2013824. Association for Computational Linguistics", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Ferosa: A faceted recommendation system for scientific articles", "author": ["T. Chakraborty", "A. Krishna", "M. Singh", "N. Ganguly", "P. Goyal", "A. Mukherjee"], "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining, pp. 528\u2013541. Springer", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "All fingers are not equal: Intensity of references in scientific articles", "author": ["T. Chakraborty", "R. Narayanam"], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 1348\u20131358. Association for Computational Linguistics, Austin, Texas", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Query-focused multi-document summarization: Automatic data annotations and supervised learning approaches", "author": ["Y. Chali", "Hasan", "S.a."], "venue": "Nat. Lang. Eng. 18(1), 109\u2013  145", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["S. Chopra", "M. Auli", "A.M. Rush"], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 93\u201398. Association for Computational Linguistics, San Diego, California", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Global inference for sentence compression an integer linear programming approach", "author": ["J. Clarke", "M. Lapata"], "venue": "J. Artif. Int. Res. 31(1), 399\u2013429", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Scientific article summarization using citation-context and article\u2019s discourse structure", "author": ["A. Cohan", "N. Goharian"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 390\u2013400. Association for Computational Linguistics, Lisbon, Portugal", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Contextualizing citations for scientific summarization using word embeddings and domain knowledge", "author": ["A. Cohan", "N. Goharian"], "venue": "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 17", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2017}, {"title": "Matching citation text and cited spans in biomedical literature: a searchoriented approach", "author": ["A. Cohan", "L. Soldaini", "N. Goharian"], "venue": "Proceedings of the 2015 NAACLHLT, pp. 1042\u20131048. Association for Computational Linguistics", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Vector space and language models for scientific document summarization", "author": ["J.M. Conroy", "S.T. Davis"], "venue": "Proceedings of NAACL-HLT, pp. 186\u2013191", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Classy 2011 at tac: Guided and multilingual summaries and evaluation metrics", "author": ["J.M. Conroy", "J.D. Schlesinger", "J. Kubina", "P.A. Rankel", "D.P. OLeary"], "venue": "Proceedings of the Text Analysis Conference", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Epistemic modality and knowledge attribution in scientific discourse: A taxonomy of types and overview of features", "author": ["A. De Waard", "H.P. Maat"], "venue": "Proceedings of the Workshop on Detecting Structure in Scholarly Discourse, pp. 47\u201355. Association for Computational Linguistics", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Learningbased single-document summarization with compression and anaphoricity constraints", "author": ["G. Durrett", "T. Berg-Kirkpatrick", "D. Klein"], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Blind men and elephants: What do citation summaries tell us about a research article", "author": ["A. Elkiss", "S. Shen", "A. Fader", "G. Erkan", "D. States", "D. Radev"], "venue": "Journal of the American Society for Information Science and Technology", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "J. Artif. Intell. Res.(JAIR) 22(1), 457\u2013479", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "Journal of Artificial Intelligence Research 22, 457\u2013479", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["M. Faruqui", "J. Dodge", "K.S. Jauhar", "C. Dyer", "E. Hovy", "A.N. Smith"], "venue": "NAACL-HLT, pp. 1606\u20131615. Associ Scientific document summarization via citation contextualization and scientific discourse  17 ation for Computational Linguistics", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards an automated citation classifier", "author": ["M. Garzone", "R.E. Mercer"], "venue": "Conference of the Canadian Society for Computational Studies of Intelligence, pp. 337\u2013346. Springer", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2000}, {"title": "Generic text summarization using relevance measure and latent semantic analysis", "author": ["Y. Gong", "X. Liu"], "venue": "Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pp. 19\u201325. ACM", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2001}, {"title": "Probabilistic latent maximal marginal relevance", "author": ["S. Guo", "S. Sanner"], "venue": "SIGIR, pp. 833\u2013834. ACM", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Word 10(2-3), 146\u2013 162", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1954}, {"title": "Survey about citation context analysis: Tasks, techniques, and resources", "author": ["M. Hern\u00e1ndez-alvarez", "J.M. Gomez"], "venue": "Natural Language Engineering 22(03), 327\u2013349", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Trec genomics special issue overview", "author": ["W. Hersh", "E. Voorhees"], "venue": "Information Retrieval 12(1), 1\u201315", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Simlex-999: Evaluating semantic models with genuine similarity estimation", "author": ["F. Hill", "R. Reichart", "A. Korhonen"], "venue": "Comput. Linguist. 41(4), 665\u2013695", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Improved automatic keyword extraction given more linguistic knowledge", "author": ["A. Hulth"], "venue": "Proceedings of the 2003 conference on Empirical methods in natural language processing, pp. 216\u2013223. Association for Computational Linguistics", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2003}, {"title": "Evaluating verbose query processing techniques", "author": ["S. Huston", "W.B. Croft"], "venue": "Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pp. 291\u2013298. ACM", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Overview of the 2nd computational linguistics scientific document summarization shared task (cl-scisumm 2016)", "author": ["K. Jaidka", "M.K. Chandrasekaran", "S. Rustagi", "M.Y. Kan"], "venue": "Proceedings of the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2016)", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Surveyor: A system for generating coherent survey articles for scientific topics", "author": ["R. Jha", "R. Coke", "D. Radev"], "venue": "Ann Arbor 1001, 48,109", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "A simple enhancement for ad-hoc information retrieval via topic modelling", "author": ["F. Jian", "J.X. Huang", "J. Zhao", "T. He", "P. Hu"], "venue": "SIGIR, pp. 733\u2013736. ACM", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}, {"title": "A probabilistic model of information retrieval: development and comparative experiments: Part 2", "author": ["K.S. Jones", "S. Walker", "S.E. Robertson"], "venue": "Information Processing & Management 36(6), 809\u2013840", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2000}, {"title": "Citation classification for behavioral analysis of a scientific field", "author": ["D. Jurgens", "S. Kumar", "R. Hoover", "D. McFarland", "D. Jurafsky"], "venue": "CoRR", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Utilizing context in generative bayesian models for linked corpus", "author": ["S. Kataria", "P. Mitra", "S. Bhatia"], "venue": "AAAI, vol. 10, p. 1", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Identifying referenced text in scientific publications by summarisation and classification techniques", "author": ["S. Klampfl", "A. Rexha", "R. Kern"], "venue": "BIRNDL@ JCDL, pp. 122\u2013131", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Q. Le", "T. Mikolov"], "venue": "ICML, pp. 1188\u20131196", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Cist system for cl-scisumm 2016 shared  task", "author": ["L. Li", "L. Mao", "Y. Zhang", "J. Chi", "T. Huang", "X. Cong", "H. Peng"], "venue": "BIRNDL 2016 Joint Workshop on Bibliometricenhanced Information Retrieval and NLP for Digital Libraries", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2016}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["C.Y. Lin"], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pp. 74\u201381", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2004}, {"title": "Putting the user in the loop: interactive maximal marginal relevance for queryfocused summarization", "author": ["J. Lin", "N. Madnani", "B.J. Dorr"], "venue": "NAACL-HLT, pp. 305\u2013308. Association for Computational Linguistics", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2010}, {"title": "Medical subject headings (mesh)", "author": ["C.E. Lipscomb"], "venue": "Bulletin of the Medical Library Association 88(3), 265", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2000}, {"title": "Textrank: Bringing order into texts", "author": ["R. Mihalcea", "P. Tarau"], "venue": "Association for Computational Linguistics", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2004}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS, pp. 3111\u20133119", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["G.A. Miller"], "venue": "Communications of the ACM 38(11), 39\u201341", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1995}, {"title": "University of houston at cl-scisumm 2016: Svms with tree kernels and sentence similarity", "author": ["L. Moraes", "S. Baki", "R. Verma", "D. Lee"], "venue": "BIRNDL@ JCDL, pp. 113\u2013121", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2016}, {"title": "Counter-fitting word vectors to linguistic constraints", "author": ["N. Mrk\u0161i\u0107", "D.\u00d3. S\u00e9aghdha", "B. Thomson", "M. Ga\u0161i\u0107", "L. Rojas-Barahona", "P.H. Su", "D. Vandyke", "T.H. Wen", "S. Young"], "venue": "NAACL-HLT", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2016}, {"title": "Citances: Citation sentences for semantic analysis of bioscience text", "author": ["P.I. Nakov", "A.S. Schwartz", "M. Hearst"], "venue": "Proceedings of the SIGIR\u201904 workshop on Search and Discovery in Bioinformatics, pp. 81\u201388", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2004}, {"title": "Neal: A neurally enhanced approach to linking citation and reference", "author": ["T. Nomoto"], "venue": "BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2016}, {"title": "Using maximum entropy for sentence extraction", "author": ["M. Osborne"], "venue": "Proceedings of the ACL-02 Workshop on Automatic Summarization-Volume 4, pp. 1\u20138. Association for Computational Linguistics", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2002}, {"title": "The pagerank citation ranking: Bringing order to the web", "author": ["L. Page", "S. Brin", "R. Motwani", "T. Winograd"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1999}, {"title": "Summarizing contrastive viewpoints in opinionated text", "author": ["M. Paul", "C. Zhai", "R. Girju"], "venue": "EMNLP, pp. 66\u201376. Association for Computational Linguistics", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2010}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP 12, 1532\u20131543", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2014}, {"title": "A language modeling approach to information retrieval", "author": ["J.M. Ponte", "W.B. Croft"], "venue": "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pp. 275\u2013281. ACM", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1998}, {"title": "Generating Extractive Summaries of Scientific Paradigms", "author": ["V. Qazvinian", "D. Radev", "S. Mohammad"], "venue": "J. Artif. Intell. Res.( . . .46, 165\u2013201", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2013}, {"title": "Scientific paper summarization using citation summary networks", "author": ["V. Qazvinian", "D.R. Radev"], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pp. 689\u2013696. Association for Computational Linguistics", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2008}, {"title": "Identifying non-explicit citing sentences for citation-based summarization", "author": ["V. Qazvinian", "D.R. Radev"], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pp. 555\u2013564. Association for Computational Linguistics", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2010}, {"title": "Generating extractive summaries of scientific paradigms", "author": ["V. Qazvinian", "D.R. Radev", "S.M. Mohammad", "B. Dorr", "D. Zajic", "M. Whidby", "T. Moon"], "venue": "J. Artif. Int. Res. 46(1), 165\u2013201", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2013}, {"title": "The probabilistic relevance framework: BM25 and beyond", "author": ["S. Robertson", "H. Zaragoza"], "venue": "Now Publishers Inc", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2009}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["A.M. Rush", "S. Chopra", "J. Weston"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 379\u2013389. Association for Computational Linguistics, Lisbon, Portugal", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2015}, {"title": "Trainable citation-enhanced summarization of scientific articles", "author": ["H. Saggion", "A. AbuRaed", "F. Ronzano"], "venue": "Cabanac G, Chandrasekaran MK, Frommholz I, Jaidka K, Kan M, Mayr P, Wolfram D, editors. Proceedings of the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL); 2016 June 23; Newark, United States. CEUR Workshop Proceedings:[Sl]; 2016. p. 17586. CEUR Workshop Proceedings", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2016}, {"title": "Systematized nomenclature of medicineclinical terms", "author": ["C. Snomed"], "venue": "International Health Terminology Standards Development Organisation", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2011}, {"title": "A statistical interpretation of term specificity and its application in retrieval", "author": ["K. Sparck Jones"], "venue": "Journal of doc umentation 28(1), 11\u201321", "citeRegEx": "73", "shortCiteRegEx": null, "year": 1972}, {"title": "Using latent semantic analysis in text summarization and summary evaluation", "author": ["J. Steinberger", "K. Jezek"], "venue": "Proc. ISIM04, pp. 93\u2013100", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2004}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, pp. 3104\u20133112", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2014}, {"title": "Summarizing scientific articles: Experiments with relevance and rhetorical status", "author": ["S. Teufel", "M. Moens"], "venue": "Comput. Linguist. 28(4), 409\u2013445", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2002}, {"title": "Automatic classification of citation function", "author": ["S. Teufel", "A. Siddharthan", "D. Tidhar"], "venue": "EMNLP \u201906 p. 103", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2006}, {"title": "Beyond sumbasic: Task-focused summarization with sentence simplification and lexical expansion", "author": ["L. Vanderwende", "H. Suzuki", "C. Brockett", "A. Nenkova"], "venue": "Information Processing & Management 43(6), 1606\u20131618", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2007}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["S. Wang", "C.D. Manning"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pp. 90\u201394. Association for Computational Linguistics", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2012}, {"title": "A study of smoothing methods for language models applied to information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "ACM Transactions on Information Systems (TOIS) 22(2), 179\u2013 214", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 10, "context": "According to a recent study by bibliometric analysts, the global scientific output doubles approximately every nine years [11], further signifying this challenge.", "startOffset": 122, "endOffset": 126}, {"referenceID": 27, "context": "While abstracts provide an overview of the paper, they do not necessarily convey all the important contributions and impacts of the paper [28]: (i) The authors might ascribe contributions to their papers that are not existent.", "startOffset": 138, "endOffset": 142}, {"referenceID": 3, "context": "(v) The content distribution in the abstracts are not evenly drawn from different sections of the papers [4].", "startOffset": 105, "endOffset": 108}, {"referenceID": 65, "context": "These problems have inspired another type of scientific summaries which are obtained by utilizing a set of citations referencing the original paper [66,68].", "startOffset": 148, "endOffset": 155}, {"referenceID": 67, "context": "These problems have inspired another type of scientific summaries which are obtained by utilizing a set of citations referencing the original paper [66,68].", "startOffset": 148, "endOffset": 155}, {"referenceID": 57, "context": "This short description is called citation text or citance [58].", "startOffset": 58, "endOffset": 62}, {"referenceID": 25, "context": "1 Example of epistemic value drift [26].", "startOffset": 35, "endOffset": 39}, {"referenceID": 25, "context": "Another serious problem is the modification of the epistemic value of claims, which states that many claims by the original author might be stated as facts in the future citations [26].", "startOffset": 180, "endOffset": 184}, {"referenceID": 76, "context": "The rhetorical status of a citation provides additional useful information that can be used in applications such as information extraction, retrieval, and summarization [77].", "startOffset": 169, "endOffset": 173}, {"referenceID": 40, "context": "Scientific summarization is recently further motivated by TAC 2014 summarization track, and the 2016 computation linguistics summarization shared task [41].", "startOffset": 151, "endOffset": 155}, {"referenceID": 6, "context": "We propose to address this challenge by query reformulations, utilizing word embeddings [7], and domain-specific knowledge.", "startOffset": 88, "endOffset": 91}, {"referenceID": 64, "context": "Our approach extends the previous works on citation-based summarization [65, 66,67] by including the reference context to address the inaccuracy problem associated with the citation texts.", "startOffset": 72, "endOffset": 83}, {"referenceID": 65, "context": "Our approach extends the previous works on citation-based summarization [65, 66,67] by including the reference context to address the inaccuracy problem associated with the citation texts.", "startOffset": 72, "endOffset": 83}, {"referenceID": 66, "context": "Our approach extends the previous works on citation-based summarization [65, 66,67] by including the reference context to address the inaccuracy problem associated with the citation texts.", "startOffset": 72, "endOffset": 83}, {"referenceID": 35, "context": "They help disseminate the new findings and they allow new works to be grounded on previous efforts [36].", "startOffset": 99, "endOffset": 103}, {"referenceID": 45, "context": "Examples of the proposed approaches for extracting the citation texts include jointly modeling the link information and the citation texts [46], supervised Markov Random Fields classifiers [67], and sequence labeling with segment classification [3].", "startOffset": 139, "endOffset": 143}, {"referenceID": 66, "context": "Examples of the proposed approaches for extracting the citation texts include jointly modeling the link information and the citation texts [46], supervised Markov Random Fields classifiers [67], and sequence labeling with segment classification [3].", "startOffset": 189, "endOffset": 193}, {"referenceID": 2, "context": "Examples of the proposed approaches for extracting the citation texts include jointly modeling the link information and the citation texts [46], supervised Markov Random Fields classifiers [67], and sequence labeling with segment classification [3].", "startOffset": 245, "endOffset": 248}, {"referenceID": 76, "context": "There exists some related work on further analyzing the citations for finding their function or rhetorical status [77,32,1,36].", "startOffset": 114, "endOffset": 126}, {"referenceID": 31, "context": "There exists some related work on further analyzing the citations for finding their function or rhetorical status [77,32,1,36].", "startOffset": 114, "endOffset": 126}, {"referenceID": 0, "context": "There exists some related work on further analyzing the citations for finding their function or rhetorical status [77,32,1,36].", "startOffset": 114, "endOffset": 126}, {"referenceID": 35, "context": "There exists some related work on further analyzing the citations for finding their function or rhetorical status [77,32,1,36].", "startOffset": 114, "endOffset": 126}, {"referenceID": 8, "context": "The distribution of citations within the structure of scientific papers have been also studied [9].", "startOffset": 95, "endOffset": 98}, {"referenceID": 16, "context": "The authors of [17] have investigated the problem of measuring the intensity of the citations in scientific papers and in [16], the authors proposed using the discourse facets for scientific article recommendation.", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "The authors of [17] have investigated the problem of measuring the intensity of the citations in scientific papers and in [16], the authors proposed using the discourse facets for scientific article recommendation.", "startOffset": 122, "endOffset": 126}, {"referenceID": 44, "context": "Recently, a framework for understanding citation function has been proposed [45] which unifies all the previous efforts in terms of definition of citation functions.", "startOffset": 76, "endOffset": 80}, {"referenceID": 40, "context": "In particular, TAC 2014 summarization track, and the CL-SciSumm 2016 shared task on computational linguistic summarization [41] have released datasets to promote research for citation contextualization.", "startOffset": 123, "endOffset": 127}, {"referenceID": 12, "context": "The authors of [13] used an SVM-rank approach with features such as tf-idf cosine similarity, position of the reference sentence, section position, and named entity features.", "startOffset": 15, "endOffset": 19}, {"referenceID": 48, "context": "In another approach [49], the authors used an SVM classifier with sentence similarity and lexicon based features.", "startOffset": 20, "endOffset": 24}, {"referenceID": 58, "context": "The authors of [59] proposed a hybrid model based on tf-idf similarity and a single layer neural network that scores the relevant reference texts above the irrelevant ones.", "startOffset": 15, "endOffset": 19}, {"referenceID": 46, "context": "Finally, in the work by [47], the authors proposed the use of TextSentenceRank algorithm which is an enhanced version of the TextRank algorithm for ranking keywords in the documents.", "startOffset": 24, "endOffset": 28}, {"referenceID": 32, "context": "Approaches towards summarization can be divided into the following categories: (i) topic modeling based [33,74,78,15]: In these approaches, the content or topical distribution of the final summary is estimated using a probabilistic framework.", "startOffset": 104, "endOffset": 117}, {"referenceID": 73, "context": "Approaches towards summarization can be divided into the following categories: (i) topic modeling based [33,74,78,15]: In these approaches, the content or topical distribution of the final summary is estimated using a probabilistic framework.", "startOffset": 104, "endOffset": 117}, {"referenceID": 77, "context": "Approaches towards summarization can be divided into the following categories: (i) topic modeling based [33,74,78,15]: In these approaches, the content or topical distribution of the final summary is estimated using a probabilistic framework.", "startOffset": 104, "endOffset": 117}, {"referenceID": 14, "context": "Approaches towards summarization can be divided into the following categories: (i) topic modeling based [33,74,78,15]: In these approaches, the content or topical distribution of the final summary is estimated using a probabilistic framework.", "startOffset": 104, "endOffset": 117}, {"referenceID": 19, "context": "(ii) solving an optimization problem [20,8,27]: these approaches cast the summarization problem as", "startOffset": 37, "endOffset": 46}, {"referenceID": 7, "context": "(ii) solving an optimization problem [20,8,27]: these approaches cast the summarization problem as", "startOffset": 37, "endOffset": 46}, {"referenceID": 26, "context": "(ii) solving an optimization problem [20,8,27]: these approaches cast the summarization problem as", "startOffset": 37, "endOffset": 46}, {"referenceID": 59, "context": "(iii) supervised models [60,25,18], where selection of sentences in the summary are learned using a supervised framework.", "startOffset": 24, "endOffset": 34}, {"referenceID": 24, "context": "(iii) supervised models [60,25,18], where selection of sentences in the summary are learned using a supervised framework.", "startOffset": 24, "endOffset": 34}, {"referenceID": 17, "context": "(iii) supervised models [60,25,18], where selection of sentences in the summary are learned using a supervised framework.", "startOffset": 24, "endOffset": 34}, {"referenceID": 28, "context": "(iv) graph based [29,53,62]: these approaches seek to find the most central sentences in a document\u2019s graph where sentences are nodes and edges are similarities.", "startOffset": 17, "endOffset": 27}, {"referenceID": 52, "context": "(iv) graph based [29,53,62]: these approaches seek to find the most central sentences in a document\u2019s graph where sentences are nodes and edges are similarities.", "startOffset": 17, "endOffset": 27}, {"referenceID": 61, "context": "(iv) graph based [29,53,62]: these approaches seek to find the most central sentences in a document\u2019s graph where sentences are nodes and edges are similarities.", "startOffset": 17, "endOffset": 27}, {"referenceID": 13, "context": "(v) Heuristic based [14,34,51]: these works approach the summarization problem by greedy selection of the content.", "startOffset": 20, "endOffset": 30}, {"referenceID": 33, "context": "(v) Heuristic based [14,34,51]: these works approach the summarization problem by greedy selection of the content.", "startOffset": 20, "endOffset": 30}, {"referenceID": 50, "context": "(v) Heuristic based [14,34,51]: these works approach the summarization problem by greedy selection of the content.", "startOffset": 20, "endOffset": 30}, {"referenceID": 74, "context": "(vi) Neural networks: More recently, there has been some efforts on utilizing neural networks and sequence-to-sequence models [75] for generating summaries of short texts and sentences [70,19].", "startOffset": 126, "endOffset": 130}, {"referenceID": 69, "context": "(vi) Neural networks: More recently, there has been some efforts on utilizing neural networks and sequence-to-sequence models [75] for generating summaries of short texts and sentences [70,19].", "startOffset": 185, "endOffset": 192}, {"referenceID": 18, "context": "(vi) Neural networks: More recently, there has been some efforts on utilizing neural networks and sequence-to-sequence models [75] for generating summaries of short texts and sentences [70,19].", "startOffset": 185, "endOffset": 192}, {"referenceID": 75, "context": "Scientific articles are much different than news articles in elements such as length, language, complexity and structure [76].", "startOffset": 121, "endOffset": 125}, {"referenceID": 75, "context": "One of the first works in scientific article summarization is done by [76] where the authors trained a supervised Naive Bayes classifier to select informative content for the summary.", "startOffset": 70, "endOffset": 74}, {"referenceID": 27, "context": "Later, the impact of citations to generate scientific summaries was realized [28].", "startOffset": 77, "endOffset": 81}, {"referenceID": 64, "context": "In the work by [65], the authors proposed an approach for citation-based summarization based on a clustering approach, while in [2] and [42], the focused on producing coherent scientific summaries.", "startOffset": 15, "endOffset": 19}, {"referenceID": 1, "context": "In the work by [65], the authors proposed an approach for citation-based summarization based on a clustering approach, while in [2] and [42], the focused on producing coherent scientific summaries.", "startOffset": 128, "endOffset": 131}, {"referenceID": 41, "context": "In the work by [65], the authors proposed an approach for citation-based summarization based on a clustering approach, while in [2] and [42], the focused on producing coherent scientific summaries.", "startOffset": 136, "endOffset": 140}, {"referenceID": 22, "context": "We present a comprehensive framework for scientific summarization which utilizes and builds upon our earlier efforts [23,21,22].", "startOffset": 117, "endOffset": 127}, {"referenceID": 20, "context": "We present a comprehensive framework for scientific summarization which utilizes and builds upon our earlier efforts [23,21,22].", "startOffset": 117, "endOffset": 127}, {"referenceID": 21, "context": "We present a comprehensive framework for scientific summarization which utilizes and builds upon our earlier efforts [23,21,22].", "startOffset": 117, "endOffset": 127}, {"referenceID": 4, "context": "Previous works have shown that noun phrases are good representation of informative concepts in the query [5,40,39].", "startOffset": 105, "endOffset": 114}, {"referenceID": 39, "context": "Previous works have shown that noun phrases are good representation of informative concepts in the query [5,40,39].", "startOffset": 105, "endOffset": 114}, {"referenceID": 38, "context": "Previous works have shown that noun phrases are good representation of informative concepts in the query [5,40,39].", "startOffset": 105, "endOffset": 114}, {"referenceID": 72, "context": "We use the Inverted Document Frequency (IDF) [73] measure to find the key concepts.", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "Since the TAC dataset is in the biomedical domain, we use the UMLS [10] thesaurus which is a comprehensive ontology of biomedical concepts.", "startOffset": 67, "endOffset": 71}, {"referenceID": 71, "context": "We specifically use the SNOMED CT [72] subset of UMLS.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "We also evaluated other query reformulation methods such as Pseudo Relevance Feedback [12]; however, they performed worse than the baseline and thus we do not discuss them further.", "startOffset": 86, "endOffset": 90}, {"referenceID": 5, "context": "Word embeddings or distributed representations of words are mapping of words to dense vectors according to a distributional space, with the goal that similar words will be located close to each other [6].", "startOffset": 200, "endOffset": 203}, {"referenceID": 63, "context": "We extend the Language Modeling (LM) for information retrieval model [64] by utilizing word embeddings to account for terminology variations.", "startOffset": 69, "endOffset": 73}, {"referenceID": 79, "context": "In LM with Dirichlet Smoothing [80], p(qi|d) is calculated using a smoothed maximum likelihood estimate:", "startOffset": 31, "endOffset": 35}, {"referenceID": 53, "context": "While high values of this product suggest syntactic and semantic relatedness between the two terms [54,63,38],", "startOffset": 99, "endOffset": 109}, {"referenceID": 62, "context": "While high values of this product suggest syntactic and semantic relatedness between the two terms [54,63,38],", "startOffset": 99, "endOffset": 109}, {"referenceID": 37, "context": "While high values of this product suggest syntactic and semantic relatedness between the two terms [54,63,38],", "startOffset": 99, "endOffset": 109}, {"referenceID": 47, "context": "While any approach for training the word embeddings could be used, we use the Word2Vec [48] method, which has proven effective in several word similarity tasks.", "startOffset": 87, "endOffset": 91}, {"referenceID": 36, "context": "Since the TAC dataset is in biomedical domain, we also train embeddings on a domain-specific collection; we use the TREC Genomics collections, 2004 and 2006 [37] which together consist of 1.", "startOffset": 157, "endOffset": 161}, {"referenceID": 34, "context": "They are based on the distributional hypothesis [35] which states that similar words appear in similar contexts.", "startOffset": 48, "endOffset": 52}, {"referenceID": 56, "context": "While these models have been very successful in capturing semantic relatedness, recent related works have shown that domain ontologies and expert curated lexicons may contain information that are not captured by embeddings [57,38,31]; hence, we account for the domain knowledge according to the following.", "startOffset": 223, "endOffset": 233}, {"referenceID": 37, "context": "While these models have been very successful in capturing semantic relatedness, recent related works have shown that domain ontologies and expert curated lexicons may contain information that are not captured by embeddings [57,38,31]; hence, we account for the domain knowledge according to the following.", "startOffset": 223, "endOffset": 233}, {"referenceID": 30, "context": "While these models have been very successful in capturing semantic relatedness, recent related works have shown that domain ontologies and expert curated lexicons may contain information that are not captured by embeddings [57,38,31]; hence, we account for the domain knowledge according to the following.", "startOffset": 223, "endOffset": 233}, {"referenceID": 30, "context": "\u2013 Retrofitting embeddings: In this method, we apply a post-processing step called retrofitting [31] to the word embeddings used in the model.", "startOffset": 95, "endOffset": 99}, {"referenceID": 51, "context": "For the ontology, since TAC data is in biomedical domain we use two domain-specific ontologies, Mesh [52] and Protein Ontology (PRO).", "startOffset": 101, "endOffset": 105}, {"referenceID": 54, "context": "For the CL-SciSum data, since it is less domain-specific, we use the WordNet lexicon [55].", "startOffset": 85, "endOffset": 89}, {"referenceID": 68, "context": "\u2013 BM25 similarity score [69] between the citation text and the candidate reference.", "startOffset": 24, "endOffset": 28}, {"referenceID": 78, "context": "We train the model using an SVM classifier [79].", "startOffset": 43, "endOffset": 47}, {"referenceID": 29, "context": "We then apply the \u201cpower method\u201d [30] which is an algorithm similar to the PageRank random walk ranking model [61], that finds the most central nodes in a graph.", "startOffset": 33, "endOffset": 37}, {"referenceID": 60, "context": "We then apply the \u201cpower method\u201d [30] which is an algorithm similar to the PageRank random walk ranking model [61], that finds the most central nodes in a graph.", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "Maximal Marginal Relevance [14] is one such heuristic that has these properties.", "startOffset": 27, "endOffset": 31}, {"referenceID": 40, "context": "The second dataset is the 2016 CL-SciSumm dataset [41] which is available on a public repository and contains scientific articles from the", "startOffset": 50, "endOffset": 54}, {"referenceID": 68, "context": "BM25 [69] 19.", "startOffset": 5, "endOffset": 9}, {"referenceID": 79, "context": "LMD [80] 21.", "startOffset": 4, "endOffset": 8}, {"referenceID": 42, "context": "LMD + LDA [43] 22.", "startOffset": 10, "endOffset": 14}, {"referenceID": 49, "context": "To consider textual similarities of the retrieved context with the gold standard, we also compute Rouge-N scores [50].", "startOffset": 113, "endOffset": 117}, {"referenceID": 43, "context": "BM25 scoring model [44] which is a probabilistic framework for ranking the relevant documents based on the query terms appearing in each document, regardless of their relative proximity.", "startOffset": 19, "endOffset": 23}, {"referenceID": 79, "context": "Language modeling with Dirichlet smoothing (LMD) [80] is a probabilistic framework that models the probability of documents generating the given query.", "startOffset": 49, "endOffset": 53}, {"referenceID": 42, "context": "An extension of the LMD retrieval model using Latent Dirichlet Allocation (LDA) which is recently proposed [43].", "startOffset": 107, "endOffset": 111}, {"referenceID": 68, "context": "BM25 [69] 8.", "startOffset": 5, "endOffset": 9}, {"referenceID": 79, "context": "LM [80] 7.", "startOffset": 3, "endOffset": 7}, {"referenceID": 46, "context": "TSR [47] 5.", "startOffset": 4, "endOffset": 8}, {"referenceID": 58, "context": "Tf-idf + Neural Net [59] 9.", "startOffset": 20, "endOffset": 24}, {"referenceID": 12, "context": "SVM Rank [13] 8.", "startOffset": 9, "endOffset": 13}, {"referenceID": 48, "context": "Jaccard Fusion [49] 8.", "startOffset": 15, "endOffset": 19}, {"referenceID": 55, "context": "Tf-idf+stem [56] 9.", "startOffset": 12, "endOffset": 16}], "year": 2017, "abstractText": "The rapid growth of scientific literature has made it difficult for the researchers to quickly learn about the developments in their respective fields. Scientific document summarization addresses this challenge by providing summaries of the important contributions of scientific papers. We present a framework for scientific summarization which takes advantage of the citations and the scientific discourse structure. Citation texts often lack the evidence and context to support the content of the cited paper and are even sometimes inaccurate. We first address the problem of inaccuracy of the citation texts by finding the relevant context from the cited paper. We propose three approaches for contextualizing citations which are based on query reformulation, word embeddings, and supervised learning. We then train a model to identify the discourse facets for each citation. We finally propose a method for summarizing scientific papers by leveraging the faceted citations and their corresponding contexts. We evaluate our proposed method on two scientific summarization datasets in the biomedical and computational linguistics domains. Extensive evaluation results show that our methods can improve over the state of the art by large margins. \u2217 This is a pre-print of an article published on IJDL. The final publication is available at Springer via http://dx.doi.org/10.1007/s00799-017-0216-8 Arman Cohan E-mail: arman@ir.cs.georgetown.edu Nazli Goharian E-mail: nazli@ir.cs.georgetown.edu 1 Information Retrieval Lab, Department of Computer Science, Georgetown University, Washington DC, USA", "creator": "LaTeX with hyperref package"}}}