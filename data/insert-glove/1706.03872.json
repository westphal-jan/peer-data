{"id": "1706.03872", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Six Challenges for Neural Machine Translation", "abstract": "We potsie explore qarmatians six challenges for mushota neural socialdemokraterne machine -50 translation: miyoshi domain mismatch, amount dehsh of dharamsala training data, percept rare diapsid words, tumanyan long rebounder sentences, favelas word alignment, and beam monteleone search. best We manikandan show oblivians both deficiencies accusers and thuong improvements escobar over rg the kittin quality of phrase - based statistical machine danzig translation.", "histories": [["v1", "Mon, 12 Jun 2017 23:57:48 GMT  (39kb)", "http://arxiv.org/abs/1706.03872v1", "12 pages; First Workshop on Neural Machine Translation, 2017"]], "COMMENTS": "12 pages; First Workshop on Neural Machine Translation, 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["philipp koehn", "rebecca knowles"], "accepted": false, "id": "1706.03872"}, "pdf": {"name": "1706.03872.pdf", "metadata": {"source": "CRF", "title": "Six Challenges for Neural Machine Translation", "authors": ["Philipp Koehn", "Rebecca Knowles"], "emails": ["phi@jhu.edu", "rknowles@jhu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n03 87\n2v 1\n[ cs\n.C L\n] 1\n2 Ju\nn 20\n17\nWe explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrasebased statistical machine translation."}, {"heading": "1 Introduction", "text": "Neural machine translation has emerged as the most promising machine translation approach in recent years, showing superior performance on public benchmarks (Bojar et al., 2016) and rapid adoption in deployments by, e.g., Google (Wu et al., 2016), Systran (Crego et al., 2016), and WIPO (Junczys-Dowmunt et al., 2016). But there have also been reports of poor performance, such as the systems built under low-resource conditions in the DARPA LORELEI program.1\nIn this paper, we examine a number of challenges to neural machine translation (NMT) and give empirical results on how well the technology currently holds up, compared to traditional statistical machine translation (SMT).\nWe find that:\n1. NMT systems have lower quality out of do-\nmain, to the point that they completely sacrifice adequacy for the sake of fluency.\n2. NMT systems have a steeper learning curve\nwith respect to the amount of training data, resulting in worse quality in low-resource settings, but better performance in highresource settings.\n1https://www.nist.gov/itl/iad/mig/lorehlt16-\nevaluations\n3. NMT systems that operate at the sub-word\nlevel (e.g. with byte-pair encoding) perform better than SMT systems on extremely lowfrequency words, but still show weakness in translating low-frequency words belonging to highly-inflected categories (e.g. verbs).\n4. NMT systems have lower translation quality\non very long sentences, but do comparably better up to a sentence length of about 60 words.\n5. The attention model for NMT does not al-\nways fulfill the role of a word alignment model, but may in fact dramatically diverge.\n6. Beam search decoding only improves trans-\nlation quality for narrow beams and deteriorates when exposed to a larger search space.\nWe note a 7th challenge that we do not examine empirically: NMT systems are much less interpretable. The answer to the question of why the training data leads these systems to decide on specific word choices during decoding is buried in large matrices of real-numbered values. There is a clear need to develop better analytics for NMT.\nOther studies have looked at the comparable performance of NMT and SMT systems. Bentivogli et al. (2016) considered different linguistic categories for English\u2013German and Toral and Sa\u0301nchez-Cartagena (2017) compared different broad aspects such as fluency and reordering for nine language directions."}, {"heading": "2 Experimental Setup", "text": "We use common toolkits for neural machine translation (Nematus) and traditional phrase-based statistical machine translation (Moses) with common data sets, drawn from WMT and OPUS."}, {"heading": "2.1 Neural Machine Translation", "text": "While a variety of neural machine translation approaches were initially proposed \u2014 such as the use of convolutional neural networks (Kalchbrenner and Blunsom, 2013) \u2014 practically all recent work has been focused on the attentionbased encoder-decoder model (Bahdanau et al., 2015).\nWe use the toolkit Nematus2 (Sennrich et al., 2017) which has been shown to give state-of-theart results (Sennrich et al., 2016a) at the WMT 2016 evaluation campaign (Bojar et al., 2016).\nUnless noted otherwise, we use default settings, such as beam search and single model decoding. The training data is processed with byte-pair encoding (Sennrich et al., 2016b) into subwords to fit a 50,000 word vocabulary limit."}, {"heading": "2.2 Statistical Machine Translation", "text": "Our machine translation systems are trained using Moses3 (Koehn et al., 2007). We build phrasebased systems using standard features that are commonly used in recent system submissions to WMT (Williams et al., 2016; Ding et al., 2016a).\nWhile we use the shorthand SMT for these phrase-based systems, we note that there are other statistical machine translation approaches such as hierarchical phrase-based models (Chiang, 2007) and syntax-based models (Galley et al., 2004, 2006) that have been shown to give superior performance for language pairs such as Chinese\u2013 English and German\u2013English."}, {"heading": "2.3 Data Conditions", "text": "We carry out our experiments on English\u2013Spanish and German\u2013English. For these language pairs, large training data sets are available. We use datasets from the shared translation task organized alongside the Conference on Machine Translation (WMT)4. For the domain experiments, we use the OPUS corpus5 (Tiedemann, 2012).\nExcept for the domain experiments, we use the WMT test sets composed of news stories, which are characterized by a broad range of topic, formal language, relatively long sentences (about 30 words on average), and high standards for grammar, orthography, and style.\n2https://github.com/rsennrich/nematus/ 3http://www.stat.org/moses/ 4http://www.statmt.org/wmt17/ 5http://opus.lingfil.uu.se/"}, {"heading": "3 Challenges", "text": ""}, {"heading": "3.1 Domain Mismatch", "text": "A known challenge in translation is that in different domains,6 words have different translations and meaning is expressed in different styles. Hence, a crucial step in developing machine translation systems targeted at a specific use case is domain adaptation. We expect that methods for domain adaptation will be developed for NMT. A currently popular approach is to train a general domain system, followed by training on in-domain data for a few epochs (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016).\nOften, large amounts of training data are only available out of domain, but we still seek to have robust performance. To test how well NMT and SMT hold up, we trained five different systems using different corpora obtained from OPUS (Tiedemann, 2012). An additional system was trained on all the training data. Statistics about corpus sizes are shown in Table 1. Note that these domains are quite distant from each other, much more so than, say, Europarl, TED Talks, News Commentary, and Global Voices.\nWe trained both SMT and NMT systems for all domains. All systems were trained for GermanEnglish, with tuning and test sets sub-sampled from the data (these were not used in training). A common byte-pair encoding is used for all training runs.\nSee Figure 1 for results. While the in-domain NMT and SMT systems are similar (NMT is better for IT and Subtitles, SMT is better for Law, Medical, and Koran), the out-of-domain performance for the NMT systems is worse in almost all cases, sometimes dramatically so. For instance the Med-\n6We use the customary definition of domain in machine translation: a domain is defined by a corpus from a specific source, and may differ from other domains in topic, genre, style, level of formality, etc.\nical system leads to a BLEU score of 3.9 (NMT) vs. 10.2 (SMT) on the Law test set.\nFigure 2 displays an example. When translating the sentence Schaue um dich herum. (reference: Look around you.) from the Subtitles corpus, we see mostly non-sensical and completely unrelated output from the NMT system. For instance, the translation from the IT system is Switches to paused.\nNote that the output of the NMT system is often quite fluent (e.g., Take heed of your own souls.) but completely unrelated to the input, while the SMT output betrays its difficulties with coping with the out-of-domain input by leaving some words untranslated (e.g., Schaue by dich around.). This is of particular concern when MT is used for information gisting \u2014 the user will be mislead by hallucinated content in the NMT output."}, {"heading": "3.2 Amount of Training Data", "text": "A well-known property of statistical systems is that increasing amounts of training data lead to better results. In SMT systems, we have previously observed that doubling the amount of training data gives a fixed increase in BLEU scores. This holds true for both parallel and monolingual data (Turchi et al., 2008; Irvine and Callison-Burch, 2013).\nBLEU Scores with Varying Amounts of Training Data\nHow do the data needs of SMT and NMT compare? NMT promises both to generalize better (exploiting word similary in embeddings) and condition on larger context (entire input and all prior output words).\nWe built English-Spanish systems on WMT data,7 about 385.7 million English words paired with Spanish. To obtain a learning curve, we used 1 1024 , 1 512 , ..., 1 2 , and all of the data. For SMT, the language model was trained on the Spanish part of each subset, respectively. In addition to a NMT and SMT system trained on each subset, we also used all additionally provided monolingual data for a big language model in contrastive SMT systems.\nResults are shown in Figure 3. NMT exhibits a much steeper learning curve, starting with abysmal results (BLEU score of 1.6 vs. 16.4 for 1 1024 of the data), outperforming SMT 25.7 vs. 24.7 with 1 16 of the data (24.1 million words), and even beating the SMT system with a big language model with the full data set (31.1 for NMT, 28.4 for SMT, 30.4 for SMT+BigLM).\n7Spanish was last represented in 2013, we used data from http://statmt.org/wmt13/translation-task.html\nThe contrast between the NMT and SMT learning curves is quite striking. While NMT is able to exploit increasing amounts of training data more effectively, it is unable to get off the ground with training corpus sizes of a few million words or less.\nTo illustrate this, see Figure 4. With 1 1024 of the training data, the output is completely unrelated to the input, some key words are properly translated with 1 512 and 1 256 of the data (estrategia for strategy, eleccio\u0301n or elecciones for election), and starting with 1 64 the translations become respectable."}, {"heading": "3.3 Rare Words", "text": "Conventional wisdom states that neural machine translation models perform particularly poorly on rare words, (Luong et al., 2015; Sennrich et al., 2016b; Arthur et al., 2016) due in part to the smaller vocabularies used by NMT systems. We examine this claim by comparing performance on rare word translation between NMT and SMT systems of similar quality for German\u2013English and find that NMT systems actually outperform SMT systems on translation of very infrequent words. However, both NMT and SMT systems do continue to have difficulty translating some infrequent words, particularly those belonging to highly-inflected categories.\nFor the neural machine translation model, we use a publicly available model8 with the training settings of Edinburgh\u2019s WMT submission (Sennrich et al., 2016a). This was trained using\n8https://github.com/rsennrich/wmt16-scripts/\nNematus9 (Sennrich et al., 2017), with byte-pair encodings (Sennrich et al., 2016b) to allow for open-vocabulary NMT.\nThe phrase-based model that we used was trained using Moses (Koehn et al., 2007), and the training data and parameters match those described in Johns Hopkins University\u2019s submission to the WMT shared task (Ding et al., 2016b).\nBoth models have case-sensitive BLEU scores of 34.5 on the WMT 2016 news test set (for the NMT model, this reflects the BLEU score resulting from translation with a beam size of 1). We use a single corpus for computing our lexical frequency counts (a concatenation of Common Crawl, Europarl, and News Commentary).\nWe follow the approach described by Koehn and Haddow (2012) for examining the effect of source word frequency on translation accuracy.10\n9https://github.com/rsennrich/nematus/ 10First, we automatically align the source sentence and the machine translation output. We use fast-align (Dyer et al., 2013) to align the full training corpus (source and reference) along with the test source and MT output. We use the suggested standard options for alignment and then symmetrize the alignment with grow-diag-final-and.\nEach source word is either unaligned (\u201cdropped\u201d) or aligned to one or more target language words. For each target word to which the source word is aligned, we check if that target word appears in the reference translation. If the target word appears the same number of times in the MT output as in the reference, we award that alignment a score of one. If the target word appears more times in the MT output\nThe overall average precision is quite similar between the NMT and SMT systems, with the SMT system scoring 70.1% overall and the NMT system scoring 70.3%. This reflects the similar overall quality of the MT systems. Figure 5 gives a detailed breakdown. The values above the horizontal axis represent precisions, while the lower portion represents what proportion of the words were deleted. The first item of note is that the NMT system has an overall higher proportion of deleted words. Of the 64379 words examined, the NMT system is estimated to have deleted 3769 of them, while the SMT system deleted 2274. Both the NMT and SMT systems delete very frequent and very infrequent words at higher proportions than words that fall into the middle range. Across frequencies, the NMT systems delete a higher proportion of words than the SMT system does. (The related issue of translation length is discussed in more detail in Section 3.4.)\nThe next interesting observation is what happens with unknown words (words which were never observed in the training corpus). The SMT system translates these correctly 53.2% of the time, while the NMT system translates them cor-\nthan in the reference, we award fractional credit. If the target word does not appear in the reference, we award zero credit. We then average these scores over the full set of target words aligned to the given source word to compute the precision for that source word. Source words can then be binned by frequency and average translation precisions can be computed.\nrectly 60.1% of the time. This is reflected in Figure 5, where the SMT system shows a steep curve up from the unobserved words, while the NMT system does not see a great jump.\nBoth SMT and NMT systems actually have their worst performance on words that were observed a single time in the training corpus, dropping to 48.6% and 52.2%, respectively; even worse than for unobserved words. Table 2 shows a breakdown of the categories of words that were unobserved in the training corpus or observed only once. The most common categories across both are named entity (including entity and location names) and nouns. The named entities can often be passed through unchanged (for example, the surname \u201cElabdellaoui\u201d is broken into \u201cE@@ lab@@ d@@ ell@@ a@@ oui\u201d by the bytepair encoding and is correctly passed through unchanged by both the NMT and SMT systems). Many of the nouns are compound nouns; when these are correctly translated, it may be attributed to compound-splitting (SMT) or byte-pair encoding (NMT). The factored SMT system also has access to the stemmed form of words, which can also play a similar role to byte-pair encoding in enabling translation of unobserved inflected forms (e.g. adjectives, verbs). Unsurprisingly, there are many numbers that were unobserved in the training data; these tend to be translated correctly (with occasional errors due to formatting of commas and periods, resolvable by post-processing).\nThe categories which involve more extensive inflection (adjectives and verbs) are arguably the most interesting. Adjectives and verbs have worse accuracy rates and higher deletion rates than nouns across most word frequencies. We show examples in Figure 6 of situations where the NMT system succeeds and fails, and contrast it with the failures of the SMT system. In Example 1, the NMT system successfully translates the unobserved adjective choreographiertes (choreographed), while\nthe SMT system does not. In Example 2, the SMT system simply passes the German verb einkesselte (closed in on) unchanged into the output, while the NMT system fails silently, selecting the fluent-sounding but semantically inappropriate \u201cstabbed\u201d instead.\nWhile there remains room for improvement, NMT systems (at least those using byte-pair encoding) perform better on very low-frequency words then SMT systems do. Byte-pair encoding is sometimes sufficient (much like stemming or compound-splitting) to allow the successful translation of rare words even though it does not necessarily split words at morphological boundaries. As with the fluent-sounding but semantically inappropriate examples from domain-mismatch, NMT may sometimes fail similarly when it encounters unknown words even in-domain."}, {"heading": "3.4 Long Sentences", "text": "A well-known flaw of early encoder-decoder NMT models was the inability to properly translate long sentences (Cho et al., 2014; Pouget-Abadie et al., 2014). The introduction of the attention model remedied this problem somewhat. But how well?\nWe used the large English-Spanish system from the learning curve experiments (Section 3.2), and used it to translate a collection of news test sets from the WMT shared tasks. We broke up these sets into buckets based on source sentence length (1-9 subword tokens, 10-19 subword tokens, etc.) and computed corpus-level BLEU scores for each.\nFigure 7 shows the results. While overall NMT is better than SMT, the SMT system outperforms NMT on sentences of length 60 and higher. Quality for the two systems is relatively close, except for the very long sentences (80 and more tokens). The quality of the NMT system is dramatically\nBLEU Scores with Varying Sentence Length\nlower for these since it produces too short translations (length ratio 0.859, opposed to 1.024)."}, {"heading": "3.5 Word Alignment", "text": "The key contribution of the attention model in neural machine translation (Bahdanau et al., 2015) was the imposition of an alignment of the output words to the input words. This takes the shape of a probability distribution over the input words which is used to weigh them in a bag-of-words representation of the input sentence.\nArguably, this attention model does not functionally play the role of a word alignment between the source in the target, at least not in the same way as its analog in statistical machine translation. While in both cases, alignment is a latent variable that is used to obtain probability distributions over words or phrases, arguably the attention model has a broader role. For instance, when translating a verb, attention may also be paid to its subject and object since these may disambiguate it. To further complicate matters, the word representations are products of bidirectional gated recurrent neural networks that have the effect that each word representation is informed by the entire sentence context.\nBut there is a clear need for an alignment mechanism between source and target words. For instance, prior work used the alignments provided by the attention model to interpolate word translation decisions with traditional probabilistic dic-\nre la ti o n s b et w ee n O b am a an d N et an y ah u h av e b ee n st ra in ed fo r y ea rs .\ntionaries (Arthur et al., 2016), for the introduction of coverage and fertility models (Tu et al., 2016), etc.\nBut is the attention model in fact the proper means? To examine this, we compare the soft alignment matrix (the sequence of attention vectors) with word alignments obtained by traditional word alignment methods. We use incremental fast-align (Dyer et al., 2013) to align the input and output of the neural machine system.\nSee Figure 8 for an illustration. We compare the word attention states (green boxes) with the word alignments obtained with fast align (blue outlines). For most words, these match up pretty well. Both attention states and fast-align alignment points are a bit fuzzy around the function words have-been/sind.\nHowever, the attention model may settle on alignments that do not correspond with our intuition or alignment points obtained with fast-align. See Figure 9 for the reverse language direction, German\u2013English. All the alignment points appear to be off by one position. We are not aware of any intuitive explanation for this divergent behavior \u2014 the translation quality is high for both systems.\nWe measure how well the soft alignment (attention model) of the NMT system match the alignments of fast-align with two metrics:\n\u2022 a match score that checks for each output\nif the aligned input word according to fast-\nd as V er h a\u0308l tn is zw is ch en O b am a u n d N et an y ah u is t se it Ja h re n g es p an n t .\nalign is indeed the input word that received the highest attention probability, and\n\u2022 a probability mass score that sums up the\nprobability mass given to each alignment point obtained from fast-align.\nIn these scores, we have to handle byte pair encoding and many-to-many alignments11\nIn out experiment, we use the neural machine translation models provided by Edinburgh12 (Sennrich et al., 2016a). We run fast-align on the same parallel data sets to obtain alignment models and used them to align the input and output of the NMT system. Table 3 shows alignment scores for the systems. The results suggest that, while drastic, the divergence for German\u2013English is an outlier. We note, however, that we have seen such large a divergence also under different data conditions.\n11(1) NMT operates on subwords, but fast-align is run on full words. (2) If an input word is split into subwords by byte pair encoding, then we add their attention scores. (3) If an output word is split into subwords, then we take the average of their attention vectors. (4) The match scores and probability mass scores are computed as average over output word-level scores. (5) If an output word has no fast-align alignment point, it is ignored in this computation. (6) If an output word is fast-aligned to multiple input words, then (6a) for the match score: count it as correct if the n aligned words among the top n highest scoring words according to attention and (6b) for the probability mass score: add up their attention scores.\n12https://github.com/rsennrich/wmt16-scripts\nNote that the attention model may produce better word alignments by guided alignment training (Chen et al., 2016; Liu et al., 2016) where supervised word alignments (such as the ones produced by fast-align) are provided to model training."}, {"heading": "3.6 Beam Search", "text": "The task of decoding is to find the full sentence translation with the highest probability. In statistical machine translation, this problem has been addressed with heuristic search techniques that explore a subset of the space of possible translation. A common feature of these search techniques is a beam size parameter that limits the number of partial translations maintained per input word.\nThere is typically a straightforward relationship between this beam size parameter and the model score of resulting translations and also their quality score (e.g., BLEU). While there are diminishing returns for increasing the beam parameter, typically improvements in these scores can be expected with larger beams.\nDecoding in neural translation models can be set up in similar fashion. When predicting the next output word, we may not only commit to the highest scoring word prediction but also maintain the next best scoring words in a list of partial translations. We record with each partial translation the word translation probabilities (obtained from the softmax), extend each partial translation with subsequent word predictions and accumulate these scores. Since the number of partial translation explodes exponentially with each new output word, we prune them down to a beam of highest scoring partial translations.\nAs in traditional statistical machine translation decoding, increasing the beam size allows us to explore a larger set of the space of possible translation and hence find translations with better model\nscores.\nHowever, as Figure 10 illustrates, increasing the beam size does not consistently improve translation quality. In fact, in almost all cases, worse translations are found beyond an optimal beam size setting (we are using again Edinburgh\u2019s WMT 2016 systems). The optimal beam size varies from 4 (e.g., Czech\u2013English) to around 30 (English\u2013 Romanian).\nNormalizing sentence level model scores by length of the output alleviates the problem somewhat and also leads to better optimal quality in most cases (5 of the 8 language pairs investigated). Optimal beam sizes are in the range of 30\u201350 in almost all cases, but quality still drops with larger beams. The main cause of deteriorating quality are shorter translations under wider beams."}, {"heading": "4 Conclusions", "text": "We showed that, despite its recent successes, neural machine translation still has to overcome various challenges, most notably performance out-ofdomain and under low resource conditions. We hope that this paper motivates research to address these challenges.\nWhat a lot of the problems have in common is that the neural translation models do not show robust behavior when confronted with conditions that differ significantly from training conditions \u2014 may it be due to limited exposure to training data, unusual input in case of out-of-domain test sentences, or unlikely initial word choices in beam search. The solution to these problems may hence lie in a more general approach of training that steps outside optimizing single word predictions given perfectly matching prior sequences."}, {"heading": "Acknowledgment", "text": "This work was partially supported by a Amazon Research Award (to the first author) and a National Science Foundation Graduate Research Fellowship under Grant No. DGE-1232825 (to the second author)."}], "references": [{"title": "Incorporating discrete translation lexicons into neural machine translation", "author": ["Graham Neubig", "Satoshi Nakamura."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Neubig and Nakamura.,? 2016", "shortCiteRegEx": "Neubig and Nakamura.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR. http://arxiv.org/pdf/1409.0473v6.pdf.", "citeRegEx": "Cho and Bengio.,? 2015", "shortCiteRegEx": "Cho and Bengio.", "year": 2015}, {"title": "Neural versus phrase-based machine translation quality: a case study", "author": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language", "citeRegEx": "Bentivogli et al\\.,? 2016", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "Guided alignment training for topic-aware neural machine translation", "author": ["Wenhu Chen", "Evgeny Matusov", "Shahram Khadivi", "Jan-Thorsten Peter."], "venue": "CoRR", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statis", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "The jhu machine translation systems for wmt 2016", "author": ["Shuoyang Ding", "Kevin Duh", "Huda Khayrallah", "Philipp Koehn", "Matt Post."], "venue": "Proceedings of the First Conference on Machine Translation. Association for Computational", "citeRegEx": "Ding et al\\.,? 2016a", "shortCiteRegEx": "Ding et al\\.", "year": 2016}, {"title": "The JHU machine translation systems for WMT 2016", "author": ["Shuoyang Ding", "Kevin Duh", "Huda Khayrallah", "Philipp Koehn", "Matt Post."], "venue": "Proceedings of the First Conference on Machine Translation (WMT).", "citeRegEx": "Ding et al\\.,? 2016b", "shortCiteRegEx": "Ding et al\\.", "year": 2016}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Victor Chahuneau", "Noah A. Smith."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for", "citeRegEx": "Chahuneau and Smith.,? 2013", "shortCiteRegEx": "Chahuneau and Smith.", "year": 2013}, {"title": "Fast domain adaptation for neural machine translation", "author": ["Markus Freitag", "Yaser Al-Onaizan."], "venue": "arXiv preprint arXiv:1612.06897 .", "citeRegEx": "Freitag and Al.Onaizan.,? 2016", "shortCiteRegEx": "Freitag and Al.Onaizan.", "year": 2016}, {"title": "Scalable inference and training of context-rich syntactic translation models", "author": ["Michel Galley", "Jonathan Graehl", "Kevin Knight", "Daniel Marcu", "Steve DeNeefe", "Wei Wang", "Ignacio Thayer."], "venue": "Proceedings of the 21st International Confer-", "citeRegEx": "Galley et al\\.,? 2006", "shortCiteRegEx": "Galley et al\\.", "year": 2006}, {"title": "What\u2019s in a translation rule", "author": ["Michel Galley", "Mark Hopkins", "Kevin Knight", "Daniel Marcu"], "venue": "In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Associ-", "citeRegEx": "Galley et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2004}, {"title": "Combining bilingual and comparable corpora for low resource machine translation", "author": ["Ann Irvine", "Chris Callison-Burch."], "venue": "Proceedings of the Eighth Workshop on Statistical Machine Translation. Association for Computational Linguistics, Sofia, Bulgaria, pages 262\u2013270.", "citeRegEx": "Irvine and Callison.Burch.,? 2013", "shortCiteRegEx": "Irvine and Callison.Burch.", "year": 2013}, {"title": "Is neural machine translation ready for deployment? a case study on 30 translation directions", "author": ["Tomasz Dwojak", "Hieu Hoang."], "venue": "Proceedings of the International Workshop on Spoken Language Translation (IWSLT).", "citeRegEx": "Dwojak and Hoang.,? 2016", "shortCiteRegEx": "Dwojak and Hoang.", "year": 2016}, {"title": "Interpolated backoff for factored translation models", "author": ["Philipp Koehn", "Barry Haddow."], "venue": "Proceedings of the Tenth Conference of the Association for Machine Translation in the Americas (AMTA).", "citeRegEx": "Koehn and Haddow.,? 2012", "shortCiteRegEx": "Koehn and Haddow.", "year": 2012}, {"title": "Neural machine translation with supervised attention", "author": ["Lemao Liu", "Masao Utiyama", "Andrew Finch", "Eiichiro Sumita."], "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics:", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Stanford neural machine translation systems for spoken language domains", "author": ["Mi h-Thang Luong", "Christopher DManning."], "venue": "Proceedings of the International Workshop on Spoken Language Translation.", "citeRegEx": "Luong and DManning.,? 2015", "shortCiteRegEx": "Luong and DManning.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Overcoming the curse of sentence length for neural machine translation CoRR", "author": ["Jean Poug t-Abadie", "Dzmitry Bahdanau", "Bart van Merrienboer", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "t.Abadie et al\\.,? \\Q2014\\E", "shortCiteRegEx": "t.Abadie et al\\.", "year": 2014}, {"title": "Edinburgh neural machine translation systems for WMT 16", "author": ["Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the First Conference on Machine Translation (WMT). Association for Computa-", "citeRegEx": "Haddow and Birch.,? 2016a", "shortCiteRegEx": "Haddow and Birch.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Haddow and Birch.,? 2016b", "shortCiteRegEx": "Haddow and Birch.", "year": 2016}, {"title": "Parallel data, tools and interfaces in opus", "author": ["J\u00f6rg Tiedemann."], "venue": "Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, edi-", "citeRegEx": "Tiedemann.,? 2012", "shortCiteRegEx": "Tiedemann.", "year": 2012}, {"title": "A multifaceted evaluation of neural versus phrase-based machine translation for 9 language directions", "author": ["Antonio Toral", "V\u0131\u0301ctorM. S\u00e1nchez-Cartagena"], "venue": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume", "citeRegEx": "Toral and S\u00e1nchez.Cartagena.,? \\Q2017\\E", "shortCiteRegEx": "Toral and S\u00e1nchez.Cartagena.", "year": 2017}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Learning performance of a machine translation system: a statistical and computational analysis", "author": ["Marco Turchi", "Tijl De Bie", "Nello Cristianini."], "venue": "Proceedings of the Third Workshop on Statistical Machine Translation. Association for Computational Linguistics, Columbus, Ohio, pages 35\u201343.", "citeRegEx": "Turchi et al\\.,? 2008", "shortCiteRegEx": "Turchi et al\\.", "year": 2008}, {"title": "Edinburgh\u2019s statistical machine translation systems for wmt16", "author": ["Philip Williams", "Rico Sennrich", "Maria Nadejde", "Matthias Huck", "Barry Haddow", "Ond\u0159ej Bojar."], "venue": "Proceedings of the First Conference on Machine", "citeRegEx": "Williams et al\\.,? 2016", "shortCiteRegEx": "Williams et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Bentivogli et al. (2016) considered different linguistic categories for English\u2013German and Toral and S\u00e1nchez-Cartagena (2017) compared different broad aspects such as fluency and reordering for nine language directions.", "startOffset": 0, "endOffset": 25}, {"referenceID": 2, "context": "Bentivogli et al. (2016) considered different linguistic categories for English\u2013German and Toral and S\u00e1nchez-Cartagena (2017) compared different broad aspects such as fluency and reordering for nine language directions.", "startOffset": 0, "endOffset": 126}, {"referenceID": 24, "context": "We build phrasebased systems using standard features that are commonly used in recent system submissions to WMT (Williams et al., 2016; Ding et al., 2016a).", "startOffset": 112, "endOffset": 155}, {"referenceID": 5, "context": "We build phrasebased systems using standard features that are commonly used in recent system submissions to WMT (Williams et al., 2016; Ding et al., 2016a).", "startOffset": 112, "endOffset": 155}, {"referenceID": 20, "context": "For the domain experiments, we use the OPUS corpus (Tiedemann, 2012).", "startOffset": 51, "endOffset": 68}, {"referenceID": 8, "context": "A currently popular approach is to train a general domain system, followed by training on in-domain data for a few epochs (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016).", "startOffset": 122, "endOffset": 177}, {"referenceID": 20, "context": "To test how well NMT and SMT hold up, we trained five different systems using different corpora obtained from OPUS (Tiedemann, 2012).", "startOffset": 115, "endOffset": 132}, {"referenceID": 23, "context": "This holds true for both parallel and monolingual data (Turchi et al., 2008; Irvine and Callison-Burch, 2013).", "startOffset": 55, "endOffset": 109}, {"referenceID": 11, "context": "This holds true for both parallel and monolingual data (Turchi et al., 2008; Irvine and Callison-Burch, 2013).", "startOffset": 55, "endOffset": 109}, {"referenceID": 16, "context": "Conventional wisdom states that neural machine translation models perform particularly poorly on rare words, (Luong et al., 2015; Sennrich et al., 2016b; Arthur et al., 2016) due in part to the smaller vocabularies used by NMT systems.", "startOffset": 109, "endOffset": 174}, {"referenceID": 6, "context": ", 2007), and the training data and parameters match those described in Johns Hopkins University\u2019s submission to the WMT shared task (Ding et al., 2016b).", "startOffset": 132, "endOffset": 152}, {"referenceID": 5, "context": ", 2007), and the training data and parameters match those described in Johns Hopkins University\u2019s submission to the WMT shared task (Ding et al., 2016b). Both models have case-sensitive BLEU scores of 34.5 on the WMT 2016 news test set (for the NMT model, this reflects the BLEU score resulting from translation with a beam size of 1). We use a single corpus for computing our lexical frequency counts (a concatenation of Common Crawl, Europarl, and News Commentary). We follow the approach described by Koehn and Haddow (2012) for examining the effect of source word frequency on translation accuracy.", "startOffset": 133, "endOffset": 528}, {"referenceID": 4, "context": "A well-known flaw of early encoder-decoder NMT models was the inability to properly translate long sentences (Cho et al., 2014; Pouget-Abadie et al., 2014).", "startOffset": 109, "endOffset": 155}, {"referenceID": 22, "context": ", 2016), for the introduction of coverage and fertility models (Tu et al., 2016), etc.", "startOffset": 63, "endOffset": 80}, {"referenceID": 3, "context": "Note that the attention model may produce better word alignments by guided alignment training (Chen et al., 2016; Liu et al., 2016) where supervised word alignments (such as the ones produced by fast-align) are provided to model training.", "startOffset": 94, "endOffset": 131}, {"referenceID": 14, "context": "Note that the attention model may produce better word alignments by guided alignment training (Chen et al., 2016; Liu et al., 2016) where supervised word alignments (such as the ones produced by fast-align) are provided to model training.", "startOffset": 94, "endOffset": 131}], "year": 2017, "abstractText": "We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrasebased statistical machine translation.", "creator": "LaTeX with hyperref package"}}}