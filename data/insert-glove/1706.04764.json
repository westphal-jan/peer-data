{"id": "1706.04764", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Efficient Streaming Algorithms for Submodular Maximization with Multi-Knapsack Constraints", "abstract": "houseplants Submodular zardo maximization (SM) perl has become a silver ansumane bullet 1848-1849 for mal\u00e1 a li\u00ean broad metals class vukovi of applications ferrand such as aguado influence mrsa maximization, yene data proenza summarization, top - $ k $ representative queries, goldsberry and weightlifting recommendations. wakiihuri In this strutton paper, abramovich we dramatise study peristeri the tegn\u00e9r SM hornbill problem unsettles in data universel streams. reege Most existing rajapaksa algorithms 47.99 for h\u00f6fe streaming pre-kindergarten SM only brieg support marle the append - millones only model with cardinality 3.4-billion constraints, kamchatka which cannot 83.98 meet the uson requirements of 1,795 real - world htm problems considering either the data recency 25.35 issues hordern or tagoloan more 4-games-to-1 general $ einmal d $ - matherly knapsack sfd constraints. pro-government Therefore, we dormont first propose lerato an hadiya append - fisher-price only dyo streaming algorithm {\\ chenglingji sc rebif KnapStream} for kolodziejczyk SM subject to 1728 a $ billed d $ - hiero knapsack constraint (SMDK ). latta Furthermore, 5,800 we devise shantadurga the {\\ sc brmsg KnapWindow} welcom algorithm alt-country for 99.60 SMDK 107.93 over cashbox sliding windows superiority to 17.16 capture 34.37 the recency wxnet constraints. hachareidis Theoretically, the proposed roback algorithms rutte have constant bestrides approximation fishermen ratios starsailor for a fixed number of knapsacks postel and sa\u00f4ne sublinear complexities. courcelle We finally evaluate the boyzone efficiency and benison effectiveness ebbitt of avtotor our algorithms in 1981-1985 two aequi real - world interisland datasets. leonberg The results show that papapetrou the aldag proposed persistance algorithms feedwater achieve 1960-1980 two mccorkle orders national-level of magnitude jaun speedups pfoutch over strzegowo the greedy hoho baseline in geraardsbergen the mutantes batch setting while 9,260 preserving high quality seafoam solutions.", "histories": [["v1", "Thu, 15 Jun 2017 07:59:57 GMT  (285kb,D)", "http://arxiv.org/abs/1706.04764v1", "20 pages, 8 figures"]], "COMMENTS": "20 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.DS cs.LG cs.SI", "authors": ["yanhao wang", "yuchen li", "kian-lee tan"], "accepted": false, "id": "1706.04764"}, "pdf": {"name": "1706.04764.pdf", "metadata": {"source": "CRF", "title": "Efficient Streaming Algorithms for Submodular Maximization with Multi-Knapsack Constraints", "authors": ["Yanhao Wang", "Yuchen Li", "Kian-Lee Tan"], "emails": ["yanhao90@comp.nus.edu.sg", "liyuchen@comp.nus.edu.sg", "tankl@comp.nus.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "Extracting representative elements from massively generated data is becoming the de facto standard in the big data era. In many scenarios, the utility value (or representativeness) of the selected subset is defined by a submodular function and the problem is thus formulated as maximizing a submodular function subject to a certain constraint. In fact, submodular functions are pervasive in a broad class of applications, e.g., influence maximization [8, 20, 28, 30], data summarization [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9] and recommendations [1, 22, 23, 32]. This is because submodular functions are not only adequate and general to model different subset selection problems but also have deep theoretical consequences for designing efficient approximation algorithms.\nMost existing literature on the submodular maximization (SM) problem focuses on maximizing a monotone submodular function with a cardinality constraint [4, 8, 9, 17, 22, 28, 31]. Although the classical greedy algorithm proposed by Nemhauser et al. in [19] provides a (1 \u2212 1/e)-approximate solution, it has several drawbacks that limit its use in a broader spectrum of applications.\nFirst, the greedy algorithm can only work with a single cardinality constraint. There is a large body of real-world problems [1,13,14,16,18,20,24,25,32] that are modeled as SM with (1) more than one constraint and (2) more general constraints beyond cardinality constraints. The d-knapsack constraints are a class of general constraints that have been widely adopted in SM problems [14,16,18,21,24,32]. A knapsack constraint assigns a cost to each element and restricts the total cost of selected elements to a given budget, and the cardinality constraint is a special case of a knapsack constraint where each element is assigned to a uniform cost of 1 and the budget is constrained to k. Here, we illustrate the d-knapsack constraints with two concrete \u2217yanhao90@comp.nus.edu.sg \u2020liyuchen@comp.nus.edu.sg \u2021tankl@comp.nus.edu.sg\nar X\niv :1\n70 6.\n04 76\n4v 1\n[ cs\n.D S]\n1 5\nJu n\nexamples. One example is the Twitter stream monitoring problem in [14], which selects a set of representative tweets such that both the number and the total length of selected tweets are constrained. This problem can be seen as SM with a 2-knapsack constraint. The other example is the server resource allocation problem in [21], which considers providing video streams to clients with limited server resources. The available server resources have three measures (i.e., storage space, communication bandwidth and processing bandwidth) and a client request has costs in three measures. The objective is to process as many client requests as possible while the total cost of processed requests does not exceed the available server resources in all measures. This problem is modeled as SM with a 3-knapsack constraint. It is noted that the greedy algorithm cannot even provide a valid solution for a SM problem with a d-knapsack constraint.\nSecond, the greedy algorithm can only work in the batch setting and thus is not scalable to process datasets with extremely large volumes or high generation velocities. Many algorithms [2, 3, 5, 6, 29, 30, 32] have been developed for data stream models. They consider elements arrive one at a time and perform incremental updates. There have been several techniques [2, 12] for streaming SM, but they can only work with a single cardinality constraint. Moreover, data recency is becoming increasingly important for real-time analytics. Many practical applications [6, 29, 30] adopt the sliding window [5] model where only the last W elements in the stream are considered to meet the recency requirement. However, SM over sliding windows is still largely unexplored and, to the best of our knowledge, there are only two existing techniques [6,30] on this topic. Not surprisingly, both techniques are also specific for cardinality constraints.\nIn this paper, we study the problem of maximizing a monotone submodular function with a d-knapsack constraint (SMDK) in data streams. We propose two streaming algorithms (KnapStream and KnapWindow) for SMDK in append-only streams and over sliding windows respectively.\nThe basic idea of KnapStream is to estimate the optimal utility value from observed elements and to maintain a set of candidate solutions with different estimations. After processing all elements, the candidate solution with the largest utility value is returned as the final result. Theoretically, KnapStream is ( 11+d \u2212 O(1)) approximate for SMDK in append-only streams and has sublinear time and space complexities. It improves the state-of-the-art ( 11+2d \u2212O(1)) approximation ratio for this problem in [32].\nWe further propose KnapWindow for SMDK over sliding windows. KnapWindow always maintains a sequence of s checkpoints {x1, . . . , xs} (x1 < . . . < xs = t) over the sliding window at time t. Each checkpoint xi corresponds to an instance of KnapStream on processing a sub-stream of elements from time xi to t. The first non-expired checkpoint (x1 or x2) provides the solutions of SMDK for KnapWindow. KnapWindow maintains a logarithmic number of checkpoints w.r.t. the range of the utility values while returning solutions with provable approximation ratios. To the best of our knowledge, it is the first algorithm for SMDK over sliding windows. In addition, we devise a buffer and post-processing optimization for KnapWindow. It maintains a fixed-size buffer in each candidate solution and performs a post-processing with buffered elements before returning the results. It spends extra time but produces solutions with significantly higher utilities.\nHere, we summarize our main contributions as follows:\n\u2022 We devise a novel algorithm KnapStream for SMDK in append-only streams. It improves the approximation ratio from ( 11+2d \u2212O(1)) to ( 1 1+d \u2212O(1)). (Section 4) \u2022 We propose the first algorithm KnapWindow for SMDK over sliding windows. It provides ( 12(1+d) \u2212 O(1)) approximate solutions for SMDK. We also devise the buffer and post-processing optimization for KnapWindow, which achieves better utilities with extra overhead. (Section 5) \u2022 We experimentally evaluate the efficiency and effectiveness of our proposed algorithm with two applications in real-world datasets. The results show that our algorithms are much more efficient than existing algorithms on SMDK in both batch and append-only streaming settings while producing solutions of competitive quality. (Section 6)"}, {"heading": "2 Related Work", "text": "In this section, we survey the literature that is the most relevant to our work: (1) SM in the batch setting and (2) SM in data streams. SM in the batch setting. Due to its theoretical consequences, SM has been widely applied to various problems in databases and data mining, including influence maximization [8, 20, 28, 30], data summariza-\ntion [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9], recommendations [1, 22, 23, 32], to just name a few. This has triggered a large body of research on SM in recent years [2, 6, 11\u201313, 15, 18, 27, 32]. We focus on reviewing existing techniques for SMDK in the batch setting as they are the most related to our work. Sviridenko [27] first proposes a (1\u2212 1/e) approximation algorithm for SM subject to 1-knapsack constraints with O(n5) complexity. Kulik et al. [11] propose a (1\u2212 1/e\u2212 \u03b5) approximation algorithm for SM subject to d-knapsack constraints with O(nd\u03b5 \u22124 ) complexity. Both algorithms have high-order polynomial complexities and are not practical for real-world applications in large datasets. Efficient algorithms for SM subject to 1-knapsack constraints with approximation factors of 12 (1\u22121/e) and (1\u22121/ \u221a e) are proposed by Leskovec et al. [13] and Lin et al. [15] respectively. Both algorithms cannot be directly applied to SM with d-knapsack constraints. In this paper, we adapt the algorithm in [15] for SMDK and use the adapted algorithm as the batch baseline. We further propose algorithms for SMDK in data stream models with much higher efficiency than the baseline. SM in data streams. Next, we review existing streaming SM algorithms [2,6,12,30,32]. Badanidiyuru et al. [2] propose a (1/2\u2212\u03b5) approximation algorithm for SM with cardinality constraints in append-only streams with sublinear time and space complexities. Append-only stream algorithms for SMDK are proposed in [12] and [32]. Both algorithms achieve a ( 11+2d \u2212O(1)) approximation for SMDK. To the best of our knowledge, there is no existing algorithm with a better approximation ratio. Our proposed KnapStream algorithm in Section 4 improves the approximation ratio to ( 11+d \u2212O(1)).\nThe sliding window model [5] is widely adopted in many streaming applications [3, 6, 29, 30] to capture the data recency constraint. Exponential histograms [5] and smooth histograms [3] are common methods to estimate the values of functions over sliding windows. Both methods achieve sublinear time and space complexities w.r.t. the window size with bounded error ratios. Having said that, they are only applicable to the functions with special properties. Specifically, exponential histograms can only approximate \u201cweakly additive\u201d functions, i.e., f(A)+f(B) \u2264 f(A\u222aB) \u2264 Cf \u00b7 (f(A)+f(B)) for any disjoint sub-streams A,B and a small constant Cf . Submodular functions are not \u201cweakly additive\u201d as they have f(A) + f(B) \u2265 f(A\u222aB). Smooth histograms give non-trivial results only when the target function can be approximated with a factor of at least 0.8 in append-only streams [6]. However, there is no polynomial time algorithms for SMDK achieving an approximation factor of over (1\u2212 e\u22121) \u2248 0.632 unless P = NP [19]. Therefore, both methods cannot be directly applied to SMDK over sliding windows. To the best of our knowledge, there are only a few research efforts [6, 30] on SM over sliding windows, but they only focus on cardinality constraints. Epasto et al. [6] propose (1/3 \u2212 \u03b5) and (1/2 \u2212 \u03b5) approximation algorithms for SM with cardinality constraints over sliding windows. In [30], influence maximization in social streams is defined by SM with cardinality constraints over sliding windows. They propose (1/2 \u2212 \u03b5) and (1/4 \u2212 \u03b5) approximation algorithms for the problem. In this paper, we address the problem of SM over sliding windows with more general d-knapsack constraints."}, {"heading": "3 Preliminaries", "text": "In this section, we first introduce the definitions of monotone submodular functions and d-knapsack constraints in Section 3.1. Subsequently, we discuss the classical SMDK problem in the batch setting in Section 3.2. In Section 3.3, we present the problems of SMDK in the append-only stream model as well as the sliding window model respectively."}, {"heading": "3.1 Monotone Submodular Functions and d-knapsack Constraints", "text": "Monotone Submodular Functions. Given a ground set of elements V , we consider a set function f : 2V \u2192 R\u22650, which maps any subset of elements in V to a utility value. We first introduce the concept of marginal gain on f .\nDefinition 1 (Marginal Gain). For a set of elements S \u2286 V and an element v \u2208 V \\ S, the marginal gain of f is defined by \u2206f (v|S) := f(S \u222a {v})\u2212 f(S).\nThen, we formally define the monotonicity and submodularity1 of f based on the marginal gains of f . 1We also consider f is nonnegative as f is aligned to f(\u2205) = 0 and f is monotone.\nAlgorithm 1 CEGreedy Input: A ground set of elements V Output: A result set SG 1: vmax \u2190 argmaxv\u2208V f({v}) 2: S \u2190 \u2205 3: while \u2203v \u2208 V \\ S such that S \u222a {v} \u2208 \u03be do 4: v\u2217 \u2190 argmaxv\u2208V \\S\u2227S\u222a{v}\u2208\u03be \u2206f (v|S) maxj\u2208[1,d] cj(v)\n5: S \u2190 S \u222a {v\u2217} 6: if f(S) \u2265 f({vmax}) then 7: return SG \u2190 S 8: else 9: return SG \u2190 {vmax}\nWe say f is monotone iff \u2206f (v|S) \u2265 0 for any S \u2286 V and v \u2208 V \\ S. Furthermore, f is submodular iff \u2206f (v|S) \u2265 \u2206f (v|T ) for any S \u2286 T \u2286 V and v \u2208 V \\ T . Intuitively, monotonicity means adding more elements to a set does not decrease its utility value, and submodularity captures the \u201cdiminishing returns\u201d property that the marginal gain of adding any new element decreases as the set grows larger. The d-knapsack Constraints. A knapsack is defined by a cost function c : V \u2192 R+ that assigns a cost to each element in V . Let c(v) denote the cost of v \u2208 V . The cost c(S) for a set of elements S \u2286 V is the sum of the costs of all its members, i.e., c(S) = \u2211 v\u2208S c(v). Given a budget b, we say S satisfies the knapsack constraint if c(S) \u2264 b. W.l.o.g., we set the budget to 1 and normalize the costs of all elements to (0, 1). A d-knapsack constraint \u03be is defined by d cost functions c1(\u00b7), . . . , cd(\u00b7). Formally, we define \u03be = {S \u2286 V : cj(S) \u2264 1,\u2200j \u2208 [1, d]}. We say S satisfies the d-knapsack constraint iff S \u2208 \u03be."}, {"heading": "3.2 SMDK in the Batch Setting", "text": "We define the optimization problem of maximizing a monotone submodular function f subject to a dknapsack constraint \u03be (SMDK) as follows:\nmax S\u2286V\nf(S) s.t. S \u2208 \u03be (1)\nwe use S\u2217 to denote the optimal solution of SMDK, i.e., S\u2217 = argmaxS\u2208\u03be f(S). According to the definition of the d-knapsack constraint, a cardinality constraint with a budget k is a special case of a 1-knapsack constraint with c(v) = 1/k, \u2200v \u2208 V . As SM with a cardinality constraint has been proved to be NP-hard [19], SMDK is also NP-hard.\nThe cost-effective greedy algorithm CEGreedy proposed in [15] for SM with 1-knapsack constraints can be adopted for SMDK2. As shown in Algorithm 1, CEGreedy starts from an empty set S = \u2205 and iteratively adds an element v\u2217 into S until S\u222a{v} /\u2208 \u03be for any remaining element v. The criterion for picking v\u2217 is based on its cost-effectiveness, i.e., v\u2217 maximizes \u2206f (v|S)maxj\u2208[1,d] cj(v) at the i-th iteration among all elements v \u2208 V \\ S. Finally, CEGreedy compares S with the singleton element vmax with the largest utility value and returns the better one as the final solution."}, {"heading": "3.3 SMDK in Data Stream Models", "text": "SMDK in Append-only Streams. The append-only stream model considers all elements in V arrive one at a time and any element must be processed once it arrives. At each time t \u2208 [1, n], an element vt \u2208 V is observed from the stream and SMDK requires to maintain an optimal solution S\u2217t regarding Equation 1 with an additional constraint: S\u2217t \u2286 {v1, . . . , vt}. Formally,\nDefinition 2. SMDK in the append-only stream model returns a set S\u2217t = argmaxS\u2208\u03be\u2227S\u2286{v1,...,vt} f(S).\n2We do not use the (1\u2212 1/e\u2212 \u03b5) approximation algorithm in [11] as the baseline for its O(nd\u03b5\u22124 ) complexity.\nFor ease of presentation, we use ctj to denote the cost of vt in the j-th knapsack. SMDK over Sliding Windows. We further consider SMDK in the sliding window model [5] to capture the recency constraint. The sliding window model considers the active window At always contains the W most recent elements (or active elements) at time t, i.e., At = {vs, . . . , vt} where s = max(1, t\u2212W + 1)3. At each time t, SMDK requires to maintain a solution S\u2217[At] regarding Equation 1 with an additional constraint: S\u2217[At] \u2286 At. Formally,\nDefinition 3. SMDK in the sliding window model returns a set S\u2217[At] = argmaxS\u2208\u03be\u2227S\u2286At f(S).\nA na\u00efve approach to SMDK in the aforementioned streaming scenarios is to store all observed/active elements and rerun CEGreedy from scratch for each observed element. This na\u00efve approach is obviously undesirable to satisfy the requirements of stream processing since it iteratively scans all elements and invokes a large number of function calls. In the subsequent sections, we will present our algorithms for SMDK both in append-only streams and over sliding windows. They are more efficient than the na\u00efve approach and are adequate to process data streams with high arrival rates.\nBefore introducing the proposed techniques, we summarize the frequently used notations in Table 1."}, {"heading": "4 The Append-only Streaming Algorithm", "text": "In this section, we propose an algorithm KnapStream for SMDK in append-only streams. We first give an algorithmic description of KnapStream in Section 4.1. Then, we theoretically analyze the approximation ratio and complexity of KnapStream in Section 4.2."}, {"heading": "4.1 The Algorithmic Description", "text": "The KnapStream algorithm uses a threshold-based framework proposed in [2] and [12] for streaming SM. The basic idea is to estimate the optimal utility value OPT from the observed elements and to maintain the candidate solutions based on the estimations. As OPT cannot be determined unless P = NP , KnapStream only tracks the lower and upper bounds of OPT and maintains a set of candidate solutions with different estimated values for OPT in the range. Each candidate solution derives a unique threshold for marginal gains according to its estimated value of OPT. Whenever a new element arrives, a candidate solution includes the element if the marginal gain of adding the element exceeds the threshold. After processing all elements, the candidate solution with the largest utility value is returned as the final result. Although having similar basic ideas, KnapStream is different from the algorithms in [2] and [12] in two aspects: (1) the criterion for adding an element considers not only the marginal gain of adding the element but also its costs, i.e., it checks the cost-effectiveness of adding the element in each knapsack and includes the element only when its cost-effectivenesses reach the threshold in d knapsacks; (2) the singleton element with the largest utility value is also considered as a candidate solution, which guarantees the approximation factor of the algorithm against undesirable cost distributions. Next, we will present the procedure of KnapStream in detail.\n3We only discuss the sequence-based sliding window in this paper but the proposed algorithms naturally support the timebased sliding window.\nAlgorithm 2 KnapStream Input: A ground set of elements V , a parameter \u03b5 Output: A result set St at time t 1: O = {(1 + \u03b5)l|l \u2208 Z} 2: For each % \u2208 O, S% \u2190 \u2205 3: Initialize m\u2190 0, M \u2190 0 and vmax \u2190 nil 4: for t\u2190 1, . . . , n do 5: if f({vt}) > f({vmax}) then 6: vmax \u2190 vt 7: if M < f({vt})minj\u2208[1,d] ctj then\n8: M \u2190 f({vt})minj\u2208[1,d] ctj ,m\u2190 f({vt})\n9: Ot = {(1 + \u03b5)l|l \u2208 Z,m \u2264 (1 + \u03b5)l \u2264M(1 + d)} 10: Delete all S% such that % /\u2208 Ot 11: for all % \u2208 Ot do 12: if \u2206f (vt|S%)ctj \u2265 % 1+d ,\u2200j \u2227 S% \u222a {vt} \u2208 \u03be then 13: S% \u2190 S% \u222a {vt} 14: For returning the result set St at time t: 15: S \u2190 argmax%\u2208Ot f(S%) 16: if f(S) \u2265 f({vmax}) then 17: return St \u2190 S 18: else 19: return St \u2190 {vmax}\nThe pseudo-code of KnapStream is given in Algorithm 2. It maintains three auxiliary variables: vmax maintains the maximum singleton element among observed elements at time t; m andM track the lower and upper bounds of the optimal value OPTt at time t respectively (Lines 5\u20138). When m and M are updated, KnapStream will set up new candidate solutions with the updated estimations of optimal values in Ot and delete candidates whose estimated optimal values are outside of Ot (Lines 9\u201310). Then, each maintained candidate will process vt at time t. For each % \u2208 Ot, if the marginal gain \u2206f (vt|S%) is at least ctj%1+d for all 1 \u2264 j \u2264 d and the d-knapsack constraint is still satisfied after adding vt into S%, vt will be included into S% (Lines 11\u201313). Finally, to return the solution St at time t, it first finds the set S with the largest utility value among all candidates, then compares S with the maximum singleton element vmax, and returns the better one as St (Lines 15\u201319)."}, {"heading": "4.2 Theoretical Analysis", "text": "We analyze the approximation ratio and complexity of KnapStream in this subsection. For the theoretical analysis, we consider the cost of any element is bounded by \u03b3 and \u03b4, i.e., 0 < \u03b3 \u2264 ctj \u2264 \u03b4 < 1,\u2200t \u2208 [1, n],\u2200j \u2208 [1, d]. It is noted that KnapStream does not need to know \u03b3 and \u03b4 in advance. In summary, we show that KnapStream requires only one pass over the ground set, provides a ( 11+d \u2212 O(1)) approximate solution for SMDK in an append-only stream, stores at most O( log(d\u03b3\n\u22121) \u03b3\u03b5 ) elements, and needs O( log(d\u03b3\u22121) \u03b5 ) function\ncalls per element. The roadmap of our analysis is as follows. First of all, we show that if we knew the optimal utility OPT in advance, the candidate solution whose estimated optimal value is the closest to OPT would be a (1\u2212\u03b5)(1\u2212\u03b4)\n1+d approximate solution (Lemma 1). However, this approximation ratio depends on \u03b4 and may degrade arbitrarily when \u03b4 increases. So we further prove that by considering the maximum singleton element as another candidate, there is a lower bound for the approximation ratio regardless of \u03b4 (Lemma 2). Then, as OPT is unknown, we analyze how KnapStream keeps track of the lower and upper bounds for OPT and how many different estimations are needed to guarantee that at least one of them approximates OPT within a bounded error ratio (Theorem 1). Finally, as KnapStream maintains one candidate solution for each estimation, we can get its time and space complexities accordingly (Theorem 2).\nLemma 1. Assuming there exists % \u2208 Ot such that (1 \u2212 \u03b5)OPTt \u2264 % \u2264 OPTt, S% satisfies that f(S%) \u2265 (1\u2212\u03b5)(1\u2212\u03b4)\n1+d OPTt.\nProof. Let si be the i-th element added to S%, Si% be {s1, . . . , si} for i \u2208 [0, |S%|] (S0% = \u2205 accordingly), bj = cj(S%) for j \u2208 [1, d] be the cost of S% in the j-th knapsack, and b = maxj\u2208[1,d] bj be the maximum cost of S% among d knapsacks. According to Line 12 in Algorithm 2, we have \u2206f (si|Si\u22121% ) cj(si)\n\u2265 %1+d for j \u2208 [1, d]. Then, it holds that\nf(S%) = |S%|\u2211 i=1 \u2206f (si|Si\u22121% ) \u2265 % 1 + d cj(S%) = % 1 + d bj\nThus, f(S%) \u2265 %1+db. Next, we discuss two cases separately as follows: Case 1: When b \u2265 (1\u2212 \u03b4), we have f(S%) \u2265 %1+db \u2265 %(1\u2212\u03b4) 1+d \u2265 (1\u2212\u03b5)(1\u2212\u03b4)\n1+d OPTt. Case 2: When b < (1 \u2212 \u03b4), we have \u2200v \u2208 V \\ S%, S% \u222a {v} \u2208 \u03be. Let S\u2217t be the optimal solution and a be an element in S\u2217t \\ S%. Since a is not added to S%, there must exist \u00b5(a) \u2208 [1, d] such that \u2206f (a|S\u2032%) c\u00b5(a)(a)\n< %1+d , where S\u2032% \u2286 S% is the partial solution when a is processed. We consider S\u2217j = {a|a \u2208 S\u2217t \\ S% \u2227 \u00b5(a) = j} for j \u2208 [1, d]. Due to the submodularity of f(\u00b7), we acquire:\nf(S% \u222a S\u2217j )\u2212 f(S%) \u2264 \u2211 a\u2208S\u2217j \u2206f (a|S%) < % 1 + d cj(S \u2217 j ) \u2264 % 1 + d\nThen, due to S\u2217t \\ S% = \u222adj=1S\u2217j , we have:\nf(S\u2217t \u222a S%)\u2212 f(S%) \u2264 d\u2211 j=1 f(S% \u222a S\u2217j )\u2212 f(S%) < d% 1 + d\nFinally, we get f(S%) > OPTt \u2212 d1+dOPTt \u2265 1 1+dOPTt. Considering both cases, we conclude the proof.\nLemma 1 has proved that KnapStream achieves a good approximation ratio when \u03b4 is small. Next, we further analyze the case where \u03b4 > 0.5 and prove that the approximation ratio has a lower bound regardless of \u03b4.\nLemma 2. When \u03b4 > 0.5, it satisfies that at least one of f(S%) and f({vmax}) is greater than 0.5(1\u2212\u03b5)1+d OPTt.\nProof. Lemma 2 naturally follows when b \u2265 0.5 (Case 1 of Lemma 1) or for all a \u2208 S\u2217t \\ S%, a is excluded from S% because its marginal gain does not reach the threshold in some knapsack (Case 2 of Lemma 1).\nThus, we only need to consider the following case: there exists some elements whose marginal gains reach the threshold in all knapsacks but are excluded from S% because including them into S% violates the dknapsack constraint. Assuming a is such an element for S%, we have \u2206f (a|S\u2032%) cj(a) \u2265 %1+d and cj(S \u2032 %)+cj(a) > 1 for some j \u2208 [1, d]. In this condition, we have f(S\u2032% \u222a {a}) \u2265 % 1+d (cj(S \u2032 %) + cj(a)) > % 1+d . From the monotonicity and submodularity of f(\u00b7), we get: %\n1 + d \u2264 f(S\u2032% \u222a {a}) \u2264 f(S\u2032%) + f({a}) \u2264 f(S%) + f({a})\nTherefore, at least one of f(S%) and f({a}) is greater than 0.5%1+d . As f({vmax}) \u2265 f({a}), we prove the lemma.\nGiven Lemmas 1 and 2, we prove that KnapStream achieves an approximation factor of (1\u2212\u03b4)(1\u2212\u03b5)1+d (when \u03b4 \u2264 0.5) or 0.5(1\u2212\u03b5)1+d (when \u03b4 > 0.5).\nTheorem 1. KnapStream satisfies that: at any time t,\nf(St) \u2265\n{ (1\u2212\u03b4)(1\u2212\u03b5)\n1+d f(S \u2217 t ), \u03b4 \u2264 0.5\n0.5(1\u2212\u03b5) 1+d f(S \u2217 t ), \u03b4 > 0.5\n(2)\nProof. By Lemmas 1 and 2, we can say Theorem 1 naturally holds if there exists at least one % \u2208 Ot such that (1 \u2212 \u03b5)OPTt \u2264 % \u2264 OPTt. To prove this, we first show that m and M are the lower and upper bounds for OPTt. It is easy to see m \u2264 OPTt as m \u2264 f({vmax}) and {v} \u2208 \u03be for any v \u2208 V . M maintains the maximum cost-effectiveness of all observed elements. We have M \u2265 f({vi})cij for i \u2208 [1, t] and j \u2208 [1, d]. Let S\u2217t = {a1, . . . , a|S\u2217t |} be the optimal solution at time t. As f(\u00b7) is monotone submodular, OPTt \u2264\u2211|S\u2217t | i=1 f({ai}) \u2264 cj(S\u2217t )M for j \u2208 [1, d]. As cj(S\u2217t ) \u2264 1, we are safe to say M \u2265 OPTt. KnapStream estimates OPTt by a sequence of values {(1 + \u03b5)l|l \u2208 Z,m \u2264 (1 + \u03b5)l \u2264 M(1 + d)}. Then, we can find an estimation % such that % \u2264 OPTt \u2264 (1 + \u03b5)%. Equivalently, (1\u2212 \u03b5)OPTt \u2264 % \u2264 OPTt. Finally, we conclude the proof by combining this result with Lemmas 1 and 2.\nNext, we analyze the complexity of KnapStream. KnapStream adopts the same candidate solution maintenance strategy as used in [2]. As only one pass over the stream is permitted, to avoid missing elements with marginal gains of more than M1+d , KnapStream maintains candidates in an increased range [m, (1 + d)M ] instead of [m,M ]. The following theorem shows the complexity of KnapStream.\nTheorem 2. KnapStream requires one pass over the ground set V , stores at most O( log(d\u03b3 \u22121)\n\u03b3\u03b5 ) elements,\nand has O( log(d\u03b3 \u22121)\n\u03b5 ) update complexity per element.\nProof. We first analyze the number of candidate solutions maintained at the same time. As mM \u2264 \u03b3, the number of candidates is bounded by dlog1+\u03b5 \u03b3\u22121(1+d)e. Thus, we have KnapStream maintains O( log(d\u03b3\u22121) \u03b5 ) candidates. For each candidate, one function call is required to evaluate whether to add a new element. Thus, the update complexity per element is also O( log(d\u03b3\n\u22121) \u03b5 ). Finally, for each candidate, at most \u03b3 \u22121 elements can be maintained. Otherwise, the d-knapsack constraint must not be satisfied. Therefore, the total number of elements stored is O( log(d\u03b3\n\u22121) \u03b3\u03b5 ).\nA Comparison with [32]. The algorithm in [32] also uses the threshold-based framework and works for SMDK in append-only streams. It maintains O( log \u03b3 \u22121\n\u03b5 ) candidates and is 1\u2212\u03b5 1+2d approximate for SMDK. The\napproximation ratio of KnapStream is better than this algorithm when the maximum cost of any element \u03b4 is less than d1+2d . It is noted that practical applications often use small values of \u03b4 (e.g., 0.1 in [6] and 0.25 in [32]), where KnapStream has obviously better approximation ratios. In addition, KnapStream is more robust than the algorithm in [32] when d increases as O( log d\u03b5 ) more candidates are maintained."}, {"heading": "5 The Sliding Window Algorithm", "text": "In this section, we propose an efficient algorithm KnapWindow for SMDK over sliding windows. We first give an algorithmic description of KnapWindow in Section 5.1. Then, we analyze KnapWindow theoretically in Section 5.2. Finally, we propose a buffer-based optimization technique to further improve the solution quality of KnapWindow and discuss how to handle the scenario where the sliding window shifts for more than one element in Section 5.3."}, {"heading": "5.1 The Algorithmic Description", "text": "The high level idea of KnapWindow is to maintain several instances of KnapStream (also called checkpoints) with different starting points. Each checkpoint maintains a solution for SMDK over all elements from the element corresponding to its starting point to the up-to-date element. At time t, it returns the solution of the first non-expired checkpoint as the result for At. The idea of maintaining a sequence of checkpoints over sliding windows is inspired by smooth histograms [3]. However, as has been shown in Section 2, smooth histograms cannot be applied to SMDK because no algorithm for SMDK in append-only streams meets the requirement of approximation ratios. Therefore, we devise a novel data structure Submodular Knapsack Checkpoints (SubKnapChk) for our problem. Submodular Knapsack Checkpoints. SubKnapChk at time t comprises a sequence of s checkpoints Xt = {x1, . . . , xs} where x1 < . . . < xs = t. Each checkpoint xi maintains an instance of KnapStream denoted by H(xi). The instance H(xi) processes all elements from vxi to vt at time t and will be terminated\nAlgorithm 3 KnapWindow Input: A ground set V , the window size W , a parameter \u03b2 Output: A result set S[At] for At at time t 1: Initialize s\u2190 0, the set of checkpoints X0 \u2190 \u2205 2: for t\u2190 1, . . . , n do 3: s\u2190 s+ 1, xs \u2190 t, and Xt \u2190 Xt\u22121 \u222a {xs} 4: Initiate a KnapStream instance H(xs) 5: while t > W \u2227 x2 < t\u2212W + 1 do 6: Xt \u2190 Xt \\ {x1} and terminate H(x1) 7: Shift the remaining checkpoints accordingly 8: s\u2190 s\u2212 1 9: for i\u2190 1, . . . , s do 10: H(xi) processes vt according to KnapStream 11: while \u2203i \u2208 [1, s\u2212 2] : f [xi+2, t] \u2265 (1\u2212 \u03b2)f [xi, t] do 12: Xt \u2190 Xt \\ {xi+1} and terminate H(xi+1) 13: Shift the remaining checkpoints accordingly 14: s\u2190 s\u2212 1 15: For returning the result set S[At] at time t: 16: if x1 \u2265 max(1, t\u2212W + 1) then 17: return the result of H(x1) at time t 18: else 19: return the result of H(x2) at time t\nwhen xi is deleted from SubKnapChk. When t > W , the first checkpoint x1 is expired (i.e., x1 < t\u2212W + 1) but is not deleted from SubKnapChk immediately. It is still maintained to track the upper bound of OPT[At]. However, we do not use the result of H(x1) as the solution because it may contain expired elements. It is noted that as there is at most one expired checkpoint in SubKnapChk at the same time and x2 must not expire, the result of H(x2) is returned as the solution in this case.\nThe most critical problem is how to find an adequate sequence of checkpoints to maintain. The maintenance strategy should achieve two objectives: (1) the number of maintained checkpoints should be small for high efficiency; (2) the utility values of returned solutions are theoretically bounded. Towards both objectives, we propose the following strategy to maintain the checkpoints: (1) create a new checkpoint and a new KnapStream instance for each arrival element; (2) delete a checkpoint and terminate its KnapStream instance once it can be approximated by its successors. Let f [xi, t] denote the utility value of the result returned by H(xi) at time t and OPTxit represent the optimal utility value for SMDK over elements from vxi to vt at time t respectively. Given three checkpoints xi\u22121, xi, xi+1 and a parameter \u03b2 > 0, if f [xi+1, t] \u2265 (1 \u2212 \u03b2)f [xi\u22121, t], we consider that the second checkpoint xi can be approximated by the third one xi+1 and can be deleted from SubKnapChk. Obviously, f [xi+1, t] is (1\u2212 \u03b2) approximate to f [xi\u22121, t] at time t. According to Theorem 1, f [xi+1, t] is at least (1\u2212\u03b2)(1\u2212\u03b4)(1\u2212\u03b5) 1+d approximate to OPT xi\u22121 t . It is essential to ensure that this ratio will not degrade too much over time. Utilizing the submodularity of f(\u00b7) and the properties of KnapStream, we have proved that f [xi+1, t\u2032] is at least (1\u2212\u03b2)(1\u2212\u03b4)(1\u2212\u03b5) 2(1+d) approximate to OPT xi\u22121 t\u2032 for any t\n\u2032 > t. We will formally analyze this property in Section 5.2. For the remaining part of this subsection, we focus on presenting the procedures of maintaining SubKnapChk and providing solutions for SMDK over sliding windows.\nThe pseudo-code of KnapWindow is presented in Algorithm 3. The maintenance of SubKnapChk is as illustrated in Lines 3\u201314. At time t, a new checkpoint xs = t and a KnapStream instance H(xs) are created for vt. Then, if there is more than one expired checkpoint in SubKnapChk, all except the last one will be deleted (Lines 5\u20138). This guarantees that there is only one expired checkpoint at any time. Subsequently, all remaining checkpoints process vt and update their results accordingly. This procedure follows Lines 5\u201313 of Algorithm 2. Next, KnapWindow performs the maintenance of SubKnapChk. It identifies all checkpoints that can be approximated by its successors and deletes them from SubKnapChk (Lines 11\u201314). After the maintenance of SubKnapChk, for any xi \u2208 Xt, there is at most one checkpoint x \u2208 Xt such that x > xi and\nf [x, t] \u2265 (1 \u2212 \u03b2)f [xi, t]. Finally, the solution S[At] for At is the result of H(x1) if x1 does not expire yet; otherwise, S[At] is the result of H(x2)."}, {"heading": "5.2 Theoretical Analysis", "text": "We will analyze the approximation ratio and complexity of KnapWindow theoretically in this subsection. Generally, KnapWindow requires one pass over the stream and provides a ( 12(1+d) \u2212 O(1)) approximate solution for SMDK over sliding windows. It maintains O( log \u03b8\u03b2 ) checkpoints and KnapStream instances, requires O( log \u03b8 log(d\u03b3 \u22121)\n\u03b2\u03b5 ) function calls per element, and stores O( log \u03b8 log(d\u03b3\u22121)\n\u03b2\u03b3\u03b5 ) elements, where \u03b8 is the ratio between the maximum and minimum utility values of SMDK and \u03b2 \u2208 (0, 1) is a predefined parameter.\nFirst of all, we analyze the properties of SubKnapChk maintained by Algorithm 3.\nLemma 3. At time t, SubKnapChk contains a sequence of s checkpoints Xt = {x1, . . . , xs}. For any 1 \u2264 i < s and \u03b2 \u2208 (0, 1), xi satisfies one of the following properties:\n1. if f [xi+1, t] \u2265 (1\u2212 \u03b2)f [xi, t], then f [xi+2, t] < (1\u2212 \u03b2)f [xi, t] or xi+1 = xs. 2. if xi+1 6= xi + 1 and f [xi+1, t] < (1 \u2212 \u03b2)f [xi, t], then there exists some t\u2032 < t such that f [xi+1, t\u2032] \u2265\n(1\u2212 \u03b2)f [xi, t\u2032]. 3. xi+1 = xi + 1 and f [xi+1, t] < (1\u2212 \u03b2)f [xi, t].\nProof. We prove the lemma by induction on t. As the base case, we first consider the condition when t = 2. We have X2 = {x1 = 1, x2 = 2}. Then property (1) holds if f [x2, 2] \u2265 (1\u2212\u03b2)f [x1, 2] and otherwise property (3) holds.\nNext, we assume Lemma 3 holds at time t and show that it still holds after performing Lines 3\u201314 of Algorithm 3 at time t+ 1. Let xi be a checkpoint created before t+ 1 and not deleted at time t+ 1. Then, xi+1 is the next checkpoint of xi at time t. We discuss all possible cases during the update procedure at time t+ 1. Case 1: xi+1 6= xi+1 and xi+1 is deleted from SubKnapChk at time t+1. In this case, we have f [xi+2, t+1] \u2265 (1\u2212 \u03b2)f [xi, t+ 1] (Line 11 of Algorithm 3). As xi+2 becomes the successor of xi at time t+ 1, property (1) holds at t+ 1. Case 2: xi+1 6= xi + 1 and xi+1 is not deleted from histograms at time t+ 1. In this case, we consider xi+1 becomes the successor of xi at some time t\u2032 \u2264 t. Then, it must hold that f [xi+1, t\u2032] \u2265 (1\u2212 \u03b2)f [xi, t\u2032]. Since xi+1 is not deleted at time t + 1, we have either property (1) (when f [xi+1, t + 1] \u2265 (1 \u2212 \u03b2)f [xi, t + 1]) or property (2) holds (when f [xi+1, t+ 1] < (1\u2212 \u03b2)f [xi, t+ 1]) at time t+ 1. Case 3: xi+1 = xi + 1. No matter whether xi+1 is deleted at time t + 1, property (1) holds as long as f [xi+1, t+ 1] \u2265 (1\u2212 \u03b2)f [xi, t+ 1]; otherwise, property (3) holds.\nWe show that the properties of SubKnapChk still hold at time t+ 1 in all possible cases and conclude the proof.\nGiven the properties of SubKnapChk in Lemma 3, we can analyze the approximation ratio of S[At] returned by KnapWindow for SMDK w.r.t. At.\nTheorem 3. It holds that f(S[At]) \u2265 (1\u2212\u03b2)(1\u2212\u03b4)(1\u2212\u03b5)2(1+d) \u00b7 OPT[At] at any time t.\nProof. We consider the first two checkpoints x1 and x2 of SubKnapChk maintained by KnapWindow at time t. If t \u2264 W , x1 = 1 does not expire and H(x1) are maintained over At = {v1, . . . , vt}. Thus, f(S[At]) = f [x1, t] \u2265 (1\u2212\u03b4)(1\u2212\u03b5)1+d OPT[At] for t \u2264 W by Theorem 1. Next, we consider the case that t > W and x2 = x1 + 1. In this case, x1 expires and x2 corresponds to the starting point of At. Similarly, f(S[At]) = f [x2, t] \u2265 (1\u2212\u03b4)(1\u2212\u03b5)1+d OPT[At].\nSubsequently, we consider other cases for t > W . We use OPTxy to denote the optimal utility value of SMDK over {vx, . . . , vy}. Case 1: If f [x2, t] \u2265 (1 \u2212 \u03b2)f [x1, t], f(S[At]) = f [x2, t] \u2265 (1 \u2212 \u03b2)f [x1, t]. By Theorem 1, f [x1, t] \u2265 (1\u2212\u03b4)(1\u2212\u03b5)\n1+d OPT x1 t . As x1 < t \u2212W + 1, we have At \u2282 {vx1 , . . . , vt} and OPT[At] \u2264 OPT x1 t . Finally, we have\nf(S[At]) \u2265 (1\u2212\u03b2)(1\u2212\u03b4)(1\u2212\u03b5)1+d OPT[At].\nCase 2: If f [x2, t] < (1 \u2212 \u03b2)f [x1, t], we have f [x2, t\u2032] \u2265 (1 \u2212 \u03b2)f [x1, t\u2032] for some t\u2032 < t. Let S\u2217[x1, t] denote the optimal solution for {vx1 , . . . , vt}. We can split S\u2217[x1, t] into two subsets S1 and S2, where S1 = {vi|vi \u2208 S\u2217[x1, t] \u2227 i \u2208 [x1, t\u2032]} and S2 = {vi|vi \u2208 S\u2217[x1, t] \u2227 i \u2208 [x2, t]}. Let OPT1 = f(S1) and OPT2 = f(S2). For S\u2217[x1, t] = S1 \u222a S2 and the submodularity of f(\u00b7), OPTx1t \u2264 OPT1 + OPT2. Then, as S1 \u2208 \u03be and S2 \u2208 \u03be, it holds that OPT1 \u2264 OPTx1t\u2032 and OPT2 \u2264 OPT x2 t . In addition, for any t1 < t2, the solution returned by KnapStream satisfies that f [x, t1] \u2264 f [x, t2]. As t > t\u2032, we have:\nf [x2, t] \u2265 (1\u2212 \u03b2)(1\u2212 \u03b4)(1\u2212 \u03b5)\n1 + d OPTx1t\u2032 \u2265 (1\u2212 \u03b2)(1\u2212 \u03b4)(1\u2212 \u03b5) 1 + d OPT1 (3)\nWe also have:\nf [x2, t] \u2265 (1\u2212 \u03b4)(1\u2212 \u03b5)\n1 + d OPTx2t \u2265 (1\u2212 \u03b4)(1\u2212 \u03b5) 1 + d OPT2 (4)\nCombining Equations 3 and 4, we prove that:\nf [x2, t] \u2265 (1\u2212 \u03b2)(1\u2212 \u03b4)(1\u2212 \u03b5) 1 + d \u00b7 OPT1 + OPT2 2\n\u2265 (1\u2212 \u03b2)(1\u2212 \u03b4)(1\u2212 \u03b5) 2(1 + d) OPTx1t\n\u2265 (1\u2212 \u03b2)(1\u2212 \u03b4)(1\u2212 \u03b5) 2(1 + d) OPT[At]\nCombining both cases, we conclude the proof.\nFinally, the following theorems analyze the complexity of KnapWindow.\nTheorem 4. Let \u03b8 the ratio between the maximum and minimum utility values of SMDK for At. SubKnapChk maintained by KnapWindow has O( log \u03b8\u03b2 ) checkpoints at time t.\nProof. In Lemma 3, we have shown that either f [xi+1, t] or f [xi+2, t] is less that (1\u2212 \u03b2)f [xi, t] at any time t. Then, as f [x2,t]f [xs,t] \u2264 \u03b8, the number of checkpoints is at most 2 log \u03b8 log(1\u2212\u03b2)\u22121 + 1. Therefore, the number of checkpoints is O( log \u03b8\u03b2 ).\nTheorem 5. KnapWindow performs O( log \u03b8 log(d\u03b3 \u22121)\n\u03b2\u03b5 )\nfunction calls per element and stores at most O( log \u03b8 log(d\u03b3 \u22121)\n\u03b2\u03b3\u03b5 ) elements.\nIt is easy to prove Theorem 5 by combining the results of Theorem 2 with Theorem 4."}, {"heading": "5.3 Optimization and Discussion", "text": ""}, {"heading": "5.3.1 Optimization", "text": "Although the utilities of the results returned by KnapWindow are much better than the theoretical lower bound in practice, they are often inferior to the solutions returned by CEGreedy according to our investigation. We identify the major reasons for the discrepancy as follows: First, the solutions of H(x2) are always used as the final results for At. However, x2 is often much later than the starting point of At (i.e., x2 t \u2212 W + 1). This implies that all elements between vt\u2212W+1 and vx2\u22121 are missing from the solutions. Second, the candidate solutions maintained by KnapStream with high thresholds are hard to be filled, even if more elements could still be added without violating the d-knapsack constraints. Based on these observations, we propose the buffer and post-processing optimization to improve the solution quality of KnapWindow. The Buffer Stage. When processing a new element vt, KnapStream will discard vt from a candidate solution S% directly if the marginal gain of adding vt is less than ctj% 1+d for any j \u2208 [1, d], regardless of how much the marginal gain is. For further improvements, instead of dropping these elements directly, we maintain a buffer for each candidate solution S%. Let B% be the buffer maintained for S%. If adding vt to S% achieves a\nAlgorithm 4 KnapWindowOpt Input: A ground set V , the window size W , the buffer size \u03b7, a parameter \u03b1 Output: A result set S[At] for At at time t 1: Initialize all variables and the set of checkpoints 2: for t\u2190 1, . . . , n do 3: Perform Lines 3\u201310 of KnapWindow 4: Maintain the buffer of each candidate solution in each checkpoint w.r.t. vt according to Strategy (1)\nto (3) 5: Perform Lines 11\u201314 of KnapWindow 6: The post-processing for returning S[At] at time t: 7: if x1 \u2265 max(1, t\u2212W + 1) then 8: For each S% in H(x1), perform Lines 3\u20135 of CEGreedy to add elements in B% to S% 9: return the result of H(x1)\n10: else 11: For each S% in H(x2), (1) add any element vi in S% and B% of H(x1) to B\u2032% if i \u2265 t\u2212W + 1\u2227S% \u222a {vi} \u2208 \u03be; (2) perform Lines 3\u20135 of CEGreedy to add elements in B% and B\u2032% to S% 12: return the result of H(x2)\nmarginal gain of slightly lower than the threshold, i.e., \u2206f (vt|S%)maxj\u2208[1,d] ctj \u2265 \u03b1%\n1+d , vt will be added to B%. Here, we use \u03b1 \u2208 (0, 1) to control the threshold for adding an element to the buffer. We also restrict the buffer size to \u03b7 such that the number of buffered elements for each candidate solution is at most \u03b7. Overall, the buffer maintenance strategy is as follows: (1) if \u2206f (vt|S%)maxj\u2208[1,d] ctj \u2208 [ \u03b1% 1+d , % 1+d ) and S% \u222a {vt} \u2208 \u03be, add vt to B%; (2) for any v \u2208 B%, if S% \u222a {v} /\u2208 \u03be, drop v directly; (3) when |B%| > \u03b7, drop the elements with the least values of \u2206f (vt|S%) maxj\u2208[1,d] ctj until |B%| = \u03b7. The Post-Processing Stage. Before returning the final solutions, we perform the post-processing using the buffered elements in the candidate solutions of H(x1) and H(x2) (if t < W , we only use H(x1)). Specifically, for each S% in H(x2), we consider not only the elements in B% but also the non-expired elements in S% and B% of H(x1) (as B\u2032%) if S% exists in H(x1). This avoids missing the elements between vt\u2212W+1 and vx2\u22121 from solutions. Then, we run CEGreedy starting from S% and iteratively add the elements in B% and B\u2032% to S%. After post-processing each S% in H(x2), we also return the solution with the largest utility value. The pseudo-code of KnapWindowOpt with the buffer and post-processing optimization is shown in Algorithm 4.\nObviously, the utility values of the results do not decrease after post-processing. In addition, the buffer maintenance procedure does not change the set of checkpoints. Therefore, KnapWindowOpt achieves at least the same approximation ratio as KnapWindow. However, it brings extra overhead for maintaining the buffer and post-processing. In our implementation, the buffer is maintained in a min-heap. The complexity of adding an element is O(log \u03b7) and dropping elements is O(\u03b7). Thus, the extra computational cost brought by maintaining the buffer is O(\u03b7 log \u03b8 log(d\u03b3\n\u22121) \u03b2\u03b5 ) and the total number of buffered elements is O( \u03b7 log \u03b8 log(d\u03b3\u22121) \u03b2\u03b5 )\nas well. The post-processing procedure for any candidate solution handles at most (2\u03b7 + \u03b3\u22121) elements and runs at most \u03b3\u22121 iterations. Therefore, the post-processing requires O( (2\u03b7+\u03b3\n\u22121) log(d\u03b3\u22121) \u03b3\u03b5 ) function calls."}, {"heading": "5.3.2 Discussion", "text": "In practical scenarios, we often consider a batch update strategy. Specifically, at time t, a sliding window receives L new elements while the earliest L elements become expired. KnapWindow can handle this case with trivial adaptations. It just creates one checkpoint at each time t and updates all maintained checkpoints by processing elements from v(t\u22121)L+1 to vtL collectively. In this way, the total number of checkpoints created is dWL e but the number of maintained checkpoints is still bounded by the utility values. In addition, the adaptations do not affect the buffer and post-processing optimization.\nIt is also easy to see the properties of SubKnapChk are still preserved for sliding windows with more than one element shifted at a time. Thus, the adaptations does not affect the approximation ratios and complexities of KnapWindow and KnapWindowOpt as well."}, {"heading": "6 Experiments", "text": "We discuss the experiments in this section. First, we introduce the applications for experimental evaluation as well as the datasets used in Section 6.1. Second, we present the compared approaches and parameter settings in Section 6.2. Finally, we evaluate the effectiveness and efficiency of our proposed algorithms and analyze the results in Section 6.3."}, {"heading": "6.1 Applications and Datasets", "text": "In this subsection, we show how to formulate two real-world applications, namely representative subset selection and maximum coverage, as SMDK and describe the datasets used for both applications in our experiments. Representative Subset Selection is a popular way for data summarization. It selects a small subset of elements out of the full dataset subject to some budget constraints as the representatives. A key issue is how to measure the representativeness of the selected subset. In this paper, we adopt the widely used Informative Vector Machine (IVM) [2, 6, 7] as the metric for the representativeness of any set of elements S, which is defined as follows:\nf(S) = 1\n2 log det(I + \u03c3\u22122KS,S) (5)\nwhere KS,S is an |S| \u00d7 |S| kernel matrix indexed by S and \u03c3 > 0 is a regularization parameter. For each element vi, vj \u2208 S, the (i, j)-th entry Ki,j of K represents the similarity between vi and vj measured via a symmetric positive definite kernel function. Here, we adopt the squared exponential kernel embedded in the Euclidean space Ki,j = exp(\u2212\u2016vi\u2212vj\u2016 2 2\nh2 ). It has been proved that f(\u00b7) defined by Equation 5 is a monotone submodular function [7].\nRepresentative subset selection is formulated as maximizing the IVM function with a cardinality constraint in previous work [2,6]. In the experiments, we consider the cardinality constraint as a special case of the 1-knapsack constraint. We use the Yahoo! Webscope4 dataset consisting of 45, 811, 883 user visits from the Featured Tab of the Today module on the Yahoo! front page. Each user visit is a 5-dimensional feature vector with a timestamp. In data preprocessing, we normalize all feature vectors to unit vectors. We set parameters h = 0.75 and \u03c3 = 1 as used in [2]. The ground set V consists of all user visits in the dataset. We assign a uniform cost c(v) = c to each user visit v, which is equivalent to a cardinality constraint of k = b1/cc. The objective is to select a representative subset S such that f(S) is maximized under the 1- knapsack constraint \u03be = {S \u2286 V : c(S) \u2264 1}. In the experiments, we stream all user visits to each approach one by one in ascending order of their timestamps. Maximum Coverage with d-knapsack Constraints. The maximum coverage problem is a classic NPhard problem. Given a domain of items E = {e1, . . . , em} and a ground set of sets S = {s1, . . . , sn} with each set si \u2286 E , it aims to select a set of sets S \u2032 \u2286 S subject to a given constraint such that S \u2032 covers as many items in E as possible. Many real-world problems such as document summarization [15], Blog watching [26], and resource allocation [21] are modeled as maximum coverage with 1 or d knapsack constraints [10]. As the coverage function is monotone submodular, algorithms for SM can naturally be used for maximum coverage. In our experiments, we consider a simple application of maximum coverage. We define a problem of selecting influential tweets as maximum coverage with a 2-knapsack constraint. We use a Twitter dataset collected by the streaming APIs5 containing 6, 860, 349 users and 8, 890, 118 tweets. The objective is to monitor influential tweets in social streams by tracking a set of tweets covering as many users as possible. We consider a user is covered by a tweet if he retweets the tweet. Then, the set of all users is treated as the domain of items and the coverage of each tweet forms the ground set of sets. We adopt the constraints as used in [14] for our experiments: the number as well as the total length of selected tweets are bounded. To fulfill both constraints, we assign two costs to each tweet v. The first one is a uniform cost c1(v) = c; and the second one, i.e., c2(v), is derived from the length of v. To normalize the cost, we first compute the average number of words w\u0302 of all tweets and then assign c2(v) as follows: if a tweet v contains w words and c1(v) = c, the cost c2(v) of this tweet will be c2(v) = wcw\u0302 . E.g., if w\u0302 = 20 and c = 0.1, a tweet v containing 40 words will be assigned\n4http://webscope.sandbox.yahoo.com/ 5https://dev.twitter.com/streaming/overview\na cost of c2(v) = 0.2. In the experiments, we stream each tweet along with all users covered by it to each approach one by one in ascending order of their timestamps."}, {"heading": "6.2 Experimental Settings", "text": "Approaches. The following approaches are compared in the experiments:\n\u2022 CEGreedy (CEG): We implement CEGreedy [15] as the batch baseline. As stated in Section 3, we do not use the algorithm in [11] for its efficiency. CEGreedy is (1\u2212 1/e) approximate for representative subset selection and 12 (1\u22121/e) approximate for influential tweets monitoring in the experiments. To work for sliding windows, it stores all active elements in the current window and recomputes the result from scratch for each window slide. \u2022 Random (RAND): We use a set of randomly selected elements as a baseline. For experiments in appendonly streams, it picks elements from the ground set one by one uniformly at random until the d-knapsack constraint is not satisfied. For experiments over sliding windows, it selects elements from At one by one uniformly at random until the d-knapsack constraint is not satisfied for each window slide. \u2022 Streaming (STR): We implement the algorithm proposed in [32] as the append-only streaming baseline. For experiments over sliding windows, it requires to recompute the result from scratch for each window slide. \u2022 KnapStream (KS ): We implement KnapStream proposed in Section 4 for SMDK in append-only streams. \u2022 KnapWindow (KW ): We implement KnapWindow in Section 5.1 for SMDK over sliding windows. \u2022 KnapWindowOpt (KW +): We implement KnapWindowOpt with the buffer and post-processing opti-\nmization in Section 5.3 for SMDK over sliding windows. We use \u03b1 = 0.8 and \u03b7 = 20 across all experiments.\nFor experiments in append-only streams, we compare our proposed KS with CEG , RAND , and STR. For experiments over sliding windows, we compare KW and KW + with the same baselines. All algorithms are implemented in Java 8 and all experiments are conducted on a server running CentOS 7.2 with a 3.6 GHz Intel i7-3820 processor and 64 GB memory. Parameter settings. The parameters used in our experiments are listed as follows: (1) the parameter \u03b5 for STR, KS , KW , and KW + ranges from 0.05 to 0.25, 0.1 by default. (2) the cost parameter c ranges from 0.01 to 0.05, 0.02 by default. (3) the size of the sliding window W ranges from 100K to 500K for representative subset selection and 1M to 5M for maximum coverage, 200K and 2M by default respectively. We adopt the sliding window model with multiple elements shifted at a time and set the number of elements for each window slide to 0.1% of W . And (4) the parameter \u03b2 for KW and KW + ranges from 0.05 to 0.25, 0.1 by default. Metrics for performance and quality. (1) We use the CPU time to measure the efficiency of compared approaches. For experiments in append-only streams, we use the total time for each approach to run over the full datasets. For experiments over sliding windows, we compute the average time for each approach to process one window slide. (2) We use the utility values of the solutions returned by compared approaches as the measure for quality. For experiments in append-only streams, we use the utility values of the solutions returned by each approach after processing the full datasets. For experiments over sliding windows, we compute the average utility values of the solutions returned by each approach among all window slides. (3) We also use the number of checkpoints and the number of elements maintained to measure the space usage of KW and KW +. As other approaches require to store the full active window to work in the sliding window model, the number of elements stored is always W and thus is omitted."}, {"heading": "6.3 Experimental Results", "text": ""}, {"heading": "6.3.1 An Overview of Experimental Results", "text": "Overview for Append-only Streams. We first compare KS with CEG , RAND , and STR in append-only streams. Figure 1 illustrates the CPU time and utility values of compared approaches in the default setting. Generally speaking, CEG spends the most time while achieving the best utility values as well. KS runs much faster than CEG (9x speedups on Yahoo! Webscope and 23x speedups on Twitter) while the utility values are only slightly worse (< 6%). RAND spends little time for stream processing but returns undesirable\nsolutions. Compared with STR, KS is 70% slower but achieves 5% and 12% increases in solution quality in the two datasets respectively. This is because KS maintains O( log d\u03b5 ) more candidate solutions than STR, which needs more function calls to process each element. Since all parameters used in append-only streams are also used similarly in the experiments for sliding windows, we only present the experimental results with different parameter settings for sliding windows in the remaining part of this section. Overview for Sliding Windows. We compare KW and KW + with CEG , RAND , and STR in the sliding window model. In Figure 2, we present the CPU time and utility values of compared approaches in the default setting. KW and KW + achieve speedups of more than two orders of magnitude over CEG in both datasets. At the same time, KW + provides solutions with utility values of at least 89% of CEG and the solution quality of KW also reaches at least 87% of CEG . We can see that KW + improves the quality over KW , with 2% on Yahoo! Webscope and 7% on Twitter. Meanwhile, it spends 8% and 40% more time respectively. In addition, KW and KW + run much faster than STR while providing solutions with equivalent or better quality. Not surprisingly, RAND uses little time but cannot provide any useful results for both applications. The experimental results verify the superiority of KW and KW + for SMDK over sliding windows. Changes in Utility Values over Time. In Figure 3, we show the changes in the utility values of compared\napproaches from the first full window t = W to the end of the stream t = n. We can see the solutions of CEG have the highest utility values in most cases. KW + returns a better solution than KW and the ratios between the utility values of its solutions and the ones of CEG are mostly more than 0.9. On Twitter, KW + can always return solutions with near equivalent utility values to CEG . STR has equivalent or better solution quality than KW , but takes much longer processing time (as the results of Figure 2). KW + outperforms STR in both efficiency and quality. As expected, RAND cannot provide any useful solutions. The utility values of maximum coverage on Twitter show considerable fluctuations over time. This is because the overall coverage of selected tweets is sensitive to a few tweets that are re-tweeted by a large number of users. We identify two influential tweets with more than 20K retweets in our datasets: \u201c@spurs: After 19 seasons, Tim Duncan announces retirement #ThankYouTD\u201d and \u201c@BleacherReport: Playing with the Warriors in 2K17 gonna be like...\u201d. Since all approaches except RAND include them into the results, the utility values show a drastic increase when they are observed at time 7.7\u00d7 106 and 8.3\u00d7 106 respectively. This results verify the effectiveness of the proposed algorithms for monitoring hot spots from social streams in real time."}, {"heading": "6.3.2 Scalability", "text": "Varying d. To study the scalability of compared approaches w.r.t. the number of knapsacks, i.e., d, we generate additional constraints by assigning random costs to the elements in the ground set. Specifically, we generate 5 costs for each element in both datasets. The costs are generated independently from a uniform distribution U(0.01, 0.05). We range d from 1 to 5 in the experiments and set the j-th cost of each element to construct the j-th knapsack constraint.\nThe CPU time and utility values of compared approaches with varying d are shown in Figures 4a\u20134d. The time usage of STR and CEG decreases as d increases. With the number of knapsacks increasing, the expected size of result sets becomes smaller. For KW and KW +, different trends are observed in the two datasets. The CPU time decreases on Yahoo! Webscope but increases on Twitter with larger d. This is because KS\nmaintains the candidate solutions with estimated optimal values from m to M(1 + d) (see Algorithm 2). As the range increases with d, more candidate solutions are maintained by KS . Since KW and KW + maintain KS in checkpoints, they also have more candidate solutions when d increases. On the Twitter dataset, extra costs brought by more candidate solutions overwhelm the benefits of smaller result sets and the overall time increases. But it is a different case for Yahoo! Webscope dataset. The time complexity of the IVM function evaluation for S is O(|S|3). Therefore, as the result set becomes smaller, the time for function evaluations decreases largely. Although more candidate solutions are maintained, the overall time decreases.\nFor solution quality, the first thing to note is that although all compared approaches only achieve approximation ratios of O(d\u22121), they show good robustness when d increases. The solution returned by CEG for d = 1 is guaranteed to be (1 \u2212 \u03b4)(1 \u2212 1/e) approximate (\u03b4 = 0.05) and thus can be seen as an upper bound estimation of the optimal value in practice (see the black lines in Figures 4c\u20134d). In addition, the optimal value can only decrease when d increases as the solution space becomes smaller. When d increases, the utility values of the compared approaches decrease. However, we can see that their solutions are much better than the theoretical lower bounds as they are close to the optimal utility upper bound of the solution for d = 1. KW and KW + remain stable w.r.t. d and can always guarantee solutions with utility values of at least 80% compared to CEG . Nevertheless, the solution quality of STR becomes highly unstable as d increases. When d = 5, its solution quality degrades to only 70% of CEG . Varying W . In Figures 5a\u20135d, the CPU time and utility values of compared approaches with varying W are presented. The processing time for each window slide increases proportionally with W . This is because we set the number of elements for each window slide to 0.1% of W and the number of elements to process increases linearly withW . The utility values increase withW and the solution quality ratio between different approaches remain stable. As shown in Table 2, the number of checkpoints and the number of elements show a slightly decreasing trend as W increases. In fact, they are not directly related to W and are bounded by the ratio between the utility values of the first and the last checkpoints, where the ratio decreases with a\nlarger W . Varying c. The CPU time and utility values of compared approaches with varying the cost c are illustrated in Figures 6a\u20136d. When c decreases, it takes a longer processing for all compared approaches. This is because the size of the result set is inversely proportional to c. We can see the CPU time for the Yahoo! Webscope dataset increases rapidly when c decreases. This is because the time complexity for each IVM function evaluation is O(|S|3). Thus, all approaches spend much more CPU time for each function call of f(S) when the size of result set grows. The utility values of each approach drops with a smaller cost due to the smaller size of result sets. We also note that the relative ratios between different approaches keep steady."}, {"heading": "6.3.3 Parameters \u03b5 and \u03b2", "text": "Varying \u03b5. We compare the performance of KW , KW +, and STR with varying \u03b5 in Figures 7a\u20137d. The results show that all approaches have lower CPU time for a larger \u03b5. This can be easily explained by their time complexities, all of which are inversely proportional to \u03b5. For solution quality, all approaches keep steady w.r.t. \u03b5 in the Yahoo! Webscope dataset. On the Twitter dataset, the quality of STR degrades seriously when \u03b5 grows. The phenomenon has demonstrated that, in practical scenarios, the approximation algorithms can provide solutions with much better quality than theoretical lower bounds. We note that KW and KW + are more robust in terms of solution quality for different \u03b5. Finally, KW and KW + are much more efficient than STR in all experiments. Varying \u03b2. The results for KW and KW + with varying \u03b2 are shown in Figures 8a\u20138d. Both the running time and utility values decrease as \u03b2 increases, which is consistent with the analysis in Section 5.2. As the intervals among checkpoints increase with \u03b2, the error of approximating a checkpoint by its successors increases. At the expense of post-processing, KW + returns better solutions than KW . On the Twitter dataset, when \u03b2 is 0.2 or 0.25, the time for post-processing exceeds the time for stream processing. As a result, the solution quality hardly degrades when \u03b2 increases. As is shown in Table 2, the number of checkpoints and the number of elements stored by KW and KW + decrease as \u03b2 grows. In addition, the difference between the number of elements kept by KW and KW + is the number of buffered elements in KW +. The ratios of buffered elements remain stable in different parameter settings (about 20% on Yahoo! Webscope and about 40% on Twitter)."}, {"heading": "6.3.4 Summary", "text": "Finally, we summarize all experimental results. In the append-only stream model, KS runs at least one order of magnitude faster than CEG while achieving close solution quality (> 90%). It improves the solution quality of STR at the expense of speed. In the sliding window model, KW and KW + achieve speedups of more than two orders of magnitude over CEG in all parameter settings, while returning solutions with competitive quality (> 80% for KW and > 85% for KW +). Compared with STR, they not only run much faster but also return solutions with equivalent or better utility values. Furthermore, KW + shows better solution quality than KW with the overhead for maintaining the buffer and post-processing."}, {"heading": "7 Conclusion", "text": "In this paper, we studied the problem of maximizing a monotone submodular function with a d-knapsack constraint (SMDK) in data stream models. We first proposed an algorithm KnapStream for SMDK in append-only streams with an approximation ratio of ( 11+d \u2212 O(1)), which improved the state-of-theart ( 11+2d \u2212 O(1)) approximation algorithm. Furthermore, we proposed KnapWindow and its optimized version KnapWindowOpt for SMDK over sliding windows with an approximation factor of ( 12(1+d)\u2212O(1)). The experimental results showed the efficiency and effectiveness of our proposed algorithms compared with baselines. A broad class of real-world applications can benefit from the results in this paper due to the prevalence of submodular functions and the generality of d-knapsack constraints. For future work, we would like to explore the solution for SMDK in a completely dynamic setting, where random insertions and deletions in the ground set are allowed."}], "references": [{"title": "Diversity maximization under matroid constraints", "author": ["Z. Abbassi", "V.S. Mirrokni", "M. Thakur"], "venue": "KDD, pages 32\u201340", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Streaming submodular maximization: Massive data summarization on the fly", "author": ["A. Badanidiyuru", "B. Mirzasoleiman", "A. Karbasi", "A. Krause"], "venue": "KDD, pages 671\u2013680", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Smooth histograms for sliding windows", "author": ["V. Braverman", "R. Ostrovsky"], "venue": "FOCS, pages 283\u2013293", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Representative skylines using thresholdbased preference distributions", "author": ["A. Das Sarma", "A. Lall", "D. Nanongkai", "R.J. Lipton", "J. Xu"], "venue": "ICDE, pages 387\u2013398", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Maintaining stream statistics over sliding windows", "author": ["M. Datar", "A. Gionis", "P. Indyk", "R. Motwani"], "venue": "SIAM Journal on Computing, 31(6):1794\u20131813", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Submodular optimization over sliding windows", "author": ["A. Epasto", "S. Lattanzi", "S. Vassilvitskii", "M. Zadimoghaddam"], "venue": "WWW, pages 421\u2013430", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Fast sparse gaussian process methods: The informative vector machine", "author": ["R. Herbrich", "N.D. Lawrence", "M. Seeger"], "venue": "NIPS, pages 625\u2013632", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Maximizing the spread of influence through a social network", "author": ["D. Kempe", "J. Kleinberg", "E. Tardos"], "venue": "KDD, pages 137\u2013146", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Top-k representative queries with binary constraints", "author": ["A. Khan", "V. Singh"], "venue": "SSDBM, pages 13:1\u2013 13:10", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "The budgeted maximum coverage problem", "author": ["S. Khuller", "A. Moss", "J. Naor"], "venue": "Information Processing Letters, 70(1):39\u201345", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1999}, {"title": "Maximizing submodular set functions subject to multiple linear constraints", "author": ["A. Kulik", "H. Shachnai", "T. Tamir"], "venue": "SODA, pages 545\u2013554", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast greedy algorithms in mapreduce and streaming", "author": ["R. Kumar", "B. Moseley", "S. Vassilvitskii", "A. Vattani"], "venue": "ACM Trans. Parallel Comput., 2(3):14:1\u201314:22", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Cost-effective outbreak detection in networks", "author": ["J. Leskovec", "A. Krause", "C. Guestrin", "C. Faloutsos", "J. VanBriesen", "N. Glance"], "venue": "KDD, pages 420\u2013429", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Towards social data platform: Automatic topic-focused monitor for twitter stream", "author": ["R. Li", "S. Wang", "K.C.-C. Chang"], "venue": "Proc. VLDB Endow., 6(14):1966\u20131977", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-document summarization via budgeted maximization of submodular functions", "author": ["H. Lin", "J. Bilmes"], "venue": "NAACL, pages 912\u2013920", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "A class of submodular functions for document summarization", "author": ["H. Lin", "J. Bilmes"], "venue": "ACL, pages 510\u2013520", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Retrieving non-redundant questions to summarize a product review", "author": ["M. Liu", "Y. Fang", "D.H. Park", "X. Hu", "Z. Yu"], "venue": "SIGIR, pages 385\u2013394", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast constrained submodular maximization: Personalized data summarization", "author": ["B. Mirzasoleiman", "A. Badanidiyuru", "A. Karbasi"], "venue": "ICML, pages 1358\u20131367", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "An analysis of approximations for maximizing submodular set functions\u2014i", "author": ["G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher"], "venue": "Mathematical Programming, 14(1):265\u2013294", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1978}, {"title": "On budgeted influence maximization in social networks", "author": ["H. Nguyen", "R. Zheng"], "venue": "IEEE Journal on Selected Areas in Communications, 31(6):1084\u20131094", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Video distribution under multiple constraints", "author": ["B. Patt-Shamir", "D. Rawitz"], "venue": "Theoretical Computer Science, 412(29):3717\u20133730", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "A coverage-based approach to recommendation diversity on similarity graph", "author": ["S.A. Puthiya Parambath", "N. Usunier", "Y. Grandvalet"], "venue": "RecSys, pages 15\u201322", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Online learning to diversify from implicit feedback", "author": ["K. Raman", "P. Shivaswamy", "T. Joachims"], "venue": "KDD, pages 705\u2013713", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparsi: Partitioning sensitive data amongst multiple adversaries", "author": ["T. Rekatsinas", "A. Deshpande", "A. Machanavajjhala"], "venue": "Proc. VLDB Endow., 6(13):1594\u20131605", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Characterizing and selecting fresh data sources", "author": ["T. Rekatsinas", "X.L. Dong", "D. Srivastava"], "venue": "SIGMOD, pages 919\u2013930", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "On maximum coverage in the streaming model & application to multi-topic blog-watch", "author": ["B. Saha", "L. Getoor"], "venue": "SDM, pages 697\u2013708", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "A note on maximizing a submodular set function subject to a knapsack constraint", "author": ["M. Sviridenko"], "venue": "Operations Research Letters, 32(1):41\u201343", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Influence maximization: Near-optimal time complexity meets practical efficiency", "author": ["Y. Tang", "X. Xiao", "Y. Shi"], "venue": "SIGMOD, pages 75\u201386", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "General incremental sliding-window aggregation", "author": ["K. Tangwongsan", "M. Hirzel", "S. Schneider", "K.-L. Wu"], "venue": "Proc. VLDB Endow., 8(7):702\u2013713", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Real-time influence maximization on dynamic social streams", "author": ["Y. Wang", "Q. Fan", "Y. Li", "K.-L. Tan"], "venue": "Proc. VLDB Endow., 10(7):805\u2013816", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2017}, {"title": "Efficient summarization framework for multi-attribute uncertain data", "author": ["J. Xu", "D.V. Kalashnikov", "S. Mehrotra"], "venue": "SIGMOD, pages 421\u2013432", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Submodular maximization with multi-knapsack constraints and its applications in scientific literature recommendations", "author": ["Q. Yu", "E.L. Xu", "S. Cui"], "venue": "GlobalSIP, pages 1295\u20131299", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Data summarization with social contexts", "author": ["H. Zhuang", "R. Rahman", "X. Hu", "T. Guo", "P. Hui", "K. Aberer"], "venue": "CIKM, pages 397\u2013406", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": ", influence maximization [8, 20, 28, 30], data summarization [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9] and recommendations [1, 22, 23, 32].", "startOffset": 25, "endOffset": 40}, {"referenceID": 19, "context": ", influence maximization [8, 20, 28, 30], data summarization [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9] and recommendations [1, 22, 23, 32].", "startOffset": 25, "endOffset": 40}, {"referenceID": 27, "context": ", influence maximization [8, 20, 28, 30], data summarization [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9] and recommendations [1, 22, 23, 32].", "startOffset": 25, "endOffset": 40}, {"referenceID": 29, "context": ", influence maximization [8, 20, 28, 30], data summarization [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9] and recommendations [1, 22, 23, 32].", "startOffset": 25, "endOffset": 40}, {"referenceID": 1, "context": ", influence maximization [8, 20, 28, 30], data summarization [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9] and recommendations [1, 22, 23, 32].", "startOffset": 61, "endOffset": 84}, {"referenceID": 14, "context": ", influence maximization [8, 20, 28, 30], data summarization [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9] and recommendations [1, 22, 23, 32].", "startOffset": 61, "endOffset": 84}, {"referenceID": 16, "context": ", influence maximization [8, 20, 28, 30], data summarization [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9] and recommendations [1, 22, 23, 32].", "startOffset": 61, "endOffset": 84}, {"referenceID": 17, "context": ", influence maximization [8, 20, 28, 30], data summarization [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9] and recommendations [1, 22, 23, 32].", "startOffset": 61, "endOffset": 84}, {"referenceID": 30, "context": ", influence maximization [8, 20, 28, 30], data summarization [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9] and recommendations [1, 22, 23, 32].", "startOffset": 61, "endOffset": 84}, {"referenceID": 32, "context": ", influence maximization [8, 20, 28, 30], data summarization [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9] and recommendations [1, 22, 23, 32].", "startOffset": 61, "endOffset": 84}, {"referenceID": 3, "context": ", influence maximization [8, 20, 28, 30], data summarization [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9] and recommendations [1, 22, 23, 32].", "startOffset": 115, "endOffset": 121}, {"referenceID": 8, "context": ", influence maximization [8, 20, 28, 30], data summarization [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9] and recommendations [1, 22, 23, 32].", "startOffset": 115, "endOffset": 121}, {"referenceID": 0, "context": ", influence maximization [8, 20, 28, 30], data summarization [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9] and recommendations [1, 22, 23, 32].", "startOffset": 142, "endOffset": 157}, {"referenceID": 21, "context": ", influence maximization [8, 20, 28, 30], data summarization [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9] and recommendations [1, 22, 23, 32].", "startOffset": 142, "endOffset": 157}, {"referenceID": 22, "context": ", influence maximization [8, 20, 28, 30], data summarization [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9] and recommendations [1, 22, 23, 32].", "startOffset": 142, "endOffset": 157}, {"referenceID": 31, "context": ", influence maximization [8, 20, 28, 30], data summarization [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9] and recommendations [1, 22, 23, 32].", "startOffset": 142, "endOffset": 157}, {"referenceID": 3, "context": "Most existing literature on the submodular maximization (SM) problem focuses on maximizing a monotone submodular function with a cardinality constraint [4, 8, 9, 17, 22, 28, 31].", "startOffset": 152, "endOffset": 177}, {"referenceID": 7, "context": "Most existing literature on the submodular maximization (SM) problem focuses on maximizing a monotone submodular function with a cardinality constraint [4, 8, 9, 17, 22, 28, 31].", "startOffset": 152, "endOffset": 177}, {"referenceID": 8, "context": "Most existing literature on the submodular maximization (SM) problem focuses on maximizing a monotone submodular function with a cardinality constraint [4, 8, 9, 17, 22, 28, 31].", "startOffset": 152, "endOffset": 177}, {"referenceID": 16, "context": "Most existing literature on the submodular maximization (SM) problem focuses on maximizing a monotone submodular function with a cardinality constraint [4, 8, 9, 17, 22, 28, 31].", "startOffset": 152, "endOffset": 177}, {"referenceID": 21, "context": "Most existing literature on the submodular maximization (SM) problem focuses on maximizing a monotone submodular function with a cardinality constraint [4, 8, 9, 17, 22, 28, 31].", "startOffset": 152, "endOffset": 177}, {"referenceID": 27, "context": "Most existing literature on the submodular maximization (SM) problem focuses on maximizing a monotone submodular function with a cardinality constraint [4, 8, 9, 17, 22, 28, 31].", "startOffset": 152, "endOffset": 177}, {"referenceID": 30, "context": "Most existing literature on the submodular maximization (SM) problem focuses on maximizing a monotone submodular function with a cardinality constraint [4, 8, 9, 17, 22, 28, 31].", "startOffset": 152, "endOffset": 177}, {"referenceID": 18, "context": "in [19] provides a (1 \u2212 1/e)-approximate solution, it has several drawbacks that limit its use in a broader spectrum of applications.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "There is a large body of real-world problems [1,13,14,16,18,20,24,25,32] that are modeled as SM with (1) more than one constraint and (2) more general constraints beyond cardinality constraints.", "startOffset": 45, "endOffset": 72}, {"referenceID": 12, "context": "There is a large body of real-world problems [1,13,14,16,18,20,24,25,32] that are modeled as SM with (1) more than one constraint and (2) more general constraints beyond cardinality constraints.", "startOffset": 45, "endOffset": 72}, {"referenceID": 13, "context": "There is a large body of real-world problems [1,13,14,16,18,20,24,25,32] that are modeled as SM with (1) more than one constraint and (2) more general constraints beyond cardinality constraints.", "startOffset": 45, "endOffset": 72}, {"referenceID": 15, "context": "There is a large body of real-world problems [1,13,14,16,18,20,24,25,32] that are modeled as SM with (1) more than one constraint and (2) more general constraints beyond cardinality constraints.", "startOffset": 45, "endOffset": 72}, {"referenceID": 17, "context": "There is a large body of real-world problems [1,13,14,16,18,20,24,25,32] that are modeled as SM with (1) more than one constraint and (2) more general constraints beyond cardinality constraints.", "startOffset": 45, "endOffset": 72}, {"referenceID": 19, "context": "There is a large body of real-world problems [1,13,14,16,18,20,24,25,32] that are modeled as SM with (1) more than one constraint and (2) more general constraints beyond cardinality constraints.", "startOffset": 45, "endOffset": 72}, {"referenceID": 23, "context": "There is a large body of real-world problems [1,13,14,16,18,20,24,25,32] that are modeled as SM with (1) more than one constraint and (2) more general constraints beyond cardinality constraints.", "startOffset": 45, "endOffset": 72}, {"referenceID": 24, "context": "There is a large body of real-world problems [1,13,14,16,18,20,24,25,32] that are modeled as SM with (1) more than one constraint and (2) more general constraints beyond cardinality constraints.", "startOffset": 45, "endOffset": 72}, {"referenceID": 31, "context": "There is a large body of real-world problems [1,13,14,16,18,20,24,25,32] that are modeled as SM with (1) more than one constraint and (2) more general constraints beyond cardinality constraints.", "startOffset": 45, "endOffset": 72}, {"referenceID": 13, "context": "The d-knapsack constraints are a class of general constraints that have been widely adopted in SM problems [14,16,18,21,24,32].", "startOffset": 107, "endOffset": 126}, {"referenceID": 15, "context": "The d-knapsack constraints are a class of general constraints that have been widely adopted in SM problems [14,16,18,21,24,32].", "startOffset": 107, "endOffset": 126}, {"referenceID": 17, "context": "The d-knapsack constraints are a class of general constraints that have been widely adopted in SM problems [14,16,18,21,24,32].", "startOffset": 107, "endOffset": 126}, {"referenceID": 20, "context": "The d-knapsack constraints are a class of general constraints that have been widely adopted in SM problems [14,16,18,21,24,32].", "startOffset": 107, "endOffset": 126}, {"referenceID": 23, "context": "The d-knapsack constraints are a class of general constraints that have been widely adopted in SM problems [14,16,18,21,24,32].", "startOffset": 107, "endOffset": 126}, {"referenceID": 31, "context": "The d-knapsack constraints are a class of general constraints that have been widely adopted in SM problems [14,16,18,21,24,32].", "startOffset": 107, "endOffset": 126}, {"referenceID": 13, "context": "One example is the Twitter stream monitoring problem in [14], which selects a set of representative tweets such that both the number and the total length of selected tweets are constrained.", "startOffset": 56, "endOffset": 60}, {"referenceID": 20, "context": "The other example is the server resource allocation problem in [21], which considers providing video streams to clients with limited server resources.", "startOffset": 63, "endOffset": 67}, {"referenceID": 1, "context": "Many algorithms [2, 3, 5, 6, 29, 30, 32] have been developed for data stream models.", "startOffset": 16, "endOffset": 40}, {"referenceID": 2, "context": "Many algorithms [2, 3, 5, 6, 29, 30, 32] have been developed for data stream models.", "startOffset": 16, "endOffset": 40}, {"referenceID": 4, "context": "Many algorithms [2, 3, 5, 6, 29, 30, 32] have been developed for data stream models.", "startOffset": 16, "endOffset": 40}, {"referenceID": 5, "context": "Many algorithms [2, 3, 5, 6, 29, 30, 32] have been developed for data stream models.", "startOffset": 16, "endOffset": 40}, {"referenceID": 28, "context": "Many algorithms [2, 3, 5, 6, 29, 30, 32] have been developed for data stream models.", "startOffset": 16, "endOffset": 40}, {"referenceID": 29, "context": "Many algorithms [2, 3, 5, 6, 29, 30, 32] have been developed for data stream models.", "startOffset": 16, "endOffset": 40}, {"referenceID": 31, "context": "Many algorithms [2, 3, 5, 6, 29, 30, 32] have been developed for data stream models.", "startOffset": 16, "endOffset": 40}, {"referenceID": 1, "context": "There have been several techniques [2, 12] for streaming SM, but they can only work with a single cardinality constraint.", "startOffset": 35, "endOffset": 42}, {"referenceID": 11, "context": "There have been several techniques [2, 12] for streaming SM, but they can only work with a single cardinality constraint.", "startOffset": 35, "endOffset": 42}, {"referenceID": 5, "context": "Many practical applications [6, 29, 30] adopt the sliding window [5] model where only the last W elements in the stream are considered to meet the recency requirement.", "startOffset": 28, "endOffset": 39}, {"referenceID": 28, "context": "Many practical applications [6, 29, 30] adopt the sliding window [5] model where only the last W elements in the stream are considered to meet the recency requirement.", "startOffset": 28, "endOffset": 39}, {"referenceID": 29, "context": "Many practical applications [6, 29, 30] adopt the sliding window [5] model where only the last W elements in the stream are considered to meet the recency requirement.", "startOffset": 28, "endOffset": 39}, {"referenceID": 4, "context": "Many practical applications [6, 29, 30] adopt the sliding window [5] model where only the last W elements in the stream are considered to meet the recency requirement.", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "However, SM over sliding windows is still largely unexplored and, to the best of our knowledge, there are only two existing techniques [6,30] on this topic.", "startOffset": 135, "endOffset": 141}, {"referenceID": 29, "context": "However, SM over sliding windows is still largely unexplored and, to the best of our knowledge, there are only two existing techniques [6,30] on this topic.", "startOffset": 135, "endOffset": 141}, {"referenceID": 31, "context": "It improves the state-of-the-art ( 1 1+2d \u2212O(1)) approximation ratio for this problem in [32].", "startOffset": 89, "endOffset": 93}, {"referenceID": 7, "context": "Due to its theoretical consequences, SM has been widely applied to various problems in databases and data mining, including influence maximization [8, 20, 28, 30], data summariza-", "startOffset": 147, "endOffset": 162}, {"referenceID": 19, "context": "Due to its theoretical consequences, SM has been widely applied to various problems in databases and data mining, including influence maximization [8, 20, 28, 30], data summariza-", "startOffset": 147, "endOffset": 162}, {"referenceID": 27, "context": "Due to its theoretical consequences, SM has been widely applied to various problems in databases and data mining, including influence maximization [8, 20, 28, 30], data summariza-", "startOffset": 147, "endOffset": 162}, {"referenceID": 29, "context": "Due to its theoretical consequences, SM has been widely applied to various problems in databases and data mining, including influence maximization [8, 20, 28, 30], data summariza-", "startOffset": 147, "endOffset": 162}, {"referenceID": 1, "context": "tion [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9], recommendations [1, 22, 23, 32], to just name a few.", "startOffset": 5, "endOffset": 28}, {"referenceID": 14, "context": "tion [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9], recommendations [1, 22, 23, 32], to just name a few.", "startOffset": 5, "endOffset": 28}, {"referenceID": 16, "context": "tion [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9], recommendations [1, 22, 23, 32], to just name a few.", "startOffset": 5, "endOffset": 28}, {"referenceID": 17, "context": "tion [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9], recommendations [1, 22, 23, 32], to just name a few.", "startOffset": 5, "endOffset": 28}, {"referenceID": 30, "context": "tion [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9], recommendations [1, 22, 23, 32], to just name a few.", "startOffset": 5, "endOffset": 28}, {"referenceID": 32, "context": "tion [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9], recommendations [1, 22, 23, 32], to just name a few.", "startOffset": 5, "endOffset": 28}, {"referenceID": 3, "context": "tion [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9], recommendations [1, 22, 23, 32], to just name a few.", "startOffset": 59, "endOffset": 65}, {"referenceID": 8, "context": "tion [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9], recommendations [1, 22, 23, 32], to just name a few.", "startOffset": 59, "endOffset": 65}, {"referenceID": 0, "context": "tion [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9], recommendations [1, 22, 23, 32], to just name a few.", "startOffset": 83, "endOffset": 98}, {"referenceID": 21, "context": "tion [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9], recommendations [1, 22, 23, 32], to just name a few.", "startOffset": 83, "endOffset": 98}, {"referenceID": 22, "context": "tion [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9], recommendations [1, 22, 23, 32], to just name a few.", "startOffset": 83, "endOffset": 98}, {"referenceID": 31, "context": "tion [2, 15, 17, 18, 31, 33], top-k representative queries [4, 9], recommendations [1, 22, 23, 32], to just name a few.", "startOffset": 83, "endOffset": 98}, {"referenceID": 1, "context": "This has triggered a large body of research on SM in recent years [2, 6, 11\u201313, 15, 18, 27, 32].", "startOffset": 66, "endOffset": 95}, {"referenceID": 5, "context": "This has triggered a large body of research on SM in recent years [2, 6, 11\u201313, 15, 18, 27, 32].", "startOffset": 66, "endOffset": 95}, {"referenceID": 10, "context": "This has triggered a large body of research on SM in recent years [2, 6, 11\u201313, 15, 18, 27, 32].", "startOffset": 66, "endOffset": 95}, {"referenceID": 11, "context": "This has triggered a large body of research on SM in recent years [2, 6, 11\u201313, 15, 18, 27, 32].", "startOffset": 66, "endOffset": 95}, {"referenceID": 12, "context": "This has triggered a large body of research on SM in recent years [2, 6, 11\u201313, 15, 18, 27, 32].", "startOffset": 66, "endOffset": 95}, {"referenceID": 14, "context": "This has triggered a large body of research on SM in recent years [2, 6, 11\u201313, 15, 18, 27, 32].", "startOffset": 66, "endOffset": 95}, {"referenceID": 17, "context": "This has triggered a large body of research on SM in recent years [2, 6, 11\u201313, 15, 18, 27, 32].", "startOffset": 66, "endOffset": 95}, {"referenceID": 26, "context": "This has triggered a large body of research on SM in recent years [2, 6, 11\u201313, 15, 18, 27, 32].", "startOffset": 66, "endOffset": 95}, {"referenceID": 31, "context": "This has triggered a large body of research on SM in recent years [2, 6, 11\u201313, 15, 18, 27, 32].", "startOffset": 66, "endOffset": 95}, {"referenceID": 26, "context": "Sviridenko [27] first proposes a (1\u2212 1/e) approximation algorithm for SM subject to 1-knapsack constraints with O(n) complexity.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "[11] propose a (1\u2212 1/e\u2212 \u03b5) approximation algorithm for SM subject to d-knapsack constraints with O(n \u22124 ) complexity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] and Lin et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "In this paper, we adapt the algorithm in [15] for SMDK and use the adapted algorithm as the batch baseline.", "startOffset": 41, "endOffset": 45}, {"referenceID": 1, "context": "Next, we review existing streaming SM algorithms [2,6,12,30,32].", "startOffset": 49, "endOffset": 63}, {"referenceID": 5, "context": "Next, we review existing streaming SM algorithms [2,6,12,30,32].", "startOffset": 49, "endOffset": 63}, {"referenceID": 11, "context": "Next, we review existing streaming SM algorithms [2,6,12,30,32].", "startOffset": 49, "endOffset": 63}, {"referenceID": 29, "context": "Next, we review existing streaming SM algorithms [2,6,12,30,32].", "startOffset": 49, "endOffset": 63}, {"referenceID": 31, "context": "Next, we review existing streaming SM algorithms [2,6,12,30,32].", "startOffset": 49, "endOffset": 63}, {"referenceID": 1, "context": "[2] propose a (1/2\u2212\u03b5) approximation algorithm for SM with cardinality constraints in append-only streams with sublinear time and space complexities.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Append-only stream algorithms for SMDK are proposed in [12] and [32].", "startOffset": 55, "endOffset": 59}, {"referenceID": 31, "context": "Append-only stream algorithms for SMDK are proposed in [12] and [32].", "startOffset": 64, "endOffset": 68}, {"referenceID": 4, "context": "The sliding window model [5] is widely adopted in many streaming applications [3, 6, 29, 30] to capture the data recency constraint.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "The sliding window model [5] is widely adopted in many streaming applications [3, 6, 29, 30] to capture the data recency constraint.", "startOffset": 78, "endOffset": 92}, {"referenceID": 5, "context": "The sliding window model [5] is widely adopted in many streaming applications [3, 6, 29, 30] to capture the data recency constraint.", "startOffset": 78, "endOffset": 92}, {"referenceID": 28, "context": "The sliding window model [5] is widely adopted in many streaming applications [3, 6, 29, 30] to capture the data recency constraint.", "startOffset": 78, "endOffset": 92}, {"referenceID": 29, "context": "The sliding window model [5] is widely adopted in many streaming applications [3, 6, 29, 30] to capture the data recency constraint.", "startOffset": 78, "endOffset": 92}, {"referenceID": 4, "context": "Exponential histograms [5] and smooth histograms [3] are common methods to estimate the values of functions over sliding windows.", "startOffset": 23, "endOffset": 26}, {"referenceID": 2, "context": "Exponential histograms [5] and smooth histograms [3] are common methods to estimate the values of functions over sliding windows.", "startOffset": 49, "endOffset": 52}, {"referenceID": 5, "context": "8 in append-only streams [6].", "startOffset": 25, "endOffset": 28}, {"referenceID": 18, "context": "632 unless P = NP [19].", "startOffset": 18, "endOffset": 22}, {"referenceID": 5, "context": "To the best of our knowledge, there are only a few research efforts [6, 30] on SM over sliding windows, but they only focus on cardinality constraints.", "startOffset": 68, "endOffset": 75}, {"referenceID": 29, "context": "To the best of our knowledge, there are only a few research efforts [6, 30] on SM over sliding windows, but they only focus on cardinality constraints.", "startOffset": 68, "endOffset": 75}, {"referenceID": 5, "context": "[6] propose (1/3 \u2212 \u03b5) and (1/2 \u2212 \u03b5) approximation algorithms for SM with cardinality constraints over sliding windows.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "In [30], influence maximization in social streams is defined by SM with cardinality constraints over sliding windows.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "As SM with a cardinality constraint has been proved to be NP-hard [19], SMDK is also NP-hard.", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "The cost-effective greedy algorithm CEGreedy proposed in [15] for SM with 1-knapsack constraints can be adopted for SMDK2.", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "2We do not use the (1\u2212 1/e\u2212 \u03b5) approximation algorithm in [11] as the baseline for its O(nd\u03b5 ) complexity.", "startOffset": 58, "endOffset": 62}, {"referenceID": 4, "context": "We further consider SMDK in the sliding window model [5] to capture the recency constraint.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "1 The Algorithmic Description The KnapStream algorithm uses a threshold-based framework proposed in [2] and [12] for streaming SM.", "startOffset": 100, "endOffset": 103}, {"referenceID": 11, "context": "1 The Algorithmic Description The KnapStream algorithm uses a threshold-based framework proposed in [2] and [12] for streaming SM.", "startOffset": 108, "endOffset": 112}, {"referenceID": 1, "context": "Although having similar basic ideas, KnapStream is different from the algorithms in [2] and [12] in two aspects: (1) the criterion for adding an element considers not only the marginal gain of adding the element but also its costs, i.", "startOffset": 84, "endOffset": 87}, {"referenceID": 11, "context": "Although having similar basic ideas, KnapStream is different from the algorithms in [2] and [12] in two aspects: (1) the criterion for adding an element considers not only the marginal gain of adding the element but also its costs, i.", "startOffset": 92, "endOffset": 96}, {"referenceID": 1, "context": "KnapStream adopts the same candidate solution maintenance strategy as used in [2].", "startOffset": 78, "endOffset": 81}, {"referenceID": 31, "context": "A Comparison with [32].", "startOffset": 18, "endOffset": 22}, {"referenceID": 31, "context": "The algorithm in [32] also uses the threshold-based framework and works for SMDK in append-only streams.", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "1 in [6] and 0.", "startOffset": 5, "endOffset": 8}, {"referenceID": 31, "context": "25 in [32]), where KnapStream has obviously better approximation ratios.", "startOffset": 6, "endOffset": 10}, {"referenceID": 31, "context": "In addition, KnapStream is more robust than the algorithm in [32] when d increases as O( log d \u03b5 ) more candidates are maintained.", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "The idea of maintaining a sequence of checkpoints over sliding windows is inspired by smooth histograms [3].", "startOffset": 104, "endOffset": 107}, {"referenceID": 1, "context": "In this paper, we adopt the widely used Informative Vector Machine (IVM) [2, 6, 7] as the metric for the representativeness of any set of elements S, which is defined as follows: f(S) = 1 2 log det(I + \u03c3KS,S) (5)", "startOffset": 73, "endOffset": 82}, {"referenceID": 5, "context": "In this paper, we adopt the widely used Informative Vector Machine (IVM) [2, 6, 7] as the metric for the representativeness of any set of elements S, which is defined as follows: f(S) = 1 2 log det(I + \u03c3KS,S) (5)", "startOffset": 73, "endOffset": 82}, {"referenceID": 6, "context": "In this paper, we adopt the widely used Informative Vector Machine (IVM) [2, 6, 7] as the metric for the representativeness of any set of elements S, which is defined as follows: f(S) = 1 2 log det(I + \u03c3KS,S) (5)", "startOffset": 73, "endOffset": 82}, {"referenceID": 6, "context": "It has been proved that f(\u00b7) defined by Equation 5 is a monotone submodular function [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "Representative subset selection is formulated as maximizing the IVM function with a cardinality constraint in previous work [2,6].", "startOffset": 124, "endOffset": 129}, {"referenceID": 5, "context": "Representative subset selection is formulated as maximizing the IVM function with a cardinality constraint in previous work [2,6].", "startOffset": 124, "endOffset": 129}, {"referenceID": 1, "context": "75 and \u03c3 = 1 as used in [2].", "startOffset": 24, "endOffset": 27}, {"referenceID": 14, "context": "Many real-world problems such as document summarization [15], Blog watching [26], and resource allocation [21] are modeled as maximum coverage with 1 or d knapsack constraints [10].", "startOffset": 56, "endOffset": 60}, {"referenceID": 25, "context": "Many real-world problems such as document summarization [15], Blog watching [26], and resource allocation [21] are modeled as maximum coverage with 1 or d knapsack constraints [10].", "startOffset": 76, "endOffset": 80}, {"referenceID": 20, "context": "Many real-world problems such as document summarization [15], Blog watching [26], and resource allocation [21] are modeled as maximum coverage with 1 or d knapsack constraints [10].", "startOffset": 106, "endOffset": 110}, {"referenceID": 9, "context": "Many real-world problems such as document summarization [15], Blog watching [26], and resource allocation [21] are modeled as maximum coverage with 1 or d knapsack constraints [10].", "startOffset": 176, "endOffset": 180}, {"referenceID": 13, "context": "We adopt the constraints as used in [14] for our experiments: the number as well as the total length of selected tweets are bounded.", "startOffset": 36, "endOffset": 40}, {"referenceID": 14, "context": "The following approaches are compared in the experiments: \u2022 CEGreedy (CEG): We implement CEGreedy [15] as the batch baseline.", "startOffset": 98, "endOffset": 102}, {"referenceID": 10, "context": "As stated in Section 3, we do not use the algorithm in [11] for its efficiency.", "startOffset": 55, "endOffset": 59}, {"referenceID": 31, "context": "\u2022 Streaming (STR): We implement the algorithm proposed in [32] as the append-only streaming baseline.", "startOffset": 58, "endOffset": 62}], "year": 2017, "abstractText": "Submodular maximization (SM) has become a silver bullet for a broad class of applications such as influence maximization, data summarization, top-k representative queries, and recommendations. In this paper, we study the SM problem in data streams. Most existing algorithms for streaming SM only support the append-only model with cardinality constraints, which cannot meet the requirements of real-world problems considering either the data recency issues or more general d-knapsack constraints. Therefore, we first propose an append-only streaming algorithm KnapStream for SM subject to a d-knapsack constraint (SMDK). Furthermore, we devise the KnapWindow algorithm for SMDK over sliding windows to capture the recency constraints. Theoretically, the proposed algorithms have constant approximation ratios for a fixed number of knapsacks and sublinear complexities. We finally evaluate the efficiency and effectiveness of our algorithms in two real-world datasets. The results show that the proposed algorithms achieve two orders of magnitude speedups over the greedy baseline in the batch setting while preserving high quality solutions.", "creator": "LaTeX with hyperref package"}}}