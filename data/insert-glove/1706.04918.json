{"id": "1706.04918", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Robust Submodular Maximization: A Non-Uniform Partitioning Approach", "abstract": "takawira We study the problem of dalr maximizing oyarbide a dufault monotone loudest submodular function subject misys to a rawls cardinality re-integrated constraint $ 62.10 k $, liquidated with liese the coonabarabran added twist that choephel a zinga number bonum of items $ \\ sogang tau $ maroleng from porzecanski the boarding returned set chunyun may amuri be removed. We jerel focus re-discovered on 45m the compress worst - warbeck case setting gourevitch considered hofjes in (chularat Orlin finnigan et rings al. , 2016 ), 222.7 in yugur which a constant - maurepas factor rutnam approximation 17km guarantee 1,4-butanediol was elterman given fansite for $ \\ tau = o (\\ herer sqrt {k} ) $. kurumi In this i&m paper, aspidistra we borakove solve a samyukta key open spectre problem raised therein, herard presenting babylonia a 90.00 new Partitioned mcafee.com Robust (PRo) x\u01b0\u01a1ng submodular ramification maximization oakden algorithm that christlike achieves analco the same byetta guarantee for tieup more general $ \\ desir\u00e9 tau = dst o (jotunheimen k) $. Our algorithm mutinied constructs partitions consisting of fearfully buckets disruptor with whitecourt exponentially ponoka increasing sunlike sizes, 1/3rd and oyuki applies nex\u00f8 standard brindle submodular optimization cryptid subroutines on the buckets in order acrocorinth to bassols construct the icsu robust solution. We azizollah numerically beckhams demonstrate the performance wfnx of adelaja PRo 1,136 in sorokin data vahl summarization stada and influence staley maximization, 6-30 demonstrating cliques gains candyman over both pretyman the booterstown greedy bummer algorithm 2005-2012 and trailered the algorithm koop of (Orlin et tati al. , plainfield 2016 ).", "histories": [["v1", "Thu, 15 Jun 2017 15:15:10 GMT  (2108kb,D)", "http://arxiv.org/abs/1706.04918v1", "Accepted to ICML 2017"]], "COMMENTS": "Accepted to ICML 2017", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["ilija bogunovic", "slobodan mitrovic", "jonathan scarlett", "volkan cevher"], "accepted": true, "id": "1706.04918"}, "pdf": {"name": "1706.04918.pdf", "metadata": {"source": "META", "title": "Robust Submodular Maximization:  A Non-Uniform Partitioning Approach", "authors": ["Ilija Bogunovic", "Slobodan Mitrovi\u0107", "Jonathan Scarlett", "Volkan Cevher"], "emails": ["<ilija.bogunovic@epfl.ch>,", "<slobodan.mitrovic@epfl.ch>,", "<jonathan.scarlett@epfl.ch>,", "<volkan.cevher@epfl.ch>."], "sections": [{"heading": null, "text": "\u221a k).\nIn this paper, we solve a key open problem raised therein, presenting a new Partitioned Robust (PRO) submodular maximization algorithm that achieves the same guarantee for more general \u03c4 = o(k). Our algorithm constructs partitions consisting of buckets with exponentially increasing sizes, and applies standard submodular optimization subroutines on the buckets in order to construct the robust solution. We numerically demonstrate the performance of PRO in data summarization and influence maximization, demonstrating gains over both the greedy algorithm and the algorithm of (Orlin et al., 2016)."}, {"heading": "1. Introduction", "text": "Discrete optimization problems arise frequently in machine learning, and are often NP-hard even to approximate. In the case of a set function exhibiting submodularity, one can efficiently perform maximization subject to cardinality constraints with a ( 1\u2212 1e ) -factor approximation guarantee. Applications include influence maximization (Kempe et al., 2003), document summarization (Lin & Bilmes, 2011), sensor placement (Krause & Guestrin, 2007), and active learning (Krause & Golovin, 2012), just to name a few.\n1LIONS, EPFL, Switzerland 2LTHC, EPFL, Switzerland. Correspondence to: Ilija Bogunovic <ilija.bogunovic@epfl.ch>, Slobodan Mitrovic\u0301 <slobodan.mitrovic@epfl.ch>, Jonathan Scarlett <jonathan.scarlett@epfl.ch>, Volkan Cevher <volkan.cevher@epfl.ch>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nIn many applications of interest, one requires robustness in the solution set returned by the algorithm, in the sense that the objective value degrades as little as possible when some elements of the set are removed. For instance, (i) in influence maximization problems, a subset of the chosen users may decide not to spread the word about a product; (ii) in summarization problems, a user may choose to remove some items from the summary due to their personal preferences; (iii) in the problem of sensor placement for outbreak detection, some of the sensors might fail.\nIn situations where one does not have a reasonable prior distribution on the elements removed, or where one requires robustness guarantees with a high level of certainty, protecting against worst-case removals becomes important. This setting results in the robust submodular function maximization problem, in which we seek to return a set of cardinality k that is robust with respect to the worst-case removal of \u03c4 elements.\nThe robust problem formulation was first introduced in (Krause et al., 2008), and was further studied in (Orlin et al., 2016). In fact, (Krause et al., 2008) considers a more general formulation where a constant-factor approximation guarantee is impossible in general, but shows that one can match the optimal (robust) objective value for a given set size at the cost of returning a set whose size is larger by a logarithmic factor. In contrast, (Orlin et al., 2016) designs an algorithm that obtains the first constant-factor approximation guarantee to the above problem when \u03c4 = o( \u221a k). A key difference between the two frameworks is that the algorithm complexity is exponential in \u03c4 in (Krause et al., 2008), whereas the algorithm of (Orlin et al., 2016) runs in polynomial time.\nContributions. In this paper, we solve a key open problem posed in (Orlin et al., 2016), namely, whether a constantfactor approximation guarantee is possible for general \u03c4 = o(k), as opposed to only \u03c4 = o( \u221a k). We answer this question in the affirmative, providing a new Partitioned Robust (PRO) submodular maximization algorithm that attains a constant-factor approximation guarantee; see Table 1 for comparison of different algorithms for robust monotone submodular optimization with a cardinality constraint.\nar X\niv :1\n70 6.\n04 91\n8v 1\n[ st\nat .M\nL ]\n1 5\nJu n\n20 17\nAchieving this result requires novelty both in the algorithm and its mathematical analysis: While our algorithm bears some similarity to that of (Orlin et al., 2016), it uses a novel structure in which the constructed set is arranged into partitions consisting of buckets whose sizes increase exponentially with the partition index. A key step in our analysis provides a recursive relationship between the objective values attained by buckets appearing in adjacent partitions.\nIn addition to the above contributions, we provide the first empirical study beyond what is demonstrated for \u03c4 = 1 in (Krause et al., 2008). We demonstrate several scenarios in which our algorithm outperforms both the greedy algorithm and the algorithm of (Orlin et al., 2016)."}, {"heading": "2. Problem Statement", "text": "Let V be a ground set with cardinality |V | = n, and let f : 2V \u2192 R\u22650 be a set function defined on V . The function f is said to be submodular if for any sets X \u2286 Y \u2286 V and any element e \u2208 V \\ Y , it holds that\nf(X \u222a {e})\u2212 f(X) \u2265 f(Y \u222a {e})\u2212 f(Y ).\nWe use the following notation to denote the marginal gain in the function value due to adding the elements of a set Y to the set X:\nf(Y |X) := f(X \u222a Y )\u2212 f(X).\nIn the case that Y is a singleton of the form {e}, we adopt the shorthand f(e|X). We say that f is monotone if for any sets X \u2286 Y \u2286 V we have f(X) \u2264 f(Y ), and normalized if f(\u2205) = 0.\nThe problem of maximizing a normalized monotone submodular function subject to a cardinality constraint, i.e.,\nmax S\u2286V,|S|\u2264k f(S), (1)\nhas been studied extensively. A celebrated result of (Nemhauser et al., 1978) shows that a simple greedy algorithm that starts with an empty set and then iteratively adds elements with highest marginal gain provides a (1\u2212 1/e)-approximation.\nIn this paper, we consider the following robust version of (1), introduced in (Krause et al., 2008):\nmax S\u2286V,|S|\u2264k min Z\u2286S,|Z|\u2264\u03c4\nf(S \\ Z) (2)\nWe refer to \u03c4 as the robustness parameter, representing the size of the subset Z that is removed from the selected set S. Our goal is to find a set S such that it is robust upon the worst possible removal of \u03c4 elements, i.e., after the removal, the objective value should remain as large as possible. For \u03c4 = 0, our problem reduces to Problem (1).\nThe greedy algorithm, which is near-optimal for Problem (1) can perform arbitrarily badly for Problem (2). As an elementary example, let us fix \u2208 [0, n\u2212 1) and n \u2265 0, and consider the non-negative monotone submodular function given in Table 2. For k = 2, the greedy algorithm selects {s1, s2}. The set that maximizes mins\u2208S f(S\\s) (i.e., \u03c4 = 1) is {s1, s3}. For this set, mins\u2208{s1,s2} f({s1, s2} \\ s) = n \u2212 1, while for the greedy set the robust objective value is . As a result, the greedy algorithm can perform arbitrarily worse.\nIn our experiments on real-world data sets (see Section 5), we further explore the empirical behavior of the greedy solution in the robust setting. Among other things, we observe that the greedy solution tends to be less robust when the objective value largely depends on the first few elements selected by the greedy rule.\nRelated work. (Krause et al., 2008) introduces the following generalization of (2):\nmax S\u2286V,|S|\u2264k min i\u2208{1,\u00b7\u00b7\u00b7 ,n} fi(S), (3)\nwhere fi are normalized monotone submodular functions. The authors show that this problem is inapproximable in general, but propose an algorithm SATURATE which, when applied to (2), returns a set of size k(1+\u0398(log(\u03c4k log n))) whose robust objective is at least as good as the optimal size-k set. SATURATE requires a number of function evaluations that is exponential in \u03c4 , making it very expensive to run even for small values. The work of (Powers et al., 2016) considers the same problem for different types of submodular constraints.\nRecently, robust versions of submodular maximization have been applied to influence maximization. In (He & Kempe, 2016), the formulation (3) is used to optimize a worst-case approximation ratio. The confidence interval setting is considered in (Chen et al., 2016), where two runs of the GREEDY algorithm (one pessimistic and one optimistic) are used to optimize the same ratio. By leveraging connections to continuous submodular optimization, (Staib & Jegelka, 2017) studies a related continuous robust budget allocation problem.\n(Orlin et al., 2016) considers the formulation in (2), and provides the first constant 0.387-factor approximation result, valid for \u03c4 = o( \u221a k). The algorithm proposed therein, which we refer to via the authors surnames as OSU, uses the greedy algorithm (henceforth referred to as GREEDY) as a sub-routine \u03c4 + 1 times. On each iteration, GREEDY is applied on the elements that are not yet selected on previous iterations, with these previously-selected elements ignored in the objective function. In the first \u03c4 runs, each solution is of size \u03c4 log k, while in the last run, the solution is of size k\u2212\u03c42 log k. The union of all the obtained disjoint solutions leads to the final solution set."}, {"heading": "3. Applications", "text": "In this section, we provide several examples of applications where the robustness of the solution is favorable. The objective functions in these applications are non-negative, monotone and submodular, and are used in our numerical experiments in Section 5.\nRobust influence maximization. The goal in the influence maximization problem is to find a set of k nodes (i.e., a targeted set) in a network that maximizes some measure of influence. For example, this problem appears in viral marketing, where companies wish to spread the word of a new product by targeting the most influential individuals in a social network. Due to poor incentives or dissatisfaction with the product, for instance, some of the users from the\ntargeted set might make the decision not to spread the word about the product.\nFor many of the existing diffusion models used in the literature (e.g., see (Kempe et al., 2003)), given the targeted set S, the expected number of influenced nodes at the end of the diffusion process is a monotone and submodular function of S (He & Kempe, 2016). For simplicity, we consider a basic model in which all of the neighbors of the users in S become influenced, as well as those in S itself.\nMore formally, we are given a graph G = (V,E), where V stands for nodes and E are the edges. For a set S, letN (S) denote all of its neighboring nodes. The goal is to solve the robust dominating set problem, i.e., to find a set of nodes S of size k that maximizes\nmin |RS |\u2264\u03c4,RS\u2286S\n|(S \\RS) \u222aN (S \\RS)|, (4)\nwhere RS \u2286 S represents the users that decide not to spread the word. The non-robust version of this objective function has previously been considered in several different works, such as (Mirzasoleiman et al., 2015b) and (NorouziFard et al., 2016).\nRobust personalized image summarization. In the personalized image summarization problem, a user has a collection of images, and the goal is to find k images that are representative of the collection.\nAfter being presented with a solution, the user might decide to remove a certain number of images from the representative set due to various reasons (e.g., bad lighting, motion blur, etc.). Hence, our goal is to find a set of images that remain good representatives of the collection even after the removal of some number of them.\nOne popular way of finding a representative set in a massive dataset is via exemplar based clustering, i.e., by minimizing the sum of pairwise dissimilarities between the exemplars S and the elements of the data set V . This problem can be posed as a submodular maximization problem subject to a cardinality constraint; cf., (Lucic et al., 2016).\nHere, we are interested in solving the robust summarization problem, i.e., we want to find a set of images S of size k that maximizes\nmin |RS |\u2264\u03c4,RS\u2286S\nf({e0})\u2212 f((S \\RS) \u222a {e0}), (5)\nwhere e0 is a reference element and f(S) = 1 |V | \u2211 v\u2208V mins\u2208S d(s, v) is the k-medoid loss function, and where d(s, v) measures the dissimilarity between images s and v.\nFurther potential applications not covered here include robust sensor placement (Krause et al., 2008), robust protection of networks (Bogunovic & Krause, 2012), and robust feature selection (Globerson & Roweis, 2006)."}, {"heading": "4. Algorithm and its Guarantees", "text": ""}, {"heading": "4.1. The algorithm", "text": "Our algorithm, which we call the Partitioned Robust (PRO) submodular maximization algorithm, is presented in Algorithm 1. As the input, we require a non-negative monotone submodular function f : 2V \u2192 R\u22650, the ground set of elements V , and an optimization subroutine A. The subroutineA(k\u2032, V \u2032) takes a cardinality constraint k\u2032 and a ground set of elements V \u2032. Below, we describe the properties of A that are used to obtain approximation guarantees.\nThe output of the algorithm is a set S \u2286 V of size k that is robust against the worst-case removal of \u03c4 elements. The returned set consists of two sets S0 and S1, illustrated in Figure 1. S1 is obtained by running the subroutine A on V \\ S0 (i.e., ignoring the elements already placed into S0), and is of size k \u2212 |S0|.\nWe refer to the set S0 as the robust part of the solution set S. It consists of dlog \u03c4e + 1 partitions, where every partition i \u2208 {0, \u00b7 \u00b7 \u00b7 , dlog \u03c4e} consists of d\u03c4/2ie buckets Bj , j \u2208 {1, \u00b7 \u00b7 \u00b7 , d\u03c4/2ie}. In partition i, every bucket contains 2i\u03b7 elements, where \u03b7 \u2208 N+ is a parameter that is arbitrary for now; we use \u03b7 = log2 k in our asymptotic theory, but our numerical studies indicate that even \u03b7 = 1 works well in practice. Each bucket Bj is created afresh by using the subroutine A on V \\ S0,prev, where S0,prev contains all elements belonging to the previous buckets.\nThe following proposition bounds the cardinality of S0, and is proved in the supplementary material.\nProposition 4.1 Fix k \u2265 \u03c4 and \u03b7 \u2208 N+. The size of the robust part S0 constructed in Algorithm 1 is\n|S0| = dlog \u03c4e\u2211 i=0 d\u03c4/2ie2i\u03b7 \u2264 3\u03b7\u03c4(log k + 2).\nThis proposition reveals that the feasible values of \u03c4 (i.e., those with |S0| \u2264 k) can be as high as O ( k \u03b7\u03c4 ) . We will later set \u03b7 = O(log2 k), thus permitting all \u03c4 = o(k) up to a few logarithmic factors. In contrast, we recall that the algorithm OSU proposed in (Orlin et al., 2016) adopts a simpler approach where a robust set is used consisting of \u03c4 buckets of equal size \u03c4 log k, thereby only permitting the scaling \u03c4 = o( \u221a k).\nWe provide the following intuition as to why PRO succeeds despite having a smaller size for S0 compared to the algorithm given in (Orlin et al., 2016). First, by the design of the partitions, there always exists a bucket in partition i that at most 2i items are removed from. The bulk of our analysis is devoted to showing that the union of these buckets yields a sufficiently high objective value. While the earlier\nAlgorithm 1 Partitioned Robust Submodular optimization algorithm (PRO)\nRequire: Set V , k, \u03c4 , \u03b7 \u2208 N+, algorithm A Ensure: Set S \u2286 V such that |S| \u2264 k\n1: S0, S1 \u2190 \u2205 2: for i\u2190 0 to dlog \u03c4e do 3: for j \u2190 1 to d\u03c4/2ie do 4: Bj \u2190 A (2i\u03b7, (V \\ S0)) 5: S0 \u2190 S0 \u222aBj 6: S1 \u2190 A (k \u2212 |S0|, (V \\ S0)) 7: S \u2190 S0 \u222a S1 8: return S\nbuckets have a smaller size, they also have a higher objective value per item due to diminishing returns, and our analysis quantifies and balances this trade-off. Similarly, our analysis quantifies the trade-off between how much the adversary can remove from the (typically large) set S1 and the robust part S0."}, {"heading": "4.2. Subroutine and assumptions", "text": "PRO accepts a subroutine A as the input. We consider a class of algorithms that satisfy the \u03b2-iterative property, defined below. We assume that the algorithm outputs the final set in some specific order (v1, . . . , vk), and we refer to vi as the i-th output element.\nDefinition 4.2 Consider a normalized monotone submodular set function f on a ground set V , and an algorithmA. Given any set T \u2286 V and size k, suppose that A outputs an ordered set (v1, . . . , vk) when applied to T , and define Ai(T ) = {v1, . . . , vi} for i \u2264 k. We say that A satisfies the \u03b2-iterative property if\nf(Ai+1(T ))\u2212 f(Ai(T )) \u2265 1\n\u03b2 max v\u2208T\nf(v|Ai(T )). (6)\nIntuitively, (6) states that in every iteration, the algorithm adds an element whose marginal gain is at least a 1/\u03b2 fraction of the maximum marginal. This necessarily requires that \u03b2 \u2265 1.\nExamples. Besides the classic greedy algorithm, which satisfies (6) with \u03b2 = 1, a good candidate for our subroutine is THRESHOLDING-GREEDY (Badanidiyuru & Vondra\u0301k, 2014), which satisfies the \u03b2-iterative property with \u03b2 = 1/(1\u2212 ). This decreases the number of function evaluations to O(n/ log n/ ).\nSTOCHASTIC-GREEDY (Mirzasoleiman et al., 2015a) is another potential subroutine candidate. While it is unclear whether this algorithm satisfies the \u03b2-iterative property, it requires an even smaller number of function eval-\nuations, namely, O(n log 1/ ). We will see in Section 5 that PRO performs well empirically when used with this subroutine. We henceforth refer to PRO used along with its appropriate subroutine as PRO-GREEDY, PRO-THRESHOLDING-GREEDY, and so on.\nProperties. The following lemma generalizes a classical property of the greedy algorithm (Nemhauser et al., 1978; Krause & Golovin, 2012) to the class of algorithms satisfying the \u03b2-iterative property. Here and throughout the paper, we use OPT(k, V ) to denote the following optimal set for non-robust maximization:\nOPT(k, V ) \u2208 argmax S\u2286V,|S|=k f(S),\nLemma 4.3 Consider a normalized monotone submodular function f : 2V \u2192 R\u22650 and an algorithm A(T ), T \u2286 V , that satisfies the \u03b2-iterative property in (6). Let Al(T ) denote the set returned by the algorithm A(T ) after l iterations. Then for all k, l \u2208 N+\nf(Al(T )) \u2265 ( 1\u2212 e\u2212 l \u03b2k ) f(OPT(k, T )). (7)\nWe will also make use of the following property, which is implied by the \u03b2-iterative property.\nProposition 4.4 Consider a submodular set function f : 2V \u2192 R\u22650 and an algorithm A that satisfies the \u03b2iterative property for some \u03b2 \u2265 1. Then, for any T \u2286 V and element e \u2208 V \\ A(T ), we have\nf(e|A(T )) \u2264 \u03b2 f(A(T )) k . (8)\nIntuitively, (8) states that the marginal gain of any nonselected element cannot be more than \u03b2 times the average objective value of the selected elements. This is one of the rules used to define the \u03b2-nice class of algorithms in (Mirrokni & Zadimoghaddam, 2015); however, we note that in general, neither the \u03b2-nice nor \u03b2-iterative classes are a subset of one another."}, {"heading": "4.3. Main result: Approximation guarantee", "text": "For the robust maximization problem, we let OPT(k, V, \u03c4) denote the optimal set:\nOPT(k, V, \u03c4) \u2208 argmax S\u2286V,|S|=k min E\u2286S,|E|\u2264\u03c4 f(S \\ E).\nMoreover, for a set S, we let E\u2217S denote the minimizer\nE\u2217S \u2208 argmin E\u2286S,|E|\u2264\u03c4 f(S \\ E). (9)\nWith these definitions, the main theoretical result of this paper is as follows.\nTheorem 4.5 Let f be a normalized monotone submodular function, and let A be a subroutine satisfying the \u03b2iterative property. For a given budget k and parameters 2 \u2264 \u03c4 \u2264 k3\u03b7(log k+2) and \u03b7 \u2265 4(log k + 1), PRO returns a set S of size k such that\nf(S \\ E\u2217S) \u2265 \u03b7 5\u03b23dlog \u03c4e+\u03b7\n( 1\u2212 e\u2212 k\u2212|S0| \u03b2(k\u2212\u03c4) ) 1 + \u03b75\u03b23dlog \u03c4e+\u03b7 ( 1\u2212 e\u2212 k\u2212|S0| \u03b2(k\u2212\u03c4)\n) \u00d7 f(OPT(k, V, \u03c4) \\ E\u2217OPT(k,V,\u03c4)), (10)\nwhere E\u2217S and E \u2217 OPT(k,V,\u03c4) are defined as in (9). In addition, if \u03c4 = o (\nk \u03b7 log k\n) and \u03b7 \u2265 log2 k, then we\nhave the following as k \u2192\u221e:\nf(S \\ E\u2217S) \u2265 ( 1\u2212 e\u22121/\u03b2\n2\u2212 e\u22121/\u03b2 + o(1) ) \u00d7 f(OPT(k, V, \u03c4) \\ E\u2217OPT(k,V,\u03c4)). (11)\nIn particular, PRO-GREEDY achieves an asymptotic approximation factor of at least 0.387, and PROTHRESHOLDING-GREEDY with parameter achieves an asymptotic approximation factor of at least 0.387(1\u2212 ).\nThis result solves an open problem raised in (Orlin et al., 2016), namely, whether a constant-factor approximation guarantee can be obtained for \u03c4 = o(k) as opposed to\nonly \u03c4 = o (\u221a k ) . In the asymptotic limit, our constant factor of 0.387 for the greedy subroutine matches that of (Orlin et al., 2016), but our algorithm permits significantly \u201chigher robustness\u201d in the sense of allowing larger \u03c4 values. To achieve this, we require novel proof techniques, which we now outline."}, {"heading": "4.4. High-level overview of the analysis", "text": "The proof of Theorem 4.5 is provided in the supplementary material. Here we provide a high-level overview of the main challenges.\nLet E denote a cardinality-\u03c4 subset of the returned set S that is removed. By the construction of the partitions, it is easy to verify that each partition i contains a bucket from which at most 2i items are removed. We denote these by B0, . . . , Bdlog \u03c4e, and write EBi := E \u2229Bi. Moreover, we define E0 := E \u2229 S0 and E1 := E \u2229 S1.\nWe establish the following lower bound on the final objective function value:\nf(S \\E) \u2265 max { f(S0 \\E0), f(S1)\u2212 f(E1|(S \\E)),\nf ( dlog \u03c4e\u22c3 i=0 (Bi \\ EBi) )} . (12)\nThe arguments to the first and third terms are trivially seen to be subsets of S \\ E, and the second term represents the utility of the set S1 subsided by the utility of the elements removed from S1.\nThe first two terms above are easily lower bounded by convenient expressions via submodular and the \u03b2-iterative property. The bulk of the proof is dedicated to bounding the third term. To do this, we establish the following recursive relations with suitably-defined \u201csmall\u201d values of \u03b1j :\nf ( j\u22c3 i=0 (Bi \\ EBi) ) \u2265 ( 1\u2212 1 1 + 1\u03b1j ) f(Bj)\nf ( EBj \u2223\u2223\u2223 j\u22121\u22c3 i=0 (Bi \\ EBi) ) \u2264 \u03b1jf ( j\u22121\u22c3 i=0 (Bi \\ EBi) ) .\nIntuitively, the first equation shows that the objective value from buckets i = 0, . . . , j with removals cannot be too much smaller than the objective value in bucket j without removals, and the second equation shows that the loss in bucket j due to the removals is at most a small fraction of the objective value from buckets 0, . . . , j\u22121. The proofs of both the base case of the induction and the inductive step make use of submodularity properties and the \u03b2-iterative property (cf., Definition 4.2).\nOnce the suitable lower bounds are obtained for the terms in (12), the analysis proceeds similarly to (Orlin et al.,\n2016). Specifically, we can show that as the second term increases, the third term decreases, and accordingly lower bound their maximum by the value obtained when the two are equal. A similar balancing argument is then applied to the resulting term and the first term in (12).\nThe condition \u03c4 \u2264 k3\u03b7(log k+2) follows directly from Proposition 4.1; namely, it is a sufficient condition for |S0| \u2264 k, as is required by PRO."}, {"heading": "5. Experiments", "text": "In this section, we numerically validate the performance of PRO and the claims given in the preceding sections. In particular, we compare our algorithm against the OSU algorithm proposed in (Orlin et al., 2016) on different datasets and corresponding objective functions (see Table 3). We demonstrate matching or improved performance in a broad range of settings, as well as observing that PRO can be implemented with larger values of \u03c4 , corresponding to a greater robustness. Moreover, we show that for certain realworld data sets, the classic GREEDY algorithm can perform badly for the robust problem. We do not compare against SATURATE (Krause et al., 2008), due to its high computational cost for even a small \u03c4 .\nSetup. Given a solution set S of size k, we measure the performance in terms of the minimum objective value upon the worst-case removal of \u03c4 elements, i.e. minZ\u2286S,|Z|\u2264\u03c4 f(S\\ Z). Unfortunately, for a given solution set S, finding such a set Z is an instance of the submodular minimization problem with a cardinality constraint,1 which is known to be NP-hard with polynomial approximation factors (Svitkina & Fleischer, 2011). Hence, in our experiments, we only implement the optimal \u201cadversary\u201d (i.e., removal of items) for small to moderate values of \u03c4 and k, for which we use a fast C++ implementation of branch-and-bound.\nDespite the difficulty in implementing the optimal adversary, we observed in our experiments that the greedy adversary, which iteratively removes elements to reduce the objective value as much as possible, has a similar impact on the objective compared to the optimal adversary for the data sets considered. Hence, we also provide a larger-scale experiment in the presence of a greedy adversary. Throughout, we write OA and GA to abbreviate the optimal adversary and greedy adversary, respectively.\nIn our experiments, the size of the robust part of the solution set (i.e., |S0|) is set to \u03c42 and \u03c4 log \u03c4 for OSU and PRO, respectively. That is, we set \u03b7 = 1 in PRO, and similarly ignore constant and logarithmic factors in OSU, since both appear to be unnecessary in practice. We show\n1This can be seen by noting that for submodular f and any Z \u2286 X \u2286 V , f \u2032(Z) = f(X \\ Z) remains submodular.\nboth the \u201craw\u201d objective values of the solutions, as well as the objective values after the removal of \u03c4 elements. In all experiments, we implement GREEDY using the LAZYGREEDY implementation given in (Minoux, 1978).\nThe objective functions shown in Table 3 are given in Section 3. For the exemplar objective function, we use d(s, v) = \u2016s\u2212 v\u20162, and let the reference element e0 be the zero vector. Instead of using the whole set V , we approximate the objective by considering a smaller random subset of V for improved computational efficiency. Since the objective is additively decomposable and bounded, standard concentration bounds (e.g., the Chernoff bound) ensure that the empirical mean over a random subsample can be made arbitrarily accurate.\nData sets. We consider the following datasets, along with the objective functions given in Section 3:\nResults. In the first set of experiments, we compare PROGREEDY (written using the shorthand PRO-GR in the legend) against GREEDY and OSU on the EGO-FACEBOOK and EGO-TWITTER datasets. In this experiment, the dominating set selection objective in (4) is considered. Figure 2 (a) and (c) show the results before and after the worst-case removal of \u03c4 = 7 elements for different values of k. In Figure 2 (b) and (d), we show the objective value for fixed k = 50 and k = 100, respectively, while the robustness parameter \u03c4 is varied.\nGREEDY achieves the highest raw objective value, followed by PRO-GREEDY and OSU. However, after the worst-case removal, PRO-GREEDY-OA outperforms both OSU-OA and GREEDY-OA. In Figure 2 (a) and (b), GREEDY-OA performs poorly due to a high concentration of the objective value on the first few elements selected by GREEDY. While OSU requires k \u2265 \u03c42, PRO only requires k \u2265 \u03c4 log \u03c4 , and hence it can be run for larger values of \u03c4 (e.g., see Figure 2 (b) and (c)). Moreover, in Figure 2 (a) and (b), we can observe that although PRO uses a smaller number of elements to build the robust part of the solution set, it has better robustness in comparison with OSU.\nIn the second set of experiments, we perform the same type of comparisons on the TINY10 and CM-MOLECULES datasets. The exemplar based clustering in (5) is used as the objective function. In Figure 2 (e) and (h), the robustness parameter is fixed to \u03c4 = 7 and \u03c4 = 6, respectively, while the cardinality k is varied. In Figure 2 (f) and (h), the cardinality is fixed to k = 100 and k = 50, respectively, while the robustness parameter \u03c4 is varied.\nAgain, GREEDY achieves the highest objective value. On the TINY10 dataset, GREEDY-OA (Figure 2 (e) and (f)) has a large gap between the raw and final objective, but it still slightly outperforms PRO-GREEDY-OA. This demonstrates that GREEDY can work well in some cases, despite failing in others. We observed that it succeeds here because the objective value is relatively more uniformly spread across the selected elements. On the same dataset, PRO-GREEDY-OA outperforms OSU-OA. On our second dataset CM-MOLECULES (Figure 2 (g) and (h)), PROGREEDY-OA achieves the highest robust objective value, followed by OSU-OA and GREEDY-OA.\nIn our final experiment (see Figure 2 (i)), we compare the performance of PRO-GREEDY against two instances of PRO-STOCHASTIC-GREEDY with = 0.01 and = 0.08 (shortened to PRO-ST in the legend), seeking to understand to what extent using the more efficient stochastic subroutine impacts the performance. We also show the performance of OSU. In this experiment, we fix k = 100 and vary \u03c4 . We use the greedy adversary instead of the optimal one, since the latter becomes computationally challenging for larger values of \u03c4 .\nIn Figure 2 (i), we observe a slight decrease in the objective value of PRO-STOCHASTIC-GREEDY due to the stochastic optimization. On the other hand, the gaps between the robust and non-robust solutions remain similar, or even shrink. Overall, we observe that at least in this example, the stochastic subroutine does not compromise the quality of the solution too significantly, despite having a lower computational complexity."}, {"heading": "6. Conclusion", "text": "We have provided a new Partitioned Robust (PRO) submodular maximization algorithm attaining a constantfactor approximation guarantee for general \u03c4 = o(k), thus\nresolving an open problem posed in (Orlin et al., 2016). Our algorithm uses a novel partitioning structure with partitions consisting of buckets with exponentially decreasing size, thus providing a \u201crobust part\u201d of size O(\u03c4poly log \u03c4). We have presented a variety of numerical experiments where PRO outperforms both GREEDY and OSU. A potentially interesting direction for further research is to understand the linear regime, in which \u03c4 = \u03b1k for some constant \u03b1 \u2208 (0, 1), and in particular, to seek a constant-factor guarantee for this regime.\nAcknowledgment. This work was supported in part by the European Commission under Grant ERC Future Proof, SNF 200021-146750 and SNF CRSII2-147633, and \u2018EPFL Fellows\u2019 (Horizon2020 665667)."}, {"heading": "A. Proof of Proposition 4.1", "text": "We have\n|S0| = dlog \u03c4e\u2211 i=0 d\u03c4/2ie2i\u03b7\n\u2264 dlog \u03c4e\u2211 i=0 ( \u03c4 2i + 1 ) 2i\u03b7 \u2264 \u03b7(dlog \u03c4e+ 1)(\u03c4 + 2dlog \u03c4e) \u2264 3\u03b7\u03c4(dlog \u03c4e+ 1) \u2264 3\u03b7\u03c4(log k + 2)."}, {"heading": "B. Proof of Proposition 4.4", "text": "Recalling that Aj(T ) denotes a set constructed by the algorithm after j iterations, we have\nf(Aj(T ))\u2212 f(Aj\u22121(T )) \u2265 1\n\u03b2 max e\u2208T\nf(e|Aj\u22121(T ))\n\u2265 1 \u03b2 max e\u2208T f(e|Ak(T ))\n\u2265 1 \u03b2 max e\u2208T\\Ak(T ) f(e|Ak(T )), (13)\nwhere the first inequality follows from the \u03b2-iterative property (6), and the second inequality follows from Aj\u22121(S) \u2286 Ak(S) and the submodularity of f .\nContinuing, we have\nf(Ak(T )) = k\u2211 j=1 f(Aj(T ))\u2212 f(Aj\u22121(T ))\n\u2265 k \u03b2 max e\u2208T\\Ak(T ) f(e|Ak(T )),\nwhere the last inequality follows from (13).\nBy rearranging, we have for any e \u2208 T \\ Ak(T ) that\nf(e|Ak(T )) \u2264 \u03b2 f(Ak(T ))\nk ."}, {"heading": "C. Proof of Lemma 4.3", "text": "Recalling that Aj(T ) denotes the set constructed after j iterations when applied to T , we have\nmax e\u2208T\\Aj\u22121(T )\nf(e|Aj\u22121(T )) \u2265 1\nk \u2211 e\u2208OPT(k,T )\\Aj\u22121(T ) f(e|Aj\u22121(T ))\n\u2265 1 k f(OPT(k, T )|Aj\u22121(T )) \u2265 1 k ( f(OPT(k, T ))\u2212 f(Aj\u22121(T )) ) , (14)\nwhere the first line holds since the maximum is lower bounded by the average, the line uses submodularity, and the last line uses monotonicity.\nBy combining the \u03b2-iterative property with (14), we obtain\nf(Aj(T ))\u2212 f(Aj\u22121(T )) \u2265 1\n\u03b2 max e\u2208T\\Aj\u22121(T ) f(e|Aj\u22121(T ))\n\u2265 1 k\u03b2\n( f(OPT(k, T ))\u2212 f(Aj\u22121(T )) ) .\nBy rearranging, we obtain f(OPT(k, T ))\u2212 f(Aj\u22121(T )) \u2264 \u03b2k ( f(Aj(T ))\u2212 f(Aj\u22121(T )) ) . (15)\nWe proceed by following the steps from the proof of Theorem 1.5 in (Krause & Golovin, 2012). Defining \u03b4j := f(OPT(k, T ))\u2212 f(Aj(T )), we can rewrite (15) as \u03b4j\u22121 \u2264 \u03b2k(\u03b4j\u22121 \u2212 \u03b4j). By rearranging, we obtain\n\u03b4j \u2264 (\n1\u2212 1 \u03b2k\n) \u03b4j\u22121.\nApplying this recursively, we obtain \u03b4l \u2264 ( 1\u2212 1\u03b2k )l \u03b40, where \u03b40 = f(OPT(k, T )) since f is normalized (i.e., f(\u2205) = 0). Finally, applying 1\u2212 x \u2264 e\u2212x and rearranging, we obtain\nf(Al(T )) \u2265 ( 1\u2212 e\u2212 l \u03b2k ) f(OPT(k, T ))."}, {"heading": "D. Proof of Theorem 4.5", "text": "D.1. Technical Lemmas\nWe first provide several technical lemmas that will be used throughout the proof. We begin with a simple property of submodular functions.\nLemma D.1 For any submodular function f on a ground set V , and any sets A,B,R \u2286 V , we have\nf(A \u222aB)\u2212 f(A \u222a (B \\R)) \u2264 f(R | A).\nProof. Define R2 := A \u2229R, and R1 := R \\A = R \\R2. We have\nf(A \u222aB)\u2212 f(A \u222a (B \\R)) = f(A \u222aB)\u2212 f((A \u222aB) \\R1) = f(R1 | (A \u222aB) \\R1) \u2264 f(R1 | (A \\R1)) (16) = f(R1 | A) (17) = f(R1 \u222aR2 | A) (18) = f(R | A),\nwhere (16) follows from the submodularity of f , (17) follows since A and R1 are disjoint, and (18) follows since R2 \u2286 A. 2\nThe next lemma provides a simple lower bound on the maximum of two quantities; it is stated formally since it will be used on multiple occasions.\nLemma D.2 For any set function f , sets A,B, and constant \u03b1 > 0, we have max{f(A), f(B)\u2212 \u03b1f(A)} \u2265 ( 1\n1 + \u03b1\n) f(B), (19)\nand\nmax{\u03b1f(A), f(B)\u2212 f(A)} \u2265 ( \u03b1\n1 + \u03b1\n) f(B). (20)\nProof. Starting with (19), we observe that one term is increasing in f(A) and the other is decreasing in f(A). Hence, the maximum over all possible f(A) is achieved when the two terms are equal, i.e., f(A) = 11+\u03b1f(B). We obtain (20) via the same argument. 2\nThe following lemma relates the function values associated with two buckets formed by PRO, denoted by X and Y . It is stated with respect to an arbitrary set EY , but when we apply the lemma, this will correspond to the elements of Y that are removed by the adversary.\nLemma D.3 Under the setup of Theorem 4.5, let X and Y be buckets of PRO such that Y is constructed at a later time than X . For any set EY \u2286 Y , we have\nf(X \u222a (Y \\ EY )) \u2265 1\n1 + \u03b1 f(Y ),\nand f(EY | X) \u2264 \u03b1f(X), (21)\nwhere \u03b1 = \u03b2 |EY ||X| .\nProof. Inequality (21) follows from the \u03b2-iterative property of A; specifically, we have from (8) that\nf(e|X) \u2264 \u03b2 f(X) |X| ,\nwhere e is any element of the ground set that is neither in X nor any bucket constructed before X . Hence, we can write\nf(EY | X) \u2264 \u2211 e\u2208EY f(e|X) \u2264 \u03b2 |EY | |X| f(X) = \u03b1f(X),\nwhere the first inequality is by submodularity. This proves (21).\nNext, we write\nf(Y )\u2212 f(X \u222a (Y \\ EY )) \u2264 f(X \u222a Y )\u2212 f(X \u222a (Y \\ EY )) (22) \u2264 f(EY | X), (23)\nwhere (22) is by monotonicity, and (23) is by Lemma D.1 with A = X , B = Y , and R = EY .\nCombining (21) and (23), together with the fact that f(X \u222a (Y \\ EY )) \u2265 f(X) (by monotonicity), we have\nf(X \u222a (Y \\ EY )) \u2265 max {f(X), f(Y )\u2212 \u03b1f(X)}\n\u2265 1 1 + \u03b1 f(Y ), (24)\nwhere (24) follows from (19). 2\nFinally, we provide a lemma that will later be used to take two bounds that are known regarding the previously-constructed buckets, and use them to infer bounds regarding the next bucket.\nLemma D.4 Under the setup of Theorem 4.5, let Y and Z be buckets of PRO such that Z is constructed at a later time than Y , and let EY \u2286 Y and EZ \u2286 Z be arbitrary sets. Moreover, let X be a set (not necessarily a bucket) such that\nf((Y \\ EY ) \u222aX) \u2265 1\n1 + \u03b1 f(Y ), (25)\nand f(EY | X) \u2264 \u03b1f(X). (26)\nThen, we have f(EZ | (Y \\ EY ) \u222aX) \u2264 \u03b1nextf((Y \\ EY ) \u222aX), (27)\nand f((Z \\ EZ) \u222a (Y \\ EY ) \u222aX) \u2265\n1\n1 + \u03b1next f(Z), (28)\nwhere\n\u03b1next = \u03b2 |EZ | |Y | (1 + \u03b1) + \u03b1. (29)\nProof. We first prove (27):\nf(EZ | (Y \\ EY ) \u222aX) = f((Y \\ EY ) \u222aX \u222a EZ)\u2212 f((Y \\ EY ) \u222aX) \u2264 f(X \u222a Y \u222a EZ)\u2212 f((Y \\ EY ) \u222aX) (30) = f(EZ | X \u222a Y ) + f(X \u222a Y )\u2212 f((Y \\ EY ) \u222aX) \u2264 f(EZ | Y ) + f(X \u222a Y )\u2212 f((Y \\ EY ) \u222aX) (31)\n\u2264 \u03b2 |EZ | |Y | f(Y ) + f(X \u222a Y )\u2212 f((Y \\ EY ) \u222aX) (32)\n\u2264 \u03b2 |EZ | |Y | (1 + \u03b1)f((Y \\ EY ) \u222aX) + f(X \u222a Y )\u2212 f((Y \\ EY ) \u222aX) (33)\n\u2264 \u03b2 |EZ | |Y | (1 + \u03b1)f((Y \\ EY ) \u222aX) + f(EY | (Y \\ EY ) \u222aX) (34)\n\u2264 \u03b2 |EZ | |Y | (1 + \u03b1)f((Y \\ EY ) \u222aX) + f(EY | X) (35)\n\u2264 \u03b2 |EZ | |Y | (1 + \u03b1)f((Y \\ EY ) \u222aX) + \u03b1f(X) (36)\n\u2264 \u03b2 |EZ | |Y | (1 + \u03b1)f((Y \\ EY ) \u222aX) + \u03b1f((Y \\ EY ) \u222aX) (37)\n= ( \u03b2 |EZ | |Y | (1 + \u03b1) + \u03b1 ) f((Y \\ EY ) \u222aX)., (38)\nwhere: (30) and (31) follow by monotonicity and submodularity, respectively; (32) follows from the second part of Lemma D.3; (33) follows from (25); (34) is obtained by applying Lemma D.1 for A = X , B = Y , and R = EY ; (35) follows by submodularity; (36) follows from (26); (37) follows by monotonicity. Finally, by defining \u03b1next := \u03b2 |EZ ||Y | (1 + \u03b1) + \u03b1 in (38) we establish the bound in (27).\nIn the rest of the proof, we show that (28) holds as well. First, we have\nf((Z \\ EZ) \u222a (Y \\ EY ) \u222aX) \u2265 f(Z)\u2212 f(EZ | (Y \\ EY ) \u222aX) (39)\nby Lemma D.1 with B = Z, R = EZ and A = (Y \\EY )\u222aX . Now we can use the derived bounds (38) and (39) to obtain\nf((Z \\ EZ) \u222a (Y \\ EY ) \u222aX) \u2265 f(Z)\u2212 f(EZ | (Y \\ EY ) \u222aX) \u2265 f(Z)\u2212 ( \u03b2 |EZ | |Y | (1 + \u03b1) + \u03b1 ) f((Y \\ EY ) \u222aX).\nFinally, we have f((Z \\ EZ) \u222a (Y \\ EY ) \u222aX) \u2265 max { f((Y \\ EY ) \u222aX), f(Z)\u2212 ( \u03b2 |EZ | |Y | (1 + \u03b1) + \u03b1 ) f((Y \\ EY ) \u222aX) } \u2265 1\n1 + \u03b1next f(Z),\nwhere the last inequality follows from Lemma D.1. 2\nObserve that the results we obtain on f(EZ | (Y \\ EY ) \u222aX) and on f((Z \\ EZ) \u222a (Y \\ EY ) \u222aX) in Lemma D.4 are of the same form as the pre-conditions of the lemma. This will allow us to apply the lemma recursively.\nD.2. Characterizing the Adversary\nLet E denote a set of elements removed by an adversary, where |E| \u2264 \u03c4 . Within S0, PRO constructs dlog \u03c4e+1 partitions. Each partition i \u2208 {0, . . . , dlog \u03c4e} consists of d\u03c4/2ie buckets, each of size 2i\u03b7, where \u03b7 \u2208 N will be specified later. We let B denote a generic bucket, and define EB to be all the elements removed from this bucket, i.e. EB = B \u2229 E.\nThe following lemma identifies a bucket in each partition for which not too many elements are removed.\nLemma D.5 Under the setup of Theorem 4.5, suppose that an adversary removes a set E of size at most \u03c4 from the set S constructed by PRO. Then for each partition i, there exists a bucket Bi such that |EBi | \u2264 2i, i.e., at most 2i elements are removed from this bucket.\nProof. Towards contradiction, assume that this is not the case, i.e., assume |EBi | > 2i for every bucket of the i-th partition. As the number of buckets in partition i is d\u03c4/2ie, this implies that the adversary has to spend a budget of\n|E| \u2265 2i|EBi | > 2id\u03c4/2ie = \u03c4,\nwhich is in contradiction with |E| \u2264 \u03c4 . 2 We consider B0, . . . , Bdlog \u03c4e as above, and show that even in the worst case, f (\u22c3dlog \u03c4e i=0 (Bi \\ EBi) ) is almost as large\nas f ( Bdlog \u03c4e ) for appropriately set \u03b7. To achieve this, we apply Lemma D.4 multiple times, as illustrated in the following lemma. We henceforth write \u03b7h := \u03b7/2 for brevity.\nLemma D.6 Under the setup of Theorem 4.5, suppose that an adversary removes a set E of size at most \u03c4 from the set S constructed by PRO, and let B0, \u00b7 \u00b7 \u00b7 , Bdlog \u03c4e be buckets such that |EBi | \u2264 2i for each i \u2208 {1, \u00b7 \u00b7 \u00b7 dlog \u03c4e} (cf., Lemma D.5). Then,\nf dlog \u03c4e\u22c3 i=0 (Bi \\ EBi)  \u2265 (1\u2212 1 1 + 1\u03b1 ) f ( Bdlog \u03c4e ) = 1 1 + \u03b1 f ( Bdlog \u03c4e ) , (40)\nand\nf EBdlog \u03c4e \u2223\u2223\u2223 dlog \u03c4e\u22121\u22c3 i=0 (Bi \\ EBi)  \u2264 \u03b1f dlog \u03c4e\u22121\u22c3 i=0 (Bi \\ EBi)  , (41) for some\n\u03b1 \u2264 \u03b22 (1 + \u03b7h) dlog \u03c4e \u2212 \u03b7dlog \u03c4eh \u03b7 dlog \u03c4e h . (42)\nProof. In what follows, we focus on the case where there exists some bucketB0 in partition i = 0 such thatB0\\EB0 = B0. If this is not true, then E must be contained entirely within this partition, since it contains \u03c4 buckets. As a result, (i) we trivially obtain (40) even when \u03b1 is replaced by zero, since the union on the left-hand side contains Bdlog \u03c4e; (ii) (41) becomes trivial since the left-hand side is zero is a result of EBdlog \u03c4e = \u2205.\nWe proceed by induction. Namely, we show that\nf ( j\u22c3 i=0 (Bi \\ EBi) ) \u2265 ( 1\u2212 1 1 + 1\u03b1j ) f(Bj) =\n1\n1 + \u03b1j f(Bj), (43)\nand\nf ( EBj \u2223\u2223\u2223 j\u22121\u22c3 i=0 (Bi \\ EBi) ) \u2264 \u03b1jf ( j\u22121\u22c3 i=0 (Bi \\ EBi) ) , (44)\nfor every j \u2265 1, where\n\u03b1j \u2264 \u03b22 (1 + \u03b7h) j \u2212 \u03b7jh \u03b7jh . (45)\nUpon showing this, the lemma is concluded by setting j = dlog \u03c4e.\nBase case j = 1. In the case that j = 1, taking into account that EB0 = \u2205, we observe from (43) that our goal is to bound f(B0 \u222a (B1 \\ EB1)). Applying Lemma D.3 with X = B0, Y = B1, and EY = EB1 , we obtain\nf(B0 \u222a (B1 \\ EB1)) \u2265 1\n1 + \u03b11 f(B1),\nand f(EB1 | B0) \u2264 \u03b11f(B0),\nwhere \u03b11 = \u03b2 |EB1 | |B0| . We have |B0| = \u03b7, while |EB1 | \u2264 2 by assumption. Hence, we can upper bound \u03b11 and rewrite as\n\u03b11 \u2264 \u03b2 2\n\u03b7 = \u03b2\n1\n\u03b7h = \u03b2 (1 + \u03b7h)\u2212 \u03b7h \u03b7h \u2264 \u03b22 (1 + \u03b7h)\u2212 \u03b7h \u03b7h ,\nwhere the last inequality follows since \u03b2 \u2265 1 by definition.\nInductive step. Fix j \u2265 2. Assuming that the inductive hypothesis holds for j\u2212 1, we want to show that it holds for j as well.\nWe write\nf ( j\u22c3 i=0 (Bi \\ EBi) ) = f (( j\u22121\u22c3 i=0 (Bi \\ EBi) ) \u222a (Bj \\ EBj ) ) ,\nand apply Lemma D.4 with X = \u22c3j\u22122 i=0 (Bi \\ EBi), Y = Bj\u22121, EY = EBj\u22121 , Z = Bj , and EZ = EBj . Note that the conditions (25) and (26) of Lemma D.4 are satisfied by the inductive hypothesis. Hence, we conclude that (43) and (44) hold with\n\u03b1j = \u03b2 |EBj | |Bj\u22121| (1 + \u03b1j\u22121) + \u03b1j\u22121.\nIt remains to show that (45) holds for \u03b1j , assuming it holds for \u03b1j\u22121. We have\n\u03b1j = \u03b2 |EBj | |Bj\u22121| (1 + \u03b1j\u22121) + \u03b1j\u22121\n\u2264 \u03b2 1 \u03b7h\n( 1 + \u03b2\n(1 + \u03b7h) j\u22121 \u2212 \u03b7j\u22121h \u03b7j\u22121h\n) + \u03b2\n(1 + \u03b7h) j\u22121 \u2212 \u03b7j\u22121h \u03b7j\u22121h\n(46)\n\u2264 \u03b22 ( 1\n\u03b7h\n( 1 +\n(1 + \u03b7h) j\u22121 \u2212 \u03b7j\u22121h \u03b7j\u22121h\n) +\n(1 + \u03b7h) j\u22121 \u2212 \u03b7j\u22121h \u03b7j\u22121h\n) (47)\n= \u03b22\n( 1\n\u03b7h\n(1 + \u03b7h) j\u22121\n\u03b7j\u22121h + (1 + \u03b7h) j\u22121 \u2212 \u03b7j\u22121h \u03b7j\u22121h\n)\n= \u03b22\n( (1 + \u03b7h) j\u22121\n\u03b7jh + \u03b7h(1 + \u03b7h) j\u22121 \u2212 \u03b7jh \u03b7jh\n)\n= \u03b22 (1 + \u03b7h) j \u2212 \u03b7jh \u03b7jh ,\nwhere (46) follows from (45) and the fact that\n\u03b2 |EBj | |Bj\u22121| \u2264 \u03b2 2 j 2j\u22121\u03b7 = \u03b2 2 \u03b7 = \u03b2 1 \u03b7h ,\nby |EBj | \u2264 2j and |Bj\u22121| = 2j\u22121\u03b7; and (47) follows since \u03b2 \u2265 1. 2\nInequality (45) provides an upper bound on \u03b1j , but it is not immediately clear how the bound varies with j. The following lemma provides a more compact form.\nLemma D.7 Under the setup of Lemma D.6, we have for 2dlog \u03c4e \u2264 \u03b7h that\n\u03b1j \u2264 3\u03b22 j\n\u03b7 (48)\nProof. We unfold the right-hand side of (45) in order to express it in a simpler way. First, consider j = 1. From (45) we obtain \u03b11 \u2264 2\u03b22 1\u03b7 , as required. For j \u2265 2, we obtain the following:\n\u03b2\u22122\u03b1j \u2264 (1 + \u03b7h) j \u2212 \u03b7jh \u03b7jh\n= j\u22121\u2211 i=0 ( j i ) \u03b7ih \u03b7jh\n(49)\n= j\n\u03b7h + j\u22122\u2211 i=0 ( j i ) \u03b7ih \u03b7jh\n(50)\n= j\n\u03b7h + j\u22122\u2211 i=0 (\u220fj\u2212i t=1(j \u2212 t+ 1)\u220fj\u2212i t=1 t \u03b7ih \u03b7jh )\n\u2264 j \u03b7h + 1 2 j\u22122\u2211 i=0 jj\u2212i \u03b7ih \u03b7jh\n(51)\n= j\n\u03b7h +\n1\n2 j\u22122\u2211 i=0 ( j \u03b7h )j\u2212i\n= j\n\u03b7h +\n1\n2\n( \u22121\u2212 j\n\u03b7h + j\u2211 i=0 ( j \u03b7h )j\u2212i) ,\nwhere (49) is a standard summation identity, and (51) follows from \u220fj\u2212i t=1(j\u2212 t+ 1) \u2264 jj\u2212i and \u220fj\u2212i t=1 t \u2265 2 for j\u2212 i \u2265 2. Next, explicitly evaluating the summation of the last equality, we obtain\n\u03b2\u22122\u03b1j \u2264 j\n\u03b7h +\n1\n2 \u22121\u2212 j \u03b7h + 1\u2212 ( j \u03b7h )j+1 1\u2212 j\u03b7h  \u2264 j \u03b7h + 1 2 ( \u22121\u2212 j \u03b7h + 1\n1\u2212 j\u03b7h\n)\n= j\n\u03b7h +\n1\n2\n ( j \u03b7h )2 1\u2212 j\u03b7h  (52) = j\n\u03b7h +\nj\n2\u03b7h\n( j \u03b7h\n1\u2212 j\u03b7h\n) , (53)\nwhere (52) follows from (\u2212a\u2212 1)(\u2212a+ 1) = a2 \u2212 1 with a = j/\u03b7h.\nNext, observe that if j/\u03b7h \u2264 1/2, or equivalently 2j \u2264 \u03b7h, (54)\nthen we can weaken (53) to\n\u03b2\u22122\u03b1j \u2264 j\n\u03b7h +\nj\n2\u03b7h =\n3\n2\nj\n\u03b7h = 3\nj \u03b7 , (55)\nwhich yields (48).\n2\nD.3. Completing the Proof of Theorem 4.5\nWe now prove Theorem 4.5 in several steps. Throughout, we define \u00b5 to be a constant such that f(E1 | (S \\E)) = \u00b5f(S1) holds, and we write E0 := E\u2217S \u2229 S0, E1 := E\u2217S \u2229 S1, and EBi := E\u2217S \u2229Bi, where E\u2217S is defined in (9). We also make use of the following lemma characterizing the optimal adversary. The proof is straightforward, and can be found in Lemma 2 of (Orlin et al., 2016)\nLemma D.8 (Orlin et al., 2016) Under the setup of Theorem 4.5, we have for all X \u2282 V with |X| \u2264 \u03c4 that\nf(OPT(k, V, \u03c4) \\ E\u2217OPT(k,V,\u03c4)) \u2264 f(OPT(k \u2212 \u03c4, V \\X)).\nInitial lower bounds: We start by providing three lower bounds on f(S \\ E\u2217S). First, we observe that f(S \\ E\u2217S) \u2265 f(S0 \\ E0) and f(S \\ E\u2217S) \u2265 f (\u22c3dlog \u03c4e i=0 (Bi \\ EBi) ) . We also have\nf(S \\ E) = f(S)\u2212 f(S) + f(S \\ E) = f(S0 \u222a S1) + f(S \\ E0)\u2212 f(S \\ E0)\u2212 f(S) + f(S \\ E) (56) = f(S1) + f(S0 | S1) + f(S \\ E0)\u2212 f(S)\u2212 f(S \\ E0) + f(S \\ E) = f(S1) + f(S0 | (S \\ S0)) + f(S \\ E0)\u2212 f(E0 \u222a (S \\ E0))\u2212 f(S \\ E0) + f(S \\ E) (57) = f(S1) + f(S0 | (S \\ S0))\u2212 f(E0 | (S \\ E0))\u2212 f(S \\ E0) + f(S \\ E) = f(S1) + f(S0 | (S \\ S0))\u2212 f(E0 | (S \\ E0))\u2212 f(E1 \u222a (S \\ E)) + f(S \\ E) (58) = f(S1) + f(S0 | (S \\ S0))\u2212 f(E0 | (S \\ E0))\u2212 f(E1 | S \\ E) = f(S1)\u2212 f(E1 | S \\ E) + f(S0 | (S \\ S0))\u2212 f(E0 | (S \\ E0)) \u2265 (1\u2212 \u00b5)f(S1), (59)\nwhere (56) and (57) follow from S = S0\u222aS1, (58) follows from E\u2217S = E0\u222aE1, and (59) follows from f(S0 | (S \\S0))\u2212 f(E0 | (S \\ E0)) \u2265 0 (due to E0 \u2286 S0 and S \\ S0 \u2286 S \\ E0), along with the definition of \u00b5.\nBy combining the above three bounds on f(S \\ E\u2217S), we obtain\nf(S \\ E\u2217S) \u2265 max f(S0 \\ E0), (1\u2212 \u00b5)f(S1), f dlog \u03c4e\u22c3\ni=0\n(Bi \\ EBi)  . (60) We proceed by further bounding these terms.\nBounding the first term in (60): Defining S\u20320 := OPT(k \u2212 \u03c4, V \\E0) \u2229 (S0 \\E0) and X := OPT(k \u2212 \u03c4, V \\E0) \\ S\u20320, we have\nf(S0 \\ E0) + f(OPT(k \u2212 \u03c4, V \\ S0)) \u2265 f(S\u20320) + f(X) (61) \u2265 f(OPT(k \u2212 \u03c4, V \\ E0)) (62) \u2265 f(OPT(k, V, \u03c4) \\ E\u2217OPT(k,V,\u03c4)), (63)\nwhere (61) follows from monotonicity, i.e. (S0 \\ E0) \u2286 S\u20320 and (V \\ S0) \u2286 (V \\ E0), (62) follows from the fact that OPT(k \u2212 \u03c4, V \\E0) = S\u20320 \u222aX and submodularity,2 and (63) follows from Lemma D.8 and |E0| \u2264 \u03c4 . We rewrite (63) as\nf(S0 \\ E0) \u2265 f(OPT(k, V, \u03c4) \\ E\u2217OPT(k,V,\u03c4))\u2212 f(OPT(k \u2212 \u03c4, V \\ S0)). (64)\nBounding the second term in (60): Note that S1 is obtained by using A that satisfies the \u03b2-iterative property on the set V \\ S0, and its size is |S1| = k \u2212 |S0|. Hence, from Lemma 4.3 with k \u2212 \u03c4 in place of k, we have\nf(S1) \u2265 ( 1\u2212 e\u2212 k\u2212|S0| \u03b2(k\u2212\u03c4) ) f(OPT(k \u2212 \u03c4, V \\ S0)). (65)\n2The submodularity property can equivalently be written as f(A) + f(B) \u2265 f(A \u222aB) + f(A \u2229B).\nBounding the third term in (60): We can view S1 as a large bucket created by our algorithm after creating the buckets in S0. Therefore, we can apply Lemma D.4 with X = \u22c3dlog \u03c4e\u22121 i=0 (Bi \\ EBi), Y = Bdlog \u03c4e, Z = S1, EY = E\u2217S \u2229 Y , and EZ = E1. Conditions (25) and (26) needed to apply Lemma D.4 are provided by Lemma D.6. From Lemma D.4, we obtain the following with \u03b1 as in (42):\nf E1 \u2223\u2223\u2223\u2223\u2223 dlog \u03c4e\u22c3\ni=0\n(Bi \\ EBi)  \u222a (S1 \\ E1)  \u2264 (\u03b2 |E1|\n|Bdlog \u03c4e| (1 + \u03b1) + \u03b1\n) f dlog \u03c4e\u22c3 i=0 (Bi \\ EBi)  . (66) Furthermore, noting that the assumption \u03b7 \u2265 4(log k+ 1) implies 2dlog \u03c4e \u2264 \u03b7h, we can upper-bound \u03b1 as in Lemma D.7 by (48) for j = dlog \u03c4e. Also, we have \u03b2 |E1||Bdlog \u03c4e| \u2264 \u03b2 \u03c4 2dlog \u03c4e\u03b7\n\u2264 \u03b2\u03b7 . Putting these together, we upper bound (66) as follows:\nf E1 \u2223\u2223\u2223\u2223\u2223 dlog \u03c4e\u22c3\ni=0\n(Bi \\ EBi)  \u222a (S1 \\ E1)  \u2264 (\u03b2\n\u03b7\n( 1 +\n3\u03b22dlog \u03c4e \u03b7\n) +\n3\u03b22dlog \u03c4e \u03b7\n) f dlog \u03c4e\u22c3 i=0 (Bi \\ EBi)  \u2264 5\u03b2\n3dlog \u03c4e \u03b7 f dlog \u03c4e\u22c3 i=0 (Bi \\ EBi)  , where we have used \u03b7 \u2265 1 and dlog \u03c4e \u2265 1 (since \u03c4 \u2265 2 by assumption). We rewrite the previous equation as\nf dlog \u03c4e\u22c3 i=0 (Bi \\ EBi)  \u2265 \u03b7 5\u03b23dlog \u03c4e f E1 \u2223\u2223\u2223\u2223\u2223 dlog \u03c4e\u22c3 i=0 (Bi \\ EBi)  \u222a (S1 \\ E1) \n\u2265 \u03b7 5\u03b23dlog \u03c4e f(E1 | (S \\ E)) (67) = \u03b7\n5\u03b23dlog \u03c4e \u00b5f(S1), (68)\nwhere (67) follows from submodularity, and (68) follows from the definition of \u00b5.\nCombining the bounds: Returning to (60), we have\nf(S \\ E\u2217S) \u2265 max f(S0 \\ E0), (1\u2212 \u00b5)f(S1), f dlog \u03c4e\u22c3\ni=0\n(Bi \\ EBi)  \u2265 max { f(S0 \\ E0), (1\u2212 \u00b5)f(S1), \u03b7\n5\u03b23dlog \u03c4e \u00b5f(S1)\n} (69)\n\u2265 max{f(OPT(k, V, \u03c4) \\ E\u2217OPT(k,V,\u03c4))\u2212 f(OPT(k \u2212 \u03c4, V \\ S0)), (1\u2212 \u00b5) ( 1\u2212 e\u2212 k\u2212|S0| \u03b2(k\u2212\u03c4) ) f(OPT(k \u2212 \u03c4, V \\ S0)),\n\u03b7 5\u03b23dlog \u03c4e \u00b5 ( 1\u2212 e\u2212 k\u2212|S0| \u03b2(k\u2212\u03c4) ) f(OPT(k \u2212 \u03c4, V \\ S0))} (70)\n\u2265 max{f(OPT(k, V, \u03c4) \\ E\u2217OPT(k,V,\u03c4))\u2212 f(OPT(k \u2212 \u03c4, V \\ S0)), \u03b7\n5\u03b23dlog \u03c4e\n1 + \u03b75\u03b23dlog \u03c4e\n( 1\u2212 e\u2212 k\u2212|S0| \u03b2(k\u2212\u03c4) ) f(OPT(k \u2212 \u03c4, V \\ S0))} (71)\n= max{f(OPT(k, V, \u03c4) \\ E\u2217OPT(k,V,\u03c4))\u2212 f(OPT(k \u2212 \u03c4, V \\ S0)), \u03b7\n5\u03b23dlog \u03c4e+ \u03b7\n( 1\u2212 e\u2212 k\u2212|S0| \u03b2(k\u2212\u03c4) ) f(OPT(k \u2212 \u03c4, V \\ S0))}\n\u2265 \u03b7 5\u03b23dlog \u03c4e+\u03b7\n( 1\u2212 e\u2212 k\u2212|S0| \u03b2(k\u2212\u03c4) ) 1 + \u03b75\u03b23dlog \u03c4e+\u03b7 ( 1\u2212 e\u2212 k\u2212|S0| \u03b2(k\u2212\u03c4) )f(OPT(k, V, \u03c4) \\ E\u2217OPT(k,V,\u03c4)), (72)\nwhere (69) follows from (68), (70) follows from (64) and (65), (71) follows since max{1\u2212 \u00b5, c\u00b5} \u2265 c1+c analogously to (19), and (72) follows from (20). Hence, we have established (72).\nTurning to the permitted values of \u03c4 , we have from Proposition 4.1 that\n|S0| \u2264 3\u03b7\u03c4(log k + 2).\nFor the choice of \u03c4 to yield valid set sizes, we only require |S0| \u2264 k; hence, it suffices that\n\u03c4 \u2264 k 3\u03b7(log k + 2) . (73)\nFinally, we consider the second claim of the lemma. For \u03c4 \u2208 o (\nk \u03b7(log k) ) we have |S0| \u2208 o(k). Furthermore, by setting\n\u03b7 \u2265 log2 k (which satisfies the assumption \u03b7 \u2265 4(log k + 1) for large k), we get k\u2212|S0|\u03b2(k\u2212\u03c4) \u2192 \u03b2 \u22121 and \u03b75\u03b23dlog \u03c4e+\u03b7 \u2192 1 as k \u2192\u221e. Hence, the constant factor converges to 1\u2212e \u22121/\u03b2\n2\u2212e\u22121/\u03b2 , yielding (11). In the case that GREEDY is used as the subroutine,\nwe have \u03b2 = 1, and hence the constant factor converges t 1\u2212e \u22121\n2\u2212e\u22121 \u2265 0.387. If THRESHOLDING-GREEDY is used, we have \u03b2 = 11\u2212 , and hence the constant factor converges to 1\u2212e \u22121 2\u2212e \u22121 \u2265 (1\u2212 ) 1\u2212e\u22121 2\u2212e\u22121 \u2265 (1\u2212 )0.387."}], "references": [{"title": "Fast algorithms for maximizing submodular functions", "author": ["Badanidiyuru", "Ashwinkumar", "Vondr\u00e1k", "Jan"], "venue": "In ACMSIAM Symp. Disc. Alg. (SODA), pp", "citeRegEx": "Badanidiyuru et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Badanidiyuru et al\\.", "year": 2014}, {"title": "Robust protection of networks against cascading phenomena", "author": ["Bogunovic", "Ilija", "Krause", "Andreas"], "venue": "Tech. Report ETH Zu\u0308rich,", "citeRegEx": "Bogunovic et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bogunovic et al\\.", "year": 2012}, {"title": "Robust influence maximization", "author": ["Chen", "Wei", "Lin", "Tian", "Tan", "Zihan", "Zhao", "Mingfei", "Zhou", "Xuren"], "venue": "arXiv preprint arXiv:1601.06551,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["Globerson", "Amir", "Roweis", "Sam"], "venue": "In Int. Conf. Mach. Learn. (ICML),", "citeRegEx": "Globerson et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2006}, {"title": "Robust influence maximization", "author": ["He", "Xinran", "Kempe", "David"], "venue": "In Int. Conf. Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Maximizing the spread of influence through a social network", "author": ["Kempe", "David", "Kleinberg", "Jon", "Tardos", "\u00c9va"], "venue": "In Int. Conf. on Knowledge Discovery and Data Mining (SIGKDD),", "citeRegEx": "Kempe et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kempe et al\\.", "year": 2003}, {"title": "Submodular function maximization. Tractability: Practical Approaches to Hard Problems", "author": ["Krause", "Andreas", "Golovin", "Daniel"], "venue": null, "citeRegEx": "Krause et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2012}, {"title": "Near-optimal observation selection using submodular functions", "author": ["Krause", "Andreas", "Guestrin", "Carlos"], "venue": "In Conf. Art. Intell. (AAAI),", "citeRegEx": "Krause et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2007}, {"title": "Robust submodular observation selection", "author": ["Krause", "Andreas", "McMahan", "H Brendan", "Guestrin", "Carlos", "Gupta", "Anupam"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Krause et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "A class of submodular functions for document summarization", "author": ["Lin", "Hui", "Bilmes", "Jeff"], "venue": "In Assoc. for Comp. Ling.: Human Language Technologies-Volume", "citeRegEx": "Lin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2011}, {"title": "Horizontally scalable submodular maximization", "author": ["Lucic", "Mario", "Bachem", "Olivier", "Zadimoghaddam", "Morteza", "Krause", "Andreas"], "venue": "In Proc. Int. Conf. Mach. Learn. (ICML),", "citeRegEx": "Lucic et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lucic et al\\.", "year": 2016}, {"title": "Discovering social circles in ego networks", "author": ["Mcauley", "Julian", "Leskovec", "Jure"], "venue": "ACM Trans. Knowl. Discov. Data,", "citeRegEx": "Mcauley et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mcauley et al\\.", "year": 2014}, {"title": "Accelerated greedy algorithms for maximizing submodular set functions", "author": ["Minoux", "Michel"], "venue": "In Optimization Techniques,", "citeRegEx": "Minoux and Michel.,? \\Q1978\\E", "shortCiteRegEx": "Minoux and Michel.", "year": 1978}, {"title": "Randomized composable core-sets for distributed submodular maximization", "author": ["Mirrokni", "Vahab", "Zadimoghaddam", "Morteza"], "venue": "In ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Mirrokni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mirrokni et al\\.", "year": 2015}, {"title": "Lazier than lazy greedy", "author": ["Mirzasoleiman", "Baharan", "Badanidiyuru", "Ashwinkumar", "Karbasi", "Amin", "Vondr\u00e1k", "Jan", "Krause", "Andreas"], "venue": "In Proc. Conf. Art. Intell. (AAAI),", "citeRegEx": "Mirzasoleiman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mirzasoleiman et al\\.", "year": 2015}, {"title": "Distributed submodular cover: Succinctly summarizing massive data", "author": ["Mirzasoleiman", "Baharan", "Karbasi", "Amin", "Badanidiyuru", "Ashwinkumar", "Krause", "Andreas"], "venue": "In Adv. Neur. Inf. Proc. Sys. (NIPS),", "citeRegEx": "Mirzasoleiman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mirzasoleiman et al\\.", "year": 2015}, {"title": "An analysis of approximations for maximizing submodular set functionsi", "author": ["Nemhauser", "George L", "Wolsey", "Laurence A", "Fisher", "Marshall L"], "venue": "Mathematical Programming,", "citeRegEx": "Nemhauser et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Nemhauser et al\\.", "year": 1978}, {"title": "An efficient streaming algorithm for the submodular cover problem", "author": ["Norouzi-Fard", "Ashkan", "Bazzi", "Abbas", "Bogunovic", "Ilija", "El Halabi", "Marwa", "Hsieh", "Ya-Ping", "Cevher", "Volkan"], "venue": "In Adv. Neur. Inf. Proc. Sys. (NIPS),", "citeRegEx": "Norouzi.Fard et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Norouzi.Fard et al\\.", "year": 2016}, {"title": "Robust monotone submodular function maximization", "author": ["Orlin", "James B", "Schulz", "Andreas S", "Udwani", "Rajan"], "venue": "In Int. Conf. on Integer Programming and Combinatorial Opt. (IPCO)", "citeRegEx": "Orlin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Orlin et al\\.", "year": 2016}, {"title": "Constrained robust submodular optimization", "author": ["Powers", "Thomas", "Bilmes", "Jeff", "Wisdom", "Scott", "Krout", "David W", "Atlas", "Les"], "venue": "NIPS OPT2016 workshop,", "citeRegEx": "Powers et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Powers et al\\.", "year": 2016}, {"title": "Machine learning for quantum mechanics in a nutshell", "author": ["Rupp", "Matthias"], "venue": "Int. Journal of Quantum Chemistry,", "citeRegEx": "Rupp and Matthias.,? \\Q2015\\E", "shortCiteRegEx": "Rupp and Matthias.", "year": 2015}, {"title": "Robust budget allocation via continuous submodular functions", "author": ["Staib", "Matthew", "Jegelka", "Stefanie"], "venue": "http://people.csail.mit.edu/stefje/ papers/robust_budget.pdf,", "citeRegEx": "Staib et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Staib et al\\.", "year": 2017}, {"title": "Submodular approximation: Sampling-based algorithms and lower bounds", "author": ["Svitkina", "Zoya", "Fleischer", "Lisa"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Svitkina et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Svitkina et al\\.", "year": 2011}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["Torralba", "Antonio", "Fergus", "Rob", "Freeman", "William T"], "venue": "IEEE Trans. Patt. Ana. Mach. Intel.,", "citeRegEx": "Torralba et al\\.,? \\Q1958\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 1958}], "referenceMentions": [{"referenceID": 18, "context": "We focus on the worst-case setting considered in (Orlin et al., 2016), in which a constant-factor approximation guarantee was given for \u03c4 = o( \u221a k).", "startOffset": 49, "endOffset": 69}, {"referenceID": 18, "context": "We numerically demonstrate the performance of PRO in data summarization and influence maximization, demonstrating gains over both the greedy algorithm and the algorithm of (Orlin et al., 2016).", "startOffset": 172, "endOffset": 192}, {"referenceID": 5, "context": "Applications include influence maximization (Kempe et al., 2003), document summarization (Lin & Bilmes, 2011), sensor placement (Krause & Guestrin, 2007), and active learning (Krause & Golovin, 2012), just to name a few.", "startOffset": 44, "endOffset": 64}, {"referenceID": 8, "context": "The robust problem formulation was first introduced in (Krause et al., 2008), and was further studied in (Orlin et al.", "startOffset": 55, "endOffset": 76}, {"referenceID": 18, "context": ", 2008), and was further studied in (Orlin et al., 2016).", "startOffset": 36, "endOffset": 56}, {"referenceID": 8, "context": "In fact, (Krause et al., 2008) considers a more general formulation where a constant-factor approximation guarantee is impossible in general, but shows that one can match the optimal (robust) objective value for a given set size at the cost of returning a set whose size is larger by a logarithmic factor.", "startOffset": 9, "endOffset": 30}, {"referenceID": 18, "context": "In contrast, (Orlin et al., 2016) designs an algorithm that obtains the first constant-factor approximation guarantee to the above problem when \u03c4 = o( \u221a k).", "startOffset": 13, "endOffset": 33}, {"referenceID": 8, "context": "A key difference between the two frameworks is that the algorithm complexity is exponential in \u03c4 in (Krause et al., 2008), whereas the algorithm of (Orlin et al.", "startOffset": 100, "endOffset": 121}, {"referenceID": 18, "context": ", 2008), whereas the algorithm of (Orlin et al., 2016) runs in polynomial time.", "startOffset": 34, "endOffset": 54}, {"referenceID": 18, "context": "In this paper, we solve a key open problem posed in (Orlin et al., 2016), namely, whether a constantfactor approximation guarantee is possible for general \u03c4 = o(k), as opposed to only \u03c4 = o( \u221a k).", "startOffset": 52, "endOffset": 72}, {"referenceID": 18, "context": "Achieving this result requires novelty both in the algorithm and its mathematical analysis: While our algorithm bears some similarity to that of (Orlin et al., 2016), it uses a novel structure in which the constructed set is arranged into partitions consisting of buckets whose sizes increase exponentially with the partition index.", "startOffset": 145, "endOffset": 165}, {"referenceID": 8, "context": "In addition to the above contributions, we provide the first empirical study beyond what is demonstrated for \u03c4 = 1 in (Krause et al., 2008).", "startOffset": 118, "endOffset": 139}, {"referenceID": 18, "context": "We demonstrate several scenarios in which our algorithm outperforms both the greedy algorithm and the algorithm of (Orlin et al., 2016).", "startOffset": 115, "endOffset": 135}, {"referenceID": 16, "context": "A celebrated result of (Nemhauser et al., 1978) shows that a simple greedy algorithm that starts with an empty set and then iteratively adds elements with highest marginal gain provides a (1\u2212 1/e)-approximation.", "startOffset": 23, "endOffset": 47}, {"referenceID": 8, "context": "In this paper, we consider the following robust version of (1), introduced in (Krause et al., 2008):", "startOffset": 78, "endOffset": 99}, {"referenceID": 8, "context": "(Krause et al., 2008) introduces the following generalization of (2):", "startOffset": 0, "endOffset": 21}, {"referenceID": 19, "context": "The work of (Powers et al., 2016) considers the same problem for different types of submodular constraints.", "startOffset": 12, "endOffset": 33}, {"referenceID": 2, "context": "The confidence interval setting is considered in (Chen et al., 2016), where two runs of the GREEDY algorithm (one pessimistic and one optimistic) are used to optimize the same ratio.", "startOffset": 49, "endOffset": 68}, {"referenceID": 18, "context": "(Orlin et al., 2016) considers the formulation in (2), and provides the first constant 0.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": ", see (Kempe et al., 2003)), given the targeted set S, the expected number of influenced nodes at the end of the diffusion process is a monotone and submodular function of S (He & Kempe, 2016).", "startOffset": 6, "endOffset": 26}, {"referenceID": 10, "context": ", (Lucic et al., 2016).", "startOffset": 2, "endOffset": 22}, {"referenceID": 8, "context": "Further potential applications not covered here include robust sensor placement (Krause et al., 2008), robust protection of networks (Bogunovic & Krause, 2012), and robust feature selection (Globerson & Roweis, 2006).", "startOffset": 80, "endOffset": 101}, {"referenceID": 18, "context": "In contrast, we recall that the algorithm OSU proposed in (Orlin et al., 2016) adopts a simpler approach where a robust set is used consisting of \u03c4 buckets of equal size \u03c4 log k, thereby only permitting the scaling \u03c4 = o( \u221a k).", "startOffset": 58, "endOffset": 78}, {"referenceID": 18, "context": "We provide the following intuition as to why PRO succeeds despite having a smaller size for S0 compared to the algorithm given in (Orlin et al., 2016).", "startOffset": 130, "endOffset": 150}, {"referenceID": 16, "context": "The following lemma generalizes a classical property of the greedy algorithm (Nemhauser et al., 1978; Krause & Golovin, 2012) to the class of algorithms satisfying the \u03b2-iterative property.", "startOffset": 77, "endOffset": 125}, {"referenceID": 18, "context": "This result solves an open problem raised in (Orlin et al., 2016), namely, whether a constant-factor approximation guarantee can be obtained for \u03c4 = o(k) as opposed to", "startOffset": 45, "endOffset": 65}, {"referenceID": 18, "context": "387 for the greedy subroutine matches that of (Orlin et al., 2016), but our algorithm permits significantly \u201chigher robustness\u201d in the sense of allowing larger \u03c4 values.", "startOffset": 46, "endOffset": 66}, {"referenceID": 18, "context": "Once the suitable lower bounds are obtained for the terms in (12), the analysis proceeds similarly to (Orlin et al., 2016).", "startOffset": 102, "endOffset": 122}, {"referenceID": 18, "context": "In particular, we compare our algorithm against the OSU algorithm proposed in (Orlin et al., 2016) on different datasets and corresponding objective functions (see Table 3).", "startOffset": 78, "endOffset": 98}, {"referenceID": 8, "context": "We do not compare against SATURATE (Krause et al., 2008), due to its high computational cost for even a small \u03c4 .", "startOffset": 35, "endOffset": 56}, {"referenceID": 18, "context": "Conclusion We have provided a new Partitioned Robust (PRO) submodular maximization algorithm attaining a constantfactor approximation guarantee for general \u03c4 = o(k), thus resolving an open problem posed in (Orlin et al., 2016).", "startOffset": 206, "endOffset": 226}], "year": 2017, "abstractText": "We study the problem of maximizing a monotone submodular function subject to a cardinality constraint k, with the added twist that a number of items \u03c4 from the returned set may be removed. We focus on the worst-case setting considered in (Orlin et al., 2016), in which a constant-factor approximation guarantee was given for \u03c4 = o( \u221a k). In this paper, we solve a key open problem raised therein, presenting a new Partitioned Robust (PRO) submodular maximization algorithm that achieves the same guarantee for more general \u03c4 = o(k). Our algorithm constructs partitions consisting of buckets with exponentially increasing sizes, and applies standard submodular optimization subroutines on the buckets in order to construct the robust solution. We numerically demonstrate the performance of PRO in data summarization and influence maximization, demonstrating gains over both the greedy algorithm and the algorithm of (Orlin et al., 2016).", "creator": "LaTeX with hyperref package"}}}