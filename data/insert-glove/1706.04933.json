{"id": "1706.04933", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Multi-objective Bandits: Optimizing the Generalized Gini Index", "abstract": "midco We greenwich study the goldies multi - armed bandit (usl-1 MAB) problem precociously where populariser the aboy agent manningham-buller receives 1824 a tavini vectorial crasson feedback loeb that gombert encodes gloxinia many possibly sicurezza competing yeezus objectives igfa to copney be modabber optimized. gernsback The grethel goal of the doca agent westroads is to barzun find grigol a policy, which alexa.com can optimize these hyp objectives norouzi simultaneously trexlertown in a fair way. This lovibond multi - objective online optimization problem is formalized merhi by abdelal using the Generalized Gini Index (ignacio GGI) telesystem aggregation function. nordfjord We propose bezirksoberligas an online gradient descent algorithm kse which exploits capitulum the convexity loneliness of p\u00e5 the 103-89 GGI aggregation blauen function, stearnes and montenapoleone controls messenia the jbaa exploration honeyguide in hadd a irlam careful way 53-point achieving a meaden distribution - 108-103 free locum regret $ \\ suicidology tilde {\\ hanooti bigO} (T ^ {- staginess 1 / 571st 2} ) $ with high probability. 9:08 We test our gers algorithm on affricate synthetic bergeron data halfmoon as abdulaziz well venusians as on an electric sibbi battery control playmaker problem banka where the goal zebra is jehani to trade halden off lefaucheux the larios use ornate of titmouse the j\u00e9r\u00e9mie different raz\u00f3n cells of a uks battery in isle\u00f1os order baratos to kaftan balance m\u00e9nage their sasanians respective degradation rates.", "histories": [["v1", "Thu, 15 Jun 2017 15:43:21 GMT  (455kb,D)", "http://arxiv.org/abs/1706.04933v1", "13 pages, 3 figures, draft version of ICML'17 paper"]], "COMMENTS": "13 pages, 3 figures, draft version of ICML'17 paper", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["r\u00f3bert busa-fekete", "bal\u00e1zs sz\u00f6r\u00e9nyi", "paul weng", "shie mannor"], "accepted": true, "id": "1706.04933"}, "pdf": {"name": "1706.04933.pdf", "metadata": {"source": "META", "title": "Multi-objective Bandits: Optimizing the Generalized Gini Index", "authors": ["R\u00f3bert Busa-Fekete", "Bal\u00e1zs Sz\u00f6r\u00e9nyi", "Paul Weng", "Shie Mannor"], "emails": ["<paul@weng.fr>."], "sections": [{"heading": "1. Introduction", "text": "The multi-armed bandit (MAB) problem (or bandit problem) refers to an iterative decision making problem in which an agent repeatedly chooses amongK options, metaphorically corresponding to pulling one ofK arms of a bandit machine. In each round, the agent receives a random payoff, which is a reward or a cost that depends on the arm being selected. The agent\u2019s goal is to optimize an evaluation metric, e.g., the error rate (expected percentage of times a suboptimal arm is played) or the cumulative regret (difference between the sum of payoffs obtained and the (expected) payoffs that could have been obtained by selecting the best arm in each round). In the stochastic multi-armed bandit setup, the payoffs are assumed to obey fixed distributions that can vary with the arms but do not change with time. To achieve the desired goal, the agent has to tackle the classical exploration/exploitation dilemma: It has to properly balance the pulling of arms\n*Equal contribution 1Yahoo Research, New York, NY, USA 2Research Group on AI, Hungarian Acad. Sci. and Univ. of Szeged, Szeged, Hungary 3Technion Institute of Technology, Haifa, Israel 4SYSU-CMU JIE, SEIT, SYSU, Guangzhou, P.R. China 5SYSU-CMU JRI, Shunde, P.R. China. Correspondence to: Paul Weng <paul@weng.fr>.\nDraft\nthat were found to yield low costs in earlier rounds and the selection of arms that have not yet been tested often enough (Auer et al., 2002a; Lai & Robbins, 1985).\nThe bandit setup has become the standard modeling framework for many practical applications, such as online advertisement (Slivkins, 2014) or medical treatment design (Press, 2009) to name a few. In these tasks, the feedback is formulated as a single real value. However many real-world online learning problems are rather multi-objective, i.e., the feedback consists of a vectorial payoffs. For example, in our motivating example, namely an electric battery control problem, the learner tries to discover a \u201cbest\u201d battery controller, which balances the degradation rates of the battery cells (i.e., components of a battery), among a set of controllers while facing a stochastic power demand. Besides, there are several studies published recently that consider multi-objective sequential decision problem under uncertainty (Drugan & Nowe\u0301, 2013; Roijers et al., 2013; Mahdavi et al., 2013).\nIn this paper, we formalize the multi-objective multi-armed bandit setting where the feedback received by the agent is in the form of aD-dimensional cost vector. The goal here is to be both efficient, i.e., minimize the cumulative cost for each objective, and fair, i.e., balance the different objectives. One natural way to ensure this is to try to find a cost vector on the Pareto front that is closest to the origin or to some other ideal point. A generalization of this approach (when using the infinite norm) is the Generalized Gini Index (GGI), a wellknown inequality measure in economics (Weymark, 1981).\nGGI is convex, which suggests applying the Online Convex Optimization (OCO) techniques (Hazan, 2016; Shalev-Shwartz, 2012). However, a direct application of this technique may fail to optimize GGI under noise, because the objective can be only observed with a bias that is induced by the randomness of the cost vectors and by the fact that the performance is measured by the function value of the average cost instead of the average of the costs\u2019 function value. The solution we propose is an online learning algorithm which is based on Online Gradient Descent (OGD) (Zinkevich, 2003; Hazan, 2016) with additional exploration that enables us to control the bias of the objective function. We also show that its regret is almost optimal: up to a logarithmic factor, it matches the distribution-free lower bound of the stochastic bandit problem (Auer et al., 2002b), which naturally applies ar X\niv :1\n70 6.\n04 93\n3v 1\n[ cs\n.L G\n] 1\n5 Ju\nn 20\n17\nto our setup when the feedback is one-dimensional.\nThe paper is organized as follows: after we introduce the formal learning setup, we briefly recall the necessary notions from multi-objective optimization in Section 3. Next, in Section 4, GGI is introduced and some of its properties are described. In Sections 5 and 6, we present how to compute the optimal policy for GGI, and define the regret notion. Section 7 contains our main results where we define our OGD-based algorithm and analyze its regret. In Section 8, we test our algorithm and demonstrate its versatility in synthetic and real-world battery-control experiments. In Section 9, we provide a survey of related work, and finally conclude the paper in Section 10."}, {"heading": "2. Formal setup", "text": "The multi-armed or K-armed bandit problem is specified by real-valued random variables X1, . . . , XK associated, respectively, with K arms (that we simply identify by the numbers 1, . . . ,K). In each time step t, the online learner selects one and obtains a random sample of the corresponding distributions. These samples, which are called costs, are assumed to be independent of all previous actions and costs.1 The goal of the learner can be defined in different ways, such as minimizing the sum of costs over time (Lai & Robbins, 1985; Auer et al., 2002a).\nIn the multi-objective multi-armed bandit (MO-MAB) problem, costs are not scalar real values, but real vectors. More specifically, a D-objective K-armed bandit problem (D \u2265 2, K \u2265 2) is specified by K real-valued multivariate random variables X1, . . . ,XK over [0, 1]D. Let \u00b5k = E[Xk] denote the expected vectorial cost of arm k where\u00b5k = (\u00b5k,1, . . . , \u00b5k,D). Furthermore,\u00b5 denotes the matrix whose rows are the\u00b5k\u2019s.\nIn each time step the learner can select one of the arms and obtain a sample, which is a cost vector, from the corresponding distribution. Samples are also assumed to be independent over time and across the arms, but not necessarily across the components of cost vectors. At time step t, kt denotes the index of the arm played by the learner and X(t)kt = (X (t) kt,1 , . . . X (t) kt,D\n) the resulting payoff. After playing t time steps, the empirical estimate of the expected cost\u00b5k of the kth arm is:\n\u00b5\u0302 (t) k =\n1\nTk(t) t\u2211 \u03c4=1 X (\u03c4) k\u03c4 1(k\u03c4 = k) (1)\n1Our setup is motivated by a practical application where feedback is more naturally formulated in terms of cost. However the stochastic bandit problem is generally based on rewards, which can be easily turned into costs by using the transformation x 7\u2192 1\u2212 x assuming that the rewards are from [0, 1].\nwhere all operations are meant elementwise, Tk(t) is the number of times the kth arm has been played (i.e., Tk(t) =\u2211t \u03c4=1 1(k\u03c4 = k)) and 1(\u00b7) is the indicator function."}, {"heading": "3. Multi-objective optimization", "text": "In order to complete the MO-MAB setting, we need to introduce the notion of optimality of the arms. First, we introduce the Pareto dominance relation defined as follows, for any v,v\u2032 \u2208 RD:\nv v\u2032 \u21d4 \u2200d = 1, . . . , D, vd \u2264 v\u2032d . (2)\nLetO \u2286 RD be a set ofD-dimension vectors. The Pareto front ofO, denotedO\u2217, is the set of vectors such that:\nv\u2217 \u2208 O\u2217 \u21d4 ( \u2200v \u2208 O,v v\u2217 \u21d2 v = v\u2217 ) . (3)\nIn multi-objective optimization, one usually wants to compute the Pareto front, or search for a particular element of the Pareto front. In practice, it may be costly (and even infeasible depending on the size of the solution space) to determine all the solutions of the Pareto front. One may then prefer to directly aim for a particular solution in the Pareto front. This problem is formalized as a single objective optimization problem, using an aggregation function.\nAn aggregation (or scalarizing) function, which is a non-decreasing function \u03c6 : RD \u2192 R, allows every vector to receive a scalar value to be optimized2. The initial multi-objective problem is then rewritten as follows:\nmin\u03c6(v) s.t. v \u2208 O . (4)\nA solution to this problem yields a particular solution on the Pareto front. Note that if \u03c6 is not strictly increasing in every component, some care is needed to ensure that the solution of (4) is on the Pareto front.\nDifferent aggregation function can be used depending on the problem at hand, such as sum, weighted sum, min, max, (augmented) weighted Chebyshev norm (Steuer & Choo, 1983), Ordered Weighted Averages (OWA) (Yager, 1988) or Ordered Weighted Regret (OWR) (Ogryczak et al., 2011) and its weighted version (Ogryczak et al., 2013). In this study, we focus on the Generalized Gini Index (GGI) (Weymark, 1981), a special case of OWA."}, {"heading": "4. Generalized Gini Index", "text": "For a given n \u2208 N, [n] denotes the set {1, 2, . . . , n}. The Generalized Gini Index (GGI) (Weymark, 1981) is defined\n2A multivariate function f : RD \u2192 R is said to be monotone (non-decreasing) if for all fixed x,x\u2032 \u2208 RD such that x x\u2032 implies that f(x) \u2264 f(x\u2032).\nas follows for a cost vector x = (x1, . . . , xD) \u2208 RD:\nGw(x) = D\u2211 d=1 wdx\u03c3(d) = w \u1d40x\u03c3\nwhere \u03c3 \u2208 SD, which depends on x, is the permutation that sorts the components of x in a decreasing order, x\u03c3 = (x\u03c3(1), \u00b7 \u00b7 \u00b7 , x\u03c3(D)) is the sorted vector and weights wi\u2019s are assumed to be non-increasing, i.e., w1 \u2265 w2 \u2265 . . . \u2265 wD. Given this assumption, Gw(x) = max\u03c0\u2208SD w \u1d40x\u03c0 = max\u03c0\u2208SD w \u1d40 \u03c0x and is therefore a piecewise-linear convex function. Figure 1 illustrates GGI on a bi-objective optimization task.\nTo better understand GGI, we introduce its formulation in terms of Lorenz vectors. The Lorenz vector of x is the vector L(x) = (L1(x), . . . , LD(x)) whereLd(x) is the sum of the d smallest components of x. Then, GGI can be rewritten as follows:\nGw(x) = D\u2211 d=1 w\u2032dLd(x) (5)\nwhere w\u2032 = (w\u20321, . . . , w \u2032 D) is the vector defined by \u2200d \u2208 [D], w\u2032d = wd \u2212 wd+1 withwD+1 = 0. Note that all the components of w\u2032 are nonnegative as we assume that those of w are non-increasing.\nGGI3 was originally introduced for quantifying the inequality of income distribution in economics. It is also known in statistics (Buczolich & Sze\u0301kely, 1989) as a special case of Weighted Average Ordered Sample statistics, which does not require that weights be non-increasing and is therefore not necessarily convex. GGI has been characterized by Weymark (1981). It encodes both efficiency as it is monotone with Pareto dominance and fairness as it is non-increasing with Pigou-Dalton transfers (1912; 1920); they are two principles formulating natural requirements, which is an important reason why GGI became a well-established measure of balancedness. Informally, a Pigou-Dalton transfer amounts to increasing a lower-valued objective while decreasing another higher-valued objective by the same quantity such that the order between the two objectives is not reversed. The effect of such a transfer is to balance a cost vector. Formally, GGI satisfies the following fairness property: \u2200x such that xi < xj ,\n\u2200 \u2208 (0, xj \u2212 xi), Gw(x + ei \u2212 ej) \u2264 Gw(x)\nwhere ei and ej are two vectors of the canonical basis. As a consequence, among vectors of equal sum, the best cost vector (w.r.t. GGI) is the one with equal values in all objectives if feasible.\n3Note that in this paper GGI is expressed in terms of costs and therefore lower GGI values are preferred.\nThe original Gini index can be recovered as a special case of GGI by setting the weights as follows:\n\u2200d \u2208 [D], wd = (2(D \u2212 d) + 1)/D2. (6)\nThis yields a nice graphical interpretation. For a fixed vector x, the distribution of costs can be represented as a curve connecting the points (0, 0), (1/D,L1(x)), (2/D,L2(x)), . . . , (1, LD(x)). An ideally fair distribution with an identical total cost is given by the straight line connecting (0, 0), (1/D,LD(x)/D), (2/D, 2LD(x)/D), . . . , (1, LD(x)), which equally distributes the total cost over all components. Then 1\u2212Gw(x)/x\u0304with weights (6) and x\u0304 =\u2211 xi/D is equal to twice the area between the two curves.\nFrom now on, to simplify the presentation, we focus on GGI with strictly decreasing weights in [0, 1]D, i.e., d < d\u2032 implieswd > wd\u2032 . This means that GGI is strictly decreasing with Pigou-Dalton transfers and all the components of w\u2032 are positive. Based on formulation (5), Ogryczak & Sliwinski (2003) showed that the GGI value of a vector x can be obtained by solving a linear program. We shall recall their results and define the linear program-based formulation of GGI.\nProposition 1. The GGI score Gw(x) of vector x is the optimal value of the following linear program\nminimize D\u2211 d=1 w\u2032d ( drd+ D\u2211 j=1 bj,d ) subject to rd + bj,d \u2265 xj \u2200j, d \u2208 [D]\nbj,d \u2265 0 \u2200j, d \u2208 [D]"}, {"heading": "5. Optimal policy", "text": "In the single objective case, arms are compared in terms of their means, which induce a total ordering over arms. In the\nmulti-objective setting, we use the GGI criterion to compare arms. One can compute the GGI score of each arm k as Gw(\u00b5k) if its vectorial mean\u00b5k is known. Then an optimal arm k\u2217 minimizes the GGI score, i.e.,\nk\u2217 \u2208 argmin k\u2208[K] Gw(\u00b5k) .\nHowever, in this work, we also consider mixed strategies, which can be defined as A = {\u03b1 \u2208 RK | \u2211K k=1 \u03b1k = 1 \u2227 0 \u03b1}, because they may allow to reach lower GGI values than any fixed arm (see Figure 1). A policy parameterized by\u03b1 chooses arm k with probability \u03b1k. An optimal mixed policy can then be obtained as follows:\n\u03b1\u2217 \u2208 argmin \u03b1\u2208A Gw ( K\u2211 k=1 \u03b1k\u00b5k ) . (7)\nIn general, Gw (\u2211K k=1 \u03b1 \u2217 k\u00b5k ) \u2264 Gw(\u00b5k\u2217), therefore using mixed strategies is justified in our setting. Based on Proposition 1, if the arms\u2019 means were known,\u03b1\u2217 could be computed by solving the following linear program:\nminimize D\u2211 d=1 w\u2032d drd + D\u2211 j=1 bj,d  subject to rd + bj,d \u2265\nK\u2211 k=1\n\u03b1k\u00b5k,j \u2200j, d \u2208 [D]\u2211K k=1 \u03b1k = 1 \u03b1 0 bj,d \u2265 0 \u2200j, d \u2208 [D]\n(8)"}, {"heading": "6. Regret", "text": "After playing T rounds, the average cost can be written as\nX\u0304(T ) = 1\nT T\u2211 t=1 X (t) kt .\nOur goal is to minimize the GGI index of this term. Accordingly we expect the learner to collect costs so as their average in terms of GGI, that is, Gw ( X\u0304(T ) ) should be as small as possible. As shown in the previous section, for a given bandit instance with arm means \u00b5 = (\u00b51, . . . ,\u00b5K), the optimal policy \u03b1\u2217 achieves Gw (\u2211K k=1 \u03b1 \u2217 k\u00b5k ) = Gw (\u00b5\u03b1\n\u2217) if the randomness of the costs are not taken into account. We consider the performance of the optimal policy as a reference value, and define the regret of the learner as the difference of the GGI of its average cost and the GGI of the optimal policy:\nR(T ) = Gw ( X\u0304(T ) ) \u2212Gw (\u00b5\u03b1\u2217) . (9)\nNote that GGI is a continuous function, therefore if the learner follows a policy \u03b1(T ) that is \u201capproaching\u201d \u03b1\u2217 as T \u2192\u221e, then the regret is vanishing.\nWe shall also investigate a slightly different regret notion called pseudo-regret:\nR (T )\n= Gw\n( \u00b5\u03b1\u0304(T ) ) \u2212Gw (\u00b5\u03b1\u2217) (10)\nwhere \u03b1\u0304(T ) = 1T \u2211T t=1\u03b1\n(t). We will show that the difference between the regret and pseudo-regret of our algorithm is O\u0303(T\u22121/2) with high probability, thus having a high probability regret bound O\u0303(T\u22121/2) for one of them implies a regret bound O\u0303(T\u22121/2) for the other one.\nRemark 1. The single objective stochastic multi-armed bandit problem (Auer et al., 2002a; Bubeck & Cesa-Bianchi, 2012) can be naturally accommodated into our setup with D = 1 and w1 = 1. In this case, \u03b1\u2217 implements the pure strategy that always pulls the optimal arm with the highest mean denoted by \u00b5k\u2217 . ThusGw (\u00b5\u03b1\u2217) = \u00b5k\u2217 in this case. Assuming that the learner plays only with pure strategies, the pseudo-regret defined in (10) can be written as:\nR (T ) = 1\nT T\u2211 t=1 (\u00b5kt \u2212 T\u00b5k\u2217) = 1 T K\u2211 k=1 Tk(T ) (\u00b5k \u2212 \u00b5k\u2217)\nwhich coincides with the single objective pseudo-regret (see for example (Auer et al., 2002a)), apart from the fact that we work with costs instead of rewards. Therefore our notion of multi-objective pseudo-regret can be seen as a generalization of the single objective pseudo-regret.\nRemark 2. Single-objective bandit algorithm can be applied in our multi-objective setting by transforming the multivariate payoffs X(t)kt into a single real value Gw(X (t) kt\n) in every time step t. However, in general, this approach fails to optimize GGI as formulated in (7) due to GGI\u2019s non-linearity, even if the optimal policy is a pure strategy. Moreover, applying a multi-objective bandit algorithm such as MO-UCB (Drugan & Nowe\u0301, 2013) would be inefficient as they were developed to find all Pareto-optimal arms and then to sample them uniformly at random. This approach may be reasonable to apply only when \u03b1\u2217k = 1/#K where K = {k \u2208 [K] : \u03b1\u2217k > 0} contains all the Pareto optimal arms, which is clearly not the case for every MO-MAB instance."}, {"heading": "7. Learning algorithm based on OCO", "text": "In this section we propose an online learning algorithm called MO-OGDE, to optimize the regret defined in the previous section. Our method exploits the convexity of the GGI operator and formalizes the policy search problem as an online convex optimization problem, which is solved by Online Gradient Descent (OGD) (Zinkevich, 2003) algorithm with projection to a gradually expanding truncated probability simplex. Then we shall provide a regret analysis of our method. Due to space limitation, the proofs are deferred to the appendix.\nAlgorithm 1 MO-OGDE(\u03b4) 1: Pull each arm once 2: Set\u03b1(K+1) = (1/K, \u00b7 \u00b7 \u00b7 , 1/K) 3: for rounds t = K + 1,K + 2, . . . do 4: Choose an arm kt according to\u03b1(t)\n5: Observe the sample X(t)kt and compute f (t) 6: Set \u03b7t = \u221a\n2 (1\u22121/ \u221a K)\n\u221a ln(2/\u03b4)\nt\n7: \u03b1(t+1) = OGDEstep(\u03b1(t), \u03b7t,\u2207f (t)) return 1T \u2211T t=1\u03b1 (t)"}, {"heading": "7.1. MO-OGDE", "text": "Our objective function to be minimized can be viewed as a function of\u03b1, i.e., f(\u03b1) = Gw(\u00b5\u03b1) where the matrix\u00b5 = (\u00b51, . . . ,\u00b5K) contains the means of the arm distributions as its columns. Note that the convexity of GGI implies the convexity of f(\u03b1). Since we play with mixed strategies, the domain of our optimization problem is the K-dimensional probability simplex \u2206K = {\u03b1 \u2208 RK | \u2211K k=1 \u03b1k = 1 \u2227 0 \u03b1}, which is a convex set. Then the gradient of f(\u03b1) with respect to \u03b1k can be computed as\n\u2202f(\u03b1) \u2202\u03b1k = \u2211D d=1 wd\u00b5k,\u03c0(d)where\n\u03c0 is the permutation that sorts the components of \u00b5\u03b1 in a decreasing order. The means \u00b5k\u2019s are not known but they can be estimated based on the costs observed so far as given in (1). The objective function based on the empirical mean estimates is denoted by f (t)(\u03b1) = Gw(\u00b5\u0302(t)\u03b1) where \u00b5\u0302(t) = (\u00b5\u0302\n(t) 1 , . . . , \u00b5\u0302 (t) K ) contains the empirical estimates for\n\u00b51, . . . ,\u00b5K in time step t as its columns.\nOur Multi-Objective Online Gradient Descent algorithm with Exploration is defined in Algorithm 1, which we shall refer to as MO-OGDE. Our algorithm is based on the well-known Online Gradient Descent (Zinkevich, 2003; Hazan, 2016) that carries out the gradient step and the projection back onto the domain in each round. The MO-OGDE algorithm first pulls each arm at once as an initialization step. Then in each iteration, it chooses each arm k with probability \u03b1(t)k , and it computes f (t) based on the empirical mean estimates. Next, it carries out the gradient step based on\u2207f (t)(\u03b1(t)) with a step size \u03b7t as defined in line 6, and computes the projection \u03a0\u2206\u03b2K onto the nearest point of the convex set:\n\u2206\u03b2K =\n{ \u03b1 \u2208 RK |\nK\u2211 k=1 \u03b1k = 1 \u2227 \u03b2/K \u03b1\n}\nwith \u03b2 = \u03b7t. The gradient step and projection are carried out using\nOGDEstep(\u03b1, \u03b7, g) = \u03a0\u2206\u03b7K (\u03b1\u2212 \u03b7g(\u03b1)) (11)\nThe key ingredient of MO-OGDE is the projection step onto the truncated probability simplex \u2206\u03b7tK which ensures\nthat \u03b1(t)k > \u03b7t/K for every k \u2208 [K] and t \u2208 [T ]. This forced exploration is indispensable in our setup, since the objective function f(\u03b1) depends on the means of the arm distributions, which are not known. To control the difference between f(\u03b1) and f (t)(\u03b1), we need \u201cgood\u201d estimates for \u00b51, . . . ,\u00b5K , which are obtained via forced exploration. That is why, the original OGD (Hazan, 2016) algorithm, in general, fails to optimize GGI in our setting. Our analysis, presented in the next section, focuses on the interplay between the forced exploration and the bias of the loss functions f (1)(\u03b1), . . . , f (T )(\u03b1)."}, {"heading": "7.2. Regret analysis", "text": "The technical difficulty in optimizing GGI in an online fashion is that, in general, f (t)(\u03b1) \u2212 f(\u03b1) (= Gw(\u00b5\u0302\n(t)\u03b1) \u2212 Gw(\u00b5\u03b1)) is of order mink(Tk(t))\u22121/2, which, unless all the arms are sampled a linear number of times, incurs a regret of the same order of magnitude, which is typically too large. Nevertheless, an optimal policy \u03b1\u2217 determines a convex combination of several arms in the form of \u00b5\u03b1\u2217, which is the optimal cost vector in terms of GGI given the arm distribution at hand. Let us denote K = {k \u2208 [K] : \u03b1\u2217k > 0}. Note that #K \u2264 D. Moreover, arms in [K] \\ Kwith an individual GGI lower than arms in K do not necessarily participate in the optimal combination. Obviously, an algorithm that achieves a O(T\u22121/2) needs to pull the arms in #K linear time, and at the same time, estimate the arms in [K] \\ Kwith a certain precision.\nThe main idea in the approach proposed in this paper is, despite the above remarks, to apply some online convex optimization algorithm on the current estimate f (t)(\u03b1) = Gw(\u00b5\u0302(t)\u03b1) of the objective function f(\u03b1) = Gw(\u00b5\u03b1), use forced exploration of order T 1/2, and finally show that the estimate f (t) of the objective function has error O\u0303(T\u22121/2) along the trajectory generated by the online convex optimization algorithm. In particular, we show that f( 1T \u2211T t=1\u03b1 (t)) \u2264 1T \u2211T t=1 f\n(t)(\u03b1(t)) + O\u0303(T\u22121/2). The intuitive reason for this is that \u2016 1T \u2211T t=1 \u00b5\u0302\n(t)\u03b1(t) \u2212 1 T \u2211T t=1 \u00b5\u03b1\n(t)\u2016 = O\u0303(T\u22121/2), which is based on the following observation: an arm in K is either pulled often, thus its mean estimate is then accurate enough, or an arm in [K] \\ K is pulled only a few times, nevertheless \u2211T t=1 \u03b1 (t) k is then small enough to make the poor accuracy of its mean estimate insignificant. Below we make this argument formal by proving the following theorem:\nTheorem 1. With probability at least 1\u2212 \u03b4:\nf (\n1 T T\u2211 t=1 \u03b1(t) ) \u2212 f(\u03b1\u2217) \u2264 2L \u221a 6D ln3(8DKT 2/\u03b4) T ,\nfor any big enough T , where L is the Lipschitz constant of Gw(x).\nIts proof follows four subsequent steps presented next.\nStep 1 As a point of departure, we analyze OGDEstep in (11). In particular, we compute the regret that is commonly used in OCO setup for f (t)(\u03b1(t)).\nLemma 1. Define { \u03b1(t) } t=1,...,T as:\n\u03b1(1) = (1/K, . . . , 1/K)\n\u03b1(t+1) = OGDEstep(\u03b1(t), \u03b7t,\u2207f (t))\nwith \u03b71, . . . , \u03b7T \u2208 [0, 1]. Then the following upper bound is guaranteed for all T \u2265 1 and for any\u03b1 \u2208 \u2206K:\nT\u2211 t=1 f (t)(\u03b1(t))\u2212 T\u2211 t=1 f (t)(\u03b1) \u2264 1 \u03b7T + G2 + 1 2 T\u2211 t=1 \u03b7t\nwhere sup\u03b1\u2208\u2206K \u2016\u2207f (t)(\u03b1)\u2016 < G \u2264\n\u221a KD for all t \u2208 [T ].\nThe proof of Lemma 1 is presented in Appendix A. If the projection is carried out onto \u2206K according to the OGE algorithm instead of the truncated probability \u2206\u03b7tK , the regret bound in Lemma 1 would be improved only by a constant factor (see Theorem 3.1 in (Hazan, 2016)). As a side remark, note that Lemma 1 holds for arbitrary convex function since only the convexity of f (t)(\u03b1) is used in the proof.\nStep 2 Next, we show that f (t)(\u03b1) converges to f(\u03b1) as fast as O\u0303(T\u22121/2) along the trajectory { \u03b1(t) } t=1,...,T\ngenerated by MO-OGDE.\nProposition 2. With probability at least 1\u2212 2(DT + 1)K\u03b4,\u2223\u2223\u2223\u2223\u2223Gw ( 1 T T\u2211 t=1 \u00b5\u03b1(t) ) \u2212Gw ( 1 T T\u2211 t=1 \u00b5\u0302(t)\u03b1(t) )\u2223\u2223\u2223\u2223\u2223 \u2264 L \u221a 6D(1 + ln2 T ) ln(2/\u03b4)\nT .\nThe proof of Proposition 2 is deferred to Appendix B. Proposition 2, combined with the fact that\nGw\n( 1\nT T\u2211 t=1 \u00b5\u0302(t)\u03b1(t)\n)\n\u2264 1 T T\u2211 t=1 Gw(\u00b5\u0302 (t)\u03b1(t)) = 1 T T\u2211 t=1 f (t)(\u03b1(t))\nwhere we used the convexity of GGI, and Gw ( 1 T \u2211T t=1 \u00b5\u03b1 (t) ) = f ( 1 T \u2211T t=1\u03b1 (t) ) = f ( \u03b1\u0304(t) ) implies the following result.\nCorollary 1. With probability at least 1\u2212 2(DT + 1)K\u03b4,\nf ( \u03b1\u0304(t) ) \u2264 1 T T\u2211 t=1 f (t) ( \u03b1(t) ) + L\n\u221a 6D(1 + ln2 T ) ln 2\u03b4\nT .\nStep 3 Next, we provide a regret bound for the pseudo-regret of MO-OGDE by using Lemma 1 and Corollary 1. To this end, we introduce some further notations. First of all, let\n\u2206\u2217K = argmin \u03b1\u2208\u2206K f(\u03b1)\ndenote the set of all the minimum points of f over \u2206K . As we show later in Lemma 3, \u2206\u2217K is a convex polytope, and thus the set ext(\u2206\u2217K) of its extreme points is finite.\nProposition 3. With probability at least 1\u2212 4DT 2K\u03b4:\nf\n( 1\nT T\u2211 t=1 \u03b1(t)\n) \u2212 f(\u03b1\u2217) \u2264 L \u221a 6D(1 + ln2 T ) ln(2/\u03b4)\nT\n+ 1 T\u03b7T + KD2 + 1 T T\u2211 t=1 \u03b7t + LK T T\u2211 t=1\n\u221a D ln(2/\u03b4)\n2\u03c71(t)\nwhere \u03c71(t) = 1(t \u2264 \u03c41) + 1(t > \u03c41)(ta0/(2|ext(\u2206\u2217K)|)) and \u03c41 = [ (2|ext(\u2206\u2217K)|) [ 2 + 10 \u221a 3LKD2 g\u2217 ]\u221a 2 ln 2\u03b4 ]4 with a0 = min\u03b1\u2208ext(\u2206\u2217K) mink:\u03b1k>0 \u03b1k and g\u2217 = inf\u03b1\u2208\u2206K\\\u2206\u2217K max\u03b1\u2217\u2208\u2206 \u2217 K f(\u03b1)\u2212f(\u03b1\u2217) \u2016\u03b1\u2212\u03b1\u2217\u2016 .\nThe proof is deferred to Appendix C. In the proof first we decompose the regret into various terms which can be upper bounded based on Lemma 1 and Corollary 1. This implies that 1T \u2211T t=1\u03b1 (t) k will get arbitrarily close to some \u03b1\u2217k \u2208 \u2206\u2217K if T is big enough because, as we show later in Lemma 3, g\u2217 is strictly positive (see Appendix D). As a consequence, for a big enough T > \u03c41, the difference between the f (t)(\u03b1\u2217) and f(\u03b1\u2217) is vanishing as fast as O(T\u22121/2) which is crucial for a regret bound of orderT\u22121/2.\nStep 4 Finally, Theorem 1 yields from Proposition 3 by simplifying the right hand side. The calculation is presented in Appendix E."}, {"heading": "7.3. Regret vs. pseudo-regret", "text": "Next, we upper-bound the difference of regret defined in (9) and the pseudo-regret defined in (10). To that aim, we first upper-bound the difference of Tk(t) and \u2211t \u03c4=1 \u03b1 (\u03c4) k .\nClaim 1. For any t = 1, 2, . . . and any k = 1, . . . ,K it holds that P [\u2223\u2223\u2223Tk(t)\u2212\u2211t\u03c4=1 \u03b1(\u03c4)k \u2223\u2223\u2223 \u2265\u221a2t ln(2/\u03b4)] \u2264 \u03b4. Proof : As P[kt = k] = \u03b1k, it holds that Tk(t) \u2212 \u2211t \u03c4=1 \u03b1 (\u03c4) k = \u2211t \u03c4=1[1(k\u03c4 = k) \u2212 \u03b1 (\u03c4) k ] is a martingale. Besides, |1(k\u03c4 = k) \u2212 \u03b1k| \u2264 1 by construction. The claim then follows by Azuma\u2019s inequality.\nBased on Claim 1 and Prop. 2, we upper-bound the difference between the pseudo-regret and regret of MO-OGDE.\nCorollary 2. With probability at least 1\u2212 \u03b4 |R(T ) \u2212 R\u0304(T )| \u2264 L \u221a 12D ln(4(DT + 1)/\u03b4)\nT\nThe proof of Corollary 2 is deferred to Appendix F. According to Corollary 2, the difference between the regret and pseudo regret is O\u0303(T\u22121/2) with high probability, hence Theorem 1 implies a O\u0303(T\u22121/2) bound for the regret of MO-OGDE."}, {"heading": "8. Experiments", "text": "To test our algorithm, we carried out two sets of experiments. In the first we generated synthetic data from multi-objective bandit instances with known parameters. In this way, we could compute the pseudo-regret (10) and, thus investigate the empirical performance of the algorithms. In the second set of experiments, we run our algorithm on a complex multi-objective online optimization problem, namely an electric battery control problem. Before presenting those experiments, we introduce another algorithm that will serve as a baseline."}, {"heading": "8.1. A baseline method", "text": "In the previous section we introduced a gradient-based approach that uses the mean estimates to approximate the gradient of the objective function. Nevertheless, using the mean estimates, the optimal policy can be directly approximated by solving the the linear program given in (8). We use the same exploration as MO-OGDE, see line 6 of Algorithm 1. More concretely, the learner solves the following linear program in each time step t:\nminimize D\u2211 d=1 w\u2032d drd + D\u2211 j=1 bj,d  subject to rd + bj,d \u2265\nK\u2211 k=1 \u03b1k\u00b5\u0302 (t) k,j \u2200j, d \u2208 [D]\n\u03b1T1 = 1 \u03b1 \u2265 \u03b7t/K bj,d \u2265 0 \u2200j, d \u2208 [D]\nNote that the solution of the learner program above regarding \u03b1 is in \u2206\u03b7tK . We refer to this algorithm as MO-LP. Note that this approach is computationally expensive, since a linear program needs to be solved at each time step. But the policy of each step is always optimal restricted to the truncated simplex \u2206\u03b7tK with respect to the mean estimates, unlike the gradient descent method."}, {"heading": "8.2. Synthetic Experiments", "text": "We generated random multi-objective bandit instances for which each component of the multivariate cost distributions\nobeys Bernoulli distributions with various parameters. The parameters of each Bernoulli distributions are drawn uniformly at random from [0, 1] independently from each other. The number of armsK was set to {5, 20} and the dimension of the cost distribution was taken from D \u2208 {5, 10}. The weight vector w of GGI was set to wd = 1/2d\u22121. Since the parameters of the bandit instance are known, the regret defined in Section 6 can be computed. We ran the MOOGDE and MO-LP algorithms with 100 repetitions. The multi-objective bandit instance were regenerated after each run. The regrets of the two algorithms, which are averaged out over the repetitions, are plotted in Figure 2 along with the error bars. The results reveal some general trends. First, the average regrets of both algorithms converge to zero. Second the MO-LP algorithm outperforms the gradient descent algorithm for small number of round, typically T < 5000 on the more complex bandit instances (K = 20). This fact might be explained by the fact that the MO-LP solves a linear program for estimating \u03b1\u2217 whereas the MO-OGDE minimizes the same objective but using a gradient descent approach with projection, which might achieve slower convergence in terms of regret, nevertheless its computational time is significantly decreased compared to the baseline method."}, {"heading": "8.3. Battery control task", "text": "We also tried our algorithms on a more realistic domain: the cell balancing problem. As the performance profile of battery cells, subcomponents of an electric battery, may vary due to small physical and manufacturing differences, efficient balancing of those cells is needed for better performance and longer battery life. We model this problem as a MO-MAB where the arms are different cell control strategies and the\ngoal is to balance several objectives: state-of-charge (SOC), temperature and aging. More concretely, the learner loops over the following two steps: (1) she chooses a control strategy for a short duration and (2) she observes its effect on the objectives (due to stochastic electric consumption). For the technical details of this experiments see Appendix G.\nWe tackled this problem as a GGI optimization problem. The results (averaged over 100 runs) are presented in Figure 3 where we evaluated MO-OGDE vs. MO-LP. The dashed green lines represent the regrets of playing fixed deterministic arms. Although MO-OGDE and MO-LP both learn to play a mixed policy that is greatly better than any individual arm, MO-OGDE is computationally much more efficient."}, {"heading": "9. Related work", "text": "The single-objective MAB problem has been intensively studied especially in recent years (Bubeck & Cesa-Bianchi, 2012), nevertheless there is only a very limited number of work concerning the multi-objective setting. To the best of our best knowledge, Drugan & Nowe\u0301 (2013) considered first the multi-objective multi-armed problem in a regret optimization framework with a stochastic assumption. Their work consists of extending the UCB algorithm (Auer et al., 2002a) so as to be able to handle multi-dimensional feedback vectors with the goal of determining all arms on the Pareto front.\nAzar et al. (2014) investigated a sequential decision making problem with vectorial feedback. In their setup the agent is allowed to choose from a finite set of actions and then it observes the vectorial feedback for each action, thus it is a full information setup unlike our setup. Moreover, the feedback is non-stochastic in their setup, as it is chosen by an adversary. They propose an algorithm that can handle a general class of aggregation functions, such as the set of bounded domain, continuous, Lipschitz and quasi-concave functions.\nIn the online convex optimization setup with multiple objectives (Mahdavi et al., 2013), the learner\u2019s forecast x(t) is evaluated in terms of multiple convex loss functions f\n(t) 0 (x), f (t) 1 (x), . . . , f (t) K (x) in each time step t. The goal of the learner is then to minimize \u2211t \u03c4=1 f (\u03c4) 0 (x\n(\u03c4)) while keeping the other objectives below some predefined threshold, i.e. 1t \u2211t \u03c4=1 f (\u03c4) i (x\n(\u03c4)) \u2264 \u03b3i for all i \u2208 [K]. Note that, with linear loss functions, the multiple-objective convex optimization setup boils down to linear optimization with stochastic constraints, and thus it can be applied to solve the linear program given in Section 5 whose solution is the optimal policy in our setup. For doing this, however, each linear stochastic constraint needs to be observed, whereas we only assume bandit information.\nIn the approachability problem (Mannor et al., 2014; 2009; Abernethy et al., 2011), there are two players, say A and B. Players A and B choose actions from the compact convex sets X \u2282 RK and Y \u2282 RD, respectively. The feedback to the players is computed as a function u : X \u00d7 Y 7\u2192 Rp. A given convex set S \u2282 Rp is known to both players. Player A wants to land inside with the cumulative payoffs, i.e., player A\u2019s goal is to minimize dist( 1T \u2211T t=1 u(x (t),y(t)), S) where x(t) and y(t) are the actions chosen by player A and B respectively, and dist(s, S) = infs\u2032\u2208S \u2016s\u2032 \u2212 s\u2016, whereas player B, called adversary, wants to prevent player A to land in set S. In our setup, Player B who generates the cost vectors, is assumed to be stochastic. The set S consists of a single value which is\u00b5\u03b1\u2217, and u corresponds to\u00b5Ibalpha, thus p = D. What makes our setup essentially different from approachability is that, S is not known to any player. That is why Player A, i.e. the learner, needs to explore the action space which is achieved by forced exploration."}, {"heading": "10. Conclusion and future work", "text": "We introduced a new problem in the context of multiobjective multi-armed bandit (MOMAB). Contrary to most previously proposed approaches in MOMAB, we do not search for the Pareto front, instead we aim for a fair solution, which is important for instance when each objective corresponds to the payoff of a different agent. To encode fairness, we use the Generalized Gini Index (GGI), a well-known criterion developed in economics. To optimize this criterion, we proposed a gradient-based algorithm that exploits the convexity of GGI. We evaluated our algorithm on two domains and obtained promising experimental results.\nSeveral multi-objective reinforcement learning algorithm have been proposed in the literature (Ga\u0301bor et al., 1998; Roijers et al., 2013). Most of these methods make use of a simple linear aggregation function. As a future work, it would be interesting to extend our work to the reinforcement learning setting, which would be useful to solve the electric battery control problem even more finely."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Vikram Bhattacharjee and Orkun Karabasoglu for providing the battery model. This research was supported in part by the European Communitys Seventh Framework Programme (FP7/2007-2013) under grant agreement 306638 (SUPREL)."}, {"heading": "A. Lemma 1", "text": "Lemma 1. Define { \u03b1(t) } t=1,...,T as:\n\u03b1(1) = (1/K, . . . , 1/K)\n\u03b1(t+1) = OGDEstep(\u03b1(t), \u03b7t,\u2207f (t))\nwith \u03b71, . . . , \u03b7T \u2208 [0, 1]. Then the following upper bound is guaranteed for all T \u2265 1 and for any\u03b1 \u2208 \u2206K:\nT\u2211 t=1 f (t)(\u03b1(t))\u2212 T\u2211 t=1 f (t)(\u03b1) \u2264 1 \u03b7T + G2 + 1 2 T\u2211 t=1 \u03b7t\nwhere sup\u03b1\u2208\u2206K \u2016\u2207f (t)(\u03b1)\u2016 < G \u2264\n\u221a KD for all t \u2208 [T ].\nProof : The proof follows closely the proof of Theorem 3.1 of Hazan (2016), however the projection step is slightly different in our case. First, let us note that \u03b7t \u2264 1 for all t \u2208 [T ], thus \u2206\u03b7tK is never an empty set. Then, for an arbitrary\u03b1 \u2208 \u2206K , we have\n\u2016\u03b1(t+1) \u2212\u03b1\u20162 = \u2225\u2225\u2225\u03a0\u2206\u03b7tK (\u03b1(t) \u2212 \u03b7t\u2207\u03b1f (t)(\u03b1(t)))\u2212\u03b1\u2225\u2225\u22252\n= \u2225\u2225\u2225\u03a0\u2206\u03b7tK (\u03b1(t) \u2212 \u03b7t\u2207\u03b1f (t)(\u03b1(t)))\u2212\u03a0\u2206K (\u03b1(t) \u2212 \u03b7t\u2207\u03b1f (t)(\u03b1(t))) + \u03a0\u2206K ( \u03b1(t) \u2212 \u03b7t\u2207\u03b1f (t)(\u03b1(t)) ) \u2212\u03b1\n\u2225\u2225\u22252 \u2264 \u2225\u2225\u2225\u03a0\u2206\u03b7tK (\u03b1(t) \u2212 \u03b7t\u2207\u03b1f (t)(\u03b1(t)))\u2212\u03a0\u2206K (\u03b1(t) \u2212 \u03b7t\u2207\u03b1f (t)(\u03b1(t)))\u2225\u2225\u22252 + \u2225\u2225\u2225\u03a0\u2206K (\u03b1(t) \u2212 \u03b7t\u2207\u03b1f (t)(\u03b1(t)))\u2212\u03b1\u2225\u2225\u22252\n\u2264 \u03b7 2 t K + \u2225\u2225\u2225\u03a0\u2206K (\u03b1(t) \u2212 \u03b7t\u2207\u03b1f (t)(\u03b1(t)))\u2212\u03b1\u2225\u2225\u22252 \u2264 \u03b7 2 t K + \u2225\u2225\u2225\u03b1(t) \u2212 \u03b7t\u2207\u03b1f (t)(\u03b1(t))\u2212\u03b1\u2225\u2225\u22252 (12)\nwhere (12) follows from the convexity of the set \u2206K . Thus we have\n\u2016\u03b1(t+1) \u2212\u03b1\u20162 \u2264 \u2016\u03b1(t) \u2212\u03b1\u20162 + \u03b72t \u2016\u2207\u03b1f (t)(\u03b1(t))\u20162 \u2212 2\u03b7t ( \u2207\u03b1f (t)(\u03b1(t)) )\u1d40 ( \u03b1(t) \u2212\u03b1 ) + \u03b72t K\nThe convexity of f (t) implies that f (t)(\u03b1(t))\u2212 f (t)(\u03b1) \u2264 ( \u2207\u03b1f (t)(\u03b1(t)) )\u1d40 ( \u03b1(t) \u2212\u03b1 ) \u2264 \u2016\u03b1 (t) \u2212\u03b1\u20162 \u2212 \u2016\u03b1(t+1) \u2212\u03b1\u20162\n2\u03b7t + \u03b7t 2 G2 + \u03b7t 2K\n(13)\nTherefore the regret can be upper bounded as\nT\u2211 t=1 (f (t)(\u03b1(t))\u2212 f (t)(\u03b1)) \u2264 T\u2211 t=1 \u2016\u03b1(t) \u2212\u03b1\u20162 \u2212 \u2016\u03b1(t+1) \u2212\u03b1\u20162 2\u03b7t + G2 + 1 2 T\u2211 t=1 \u03b7t based on (13)\n\u2264 1 2 T\u2211 t=1 \u2016\u03b1(t) \u2212\u03b1\u20162 ( 1 \u03b7t \u2212 1 \u03b7t\u22121 ) + G2 + 1 2 T\u2211 t=1 \u03b7t\n\u2264 1 \u03b7T + G2 + 1 2 T\u2211 t=1 \u03b7t . (14)\nWe now show thatG \u2264 D \u221a K. By assumption w \u2208 [0, 1]D and\u00b5 \u2208 [0, 1]D\u00d7K , therefore w\u1d40\u00b5 \u2208 [0, D]K . As the gradient of f (t) in\u03b1 is given by w\u1d40\u00b5(t)\u03c3 with \u03c3 the permutation that orders\u00b5(t)\u03b1 in a decreasing order, we haveG \u2264 D \u221a K."}, {"heading": "B. O\u0303(T\u22121/2) convergence along the trajectory", "text": "Proposition 2. With probability at least 1\u2212 2(DT + 1)K\u03b4,\u2223\u2223\u2223\u2223\u2223Gw ( 1 T T\u2211 t=1 \u00b5\u03b1(t) ) \u2212Gw ( 1 T T\u2211 t=1 \u00b5\u0302(t)\u03b1(t)\n)\u2223\u2223\u2223\u2223\u2223 \u2264 L \u221a 6D(1 + ln2 T ) ln(2/\u03b4)\nT .\nProof : As the Gini index is L-Lipschitz (with L \u2264 K \u221a D), we simply need to bound the difference\u2211T\nt=1 \u00b5\u0302 (t)\u03b1(t) \u2212 \u2211T t=1 \u00b5\u03b1\n(t). The main difficulty is that the \u00b5\u0302(t) and the \u03b1(t) vectors are not independent, so one cannot directly apply the standard concentration inequalities. In order to obtain the claimed bound, we first divide the above expression into several parts, and deal with these parts separately.\nLet k \u2208 [K]. The division we need is obtained as follows:\nT\u2211 t=1 \u00b5\u0302 (t) k \u03b1 (t) k \u2212 \u00b5k T\u2211 \u03c4=1 \u03b1 (\u03c4) k = T\u2211 t=1\n( \u00b5\u0302\n(t) k\n[ t\u2211\n\u03c4=1\n\u03b1 (\u03c4) k \u2212 t\u22121\u2211 \u03c4=1 \u03b1 (\u03c4) k\n]) \u2212 \u00b5k\nT\u2211 \u03c4=1 \u03b1 (\u03c4) k\n= T\u22121\u2211 t=1\n([ \u00b5\u0302\n(t) k \u2212 \u00b5\u0302 (t+1) k ] t\u2211 \u03c4=1 \u03b1 (\u03c4) k ) + \u00b5\u0302 (T ) k T\u2211 \u03c4=1 \u03b1 (\u03c4) k \u2212 \u00b5k T\u2211 \u03c4=1 \u03b1 (\u03c4) k . (15)\nFor ease of notation, letNk(n) = argmin{\u03c4 \u2265 1 : Tk(\u03c4) \u2265 n}, and letZnk = X (Nk(n)) k .\nThe last two terms in (15) can be handled as follows:\nP [\u2225\u2225\u2225\u2225\u2225\u00b5\u0302(T )k T\u2211 \u03c4=1 \u03b1 (\u03c4) k \u2212 \u00b5k T\u2211 \u03c4=1 \u03b1 (\u03c4) k \u2225\u2225\u2225\u2225\u2225 >\u221a5DT ln(2/\u03b4) ]\n(16)\n=P [\u2225\u2225\u2225\u00b5\u0302(T )k \u2212 \u00b5k\u2225\u2225\u2225 T\u2211 \u03c4=1 \u03b1 (\u03c4) k > \u221a 5DT ln(2/\u03b4) ] \u2264P [\u2225\u2225\u2225\u00b5\u0302(T )k \u2212 \u00b5k\u2225\u2225\u2225Tk(T ) +\u221a2DT ln(2/\u03b4) >\u221a5DT ln(2/\u03b4)]+ \u03b4 (17)\n\u2264P \u2225\u2225\u2225\u2225\u2225\u2225 Tk(T )\u2211\nn=1\nZnk \u2212 Tk(T )\u00b5k \u2225\u2225\u2225\u2225\u2225\u2225+\u221a2DT ln(2/\u03b4) >\u221a5DT ln(2/\u03b4) + \u03b4 =\nT\u2211 t=1 P [\u2225\u2225\u2225\u2225\u2225 ( t\u2211 n=1 Znk ) \u2212 t\u00b5k \u2225\u2225\u2225\u2225\u2225+\u221a2DT ln(2/\u03b4) >\u221a5DT ln(2/\u03b4) ; Tk(T ) = t ] + \u03b4\n\u2264 T\u2211 t=1 P [\u221a D(t/2) ln(2/\u03b4) + \u221a 2DT ln(2/\u03b4) > \u221a 5DT ln(2/\u03b4) ] + (DT + 1)\u03b4 (18)\n=(DT + 1)\u03b4 , (19)\nwhere in (17) we used Claim 1 and the fact that each component of \u00b5\u0302(T )k \u2212 \u00b5k is bounded by 1 in absolute value, and in (18) we used the Chernoff-Hoeffding\u2019s inequality with bound \u221a (t/2) ln(2/\u03b4) on each of theD components.\nHandling the first term requires significantly more work, because the terms within the sum are neither independent nor subor supermartingales. In order to overcome this, we apply a series of rewriting/decoupling iterations. First of all, note that\nT\u22121\u2211 t=1 ([ \u00b5\u0302 (t) k \u2212 \u00b5\u0302 (t+1) k ] Tk(t) ) = Tk(T )\u22121\u2211 n=1 ([ \u00b5\u0302 (Nk(n)) k \u2212 \u00b5\u0302 (Nk(n+1)) k ] n )\n= Tk(T )\u22121\u2211 n=1 ([( \u00b5\u0302 (Nk(n)) k \u2212 \u00b5k ) \u2212 ( \u00b5\u0302 (Nk(n+1)) k \u2212 \u00b5k )] n )\n= Tk(T )\u22121\u2211 n=1 ([ n\u2211 \u03c4=1 (Z\u03c4k \u2212 \u00b5k)\u2212 n n+ 1 n+1\u2211 \u03c4=1 (Z\u03c4k \u2212 \u00b5k) ])\n= Tk(T )\u2211 \u03c4=1 (Z\u03c4k \u2212 \u00b5k) Tk(T )\u22121\u2211 n=\u03c4 1 n+ 1 \u2212 \u03c4 \u2212 1 \u03c4  , and thus\nP \u2225\u2225\u2225\u2225\u2225\u2225 Tk(T )\u22121\u2211 n=1 ([ \u00b5\u0302 (Nk(n)) k \u2212 \u00b5\u0302 (Nk(n+1)) k ] n )\u2225\u2225\u2225\u2225\u2225\u2225 \u2265 \u221a DT (ln2 T ) ln(2/\u03b4)  \u2264 P\nTk(T )\u2211 \u03c4=1 \u2016Z\u03c4k \u2212 \u00b5k\u2016 \u2223\u2223\u2223\u2223\u2223\u2223 Tk(T )\u22121\u2211 n=\u03c4 1 n+ 1 \u2212 \u03c4 \u2212 1 \u03c4 \u2223\u2223\u2223\u2223\u2223\u2223 \u2265 \u221a DT (ln2 T ) ln(2/\u03b4)  \u2264\nT\u2211 t=1 P Tk(T ) = t ; Tk(T )\u2211 \u03c4=1 \u2016Z\u03c4k \u2212 \u00b5k\u2016 \u2223\u2223\u2223\u2223\u2223\u2223 Tk(T )\u22121\u2211 n=\u03c4 1 n+ 1 \u2212 \u03c4 \u2212 1 \u03c4 \u2223\u2223\u2223\u2223\u2223\u2223 \u2265 \u221a DT (ln2 T ) ln(2/\u03b4)  \u2264\nT\u2211 t=1 P\n[ t\u2211\n\u03c4=1\n\u2016Z\u03c4k \u2212 \u00b5k\u2016 \u2223\u2223\u2223\u2223\u2223 [ t\u22121\u2211 n=\u03c4 1 n+ 1 ] \u2212 \u03c4 \u2212 1 \u03c4 \u2223\u2223\u2223\u2223\u2223 \u2265 \u221a DT (ln2 T ) ln(2/\u03b4) ]\n\u2264 T\u2211 t=1 D exp  \u2212T (ln2 T ) ln(2/\u03b4)\u2211t \u03c4=1 ([\u2211t\u22121 n=\u03c4 1 n+1 ] \u2212 \u03c4\u22121\u03c4 )2  (20) \u2264 T\u2211 t=1 D exp ( \u2212T (ln2 T ) ln(2/\u03b4) t ln2 t ) \u2264 DT\u03b4 , (21)\nwhere in (20) we applied the Chernoff-Hoeffding\u2019s inequality with bound \u221a T (ln2 T ) ln(2/\u03b4) to each of theD components.\nNow, define \u03c7(T ) as\n\u03c7(T ) = \u221a 6DT (ln2 T ) ln(2/\u03b4) . (22)\nWe then have:\nP [\u2225\u2225\u2225\u2225\u2225 T\u22121\u2211 t=1 (( \u00b5\u0302 (t) k \u2212 \u00b5\u0302 (t+1) k ) t\u2211 \u03c4=1 \u03b1 (\u03c4) k )\u2225\u2225\u2225\u2225\u2225 > \u03c7(T ) ]\n=P \u2225\u2225\u2225\u2225\u2225\u2225 Tk(T )\u22121\u2211 n=1 (\u00b5\u0302(Nk(n))k \u2212 \u00b5\u0302(Nk(n+1))k )Nk(n)\u2211 \u03c4=1 \u03b1 (\u03c4) k \u2225\u2225\u2225\u2225\u2225\u2225 > \u03c7(T )  (23)\n\u2264P \u2225\u2225\u2225\u2225\u2225\u2225 Tk(T )\u22121\u2211 n=1 (( \u00b5\u0302 (Nk(n)) k \u2212 \u00b5\u0302 (Nk(n+1)) k ) n )\u2225\u2225\u2225\u2225\u2225\u2225+ \u2225\u2225\u2225\u2225\u2225\u2225 Tk(T )\u22121\u2211 n=1 (\u00b5\u0302(Nk(n))k \u2212 \u00b5\u0302(Nk(n+1))k ) n\u2212 Nk(n)\u2211 \u03c4=1 \u03b1 (\u03c4) k \u2225\u2225\u2225\u2225\u2225\u2225 > \u03c7(T ) \n\u2264P \u2225\u2225\u2225\u2225\u2225\u2225 Tk(T )\u22121\u2211 n=1 (( \u00b5\u0302 (Nk(n)) k \u2212 \u00b5\u0302 (Nk(n+1)) k ) n )\u2225\u2225\u2225\u2225\u2225\u2225+ Tk(T )\u22121\u2211 n=1 \u2225\u2225\u2225\u00b5\u0302(Nk(n))k \u2212 \u00b5\u0302(Nk(n+1))k \u2225\u2225\u2225 \u00b7 \u2223\u2223\u2223\u2223\u2223\u2223n\u2212 Nk(n)\u2211 \u03c4=1 \u03b1 (\u03c4) k \u2223\u2223\u2223\u2223\u2223\u2223  > \u03c7(T )  \u2264P\n\u2225\u2225\u2225\u2225\u2225\u2225 Tk(T )\u22121\u2211 n=1 (( \u00b5\u0302 (Nk(n)) k \u2212 \u00b5\u0302 (Nk(n+1)) k ) n )\u2225\u2225\u2225\u2225\u2225\u2225+\u221aD Tk(T )\u22121\u2211 n=1  2 n+ 1 \u2223\u2223\u2223\u2223\u2223\u2223n\u2212 Nk(n)\u2211 \u03c4=1 \u03b1 (\u03c4) k \u2223\u2223\u2223\u2223\u2223\u2223  > \u03c7(T )  (24) \u2264P [\u221a DT (ln2 T ) ln(2/\u03b4) + \u221a D\nTk(T )\u22121\u2211 n=1  2 n+ 1 \u2223\u2223\u2223\u2223\u2223\u2223n\u2212 Nk(n)\u2211 \u03c4=1 \u03b1 (\u03c4) k \u2223\u2223\u2223\u2223\u2223\u2223  > \u03c7(T ) , E1 \u2229 E2]+ P[Ec1 ] + P[Ec2 ] (25)\n\u2264P \u221aDT (ln2 T ) ln(2/\u03b4) +\u221aD Tk(T )\u22121\u2211 n=1 ( 2 n+ 1 \u221a 2Nk(n) ln(2/\u03b4) ) > \u03c7(T ) + (DT + 1)\u03b4 (26) \u2264P [\u221a DT (ln2 T ) ln(2/\u03b4) + \u221a 2DT (ln2 T ) ln(2/\u03b4) > \u03c7(T ) ] + (DT + 1)\u03b4\n=(DT + 1)\u03b4 (27)\nwhere (23) follows from the fact that \u00b5\u0302(t)k = \u00b5\u0302 (t+1) k unless t + 1 = Nk(n) for some n, (24) follows from the fact that (1/ \u221a D) \u2223\u2223\u2223\u00b5\u0302(Nk(n))k \u2212 \u00b5\u0302(Nk(n+1))k \u2223\u2223\u2223 \u2264 1n+1\u2016Zn+1k \u2016 + \u2211ni=1( 1n \u2212 1n+1 )\u2016Zik\u2016 = 2n+1 , (25) follows from (21) and\nE1 denotes the event that \u2223\u2223\u2223Tk(t)\u2212\u2211tt=1 \u03b1(t)k \u2223\u2223\u2223 \u2264 \u221a2t ln(2/\u03b4) for all t = 1, . . . , T and Ec2 denotes the event that\u2223\u2223\u2223\u2211Tk(T )\u22121n=1 ((\u00b5\u0302(Nk(n))k \u2212 \u00b5\u0302(Nk(n+1))k )n)\u2223\u2223\u2223 \u2264 \u221aDT (ln2 T ) ln(2/\u03b4) (Ec1 and Ec2 denote the complementary events), (26) follows from Claim 1 and the fact that Tk(Nk(n)) = n.\nThe claimed bound now follows from (15), (19) and (27) because the Gini index isL-Lipschitz.\nCorollary 1. With probability at least 1\u2212 2(DT + 1)K\u03b4,\nf ( \u03b1\u0304(t) ) \u2264 1 T T\u2211 t=1 f (t) ( \u03b1(t) ) + L\n\u221a 6D(1 + ln2 T ) ln 2\u03b4\nT ."}, {"heading": "C. Proof of Proposition 3", "text": "In order to ease technicalities, we define events\nE\u00b5 = { (\u22001 \u2264 t \u2264 T ) \u221a 2Tk(t) \u2225\u2225\u2225\u00b5\u0302(t)k \u2212 \u00b5k\u2225\u2225\u2225 <\u221aD ln(2\u03b4)} ,\nE\u03b1 = { (\u22001 \u2264 t \u2264 T ) \u2223\u2223\u2223\u2223\u2223Tk(t)\u2212 t\u2211\n\u03c4=1\n\u03b1 (\u03c4) k \u2223\u2223\u2223\u2223\u2223 <\u221a2t ln(2/\u03b4) }\nand\nEf = { f ( \u03b1\u0304(t) ) \u2264 1 T T\u2211 t=1 f (t) ( \u03b1(t) ) + L \u221a 6D(1+ln2 T ) ln(2/\u03b4) T } .\nThe following technical lemma will also be useful later.\nLemma 2. Let\u03b1\u2217 \u2208 argmin\u03b1\u2208\u2206K f(\u03b1), and \u03c7 = \u03c7\u03b1\u2217 : N 7\u2192 R+ be some function and define event\nE(\u03c7,\u03b1\u2217) = {(\u22001 \u2264 t \u2264 T ) (\u2200k \u2208 [K] with \u03b1\u2217k > 0)Tk(t) > \u03c7(t)} ,\nThen, conditioned on the event E\u00b5 \u2229 Ef \u2229 E(\u03c7,\u03b1\u2217),\nf\n( 1\nT T\u2211 t=1 \u03b1(t)\n) \u2212 f(\u03b1\u2217) \u2264 \u03b6 \u03c7(T )\nT ,\nwhere\n\u03b6\u03c7(\u03c4) = L \u221a 6D\u03c4(1 + ln2 \u03c4) ln(2/\u03b4) + 1\n\u03b7\u03c4 + G2 + 1 2 \u03c4\u2211 t=1 \u03b7t + LK \u03c4\u2211 t=1\n\u221a D ln(2/\u03b4)\n2\u03c7(t) .\nProof : Throughout the proof condition on the event E\u00b5 \u2229 Ef \u2229 E(\u03c7,\u03b1\u2217).\nFirst of all, as the generalized Gini index isL-Lipschitz, it holds for any \u03b1 \u2208 \u2206K that\nT\u2211 t=1 ( f (t)(\u03b1)\u2212 f(\u03b1) ) \u2264 L T\u2211 t=1 \u2225\u2225\u2225(\u00b5\u0302(t) \u2212 \u00b5)\u03b1\u2225\u2225\u2225 \u2264 LK max k\u2208K:\u03b1k>0 T\u2211 t=1 \u2225\u2225\u2225\u00b5\u0302(t)k \u2212 \u00b5k\u2225\u2225\u2225 . Thus, due to the conditioning on the event E\u00b5 \u2229 E(\u03c7,\u03b1\u2217),\nT\u2211 t=1 ( f (t)(\u03b1\u2217)\u2212 f(\u03b1\u2217) ) \u2264 LK T\u2211 t=1\n\u221a D ln(2/\u03b4)\n2\u03c7(t) . (28)\nIt then follows, however, that\nTf\n( 1\nT T\u2211 t=1 \u03b1(t)\n) \u2212 Tf(\u03b1\u2217)=Tf ( 1\nT T\u2211 t=1 \u03b1(t)\n) \u2212 [ T\u2211 t=1 f (t) ( \u03b1(t) )] + [ T\u2211 t=1 f (t) ( \u03b1(t) )] \u2212 Tf(\u03b1\u2217)\n< L \u221a 6DT (1 + ln2 T ) ln(2/\u03b4) + 1\n\u03b7T + G2 + 1 2 T\u2211 t=1 \u03b7t + T\u2211 t=1 [ f (t)(\u03b1\u2217)\u2212 f(\u03b1\u2217) ] (29)\n< \u03b6\u03c7(T ) , (30)\nwhere (29) is due to (14) and the conditioning on Ef , and (30) is due to (28) and the definition of \u03b6\u03c7.\nNow we are ready to prove the proposition. For convenience, we recall the statement.\nProposition 3. With probability at least 1\u2212 4DT 2K\u03b4:\nf\n( 1\nT T\u2211 t=1 \u03b1(t)\n) \u2212 f(\u03b1\u2217) \u2264 L \u221a 6D(1 + ln2 T ) ln(2/\u03b4)\nT\n+ 1 T\u03b7T + KD2 + 1 T T\u2211 t=1 \u03b7t + LK T T\u2211 t=1\n\u221a D ln(2/\u03b4)\n2\u03c71(t)\nwhere \u03c71(t) = 1(t \u2264 \u03c41) + 1(t > \u03c41)(ta0/(2|ext(\u2206\u2217K)|)) and \u03c41 = [ (2|ext(\u2206\u2217K)|) [ 2 + 10 \u221a 3LKD2 g\u2217 ]\u221a 2 ln 2\u03b4 ]4 with a0 = min\u03b1\u2208ext(\u2206\u2217K) mink:\u03b1k>0 \u03b1k and g \u2217 = inf\u03b1\u2208\u2206K\\\u2206\u2217K max\u03b1\u2217\u2208\u2206 \u2217 K f(\u03b1)\u2212f(\u03b1\u2217) \u2016\u03b1\u2212\u03b1\u2217\u2016 .\nProof : Let \u2206\u2217K = argmin\u03b1\u2208\u2206K f(\u03b1) denote the set of optimal solutions of f over \u2206K . By construction,\u03b1 (t) k \u2265 \u03b7t for every t \u2265 1 and every k \u2208 [K], therefore, setting \u03c70(\u03c4) = max { 1, ( \u2211\u03c4 t=1 \u03b7t)\u2212 \u221a 2\u03c4 ln(2/\u03b4) } , it holds for every\u03b1\u2217 \u2208 \u2206\u2217K that\nE\u03b1 \u2286 E(\u03c70,\u03b1\u2217) , (31)\nwhere event E(\u03c70,\u03b1\u2217) is defined as in Lemma 2. Noting that\n\u03c4\u2211 t=1 \u03b7t = \u221a 2 ln(2/\u03b4) 1\u22121/ \u221a K \u03c4\u2211 t=1 1\u221a t \u2265 \u221a 2 ln(2/\u03b4) 1\u22121/ \u221a K \u222b \u03c4+1 1 1\u221a t dt = \u221a 2 ln(2/\u03b4) 1\u22121/ \u221a K [ 2 \u221a \u03c4 + 1\u2212 2 ] , (32)\nit follows that for every \u03c4 \u2265 K \u2212 1\n\u03c70(\u03c4) \u2265 \u221a 2 ln 2\u03b4 [ 2 \u221a \u03c4+1\u22122 1\u22121/ \u221a K \u2212 \u221a \u03c4 ] \u2265 \u221a 2 ln 2\u03b4 [ (1+1/ \u221a K) \u221a \u03c4+1\u22122 1\u22121/ \u221a K ] \u2265 \u221a 2\u03c4 ln 2\u03b4 , (33)\nwhich further implies\n\u03b6\u03c70(\u03c4)\n\u2264 L \u221a\n6D\u03c4(1 + ln2 \u03c4) ln 2\u03b4 +\n( 1\u2212 1\u221a\nK\n)\u221a \u03c4\n\u221a 2 ln(2/\u03b4)\n+ G2 + 1\n2\n\u221a 2 ln(2/\u03b4)\n1\u22121/ \u221a K\n[ 2 \u221a \u03c4 \u2212 1 ] + (K \u2212 1) + LK \u221a D 4\n\u221a ln(2/\u03b4)\n8\n\u03c4\u2211 t=K 1 4 \u221a t\n(34) \u2264 L \u221a 600D\u03c43/2 ln 2\u03b4 + \u221a \u03c4 [ 1\n2 ln 2\u03b4 + (KD2 + 1)\n\u221a 2 ln 2\u03b4 ] +K + LK \u221a D 4 \u221a ln(2/\u03b4) 8 ( 4 3\u03c4 3/4 +K\u22121/4 \u2212 43K 3/4 ) (35)\n\u2264 10LKD2 \u221a 6 ln(2/\u03b4)\u03c43/4 , (36)\nwhere (34) follows from (33) and\n\u03c4\u2211 t=1 \u03b7t = \u221a 2 ln(2/\u03b4) 1\u22121/ \u221a K \u03c4\u2211 t=1 1\u221a t \u2264 \u221a 2 ln(2/\u03b4) 1\u22121/ \u221a K [ 1 + \u222b \u03c4 1 1\u221a t dt ] = \u221a 2 ln(2/\u03b4) 1\u22121/ \u221a K [ 2 \u221a \u03c4 \u2212 1 ] , (37)\nand (35) follows from Lemma 1\n\u03c4\u2211 t=K 1 4 \u221a t \u2264 K\u22121/4 + \u222b \u03c4 K 1 4 \u221a t dt \u2264 K\u22121/4 + 43 [\u03c4 3/4 \u2212K3/4] (38)\nChoose\u03b1\u2217\u2208 argmin\u03b1\u2208\u2206\u2217K \u2016\u03b1\u2212 \u2211T t=1\u03b1 (t)\u2016. By Lemma 3, it holds that\ng\u2217max k\u2208K \u2223\u2223\u2223\u2223\u2223 [ 1 T T\u2211 t=1 \u03b1 (t) k ] \u2212\u03b1\u2217k \u2223\u2223\u2223\u2223\u2223 \u2264 f ( 1 T T\u2211 t=1 \u03b1(t) ) \u2212 f(\u03b1\u2217) . (39)\nTherefore, on event E\u00b5 \u2229 Ef \u2229 E\u03b1,\nmax k\u2208K \u2223\u2223\u2223\u2223\u2223 [ 1 T T\u2211 t=1 \u03b1 (t) k ] \u2212\u03b1\u2217k \u2223\u2223\u2223\u2223\u2223 \u2264 \u03b6\u03c70(T )g\u2217T , (40) where (40) follows from (39) by Lemma 2 due to the conditioning on event E\u00b5 \u2229 Ef \u2229 E\u03b1 and recalling (31).\nNow, as\u03b1\u2217 \u2208 \u2206\u2217K , it can be represented as a convex combination of the extreme points of \u2206\u2217K . Consider such a representation, and choose\u03b1+ \u2208 ext(\u2206\u2217K) to be the one with maximal coefficient; note that this coefficient must be at least 1/|ext(\u2206\u2217K)|; that is,\n\u03b1\u2217k \u2265 1ext(|\u2206\u2217K |)\u03b1 + k . (41)\nThen, conditioned on event E\u03b1 \u2229 E\u00b5 \u2229 Ef , for every k \u2208 [K],\nTk(\u03c4) \u2265 \u03c4\u2211 t=1 \u03b1 (t) k \u2212 \u221a 2\u03c4 ln(2/\u03b4) (42)\n\u2265 \u03c4\u03b1\u2217k \u2212 \u03b6\u03c70 (\u03c4) g\u2217 \u2212\n\u221a 2\u03c4 ln(2/\u03b4) (43)\n\u2265 \u03c4 \u03b1 + k\n|ext(\u2206\u2217K)| \u2212 \u03b6 \u03c70 (\u03c4) g\u2217 \u2212\n\u221a 2\u03c4 ln(2/\u03b4) (44)\nwhere (42) follows from conditioning on event E\u03b1, (43) follows because of (40) due to the conditioning on E\u03b1 \u2229 E\u00b5 \u2229 Ef , and (44) follows by (41).\nLet, now, a0 = min\u03b1\u2208ext(\u2206\u2217K) mink:\u03b1k>0 \u03b1k, where ext(\u2206 \u2217 K) denotes the extreme points of \u2206 \u2217 K . Note that \u2206 \u2217 K is a convex polytope due to Lemma 3, and thus has finite number of extreme points, justifying the min and implying that a0 > 0. As \u03b1+ \u2208 ext(\u2206\u2217K), it follows that \u03b1 + k \u2265 a0 for every k with \u03b1 + k > 0. Then, by (44), conditioned on event E\u03b1 \u2229 E\u00b5 \u2229 Ef , for every k \u2208 [K] with \u03b1+k > 0 and \u03c4 \u2265 1,\nTk(\u03c4) \u2265 \u03c4 a0|ext(\u2206\u2217K)| \u2212 \u03b6\u03c70 (\u03c4) g\u2217 \u2212\n\u221a 2\u03c4 ln(2/\u03b4) , (45)\nimplying Tk(\u03c4) \u2265 a0\u03c4/(2|ext(\u2206\u2217K)|) with\n\u03c41 = [ (2|ext(\u2206\u2217K)|) [ 2 + 10 \u221a 3LKD2 g\u2217 ]\u221a 2 ln 2\u03b4 ]4 \u2265 1 + max { \u03c4 \u2208 N : \u03c4a02|ext(\u2206\u2217K)| < \u03b6 \u03c70(\u03c4)/g\u2217 + \u221a 2\u03c4 ln @2 \u03b4 } ,\nwhere the inequality is due to (36). Consequently, setting\n\u03c71(\u03c4) = 1(\u03c4 \u2264 \u03c41) + 1(\u03c4 > \u03c41)(\u03c4a0/(2|ext(\u2206\u2217K)|)),\nit follows that E\u03b1\u2229E\u00b5\u2229Ef \u2286 E(\u03c71,\u03b1+). This completes the proof due to Lemma 2 and the upper bound onG from Lemma 1, noting that P[Ec\u03b1] \u2264 \u03b4 due to Claim 1 (here, for event E , we denote its complementer event by Ec), P[Ecf ] \u2264 2(DT + 1)K\u03b4 due to Corollary 1, and\nP[Ec\u00b5] \u2264 T\u2211 t=1 T\u2211 \u03c4=1 [\u221a 2Tk(t) \u2225\u2225\u2225\u00b5\u0302(t)k \u2212 \u00b5k\u2225\u2225\u2225 \u2265\u221aD ln(2\u03b4) & Tk(t) = \u03c4] (46) \u2264\nT\u2211 t=1 T\u2211 \u03c4=1 [\u2225\u2225\u2225\u2225\u2225 \u03c4\u2211 n=1 Znk \u2212 \u00b5k \u2225\u2225\u2225\u2225\u2225 \u2265 \u221a D ln(2\u03b4) 2\u03c4 ] (47) \u2264 2T 2D\u03b4 (48)\nwhere (46) follows due to the union bound,Znk = X (Nk(n)) k withNk(n) = argmin{\u03c4 \u2265 1 : Tk(\u03c4) \u2265 n} in (47), and finally (48) follows due to the Chernoff-Hoeffding bound."}, {"heading": "D. The proof of g\u2217 > 0", "text": "Lemma 3. The set \u2206\u2217K = argmin\u03b1\u2208\u2206K f(\u03b1) of optimal solutions is a convex polytope. Additionally, it also holds that g\u2217 = inf\u03b1\u2208\u2206K\\\u2206\u2217K max\u03b1\u2217\u2208\u2206 \u2217 K f(\u03b1)\u2212f(\u03b1\u2217) \u2016\u03b1\u2212\u03b1\u2217\u2016 is positive.\nProof :\nDefining, for a permutation \u03c0 over [D] = {1, 2, . . . , D}, the set A\u03c0 = {\u03b1 \u2208 \u2206K : \u2200i, j \u2208 [D], (wi \u2212 wj)((\u00b5\u03b1)\u03c0(i) \u2212 (\u00b5\u03b1)\u03c0(j)) \u2265 0}, it follows that f(\u03b1) = Gw(\u00b5\u03b1) is linear over eachA\u03c0; that is, f(\u03b1) = f\u1d40\u03c0\u03b1,\u2200\u03b1 \u2208 A\u03c0 for some f\u03c0 \u2208 RK\nwith component k defined as \u2211D d=1(wd\u00b5k,\u03c0(d)). (Observe that sets associated with different permutations can coincide when w has identical components, and that non-coincidingA\u03c0 andA\u03c0\u2032 overlap each other on some lower dimensional faces.) It is clear that \u2206K = \u222a\u03c0A\u03c0 .Additionally, it is also easy to see thatA\u03c0 is a polytope, noting thatA\u03c0 = \u2206K \u2229 \u22c2 1\u2264i<j\u2264D{\u03b1 \u2208 RK : \u03b1\u1d40bi,j,\u03c0 \u2265 0}, where vector bi,j,\u03c0 \u2208 RK has component k defined as [(wi \u2212 wj)(\u00b5k,\u03c0(i) \u2212 \u00b5k,\u03c0(j))] for 1 \u2264 k \u2264 K. Finally, from all the above it also follows that f is a piecewise-linear convex function with linear regions {A\u03c0}\u03c0, thus the minimal set \u2206\u2217K = argmin\u03b1\u2208\u2206K f(\u03b1) is a face of one of theA\u03c0 polytopes. 4 The first claim of the lemma follows.\n4See (Boyd & Vandenberghe, 2004) for more about piecewise-linear convex functions.\nLet ext(A) denote the extreme points of a convex polytopeA, and define\ng+ = min \u03c0 min \u03b1\u2208ext(A\u03c0)\\\u2206\u2217K min \u03b1\u2217\u2208\u2206\u2217K f(\u03b1)\u2212 f(\u03b1\u2217) \u2016\u03b1\u2212\u03b1\u2217\u2016 .\nNote that the definition makes sense and that 0 < g+ <\u221e ,\nbecause \u2206\u2217K is bounded and convex and ext(A\u03c0) \\\u2206\u2217K is a finite set with no elements from \u2206\u2217K .\nNow, choose some permutation \u03c0 and some \u03b1 \u2208 A\u03c0 \\ \u2206\u2217K . Then \u03b1 can be written as a convex combination \u03b1 = \u2211 \u03b1\u2032\u2208ext(A\u03c0)\u03b1 \u2032\u03c9(\u03b1\u2032) for some weight vector \u03c9 : ext(A\u03c0) \u2192 [0, 1] with \u2211 \u03b1\u2032\u2208ext(A\u03c0) \u03c9(\u03b1 \u2032) = 1. (This form is\npossibly non-unique.) LetN\u2217 = {\u03b1 \u2208 ext(A\u03c0) \u2229\u2206\u2217K : \u03c9(\u03b1) > 0}, and let\u03b1\u2217 = 1\u2211 \u03b1\u2032\u2208N\u2217 \u03c9(\u03b1 \u2032) \u2211 \u03b1\u2032\u2208N\u2217 \u03b1\n\u2032\u03c9(\u03b1\u2032) when N\u2217 6= \u2205, otherwise chose\u03b1\u2217 \u2208 \u2206\u2217K in an arbitrary fashion. In either case,\u2211\n\u03b1\u2032\u2208N\u2217 \u03c9(\u03b1\u2032)(\u03b1\u2217 \u2212\u03b1\u2032) = 0 . (49)\nThen, since f is linear overA\u03c0 ,\nf(\u03b1)\u2212 f(\u03b1\u2217) \u2016\u03b1\u2217 \u2212\u03b1\u2016 =\n\u2211 \u03b1\u2032 6\u2208N\u2217 \u03c9(\u03b1\n\u2032)(f(\u03b1\u2032)\u2212 f(\u03b1\u2217))\u2225\u2225\u2225(\u2211\u03b1\u2032 6\u2208N\u2217 \u03c9(\u03b1\u2032)(\u03b1\u2217 \u2212\u03b1\u2032))\u2212 (\u2211\u03b1\u2032\u2208N\u2217 \u03c9(\u03b1\u2032)(\u03b1\u2217 \u2212\u03b1\u2032))\u2225\u2225\u2225 \u2265 \u2211 \u03b1\u2032 6\u2208N\u2217 \u03c9(\u03b1 \u2032)(f(\u03b1\u2032)\u2212 f(\u03b1\u2217))\u2211\n\u03b1\u2032 6\u2208N\u2217 \u03c9(\u03b1 \u2032) \u2016\u03b1\u2217 \u2212\u03b1\u2032\u2016\n(50)\n\u2265 \u2211 \u03b1\u2032 6\u2208N\u2217 \u03c9(\u03b1 \u2032)(f(\u03b1\u2032)\u2212 f(\u03b1\u2217))\u2211\n\u03b1\u2032 6\u2208N\u2217 \u03c9(\u03b1 \u2032)(f(\u03b1\u2032)\u2212 f(\u03b1\u2217))/g+\n= g+ .\nwhere (50) follows because of (49) and the triangle inequality. As \u03b1\u2217 \u2208 \u2206\u2217K we obtain that g\u2217 \u2265 g+, completing the proof of the second claim of the lemma."}, {"heading": "E. Proof of Theorem 1", "text": "Theorem 1. With probability at least 1\u2212 \u03b4:\nf (\n1 T T\u2211 t=1 \u03b1(t) ) \u2212 f(\u03b1\u2217) \u2264 2L \u221a 6D ln3(8DKT 2/\u03b4) T ,\nfor any big enough T , whereL is the Lipschitz constant ofGw(x).\nProof : First we upper-bound the following sum that appears in the last term of Proposition 3:\nT\u2211 t=1 1\u221a \u03c71(t) = \u03c41 +\n\u221a 2|ext(\u2206\u2217K)|\na0\nT\u2211 \u03c4=\u03c41+1 1\u221a \u03c4\n\u2264 \u03c41 +\n\u221a 2|ext(\u2206\u2217K)|\na0\n[ \u221a \u03c41 + \u222b T \u03c41 1\u221a \u03c4 d\u03c4 ]\n\u2264\n\u221a 2|ext(\u2206\u2217K)|\na0\n[ \u03c41 + 2 \u221a T \u2212 \u221a \u03c41 ] \u2264 \u221a 2|ext(\u2206\u2217K)|\na0\n[ (2|ext(\u2206\u2217K)|) [ 2 + 10 \u221a 3LKD2 g\u2217 ]\u221a 2 ln 2\u03b4 ]4 + \u221a 8|ext(\u2206\u2217K)|\na0\n\u221a T\n\u2264 (2|ext(\u2206 \u2217 K)|)9/2\u221a a0\n[ 2 + 10 \u221a 3LKD2\ng\u2217\n]4 ( 2 ln 2\u03b4 )2 +\n\u221a 8|ext(\u2206\u2217K)|\na0\n\u221a T\nAccording to Proposition 3, using \u03b4/4DKT 2 in place of \u03b4 and recalling (37),\nf\n( 1\nT T\u2211 t=1 \u03b1(t)\n) \u2212 f(\u03b1\u2217) (51)\n\u2264L\n\u221a 6D(1 + ln2 T ) ln 8DKT 2\n\u03b4\nT +\n1\u2212 1/ \u221a K\u221a\n2T ln(2/\u03b4) + [KD2 + 1]\n\u221a 2 ln(2/\u03b4) 1\u2212 1/ \u221a K [ 2\u221a T \u2212 1T ]\n+ LK\n\u221a D ln 8DKT 2\n\u03b4\u221a 2T\n(2|ext(\u2206\u2217K)|)9/2\u221a a0\n[ 2 + 10 \u221a 3LKD2\ng\u2217\n]4 ( 2 ln 2\u03b4 )2 + 2LK\n\u221a D|ext(\u2206\u2217K)| ln 8DKT 2 \u03b4\na0T\n\u22642L\n\u221a 6D ln 8DKT 2\n\u03b4\nT max\n{ ln(2T ), 1 + 4(KD)3/2 + 2K |ext(\u2206\u2217K)| 9/2\n\u221a a0\n[ 2 + 10 \u221a 3LKD g\u2217 ]4 (2 ln(2/\u03b4)) } which completes the proof."}, {"heading": "F. Regret vs. pseudo regret", "text": "Corollary 2. With probability at least 1\u2212 \u03b4\n|R(T ) \u2212 R\u0304(T )| \u2264 L \u221a 12D ln(4(DT + 1)/\u03b4)\nT\nProof : The average cost can be written as\nX\u0304(T ) = 1\nT K\u2211 k=1 Tk(T ) 1 Tk(T ) T\u2211 t=1 1(kt = k)X (t) k = 1 T K\u2211 k=1 Tk(T )\u00b5\u0302 (T ) k\nAccording to Claim 1, with probability at least 1\u2212 \u03b4/2, it holds\n\u2016X\u0304(T ) \u2212 \u00b5\u0302(T )\u03b1\u0304(T )\u2016 = \u2225\u2225\u2225\u2225\u2225 1T K\u2211 k=1 Tk(T )\u00b5\u0302 (T ) k \u2212 1 T K\u2211 k=1 \u00b5\u0302 (T ) k T\u2211 t=1 \u03b1 (t) k \u2225\u2225\u2225\u2225\u2225 =\n\u2225\u2225\u2225\u2225\u2225 1T K\u2211 k=1 \u00b5\u0302 (T ) k ( Tk(T )\u2212 T\u2211 t=1 \u03b1 (t) k )\u2225\u2225\u2225\u2225\u2225 \u2264 K \u221a 2D ln(4K/\u03b4)\nT\nand since GGI isL-Lipschitz, with probability at least 1\u2212 \u03b4/2,\n|Gw(X\u0304(T ))\u2212Gw(\u00b5\u0302(T )\u03b1\u0304(T ))| \u2264 LK \u221a 2D ln(4K/\u03b4)\nT (52)\nIn addition, one can show (see (19)) that\nP [\u2225\u2225\u2225\u2225\u2225\u00b5\u0302(T )k T\u2211 \u03c4=1 \u03b1 (\u03c4) k \u2212 \u00b5k T\u2211 \u03c4=1 \u03b1 (\u03c4) k \u2225\u2225\u2225\u2225\u2225 >\u221a5DT ln(2/\u03b4) ] = (DT + 1)\u03b4 ,\nwhich implies that with probability 1\u2212 \u03b4/2 we have\n\u2225\u2225\u2225\u00b5\u0302(T )k \u03b1\u0304(T ) \u2212 \u00b5k\u03b1\u0304(T )\u2225\u2225\u2225 \u2264 \u221a 5D ln(4(DT + 1)/\u03b4)\nT (53)\nHence, the difference of regret and pseudo-regret can be upper-bounded with probability at least 1\u2212 \u03b4 as\n|R(T ) \u2212R(T )| = |Gw ( X\u0304(T ) ) \u2212 f(\u03b1\u0304(T ))|\n\u2264 |f (T )(\u03b1\u0304(T ))\u2212 f(\u03b1\u0304(T ))|+ LK \u221a D 2 ln(4K/\u03b4)\nT (54)\n\u2264 L\n(\u221a 5D ln(4(DT + 1)/\u03b4)\nT +K\n\u221a 2D ln(4K/\u03b4)\nT\n) (55)\n\u2264 L \u221a 12D ln(4(DT + 1)/\u03b4)\nT (56)\nwhere (54) follows from (52), (55) follows from (52) and (56) holds becauseDT + 1 \u2265 K."}, {"heading": "G. Battery control task", "text": "Efficient management of electric batteries leads to better performance and longer battery life, which is important for instance for electric vehicles whose range is still limited compared to those with petrol or diesel engines. An electric battery is composed of several cells whose capacity varies extensively due to inconsistencies in weight and volume of the active material in their individual components, different internal resistance and higher temperatures leading to different aging rates. As a consequence, the energy output at any instant is different from each cell, which results in different aging rates and ultimately leads to a premature battery failure. To address this problem, a control strategy called cell balancing is utilized, which aims at maintaining a constant energy level \u2014 mainly state-of-charge (SOC) \u2014 in each cell, while controlling for temperature and aging. Many cell-balancing controllers can be defined, depending on the importance given to the three objectives: SOC, temperature and aging. The values of those objectives should be balanced between the cells because a balanced use of all cells leads to a longer lasting system. Moreover, those objectives values should also be balanced within a cell, because for example, a cell can have higher capacity on a higher temperature, but at the same time it has a higher risk to explode.\nOur battery control task consists in learning and selecting the \u201cbest\u201d cell balancing controller in an online fashion, so that it can dynamically adapt to the consumption profile and environment, such as outdoor temperature. In the case of electric cars, this means that the controller needs to be able to adapt to the driving habits of the driver and to the terrain, such as hilly roads or desert roads. In this experiment, our goal was more specifically to test our multi-objective online learning algorithms in the battery control task, and verify that our online learning algorithms can indeed find a policy for this control task that leads to a balanced parameter values of the cells.\nThe battery is modeled using the internal resistance (Rint) model (Johnson, 2002). The estimation of SOC is based on the Ampere Hour Counting method (Dambrowski, 2013). The variation of temperature in the system is determined according to the dissipative heat loss due to the internal resistance and thermal convection (Gao et al., 2002). Cell degradation or aging is a function of temperature, charging/discharging rates and cumulative charge (Tao, 2012). Moreover, the battery model is complemented with 10 different cell-balancing controllers. The whole model is implemented in the Matlab/Simulink software package and can emulate any virtual situation whose electric consumption is described by a time series, which is given as input to the model. In practice, such a time series would correspond to a short period of battery usage (e.g., a brief segment of a driving session). For a chosen controller, the output of the model comprises of the objective values of each battery cell at the end of the simulation. Note that the output of the battery model is a multivariate random vector since the power demand is randomized, therefore this control task can be readily accommodated into our setup. In our experiments, the battery consists of 4 cells, thusD = 12 in this case. The cell-balancing controllers correspond to the arms, thusK = 10.\nThe online learning task consists of selecting among these controllers/arms so that the final objective values are as balanced as possible. The experiment was carried out as follows: in each iteration, the learner selects a controller according to its policy, then the battery model is run with the selected controller by using a random consumption time series. At the end of the simulation, the learner receives the objective values of each cell output by the battery model as feedback and updates its policy. The goal of the learner is to find a policy over the controllers, which leads to a balanced state of cells in terms of cumulative value.\nThe results are shown in Figure 3. In this experiment we do not know the means of the arms, but we estimated them based on 100000 runs of the simulator for each arm. These mean estimates were used for computing the optimal policy and the regret. We run the MO-OGDE and MO-LP over 100 repetitions. Their average regret exhibits the same trend as in the synthetic\nexperiments: the MO-LP achieved faster convergence. The blue lines shows the regret of the pure policies, which always selects the same arm, i.e., the performance of single strategies. It is important to mention that the optimal mixed controller has a lower GGI value since the regret of any arm is positive, and more importantly, the two learners converge to optimal mixed policies in terms of regret."}], "references": [{"title": "Blackwell approachability and no-regret learning are equivalent", "author": ["Abernethy", "Jacob D", "Bartlett", "Peter L", "Hazan", "Elad"], "venue": "In COLT 2011 - The 24th Annual Conference on Learning Theory, June 9-11,", "citeRegEx": "Abernethy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2011}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Sequential decision making with vector outcomes", "author": ["Azar", "Yossi", "Feige", "Uriel", "Feldman", "Michal", "Tennenholtz", "Moshe"], "venue": "In ITCS, pp", "citeRegEx": "Azar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2014}, {"title": "Convex Optimization", "author": ["Boyd", "Stephen", "Vandenberghe", "Lieven"], "venue": null, "citeRegEx": "Boyd et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2004}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi", "year": 2012}, {"title": "When is a weighted average of ordered sample elements a maximum likelihood estimator of the location parameter", "author": ["Buczolich", "Zolt\u00e1n", "Sz\u00e9kely", "G\u00e1bor J"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Buczolich et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Buczolich et al\\.", "year": 1989}, {"title": "The measurement of inequality of incomes", "author": ["H. Dalton"], "venue": "Economic J.,", "citeRegEx": "Dalton,? \\Q1920\\E", "shortCiteRegEx": "Dalton", "year": 1920}, {"title": "Review on methods of state-of-charge estimation with viewpoint to the modern LiFePO4/Li4Ti5O12 lithium-ion systems", "author": ["J. Dambrowski"], "venue": "In International Telecommunication Energy Conference,", "citeRegEx": "Dambrowski,? \\Q2013\\E", "shortCiteRegEx": "Dambrowski", "year": 2013}, {"title": "Designing multi-objective multi-armed bandits algorithms: A study", "author": ["M.M. Drugan", "A. Now\u00e9"], "venue": "In IJCNN, pp", "citeRegEx": "Drugan and Now\u00e9,? \\Q2013\\E", "shortCiteRegEx": "Drugan and Now\u00e9", "year": 2013}, {"title": "Multi-criteria reinforcement learning", "author": ["G\u00e1bor", "Zolt\u00e1n", "Kalm\u00e1r", "Zsolt", "Szepesv\u00e1ri", "Csaba"], "venue": "In ICML, pp", "citeRegEx": "G\u00e1bor et al\\.,? \\Q1998\\E", "shortCiteRegEx": "G\u00e1bor et al\\.", "year": 1998}, {"title": "Dynamic lithium-ion battery model for system simulation", "author": ["Gao", "Lijun", "Chen", "Shenyi", "Dougal", "Roger A"], "venue": "IEEE Trans. on Components and Packaging Technologies,", "citeRegEx": "Gao et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2002}, {"title": "Introduction to Online", "author": ["Hazan", "Elad"], "venue": "Convex Optimization. NOW,", "citeRegEx": "Hazan and Elad.,? \\Q2016\\E", "shortCiteRegEx": "Hazan and Elad.", "year": 2016}, {"title": "Battery performance models in ADVISOR", "author": ["V.H. Johnson"], "venue": "Journal of Power Sources,", "citeRegEx": "Johnson,? \\Q2002\\E", "shortCiteRegEx": "Johnson", "year": 2002}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "Robbins", "Herbert"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Lai et al\\.", "year": 1985}, {"title": "Stochastic convex optimization with multiple objectives", "author": ["Mahdavi", "Mehrdad", "Yang", "Tianbao", "Jin", "Rong"], "venue": "In NIPS, pp", "citeRegEx": "Mahdavi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mahdavi et al\\.", "year": 2013}, {"title": "Online learning with sample path constraints", "author": ["Mannor", "Shie", "Tsitsiklis", "John N", "Yu", "Jia Yuan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mannor et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2009}, {"title": "Approachability in unknown games: Online learning meets multi-objective optimization", "author": ["Mannor", "Shie", "Perchet", "Vianney", "Stoltz", "Gilles"], "venue": "In Proceedings of The 27th Conference on Learning Theory, COLT", "citeRegEx": "Mannor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2014}, {"title": "On solving linear programs with the ordered weighted averaging objective", "author": ["W. Ogryczak", "T. Sliwinski"], "venue": "Eur. J. Operational Research,", "citeRegEx": "Ogryczak and Sliwinski,? \\Q2003\\E", "shortCiteRegEx": "Ogryczak and Sliwinski", "year": 2003}, {"title": "On minimizing ordered weighted regrets in multiobjective Markov decision processes", "author": ["W. Ogryczak", "P. Perny", "P. Weng"], "venue": "In ADT, Lecture Notes in Artificial Intelligence,", "citeRegEx": "Ogryczak et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ogryczak et al\\.", "year": 2011}, {"title": "A compromise programming approach to multiobjective Markov decision processes", "author": ["W. Ogryczak", "P. Perny", "P. Weng"], "venue": "International Journal of Information Technology & Decision Making,", "citeRegEx": "Ogryczak et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ogryczak et al\\.", "year": 2013}, {"title": "Bandit solutions provide unified ethical models for randomized clinical trials and comparative effectiveness research", "author": ["W.H. Press"], "venue": "PNAS, 106(52):22398\u201322392,", "citeRegEx": "Press,? \\Q2009\\E", "shortCiteRegEx": "Press", "year": 2009}, {"title": "A survey of multi-objective sequential decision-making", "author": ["Roijers", "Diederik M", "Vamplew", "Peter", "Whiteson", "Shimon", "Dazeley", "Richard"], "venue": "J. Artif. Intell. Res.,", "citeRegEx": "Roijers et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Roijers et al\\.", "year": 2013}, {"title": "Online learning and online convex optimization", "author": ["Shalev-Shwartz", "Shai"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz and Shai.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz and Shai.", "year": 2012}, {"title": "Contextual bandits with similarity information", "author": ["A. Slivkins"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Slivkins,? \\Q2014\\E", "shortCiteRegEx": "Slivkins", "year": 2014}, {"title": "An interactive weighted Tchebycheff procedure for multiple objective programming", "author": ["R.E. Steuer", "Choo", "E.-U"], "venue": "Mathematical Programming,", "citeRegEx": "Steuer et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Steuer et al\\.", "year": 1983}, {"title": "Research on LiMn2O4 battery\u2019s state of charge estimation with the consideration of degradation", "author": ["Tao", "Gao"], "venue": "Technical report, Tsinghua University,", "citeRegEx": "Tao and Gao.,? \\Q2012\\E", "shortCiteRegEx": "Tao and Gao.", "year": 2012}, {"title": "Generalized gini inequality indices", "author": ["Weymark", "John A"], "venue": "Mathematical Social Sciences,", "citeRegEx": "Weymark and A.,? \\Q1981\\E", "shortCiteRegEx": "Weymark and A.", "year": 1981}, {"title": "On ordered weighted averaging aggregation operators in multi-criteria decision making", "author": ["R.R. Yager"], "venue": "IEEE Trans. on Syst., Man and Cyb.,", "citeRegEx": "Yager,? \\Q1988\\E", "shortCiteRegEx": "Yager", "year": 1988}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Zinkevich", "Martin"], "venue": "In ICML,", "citeRegEx": "Zinkevich and Martin.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich and Martin.", "year": 2003}], "referenceMentions": [{"referenceID": 24, "context": "The bandit setup has become the standard modeling framework for many practical applications, such as online advertisement (Slivkins, 2014) or medical treatment design (Press, 2009) to name a few.", "startOffset": 122, "endOffset": 138}, {"referenceID": 21, "context": "The bandit setup has become the standard modeling framework for many practical applications, such as online advertisement (Slivkins, 2014) or medical treatment design (Press, 2009) to name a few.", "startOffset": 167, "endOffset": 180}, {"referenceID": 22, "context": "Besides, there are several studies published recently that consider multi-objective sequential decision problem under uncertainty (Drugan & Now\u00e9, 2013; Roijers et al., 2013; Mahdavi et al., 2013).", "startOffset": 130, "endOffset": 195}, {"referenceID": 15, "context": "Besides, there are several studies published recently that consider multi-objective sequential decision problem under uncertainty (Drugan & Now\u00e9, 2013; Roijers et al., 2013; Mahdavi et al., 2013).", "startOffset": 130, "endOffset": 195}, {"referenceID": 28, "context": "Different aggregation function can be used depending on the problem at hand, such as sum, weighted sum, min, max, (augmented) weighted Chebyshev norm (Steuer & Choo, 1983), Ordered Weighted Averages (OWA) (Yager, 1988) or Ordered Weighted Regret (OWR) (Ogryczak et al.", "startOffset": 205, "endOffset": 218}, {"referenceID": 19, "context": "Different aggregation function can be used depending on the problem at hand, such as sum, weighted sum, min, max, (augmented) weighted Chebyshev norm (Steuer & Choo, 1983), Ordered Weighted Averages (OWA) (Yager, 1988) or Ordered Weighted Regret (OWR) (Ogryczak et al., 2011) and its weighted version (Ogryczak et al.", "startOffset": 252, "endOffset": 275}, {"referenceID": 20, "context": ", 2011) and its weighted version (Ogryczak et al., 2013).", "startOffset": 33, "endOffset": 56}, {"referenceID": 7, "context": "This means that GGI is strictly decreasing with Pigou-Dalton transfers and all the components of w\u2032 are positive. Based on formulation (5), Ogryczak & Sliwinski (2003) showed that the GGI value of a vector x can be obtained by solving a linear program.", "startOffset": 54, "endOffset": 168}, {"referenceID": 15, "context": "In the online convex optimization setup with multiple objectives (Mahdavi et al., 2013), the learner\u2019s forecast x is evaluated in terms of multiple convex loss functions f (t) 0 (x), f (t) 1 (x), .", "startOffset": 65, "endOffset": 87}, {"referenceID": 17, "context": "In the approachability problem (Mannor et al., 2014; 2009; Abernethy et al., 2011), there are two players, say A and B.", "startOffset": 31, "endOffset": 82}, {"referenceID": 0, "context": "In the approachability problem (Mannor et al., 2014; 2009; Abernethy et al., 2011), there are two players, say A and B.", "startOffset": 31, "endOffset": 82}, {"referenceID": 10, "context": "Several multi-objective reinforcement learning algorithm have been proposed in the literature (G\u00e1bor et al., 1998; Roijers et al., 2013).", "startOffset": 94, "endOffset": 136}, {"referenceID": 22, "context": "Several multi-objective reinforcement learning algorithm have been proposed in the literature (G\u00e1bor et al., 1998; Roijers et al., 2013).", "startOffset": 94, "endOffset": 136}, {"referenceID": 0, "context": "Their work consists of extending the UCB algorithm (Auer et al., 2002a) so as to be able to handle multi-dimensional feedback vectors with the goal of determining all arms on the Pareto front. Azar et al. (2014) investigated a sequential decision making problem with vectorial feedback.", "startOffset": 52, "endOffset": 212}, {"referenceID": 13, "context": "The battery is modeled using the internal resistance (Rint) model (Johnson, 2002).", "startOffset": 66, "endOffset": 81}, {"referenceID": 8, "context": "The estimation of SOC is based on the Ampere Hour Counting method (Dambrowski, 2013).", "startOffset": 66, "endOffset": 84}, {"referenceID": 11, "context": "The variation of temperature in the system is determined according to the dissipative heat loss due to the internal resistance and thermal convection (Gao et al., 2002).", "startOffset": 150, "endOffset": 168}], "year": 2017, "abstractText": "We study the multi-armed bandit (MAB) problem where the agent receives a vectorial feedback that encodes many possibly competing objectives to be optimized. The goal of the agent is to find a policy, which can optimize these objectives simultaneously in a fair way. This multi-objective online optimization problem is formalized by using the Generalized Gini Index (GGI) aggregation function. We propose an online gradient descent algorithm which exploits the convexity of the GGI aggregation function, and controls the exploration in a careful way achieving a distribution-free regret \u00d5(T\u22121/2) with high probability. We test our algorithm on synthetic data as well as on an electric battery control problem where the goal is to trade off the use of the different cells of a battery in order to balance their respective degradation rates.", "creator": "LaTeX with hyperref package"}}}