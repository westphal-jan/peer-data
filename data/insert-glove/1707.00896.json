{"id": "1707.00896", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2017", "title": "Multilingual Hierarchical Attention Networks for Document Classification", "abstract": "rusk Hierarchical cra attention mimsy networks 320.1 have recently madc achieved keratinocytes remarkable naotaka performance for lenexa document classification 1255 in carnelia a oroz given language. kocaeli However, propiedad when multilingual document espadas collections cs4 are considered, training such 27.33 models separately deniss for fuiste each helmbreck language entails teollisuuden linear parameter reunidos growth and lack of cross - language 21.68 transfer. Learning a single cheyenne multilingual model with culkin fewer parameters is modulation therefore galip a envisages challenging but potentially beneficial antisymmetry objective. peraino To pollack this ronca end, archetypes we 5/6 propose cullison multilingual mucke hierarchical attention networks prosti for learning document feiss structures, mahato with shared encoders and / novenas or attention fabasoft mechanisms guntersville across ferritin languages, using freitas multi - task rafidiyeh learning tenno and welchman an aligned semantic dfb-pokal space caponi as input. duncanson We karkin evaluate the proposed simples models ajayi on multilingual christianize document tanevski classification estuviera with disjoint label in-ear sets, proportionally on a raimond large beseeching dataset which eibar we provide, melotone with 600k news emissaries documents in 8 \u0628\u0646 languages, and 5k moneyball labels. jori The timi\u015f multilingual maligawa models jianrong outperform posture strong monolingual vff ones ipec in maurus low - resource as well democr\u00e1tica as chastang full - portugues resource settings, 204.3 and elvie use jammed fewer 104.0 parameters, thus gallegly confirming their conservatives computational efficiency and para\u00edso the maging utility of semna cross - abulhassan language bandas transfer.", "histories": [["v1", "Tue, 4 Jul 2017 10:28:04 GMT  (2326kb,D)", "http://arxiv.org/abs/1707.00896v1", null], ["v2", "Sun, 9 Jul 2017 10:37:52 GMT  (2334kb,D)", "http://arxiv.org/abs/1707.00896v2", null], ["v3", "Wed, 6 Sep 2017 15:06:16 GMT  (2337kb,D)", "http://arxiv.org/abs/1707.00896v3", null], ["v4", "Fri, 15 Sep 2017 10:47:26 GMT  (2337kb,D)", "http://arxiv.org/abs/1707.00896v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nikolaos pappas", "andrei popescu-belis"], "accepted": true, "id": "1707.00896"}, "pdf": {"name": "1707.00896.pdf", "metadata": {"source": "CRF", "title": "Multilingual Hierarchical Attention Networks for Document Classification", "authors": ["Nikolaos Pappas", "Andrei Popescu-Belis"], "emails": ["nikolaos.pappas@idiap.ch", "andrei.popescu-belis@idiap.ch"], "sections": [{"heading": "1 Introduction", "text": "Learning word sequence representations has become increasingly useful for a variety of NLP tasks such as document classification (Tang et al., 2015; Yang et al., 2016), neural machine translation (NMT) (Cho et al., 2014; Luong et al., 2015), question answering (Chen et al., 2015; Kumar et al., 2015) and summarization (Rush et al., 2015). However, when data are available in multiple languages, representation learning must ad-\ndress two main challenges. Firstly, the computational cost of training separate models for each language grows linearly with their number, or even quadratically in the case of multi-way multilingual NMT (Firat et al., 2016a). Secondly, the models should be capable of cross-language transfer, which is an important component in human language learning (Ringbom, 2007).\nPrevious studies in document classification attempted to overcome these issues by employing multilingual word embeddings which allow direct comparisons and groupings across languages (Klementiev et al., 2012; Hermann and Blunsom, 2014; Ferreira et al., 2016). However, they are only applicable when common label sets are available across languages which is often not the case (e.g. Wikipedia or news). Moreover, despite recent advances in monolingual document modeling (Tang et al., 2015; Yang et al., 2016), multilingual models are still based on shallow networks.\nIn this paper, we propose Multilingual Hierarchical Attention Networks to learn shared document structures across languages for document classification with disjoint label sets, as opposed to training hierarchical attention networks (HANs) in a language-specific manner (Yang et al., 2016).\nar X\niv :1\n70 7.\n00 89\n6v 1\n[ cs\n.C L\n] 4\nJ ul\n2 01\n7\nOur networks have a hierarchical structure with word and sentence encoders, along with attention mechanisms. Each of these can either be shared across languages or kept language-specific. To enable cross-language transfer, the networks are trained with multi-task learning across languages using an aligned semantic space as input. Fig. 1 displays document vectors, projected with t-SNE (van der Maaten, 2009), for two topics and two languages, either learned by monolingual HANs (a) or by our multilingual HAN (b). The multilingual HAN achieves better separation between \u2018Europe\u2019 and \u2018Culture\u2019 topics in English as a result of the knowledge transfer from Arabic.\nWe evaluate our model against strong monolingual baselines, in low-resource and full-resource scenarios, on a large multilingual document collection with 600k documents, labeled with general (1.2k) and specific topics (4.4k), from Deutsche Welle \u2013 which we will make available. Our multilingual models outperform monolingual ones in both scenarios, thus confirming the utility of crosslanguage transfer and the computational efficiency of the proposed architecture."}, {"heading": "2 Related Work", "text": "Research on learning multilingual word representations is based on early work on word embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014). The goal is to learn an aligned word embedding space for multiple languages by leveraging bilingual dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016), parallel sentences (Gouws et al., 2015) or comparable documents such as Wikipedia pages (Yih et al., 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages. These sets are costly to produce, and are often not available in real world settings; here, we remove this constraint. Moreover, we develop new, deeper multilingual document models with hierarchical structure based on prior art at the word level.\nEarly work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014). Later studies focused on neural networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification. Johnson and Zhang (2015) proposed a CNN for high-dimensional data classification, while Zhang et al. (2015) adopted a character-level CNN for text classification. Lai et al. (2015) proposed a recurrent CNN to capture sequential information, which outperformed simpler CNNs. Lin et al. (2015) and Tang et al. (2015) proposed hierarchical recurrent NNs and showed that they were superior to CNN-based models. Recently, Yang et al. (2016) demonstrated that a hierarchical attention network with bi-directional gated encoders outperforms traditional and neural baselines. Using such networks in multilingual settings has two drawbacks: the computational complexity increases linearly with the number of languages, and knowledge is acquired separately for each language. We address these issues by proposing a new multilingual model based on HANs, which learns shared document structures and to transfer knowledge across languages.\nEarly examples of attention mechanisms within neural models appeared in computer vision (Larochelle and Hinton, 2010; Denil et al., 2012). In document classification, studies which aimed to learn the importance or saliency of sentences with respect to the classification objective, included Yessenalina et al. (2010); Pappas and PopescuBelis (2014); Yang et al. (2016) and more recently (Pappas and Popescu-Belis, 2017; Ji and Smith, 2017). In NMT, Bahdanau et al. (2015) proposed an attention-based encoder-decoder network, while Luong et al. (2015) proposed a local and ensemble attention model for NMT. Firat et al. (2016a) proposed a single encoder-decoder model with shared attention across language pairs for multi-way, multilingual NMT. Hermann et al. (2015) developed attention-based document readers for question answering. Chen et al. (2015) proposed a recurrent attention model over an external memory. Similarly, Kumar et al. (2015) introduced a dynamic memory network for question answering and other tasks. We propose hierarchical models for long word sequences with shared multi-level attention across languages, which, to our knowledge, has not been attempted before."}, {"heading": "3 Background: Hierarchical Attention Networks for Document Classification", "text": "We adopt the hierarchical attention networks for document representation proposed by Yang et al. (2016), as displayed in Figure 2. We consider a dataset D = {(xi, yi), i = 1, . . . , N} made of N documents xi with labels yi \u2208 {0, 1}k. Each document xi = {w11, w12, . . . , wKT } is represented by the sequence of d-dimensional embeddings of their words grouped into sentences, T being the maximum number of words in a sentence, and K the maximum number of sentences in a document.\nThe network takes as input a document xi and outputs a document vector ui. In particular, it has two levels of abstraction, word vs. sentence. The former is made of an encoder gw with parameters Hw and an attention aw with parameters Aw, while the latter similarly includes an encoder and an attention (gs, Hs and as, As). The output ui is used by the classification layer to determine yi."}, {"heading": "3.1 Encoder Layers", "text": "At the word level, the function gw encodes the sequence of input words {wit | t = 1, . . . , T} for each sentence i of the document, noted as:\nh(it)w = gw(wit), t \u2208 [1, T ], (1)\nand at the sentence level, after combining the intermediate word vectors {h(it)w | t = 1, . . . ,K} to a sentence vector si (see Section 3.2), the function gs encodes the sequence of sentence vectors {si | i = 1, . . . , S}, noted as h(i)s .\nThe gw and gs functions can be any feedforward or recurrent networks with parameters Hw and Hs respectively. We consider the following networks: a fully-connected one, noted as Dense, a Gated Recurrent Unit network (Cho et al., 2014) noted as GRU, and a bi-directional GRU, noted as biGRU.1 The latter is defined as a\n1GRU is a simplified version of Long-Short Term Mem-\nconcatenation of the hidden states for each input vector resulting from the forward GRU, ~gw, and the backward GRU, ~gw, written as:\nh(it)w = [ ~gw(h (it) w ); ~gw(h (it) w ) ]\n(2)\nThe same concatenation is applied for the hiddenstate representation of a sentence h(i)s ."}, {"heading": "3.2 Attention Layers", "text": "A typical way to obtain a representation for a given word sequence at each level is by taking the last hidden-state vector that is output by the encoder. However, it is hard to encode all the relevant input information needed in a fixed-length vector. This problem is addressed by introducing an attention mechanism at each level (noted \u03b1w and \u03b1s) that estimates the importance of each hidden state vector to the representation of the sentence or document meaning respectively. The sentence vector si \u2208 Rdw , where dw is the dimension of the word encoder, is thus obtained as follows:\n1\nT T\u2211 t=1 \u03b1(it)w h (it) w = 1 T T\u2211 t=1 exp(v>ituw)\u2211 j exp(v > ijuw) h(it)w\n(3) where vit = fw(h (it) w ) is a fully-connected neural network with Ww parameters. Similarly, the document vector u \u2208 Rds , where ds is the dimension of the sentence encoder, is obtained as follows:\n1\nK K\u2211 i=1 \u03b1(i)s h (i) s = 1 K K\u2211 i=1 exp(v>i us)\u2211 j exp(v > j us) h(i)s\n(4) where vi = fs(h (i) w ) is a fully-connected neural network with Ws parameters. The vectors uw and us are parameters which encode the word context and sentence context respectively, and are learned jointly with the rest of the parameters. The total set of parameters for aw is Aw = {Ww, uw} and for as is As = {Ws, us}."}, {"heading": "3.3 Classification Layers", "text": "The output of such a network is typically fed to a softmax layer for classification, with a loss based on the cross-entropy between gold and predicted labels (Tang et al., 2015) or a loss based on the negative log-likelihood of the correct labels (Yang et al., 2016). However, softmax overemphasizes the probability of the most likely label, which may\nory, LSTM (Hochreiter and Schmidhuber, 1997) and is able to capture temporal information forward or backward in time.\nnot be ideal for multi-label classification. Instead, it is more appropriate to have independent predictions for each label. Hence, we replace the softmax with a sigmoid function, and for each document i represented by the vector ui we model the probability of the k labels as follows:\ny\u0302i = p(y|ui) = 1\n1 + e\u2212(Wcui+bc) \u2208 [0, 1]k (5)\nwhereWc is a ds\u00d7k matrix and bc is the bias term for the classification layer. The training loss based on cross-entropy is computed as follows:\nL(\u03b8) = \u2212 1 N N\u2211 i=1 H(yi, y\u0302i) (6)\nwhere \u03b8 is a notation for all the parameters of the model (i.e. Hw, Aw, Hs, As,Wc), andH is the binary cross-entropy of the gold labels yi and predicted labels y\u0302i for a document i. The above objective is differentiable and can be minimized with stochastic gradient descent (Bottou, 1998) or variants such as Adam (Kingma and Ba, 2014), to maximize classification performance."}, {"heading": "4 Multilingual Hierarchical Attention Networks: MHANs", "text": "When multilingual data is available, the above network can be trained on each language separately but the needed parameters grow linearly with the number of languages. Moreover, they do not exploit common knowledge across languages or to transfer it from one to another. We propose here a hierarchical attention network with shared components across languages, hence with sublinear parameter growth, which enables knowledge transfer across languages. We now consider M languages noted L = {Ll | l = 1, . . . ,M}, and a multilingual set of topic-labeled documents D = {(x(l)i , y (l) i ) | i = 1, . . . , Nl} defined as above."}, {"heading": "4.1 Sharing Components across Languages", "text": "To enable multilingual learning, we propose three distinct ways for sharing components between networks in a multi-task learning setting, depicted in Figure 3, namely: (a) sharing the parameters of word and sentence encoders, hence \u03b8enc = {Hw,W (l)w , Hs,W (l)s ,W (l)c }; (b) sharing the parameters of word and sentence attention models, hence \u03b8att = {H(l)w ,Ww, H(l)s ,Ws,W (l)c }; and (c) sharing both previous sets of parameters, hence \u03b8both = {Hw,Ww, Hs,Ws,W (l) c }. If \u03b8mono =\n{H(l)w ,W (l)w , H(l)s ,W (l)s ,W (l)c } are the parameters of multiple independent monolingual models with Dense encoders, then we have:\n|\u03b8mono| > |\u03b8enc| > |\u03b8att| > |\u03b8both| (7)\nwhere | \u00b7 | is the number of parameters in a set. For GRU and biGRU encoders, the inequalities still hold, but swapping |\u03b8enc| and |\u03b8att|. Excluding the classification layer, the (a) and (b) networks have sublinear numbers of parameters and the (c) network has a constant number of parameters with respect to the number of languages.\nDepending on the label sets, several types of document classification problems can be solved with such architectures. First, label sets can be common or disjoint across languages. Second, considering labels as k-hot vectors, k = 1 corresponds to a multi-class task, while k > 1 is a multi-label task. We focus here on the multi-label problem with disjoint label sets. Moreover, we assume an aligned input space i.e with multilingual word embeddings that have aligned meanings across languages (Ammar et al., 2016). With nonaligned word embeddings, the multilingual transfer is harder due to the complete lack of parallel information, as we show in Section 6."}, {"heading": "4.2 Training over Disjoint Label Sets", "text": "For training, we replace the monolingual training objective (Eq. 6) with a joint multilingual objective that facilitates the sharing of components, i.e. a subset of parameters for each language \u03b81, \u03b82, ..., \u03b8M , across different language networks:\nL(\u03b81,...,\u03b8M ) = \u2212 1\nZ M\u2211 l \u03b3l Ne\u2211 i H(y(l)i , y\u0302 (l) i ) (8)\nwhere Z = M \u00d7Ne with Ne being the epoch size and \u03b3l the importance to give for the objective of\neach language. The joint objective L can be minimized with respect to the parameters \u03b81, \u03b82, ..., \u03b8M using SGD as before. However, when training on examples from different languages consecutively it is difficult to learn a shared space which works well across languages. This is because each language updates apply only on a subset of parameters and may bias the model away from other languages. To address this, we employ a training strategy similar to Firat et al. (2016a), who sampled parallel sentences for multi-way machine translation from different language pairs in a cyclic fashion. Here, we sample a document-label pair for each language at each iteration."}, {"heading": "5 A New Corpus for Multilingual Document Classification: DW", "text": "Multilingual document classification datasets are usually limited in size, have target categories aligned across languages, and assign documents to only one category. However, classification is often necessary in cases where the categories are not strictly aligned and can be multiple per document. For instance, this is the case for online news agencies keeping track of multilingual news written and annotated by different journalists, a process which is expensive and time-consuming.\nTwo datasets for multilingual document classification have been used in previous studies: Reuters RCV1/RCV2 (6,000 documents, 2 languages and 4 labels) and TED talk transcripts (12,078 documents, 12 languages and 15 labels) introduced by Hermann and Blunsom (2014). The former is tailored for evaluating word embeddings aligned across languages, rather than complex multilingual document models. The latter is twice as large and covers more languages, in a multi-label setting, but biases evaluation by including translations of talks in all languages.\nHere, we present and use a much larger dataset collected from Deutsche Welle, Germany\u2019s public international broadcaster.2 The DW dataset contains nearly 600,000 documents annotated by journalists with several topic labels in 8 languages, shown in Table 1. Documents are on average 2.6 times longer than in Yang et al.\u2019s (2016) monolingual dataset (436 vs. 163 words). There are two types of labels, namely general topics (Yg) and specific ones (Ys) described by one or more words.\n2The dataset contains news visible at www.dw.com and will be made available upon acceptance along with our code.\nWe consider (and count in Table 1) only those specific labels that appear at least 100 times, to avoid sparsity issues.\nThe number of labels varies greatly across DW\u2019s language services. Moreover, we found for instance that only 25-30% of the labels could be manually aligned between English and German. The commonalities are mainly concentrated on the most frequent labels, reflecting a shared top-level division of the news domain, but the long tail exhibits significant independence across languages."}, {"heading": "6 Evaluation", "text": "We evaluate our multilingual models on fullresource and low-resource scenarios of multilingual document classification over disjoint label sets on the Deutsche Welle corpus."}, {"heading": "6.1 Settings", "text": "The corpus is split per language into 80% for training, 10% for validation and 10% for testing. We evaluate both type of labels (Yg, Ys) on a fullresource scenario and only the general topics (Yg) on a low-resource scenario. We report the microaveraged F1 scores for each test set, as in previous work (Hermann and Blunsom, 2014).\nModel configuration. For all models, we use the pre-trained 40-dimensional multilingual embeddings trained on the Leipzig corpus from Ammar et al. (2016). We zero-pad documents up to a maximum of 30 words per sentence and 30 sentences per document. The hyper-parameters were selected on the validation sets. We made the following settings: 100-dimensional encoder and attention embeddings (at every level), relu activation function for all intermediate layers, batch size of 16, epoch size of 25k, \u03b3l of 0.5, and optimization using SGD with Adam until convergence.\nAll the hierarchical models have Dense encoders in both scenarios (Tables 2, 4, and 5), and GRU and biGRU in the full-resource scenario for English+Arabic (Table 3). For the low-resource scenario, we define three levels of data availability: tiny from 0.1% to 0.5%, small from 1% to 5% and medium from 10% to 50% of the original training set. We report the average F1 scores on the test set for each level based on discrete increments of 0.1, 1 and 10 respectively. The decision threshold for the full-resource scenario is set to 0.4 for |Ys| < 400 and 0.2 for |Ys| \u2265 400, and for the low-resource scenario it is 0.3 for all sets.\nBaselines. We compare against the following monolingual neural networks, with shallow or hierarchical structure. These networks are based on the state of the art in the field, reviewed in Section 2, and thus represent strong baselines.\n\u2022 NN : A neural network which feeds the average vector of the input words directly to a classification layer, as the common baseline for multilingual document classification by Klementiev et al. (2012).\n\u2022 HNN : A hierarchical network with encoders and average pooling at every level, followed by a classification layer.\n\u2022 HAN: A hierarchical network with encoders and attention, followed by a classification layer. This model is the one proposed by Yang et al. (2016) adapted to our task.\nWhen training our multilingual system with the three options described in Section 4.1 noted as Enc, Att and Both, we use in fact the multilingual version of the third monolingual baseline, as illustrated in Fig. 3, trained with the objective of Eq. 8."}, {"heading": "6.2 Results", "text": "Full-resource Scenario. Table 2 displays the results of full-resource document classification for general and specific labels. In particular, on the left, the performance on the English sub-corpus is shown when an auxiliary sub-corpus from another language is used for training, and on the right the performance on a target sub-corpus is shown when English is used as an auxiliary sub-corpus.\nThe multilingual model trained on pairs of languages outperforms on average all the examined monolingual models, from simple bag-of-words neural models to hierarchical neural models using average pooling or attention. The best-performing multilingual architecture is the one with shared attention across languages, especially when tested on English (documents and labels). Interestingly, this reveals that the transfer of knowledge across languages in a full-resource setting is maximized with language-specific word and sentence encoders, but language-independent (i.e. shared) attention for both words and sentences.\nHowever, when transferring from English to Portuguese (en\u2192pt), Russian (en\u2192ru) and Persian (en\u2192fa) on general categories, it is more effective to have only language-independent compo-\nnents. We hypothesize that this is due to the underlying commonness between the label sets rather than the relationship between languages. The consistent gain for English as target could be attributed to the alignment of the word embeddings to English and to the large number of English labels, which makes it easier to find multilingual labels from which to transfer knowledge.\nWe quantify now the impact of three important model choices on the performance: encoder type, word embeddings, and number of languages used for training. In Table 3, we observe that when we replace the Dense encoder layers with GRU or biGRU layers, the improvement from the multilingual training is still present. In particular, the multilingual models with shared attention are superior to alternatives regardless of the employed encoders. For reference, using simply logistic regression with bag-of-words (counts) for classification leads to F1 scores of 75.8% in English and 81.9% in Arabic, using many more parameters than biGRU: 56.5M vs. 410k in English and 5.8M vs. 364k in Arabic.\nIn Table 4, we observe that when we train our multilingual model (MHAN-att) on eight languages at the same time, the F1 scores improve across languages \u2013 for both types of labels, general or specific \u2013 while the number of parameters per language decreases, by 36% for Ygeneral and 20% for Yspecific . Lastly, when we train the same model with word embeddings that are not aligned across languages, the performance of the multilingual model drops significantly. An input space that\nis aligned across languages is thus crucial. Low-resource Scenario. We assess the ability of the multilingual attention networks to transfer knowledge across languages in a low-resource scenario. The results for seven languages when trained jointly with English are displayed in Table 5. In all cases, at least one of the multilingual models outperforms the monolingual one, which demonstrates the usefulness of multilingual training for low-resource document classification.\nMoreover, the improvements obtained from our multilingual models for lower levels of availability (tiny and small) are larger than in higher levels (medium). This is also clearly observed in Figure 4 with our multilingual attention network ensemble, i.e. when we do model selection among the three multilingual variants on the development set. The best performing architecture in a majority of cases is the one which shares both the encoders and the attention mechanisms across languages. Moreover, this architecture also has the fewest number\nof parameters for the document modeling.\nThis promising finding for the low-resource scenario means that the classification performance can greatly benefit from the multilingual training (sharing encoders and attention) without increasing the parameters beyond that of a single monolingual document model. Nevertheless, in a few cases, we observe that the other two architectures, with increased complexity perform better than the \u201cshared both\u201d model. For instance, sharing encoders is superior to alternatives for Arabic language, i.e. the knowledge transfer benefits from shared word and sentence representations. Hence, to generalize to a large number of languages, we may need to consider more sophisticated models than sharing a monolingual document model across languages. Lastly, we did not generally observe a negative (or positive) correlation of the closeness between languages with the performance in the low-resource scenario, although the largest improvements were observed on languages more related to English (German, Spanish, Portuguese) than others (Arabic)."}, {"heading": "6.3 Qualitative Analysis", "text": "We analyze the performance of the model over the full range of labels, to observe on which type of labels it performs better than the monolingual model, and provide some qualitative examples. Figure 5 shows the cumulative true positive (TP) difference between the monolingual and multilingual models on the Arabic, German, Portuguese and Russian test sets, ordered by label frequency. We can observe that the cumulative TP difference of multilingual model consistently keeps increasing as the frequency of the label decreases. This shows that labels across the entire range of frequencies benefit from joint training with English and not only a subset, e.g. only frequent labels.\nFor example, the top 5 labels on which the multilingual model performed better than the monolingual one for en \u2192 de were: russland (21), berlin (19), irak (14), wahlen (13) and nato (13), while for the opposite direction those were: germany (259), german (97), soccer (73), football (47) and merkel (25). These topics are likely better covered in the respective auxiliary language which helps the multilingual model to better distinguish them in the target language. This is also observed in Figure 1, as the improved separation of topics using multilingual model vs. monolingual ones."}, {"heading": "7 Conclusion", "text": "We proposed multilingual hierarchical attention networks for document classification and showed that they can benefit both full-resource and lowresource scenarios by using fewer parameters than with monolingual networks. In the former scenario, the most beneficial was to share only the attention mechanisms, while in the latter one, to share the encoders along with the attention mechanisms. These results confirm the merits of language transfer, which is also an important component of human language learning (Odlin, 1989; Ringbom, 2007). Moreover, our study broadens the applicability of multilingual document classification, since our framework is not restricted to common label sets.\nThere are several future directions for this study. In their current form, our models cannot generalize to languages without any example, as attempted by Firat et al. (2016b) for neural MT. This could be achieved by a label-size independent classification layer as in zero-shot classification (Qiao et al., 2016; Nam et al., 2016). More-\nover, although we explored three distinct architectures, other configurations could be examined to improve document modeling, for example by sharing the attention mechanism at the sentence-level only. Lastly, the learning objective could be further constrained with document-level or sentencelevel parallel information, to embed multilingual vectors of similar topics more closely together in the learned space."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning. Sofia, Bulgaria.", "citeRegEx": "Al.Rfou et al\\.,? 2013", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Massively multilingual word embeddings", "author": ["Waleed Ammar", "George Mulcaire", "Yulia Tsvetkov", "Guillaume Lample", "Chris Dyer", "Noah A. Smith."], "venue": "CoRR abs/1602.01925.", "citeRegEx": "Ammar et al\\.,? 2016", "shortCiteRegEx": "Ammar et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 5th International Conference on Learning Representations. San Diego, CA, USA.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "On-line learning and stochastic approximations", "author": ["L\u00e9on Bottou."], "venue": "David Saad, editor, On-line Learning in Neural Networks, Cambridge University Press, pages 9\u201342.", "citeRegEx": "Bottou.,? 1998", "shortCiteRegEx": "Bottou.", "year": 1998}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Sarath Chandar", "Stanislas Lauly", "Hugo Larochelle", "Mitesh Khapra", "Balaraman Ravindran", "Vikas C. Raykar", "Amrita Saha."], "venue": "Advances in Neural Information Processing Sys-", "citeRegEx": "Chandar et al\\.,? 2014", "shortCiteRegEx": "Chandar et al\\.", "year": 2014}, {"title": "End-to-end learning of LDA by mirrordescent back propagation over a deep architecture", "author": ["Jianshu Chen", "Ji He", "Yelong Shen", "Lin Xiao", "Xiaodong He", "Jianfeng Gao", "Xinying Song", "Li Deng."], "venue": "Advances in Neural Information Processing Sys-", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["Misha Denil", "Loris Bazzani", "Hugo Larochelle", "Nando de Freitas."], "venue": "Neural Computation 24(8):2151\u20132184.", "citeRegEx": "Denil et al\\.,? 2012", "shortCiteRegEx": "Denil et al\\.", "year": 2012}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Manaal Faruqui", "Chris Dyer."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. Gothenburg, Sweden, pages", "citeRegEx": "Faruqui and Dyer.,? 2014", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "Jointly learning to embed and predict with multiple languages", "author": ["Daniel C. Ferreira", "Andr\u00e9 F.T. Martins", "Mariana S.C. Almeida."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-", "citeRegEx": "Ferreira et al\\.,? 2016", "shortCiteRegEx": "Ferreira et al\\.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Firat et al\\.,? 2016a", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Zero-resource translation with multi-lingual neural machine translation", "author": ["Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T. Yarman Vural", "Kyunghyun Cho."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Firat et al\\.,? 2016b", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "BilBOWA: Fast bilingual distributed representations without word alignments", "author": ["Stephan Gouws", "Yoshua Bengio", "Gregory S. Corrado."], "venue": "Proceedings of the 32nd International Conference on Machine Learning. Lille, France, pages 748\u2013756.", "citeRegEx": "Gouws et al\\.,? 2015", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Multilingual models for compositional distributed semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Baltimore, Maryland, pages 58\u201368.", "citeRegEx": "Hermann and Blunsom.,? 2014", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Proceedings of the 28th International Conference on Neural In-", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, MIT Press, volume 9 (8), pages 1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Neural discourse structure for text categorization", "author": ["Yangfeng Ji", "Noah Smith."], "venue": "CoRR abs/1702.01829. http://arxiv.org/abs/1702.01829.", "citeRegEx": "Ji and Smith.,? 2017", "shortCiteRegEx": "Ji and Smith.", "year": 2017}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["Rie Johnson", "Tong Zhang."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "Johnson and Zhang.,? 2015", "shortCiteRegEx": "Johnson and Zhang.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Doha, Qatar, pages 1746\u20131751.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba."], "venue": "Proceedings of the International Conference on Learning Representations. Banff, Canada.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai."], "venue": "Proceedings of the International Conference on Computational Linguistics. Bombay, India.", "citeRegEx": "Klementiev et al\\.,? 2012", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher."], "venue": "Proceedings of the 33rd In-", "citeRegEx": "Kumar et al\\.,? 2015", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao."], "venue": "Proceedings of the 29th AAAI Conference on Artificial Intelligence. Austin, Texas, pages 2267\u20132273.", "citeRegEx": "Lai et al\\.,? 2015", "shortCiteRegEx": "Lai et al\\.", "year": 2015}, {"title": "Learning to combine foveal glimpses with a third-order Boltzmann machine", "author": ["Hugo Larochelle", "Geoffrey Hinton."], "venue": "Proceedings of the 23rd International Conference on Neural Information Processing Systems. Vancouver, Canada, pages 1243\u20131251.", "citeRegEx": "Larochelle and Hinton.,? 2010", "shortCiteRegEx": "Larochelle and Hinton.", "year": 2010}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V. Le", "Tomas Mikolov."], "venue": "Proceedings of the 31st International Conference on Machine Learning. Beijing, China, pages 1188\u2013 1196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["Rui Lin", "Shujie Liu", "Muyun Yang", "Mu Li", "Ming Zhou", "Sheng Li."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal, pages", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal, pages", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of the International Conference on Learning Representations. Scottsdale, AZ, USA.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "All-in text: Learning document, label, and word representations jointly", "author": ["Jinseok Nam", "Eneldo Loza Menc\u0131\u0301a", "Johannes F\u00fcrnkranz"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence", "citeRegEx": "Nam et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nam et al\\.", "year": 2016}, {"title": "Language transfer: Crosslinguistic influence in language learning", "author": ["Terence Odlin."], "venue": "Cambridge Applied Linguistics, Cambridge University Press.", "citeRegEx": "Odlin.,? 1989", "shortCiteRegEx": "Odlin.", "year": 1989}, {"title": "Explaining the stars: Weighted multiple-instance learning for aspect-based sentiment analysis", "author": ["Nikolaos Pappas", "Andrei Popescu-Belis."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Doha, Qatar, pages", "citeRegEx": "Pappas and Popescu.Belis.,? 2014", "shortCiteRegEx": "Pappas and Popescu.Belis.", "year": 2014}, {"title": "Explicit document modeling through weighted multiple-instance learning", "author": ["Nikolaos Pappas", "Andrei Popescu-Belis."], "venue": "Journal of Artificial Intelligence Research pages 591\u2013626. https://doi.org/doi:10.1613/jair.5240.", "citeRegEx": "Pappas and Popescu.Belis.,? 2017", "shortCiteRegEx": "Pappas and Popescu.Belis.", "year": 2017}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Doha, Qatar, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Less is more: Zero-shot learning from online textual documents with noise suppression", "author": ["Ruizhi Qiao", "Lingqiao Liu", "Chunhua Shen", "Anton van den Hengel."], "venue": "Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recogni-", "citeRegEx": "Qiao et al\\.,? 2016", "shortCiteRegEx": "Qiao et al\\.", "year": 2016}, {"title": "Cross-linguistic Similarity in Foreign Language Learning", "author": ["Hakan Ringbom."], "venue": "Second language acquisition. Multilingual Matters.", "citeRegEx": "Ringbom.,? 2007", "shortCiteRegEx": "Ringbom.", "year": 2007}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal, pages 379\u2013389.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu."], "venue": "Empirical Methods on Natural Language Processing. Lisbon, Spain, pages 1422\u20131432.", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Uppsala, Sweden, pages 384\u2013", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Learning a parametric embedding by preserving local structure", "author": ["Laurens van der Maaten."], "venue": "Proceedings of the 12th International Conference on Artificial Intelligence and Statistics. Clearwater Beach, FL, USA, pages 384\u2013391.", "citeRegEx": "Maaten.,? 2009", "shortCiteRegEx": "Maaten.", "year": 2009}, {"title": "Hierarchical attention networks for document classification", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Multi-level structured models for documentlevel sentiment classification", "author": ["Ainur Yessenalina", "Yisong Yue", "Claire Cardie."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Compu-", "citeRegEx": "Yessenalina et al\\.,? 2010", "shortCiteRegEx": "Yessenalina et al\\.", "year": 2010}, {"title": "Learning discriminative projections for text similarity measures", "author": ["Wen-tau Yih", "Kristina Toutanova", "John C. Platt", "Christopher Meek."], "venue": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning. Portland, OR, USA,", "citeRegEx": "Yih et al\\.,? 2011", "shortCiteRegEx": "Yih et al\\.", "year": 2011}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "Advances in Neural Information Processing Systems 28. Montreal, Canada, pages 649\u2013 657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Seattle, Washing-", "citeRegEx": "Zou et al\\.,? 2013", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 36, "context": "Learning word sequence representations has become increasingly useful for a variety of NLP tasks such as document classification (Tang et al., 2015; Yang et al., 2016), neural machine translation (NMT) (Cho et al.", "startOffset": 129, "endOffset": 167}, {"referenceID": 39, "context": "Learning word sequence representations has become increasingly useful for a variety of NLP tasks such as document classification (Tang et al., 2015; Yang et al., 2016), neural machine translation (NMT) (Cho et al.", "startOffset": 129, "endOffset": 167}, {"referenceID": 6, "context": ", 2016), neural machine translation (NMT) (Cho et al., 2014; Luong et al., 2015), question answering (Chen et al.", "startOffset": 42, "endOffset": 80}, {"referenceID": 26, "context": ", 2016), neural machine translation (NMT) (Cho et al., 2014; Luong et al., 2015), question answering (Chen et al.", "startOffset": 42, "endOffset": 80}, {"referenceID": 5, "context": ", 2015), question answering (Chen et al., 2015; Kumar et al., 2015) and summarization (Rush et al.", "startOffset": 28, "endOffset": 67}, {"referenceID": 21, "context": ", 2015), question answering (Chen et al., 2015; Kumar et al., 2015) and summarization (Rush et al.", "startOffset": 28, "endOffset": 67}, {"referenceID": 35, "context": ", 2015) and summarization (Rush et al., 2015).", "startOffset": 26, "endOffset": 45}, {"referenceID": 10, "context": "NMT (Firat et al., 2016a).", "startOffset": 4, "endOffset": 25}, {"referenceID": 34, "context": "Secondly, the models should be capable of cross-language transfer, which is an important component in human language learning (Ringbom, 2007).", "startOffset": 126, "endOffset": 141}, {"referenceID": 20, "context": "Previous studies in document classification attempted to overcome these issues by employing multilingual word embeddings which allow direct comparisons and groupings across languages (Klementiev et al., 2012; Hermann and Blunsom, 2014; Ferreira et al., 2016).", "startOffset": 183, "endOffset": 258}, {"referenceID": 13, "context": "Previous studies in document classification attempted to overcome these issues by employing multilingual word embeddings which allow direct comparisons and groupings across languages (Klementiev et al., 2012; Hermann and Blunsom, 2014; Ferreira et al., 2016).", "startOffset": 183, "endOffset": 258}, {"referenceID": 9, "context": "Previous studies in document classification attempted to overcome these issues by employing multilingual word embeddings which allow direct comparisons and groupings across languages (Klementiev et al., 2012; Hermann and Blunsom, 2014; Ferreira et al., 2016).", "startOffset": 183, "endOffset": 258}, {"referenceID": 36, "context": "Moreover, despite recent advances in monolingual document modeling (Tang et al., 2015; Yang et al., 2016), multilingual models are still based on shallow networks.", "startOffset": 67, "endOffset": 105}, {"referenceID": 39, "context": "Moreover, despite recent advances in monolingual document modeling (Tang et al., 2015; Yang et al., 2016), multilingual models are still based on shallow networks.", "startOffset": 67, "endOffset": 105}, {"referenceID": 39, "context": "In this paper, we propose Multilingual Hierarchical Attention Networks to learn shared document structures across languages for document classification with disjoint label sets, as opposed to training hierarchical attention networks (HANs) in a language-specific manner (Yang et al., 2016).", "startOffset": 270, "endOffset": 289}, {"referenceID": 8, "context": "The goal is to learn an aligned word embedding space for multiple languages by leveraging bilingual dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016), parallel sentences (Gouws et al.", "startOffset": 113, "endOffset": 157}, {"referenceID": 1, "context": "The goal is to learn an aligned word embedding space for multiple languages by leveraging bilingual dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016), parallel sentences (Gouws et al.", "startOffset": 113, "endOffset": 157}, {"referenceID": 12, "context": ", 2016), parallel sentences (Gouws et al., 2015) or comparable documents such as Wikipedia pages (Yih et al.", "startOffset": 28, "endOffset": 48}, {"referenceID": 41, "context": ", 2015) or comparable documents such as Wikipedia pages (Yih et al., 2011; Al-Rfou et al., 2013).", "startOffset": 56, "endOffset": 96}, {"referenceID": 0, "context": ", 2015) or comparable documents such as Wikipedia pages (Yih et al., 2011; Al-Rfou et al., 2013).", "startOffset": 56, "endOffset": 96}, {"referenceID": 20, "context": "Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al.", "startOffset": 84, "endOffset": 127}, {"referenceID": 43, "context": "Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al.", "startOffset": 84, "endOffset": 127}, {"referenceID": 4, "context": ", 2013), including auto-encoders (Chandar et al., 2014).", "startOffset": 33, "endOffset": 55}, {"referenceID": 24, "context": "Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014).", "startOffset": 130, "endOffset": 152}, {"referenceID": 0, "context": ", 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al.", "startOffset": 8, "endOffset": 354}, {"referenceID": 0, "context": ", 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages.", "startOffset": 8, "endOffset": 380}, {"referenceID": 0, "context": ", 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages. These sets are costly to produce, and are often not available in real world settings; here, we remove this constraint. Moreover, we develop new, deeper multilingual document models with hierarchical structure based on prior art at the word level. Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014). Later studies focused on neural networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification.", "startOffset": 8, "endOffset": 981}, {"referenceID": 0, "context": ", 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages. These sets are costly to produce, and are often not available in real world settings; here, we remove this constraint. Moreover, we develop new, deeper multilingual document models with hierarchical structure based on prior art at the word level. Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014). Later studies focused on neural networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification. Johnson and Zhang (2015) proposed a CNN for high-dimensional data classification, while Zhang et al.", "startOffset": 8, "endOffset": 1081}, {"referenceID": 0, "context": ", 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages. These sets are costly to produce, and are often not available in real world settings; here, we remove this constraint. Moreover, we develop new, deeper multilingual document models with hierarchical structure based on prior art at the word level. Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014). Later studies focused on neural networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification. Johnson and Zhang (2015) proposed a CNN for high-dimensional data classification, while Zhang et al. (2015) adopted a character-level CNN for text classification.", "startOffset": 8, "endOffset": 1164}, {"referenceID": 0, "context": ", 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages. These sets are costly to produce, and are often not available in real world settings; here, we remove this constraint. Moreover, we develop new, deeper multilingual document models with hierarchical structure based on prior art at the word level. Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014). Later studies focused on neural networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification. Johnson and Zhang (2015) proposed a CNN for high-dimensional data classification, while Zhang et al. (2015) adopted a character-level CNN for text classification. Lai et al. (2015) proposed a recurrent CNN to capture sequential information, which outperformed simpler CNNs.", "startOffset": 8, "endOffset": 1237}, {"referenceID": 0, "context": ", 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages. These sets are costly to produce, and are often not available in real world settings; here, we remove this constraint. Moreover, we develop new, deeper multilingual document models with hierarchical structure based on prior art at the word level. Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014). Later studies focused on neural networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification. Johnson and Zhang (2015) proposed a CNN for high-dimensional data classification, while Zhang et al. (2015) adopted a character-level CNN for text classification. Lai et al. (2015) proposed a recurrent CNN to capture sequential information, which outperformed simpler CNNs. Lin et al. (2015) and Tang et al.", "startOffset": 8, "endOffset": 1348}, {"referenceID": 0, "context": ", 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages. These sets are costly to produce, and are often not available in real world settings; here, we remove this constraint. Moreover, we develop new, deeper multilingual document models with hierarchical structure based on prior art at the word level. Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014). Later studies focused on neural networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification. Johnson and Zhang (2015) proposed a CNN for high-dimensional data classification, while Zhang et al. (2015) adopted a character-level CNN for text classification. Lai et al. (2015) proposed a recurrent CNN to capture sequential information, which outperformed simpler CNNs. Lin et al. (2015) and Tang et al. (2015) proposed hierarchical recurrent NNs and showed that they were superior to CNN-based models.", "startOffset": 8, "endOffset": 1371}, {"referenceID": 0, "context": ", 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored: the approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages. These sets are costly to produce, and are often not available in real world settings; here, we remove this constraint. Moreover, we develop new, deeper multilingual document models with hierarchical structure based on prior art at the word level. Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014). Later studies focused on neural networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification. Johnson and Zhang (2015) proposed a CNN for high-dimensional data classification, while Zhang et al. (2015) adopted a character-level CNN for text classification. Lai et al. (2015) proposed a recurrent CNN to capture sequential information, which outperformed simpler CNNs. Lin et al. (2015) and Tang et al. (2015) proposed hierarchical recurrent NNs and showed that they were superior to CNN-based models. Recently, Yang et al. (2016) demonstrated that a hierarchical attention network with bi-directional gated encoders outperforms traditional and neu-", "startOffset": 8, "endOffset": 1492}, {"referenceID": 23, "context": "Early examples of attention mechanisms within neural models appeared in computer vision (Larochelle and Hinton, 2010; Denil et al., 2012).", "startOffset": 88, "endOffset": 137}, {"referenceID": 7, "context": "Early examples of attention mechanisms within neural models appeared in computer vision (Larochelle and Hinton, 2010; Denil et al., 2012).", "startOffset": 88, "endOffset": 137}, {"referenceID": 31, "context": "(2016) and more recently (Pappas and Popescu-Belis, 2017; Ji and Smith, 2017).", "startOffset": 25, "endOffset": 77}, {"referenceID": 16, "context": "(2016) and more recently (Pappas and Popescu-Belis, 2017; Ji and Smith, 2017).", "startOffset": 25, "endOffset": 77}, {"referenceID": 29, "context": "learn the importance or saliency of sentences with respect to the classification objective, included Yessenalina et al. (2010); Pappas and PopescuBelis (2014); Yang et al.", "startOffset": 101, "endOffset": 127}, {"referenceID": 29, "context": "learn the importance or saliency of sentences with respect to the classification objective, included Yessenalina et al. (2010); Pappas and PopescuBelis (2014); Yang et al.", "startOffset": 101, "endOffset": 159}, {"referenceID": 29, "context": "(2010); Pappas and PopescuBelis (2014); Yang et al. (2016) and more recently (Pappas and Popescu-Belis, 2017; Ji and Smith, 2017).", "startOffset": 40, "endOffset": 59}, {"referenceID": 2, "context": "In NMT, Bahdanau et al. (2015) proposed an attention-based encoder-decoder network, while Luong et al.", "startOffset": 8, "endOffset": 31}, {"referenceID": 2, "context": "In NMT, Bahdanau et al. (2015) proposed an attention-based encoder-decoder network, while Luong et al. (2015) proposed a local and ensemble attention model for NMT.", "startOffset": 8, "endOffset": 110}, {"referenceID": 2, "context": "In NMT, Bahdanau et al. (2015) proposed an attention-based encoder-decoder network, while Luong et al. (2015) proposed a local and ensemble attention model for NMT. Firat et al. (2016a) proposed a single encoder-decoder model with shared attention across language pairs for multi-way, multilingual NMT.", "startOffset": 8, "endOffset": 186}, {"referenceID": 2, "context": "In NMT, Bahdanau et al. (2015) proposed an attention-based encoder-decoder network, while Luong et al. (2015) proposed a local and ensemble attention model for NMT. Firat et al. (2016a) proposed a single encoder-decoder model with shared attention across language pairs for multi-way, multilingual NMT. Hermann et al. (2015) developed attention-based document readers for question answering.", "startOffset": 8, "endOffset": 325}, {"referenceID": 2, "context": "In NMT, Bahdanau et al. (2015) proposed an attention-based encoder-decoder network, while Luong et al. (2015) proposed a local and ensemble attention model for NMT. Firat et al. (2016a) proposed a single encoder-decoder model with shared attention across language pairs for multi-way, multilingual NMT. Hermann et al. (2015) developed attention-based document readers for question answering. Chen et al. (2015) proposed a recurrent attention model over an external memory.", "startOffset": 8, "endOffset": 411}, {"referenceID": 2, "context": "In NMT, Bahdanau et al. (2015) proposed an attention-based encoder-decoder network, while Luong et al. (2015) proposed a local and ensemble attention model for NMT. Firat et al. (2016a) proposed a single encoder-decoder model with shared attention across language pairs for multi-way, multilingual NMT. Hermann et al. (2015) developed attention-based document readers for question answering. Chen et al. (2015) proposed a recurrent attention model over an external memory. Similarly, Kumar et al. (2015) introduced a dynamic memory network for question answering and other tasks.", "startOffset": 8, "endOffset": 504}, {"referenceID": 39, "context": "We adopt the hierarchical attention networks for document representation proposed by Yang et al. (2016), as displayed in Figure 2.", "startOffset": 85, "endOffset": 104}, {"referenceID": 6, "context": "We consider the following networks: a fully-connected one, noted as Dense, a Gated Recurrent Unit network (Cho et al., 2014) noted as GRU, and a bi-directional GRU, noted as biGRU.", "startOffset": 106, "endOffset": 124}, {"referenceID": 36, "context": "The output of such a network is typically fed to a softmax layer for classification, with a loss based on the cross-entropy between gold and predicted labels (Tang et al., 2015) or a loss based on the negative log-likelihood of the correct labels (Yang et al.", "startOffset": 158, "endOffset": 177}, {"referenceID": 39, "context": ", 2015) or a loss based on the negative log-likelihood of the correct labels (Yang et al., 2016).", "startOffset": 77, "endOffset": 96}, {"referenceID": 15, "context": "ory, LSTM (Hochreiter and Schmidhuber, 1997) and is able to capture temporal information forward or backward in time.", "startOffset": 10, "endOffset": 44}, {"referenceID": 3, "context": "The above objective is differentiable and can be minimized with stochastic gradient descent (Bottou, 1998) or vari-", "startOffset": 92, "endOffset": 106}, {"referenceID": 19, "context": "ants such as Adam (Kingma and Ba, 2014), to maximize classification performance.", "startOffset": 18, "endOffset": 39}, {"referenceID": 1, "context": "e with multilingual word embeddings that have aligned meanings across languages (Ammar et al., 2016).", "startOffset": 80, "endOffset": 100}, {"referenceID": 10, "context": "To address this, we employ a training strategy similar to Firat et al. (2016a), who sampled parallel sentences for multi-way machine translation from different language pairs in a cyclic fashion.", "startOffset": 58, "endOffset": 79}, {"referenceID": 13, "context": "4 labels) and TED talk transcripts (12,078 documents, 12 languages and 15 labels) introduced by Hermann and Blunsom (2014). The former is tailored for evaluating word embeddings aligned across languages, rather than complex multilingual document models.", "startOffset": 96, "endOffset": 123}, {"referenceID": 39, "context": "6 times longer than in Yang et al.\u2019s (2016) monolingual dataset (436 vs.", "startOffset": 23, "endOffset": 44}, {"referenceID": 13, "context": "We report the microaveraged F1 scores for each test set, as in previous work (Hermann and Blunsom, 2014).", "startOffset": 77, "endOffset": 104}, {"referenceID": 1, "context": "For all models, we use the pre-trained 40-dimensional multilingual embeddings trained on the Leipzig corpus from Ammar et al. (2016). We zero-pad documents up to a maximum of 30 words per sentence and 30 sentences per document.", "startOffset": 113, "endOffset": 133}, {"referenceID": 20, "context": "\u2022 NN : A neural network which feeds the average vector of the input words directly to a classification layer, as the common baseline for multilingual document classification by Klementiev et al. (2012).", "startOffset": 177, "endOffset": 202}, {"referenceID": 39, "context": "This model is the one proposed by Yang et al. (2016) adapted to our task.", "startOffset": 34, "endOffset": 53}, {"referenceID": 29, "context": "These results confirm the merits of language transfer, which is also an important component of human language learning (Odlin, 1989; Ringbom, 2007).", "startOffset": 119, "endOffset": 147}, {"referenceID": 34, "context": "These results confirm the merits of language transfer, which is also an important component of human language learning (Odlin, 1989; Ringbom, 2007).", "startOffset": 119, "endOffset": 147}, {"referenceID": 33, "context": "This could be achieved by a label-size independent classification layer as in zero-shot classification (Qiao et al., 2016; Nam et al., 2016).", "startOffset": 103, "endOffset": 140}, {"referenceID": 28, "context": "This could be achieved by a label-size independent classification layer as in zero-shot classification (Qiao et al., 2016; Nam et al., 2016).", "startOffset": 103, "endOffset": 140}, {"referenceID": 10, "context": "In their current form, our models cannot generalize to languages without any example, as attempted by Firat et al. (2016b) for neural MT.", "startOffset": 102, "endOffset": 123}], "year": 2017, "abstractText": "Hierarchical attention networks have recently achieved remarkable performance for document classification in a given language. However, when multilingual document collections are considered, training such models separately for each language entails linear parameter growth and lack of cross-language transfer. Learning a single multilingual model with fewer parameters is therefore a challenging but potentially beneficial objective. To this end, we propose multilingual hierarchical attention networks for learning document structures, with shared encoders and/or attention mechanisms across languages, using multi-task learning and an aligned semantic space as input. We evaluate the proposed models on multilingual document classification with disjoint label sets, on a large dataset which we provide, with 600k news documents in 8 languages, and 5k labels. The multilingual models outperform strong monolingual ones in lowresource as well as full-resource settings, and use fewer parameters, thus confirming their computational efficiency and the utility of cross-language transfer.", "creator": "LaTeX with hyperref package"}}}