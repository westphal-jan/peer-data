{"id": "1707.05436", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jul-2017", "title": "Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder", "abstract": "Most neural steinacher machine 123.25 translation (NMT) khovansky models millerton are based bakeshop on 3,509 the giovannetti sequential encoder - hlukhiv decoder framework, sreekumar which makes purbeck no 97.75 use of travelstead syntactic eggar information. rongkai In this paper, unhrc we housebuilding improve arithmetica this model by explicitly 317,000 incorporating source - side 3.65 syntactic remigijus trees. zhong More alans specifically, underfire we 14.89 propose (1) bidlake a simonides bidirectional tree encoder montcada which dragnets learns capodanno both kalejs sequential and tree macenka structured sardanapalus representations; (ambikapur 2) a symmetrical tree - seyne coverage hordern model recuperation that lets gaana the itachi attention depend wala on the 5-90 source - lagha side syntax. Experiments on 48.14 Chinese - 2,382 English khuzai translation demonstrate that 300m our kunming proposed models geddes outperform gebirge the sumner sequential attentional chandrashekar model shinkai as #a well juanma as a stronger juans baseline with 11-2 a runamuck bottom - up tree encoder and batorfi word coverage.", "histories": [["v1", "Tue, 18 Jul 2017 01:53:58 GMT  (127kb,D)", "http://arxiv.org/abs/1707.05436v1", "Accepted as a long paper by ACL 2017"]], "COMMENTS": "Accepted as a long paper by ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["huadong chen", "shujian huang", "david chiang", "jiajun chen"], "accepted": true, "id": "1707.05436"}, "pdf": {"name": "1707.05436.pdf", "metadata": {"source": "CRF", "title": "Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder", "authors": ["Huadong Chen", "Shujian Huang", "David Chiang", "Jiajun Chen"], "emails": ["chenhd@nlp.nju.edu.cn", "huangsj@nlp.nju.edu.cn", "chenjj@nlp.nju.edu.cn", "dchiang@nd.edu"], "sections": [{"heading": "1 Introduction", "text": "Recently, neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015) have obtained state-of-the-art performance on many language pairs. Their success depends on the representation they use to bridge the source and target language sentences. However, this representation, a sequence of fixed-dimensional vectors, differs considerably from most theories about mental representations of sentences, and from traditional natural language processing pipelines, in which semantics is built up compositionally using a recursive syntactic structure.\nPerhaps as evidence of this, current NMT models still suffer from syntactic errors such as attachment (Shi et al., 2016). We argue that instead of letting the NMT model rely solely on the implicit structure it learns during training (Cho et al.,\n\u2217 Corresponding author. 1Our code is publicly available at https://github.\ncom/howardchenhd/Syntax-awared-NMT/\n2014a), we can improve its performance by augmenting it with explicit structural information and using this information throughout the model. This has two benefits.\nFirst, the explicit syntactic information will help the encoder generate better source side representations. Li et al. (2015) show that for tasks in which long-distance semantic dependencies matter, representations learned from recursive models using syntactic structures may be more powerful than those from sequential recurrent models. In the NMT case, given syntactic information, it will be easier for the encoder to incorporate long distance dependencies into better representations, which is especially important for the translation of long sentences.\nSecond, it becomes possible for the decoder to\nar X\niv :1\n70 7.\n05 43\n6v 1\n[ cs\n.C L\n] 1\n8 Ju\nl 2 01\n7\nuse syntactic information to guide its reordering decisions better (especially for language pairs with significant reordering, like Chinese-English). Although the attention model (Bahdanau et al., 2015) and the coverage model (Tu et al., 2016; Mi et al., 2016) provide effective mechanisms to control the generation of translation, these mechanisms work at the word level and cannot capture phrasal cohesion between the two languages (Fox, 2002; Kim et al., 2017). With explicit syntactic structure, the decoder can generate the translation more in line with the source syntactic structure. For example, when translating the phrase zhu manila dashiguan in Figure 1, the tree structure indicates that zhu \u2018in\u2019 and manila form a syntactic unit, so that the model can avoid breaking this unit up to make an incorrect translation like \u201cin embassy of manila\u201d 2.\nIn this paper, we propose a novel encoderdecoder model that makes use of a precomputed source-side syntactic tree in both the encoder and decoder. In the encoder (\u00a73.3), we improve the tree encoder of Eriguchi et al. (2016) by introducing a bidirectional tree encoder. For each source tree node (including the source words), we generate a representation containing information both from below (as with the original bottom-up encoder) and from above (using a top-down encoder). Thus, the annotation of each node summarizes the surrounding sequential context, as well as the entire syntactic context.\nIn the decoder (\u00a73.4), we incorporate source syntactic tree structure into the attention model via an extension of the coverage model of Tu et al. (2016). With this tree-coverage model, we can better guide the generation phase of translation, for example, to learn a preference for phrasal cohesion (Fox, 2002). Moreover, with a tree encoder, the decoder may try to translate both a parent and a child node, even though they overlap; the treecoverage model enables the decoder to learn to avoid this problem.\nTo demonstrate the effectiveness of the proposed model, we carry out experiments on Chinese-English translation. Our experiments show that: (1) our bidirectional tree encoder based NMT system achieves significant improvements over the standard attention-based NMT system, and (2) incorporating source tree structure into the attention model yields a further improvement.\n2According to the source sentence, \u201cembassy\u201d belongs to \u201caustralia\u201d, not \u201cmanila\u201d.\nIn all, we demonstrate an improvement of +3.54 BLEU over a standard attentional NMT system, and +1.90 BLEU over a stronger NMT system with a Tree-LSTM encoder (Eriguchi et al., 2016) and a coverage model (Tu et al., 2016). To the best of our knowledge, this is the first work that uses source-side syntax in both the encoder and decoder of an NMT system."}, {"heading": "2 Neural Machine Translation", "text": "Most NMT systems follow the encoder-decoder framework with attention, first proposed by Bahdanau et al. (2015). Given a source sentence x = x1 \u00b7 \u00b7 \u00b7 xi \u00b7 \u00b7 \u00b7 xI and a target sentence y = y1 \u00b7 \u00b7 \u00b7 y j \u00b7 \u00b7 \u00b7 yJ , NMT aims to directly model the translation probability:\nP(y | x; \u03b8) = J\u220f 1 P(y j | y<j, x; \u03b8), (1)\nwhere \u03b8 is a set of parameters and y< j is the sequence of previously generated target words. Here, we briefly describe the underlying framework of the encoder-decoder NMT system."}, {"heading": "2.1 Encoder Model", "text": "Following Bahdanau et al. (2015), we use a bidirectional gated recurrent unit (GRU) (Cho et al., 2014b) to encode the source sentence, so that the annotation of each word contains a summary of both the preceding and following words. The bidirectional GRU consists of a forward and a backward GRU, as shown in Figure 2. The forward GRU reads the source sentence from left to right and calculates a sequence of forward hidden states ( \u2212\u2192 h1, . . . , \u2212\u2192 hI). The backward GRU scans the source sentence from right to left, resulting in a sequence of backward hidden states ( \u2190\u2212 h1, . . . , \u2190\u2212 hI). Thus\n\u2212\u2192 hi = GRU( \u2212\u2212\u2192 hi\u22121, si) \u2190\u2212 hi = GRU( \u2190\u2212\u2212 hi\u22121, si)\n(2)\nwhere si is the i-th source word\u2019s word embedding, and GRU is a gated recurrent unit; see the paper by Cho et al. (2014b) for a definition.\nThe annotation of each source word xi is obtained by concatenating the forward and backward hidden states:\n\u2190\u2192 hi = \u2212\u2192hi\u2190\u2212 hi  . The whole sequence of these annotations is used by the decoder."}, {"heading": "2.2 Decoder Model", "text": "The decoder is a forward GRU predicting the translation y word by word. The probability of generating the j-th word y j is:\nP(y j | y<j, x; \u03b8) = softmax(t j\u22121, d j, c j) (3)\nwhere t j\u22121 is the word embedding of the ( j \u2212 1)- th target word, d j is the decoder\u2019s hidden state of time j, and c j is the context vector at time j. The state d j is computed as\nd j = GRU(d j\u22121, t j\u22121, c j), (4)\nwhere GRU(\u00b7) is extended to more than two arguments by first concatenating all arguments except the first.\nThe attention mechanism computes the context vector ci as a weighted sum of the source annotations,\nc j = I\u2211\ni=1\n\u03b1 j,i \u2190\u2192 hi (5)\nwhere the attention weight \u03b1 j,i is\n\u03b1 j,i = exp (e j,i)\u2211I\ni\u2032=1 exp (e j,i\u2032) (6)\nand\ne j,i = vTa tanh (Wad j\u22121 + Ua \u2190\u2192 hi ) (7)\nwhere va, Wa and Ua are the weight matrices of the attention model, and e j,i is an attention model that scores how well d j\u22121 and \u2190\u2192 hi match.\nWith this strategy, the decoder can attend to the source annotations that are most relevant at a given time."}, {"heading": "3 Tree Structure Enhanced Neural Machine Translation", "text": "Although syntax has shown its effectiveness in non-neural statistical machine translation (SMT) systems (Yamada and Knight, 2001; Koehn et al., 2003; Liu et al., 2006; Chiang, 2007), most proposed NMT models (a notable exception being that of Eriguchi et al. (2016)) process a sentence only as a sequence of words, and do not explicitly exploit the inherent structure of natural language sentences. In this section, we present models which directly incorporate source syntactic trees into the encoder-decoder framework."}, {"heading": "3.1 Preliminaries", "text": "Like Eriguchi et al. (2016), we currently focus on source side syntactic trees, which can be computed prior to translation. Whereas Eriguchi et al. (2016) use HPSG trees, we use phrase-structure trees as in the Penn Chinese Treebank (Xue et al., 2005). Currently, we are only using the structure information from the tree without the syntactic labels. Thus our approach should be applicable to any syntactic grammar that provides such a tree structure (Figure 1(b)).\nMore formally, the encoder is given a source sentence x = x1 \u00b7 \u00b7 \u00b7 xI as well as a source tree whose leaves are labeled x1, . . . , xI . We assume that this tree is strictly binary branching. For convenience, each node is assigned an index. The leaf nodes get indices 1, . . . , I, which is the same as their word indices. For any node with index k, let p(k) denote the index of the node\u2019s parent (if it exists), and L(k) and R(k) denote the indices of the node\u2019s left and right children (if they exist)."}, {"heading": "3.2 Tree-GRU Encoder", "text": "We first describe tree encoders (Tai et al., 2015; Eriguchi et al., 2016), and then discuss our improvements.\nFollowing Eriguchi et al. (2016), we build a tree encoder on top of the sequential encoder (as shown in Figure 3(a)). If node k is a leaf node, its hidden state is the annotation produced by the sequential encoder:\nh\u2191k = \u2190\u2192 hk .\nThus, the encoder is able to capture both sequential context and syntactic context.\nIf node k is an interior node, its hidden state is the combination of its previously calculated left\nchild hidden state hL(k) and right child hidden state hR(k):\nh\u2191k = f (h \u2191 L(k), h \u2191 R(k)) (8)\nwhere f (\u00b7) is a nonlinear function, originally a Tree-LSTM (Tai et al., 2015; Eriguchi et al., 2016).\nThe first improvement we make to the above tree encoder is that, to be consistent with the sequential encoder model, we use Tree-GRU units instead of Tree-LSTM units. Similar to TreeLSTMs, the Tree-GRU has gating mechanisms to control the information flow inside the unit for every node without separate memory cells. Then, Eq. 8 is calculated by a Tree-GRU as follows:\nrL = \u03c3(U (rL) L h \u2191 L(k) + U (rL) R h \u2191 R(k) + b (rL)) rR = \u03c3(U (rR) L h \u2191 L(k) + U (rR) R h \u2191 R(k) + b (rR)) zL = \u03c3(U (zL) L h \u2191 L(k) + U (zL) R h \u2191 R(k) + b (zL)) zR = \u03c3(U (zR) L h \u2191 L(k) + U (zR) R h \u2191 R(k) + b (zR))\nz = \u03c3(U(z)L h \u2191 L(k) + U (z) R h \u2191 R(k) + b (z)) h\u0303\u2191k = tanh ( UL(rL h\u2191L(k)) + UR(rR h \u2191 R(k)) ) h\u2191k = zL h \u2191 L(k) + zR h \u2191 R(k) + z h\u0303 \u2191 k\nwhere rL, rR are the reset gates and zL, zR are the update gates for the left and right children, and z is the update gate for the internal hidden state h\u0303\u2191k . The U(\u00b7) and b(\u00b7) are the weight matrices and bias vectors."}, {"heading": "3.3 Bidirectional Tree Encoder", "text": "Although the bottom-up tree encoder can take advantage of syntactic structure, the learned representation of a node is based on its subtree only; it contains no information from higher up in the tree. In particular, the representation of leaf nodes is still the sequential one. Thus no syntactic information is fed into words. By analogy with the bidirectional sequential encoder, we propose a natural extension of the bottom-up tree encoder: the bidirectional tree encoder (Figure 3(b)).\nUnlike the bottom-up tree encoder or the rightto-left sequential encoder, the top-down encoder by itself would have no lexical information as input. To address this issue, we feed the hidden states of the bottom-up encoder to the top-down encoder. In this way, the information of the whole syntactic tree is handed to the root node and propagated to its offspring by the top-down encoder.\nIn the top-down encoder, each hidden state has only one predecessor. In fact, the top-down path from root of a tree to any node can be viewed as a sequential recurrent neural network. We can calculate the hidden states of each node top-down using a standard sequential GRU.\nFirst, the hidden state of the root node \u03c1 is simply computed as follows:\nh\u2193\u03c1 = tanh (Wh \u2191 \u03c1 + b) (9)\nwhere W and b are a weight matrix and bias vector. Then, other nodes are calculated by a GRU. For hidden state h\u2193k :\nh\u2193k = GRU(h \u2193 p(k), h \u2191 k) (10)\nwhere p(k) is the parent index of k. We replace the weight matrices Wr, Ur, Wz, Uz, W and U in the standard GRU with PrD, Q r D, P z D, Q z D, PD, and QD, respectively. The subscript D is either L or R depending on whether node k is a left or right child, respectively.\nFinally, the annotation of each node is obtained by concatenating its bottom-up hidden state and top-down hidden state:\nhlk = h\u2191kh\u2193k  .\nThis allows the tree structure information flow from the root to the leaves (words). Thus, all the annotations are based on the full context of word sequence and syntactic tree structure.\nKokkinos and Potamianos (2017) propose a similar bidirectional Tree-GRU for sentiment analysis, which differs from ours in several respects: in the bottom-up encoder, we use separate reset/update gates for left and right children, analogous to Tree-LSTMs (Tai et al., 2015); in the topdown encoder, we use separate weights for left and right children.\nTeng and Zhang (2016) also propose a bidirectional Tree-LSTM encoder for classification tasks. They use a more complex head-lexicalization scheme to feed the top-down encoder. We will compare their model with ours in the experiments."}, {"heading": "3.4 Tree-Coverage Model", "text": "We also extend the decoder to incorporate information about the source syntax into the attention model. We have observed two issues in translations produced using the tree encoder. First, a syntactic phrase in the source sentence is often incorrectly translated into discontinuous words in the output. Second, since the non-leaf node annotations contain more information than the leaf node annotations, the attention model prefers to attend to the non-leaf nodes, which may aggravate the over-translation problem (translating the same part of the sentence more than once).\nAs shown in Figure 4(a), almost all the non-leaf nodes are attended too many times during decoding. As a result, the Chinese phrase zhu manila is translated twice because the model attends to the node spanning zhu manila even though both words have already been translated; there is no mechanism to prevent this.\nInspired by the approaches of Cohn et al. (2016), Feng et al. (2016), Tu et al. (2016) and Mi et al. (2016), we propose to use prior knowledge to control the attention mechanism. In our case, the prior knowledge is the source syntactic information.\nIn particular, we build our model on top of the word coverage model proposed by Tu et al. (2016), which alleviate the problems of over-translation and under-translation (failing to translate part of a sentence). The word coverage model makes the attention at a given time step j dependent on the attention at previous time steps via coverage vectors:\nC j,i = GRU(C j\u22121,i, \u03b1 j,i, d j\u22121, hi). (11)\nThe coverage vectors are, in turn, used to update the attention at the next time step, by a small modification to the calculation of e j,i in Eq. (7):\ne j,i = vTa tanh (Wad j\u22121 + Uahi + VaC j\u22121,i). (12)\nThe word coverage model could be interpreted as a control mechanism for the attention model. Like the standard attention model, this coverage model sees the source-sentence annotations as a bag of vectors; it knows nothing about word order, still less about syntactic structure.\nFor our model, we extend the word coverage model to coverage on the tree structure by adding a coverage vector for each node in the tree. We further incorporate source tree structure information into the calculation of the coverage vector by requiring each node\u2019s coverage vector to depend on its children\u2019s coverage vectors and attentions at the previous time step:\nC j,i = GRU(C j\u22121,i, \u03b1 j,i, d j\u22121, hi,\nC j\u22121,L(i), \u03b1 j,L(i),\nC j\u22121,R(i), \u03b1 j,R(i)).\n(13)\nAlthough both child and parent nodes of a subtree are helpful for translation, they may supply redundant information. With our mechanism, when the child node is used to produce a translation, the coverage vector of its parent node will reflect this fact, so that the decoder may avoid using the redundant information in the parent node. Figure 4(b) shows a heatmap of the attention of our tree structure enhanced attention model. The attention of non-leaf nodes becomes more concentrated and the over-translation of zhu manila is corrected."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Data", "text": "We conduct experiments on the NIST ChineseEnglish translation task. The parallel training data consists of 1.6M sentence pairs extracted from LDC corpora,3 with 46.6M Chinese words and 52.5M English words, respectively. We use NIST MT02 as development data, and NIST MT03\u201306 as test data. These data are mostly in the same genre (newswire), avoiding the extra consideration of domain adaptation. Table 1 shows the statistics of the data sets. The Chinese side of the corpora is word segmented using ICTCLAS.4 We\n3LDC2002E18, LDC2003E14, the Hansards portion of LDC2004T08, and LDC2005T06.\n4http://ictclas.nlpir.org\nparse the Chinese sentences with the Berkeley Parser5 (Petrov and Klein, 2007) and binarize the resulting trees following Zhang and Clark (2009). The English side of the corpora is lowercased and tokenized.\nWe filter out any translation pairs whose source sentences fail to be parsed. For efficient training, we also filter out the sentence pairs whose source or target lengths are longer than 50. We use a shortlist of the 30,000 most frequent words in each language to train our models, covering approximately 98.2% and 99.5% of the Chinese and English tokens, respectively. All out-of-vocabulary words are mapped to a special symbol UNK."}, {"heading": "4.2 Model and Training Details", "text": "We compare our proposed models with several state-of-the-art NMT systems and techniques:\n\u2022 NMT: the standard attentional NMT model (Bahdanau et al., 2015).\n\u2022 Tree-LSTM: the attentional NMT model extended with the Tree-LSTM encoder (Eriguchi et al., 2016).\n\u2022 Coverage: the attentional NMT model extended with word coverage (Tu et al., 2016).\nWe used the dl4mt implementation of the attentional model,6 reimplementing the tree encoder and word coverage models. The word embedding dimension is 512. The hidden layer sizes of both forward and backward sequential encoder are 1024 (except where indicated). Since our TreeGRU encoders are built on top of the bidirectional sequential encoder, the size of the hidden layer (in each direction) is 2048. For the coverage model, we set the size of coverage vectors to 50.\n5https://github.com/slavpetrov/ berkeleyparser\n6https://github.com/nyu-dl/dl4mt-tutorial\nWe use Adadelta (Zeiler, 2012) for optimization using a mini-batch size of 32. All other settings are the same as in Bahdanau et al. (2015).\nWe use case insensitive 4-gram BLEU (Papineni et al., 2002) for evaluation, as calculated by multi-bleu.perl in the Moses toolkit.7"}, {"heading": "4.3 Tree Encoders", "text": "This set of experiments evaluates the effectiveness of our proposed tree encoders. Table 2, row 2 confirms the finding of Eriguchi et al. (2016) that a Tree-LSTM encoder helps, and row 3 shows that our Tree-GRU encoder gets a better result (+0.87 BLEU, v.s. row 2). To verify our assumption that model consistency is important for performance, we also conduct experiments to compare TreeLSTM and Tree-GRU on top of LSTM-based encoder-decoder settings. Tree-Lstm with LSTM based sequential model can obtain 1.02 BLEU improvement(Table 3, row 13\u2032), while Tree-LSTM with GRU based sequential model only gets 0.75 BLEU improvement. Although Tree-Lstm with LSTM based sequential model obtain a slightly better result(+0.22 BLEU, v.s. Table 2, row 3), it\n7http://www.statmt.org/moses\nhas more parameters(+1.6M) and takes 1.3 times longer for training.\nSince the annotation size of our bidirectional tree encoder is twice of the Tree-LSTM encoder, we halved the size of the hidden layers in the sequential encoder to 512 in each direction, to make fair comparison. These results are shown in Table 4. Row 4\u2032 shows that, even with the same annotation size, our bidirectional tree encoder works better than the original Tree-LSTM encoder (row 2). In fact, our halved-sized unidirectional TreeGRU encoder (row 3\u2032) also works better than the Tree-LSTM encoder (row 2) with half of its annotation size.\nWe also compared our bidirectional tree encoder with the head-lexicalization based bidirectional tree encoder proposed by Teng and Zhang (2016), which forms the input vector for each nonleaf node by a bottom-up head propagation mechanism (Table 4, row 14\u2032). Our bidirectional tree encoder gives a better result, suggesting that head word information may not be as helpful for machine translation as it is for syntactic parsing.\nWhen we set the hidden size back to 1024, we found that training the bidirectional tree encoder\nwas more difficult. Therefore, we adopted a twophase training strategy: first, we train the parameters of the bottom-up encoder based NMT system; then, with the initialization of bottom-up encoder and random initialization of the top-down part and decoder, we train the bidirectional tree encoder based NMT system. Table 2, row 4 shows the results of this two-phase training: the bidirectional model (row 4) is 0.79 BLEU better than our unidirectional Tree-GRU (row 3)."}, {"heading": "4.4 Tree-Coverage Model", "text": "Rows 5\u20138 in Table 2 show that the word coverage model of Tu et al. (2016) consistently helps when used with our proposed tree encoders, with the bidirectional tree encoder remaining the best. However, the improvements of the tree encoder models are smaller than that of the baseline system. This may be caused by the fact that the word coverage model neglects the relationship among the trees, e.g. the relationship between children and parent nodes. Our tree-coverage model consistently improves performance further (rows 9\u201311).\nOur best model combines our bidirectional tree encoder with our tree-coverage model (row 11), yielding a net improvement of +3.54 BLEU over the standard attentional model (row 1), and +1.90 BLEU over the stronger baseline that implements both the bottom-up tree encoder and coverage model from previous work (row 6).\nAs noted before, the original coverage model does not take word order into account. For comparison, we also implement an extension of the coverage model that lets each coverage vector also depend on those of its left and right neighbors at the previous time step. This model does not help; in fact, it reduces BLEU by about 0.2."}, {"heading": "4.5 Analysis By Sentence Length", "text": "Following Bahdanau et al. (2015), we bin the development and test sentences by length and show BLEU scores for each bin in Figure 5. The proposed bidirectional tree encoder outperforms the\nsequential NMT system and the Tree-GRU encoder across all lengths. The improvements become larger for sentences longer than 20 words, and the biggest improvement is for sentences longer than 50 words. This provides some evidence for the importance of syntactic information for long sentences."}, {"heading": "5 Related Work", "text": "Recently, many studies have focused on using explicit syntactic tree structure to help learn sentence representations for various sentence classification tasks. For example, Teng and Zhang (2016) and Kokkinos and Potamianos (2017) extend the bottom-up model to a bidirectional model for classification tasks, using Tree-LSTMs with head lexicalization and Tree-GRUs, respectively. We draw on some of these ideas and apply them to machine translation. We use the representation learnt from tree structures to enhance the original sequential model, and make use of these syntactic information during the generation phase.\nIn NMT systems, the attention model (Bahdanau et al., 2015) becomes a crucial part of the\ndecoder model. Cohn et al. (2016) and Feng et al. (2016) extend the attentional model to include structural biases from word based alignment models. Kim et al. (2017) incorporate richer structural distributions within deep networks to extend the attention model. Our contribution to the decoder model is to directly exploit structural information in the attention model combined with a coverage mechanism."}, {"heading": "6 Conclusion", "text": "We have investigated the potential of using explicit source-side syntactic trees in NMT by proposing a novel syntax-aware encoder-decoder model. Our experiments have demonstrated that a top-down encoder is a useful enhancement for the original bottom-up tree encoder (Eriguchi et al., 2016); and incorporating syntactic structure information into the decoder can better control the translation. Our analysis suggests that the benefit of source-side syntax is especially strong for long sentences.\nOur current work only uses the structure part of the syntactic tree, without the labels. For future work, it will be interesting to make use of node labels from the tree, or to use syntactic information on the target side, as well."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the anonymous reviewers for their valuable comments. This work is supported by the National Science Foundation of China (No. 61672277, 61300158 and 61472183). Part of Huadong Chen\u2019s contribution was made when visiting University of Notre Dame. His visit was supported by the joint PhD program of China Scholarship Council."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR 2015. http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "Compututational Linguistics 33(2):201\u2013228. https://doi.org/10.1162/coli.2007.33.2.201.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proc. Eighth Workshop on Syntax, Semantics and Struc-", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Incorporating structural alignment biases into an attentional neural translation model", "author": ["Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari."], "venue": "Proc. NAACL HLT . pages 876\u2013885.", "citeRegEx": "Cohn et al\\.,? 2016", "shortCiteRegEx": "Cohn et al\\.", "year": 2016}, {"title": "Tree-to-sequence attentional neural machine translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proc. ACL. pages 823\u2013 833. http://www.aclweb.org/anthology/P16-1078.", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "Improving attention modeling with implicit distortion and fertility for machine translation", "author": ["Shi Feng", "Shujie Liu", "Nan Yang", "Mu Li", "Ming Zhou", "Kenny Q. Zhu."], "venue": "Proc. COLING. pages 3082\u2013 3092. http://aclweb.org/anthology/C16-1290.", "citeRegEx": "Feng et al\\.,? 2016", "shortCiteRegEx": "Feng et al\\.", "year": 2016}, {"title": "Phrasal cohesion and statistical machine translation", "author": ["Heidi J. Fox."], "venue": "Proc. EMNLP. pages 304\u2013 3111. https://doi.org/10.3115/1118693.1118732.", "citeRegEx": "Fox.,? 2002", "shortCiteRegEx": "Fox.", "year": 2002}, {"title": "Structured attention networks", "author": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush."], "venue": "Proc. ICLR. http://arxiv.org/abs/1702.00887.", "citeRegEx": "Kim et al\\.,? 2017", "shortCiteRegEx": "Kim et al\\.", "year": 2017}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proc. NAACL HLT . pages 48\u201354. https://doi.org/10.3115/1073445.1073462.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Structural attention neural networks for improved sentiment analysis", "author": ["Filippos Kokkinos", "Alexandros Potamianos."], "venue": "Proc. EACL. pages 586\u2013591. http://www.aclweb.org/anthology/E17-2093.", "citeRegEx": "Kokkinos and Potamianos.,? 2017", "shortCiteRegEx": "Kokkinos and Potamianos.", "year": 2017}, {"title": "When are tree structures necessary for deep learning of representations? In Proc", "author": ["Jiwei Li", "Thang Luong", "Dan Jurafsky", "Eduard Hovy."], "venue": "EMNLP. pages 2304\u20132314. http://aclweb.org/anthology/D15-1278.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Treeto-string alignment template for statistical machine translation", "author": ["Yang Liu", "Qun Liu", "Shouxun Lin."], "venue": "Proc. ACL. pages 609\u2013616. https://doi.org/10.3115/1220175.1220252.", "citeRegEx": "Liu et al\\.,? 2006", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "Coverage embedding models for neural machine translation", "author": ["Haitao Mi", "Baskaran Sankaran", "Zhiguo Wang", "Abe Ittycheriah."], "venue": "Proc. EMNLP. pages 955\u2013960. https://aclweb.org/anthology/D16-1096.", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proc. ACL. pages 311\u2013318. https://doi.org/10.3115/1073083.1073135.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Improved inference for unlexicalized parsing", "author": ["Slav Petrov", "Dan Klein."], "venue": "Proc. NAACL HLT . pages 404\u2013411. http://www.aclweb.org/anthology/N/N07/N071051.", "citeRegEx": "Petrov and Klein.,? 2007", "shortCiteRegEx": "Petrov and Klein.", "year": 2007}, {"title": "Does string-based neural MT learn source syntax? In Proc", "author": ["Xing Shi", "Inkit Padhi", "Kevin Knight."], "venue": "EMNLP. pages 1526\u20131534. https://aclweb.org/anthology/D16-1159.", "citeRegEx": "Shi et al\\.,? 2016", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems 27, pages 3104\u2013 3112. http://papers.nips.cc/paper/5346-sequence-", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proc. ACL-IJCNLP. pages 1556\u20131566. http://www.aclweb.org/anthology/P15-1150.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Bidirectional tree-structured LSTM with head lexicalization", "author": ["Zhiyang Teng", "Yue Zhang."], "venue": "arXiv:1611.06788. http://arxiv.org/abs/1611.06788.", "citeRegEx": "Teng and Zhang.,? 2016", "shortCiteRegEx": "Teng and Zhang.", "year": 2016}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Proc. ACL. pages 76\u201385. http://www.aclweb.org/anthology/P16-1008.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "The penn chinese treebank: Phrase structure annotation of a large corpus", "author": ["Naiwen Xue", "Fei Xia", "Fu-dong Chiou", "Marta Palmer."], "venue": "Nat. Lang. Eng. 11(2):207\u2013238. https://doi.org/10.1017/S135132490400364X.", "citeRegEx": "Xue et al\\.,? 2005", "shortCiteRegEx": "Xue et al\\.", "year": 2005}, {"title": "A syntax-based statistical translation model", "author": ["Kenji Yamada", "Kevin Knight."], "venue": "Proc. ACL. pages 523\u2013530. https://doi.org/10.3115/1073012.1073079.", "citeRegEx": "Yamada and Knight.,? 2001", "shortCiteRegEx": "Yamada and Knight.", "year": 2001}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR abs/1212.5701. http://arxiv.org/abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Transition-based parsing of the Chinese Treebank using a global discriminative model", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proc. IWPT . pages 162\u2013171. http://www.aclweb.org/anthology/W09-3825.", "citeRegEx": "Zhang and Clark.,? 2009", "shortCiteRegEx": "Zhang and Clark.", "year": 2009}], "referenceMentions": [{"referenceID": 17, "context": "Recently, neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015) have obtained state-of-the-art performance on many language pairs.", "startOffset": 50, "endOffset": 97}, {"referenceID": 0, "context": "Recently, neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015) have obtained state-of-the-art performance on many language pairs.", "startOffset": 50, "endOffset": 97}, {"referenceID": 16, "context": "Perhaps as evidence of this, current NMT models still suffer from syntactic errors such as attachment (Shi et al., 2016).", "startOffset": 102, "endOffset": 120}, {"referenceID": 11, "context": "Li et al. (2015) show that for tasks in which long-distance semantic dependencies matter, representations learned from recursive models using syntactic structures may be more powerful than those from sequential recurrent models.", "startOffset": 0, "endOffset": 17}, {"referenceID": 0, "context": "Although the attention model (Bahdanau et al., 2015) and the coverage model (Tu et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 20, "context": ", 2015) and the coverage model (Tu et al., 2016; Mi et al., 2016) provide effective mechanisms to control the generation of translation, these mechanisms work at the word level and cannot capture phrasal cohesion between the two languages (Fox, 2002; Kim et al.", "startOffset": 31, "endOffset": 65}, {"referenceID": 13, "context": ", 2015) and the coverage model (Tu et al., 2016; Mi et al., 2016) provide effective mechanisms to control the generation of translation, these mechanisms work at the word level and cannot capture phrasal cohesion between the two languages (Fox, 2002; Kim et al.", "startOffset": 31, "endOffset": 65}, {"referenceID": 7, "context": ", 2016) provide effective mechanisms to control the generation of translation, these mechanisms work at the word level and cannot capture phrasal cohesion between the two languages (Fox, 2002; Kim et al., 2017).", "startOffset": 181, "endOffset": 210}, {"referenceID": 8, "context": ", 2016) provide effective mechanisms to control the generation of translation, these mechanisms work at the word level and cannot capture phrasal cohesion between the two languages (Fox, 2002; Kim et al., 2017).", "startOffset": 181, "endOffset": 210}, {"referenceID": 5, "context": "3), we improve the tree encoder of Eriguchi et al. (2016) by introducing a bidirectional tree encoder.", "startOffset": 35, "endOffset": 58}, {"referenceID": 7, "context": "With this tree-coverage model, we can better guide the generation phase of translation, for example, to learn a preference for phrasal cohesion (Fox, 2002).", "startOffset": 144, "endOffset": 155}, {"referenceID": 19, "context": "4), we incorporate source syntactic tree structure into the attention model via an extension of the coverage model of Tu et al. (2016). With this tree-coverage model, we can better guide the generation phase of translation, for example, to learn a preference for phrasal cohesion (Fox, 2002).", "startOffset": 118, "endOffset": 135}, {"referenceID": 5, "context": "90 BLEU over a stronger NMT system with a Tree-LSTM encoder (Eriguchi et al., 2016) and a coverage model (Tu et al.", "startOffset": 60, "endOffset": 83}, {"referenceID": 20, "context": ", 2016) and a coverage model (Tu et al., 2016).", "startOffset": 29, "endOffset": 46}, {"referenceID": 0, "context": "Most NMT systems follow the encoder-decoder framework with attention, first proposed by Bahdanau et al. (2015). Given a source sentence x = x1 \u00b7 \u00b7 \u00b7 xi \u00b7 \u00b7 \u00b7 xI and a target sentence y = y1 \u00b7 \u00b7 \u00b7 y j \u00b7 \u00b7 \u00b7 yJ , NMT aims to directly model the translation probability:", "startOffset": 88, "endOffset": 111}, {"referenceID": 0, "context": "Following Bahdanau et al. (2015), we use a bidirectional gated recurrent unit (GRU) (Cho et al.", "startOffset": 10, "endOffset": 33}, {"referenceID": 2, "context": "where si is the i-th source word\u2019s word embedding, and GRU is a gated recurrent unit; see the paper by Cho et al. (2014b) for a definition.", "startOffset": 103, "endOffset": 122}, {"referenceID": 22, "context": "Although syntax has shown its effectiveness in non-neural statistical machine translation (SMT) systems (Yamada and Knight, 2001; Koehn et al., 2003; Liu et al., 2006; Chiang, 2007), most proposed NMT models (a notable exception being that of Eriguchi et al.", "startOffset": 104, "endOffset": 181}, {"referenceID": 9, "context": "Although syntax has shown its effectiveness in non-neural statistical machine translation (SMT) systems (Yamada and Knight, 2001; Koehn et al., 2003; Liu et al., 2006; Chiang, 2007), most proposed NMT models (a notable exception being that of Eriguchi et al.", "startOffset": 104, "endOffset": 181}, {"referenceID": 12, "context": "Although syntax has shown its effectiveness in non-neural statistical machine translation (SMT) systems (Yamada and Knight, 2001; Koehn et al., 2003; Liu et al., 2006; Chiang, 2007), most proposed NMT models (a notable exception being that of Eriguchi et al.", "startOffset": 104, "endOffset": 181}, {"referenceID": 1, "context": "Although syntax has shown its effectiveness in non-neural statistical machine translation (SMT) systems (Yamada and Knight, 2001; Koehn et al., 2003; Liu et al., 2006; Chiang, 2007), most proposed NMT models (a notable exception being that of Eriguchi et al.", "startOffset": 104, "endOffset": 181}, {"referenceID": 1, "context": ", 2006; Chiang, 2007), most proposed NMT models (a notable exception being that of Eriguchi et al. (2016)) process a sentence only as a sequence of words, and do not explicitly exploit the inherent structure of natural language sentences.", "startOffset": 8, "endOffset": 106}, {"referenceID": 21, "context": "(2016) use HPSG trees, we use phrase-structure trees as in the Penn Chinese Treebank (Xue et al., 2005).", "startOffset": 85, "endOffset": 103}, {"referenceID": 5, "context": "Like Eriguchi et al. (2016), we currently focus on source side syntactic trees, which can be computed prior to translation.", "startOffset": 5, "endOffset": 28}, {"referenceID": 5, "context": "Like Eriguchi et al. (2016), we currently focus on source side syntactic trees, which can be computed prior to translation. Whereas Eriguchi et al. (2016) use HPSG trees, we use phrase-structure trees as in the Penn Chinese Treebank (Xue et al.", "startOffset": 5, "endOffset": 155}, {"referenceID": 18, "context": "We first describe tree encoders (Tai et al., 2015; Eriguchi et al., 2016), and then discuss our improvements.", "startOffset": 32, "endOffset": 73}, {"referenceID": 5, "context": "We first describe tree encoders (Tai et al., 2015; Eriguchi et al., 2016), and then discuss our improvements.", "startOffset": 32, "endOffset": 73}, {"referenceID": 5, "context": "Following Eriguchi et al. (2016), we build a tree encoder on top of the sequential encoder (as shown in Figure 3(a)).", "startOffset": 10, "endOffset": 33}, {"referenceID": 18, "context": "where f (\u00b7) is a nonlinear function, originally a Tree-LSTM (Tai et al., 2015; Eriguchi et al., 2016).", "startOffset": 60, "endOffset": 101}, {"referenceID": 5, "context": "where f (\u00b7) is a nonlinear function, originally a Tree-LSTM (Tai et al., 2015; Eriguchi et al., 2016).", "startOffset": 60, "endOffset": 101}, {"referenceID": 18, "context": "Kokkinos and Potamianos (2017) propose a similar bidirectional Tree-GRU for sentiment analysis, which differs from ours in several respects: in the bottom-up encoder, we use separate reset/update gates for left and right children, analogous to Tree-LSTMs (Tai et al., 2015); in the topdown encoder, we use separate weights for left and right children.", "startOffset": 255, "endOffset": 273}, {"referenceID": 4, "context": "Inspired by the approaches of Cohn et al. (2016), Feng et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 4, "context": "Inspired by the approaches of Cohn et al. (2016), Feng et al. (2016), Tu et al.", "startOffset": 30, "endOffset": 69}, {"referenceID": 4, "context": "Inspired by the approaches of Cohn et al. (2016), Feng et al. (2016), Tu et al. (2016) and Mi et al.", "startOffset": 30, "endOffset": 87}, {"referenceID": 4, "context": "Inspired by the approaches of Cohn et al. (2016), Feng et al. (2016), Tu et al. (2016) and Mi et al. (2016), we propose to use prior knowledge to control the attention mechanism.", "startOffset": 30, "endOffset": 108}, {"referenceID": 20, "context": "In particular, we build our model on top of the word coverage model proposed by Tu et al. (2016), which alleviate the problems of over-translation and under-translation (failing to translate part of a sentence).", "startOffset": 80, "endOffset": 97}, {"referenceID": 15, "context": "parse the Chinese sentences with the Berkeley Parser5 (Petrov and Klein, 2007) and binarize the resulting trees following Zhang and Clark (2009).", "startOffset": 54, "endOffset": 78}, {"referenceID": 15, "context": "parse the Chinese sentences with the Berkeley Parser5 (Petrov and Klein, 2007) and binarize the resulting trees following Zhang and Clark (2009). The English side of the corpora is lowercased and tokenized.", "startOffset": 55, "endOffset": 145}, {"referenceID": 0, "context": "\u2022 NMT: the standard attentional NMT model (Bahdanau et al., 2015).", "startOffset": 42, "endOffset": 65}, {"referenceID": 5, "context": "\u2022 Tree-LSTM: the attentional NMT model extended with the Tree-LSTM encoder (Eriguchi et al., 2016).", "startOffset": 75, "endOffset": 98}, {"referenceID": 20, "context": "\u2022 Coverage: the attentional NMT model extended with word coverage (Tu et al., 2016).", "startOffset": 66, "endOffset": 83}, {"referenceID": 20, "context": "\u201cno\u201d, \u201cword\u201d and \u201ctree\u201d in column \u201cCoverage\u201d represents the decoder part for using no coverage (standard attention), word coverage (Tu et al., 2016) and our proposed tree-coverage model, respectively.", "startOffset": 131, "endOffset": 148}, {"referenceID": 23, "context": "We use Adadelta (Zeiler, 2012) for optimization using a mini-batch size of 32.", "startOffset": 16, "endOffset": 30}, {"referenceID": 0, "context": "All other settings are the same as in Bahdanau et al. (2015).", "startOffset": 38, "endOffset": 61}, {"referenceID": 14, "context": "We use case insensitive 4-gram BLEU (Papineni et al., 2002) for evaluation, as calculated by multi-bleu.", "startOffset": 36, "endOffset": 59}, {"referenceID": 5, "context": "Table 2, row 2 confirms the finding of Eriguchi et al. (2016) that a Tree-LSTM encoder helps, and row 3 shows that our Tree-GRU encoder gets a better result (+0.", "startOffset": 39, "endOffset": 62}, {"referenceID": 19, "context": "We also compared our bidirectional tree encoder with the head-lexicalization based bidirectional tree encoder proposed by Teng and Zhang (2016), which forms the input vector for each nonleaf node by a bottom-up head propagation mechanism (Table 4, row 14\u2032).", "startOffset": 122, "endOffset": 144}, {"referenceID": 19, "context": "The bidirectional tree encoder using head-lexicalization (Bidirectional-head), proposed by (Teng and Zhang, 2016), does not work as well as our simpler bidirectional tree encoder (Bidirectional).", "startOffset": 91, "endOffset": 113}, {"referenceID": 20, "context": "Rows 5\u20138 in Table 2 show that the word coverage model of Tu et al. (2016) consistently helps when used with our proposed tree encoders, with the bidirectional tree encoder remaining the best.", "startOffset": 57, "endOffset": 74}, {"referenceID": 0, "context": "Following Bahdanau et al. (2015), we bin the development and test sentences by length and show BLEU scores for each bin in Figure 5.", "startOffset": 10, "endOffset": 33}, {"referenceID": 18, "context": "For example, Teng and Zhang (2016) and Kokkinos and Potamianos (2017) extend the bottom-up model to a bidirectional model for classification tasks, using Tree-LSTMs with head lexicalization and Tree-GRUs, respectively.", "startOffset": 13, "endOffset": 35}, {"referenceID": 10, "context": "For example, Teng and Zhang (2016) and Kokkinos and Potamianos (2017) extend the bottom-up model to a bidirectional model for classification tasks, using Tree-LSTMs with head lexicalization and Tree-GRUs, respectively.", "startOffset": 39, "endOffset": 70}, {"referenceID": 0, "context": "In NMT systems, the attention model (Bahdanau et al., 2015) becomes a crucial part of the", "startOffset": 36, "endOffset": 59}, {"referenceID": 4, "context": "Cohn et al. (2016) and Feng et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "Cohn et al. (2016) and Feng et al. (2016) extend the attentional model to include structural biases from word based alignment models.", "startOffset": 0, "endOffset": 42}, {"referenceID": 4, "context": "Cohn et al. (2016) and Feng et al. (2016) extend the attentional model to include structural biases from word based alignment models. Kim et al. (2017) incorporate richer structural distributions within deep networks to extend the attention model.", "startOffset": 0, "endOffset": 152}, {"referenceID": 5, "context": "Our experiments have demonstrated that a top-down encoder is a useful enhancement for the original bottom-up tree encoder (Eriguchi et al., 2016); and incorporating syntactic structure information into the decoder can better control the translation.", "startOffset": 122, "endOffset": 145}], "year": 2017, "abstractText": "Most neural machine translation (NMT) models are based on the sequential encoder-decoder framework, which makes no use of syntactic information. In this paper, we improve this model by explicitly incorporating source-side syntactic trees. More specifically, we propose (1) a bidirectional tree encoder which learns both sequential and tree structured representations; (2) a tree-coverage model that lets the attention depend on the source-side syntax. Experiments on Chinese-English translation demonstrate that our proposed models outperform the sequential attentional model as well as a stronger baseline with a bottom-up tree encoder and word coverage.1", "creator": "LaTeX with hyperref package"}}}