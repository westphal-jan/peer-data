{"id": "1707.07265", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jul-2017", "title": "Composing Distributed Representations of Relational Patterns", "abstract": "mosuo Learning corralled distributed representations motile for relation instances blackshirt is bolling a forwood central technique rabble in touchwiz downstream beamon NLP harassing applications. In kamikawa order to 1,319 address nearfest semantic courtemanche modeling semi-nomadic of 283.6 relational patterns, albats this kitchee paper acyltransferases constructs a new dataset specialness that madaris provides cebuanos multiple 116-member similarity ratings c&o for 7-to-2 every motet pair pkware of cretins relational patterns lrrp on the nitrites existing glen dataset. In addition, we 15.95 conduct llf a comparative study 51.4 of predominating different encoders including sredin additive composition, ramil RNN, LSTM, corretja and GRU for frasor composing distributed representations of relational patterns. We akhmed also present Gated Additive guarnaschelli Composition, which is yelagin an enhancement of meiri additive freneau composition with deutsche the cashman gating demented mechanism. Experiments milkins show that the overhang new quilombo dataset cintra does not ralston only luteal enable detailed 20-over analyses mekka of the inge different rafto encoders, gareth but also provides a gauge froy to predict successes schwentke of beechen distributed giriraj representations of fcoe relational patterns in atria the c\u00e1diz relation classification whitelocke task.", "histories": [["v1", "Sun, 23 Jul 2017 08:12:59 GMT  (233kb,D)", "http://arxiv.org/abs/1707.07265v1", "Published as a conference paper at ACL 2016"]], "COMMENTS": "Published as a conference paper at ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sho takase", "naoaki okazaki", "kentaro inui"], "accepted": true, "id": "1707.07265"}, "pdf": {"name": "1707.07265.pdf", "metadata": {"source": "CRF", "title": "Composing Distributed Representations of Relational Patterns", "authors": ["Sho Takase", "Naoaki Okazaki", "Kentaro Inui"], "emails": ["inui}@ecei.tohoku.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "Knowledge about entities and their relations (relation instances) are crucial for a wide spectrum of NLP applications, e.g., information retrieval, question answering, and recognizing textual entailment. Learning distributed representations for relation instances is a central technique in downstream applications as a number of recent studies demonstrated the usefulness of distributed representations for words (Mikolov et al., 2013; Pennington et al., 2014) and sentences (Sutskever et al., 2014; Cho et al., 2014; Kiros et al., 2015).\nIn particular, semantic modeling of relations and their textual realizations (relational patterns hereafter) is extremely important because a rela-\ntion (e.g., causality) can be mentioned by various expressions (e.g., \u201cX cause Y\u201d, \u201cX lead to Y\u201d, \u201cY is associated with X\u201d). To make matters worse, relational patterns are highly productive: we can produce a emphasized causality pattern \u201cX increase the severe risk of Y\u201d from \u201cX increase the risk of Y\u201d by inserting severe to the pattern.\nTo model the meanings of relational patterns, the previous studies built a co-occurrence matrix between relational patterns (e.g., \u201cX increase the risk of Y\u201d) and entity pairs (e.g., \u201cX: smoking, Y: cancer\u201d) (Lin and Pantel, 2001; Nakashole et al., 2012). Based on the distributional hypothesis (Harris, 1954), we can compute a semantic vector of a relational pattern from the co-occurrence matrix, and measure the similarity of two relational patterns as the cosine similarity of the vectors. Nowadays, several studies adopt distributed representations computed by neural networks for semantic modeling of relational patterns (Yih et al., 2014; Takase et al., 2016).\nNotwithstanding, the previous studies paid little attention to explicitly evaluate semantic modeling of relational patterns. In this paper, we construct a new dataset that contains a pair of relational patterns with five similarity ratings judged by human annotators. The new dataset shows a\nar X\niv :1\n70 7.\n07 26\n5v 1\n[ cs\n.C L\n] 2\n3 Ju\nl 2 01\n7\nhigh inter-annotator agreement, following the annotation guideline of Mitchell and Lapata (2010). The dataset is publicly available on the Web site1.\nIn addition, we conduct a comparative study of different encoders for composing distributed representations of relational patterns. During the comparative study, we present Gated Additive Composition, which is an enhancement of additive composition with the gating mechanism. We utilize the Skip-gram objective for training the parameters of the encoders on a large unlabeled corpus. Experiments show that the new dataset does not only enable detailed analyses of the different encoders, but also provides a gauge to predict successes of distributed representations of relational patterns in another task (relation classification). Figure 1 illustrates the overview of this study."}, {"heading": "2 Data Construction", "text": ""}, {"heading": "2.1 Target relation instances", "text": "We build a new dataset upon the work of Zeichner et al. (2012), which consists of relational patterns with semantic inference labels annotated. The dataset includes 5,555 pairs2 extracted by Reverb (Fader et al., 2011), 2,447 pairs with inference relation and 3,108 pairs (the rest) without one.\nInitially, we considered using this high-quality dataset as it is for semantic modeling of relational patterns. However, we found that inference relations exhibit quite different properties from those of semantic similarity. Take a relational pattern pair \u201cX be the part of Y\u201d and \u201cX be an essential part of Y\u201d filled with \u201cX = the small intestine, Y = the digestive system\u201d as an instance. The pattern \u201cX be the part of Y\u201d does not entail \u201cX be an essential part of Y\u201d because the meaning of the former does not include \u2018essential\u2019. Nevertheless, both statements are similar, representing the same relation (PART-OF). Another uncomfortable pair is \u201cX fall down Y\u201d and \u201cX go up Y\u201d filled with \u201cX = the dude, Y = the stairs\u201d. The dataset indicates that the former entails the latter probably because falling down from the stairs requires going up there, but they present the opposite meaning. For this reason, we decided to re-annotate semantic similarity\n1http://github.com/takase/relPatSim 2More precisely, the dataset includes 1,012 meaningless pairs in addition to 5,555 pairs. A pair of relational patterns was annotated as meaningless if the annotators were unable to understand the meaning of the patterns easily. We ignore the meaningless pairs in this study.\njudgments on every pair of relational patterns on the dataset."}, {"heading": "2.2 Annotation guideline", "text": "We use instance-based judgment in a similar manner to that of Zeichner et al. (2012) to secure a high inter-annotator agreement. In instancebased judgment, an annotator judges a pair of relational patterns whose variable slots are filled with the same entity pair. In other words, he or she does not make a judgment for a pair of relational patterns with variables, \u201cX prevent Y\u201d and \u201cX reduce the risk of Y\u201d, but two instantiated statements \u201cCephalexin prevent the bacteria\u201d and \u201cCephalexin reduce the risk of the bacteria\u201d (\u201cX = Cephalexin, Y = the bacteria\u201d). We use the entity pairs provided in Zeichner et al. (2012).\nWe asked annotators to make a judgment for a pair of relation instances by choosing a rating from 1 (dissimilar) to 7 (very similar). We provided the following instructions for judgment, which is compatible with Mitchell and Lapata (2010): (1) rate 6 or 7 if the meanings of two statements are the same or mostly the same (e.g., \u201cPalmer team with Jack Nicklaus\u201d and \u201cPalmer join with Jack Nicklaus\u201d); (2) rate 1 or 2 if two statements are dissimilar or unrelated (e.g., \u201cthe kids grow up with him\u201d and \u201cthe kids forget about him\u201d); (3) rate 3, 4, or 5 if two statements have some relationships (e.g., \u201cMany of you know about the site\u201d and \u201cMany of you get more information about the site\u201d, where the two statements differ but also reasonably resemble to some extent)."}, {"heading": "2.3 Annotation procedure", "text": "We use a crowdsourcing service CrowdFlower3 to collect similarity judgments from the crowds. CrowdFlower has the mechanism to assess the reliability of annotators using Gold Standard Data (Gold, hereafter), which consists of pairs of relational patterns with similarity scores assigned. Gold examples are regularly inserted throughout the judgment job to enable measurement of the performance of each worker4. Two authors of this paper annotated 100 pairs extracted randomly from 5,555 pairs, and prepared 80 Gold examples showing high agreement. Ratings of the Gold examples were used merely for quality assessment of the workers. In other words, we discarded the\n3http://www.crowdflower.com/ 4We allow \u00b11 differences in rating when we measure the\nperformance of the workers.\nsimilarity ratings of the Gold examples, and used those judged by the workers.\nTo build a high quality dataset, we use judgments from workers whose confidence values (reliability scores) computed by CrowdFlower are greater than 75%. Additionally, we force every pair to have at least five judgments from the workers. Consequently, 60 workers participated in this job. In the final version of this dataset, each pair has five similarity ratings judged by the five most reliable workers who were involved in the pair.\nFigure 2 presents the number of judgments for each similarity rating. Workers seldom rated 7 for a pair of relational patterns, probably because most pairs have at least one difference in content words. The mean of the standard deviations of similarity ratings of all pairs is 1.16. Moreover, we computed Spearman\u2019s \u03c1 between similarity judgments from each worker and the mean of five judgments in the dataset. The mean of Spearman\u2019s \u03c1 of workers involved in the dataset is 0.728. These statistics show a high inter-annotator agreement of the dataset."}, {"heading": "3 Encoder for Relational Patterns", "text": "The new dataset built in the previous section raises two new questions \u2014 What is the reasonable method (encoder) for computing the distributed representations of relational patterns? Is this dataset useful to predict successes of distributed representations of relational patterns in real applications? In order to answer these questions, this section explores various methods for learning distributed representations of relational patterns."}, {"heading": "3.1 Baseline methods without supervision", "text": "A na\u0131\u0308ve approach would be to regard a relational pattern as a single unit (word) and to train word/pattern embeddings as usual. In fact, Mikolov et al. (2013) implemented this approach\nas a preprocessing step, mining phrasal expressions with strong collocations from a training corpus. However, this approach might be affected by data sparseness, which lowers the quality of distributed representations.\nAnother simple but effective approach is additive composition (Mitchell and Lapata, 2010), where the distributed representation of a relational pattern is computed by the mean of embeddings of constituent words. Presuming that a relational pattern consists of a sequence of T words w1, ..., wT , then we let xt \u2208 Rd the embedding of the word wt. This approach computes 1T \u2211T t=1 xt as the embedding of the relational pattern. Muraoka et al. (2014) reported that the additive composition is a strong baseline among various methods."}, {"heading": "3.2 Recurrent Neural Network", "text": "Recently, a number of studies model semantic compositions of phrases and sentences by using (a variant of) Recurrent Neural Network (RNN) (Sutskever et al., 2014; Tang et al., 2015). For a given embedding xt at position t, the vanilla RNN (Elman, 1990) computes the hidden state ht \u2208 Rd by the following recursive equation5,\nht = g(Wxxt +Whht\u22121). (1)\nHere, Wx andWh are d\u00d7dmatrices (parameters), g(.) is the elementwise activation function (tanh). We set h0 = 0 at t = 1. In essence, RNN computes the hidden state ht based on the one at the previous position (ht\u22121) and the word embedding xt. Applying Equation 1 from t = 1 to T , we use hT as the distributed representation of the relational pattern."}, {"heading": "3.3 RNN variants", "text": "We also employ Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) as an encoder for relational patterns. LSTM has been applied successfully to various NLP tasks including word segmentation (Chen et al., 2015), dependency parsing (Dyer et al., 2015), machine translation (Sutskever et al., 2014), and sentiment analysis (Tai et al., 2015). GRU is also successful in machine translation (Cho et al., 2014) and various\n5We do not use a bias term in this study. We set the number of dimensions of hidden states identical to that of word embeddings (d) so that we can adapt the objective function of the Skip-gram model for training (Section 3.5).\npassive smoking increases the risk of lung cancer\nxs xs+1 xs+2 xs+L-1\nxs+L xs+L+1xs-2 xs-1\n(3)\n(4) (5)\nhs hs+1 hs+2\nhs+L-1 (3)\nws ws+1 ws+2 ws+L-1 (3)\nws+L ws+L+1 (4) (5) ws-2 ws-1\nf s f s+1 f s+2\ni s i s+1 i s+2 i s+3 ~ ~ ~ ~\nParameter update by Skip-gram model\nParameter update by Skip-gram model Pattern vector\nT = L = 4 \u03b4 = 2\u03b4 = 2\nContext window Context windowRelation pattern\n(word vectors) (context vectors) (context vectors)\n(hidden vectors)\nGated Additive Composition (GAC)\nFigure 3: Overview of GAC trained with Skipgram model. GAC computes the distributed representation of a relational pattern using the input gate and forget gate, and learns parameters by predicting surrounding words (Skip-gram model).\ntasks including sentence similarity, paraphrase detection, and sentiment analysis (Kiros et al., 2015).\nLSTM and GRU are similar in that the both architectures have gates (input, forget, and output for LSTM; reset and update for GRU) to remedy the gradient vanishing or explosion problem in training RNNs. Although some researchers reported that GRU is superior to LSTM (Chung et al., 2014), we have no consensus about the superiority. Besides, we are not sure whether LSTM or GRU is really necessary for relational patterns, which ususlly consist of a few words. Thus, we compare RNN, LSTM, and GRU empirically with the same training data and the same training procedure. Similarly to RNN, we use the hidden state hT of LSTM6 or GRU as the distributed representation of a relation pattern."}, {"heading": "3.4 Gated Additive Composition (GAC)", "text": "In addition to the gradient problem, LSTM or GRU may be suitable for relational patterns, having the mechanism of adaptive control of gates for input words and hidden states. Consider the relational pattern \u201cX have access to Y\u201d, whose meaning is mostly identical to that of \u201cX access Y\u201d. Because \u2018have\u2019 in the pattern is a light verb, it may be harmful to incorporate the semantic vector of \u2018have\u2019 into the distributed representation of the pattern. The same may be true for the functional word \u2018to\u2019 in the pattern. However, the additive composition nor RNN does not have a mechanism to ignore the semantic vectors of these words. It is interesting to explore a method somewhere between additive composition and LSTM/GRU: additive composition with the gating mechanism.\nFor this reason, we present an another variant of RNN in this study. Inspired by the input and\n6We omitted peephole connections and bias terms.\nforget gates in LSTM, we compute the input gate it \u2208 Rd and forget gate ft \u2208 Rd at position t. We use them to control the amount to propagate to the hidden state ht from the current word xt and the previous state ht\u22121.\nit = \u03c3(Wixxt +Wihht\u22121) (2)\nft = \u03c3(Wfxxt +Wfhht\u22121) (3)\nht = g(ft ht\u22121 + it xt) (4)\nHere, Wix, Wih, Wfx, Wfh are d \u00d7 d matrices. Equation 4 is interpreted as a weighted additive composition between the vector of the current word xt and the vector of the previous hidden state ht\u22121. The elementwise weights are controlled by the input gate it and forget gate ft; we expect that input gates are closed (close to zero) and forget gates are opened (close to one) when the current word is a control verb or function word. We name this architecture gated additive composition (GAC)."}, {"heading": "3.5 Parameter estimation: Skip-gram model", "text": "To train the parameters of the encoders (RNN, LSTM, GRU, and GAC) on an unlabeled text corpus, we adapt the Skip-gram model (Mikolov et al., 2013). Formally, we designate an occurrence of a relational pattern p as a subsequence of L words ws, ..., ws+L\u22121 in a corpus. We define \u03b4 words appearing before and after pattern p as the context words, and let Cp = (s \u2212 \u03b4, ..., s \u2212 1, s + L, ..., s + L + \u03b4) denote the indices of the context words. We define the log-likelihood of the relational pattern lp, following the objective function of Skip-gram with negative sampling (SGNS) (Levy and Goldberg, 2014).\nlp = \u2211 \u03c4\u2208Cp ( log \u03c3(h>p x\u0303\u03c4 ) + K\u2211 k=1 log \u03c3(\u2212h>p x\u0303\u03c4\u0306 ) ) (5)\nIn this formula: K denotes the number of negative samples; hp \u2208 Rd is the vector for the relational pattern p computed by each encoder such as RNN; x\u0303\u03c4 \u2208 Rd is the context vector for the word w\u03c4\n7; x\u03c4\u0306 \u2032 \u2208 Rd is the context vector for the word 7The Skip-gram model has two kinds of vectors xt and x\u0303t assigned for a word wt. Equation 2 of the original paper (Mikolov et al., 2013) denotes xt (word vector) as v (input vector) and x\u0303t (context vector) as v\u2032 (output vector). The word2vec implementation does not write context (output) vectors but only word (input) vectors to a model file. Therefore, we modified the source code to save context vectors, and use them in Equation 5. This modification ensures the consistency of the entire model.\nthat were sampled from the unigram distribution8 at every iteration of \u2211\nk. At every occurrence of a relational pattern in the corpus, we use Stochastic Gradient Descent (SGD) and backpropagation through time (BPTT) for training the parameters (matrices) in encoders. More specifically, we initialize the word vectors xt and context vectors x\u0303t with pre-trained values, and compute gradients for Equation 5 to update the parameters in encoders. In this way, each encoder is trained to compose a vector of a relational pattern so that it can predict the surrounding context words. An advantage of this parameter estimation is that the distributed representations of words and relational patterns stay in the same vector space. Figure 3 visualizes the training process for GAC."}, {"heading": "4 Experiments", "text": "In Section 4.1, we investigate the performance of the distributed representations computed by different encoders on the pattern similarity task. Section 4.2 examines the contribution of the distributed representations on SemEval 2010 Task 8, and discusses the usefulness of the new dataset to predict successes of the relation classification task."}, {"heading": "4.1 Relational pattern similarity", "text": "For every pair in the dataset built in Section 2, we compose the vectors of the two relational patterns using an encoder described in Section 3, and compute the cosine similarity of the two vectors. Repeating this process for all pairs in the dataset, we measure Spearman\u2019s \u03c1 between the similarity values computed by the encoder and similarity ratings assigned by humans."}, {"heading": "4.1.1 Training procedure", "text": "We used ukWaC9 as the training corpus for the encoders. This corpus includes the text of 2 billion words from Web pages crawled in the .uk domain. Part-of-speech tags and lemmas are annotated by TreeTagger10. We used lowercased lemmas throughout the experiments. We apply word2vec to this corpus to pre-train word vectors xt and context vectors x\u0303t. All encoders use word vectors xt to compose vectors of relational patterns; and the Skip-gram model uses context\n8We use the probability distribution of words raised to the 3/4 power (Mikolov et al., 2013).\n9http://wacky.sslmit.unibo.it 10http://www.cis.uni-muenchen.de/\n\u02dcschmid/tools/TreeTagger/\nvectors x\u0303t to compute the objective function and gradients. We fix the vectors xt and x\u0303t with pretrained values during training.\nWe used Reverb (Fader et al., 2011) to the ukWaC corpus to extract relational pattern candidates. To remove unuseful relational patterns, we applied filtering rules that are compatible with those used in the publicly available extraction result11. Additionally, we discarded relational patterns appearing in the evaluation dataset throughout the experiments to assess the performance under which an encoder composes vectors of unseen relational patterns. This preprocessing yielded 127, 677 relational patterns.\nAll encoders were implemented on Chainer12, a flexible framework of neural networks. The hyperparameters of the Skip-gram model are identical to those in Mikolov et al. (2013): the width of context window \u03b4 = 5, the number of negative samples K = 5, the subsampling of 10\u22125. For each encoder that requires training, we tried 0.025, 0.0025, and 0.00025 as an initial learning rate, and selected the best value for the encoder. In contrast to the presentation of Section 3, we compose a pattern vector in backward order (from the last to the first) because preliminary experiments showed a slight improvement with this treatment."}, {"heading": "4.1.2 Results and discussions", "text": "Figure 4 shows Spearman\u2019s rank correlations of different encoders when the number of dimensions of vectors is 100\u2013500. The figure shows that GAC achieves the best performance on all dimensions.\nFigure 4 includes the performance of the na\u0131\u0308ve approach, \u201cNoComp\u201d, which regards a relational pattern as a single unit (word). In this approach,\n11http://reverb.cs.washington.edu/ 12http://chainer.org/\nwe allocated a vector hp for each relational pattern p in Equation 5 instead of the vector composition, and trained the vectors of relational patterns using the Skip-gram model. The performance was poor for two reasons: we were unable to compute similarity values for 1,744 pairs because relational patterns in these pairs do not appear in ukWaC; and relational patterns could not obtain sufficient statistics because of data sparseness.\nTable 1 reports Spearman\u2019s rank correlations computed for each pattern length. Here, the length of a relational-pattern pair is defined by the maximum of the lengths of two patterns in the pair. In length of 1, all methods achieve the same correlation score because they use the same word vector xt. The table shows that additive composition (Add) performs well for shorter relational patterns (lengths of 2 and 3) but poorly for longer ones (lengths of 4 and 5+). GAC also exhibits the similar tendency to Add, but it outperforms Add for shorter patterns (lengths of 2 and 3) probably because of the adaptive control of input and forget gates. In contrast, RNN and its variants (RNN, GRU, and LSTM) enjoy the advantage on longer patterns (lengths of 4 and 5+).\nTo examine the roles of input and forget gates of GAC, we visualize the moments when input/forget gates are wide open or closed. More precisely, we extract the input word and scanned words when |it|2 or |ft|2 is small (close to zero) or large (close to one) on the relational-pattern dataset. We restate that we compose a pattern vector in backward order (from the last to the first): GAC scans \u2018of\u2019, \u2018author\u2019, and \u2018be\u2019 in this order for composing the vector of the relational pattern \u2018be author of\u2019.\nTable 2 displays the top three examples identified using the procedure. The table shows two groups of tendencies. Input gates open and forget gates close when scanned words are only a preposition and the current word is a content word. In these situations, GAC tries to read the semantic\nvector of the content word and to ignore the semantic vector of the preposition. In contrast, input gates close and forget gates open when the current word is \u2018be\u2019 or \u2018a\u2019 and scanned words form a noun phrase (e.g., \u201ccharter member of\u201d), a complement (e.g., \u201celigible to participate in\u201d), or a passive voice (e.g., \u201crequire(d) to submit\u201d). This behavior is also reasonable because GAC emphasizes informative words more than functional words."}, {"heading": "4.2 Relation classification", "text": ""}, {"heading": "4.2.1 Experimental settings", "text": "To examine the usefulness of the dataset and distributed representations for a different application, we address the task of relation classification on the SemEval 2010 Task 8 dataset (Hendrickx et al., 2010). In other words, we explore whether high-quality distributed representations of relational patterns are effective to identify a relation type of an entity pair.\nThe dataset consists of 10, 717 relation instances (8, 000 training and 2, 717 test instances) with their relation types annotated. The dataset defines 9 directed relations (e.g.,CAUSE-EFFECT)\nand 1 undirected relation OTHER. Given a pair of entity mentions, the task is to identify a relation type in 19 candidate labels (2 \u00d7 9 directed + 1 undirected relations). For example, given the pair of entity mentions e1 = \u2018burst\u2019 and e2 = \u2018pressure\u2019 in the sentence \u201cThe burst has been caused by water hammer pressure\u201d, a system is expected to predict CAUSE-EFFECT(e2, e1).\nWe used Support Vector Machines (SVM) with a Radial Basis Function (RBF) kernel implemented in libsvm13. Basic features are: partof-speech tags (predicted by TreeTagger), surface forms, lemmas of words appearing between an entity pair, and lemmas of the words in the entity pair. Additionally, we incorporate distributed representations of a relational pattern, entities, and a word before and after the entity pair (number of dimensions d = 500). In this task, we regard words appearing between an entity pair as a relational pattern. We compare the vector represen-\n13https://www.csie.ntu.edu.tw/\u02dccjlin/ libsvm/\ntations of relational patterns computed by the five encoders presented in Section 4.1: additive composition, RNN, GRU, LSTM, and GAC. Hyperparameters related to SVM were tuned by 5-fold cross validation on the training data."}, {"heading": "4.2.2 Results and discussions", "text": "Table 3 presents the macro-averaged F1 scores on the SemEval 2010 Task 8 dataset. The first group of the table provides basic features and enhancements with the distributed representations. We can observe a significant improvement even from the distributed representation of NoComp (77.3 to 79.9). Moreover, the distributed representation that exhibited the high performance on the pattern similarity task was also successful on this task; GAC, which yielded the highest performance on the pattern similarity task, also achieved the best performance (82.0) of all encoders on this task.\nIt is noteworthy that the improvements brought by the different encoders on this task roughly correspond to the performance on the pattern similar-\nity task. This fact implies two potential impacts. First, the distributed representations of relational patterns are useful and easily transferable to other tasks such as knowledge base population. Second, the pattern similarity dataset provides a gauge to predict successes of distributed representations in another task.\nWe could further improve the performance of SVM + GAC by incorporating external resources in the similar manner as the previous studies did. Concretely, SVM + GAC achieved 83.7 F1 score by adding features for WordNet, named entities (NE), and dependency paths explained in Hashimoto et al. (2015). Moreover, we could obtain 84.2 F1 score, using the ranking based loss function (dos Santos et al., 2015) and fine-tuning of the distributed representations initially trained by GAC. Currently, this is the second best score among the performance values reported in the previous studies on this task (the second group of Table 3). If we could use the negative sampling technique proposed by Xu et al. (2015), we might improve the performance further14."}, {"heading": "5 Related Work", "text": "Mitchell and Lapata (2010) was a pioneering work in semantic modeling of short phrases. They constructed the dataset that contains two-word phrase pairs with semantic similarity judged by human annotators. Korkontzelos et al. (2013) provided a semantic similarity dataset with pairs of two words and a single word. Wieting et al. (2015) annotated a part of PPDB (Ganitkevitch et al., 2013) to evaluate semantic modeling of paraphrases. Although the target unit of semantic modeling is different from that for these previous studies, we follow the annotation guideline and instruction of Mitchell and Lapata (2010) to build the new dataset.\nThe task addressed in this paper is also related to the Semantic Textual Similarity (STS) task (Agirre et al., 2012). STS is the task to measure the degree of semantic similarity between two sentences. Even though a relational pattern appears as a part of a sentence, it may be difficult to transfer findings from one to another: for example, the encoders of RNN and its variants explored in this study may exhibit different characteristics,\n14In fact, we made substantial efforts to introduce the negative sampling technique. However, Xu et al. (2015) omits the detail of the technique probably because of the severe page limit of short papers. For this reason, we could not reproduce their method in this study.\ninfluenced by the length and complexity of input text expressions.\nIn addition to data construction, this paper addresses semantic modeling of relational patterns. Nakashole et al. (2012) approached the similar task by constructing a taxonomy of relational patterns. They represented a vector of a relational pattern as the distribution of entity pairs co-occurring with the relational pattern. Grycner et al. (2015) extended Nakashole et al. (2012) to generalize dimensions of the vector space (entity pairs) by incorporating hyponymy relation between entities. They also used external resources to recognize the transitivity of pattern pairs and applied transitivities to find patterns in entailment relation. These studies did not consider semantic composition of relational patterns. Thus, they might suffer from the data sparseness problem, as shown by NoComp in Figure 4.\nNumerous studies have been aimed at encoding distributed representations of phrases and sentences from word embeddings by using: Recursive Neural Network (Socher et al., 2011), Matrix Vector Recursive Neural Network (Socher et al., 2012), Recursive Neural Network with different weight matrices corresponding to syntactic categories (Socher et al., 2013) or word types (Takase et al., 2016), RNN (Sutskever et al., 2011), LSTM (Sutskever et al., 2014), GRU (Cho et al., 2014), PAS-CLBLM (Hashimoto et al., 2014), etc. As described in Section 3, we applied RNN, GRU, and LSTM to compute distributed representations of relational patterns because recent papers have demonstrated their superiority in semantic composition (Sutskever et al., 2014; Tang et al., 2015). In this paper, we presented a comparative study of different encoders for semantic modeling of relational patterns.\nTo investigate usefulness of the distributed representations and the new dataset, we adopted the relation classification task (SemEval 2010 Task 8) as a real application. On the SemEval 2010 Task 8, several studies considered semantic composition. Gormley et al. (2015) proposed Feature-rich Compositional Embedding Model (FCM) that can combine binary features (e.g., positional indicators) with word embeddings via outer products. dos Santos et al. (2015) addressed the task using Convolutional Neural Network (CNN). Xu et al. (2015) achieved a higher performance than dos Santos et al. (2015) by application of CNN on de-\npendency paths. In addition to the relation classification task, we briefly describe other applications. To populate a knowledge base, Riedel et al. (2013) jointly learned latent feature vectors of entities, relational patterns, and relation types in the knowledge base. Toutanova et al. (2015) adapted CNN to capture the compositional structure of a relational pattern during the joint learning. For open domain question answering, Yih et al. (2014) proposed the method to map an interrogative sentence on an entity and a relation type contained in a knowledge base by using CNN.\nAlthough these reports described good performance on the respective tasks, we are unsure of the generality of distributed representations trained for a specific task such as the relation classification. In contrast, this paper demonstrated the contribution of distributed representations trained in a generic manner (with the Skip-gram objective) to the task of relation classification."}, {"heading": "6 Conclusion", "text": "In this paper, we addressed the semantic modeling of relational patterns. We introduced the new dataset in which humans rated multiple similarity scores for every pair of relational patterns on the dataset of semantic inference (Zeichner et al., 2012). Additionally, we explored different encoders for composing distributed representations of relational patterns. The experimental results shows that Gated Additive Composition (GAC), which is a combination of additive composition and the gating mechanism, is effective to compose distributed representations of relational patterns. Furthermore, we demonstrated that the presented dataset is useful to predict successes of the distributed representations in the relation classification task.\nWe expect that several further studies will use the new dataset not only for distributed representations of relational patterns but also for other NLP tasks (e.g., paraphrasing). Analyzing the internal mechanism of LSTM, GRU, and GAC, we plan to explore an alternative architecture of neural networks that is optimal for relational patterns."}, {"heading": "Acknowledgments", "text": "We thank the reviewers and Jun Suzuki for valuable comments. This work was partially supported by Grant-in-Aid for JSPS Fellows Grant\nno. 26.5820, JSPS KAKENHI Grant number 15H05318, and JST, CREST."}], "references": [{"title": "Semeval2012 task 6: A pilot on semantic textual similarity", "author": ["Agirre et al.2012] Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre"], "venue": "In The First Joint Conference on Lexical and Computational Semantics (*SEM", "citeRegEx": "Agirre et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "Long short-term memory neural networks for chinese word segmentation", "author": ["Chen et al.2015] Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung et al.2014] Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["Bing Xiang", "Bowen Zhou"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th In-", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Identifying relations for open information extraction", "author": ["Fader et al.2011] Anthony Fader", "Stephen Soderland", "Oren Etzioni"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Fader et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2011}, {"title": "Ppdb: The paraphrase database", "author": ["Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Improved relation extraction with feature-rich compositional embedding models", "author": ["Mo Yu", "Mark Dredze"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Gormley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gormley et al\\.", "year": 2015}, {"title": "Relly: Inferring hypernym relationships between relational phrases", "author": ["Grycner et al.2015] Adam Grycner", "Gerhard Weikum", "Jay Pujara", "James Foulds", "Lise Getoor"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language", "citeRegEx": "Grycner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grycner et al\\.", "year": 2015}, {"title": "Jointly learning word representations and composition functions using predicate-argument structures", "author": ["Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka"], "venue": "In Proceedings of the 2014 Conference", "citeRegEx": "Hashimoto et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2014}, {"title": "Task-oriented learning of word embeddings for semantic relation classification", "author": ["Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka"], "venue": "In Proceedings of the 19th Conference on Computational Natural", "citeRegEx": "Hashimoto et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2015}, {"title": "Semeval-2010 task 8: Multi-way classification", "author": ["Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2010}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Semeval-2013 task 5: Evaluating phrasal semantics", "author": ["Torsten Zesch", "Fabio Massimo Zanzotto", "Chris Biemann"], "venue": "In Second Joint Conference on Lexical and Computational Semantics (*SEM", "citeRegEx": "Korkontzelos et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Korkontzelos et al\\.", "year": 2013}, {"title": "Dirt \u2013 discovery of inference rules from text", "author": ["Lin", "Pantel2001] Dekang Lin", "Patrick Pantel"], "venue": "In Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD", "citeRegEx": "Lin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2001}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Composition in distributional models of semantics", "author": ["Mitchell", "Lapata2010] Jeff Mitchell", "Mirella Lapata"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2010}, {"title": "Finding the best model among representative compositional models", "author": ["Sonse Shimaoka", "Kazeto Yamamoto", "Yotaro Watanabe", "Naoaki Okazaki", "Kentaro Inui"], "venue": "In Proceedings of the 28th Pacific Asia", "citeRegEx": "Muraoka et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Muraoka et al\\.", "year": 2014}, {"title": "Patty: A taxonomy of relational patterns with semantic types", "author": ["Gerhard Weikum", "Fabian Suchanek"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Nakashole et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakashole et al\\.", "year": 2012}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M. Marlin"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Asso-", "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Utd: Classifying semantic relations by combining lexical and semantic resources", "author": ["Rink", "Harabagiu2010] Bryan Rink", "Sanda Harabagiu"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Rink et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rink et al\\.", "year": 2010}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Cliff Chiung-Yu Lin", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proceedings of the 28th International Conference on Machine", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Parsing with compositional vector grammars", "author": ["John Bauer", "Christopher D. Manning", "Ng Andrew Y"], "venue": "In Proceedings of the 51st Annual Meeting of the Association", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Generating text with recurrent neural networks", "author": ["James Martens", "Geoffrey Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long shortterm memory networks", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computa-", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Modeling semantic compositionality of relational patterns", "author": ["Takase et al.2016] Sho Takase", "Naoaki Okazaki", "Kentaro Inui"], "venue": "Engineering Applications of Artificial Intelligence,", "citeRegEx": "Takase et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Takase et al\\.", "year": 2016}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Tang et al.2015] Duyu Tang", "Bing Qin", "Ting Liu"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Representing text for joint embedding of text and knowledge bases", "author": ["Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon"], "venue": "In Proceedings of the 2015 Conference on Empirical", "citeRegEx": "Toutanova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2015}, {"title": "From paraphrase database to compositional paraphrase model and back. Transactions of the Association for Computational Linguistics (TACL", "author": ["Wieting et al.2015] John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": null, "citeRegEx": "Wieting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Semantic relation classification via convolutional neural networks with simple negative sampling", "author": ["Xu et al.2015] Kun Xu", "Yansong Feng", "Songfang Huang", "Dongyan Zhao"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Nat-", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Semantic parsing for single-relation question answering", "author": ["Yih et al.2014] Wen-tau Yih", "Xiaodong He", "Christopher Meek"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Yih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2014}, {"title": "Crowdsourcing inference-rule evaluation", "author": ["Jonathan Berant", "Ido Dagan"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Zeichner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zeichner et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 36, "context": "tional patterns, this paper constructs a new dataset that provides multiple similarity ratings for every pair of relational patterns on the existing dataset (Zeichner et al., 2012).", "startOffset": 157, "endOffset": 180}, {"referenceID": 17, "context": "Learning distributed representations for relation instances is a central technique in downstream applications as a number of recent studies demonstrated the usefulness of distributed representations for words (Mikolov et al., 2013; Pennington et al., 2014) and sentences (Sutskever et al.", "startOffset": 209, "endOffset": 256}, {"referenceID": 21, "context": "Learning distributed representations for relation instances is a central technique in downstream applications as a number of recent studies demonstrated the usefulness of distributed representations for words (Mikolov et al., 2013; Pennington et al., 2014) and sentences (Sutskever et al.", "startOffset": 209, "endOffset": 256}, {"referenceID": 28, "context": ", 2014) and sentences (Sutskever et al., 2014; Cho et al., 2014; Kiros et al., 2015).", "startOffset": 22, "endOffset": 84}, {"referenceID": 2, "context": ", 2014) and sentences (Sutskever et al., 2014; Cho et al., 2014; Kiros et al., 2015).", "startOffset": 22, "endOffset": 84}, {"referenceID": 20, "context": ", \u201cX: smoking, Y: cancer\u201d) (Lin and Pantel, 2001; Nakashole et al., 2012).", "startOffset": 27, "endOffset": 73}, {"referenceID": 35, "context": "Nowadays, several studies adopt distributed representations computed by neural networks for semantic modeling of relational patterns (Yih et al., 2014; Takase et al., 2016).", "startOffset": 133, "endOffset": 172}, {"referenceID": 30, "context": "Nowadays, several studies adopt distributed representations computed by neural networks for semantic modeling of relational patterns (Yih et al., 2014; Takase et al., 2016).", "startOffset": 133, "endOffset": 172}, {"referenceID": 7, "context": "The dataset includes 5,555 pairs2 extracted by Reverb (Fader et al., 2011), 2,447 pairs with inference relation and 3,108 pairs (the rest) without one.", "startOffset": 54, "endOffset": 74}, {"referenceID": 35, "context": "We build a new dataset upon the work of Zeichner et al. (2012), which consists of relational patterns with semantic inference labels annotated.", "startOffset": 40, "endOffset": 63}, {"referenceID": 36, "context": "We use instance-based judgment in a similar manner to that of Zeichner et al. (2012) to secure a high inter-annotator agreement.", "startOffset": 62, "endOffset": 85}, {"referenceID": 36, "context": "We use the entity pairs provided in Zeichner et al. (2012).", "startOffset": 36, "endOffset": 59}, {"referenceID": 17, "context": "In fact, Mikolov et al. (2013) implemented this approach as a preprocessing step, mining phrasal expressions with strong collocations from a training corpus.", "startOffset": 9, "endOffset": 31}, {"referenceID": 19, "context": "Muraoka et al. (2014) reported that the additive composition is a strong baseline among various methods.", "startOffset": 0, "endOffset": 22}, {"referenceID": 28, "context": "tic compositions of phrases and sentences by using (a variant of) Recurrent Neural Network (RNN) (Sutskever et al., 2014; Tang et al., 2015).", "startOffset": 97, "endOffset": 140}, {"referenceID": 31, "context": "tic compositions of phrases and sentences by using (a variant of) Recurrent Neural Network (RNN) (Sutskever et al., 2014; Tang et al., 2015).", "startOffset": 97, "endOffset": 140}, {"referenceID": 6, "context": "For a given embedding xt at position t, the vanilla RNN (Elman, 1990) computes the hidden state ht \u2208 Rd by the following recursive equation5,", "startOffset": 56, "endOffset": 69}, {"referenceID": 2, "context": "We also employ Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) as an encoder for relational patterns.", "startOffset": 111, "endOffset": 129}, {"referenceID": 1, "context": "LSTM has been applied successfully to various NLP tasks including word segmentation (Chen et al., 2015), dependency parsing (Dyer et al.", "startOffset": 84, "endOffset": 103}, {"referenceID": 5, "context": ", 2015), dependency parsing (Dyer et al., 2015), machine translation (Sutskever et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 28, "context": ", 2015), machine translation (Sutskever et al., 2014), and sentiment analysis (Tai et al.", "startOffset": 29, "endOffset": 53}, {"referenceID": 29, "context": ", 2014), and sentiment analysis (Tai et al., 2015).", "startOffset": 32, "endOffset": 50}, {"referenceID": 2, "context": "GRU is also successful in machine translation (Cho et al., 2014) and various", "startOffset": 46, "endOffset": 64}, {"referenceID": 3, "context": "Although some researchers reported that GRU is superior to LSTM (Chung et al., 2014), we have no consensus about the superiority.", "startOffset": 64, "endOffset": 84}, {"referenceID": 17, "context": "LSTM, GRU, and GAC) on an unlabeled text corpus, we adapt the Skip-gram model (Mikolov et al., 2013).", "startOffset": 78, "endOffset": 100}, {"referenceID": 17, "context": "Equation 2 of the original paper (Mikolov et al., 2013) denotes xt (word vector) as v (input vector) and x\u0303t (context vector) as v\u2032 (output vector).", "startOffset": 33, "endOffset": 55}, {"referenceID": 17, "context": "We use the probability distribution of words raised to the 3/4 power (Mikolov et al., 2013).", "startOffset": 69, "endOffset": 91}, {"referenceID": 7, "context": "We used Reverb (Fader et al., 2011) to the ukWaC corpus to extract relational pattern can-", "startOffset": 15, "endOffset": 35}, {"referenceID": 17, "context": "The hyperparameters of the Skip-gram model are identical to those in Mikolov et al. (2013): the width of context window \u03b4 = 5, the number of negative samples K = 5, the subsampling of 10\u22125.", "startOffset": 69, "endOffset": 91}, {"referenceID": 13, "context": "To examine the usefulness of the dataset and distributed representations for a different application, we address the task of relation classification on the SemEval 2010 Task 8 dataset (Hendrickx et al., 2010).", "startOffset": 184, "endOffset": 208}, {"referenceID": 25, "context": "MV-RNN (Socher et al., 2012) embeddings, parse trees 79.", "startOffset": 7, "endOffset": 28}, {"referenceID": 9, "context": "FCM (Gormley et al., 2015) w/o fine-tuning embeddings, dependency 79.", "startOffset": 4, "endOffset": 26}, {"referenceID": 12, "context": "RelEmb (Hashimoto et al., 2015) embeddings 82.", "startOffset": 7, "endOffset": 31}, {"referenceID": 34, "context": "depLCNN (Xu et al., 2015) embeddings, dependency 81.", "startOffset": 8, "endOffset": 25}, {"referenceID": 10, "context": "7 F1 score by adding features for WordNet, named entities (NE), and dependency paths explained in Hashimoto et al. (2015). Moreover, we could obtain 84.", "startOffset": 98, "endOffset": 122}, {"referenceID": 34, "context": "If we could use the negative sampling technique proposed by Xu et al. (2015), we might improve the performance further14.", "startOffset": 60, "endOffset": 77}, {"referenceID": 8, "context": "(2015) annotated a part of PPDB (Ganitkevitch et al., 2013) to evaluate semantic modeling of paraphrases.", "startOffset": 32, "endOffset": 59}, {"referenceID": 14, "context": "Korkontzelos et al. (2013) provided a semantic similarity dataset with pairs of two words and a single word.", "startOffset": 0, "endOffset": 27}, {"referenceID": 14, "context": "Korkontzelos et al. (2013) provided a semantic similarity dataset with pairs of two words and a single word. Wieting et al. (2015) annotated a part of PPDB (Ganitkevitch et al.", "startOffset": 0, "endOffset": 131}, {"referenceID": 8, "context": "(2015) annotated a part of PPDB (Ganitkevitch et al., 2013) to evaluate semantic modeling of paraphrases. Although the target unit of semantic modeling is different from that for these previous studies, we follow the annotation guideline and instruction of Mitchell and Lapata (2010) to build the new dataset.", "startOffset": 33, "endOffset": 284}, {"referenceID": 0, "context": "The task addressed in this paper is also related to the Semantic Textual Similarity (STS) task (Agirre et al., 2012).", "startOffset": 95, "endOffset": 116}, {"referenceID": 34, "context": "However, Xu et al. (2015) omits the detail of the technique probably because of the severe page limit of short papers.", "startOffset": 9, "endOffset": 26}, {"referenceID": 19, "context": "Nakashole et al. (2012) approached the similar task by constructing a taxonomy of relational patterns.", "startOffset": 0, "endOffset": 24}, {"referenceID": 10, "context": "Grycner et al. (2015) extended Nakashole et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "Grycner et al. (2015) extended Nakashole et al. (2012) to generalize dimensions of the vector space (entity pairs) by incorporating hyponymy relation between entities.", "startOffset": 0, "endOffset": 55}, {"referenceID": 24, "context": "Numerous studies have been aimed at encoding distributed representations of phrases and sentences from word embeddings by using: Recursive Neural Network (Socher et al., 2011), Matrix", "startOffset": 154, "endOffset": 175}, {"referenceID": 25, "context": "Vector Recursive Neural Network (Socher et al., 2012), Recursive Neural Network with different weight matrices corresponding to syntactic categories (Socher et al.", "startOffset": 32, "endOffset": 53}, {"referenceID": 26, "context": ", 2012), Recursive Neural Network with different weight matrices corresponding to syntactic categories (Socher et al., 2013) or word types (Takase et al.", "startOffset": 103, "endOffset": 124}, {"referenceID": 30, "context": ", 2013) or word types (Takase et al., 2016), RNN (Sutskever et al.", "startOffset": 22, "endOffset": 43}, {"referenceID": 27, "context": ", 2016), RNN (Sutskever et al., 2011),", "startOffset": 13, "endOffset": 37}, {"referenceID": 28, "context": "LSTM (Sutskever et al., 2014), GRU (Cho et al.", "startOffset": 5, "endOffset": 29}, {"referenceID": 2, "context": ", 2014), GRU (Cho et al., 2014), PAS-CLBLM (Hashimoto et al.", "startOffset": 13, "endOffset": 31}, {"referenceID": 11, "context": ", 2014), PAS-CLBLM (Hashimoto et al., 2014), etc.", "startOffset": 19, "endOffset": 43}, {"referenceID": 28, "context": "As described in Section 3, we applied RNN, GRU, and LSTM to compute distributed representations of relational patterns because recent papers have demonstrated their superiority in semantic composition (Sutskever et al., 2014; Tang et al., 2015).", "startOffset": 201, "endOffset": 244}, {"referenceID": 31, "context": "As described in Section 3, we applied RNN, GRU, and LSTM to compute distributed representations of relational patterns because recent papers have demonstrated their superiority in semantic composition (Sutskever et al., 2014; Tang et al., 2015).", "startOffset": 201, "endOffset": 244}, {"referenceID": 8, "context": "Gormley et al. (2015) proposed Feature-rich Compositional Embedding Model (FCM) that can combine binary features (e.", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": "dos Santos et al. (2015) addressed the task using Convolutional Neural Network (CNN).", "startOffset": 4, "endOffset": 25}, {"referenceID": 4, "context": "dos Santos et al. (2015) addressed the task using Convolutional Neural Network (CNN). Xu et al. (2015) achieved a higher performance than dos Santos et al.", "startOffset": 4, "endOffset": 103}, {"referenceID": 4, "context": "dos Santos et al. (2015) addressed the task using Convolutional Neural Network (CNN). Xu et al. (2015) achieved a higher performance than dos Santos et al. (2015) by application of CNN on de-", "startOffset": 4, "endOffset": 163}, {"referenceID": 22, "context": "To populate a knowledge base, Riedel et al. (2013) jointly learned latent feature vectors of entities, relational patterns, and relation types in the knowledge base.", "startOffset": 30, "endOffset": 51}, {"referenceID": 22, "context": "To populate a knowledge base, Riedel et al. (2013) jointly learned latent feature vectors of entities, relational patterns, and relation types in the knowledge base. Toutanova et al. (2015) adapted CNN to capture the compositional structure of a relational pattern during the joint learning.", "startOffset": 30, "endOffset": 190}, {"referenceID": 22, "context": "To populate a knowledge base, Riedel et al. (2013) jointly learned latent feature vectors of entities, relational patterns, and relation types in the knowledge base. Toutanova et al. (2015) adapted CNN to capture the compositional structure of a relational pattern during the joint learning. For open domain question answering, Yih et al. (2014) proposed the method to map an interrogative sentence on an entity and a relation type contained in a knowledge base by using CNN.", "startOffset": 30, "endOffset": 346}], "year": 2017, "abstractText": "Learning distributed representations for relation instances is a central technique in downstream NLP applications. In order to address semantic modeling of relational patterns, this paper constructs a new dataset that provides multiple similarity ratings for every pair of relational patterns on the existing dataset (Zeichner et al., 2012). In addition, we conduct a comparative study of different encoders including additive composition, RNN, LSTM, and GRU for composing distributed representations of relational patterns. We also present Gated Additive Composition, which is an enhancement of additive composition with the gating mechanism. Experiments show that the new dataset does not only enable detailed analyses of the different encoders, but also provides a gauge to predict successes of distributed representations of relational patterns in the relation classification task.", "creator": "LaTeX with hyperref package"}}}