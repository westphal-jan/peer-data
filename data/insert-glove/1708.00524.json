{"id": "1708.00524", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Aug-2017", "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm", "abstract": "NLP cant\u00fa tasks are fischeri often limited 47-42-80-44 by scarcity sensacion of haute-provence manually bataineh annotated data. In social media ajasin sentiment pathbreaking analysis and la\u00efcit\u00e9 related tasks, charents researchers abimbola have therefore used untso binarized gendler emoticons and specific bics hashtags intentional as ronaldo forms of mcareavey distant proudfoot supervision. tartus Our vannucchi paper shows that 117.40 by sundry extending bussie the distant lourenco supervision mem to ichthyologist a seborrheic more microcrystalline diverse parke-davis set of pumper noisy labels, gunowners the models saya can learn richer representations. Through emoji prediction urim on a bartered dataset of never 1246 million crankcase tweets pickney containing one 52.84 of gyurkovics 64 common emojis we 102.4 obtain state - of - 73-page the - art performance branes on orao 8 benchmark dinanath datasets hemisphere within bethge sentiment, idrissou emotion and sarcasm detection nbm using lanxess a single milutinovi\u0107 pretrained kunlun model. tigerland Our zuquilanda analyses confirm that the diversity of beech our anze emotional tinkle labels dacca yield ankergren a performance improvement 32.3 over wepn previous distant euro501 supervision 93,500 approaches.", "histories": [["v1", "Tue, 1 Aug 2017 21:28:42 GMT  (1811kb,D)", "http://arxiv.org/abs/1708.00524v1", "Accepted at EMNLP 2017. Please include EMNLP in any citations. Minor changes from the submitted camera-ready version. 9 pages + references and supplementary material"], ["v2", "Sat, 7 Oct 2017 19:21:48 GMT  (1811kb,D)", "http://arxiv.org/abs/1708.00524v2", "Accepted at EMNLP 2017. Please include EMNLP in any citations. Minor changes from the EMNLP camera-ready version. 9 pages + references and supplementary material"]], "COMMENTS": "Accepted at EMNLP 2017. Please include EMNLP in any citations. Minor changes from the submitted camera-ready version. 9 pages + references and supplementary material", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["bjarke felbo", "alan mislove", "anders s\u00f8gaard", "iyad rahwan", "sune lehmann"], "accepted": true, "id": "1708.00524"}, "pdf": {"name": "1708.00524.pdf", "metadata": {"source": "CRF", "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm", "authors": ["Bjarke Felbo", "Alan Mislove", "Anders S\u00f8gaard", "Iyad Rahwan", "Sune Lehmann"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "A variety of NLP tasks are limited by scarcity of manually annotated data. Therefore, co-occurring emotional expressions have been used for distant supervision in social media sentiment analysis and related tasks to make the models learn useful text representations before modeling these tasks directly. For instance, the state-of-the-art approaches within sentiment analysis of social media data use positive/negative emoticons for training their models (Deriu et al., 2016; Tang et al., 2014). Similarly, hashtags such as #anger, #joy, #happytweet, #ugh, #yuck and #fml have in previous research been mapped into emotional categories for emotion analysis (Mohammad, 2012).\nDistant supervision on noisy labels often enables a model to obtain better performance on the target task. In this paper, we show that extend-\ning the distant supervision to a more diverse set of noisy labels enables the models to learn richer representations of emotional content in text, thereby obtaining better performance on benchmarks for detecting sentiment, emotions and sarcasm. We show that the learned representation of a single pretrained model generalizes across 5 domains.\nTable 1: Example sentences scored by our model. For each text the top five most likely emojis are shown with the model\u2019s probability estimates.\nI love mom's cooking 49.1% 8.8% 3.1% 3.0% 2.9%\nI love how you never reply back.. 14.0% 8.3% 6.3% 5.4% 5.1%\nI love cruising with my homies 34.0% 6.6% 5.7% 4.1% 3.8%\nI love messing with yo mind!! 17.2% 11.8% 8.0% 6.4% 5.3%\nI love you and now you're just gone.. 39.1% 11.0% 7.3% 5.3% 4.5%\nThis is shit 7.0% 6.4% 6.0% 6.0% 5.8%\nThis is the shit 10.9% 9.7% 6.5% 5.7% 4.8%\nEmojis are not always a direct labeling of emotional content. For instance, a positive emoji may serve to disambiguate an ambiguous sentence or to complement an otherwise relatively negative text. Kunneman et al. (2014) discuss a similar duality in the use of emotional hashtags such as #nice and #lame. Nevertheless, our work shows that emojis can be used to classify the emotional content of texts accurately in many cases. For instance, our DeepMoji model captures varied usages of the word \u2018love\u2019 as well as slang such as \u2018this is the shit\u2019 being a positive statement (see Table 1). We provide an online demo at deepmoji.mit.edu to allow others to explore the predictions of our model.\nContributions We show how millions of readily available emoji occurrences on Twitter can be used to pretrain models to learn a richer emotional\nar X\niv :1\n70 8.\n00 52\n4v 1\n[ st\nat .M\nL ]\n1 A\nug 2\n01 7\nrepresentation than traditionally obtained through distant supervision. We transfer this knowledge to the target tasks using a new layer-wise fine-tuning method, obtaining improvements over the stateof-the-art within a range of tasks: emotion, sarcasm and sentiment detection. We present multiple analyses on the effect of pretraining, including results that show that the diversity of our emoji set is important for the transfer learning potential of our model. Our pretrained DeepMoji model is released with the hope that other researchers can use it for various NLP tasks1."}, {"heading": "2 Related work", "text": "Using emotional expressions as noisy labels in text to counter scarcity of labels is not a new idea (Read, 2005; Go et al., 2009). Originally, binarized emoticons were used as noisy labels, but later also hashtags and emojis have been used. To our knowledge, previous research has always manually specified which emotional category each emotional expression belong to. Prior work has used theories of emotion such as Ekman\u2019s six basic emotions and Plutchik\u2019s eight basic emotions (Mohammad, 2012; Suttles and Ide, 2013).\nSuch manual categorization requires an understanding of the emotional content of each expression, which is difficult and time-consuming for sophisticated combinations of emotional content. Moreover, any manual selection and categorization is prone to misinterpretations and may omit important details regarding usage. In contrast, our approach requires no prior knowledge of the corpus and can capture diverse usage of 64 types of emojis (see Table 1 for examples and Figure 3 for how the model implicitly groups emojis).\nAnother way of automatically interpreting the emotional content of an emoji is to learn emoji embeddings from the words describing the emojisemantics in official emoji tables (Eisner et al., 2016). This approach, in our context, suffers from two severe limitations: a) It requires emojis at test time while there are many domains with limited or no usage of emojis. b) The tables do not capture the dynamics of emoji usage, i.e., drift in an emoji\u2019s intended meaning over time.\nKnowledge can be transferred from the emoji dataset to the target task in many different ways. In particular, multitask learning with simultaneous\n1Available with preprocessing code, examples of usage, benchmark datasets etc. at github.com/bfelbo/deepmoji\ntraining on multiple datasets has shown promising results (Collobert and Weston, 2008). However, multitask learning requires access to the emoji dataset whenever the classifier needs to be tuned for a new target task. Requiring access to the dataset is problematic in terms of violating data access regulations. There are also issues from a data storage perspective as the dataset used for this research contains hundreds of millions of tweets (see Table 2). Instead we use transfer learning (Bengio et al., 2012) as described in \u00a73.3, which does not require access to the original dataset, but only the pretrained classifier."}, {"heading": "3 Method", "text": ""}, {"heading": "3.1 Pretraining", "text": "In many cases, emojis serve as a proxy for the emotional contents of a text. Therefore, pretraining on the classification task of predicting which emoji were initially part of a text can improve performance on the target task (see \u00a75.3 for an analysis of why our pretraining helps). Social media contains large amounts of short texts with emojis that can be utilized as noisy labels for pretraining. Here, we use data from Twitter from January 1st 2013 to June 1st 2017, but any dataset with emoji occurrences could be used.\nOnly English tweets without URL\u2019s are used for the pretraining dataset. Our hypothesis is that the content obtained from the URL is likely to be important for understanding the emotional content of the text in the tweet. Therefore, we expect emojis associated with these tweets to be noiser labels\nthan for tweets without URLs, and the tweets with URLs are thus removed.\nProper tokenization is important for generalization. All tweets are tokenized on a word-by-word basis. Words with 2 or more repeated characters are shortened to the same token (e.g. \u2018loool\u2019 and \u2018looooool\u2019 are tokenized such that they are treated the same). Similarly, we use a special token for all URLs (only relevant for benchmark datasets), user mentions (e.g. \u2018@acl2017\u2019 and \u2018@emnlp2017\u2019 are thus treated the same) and numbers. To be included in the training set the tweet must contain at least 1 token that is not a punctuation symbol, emoji or special token2.\nMany tweets contain multiple repetitions of the same emoji or multiple different emojis. In the training data, we address this in the following way. For each unique emoji type, we save a separate tweet for the pretraining with that emoji type as the label. We only save a single tweet for the pretraining per unique emoji type regardless of the number of emojis associated with the tweet. This data preprocessing allows the pretraining task to capture that multiple types of emotional content are associated with the tweet while making our pretraining task a single-label classification instead of a more complicated multi-label classification.\nTo ensure that the pretraining encourages the models to learn a rich understanding of emotional content in text rather than only emotional content associated with the most used emojis, we create a balanced pretraining dataset. The pretraining data is split into a training, validation and test set, where the validation and test set is randomly sampled in such a way that each emoji is equally represented. The remaining data is upsampled to create a balanced training dataset."}, {"heading": "3.2 Model", "text": "With the millions of emoji occurrences available, we can train very expressive classifiers with limited risk of overfitting. We use a variant of the Long Short-Term Memory (LSTM) model that has been successful at many NLP tasks (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014). Our DeepMoji model uses an embedding layer of 256 dimensions to project each word into a vector space. A hyperbolic tangent activation function is used to enforce a constraint of each embedding dimension being within [\u22121, 1]. To capture the con-\n2Details available at github.com/bfelbo/deepmoji\ntext of each word we use two bidirectional LSTM layers with 1024 hidden units in each (512 in each direction). Finally, an attention layer that take all of these layers as input using skip-connections is used (see Figure 1 for an illustration).\nThe attention mechanism lets the model decide the importance of each word for the prediction task by weighing them when constructing the representation of the text. For instance, a word such as \u2018amazing\u2019 is likely to be very informative of the emotional meaning of a text and it should thus be treated accordingly. We use a simple approach inspired by (Bahdanau et al., 2014; Yang et al., 2016) with a single parameter pr. input channel:\net = htwa at = exp(et)\u2211T i=1 exp(ei)\nv = T\u2211 i=1 aihi\nHere ht is the representation of the word at time step t and wa is the weight matrix for the attention layer. The attention importance scores for each time step, at, are obtained by multiplying the representations with the weight matrix and then normalizing to construct a probability distribution over the words. Lastly, the representation vector for the text, v, is found by a weighted summation over all the time steps using the attention importance scores as weights. This representation vector obtained from the attention layer is a high-level encoding of the entire text, which is used as input to the final Softmax layer for classification. We find that adding the attention mechanism and skipconnections improves the model\u2019s capabilities for transfer learning (see \u00a75.2 for more details).\nThe only regularization used for the pretraining task is a L2 regularization of 1E\u22126 on the embedding weights. For the finetuning additional regularization is applied (see \u00a74.2). Our model is implemented using Theano (Theano Development Team, 2016) and we make an easy-to-use version available that uses Keras (Chollet et al., 2015)."}, {"heading": "3.3 Transfer learning", "text": "Our pretrained model can be fine-tuned to the target task in multiple ways with some approaches \u2018freezing\u2019 layers by disabling parameters updates to prevent overfitting. One common approach is\nto use the network as a feature extractor (Donahue et al., 2014), where all layers in the model are frozen when fine-tuning on the target task except the last layer (hereafter referred to as the \u2018last\u2019 approach). Alternatively, another common approach is to use the pretrained model as an initialization (Erhan et al., 2010), where the full model is unfrozen (hereafter referred to as \u2018full\u2019).\nWe propose a new simple transfer learning approach, \u2018chain-thaw\u2019, that sequentially unfreezes and fine-tunes a single layer at a time. This approach increases accuracy on the target task at the expense of extra computational power needed for the fine-tuning. By training each layer separately the model is able to adjust the individual patterns across the network with a reduced risk of overfitting. The sequential fine-tuning seems to have a regularizing effect similar to what has been examined with layer-wise training in the context of unsupervised learning (Erhan et al., 2010).\nMore specifically, the chain-thaw approach first fine-tunes any new layers (often only a Softmax layer) to the target task until convergence on a validation set. Then the approach fine-tunes each layer individually starting from the first layer in the network. Lastly, the entire model is trained with all layers. Each time the model converges as measured on the validation set, the weights are reloaded to the best setting, thereby preventing overfitting in a similar manner to early stopping (Sjo\u0308berg and Ljung, 1995). This process is illustrated in Figure 2. Note how only performing step a) in the figure is identical to the \u2018last\u2019 approach, where the existing network is used as a feature extractor. Similarly, only doing step d) is identical to the \u2018full\u2019 approach, where the pretrained weights are used as an initialization for a fully trainable network. Although the chain-thaw procedure may seem extensive it is easily implemented with only a few lines of code. Similarly, the additional time spent on fine-tuning is limited when the target task uses GPUs on small datasets of manually annotated data as is often the case.\nA benefit of the chain-thaw approach is the ability to expand the vocabulary to new domains with little risk of overfitting. For a given dataset up to 10000 new words from the training set are added to the vocabulary. \u00a75.3 contains analysis on the added word coverage gained from this approach."}, {"heading": "233.7 82.2 79.5 78.1 60.8 54.7 54.6 51.7 50.5 44.0 39.5 39.1 34.8 34.4 32.1 28.1", "text": ""}, {"heading": "24.8 23.4 21.6 21.0 20.5 20.3 19.9 19.6 18.9 17.5 17.0 16.9 16.1 15.3 15.2 15.0", "text": ""}, {"heading": "14.9 14.3 14.2 14.2 12.9 12.4 12.0 12.0 11.7 11.7 11.3 11.2 11.1 11.0 11.0 10.8", "text": ""}, {"heading": "10.2 9.6 9.5 9.3 9.2 8.9 8.7 8.6 8.1 6.3 6.0 5.7 5.6 5.5 5.4 5.1", "text": ""}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Emoji prediction", "text": "We use a raw dataset of 56.6 billion tweets, which is then filtered to 1.2 billion relevant tweets (see details in \u00a73.1). In the pretraining dataset a copy of a single tweet is stored once for each unique emoji, resulting in a dataset consisting of 1.6 billion tweets. Table 2 shows the distribution of tweets across different emoji types. To evaluate performance on the pretraining task a validation set and a test set both containing 640K tweets (10K of each emoji type) are used. The remaining tweets are used for the training set, which is balanced using upsampling.\nThe performance of the DeepMoji model is evaluated on the pretraining task with the results shown in Table 3. Both top 1 and top 5 accuracy is used for the evaluation as the emoji labels are noisy with multiple emojis being potentially correct for any given sentence. For comparison we also train a version of our DeepMoji model with smaller LSTM layers and a bag-of-words classifier, fastText, that has recently shown competitive results (Joulin et al., 2016). We use 256 dimen-\nsions for this fastText classifier, thereby making it almost identical to only using the embedding layer from the DeepMoji model. The difference in top 5 accuracy between the fastText classifier (36.2%) and the largest DeepMoji model (43.8%) underlines the difficulty of the emoji prediction task. As the two classifiers only differ in that the DeepMoji model has LSTM layers and an attention layer between the embedding and Softmax layer, this difference in accuracy demonstrates the importance of capturing the context of each word."}, {"heading": "4.2 Benchmarking", "text": "We benchmark our method on 3 different NLP tasks using 8 datasets across 5 domains. To make for a fair comparison, we compare DeepMoji to other methods that also utilize external data sources in addition to the benchmark dataset. An averaged F1-measure across classes is used for evaluation in emotion analysis and sarcasm detection as these consist of unbalanced datasets while sentiment datasets are evaluated using accuracy.\nAn issue with many of the benchmark datasets is data scarcity, which is particularly problematic within emotion analysis. Many recent papers proposing new methods for emotion analysis such as (Staiano and Guerini, 2014) only evaluate performance on a single benchmark dataset, SemEval 2007 Task 14, that contains 1250 observations. Recently, criticism has been raised concerning the use of correlation with continuous ratings as a measure (Buechel and Hahn, 2016), making only the somewhat limited binary evaluation possible. We only evaluate the emotions {Fear, Joy, Sadness} as the remaining emotions occur in less than 5% of the observations.\nTo fully evaluate our method on emotion analysis against the current methods we thus make use of two other datasets: A dataset of emotions in tweets related to the Olympic Games created by Sintsova et al. that we convert to a single-label\nclassification task and a dataset of self-reported emotional experiences created by a large group of psychologists (Wallbott and Scherer, 1986). See the supplementary material for details on the datasets and the preprocessing. As these two datasets do not have prior evaluations, we evaluate against a state-of-the-art approach, which is based on a valence-arousal-dominance framework (Buechel and Hahn, 2016). The scores extracted using this approach are mapped to the classes in the datasets using a logistic regression with parameter optimization using crossvalidation. We release our preprocessing code and hope that these 2 two datasets will be used for future benchmarking within emotion analysis.\nWe evaluate sentiment analysis performance on three benchmark datasets. These small datasets are chosen to emphasize the importance of the transfer learning ability of the evaluated models. Two of the datasets are from SentiStrength (Thelwall et al., 2010), SS-Twitter and SS-Youtube, and follow the relabeling described in (Saif et al., 2013) to make the labels binary. The third dataset is from SemEval 2016 Task4A (Nakov et al., 2016). Due to tweets being deleted from Twitter, the SemEval dataset suffers from data decay, making it difficult to compare results across papers. At the time of writing, roughly 15% of the training dataset for SemEval 2016 Task 4A was impossible to obtain. We choose not to use review datasets for sentiment benchmarking as these datasets contain so many words pr. observation that even bag-ofwords classifiers and unsupervised approaches can obtain a high accuracy (Joulin et al., 2016; Radford et al., 2017).\nThe current state of the art for sentiment analysis on social media (and winner of SemEval 2016 Task 4A) uses an ensemble of convolutional neural networks that are pretrained on a private dataset of tweets with emoticons, making it difficult to replicate (Deriu et al., 2016). Instead we pretrain a model with the hyperparameters of the largest model in their ensemble on the positive/negative emoticon dataset from Go et al. (2009). Using this pretraining as an initialization we finetune the model on the target tasks using early stopping on a validation set to determine the amount of training. We also implemented the SentimentSpecific Word Embedding (SSWE) using the embeddings available on the authors\u2019 website (Tang et al., 2014), but found that it performed worse\nthan the pretrained convolutional neural network. These results are therefore excluded.\nFor sarcasm detection we use the sarcasm dataset version 1 and 2 from the Internet Argument Corpus (Walker et al., 2012). Note that results presented on these benchmarks in e.g. Oraby et al. (2016) are not directly comparable as only a subset of the data is available online.3 A state-of-the-art baseline is found by modeling the embedding-based features from Joshi et al. (2016) alongside unigrams, bigrams and trigrams with an SVM. GoogleNews word2vec embeddings (Mikolov et al., 2013) are used for computing the embedding-based features. A hyperparameter search for regularization parameters is carried out using cross-validation. Note that the sarcasm dataset version 2 contains both a quoted text and a sarcastic response, but to keep the models identical across the datasets only the response is used.\nFor training we use the Adam optimizer (Kingma and Ba, 2015) with gradient clipping of the norm to 1. Learning rate is set to 1E\u22123 for training of all new layers and 1E\u22124\n3We contacted the authors, but were unable to obtain the full dataset for neither version 1 or version 2.\nfor finetuning any pretrained layers. To prevent overfitting on the small datasets, 10% of the channels across all words in the embedding layer are dropped out during training. Unlike e.g. (Gal and Ghahramani, 2016) we do not drop out entire words in the input as some of our datasets contain observations with so few words that it could change the meaning of the text. In addition to the embedding dropout, L2 regularization for the embedding weights is used and 50% dropout is applied to the penultimate layer.\nTable 5 shows that the DeepMoji model outperforms the state of the art across all benchmark datasets and that our new \u2018chain-thaw\u2019 approach consistently yields the highest performance for the transfer learning, albeit often only slightly better or equal to the \u2018last\u2019 approach. Results are averaged across 5 runs to reduce the variance. We test the statistical significance of our results by comparing the performance of DeepMoji (chain-thaw) vs. the state of the art. Bootstrap testing with 10000 samples is used. Our results are statistically significantly better than the state of the art with p < 0.001 on every benchmark dataset.\nOur model is able to out-perform the state-of-\nthe-art on datasets that originate from domains that differ substantially from the tweets on which it was pretrained. A key difference between the pretraining dataset and the benchmark datasets is the length of the observations. The average number of tokens pr. tweet in the pretraining dataset is 11, whereas e.g. the board posts from the Internet Argument Corpus version 1 (Oraby et al., 2016) has an average of 66 tokens with some observations being much longer."}, {"heading": "5 Model Analysis", "text": ""}, {"heading": "5.1 Importance of emoji diversity", "text": "One of the major differences between this work compared to previous papers using distant supervision is the diversity of the noisy labels used (see \u00a72). For instance, both Deriu et al. (2016) and Tang et al. (2014) only used positive and negative emoticons as noisy labels. Other instances of previous work have used slightly more nuanced sets of noisy labels (see \u00a72), but to our knowledge our set of noisy labels is the most diverse yet. To analyze the effect of using a diverse emoji set we create a subset of our pretraining data containing tweets with one of 8 emojis that are similar to the positive/negative emoticons used by Tang et al. (2014) and Hu et al. (2013) (the set of emoticons and corresponding emojis are available in the supplemental material). As the dataset based on this reduced set of emojis contains 433M tweets, any difference in performance on benchmark datasets is likely linked to the diversity of labels rather than differences in dataset sizes.\nWe train our DeepMoji model to predict whether the tweets contain a positive or negative emoji and evaluate this pretrained model across the benchmark datasets. We refer to the model trained on the subset of emojis as DeepMojiPosNeg (as opposed to DeepMoji). To test the emotional representations learned by the two pretrained models the \u2018last\u2019 transfer learning approach is used for the comparison, thereby only allowing the models to map already learned features to classes in the target dataset. Table 6 shows that DeepMoji-PosNeg yields lower performance compared to DeepMoji across all 8 benchmarks, thereby showing that the diversity of our emoji types encourage the model to learn a richer representation of emotional content in text that is more useful for transfer learning.\nMany of the emojis carry similar emotional\ncontent, but have subtle differences in usage that our model is able to capture. Through hierarchical clustering on the correlation matrix of the DeepMoji model\u2019s predictions on the test set we can see that the model captures many similarities that one would intuitively expect (see Figure 3). For instance, the model groups emojis into overall categories associated with e.g. negativity, positivity or love. Similarly, the model learns to differentiate within these categories, mapping sad emojis in one subcategory of negativity, annoyed in another subcategory and angry in a third one."}, {"heading": "5.2 Model architecture", "text": "Our DeepMoji model architecture as described in \u00a73.2 use an attention mechanism and skipconnections to ease the transfer of the learned representation to new domains and tasks. Here we compare the DeepMoji model architecture to that of a standard 2-layer LSTM, both compared using the \u2018last\u2019 transfer learning approach. We use the same regularization and training parameters.\nAs seen in Table 6 the DeepMoji model performs better than a standard 2-layer LSTM across all benchmark datasets. The two architectures performed equally on the pretraining task, suggesting that while the DeepMoji model architecture is indeed better for transfer learning, it may not necessarily be better for single supervised classification task with ample available data.\nA reasonable conjecture is that the improved transfer learning performance is due to two factors: a) the attention mechanism with skipconnections provide easy access to learned low-\nlevel features for any time step, making it easy to use this information if needed for a new task b) the improved gradient-flow from the output layer to the early layers in the network due to skipconnections (Graves, 2013) is important when adjusting parameters in early layers as part of transfer learning to small datasets. Detailed analysis of whether these factors actually explain why our architecture outperform a standard 2-layer LSTM is left for future work."}, {"heading": "5.3 Analyzing the effect of pretraining", "text": "Performance on the target task benefits strongly from pretraining as shown in Table 5 by comparing DeepMoji (new) to DeepMoji (chain-thaw). In this section we experimentally decompose the benefit of pretraining into 2 effects: word coverage and phrase coverage. These two effects help regularize the model by preventing overfitting (see the supplementary details for an visualization of the effect of this regularization).\nThere are numerous ways to express a specific sentiment, emotion or sarcastic comment. Consequently, the test set may contain specific language use not present in the training set. The pretraining helps the target task models attend to low-support evidence by having previously observed similar usage in the pretraining dataset. We first examine this effect by measuring the improvement in word coverage on the test set when using the pretraining with word coverage being defined as the % of words in the test dataset seen in the training/pretraining dataset (see Table 7). An important reason why the \u2018chain-thaw\u2019 approach outperforms other transfer learning approaches is can be used to tune the embedding layer with limited risk of overfitting. Table 7 shows the increased word\ncoverage from adding new words to the vocabulary as part of that tuning.\nNote that word coverage can be a misleading metric in this context as for many of these small datasets a word will often occur only once in the training set. In contrast, all of the words in the pretraining vocabulary are present in thousands (if not millions) of observations in the emoji pretraining dataset thus making it possible for the model to learn a good representation of the emotional and semantic meaning. The added benefit of pretraining for learning word representations therefore likely extends beyond the differences seen in Table 7.\nTo examine the importance of capturing phrases and the context of each word, we evaluate the accuracy on the SS-Youtube dataset using a fastText classifier pretrained on the same emoji dataset as our DeepMoji model. This fastText classifier is almost identical to only using the embedding layer\nfrom the DeepMoji model. We evaluate the representations learned by fine-tuning the models as feature extractors (i.e. using the \u2018last\u2019 transfer learning approach). The fastText model achieves an accuracy of 63% as compared to 93% for our DeepMoji model, thereby emphasizing the importance of phrase coverage. One concept that the LSTM layers likely learn is negation, which is known to be important for sentiment analysis (Wiegand et al., 2010)."}, {"heading": "5.4 Comparing with human-level agreement", "text": "To understand how well our DeepMoji classifier performs compared to humans, we created a new dataset of random tweets annotated for sentiment. Each tweet was annotated by a minimum of 10 English-speaking Amazon Mechanical Turkers (MTurk\u2019s) living in USA. Tweets were rated on a scale from 1 to 9 with a \u2018Do not know\u2019 option, and guidelines regarding how to rate the tweets were provided to the human raters. The tweets were selected to contain only English text, no mentions and no URL\u2019s to make it possible to rate them without any additional contextual information. Tweets where more than half of the evaluators chose \u2018Do not know\u2019 were removed (98 tweets).\nFor each tweet, we select a MTurk rating random to be the \u2018human evaluation\u2019, and average over the remaining nine MTurk ratings are averaged to form the ground truth. The \u2018sentiment label\u2019 for a given tweet is thus defined as the overall consensus among raters (excluding the randomly-selected \u2018human evaluation\u2019 rating). To ensure that the label categories are clearly separated, we removed neutral tweets in the interval [4.5, 5.5] (roughly 29% of the tweets). The remaining dataset consists of 7 347 tweets. Of these tweets, 5000 are used for training/validation and the remaining are used as the test set. Our DeepMoji model is trained using the chain-thaw transfer learning approach.\nTable 8 shows that the agreement of the random MTurk rater is 76.1%, meaning that the randomly selected rater will agree with the average of the nine other MTurk-ratings of the tweet\u2019s polarity 76.1% of the time. Our DeepMoji model achieves 82.4% agreement, which means it is better at capturing the average human sentiment-rating than a single MTurk rater."}, {"heading": "6 Conclusion", "text": "We have shown how the millions of texts on social media with emojis can be used for pretraining models, thereby allowing them to learn representations of emotional content in texts. Through comparison with an identical model pretrained on a subset of emojis, we find that the diversity of our emoji set is important for the performance of our method. We release our pretrained DeepMoji model with the hope that other researchers will find good use of them for various emotion-related NLP tasks4."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Janys Analytics for generously allowing us to use their dataset of human-rated tweets and the associated code to analyze it. Furthermore, we would like to thank Max Lever, who helped design the online demo, and Han Thi Nguyen, who helped code the software that is provided alongside the pretrained model."}, {"heading": "A Supplemental Material", "text": "A.1 Preprocessing Emotion Datasets\nIn the Olympic Games dataset by Sintsova et al. each tweet can be assigned multiple emotions out of 20 possible emotions, making evaluation difficult. To counter this difficulty, we have chosen to convert the labels to 4 classes of low/high valence and low/high arousal based on the Geneva Emotion Wheel that the study used. A tweet is deemed as having emotions within the valence/arousal class if the average evaluation by raters for that class is 2.0 or higher, where \u2018Low\u2019 = 1, \u2018Medium\u2019 = 2 and \u2018High\u2019 = 3.\nWe also evaluate on the ISEAR databank (Wallbott and Scherer, 1986), which was created over many years by a large group of psychologists that interviewed respondents in 37 countries. Each observation in the dataset is a self-reported experience mapped to 1 of 7 possible emotions, making for an interesting benchmark dataset.\nA.2 Pretraining as Regularization\nFigure 4 shows an example of how the pretraining helps to regularize the target task model, which otherwise quickly overfits. The chain-thaw transfer learning approach further increases this regularization by fine-tuning the model layer wise, thereby adding additional regularization.\nA.3 Emoticon to Emoji mapping To analyze the effect of using a diverse emoji set we create a subset of our pretraining data containing tweets with one of 8 emojis that are similar to the positive/negative emoticons used by Tang et al. (2014) and Hu et al. (2013). The positive emoticons are :) : ) :-) :D =) and the negative emoticons are :( : ( :-(. We find the 8 similar emojis in our dataset seen in Figure 5 as use these for creating the reduced subset.\nNegative\nPositive\nA.4 Emoji Clustering We compute the predictions of the DeepMoji model on the pretraining test set containing 640K tweets and compute the correlation matrix of the predicted probabilities seen in Figure 7. Then we use hierarchical clustering with average linkage on the correlation matrix to generate the dendrogram seen in Figure 6. We visualized dendrograms for various versions of our model and the overall structure is very stable with only a few emojis changing places in the hierarchy."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "3rd International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Yoshua Bengio"], "venue": "29th International Conference on Machine learning (ICML) \u2013 Workshop on Unsupervised and Transfer Learning, volume 27, pages 17\u201336.", "citeRegEx": "Bengio,? 2012", "shortCiteRegEx": "Bengio", "year": 2012}, {"title": "Emotion analysis as a regression problem - dimensional models and their implications on emotion representation and metrical evaluation", "author": ["Sven Buechel", "Udo Hahn."], "venue": "22nd European Conference on Artificial Intelligence (ECAI).", "citeRegEx": "Buechel and Hahn.,? 2016", "shortCiteRegEx": "Buechel and Hahn.", "year": 2016}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet"], "venue": "https:// github.com/fchollet/keras.", "citeRegEx": "Chollet,? 2015", "shortCiteRegEx": "Chollet", "year": 2015}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "25th International Conference on Machine learning (ICML), pages 160\u2013167.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Swisscheese at semeval-2016 task 4: Sentiment classification using an ensemble of convolutional neural networks with distant supervision", "author": ["Jan Deriu", "Maurice Gonzenbach", "Fatih Uzdilli", "Aurelien Lucchi", "Valeria De Luca", "Martin Jaggi"], "venue": null, "citeRegEx": "Deriu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Deriu et al\\.", "year": 2016}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell."], "venue": "31th International Conference on Machine Learning (ICML),", "citeRegEx": "Donahue et al\\.,? 2014", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "emoji2vec: Learning emoji representations from their description", "author": ["Ben Eisner", "Tim Rockt\u00e4schel", "Isabelle Augenstein", "Matko Bo\u0161njak", "Sebastian Riedel."], "venue": "4th International Workshop on Natural Language Processing for Social Media (So-", "citeRegEx": "Eisner et al\\.,? 2016", "shortCiteRegEx": "Eisner et al\\.", "year": 2016}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent", "Samy Bengio"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani."], "venue": "30th Conference on Neural Information Processing Systems (NIPS), pages 1019\u2013 1027.", "citeRegEx": "Gal and Ghahramani.,? 2016", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Twitter sentiment classification using distant supervision", "author": ["Alec Go", "Richa Bhayani", "Lei Huang."], "venue": "CS224N Project Report, Stanford, 1(12).", "citeRegEx": "Go et al\\.,? 2009", "shortCiteRegEx": "Go et al\\.", "year": 2009}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1308.0850.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Unsupervised sentiment analysis with emotional signals", "author": ["Xia Hu", "Jiliang Tang", "Huiji Gao", "Huan Liu."], "venue": "Proceedings of the 22nd international conference on World Wide Web (WWW), pages 607\u2013618. ACM.", "citeRegEx": "Hu et al\\.,? 2013", "shortCiteRegEx": "Hu et al\\.", "year": 2013}, {"title": "Are word embedding-based features useful for sarcasm detection", "author": ["Aditya Joshi", "Vaibhav Tripathi", "Kevin Patel", "Pushpak Bhattacharyya", "Mark Carman"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Joshi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2016}, {"title": "Bag of tricks for efficient text classification", "author": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1607.01759.", "citeRegEx": "Joulin et al\\.,? 2016", "shortCiteRegEx": "Joulin et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "3rd International Conference on Learning Representations (ICLR).", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "The (un)predictability of emotional hashtags in twitter", "author": ["FA Kunneman", "CC Liebrecht", "APJ van den Bosch."], "venue": "52th Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics.", "citeRegEx": "Kunneman et al\\.,? 2014", "shortCiteRegEx": "Kunneman et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "27th Conference on Neural Information Processing Systems (NIPS), pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "emotional tweets", "author": ["Saif Mohammad."], "venue": "The First Joint Conference on Lexical and Computational Semantics (*SEM), pages 246\u2013255. Association for Computational Linguistics.", "citeRegEx": "Mohammad.,? 2012", "shortCiteRegEx": "Mohammad.", "year": 2012}, {"title": "Semeval2016 task 4: Sentiment analysis in twitter", "author": ["Preslav Nakov", "Alan Ritter", "Sara Rosenthal", "Fabrizio Sebastiani", "Veselin Stoyanov."], "venue": "10th International Workshop on Semantic Evaluation (SemEval), pages 1\u201318.", "citeRegEx": "Nakov et al\\.,? 2016", "shortCiteRegEx": "Nakov et al\\.", "year": 2016}, {"title": "Creating and characterizing a diverse corpus of sarcasm in dialogue", "author": ["Shereen Oraby", "Vrindavan Harrison", "Lena Reed", "Ernesto Hernandez", "Ellen Riloff", "Marilyn Walker."], "venue": "17th Annual Meeting of the Special Interest Group on Discourse and", "citeRegEx": "Oraby et al\\.,? 2016", "shortCiteRegEx": "Oraby et al\\.", "year": 2016}, {"title": "Learning to generate reviews and discovering sentiment", "author": ["Alec Radford", "Rafal Jozefowicz", "Ilya Sutskever."], "venue": "arXiv preprint arXiv:1704.01444.", "citeRegEx": "Radford et al\\.,? 2017", "shortCiteRegEx": "Radford et al\\.", "year": 2017}, {"title": "Using emoticons to reduce dependency in machine learning techniques for sentiment classification", "author": ["Jonathon Read."], "venue": "ACL student research workshop, pages 43\u201348. Association for Computational Linguistics.", "citeRegEx": "Read.,? 2005", "shortCiteRegEx": "Read.", "year": 2005}, {"title": "Evaluation datasets for twitter sentiment analysis: a survey and a new dataset, the stsgold", "author": ["Hassan Saif", "Miriam Fernandez", "Yulan He", "Harith Alani."], "venue": "Workshop: Emotion and Sentiment in Social and Expressive Media: approaches and per-", "citeRegEx": "Saif et al\\.,? 2013", "shortCiteRegEx": "Saif et al\\.", "year": 2013}, {"title": "Fine-grained emotion recognition in olympic tweets based on human computation", "author": ["Valentina Sintsova", "Claudiu-Cristian Musat", "Pearl Pu."], "venue": "4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis", "citeRegEx": "Sintsova et al\\.,? 2013", "shortCiteRegEx": "Sintsova et al\\.", "year": 2013}, {"title": "Overtraining, regularization and searching for a minimum, with application to neural networks", "author": ["Jonas Sj\u00f6berg", "Lennart Ljung."], "venue": "International Journal of Control, 62(6):1391\u20131407.", "citeRegEx": "Sj\u00f6berg and Ljung.,? 1995", "shortCiteRegEx": "Sj\u00f6berg and Ljung.", "year": 1995}, {"title": "Depechemood: A lexicon for emotion analysis from crowd-annotated news", "author": ["Jacopo Staiano", "Marco Guerini."], "venue": "52th Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics.", "citeRegEx": "Staiano and Guerini.,? 2014", "shortCiteRegEx": "Staiano and Guerini.", "year": 2014}, {"title": "Semeval2007 task 14: Affective text", "author": ["Carlo Strapparava", "Rada Mihalcea."], "venue": "4th International Workshop on Semantic Evaluations (SemEval), pages 70\u201374. Association for Computational Linguistics.", "citeRegEx": "Strapparava and Mihalcea.,? 2007", "shortCiteRegEx": "Strapparava and Mihalcea.", "year": 2007}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "28th Conference on Neural Information Processing Systems (NIPS), pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Distant supervision for emotion classification with discrete binary values", "author": ["Jared Suttles", "Nancy Ide."], "venue": "International Conference on Intelligent Text Processing and Computational Linguistics (CICLing), pages 121\u2013136. Springer.", "citeRegEx": "Suttles and Ide.,? 2013", "shortCiteRegEx": "Suttles and Ide.", "year": 2013}, {"title": "Learning sentimentspecific word embedding for twitter sentiment classification", "author": ["Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin."], "venue": "52th Annual Meeting of the Association for Computational Linguistics (ACL), pages", "citeRegEx": "Tang et al\\.,? 2014", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team."], "venue": "arXiv e-prints, abs/1605.02688.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "Sentiment strength detection for the social web", "author": ["Mike Thelwall", "Kevan Buckley", "Georgios Paltoglou."], "venue": "Journal of the American Society for Information Science and Technology (JASIST), 63(1):163\u2013173.", "citeRegEx": "Thelwall et al\\.,? 2012", "shortCiteRegEx": "Thelwall et al\\.", "year": 2012}, {"title": "Sentiment strength detection in short informal text", "author": ["Mike Thelwall", "Kevan Buckley", "Georgios Paltoglou", "Di Cai", "Arvid Kappas."], "venue": "Journal of the American Society for Information Science and Technology, 61(12):2544\u20132558.", "citeRegEx": "Thelwall et al\\.,? 2010", "shortCiteRegEx": "Thelwall et al\\.", "year": 2010}, {"title": "A corpus for research on deliberation and debate", "author": ["Marilyn A Walker", "Jean E Fox Tree", "Pranav Anand", "Rob Abbott", "Joseph King."], "venue": "International Conference on Language Resources and Evaluation (LREC), pages 812\u2013817.", "citeRegEx": "Walker et al\\.,? 2012", "shortCiteRegEx": "Walker et al\\.", "year": 2012}, {"title": "How universal and specific is emotional experience? evidence from 27 countries on five continents", "author": ["Harald G Wallbott", "Klaus R Scherer."], "venue": "International Social Science Council, 25(4):763\u2013795.", "citeRegEx": "Wallbott and Scherer.,? 1986", "shortCiteRegEx": "Wallbott and Scherer.", "year": 1986}, {"title": "A survey on the role of negation in sentiment analysis", "author": ["Michael Wiegand", "Alexandra Balahur", "Benjamin Roth", "Dietrich Klakow", "Andr\u00e9s Montoyo."], "venue": "Workshop on Negation and Speculation in Natural Language Processing (NeSp-NLP), pages 60\u201368.", "citeRegEx": "Wiegand et al\\.,? 2010", "shortCiteRegEx": "Wiegand et al\\.", "year": 2010}, {"title": "The positive emoticons", "author": ["Hu"], "venue": null, "citeRegEx": "Hu,? \\Q2013\\E", "shortCiteRegEx": "Hu", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "For instance, the state-of-the-art approaches within sentiment analysis of social media data use positive/negative emoticons for training their models (Deriu et al., 2016; Tang et al., 2014).", "startOffset": 151, "endOffset": 190}, {"referenceID": 31, "context": "For instance, the state-of-the-art approaches within sentiment analysis of social media data use positive/negative emoticons for training their models (Deriu et al., 2016; Tang et al., 2014).", "startOffset": 151, "endOffset": 190}, {"referenceID": 19, "context": "Similarly, hashtags such as #anger, #joy, #happytweet, #ugh, #yuck and #fml have in previous research been mapped into emotional categories for emotion analysis (Mohammad, 2012).", "startOffset": 161, "endOffset": 177}, {"referenceID": 17, "context": "Kunneman et al. (2014) discuss a similar duality in the use of emotional hashtags such as #nice and #lame.", "startOffset": 0, "endOffset": 23}, {"referenceID": 23, "context": "Using emotional expressions as noisy labels in text to counter scarcity of labels is not a new idea (Read, 2005; Go et al., 2009).", "startOffset": 100, "endOffset": 129}, {"referenceID": 10, "context": "Using emotional expressions as noisy labels in text to counter scarcity of labels is not a new idea (Read, 2005; Go et al., 2009).", "startOffset": 100, "endOffset": 129}, {"referenceID": 19, "context": "Prior work has used theories of emotion such as Ekman\u2019s six basic emotions and Plutchik\u2019s eight basic emotions (Mohammad, 2012; Suttles and Ide, 2013).", "startOffset": 111, "endOffset": 150}, {"referenceID": 30, "context": "Prior work has used theories of emotion such as Ekman\u2019s six basic emotions and Plutchik\u2019s eight basic emotions (Mohammad, 2012; Suttles and Ide, 2013).", "startOffset": 111, "endOffset": 150}, {"referenceID": 7, "context": "Another way of automatically interpreting the emotional content of an emoji is to learn emoji embeddings from the words describing the emojisemantics in official emoji tables (Eisner et al., 2016).", "startOffset": 175, "endOffset": 196}, {"referenceID": 4, "context": "training on multiple datasets has shown promising results (Collobert and Weston, 2008).", "startOffset": 58, "endOffset": 86}, {"referenceID": 12, "context": "We use a variant of the Long Short-Term Memory (LSTM) model that has been successful at many NLP tasks (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014).", "startOffset": 103, "endOffset": 161}, {"referenceID": 29, "context": "We use a variant of the Long Short-Term Memory (LSTM) model that has been successful at many NLP tasks (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014).", "startOffset": 103, "endOffset": 161}, {"referenceID": 0, "context": "We use a simple approach inspired by (Bahdanau et al., 2014; Yang et al., 2016) with a single parameter pr.", "startOffset": 37, "endOffset": 79}, {"referenceID": 6, "context": "to use the network as a feature extractor (Donahue et al., 2014), where all layers in the model are frozen when fine-tuning on the target task except the last layer (hereafter referred to as the \u2018last\u2019 approach).", "startOffset": 42, "endOffset": 64}, {"referenceID": 8, "context": "Alternatively, another common approach is to use the pretrained model as an initialization (Erhan et al., 2010), where the full model is unfrozen (hereafter referred to as \u2018full\u2019).", "startOffset": 91, "endOffset": 111}, {"referenceID": 8, "context": "The sequential fine-tuning seems to have a regularizing effect similar to what has been examined with layer-wise training in the context of unsupervised learning (Erhan et al., 2010).", "startOffset": 162, "endOffset": 182}, {"referenceID": 26, "context": "Each time the model converges as measured on the validation set, the weights are reloaded to the best setting, thereby preventing overfitting in a similar manner to early stopping (Sj\u00f6berg and Ljung, 1995).", "startOffset": 180, "endOffset": 205}, {"referenceID": 15, "context": "For comparison we also train a version of our DeepMoji model with smaller LSTM layers and a bag-of-words classifier, fastText, that has recently shown competitive results (Joulin et al., 2016).", "startOffset": 171, "endOffset": 192}, {"referenceID": 27, "context": "Many recent papers proposing new methods for emotion analysis such as (Staiano and Guerini, 2014) only evaluate performance on a single benchmark dataset, SemEval 2007 Task 14, that contains 1250 observations.", "startOffset": 70, "endOffset": 97}, {"referenceID": 2, "context": "Recently, criticism has been raised concerning the use of correlation with continuous ratings as a measure (Buechel and Hahn, 2016), making only the somewhat limited binary evaluation possible.", "startOffset": 107, "endOffset": 131}, {"referenceID": 36, "context": "that we convert to a single-label classification task and a dataset of self-reported emotional experiences created by a large group of psychologists (Wallbott and Scherer, 1986).", "startOffset": 149, "endOffset": 177}, {"referenceID": 2, "context": "As these two datasets do not have prior evaluations, we evaluate against a state-of-the-art approach, which is based on a valence-arousal-dominance framework (Buechel and Hahn, 2016).", "startOffset": 158, "endOffset": 182}, {"referenceID": 34, "context": "Two of the datasets are from SentiStrength (Thelwall et al., 2010), SS-Twitter and SS-Youtube, and follow the relabeling described in (Saif et al.", "startOffset": 43, "endOffset": 66}, {"referenceID": 24, "context": ", 2010), SS-Twitter and SS-Youtube, and follow the relabeling described in (Saif et al., 2013) to make the labels binary.", "startOffset": 75, "endOffset": 94}, {"referenceID": 20, "context": "The third dataset is from SemEval 2016 Task4A (Nakov et al., 2016).", "startOffset": 46, "endOffset": 66}, {"referenceID": 15, "context": "observation that even bag-ofwords classifiers and unsupervised approaches can obtain a high accuracy (Joulin et al., 2016; Radford et al., 2017).", "startOffset": 101, "endOffset": 144}, {"referenceID": 22, "context": "observation that even bag-ofwords classifiers and unsupervised approaches can obtain a high accuracy (Joulin et al., 2016; Radford et al., 2017).", "startOffset": 101, "endOffset": 144}, {"referenceID": 5, "context": "The current state of the art for sentiment analysis on social media (and winner of SemEval 2016 Task 4A) uses an ensemble of convolutional neural networks that are pretrained on a private dataset of tweets with emoticons, making it difficult to replicate (Deriu et al., 2016).", "startOffset": 255, "endOffset": 275}, {"referenceID": 31, "context": "We also implemented the SentimentSpecific Word Embedding (SSWE) using the embeddings available on the authors\u2019 website (Tang et al., 2014), but found that it performed worse", "startOffset": 119, "endOffset": 138}, {"referenceID": 5, "context": "The current state of the art for sentiment analysis on social media (and winner of SemEval 2016 Task 4A) uses an ensemble of convolutional neural networks that are pretrained on a private dataset of tweets with emoticons, making it difficult to replicate (Deriu et al., 2016). Instead we pretrain a model with the hyperparameters of the largest model in their ensemble on the positive/negative emoticon dataset from Go et al. (2009). Using this pretraining as an initialization we finetune the model on the target tasks using early stopping on a validation set to determine the amount of training.", "startOffset": 256, "endOffset": 433}, {"referenceID": 28, "context": "SE0714 (Strapparava and Mihalcea, 2007) Emotion Headlines 3 250 1000 Olympic (Sintsova et al.", "startOffset": 7, "endOffset": 39}, {"referenceID": 25, "context": "SE0714 (Strapparava and Mihalcea, 2007) Emotion Headlines 3 250 1000 Olympic (Sintsova et al., 2013) Emotion Tweets 4 250 709 PsychExp (Wallbott and Scherer, 1986) Emotion Experiences 7 1000 6480", "startOffset": 77, "endOffset": 100}, {"referenceID": 36, "context": ", 2013) Emotion Tweets 4 250 709 PsychExp (Wallbott and Scherer, 1986) Emotion Experiences 7 1000 6480", "startOffset": 42, "endOffset": 70}, {"referenceID": 33, "context": "SS-Twitter (Thelwall et al., 2012) Sentiment Tweets 2 1000 1113 SS-Youtube (Thelwall et al.", "startOffset": 11, "endOffset": 34}, {"referenceID": 33, "context": ", 2012) Sentiment Tweets 2 1000 1113 SS-Youtube (Thelwall et al., 2012) Sentiment Video Comments 2 1000 1142 SE1604 (Nakov et al.", "startOffset": 48, "endOffset": 71}, {"referenceID": 20, "context": ", 2012) Sentiment Video Comments 2 1000 1142 SE1604 (Nakov et al., 2016) Sentiment Tweets 3 7155 31986", "startOffset": 52, "endOffset": 72}, {"referenceID": 35, "context": "SCv1 (Walker et al., 2012) Sarcasm Debate Forums 2 1000 995 SCv2-GEN (Oraby et al.", "startOffset": 5, "endOffset": 26}, {"referenceID": 21, "context": ", 2012) Sarcasm Debate Forums 2 1000 995 SCv2-GEN (Oraby et al., 2016) Sarcasm Debate Forums 2 1000 2260", "startOffset": 50, "endOffset": 70}, {"referenceID": 35, "context": "For sarcasm detection we use the sarcasm dataset version 1 and 2 from the Internet Argument Corpus (Walker et al., 2012).", "startOffset": 99, "endOffset": 120}, {"referenceID": 18, "context": "GoogleNews word2vec embeddings (Mikolov et al., 2013) are used for computing the embedding-based features.", "startOffset": 31, "endOffset": 53}, {"referenceID": 19, "context": "Oraby et al. (2016) are not directly comparable as only a subset of the data is available online.", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": "3 A state-of-the-art baseline is found by modeling the embedding-based features from Joshi et al. (2016) alongside unigrams, bigrams and trigrams with an SVM.", "startOffset": 85, "endOffset": 105}, {"referenceID": 16, "context": "For training we use the Adam optimizer (Kingma and Ba, 2015) with gradient clipping of the norm to 1.", "startOffset": 39, "endOffset": 60}, {"referenceID": 9, "context": "(Gal and Ghahramani, 2016) we do not drop out entire words in the input as some of our datasets contain observations with so few words that it could change the meaning of the text.", "startOffset": 0, "endOffset": 26}, {"referenceID": 21, "context": "the board posts from the Internet Argument Corpus version 1 (Oraby et al., 2016) has an average of 66 tokens with some observations being much longer.", "startOffset": 60, "endOffset": 80}, {"referenceID": 5, "context": "For instance, both Deriu et al. (2016) and Tang et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 5, "context": "For instance, both Deriu et al. (2016) and Tang et al. (2014) only used positive and negative emoticons as noisy labels.", "startOffset": 19, "endOffset": 62}, {"referenceID": 5, "context": "For instance, both Deriu et al. (2016) and Tang et al. (2014) only used positive and negative emoticons as noisy labels. Other instances of previous work have used slightly more nuanced sets of noisy labels (see \u00a72), but to our knowledge our set of noisy labels is the most diverse yet. To analyze the effect of using a diverse emoji set we create a subset of our pretraining data containing tweets with one of 8 emojis that are similar to the positive/negative emoticons used by Tang et al. (2014) and Hu et al.", "startOffset": 19, "endOffset": 499}, {"referenceID": 5, "context": "For instance, both Deriu et al. (2016) and Tang et al. (2014) only used positive and negative emoticons as noisy labels. Other instances of previous work have used slightly more nuanced sets of noisy labels (see \u00a72), but to our knowledge our set of noisy labels is the most diverse yet. To analyze the effect of using a diverse emoji set we create a subset of our pretraining data containing tweets with one of 8 emojis that are similar to the positive/negative emoticons used by Tang et al. (2014) and Hu et al. (2013) (the set of emoticons and corresponding emojis are available in the supplemental material).", "startOffset": 19, "endOffset": 520}, {"referenceID": 11, "context": "level features for any time step, making it easy to use this information if needed for a new task b) the improved gradient-flow from the output layer to the early layers in the network due to skipconnections (Graves, 2013) is important when adjusting parameters in early layers as part of transfer learning to small datasets.", "startOffset": 208, "endOffset": 222}, {"referenceID": 37, "context": "One concept that the LSTM layers likely learn is negation, which is known to be important for sentiment analysis (Wiegand et al., 2010).", "startOffset": 113, "endOffset": 135}], "year": 2017, "abstractText": "NLP tasks are often limited by scarcity of manually annotated data. In social media sentiment analysis and related tasks, researchers have therefore used binarized emoticons and specific hashtags as forms of distant supervision. Our paper shows that by extending the distant supervision to a more diverse set of noisy labels, the models can learn richer representations. Through emoji prediction on a dataset of 1246 million tweets containing one of 64 common emojis we obtain state-of-theart performance on 8 benchmark datasets within sentiment, emotion and sarcasm detection using a single pretrained model. Our analyses confirm that the diversity of our emotional labels yield a performance improvement over previous distant supervision approaches.", "creator": "LaTeX with hyperref package"}}}