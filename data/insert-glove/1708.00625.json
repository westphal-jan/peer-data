{"id": "1708.00625", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Aug-2017", "title": "Deep Recurrent Generative Decoder for Abstractive Text Summarization", "abstract": "caverly We pallisa propose gnsr a new 235.8 framework for mignard abstractive zucchi text g\u00f6rlitz summarization besting based wizna on a egede-nissen sequence - hsf to - palanka sequence oriented encoder - decoder model decapitates equipped andrejevs with a ashbee deep recurrent generative decoder (3,633 DRGN ).", "histories": [["v1", "Wed, 2 Aug 2017 07:47:14 GMT  (413kb,D)", "http://arxiv.org/abs/1708.00625v1", "10 pages, EMNLP 2017"]], "COMMENTS": "10 pages, EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["piji li", "wai lam", "lidong bing", "zihao wang"], "accepted": true, "id": "1708.00625"}, "pdf": {"name": "1708.00625.pdf", "metadata": {"source": "CRF", "title": "Deep Recurrent Generative Decoder for Abstractive Text Summarization\u2217", "authors": ["Piji Li", "Wai Lam", "Lidong Bing", "Zihao Wang"], "emails": ["zhwang}@se.cuhk.edu.hk,", "lyndonbing@tencent.com", "@POTUS", "@POTUS"], "sections": [{"heading": "1 Introduction", "text": "Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document (Edmundson, 1969; Luhn, 1958; Nenkova and McKeown, 2012). Different from the common extraction-based and compression-based methods, abstraction-based methods aim at constructing new sentences as summaries, thus they require a deeper understanding of the text and the capability of generating new sentences, which provide an obvious advantage in improving the focus of a summary, reducing the redundancy, and keeping a good compression rate (Bing et al., 2015; Rush et al., 2015; Nallapati et al., 2016).\n\u2217The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414).\nSome previous research works show that human-written summaries are more abstractive (Jing and McKeown, 2000). Moreover, our investigation reveals that people may naturally follow some inherent structures when they write the abstractive summaries. To illustrate this observation, we show some examples in Figure 1, which are some top story summaries or headlines from the channel \u201cTechnology\u201d of CNN. After analyzing the summaries carefully, we can find some common structures from them, such as \u201cWhat\u201d, \u201cWhat-Happened\u201d , \u201cWho Action What\u201d, etc. For example, the summary \u201cApple sues Qualcomm for nearly $1 billion\u201d can be structuralized as \u201cWho (Apple) Action (sues) What (Qualcomm)\u201d. Similarly, the summaries \u201c[Twitter] [fixes] [botched @POTUS account transfer]\u201d, \u201c[Uber] [to pay] [$20 million] for misleading drivers\u201d, and \u201c[Bipartisan bill] aims to [reform] [H-1B visa system]\u201d also follow the structure of \u201cWho Action What\u201d. The summary \u201cThe emergence of the \u2018cyber cold war\u201d\u2019 matches with the structure of \u201cWhat\u201d, and the summary \u201cSt. Louis\u2019 public library computers hacked\u201d follows ar X iv :1 70 8. 00 62\n5v 1\n[ cs\n.C L\n] 2\nA ug\n2 01\n7\nthe structure of \u201cWhat-Happened\u201d.\nIntuitively, if we can incorporate the latent structure information of summaries into the abstractive summarization model, it will improve the quality of the generated summaries. However, very few existing works specifically consider the latent structure information of summaries in their summarization models. Although a very popular neural network based sequence-to-sequence (seq2seq) framework has been proposed to tackle the abstractive summarization problem (Lopyrev, 2015; Rush et al., 2015; Nallapati et al., 2016), the calculation of the internal decoding states is entirely deterministic. The deterministic transformations in these discriminative models lead to limitations on the representation ability of the latent structure information. Miao and Blunsom (2016) extended the seq2seq framework and proposed a generative model to capture the latent summary information, but they did not consider the recurrent dependencies in their generative model leading to limited representation ability.\nTo tackle the above mentioned problems, we design a new framework based on sequenceto-sequence oriented encoder-decoder model equipped with a latent structure modeling component. We employ Variational Auto-Encoders (VAEs) (Kingma and Welling, 2013; Rezende et al., 2014) as the base model for our generative framework which can handle the inference problem associated with complex generative modeling. However, the standard framework of VAEs is not designed for sequence modeling related tasks. Inspired by (Chung et al., 2015), we add historical dependencies on the latent variables of VAEs and propose a deep recurrent generative decoder (DRGD) for latent structure modeling. Then the standard discriminative deterministic decoder and the recurrent generative decoder are integrated into a unified decoding framework. The target summaries will be decoded based on both the discriminative deterministic variables and the generative latent structural information. All the neural parameters are learned by back-propagation in an end-to-end training paradigm.\nThe main contributions of our framework are summarized as follows: (1) We propose a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent generative decoder (DRGD) to model and learn the latent\nstructure information implied in the target summaries of the training data. Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables. (2) Both the generative latent structural information and the discriminative deterministic variables are jointly considered in the generation process of the abstractive summaries. (3) Experimental results on some benchmark datasets in different languages show that our framework achieves better performance than the state-of-the-art models."}, {"heading": "2 Related Works", "text": "Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document (Nenkova and McKeown, 2012). Traditionally, the summarization methods can be classified into three categories: extraction-based methods (Erkan and Radev, 2004; Goldstein et al., 2000; Wan et al., 2007; Min et al., 2012; Nallapati et al., 2017; Cheng and Lapata, 2016; Cao et al., 2016; Song et al., 2017), compression-based methods (Li et al., 2013; Wang et al., 2013; Li et al., 2015, 2017), and abstraction-based methods. In fact, previous investigations show that human-written summaries are more abstractive (Barzilay and McKeown, 2005; Bing et al., 2015). Abstraction-based approaches can generate new sentences based on the facts from different source sentences. Barzilay and McKeown (2005) employed sentence fusion to generate a new sentence. Bing et al. (2015) proposed a more fine-grained fusion framework, where new sentences are generated by selecting and merging salient phrases. These methods can be regarded as a kind of indirect abstractive summarization, and complicated constraints are used to guarantee the linguistic quality.\nRecently, some researchers employ neural network based framework to tackle the abstractive summarization problem. Rush et al. (2015) proposed a neural network based model with local attention modeling, which is trained on the Gigaword corpus, but combined with an additional loglinear extractive summarization model with handcrafted features. Gu et al. (2016) integrated a copying mechanism into a seq2seq framework to improve the quality of the generated summaries. Chen et al. (2016) proposed a new attention mechanism that not only considers the important source\nsegments, but also distracts them in the decoding step in order to better grasp the overall meaning of input documents. Nallapati et al. (2016) utilized a trick to control the vocabulary size to improve the training efficiency. The calculations in these methods are all deterministic and the representation ability is limited. Miao and Blunsom (2016) extended the seq2seq framework and proposed a generative model to capture the latent summary information, but they do not consider the recurrent dependencies in their generative model leading to limited representation ability.\nSome research works employ topic models to capture the latent information from source documents or sentences. Wang et al. (2009) proposed a new Bayesian sentence-based topic model by making use of both the term-document and termsentence associations to improve the performance of sentence selection. Celikyilmaz and HakkaniTur (2010) estimated scores for sentences based on their latent characteristics using a hierarchical topic model, and trained a regression model to extract sentences. However, they only use the latent topic information to conduct the sentence salience estimation for extractive summarization. In contrast, our purpose is to model and learn the latent structure information from the target summaries and use it to enhance the performance of abstractive summarization."}, {"heading": "3 Framework Description", "text": ""}, {"heading": "3.1 Overview", "text": "As shown in Figure 2, the basic framework of our approach is a neural network based encoderdecoder framework for sequence-to-sequence learning. The input is a variable-length sequence X = {x1,x2, . . . ,xm} representing the source text. The word embedding xt is initialized randomly and learned during the optimization process. The output is also a sequence Y = {y1,y2, . . . ,yn}, which represents the generated abstractive summaries. Gated Recurrent Unit (GRU) (Cho et al., 2014) is employed as the basic sequence modeling component for the encoder and the decoder. For latent structure modeling, we add historical dependencies on the latent variables of Variational Auto-Encoders (VAEs) and propose a deep recurrent generative decoder (DRGD) to distill the complex latent structures implied in the target summaries of the training data. Finally, the abstractive summaries will be decoded out based\non both the discriminative deterministic variables H and the generative latent structural information Z."}, {"heading": "3.2 Recurrent Generative Decoder", "text": "Assume that we have obtained the source text representation he \u2208 Rkh . The purpose of the decoder is to translate this source code he into a series of hidden states {hd1,hd2, . . . ,hdn}, and then revert these hidden states to an actual word sequence and generate the summary.\nFor standard recurrent decoders, at each time step t, the hidden state hdt \u2208 Rkh is calculated using the dependent input symbol yt\u22121 \u2208 Rkw and the previous hidden state hdt\u22121:\nhdt = f(yt\u22121,h d t\u22121) (1)\nwhere f(\u00b7) is a recurrent neural network such as vanilla RNN, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), and Gated Recurrent Unit (GRU) (Cho et al., 2014). No matter which one we use for f(\u00b7), the common transformation operation is as follows:\nhdt = g(W d yhyt\u22121 +W d hhh d t\u22121 + b d h) (2)\nwhere Wdyh \u2208 Rkh\u00d7kw and Wdhh \u2208 Rkh\u00d7kh are the linear transformation matrices. bdh is the bias. kh is the dimension of the hidden layers, and kw is the dimension of the word embeddings. g(\u00b7) is the non-linear activation function. From Equation 2, we can see that all the transformations are deterministic, which leads to a deterministic recurrent hidden state hdt . From our investigations, we find that the representational power of such deterministic variables are limited. Some more complex latent structures in the target summaries, such as the high-level syntactic features and latent topics, cannot be modeled effectively by the deterministic operations and variables.\nRecently, a generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2013; Rezende et al., 2014) shows strong capability in modeling latent random variables and improves the performance of tasks in different fields such as sentence generation (Bowman et al., 2016) and image generation (Gregor et al., 2015). However, the standard VAEs is not designed for modeling sequence directly. Inspired by (Chung et al., 2015), we extend the standard VAEs by\nintroducing the historical latent variable dependencies to make it be capable of modeling sequence data. Our proposed latent structure modeling framework can be viewed as a sequence generative model which can be divided into two parts: inference (variational-encoder) and generation (variational-decoder). As shown in the decoder component of Figure 2, the input of the original VAEs only contains the observed variable yt, and the variational-encoder can map it to a latent variable z \u2208 Rkz , which can be used to reconstruct the original input. For the task of summarization, in the sequence decoder component, the previous latent structure information needs to be considered for constructing more effective representations for the generation of the next state.\nFor the inference stage, the variational-encoder can map the observed variable y<t and the previous latent structure information z<t to the posterior probability distribution of the latent structure variable p\u03b8(zt|y<t, z<t). It is obvious that this is a recurrent inference process in which zt contains the historical dynamic latent structure information. Compared with the variational inference process p\u03b8(zt|yt) of the typical VAEs model, the recurrent framework can extract more complex and effective latent structure features implied in the sequence data.\nFor the generation process, based on the latent structure variable zt, the target word yt at the time step t is drawn from a conditional probability distribution p\u03b8(yt|zt). The target is to maximize the probability of each generated summary y = {y1,y2, . . . ,yT } based on the generation process according to:\np\u03b8(y) = T\u220f t=1 \u222b p\u03b8(yt|zt)p\u03b8(zt)dzt (3)\nFor the purpose of solving the intractable integral of the marginal likelihood as shown in Equation 3, a recognition model q\u03c6(zt|y<t, z<t) is introduced as an approximation to the intractable true posterior p\u03b8(zt|y<t, z<t). The recognition model parameters \u03c6 and the generative model parameters \u03b8 can be learned jointly. The aim is to reduce the Kulllback-Leibler divergence (KL) between q\u03c6(zt|y<t, z<t) and p\u03b8(zt|y<t, z<t):\nDKL[q\u03c6(zt|y<t, z<t)\u2016p\u03b8(zt|y<t, z<t)]\n= \u222b z q\u03c6(zt|y<t, z<t) log q\u03c6(zt|y<t, z<t) p\u03b8(zt|y<t, z<t) dz = Eq\u03c6(zt|y<t,z<t)[log q\u03c6(zt|\u00b7)\u2212 log p\u03b8(zt|\u00b7)]\nwhere \u00b7 denotes the conditional variables y<t and z<t. Bayes rule is applied to p\u03b8(zt|y<t, z<t), and we can extract log p\u03b8(z) from the expectation, transfer the expectation term Eq\u03c6(zt|y<t,z<t) back to KL-divergence, and rearrange all the terms. Consequently the following holds:\nlog p\u03b8(y<t) = DKL[q\u03c6(zt|y<t, z<t)\u2016p\u03b8(zt|y<t, z<t)] + Eq\u03c6(zt|y<t,z<t)[log p\u03b8(y<t|zt)] \u2212DKL[q\u03c6(zt|y<t, z<t)\u2016p\u03b8(zt)]\n(4)\nLetL(\u03b8, \u03c6; y) represent the last two terms from the right part of Equation 4:\nL(\u03b8, \u03d5; y) = Eq\u03c6(zt|y<t,z<t) {\u2211T\nt=1 log p\u03b8(yt|zt) \u2212DKL[q\u03c6(zt|y<t, z<t)\u2016p\u03b8(zt)] } (5)\nSince the first KL-divergence term of Equation 4 is non-negative, we have log p\u03b8(y<t) \u2265 L(\u03b8, \u03c6; y) meaning that L(\u03b8, \u03c6; y) is a lower bound (the objective to be maximized) on the marginal likelihood. In order to differentiate and optimize the\nlower bound L(\u03b8, \u03c6; y), following the core idea of VAEs, we use a neural network framework for the probabilistic encoder q\u03c6(zt|y<t, z<t) for better approximation."}, {"heading": "3.3 Abstractive Summary Generation", "text": "We also design a neural network based framework to conduct the variational inference and generation for the recurrent generative decoder component similar to some design in previous works (Kingma and Welling, 2013; Rezende et al., 2014; Gregor et al., 2015). The encoder component and the decoder component are integrated into a unified abstractive summarization framework. Considering that GRU has comparable performance but with less parameters and more efficient computation, we employ GRU as the basic recurrent model which updates the variables according to the following operations:\nrt = \u03c3(Wxrxt +Whrht\u22121 + br) zt = \u03c3(Wxzxt +Whzht\u22121 + bz) gt = tanh(Wxhxt +Whh(rt ht\u22121) + bh) ht = zt ht\u22121 + (1\u2212 zt) gt\nwhere rt is the reset gate, zt is the update gate. denotes the element-wise multiplication. tanh is the hyperbolic tangent activation function.\nAs shown in the left block of Figure 2, the encoder is designed based on bidirectional recurrent neural networks. Let xt be the word embedding vector of the t-th word in the source sequence. GRU maps xt and the previous hidden state ht\u22121 to the current hidden state ht in feed-forward direction and back-forward direction respectively:\n\u21c0 ht = GRU(xt, \u21c0\nht\u22121) \u21bc ht = GRU(xt, \u21bc ht\u22121) (6)\nThen the final hidden state het \u2208 R2kh is concatenated using the hidden states from the two directions: het = \u21c0 ht|| \u21bc\nh. As shown in the middle block of Figure 2, the decoder consists of two components: discriminative deterministic decoding and generative latent structure modeling.\nThe discriminative deterministic decoding is an improved attention modeling based recurrent sequence decoder. The first hidden state hd1 is initialized using the average of all the source input\nstates: hd1 = 1 T e T e\u2211 t=1 het , where h e t is the source input hidden state. T e is the input sequence length.\nThe deterministic decoder hidden state hdt is calculated using two layers of GRUs. On the first layer, the hidden state is calculated only using the current input word embedding yt\u22121 and the previous hidden state hd1t\u22121:\nhd1t = GRU1(yt\u22121,h d1 t\u22121) (7)\nwhere the superscript d1 denotes the first decoder GRU layer. Then the attention weights at the time step t are calculated based on the relationship of hd1t and all the source hidden states {het}. Let ai,j be the attention weight between hd1i and h e j , which can be calculated using the following formulation:\nai,j = exp(ei,j)\u2211T e j\u2032=1 exp(ei,j\u2032) ei,j = v T tanh(Wdhhh d1 i +W e hhh e j + ba)\nwhere Wdhh \u2208 Rkh\u00d7kh , Wehh \u2208 Rkh\u00d72kh , ba \u2208 Rkh , and v \u2208 Rkh . The attention context is obtained by the weighted linear combination of all the source hidden states:\nct = \u2211T e\nj\u2032=1 at,j\u2032h\ne j\u2032 (8)\nThe final deterministic hidden state hd2t is the output of the second decoder GRU layer, jointly considering the word yt\u22121, the previous hidden state hd2t\u22121, and the attention context ct:\nhd2t = GRU2(yt\u22121,h d2 t\u22121, ct) (9)\nFor the component of recurrent generative model, inspired by some ideas in previous works (Kingma and Welling, 2013; Rezende et al., 2014; Gregor et al., 2015), we assume that both the prior and posterior of the latent variables are Gaussian, i.e., p\u03b8(zt) = N (0, I) and q\u03c6(zt|y<t, z<t) = N (zt;\u00b5,\u03c32I), where \u00b5 and \u03c3 denote the variational mean and standard deviation respectively, which can be calculated via a multilayer perceptron. Precisely, given the word embedding yt\u22121, the previous latent structure variable zt\u22121, and the previous deterministic hidden state hdt\u22121, we first project it to a new hidden space:\nhezt = g(W ez yhyt\u22121+W ez zhzt\u22121+W ez hhh d t\u22121+b ez h )\nwhere Wezyh \u2208 R kh\u00d7kw , Wezzh \u2208 R kh\u00d7kz , Wezhh \u2208 Rkh\u00d7kh , and bezh \u2208 R\nkh . g is the sigmoid activation function: \u03c3(x) = 1/(1 + e\u2212x). Then the\nGaussian parameters \u00b5t \u2208 Rkz and \u03c3t \u2208 Rkz can be obtained via a linear transformation based on hezt :\n\u00b5t = W ez h\u00b5h ez t + b ez \u00b5\nlog(\u03c32t ) = Wh\u03c3h ez t + b ez \u03c3\n(10)\nThe latent structure variable zt \u2208 Rkz can be calculated using the reparameterization trick:\n\u03b5 \u223c N (0, I), zt = \u00b5t + \u03c3t \u2297 \u03b5 (11)\nwhere \u03b5 \u2208 Rkz is an auxiliary noise variable. The process of inference for finding zt based on neural networks can be teated as a variational encoding process.\nTo generate summaries precisely, we first integrate the recurrent generative decoding component with the discriminative deterministic decoding component, and map the latent structure variable zt and the deterministic decoding hidden state hd2t to a new hidden variable:\nh dy t = tanh(W dy zhzt +W dz hhh d2 t + b dy h ) (12)\nGiven the combined decoding state hdyt at the time t, the probability of generating any target word yt is given as follows:\nyt = \u03c2(W d hyh dy t + b d hy) (13)\nwhere Wdhy \u2208 Rky\u00d7kh and bdhy \u2208 Rky . \u03c2(\u00b7) is the softmax function. Finally, we use a beam search algorithm (Koehn, 2004) for decoding and generating the best summary."}, {"heading": "3.4 Learning", "text": "Although the proposed model contains a recurrent generative decoder, the whole framework is fully differentiable. As shown in Section 3.3, both the recurrent deterministic decoder and the recurrent generative decoder are designed based on neural networks. Therefore, all the parameters in our model can be optimized in an end-to-end paradigm using back-propagation. We use {X}N and {Y }N to denote the training source and target sequence. Generally, the objective of our framework consists of two terms. One term is the negative loglikelihood of the generated summaries, and the other one is the variational lower boundL(\u03b8, \u03c6;Y ) mentioned in Equation 5. Since the variational lower bound L(\u03b8, \u03c6;Y ) also contains a likelihood term, we can merge it with the likelihood term of summaries. The final objective function, which\nneeds to be minimized, is formulated as follows:\nJ = 1 N N\u2211 n=1 T\u2211 t=1\n{ \u2212 log [ p(y\n(n) t |y (n) <t , X (n))\n]\n+DKL [ q\u03c6(z (n) t |y (n) <t , z (n) <t )\u2016p\u03b8(z (n) t ) ]} (14)"}, {"heading": "4 Experimental Setup", "text": ""}, {"heading": "4.1 Datesets", "text": "We train and evaluate our framework on three popular datasets. Gigawords is an English sentence summarization dataset prepared based on Annotated Gigawords1 by extracting the first sentence from articles with the headline to form a sourcesummary pair. We directly download the prepared dataset used in (Rush et al., 2015). It roughly contains 3.8M training pairs, 190K validation pairs, and 2,000 test pairs. DUC-20042 is another English dataset only used for testing in our experiments. It contains 500 documents. Each document contains 4 model summaries written by experts. The length of the summary is limited to 75 bytes. LCSTS is a large-scale Chinese short text summarization dataset, consisting of pairs of (short text, summary) collected from Sina Weibo3 (Hu et al., 2015). We take Part-I as the training set, Part-II as the development set, and Part-III as the test set. There is a score in range 1 \u223c 5 labeled by human to indicate how relevant an article and its summary is. We only reserve those pairs with scores no less than 3. The size of the three sets are 2.4M, 8.7k, and 725 respectively. In our experiments, we only take Chinese character sequence as input, without performing word segmentation."}, {"heading": "4.2 Evaluation Metrics", "text": "We use ROUGE score (Lin, 2004) as our evaluation metric with standard options. The basic idea of ROUGE is to count the number of overlapping units between generated summaries and the reference summaries, such as overlapped n-grams, word sequences, and word pairs. F-measures of ROUGE-1 (R-1), ROUGE-2 (R-2), ROUGE-L (RL) and ROUGE-SU4 (R-SU4) are reported."}, {"heading": "4.3 Comparative Methods", "text": "We compare our model with some baselines and state-of-the-art methods. Because the datasets are\n1https://catalog.ldc.upenn.edu/ldc2012t21 2http://duc.nist.gov/duc2004 3http://www.weibo.com\nquite standard, so we just extract the results from their papers. Therefore the baseline methods on different datasets may be slightly different.\n\u2022 TOPIARY (Zajic et al., 2004) is the best on DUC2004 Task-1 for compressive text summarization. It combines a system using linguistic based transformations and an unsupervised topic detection algorithm for compressive text summarization.\n\u2022 MOSES+ (Rush et al., 2015) uses a phrasebased statistical machine translation system trained on Gigaword to produce summaries. It also augments the phrase table with \u201cdeletion\u201d rulesto improve the baseline performance, and MERT is also used to improve the quality of generated summaries.\n\u2022 ABS and ABS+ (Rush et al., 2015) are both the neural network based models with local attention modeling for abstractive sentence summarization. ABS+ is trained on the Gigaword corpus, but combined with an additional log-linear extractive summarization model with handcrafted features.\n\u2022 RNN and RNN-context (Hu et al., 2015) are two seq2seq architectures. RNN-context integrates attention mechanism to model the context.\n\u2022 CopyNet (Gu et al., 2016) integrates a copying mechanism into the sequence-tosequence framework.\n\u2022 RNN-distract (Chen et al., 2016) uses a new attention mechanism by distracting the historical attention in the decoding steps.\n\u2022 RAS-LSTM and RAS-Elman (Chopra et al., 2016) both consider words and word positions as input and use convolutional encoders to handle the source information. For the attention based sequence decoding process, RAS-Elman selects Elman RNN (Elman, 1990) as decoder, and RAS-LSTM selects Long Short-Term Memory architecture (Hochreiter and Schmidhuber, 1997).\n\u2022 LenEmb (Kikuchi et al., 2016) uses a mechanism to control the summary length by considering the length embedding vector as the input.\n\u2022 ASC+FSC1 (Miao and Blunsom, 2016) uses a generative model with attention mechanism to conduct the sentence compression problem. The model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary.\n\u2022 lvt2k-1sent and lvt5k-1sent (Nallapati et al., 2016) utilize a trick to control the vocabulary size to improve the training efficiency."}, {"heading": "4.4 Experimental Settings", "text": "For the experiments on the English dataset Gigawords, we set the dimension of word embeddings to 300, and the dimension of hidden states and latent variables to 500. The maximum length of documents and summaries is 100 and 50 respectively. The batch size of mini-batch training is 256. For DUC-2004, the maximum length of summaries is 75 bytes. For the dataset of LCSTS, the dimension of word embeddings is 350. We also set the dimension of hidden states and latent variables to 500. The maximum length of documents and summaries is 120 and 25 respectively, and the batch size is also 256. The beam size of the decoder was set to be 10. Adadelta (Schmidhuber, 2015) with hyperparameter \u03c1 = 0.95 and = 1e \u2212 6 is used for gradient based optimization. Our neural network based framework is implemented using Theano (Theano Development Team, 2016)."}, {"heading": "5 Results and Discussions", "text": ""}, {"heading": "5.1 ROUGE Evaluation", "text": "We first depict the performance of our model DRGD by comparing to the standard decoders (StanD) of our own implementation. The comparison results on the validation datasets of Gigawords and LCSTS are shown in Table 1. From the results we can see that our proposed generative decoders DRGD can obtain obvious improvements on abstractive summarization than the standard decoders. Actually, the performance of the standard\ndecoders is similar with those mentioned popular baseline methods.\nThe results on the English datasets of Gigawords and DUC-2004 are shown in Table 2 and Table 3 respectively. Our model DRGD achieves the best summarization performance on all the ROUGE metrics. Although ASC+FSC1 also uses a generative method to model the latent summary variables, the representation ability is limited and it cannot bring in noticeable improvements. It is worth noting that the methods lvt2k-1sent and lvt5k-1sent (Nallapati et al., 2016) utilize linguistic features such as parts-of-speech tags, namedentity tags, and TF and IDF statistics of the words as part of the document representation. In fact, extracting all such features is a time consuming work, especially on large-scale datasets such as\nGigawords. lvt2k and lvt5k are not end-to-end style models and are more complicated than our model in practical applications.\nThe results on the Chinese dataset LCSTS are shown in Table 4. Our model DRGD also achieves the best performance. Although CopyNet employs a copying mechanism to improve the summary quality and RNN-distract considers attention information diversity in their decoders, our model is still better than those two methods demonstrating that the latent structure information learned from target summaries indeed plays a role in abstractive summarization. We also believe that integrating the copying mechanism and coverage diversity in our framework will further improve the summarization performance."}, {"heading": "5.2 Summary Case Analysis", "text": "In order to analyze the reasons of improving the performance, we compare the generated summaries by DRGD and the standard decoders StanD used in some other works such as (Chopra et al., 2016). The source texts, golden summaries, and the generated summaries are shown in Table 5. From the cases we can observe that DRGD can indeed capture some latent structures which are consistent with the golden summaries. For example, our result for S(1) \u201cWuhan wins men\u2019s soccer title at Chinese city games\u201d matches the \u201cWho Action What\u201d structure. However, the standard decoder StanD ignores the latent structures and generates some loose sentences, such as the results for S(1) \u201cResults of men\u2019s volleyball at Chinese city games\u201d does not catch the main points. The reason is that the recurrent variational auto-encoders used in our framework have better representation ability and can capture more effective and complicated latent structures from the sequence data. Therefore, the summaries generated by DRGD have consistent latent structures with the ground truth, leading to a better ROUGE evaluation."}, {"heading": "6 Conclusions", "text": "We propose a deep recurrent generative decoder (DRGD) to improve the abstractive summarization performance. The model is a sequenceto-sequence oriented encoder-decoder framework equipped with a latent structure modeling component. Abstractive summaries are generated based on both the latent variables and the deterministic states. Extensive experiments on benchmark\ndatasets show that DRGD achieves improvements over the state-of-the-art methods."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We propose a new framework for abstractive text summarization based on a sequence-to-sequence oriented encoderdecoder model equipped with a deep recurrent generative decoder (DRGN). Latent structure information implied in the target summaries is learned based on a recurrent latent random model for improving the summarization quality. Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables. Abstractive summaries are generated based on both the generative latent variables and the discriminative deterministic states. Extensive experiments on some benchmark datasets in different languages show that DRGN achieves improvements over the state-ofthe-art methods.", "creator": "LaTeX with hyperref package"}}}