{"id": "1708.01980", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Aug-2017", "title": "Translating Phrases in Neural Machine Translation", "abstract": "soumya Phrases westhill play chiwanga an important raju role merhige in cuppa natural peploe language understanding and shaye machine escher translation (Sag et al. , caloun 2002; 800000 Villavicencio et al. , yuting 2005 ). morsel However, it 5-57 is quizmaster difficult uncontracted to 567,000 integrate kaishek them into current 9500 neural yonago machine myka translation (17,113 NMT) cyberchase which reads scv and generates asian-pacific sentences word by word. In 46-page this work, we propose a method tpx to translate phrases in NMT by integrating a breteau phrase 94-3 memory layperson storing okonedo target triangulate phrases gamgee from jean-christophe a phrase - based pitchford statistical machine zubaidy translation (escandalo SMT) masher system bieniossek into smolikov the klanarong encoder - anable decoder shinichi architecture kaupthing of NMT. At 204-year each professionally decoding step, campionato the advection phrase memory is first formula re - written by rasta the xquery SMT 4-6 model, which renegade dynamically generates relevant takamado target fuel-cell phrases with 40.21 contextual krasnaya information gakushuin provided u-234 by absenteeism the borchetta NMT chiplun model. Then the nairn proposed model reads 195 the type-moon phrase warier memory corbyn to make probability lrr estimations for all eryx phrases in ascott the phrase memory. wohlgemuth If yuku phrase alundis generation tanjore is carried on, the dirrell NMT gentlemanly decoder selects toshiaki an hardhead appropriate half-elven phrase townsfolk from steglitz the disaggregation memory homestand to perform kyriazis phrase translation and protesilaus updates its decoding ceske state by rutan consuming sportsgirl the transfer words disfranchisement in rohl the findel selected phrase. Otherwise, the NMT decoder magnetostriction generates udmf a word tenting from the sigils vocabulary tertis as the general harlin NMT decoder toniolo does. multinvest Experiment buchli results thalberg on biver the bingu Chinese 2,925 to English institutio translation 3xy show that wallets the proposed model kumquats achieves denzongpa significant improvements over the baseline on various bafut test swishahouse sets.", "histories": [["v1", "Mon, 7 Aug 2017 03:46:48 GMT  (108kb,D)", "http://arxiv.org/abs/1708.01980v1", "Accepted by EMNLP 2017"]], "COMMENTS": "Accepted by EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xing wang", "zhaopeng tu", "deyi xiong", "min zhang"], "accepted": true, "id": "1708.01980"}, "pdf": {"name": "1708.01980.pdf", "metadata": {"source": "CRF", "title": "Translating Phrases in Neural Machine Translation", "authors": ["Xing Wang", "Zhaopeng Tu", "Deyi Xiong", "Min Zhang"], "emails": ["xingwsuda@gmail.com,", "minzhang}@suda.edu.cn", "tuzhaopeng@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation (NMT) has been receiving increasing attention due to its impressive\n\u2217Corresponding author\ntranslation performance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016). Significantly different from conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT adopts a big neural network to perform the entire translation process in one shot, for which an encoderdecoder architecture is widely used. Specifically, the encoder encodes a source sentence into a continuous vector representation, then the decoder uses the continuous vector representation to generate the corresponding target translation word by word.\nThe word-by-word generation philosophy in NMT makes it difficult to translate multi-word phrases. Phrases, especially multi-word expressions, are crucial for natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005) as the meaning of a phrase cannot be always deducible from the meanings of its individual words or parts. Unfortunately current NMT is essentially a word-based or character-based (Chung et al., 2016; Costa-jussa\u0300 and Fonollosa, 2016; Luong and Manning, 2016) translation system where phrases are not considered as translation units. In contrast, phrases are much better than words as translation units in SMT and have made a significant advance in translation quality. Therefore, a natural question arises: Can we translate phrases in NMT?\nRecently, there have been some attempts on multi-word phrase generation in NMT (Stahlberg et al., 2016b; Zhang and Zong, 2016). However these efforts constrain NMT to generate either syntactic phrases or domain phrases in the wordby-word generation framework. To explore the phrase generation in NMT beyond the word-byword generation framework, we propose a novel architecture that integrates a phrase-based SMT\nar X\niv :1\n70 8.\n01 98\n0v 1\n[ cs\n.C L\n] 7\nA ug\n2 01\n7\nmodel into NMT. Specifically, we add an auxiliary phrase memory to store target phrases in symbolic form. At each decoding step, guided by the decoding information from the NMT decoder, the SMT model dynamically generates relevant target phrase translations and writes them to the memory. Then the NMT decoder scores phrases in the phrase memory and selects a proper phrase or word with the highest probability. If the phrase generation is carried out, the NMT decoder generates a multi-word phrase and updates its decoding state by consuming the words in the selected phrase.\nFurthermore, in order to enhance the ability of the NMT decoder to effectively select appropriate target phrases, we modify the encoder of NMT to make it fit for exploring structural information of source sentences. Particularly, we integrate syntactic chunk information into the NMT encoder, to enrich the source-side representation. We validate our proposed model on the Chinese\u2192English translation task. Experiment results show that the proposed model significantly outperforms the conventional attention-based NMT by 1.07 BLEU points on multiple NIST test sets.\nThe rest of this paper is organized as follows. Section 2 briefly introduces the attentionbased NMT as background knowledge. Section 3 presents our proposed model which incorporates the phrase memory into the NMT encoder-decoder architecture, as well as the reading and writing procedures of the phrase memory. Section 4 presents our experiments on the Chinese\u2192English translation task and reports the experiment results. Finally we discuss related work in Section 5 and conclude the paper in Section 6."}, {"heading": "2 Background", "text": "Neural machine translation often adopts the encoder-decoder architecture with recurrent neural networks (RNN) to model the translation process. The bidirectional RNN encoder which consists of a forward RNN and a backward RNN reads a source sentence x = x1, x2, ..., xTx and transforms it into word annotations of the entire source sentence h = h1, h2, ..., hTx . The decoder uses the annotations to emit a target sentence y = y1, y2, ..., yTy in a word-by-word manner.\nIn the training phase, given a parallel sentence (x,y), NMT models the conditional probability as\nfollows,\nP (y|x) = Ty\u220f i=1 P (yi|y<i,x) (1)\nwhere yi is the target word emitted by the decoder at step i and y<i = y1, y2, ..., yi\u22121. The conditional probability P (yi|y<i,x) is computed as\nP (yi|y<i,x) = softmax(f(si, yi\u22121, ci)) (2)\nwhere f(\u00b7) is a non-linear function and si is the hidden state of the decoder at step i:\nsi = g(si\u22121, yi\u22121, ci) (3)\nwhere g(\u00b7) is a non-linear function. Here we adopt Gated Recurrent Unit (Cho et al., 2014) as the recurrent unit for the encoder and decoder. ci is the context vector, computed as a weighted sum of the annotations h:\nci = Tx\u2211 j=1 \u03b1t,jhj (4)\nwhere hj is the annotation of source word xj and its weight \u03b1t,j is computed by the attention model.\nWe train the attention-based NMT model by maximizing the log-likelihood:\nC(\u03b8) = N\u2211\nn=1 Ty\u2211 i=1 logP (yni |yn<i,xn) (5)\ngiven the training data with N bilingual sentences (Cho, 2015).\nIn the testing phase, given a source sentence x, we use beam search strategy to search a target sentence y\u0302 that approximately maximizes the conditional probability P (y|x)\ny\u0302 = argmax y\nP (y|x) (6)"}, {"heading": "3 Approach", "text": "In this section, we introduce the proposed model which incorporates a phrase memory into the encoder-decoder architecture of NMT. Inspired by the recent work on attaching an external structure to the encoder-decoder architecture (Gulcehre et al., 2016; Gu et al., 2016; Tang et al., 2016; Wang et al., 2017), we adopt a similar approach to incorporate the phrase memory into NMT."}, {"heading": "3.1 Framework", "text": "Figure 1 shows an example. Given the generated words \u201cPresident Bush emphasized that\u201d, the model generates the next fragment either from a word generation mode or a phrase generation mode. If the model selects the word generation mode, it generates a word by the NMT decoder as in the standard NMT framework. Otherwise, it generates a multi-word phrase by enquiring a phrase memory, which is written by an SMT decoder based on the dynamic decoding information from the NMT model for each step. The trade-off between word generation mode and phrase generation mode is balanced by a weight \u03bb, which is produced by a neural network based balancer.\nFormally, a generated translation y = {y1, y2, . . . , yTy} consists of two sets of fragments: words generated by NMT decoder w = {w1, w2, . . . , wK} and phrases generated from the phrase memory p = {p1, p2, . . . , pL} . The probability of generating y is calculated by\nP (y|x) = \u220f\nwk\u2208w (1\u2212 \u03bbt(wk))Pword(wk)\n\u00d7 \u220f pl\u2208p \u03bbt(pl)Pphrase(pl) (7)\nwhere Pword(wk) is the probability of generating the word wk (see Equation 2), Pphrase(pl) is that of generating the phrase pl which will be described in Section 3.2, and t(\u00b7) is the decoding step to generate the corresponding fragment.\nThe balancing weight \u03bb is produced by the balancer \u2013 a multi-layer network. The balancer network takes as input the decoding information, including the context vector ci, the previous decoding state si\u22121 and the previous generated word yi\u22121:\n\u03bbi = \u03c3(fb(si, yi\u22121, ci)) (8)\nwhere \u03c3(\u00b7) is a sigmoid function and fb(\u00b7) is the activation function. Intuitively, the weight \u03bb can be treated as the estimated importance of the phrase to be generated. We expect \u03bb to be high if the phrase is appropriate at the current decoding step.\nWell-Formed Phrases We employ a sourceside chunker to chunk the source sentence, and only phrases that corresponds to a source chunk are used in our model. We restrict ourselves to the well-formed chunk phrases based on the following considerations: (1) In order to take advantage of dynamic programming, we restrict ourselves to non-overlap phrases.1 (2) We explicitly utilize the boundary information of the source-side chunk phrases, to better guide the proposed model to adopt a target phrase at an appropriate decoding step. (3) We enable the model to exploit the syntactic categories of chunk phrases to enhance the proposed model with its selection preference for special target phrases. With these information, we enrich the context vector ci to enable the proposed model to make better decisions, as described below.\nFollowing the commonly-used strategy in sequence tagging tasks (Xue and Shen, 2003), we allow the words in a phrase to share the same chunk tag and introduce a special tag for the beginning word. For example, the phrase \u201c \u4fe1\u606f \u5b89\u5168 (information security)\u201d is tagged as a noun phrase \u201cNP\u201d, and the tag sequence should be \u201cNP B NP\u201d. Partially motivated by the work on integrating linguistic features into NMT (Sennrich and Haddow, 2016), we represent the encoder input as the combination of word embeddings and chunking tag embeddings, instead of word embeddings alone in the conventional NMT. The new input is formulated as follows:\n[Ewxi, E tti] (9)\n1Overlapped phrases may result in a high dimensionality in translation hypothesis representation and make it hard to employ shared fragments for efficient dynamic programming.\nwhere Ew \u2208 Rdw\u00d7|V NMT | is a word embedding matrix and dw is the word embedding dimensionality, Et \u2208 Rdt\u00d7|V TAG| is a tag embedding matrix and dt is the tag embedding dimensionality. [\u00b7] is the vector concatenation operation."}, {"heading": "3.2 Phrase Memory", "text": "The phrase memory stores relevant target phrases provided by an SMT model, which is trained on the same bilingual corpora. At each decoding step, the memory is firstly erased and re-written by the SMT model, the decoding of which is based on the translation information provided by the NMT model. Then, the proposed model enquires phrases along with their probabilities Pphrase from the memory.\nWriting to Phrase Memory Given a partial translation y<i = {y1, y2, . . . , yt\u22121} generated from NMT, the SMT model picks potential phrases extracted from the translation table. The phrases are scored with multiple SMT features, including the language model score, the translation probabilities, the reordering score, and so on. Specially, the reordering score depends on alignment information between source and target words, which is derived from attention distribution produced by the NMT model (Wang et al., 2017). SMT coverage vector in (Wang et al., 2017) is also introduced to avoid repeat phrasal recommendations. In our work, the potential phrase is phrase with high SMT score which is defined as following:\nSMTscore(pl|y<t,x) = M\u2211\nm=1\nwmhm(pl, x(pl))\n(10) where pl is a target phrase and x(pl) is its corresponding source span. hm(pl, x(pl)) is a SMT feature function and wm is its weight. The feature weights can be tuned by the minimum error rate training (MERT) algorithm (Och, 2003).\nThis leads to a better interaction between SMT and NMT models. It should be emphasized that our memory is dynamically updated at each decoding step based on the decoding history from both SMT and NMT models.\nThe proposed model is very flexible, where the phrase memory can be either fully dynamically generated by an SMT model or directly extracted from a bilingual dictionary, or any other bilingual resources storing idiomatic translations or bilin-\ngual multi-word expressions, which may lead to a further improvement. 2\nReading Phrase Memory When phrases are read from the memory, they are rescored by a neural network based score function. The score function takes as input the phrase itself and decoding information from NMT (i = t(pl) denotes the current decoding step):\nscorephrase(pl) = gs ( e(pl), si, yi\u22121, ci ) (11)\nwhere gs(\u00b7) is either an identity or a non-linear function. e(pl) is the representation of phrase pl, which is modeled by a recurrent neural networks. Again, si is the decoder state, yi\u22121 is the lastly generated word, and ci is the context vector. The scores are normalized for all phrases in the phrase memory, and the probability for phrase pl is calculated as\nPphrase(pl) = softmax(scorephrase(pl)) (12)\nThe probability calculation is controlled with parameters, which are trained together with the parameters from the NMT model."}, {"heading": "3.3 Training", "text": "Formally, we train both the default parameters of standard NMT and the new parameters associated with phrase generation on a set of training examples {[xn,yn]}Nn=1:\nC(\u03b8) = N\u2211\nn=1\nlogP (yn|xn) (13)\nwhere P (yn|xn) is defined in Equation 7. Ideally, the trained model is expected to produce a higher balance weight \u03bb and phrase probability Pphrase when a phrase is selected from the memory, and lower scores in other cases."}, {"heading": "3.4 Decoding", "text": "During testing, the NMT decoder generates a target sentence which consists of a mixture of words and phrases. Due to the different granularities of words and phrases, we design a variant of beam search strategy: At decoding step i, we first compute Pphrase for all phrases in the phrase memory\n2Bilingual resources can be utilized in two ways: First, we can store the bilingual resources in a static memory and keep all items available to NMT in the whole decoding period. Second, we can integrate the bilingual resources into SMT and then dynamically feed them into the phrase memory.\nand Pword for all words in NMT vocabulary. Then the balancer outputs a balancing weight \u03bbi, which is used to scale the phrase and word probabilities : \u03bbi \u00d7 Pphrase and (1 \u2212 \u03bbi) \u00d7 Pword. Now outputs are normalized probabilities on the concatenation of phrase memory and the general NMT vocabulary. At last, the NMT decoder generates a proper phrase or word of the highest probability.\nIf a target phrase in the phrase memory has the highest probability, the decoder generates the target phrase to complete the multi-word phrase generation process, and updates its decoding state by consuming the words in the selected phrase as described in Equation 3. All translation hypotheses are placed in the corresponding beams according to the number of generated target words."}, {"heading": "4 Experiments", "text": "In this section, we evaluated the effectiveness of our model on the Chinese\u2192English machine translation task. The training corpora consisted of about 1.25 million sentence pairs3 with 27.9 million Chinese words and 34.5 million English words respectively. We used NIST 2006 (NIST06) dataset as development set, and NIST 2004 (NIST04), 2005 (NIST05) and 2008 (NIST08) datasets as test sets. We report experiment results with case-insensitive BLEU score4.\nWe compared our proposed model with two state-of-the-art systems:\n* Moses: a state-of-the-art phrase-based SMT system (Koehn et al., 2007) with its default settings, where feature function weights are tuned by the minimum error rate training (MERT) algorithm (Och, 2003).\n* RNNSearch: an in-house implementation of the attention-based NMT system (Bahdanau et al., 2015) with its default settings.\nFor Moses, we used the full bilingual training data to train the phrase-based SMT model and the target portion of the bilingual training data to train a 4-gram language model using KenLM5. We ran Giza++ on the training data in both Chinese-to-English and English-to-Chinese\n3The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.\n4ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 5https://kheafield.com/code/kenlm/\ndirections and applied the \u201cgrow-diag-final\u201d refinement rule (Koehn et al., 2003) to obtain word alignments. The maximum phrase length is set to 7.\nFor RNNSearch, we generally followed settings in the previous work (Bahdanau et al., 2015; Tu et al., 2017a,b). We only kept a shortlist of the most frequent 30,000 words in Chinese and English, covering approximately 97.7% and 99.3% of the data in the two languages respectively. We constrained our source and target sequences to have a maximum length of 50 words in the training data. The size of embedding layer of both sides was set to 620 and the size of hidden layer was set to 1000. We used a minibatch stochastic gradient descent (SGD) algorithm of size 80 together with Adadelta (Zeiler, 2012) to train the NMT models. The decay rates \u03c1 and were set as 0.95 and 10\u22126. We clipped the gradient norm to 1.0 (Pascanu et al., 2013). We also adopted the dropout technique. Dropout was applied only on the output layer and the dropout rate was set to 0.5. We used a simple beam search decoder with beam size 10 to find the most likely translation.\nFor the proposed model, we used a Chinese chunker6 (Zhu et al., 2015) to chunk the sourceside Chinese sentences. 13 chunking tags appeared in our chunked sentences and the size of chunking tag embedding was set to 10. We used the trained phrase-based SMT to translate the source-side chunks. The top 5 translations according to their translation scores (Equation 10) were kept and among them multi-word phrases were used as phrasal recommendations for each source chunk phrase. For a source-side chunk phrase, if there exists phrasal recommendations from SMT, the output chunk tag was used as its chunking tag feature as described in Section 3.1. Otherwise, the words in the chunk were treated as general words by being tagged with the default tag. In the phrase memory, we only keep the top 7 target translations with highest SMT scores at each decoding step. We used a forward neural network with two hidden layers for both the balancer (Equation 8) and the scoring function (Equation 11). The numbers of units in the hidden layers were set to 2000 and 500 respectively. We used a backward RNN encoder to learn the phrase representations of target phrases in the phrase memory.\n6http://www.niuparser.com/"}, {"heading": "4.1 Main Results", "text": "Table 1 reports main results of different models measured in terms of BLEU score. We observe that our implementation of RNNSearch outperforms Moses by 2.34 BLEU points. (+memory) which is the proposed model with the phrase memory obtains an improvement of 0.47 BLEU points over the baseline RNNSearch. With the sourceside chunking tag feature, (+memory+chunking tag) outperforms the baseline RNNSearch by 1.07 BLEU points, showing the effectiveness of chunking syntactic categories on the selection of appropriate target phrases. From here on, we use \u201c+memory+chunking tag\u201d as the default setting in the following experiments if not otherwise stated.\nNumber of Sentences Affected by Generated Phrases We also check the number of translations that contain phrases generated by the proposed model, as shown in Table 2. As seen, a large portion of translations take the recommended phrases, and the number increases when the chunking tag feature is used.7 Considering BLEU scores reported in Table 1, we believe that the chunking tag feature benefits the proposed model on its phrase generation."}, {"heading": "4.2 Analysis on Generated Phrases", "text": "Syntactic Categories of Generated Phrases We first investigate which category of phrases is more likely to be selected by the proposed\n7The numbers on NIST08 are relatively lower since part of the test set contains sentences from Web forums, which contain less multi-word expressions.\napproach. There are some phrases, such as noun phrases (NPs, e.g., \u201cnational laboratory\u201d and \u201cvietnam airlines\u201d) and quantifier phrases (QPs, e.g., \u201c15 seconds\u201d and \u201ctwo weeks\u201d) , that we expect to be favored by our approach. Statistics shown in Table 3 confirm our hypothesis. Let\u2019s first concern all generated phrases (i.e., column \u201cAll\u201d): most selected phrases are noun phrases (81.0%) and quantifier phrases (10.8%). Among them, 44.5% percent of them are fully correct8. Specifically, NPs have relative higher generation accuracy (i.e., 47.8% = 38.7%/81.0%) while VPs have lower accuracy (i.e., 21.2% = 1.7%/8.0%). By looking into the wrong cases, we found most errors are related to verb tense, which is the drawback of SMT models.\nConcerning the newly introduced phrases that cannot be found in baseline translations (i.e., column \u201cNew\u201d), 13.2% of generated phrases are both new and fully correct, which contribute most to the performance improvement. We can also find that most newly introduced verb phrases and quantifier phrases are not correct, the patterns of which can\n8Fully correct means that the generated phrases can be retrieved in corresponding references as a whole unit.\nbe well learned by word-based NMT models.\nNumber of Words in Generated Phrases Table 4 lists the distribution of generated phrases based on the number of inside words. As seen, most generated phrases are short phrases (e.g., 2- gram and 3-gram phrases), which also contribute most to the new and fully correct phrases (i.e., 12.3% = 9.1%+3.2%). Focusing on long phrases (e.g., order> 4), most of them are newly introduced (10.6% out of 13.1%). Unfortunately, only a few portion of these phrases are fully correct, since long phrases have higher chance to contain one or two unmatched words.\nEffect of Generated Phrases on Translation Performance Note that the proposed model benefits not only from fully matched phrases, but also from partially matched phrases. For example, the baseline system translates \u201c \u56fd\u5bb6 \u822a\u7a7a \u66a8 \u592a \u7a7a \u603b\u7f72\u201d in a word-by-word manner and outputs \u201cstate aviation and space department\u201d. The generated phrase provided by SMT is \u201cnational aviation and space administration\u201d, but the only correct reference is \u201cnational aeronautics and space administration\u201d. The generated phrase is not fully correct but still useful.\nTo directly measure the improvement obtained by the phrase generation, we replace the generated target phrases with a special symbol \u201cNULL\u201d in test sets. As shown in Table 5, when deleting the generated target phrases, (\u201c+memory+chunking tag\u201d) and (\u201c+memory\u201d) translation performances decrease by 2.74 BLEU points and 1.32 BLEU points respectively. Moreover, translation performances on NIST08 decrease less than those on NIST04 and NIST05 in both settings. The reason is that NIST08 which contains sentences from web data has little influence on generating target phrases which are provided from a different domain 9. The overall results demonstrate that neural machine translation benefits from phrase translation."}, {"heading": "4.3 Effect of Balancer", "text": "The balancer which is used to coordinate the phrase generation and word generation is very crucial for the proposed model. We conducted an additional experiment to validate the effectiveness of the neural network based balancer. We use the setting \u201c+memory +chunking tag\u201d as baseline system to conduct the experiments. In this experiment, we fixed the balancing weight \u03bb (Equation 8) to 0.1 during training and testing and report the results. As shown in Table 6, we find that using the fixed value for the balancing weight (Constant (\u03bb = 0.1) ) decreases the translation performance sharply. This demonstrates that the neural network based balancer is an essential component for the proposed model."}, {"heading": "4.4 Comparison to Word-Level Recommendations and Discussions", "text": "Our approach is related to our previous work (Wang et al., 2017) which integrates the SMT word-level knowledge into NMT. To make a comparison, we conducted experiments followed set-\n9The parallel training data are mainly from news domain.\ntings in (Wang et al., 2017). The comparison results are reported in Table 7. We find that our approach is marginally better than the word-level model proposed in (Wang et al., 2017) by 0.28 BLEU points.\nIn our approach, the SMT model translates source-side chunk phrases using the NMT decoding information. Although we use high-quality target phrases as phrasal recommendations, our approach still suffers from the errors in segmentation and chunking. For example, the target phrase \u201claptop computers\u201d cannot be recommended by the SMT model if the Chinese phrase \u201c\u624b \u63d0 \u7535 \u8111\u201d is not chunked as a phrase unit. This is the reason why some sentences do not have corresponding phrasal recommendations (Table 2). Therefore, our approach can be further enhanced if we can reduce the error propagations from the segmenter or chunker, for example, by using n-best chunk sequences instead of the single best chunk sequence.\nAdditionally, we also observe that some target phrasal recommendations have been also generated by the baseline system in a word-by-word manner. These phrases, even taken as parts of final translations by the proposed model, do not lead to improvements in terms of BLEU as they have already occurred in translations from the baseline system. For example, the proposed model successfully carries out the phrase generation mode to generate a target phrase \u201cguangdong province\u201d (the translation of Chinese phrase \u201c\u5e7f\u4e1c\u7701\u201d) which has appeared in the baseline system.\nAs external resources, e.g., bilingual dictionary, which are complementary to the SMT phrasal recommendations, are compatible with the proposed model, we believe that the proposed model will get further improvement by using external resources."}, {"heading": "5 Related work", "text": "Our work is related to the following research topics on NMT:\nGenerating phrases for NMT In these studies, the generated NMT multi-word phrases are either from an SMT model or a bilingual dictionary. In syntactically guided neural machine translation (SGNMT), the NMT decoder uses phrase translations produced by the hierarchical phrasebased SMT system Hiero, as hard decoding constraints. In this way, syntactic phrases are generated by the NMT decoder (Stahlberg et al., 2016b). Zhang and Zong (2016) use an SMT translation system, which is integrated an additional bilingual dictionary, to synthesize pseudo-parallel sentences and feed the sentences into the training of NMT in order to translate low-frequency words or phrases. Tang et al. (2016) propose an external phrase memory that stores phrase pairs in symbolic forms for NMT. During decoding, the NMT decoder enquires the phrase memory and properly generates phrase translations. The significant differences between these efforts and ours are 1) that we dynamically generate phrase translations via an SMT model, and 2) that at the same time we modify the encoder to incorporate structural information to enhance the capability of NMT in phrase translation.\nIncorporating linguistic information into NMT NMT is essentially a sequence to sequence mapping network that treats the input/output units, eg., words, subwords (Sennrich et al., 2016), characters (Chung et al., 2016; Costa-jussa\u0300 and Fonollosa, 2016), as non-linguistic symbols. However, linguistic information can be viewed as the taskspecific knowledge, which may be a useful supplementary to the sequence to sequence mapping network. To this end, various kinds of linguistic annotations have been introduced into NMT to improve its translation performance. Sennrich and Haddow (2016) enrich the input units of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc\u0131\u0301a-Mart\u0131\u0301nez et al. (2016) propose factored NMT using the morphological and grammatical decomposition of the words (factors) in output units. Eriguchi et al. (2016) explore the phrase structures of input sentences and propose a tree-to-sequence attention model for the vanilla NMT model. Li et al. (2017) propose to linearize source-side parse trees to obtain structural label sequences and explicitly incorporated the structural sequences into NMT, while Aharoni and Goldberg (2017) propose to\nincorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation.\nCombining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al., 2016). Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT. They combine NMT posteriors with SMT word recommendations through linear interpolation implemented by a gating function which dynamically assigns the weights. Niehues et al. (2016) propose to use SMT to pre-translate the inputs into target translations and employ the target pre-translations as input sequences in NMT. Zhou et al. (2017) propose a neural system combination framework to directly combine NMT and SMT outputs. The combination of NMT and SMT has been also introduced in interactive machine translation to improve the system\u2019s suggestion quality (Wuebker et al., 2016). In addition, word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT (Cohn et al., 2016; Mi et al., 2016; Liu et al., 2016)."}, {"heading": "6 Conclusion", "text": "In this paper, we have presented a novel model to translate source phrases and generate target phrase translations in NMT by integrating the phrase memory into the encoder-decoder architecture. At decoding, the SMT model dynamically generates relevant target phrases with contextual information provided by the NMT model and writes them to the phrase memory. Then the proposed model\nreads the phrase memory and uses the balancer to make probability estimations for the phrases in the phrase memory. Finally the NMT decoder selects a phrase from the phrase memory or a word from the vocabulary of the highest probability to generate. Experiment results on Chinese\u2192English translation have demonstrated that the proposed model can significantly improve the translation performance."}, {"heading": "Acknowledgments", "text": "We would like to thank three anonymous reviewers for their insightful comments, and also acknowledge Zhengdong Lu, Lili Mou for useful discussions. This work was supported by the National Natural Science Foundation of China (Grants No.61525205, 61373095 and 61622209)."}], "references": [{"title": "Towards string-to-tree neural machine translation", "author": ["Roee Aharoni", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1704.04743.", "citeRegEx": "Aharoni and Goldberg.,? 2017", "shortCiteRegEx": "Aharoni and Goldberg.", "year": 2017}, {"title": "Incorporating discrete translation lexicons into neural machine translation", "author": ["Philip Arthur", "Graham Neubig", "Satoshi Nakamura."], "venue": "Proceedings of the 2016 Conference on EMNLP.", "citeRegEx": "Arthur et al\\.,? 2016", "shortCiteRegEx": "Arthur et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Peter F Brown", "Vincent J Della Pietra", "Stephen A Della Pietra", "Robert L Mercer."], "venue": "Computational linguistics.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang."], "venue": "Proceedings of the 43rd ACL.", "citeRegEx": "Chiang.,? 2005", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "Natural language understanding with distributed representation", "author": ["Kyunghyun Cho."], "venue": "arXiv preprint.", "citeRegEx": "Cho.,? 2015", "shortCiteRegEx": "Cho.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 54th ACL, pages 1693\u20131703, Berlin, Germany.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Incorporating structural alignment biases into an attentional neural translation model", "author": ["Haffari."], "venue": "Proceedings of NAACL 2016, San Diego, California.", "citeRegEx": "Haffari.,? 2016", "shortCiteRegEx": "Haffari.", "year": 2016}, {"title": "Character-based neural machine translation", "author": ["Marta R. Costa-juss\u00e0", "Jos\u00e9 A.R. Fonollosa."], "venue": "Proceedings of the 54th ACL, pages 357\u2013361, Berlin, Germany.", "citeRegEx": "Costa.juss\u00e0 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.juss\u00e0 and Fonollosa.", "year": 2016}, {"title": "Tree-to-sequence attentional neural machine translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 54th ACL, pages 823\u2013833, Berlin, Germany.", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "Factored neural machine translation", "author": ["Mercedes Garc\u0131\u0301a-Mart\u0131\u0301nez", "Lo\u0131\u0308c Barrault", "Fethi Bougares"], "venue": "arXiv preprint arXiv:1609.04621", "citeRegEx": "Garc\u0131\u0301a.Mart\u0131\u0301nez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Mart\u0131\u0301nez et al\\.", "year": 2016}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li."], "venue": "Proceedings of the 54th ACL, Berlin, Germany.", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "Proceedings of ACL 2016, Berlin, Germany.", "citeRegEx": "Gulcehre et al\\.,? 2016", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Improved neural machine translation with smt features", "author": ["Wei He", "Zhongjun He", "Hua Wu", "Haifeng Wang."], "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the 53rd ACL and the 7th IJCNLP.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on EMNLP.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 NAACL.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Modeling source syntax for neural machine translation", "author": ["Junhui Li", "Deyi Xiong", "Zhaopeng Tu", "Muhua Zhu", "Min Zhang", "Guodong Zhou."], "venue": "arXiv preprint arXiv:1705.01020.", "citeRegEx": "Li et al\\.,? 2017", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Neural machine translation with supervised attention", "author": ["Lemao Liu", "Masao Utiyama", "Andrew Finch", "Eiichiro Sumita."], "venue": "Proceedings of COLING 2016, pages 3093\u20133102, Osaka, Japan.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "Proceedings of the 54th ACL.", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53rd ACL and the 7th IJCNLP.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Supervised attentions for neural machine translation", "author": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah."], "venue": "Proceedings of EMNLP 2016, pages 2283\u20132288, Austin, Texas.", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Pre-translation for neural machine translation", "author": ["Jan Niehues", "Eunah Cho", "Thanh-Le Ha", "Alex Waibel."], "venue": "Proceedings of COLING 2016, Osaka, Japan.", "citeRegEx": "Niehues et al\\.,? 2016", "shortCiteRegEx": "Niehues et al\\.", "year": 2016}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of the 41st ACL.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "Proceedings of the 30th International Conference on Machine Learning, ICML 2013, pages 1310C\u20131318.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Multiword expressions: A pain in the neck for nlp", "author": ["Ivan A Sag", "Timothy Baldwin", "Francis Bond", "Ann Copestake", "Dan Flickinger."], "venue": "International Conference on Intelligent Text Processing and Computational Linguistics, pages 1\u201315. Springer.", "citeRegEx": "Sag et al\\.,? 2002", "shortCiteRegEx": "Sag et al\\.", "year": 2002}, {"title": "Linguistic input features improve neural machine translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the First Conference on Machine Translation, pages 83\u201391, Berlin, Germany.", "citeRegEx": "Sennrich and Haddow.,? 2016", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th ACL, pages 1715\u20131725, Berlin, Germany.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation by minimising the bayes-risk with respect to syntactic translation lattices", "author": ["Felix Stahlberg", "Adri\u00e0 de Gispert", "Eva Hasler", "Bill Byrne."], "venue": "arXiv preprint arXiv:1612.03791.", "citeRegEx": "Stahlberg et al\\.,? 2016a", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "Syntactically guided neural machine translation", "author": ["Felix Stahlberg", "Eva Hasler", "Aurelien Waite", "Bill Byrne."], "venue": "Proceedings of the 54th ACL (Volume 2: Short Papers), Berlin, Germany.", "citeRegEx": "Stahlberg et al\\.,? 2016b", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems 27.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Neural machine translation with external phrase memory", "author": ["Yaohua Tang", "Fandong Meng", "Zhengdong Lu", "Hang Li", "Philip LH Yu."], "venue": "arXiv preprint arXiv:1606.01792.", "citeRegEx": "Tang et al\\.,? 2016", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "Context gates for neural machine translation", "author": ["Zhaopeng Tu", "Yang Liu", "Zhengdong Lu", "Xiaohua Liu", "Hang Li."], "venue": "Transactions of the Association of Computational Linguistics.", "citeRegEx": "Tu et al\\.,? 2017a", "shortCiteRegEx": "Tu et al\\.", "year": 2017}, {"title": "Neural machine translation with reconstruction", "author": ["Zhaopeng Tu", "Yang Liu", "Lifeng Shang", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of the 31th AAAI Conference on Artificial Intelligence.", "citeRegEx": "Tu et al\\.,? 2017b", "shortCiteRegEx": "Tu et al\\.", "year": 2017}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of the 54th ACL.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Introduction to the special issue on multiword expressions: Having a crack at a hard nut", "author": ["Aline Villavicencio", "Francis Bond", "Anna Korhonen", "Diana McCarthy."], "venue": "Computer Speech & Language, 19(4):365\u2013377.", "citeRegEx": "Villavicencio et al\\.,? 2005", "shortCiteRegEx": "Villavicencio et al\\.", "year": 2005}, {"title": "Neural machine translation advised by statistical machine translation", "author": ["Xing Wang", "Zhengdong Lu", "Zhaopeng Tu", "Hang Li", "Deyi Xiong", "Min Zhang."], "venue": "Proceedings of the 31th AAAI Conference on Artificial Intelligence.", "citeRegEx": "Wang et al\\.,? 2017", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Models and inference for prefix-constrained machine translation", "author": ["Joern Wuebker", "Spence Green", "John DeNero", "Sasa Hasan", "Minh-Thang Luong."], "venue": "Proceedings of the 54th ACL, pages 66\u2013 75, Berlin, Germany.", "citeRegEx": "Wuebker et al\\.,? 2016", "shortCiteRegEx": "Wuebker et al\\.", "year": 2016}, {"title": "Chinese word segmentation as lmr tagging", "author": ["Nianwen Xue", "Libin Shen."], "venue": "Proceedings of the Second SIGHAN Workshop on Chinese Language Processing, pages 176\u2013179, Sapporo, Japan.", "citeRegEx": "Xue and Shen.,? 2003", "shortCiteRegEx": "Xue and Shen.", "year": 2003}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Bridging neural machine translation and bilingual dictionaries", "author": ["Jiajun Zhang", "Chengqing Zong."], "venue": "arXiv preprint arXiv:1610.07272.", "citeRegEx": "Zhang and Zong.,? 2016", "shortCiteRegEx": "Zhang and Zong.", "year": 2016}, {"title": "Topic-informed neural machine translation", "author": ["Jian Zhang", "Liangyou Li", "Andy Way", "Qun Liu."], "venue": "Proceedings of COLING 2016, pages 1807\u20131817, Osaka, Japan.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Neural system combination for machine translation", "author": ["Long Zhou", "Wenpeng Hu", "Jiajun Zhang", "Chengqing Zong."], "venue": "arXiv preprint arXiv:1704.06393.", "citeRegEx": "Zhou et al\\.,? 2017", "shortCiteRegEx": "Zhou et al\\.", "year": 2017}, {"title": "Niuparser: A chinese syntactic and semantic parsing toolkit", "author": ["Jingbo Zhu", "Muhua Zhu", "Qiang Wang", "Tong Xiao."], "venue": "Proceedings of ACL-IJCNLP 2015 System Demonstrations, Beijing, China.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "Phrases play an important role in natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005).", "startOffset": 89, "endOffset": 135}, {"referenceID": 36, "context": "Phrases play an important role in natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005).", "startOffset": 89, "endOffset": 135}, {"referenceID": 16, "context": "\u2217Corresponding author translation performance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 46, "endOffset": 160}, {"referenceID": 6, "context": "\u2217Corresponding author translation performance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 46, "endOffset": 160}, {"referenceID": 31, "context": "\u2217Corresponding author translation performance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 46, "endOffset": 160}, {"referenceID": 2, "context": "\u2217Corresponding author translation performance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 46, "endOffset": 160}, {"referenceID": 3, "context": "Significantly different from conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT adopts a big neural network to perform the entire translation process in one shot, for which an encoderdecoder architecture is widely used.", "startOffset": 80, "endOffset": 134}, {"referenceID": 17, "context": "Significantly different from conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT adopts a big neural network to perform the entire translation process in one shot, for which an encoderdecoder architecture is widely used.", "startOffset": 80, "endOffset": 134}, {"referenceID": 4, "context": "Significantly different from conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT adopts a big neural network to perform the entire translation process in one shot, for which an encoderdecoder architecture is widely used.", "startOffset": 80, "endOffset": 134}, {"referenceID": 26, "context": "Phrases, especially multi-word expressions, are crucial for natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005) as the meaning of a phrase cannot be always deducible from the meanings of its individual words or parts.", "startOffset": 115, "endOffset": 161}, {"referenceID": 36, "context": "Phrases, especially multi-word expressions, are crucial for natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005) as the meaning of a phrase cannot be always deducible from the meanings of its individual words or parts.", "startOffset": 115, "endOffset": 161}, {"referenceID": 7, "context": "Unfortunately current NMT is essentially a word-based or character-based (Chung et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016; Luong and Manning, 2016) translation system where phrases are not considered as translation units.", "startOffset": 73, "endOffset": 151}, {"referenceID": 9, "context": "Unfortunately current NMT is essentially a word-based or character-based (Chung et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016; Luong and Manning, 2016) translation system where phrases are not considered as translation units.", "startOffset": 73, "endOffset": 151}, {"referenceID": 20, "context": "Unfortunately current NMT is essentially a word-based or character-based (Chung et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016; Luong and Manning, 2016) translation system where phrases are not considered as translation units.", "startOffset": 73, "endOffset": 151}, {"referenceID": 30, "context": "Recently, there have been some attempts on multi-word phrase generation in NMT (Stahlberg et al., 2016b; Zhang and Zong, 2016).", "startOffset": 79, "endOffset": 126}, {"referenceID": 41, "context": "Recently, there have been some attempts on multi-word phrase generation in NMT (Stahlberg et al., 2016b; Zhang and Zong, 2016).", "startOffset": 79, "endOffset": 126}, {"referenceID": 6, "context": "Here we adopt Gated Recurrent Unit (Cho et al., 2014) as the recurrent unit for the encoder and decoder.", "startOffset": 35, "endOffset": 53}, {"referenceID": 5, "context": "given the training data with N bilingual sentences (Cho, 2015).", "startOffset": 51, "endOffset": 62}, {"referenceID": 13, "context": "Inspired by the recent work on attaching an external structure to the encoder-decoder architecture (Gulcehre et al., 2016; Gu et al., 2016; Tang et al., 2016; Wang et al., 2017), we adopt a similar approach to incorporate the phrase memory into NMT.", "startOffset": 99, "endOffset": 177}, {"referenceID": 12, "context": "Inspired by the recent work on attaching an external structure to the encoder-decoder architecture (Gulcehre et al., 2016; Gu et al., 2016; Tang et al., 2016; Wang et al., 2017), we adopt a similar approach to incorporate the phrase memory into NMT.", "startOffset": 99, "endOffset": 177}, {"referenceID": 32, "context": "Inspired by the recent work on attaching an external structure to the encoder-decoder architecture (Gulcehre et al., 2016; Gu et al., 2016; Tang et al., 2016; Wang et al., 2017), we adopt a similar approach to incorporate the phrase memory into NMT.", "startOffset": 99, "endOffset": 177}, {"referenceID": 37, "context": "Inspired by the recent work on attaching an external structure to the encoder-decoder architecture (Gulcehre et al., 2016; Gu et al., 2016; Tang et al., 2016; Wang et al., 2017), we adopt a similar approach to incorporate the phrase memory into NMT.", "startOffset": 99, "endOffset": 177}, {"referenceID": 39, "context": "Following the commonly-used strategy in sequence tagging tasks (Xue and Shen, 2003), we allow the words in a phrase to share the same chunk tag and introduce a special tag for the beginning word.", "startOffset": 63, "endOffset": 83}, {"referenceID": 27, "context": "Partially motivated by the work on integrating linguistic features into NMT (Sennrich and Haddow, 2016), we represent the encoder input as the combination of word embeddings and chunking tag embeddings, instead of word embeddings alone in the conventional NMT.", "startOffset": 76, "endOffset": 103}, {"referenceID": 37, "context": "Specially, the reordering score depends on alignment information between source and target words, which is derived from attention distribution produced by the NMT model (Wang et al., 2017).", "startOffset": 169, "endOffset": 188}, {"referenceID": 37, "context": "SMT coverage vector in (Wang et al., 2017) is also introduced to avoid repeat phrasal recommendations.", "startOffset": 23, "endOffset": 42}, {"referenceID": 24, "context": "The feature weights can be tuned by the minimum error rate training (MERT) algorithm (Och, 2003).", "startOffset": 85, "endOffset": 96}, {"referenceID": 24, "context": ", 2007) with its default settings, where feature function weights are tuned by the minimum error rate training (MERT) algorithm (Och, 2003).", "startOffset": 128, "endOffset": 139}, {"referenceID": 2, "context": "* RNNSearch: an in-house implementation of the attention-based NMT system (Bahdanau et al., 2015) with its default settings.", "startOffset": 74, "endOffset": 97}, {"referenceID": 17, "context": "com/code/kenlm/ directions and applied the \u201cgrow-diag-final\u201d refinement rule (Koehn et al., 2003) to obtain word alignments.", "startOffset": 77, "endOffset": 97}, {"referenceID": 40, "context": "We used a minibatch stochastic gradient descent (SGD) algorithm of size 80 together with Adadelta (Zeiler, 2012) to train the NMT models.", "startOffset": 98, "endOffset": 112}, {"referenceID": 25, "context": "0 (Pascanu et al., 2013).", "startOffset": 2, "endOffset": 24}, {"referenceID": 44, "context": "For the proposed model, we used a Chinese chunker6 (Zhu et al., 2015) to chunk the sourceside Chinese sentences.", "startOffset": 51, "endOffset": 69}, {"referenceID": 37, "context": "Our approach is related to our previous work (Wang et al., 2017) which integrates the SMT word-level knowledge into NMT.", "startOffset": 45, "endOffset": 64}, {"referenceID": 37, "context": "\u201c+word level recommendation\u201d is the proposed model in (Wang et al., 2017).", "startOffset": 54, "endOffset": 73}, {"referenceID": 37, "context": "tings in (Wang et al., 2017).", "startOffset": 9, "endOffset": 28}, {"referenceID": 37, "context": "We find that our approach is marginally better than the word-level model proposed in (Wang et al., 2017) by 0.", "startOffset": 85, "endOffset": 104}, {"referenceID": 30, "context": "In this way, syntactic phrases are generated by the NMT decoder (Stahlberg et al., 2016b).", "startOffset": 64, "endOffset": 89}, {"referenceID": 29, "context": "In this way, syntactic phrases are generated by the NMT decoder (Stahlberg et al., 2016b). Zhang and Zong (2016) use an SMT translation system, which is integrated an additional bilingual dictionary, to synthesize pseudo-parallel sentences and feed the sentences into the training of NMT in order to translate low-frequency words or phrases.", "startOffset": 65, "endOffset": 113}, {"referenceID": 29, "context": "In this way, syntactic phrases are generated by the NMT decoder (Stahlberg et al., 2016b). Zhang and Zong (2016) use an SMT translation system, which is integrated an additional bilingual dictionary, to synthesize pseudo-parallel sentences and feed the sentences into the training of NMT in order to translate low-frequency words or phrases. Tang et al. (2016) propose an external phrase memory that stores phrase pairs in symbolic forms for NMT.", "startOffset": 65, "endOffset": 361}, {"referenceID": 28, "context": ", words, subwords (Sennrich et al., 2016), characters (Chung et al.", "startOffset": 18, "endOffset": 41}, {"referenceID": 7, "context": ", 2016), characters (Chung et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016), as non-linguistic symbols.", "startOffset": 20, "endOffset": 73}, {"referenceID": 9, "context": ", 2016), characters (Chung et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016), as non-linguistic symbols.", "startOffset": 20, "endOffset": 73}, {"referenceID": 6, "context": ", 2016), characters (Chung et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016), as non-linguistic symbols. However, linguistic information can be viewed as the taskspecific knowledge, which may be a useful supplementary to the sequence to sequence mapping network. To this end, various kinds of linguistic annotations have been introduced into NMT to improve its translation performance. Sennrich and Haddow (2016) enrich the input units of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features.", "startOffset": 21, "endOffset": 410}, {"referenceID": 6, "context": ", 2016), characters (Chung et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016), as non-linguistic symbols. However, linguistic information can be viewed as the taskspecific knowledge, which may be a useful supplementary to the sequence to sequence mapping network. To this end, various kinds of linguistic annotations have been introduced into NMT to improve its translation performance. Sennrich and Haddow (2016) enrich the input units of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc\u0131\u0301a-Mart\u0131\u0301nez et al. (2016) propose factored NMT using the morphological and grammatical decomposition of the words (factors) in output units.", "startOffset": 21, "endOffset": 601}, {"referenceID": 6, "context": ", 2016), characters (Chung et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016), as non-linguistic symbols. However, linguistic information can be viewed as the taskspecific knowledge, which may be a useful supplementary to the sequence to sequence mapping network. To this end, various kinds of linguistic annotations have been introduced into NMT to improve its translation performance. Sennrich and Haddow (2016) enrich the input units of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc\u0131\u0301a-Mart\u0131\u0301nez et al. (2016) propose factored NMT using the morphological and grammatical decomposition of the words (factors) in output units. Eriguchi et al. (2016) explore the phrase structures of input sentences and propose a tree-to-sequence attention model for the vanilla NMT model.", "startOffset": 21, "endOffset": 739}, {"referenceID": 6, "context": ", 2016), characters (Chung et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016), as non-linguistic symbols. However, linguistic information can be viewed as the taskspecific knowledge, which may be a useful supplementary to the sequence to sequence mapping network. To this end, various kinds of linguistic annotations have been introduced into NMT to improve its translation performance. Sennrich and Haddow (2016) enrich the input units of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc\u0131\u0301a-Mart\u0131\u0301nez et al. (2016) propose factored NMT using the morphological and grammatical decomposition of the words (factors) in output units. Eriguchi et al. (2016) explore the phrase structures of input sentences and propose a tree-to-sequence attention model for the vanilla NMT model. Li et al. (2017) propose to linearize source-side parse trees to obtain structural label sequences and explicitly incorporated the structural sequences into NMT, while Aharoni and Goldberg (2017) propose to", "startOffset": 21, "endOffset": 879}, {"referenceID": 0, "context": "(2017) propose to linearize source-side parse trees to obtain structural label sequences and explicitly incorporated the structural sequences into NMT, while Aharoni and Goldberg (2017) propose to", "startOffset": 158, "endOffset": 186}, {"referenceID": 42, "context": "Zhang et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation.", "startOffset": 0, "endOffset": 20}, {"referenceID": 21, "context": "(2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al.", "startOffset": 142, "endOffset": 181}, {"referenceID": 15, "context": "(2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al.", "startOffset": 142, "endOffset": 181}, {"referenceID": 35, "context": ", 2015) and coverage problem (Tu et al., 2016).", "startOffset": 29, "endOffset": 46}, {"referenceID": 37, "context": "(2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017).", "startOffset": 232, "endOffset": 251}, {"referenceID": 38, "context": "The combination of NMT and SMT has been also introduced in interactive machine translation to improve the system\u2019s suggestion quality (Wuebker et al., 2016).", "startOffset": 134, "endOffset": 156}, {"referenceID": 22, "context": "In addition, word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT (Cohn et al., 2016; Mi et al., 2016; Liu et al., 2016).", "startOffset": 119, "endOffset": 173}, {"referenceID": 19, "context": "In addition, word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT (Cohn et al., 2016; Mi et al., 2016; Liu et al., 2016).", "startOffset": 119, "endOffset": 173}, {"referenceID": 13, "context": "He et al. (2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 1, "context": "Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT.", "startOffset": 0, "endOffset": 465}, {"referenceID": 1, "context": "Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT. They combine NMT posteriors with SMT word recommendations through linear interpolation implemented by a gating function which dynamically assigns the weights. Niehues et al. (2016) propose to use SMT to pre-translate the inputs into target translations and employ the target pre-translations as input sequences in NMT.", "startOffset": 0, "endOffset": 721}, {"referenceID": 1, "context": "Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT. They combine NMT posteriors with SMT word recommendations through linear interpolation implemented by a gating function which dynamically assigns the weights. Niehues et al. (2016) propose to use SMT to pre-translate the inputs into target translations and employ the target pre-translations as input sequences in NMT. Zhou et al. (2017) propose a neural system combination framework to directly combine NMT and SMT outputs.", "startOffset": 0, "endOffset": 878}], "year": 2017, "abstractText": "Phrases play an important role in natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005). However, it is difficult to integrate them into current neural machine translation (NMT) which reads and generates sentences word by word. In this work, we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of NMT. At each decoding step, the phrase memory is first re-written by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model. Then the proposed model reads the phrase memory to make probability estimations for all phrases in the phrase memory. If phrase generation is carried on, the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase. Otherwise, the NMT decoder generates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese\u2192English translation show that the proposed model achieves significant improvements over the baseline on various test sets.", "creator": "LaTeX with hyperref package"}}}