{"id": "1708.04116", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "Early Improving Recurrent Elastic Highway Network", "abstract": "morriston To model christo time - varying nonlinear temporal sandberger dynamics in 4,925 sequential double-hung data, a recurrent marshawn network capable 52-38 of arabinda varying and reuther adjusting 137.50 the hayak recurrence 4:03 depth between input intervals chinlund is examined. The recurrence depth khiva is flattening extended endeavoured by several treuchtlingen intermediate hidden oleander state reitzle units, wapentakes and the welshmen weight parameters .428 involved norilsk in determining strong-arm these units are murfree dynamically calculated. The motivation animotion behind the explorer paper lies on bayelsa overcoming guas a lucrece deficiency manfredonia in multi-tier Recurrent Highway implemented Networks and cosmologies improving mogae their performances hugger which speleologists are currently at lloran the estenssoro forefront of RNNs: 1) grabens Determining the 1911 appropriate number hospitalier of recurrent nabiha depth in RHN bushs for different tasks is handcrafted a amfi huge kudzu burden and corfo just setting 26.60 it 2,435 to a sabermetrics large philoctetes number kuusysi is computationally wasteful ayreon with possible 2,081 repercussion in terms of bertholle performance degradation kouk and eh101 high latency. Expanding robina on casillo the idea of adaptive darcie computation brune time (thrush ACT ), low-altitude with dannay the outdrove use geda of 3l an enroth elastic oberweiler gate photosynthesize in the form tr\u01b0\u01a1ng of ayyubid a rectified exponentially decreasing larranaga function taking on as arguments mohapi as debatably previous mizuho hidden state and kilcommons input, engadine the proposed model 25min is xibe able bloxom to militarily evaluate reportorial the pinnule appropriate recurrent livingroom depth for each input. The commmunist rectified whitcomb gating 59.34 function saadat enables crabbe the hoberg most significant chernovetsky intermediate hidden state atsugi updates morra to come 74.47 early such scablands that significant performance bermudo gain is havenstein achieved early. 2) av-8bs Updating the lewald weights spiraea from that 1.128 of tomar previous 132,500 intermediate layer kuhan offers a stanes richer representation 72.68 than the beregovoy use of stil shared spyders weights gilort across 56-46 all mazzocchi intermediate gopalpur recurrence layers. engman The cashman weight 75-61 update raffaella procedure is just mao an rdq expansion domokos of plasticizer the idea underlying deus hypernetworks. To substantiate the 715,000 effectiveness nirvikalpa of rottmann the proposed injudicious network, we magnitude-4 conducted fibulae three jiiva experiments: regression mulla on synthetic data, 178.5 human manzullo activity wooed recognition, sergiyev and darroze language modeling on montena the Penn ini Treebank libous dataset. 2.39 The proposed networks showed better metap\u00e1n performance 53.29 than other state - of - the - art jovan recurrent 18-nation networks keltie in all three debido experiments.", "histories": [["v1", "Mon, 14 Aug 2017 13:39:28 GMT  (103kb,D)", "http://arxiv.org/abs/1708.04116v1", "9 pages, 3 figures"]], "COMMENTS": "9 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hyunsin park", "chang d yoo"], "accepted": false, "id": "1708.04116"}, "pdf": {"name": "1708.04116.pdf", "metadata": {"source": "CRF", "title": "Early Improving Recurrent Elastic Highway Network", "authors": ["Hyunsin Park", "Chang D. Yoo"], "emails": ["hs.park@kaist.ac.kr", "cd_yoo@kaist.ac.kr"], "sections": [{"heading": "1 Introduction", "text": "Recurrent Neural Networks (RNNs) have been successfully applied to a diverse array of tasks, such as speech recognition [1], language modeling [2], machine translation [3], music generation [4], image description [5], video description [6]. In these tasks, the non-linear transition of the internal state in the RNNs represented the temporal dynamic behavior of the sequence. This paper investigates the effectiveness of elastically varying the intermediate recurrence depth of the RNN in modeling the time-varying temporal dynamics of the sequential input data. The intermediate recurrence depth at each time interval will depend on the input and hidden state at a corresponding time interval.\nWhile deeper networks have been known to be more efficient in representing the input-output relationship compared to shallow networks, they require more data to train and are far more susceptible to the vanishing and exploding gradient problem. As a measure to safeguard against such problem when constructing a very deep network, ResNet [7] and Highway Networks [8] were proposed. Residual units in these models provide learning stability in building a very deep model. In creating temporally deep RNNs, Long Short-Term Memory (LSTM, [9]), Gated Recurrent Unit (GRU, [3]), and Recurrent Highway Network (RHN, [10]) were introduced to both find learning stability and model long-range temporal dependency.\nar X\niv :1\n70 8.\n04 11\n6v 1\n[ cs\n.L G\n] 1\n4 A\nug 2\n01 7\nOne factor that affects the computational time is the depth (or the number of layers) of the network. Given a training dataset, to get good performance, various models with different depth are empirically compared, and this can be a very time-consuming procedure if the dataset is large.\nRecently, two studies on determining appropriate depth of a network have been considered. A general concept of time-dependent adaptive computation time (ACT) is introduced into RNN in [11]. The algorithm learns the number of computational steps to take between receiving an input and emitting an output. Here, a sigmoidal halting unit determines the probability for continuing the computation. In Spatially Adaptive Computation Time [12], the number of executed layers for the regions of the image is dynamically adjusted.\nTo model time-varying nonlinear temporal dynamics in sequential data, a recurrent network capable of varying and adjusting the recurrence depth between input intervals is examined. The recurrence depth is extended by several intermediate hidden state units, and the weight parameters involved in determining these units are dynamically calculated. The motivation behind the paper lies on overcoming a deficiency in Recurrent Highway Networks and improving their performances which are currently at the forefront of RNNs: 1) Determining the appropriate number of recurrent depth in RHN for different tasks is a huge burden and just setting it to a large number is computationally wasteful with possible repercussion in terms of performance degradation and high latency. Expanding on the idea recently proposed in ACT [11], with the use of an elastic gate in the form of a rectified exponentially decreasing function taking on as arguments as previous hidden state and input, the proposed model is able to evaluate the appropriate recurrent depth for each input. The rectified gating function enables the most significant intermediate hidden state updates to come early such that significant performance gain is achieved early. 2) Updating the weights from that of previous intermediate layer offers a richer representation than the use of shared weights across all intermediate recurrence layers. The weight update procedure is just an expansion of the idea underlying hypernetworks [13]. This structure is referred to as Early Improving Recurrent Elastic Highway Network (EI-REHN).\nThis paper is organized as follows. Section 2 describes background information for the proposed method. Section 3 presents the proposed method of EI-REHN. Section 4 shows the experimental results, and finally, Section 5 concludes the paper."}, {"heading": "2 Background", "text": "Consider a hidden state transition model S of a recurrent network, ht = S(ht\u22121,xt), (1)\nwhere xt \u2208 RDx and ht\u22121 \u2208 RDh are the input vector at time step t and hidden state vector at time step t\u2212 1, respectively. The hidden state transition model of the standard RNN can be represented as follows,\nSRNN (ht\u22121,xt) = tanh ( WR [ ht\u22121 xt 1 ]) , (2)\nwhere WR \u2208 RDh\u00d7(Dh+Dx+1) and tanh are a weight matrix including bias and element-wise hyperbolic tangent function, respectively.\nOn the other hand, the hidden state transition model of an LSTM is as follows,\nSLSTM (ht\u22121, ct\u22121,xt) = ot \u2297 tanh(ct), (3) where\nct = ft \u2297 ct\u22121 + it \u2297 at, (4) atitft ot  =  tanhsigmsigm sigm WL [ ht\u22121xt 1 ] . (5)\nHere, at \u2208 RDh , it \u2208 RDh , ft \u2208 RDh , ot \u2208 RDh , ct \u2208 RDh , WL \u2208 R4Dh\u00d7(Dh+Dx+1), sigm, and \u2297 are cell proposal, input gate, forget gate, output gate, cell state, weight matrix, element-wise sigmoid function, and element-wise multiplication, respectively.\nThe hidden state transition model of RHN with a fixed recurrence depth is as follows,\nSRHN (ht\u22121,xt) = hRt , (6) where\nh0t = ht\u22121\nhrt = t r t \u2297 srt + crt \u2297 hr\u22121t , (7)(\nsrt trt crt\n) = ( tanh sigm sigm ) WrH  hr\u22121tI{r=1}xt 1  . (8) Here, srt \u2208 RDh , trt \u2208 RDh , crt \u2208 RDh , and WrH \u2208 R3Dh\u00d7(Dh+Dx+1), are residual component, transform gate, carry gate, and weight matrix, respectively."}, {"heading": "3 Early Improving Recurrent Elastic Highway Network", "text": "In this section, Early Improving Recurrent Elastic Highway Network (EI-REHN) which is a recurrent network capable of varying and adjusting the recurrence depth between input intervals is introduced. The intermediate state transition from the (r \u2212 1)-th intermediate recurrence layer to the r-th layer is given as\nhrt = g r t \u2297 srt + (1\u2212 grt )\u2297 hr\u22121t , (9)\nwhere git and s r t are a gating function for adaptive recurrence depth and residual component, respectively. The gating function is designed to exponentially decrease as the intermediate recurrence layer increases. Consequently, intermediate recurrence state transition halts when the gating function reaches zero. The exponentially decreasing gating function enables the most significant intermediate hidden state updates to come early such that significant performance gain is achieved early. In order to reduce the number of parameters, a recurrence relationship is formulated between parameters of adjacent layers such that only the significant parameters are updated. And the weight parameters for all the intermediate recurrence layers are calculated based on a hypernetwork [13] that is a sub-network to generate the weights for another network."}, {"heading": "3.1 Adaptive recurrence depth", "text": "A gating function to adaptively determine the intermediate recurrence depth depending on xt and ht\u22121 is given as follows:\ngrt = d r t \u2297 g\u0302rt , (10)\nwhere drt and g\u0302 r t are elastic gating and residual gating, respectively. The elastic gating function is obtained as follows,\ndrt = max(\u03b2 + e \u03b1 \u2212 e(\u03b1+\u03b1t)r, 0), (11)\n\u03b2 = sigm(\u03b2\u0302), (12) \u03b1 = softplus(\u03b1\u0302), (13)\n\u03b1t = sigm ( Wa [ ht\u22121 xt 1 ]) , (14)\nwhere \u03b2, \u03b1, \u03b1t, and Wa \u2208 RDh\u00d7(Dh+Dx+1) are initial gating bias, global decreasing rate, local decreasing rate, and weight matrix for residual gating function, respectively. The model parameters of \u03b1\u0302 and \u03b2\u0302 are estimated when training, while the local decreasing rate, \u03b1t, is time-dependent and activated when forward propagation. It makes the model to have adaptive recurrence depth at every time step.\nAt time t, adaptive recurrence depth Rt is determined as the maximum depth that satisfies ||grt ||1 > 0. This means that the recurrence layer repeats until all hidden units have zero gating activations. Based on \u03b1 and \u03b2, upper-bound to Rt for all the time steps is\nRt = max ||grt ||1>0 r \u2264 max i\n\u230a 1\n\u03b1i log(\u03b2i + e\n\u03b1i) \u230b . (15)\nFigure 1 shows examples of elastic gating functions for a hidden unit when \u03b1rt = 0. The elastic gating function is exponentially decreasing with respect to recurrence depth. Smaller value of \u03b1 and bigger value of \u03b2 give longer adaptive recurrence depth.\nIf an element of dr=1t is zero, the corresponding hidden unit is not updated at time step t and the hidden information from the previous time step is passed to the next time step due to the exactly zero gating activation."}, {"heading": "3.2 Dynamic weight matrix", "text": "At each intermediate recurrence layer, the residual component is calculated as follows,\nsrt = tanh(WxxtIr=1 + Wrsh r\u22121 t + b r s), (16)\nwhere Wx \u2208 RDh\u00d7Dx , Wrs \u2208 RDh\u00d7Dh , and brs \u2208 RDh are input-to-residual weight matrix, hiddento-residual weight matrix, and residual bias, respectively. The residual gating g\u0302rt is also calculated in similar manner by replacing \u2019tanh\u2019 with \u2019sigm\u2019. Here, Wrs and b\nr are dependent on the recurrence layer r. In the proposed model, the recurrence depth is time-varying and can be very deep. Learning all weight matrices for all possible depth is impractical. One solution is to use a shared weight matrix. However, this constraint reduces the ability of feature representation. To solve this problem, we propose a dynamic weight matrix utilizing the concept of hypernetworks [13].\nAt t time step and r intermediate recurrence layer, for the calculations of the residual component and residual gating, we use a dynamic weight matrix that is different for each calculation and defined as follows.\nWrt = W r\u22121 t + \u2206W r t , (17)\nwhere, \u2206Wrt is obtained by hypernetwork that is a simple RNN with small number of hidden units compared with the number of main hidden units. The hypernetwork takes input as the previous residual component sr\u22121t and residual gating g\u0302 r\u22121 t and calculates the hidden hypernetwork state zrt \u2208 RDz ,\nzrt = tanh(Wzhs r\u22121 t + Wzgg\u0302 r\u22121 t + Wzz r\u22121 t + bz), (18)\nwhere Wzh \u2208 RDz\u00d7Dh , Wzg \u2208 RDz\u00d7Dh , Wz \u2208 RDz\u00d7Dz , and bz \u2208 RDz are model parameters of the hypernetwork. From the hidden state of the hypernetwork, we calculate a diagonal \u2206Wrt as follows,\nwrt = Pz r t , (19) \u2206Wrt = diag(w r t ), (20)\nwhere P \u2208 RDh\u00d7Dz is a projection matrix. With the \u2206Wrt , we consider gated weight update as follows.\ng\u0304rt = sigm(P\u0304z r t + b\u0304), (21) srt = tanh(g\u0304 r t \u2297Wr\u22121t hr\u22121t + (1\u2212 g\u0304rt )\u2297\u2206Wrthr\u22121t + WxxtIr=1 + b). (22)\nWe tried various ways of updating the parameters (e.g. weight scaling of hypernetworks in [13], weight matrix decomposition proposed by Jurgen Schmidhuber in [14], etc.) and found diagonal update to be the most effective.\nFinally, the procedure of the state transition with adaptive recurrence depth is described in Algorithm 1. Computation graph of the proposed network is shown in Figure 2.\nAlgorithm 1 State transition with adaptive recurrence depth\nInput: xt, ht\u22121, Rmax r = 0, Rt = 0,h 0 t = ht\u22121 Calculate decreasing rate \u03b1t in Eq.(14) while true do r = r + 1\nUpdate hypernetwork state zrt in Eq.(18) Calculate residual component srt in Eq.(22) Get gating function grt in Eq.(10) if ||grt ||1 > 0 and r \u2264 Rmax then Rt = r\nUpdate intermediate hidden state hrt in Eq.(9) else\nbreak end if\nend while Output: ht = hRtt\nAlgorithm 2 Synthetic data generation Input: N,T,Rmax, \u03b8 Make an empty synthetic dataset X = {} for n = 1 to N do Draw h0 \u223c U [\u22121, 1] for t = 1 to T do h0t = ht\u22121 Rt = round((Rmax \u2212 1) \u2217 ||ht\u22121||22) + 1 for r = 1 to Rt do nr \u223c N (0, 0.1\u00d7 I)\nhrt = tanh ([ cos(\u03b8) \u2212 sin(\u03b8) sin(\u03b8) cos(\u03b8) ] hr\u22121t + nr ) end for ht = h Rt t\nxt = Rt Rmax \u00d7 [ tanh(ht(1) + ht(2)) tanh(ht(1)\u2212 ht(2)) ] end for Add sequence [x1, \u00b7 \u00b7 \u00b7 ,xT ] to X\nend for Output: X"}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Synthetic data", "text": "This section aims to show the effectiveness of the adaptive recurrence depth on a synthetic dataset by comparing with other recurrent networks. To this end, we first constructed a synthetic dataset for sequential regression task.\nThe considered task is defined as predicting two-dimensional real vector of next step after observing real vector sequence up to the current step. The synthetic dataset is generated as described in Algorithm 2. The inputs, N,T,Rmax, \u03b8, are the number of samples, sequence length, maximum recurrence depth, and affine transform parameter, respectively. We set the inputs as (N,T,Rmax, \u03b8) = (10000, 21, 10, \u03c0/6). The synthetic dataset is divided into the training set, validation set, and test set with the size of 8000, 1000, and 1000, respectively. For all the experiments in this paper, Tensorflow toolkit [15] was used. For training the network, Adam optimizer [16] was adopted with 20 mini-batch size, 100 epochs, and 0.01 learning rate. Each model was trained five times with different initial conditions.\nTable 2: Human action recognition results on the HAR dataset.\nModel Dh(R) # of param. Accuracy [%]\nRNN 32 3924 82.26 \u00b1 2.57 48 8214 81.31 \u00b1 5.53 64 14022 81.11 \u00b1 4.16 LSTM 16 4048 90.88 \u00b1 0.96 24 8358 91.42 \u00b1 1.06 32 14214 92.12 \u00b1 0.40 RHN 32(1) 7366 90.58 \u00b1 0.95 32(2) 11590 89.06 \u00b1 0.61 32(3) 15814 83.06 \u00b1 4.62 EI-REHN 32 8102 91.50 \u00b1 0.51 40 12126 91.84 \u00b1 0.63 48 16950 92.48\u00b1 0.61\nTo confirm the effectiveness of the elastic gating, we compared 1) an RHN with shared parameters for the intermediate recurrent layers (SRHN), with 2) an RHN with shared parameters and elastic gate (SREHN) for the synthetic dataset. The hidden dimension was set to 20. SREHN showed 0.54 MSE, Now, performances by varying the recurrent depth of SRHN from 1 to 6 produced 0.81, 0.63, 0.58, 0.55, 0.55, and 0.54 MSE. From the results, we can say that the elastic gate helps in finding a recurrent depth that performs comparable to the best RHN with fixed recurrent depth. And the proposed model EI-REHN which is a model that the dynamic weight matrix is added to SREHN showed 0.47 MSE. From this result, we can say that the dynamic weight matrix gives rise to performance gain from 0.54 to 0.47.\nWe compare the proposed model with RNN, LSTM and RHN on the synthetic dataset by varying model structure parameters such as hidden dimension Dh and recurrence depth R. Table 1 shows the MSE results with the number of parameters corresponding to each model. As shown in Table 1, RNN shows the worst performance. The proposed model shows better MSE performance with a smaller size model than other models."}, {"heading": "4.2 Human activity recognition", "text": "In this subsection, we describe sequence classification experiments by using Human Activity Recognition Using Smartphones Data Set (HAR) [17]. In the HAR dataset, 30 persons performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone on the waist. 3-axial linear acceleration and 3-axial angular\nvelocity at are captured by using its embedded accelerometer and gyroscope. The dataset is divided into 7352 training sequences and 2947 test sequences.\nFor the human activity recognition on the HAR dataset, the input sequence is modeled by RNN, LSTM, RHN, and EI-REHN. Two-layer recurrent networks are used for this experiments. Six-class softmax layer is added to the last hidden units in the top hidden sequence for classification. To train the models, cross-entropy loss and Adam optimizer with 200 mini-batch size, 100 epochs, and 0.0025 learning rate are used.\nTable 2 shows the sequence classification results. In sequence classification, it is important that the last hidden units in recurrent neural networks have to contain information about the whole sequence. In the RHN case, as the recurrence depth increases, the performance decreases. We believe that the performance degradation of the RHNs from shallow recurrence depth occurs since the intermediate state transitions vanish the early information of sequence contained in the hidden units in spite of the short-cut path in the RHNs. Nonetheless, the proposed model shows better performance than other models in terms of classification accuracy."}, {"heading": "4.3 Language modeling", "text": "In this subsection, word level language modeling on the Penn TreeBank dataset [18] is described. The model architecture is represented in Figure 3. For vocabulary of size C, the one-hot vector is used to represent the word input at time t, wt \u2208 RC and word embedding vector is obtained by Uwt, where U \u2208 RH\u00d7C . The word embedding vector is passed into EI-REHN. The hidden activation of EI-REHN, ht, is multiplied by the transposed word embedding matrix, UT based on [19, 20]. Finally, softmax layer is applied to calculate the next word prediction w\u0302t+1.\nIn Table 4, we compare our models (Dh = 1000, 1200) with the basic LSTM [21], Pointer Sentinel networks [22], Ensemble of LSTMs [21] and RHN [10]. From the results, the proposed model shows comparable performance with RHN and better than the other models in terms of perplexity. The reported result of RHN was with ten recurrence depth, whereas the proposed model reached the maximum performance with four recurrence depth."}, {"heading": "5 Conclusion", "text": "To model time-varying nonlinear temporal dynamics in sequential data, a recurrent network capable of varying and adjusting the recurrence depth between input intervals was examined. By incorporating into the recurrent network that combines a shortcut path with a residual path with a rectified residual gating function which is best described as a rectified exponentially decreasing function, the network is capable of having varying recurrence depth. Moreover, we propose dynamic weight matrix construction for recurrence layers. This capability extends the capacity of existing recurrent network. To substantiate the effectiveness of the proposed network, we conducted three experiments that are a regression on the synthetic data, human activity recognition, and language modeling on the Penn Treebank dataset. The proposed networks showed better performance than other state-of-the-art recurrent networks in all three experiments."}], "references": [{"title": "Towards end-to-end speech recognition with recurrent neural networks.", "author": ["A. Graves", "N. Jaitly"], "venue": "in ICML, vol", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Statistical language models based on neural networks", "author": ["T. Mikolov"], "venue": "Presentation at Google, Mountain View, 2nd April, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-lewandowski", "Y. Bengio", "P. Vincent"], "venue": "ICML, 2012, pp. 1159\u20131166.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Show, attend and tell: Neural image caption generation with visual attention.", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "in ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "Proceedings of the IEEE international conference on computer vision, 2015, pp. 4507\u20134515.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770\u2013778.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Training very deep networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "Advances in neural information processing systems, 2015, pp. 2377\u20132385.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Recurrent highway networks", "author": ["J.G. Zilly", "R.K. Srivastava", "J. Koutn\u00edk", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1607.03474, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1603.08983, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Spatially adaptive computation time for residual networks", "author": ["M. Figurnov", "M.D. Collins", "Y. Zhu", "L. Zhang", "J. Huang", "D. Vetrov", "R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1612.02297, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Hypernetworks", "author": ["D. Ha", "A. Dai", "Q.V. Le"], "venue": "arXiv preprint arXiv:1609.09106, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to control fast-weight memories: An alternative to dynamic recurrent networks", "author": ["J. Schmidhuber"], "venue": "Neural Computation, vol. 4, no. 1, pp. 131\u2013139, 1992. 8", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1992}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": "2015, software available from tensorflow.org. [Online]. Available: http://tensorflow.org/", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "A public domain dataset for human activity recognition using smartphones.", "author": ["D. Anguita", "A. Ghio", "L. Oneto", "X. Parra", "J.L. Reyes-Ortiz"], "venue": "ESANN,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics, vol. 19, no. 2, pp. 313\u2013330, 1993.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1993}, {"title": "Tying word vectors and word classifiers: A loss framework for language modeling", "author": ["H. Inan", "K. Khosravi", "R. Socher"], "venue": "arXiv preprint arXiv:1611.01462, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Using the output embedding to improve language models", "author": ["O. Press", "L. Wolf"], "venue": "arXiv preprint arXiv:1608.05859, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Pointer sentinel mixture models", "author": ["S. Merity", "C. Xiong", "J. Bradbury", "R. Socher"], "venue": "arXiv preprint arXiv:1609.07843, 2016. 9", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Recurrent Neural Networks (RNNs) have been successfully applied to a diverse array of tasks, such as speech recognition [1], language modeling [2], machine translation [3], music generation [4], image description [5], video description [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 1, "context": "Recurrent Neural Networks (RNNs) have been successfully applied to a diverse array of tasks, such as speech recognition [1], language modeling [2], machine translation [3], music generation [4], image description [5], video description [6].", "startOffset": 143, "endOffset": 146}, {"referenceID": 2, "context": "Recurrent Neural Networks (RNNs) have been successfully applied to a diverse array of tasks, such as speech recognition [1], language modeling [2], machine translation [3], music generation [4], image description [5], video description [6].", "startOffset": 168, "endOffset": 171}, {"referenceID": 3, "context": "Recurrent Neural Networks (RNNs) have been successfully applied to a diverse array of tasks, such as speech recognition [1], language modeling [2], machine translation [3], music generation [4], image description [5], video description [6].", "startOffset": 190, "endOffset": 193}, {"referenceID": 4, "context": "Recurrent Neural Networks (RNNs) have been successfully applied to a diverse array of tasks, such as speech recognition [1], language modeling [2], machine translation [3], music generation [4], image description [5], video description [6].", "startOffset": 213, "endOffset": 216}, {"referenceID": 5, "context": "Recurrent Neural Networks (RNNs) have been successfully applied to a diverse array of tasks, such as speech recognition [1], language modeling [2], machine translation [3], music generation [4], image description [5], video description [6].", "startOffset": 236, "endOffset": 239}, {"referenceID": 6, "context": "As a measure to safeguard against such problem when constructing a very deep network, ResNet [7] and Highway Networks [8] were proposed.", "startOffset": 93, "endOffset": 96}, {"referenceID": 7, "context": "As a measure to safeguard against such problem when constructing a very deep network, ResNet [7] and Highway Networks [8] were proposed.", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "In creating temporally deep RNNs, Long Short-Term Memory (LSTM, [9]), Gated Recurrent Unit (GRU, [3]), and Recurrent Highway Network (RHN, [10]) were introduced to both find learning stability and model long-range temporal dependency.", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "In creating temporally deep RNNs, Long Short-Term Memory (LSTM, [9]), Gated Recurrent Unit (GRU, [3]), and Recurrent Highway Network (RHN, [10]) were introduced to both find learning stability and model long-range temporal dependency.", "startOffset": 97, "endOffset": 100}, {"referenceID": 9, "context": "In creating temporally deep RNNs, Long Short-Term Memory (LSTM, [9]), Gated Recurrent Unit (GRU, [3]), and Recurrent Highway Network (RHN, [10]) were introduced to both find learning stability and model long-range temporal dependency.", "startOffset": 139, "endOffset": 143}, {"referenceID": 10, "context": "A general concept of time-dependent adaptive computation time (ACT) is introduced into RNN in [11].", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "In Spatially Adaptive Computation Time [12], the number of executed layers for the regions of the image is dynamically adjusted.", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "Expanding on the idea recently proposed in ACT [11], with the use of an elastic gate in the form of a rectified exponentially decreasing function taking on as arguments as previous hidden state and input, the proposed model is able to evaluate the appropriate recurrent depth for each input.", "startOffset": 47, "endOffset": 51}, {"referenceID": 12, "context": "The weight update procedure is just an expansion of the idea underlying hypernetworks [13].", "startOffset": 86, "endOffset": 90}, {"referenceID": 12, "context": "And the weight parameters for all the intermediate recurrence layers are calculated based on a hypernetwork [13] that is a sub-network to generate the weights for another network.", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "To solve this problem, we propose a dynamic weight matrix utilizing the concept of hypernetworks [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 12, "context": "weight scaling of hypernetworks in [13], weight matrix decomposition proposed by Jurgen Schmidhuber in [14], etc.", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "weight scaling of hypernetworks in [13], weight matrix decomposition proposed by Jurgen Schmidhuber in [14], etc.", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "For all the experiments in this paper, Tensorflow toolkit [15] was used.", "startOffset": 58, "endOffset": 62}, {"referenceID": 15, "context": "For training the network, Adam optimizer [16] was adopted with 20 mini-batch size, 100 epochs, and 0.", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "In this subsection, we describe sequence classification experiments by using Human Activity Recognition Using Smartphones Data Set (HAR) [17].", "startOffset": 137, "endOffset": 141}, {"referenceID": 17, "context": "In this subsection, word level language modeling on the Penn TreeBank dataset [18] is described.", "startOffset": 78, "endOffset": 82}, {"referenceID": 18, "context": "The hidden activation of EI-REHN, ht, is multiplied by the transposed word embedding matrix, U based on [19, 20].", "startOffset": 104, "endOffset": 112}, {"referenceID": 19, "context": "The hidden activation of EI-REHN, ht, is multiplied by the transposed word embedding matrix, U based on [19, 20].", "startOffset": 104, "endOffset": 112}, {"referenceID": 9, "context": "For the parameter optimization, we follow the experimental settings described in [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "We just replace the \u2018RHNCell\u2019 class in the code of [10] with an \u2019EIREHNCell\u2019 class that is an implementation of the proposed model.", "startOffset": 51, "endOffset": 55}, {"referenceID": 20, "context": "In Table 4, we compare our models (Dh = 1000, 1200) with the basic LSTM [21], Pointer Sentinel networks [22], Ensemble of LSTMs [21] and RHN [10].", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "In Table 4, we compare our models (Dh = 1000, 1200) with the basic LSTM [21], Pointer Sentinel networks [22], Ensemble of LSTMs [21] and RHN [10].", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "In Table 4, we compare our models (Dh = 1000, 1200) with the basic LSTM [21], Pointer Sentinel networks [22], Ensemble of LSTMs [21] and RHN [10].", "startOffset": 128, "endOffset": 132}, {"referenceID": 9, "context": "In Table 4, we compare our models (Dh = 1000, 1200) with the basic LSTM [21], Pointer Sentinel networks [22], Ensemble of LSTMs [21] and RHN [10].", "startOffset": 141, "endOffset": 145}, {"referenceID": 20, "context": "Test LSTM [21] 66M 82.", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "4 Pointer Sentinel networks [22] 21M 72.", "startOffset": 28, "endOffset": 32}, {"referenceID": 20, "context": "9 Ensemble of LSTMs [21] - 71.", "startOffset": 20, "endOffset": 24}, {"referenceID": 9, "context": "7 RHN [10] 24M 68.", "startOffset": 6, "endOffset": 10}], "year": 2017, "abstractText": "To model time-varying nonlinear temporal dynamics in sequential data, a recurrent network capable of varying and adjusting the recurrence depth between input intervals is examined. The recurrence depth is extended by several intermediate hidden state units, and the weight parameters involved in determining these units are dynamically calculated. The motivation behind the paper lies on overcoming a deficiency in Recurrent Highway Networks and improving their performances which are currently at the forefront of RNNs: 1) Determining the appropriate number of recurrent depth in RHN for different tasks is a huge burden and just setting it to a large number is computationally wasteful with possible repercussion in terms of performance degradation and high latency. Expanding on the idea of adaptive computation time (ACT), with the use of an elastic gate in the form of a rectified exponentially decreasing function taking on as arguments as previous hidden state and input, the proposed model is able to evaluate the appropriate recurrent depth for each input. The rectified gating function enables the most significant intermediate hidden state updates to come early such that significant performance gain is achieved early. 2) Updating the weights from that of previous intermediate layer offers a richer representation than the use of shared weights across all intermediate recurrence layers. The weight update procedure is just an expansion of the idea underlying hypernetworks. To substantiate the effectiveness of the proposed network, we conducted three experiments: regression on synthetic data, human activity recognition, and language modeling on the Penn Treebank dataset. The proposed networks showed better performance than other state-of-theart recurrent networks in all three experiments.", "creator": "LaTeX with hyperref package"}}}