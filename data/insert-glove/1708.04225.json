{"id": "1708.04225", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "Deep Object-Centric Representations for Generalizable Robot Learning", "abstract": "Robotic tshimanga manipulation in complex existentially open - 14.93 world matsuda scenarios phosphofructokinase requires both rahnavard reliable peachpit physical hereditary manipulation cavani skills putbacks and diener effective and hellenised generalizable perception. edf In 2,791 this paper, we propose spahn a method where 57.94 general purpose ferrocene pretrained gyrocompass visual models ekiden serve evergreens as onur an nizer object - siyar centric 12/09 prior pavlides for the crisscrossing perception akaphol system 5x5 of a learned dwayk policy. We devise an object - hammon level sharifs attentional rudna mechanism giraffa that nene can be used to determine relevant objects 96-93 from svs a blaes few demonstrations, deleanu and then immediately 358.5 incorporate 3.85 those samajwadi objects godolphin into combretum a learned policy. A 10a-3a task - protrude independent cadix meta - attention locates agri-food possible objects cornstalks in the wwwa scene, and nolberto a mamenchisaurus task - specific n.j. attention identifies which objects marque are nessen predictive 8:21 of slipcased the demonstrations. The 280.8 scope pfennigs of biochips the peni task - .667 specific attention riserva is oin easily adjusted by showing journalistically demonstrations dictating with distractor customhouse objects representives or negating with diverse invites relevant petite-patrie objects. Our silos results indicate that this reprieves approach exhibits good generalization across object instances flatboat using very few jharkhand samples, box-like and luftman can be moondance used free-to-view to scrutinise learn terris a marrakech variety mbete of manipulation tasks lighten using reinforcement learning.", "histories": [["v1", "Mon, 14 Aug 2017 17:42:59 GMT  (9157kb,D)", "http://arxiv.org/abs/1708.04225v1", null], ["v2", "Fri, 25 Aug 2017 00:14:15 GMT  (9157kb,D)", "http://arxiv.org/abs/1708.04225v2", null], ["v3", "Tue, 26 Sep 2017 17:06:36 GMT  (5898kb,D)", "http://arxiv.org/abs/1708.04225v3", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.CV", "authors": ["coline devin", "pieter abbeel", "trevor darrell", "sergey levine"], "accepted": false, "id": "1708.04225"}, "pdf": {"name": "1708.04225.pdf", "metadata": {"source": "CRF", "title": "Deep Object-Centric Representations for Generalizable Robot Learning", "authors": ["Coline Devin", "Pieter Abbeel", "Trevor Darrell", "Sergey Levine"], "emails": ["coline@eecs.berkeley.edu", "pabbeel@eecs.berkeley.edu", "trevor@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "Keywords: vision, reinforcement learning, manipulation\n1 Introduction\nThe standard approach in robotics is to separate perception from control by using hand-designed computer vision systems to detect objects of interest and then supply their locations to the learning algorithm. Learning-based approaches for robotic manipulation offer the promise of automating the acquisition of complex behaviors. However, the robustness and generalization capabilities of these methods are limited by the range of experience seen at training time. In this paper, we explore the question of how we might design a perception system for learning-based robotic manipulation, while leveraging related vision tasks and without requiring extensive and time-consuming training of visual models for each new skill. The problem we address is that of learning policies for object manipulation tasks that require vision and that can generalize to new visual conditions. We assume a small number of demonstrations are available, but no other prior information or dataset about the specific objects to be manipulated.\nIn order to train a detector for a particular object that is robust to visual conditions, one needs either a diverse dataset for that object or hand crafted image features. With a detector trained on diverse data, this approach can generalize to different instances of an object type (e.g., different mugs). However, these methods require time and supervision, and the objects of interest must be chosen ahead of time. On the other side of the spectrum, purely end-to-end deep reinforcement learning (RL) methods are typically optimized only for the environment they were trained on, and\nar X\niv :1\n70 8.\n04 22\n5v 1\n[ cs\n.R O\n] 1\n4 A\nug 2\n01 7\nare unlikely to generalize well to environmental changes. Methods that do not impose priors on image structure require enormous amounts of data to learn from pixels. A major advantage of endto-end methods is that no object labels or supervision about what parts of the scene are important are needed.\nIn this work, we propose a synthesis of these two approaches: an object-centric prior is placed on the perceptive layers of the policy while taking advantage of deep visual representations to efficiently find task relevant objects. Our model uses region proposals to generate a set of task-independent object hypotheses and learn a task-specific attention over their deep classification features. Using models trained for object detection and classification provides robustness to environmental conditions with minimal per-task effort. By learning object-level attention from demonstrations, we avoid needing to hand-specify which objects are important. Additionally, this approach provides an easy way to \u201cfix\u201d undesired generalization. Imagine that you want a pouring policy to generalize across mugs but not cups, which would be under-specified given only demonstrations of pouring into a mug. If a couple more demonstrations are provided where a distractor object (e.g., a cup) is present and ignored, we show that finetuning the task-specific attention on these demonstrations will decrease the attention\u2019s affinity for the distractor\u2019s features.\nWe describe an object-centric approach to robot learning, where the object prior is provided by region proposal networks [16] and object information is provided by high level features from AlexNet [9]. We show that this provides the necessary information to learn real-world visual robotic tasks with RL, such as pouring and sweeping, while outperforming prior learningbased methods in generalizing across objects and environment distractors. Further analysis shows that modifying the demonstrations provides control over the scope of generalization. Videos of our results are available at https://sites.google.com/berkeley.edu/ object-representations. Code for using this method is available at https://github. com/cdevin/object-representations."}, {"heading": "2 Related Work", "text": "Our method combines prior knowledge about \u201cobjectness\u201d from pretrained visual models with an attentional mechanism for learning to detect specific task relevant objects. A number of previous works have sought to combine general objectness priors with more specific object detection in the context of robotics and other visual tasks. Ekvall et al. used region proposals and SIFT features for quickly detecting objects in the context of SLAM [3]. Prior work used an active search approach where the camera could zoom in certain parts of the receptive field to search at higher resolutions [7]. In manipulation, SIFT features have also been used for 3D pose estimation and object localization, using object-specific training data gathered individually for the task [2, 20]. Similarly to these prior works, our method constrains the observations using an object-centric prior. However, we do not require object level supervision for each task, instead using visual features from a pretrained visual model to index into the proposals from the object-centric prior. This approach drastically reduces the engineering burden for each new task, picking out task-relevant objects from a few demonstrations, and provides good generalization to object appearance, lighting, and scale, as demonstrated in our experiments.\nAn alternative to building perception systems for task-specific objects, or using pretrained models, is to learn the entire perception system end-to-end together with the control policy. A number of recent works have explored such end-to-end approaches in the context of skill learning, either for direct policy search [11, 15, 14], unsupervised learning of representations for control [6, 5], or learning predictive models [1, 4]. A major challenge with such methods is that their ability to generalize to varied scenes and objects depends entirely on the variety of objects and scenes seen during policy training. Some methods have sought to address this by collecting large amounts of data with many objects [14, 12]. In this work, we instead study how we can incorporate prior knowledge about objects from pretrained visual models, while still being able to train rich neural network control policies. In this way, we can obtain good generalization to appearance, lighting, and other nuisance variables, while only training the final policy on a single object and in a single scene."}, {"heading": "3 Deep Visuomotor Object Centric Representations", "text": "The goal of our method is to provide a simple and efficient process for quickly acquiring taskspecific visual representations in the context of policy learning. Specifically, we aim to compress an image observation into a vector \u03bd such that a robotic policy can easily be learned with \u03bd as part of the observation space. If performed properly, this compression can substantially simplify policy learning, and also drastically improve the generalization capabilities of the resulting policy when it is trained in scenes with limited diversity and visual variability. However, in order to achieve this improvement, the compression process itself must incorporate some prior knowledge about the visual world. To that end, we impose an object-centric structure on our representation, which itself is learned from prior visual data in the form of standard computer vision image datasets.\nWe define a 2-level hierarchy of attention over scenes for policy learning. The high level, which we call the meta-attention, is shared for all tasks. The meta-attention is intended to identify possible objects in the scene regardless of the task. The lower level, which we call task-specific attention, is learned per-task and identifies which of the possible objects is relevant to the task being performed.\nThe meta-attention is a function that takes an image and returns a set of object hypotheses {oi : i \u2208 [0,N)}. Each object hypothesis consists of a semantic component (given by f (oi)) and a position component (given by g(oi)). The semantic component describes the identity of the object with a feature vector, while the position component describes where the object is located within the image. Importantly, this meta-attention is reused over all tasks without retraining. Its proposals are taskindependent and it constitutes our object-centric perception prior.\nOnce the task-independent object hypotheses are obtained, we use a task-specific attention to choose objects to attend to for a given task. In the tasks we examine, an object\u2019s relevance is determined by its identity: if the task is to pour into a mug, the object that looks like a mug is the relevant object. To select which objects to pay attention to, the model learns the task-specific attention over the semantic features f (oi). In order to be able to quickly learn the task-specific attention from only a small number of demonstrations, it is parametrized as a vector w such that attention paid to oi is proportional to ew > f (oi). To attend to several different objects, multiple attention vectors can be stacked into a matrix W .\nBecause the attention is linear with respect to the semantic features, the choice of these semantic features is crucial for the flexibility and generalization capability of the method. While we could choose the features to simply correspond to semantic class (e.g., using classes from a standard image classification dataset), this would limit the flexibility of the method to identifying only those\nobject classes. If we choose overly general features, such as a histogram of oriented gradients or even raw pixels, the task-specific attention would be too limited in its ability to generalize in the presence of changes in appearance, lighting, viewpoint, or pose. To strike the right balance between flexibility and generalization, we use visual features obtained from the upper layers of a convolutional neural network trained for image classification. Such features have previously been shown to transfer effectively across visual perception tasks and provide a good general-purpose visual representation [19, 18]. An overview of the method is provided in Algorithm 1.\nAlgorithm 1 Robot Learning with Object Centric Representations 1: Train RPN on an object detection dataset (shared for all tasks). 2: Train a convolutional network for image classification (shared for all tasks). 3: for each task do 4: Collect kinesthetic demonstrations. 5: Learn W as described in Section 3.1 from the demonstrations. 6: Train control policy using reinforcement learning, with the robot\u2019s configuration and \u03bd as\nthe observation inputs. W is fixed during this step. 7: end for"}, {"heading": "3.1 Meta-Attention with Region Proposal Networks and Visual Features", "text": "Although a number of meta-attention mechanisms are possible, we use a region proposal network (RPN) [16] to provide meta-attention in our method. The RPN is a fully convolutional and end-toend trained method for object detection, where the proposals are regressed and scored in parallel. The object proposals o0, ...,oN are the crops proposed by the RPN and the position component g of oi is the bounding box coordinates of the proposal. We define the semantic component f to be a mean-pool over the region proposal crop of the convolutional features pretrained on ImageNet classification [17]. In the experiments, we use conv5 of AlexNet [9], resulting in a 256-dimensional feature vector for f (oi), which is then normalized to have magnitude 1. As the convolutional layers were pretrained for classification with a diverse dataset, the vector will include information about the contents of the crop (e.g., it may have \u201cmug-like\u201d features) that should be invariant to translation, rotation, illumination, and other variables."}, {"heading": "3.2 Learning Task-Specific Attention from Demonstrations", "text": "The learnable weights in the local attention are the values of W , which attend over the visual features of each crop. W should learn to identify what kinds of object to pay attention to for a given task. The aim of this section is to pretrain W on demonstrations to attend to task-relevant objects. Once W is learned, any reinforcement learning algorithm could be used to obtain a policy as a function of the attended objects.\nGiven a demonstration of a task, the objects that are relevant to the task will be predictive of future robot configurations. We optimize W as part of a larger neural network shown at the top of Figure 2 that aims to predict the future movement of the robot\u2019s end-effector position. The network for this is two hidden layers with 80 units each. In order to backpropagate through W , a soft attention is used.\nFirst, we use a Boltzman distribution to obtain a probability p(oi|w j) for each object proposal.\np(oi|w j) = e\nw>j f (oi) || f (oi)||2\n\u2211Ni=0 e w>j f (oi) || f (oi)||2\nThen, the soft attention is calculated by taking a weighted sum of the object locations.\n\u03bd j,soft = N\n\u2211 i=0 g(oi)p(oi|w j).\nTo obtain the prediction, \u03bdso f t is concatenated with robot joint state and end-effector state before being fed into the movement prediction network at the top of Figure 3. While f (oi) is normalized for each oi, W is not normalized to allow the optimization to control the peakiness of the attention. To encourage more discrete attention distributions, the attention is regularized to have low entropy.\nLent(w) = M\n\u2211 j=0\nN\n\u2211 i=0 \u2212p(oi|w j) log p(oi|w j)\nThe network is optimized with the Adam optimizer [8]."}, {"heading": "3.3 Handpicking Task-Specific Attention", "text": "If demonstrations of a task are not available, the local attention can be specified by choosing which object hypothesis o are relevant to the task and saving their f (o). Then during policy learning the local attention attends to the object with the most similar f (o). Unlike the prior approach, this method can only weigh each dimension of the feature vectors equally. Thus, we recommend using the handpicked features as an initialization for the method in Section 3.2."}, {"heading": "4 Experiments", "text": "We evaluate our proposed object-centric model on several real-world robotic manipulation tasks. The experiments are chosen to evaluate two metrics: the reliability of this representation for robot learning, and how it generalizes to visual changes in the environment. By attending over features trained on the data diversity found in ImageNet, we expect that policies learned from our visual representation will naturally generalize to visual changes. The hard attention over region proposals should provide robustness to distracting clutter that in a soft-argmax setting would nudge features away from the object. The aim of this evaluation is to demonstrate that the model enables both policy learning and generalization to new object instances and environments. Additionally, we show that the scope of generalization can be controlled by the objects seen during demonstrations.\nFor each experiment, 6 kinesthetic demonstrations are provided by a human. The attention vector w is learned by training a model on the demonstration data as described in Section 3.2. The attended regions learned for both tasks are shown in Figure 3. To learn to perform the task, we use the guided policy search algorithm [10], which involves training local time-varying linear-Gaussian controllers for a number of different instances of the task, which in our case correspond to different positions of the objects, and then using supervised learning to learn a single global neural network policy that can perform the task for all of the different object positions. The neural network policy takes as input the joint angles, joint velocities, end-effector positions and velocities, and the output of the perception system \u03bd , which corresponds to the attended region\u2019s bounding box coordinates. Our learned policies have 4 hidden layers with 80 hidden units each, and directly output the torques for the 7 joints of the PR2 robot used in our experiments. Note that our method can be used with any reinforcement learning algorithm, including both deep reinforcement learning methods (such as the one used in our experiments) and trajectory-centric reinforcement learning algorithms such as PI2 [21] or REPS [13]. Videos of the results can be found at the URL listed in the introduction."}, {"heading": "4.1 Generalizing across visual changes", "text": "The goal of the this task is to position a bottle to pour into a mug. Success requires the ability to locate a mug from an image and the global policy is not given the mug location. We show that\nwhile previous methods [5] are able to learn this task for the particular mug seen during training, our method can generalize to new mug instances and to cluttered environments. Success is measured by running the policy with almonds in the bottle. A rollout is marked as successful if more almonds fall into the mug than are spilled, as seen in the included video. For evaluation, eight rollouts at different mug positions are performed for the uncluttered environments and three for the cluttered ones; results are in Table 1 and environment photos are in Figure 6. While the policy was only trained on a single brown mug in a plain environment, it successfully generalizes to other mugs of various colors. By using hard attention, the visual features are robust to clutter. Interestingly, when presented with all four mugs, the policy chose to pour into the pink mug rather than the brown mug the attention was trained with. The \u201cno vision\u201d baseline is a policy trained without visual features; its behavior is to pour to the average of the different targets seen during training. The low performance of this baseline indicates that the task requires a high level of precision. We compare to the method described in [11], where policies are learned directly from raw pixels and pretrained on a labeled data for detecting the target object.\nOur model is able to generalize to new mugs of different appearances because it uses deep classification features that were trained on a wide variety of objects including mugs. An approach that learns robot skills directly from pixels such as [12] could not be expected to know that the brown mug and the pink mug are similar objects. We investigate this by training a policy from raw pixels with the architecture described in [12]. The convolution layers are pretrained on detecting the training mug in 2000 labeled images. As shown in Table 1, this policy can perform the task on the training mug and on another brown mug, but completely ignores the other mugs. This indicates that a policy learned from raw pixel images can perform well on the training environments, the kinds of features it pays attention to have no incentive to be semantic. Our method of using features pretrained on image classification defines how a policy should generalize.\nWe repeat the same procedure as above with a sweeping task. The task-specific attention is learned purely from demonstrations of apple-sweeping without a handpicked initialization. As seen in Figure 3, the learned W only attends to the apple when it is upright, a solution that is sufficient for predicting the motion of the demonstrations. The same plastic apple was used for the demonstrations and the policy learning, so the scope of attention is left up to the model. The policy was evaluated over 21 rollouts with 7 different positions. The alternative environments were evaluated for 7 rollouts; results are shown in Table 5a. A trial is marked successful if the object enters the dustpan; examples of the environment are in Figure 5b. Without vision, a policy can succeed most of the\ntime at the two center conditions by just sweeping in a straight line. However, the side conditions require positioning the broom behind the object at the correct angle before sweeping. To investigate what kinds of objects are being attended to, we ran the policy on a variety of round fruit as well as a red plush triangle. The policy adapted to the positions of the peaches and apples by positioning the broom correctly for the side conditions. The apricot and red triangle were ignored when placed at the side conditions. The results indicate that the attention is not just over color (otherwise the triangle would have been attended) nor just over shape (as the round apricot was ignored)."}, {"heading": "4.2 Learning to ignore distractors", "text": "In the previous experiment, generalizing across mugs was a desired outcome. However, it is easy to imagine that a task might require pouring specifically into the brown mug and ignoring all other\nmugs. Our method provides a simple way to tweak which features the task-specific attention is sensitive to. In order to learn a task-specific attention that has a smaller scope, a pink mug is added as a distractor during the demonstrations. At test time, the pouring policy consistently chooses the brown training mug over both the pink mug and the black-and-white mug. This indicates that including distractors in the demonstrations helps restrict the scope of attention to ignore these distractors. Figure 6a shows how an attention initialized just on the brown mug is distracted by the distractor mug. After finetuning on the demonstrations, the attention is firmly on the brown mug. In experiments, the robot poured into the correct mug 100% of the time with either the pink mug or the black and white mug present as distractors. In comparison, the attention trained solely on demonstrations without distractors preferred the pink mug over the brown mug and obtained 50% success."}, {"heading": "4.3 Increasing the scope of generalization", "text": "The attention can also be pushed to attend to more objects. For example, an attention that attends to oranges may not always generalize to other citrus fruit. However, by adding a couple demonstrations with limes and lemons, we show that the same attention can be fine tuned to attend to these additional fruit. As shown in Figure 6b, the attention only attends to oranges when first initialized, but finetuning expands the scope of the attention to include the lime and lemon present in the demonstration data. The resulting sweeping policy is robust to distractors including an apple, apricot, and a purple cup, but is confused by the orange and green cups. The round base of the citrus-colored cups perhaps appear to be idealized fruit."}, {"heading": "5 Discussion", "text": "In this paper, we proposed a visual representation for robotic skill learning that makes use of objectcentric priors from pretrained visual models to achieve robust perception for policies trained in just a single scene with a single object. Our approach uses region proposal networks as a meta-attention that picks out potential objects in the scene independently of the task, and then rapidly selects a task-specific representation via an attentional mechanism based on visual features, which can be trained from a few demonstrations. Since the visual features used to index into the object proposals are themselves invariant to differences in lighting, appearance, viewpoint, and object instance, the resulting vision system can generalize effectively to new object instances with no additional training. The attention\u2019s scope is easily controlled able by the objects seen during demonstrations. Our results indicate that this provides for a robust, generalizable, and customizable visual representation for sensorimotor skills.\nWhile our method attains good results on two real-world manipulation tasks, it has a number of limitations. First, the visual representation that is provided to the policy is constrained to correspond to image-space object coordinates. Although this is sufficient for many manipulation tasks, some tasks, such as those that require fine-grained understanding of the configuration of articulated or deformable objects, might require a more detailed representation. Second, our current system requires pretraining on standard computer vision datasets. While this limitation is also one of the strengths, leading to improved generalization capability, it requires access to auxiliary visual datasets. Lastly, our current system is still trained in a stagewise manner, with the region proposals trained on prior vision data, the attention trained from demonstration, and the policy trained from experience. An exciting direction for future work would be to enable end-to-end finetuning of the entire system, which would lift many of these limitations. Since each stage in the current method is trained with simple and scalable gradient descent methods, end-to-end training should be entirely feasible, and should improve the performance of the resulting policies on tasks that require more subtle perception mechanisms."}, {"heading": "Acknowledgments", "text": "Coline Devin is supported by Huawei Technologies and is an NSF Graduate Research Fellow. We would like to thank Ronghang Hu for providing a tensorflow port of Region Proposal Networks."}], "references": [{"title": "Learning to poke by poking: Experiential learning of intuitive physics", "author": ["P. Agrawal", "A.V. Nair", "P. Abbeel", "J. Malik", "S. Levine"], "venue": "Advances in Neural Information Processing Systems, pages 5074\u20135082,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Object recognition and full pose registration from a single image for robotic manipulation", "author": ["A. Collet", "D. Berenson", "S.S. Srinivasa", "D. Ferguson"], "venue": "Robotics and Automation, 2009. ICRA\u201909. IEEE International Conference on, pages 48\u201355. IEEE,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Object detection and mapping for service robot tasks", "author": ["S. Ekvall", "D. Kragic", "P. Jensfelt"], "venue": "Robotica, 25(2):175\u2013187,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep visual foresight for planning robot motion", "author": ["C. Finn", "S. Levine"], "venue": "International Conference on Robotics and Automation (ICRA),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep spatial autoencoders for visuomotor learning", "author": ["C. Finn", "X.Y. Tan", "Y. Duan", "T. Darrell", "S. Levine", "P. Abbeel"], "venue": "Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 512\u2013519. IEEE,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep predictive policy training using reinforcement learning", "author": ["A. Ghadirzadeh", "A. Maki", "D. Kragic", "M. Bj\u00f6rkman"], "venue": "arXiv preprint arXiv:1703.00727,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Quick 3d object detection and localization by dynamic active search with multiple active cameras", "author": ["T. Kawanishi", "H. Murase", "S. Takagi"], "venue": "Pattern Recognition, 2002. Proceedings. 16th International Conference on, volume 2, pages 605\u2013608. IEEE,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["S. Levine", "P. Abbeel"], "venue": "Advances in Neural Information Processing Systems, pages 1071\u20131079,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection", "author": ["S. Levine", "P. Pastor", "A. Krizhevsky", "D. Quillen"], "venue": "International Symposium on Experimental Robotics (ISER 2016),", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Relative entropy policy search", "author": ["J. Peters", "K. M\u00fclling", "Y. Altun"], "venue": "AAAI,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours", "author": ["L. Pinto", "A. Gupta"], "venue": "Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 3406\u20133413. IEEE,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "The curious robot: Learning visual representations via physical interactions", "author": ["L. Pinto", "D. Gandhi", "Y. Han", "Y.-L. Park", "A. Gupta"], "venue": "European Conference on Computer Vision, pages 3\u201318. Springer,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "Advances in neural information processing systems, pages 91\u201399,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Advances in neural information processing systems, pages 568\u2013576,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "A textured object recognition pipeline for color and depth image data", "author": ["J. Tang", "S. Miller", "A. Singh", "P. Abbeel"], "venue": "Robotics and Automation (ICRA), 2012 IEEE International Conference on, pages 3467\u20133474. IEEE,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "A generalized path integral control approach to reinforcement learning", "author": ["E. Theodorou", "J. Buchli", "S. Schaal"], "venue": "JMLR, 11,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": "We describe an object-centric approach to robot learning, where the object prior is provided by region proposal networks [16] and object information is provided by high level features from AlexNet [9].", "startOffset": 121, "endOffset": 125}, {"referenceID": 8, "context": "We describe an object-centric approach to robot learning, where the object prior is provided by region proposal networks [16] and object information is provided by high level features from AlexNet [9].", "startOffset": 197, "endOffset": 200}, {"referenceID": 2, "context": "used region proposals and SIFT features for quickly detecting objects in the context of SLAM [3].", "startOffset": 93, "endOffset": 96}, {"referenceID": 6, "context": "Prior work used an active search approach where the camera could zoom in certain parts of the receptive field to search at higher resolutions [7].", "startOffset": 142, "endOffset": 145}, {"referenceID": 1, "context": "In manipulation, SIFT features have also been used for 3D pose estimation and object localization, using object-specific training data gathered individually for the task [2, 20].", "startOffset": 170, "endOffset": 177}, {"referenceID": 19, "context": "In manipulation, SIFT features have also been used for 3D pose estimation and object localization, using object-specific training data gathered individually for the task [2, 20].", "startOffset": 170, "endOffset": 177}, {"referenceID": 10, "context": "A number of recent works have explored such end-to-end approaches in the context of skill learning, either for direct policy search [11, 15, 14], unsupervised learning of representations for control [6, 5], or learning predictive models [1, 4].", "startOffset": 132, "endOffset": 144}, {"referenceID": 14, "context": "A number of recent works have explored such end-to-end approaches in the context of skill learning, either for direct policy search [11, 15, 14], unsupervised learning of representations for control [6, 5], or learning predictive models [1, 4].", "startOffset": 132, "endOffset": 144}, {"referenceID": 13, "context": "A number of recent works have explored such end-to-end approaches in the context of skill learning, either for direct policy search [11, 15, 14], unsupervised learning of representations for control [6, 5], or learning predictive models [1, 4].", "startOffset": 132, "endOffset": 144}, {"referenceID": 5, "context": "A number of recent works have explored such end-to-end approaches in the context of skill learning, either for direct policy search [11, 15, 14], unsupervised learning of representations for control [6, 5], or learning predictive models [1, 4].", "startOffset": 199, "endOffset": 205}, {"referenceID": 4, "context": "A number of recent works have explored such end-to-end approaches in the context of skill learning, either for direct policy search [11, 15, 14], unsupervised learning of representations for control [6, 5], or learning predictive models [1, 4].", "startOffset": 199, "endOffset": 205}, {"referenceID": 0, "context": "A number of recent works have explored such end-to-end approaches in the context of skill learning, either for direct policy search [11, 15, 14], unsupervised learning of representations for control [6, 5], or learning predictive models [1, 4].", "startOffset": 237, "endOffset": 243}, {"referenceID": 3, "context": "A number of recent works have explored such end-to-end approaches in the context of skill learning, either for direct policy search [11, 15, 14], unsupervised learning of representations for control [6, 5], or learning predictive models [1, 4].", "startOffset": 237, "endOffset": 243}, {"referenceID": 13, "context": "Some methods have sought to address this by collecting large amounts of data with many objects [14, 12].", "startOffset": 95, "endOffset": 103}, {"referenceID": 11, "context": "Some methods have sought to address this by collecting large amounts of data with many objects [14, 12].", "startOffset": 95, "endOffset": 103}, {"referenceID": 18, "context": "Such features have previously been shown to transfer effectively across visual perception tasks and provide a good general-purpose visual representation [19, 18].", "startOffset": 153, "endOffset": 161}, {"referenceID": 17, "context": "Such features have previously been shown to transfer effectively across visual perception tasks and provide a good general-purpose visual representation [19, 18].", "startOffset": 153, "endOffset": 161}, {"referenceID": 15, "context": "Although a number of meta-attention mechanisms are possible, we use a region proposal network (RPN) [16] to provide meta-attention in our method.", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "We define the semantic component f to be a mean-pool over the region proposal crop of the convolutional features pretrained on ImageNet classification [17].", "startOffset": 151, "endOffset": 155}, {"referenceID": 8, "context": "In the experiments, we use conv5 of AlexNet [9], resulting in a 256-dimensional feature vector for f (oi), which is then normalized to have magnitude 1.", "startOffset": 44, "endOffset": 47}, {"referenceID": 7, "context": "The network is optimized with the Adam optimizer [8].", "startOffset": 49, "endOffset": 52}, {"referenceID": 9, "context": "To learn to perform the task, we use the guided policy search algorithm [10], which involves training local time-varying linear-Gaussian controllers for a number of different instances of the task, which in our case correspond to different positions of the objects, and then using supervised learning to learn a single global neural network policy that can perform the task for all of the different object positions.", "startOffset": 72, "endOffset": 76}, {"referenceID": 20, "context": "Note that our method can be used with any reinforcement learning algorithm, including both deep reinforcement learning methods (such as the one used in our experiments) and trajectory-centric reinforcement learning algorithms such as PI2 [21] or REPS [13].", "startOffset": 238, "endOffset": 242}, {"referenceID": 12, "context": "Note that our method can be used with any reinforcement learning algorithm, including both deep reinforcement learning methods (such as the one used in our experiments) and trajectory-centric reinforcement learning algorithms such as PI2 [21] or REPS [13].", "startOffset": 251, "endOffset": 255}, {"referenceID": 4, "context": "while previous methods [5] are able to learn this task for the particular mug seen during training, our method can generalize to new mug instances and to cluttered environments.", "startOffset": 23, "endOffset": 26}, {"referenceID": 10, "context": "We compare to the method described in [11], where policies are learned directly from raw pixels and pretrained on a labeled data for detecting the target object.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "An approach that learns robot skills directly from pixels such as [12] could not be expected to know that the brown mug and the pink mug are similar objects.", "startOffset": 66, "endOffset": 70}, {"referenceID": 11, "context": "We investigate this by training a policy from raw pixels with the architecture described in [12].", "startOffset": 92, "endOffset": 96}], "year": 2017, "abstractText": "Robotic manipulation in complex open-world scenarios requires both reliable physical manipulation skills and effective and generalizable perception. In this paper, we propose a method where general purpose pretrained visual models serve as an object-centric prior for the perception system of a learned policy. We devise an object-level attentional mechanism that can be used to determine relevant objects from a few demonstrations, and then immediately incorporate those objects into a learned policy. A task-independent meta-attention locates possible objects in the scene, and a task-specific attention identifies which objects are predictive of the demonstrations. The scope of the task-specific attention is easily adjusted by showing demonstrations with distractor objects or with diverse relevant objects. Our results indicate that this approach exhibits good generalization across object instances using very few samples, and can be used to learn a variety of manipulation tasks using reinforcement learning.", "creator": "LaTeX with hyperref package"}}}