{"id": "1708.04828", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2017", "title": "Multi-task Neural Network for Non-discrete Attribute Prediction in Knowledge Graphs", "abstract": "Many popular knowledge almazan graphs distillates such dopey as Freebase, papillae YAGO or torturously DBPedia phototropism maintain a list jyotish of 440th non - discrete rognoni attributes for bonds.market each b\u00f4cher entity. turnips Intuitively, these duoc attributes such meadowhall as height, sherbourne price weaponised or carlucci population word-of-mouth count cscec are able dharmasena to richly s\u00f3 characterize entities 2067 in courtown knowledge test-taker graphs. This 1996-1998 additional source abeokuta of pfandbrief information laurian may krishnamurthi help phallic to alleviate uckg the eretmochelys inherent sparsity kreta and kerr incompleteness problem that are ludwell prevalent 330th in lexicon knowledge 1.2922 graphs. Unfortunately, denounces many state - of - the - hyphy art relational southwesternmost learning milecastles models bocock ignore sumarlin this dessler information due nurkadilov to 79.28 the challenging nature of prolongation dealing with manninger non - 40.3 discrete data c\u00f6ln types mwai in attan the sternlicht inherently gregarious binary - natured killilea knowledge babajani graphs. gumede In 9.1-billion this partho paper, we cuddesdon propose a 34-25 novel multi - agip task neural network approach for b.d. both encoding and menzer prediction of non - discrete m\u00fcnsterland attribute dingdong information ghibellines in gousis a 61.85 relational setting. ancestral Specifically, hauptmann we unichem train inderjit a streete neural pengshui network servicemembers for 996-0075 triplet hutton prediction 164.5 along with a aurangabad separate network northcom for attribute value regression. Via multi - task learning, zh\u00e0o we padaung are justice able to learn jawi representations toberman of entities, boeckmann relations and anise attributes numinous that encode grignard information about stalinesque both multi-colored tasks. Moreover, risings such ewing attributes crokes are not swamping only steam-operated central leendert to nunery many ecaterina predictive tasks rutas as an double-bassist information source but intermittent also jigging as a 17.04 prediction target. Therefore, arrowed models that 120m are bludenz able to encode, gbomo incorporate and predict such information :26 in 9.83 a relational participate learning marshfield context tarabin are highly melodiya attractive as disablement well. We 2,241 show trisagion that our approach outperforms many dehumidifier state - opportunities of - spangle the - 7th-8th art troilo methods for the tasks culler of relational triplet classification and attribute tewahedo value wreath-laying prediction.", "histories": [["v1", "Wed, 16 Aug 2017 09:55:15 GMT  (1005kb,D)", "http://arxiv.org/abs/1708.04828v1", "Accepted at CIKM 2017"]], "COMMENTS": "Accepted at CIKM 2017", "reviews": [], "SUBJECTS": "cs.AI cs.IR", "authors": ["yi tay", "luu anh tuan", "minh c phan", "siu cheung hui"], "accepted": false, "id": "1708.04828"}, "pdf": {"name": "1708.04828.pdf", "metadata": {"source": "CRF", "title": "Multi-Task Neural Network for Non-discrete A\u0082ribute Prediction in Knowledge Graphs", "authors": ["Yi Tay", "Luu Anh Tuan", "Minh C. Phan", "Siu Cheung Hui"], "emails": ["ytay017@e.ntu.edu.sg", "at.luu@i2r.a-star.edu.sg", "phan0005@e.ntu.edu.sg", "asschui@ntu.edu.sg"], "sections": [{"heading": null, "text": "KEYWORDS Knowledge Graphs, Deep Learning, Neural Networks, Multi-Task Learning ACM Reference format: Yi Tay, Luu Anh Tuan, Minh C. Phan, and Siu Cheung Hui. 2017. Multi-Task Neural Network for Non-discrete A ribute Prediction in Knowledge Graphs. In Proceedings of ACM CIKM, Singapore, Nov 2017 (CIKM\u201917), 10 pages. DOI: 10.475/123 4"}, {"heading": "1 INTRODUCTION", "text": "Knowledge graphs (KGs) are semantic networks that are highly popular in many intelligent applications such as question answering, entity linking or recommendation systems. KGs are typically\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). CIKM\u201917, Singapore \u00a9 2017 Copyright held by the owner/author(s). 123-4567-24-567/08/06. . .$15.00 DOI: 10.475/123 4\nexpressed as triplets in the form of (head, relation, tail) where each entity may be connected to another entity via a semantic relation, e.g., (SteveJobs, isFounderOf, Apple). Knowledge graphs, aside from being a popular universal representation of knowledge such as ConceptNet or WordNet, are also widely adopted in practical applications. Examples of popular large scale knowledge graphs include YAGO, Freebase and Google Knowledge Graph. Given the immense popularity in many AI applications, reasoning with knowledge graphs is an extremely desirable technique. In this context, machine learning algorithms for relational reasoning are highly sought a er.\nAt its core, relational learning aims to reason and infer probabilistic estimates regarding new or unseen facts in knowledge graphs. Intuitively, this supports applications ranging from knowledge graph completion to recommendation systems. Recently, many state-of-the-art relational learning techniques [4, 9, 22, 30] have been proposed. ese techniques, which can also be considered as representation learning techniques, are concerned with modeling multi-relational data in latent space and learning continuous vector representations of entities and relations in KGs. rough reasoning in latent embedding space, probabilistic estimates can be inferred which contribute to alleviating the intrinsic problem of knowledge graph incompleteness. Furthermore, relations in knowledge graphs can be extremely sparse [38] in which relational information might be insu cient for making solid predictions or recommendations.\nUnfortunately and additionally, many of the state-of-the-art approaches are solely focused on exploiting structural information and neglect the rich a ribute information that are readily available in knowledge graphs. Intuitively, a ribute information can richly characterize entities that would help combat relational sparsity. As such, in this paper, we propose that this information should be and can be easily incorporated into relational learning algorithms in order to improve the relational learning and at the same time, enable prediction of these non-discrete a ribute information. To this end, we propose an end-to-end multi-task neural network. To give readers a be er insight behind our intuitions and motivations behind this work, we begin by providing some context pertaining to the problem at hand."}, {"heading": "1.1 Motivation", "text": "Firstly and naturally, non-discrete a ributes are commonplace in many KGs. For example, non-discrete a ributes such as the price of a product (iPhone, hasPrice, 1000) or the height of an actor (DanielRadcli e, hasHeight, 1.7m) are o en present in knowledge graphs.\nar X\niv :1\n70 8.\n04 82\n8v 1\n[ cs\n.A I]\n1 6\nA ug\n2 01\nAs such, our work is mainly concerned with both leveraging these non-discrete a ributes to improve the relational learning process and enable prediction of these a ributes. To this end, we answer the following four important questions:\n(1) Can a ribute information really help in relational learning? (2) Why do we want to predict a ributes? (3) Are there so many a ributes in KGs? (4) Why is this di cult?\n1.1.1 Can a ribute information really help in relational learning? Consider a relational learning task of completing the following triplet - (person, hasGender, ?). Traditionally, relational learning exploits statistical pa erns across relations to make predictions. e rule R1: (personA, isMotherOf, personB)\u2192 (personA, hasGender, Female) is an example of what relational learning models are capable of learning in order to make predictions. However, statistical information available through relations can be really sparse. Additionally, knowledge graphs can also be highly incomplete. As such, there is a good chance that, at test time, an entity arrives without any relation such as (isMotherOf) to make an informed decision. is is where a ribute information can be complementary and also enhance the relational learning model. In this case, it might be possible that the (hasHeight) a ribute alone is able to allow the model to predict the gender correctly. Intuitively, a ribute information can richly characterize entities in a knowledge graph. In the studied example, the non-discrete value of the a ribute (hasHeight) is strongly correlated with the relation (hasGender). In another context, the height or weight a ributes of entities (such as products) may allow relational learning models to di erentiate between cars and iphones. Intuitively, this can be also seen as leveraging entity schema information. erefore, we believe that a ribute information that can richly characterize entities will provide relational learning models with valuable knowledge.\n1.1.2 Why do we want to predict a ributes? Additionally, the importance of non-discrete a ributes goes beyond serving as an auxiliary information source but also as a prediction target. ere are mainly two ways to interpret this task. Firstly, it can be interpreted as using relational structural information as features for a regression task which forms a huge chunk of standard machine learning tasks in both research and industry. In this case there is a target a ribute that is important to be predicted such as sales forecast, the GPA of a student, or prices of products. Secondly, the motivation of a ribute prediction can also be considered identical to that of knowledge graph completion for relational triplets, e.g., a person\u2019s height might be missing in the knowledge graph and a ribute value prediction can help infer this information.\n1.1.3 Are there so many a ributes in KGs? For each entity in knowledge graphs such as Freebase, a list of relevant a ributes are maintained. In the Freebase dump Easy Freebase [2], there are already 27 million non-discrete a ribute triplets (\u2248 10%). In our extracted Freebase subgraph, we easily obtain a percentage of \u2248 33% non-discrete a ribute triplets. Hence, this further motivates our problem, i.e., we are ensured that there is su cient and reasonable amount of a ribute information. Additionally, the non-discrete nature of a ributes suggests that each triplet contains more encoded information over binary-natured relational triplets. Table 1\ndescribes some common non-discrete a ributes and their respective entity types from Freebase.\n1.1.4 Why is this di icult? Knowledge graphs are typically considered as 3D graphs, i.e., (head, relation, tail). Non-discrete attributes, can o en act as relations in which their a ributes are binned and casted as entities. However, this can easily cause the size of the knowledge graph to blow up and aggravate the scalability issue faced by many relational learning techniques. A ributes, intuitively speaking, seem to cast knowledge graphs out of its comfort zone of binary truths (0 or 1). As such, there are limited works that a empt to exploit a ribute information in the se ing of KGs [28]. While this has been done on bipartite graphs or similar networks with only one entity type, the diversity of entity types in KGs can be large. In these cases, the feature vectors of a ributes can be extremely sparse and ine ective when there are a signi cant number of entity types in the knowledge base. Moreover, incorporating feature vectors of a ributes will not enable the model to predict a ribute values. Overall, it is evident that many of the current approaches do not handle such information and more o en than not discard these a ributes completely during the training process."}, {"heading": "1.2 Contributions", "text": "In this paper, we propose a neural network approach that elegantly incorporates a ribute information and enables regression in the context of relational learning. e primary contributions of this paper are as follows:\n\u2022 We propose a novel deep learning architecture for representation learning of entities, relations and a ributes in KGs. Our uni ed framework which we call Multi-Task Knowledge Graph Neural Network (MT-KGNN) trains two networks in a multi-task fashion. Via using a shared embedding space, we are able to encode a ribute value information while learning representations. Furthermore, our network also supports the prediction of a ribute values which is a feature lacking in many relational learning methods. \u2022 For the rst time, we evaluate the ability of many relational learning approaches such as [4, 27] in the task of a ribute value prediction. is is a new experiment designed to evaluate the competency and suitability of KG representation learning techniques in encoding a ribute information.\n\u2022 Aside from being able to perform regression typed predictions in a relational context, our approach also demonstrates state-of-the-art performance on traditional relational learning benchmarks such as triplet classi cation."}, {"heading": "2 RELATEDWORK", "text": "Across the rich history of relational learning, there have been a myriad of machine learning techniques proposed to model multirelational data. While traditional relational learning dates back to probabilistic, feature engineering or rule mining approaches, it has been recently fashionable to reason over knowledge graphs in latent embedding space. is eld, also known as deep learning or representation learning, is responsible for many state-of-the-art applications in the elds of NLP and Arti cial Intelligence.\ne popular latent embedding models can be generally classi ed yet again into three categories, namely the neural network approach, the knowledge graph embedding approach and nally the factorization approach. Before we brie y describe each category, let us begin with a formal de nition of the notation that will be used throughout this paper."}, {"heading": "2.1 Problem Formulation and Notation", "text": "In this section, we brie y describe the notation used in this paper along with a simple introduction of relational learning problems.\n2.1.1 Notation. Let \u2206 = (E,R) denote a knowledge graph. E = {e1, e2...e |E |} is the set of all entities, and R = {r1, r2...r |R |} is the set of all relations. A relational triplet \u03bei \u2208 \u03be is de ned as (ei , rk , ej ) where two entities are in a relation rk and can be interpreted as a fact that exists in the knowledge graph \u2206. Since a ributes are speci c to our model, we introduce them in later sections. For latent embedding models, entities ei \u2208 E and relation ri \u2208 R are o en represented as real-valued vector or matrix representations. In order to facilitate such a design, embedding matricesWe \u2208 R |E |\u00d7n and likewise,Wr \u2208 R |R |\u00d7n are parameters of the model. To select the relevant entity or relation, we can simply perform a look-up operation via one-hot vector. As such, the inputs to our model are o en represented as indices that map into these embedding matrices.\n2.1.2 Problem Formulation. ere are mainly two popular relational learning tasks, namely triplet classi cation and link prediction. Simply speaking, the goals of both tasks are similar and are complementary to many semantic applications. Making probabilistic recommendations and mitigating the inherent incompleteness of knowledge graphs are examples of such applications. As such, the crux of many relational learning tasks is to produce a score\ns(ei , rk , ej ) \u2208 [0, 1] (1)\nthat denotes the probability or strength of a fact or triplet. Naturally, similar to many ranking problems or classi cation problems, this task can be casted into three di erent variations, i.e., pointwise, pairwise and listwise. Pointwise considers a simple binary classi - cation problem in which a sigmoid or 2-class so max layer might be used. Pairwise tries to maximize the margin between the golden facts and negative (corrupted) samples. e pairwise hinge loss can\nbe described as follows: L = \u2211 \u03be \u2208\u2206 \u2211 \u03be \u2032<\u2206 max(0,El (\u03be ) + \u03bb \u2212 El (\u03be \u2032)) (2)\nFinally, listwise considers all candidate entities, i.e., for (ei , rk , ?), consider all ej \u2208 E. Since the listwise approach is rarely used in the literature of relational learning, we do not consider the listwise approach in our paper. Moreover, the pairwise and listwise approaches require a signi cant amount of implementation e ort over the pointwise approach and as such, we consider a pointwise approach when designing our model. Finally and also intuitively, the pairwise and listwise approaches are also not suitable for attribute value regression."}, {"heading": "2.2 Neural Networks for Relational Learning", "text": "In this section, we give a brief overview of the existing state-ofthe-art neural network (NN) approaches for relational learning. Neural networks are connectionist models that form the heart of the deep learning revolution. e key driving force behind these models are that the parameters are learned via stochastic gradient descent (SGD) and backpropagation. ere are two popular neural network approaches for relational learning. e rst approach is the ER-MLP (Entity Relation Multi-Layered Perceptron) which was incepted as part of the Google Knowledge Vault Project [9]. e other approach is the Neural Tensor Network (NTN) [30] which models multiple views of dyadic interactions between entities with tensor products.\n2.2.1 ER-MLP. ER-MLP was studied as a simple neural baseline for relational learning in the Google Knowledge Vault Project. e scoring function of ER-MLP can be represented as follows:\ns(ei , rk , ej ) = \u03c3 (\u00aev> f (W>\u03d5 [ \u00aeei ; \u00aeej ; \u00aerk ])) (3)\nwhere f is a non-linear activation function such as tanh applied element-wise. v \u2208 Rh\u00d71 and W\u03d5 \u2208 R3n\u00d7h are parameters of the network. \u03c3 is the sigmoid function. h is the size of the hidden layer and n is the size of the embeddings. [; ] denotes a concatenation operator. Intuitively, ER-MLP extracts the embeddings of the entities and relations involved in the triplet and simply concatenates them. Despite its simplicity, it has been proven to be able to produce highly competitive performance in relational learning tasks.\n2.2.2 Neural Tensor Network (NTN). is highly expressive neural model was rst incepted in the elds of NLP for sentiment analysis and subsequently applied in multiple domains such as knowledge base completion [30] and question answering [29, 33]. e key intuition behind NTN is that it models multiple views of dyadic interactions between entities with a relation-speci c tensor [30]. NTN is the most expressive amongst the relational neural networks due to the high number of parameters and incredible modeling capability. e scoring function of NTN is de ned as follows:\ns(ei , rk , ej ) = u>rk f (e > i W [1:s] rk ej +Vrk [ \u00aeei ; \u00aeej ] + brk ) (4)\nwhere f is a nonlinear function applied element-wise. W [1:s]R \u2208 Rn\u00d7n\u00d7k is a tensor and each bilinear product e>i W [k ] rk ej results in a scalar value. e scalar values across s slices are concatenated\nto form the output vector h \u2208 Rk . e other parameters are in the standard form of neural networks."}, {"heading": "2.3 Embedding and Factorization Models", "text": "It is worthy to note that there exist rich and extensive research that are concerned with latent embedding models for relational learning. ese models are commonly known as translational embedding models and are improvements over the highly in uential work, TransE [4], proposed by Bordes et al. While there have been many extensions [17, 22, 31, 36, 37] over this model, the key intuition behind translational embedding models is the concept of the translation principle, i.e., |ei + rk \u2212 ej | = 0 for a golden fact.\nOn the other hand, the rst and early latent embedding models for relational learning are mostly based on tensor factorization which exploits the fact that knowledge graphs inherently follow a 3D structure. Speci cally, the general purpose Candecomp (CP) decomposition [5] was rst used in [10] to rank triplets in a semantic web se ing. Nickel et al. then proposed RESCAL [16, 28] which is an improved tensor factorization technique based on the bilinear form for relational learning. Subsequently, extensions of RESCAL such as TRESCAL [7] and RSTE [32] were proposed.\nIt is easy to see that all latent embedding models can be interpreted as neural architectures. Speci cally, in a review work by Nickel et al. [26], the tensor factorization model RESCAL is casted as a neural network. As such, the scoring function of RESCAL is de ned as:\n\u0434(ei , rk , ej ) =W >k ( \u00aeej \u2297 \u00aeei ) (5) where Wk is a relation-speci c parameter of the network and \u2297 is the tensor product. We can easily see that the parameterW \u2208 R |k |\u00d7n\u00d7n is the middle tensor in the original RESCAL. Likewise, CP can also be casted similarly but we omit the discussion due to the lack of space. Similarly, translational embedding models, i.e., TransE, TransR, etc. are all neural architectures that optimize parameters via gradient descent."}, {"heading": "2.4 Multi-Task Learning (MTL)", "text": "e key idea of Multi-Task Learning (MTL) is to utilize the correlation between related tasks to improve the performance by learning the tasks in parallel. e success of multi-task learning [6] has spurred on many multi-task neural network models especially in the eld of NLP [8, 13, 23]. While there are other variants of MTL such as multi-task feature learning [19], our work is concerned with shared representation learning, i.e., sharing parameters between two networks such that they bene t from training on multiple tasks. Many works [13, 23] have shown that multi-task learning can lead to improved performance. is serves as the major motivation behind our work. To the best of our knowledge, we are the rst multi-task learning approach in the context of knowledge graphs."}, {"heading": "2.5 Multi-Source Relational Learning", "text": "Many works have proposed incorporating an external source of information to enhance representation learning in KGs. ough similar, this is di erent from multi-task learning. In this case, external information is used to enhance learning but not as an extra task. As such, prediction of this additional information is usually\nnot supported. For example, a recently fashionable line of research is concerned with joint representations of textual information with knowledge graphs [34, 35, 40]. ere have been also many extensions of relational learning algorithms to various sources of external information such as hierarchical information [39], logic rules [11], schema information [7], path information [20] and degree-aware property information [15]."}, {"heading": "2.6 Handling Attributes in Knowledge Graphs", "text": "As mentioned, a prevalent problem is that none of the current models a empt to incorporate a ribute information especially for non-discrete a ributes. In [28], the authors proposed to use a separate matrix factorization to learn a ributes. However, the authors only explore the possibility of adding tokenized text a ributes. Furthermore, a huge problem is that each a ribute would require an additional matrix factorization operation which can be impractical. A recent work, KR-EAR [21] is a translational embedding model that was proposed to model \u2018a ribute information\u2019. eir main idea is that modeling a ributes separately can result in be er relational learning performance. However, their approach is concerned with relations that are actually a ributes, e.g, gender is an a ribute that is o en considered as a relation in most KGs. is means their approach only works with prior knowledge of knowing which relation should have been an a ribute. Furthermore, their approach does not deal with non-discrete data types. As a result, though deceptively similar, our work is distinctly di erent from theirs and cannot be meaningfully compared.\nTo the best of our knowledge, there is no approach that is able to elegantly incorporate continuous a ribute information in the context of relational learning. is can be a ributed to the inherent di culty of dealing with non-discrete oat data types in the context of the binary-natured KG. Essentially, a ributes and their nondiscrete values extend a KG to the fourth dimension."}, {"heading": "3 MULTI-TASK KNOWLEDGE GRAPH NEURAL NETWORK (MT-KGNN)", "text": "In this section, we introduce MT-KGNN, our novel deep learning architecture for both relational learning and non-discrete a ribute prediction on knowledge graphs. ere are two networks in our model, namely the Relational Network (RelNet) and the A ribute Network (A rNet). Figure 1 describes the overall model architecture. Let us begin with an introduction of the new notation pertaining to the a ribute data."}, {"heading": "3.1 Relational Learning with Attributes", "text": "Our work is concerned with utilizing a ribute triplets to both improve the relational learning process and enable regression in a relational learning se ing. First, we formally de ne the additional notations that are supplementary to the earlier problem formulation. Let A be the set of all non-discrete a ributes such as height, price or population in a knowledge graph. e new notation for a knowledge graph can then be de ned as \u2206 = (E,R,A). Note that a ribute triplets are non-discrete, i.e., a non-discrete a ribute triplet, \u03c8i \u2208 \u03c8 is de ned as (ei ,ak ,v) where ak is an a ribute in A and v is a normalized continuous value from [0, 1]. We assume that the range of each a ribute can be su ciently deduced from\nthe training data. Any value at test time that exceeds the max-min of the normalization will automatically be casted to 0 or 1."}, {"heading": "3.2 Embedding Layer", "text": "At training time, the inputs to the Relational Network are [ei , rk , ej , t] where ei , ej \u2208 Rn , rk \u2208 Rm and t , the target of classi cation is either 0 or 1. e inputs to the A ribute Network are [ai ,vi ,aj ,vj ] where ai ,aj \u2208 Rl andvi ,vj \u2208 [0, 1]. For simplicity, we will consider that m = n = l , i.e., all entity, relation and a ribute embeddings have equal dimensions. e inputs of our model are discrete onehot-encoded indices which will be passed into an embedding lookup layer to retrieve the corresponding vector representations. Here, We \u2208 R |E |\u00d7n , Wr \u2208 R |R |\u00d7n and Wa \u2208 R |A |\u00d7n are the representations learned by the model."}, {"heading": "3.3 Relational Network (RelNet)", "text": "is section introduces the Relational Network (RelNet) used in our model that models the structural and relational aspect of the knowledge graph. RelNet is a simple concatenation of the triplet passed through a nonlinear transform and nally a linear transform with sigmoid activation. e RelNet component of our model is de ned as follows:\n\u0434r el (ei , rk , ej ) = \u03c3 ( \u00aew> f (W>d [ \u00aeei ; \u00aeej ; \u00aerk ]) + br el ) (6)\nwhere f is the hyperbolic tangent function tanh. w \u2208 Rh\u00d71 and Wd \u2208 R3n\u00d7h are parameters of the network. \u03c3 is the sigmoid function and br el is a scalar bias. In order to train RelNet, we minimize the cross entropy loss function as follows:\nLr el = \u2212 N\u2211 i=1 ti log\u0434r el (\u03bei ) + (1 \u2212 ti ) log(1 \u2212 \u0434r el (\u03bei )) (7)\nwhere \u03bei denotes triplet i in batch of size N . ti is a value of {0, 1}. Cross entropy is a common loss function for non-continuous targets and is commonly used in many binary classi cation problems. At this point, it is good to note that RelNet remains identical to the ER-MLP model [9]. e contribution and novelty of our approach lies with the incorporation of the A ribute Network."}, {"heading": "3.4 Attribute Network (AttrNet)", "text": "is section introduces the A ribute Network (A rNet) used in our model. Similar to RelNet, we train a single hidden layer network by concatenation of the a ribute and entity embeddings to predict the continuous value which is a normalized value \u2208 [0, 1]. However, there are two entities in each relational triplet \u03bei \u2208 \u03be , namely the head and tail entities. erefore, there are two sides to A rNet which are described as A rNet (le ) and A rNet (right) as shown in Figure 1. e motivation of using two networks are as follows: since entities in knowledge graphs are generally considered as antisymmetric relations, e.g., entities behave di erently when they are at the head or tail position, we design the network in order to capture this relational information. As such, A rNet optimizes a joint loss function of both regression tasks. We de ne the score functions as follows:\n\u0434h (ai ) = \u03c3 (\u00aeu> f (B>[ \u00aeai ; \u00aeei ]) + bz1 ) (8)\n\u0434t (aj ) = \u03c3 (\u00aey> f (C>[ \u00aeaj ; \u00aeej ]) + bz2 ) (9) where u,y \u2208 Rha\u00d71 and B,C \u2208 R2n\u00d7ha are parameters of A rNet. ha is the size of the hidden layer and bz1 ,bz2 are scalar biases. Similar to RelNet, the nal output of each side of A rNet is a scalar value. Moreover, since A rNet models a continuous target and is formulated as a regression problem, we optimize the mean squared error (MSE) instead of the cross entropy loss function. e MSE loss function is given by:\nMSE(s, s\u2217) = 1 N N\u2211 i=1 (si \u2212 s\u2217i ) 2 (10)\nLet Lhead and Ltail be the loss functions for each side of the Attribute Network which both use the MSE loss function. e overall loss of the A ribute Network is simply the sum of both sides of the network.\nLattr = Lhead + Ltail (11) Lhead = MSE(\u0434h (ai ), (ai )\u2217) (12) Ltail = MSE(\u0434t (aj ), (aj )\u2217) (13)\nwhere (ai )\u2217, (aj )\u2217 are the ground truth labels. Similar to RelNet, we apply the constraints |e |2 \u2264 1 and |a |2 \u2264 1 for regularization."}, {"heading": "3.5 Multi-Task Optimization and Learning", "text": "In this section, we introduce a multi-task learning scheme for training both A rNet and RelNet together. Our proposed Multi-Task Knowledge Graph Neural Network (MT-KGNN) is a multi-task learning scheme for simultaneously learning from relational and attribute information. Algorithm 1 details the multi-task learning process of our approach.\n3.5.1 Multi-Task Learning Scheme. Similar to many other multitask learning techniques [23], both RelNet and A rNet are trained in an alternating fashion. Note that the input vector of A rNet contains a ributes belonging to the head and tail entities contained within the relational triplet. is not only optimizes the parameters of the network with both relational and a ribute information but also allows A rNet to gain some structural and relational context. e overall learning process can be summarized as follows:\n\u2022 Relational Training (RT) - Sample a relational triplet and train RelNet (line 7). \u2022 Attribute Training (AT) - Build input for A rNet, i.e., for each entity, head and tail in the triplet, randomly sample an a ribute to form the input vector. If either entity has no a ribute, then append a zero vector, i.e., [0,0] to the side of A rNet that corresponds to this entity (line 9). e parameters of A rNet are then updated as we train A rNet (line 10).\nAlgorithm 1 Multi-Task Knowledge Graph Neural Network Input: KG \u2206, rel triplets \u03be , a r triplets \u03c8 , batch size=\u03b2 , model hyperpa-\nrameters Output: Trained Model Parameters \u03b8 = {\u03b8r , \u03b8a }\n1: E, R, A\u2190 all entities, relations and a ributes in \u03be respectively 2: Initialize uniform (\u2212 6\u221a\nk , 6\u221a k ) for ei \u2208 E, ri \u2208 R, ai \u2208 A\n3: Initialize random normal N(0,1) for \u03b8r = { \u00aew, \u00aev, D } 4: Initialize random normal N(0,1) for \u03b8a = { \u00aeu, \u00aev, B, C } 5: for each epoch do 6: \u03c7r \u2190 shu e(\u03c7r ) 7: \u03c7r \u2190 sample batch of size \u03b2 from \u03be 8: \u03b8r , E, R \u2190 Update w.r.t Lr el (\u03c7r ) 9: \u03c7a \u2190 buildA ributes(\u03c8 , \u03c7r )\n10: \u03b8a, E, A\u2190 Update w.r.t Lattr (\u03c7a ) 11: for each k do 12: i \u2190 sample random a ribute from A 13: \u03c8 i \u2190 select all a r triplets containing i 14: \u03c7 ia \u2190 sample batch of size \u03b2 from \u03c8 i 15: \u03b8a, E, A\u2190 Update w.r.t Lattr (\u03c7 ia ) 16: end for 17: end for\n3.5.2 A ribute Specific Training (AST). Next, we introduce an alternative method for training a ributes which corresponds to line 11 to line 15 in Algorithm 1. We call this process A ribute Speci c Training (AST) which may be used in conjunction with A ribute Training (AT) or to replace the A ribute Training step mentioned earlier. In this step, we train A rNet an additional k times in an attribute speci c manner, i.e., for k times, we sample an a ribute and select \u03b2 a ribute triplets, and run them through A rNet (both le and right). e key intuition here is that we want to supply the network with a ribute information in an orderly\na ribute-speci c and focused fashion which allows the model to concentrate on a single a ribute. Furthermore, this also controls the amount of a ribute information that is learned relative to the relational information since each AST call essentially trains the network once with a ribute information. Empirically, we found that for both triplet classi cation and a ribute value prediction, using AST improves the performance.\n3.5.3 Regularization. Finally, the last post-processing step that our model takes is to normalize all embeddings to be constrained within the Euclidean ball. At the end of training RelNet and A rNet (multiple times due to AST), we apply the constraints of |e\u2217 |2 \u2264 1, |r\u2217 |2 \u2264 1 and |a\u2217 |2 \u2264 1 for regularization and preventing over ing. Moreover, we apply dropout of d = 0.5 at the hidden layers of both RelNet and A rNet.\n3.5.4 Shared Representations. In a nutshell, the core motivation to this multi-task scheme is as follows: Since the entity embeddings \u00aeei \u2208 E are used in both Lattr and Lr el when RelNet and A rNet are trained separately, the entity embeddings are updated from both tasks. As such, the entity embeddings are learned from both the relational task as well as the a ribute task."}, {"heading": "3.6 Complexity Analysis", "text": "In this section, we study the complexity of our proposed model relative to many other state-of-the-art models. Table 2 reports the space and time complexity of all models. Additionally, we include an estimate on the number of parameters on the YAGO subgraph that we use in our experiments. Our proposed MT-KGNN does not incur much parameter cost over the baseline ER-MLP. MT-KGNN also shares entity embeddings across RelNet and A rNet. erefore, the additional cost is only derived from a ribute embeddings (which are usually much smaller) and additional hidden layer parameters for A rNet.\nPertaining to time complexity, our model is equivalent to training ER-MLP multiple times (which is dependent on the hyperparameter k if AST is used). Similar to space complexity, this is still less operations compared to models requiring operations in quadratic scale such as TransR, NTN or RESCAL."}, {"heading": "4 EXPERIMENTAL EVALUATION", "text": "We conduct two experiments to evaluate our model. First, we provide an experimental analysis of our model in the standard relational learning benchmark task of Relational Triplet Classi cation. is is in concert with many works such as [22, 30]. Second, we design another experiment of a ribute value prediction to investigate our model\u2019s ability to perform regression-like prediction which contributes to the key feature of our model."}, {"heading": "4.1 Datasets", "text": "We use two datasets in our experiments constructed from two carefully selected web-scale KGs.\n\u2022 YAGO [14] is a semantic knowledge base that aggregates data from various sources including WikiPedia. YAGO contains many famous places and people which naturally contain many a ribute values such as height, weight, age, population size, etc. We construct a subset of YAGO by removing entities that appear less than 25 times and further ltered away entities that do not contain a ribute information. We name our dataset YG24K since it contains \u2248 24K entities. \u2022 Freebase [3] is a widely used benchmark knowledge base for many relational learning tasks. We use the dataset dump Easy Freebase [2] since the public API is no longer available. Due to the large size of Freebase, we extract a domain-speci c dataset from Easy Freebase involving organizations. e construction of this dataset is detailed as follows: \u2013 We randomly selected entities of type organization\nand selected 22 relations that are closely related to the domain such as (industry), (school type), (academic adviser) and (founded by). \u2013 Subsequently, using these seed entity nodes we randomly included relational and a ribute triplets within a k-hop distance. Examples of a ributes in this dataset include (num employees), (num postgraduates) and (height), etc. e end result is a dataset with 28K entities which we call FB28K. Note that since we applied the random k-hop selection of nodes in the knowledge graph, the extracted subgraph would contain entities and relations across several domains.\nFor all datasets, we split the relational and a ribute triplets into an 80/10/10 split of train/development/test. Subsequently, we lter triplets in the test and development sets that contain entities not found in the training set. e statistics of the two datasets is given in Table 3.\nOn FB28K, 33% of the total number of triplets are a ribute triplets. On the other hand, 15% of the total number of triplets in YG24K are a ribute triplets."}, {"heading": "4.2 Algorithms Compared", "text": "For our experiments, we compare our proposed MT-KGNN approach with many state-of-the-art models.\n\u2022 CANDECOMP (CP) [10] is a classical tensor modeling technique that scores each triplet using s(ei , rk , ej ) = ei rk ej . \u2022 RESCAL [27] is a tensor factorization approach based on\nbilinear tensor products. e scoring function of RESCAL isW >k ( \u00aeej \u2297 \u00aeei ). \u2022 TransE [4] is a translational embedding model. A seminal work by Bordes et al. that models relational triplets with |h + r \u2212 t | \u2248 0. \u2022 TransR [22] is an extension of TransE that proposes using matrix projection to model relation speci c vector space. e scoring function of TransR is |Mrh + r \u2212Mt t | \u2248 0. \u2022 ER-MLP [9] is the baseline neural network approach that learns representations via a concatenation operator of triplets. ER-MLP enables a direct comparison with our model to study the e ects of incorporating non-discrete a ributes. \u2022 Neural Tensor Network (NTN) [30] is a highly expressive model that combines a bilinear tensor product with a MLP. We use a se ing of s = 4 following [30] where s is the number of tensor slices. \u2022 Multi-task Knowledge Graph Neural Network (MTKGNN) is the proposed approach in this paper. We use k = 4 for AST (A ribute Speci c Training) in conjunction with AT (A ribute Training)."}, {"heading": "4.3 Implementation Details", "text": "We implement all models ourselves in TensorFlow [1]. All algorithms are optimized with the Adam optimizer [18] with an initial learning rate of 10\u22123 and have their embeddings (entity, relation and a ribute when applicable) set to 50 dimensions. All algorithms optimize the sigmoid cross entropy loss function except TransE and TransR which we minimize the pairwise hinge loss as stated in their original papers. For both RESCAL and CP, we consider the neural network adaptation [26]. For ER-MLP and MT-KGNN, we set the size of all hidden layers to be 100, and dropout to be d = 0.5 with the tanh activation function. All models have their entity and relation embeddings constrained to |e |2 \u2264 1 and |r |2 \u2264 1. If relation embeddings are matrices instead of vectors, the norm is restricted to 3 instead. e batch size is set to 500 and the number of iterations is xed to 500 iterations for all models. Additionally, due to the sensitivity to hyperparameters of TransE and TransR, we consider margins amongst {1.0, 2.0, 4.0} and take the model with the best performance on the development set. All experiments were conducted on a machine running Linux with a NVIDIA GTX1070 GPU."}, {"heading": "4.4 Experiment 1 - Relational Triplet Classi cation", "text": "In this experiment, we demonstrate the ability of our model on a standard relational learning benchmark of triplet classi cation which is essentially a binary classi cation task. is benchmark task has been widely adopted in many works [9, 17, 30]. e task\nis as follows: Given a triplet of (h, r , t), we classify it as being 1 or 0 (true or false). We follow the experimental procedure of [30] closely. e aim of this experiment is to show that additional a ribute information help in standard relational learning tasks.\n4.4.1 Evaluation Procedure and Metrics. In order to perform classi cation, we perform negative sampling following [30]. For each positive triplet in our train/development/test sets, we include a corrupt triplet. We do this by randomly replacing either the head or tail entity. e nal positive to negative ratio is therefore 1:1. For evaluation purposes, we use two popular metrics for binary classi cation, namely accuracy and AUC (Area Under Curve). Regarding the metric of accuracy, we determine a threshold where scores above are classi ed as positive and vice versa. is threshold is determined by maximizing the accuracy across the development set.\n4.4.2 Experimental Results. Table 4 shows the results of the triplet classi cation experiment. Our proposed MT-KGNN achieves the state-of-the-art performance on both YG24K and FB28K. Our proposed approach outperforms the baseline ER-MLP by at least 2% \u2212 3% and models like TransE by 3% \u2212 4%. is suggests that our multi-task learning scheme is highly e ective for the relational learning task. Evidently, the a ribute information is able to increase the accuracy by 3% in both datasets and as a whole achieve about an improvement of > 5% \u2212 6% accuracy as compared to models such as CP or TransR. Pertaining to the relative performance of the compared baselines, the best performing result is obtained from ER-MLP. On the other hand, NTN produces poorer results. We also observe that models of higher complexity seem to perform worse than their counterparts with smaller number of parameters. For example, RESCAL performs be er than NTN, and TransE outperforms TransR."}, {"heading": "4.5 Experiment 2 - Attribute Value Prediction", "text": "In this task, we evaluate the capability of our model to perform regression, i.e., predictions of non-discrete a ributes in KGs. For all compared baselines other than ours, prediction is performed using a ribute-speci c Linear Regression classi ers trained using the learned embeddings as features. Apart from validating the performance of our proposed model, the additional aims of this experiment are as follows:\n(1) RQ1: In the context of a ribute prediction, are there useful information encoded in entity embeddings produced by relational learning models? (2) RQ2: Is there any di erence between di erent relational learning models in encoding a ribute information, e.g., is TransE or RESCAL be er than ER-MLP when the learned embeddings are used as features? (3) RQ3: Which is be er for encoding a ribute information? Knowledge graph embeddings or word embeddings?\ne impact of this experiment is meaningful as it derives insight regarding the plausibility of knowledge graph embeddings as features for standard machine learning tasks. Note that the usage of supervised classi ers for evaluating the quality of embeddings has been substantiated in many works [12, 24].\n4.5.1 Implementation Details. For all KG embedding methods, we train embeddings of size n = 50 for 500 iterations with the same se ings as Experiment 1. By using these embeddings as features, we train a separate linear regression classi er using Stochastic Gradient Descent (SGD) with learning rate amongst {10\u22122, 10\u22123, 10\u22124} for 25 epochs since we empirically found that 25 epochs are su cient for the model to converge on the development set. Similar to the rst experiment, we report the performance of the model that performed best on the development set.\n4.5.2 Additional Baselines. Aside from knowledge graph embeddings, we also introduce several baselines.\n\u2022 Random Guess (R-GUESS) randomly generates a value v \u2208 [0, 1] as a prediction. \u2022 Random Init (R-INIT) trains the supervised classi er\non an embedding matrix that is initialized randomly with U (\u22120.01, 0.01). As such, the purpose is to detect if the knowledge graph embedding is producing be er performance than random. \u2022 SkipGram [25] or also known as Word2Vec is a highly popular language modeling approach that learns continuous vector representations of words from textual documents. For SkipGram, we use the pre-trained vectors of 300 dimensions released by Google. For each entity in the KG, we form a term embedding by averaging the vectors of multiple words.\nFor our proposed approach, we use the MT-KGNN network directly for prediction. For this task, we removed the optional a ribute training step and simply use AST with k = 4.\n4.5.3 Evaluation Metrics. In this task, we use three popular evaluation metrics for evaluating the regression problem, namely the RMSE (Root Mean Square Error), Mean Absolute Error (MAE) and R2 (R squared). ese metrics are de ned as follows:\nRMSE = \u221a 1 n n\u2211 i=1 (yi \u2212 y\u0302i )2 (14)\nMAE = 1 n n\u2211 i=1 |yi \u2212 y\u0302i | (15)\nR2 = 1 \u2212 \u2211n i=1(yi \u2212 y\u0302i )2\u2211n i=1 (y\u0302i \u2212 y\u0304i )2\n(16)\nwhere yi is the ground truth, y\u0302i is the prediction and y\u0304i is the mean y value. RMSE and MAE measure the t of the prediction by reporting the amount of error relative to the ground truth. e main di erence is that RMSE penalizes larger mistakes to a larger extent due to its quadratic nature. e R2 metric is a common metric in linear regression that expresses the percentage of the variation in the ground truth that is explained by the prediction. Note that the R2 metric can result in negative values which essentially means that the prediction ts worse than a horizontal line.\n4.5.4 Experimental Results. Table 5 reports the results of the a ribute value prediction. Firstly, we observe that MT-KGNN achieves the best results. Moreover, there is a distinct and signi - cant margin as compared to all other relational learning methods. Notably, there is about\u2248 30% decrease in RMSE across both datasets relative to almost all relational learning embedding models. In addition, our model is the only model that achieves a decent R2 score (> 0). Hence, to the best of our knowledge, our approach is the only model that is able to predict a ribute values in knowledge graphs by exploiting relational information without the use of any feature engineering. Moreover, aside from relation-speci c linear regression models that have to be built for each relation, our model handles this with only a single model that works with all a ributes collectively.\nSecondly, we observe that relational learning models perform signi cantly be er over R-INIT and R-GUESS. As such, we can conclude that there is at least some useful information encoded in relational learning models that are useful for a ribute prediction. is answers RQ1 though we believe that the information encoded to be minimal. irdly, we observe that the performance of all relational learning baselines are relatively indistinguishable from one another. e performance of TransE, TransR, NTN, RESCAL, ER-MLP and CP produce approximately similar results. As such, pertaining to RQ2, we conclude that the performance of di erent relational learning models do not di er much from each other. Finally, the performance of SkipGram is slightly worse than relational learning models. To answer RQ3, we are able to conclude, based\non the empirical evidence, that relational learning models produce slightly more useful features compared to word embeddings."}, {"heading": "4.6 Discussion and Analysis", "text": "e aim of this section is to provide additional insights pertaining to our proposed approach. Firstly, we perform an ablation study to show the relative e ect of RelNet and AST. Secondly, we extract the a ribute embeddings from MT-KGNN and analyze them.\n4.6.1 Ablation Studies. Table 6 reports our ablation study on FB28K for a ribute value prediction. Speci cally, we removed RelNet and AST from MT-KGNN to see how much each process contributes to the overall performance of MT-KGNN. Naturally, AST would be expected to be more important. As such, the main investigation is pertaining to the e ect of relational triplets on a ribute prediction. We observe that there are indeed observable improvements in performance when using RelNet. For instance, the R2 metric is improved by 17 points while MAE and RMSE are also improved considerably.\n4.6.2 alitative Analysis. In this section, we inspect the learned a ribute representations of our proposed MT-KGNN. Using cosine similarity, we nd the nearest neighbors of the a ribute embeddings. Table 7 shows some examples. It can be clearly seen semantically relevant a ributes are closer to each other in the vector space. For example, num postgraduates and num undergraduates are close in the vector space probably due to their belonging to similar entities (universities). Additionally, a ributes such as oors, space and height are a ributes belonging to building entities. As such, this ascertains the representation learning ability of our model since semantically similar a ributes are clustered together in the vector space."}, {"heading": "5 CONCLUSION", "text": "We introduced a novel concept of incorporating non-discrete attribute values in relational learning. Non-discrete a ributes have traditionally been challenging to deal with as they do not t intuitively into the binary-nature of KGs. As such, our proposed MT-KGNN is a multi-task neural architecture that can elegantly\nincorporate and leverage this information. It has demonstrated the state-of-the-art performance in the relational task of triplet classication and a ribute value prediction. In both tasks, we observe that the relational and a ribute information are complementary to each other."}], "references": [{"title": "TensorFlow: Large-Scale Machine", "author": ["Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S. Corrado", "Andy Davis", "Je\u0082rey Dean", "Ma\u008ahieu Devin", "Sanjay Ghemawat", "Ian Goodfellow", "Andrew Harp", "Geo\u0082rey Irving", "Michael Isard", "Yangqing Jia", "Rafal Jozefowicz", "Lukasz Kaiser", "Manjunath Kudlur", "Josh Levenberg", "Dan Man\u00e9", "Rajat Monga", "Sherry Moore", "Derek Murray", "Chris Olah", "Mike Schuster", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wa\u008aenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": "Learning on Heterogeneous Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Easy access to the freebase dataset", "author": ["Hannah Bast", "Florian B\u00e4urle", "Bj\u00f6rn Buchhold", "Elmar Hau\u00dfmann"], "venue": "In 23rd International World Wide Web Conference,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Freebase: A Shared Database of Structured General Human Knowledge", "author": ["Kurt D. Bollacker", "Robert P. Cook", "Patrick Tu\u0089s"], "venue": "In Proceedings of the Twenty-Second AAAI Conference on Arti\u0080cial Intelligence, July 22-26,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Translating Embeddings for Modeling Multi-relational Data. In Advances in Neural Information Processing Systems", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garc\u0131\u0301a-Dur\u00e1n", "Jason Weston", "Oksana Yakhnenko"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "PARAFAC: Tutorial and applications", "author": ["Richard Bro"], "venue": "Chemometrics and Intelligent Lab. Syst. 38,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Multitask learning", "author": ["Rich Caruana"], "venue": "In Learning to learn", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Typed Tensor Decomposition of Knowledge Bases for Relation Extraction", "author": ["Kai-Wei Chang", "Wen-tau Yih", "Bishan Yang", "Christopher Meek"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "A uni\u0080ed architecture for natural language processing: deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Machine Learning, Proceedings of the Twenty-Fi\u0087h International Conference (ICML", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Knowledge vault: a web-scale approach to probabilistic knowledge fusion", "author": ["Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "\u008comas Strohmann", "Shaohua Sun", "Wei Zhang"], "venue": "In \u008ae 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201914,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "TripleRank: Ranking Semantic Web Data by Tensor Decomposition", "author": ["\u008comas Franz", "Antje Schultz", "Sergej Sizov", "Ste\u0082en Staab"], "venue": "In \u008ae Semantic Web - ISWC 2009, 8th International Semantic Web Conference,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Jointly Embedding Knowledge Graphs and Logical Rules", "author": ["Shu Guo", "\u008ban Wang", "Lihong Wang", "Bin Wang", "Li Guo"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Distributional vectors encode referential a\u008aributes", "author": ["Abhijeet Gupta", "Gemma Boleda", "Marco Baroni", "Sebastian Pad\u00f3"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "author": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "venue": "CoRR abs/1611.01587", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "YAGO2: A Spatially and Temporally Enhanced Knowledge Base from Wikipedia: Extended Abstract", "author": ["Johannes Ho\u0082art", "Fabian M. Suchanek", "Klaus Berberich", "Gerhard Weikum"], "venue": "IJCAI", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "MEmbER: Max-Margin Based Embeddings for Entity Retrieval", "author": ["Shoaib Jameel", "Zied Bouraoui", "Steven Schockaert"], "venue": "In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "A latent factor model for highly multi-relational data", "author": ["Rodolphe Jena\u008aon", "Nicolas Le Roux", "Antoine Bordes", "Guillaume Obozinski"], "venue": "In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Knowledge Graph Embedding via Dynamic Mapping Matrix", "author": ["Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations (ICLR)", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Multi-Task Model and Feature Joint Learning", "author": ["Ya Li", "Xinmei Tian", "Tongliang Liu", "Dacheng Tao"], "venue": "In Proceedings of the Twenty-Fourth International Joint Conference on Arti\u0080cial Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Modeling Relation Paths for Representation Learning of Knowledge Bases", "author": ["Yankai Lin", "Zhiyuan Liu", "Huan-Bo Luan", "Maosong Sun", "Siwei Rao", "Song Liu"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Knowledge Representation Learning with Entities, A\u008aributes and Relations", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun"], "venue": "In Proceedings of the Twenty-Fi\u0087h International Joint Conference on Arti\u0080cial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Arti\u0080cial Intelligence, January 25-30,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Recurrent Neural Network for Text Classi\u0080cation with Multi-Task Learning", "author": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang"], "venue": "In Proceedings of the Twenty-Fi\u0087h International Joint Conference on Arti\u0080cial Intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Learning Term Embeddings for Taxonomic Relation Identi\u0080cation Using Dynamic Weighting Neural Network", "author": ["Anh Tuan Luu", "Yi Tay", "Siu Cheung Hui", "See-Kiong Ng"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Je\u0082rey Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "A Review of Relational Machine Learning for Knowledge Graphs", "author": ["Maximilian Nickel", "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich"], "venue": "Proc. IEEE 104,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "A \u008cree-Way Model for Collective Learning on Multi-Relational Data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Factorizing YAGO: scalable machine learning for linked data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 21st World Wide Web Conference", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Convolutional Neural Tensor Network Architecture for Community-Based \u008bestion Answering", "author": ["Xipeng Qiu", "Xuanjing Huang"], "venue": "In Proceedings of the Twenty-Fourth International Joint Conference on Arti\u0080cial Intelligence,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Non-Parametric Estimation of Multiple Embeddings for Link Prediction on Dynamic Knowledge Graphs", "author": ["Yi Tay", "Anh Tuan Luu", "Siu Cheung Hui"], "venue": "In Proceedings of the \u008airty- First AAAI Conference on Arti\u0080cial Intelligence, February", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2017}, {"title": "Random Semantic Tensor Ensemble for Scalable Knowledge Graph Link Prediction", "author": ["Yi Tay", "Anh Tuan Luu", "Siu Cheung Hui", "Falk Brauer"], "venue": "In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, WSDM 2017,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2017}, {"title": "Learning to Rank \u008bestion Answer Pairs with Holographic Dual LSTM Architecture", "author": ["Yi Tay", "Minh C. Phan", "Anh Tuan Luu", "Siu Cheung Hui"], "venue": "In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2017}, {"title": "Text-enhanced representation learning for knowledge graph", "author": ["Zhigang Wang", "Juanzi Li"], "venue": "In International Joint Conference on Arti\u0080cial Intelligence", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Knowledge Graph and Text Jointly Embedding", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Knowledge Graph Embedding by Translating on Hyperplanes", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Arti\u0080cial Intelligence, July", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "TransG : A Generative Model for Knowledge Graph Embedding", "author": ["Han Xiao", "Minlie Huang", "Xiaoyan Zhu"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "An Interpretable Knowledge Transfer Model for Knowledge Base Completion", "author": ["Qizhe Xie", "Xuezhe Ma", "Zihang Dai", "Eduard H. Hovy"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2017}, {"title": "Representation Learning of Knowledge Graphs with Hierarchical Types", "author": ["Ruobing Xie", "Zhiyuan Liu", "Maosong Sun"], "venue": "In Proceedings of the Twenty-Fi\u0087h International Joint Conference on Arti\u0080cial Intelligence,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Aligning Knowledge and Text Embeddings by Entity Descriptions", "author": ["Huaping Zhong", "Jianwen Zhang", "Zhen Wang", "Hai Wan", "Zheng Chen"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "Recently, many state-of-the-art relational learning techniques [4, 9, 22, 30] have been proposed.", "startOffset": 63, "endOffset": 77}, {"referenceID": 8, "context": "Recently, many state-of-the-art relational learning techniques [4, 9, 22, 30] have been proposed.", "startOffset": 63, "endOffset": 77}, {"referenceID": 21, "context": "Recently, many state-of-the-art relational learning techniques [4, 9, 22, 30] have been proposed.", "startOffset": 63, "endOffset": 77}, {"referenceID": 29, "context": "Recently, many state-of-the-art relational learning techniques [4, 9, 22, 30] have been proposed.", "startOffset": 63, "endOffset": 77}, {"referenceID": 37, "context": "Furthermore, relations in knowledge graphs can be extremely sparse [38] in which relational information might be insu\u0081cient for making solid predictions or recommendations.", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "In the Freebase dump Easy Freebase [2], there are already 27 million non-discrete a\u008aribute triplets (\u2248 10%).", "startOffset": 35, "endOffset": 38}, {"referenceID": 27, "context": "As such, there are limited works that a\u008aempt to exploit a\u008aribute information in the se\u008aing of KGs [28].", "startOffset": 98, "endOffset": 102}, {"referenceID": 3, "context": "\u2022 For the \u0080rst time, we evaluate the ability of many relational learning approaches such as [4, 27] in the task of a\u008aribute value prediction.", "startOffset": 92, "endOffset": 99}, {"referenceID": 26, "context": "\u2022 For the \u0080rst time, we evaluate the ability of many relational learning approaches such as [4, 27] in the task of a\u008aribute value prediction.", "startOffset": 92, "endOffset": 99}, {"referenceID": 0, "context": "s(ei , rk , ej ) \u2208 [0, 1] (1)", "startOffset": 19, "endOffset": 25}, {"referenceID": 8, "context": "\u008ce \u0080rst approach is the ER-MLP (Entity Relation Multi-Layered Perceptron) which was incepted as part of the Google Knowledge Vault Project [9].", "startOffset": 139, "endOffset": 142}, {"referenceID": 29, "context": "\u008ce other approach is the Neural Tensor Network (NTN) [30] which models multiple views of dyadic interactions between entities with tensor products.", "startOffset": 53, "endOffset": 57}, {"referenceID": 29, "context": "\u008cis highly expressive neural model was \u0080rst incepted in the \u0080elds of NLP for sentiment analysis and subsequently applied in multiple domains such as knowledge base completion [30] and question answering [29, 33].", "startOffset": 175, "endOffset": 179}, {"referenceID": 28, "context": "\u008cis highly expressive neural model was \u0080rst incepted in the \u0080elds of NLP for sentiment analysis and subsequently applied in multiple domains such as knowledge base completion [30] and question answering [29, 33].", "startOffset": 203, "endOffset": 211}, {"referenceID": 32, "context": "\u008cis highly expressive neural model was \u0080rst incepted in the \u0080elds of NLP for sentiment analysis and subsequently applied in multiple domains such as knowledge base completion [30] and question answering [29, 33].", "startOffset": 203, "endOffset": 211}, {"referenceID": 29, "context": "\u008ce key intuition behind NTN is that it models multiple views of dyadic interactions between entities with a relation-speci\u0080c tensor [30].", "startOffset": 132, "endOffset": 136}, {"referenceID": 3, "context": "\u008cese models are commonly known as translational embedding models and are improvements over the highly in\u0083uential work, TransE [4], proposed by Bordes et al.", "startOffset": 126, "endOffset": 129}, {"referenceID": 16, "context": "While there have been many extensions [17, 22, 31, 36, 37] over this model, the key intuition behind translational embedding models is the concept of the translation principle, i.", "startOffset": 38, "endOffset": 58}, {"referenceID": 21, "context": "While there have been many extensions [17, 22, 31, 36, 37] over this model, the key intuition behind translational embedding models is the concept of the translation principle, i.", "startOffset": 38, "endOffset": 58}, {"referenceID": 30, "context": "While there have been many extensions [17, 22, 31, 36, 37] over this model, the key intuition behind translational embedding models is the concept of the translation principle, i.", "startOffset": 38, "endOffset": 58}, {"referenceID": 35, "context": "While there have been many extensions [17, 22, 31, 36, 37] over this model, the key intuition behind translational embedding models is the concept of the translation principle, i.", "startOffset": 38, "endOffset": 58}, {"referenceID": 36, "context": "While there have been many extensions [17, 22, 31, 36, 37] over this model, the key intuition behind translational embedding models is the concept of the translation principle, i.", "startOffset": 38, "endOffset": 58}, {"referenceID": 4, "context": "Speci\u0080cally, the general purpose Candecomp (CP) decomposition [5] was \u0080rst used in [10] to rank triplets in a semantic web se\u008aing.", "startOffset": 62, "endOffset": 65}, {"referenceID": 9, "context": "Speci\u0080cally, the general purpose Candecomp (CP) decomposition [5] was \u0080rst used in [10] to rank triplets in a semantic web se\u008aing.", "startOffset": 83, "endOffset": 87}, {"referenceID": 15, "context": "then proposed RESCAL [16, 28] which is an improved tensor factorization technique based on the bilinear form for relational learning.", "startOffset": 21, "endOffset": 29}, {"referenceID": 27, "context": "then proposed RESCAL [16, 28] which is an improved tensor factorization technique based on the bilinear form for relational learning.", "startOffset": 21, "endOffset": 29}, {"referenceID": 6, "context": "Subsequently, extensions of RESCAL such as TRESCAL [7] and RSTE [32] were proposed.", "startOffset": 51, "endOffset": 54}, {"referenceID": 31, "context": "Subsequently, extensions of RESCAL such as TRESCAL [7] and RSTE [32] were proposed.", "startOffset": 64, "endOffset": 68}, {"referenceID": 25, "context": "[26], the tensor factorization model RESCAL is casted as a neural network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "\u008ce success of multi-task learning [6] has spurred on many multi-task neural network models especially in the \u0080eld of NLP [8, 13, 23].", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "\u008ce success of multi-task learning [6] has spurred on many multi-task neural network models especially in the \u0080eld of NLP [8, 13, 23].", "startOffset": 121, "endOffset": 132}, {"referenceID": 12, "context": "\u008ce success of multi-task learning [6] has spurred on many multi-task neural network models especially in the \u0080eld of NLP [8, 13, 23].", "startOffset": 121, "endOffset": 132}, {"referenceID": 22, "context": "\u008ce success of multi-task learning [6] has spurred on many multi-task neural network models especially in the \u0080eld of NLP [8, 13, 23].", "startOffset": 121, "endOffset": 132}, {"referenceID": 18, "context": "While there are other variants of MTL such as multi-task feature learning [19], our work is concerned with shared representation learning, i.", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "Many works [13, 23] have shown that multi-task learning can lead to improved performance.", "startOffset": 11, "endOffset": 19}, {"referenceID": 22, "context": "Many works [13, 23] have shown that multi-task learning can lead to improved performance.", "startOffset": 11, "endOffset": 19}, {"referenceID": 33, "context": "For example, a recently fashionable line of research is concerned with joint representations of textual information with knowledge graphs [34, 35, 40].", "startOffset": 138, "endOffset": 150}, {"referenceID": 34, "context": "For example, a recently fashionable line of research is concerned with joint representations of textual information with knowledge graphs [34, 35, 40].", "startOffset": 138, "endOffset": 150}, {"referenceID": 39, "context": "For example, a recently fashionable line of research is concerned with joint representations of textual information with knowledge graphs [34, 35, 40].", "startOffset": 138, "endOffset": 150}, {"referenceID": 38, "context": "\u008cere have been also many extensions of relational learning algorithms to various sources of external information such as hierarchical information [39], logic rules [11], schema information [7], path information [20] and degree-aware property information [15].", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "\u008cere have been also many extensions of relational learning algorithms to various sources of external information such as hierarchical information [39], logic rules [11], schema information [7], path information [20] and degree-aware property information [15].", "startOffset": 164, "endOffset": 168}, {"referenceID": 6, "context": "\u008cere have been also many extensions of relational learning algorithms to various sources of external information such as hierarchical information [39], logic rules [11], schema information [7], path information [20] and degree-aware property information [15].", "startOffset": 189, "endOffset": 192}, {"referenceID": 19, "context": "\u008cere have been also many extensions of relational learning algorithms to various sources of external information such as hierarchical information [39], logic rules [11], schema information [7], path information [20] and degree-aware property information [15].", "startOffset": 211, "endOffset": 215}, {"referenceID": 14, "context": "\u008cere have been also many extensions of relational learning algorithms to various sources of external information such as hierarchical information [39], logic rules [11], schema information [7], path information [20] and degree-aware property information [15].", "startOffset": 254, "endOffset": 258}, {"referenceID": 27, "context": "In [28], the authors proposed to use a separate matrix factorization to learn a\u008aributes.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "A recent work, KR-EAR [21] is a translational embedding model that was proposed to model \u2018a\u008aribute information\u2019.", "startOffset": 22, "endOffset": 26}, {"referenceID": 0, "context": ", a non-discrete a\u008aribute triplet, \u03c8i \u2208 \u03c8 is de\u0080ned as (ei ,ak ,v) where ak is an a\u008aribute in A and v is a normalized continuous value from [0, 1].", "startOffset": 140, "endOffset": 146}, {"referenceID": 0, "context": "\u008ce inputs to the A\u008aribute Network are [ai ,vi ,aj ,vj ] where ai ,aj \u2208 Rl andvi ,vj \u2208 [0, 1].", "startOffset": 86, "endOffset": 92}, {"referenceID": 8, "context": "At this point, it is good to note that RelNet remains identical to the ER-MLP model [9].", "startOffset": 84, "endOffset": 87}, {"referenceID": 0, "context": "Similar to RelNet, we train a single hidden layer network by concatenation of the a\u008aribute and entity embeddings to predict the continuous value which is a normalized value \u2208 [0, 1].", "startOffset": 175, "endOffset": 181}, {"referenceID": 22, "context": "Similar to many other multitask learning techniques [23], both RelNet and A\u008arNet are trained in an alternating fashion.", "startOffset": 52, "endOffset": 56}, {"referenceID": 26, "context": "RESCAL [27, 28] Nem + Nrn2 1.", "startOffset": 7, "endOffset": 15}, {"referenceID": 27, "context": "RESCAL [27, 28] Nem + Nrn2 1.", "startOffset": 7, "endOffset": 15}, {"referenceID": 3, "context": "278M (m2 +m)Nt TransE [4] Nem + Nrn 1.", "startOffset": 22, "endOffset": 25}, {"referenceID": 21, "context": "202M Nt TransR [22] Nem + Nr (mn) 1.", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "278M 2mnNt ER-MLP [9] Nem + Nrn + ((2m + n) \u00d7 h) + h 1.", "startOffset": 18, "endOffset": 21}, {"referenceID": 29, "context": "217M (3mh + h)Nt NTN [30] Nem + Nr (n2s + 2ns + 2s) 1.", "startOffset": 21, "endOffset": 25}, {"referenceID": 21, "context": "\u008cis is in concert with many works such as [22, 30].", "startOffset": 42, "endOffset": 50}, {"referenceID": 29, "context": "\u008cis is in concert with many works such as [22, 30].", "startOffset": 42, "endOffset": 50}, {"referenceID": 13, "context": "\u2022 YAGO [14] is a semantic knowledge base that aggregates data from various sources including WikiPedia.", "startOffset": 7, "endOffset": 11}, {"referenceID": 2, "context": "\u2022 Freebase [3] is a widely used benchmark knowledge base for many relational learning tasks.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "We use the dataset dump Easy Freebase [2] since the public API is no longer available.", "startOffset": 38, "endOffset": 41}, {"referenceID": 9, "context": "\u2022 CANDECOMP (CP) [10] is a classical tensor modeling technique that scores each triplet using s(ei , rk , ej ) = ei rk ej .", "startOffset": 17, "endOffset": 21}, {"referenceID": 26, "context": "\u2022 RESCAL [27] is a tensor factorization approach based on bilinear tensor products.", "startOffset": 9, "endOffset": 13}, {"referenceID": 3, "context": "\u2022 TransE [4] is a translational embedding model.", "startOffset": 9, "endOffset": 12}, {"referenceID": 21, "context": "\u2022 TransR [22] is an extension of TransE that proposes using matrix projection to model relation speci\u0080c vector space.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "\u2022 ER-MLP [9] is the baseline neural network approach that learns representations via a concatenation operator of triplets.", "startOffset": 9, "endOffset": 12}, {"referenceID": 29, "context": "\u2022 Neural Tensor Network (NTN) [30] is a highly expressive model that combines a bilinear tensor product with a MLP.", "startOffset": 30, "endOffset": 34}, {"referenceID": 29, "context": "We use a se\u008aing of s = 4 following [30] where s is the number of tensor slices.", "startOffset": 35, "endOffset": 39}, {"referenceID": 0, "context": "We implement all models ourselves in TensorFlow [1].", "startOffset": 48, "endOffset": 51}, {"referenceID": 17, "context": "All algorithms are optimized with the Adam optimizer [18] with an initial learning rate of 10\u22123 and have their embeddings (entity, relation and a\u008aribute when applicable) set to 50 dimensions.", "startOffset": 53, "endOffset": 57}, {"referenceID": 25, "context": "For both RESCAL and CP, we consider the neural network adaptation [26].", "startOffset": 66, "endOffset": 70}, {"referenceID": 8, "context": "\u008cis benchmark task has been widely adopted in many works [9, 17, 30].", "startOffset": 57, "endOffset": 68}, {"referenceID": 16, "context": "\u008cis benchmark task has been widely adopted in many works [9, 17, 30].", "startOffset": 57, "endOffset": 68}, {"referenceID": 29, "context": "\u008cis benchmark task has been widely adopted in many works [9, 17, 30].", "startOffset": 57, "endOffset": 68}, {"referenceID": 29, "context": "We follow the experimental procedure of [30] closely.", "startOffset": 40, "endOffset": 44}, {"referenceID": 29, "context": "In order to perform classi\u0080cation, we perform negative sampling following [30].", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "Note that the usage of supervised classi\u0080ers for evaluating the quality of embeddings has been substantiated in many works [12, 24].", "startOffset": 123, "endOffset": 131}, {"referenceID": 23, "context": "Note that the usage of supervised classi\u0080ers for evaluating the quality of embeddings has been substantiated in many works [12, 24].", "startOffset": 123, "endOffset": 131}, {"referenceID": 0, "context": "\u2022 Random Guess (R-GUESS) randomly generates a value v \u2208 [0, 1] as a prediction.", "startOffset": 56, "endOffset": 62}, {"referenceID": 24, "context": "\u2022 SkipGram [25] or also known as Word2Vec is a highly popular language modeling approach that learns continuous vector representations of words from textual documents.", "startOffset": 11, "endOffset": 15}], "year": 2017, "abstractText": "Many popular knowledge graphs such as Freebase, YAGO or DBPedia maintain a list of non-discrete a\u008aributes for each entity. Intuitively, these a\u008aributes such as height, price or population count are able to richly characterize entities in knowledge graphs. \u008cis additional source of information may help to alleviate the inherent sparsity and incompleteness problem that are prevalent in knowledge graphs. Unfortunately, many state-of-the-art relational learning models ignore this information due to the challenging nature of dealing with non-discrete data types in the inherently binary-natured knowledge graphs. In this paper, we propose a novel multi-task neural network approach for both encoding and prediction of non-discrete a\u008aribute information in a relational setting. Speci\u0080cally, we train a neural network for triplet prediction along with a separate network for a\u008aribute value regression. Via multi-task learning, we are able to learn representations of entities, relations and a\u008aributes that encode information about both tasks. Moreover, such a\u008aributes are not only central to many predictive tasks as an information source but also as a prediction target. \u008cerefore, models that are able to encode, incorporate and predict such information in a relational learning context are highly a\u008aractive as well. We show that our approach outperforms many state-ofthe-art methods for the tasks of relational triplet classi\u0080cation and a\u008aribute value prediction.", "creator": "LaTeX with hyperref package"}}}