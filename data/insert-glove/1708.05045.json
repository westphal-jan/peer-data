{"id": "1708.05045", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2017", "title": "Cross-lingual Entity Alignment via Joint Attribute-Preserving Embedding", "abstract": "skurdal Entity alignment xiaopeng is the weer task spazio of bizilj finding irishwoman entities nadelmann in two knowledge bases (KBs) that promoters represent crappy the inter-group same real - world arvidson object. 39-24 When facing KBs in 12:30 different employer-employee natural guaymas languages, conventional youyi cross - lingual 40c entity alignment methods rely ubaidi on cornflakes machine zawidz translation to snowcocks eliminate locators the language barriers. These approaches often kiseki suffer from pitscottie the expandability uneven quality of foreclosing translations kropf between kaling languages. fenway While jiong recent semi-independent embedding - manzala based techniques encode entities megalonyx and adumin relationships oosthuizen in edlund KBs cometh and kita do not need operadora machine translation vallette for bangalter cross - newtok lingual entity alignment, a waclaw significant loped number of one-to-many attributes remain 21.83 largely unexplored. In this paper, mooroopna we propose tarino a dreamlands joint rikshospitalet attribute - preserving a300-600 embedding kastelorizo model jihadist for bevington cross - gennosuke lingual pshaw entity transhydrogenase alignment. dvir It jointly stellated embeds the structures of mnet two langi KBs into a unified spacetime vector space un-sponsored and gabras further e-journal refines 69.10 it parviainen by leveraging firewalking attribute delaye correlations in romal the KBs. dreith Our experimental variegated results ephialtes on real - world 1859-60 datasets 26.16 show shiloah that this appanoose approach vonette significantly maar outperforms the mugshot state - socialisme of - malayasia the - art goon embedding approaches for rear-drive cross - lingual lato entity enchanter alignment 3-2 and could 34.81 be complemented farokh with sidewalks methods based on machine blueprinting translation.", "histories": [["v1", "Wed, 16 Aug 2017 19:30:17 GMT  (898kb)", "http://arxiv.org/abs/1708.05045v1", null], ["v2", "Tue, 26 Sep 2017 02:06:08 GMT  (898kb)", "http://arxiv.org/abs/1708.05045v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.DB", "authors": ["zequn sun", "wei hu", "chengkai li"], "accepted": false, "id": "1708.05045"}, "pdf": {"name": "1708.05045.pdf", "metadata": {"source": "CRF", "title": "Cross-lingual Entity Alignment via Joint Attribute-Preserving Embedding", "authors": ["Zequn Sun", "Wei Hu", "Chengkai Li"], "emails": ["zqsun.nju@gmail.com,", "whu@nju.edu.cn", "cli@uta.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 8.\n05 04\n5v 1\n[ cs\n.C L\n] 1\nKeywords: cross-lingual entity alignment, knowledge base embedding, joint attribute-preserving embedding"}, {"heading": "1 Introduction", "text": "In the past few years, knowledge bases (KBs) have been successfully used in lots of AI-related areas such as Semantic Web, question answering and Web mining. Various KBs cover a broad range of domains and store rich, structured real-world facts. In a KB, each fact is stated in a triple of the form (entity, property, value), in which value can be either a literal or an entity. The sets of entities, properties, literals and triples are denoted by E,P, L and T , respectively. Blank nodes are ignored for simplicity. There are two types of properties\u2014relationships (R) and attributes (A)\u2014and correspondingly two types of triples, namely relationship triples and attribute triples. A relationship triple tr \u2208 E \u00d7 R \u00d7 E describes the relationship between two entities, e.g. (Texas, hasCapital, Austin), while\nan attribute triple tr \u2208 E\u00d7A\u00d7L gives a literal attribute value to an entity, e.g. (Texas, areaTotal, \u201c696241.0\u201d).\nAs widely noted, KBs often suffer from two problems: (i) Low coverage. Different KBs are constructed by different parties using different data sources. They contain complementary facts, which makes it imperative to integrate multiple KBs. (ii) Multi-linguality gap. To support multi-lingual applications, a growing number of multi-lingual KBs and language-specific KBs have been built. This makes it both necessary and beneficial to integrate cross-lingual KBs.\nEntity alignment is the task of finding entities in two KBs that refer to the same real-world object. It plays a vital role in automatically integrating multiple KBs. This paper focuses on cross-lingual entity alignment. It can help construct a coherent KB and deal with different expressions of knowledge across diverse natural languages. Conventional cross-lingual entity alignment methods rely on machine translation, of which the accuracy is still far from perfect. Spohr et al. [21] argued that the quality of alignment in cross-lingual scenarios heavily depends on the quality of translations between multiple languages.\nFollowing the popular translation-based embedding models [1,15,22], a few studies leveraged KB embeddings for entity alignment and achieved promising results [5,11]. Embedding techniques learn low-dimensional vector representations (i.e., embeddings) of entities and encode various semantics (e.g. types) into them. Focusing on KB structures, the embedding-based methods provide an alternative for cross-lingual entity alignment without considering their natural language labels.\nThere remain several challenges in applying embedding methods to crosslingual entity alignment. First, to the best of our knowledge, most existing KB embedding models learn embeddings based solely on relationship triples. However, we observe that attribute triples account for a significant portion of KBs. For example, we count triples of infobox facts from English DBpedia (2016-04),3 and find 58,181,947 attribute triples, which are three times as many as relationship triples (the number is 18,598,409). Facing the task of entity alignment, attribute triples can provide additional information to embed entities, but how to incorporate them into cross-lingual embedding models remains largely unexplored. Second, thanks to the Linking Open Data initiative, there exist some aligned entities and properties between KBs, which can serve as bridge between them. However, as discovered in [5], the existing alignment between cross-lingual KBs usually accounts for a small proportion. So how to make the best use of it is crucial for embedding cross-lingual KBs.\nTo deal with the above challenges, we introduce a joint attribute-preserving embedding model for cross-lingual entity alignment. It employs two modules, namely structure embedding (SE) and attribute embedding (AE), to learn embeddings based on two facets of knowledge (relationship triples and attribute triples) in two KBs, respectively. SE focuses on modeling relationship structures of two KBs and leverages existing alignment given beforehand as bridge to overlap their structures. AE captures the correlations of attributes (i.e. whether these\n3 http://wiki.dbpedia.org/downloads-2016-04\nattributes are commonly used together to describe an entity) and clusters entities based on attribute correlations. Finally, it combines SE and AE to jointly embed all the entities in the two KBs into a unified vector space Rd, where d denotes the dimension of the vectors. The aim of our approach is to find latent cross-lingual target entities (i.e. truly-aligned entities that we want to discover) for a source entity by searching its nearest neighbors in Rd. We expect the embeddings of latent aligned cross-lingual entities to be close to each other.\nIn summary, the main contributions of this paper are as follows:\n\u2013 We propose an embedding-based approach to cross-lingual entity alignment, which does not depend on machine translation between cross-lingual KBs. \u2013 We jointly embed the relationship triples of two KBs with structure embedding and further refine the embeddings by leveraging attribute triples of KBs with attribute embedding. To the best of our knowledge, there is no prior work learning embeddings of cross-lingual KBs while preserving their attribute information. \u2013 We evaluated our approach on real-world cross-lingual datasets from DBpedia. The experimental results show that our approach largely outperformed two state-of-the-art embedding-based methods for cross-lingual entity alignment. Moreover, it could be complemented with conventional methods based on machine translation.\nThe rest of this paper is organized as follows. We discuss the related work on KB embedding and cross-lingual KB alignment in Section 2. We describe our approach in detail in Section 3, and report experimental results in Section 4. Finally, we conclude this paper with future work in Section 5."}, {"heading": "2 Related Work", "text": "We divide the related work into two subfields: KB embedding and cross-lingual KB alignment. We discuss them in the rest of this section."}, {"heading": "2.1 KB Embedding", "text": "In recent years, significant efforts have been made towards learning embeddings of KBs. TransE [1], the pioneer of translation-based methods, interprets a relationship vector as the translation from the head entity vector to its tail entity vector. In other words, if a relationship triple (h, r, t) holds, h + r \u2248 t is expected. TransE has shown its great capability of modeling 1-to-1 relations and achieved promising results for KB completion. To further improve TransE, later work including TransH [22] and TransR [15] was proposed. Additionally, there exist a few non-translation-based approaches to KB embedding [2,18,20].\nBesides, several studies take advantage of knowledge in KBs to improve embeddings. Krompa\u00df et al. [13] added type constraints to KB embedding models and enhanced their performance on link prediction. KR-EAR [14] embeds attributes additionally by modeling attribute correlations and obtains good results\non predicting entities, relationships and attributes. But it only learns attribute embeddings in a single KB, which hinders its application to cross-lingual cases. Besides, KR-EAR focuses on the attributes whose values are from a small set of entries, e.g. values of \u201cgender\u201d are {Female, Male}. It may fail to model attributes whose values are very sparse and heterogeneous, e.g. \u201cname\u201d, \u201clabel\u201d and \u201ccoordinate\u201d. RDF2Vec [19] uses local information of KB structures to generate sequences of entities and employs language modeling approaches to learn entity embeddings for machine learning tasks. For cross-lingual tasks, [12] extends NTNKBC [4] for cross-lingual KB completion. [7] uses a neural network approach that translates English KBs into Chinese to expand Chinese KBs."}, {"heading": "2.2 Cross-lingual KB Alignment", "text": "Existing work on cross-lingual KB alignment generally falls into two categories: cross-lingual ontology matching and cross-lingual entity alignment. For crosslingual ontology matching, Fu et al. [8,9] presented a generic framework, which utilizes machine translation tools to translate labels to the same language and uses monolingual ontology matching methods to find mappings. Spohr et al. [21] leveraged translation-based label similarities and ontology structures as features for learning cross-lingual mapping functions by machine learning techniques (e.g. SVM). In all these works, machine translation is an integral component.\nFor cross-lingual entity alignment, MTransE [5] incorporates TransE to encode KB structures into language-specific vector spaces and designs five alignment models to learn translation between KBs in different languages with seed alignment. JE [11] utilizes TransE to embed different KBs into a unified space with the aim that each seed alignment has similar embeddings, which is extensible to the cross-lingual scenario. Wang et al. [23] proposed a graph model, which only leverages language-independent features (e.g. out-/inlinks) to find cross-lingual links between Wiki knowledge bases. Gentile et al. [10] exploited embedding-based methods for aligning entities in Web tables. Different from them, our approach jointly embeds two KBs together and leverages attribute embedding for improvement."}, {"heading": "3 Cross-lingual Entity Alignment via KB Embedding", "text": "In this section, we first introduce notations and the general framework of our joint attribute-preserving embedding model. Then, we elaborate on the technical details of the model and discuss several key design issues.\nWe use lower-case bold-face letters to denote the vector representations of the corresponding terms, e.g., (h, r, t) denotes the vector representation of triple (h, r, t). We use capital bold-face letters to denote matrices, and we use superscripts to denote different KBs. For example, E(1) denotes the representation matrix for entities in KB1 in which each row is an entity vector e (1)."}, {"heading": "3.1 Overview", "text": "The framework of our joint attribute-preserving embedding model is depicted in Fig. 1. Given two KBs, denoted by KB1 and KB2, in different natural languages and some pre-aligned entity or property pairs (called seed alignment, denoted by superscript (1,2)), our model learns the vector representations of KB1 and KB2 and expects the latent aligned entities to be embedded closely.\nFollowing TransE [1], we interpret a relationship as the translation from the head entity to the tail entity, to characterize the structure information of KBs. We let each pair in the seed alignment share the same representation to serve as bridge between KB1 and KB2 to build an overlay relationship graph, and learn representations of all the entities jointly under a unified vector space via structure embedding (SE). The intuition is that two alignable KBs are likely to have a number of aligned triples, e.g. (Washington, capitalOf,America) in English and its correspondence (Washington, capitaleDes, E\u0301tats-Unis) in French. Based on this, SE aims at learning approximate representations for the latent aligned triples between the two KBs.\nHowever, SE only constrains that the learned representations must be compatible within each relationship triple, which causes the disorganized distribution of some entities due to the sparsity of their relationship triples. To alleviate this incoherent distribution, we leverage attribute triples for helping embed entities based on the observation that the latent aligned entities usually have a high degree of similarity in attribute values. Technically, we overlook specific attribute values by reason of their complexity, heterogeneity and cross-linguality. Instead, we abstract attribute values to their range types, e.g. (Tom, age, \u201c12\u201d) to (Tom, age, Integer), where Integer is the abstract range type of value \u201c12\u201d. Then, we carry out attribute embedding (AE) on abstract attribute triples to capture the correlations of cross-lingual and mono-lingual attributes, and calculate the similarities of entities based on them. Finally, the attribute similarity constraints are combined with SE to refine representations by clustering entities with high attribute correlations. In this way, our joint model preserves both relationship and attribute information of the two KBs.\nWith entities represented as vectors in a unified embedding space, the alignment of latent cross-lingual target entities for a source entity can be conducted by searching the nearest cross-lingual neighbors in this space."}, {"heading": "3.2 Structure Embedding", "text": "The aim of SE is to model the geometric structures of two KBs and learn approximate representations for latent aligned triples. Formally, given a relationship triple tr = (h, r, t), we expect h + r = t. To measure the plausibility of tr, we define the score function f(tr) = \u2016h+ r\u2212 t\u2016 2 2. We prefer a lower value of f(tr) and want to minimize it for each relationship triple.\nFig 2 gives an example about how SE models the geometric structures of two KBs with seed alignment. In Phase (1), we initialize all the vectors randomly and let each pair in seed alignment overlap to build the overlay relationship graph. In order to show the triples intuitively in the figure, we regard an entity as a point in the vector space and move relationship vectors to start from their head entities. Note that, currently, entities and relationships distribute randomly. In Phase (2), we minimize scores of triples and let vector representations compatible within each relationship triple. For example, the relationship capitalOf would tend to be close to capitaleDes because they share the same head entity and tail entity. In the meantime, the entity America and its correspondence E\u0301tats-Unis would move closely to each other due to their common head entity and approximate relationships. Therefore, SE is a dynamic spreading process. The ideal state after training is shown as Phase (3). We can see that the latent aligned entities America and E\u0301tats-Unis lie together.\nFurthermore, we detect that negative triples (a.k.a. corrupted triples), which have been widely used in translation-based embedding models [1,15,22], are also valuable to SE. Considering that another English entity China and its latent aligned French one Chine happen to lie closely to America, SE may take the Chine as a candidate for America by mistake due to their short distance. Negative triples would help reduce the occurrence of this coincidence. If we generate a negative triple tr\u2032 = (Washington, capitalOf, China) and learn a high score for tr\u2032, China would keep a distance away from America. As we enforce the length of any embedding vector to 1, the score function f has a constant maximum. Thus, we would like to minimize \u2212f(tr\u2032) to learn a high score for tr\u2032.\nIn summary, we prefer lower scores for existing triples (positives) and higher scores for negatives, which leads to minimize the following objective function:\nOSE = \u2211\ntr\u2208T\n\u2211\ntr\u2032\u2208T \u2032 tr\n( f(tr) \u2212 \u03b1f(tr\u2032) ) , (1)\nwhere T denotes the set of all positive triples and T \u2032tr denotes the associated negative triples for tr generated by replacing either its head or tail by a random entity (but not both at the same time). \u03b1 is a ratio hyper-parameter that weights positive and negative triples and its range is [0, 1]. It is important to remember that each pair in the seed alignment share the same embedding during training, in order to bridge two KBs."}, {"heading": "3.3 Attribute Embedding and Entity Similarity Calculation", "text": "Attribute Embedding We call a set of attributes correlated if they are commonly used together to describe an entity. For example, attributes longitude, latitude and place name are correlated because they are widely used together to describe a place. Moreover, we want to assign a higher correlation to the pair of longitude and latitude because they have the same range type. We use seed entity pairs to establish correlations between cross-lingual attributes. Given an aligned entity pair (e(1), e(2)), we regard the attributes of e(1) as correlated ones for each attribute of e(2), and vice versa. We expect attributes with high correlations to be embedded closely.\nTo capture the correlations of attributes, AE borrows the idea from Skipgram [16], a very popular model that learns word embeddings by predicting the context of a word given the word itself. Similarly, given an attribute, AE wants to predict its correlated attributes. In order to leverage the range type information, AE minimizes the following objective function:\nOAE = \u2212 \u2211\n(a,c)\u2208H\nwa,c \u00b7 log p(c|a), (2)\nwhere H denotes the set of positive (a, c) pairs, i.e., c is actually a correlated attribute of a, and the term log p(c|a) denotes the probability. To prevent all the vectors from having the same value, we adopt the negative sampling approach [17] to efficiently parameterize Eq. (2), and log p(c|a) is replaced with the term as follows:\nlog \u03c3(a \u00b7 c) + \u2211\n(a,c\u2032)\u2208H\u2032 a\nlog \u03c3(\u2212a \u00b7 c\u2032), (3)\nwhere \u03c3(x) = 11+e\u2212x . H \u2032 a is the set of negative pairs for attribute a generated according to a log-uniform base distribution, assuming that they are all incorrect. We set wa,c = 1 if a and c have different range types, otherwise wa,c = 2 to increase their probability of tending to be similar. In this paper, we distinguish four kinds of abstract range types, i.e., Integer, Double,Datetime and String (as default). Note that it is easy to extend to more types.\nEntity Similarity Calculation Given attribute embeddings, we take the representation of an entity to be the normalized average of its attribute vectors, i.e., e = [ \u2211\na\u2208Ae a] 1 , where Ae is the set of attributes of e and [.]1 denotes the\nnormalized vector. We have two matrices of vector representations for entities in two KBs, E (1) AE \u2208 R n(1) e \u00d7d for KB1 and E (2) AE \u2208 R n(2) e \u00d7d for KB2, where each row is an entity vector, and n (1) e , n (2) e are the numbers of entities in KB1,KB2, respectively. We use the cosine distance to measure the similarities between entities. For two entities e, e\u2032, we have sim(e, e\u2032) = cos(e, e\u2032) = e\u00b7e \u2032\n||e||||e\u2032|| = e \u00b7 e \u2032, as the length\nof any embedding vector is enforced to 1. The cross-KB similarity matrix S(1,2) \u2208 R n(1) e \u00d7n(2) e between KB1 and KB2, as well as the inner similarity matrices S (1) \u2208 R n(1) e \u00d7n(1) e for KB1 and S (2) \u2208 Rn (2) e \u00d7n(2) e for KB2, are defined as follows:\nS(1,2) = E (1) AEE (2)\u22a4 AE , S (1) = E (1) AEE (1)\u22a4 AE , S (2) = E (2) AEE (2)\u22a4 AE . (4)\nA similarity matrix S holds the cosine similarities among entities and Si,j is the similarity between the i-th entity in one KB and the j-th entity in the same or the other KB. We discard lower values of S because a low similarity of two entities indicates that they are likely to be different. So, we set the entry Si,j = 0 if Si,j < \u03c4 , where \u03c4 is a threshold and can be set based on the average similarity of seed entity pairs. In this paper, we fix \u03c4 = 0.95 for inner similarity matrices and 0.9 for cross-KB similarity matrix, to achieve high accuracy."}, {"heading": "3.4 Joint Attribute-Preserving Embedding", "text": "We want similar entities across KBs to be clustered to refine their vector representations. Inspired by [25], we use the matrices of pairwise similarities between entities as supervised information and minimize the following objective function:\nOS = \u2016E (1) SE \u2212 S (1,2)E (2) SE\u2016\n2\nF\n+ \u03b2(\u2016E (1) SE \u2212 S (1)E (1) SE\u2016\n2 F + \u2016E (2) SE \u2212 S (2)E (2) SE\u2016 2 F ), (5)\nwhere \u03b2 is a hyper-parameter that balances similarities between KBs and their inner similarities. ESE \u2208 R ne\u00d7d denotes the matrix of entity vectors for one KB in SE with each row an entity vector. S(1,2)E (2) SE calculates latent vectors of entities in KB1 by accumulating vectors of entities in KB2 based on their similarities. By minimizing \u2016E (1) SE \u2212 S (1,2)E (2) SE\u2016 2 F , we expect similar entities across KBs to be embedded closely. The two inner similarity matrices work in the same way.\nTo preserve both the structure and attribute information of two KBs, we jointly minimize the following objective function:\nOjoint = OSE + \u03b4OS , (6)\nwhere \u03b4 is a hyper-parameter weighting OS ."}, {"heading": "3.5 Discussions", "text": "We discuss and analyze our joint attribute-preserving embedding model in the following aspects:\nObjective Function for Structure Embedding SE is translation-based embedding model but its objective function (see Eq. (1)) does not follow the margin-based ranking loss function below, which is used by many previous KB embedding models [1]:\nO = \u2211\ntr\u2208T\n\u2211\ntr\u2032\u2208T \u2032 tr\nmax[\u03b3 + f(tr)\u2212 f(tr\u2032), 0]. (7)\nEq. (7) aims at distinguishing positive and negative triples, and expects that their scores can be separated by a large margin. However, for the cross-lingual entity alignment task, in addition to the large margin between their scores, we also want to assign lower scores to positive triples and higher scores to negative triples. Therefore, we choose Eq. (1) instead of Eq. (7).\nIn contrast, JE [11] uses the margin-based ranking loss from TransE [1], while MTransE [5] does not have this as it does not use negative triples. However, as explained in Section 3.2, we argue that negative triples are effective in distinguishing the relations between entities. Our experimental results reported in Section 4.4 also demonstrate the effectiveness of negative triples.\nTraining We initialize parameters such as vectors of entities, relations and attributes randomly based on a truncated normal distribution, and then optimize Eqs. (2) and (6) with a gradient descent optimization algorithm called AdaGrad [6]. Instead of directly optimizing Ojoint, our training process involves two optimizers to minimize OSE and \u03b4OS independently. At each epoch, the two optimizers are executed alternately. When minimizing OSE , f(tr) and \u2212\u03b1f(tr\n\u2032) can also be optimized alternately.\nThe length of any embedding vector is enforced to 1 for the following reasons: (i) this constraint prevents the training process from trivially minimizing the objective function by increasing the embedding norms and shaping the embeddings, (ii) it limits the randomness of entity and relationship distribution in the training process, and (iii) it fixes the mismatch between the inner product in Eq. (3) and the cosine similarity to measure embeddings [24].\nOur model is also scalable in training. The structure embedding belongs to the translation-based embedding models, which have already been proved to be capable of learning embeddings at large scale [1]. We use sparse representations for matrices in Eq. (5) for saving memory. Additionally, the memory cost to compute Eq. (4) can be reduced using a divide-and-conquer strategy.\nParameter Complexity The parameter complexity of our joint model is O ( d(ne + nr + na) ) , where ne, nr, na are the numbers of entities, relationships\nand attributes, respectively. d is the dimension of the embeddings. Considering that nr, na \u226a ne in practice and the seed alignment share vectors in training, the complexity of the model is roughly linear to the number of total entities.\nSearching Latent Aligned Entities Because the length of each vector always equals 1, the cosine distance between entities of the two KBs can be calculated as D = E (1) SEE (2)\u22a4 SE . Thus, the nearest entities can be obtained by simply sorting each row of D in descending order. For each source entity, we expect the rank of its truly-aligned target entity to be the first few."}, {"heading": "4 Evaluation", "text": "In this section, we report our experiments and results on real-world cross-lingual datasets. We developed our approach, called JAPE, using TensorFlow4\u2014a very popular open-source software library for numerical computation. Our experiments were conducted on a personal workstation with an Intel Xeon E3 3.3 GHz CPU and 128 GB memory. The datasets, source code and experimental results are accessible at this website5."}, {"heading": "4.1 Datasets", "text": "We selected DBpedia (2016-04) to build three cross-lingual datasets. DBpedia is a large-scale multi-lingual KB including inter-language links (ILLs) from entities of English version to those in other languages. In our experiments, we extracted 15 thousand ILLs with popular entities from English to Chinese, Japanese and French respectively, and considered them as our reference alignment (i.e., gold standards). Our strategy to extract datasets is that we randomly selected an ILL pair s.t. the involved entities have at least 4 relationship triples and then extracted relationship and attribute infobox triples for selected entities. The statistics of the three datasets are listed in Table 1, which indicate that the number of involved entities in each language is much larger than 15 thousand, and attribute triples contribute to a significant portion of the datasets.\n4 https://www.tensorflow.org/ 5 https://github.com/nju-websoft/JAPE"}, {"heading": "4.2 Comparative Approaches", "text": "As aforementioned, JE [11] and MTransE [5] are two representative embeddingbased methods for entity alignment. In our experiments, we used our best effort to implement the two models as they do not release any source code or software currently. We conducted them on the above datasets as comparative approaches. Specifically, MTransE has five variants in its alignment model, where the fourth performs best according to the experiments of its authors. Thus, we chose this variant to represent MTransE. We followed the implementation details reported in [5,11] and complemented other unreported details with careful consideration. For example, we added a strong orthogonality constraint for the linear transformation matrix in MTransE to ensure the invertibility, because we found it leads to better results. For JAPE, we tuned various parameter values and set d = 75, \u03b1 = 0.1, \u03b2 = 0.05, \u03b4 = 0.05 for the best performance. The learning rates of SE and AE were empirically set to 0.01 and 0.1, respectively."}, {"heading": "4.3 Evaluation Metrics", "text": "Following the conventions [1,5,11], we used Hits@k and Mean to assess the performance of the three approaches. Hits@k measures the proportion of correctly aligned entities ranked in the top k, while Mean calculates the mean of these ranks. A higher Hits@k and a lower Mean indicate better performance. It is a phenomenon worth noting that the optimal Hits@k and Mean usually do not come at the same epoch in all the three approaches. For fair comparison, we did not fix the number of epochs but used early stopping to avoid overtraining. The training process is stopped as long as the change ratio of Mean is less than 0.0005. Besides, the training of AE on each dataset takes 100 epochs."}, {"heading": "4.4 Experimental Results", "text": "Results on DBP15K We used a certain proportion of the gold standards as seed alignment while left the remaining as testing data, i.e., the latent aligned entities to discover. We tested the proportion from 10% to 50% with step 10%, and Table 2 lists the results using 30% of the gold standards. The variation of Hits@k with different proportions will be shown shortly. For relationships and attributes, we simply extracted the property pairs with exactly the same labels, which only account for a small portion of the seed alignment.\nTable 2 indicates that JAPE largely outperformed JE and MTransE, since it captures both structure and attribute information of KBs. For JE, it employs TransE as its basic model, which is not suitable to be directly applied to entity alignment as discussed in Section 3.5. Besides, JE does not give a mandatory constraint on the length of vectors. Instead, it only minimizes \u2016v\u201622\u22121 to restrain vector length and brings adverse effect. For MTransE, it models the structures of KBs in different vector spaces, and information loss happens when learning the translation between vector spaces.\nAdditionally, we divided JAPE into three variants for ablation study, and the results are shown in Table 2 as well. We found that involving negative triples in structure embedding reduces the random distribution of entities, and involving attribute embedding as constraint further refines the distribution of entities. The two improvements demonstrate that systematic distribution of entities makes for the cross-lingual entity alignment task.\nIt is worth noting that the alignment direction (e.g. ZH \u2192 EN vs. EN \u2192 ZH) also causes performance difference. As shown in Table 1, the relationship triples in a non-English KB are much sparser than those in an English KB, so that the approaches based on the relationship triples cannot learn good representations to model the structures of non-English KBs, as restraints for entities are relatively insufficient. When performing alignment from an English KB to a nonEnglish KB, we search for the nearest non-English entity as the aligned one to an English entity, the sparsity of the non-English KB leads to the disorganized distribution of its entities, which brings negative effects on the task. However, it is comforting to see that the performance difference becomes narrower when involving attribute embedding, because the attribute triples provide additional information to embed entities, especially for sparse KBs.\nFig. 3 provides the visualization of sample results for entity alignment and attribute correlations. We projected the embeddings of aligned entity pairs and involved attribute embeddings to two dimensions using PCA. The left part indicates that universities, countries, cities and cellphones were divided widely while aligned entities from Chinese to English were laid closely, which met our expectation of JAPE. The right part shows our attribute embedding clustered three\ngroups of monolingual attributes (about cellphones, cities and universities) and one group of cross-lingual ones (about countries).\nSensitivity to Proportion of Seed Alignment Fig. 4 illustrates the change of Hits@k with varied proportion of seed alignment. In accordance with our expectation, the results on all the datasets become better with the increase of the proportion, because more seed alignment can provide more information to overlay the two KBs. It can be seen that, when using half of the gold standards as seed alignment, JAPE performed encouragingly, e.g. Hits@1 and Hits@10 on DBP15KZH-EN are 53.27% and 82.91%, respectively. Moreover, even with a very small proportion of seed alignment like 10%, JAPE still achieved promising results, e.g. Hits@10 on DBP15KZH-EN reaches 55.04% and on DBP15KJA-EN reaches 44.69%. Therefore, it is feasible to deploy JAPE to various entity alignment tasks, even with limited seed alignment.\nCombination with Machine Translation Since machine translation is often used in cross-lingual ontology matching [9,21], we designed a machine translation based approach that employs Google Translate to translate the labels of entities in one KB and computes similarities between the translations and the labels of entities in the other KB. For similarity measurement, we chose Levenshtein distance because of its popularity in ontology matching [3].\nWe chose DBP15KZH-EN and DBP15KJA-EN, which have big barriers in linguistics. As depicted in Table 3, machine translation achieves satisfying results, especially for Hits@1, and we think that it is due to the high accuracy of Google Translate. However, the gap between machine translation and JAPE becomes smaller for Hits@10 and Hits@50. The reason is as follows. When Google misunderstands the meaning of labels (e.g. polysemy), the top-ranked entities are all very likely to be wrong. On the contrary, JAPE relies on the structure information of KBs, so the correct entities often appear slightly behind. Besides, we found that translating from Chinese (or Japanese) to English is more accurate than the reverse direction.\nTo further investigate the possibility of combination, for each latent aligned entities, we considered the lower rank of the two results as the combined rank. It is surprising to find that the combined results are significantly better, which reveals the mutual complementarity between JAPE and machine translation. We believe that, when aligning entities between cross-lingual KBs where the quality of machine translation is difficult to guarantee, or many entities lack meaningful labels, JAPE can be a practical alternative.\nResults at Larger Scale To test the scalability of JAPE, we built three larger datasets by choosing 100 thousand ILLs between English and Chinese, Japanese and French in the same way as DBP15K. The threshold of relationship triples to select ILLs was set to 2. Each dataset contains several hundred thousand entities and several million triples. We set d = 100, \u03b2 = 0.1 and keep other parameters the same as DBP15K. For JE, the training takes 2000 epochs as reported in its paper. The results on DBP100K are listed in Table 4. Due to lack of space, only Hits@10 is reported. We found that similar results and conclusions stand for\nDBP100K compared with DBP15K, which indicate the scalability and stability of JAPE.\nFurthermore, the performance of all the methods decreases to some extent on DBP100K. We think that the reasons are twofold: (i) DBP100K contains quite a few \u201csparse\u201d entities involved in a very limited number of triples, which affect embedding the structure information of KBs; and (ii) as the number of latent aligned entities in DBP100K are several times larger than DBP15K, the TransEbased models suffer from the increased occurrence of multi-mapping relations as explained in [22]. Nevertheless, JAPE still outperformed JE and MTransE."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we introduced a joint attribute-preserving embedding model for cross-lingual entity alignment. We proposed structure embedding and attribute embedding to represent the relationship structures and attribute correlations of KBs and learn approximate embeddings for latent aligned entities. Our experiments on real-world datasets demonstrated that our approach achieved superior results than two state-of-the-art embedding approaches and could be complemented with conventional methods based on machine translation.\nIn future work, we look forward to improving our approach in several aspects. First, the structure embedding suffered from multi-mapping relations, thus we plan to extend it with cross-lingual hyperplane projection. Second, our attribute embedding discarded attribute values due to their diversity and cross-linguality, which we want to use cross-lingual word embedding techniques to incorporate. Third, we would like to evaluate our approach on more heterogeneous KBs developed by different parties, such as between DBpedia and Wikidata.\nAcknowledgements. This work is supported by the National Natural Science Foundation of China (Nos. 61370019, 61572247 and 61321491)."}], "references": [{"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko"], "venue": "NIPS. pp. 2787\u20132795", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y. Bengio"], "venue": "AAAI. pp. 301\u2013306", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "String similarity metrics for ontology alignment", "author": ["M. Cheatham", "P. Hitzler"], "venue": "Alani, H., et al. (eds.) ISWC. pp. 294\u2013309", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning new facts from knowledge bases with neural tensor networks and semantic word vectors", "author": ["D. Chen", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "arXiv:1301.3618", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-lingual knowledge graph embeddings for cross-lingual knowledge alignment", "author": ["M. Chen", "Y. Tian", "M. Yang", "C. Zaniolo"], "venue": "IJCAI", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2017}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research 12(7), 2121\u2013 2159", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "English-chinese knowledge base translation with neural network", "author": ["X. Feng", "D. Tang", "B. Qin", "T. Liu"], "venue": "COLING. pp. 2935\u20132944", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Cross-lingual ontology mapping \u2013 an investigation of the impact of machine translation", "author": ["B. Fu", "R. Brennan", "D. O\u2019Sullivan"], "venue": "G\u00f3mez-P\u00e9rez, A., et al. (eds.) ASWC. pp. 1\u201315", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Cross-lingual ontology mapping and its use on the multilingual semantic web", "author": ["B. Fu", "R. Brennan", "D. O\u2019Sullivan"], "venue": "WWW Workshop on Multilingual Semantic Web. pp. 13\u201320", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Entity matching on web tables : a table embeddings approach for blocking", "author": ["A.L. Gentile", "P. Ristoski", "S. Eckel", "D. Ritze", "H. Paulheim"], "venue": "EDBT. pp. 510\u2013513", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "A joint embedding method for entity alignment of knowledge bases", "author": ["Y. Hao", "Y. Zhang", "S. He", "K. Liu", "J. Zhao"], "venue": "Chen, H., et al. (eds.) CCKS. pp. 3\u201314", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Improving neural knowledge base completion with cross-lingual projections", "author": ["P. Klein", "S.P. Ponzetto", "G. Glava\u0161"], "venue": "EACL. pp. 516\u2013522", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Type-constrained representation learning in knowledge graphs", "author": ["D. Krompa\u00df", "S. Baier", "V. Tresp"], "venue": "Arenas, M., et al. (eds.) ISWC. pp. 640\u2013655", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Knowledge representation learning with entities, attributes and relations", "author": ["Y. Lin", "Z. Liu", "M. Sun"], "venue": "IJCAI. pp. 2866\u20132872", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Y. Lin", "Z. Liu", "M. Sun", "Y. Liu", "X. Zhu"], "venue": "AAAI. pp. 2181\u20132187", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv:1301.3781", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS. pp. 3111\u20133119", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["M. Nickel", "V. Tresp", "H. Kriegel"], "venue": "ICML. pp. 809\u2013816", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Rdf2vec: RDF graph embeddings for data mining", "author": ["P. Ristoski", "H. Paulheim"], "venue": "Groth, P., et al. (eds.) ISWC. pp. 498\u2013514", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A.Y. Ng"], "venue": "NIPS. pp. 926\u2013934", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "A machine learning approach to multilingual and cross-lingual ontology matching", "author": ["D. Spohr", "L. Hollink", "P. Cimiano"], "venue": "Aroyo, L., et al. (eds.) ISWC. pp. 665\u2013680", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "AAAI. pp. 1112\u20131119", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Cross-lingual knowledge linking across wiki knowledge bases", "author": ["Z. Wang", "J. Li", "Z. Wang", "J. Tang"], "venue": "WWW. pp. 459\u2013468", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Normalized word embedding and orthogonal transform for bilingual word translation", "author": ["C. Xing", "D. Wang", "C. Liu", "Y. Lin"], "venue": "HLT-NAACL. pp. 1006\u20131011", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["W.Y. Zou", "R. Socher", "D.M. Cer", "C.D. Manning"], "venue": "EMNLP. pp. 1393\u20131398", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 20, "context": "[21] argued that the quality of alignment in cross-lingual scenarios heavily depends on the quality of translations between multiple languages.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Following the popular translation-based embedding models [1,15,22], a few studies leveraged KB embeddings for entity alignment and achieved promising results [5,11].", "startOffset": 57, "endOffset": 66}, {"referenceID": 14, "context": "Following the popular translation-based embedding models [1,15,22], a few studies leveraged KB embeddings for entity alignment and achieved promising results [5,11].", "startOffset": 57, "endOffset": 66}, {"referenceID": 21, "context": "Following the popular translation-based embedding models [1,15,22], a few studies leveraged KB embeddings for entity alignment and achieved promising results [5,11].", "startOffset": 57, "endOffset": 66}, {"referenceID": 4, "context": "Following the popular translation-based embedding models [1,15,22], a few studies leveraged KB embeddings for entity alignment and achieved promising results [5,11].", "startOffset": 158, "endOffset": 164}, {"referenceID": 10, "context": "Following the popular translation-based embedding models [1,15,22], a few studies leveraged KB embeddings for entity alignment and achieved promising results [5,11].", "startOffset": 158, "endOffset": 164}, {"referenceID": 4, "context": "However, as discovered in [5], the existing alignment between cross-lingual KBs usually accounts for a small proportion.", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "TransE [1], the pioneer of translation-based methods, interprets a relationship vector as the translation from the head entity vector to its tail entity vector.", "startOffset": 7, "endOffset": 10}, {"referenceID": 21, "context": "To further improve TransE, later work including TransH [22] and TransR [15] was proposed.", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "To further improve TransE, later work including TransH [22] and TransR [15] was proposed.", "startOffset": 71, "endOffset": 75}, {"referenceID": 1, "context": "Additionally, there exist a few non-translation-based approaches to KB embedding [2,18,20].", "startOffset": 81, "endOffset": 90}, {"referenceID": 17, "context": "Additionally, there exist a few non-translation-based approaches to KB embedding [2,18,20].", "startOffset": 81, "endOffset": 90}, {"referenceID": 19, "context": "Additionally, there exist a few non-translation-based approaches to KB embedding [2,18,20].", "startOffset": 81, "endOffset": 90}, {"referenceID": 12, "context": "[13] added type constraints to KB embedding models and enhanced their performance on link prediction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "KR-EAR [14] embeds attributes additionally by modeling attribute correlations and obtains good results", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "RDF2Vec [19] uses local information of KB structures to generate sequences of entities and employs language modeling approaches to learn entity embeddings for machine learning tasks.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "For cross-lingual tasks, [12] extends NTNKBC [4] for cross-lingual KB completion.", "startOffset": 25, "endOffset": 29}, {"referenceID": 3, "context": "For cross-lingual tasks, [12] extends NTNKBC [4] for cross-lingual KB completion.", "startOffset": 45, "endOffset": 48}, {"referenceID": 6, "context": "[7] uses a neural network approach that translates English KBs into Chinese to expand Chinese KBs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8,9] presented a generic framework, which utilizes machine translation tools to translate labels to the same language and uses monolingual ontology matching methods to find mappings.", "startOffset": 0, "endOffset": 5}, {"referenceID": 8, "context": "[8,9] presented a generic framework, which utilizes machine translation tools to translate labels to the same language and uses monolingual ontology matching methods to find mappings.", "startOffset": 0, "endOffset": 5}, {"referenceID": 20, "context": "[21] leveraged translation-based label similarities and ontology structures as features for learning cross-lingual mapping functions by machine learning techniques (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "For cross-lingual entity alignment, MTransE [5] incorporates TransE to encode KB structures into language-specific vector spaces and designs five alignment models to learn translation between KBs in different languages with seed alignment.", "startOffset": 44, "endOffset": 47}, {"referenceID": 10, "context": "JE [11] utilizes TransE to embed different KBs into a unified space with the aim that each seed alignment has similar embeddings, which is extensible to the cross-lingual scenario.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "[23] proposed a graph model, which only leverages language-independent features (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] exploited embedding-based methods for aligning entities in Web tables.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Following TransE [1], we interpret a relationship as the translation from the head entity to the tail entity, to characterize the structure information of KBs.", "startOffset": 17, "endOffset": 20}, {"referenceID": 0, "context": "corrupted triples), which have been widely used in translation-based embedding models [1,15,22], are also valuable to SE.", "startOffset": 86, "endOffset": 95}, {"referenceID": 14, "context": "corrupted triples), which have been widely used in translation-based embedding models [1,15,22], are also valuable to SE.", "startOffset": 86, "endOffset": 95}, {"referenceID": 21, "context": "corrupted triples), which have been widely used in translation-based embedding models [1,15,22], are also valuable to SE.", "startOffset": 86, "endOffset": 95}, {"referenceID": 0, "context": "\u03b1 is a ratio hyper-parameter that weights positive and negative triples and its range is [0, 1].", "startOffset": 89, "endOffset": 95}, {"referenceID": 15, "context": "To capture the correlations of attributes, AE borrows the idea from Skipgram [16], a very popular model that learns word embeddings by predicting the context of a word given the word itself.", "startOffset": 77, "endOffset": 81}, {"referenceID": 16, "context": "To prevent all the vectors from having the same value, we adopt the negative sampling approach [17] to efficiently parameterize Eq.", "startOffset": 95, "endOffset": 99}, {"referenceID": 24, "context": "Inspired by [25], we use the matrices of pairwise similarities between entities as supervised information and minimize the following objective function:", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "(1)) does not follow the margin-based ranking loss function below, which is used by many previous KB embedding models [1]:", "startOffset": 118, "endOffset": 121}, {"referenceID": 10, "context": "In contrast, JE [11] uses the margin-based ranking loss from TransE [1], while MTransE [5] does not have this as it does not use negative triples.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "In contrast, JE [11] uses the margin-based ranking loss from TransE [1], while MTransE [5] does not have this as it does not use negative triples.", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": "In contrast, JE [11] uses the margin-based ranking loss from TransE [1], while MTransE [5] does not have this as it does not use negative triples.", "startOffset": 87, "endOffset": 90}, {"referenceID": 5, "context": "(2) and (6) with a gradient descent optimization algorithm called AdaGrad [6].", "startOffset": 74, "endOffset": 77}, {"referenceID": 23, "context": "(3) and the cosine similarity to measure embeddings [24].", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "The structure embedding belongs to the translation-based embedding models, which have already been proved to be capable of learning embeddings at large scale [1].", "startOffset": 158, "endOffset": 161}, {"referenceID": 10, "context": "As aforementioned, JE [11] and MTransE [5] are two representative embeddingbased methods for entity alignment.", "startOffset": 22, "endOffset": 26}, {"referenceID": 4, "context": "As aforementioned, JE [11] and MTransE [5] are two representative embeddingbased methods for entity alignment.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "We followed the implementation details reported in [5,11] and complemented other unreported details with careful consideration.", "startOffset": 51, "endOffset": 57}, {"referenceID": 10, "context": "We followed the implementation details reported in [5,11] and complemented other unreported details with careful consideration.", "startOffset": 51, "endOffset": 57}, {"referenceID": 0, "context": "Following the conventions [1,5,11], we used Hits@k and Mean to assess the performance of the three approaches.", "startOffset": 26, "endOffset": 34}, {"referenceID": 4, "context": "Following the conventions [1,5,11], we used Hits@k and Mean to assess the performance of the three approaches.", "startOffset": 26, "endOffset": 34}, {"referenceID": 10, "context": "Following the conventions [1,5,11], we used Hits@k and Mean to assess the performance of the three approaches.", "startOffset": 26, "endOffset": 34}, {"referenceID": 8, "context": "Combination with Machine Translation Since machine translation is often used in cross-lingual ontology matching [9,21], we designed a machine translation based approach that employs Google Translate to translate the labels of entities in one KB and computes similarities between the translations and the labels of entities in the other KB.", "startOffset": 112, "endOffset": 118}, {"referenceID": 20, "context": "Combination with Machine Translation Since machine translation is often used in cross-lingual ontology matching [9,21], we designed a machine translation based approach that employs Google Translate to translate the labels of entities in one KB and computes similarities between the translations and the labels of entities in the other KB.", "startOffset": 112, "endOffset": 118}, {"referenceID": 2, "context": "For similarity measurement, we chose Levenshtein distance because of its popularity in ontology matching [3].", "startOffset": 105, "endOffset": 108}, {"referenceID": 21, "context": "We think that the reasons are twofold: (i) DBP100K contains quite a few \u201csparse\u201d entities involved in a very limited number of triples, which affect embedding the structure information of KBs; and (ii) as the number of latent aligned entities in DBP100K are several times larger than DBP15K, the TransEbased models suffer from the increased occurrence of multi-mapping relations as explained in [22].", "startOffset": 395, "endOffset": 399}], "year": 2017, "abstractText": "Entity alignment is the task of finding entities in two knowledge bases (KBs) that represent the same real-world object. When facing KBs in different natural languages, conventional cross-lingual entity alignment methods rely on machine translation to eliminate the language barriers. These approaches often suffer from the uneven quality of translations between languages. While recent embedding-based techniques encode entities and relationships in KBs and do not need machine translation for cross-lingual entity alignment, a significant number of attributes remain largely unexplored. In this paper, we propose a joint attribute-preserving embedding model for cross-lingual entity alignment. It jointly embeds the structures of two KBs into a unified vector space and further refines it by leveraging attribute correlations in the KBs. Our experimental results on real-world datasets show that this approach significantly outperforms the state-of-the-art embedding approaches for cross-lingual entity alignment and could be complemented with methods based on machine translation.", "creator": "LaTeX with hyperref package"}}}