{"id": "1708.06989", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2017", "title": "A Neural Network Approach for Mixing Language Models", "abstract": "The performance teena of monarchial Neural Network (NN) - based kazuyuki language cassopolis models inure is luminously steadily whitest improving due to scaroni the emergence superspeedways of hirayama new architectures, roeser which are 28-acre able ferrer to learn fakty different ince natural language ephod characteristics. This 9.15 paper goeteborg presents approximate a sendoff novel hawkesworth framework, hightown which shows that band a subtriangular significant improvement can barooah be 15-cent achieved 2,500-meter by combining cornuke different facet existing riffraff heterogeneous octets models in allianz a cheekbone single architecture. This gracchi is matraville done through feightner 1) a feature layer, which separately rq-1 learns different 1727 NN - based then-president models and a.h. 2) tamsen a jabhat mixture layer, which hereditarily merges 96.01 the rup resulting stir-fried model features. In parasol doing so, ol\u0161any this architecture darfield benefits 73.50 from jahf the twenty20 learning capabilities of masn each model with no nesh noticeable increase shatin in the number of cantel model mowatt parameters priestman or tuddenham the training asko time. huntingdale Extensive anesta experiments conducted antonian on syafiuddin the cobwebbed Penn marzabotto Treebank (sloate PTB) maredsous and the Large Text half-year Compression Benchmark (hadari LTCB) corpus joinery showed ebola a significant reduction of garrison the dialectology perplexity prepress when rubbo compared champus to low-end state - tiankai of - qeis the - art feedforward 36-28 as well pouliches as recurrent waltzing neural network architectures.", "histories": [["v1", "Wed, 23 Aug 2017 13:27:16 GMT  (55kb,D)", "http://arxiv.org/abs/1708.06989v1", "Published at IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2017. arXiv admin note: text overlap witharXiv:1703.08068"]], "COMMENTS": "Published at IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2017. arXiv admin note: text overlap witharXiv:1703.08068", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["youssef oualil", "dietrich klakow"], "accepted": false, "id": "1708.06989"}, "pdf": {"name": "1708.06989.pdf", "metadata": {"source": "CRF", "title": "A NEURAL NETWORK APPROACH FOR MIXING LANGUAGE MODELS", "authors": ["Youssef Oualil", "Dietrich Klakow"], "emails": ["firstname.lastname@lsv.uni-saarland.de"], "sections": [{"heading": null, "text": "Index Terms\u2014 Neural networks, mixture models, language modeling"}, {"heading": "1. INTRODUCTION", "text": "For many language technology applications such as speech recognition [1] and machine translation [2], a high quality Language Model (LM) is considered to be a key component to success. Traditionally, LMs aim to predict probable sequences of predefined linguistic units, which are typically words. These predictions are guided by the semantic and syntactic properties that are encoded by the LM.\nThe recent advances in neural network-based approaches for language modeling led to a significant improvement over the standard N -gram models [3, 4]. This is mainly due to the continuous word representations they provide, which typically overcome the exponential growth of parameters that N -gram models require. The NN-based LMs were first introduced by Bengio et al. [5], who proposed a Feedforward Neural Network (FNN) model as an alternative to N -grams. Although FNNs were shown to perform very well for different tasks [6, 7], their fixed context (word history) size constraint was a limiting factor for their performance. In order to overcome this constraint, Mikolov et al. [8, 9] proposed a Recurrent Neural Network (RNN), which allows context information to cycle in the network. Investigating the inherent shortcomings of RNNs led to the Long-Short Term Memory (LSTM)-based LMs [10], which explicitly control the longevity of context information in the network. This chain of novel NN-based LMs continued with more complex and advanced models such as Convolutional Neural Networks (CNN) [11] and autoencoders [12], to name a few.\nThis research was funded by the German Research Foundation (DFG) as part of SFB 1102.\nLMs performance has been shown to significantly improve using model combination. This is typically done by either 1) designing deep networks with different architectures at the different layers, as it was done in [11], which combines LSTM, CNN and a highway network, or by 2) combining different models at the output layer, as it is done in the maximum entropy RNN model [13], which uses direct N -gram connections to the output layer, or using the classical linear interpolation [14]. While the former category, requires a careful selection of the architectures to combine for a well-suited feature design, and can be difficult/slow to train, the second category knows a significant increase in the number of parameters when combining multiple models.\nMotivated by the work in [13], we have recently proposed a Sequential Recurrent Neural Network (SRNN) [15], which combines FFN information and RNN. In this paper, we continue along this line of work by proposing a generalized framework to combine different heterogeneous NN-based architectures in a single mixture model. More particularly, the proposed architecture uses 1) a hidden feature layer to, separately, learn each of the models to be combined, and 2) a hidden mixture layer, which combines the resulting model features. Moreover, this architecture uses a single word embedding matrix, which is learned from all models, and a single output layer. This framework is, in principle, able to combine different NN-based LMs (e.g., FNN, RNN, LSTM, etc.) with no direct constraints on the number of models to combine or their configurations.\nWe proceed as follows. Section 2 presents an overview of the basic NN-based LMs. Section 3 introduces the proposed neural mixture model. Then, Section 4 evaluates the proposed network in comparison to different state-of-the-art language models for perplexity on the PTB and the LTCB corpus. Finally, we conclude in Section 5."}, {"heading": "2. NEURAL NETWORK LANGUAGE MODELS", "text": "The goal of a language model is to estimate the probability distribution p(wT1 ) of word sequences wT1 = w1, \u00b7 \u00b7 \u00b7 , wT . Using the chain rule, this distribution can be expressed as\np(wT1 ) = T\u220f t=1 p(wt|wt\u221211 ) (1)\nLet U be a word embedding matrix and let W be the hidden-tooutput weights. NN-based LMs (NNLMs), that consider word embeddings as input, approximate each of the terms involved in this product in a bottom-up evaluation of the network according to\nHt =M(P,Rt\u22121, U) (2) Ot = g ( Ht \u00b7W ) (3)\nar X\niv :1\n70 8.\n06 98\n9v 1\n[ cs\n.C L\n] 2\n3 A\nug 2\n01 7\nwhereM represents a particular NN-based model, which can be a deep architecture, P denotes its parameters and Rt\u22121 denotes its recurrent information at time t. g(\u00b7) is the softmax function.\nThe rest of this section briefly introducesM, P and Rt\u22121 for the basic architectures, namely FNN, RNN and LSTM, which were investigated and evaluated as different components in the proposed mixture model. The proposed architecture, however, is general and can include all NNLMs that consider world embeddings as input."}, {"heading": "2.1. Feedforward Neural Networks", "text": "Similarly to N -gram models, FNN uses the Markov assumption of order N \u2212 1 to approximate (1). That is, the current word depends only on the last N \u2212 1 words. Subsequently,M is given by\nEt\u2212i = Xt\u2212i \u00b7 U , i = N \u2212 1, \u00b7 \u00b7 \u00b7 , 1 (4)\nHt = f ( N\u22121\u2211 i=1 Et\u2212i \u00b7 V i )\n(5)\nXt\u2212i is a one-hot encoding of the wordwt\u2212i. Thus,Et\u2212i is the continuous representation of the word wt\u2212i. f(\u00b7) is an activation function. Hence, for an FNN modelM, P = {V i}N\u22121i=1 andR t\u22121 = \u2205."}, {"heading": "2.2. Recurrent Neural Networks", "text": "RNN attempts to capture the complete history in a context vector ht, which represents the state of the network and evolves in time. Therefore, RNN approximates each term in (1) as p(wt|wT1 ) \u2248 p(wt|ht). As a result,M for an RNN is given by\nHt = f ( Xt\u22121 \u00b7 U +Ht\u22121 \u00b7 V ) (6)\nThus, for an RNN modelM, P = V andRt\u22121 = Ht\u22121."}, {"heading": "2.3. Long-Short Term Memory Networks", "text": "In order to alleviate the rapidly changing context issue in standard RNNs and control the longevity of the dependencies modeling in the network, the LSTM architecture [10] introduces an internal memory state Ct, which explicitly controls the amount of information, to forget or to add to the network, before estimating the current hidden state. Formally, an LSTM modelM is given by\nEt\u22121 = Xt\u22121 \u00b7 U (7) {i, f, o}t = \u03c3 ( V i,f,ow \u00b7 Et\u22121 + V i,f,oh \u00b7H t\u22121 ) (8)\nC\u0303t = f ( V cw \u00b7 Et\u22121 + V ch \u00b7Ht\u22121 ) (9)\nCt = f t Ct\u22121 + it C\u0303t (10) Ht = ot f ( Ct ) (11)\nwhere is the element-wise product, C\u0303t is the memory candidate, whereas it, f t and ot are the input, forget and output gates of the network, respectively. Hence, for an LSTM modelM,Rt = {Ht, Ct} and P = {V i,f,o,cw , V i,f,o,ch }."}, {"heading": "3. NEURAL NETWORK MIXTURE MODELS", "text": "On the contrary to a large number of research directions on improving or designing (new) particular neural architectures for language modeling, the work presented in this paper is an attempt to design a general architecture, which is able to combine different types of existing heterogeneous models rather than investigating new ones."}, {"heading": "3.1. Model Combination for Language Modeling", "text": "The work presented in this paper is motivated by recent research showing that model combination can lead to a significant improvement in LM performance [14]. This is typically done by either 1) designing deep networks with different architectures at the different layers, as it was done in [11]. This category of model combination, however, requires a careful selection of the architectures to combine for a well-suited feature design, as it can be difficult/slow to train, whereas the second category 2) combines different models at the output layer, as it is done in the maximum entropy RNN model [13] or using the classical linear interpolation [14]. This category typically leads to a significant increase in the number of parameters when combining multiple models.\nIn a first attempt to circumvent these problems, we have recently proposed an SRNN model [15], which combines FFN information and RNN through additional sequential connections at the hidden layer. Although SRNN was successful and did not noticeably suffer from the aforementioned problems, it was solely designed to combine RNN and FNN and is, therefore, not well-suited for other architectures. This paper continues along this line of work by proposing a general architecture to combine different heterogeneous neural models with no direct constraints on the number or type of models."}, {"heading": "3.2. Neural Network Mixture Models", "text": "This section introduces the mathematical formulation of the proposed mixture model. Let {Mm}Mm=1 be a set of M models to combine, and let {Pm,Rtm}Mm=1 be their corresponding model parameters and recurrent information at time t, respectively. For the basic NNLMs, namely FNN, RNN and LSTM,Mm, Pm and Rtm were introduced in Section 2.\nLet U be the shared word embedding matrix, which is learned during training from all models in the mixture. The mixture model is given by the following steps (see illustration in Fig. 1): 1) Feature layer: update each model and calculate its features\nHtm =Mm(Pm,Rt\u22121m , U), m = 1, \u00b7 \u00b7 \u00b7 ,M (12)"}, {"heading": "2) Mixture layer: combine the different features", "text": "Htmixture = fmixture\n( M\u2211\nm=1\nHtm \u00b7 Sm ) (13)"}, {"heading": "3) Output layer: calculate the output using a softmax function", "text": "Ot = g ( Htmixture \u00b7W ) (14)\nfmixture is a non-linear mixing function, whereas Sm, m = 1, \u00b7 \u00b7 \u00b7 ,M are the mixture weights (matrices).\nAlthough the experiments conducted in this work mainly include FNN, RNN and LSTM, the set of possible model selection forMm is not restricted to these but includes all NN-based models that take word embeddings as input.\nThe proposed mixture model uses a single word embedding matrix and a single output layer with predefined and fixed sizes. The latter are independent of the sizes of the mixture models. In doing so, this model does not suffer from the significant parameter growth when increasing the number of models in the mixture. We can also see that this architecture does not impose any direct constraints on the number of models to combine, their size or their type. Hence, we can combine, for instance, models of the same type but with different sizes/configurations, as we can combine heterogeneous models such as recurrent and non-recurrent models, in a single mixture.\nMoreover, the mixture models can also be deep architectures with multiple hidden layers."}, {"heading": "3.3. Training of Neural Mixture Models", "text": "NMM training follows the standard back-propagation algorithm used to train neural architectures. More particularly, the error at the output layer is propagated to all models in the mixture. At this stage, each model receives a network error, updates its parameters, and propagates its error to the shared word embedding (input) layer. We should also mention here that recurrent models can be \u201cunfolded\u201d in time, independently of the other models in the mixture, as it is done for standard networks. Once each model is updated, the continuous word representations are then updated as well while taking into account the individual network errors emerging from the different models in the mixture (see illustration in Fig. 1).\nThe joint training of the mixture models is expected to lead to a \u201ccomplementarity\u201d effect. We mean by \u201ccomplementarity\u201d that the mixture models perform poorly when evaluated separately but lead to a much better performance when tested jointly. This is typically a result of the models learning and modeling, eventually, different features. Moreover, the joint learning is also expected to lead to a richer and more expressive word embeddings."}, {"heading": "3.4. Model Dropout", "text": "In order to 1) enforce models co-training and 2) avoid network overfitting when the number of models in the mixture is large. We use a model dropout technique, which is inspired by the standard dropout regularization [16] that is widely used to train neural networks. The idea here is to have \u201cmodels\u201d replace \u201cneurons\u201d in the standard dropout. Therefore, for each training example, a model is to be dropped with a probability pd. Then, only models that are selected contribute to the mixture and have their parameters and mixing weights Sm updated. Similarly to standard dropout, model dropout is applied only to non-recurrent models in the mixture."}, {"heading": "4. EXPERIMENTS AND RESULTS", "text": ""}, {"heading": "4.1. Experimental Setup", "text": "We evaluated the proposed architecture on two different benchmark tasks. The first set of experiments was conducted on the Penn Treebank (PTB) corpus using the standard division, e.g. [9, 17]; sections 0-20 are used for training while sections 21-22 and 23-24 are used for validation and testing. The vocabulary was limited to the 10k\nmost frequent words while the remaining words were all mapped to the token <unk>. In order to evaluate how the proposed approach scales to large corpora, we run a set of experiments on the Large Text Compression Benchmark (LTCB) [18]. This corpus is based on the enwik9 dataset which contains the first 109 bytes of enwiki-20060303-pages-articles.xml. We adopted the same trainingtest-validation data split and pre-processing from [17]. The vocabulary was limited to the 80k most frequent words. Details about the sizes of these two corpora and the percentage of Out-Of-Vocabulary (OOV) words that were mapped to <unk> can be found in Table 1.\nThe results reported below compare the proposed Neural Mixture Model (NMM) approach to the baseline NNLMs. In particular, we compare our model to the FNN-based LM [5], the full RNN [9] (without classes) as well as RNN with maximum entropy (RNNME) [13]. We also report results for the LSTM architecture [10], and the recently proposed SRNN model [15].\nAlthough the proposed approach was not designed for a particular mixture of models, we only report results for different combinations of FNN, RNN and LSTM, which are considered to be the baseline NNLMs. For clarity, an NMM result is presented as F\nN1,\u00b7\u00b7\u00b7 ,Nf S1,\u00b7\u00b7\u00b7 ,Sf +RS1,\u00b7\u00b7\u00b7 ,Sr + LS1,\u00b7\u00b7\u00b7 ,Sl , where f is the number of FNNs in the mixture, Sm,m = 1, \u00b7 \u00b7 \u00b7 , f are their corresponding hidden layer sizes (that are fed to the mixture) and Nm,m = 1, \u00b7 \u00b7 \u00b7 , f are their fixed history sizes. The same notation holds for RNN and LSTM, where r and l are the number of RNNs and LSTMs in the mixture, respectively, and Nr, Nl = 1. The number of models in the mixture is given by f + r + l. Moreover, the notation F\nNb\u2212Ne Sf means that this model combines Ne \u2212Nb +1 consecutive FNN models with respective history sizesNb, Nb+1, \u00b7 \u00b7 \u00b7 , Ne, with all models having the same hidden layer size Sf ."}, {"heading": "4.2. PTB Experiments", "text": "For the PTB experiments, all models have a hidden layer size of 400, with FFNN and SRNN using the Rectified Linear Unit (ReLu) i.e., f(x) = max(0, x) as activation function and having 2 hidden layers. ReLu is also used as activation function for the mixture layer in NMMs, which use a single hidden layer. The embedding size is 100 for SRNN and NMMs, whereas it is set to 400 for RNN and 200 for FNN and LSTM. The training is performed using the stochastic gradient descent algorithm with a mini-batch size of 200. the learning rate is initialized to 0.4, the momentum is set to 0.9, the weight decay is fixed at 4x10\u22125, the model dropout is set to 0.4 and the training is done in epochs. The weights initialization follows the normalized initialization proposed in [19]. Similarly to [8], the learning rate is halved when no significant improvement in the log-likelihood of the validation data is observed. The BPTT was set to 5 time steps for all recurrent models. In the tables below, the results are reported in terms of perplexity (PPL), Number of model Parameters (NoP) and the Parameter Growth (PG) for NMM, which is defined as the relative increase in the number of parameters of NMM w.r.t. the baseline model in the table. In order to demonstrate the power of\nthe joint training, we also report the perplexity PPL and NoP of the Linearly Interpolated (LI) models in the mixture after training them separately. In this case, each model learns its own word embedding and output layer.\nThe PTB results reported in Table 2 show clearly that combining different small-size models with a reduced word embedding size results in a better perplexity performance compared to the baseline models, with a significant decrease in the NoP required by the mixture. More particularly, we can see that adding a single FNN model to a small size LSTM or RNN is sufficient to outperform the baseline models while reducing the number of parameters by 24% and 36%, respectively. The same conclusion can be drawn when combining an RNN with an LSTM. We can also see that adding more FNN models to each of these mixtures leads to additional improvements while keeping the number of parameters significantly small. Table 2 also shows that training the small size models (in the mixture) separately, and then linearly interpolating them, results in a slightly worse performance compared to the mixture model with a noticeable increase in the NoP. This conclusion emphasizes the importance of the joint training. Moreover, we can also see that mixing RNN and FNNs leads to a comparable performance to SRNN, which was particularly designed to enhance RNN with FNN information. The proposed approach, however, does not particularity encode the individual characteristics of the models in the mixture, which reflects its ability to include different types of NNLMs. We can also conclude that combining FNN with recurrent models leads to a more significant improvement when compared to mixtures of FNNs. This conclusion shows, similarly to other work e.g. [15, 13], that recurrent models can be further improved using N-gram/feedforward information, given that they model different linguistic features.\nFig. 2 is an extension of Table 2, which shows the change in the perplexity and NoP of different NMMs when iteratively adding more FFN models to the mixture. This figure confirms that combining heterogeneous models (combining LSTM or RNN with FNNs) achieves a better performance compared to combining only FNN models. We can also conclude from this figure that the improvement becomes very slow after adding 4 FNN models to each mixture."}, {"heading": "4.3. LTCB Experiments", "text": "The LTCB experiments use the same PTB setup with minor changes. The results shown in Table 3 follow the same experimental setup used in [15]. More precisely, these results were obtained without usage of momentum, model dropout or weight decay whereas the\nmini-batch size was set to 400. The FNN architecture contains 2 hidden layers of size 600 whereas RNN, LSTM, SRNN and NMM have a single hidden layer of size 600.\nThe LTCB results shown in Table 3 generally confirm the PTB conclusions. In particular, we can see that combining recurrent models, with half (third for RNN) their original size, with a single FNN model leads to a comparable performance to the baseline models. Moreover, increasing the mixture models size (for LSTM) or increasing the number of FNNs (for RNN) improves the performance further with no noticeable increase in the NoP. Similarly to the PTB, we can also see that NMM achieves the same performance as the WISRNN model with a NoP reduction of 31% compared to the original RNN model. The FFN mixture results show a more significant improvement when combining multiple small-size (100) models compared to mixing few large models (600). This conclusion shows that the strength of mixture models lies in their ability to combine the learning capabilities of different models, even with small sizes."}, {"heading": "5. CONCLUSION AND FUTURE WORK", "text": "We have presented a neural mixture model which is able to combine heterogeneous NN-based LMs in a single architecture. Experiments on PTB and LTCB corpora have shown that this architecture substantially outperforms many state-of-the-art neural systems, due to its ability to combine learning capabilities of different architectures. Further gains could be made using a more advanced model selection or feature combination at the mixing layer instead of the simple model weighting. These will be investigated in future work."}, {"heading": "6. REFERENCES", "text": "[1] Slava M. Katz, \u201cEstimation of probabilities from sparse data for the language model component of a speech recognizer,\u201d IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 35, no. 3, pp. 400\u2013401, Mar. 1987.\n[2] Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin, \u201cA statistical approach to machine translation,\u201d Computational Linguistics, vol. 16, no. 2, pp. 79\u201385, Jun. 1990.\n[3] Ronald Rosenfeld, \u201cTwo decades of statistical language modeling: where do we go from here?,\u201d Proceedings of the IEEE, vol. 88, no. 8, pp. 1270\u20131278, Aug. 2000.\n[4] Reinhard Kneser and Hermann Ney, \u201cImproved backingoff for m-gram language modeling,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Detroit, Michigan, USA, May 1995, pp. 181\u2013184.\n[5] Yoshua Bengio, Re\u0301jean Ducharme, Pascal Vincent, and Christian Jauvin, \u201cA neural probabilistic language model,\u201d Journal of Machine Learning Research, vol. 3, pp. 1137\u20131155, Mar. 2003.\n[6] Holger Schwenk and Jean-Luc Gauvain, \u201cTraining neural network language models on very large corpora,\u201d in Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (EMNLP), Oct. 2005, pp. 201\u2013208.\n[7] Joshua Goodman, \u201cA bit of progress in language modeling, extended version,\u201d Tech. Rep. MSR-TR-2001-72, Microsoft Research, 2001.\n[8] Tomas Mikolov, Martin Karafia\u0301t, Luka\u0301s Burget, Jan Cernocky\u0301, and Sanjeev Khudanpur, \u201cRecurrent neural network based language model,\u201d in 11th Annual Conference of the International Speech Communication Association (INTERSPEECH), Makuhari, Chiba, Japan, Sep. 2010, pp. 1045\u20131048.\n[9] Tomas Mikolov, Stefan Kombrink, Luka\u0301s Burget, Jan Cernocky\u0301, and Sanjeev Khudanpur, \u201cExtensions of recurrent neural network language model,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Prague, Czech Republic, May 2011, pp. 5528\u20135531.\n[10] Martin Sundermeyer, Ralf Schlu\u0308ter, and Hermann Ney, \u201cLSTM neural networks for language modeling,\u201d in 13th Annual Conference of the International Speech Communication Association (INTERSPEECH), Portland, Oregon, USA, Sep. 2012, pp. 194\u2013197.\n[11] Kim Yoon, Jernite Yacine, Sontag David, and Rush Alexander M., \u201cCharacter-aware neural language models,\u201d in 30th AAAI Conference on Artificial Intelligence, 2016.\n[12] Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle, Mitesh Khapra, Balaraman Ravindran, Vikas C Raykar, and Amrita Saha, \u201cAn autoencoder approach to learning bilingual word representations,\u201d in Advances in Neural Information Processing Systems 27, pp. 1853\u20131861. 2014.\n[13] Tomas Mikolov, Anoop Deoras, Daniel Povey, Luka\u0301s Burget, and Jan Cernocky\u0301, \u201cStrategies for training large scale neural network language models,\u201d in IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), Waikoloa, Hawaii, USA, Dec. 11-15, 2011, pp. 196\u2013201.\n[14] Tomas Mikolov, Anoop Deoras, Stefan Kombrink, Luka\u0301s Burget, and Jan Cernocky\u0301, \u201cEmpirical evaluation and combination of advanced language modeling techniques,\u201d in 12th Annual Conference of the International Speech Communication Association (INTERSPEECH), Florence, Italy, Aug. 27-31, 2011, pp. 605\u2013608.\n[15] Youssef Oualil, Clayton Greenberg, Mittul Singh, and Dietrich Klakow, \u201cSequential recurrent neural network for language modeling,\u201d in 17th Annual Conference of the International Speech Communication Association (INTERSPEECH), San Francisco, California, USA, Sep. 2016.\n[16] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, \u201cDropout: a simple way to prevent neural networks from overfitting.,\u201d Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.\n[17] Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou, and Li-Rong Dai, \u201cThe fixed-size ordinally-forgetting encoding method for neural network language models,\u201d in 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing ACL, July 2015, vol. 2, pp. 495\u2013500.\n[18] Matt Mahoney, \u201cLarge text compression benchmark,\u201d 2011.\n[19] Xavier Glorot and Yoshua Bengio, \u201cUnderstanding the difficulty of training deep feedforward neural networks,\u201d in Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS), Chia Laguna Resort, Sardinia, Italy, May 2010, pp. 249\u2013256."}], "references": [{"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["Slava M. Katz"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 35, no. 3, pp. 400\u2013401, Mar. 1987.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1987}, {"title": "A statistical approach to machine translation", "author": ["Peter F. Brown", "John Cocke", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Fredrick Jelinek", "John D. Lafferty", "Robert L. Mercer", "Paul S. Roossin"], "venue": "Computational Linguistics, vol. 16, no. 2, pp. 79\u201385, Jun. 1990.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1990}, {"title": "Two decades of statistical language modeling: where do we go from here", "author": ["Ronald Rosenfeld"], "venue": "Proceedings of the IEEE, vol. 88, no. 8, pp. 1270\u20131278, Aug. 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Improved backingoff for m-gram language modeling", "author": ["Reinhard Kneser", "Hermann Ney"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Detroit, Michigan, USA, May 1995, pp. 181\u2013184.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 1137\u20131155, Mar. 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Training neural network language models on very large corpora", "author": ["Holger Schwenk", "Jean-Luc Gauvain"], "venue": "Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (EMNLP), Oct. 2005, pp. 201\u2013208.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "A bit of progress in language modeling, extended version", "author": ["Joshua Goodman"], "venue": "Tech. Rep. MSR-TR-2001-72, Microsoft Research, 2001.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "11th Annual Conference of the International Speech Communication Association (INTERSPEECH), Makuhari, Chiba, Japan, Sep. 2010, pp. 1045\u20131048.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Tomas Mikolov", "Stefan Kombrink", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Prague, Czech Republic, May 2011, pp. 5528\u20135531.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "13th Annual Conference of the International Speech Communication Association (INTERSPEECH), Portland, Oregon, USA, Sep. 2012, pp. 194\u2013197.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Character-aware neural language models", "author": ["Kim Yoon", "Jernite Yacine", "Sontag David", "Rush Alexander M."], "venue": "30th AAAI Conference on Artificial Intelligence, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Sarath Chandar A P", "Stanislas Lauly", "Hugo Larochelle", "Mitesh Khapra", "Balaraman Ravindran", "Vikas C Raykar", "Amrita Saha"], "venue": "Advances in Neural Information Processing Systems 27, pp. 1853\u20131861. 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1853}, {"title": "Strategies for training large scale neural network language models", "author": ["Tomas Mikolov", "Anoop Deoras", "Daniel Povey", "Luk\u00e1s Burget", "Jan Cernock\u00fd"], "venue": "IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), Waikoloa, Hawaii, USA, Dec. 11-15, 2011, pp. 196\u2013201.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Empirical evaluation and combination of advanced language modeling techniques", "author": ["Tomas Mikolov", "Anoop Deoras", "Stefan Kombrink", "Luk\u00e1s Burget", "Jan Cernock\u00fd"], "venue": "12th Annual Conference of the International Speech Communication Association (INTERSPEECH), Florence, Italy, Aug. 27-31, 2011, pp. 605\u2013608.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequential recurrent neural network for language modeling", "author": ["Youssef Oualil", "Clayton Greenberg", "Mittul Singh", "Dietrich Klakow"], "venue": "17th Annual Conference of the International Speech Communication Association (INTERSPEECH), San Francisco, California, USA, Sep. 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1929}, {"title": "The fixed-size ordinally-forgetting encoding method for neural network language models", "author": ["Shiliang Zhang", "Hui Jiang", "Mingbin Xu", "Junfeng Hou", "Li-Rong Dai"], "venue": "53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing ACL, July 2015, vol. 2, pp. 495\u2013500.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Large text compression benchmark", "author": ["Matt Mahoney"], "venue": "2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS), Chia Laguna Resort, Sardinia, Italy, May 2010, pp. 249\u2013256.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "For many language technology applications such as speech recognition [1] and machine translation [2], a high quality Language Model (LM) is considered to be a key component to success.", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "For many language technology applications such as speech recognition [1] and machine translation [2], a high quality Language Model (LM) is considered to be a key component to success.", "startOffset": 97, "endOffset": 100}, {"referenceID": 2, "context": "The recent advances in neural network-based approaches for language modeling led to a significant improvement over the standard N -gram models [3, 4].", "startOffset": 143, "endOffset": 149}, {"referenceID": 3, "context": "The recent advances in neural network-based approaches for language modeling led to a significant improvement over the standard N -gram models [3, 4].", "startOffset": 143, "endOffset": 149}, {"referenceID": 4, "context": "[5], who proposed a Feedforward Neural Network (FNN) model as an alternative to N -grams.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Although FNNs were shown to perform very well for different tasks [6, 7], their fixed context (word history) size constraint was a limiting factor for their performance.", "startOffset": 66, "endOffset": 72}, {"referenceID": 6, "context": "Although FNNs were shown to perform very well for different tasks [6, 7], their fixed context (word history) size constraint was a limiting factor for their performance.", "startOffset": 66, "endOffset": 72}, {"referenceID": 7, "context": "[8, 9] proposed a Recurrent Neural Network (RNN), which allows context information to cycle in the network.", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "[8, 9] proposed a Recurrent Neural Network (RNN), which allows context information to cycle in the network.", "startOffset": 0, "endOffset": 6}, {"referenceID": 9, "context": "Investigating the inherent shortcomings of RNNs led to the Long-Short Term Memory (LSTM)-based LMs [10], which explicitly control the longevity of context information in the network.", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "This chain of novel NN-based LMs continued with more complex and advanced models such as Convolutional Neural Networks (CNN) [11] and autoencoders [12], to name a few.", "startOffset": 125, "endOffset": 129}, {"referenceID": 11, "context": "This chain of novel NN-based LMs continued with more complex and advanced models such as Convolutional Neural Networks (CNN) [11] and autoencoders [12], to name a few.", "startOffset": 147, "endOffset": 151}, {"referenceID": 10, "context": "This is typically done by either 1) designing deep networks with different architectures at the different layers, as it was done in [11], which combines LSTM, CNN and a highway network, or by 2) combining different models at the output layer, as it is done in the maximum entropy RNN model [13], which uses direct N -gram connections to the output layer, or using the classical linear interpolation [14].", "startOffset": 132, "endOffset": 136}, {"referenceID": 12, "context": "This is typically done by either 1) designing deep networks with different architectures at the different layers, as it was done in [11], which combines LSTM, CNN and a highway network, or by 2) combining different models at the output layer, as it is done in the maximum entropy RNN model [13], which uses direct N -gram connections to the output layer, or using the classical linear interpolation [14].", "startOffset": 290, "endOffset": 294}, {"referenceID": 13, "context": "This is typically done by either 1) designing deep networks with different architectures at the different layers, as it was done in [11], which combines LSTM, CNN and a highway network, or by 2) combining different models at the output layer, as it is done in the maximum entropy RNN model [13], which uses direct N -gram connections to the output layer, or using the classical linear interpolation [14].", "startOffset": 399, "endOffset": 403}, {"referenceID": 12, "context": "Motivated by the work in [13], we have recently proposed a Sequential Recurrent Neural Network (SRNN) [15], which combines FFN information and RNN.", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "Motivated by the work in [13], we have recently proposed a Sequential Recurrent Neural Network (SRNN) [15], which combines FFN information and RNN.", "startOffset": 102, "endOffset": 106}, {"referenceID": 9, "context": "In order to alleviate the rapidly changing context issue in standard RNNs and control the longevity of the dependencies modeling in the network, the LSTM architecture [10] introduces an internal memory state C, which explicitly controls the amount of information, to forget or to add to the network, before estimating the current hidden state.", "startOffset": 167, "endOffset": 171}, {"referenceID": 13, "context": "The work presented in this paper is motivated by recent research showing that model combination can lead to a significant improvement in LM performance [14].", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "This is typically done by either 1) designing deep networks with different architectures at the different layers, as it was done in [11].", "startOffset": 132, "endOffset": 136}, {"referenceID": 12, "context": "This category of model combination, however, requires a careful selection of the architectures to combine for a well-suited feature design, as it can be difficult/slow to train, whereas the second category 2) combines different models at the output layer, as it is done in the maximum entropy RNN model [13] or using the classical linear interpolation [14].", "startOffset": 303, "endOffset": 307}, {"referenceID": 13, "context": "This category of model combination, however, requires a careful selection of the architectures to combine for a well-suited feature design, as it can be difficult/slow to train, whereas the second category 2) combines different models at the output layer, as it is done in the maximum entropy RNN model [13] or using the classical linear interpolation [14].", "startOffset": 352, "endOffset": 356}, {"referenceID": 14, "context": "In a first attempt to circumvent these problems, we have recently proposed an SRNN model [15], which combines FFN information and RNN through additional sequential connections at the hidden layer.", "startOffset": 89, "endOffset": 93}, {"referenceID": 15, "context": "We use a model dropout technique, which is inspired by the standard dropout regularization [16] that is widely used to train neural networks.", "startOffset": 91, "endOffset": 95}, {"referenceID": 8, "context": "[9, 17]; sections 0-20 are used for training while sections 21-22 and 23-24 are used for validation and testing.", "startOffset": 0, "endOffset": 7}, {"referenceID": 16, "context": "[9, 17]; sections 0-20 are used for training while sections 21-22 and 23-24 are used for validation and testing.", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "In order to evaluate how the proposed approach scales to large corpora, we run a set of experiments on the Large Text Compression Benchmark (LTCB) [18].", "startOffset": 147, "endOffset": 151}, {"referenceID": 16, "context": "We adopted the same trainingtest-validation data split and pre-processing from [17].", "startOffset": 79, "endOffset": 83}, {"referenceID": 4, "context": "In particular, we compare our model to the FNN-based LM [5], the full RNN [9] (without classes) as well as RNN with maximum entropy (RNNME) [13].", "startOffset": 56, "endOffset": 59}, {"referenceID": 8, "context": "In particular, we compare our model to the FNN-based LM [5], the full RNN [9] (without classes) as well as RNN with maximum entropy (RNNME) [13].", "startOffset": 74, "endOffset": 77}, {"referenceID": 12, "context": "In particular, we compare our model to the FNN-based LM [5], the full RNN [9] (without classes) as well as RNN with maximum entropy (RNNME) [13].", "startOffset": 140, "endOffset": 144}, {"referenceID": 9, "context": "We also report results for the LSTM architecture [10], and the recently proposed SRNN model [15].", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "We also report results for the LSTM architecture [10], and the recently proposed SRNN model [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 18, "context": "The weights initialization follows the normalized initialization proposed in [19].", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "Similarly to [8], the learning rate is halved when no significant improvement in the log-likelihood of the validation data is observed.", "startOffset": 13, "endOffset": 16}, {"referenceID": 14, "context": "[15, 13], that recurrent models can be further improved using N-gram/feedforward information, given that they model different linguistic features.", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "[15, 13], that recurrent models can be further improved using N-gram/feedforward information, given that they model different linguistic features.", "startOffset": 0, "endOffset": 8}, {"referenceID": 14, "context": "The results shown in Table 3 follow the same experimental setup used in [15].", "startOffset": 72, "endOffset": 76}], "year": 2017, "abstractText": "The performance of Neural Network (NN)-based language models is steadily improving due to the emergence of new architectures, which are able to learn different natural language characteristics. This paper presents a novel framework, which shows that a significant improvement can be achieved by combining different existing heterogeneous models in a single architecture. This is done through 1) a feature layer, which separately learns different NN-based models and 2) a mixture layer, which merges the resulting model features. In doing so, this architecture benefits from the learning capabilities of each model with no noticeable increase in the number of model parameters or the training time. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures.", "creator": "LaTeX with hyperref package"}}}