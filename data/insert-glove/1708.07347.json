{"id": "1708.07347", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2017", "title": "An LSTM-Based Dynamic Customer Model for Fashion Recommendation", "abstract": "Online heidinger fashion sales brumfield present kjellman a walder challenging m-16 use kfox case for nortel personalized sanomawsoy recommendation: tahan Stores smilin offer a secondmarket huge variety tillack of homogenous items in multiple r\u00e9jean sizes. sadomasochism Small stocks, substrings high return prakaram rates, seasonality, decandido and changing adio trends 180-seat cause continuous 23.24 turnover shuxian of akst articles aschatz for suge sale on all time dolgoprudny scales. morisset Customers wesemann tend gregariously to bizet shop vulgarians rarely, cosens but often buy multiple ginta items 2ndtake at morganroth once. 2-7 We cornhuskers report fusionist on repressor backtest macentee experiments with sales data of 100k rasin frequent shoppers gedser at Zalando, Europe ' almirola s towe leading online outro fashion platform. reverse To model changing perton customer and 80-plus store 5.27 environments, our windermere recommendation 603 method d'\u00e9ducation employs a klosters pair sodrel of neural madi networks: To overcome infringe the cold shutoffs start jacobsville problem, a feedforward network pennington generates article embeddings keilman in \" seraphic fashion well-stocked space, \" giana which serve lennartsson as input kelvinside to a haberer recurrent neural factfiles network rinchen that rambaldi predicts serageldin a 5-43 style vector in this veneziani space for gori each urate client, based megatrend on buvaysa their pa-risc past outdoors purchase sequence. We heliodorus compare our results with lovely a slunk static collaborative filtering approach, freshest and jauss a popularity ranking baseline.", "histories": [["v1", "Thu, 24 Aug 2017 10:35:24 GMT  (169kb,D)", "http://arxiv.org/abs/1708.07347v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["sebastian heinz", "christian bracher", "roland vollgraf"], "accepted": false, "id": "1708.07347"}, "pdf": {"name": "1708.07347.pdf", "metadata": {"source": "CRF", "title": "An LSTM-Based Dynamic Customer Model for Fashion Recommendation", "authors": ["Sebastian Heinz", "Christian Bracher", "Roland Vollgraf"], "emails": ["roland.vollgraf}@zalando.de"], "sections": [{"heading": "1 Introduction", "text": "The recommendation task in the setting of online fashion sales presents unique challenges. Consumer tastes and body shapes are idiosyncratic, so a huge selection of items in different sizes must be kept on offer. On a typical day, Zalando, Europe\u2019s leading online fashion platform with \u223c20M active customers, offers \u223c200k product choices for sale. Being physical goods rather\nar X\niv :1\n70 8.\n07 34\n7v 1\n[ cs\n.I R\nthan digital information, fashion articles must be stocked in warehouses; as most of them are rarely ordered, items are generally available in small, fluctuating numbers. In addition, shoppers commonly return articles. The result is a rapid turnover of the inventory, with many items going in and out of stock daily. Superimposed on short-scale variations, there are periodic alterations associated with the seasonal cycle, and secular changes caused by fashion trends. Regarding consumer behavior, a noteworthy difference to e.g. streaming media services is their propensity to buy rarely (a few sales annually), but then multiple items at once. Hence, their purchase histories are sparse, only partially ordered sequences.\nWe previously introduced a recommendation algorithm for fashion items that combines article images, tags, and other catalog information with customer response, tethering curated content to collaborative filtering by minimizing the cross-entropy loss of a deep neural network for the sales record across a large selection of customers [1]. Like logistic matrix factorization methods [9, 7], our technique yields low-dimensional embeddings for articles (\u201cFashion DNA\u201d) and customers (\u201cstyle vectors\u201d), but has the advantage to circumvent the cold-start problem that plagues collaborative methods by injecting catalog information for newly added articles. Our model proves capable of recognizing individual style preferences from a modest number of purchases; as cumulative sales events extend over a multi-year period, however, it creates only a static style \u201cfingerprint\u201d of a customer.\nIn this contribution, we start from the static model, but extend it by including time-of-sale information. To contend with the ever-varying article stock, we use the static model to generate Fashion DNA from curated article data, and employ it as a fixed item descriptor. This allows us to focus on the temporal sequence of sales events for individual customers, which we feed into a neural network to estimate their style vectors. As these are updated with every purchase, the approach models the evolution of our customers\u2019 tastes, and we may employ the style vectors at a given date to create a personalized preference ranking of the articles then in store, in a way fully analogous to the static model. Recurrent neural networks (RNN) are specifically designed to handle sequential data (see Chapter 10 in Ref. [3] for an overview). Our network, introduced in Section 2, employs long short-term memory (LSTM) cells [6] to learn temporal correlations between sales. As the model shares network weights between customers, it has comparatively few parameters, and easily scales to millions of clients during inference.\nRecently, evaluations have appeared in the literature [2, 8, 10] that indi-\ncate superiority of RNN-based recommender systems on standard data sets (LastFM, Netflix) over static models. Comparing the dynamic customer style model with predictions from the static counterpart [1], and a baseline model build on global customer preferences, we confirm that fashion recommendation benefits from temporal information (Section 3). However, we also find that peculiarities innate to the fashion context, like the prevalence of partially ordered purchase sequences and the variability of in-store content, are prone to impact recommendation quality; care must be taken in designing RNN architecture, training, and evaluation schemes to accommodate them. Further avenues for research are discussed in Section 4."}, {"heading": "2 A dynamic recommender system", "text": "We now lay out the elements of our proposed model \u2013 the data used for training and validation, the static network learning the article embeddings (Fashion DNA), the recurrent network responsible for predicting the customer response, and the training scheme."}, {"heading": "2.1 Data overview", "text": "This study is based on article and sales data from Zalando\u2019s online fashion store, collected from its start in 2008, up to a cutoff date of July 1, 2015. The data set contains information about \u223c1M fashion items and millions of individual sales events (excluding customer returns). Merchandise is characterized by a thumbnail image of each item (size 108\u00d7156), categorical data (brand, color, gender, etc.) that has been rolled out into\u223c7k one-hot encoded \u201ctags,\u201d and as numerical data, the logarithm of the manufacturer-suggested retail price, and, for garments only, the fabric composition across \u223c50 fibers as percentages. Each sales record contains a unique, anonymized customer ID, the article bought (disregarding size information), and the time of sale, with one minute granularity. Customer data is limited to sales; in particular, article ratings were not available."}, {"heading": "2.2 Fashion DNA", "text": "Our first task is to encode the properties of the articles in a dense numerical representation. As the curated data has multiple formats and carries diverse\ninformation, a natural vehicle for this transformation is a deep neural network that learns suitable combinations of features on its own. We discussed such a model at length in an earlier paper [1], and we will only give an overview here.\nThe representation of an article \u03bd, its \u201cFashion DNA\u201d vector f\u03bd , is obtained as the activation in a low-dimensional \u201cbottleneck\u201d layer near the top of the network. At its base, the network receives the catalog information as its input: RGB image data is first processed with a pretrained residual neural network [4] whose output is concatenated with the categorical and numerical article data and further transformed with a stack of fully connected layers, resulting in Fashion DNA. As we are ultimately interested in customer preferences, it is sensible to train the model on the sales record: Disregarding the timestamp information, we arrange the sales information for a large number of frequent customers (\u223c100k) into a sparse binary purchase matrix \u03a0 whose elements \u03a0\u03bdk \u2208 {0, 1} indicate whether customer k has bought item \u03bd. The network is then trained to minimize the average cross-entropy loss per article over these customers. In effect, the network learns both an optimal representation of the article f\u03bd across the customer base, and a logistic regression from Fashion DNA to the sales record for each customer k, with weight vectors sk and bias \u03b2k that encode their style preferences and purchase propensity, respectively. The model architecture is sketched in Figure 1.\nThe result is a low-rank logistic factorization of the purchase matrix akin\nto collaborative filtering [9, 7],\n\u03a0\u03bdk \u2248 p\u03bdk = \u03c3 (f\u03bd \u00b7 sk + \u03b2k) , (1)\n(where \u03c3(\u00b7) denotes the logistic function), except that the Fashion DNA f\u03bd is now clamped to the catalog data via the encoding neural network. This is a decisive advantage for our setting where we are faced with a continuously changing inventory of goods, as the Fashion DNA for new articles is obtained from their curated data by a simple forward pass through the neural network.\nRanking the purchase probabilities p\u03bdk in Eq. (1) naturally induces recommendations [1], a model we use for comparison in Section 3.2. We emphasize that the lack of time of sale information enforces static customer styles. Hence, to invoke dynamically evolving customer tastes, we have to modify the style vectors sk."}, {"heading": "2.3 LSTM network for purchase sequences", "text": "Fashion DNA provides a compact encoding of all available content information of an item, and largely solves the cold-start problem for new articles entering the store. For these reasons, we use the static model Fashion DNA as article representations in the dynamic model. We also want to preserve the association between customer-item affinity, and the scalar product of Fashion DNA and customer style, akin to Eq. (1). Hence, we make our model dynamic by allowing the customer style to change over time t. To distinguish between static and dynamic customer styles, we denote the latter dk(t).\nWhile we could add time as a dimension to the static model, and attempt to factorize the resulting three-dimensional purchase data tensor (as is done, for example, in [11]), we chose to follow a different approach featuring LSTM cells. We also reverse the role of articles and customers: While our implementation of the static model used batches of articles as input, and learned the response of all customers simultaneously, the input to the LSTM network is customer based. Batches now contain Fashion DNA sequences of the form (fk,1, . . . , fk,Nk), representing the purchase history \u03bdk,1, . . . , \u03bdk,Nk of customer k. When customers buy multiple items at once, the purchase sequence is ambiguous. To prevent the LSTM from interpreting these non-sequential parts as time series, we put purchases with the same time stamp in random order. Beyond the order sequence, the absolute time of purchases tk,1, . . . , tk,Nk carries important context information for our problem. For example, the model\nmay use temporal data to infer the in-store availability of an article, and the season. We thus additionally supply the time stamp of each purchase to the network.\nA single pass of the LSTM network processing customer purchase histories is illustrated in Figure 2. For a fixed customer k and purchase number i, the LSTM takes as input the concatenation of the time stamp tk,i\u22121 and Fashion DNA fk,i\u22121 of the previous purchase, and the time stamp tk,i of the current purchase. In addition, the LSTM accesses the content of its own memory, mk,i\u22121, which stores information on the purchase history of customer k it has seen so far. The output of the LSTM is projected by a fully connected layer which results in the current customer style dk,i. Note that the first purchase of the sequence (i = 1) is treated specially: Since there is no previous purchase, we flush fk,0, tk,0, and mk,0 with zero entries. Consequently, the customer style dk,1 just depends on the time stamp tk,1 and favors the most popular items at that time."}, {"heading": "2.4 Training scheme", "text": "For recommendation, we aim to predict customer style vectors dk,i that maximize the affinity fk,i\u00b7dk,i to the next-bought article, while minimizing the affinity to all other items in store at that time. Because it is expensive to compute the customer affinities for every article, we only pick a small sample of \u201cnegative\u201d examples among the articles not bought. We denote their corresponding Fashion DNA vectors by f\u0303k,i,1, . . . , f\u0303k,i,n. The number of negative examples n > 0 is a hyperparameter of the model.\nWe tested three choices of loss functions for training the network, sigmoid cross-entropy loss L\u03c3 (as in the static model), softmax loss Lsmax, and sigmoid-rank loss Lrank [12], and varied the number n of negative examples. The loss functions are given by:\nL\u03c3 = \u2212 log \u03c3 (fk,i \u00b7 dk,i)\u2212 n\u2211 j=1 log \u03c3 ( \u2212f\u0303k,i,j \u00b7 dk,i ) ,\nLsmax = \u2212 log  exp(fk,i\u00b7dk,i) exp(fk,i\u00b7dk,i)+\nn\u2211 j=1 exp(f\u0303k,i,j \u00b7dk,i)  , Lrank = 1n n\u2211 j=1 \u03c3 ( f\u0303k,i,j \u00b7 dk,i \u2212 fk,i \u00b7 dk,i ) .\n(2)\nOnly Lsmax permits a probabilistic interpretation of the dynamical model\n(when n reaches the number of all available articles). The minimization landscape for L\u03c3 and Lsmax depends on the number of negative examples, as their contribution to the loss increases with n. Our experiments show that recommendation quality improves when we use more negative examples. Yet, no significant additional benefit is observed when n exceeds 50. In contrast, n has no effect on the minimization landscape for the sigmoid-rank loss. Still, for larger n fewer training epochs are needed to adjust the network parameters. We find that n = 20 is a good tradeoff between faster convergence of the weights, and the computational costs caused by using more negative examples.\nA subtle yet important aspect of the recommendation problem is that we try to predict items in the next order of the customer, rather than inferring articles within a single order. As items that are bought together tend to be related (consider, e.g., a swimwear top and bottom), an LSTM network trained on full purchase sequences quickly focuses on multiple orders and overfits. To circumvent the problem, we let only the first article in the purchase sequence contribute to the loss when a multiple order is encountered. (Because purchases with the same time stamp are always shuffled before feeding, the LSTM receives a variety of article sequences during training.)"}, {"heading": "2.5 Inference and ranking", "text": "For each customer k, we now define an \u201cintent-of-purchase\u201d ip\u03bd,k(t) for all articles \u03bd in store at time t, akin to Eq. (1):\nip\u03bd,k(t) = f\u03bd \u00b7 dk(t) . (3)\nHere, dk(t) is the dynamic style vector emitted by the LSTM network after feeding all sales to customer k that occurred before the time t (with randomly assigned sequence for items purchased together); for the final sale, we replace the time stamp of the next purchase by the evaluation time t. We note that ip\u03bd,k(t), unlike p\u03bdk (1), cannot be interpreted as a likelihood of sale."}, {"heading": "3 Comparison of models", "text": "To evaluate our dynamic customer model, we assembled sales data from the online fashion store for an eight day period immediately following training, July 1\u20138, 2015. We identified customers with orders during this test interval, representing \u223c105 individual sales, among \u223c190k items that were available for purchase in at least one size, for at least one day in this period. For comparison, we score also the static recommendation model (Section 2.2), and a simple empirical baseline that disregards customer specifics."}, {"heading": "3.1 Empirical baseline", "text": "Fashion articles in the Zalando catalog vary greatly in popularity, with few articles representing most of the sales. This skewed distribution enables a simple, non-personalized baseline recommender that projects the recent popularity of items into the future. In detail, we accumulated article sales for the week immediately preceding the evaluation interval (June 23\u201330, 2015), and defined a popularity score for each article by their sales count if they were still available after July 1. For those articles (re-)entering inventory during the evaluation period, we assigned the average number of sales among all articles as a preliminary score. The empirical baseline model then ranks the articles by descending popularity score."}, {"heading": "3.2 Static Fashion DNA model", "text": "The Fashion DNA network (Section 2.2) provides the basis for a more sophisticated, personalized recommender system, based on the customer static style vectors sk and the predicted probability of purchase p\u03bdk (1), as detailed in Ref. [1]. Indeed, p\u03bdk proves to be an unbiased estimate for the probability of purchase over the lifetime of customer and article. These assumptions are not met here, because the evaluation interval is outside the training period, and lasts only eight days. Still, we may assume that the inner products f\u03bd \u00b7sk underlying Eq. (1) are a measure of the affinity of an individual customer k to the in-store items {\u03bd}(t) during the time of evaluation, and sort them by decreasing value to create a static article ranking."}, {"heading": "3.3 Dynamic recommender system", "text": "For the dynamic customer model, we rank the in-store articles for each customer k according to their intent-of-purchase ip\u03bd,k(tk), see (3), evaluated at the time of first sale tk during the evaluation period. We experimented with the three loss models detailed in Section 2.4, and found comparable results for the sigmoid cross-entropy loss L\u03c3 and sigmoid-rank loss Lrank, while the softmax loss Lsmax performed significantly worse. The following results are based on a pretrained 128-float Fashion DNA and an LSTM implementation with 256 cells, sigmoid-rank loss and n = 20 negative examples. Note that 1\u2212Lrank provides a smooth approximation for the area under the ROC curve [5], used for model evaluation below."}, {"heading": "3.4 Results", "text": "To compare model performance, we compile recommendation rankings of the z \u2248 190k items in store for each customer (for the baseline, the ranking is shared among customers), and identify the positions r\u03bdk of the articles {\u03bd}(k) purchased by customer k during evaluation. We then determine the cumulative distribution of ranks:\nRj = \u2211\nk \u2211 \u03bd\u2208{\u03bd}(k) H (j \u2212 r\u03bdk) . (4)\nH(\u00b7) denotes the Heaviside function. The normalized cumulative rank Rj/Rz interpolates among customers and serves as a collective receiver operating characteristic (ROC) of the recommender schemes (Figure 3). The inset\ndisplays a double-logarithmic detail of the origin region, representing highquality recommendations.\nTable 1 lists the area under the curves (AUC) as a global performance measure, together with quantiles of the distributions Rj. We find that our dynamic model outperforms the static model throughout, and both models are superior to the baseline popularity model, except for the leading \u223c10 recommendations, representing less than 0.5% of the purchases (inset in Figure 3). The table also lists the number of model parameters. Weights are shared among customers for the LSTM network, but not the static model, resulting in reduction of complexity by orders of magnitude.\nMore than 3% of the purchased articles from the test interval have not been sold before and, hence, were completely ignored during training. For those new articles, the cold start problem applies and the AUC of the baseline, static, and dynamic model decreases to 64.4%, 83.3%, and 87.7%, re-\nspectively. In comparison to the numbers displayed in Table 1, the baseline shows a drastic performance drop as would also be expected from any other recommender system solely based on collaborative filtering. Static and dynamic model, however, circumvent this problem thanks to Fashion DNA."}, {"heading": "4 Outlook", "text": "We find that a personalized recommendation model, based on a recurrent network, outperforms a static customer model in the fashion context. By encoding temporal awareness into the LSTM memory of the network, the dynamic model can infer the seasonality of items, and also record when certain articles are trending\u2014a distinct advantage over the static model, which is limited to learning only long-term customer style preferences.\nAn important element currently missing in the recommendation model is short-term customer intent. In the fashion setting, goods for sale belong to varied classes (clothes, shoes, accessories, etc.), and shoppers, irrespective of their style profile, often have a particular category in mind during a session. These implicit interests strongly influence item preference, but due to their transient nature, are hard to infer from the purchase record. Complementary data sources like search queries, or the sequence of items viewed online, will pick up the relevant signals instead. Models that successfully integrate long-term style evolution and short-term customer intent promise to greatly enhance recommendation quality and relevance, and we plan to investigate them in future studies."}], "references": [{"title": "Fashion DNA: Merging content and sales data for recommendation and article mapping", "author": ["C. Bracher", "S. Heinz", "R. Vollgraf"], "venue": "Workshop Machine learning meets fashion, KDD", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Long and Short-Term Recommendations with Recurrent Neural Networks", "author": ["R. Devooght", "H. Bersini"], "venue": "Proceedings of the 25th Conference on User Modeling, Adaptation and Personalization ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "MIT Press (Cambridge, Mass., USA)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR abs/1512.03385 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimising area under the ROC curve using gradient descent", "author": ["A. Herschtal", "B. Raskutti"], "venue": "ICML: Conference Proceedings ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput. 9 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Logistic matrix factorization for implicit feedback data", "author": ["C. Johnson"], "venue": "NIPS Workshop on Distributed Matrix Computations", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Collaborative recurrent neural networks for dynamic recommender systems", "author": ["Y.\u2013J. Ko", "L. Maystre", "M. Grossglauser"], "venue": "JMLR: Workshop and Conference Proceedings", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "IEEE Computer 42 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Collaborative recurrent autoencoder: recommend while learning to fill in the blanks", "author": ["H. Wang", "X. Shi", "D. Yeung"], "venue": "Advances in Neural Information Processing Systems 29 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "X", "author": ["L. Xiong"], "venue": "Chen, T.\u2013K. Huang, J. Schneider and J. G. Carbonell. Temporal collaborative filtering with Bayesian probabilistic tensor factorization. Proceedings of the 2010 SIAM International Conference on Data Mining ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimizing classifier performance via approximation to the Wilcoxon\u2013Mann\u2013Witney statistic", "author": ["L. Yan", "R. Dodier", "M.C. Mozer", "R. Wolniewicz"], "venue": "ICML: Conference Proceedings ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "We previously introduced a recommendation algorithm for fashion items that combines article images, tags, and other catalog information with customer response, tethering curated content to collaborative filtering by minimizing the cross-entropy loss of a deep neural network for the sales record across a large selection of customers [1].", "startOffset": 334, "endOffset": 337}, {"referenceID": 8, "context": "Like logistic matrix factorization methods [9, 7], our technique yields low-dimensional embeddings for articles (\u201cFashion DNA\u201d) and customers (\u201cstyle vectors\u201d), but has the advantage to circumvent the cold-start problem that plagues collaborative methods by injecting catalog information for newly added articles.", "startOffset": 43, "endOffset": 49}, {"referenceID": 6, "context": "Like logistic matrix factorization methods [9, 7], our technique yields low-dimensional embeddings for articles (\u201cFashion DNA\u201d) and customers (\u201cstyle vectors\u201d), but has the advantage to circumvent the cold-start problem that plagues collaborative methods by injecting catalog information for newly added articles.", "startOffset": 43, "endOffset": 49}, {"referenceID": 2, "context": "[3] for an overview).", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Our network, introduced in Section 2, employs long short-term memory (LSTM) cells [6] to learn temporal correlations between sales.", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "Recently, evaluations have appeared in the literature [2, 8, 10] that indi-", "startOffset": 54, "endOffset": 64}, {"referenceID": 7, "context": "Recently, evaluations have appeared in the literature [2, 8, 10] that indi-", "startOffset": 54, "endOffset": 64}, {"referenceID": 9, "context": "Recently, evaluations have appeared in the literature [2, 8, 10] that indi-", "startOffset": 54, "endOffset": 64}, {"referenceID": 0, "context": "Comparing the dynamic customer style model with predictions from the static counterpart [1], and a baseline model build on global customer preferences, we confirm that fashion recommendation benefits from temporal information (Section 3).", "startOffset": 88, "endOffset": 91}, {"referenceID": 0, "context": "We discussed such a model at length in an earlier paper [1], and we will only give an overview here.", "startOffset": 56, "endOffset": 59}, {"referenceID": 3, "context": "At its base, the network receives the catalog information as its input: RGB image data is first processed with a pretrained residual neural network [4] whose output is concatenated with the categorical and numerical article data and further transformed with a stack of fully connected layers, resulting in Fashion DNA.", "startOffset": 148, "endOffset": 151}, {"referenceID": 8, "context": "to collaborative filtering [9, 7],", "startOffset": 27, "endOffset": 33}, {"referenceID": 6, "context": "to collaborative filtering [9, 7],", "startOffset": 27, "endOffset": 33}, {"referenceID": 0, "context": "(1) naturally induces recommendations [1], a model we use for comparison in Section 3.", "startOffset": 38, "endOffset": 41}, {"referenceID": 10, "context": "While we could add time as a dimension to the static model, and attempt to factorize the resulting three-dimensional purchase data tensor (as is done, for example, in [11]), we chose to follow a different approach featuring LSTM cells.", "startOffset": 167, "endOffset": 171}, {"referenceID": 11, "context": "We tested three choices of loss functions for training the network, sigmoid cross-entropy loss L\u03c3 (as in the static model), softmax loss Lsmax, and sigmoid-rank loss Lrank [12], and varied the number n of negative examples.", "startOffset": 172, "endOffset": 176}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Note that 1\u2212Lrank provides a smooth approximation for the area under the ROC curve [5], used for model evaluation below.", "startOffset": 83, "endOffset": 86}], "year": 2017, "abstractText": "Online fashion sales present a challenging use case for personalized recommendation: Stores offer a huge variety of items in multiple sizes. Small stocks, high return rates, seasonality, and changing trends cause continuous turnover of articles for sale on all time scales. Customers tend to shop rarely, but often buy multiple items at once. We report on backtest experiments with sales data of 100k frequent shoppers at Zalando, Europe\u2019s leading online fashion platform. To model changing customer and store environments, our recommendation method employs a pair of neural networks: To overcome the cold start problem, a feedforward network generates article embeddings in \u201cfashion space,\u201d which serve as input to a recurrent neural network that predicts a style vector in this space for each client, based on their past purchase sequence. We compare our results with a static collaborative filtering approach, and a popularity ranking baseline.", "creator": "LaTeX with hyperref package"}}}