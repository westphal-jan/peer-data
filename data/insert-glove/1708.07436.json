{"id": "1708.07436", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2017", "title": "Differentially Private Regression for Discrete-Time Survival Analysis", "abstract": "In survival secp analysis, regression cupful models suffusion are used 8-1 to glavine understand masovia the effects nanometric of apprehended explanatory variables (nock e. g. , age, sex, weight, gorzow etc.) to biopharma the r2-d2 survival probability. tsirekidze However, for sensitive acar survival philmont data herge such guptas as medical data, zenit-2 there are serious concerns about the wien privacy 33-mile of individuals nandurbar in guchang the data set when slosberg medical westerinen data silambarasan is kuno used shorthair to barner fit fuzz the 65.28 regression models. The pedini closest ringwork work eslamabad addressing rhapsodizes such gu\u00f3 privacy gai concerns heavenly6 is formula the 51.18 work 1983-1990 on Cox regression which linearly 23.95 projects chatterer the honeoye original mujer data to montenegro a lower dimensional sechin space. However, schaum the saverin weakness brog of this approach fellers is that there is no mcgonagall formal privacy guarantee haemorrhages for tallo such moleskine projection. In colonies this work, wewoka we echad aim welter to propose lifeco solutions pinchos for diaconu the regression problem resentments in survival og analysis 226.9 with the payerne protection yasein of differential privacy which pender is a golden jpb standard of 55-cent privacy meddle protection in data enquire privacy waitresses research. prinsep To this 97.55 end, we cordierite extend leitgeb the placeless Output missense Perturbation lipizzan and tazz Objective mccanns Perturbation approaches oriens which are originally proposed to hoist protect differential 9e9e9 privacy 5-75 for the Empirical Risk Minimization (2 ERM) vize problems. 37.19 In lifeguards addition, taubmann we nothobranchius also propose a onemi novel euro695 sampling approach based on pelagicus the windu Markov jonben\u00e9t Chain '28 Monte 1979-81 Carlo (MCMC) renmark method makanda to practically antigenic guarantee differential peptidyl privacy janick with huskie better carrolls accuracy. newsline We show obligatory that 4.5-mile our ovett proposed approaches csimonscoxnews.com achieve good accuracy chenevix as compared to trm the non - krashes private results while multilinear guaranteeing differential fauconnier privacy nystedt for mcbay individuals in phakisa the private re-elections data set.", "histories": [["v1", "Thu, 24 Aug 2017 14:25:44 GMT  (446kb,D)", "http://arxiv.org/abs/1708.07436v1", "10 pages, CIKM17"], ["v2", "Fri, 25 Aug 2017 01:42:35 GMT  (446kb,D)", "http://arxiv.org/abs/1708.07436v2", "19 pages, CIKM17"]], "COMMENTS": "10 pages, CIKM17", "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.DB", "authors": ["th\\^ong t nguy\\^en", "siu cheung hui"], "accepted": false, "id": "1708.07436"}, "pdf": {"name": "1708.07436.pdf", "metadata": {"source": "CRF", "title": "Di\u0080erentially Private Regression for Discrete-Time Survival Analysis", "authors": ["TH\u00d4NG T. NGUY\u00caN", "SIU CHEUNG HUI"], "emails": [], "sections": [{"heading": "Di erentially Private Regression for Discrete-Time Survival Analysis", "text": "THO\u0302NG T. NGUYE\u0302N, Nanyang Technological University\nSIU CHEUNG HUI, Nanyang Technological University\nIn survival analysis, regression models are used to understand the e ects of explanatory variables (e.g., age, sex, weight, etc.) to the survival probability. However, for sensitive survival data such as medical data, there are serious concerns about the privacy of individuals in the data set when medical data is used to t the regression models. e closest work addressing such privacy concerns is the work on Cox regression which linearly projects the original data to a lower dimensional space. However, the weakness of this approach is that there is no formal privacy guarantee for such projection. In this work, we aim to propose solutions for the regression problem in survival analysis with the protection of di erential privacy which is a golden standard of privacy protection in data privacy research. To this end, we extend the Output Perturbation and Objective Perturbation approaches which are originally proposed to protect di erential privacy for the Empirical Risk Minimization (ERM) problems. In addition, we also propose a novel sampling approach based on the Markov Chain Monte Carlo (MCMC) method to practically guarantee di erential privacy with be er accuracy. We show that our proposed approaches achieve good accuracy as compared to the non-private results while guaranteeing di erential privacy for individuals in the private data set.\nCCS Concepts: \u2022Mathematics of computing\u2192 Survival analysis; \u2022Security and privacy\u2192 Privacy protections;\nAdditional Key Words and Phrases: survival analysis; discrete-time models; di erential privacy; regression models\nACM Reference format: o\u0302ng T. Nguye\u0302n and Siu Cheung Hui. 2016. Di erentially Private Regression for Discrete-Time Survival Analysis. 1, 1, Article 1 (January 2016), 19 pages. DOI: 10.1145/nnnnnnn.nnnnnnn"}, {"heading": "1 INTRODUCTION", "text": "Survival analysis studies and models probability of failure of time-related processes (e.g., time to death of HIV patients, time to divorce of married couples, time to graduation of Ph.D. students, etc.). Two important concepts in survival analysis are (1) the hazard rate function h(t ) which is the probability of failure (death) at time t , and (2) the survival function S(t ) which is the probability of survival to time t . An example of survival data set is the electronic health records (EHRs) which have been widely used and collected at large scale in modern hospitals (Blumenthal and Tavenner 2010; DesRoches et al. 2008; Jha et al. 2009). ese health records are very useful for ing the regression models to assist doctors in the medical decision processes for treatment, diagnosis, etc. In general, regression models are used to analyze the e ects of explanatory variables (e.g., age, sex, weight, etc.) to the survival probability of patients. However, these models may also have serious problems of breaching patient\u2019s privacy as there is no guarantee that these models do not leak any personal information of individual patients in the data set.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2016 ACM. Manuscript submi ed to ACM\nManuscript submi ed to ACM 1\nar X\niv :1\n70 8.\n07 43\n6v 1\n[ cs\n.L G\n] 2\n4 A\nug 2\n01 7\nIn this work, we focus on the privacy problems of regression models used in survival analysis. We consider the se ing in which privacy-preserving algorithms use data in the private data set to t a survival regression model. e model is then published and available to the public for the bene ts of society. erefore, in this se ing, the adversaries are assumed to know the output model, i.e., the parameters of the regression model. e goal is to design algorithms that can t the survival regression model to the data set with high accuracy while guaranteeing that the adversaries cannot learn much information about the individuals in the data set when knowing the output model.\nere are two di erent kinds of regression models in survival analysis, namely continuous-time models and discretetime models. For continuous-time models, time is a continuous variable and failure events can happen at any moment. Cox regression is a well-known continuous-time model (Andersen and Gill 1982; Cox 1992) which allows estimation without any assumption on the baseline hazard e ects. However, we have to assume the proportional hazard property (i.e., a unit increase in an explanatory variable will cause a multiplicative e ect on the hazard rate). For discrete-time models (Allison 1982; Cox and Oakes 1984; Muthe\u0301n and Masyn 2005), time is discrete and failure events only happen at discrete values of time. Discrete-time regression models are be er than Cox regression when dealing with tied events (i.e., events which have the same value of survival time) and unobserved population heterogeneity (i.e., unobserved explanatory variables may cause bias to the estimation). Moreover, it does not need the proportional hazard property assumption as Cox regression does (Hess and Persson 2012).\nIn this paper, we propose solutions for the problem of guaranteeing discrete-time models not to leak personal information of the patients. Our proposed approaches guarantee di erential privacy protection, which is the state-ofthe-art privacy-preserving technique in data privacy research. Informally, a di erentially private algorithm guarantees that two neighboring data sets which are di erent at only one patient\u2019s record are guaranteed to produce two outputs whose probability densities are very similar. is prevents an adversary from recognizing a data set from the collection of its neighbors. erefore, an adversary cannot infer the personal information of a particular patient in the data set even in the case when the adversary knew all the information of all other patients in the data set (if otherwise, then the adversary can easily distinct two neighboring data sets).\nIn our solutions, we use the maximum likelihood estimation to transform the estimation problem to the optimization problem of choosing parameters to maximize the log-likelihood of the observed data set with respect to the discrete-time model. Coincidentally, our problem has a similar likelihood form as a logistic regression problem. is allows us to use the Output Perturbation (Out-Pert) and Objective Perturbation (Obj-Pert) proposed by Chaudhuri et al. (Chaudhuri et al. 2011) for our problem. ese methods were originally proposed to protect di erential privacy for the Empirical Risk Minimization (ERM) problems which include the logistic regression problem. e Out-Pert approach adds noise to the optimization solution to protect di erential privacy. e Obj-Pert approach randomly perturbs the objective function, thereby ensuring the randomness of its optimization solution which can guarantee di erential privacy for the solution. However, these approaches cannot be applied directly to our problem due to the di erence in the loss function. Especially, this is due to the fact that our loss function is not a logistic loss function but a sum of logistic loss functions as the result of the discrete-time models. erefore, we propose generalized extensions of the Out-Pert and Obj-Pert approaches to cater for our loss function.\nA disadvantage of the above perturbation approaches is that for them to work properly they require a non-negligible regularization term in the objective function which incurs bias to the output model. To tackle this, we propose a sampling approach which protects di erential privacy by directly sampling parameters from the objective function without the need of a regularization term to guarantee di erential privacy. Similar ideas on sampling the objective\nManuscript submi ed to ACM\nfunctions to provide di erential privacy are also proposed in (Bassily et al. 2014; Kifer et al. 2012; Wang et al. 2015) for the ERM problems. However, it is required that the loss function has to have a nite maximum value. e previous works guarantee this property by boxing the output parameters in a nite-volume space (e.g., a sphere). is approach does not work well when the optimal parameter has a large magnitude. In this work, to guarantee the nite constraint, we wrap the loss function inside a sanitizer function (i.e., a scaled tanh function) to create a new nite loss function. We intentionally pick the sanitizer function that can keep the loss function in its original form when the value of the loss function is small. Meanwhile, the sanitizer function deforms the loss function at large values to make the function nite. e advantage of this approach is that the sampled parameter can arbitrary large while the objective function is kept almost the same around the optimal parameter which minimizes the objective function.\nIn order to sample an output parameter from the posterior distribution, Bassily et al. (Bassily et al. 2014) proposed a polynomial run-time algorithm to sampling the log-concave objective function but their algorithm is still impractical due to the high degree of its polynomial run-time complexity. On the other hand, Wang et al. (Wang et al. 2015) proposed to use a stochastic gradient Nose\u0301-Hoover thermostat algorithm (Ding et al. 2014) to sample the posterior distribution. In this work, we propose to use Preconditioned Stochastic Gradient Langevin Dynamics (pSGLD) sampling algorithm (Li et al. 2015) to sample the objective function due to its advantages in sampling multi-dimensional parameters with di erent scales. It is worth to note that even though the sampling approach gives be er accuracy (as we will see in Section 6), due to the property of its Markov chain, it cannot sample the objective function exactly. erefore, the sampling approach does not mathematically guarantee di erential privacy but only guarantees it approximately in practice.\nIn summary, the main contributions of this paper are as follows:\n\u2022 We propose two privacy-preserving approaches, namely the Extended Output Perturbation and Extended Objective Perturbation, for the discrete-time survival regression problem. e proposed approaches guarantee di erential privacy for the survival regression models. We formally prove these guarantees based on the de nition of di erential privacy. \u2022 We propose a sampling approach to output a random model from its posterior distribution. e proposed sampling approach is based on pSGLD, which is a particular kind of the Markov Chain Monte Carlo (MCMC) method, to e ciently sample the random output which guarantees di erential privacy approximately in practice. \u2022 We show the e ectiveness of our proposed approaches on four real survival data sets. In addition, we show that the results obtained from the discrete-time models are very close to the results obtained from Cox regression. We also show experimentally the convergence of our proposed sampling approach.\ne rest of the paper is organized as follows: In Section 2, we review the related work on di erential privacy and discrete-time survival analysis. Section 3 presents the regression models used in this work. Sections 4 discusses the proposed approaches of the Extended Output Perturbation and Extended Objective Perturbation along with their privacy guarantees. Section 5 discusses the proposed sampling approach. Section 6 presents the experimental results from real data sets. Finally, we conclude the paper in Section 7."}, {"heading": "2 RELATEDWORK", "text": "Even though it is important to protect privacy in medical data, as far as we know the work of Yu et al. (Yu et al. 2008) is the only work on privacy protection for Cox regression. eir work considers the se ing in which Cox regression is\nManuscript submi ed to ACM\nexecuted on a distributed data set over many institutions. ey proposed to project patient\u2019s data to a lower dimensional space by a linear projection. e projection is satis ed by an optimization constraint to preserve good properties of the original data. However, their work is not based on a formal privacy de nition such as di erential privacy. Our work on discrete-time models for survival analysis is the rst to propose a solution for the privacy problem of discrete-time survival models and also the rst to apply di erential privacy to survival analysis."}, {"heading": "2.1 Di erential Privacy", "text": "e state-of-the-art technique for the data privacy problem is di erential privacy (Dwork 2009, 2011; Dwork et al. 2014). Basically, di erential privacy is a promise to individuals in the data set that their information will not in uence much on the nal published results from the analysis. Di erential privacy is used in many applications such as histogram publication (Li et al. 2010; Zhang et al. 2014), graph analysis (Borgs et al. 2015; Kasiviswanathan et al. 2013; Lu and Miklau 2014), regression and classi cation (Bassily et al. 2014; Chaudhuri and Monteleoni 2009; Kifer et al. 2012; Wang et al. 2015), recommender systems (Machanavajjhala et al. 2011; McSherry and Mironov 2009), etc. Here, we give a brief overview of di erential privacy, interested readers can refer to (Dwork et al. 2014) for a detailed discussion on this subject.\nTo formalize the de nition of di erential privacy, we rst need to introduce the de nition of two neighboring data sets.\nDe nition 2.1 (Neighboring data sets). Two data sets D and D \u2032 are neighbors (denoted as d(D,D \u2032) = 1) if they agree in all except one record.\nFrom that, we have a formal de nition of di erential privacy.\nDe nition 2.2 (Di erential privacy). An algorithmA is \u03f5\u2212di erentially private if for any output value x ofA and for any pair of neighboring data sets D and D \u2032:\npdf(A(D) = x ) \u2264 exp(\u03f5) \u00b7 pdf(A(D \u2032) = x )\nwhere \u03f5 is the privacy budget of the algorithm A."}, {"heading": "2.2 Discrete-time Survival Analysis", "text": "For discrete-time models, let time be divided into intervals [a0,a1), [a1,a2), . . . , [aq\u22121,aq ],a0 = 0,aq = 1, where q is the number of discrete times. e discrete time t refers to the interval [at\u22121,at ). A discrete random variable T represents the discrete failure time. T = t denotes the failure within the time interval t = [at\u22121,at ). e characteristic function of T is the discrete hazard function:\nh(t ) = Pr(T = t | T \u2265 t ), t = 1, . . . ,q\nwhich is the conditional probability for the risk of failure in interval t given the survival in all previous intervals. e discrete survival function for reaching interval t is:\nS(t ) = Pr(T \u2265 t ) = t\u22121\u220f s=1 (1 \u2212 h(s)) (1)\nDiscrete-time data sets are given by (xi ,\u03b4i , ti ), i = 1, . . . ,n, where ti = min(Ti , ci ) is the minimum of the survival time Ti and censoring time ci , and \u03b4i is the indicator variable for failure (\u03b4i = 1) or censoring (\u03b4i = 0). When \u03b4i = 0, the ith patient is known to survive until time ci but the survival time Ti is not observed (Ti > ci ). xi is a real vector of Manuscript submi ed to ACM\nexplanatory variables (e.g., sex, age, weight, etc.) which a ect the survival probability. We assume that xi is inside the unit-sphere, \u2016xi \u2016\u2264 1. is is actually a common practice in machine learning. Without loss of generality, we assume that 0 \u2264 ti \u2264 1. For convenience, we use yi to refer to the term (2\u03b4i \u2212 1) and di to refer to the tuple (xi ,yi , ti )."}, {"heading": "3 DISCRETE-TIME REGRESSION MODEL", "text": "In this section, we introduce the discrete-time regression models which are used to model the relationship between explanatory variables and the hazard rate, i.e., the predictive variable. From that, the subsequent sections will discuss the proposed di erentially private approaches to guarantee that the estimated parameters from the regression model satisfy the de nition of di erential privacy."}, {"heading": "3.1 Generalized Linear Models", "text": "We model the e ects of explanatory variables xi to the survival probability by using a generalized linear model:\n\u0434(h(ti | xi )) = \u03b3 (ti ) + x \u2032i \u03b2 (2)\nwhere \u0434(\u00b7) is the link function, \u03b2 is the parameter vector representing the e ects of explanatory variables and \u03b3 (ti ) is a time-varying baseline hazard e ect.\nA commonly used link function in survival probability is the logit link function \u0434(x ) = lo\u0434it (x ) = log ( x 1\u2212x ) . e logit link function allows the model to have a nice interpretation of the proportional odds ratio. e other two link functions, which are also used in survival analysis, are the complementary log-log link function\u0434(x ) = clo\u0434lo\u0434(x ) = log(\u2212 log(1\u2212x )), and the probit link function \u0434(x ) = probit (x ) = \u03a6\u22121(x ), where \u03a6(\u00b7) is the cumulative distribution function of the standard normal distribution. Interestingly, the complementary log-log link function has the same interpretation of proportional hazard ratio as the Cox regression. We refer interested readers to (Allison 1982) for more details.\nAs illustrated in Figure 1, the three link functions have similar shapes which lead to similar estimation results. In this work, we have selected the logit link function because it has a bounded derivative for the loss function which is\nManuscript submi ed to ACM\nrequired by our proposed Extended Output Perturbation and Extended Objective Perturbation approaches. However, our proposed sampling approach can work with all three link functions."}, {"heading": "3.2 Baseline Hazard E ect", "text": "We model the baseline hazard e ect \u03b3 (t ) using natural cubic spline (Friedman et al. 2001) with e knots equally distributed over the interval [0, 1], 0 = k1 < k2 < \u00b7 \u00b7 \u00b7 < ke = 1.\nLet\ndj (t ) = max(t \u2212 kj , 0)3 \u2212max(t \u2212 ke , 0)3\nke \u2212 kj and\nb1(t ) = 1,b2(t ) = t ,bi+2 = di (t ) \u2212 de\u22121(t )\ne baseline hazard e ect \u03b3 (t ) is approximated as a linear combination of e basis functions:\n\u03b3 (t ) = \u03b11b1(t ) + \u00b7 \u00b7 \u00b7 + \u03b1ebe (t )\nIn particular, let Ai = [b1(ti ), . . . ,be (ti )]\u2032 and \u03b1 = [\u03b11, . . . ,\u03b1e ]\u2032, then we can write \u03b3 (ti ) = \u03b1 \u2032Ai ."}, {"heading": "3.3 Maximum Likelihood Estimation (MLE)", "text": "Traditionally, we use MLE to estimate parameters \u03b1 and \u03b2 in our models. e aim is to maximize the log-likelihood of\nthe observed data. For simplicity, let f =\n( \u03b1\n\u03b2\n) and xti = ( At\nxi\n) . e log-likelihood function is:\nlogL(f ) = n\u2211 i=1 log [ h(ti | f ,xi )\u03b4i (1 \u2212 h(ti | f ,xi ))1\u2212\u03b4i S(ti | f ,xi ) ] Let yi = 2\u03b4i \u2212 1, from (1), (2) and substituting \u0434(x ) = lo\u0434it (x ), we can rewrite our problem as:\nlogL(f ) = \u2212 n\u2211 i=1\n[ `LR(yi f \u2032x ti i ) + ti\u22121\u2211 s=1 `LR(\u2212f \u2032xsi ) ] where `LR(x) = log(1 + exp(\u2212x)) is the logistic loss function. To further simplify the formula, let di = (xi ,yi , ti ), i = 1, . . . ,n, and let\n`(f ;di ) = `LR(yi f \u2032x ti i ) + ti\u22121\u2211 s=1 `LR(\u2212f \u2032xsi ) (3)\nbe the loss function. en, we get an ERM problem as follows:\nf \u2217 = arg min f n\u2211 i=1 `(f ;di ) (4)\nIn this work, our main goal is to propose algorithms which protect di erential privacy for f \u2217 in Equation (4)."}, {"heading": "4 PERTURBATION APPROACHES", "text": ""}, {"heading": "4.1 Extended Output Perturbation", "text": "In this section, we present our proposed algorithm which is the extension of the Output Perturbation approach in (Chaudhuri et al. 2011). For our problem, the loss function is a sum of logistic loss functions instead of a single logistic loss function as in (Chaudhuri et al. 2011). e proposed algorithm is in fact based on the generalized version of the Manuscript submi ed to ACM\nAlgorithm 1 AExt\u2212Out\u2212Pert: Extended Output Perturbation Input: Data set D = {d1, . . . ,dn }, loss function `(f ;di ), privacy budget \u03f5 Output: fpr iv\n1: J (f ;D) = 1n \u2211n i=1 `(f ;di ) + \u039b 2 \u2016 f \u20162 2: Minimize J (f ;D) by using the BFGS algorithm to get the non-private solution f \u2217\n3: Compute t \u2190 \u2211q s=1 \u221a 4+\u2016As \u20162+maxs\u2208{1, . . .,q} \u221a \u20162As \u20162+4 n \u00b7\u039b 4: Sample a random vector b such that pdf(b) \u221d exp ( \u2212\u03f5 \u2016b \u2016t\n) 5: Compute and output fpr iv \u2190 f \u2217 + b\nLaplace mechanism (Dwork 2008) which is described as follows: Let f \u2217 = G(D) be the value that we want to guarantee di erential privacy. f \u2217 is the result of applying a function G on the private data set D (e.g., it is in our case to minimize the objective function). We de ne the sensitivity of the function G as follows:\nsen(G) = max D,D\u2032\n\u2016G(D) \u2212G(D \u2032)\u2016\nwhere D and D \u2032 are two neighboring data sets. en, the di erentially private version of f \u2217 = G(x ) is:\nfpr iv = f \u2217 + \u00b5\nwhere \u00b5 is a noisy random variable with probability density function pdf(\u00b5) \u221d exp(\u2212\u03f5 \u2016\u00b5\u2016/sen(G)). As required by the Output Perturbation approach, we consider the following regularized objective function:\nJ (f ;D) = 1 n n\u2211 i=1 `(f ;di ) + \u039b 2 \u2016 f \u20162 (5)\nwhere D = {di }ni=1, `(\u00b7) is the loss function as de ned in (3) and \u039b is the regularization parameter. In this approach, our goal is to compute the sensitivity of:\nf \u2217 = arg min f J (f ;D)\nen, we use the sensitivity to control the amount of noise added to f \u2217.\n4.1.1 Proposed Algorithm. Algorithm 1 shows the proposed Extended Output Perturbation approach. It returns a vector fpr iv as the minimizer of J (\u00b7) while guaranteeing di erential privacy. At Line 2, we compute the non-private solution f \u2217 = arg minf = J (f ;D) using the well-known BFGS algorithm (Fletcher 2013). f \u2217 is guaranteed to exist due to the strongly convexity of J (f ;D). At Line 3, we compute t which is the sensitivity of f \u2217. Lines 4-5 add noise to the value of f \u2217.\nIn order to sample a random vector b in Algorithm 1 from the distribution pdf(b) \u221d exp (\u2212\u03f5 \u2016b\u2016/t), we observe that the length of the vector b follows a Gamma distribution:\n\u2016b\u2016\u223c \u0393(d, t/\u03f5)\nwhere d is the number of components of b. us, in order to sample b we rst sample its length r = \u2016b\u2016 from the Gamma distribution and then sample b as a uniform random point on the surface of a sphere with radius r .\n4.1.2 Privacy Guarantee. In order to prove the di erential privacy protection, we focus on proving that the sensitivity of f \u2217 at Line 2 in Algorithm 1 is equal to the value of t which is computed at Line 3. Here, we use Lemma 4.1 from (Chaudhuri et al. 2011) to bound the sensitivity of f \u2217.\nManuscript submi ed to ACM\nLemma 4.1. Let G(f ) and \u0434(f ) be two vector-valued functions, which are continuous and di erentiable at all points. In addition, let G(f ) and G(f ) + \u0434(f ) be \u03bb\u2212strongly convex. If f1 = arg minf G(f ) and f2 = arg minf G(f ) + \u0434(f ), then\n\u2016 f1 \u2212 f2\u2016\u2264 1 \u03bb max f \u2016\u2207\u0434(f )\u2016\nFrom Lemma 4.1, our goal now is to bound the magnitude of the di erence in the gradients of the objective function J (\u00b7) on any two neighboring data sets.\nLemma 4.2. For any pair of patient\u2019s records di = (xi ,yi , ti ) and dj = (x j ,yj , tj ), and for any f ,\n\u2016\u2207`(f ;di ) \u2212 \u2207`(f ;dj )\u2016\u2264 q\u2211 s=1 \u221a \u2016As \u20162+4 + max s \u2208{1, ...,q } \u221a \u20162As \u20162+4\nProof.\n\u2207`(f ;di ) = \u2207`LR(yi f \u2032xtii ) + ti\u22121\u2211 s=1 \u2207`LR(\u2212f \u2032xsi )\n= \u2212yixtii 1 + exp(yi f \u2032xtii ) + ti\u22121\u2211 s=1 xsi 1 + exp(\u2212f \u2032xsi )\nerefore, we can write \u2207`(f ;di ) = \u2211q s=1 l s i , where\nlsi =  x si 1+exp(\u2212f \u2032x si ) , i f s < ti \u2212yix si 1+exp(yi f \u2032x si ) , i f s = ti \u00ae0, i f s > ti\nSimilarly, we can also write \u2207`(f ;dj ) = \u2211q s=1 l s j . erefore,\n\u2207`(f ;di ) \u2212 \u2207`(f ;dj ) = q\u2211 s=1 lsi \u2212 l s j\nWe have | \u2212yi1+exp(yi f \u2032x si ) | \u2264 1, \u2016xi \u2016 \u2264 1, \u2016x j \u2016 \u2264 1, for any s \u2208 {1 . . .q}, we consider four possible cases as follows: Case 1: if s < ti and s < tj , then\n\u2016lsi \u2212 l s j \u2016= ( (e1 \u2212 e2)As e1xi \u2212 e2x j ) \u2264 \u221a\u2016As \u20162+(\u2016xi \u2016+\u2016x j \u2016)2 \u2264 \u221a \u2016As \u20162+4\nwhere e1 = 11+exp(\u2212f \u2032x si ) and e2 = 11+exp(\u2212f \u2032x sj ) .\nCase 2: if s > ti or s > tj , then \u2016lsi \u2212 l s j |\u2264 max(\u2016x s i \u2016, \u2016x s j \u2016) \u2264 \u221a \u2016As \u20162+1 < \u221a \u2016As \u20162+4. Case 3: if lsi = \u2212x si 1+exp(f \u2032x si ) and lsj = x sj 1+exp(\u2212f \u2032x sj ) , then\n\u2016lsi \u2212 l s j \u2016= \u2212 ( (e1 + e2)As e1xi + e2x j ) \u2264 \u221a\u20162As \u20162+4 where e1 = 11+exp(f \u2032x si ) and e2 = 11+exp(\u2212f \u2032x sj ) .\nCase 4: if lsi = x si 1+exp(\u2212f \u2032x si ) and lsj = \u2212x sj 1+exp(f \u2032x sj ) , then \u2016lsi \u2212 l s j \u2016\u2264\n\u221a \u20162As \u20162+4. is case is similar to Case 3.\nManuscript submi ed to ACM\nWe observe that there is at most one value of s, 1 \u2264 s \u2264 q, belonging to Case 3 or Case 4 in which \u2016lsi \u2212 l s j \u2016\u2264\u221a\n\u20162As \u20162+4. erefore, from the triangle inequality:\n\u2016 q\u2211 s=1 lsi \u2212 l s j \u2016\u2264 q\u2211 s=1 \u221a \u2016As \u20162+4 + max s \u2208{1, ...,q } \u221a \u20162As \u20162+4\nerefore, the lemma follows.\nFinally, we can bound the sensitivity of f \u2217 = arg minf J (f ;D) by the following lemma. Lemma 4.3. e `2\u2212sensitivity of f \u2217 = arg minf J (f ;D) is at most \u2211q s=1 \u221a 4+\u2016As \u20162+maxs\u2208{1, . . .,q} \u221a \u20162As \u20162+4 n\u039b .\nProof. Without loss of generality, we assume that two neighboring data sets D and D \u2032 are di erent at nth patient with (xn ,yn , tn ) \u2208 D and (x \u2032n ,y\u2032n , t \u2032n ) \u2208 D \u2032.\nLetG(f ) = J (f ;D),\u0434(f ) = J (f ;D \u2032)\u2212J (f ;D) = 1n (`(f ;d \u2032n )\u2212`(f ;dn )), f1 = arg minf J (f ;D), and f2 = arg minf J (f ;D \u2032). Because 12 \u2016 f \u20162 is 1\u2212strongly convex,G(f ) = J (f ;D) is \u039b\u2212strongly convex andG(f )+\u0434(f ) = J (f ;D \u2032) is also \u039b\u2212strongly convex. From Lemma 4.2,\n\u2016\u2207\u0434(f )\u2016 = 1n (\u2207`(f ;d \u2032n ) \u2212 \u2207`(f ;dn )) \u2264 \u2211q s=1 \u221a 4 + \u2016As \u20162 + maxs \u2208{1, ...,q } \u221a \u20162As \u20162+4\nn\nFrom Lemma 4.1,\n\u2016 f1 \u2212 f2\u2016\u2264 1 \u039b\n\u2211q s=1 \u221a 4 + \u2016As \u20162 + maxs \u2208{1, ...,q } \u221a \u20162As \u20162+4\nn erefore, the lemma follows.\nTheorem 4.1. Algorithm 1 is \u03f5\u2212di erentially private.\nProof. For any pair of neighboring data sets D and D \u2032 and for any fpr iv ,\npdf(fpr iv | D) pdf(fpr iv | D \u2032) = pdf(b1) pdf(b2) = exp (\u2212\u03f5/t (\u2016b1\u2016\u2212\u2016b2\u2016))\nwhere b1 and b2 are the corresponding noise vectors at Line 4 in Algorithm 1 with respect to the data sets D and D \u2032. If f \u22171 (resp., f \u2217 2 ) is the solution at Line 2 of Algorithm 1 on the data set D (resp., D \u2032), then f \u22171 + b1 = f \u2217 2 + b2 = fpr iv . From Lemma 4.3 and the triangle inequality:\n\u2016b1\u2016\u2212\u2016b2\u2016\u2264 \u2016b1 \u2212 b2\u2016= \u2016 f1 \u2212 f2\u2016\u2264 t\nwhere t = \u2211q s=1 \u221a 4+\u2016As \u20162+maxs\u2208{1, . . .,q} \u221a \u20162As \u20162+4\nn \u00b7\u039b . erefore, pdf(b1) pdf(b2) \u2264 exp(\u03f5). us, Algorithm 1 is \u03f5\u2212di erentially\nprivate."}, {"heading": "4.2 Extended Objective Perturbation", "text": "In this section, we present a solution based on the Objective Perturbation approach proposed in (Chaudhuri et al. 2011). Similarly to the Extended Objective Perturbation approach, we also consider the objective function as described in Equation (5). In this approach, instead of adding noise to the solution of the optimization problem as the output perturbation does, it adds noise to the objective function.\nManuscript submi ed to ACM\nAlgorithm 2 AExt\u2212Obj\u2212Pert: Extended Objective Perturbation Input: Data set D = {d1, . . . ,dn }, objective function J (f ;D), privacy budget \u03f5 , parameter \u039b Output: f \u2217\n1: \u2206\u2190 0 2: Compute \u03f5 \u2032 \u2190 \u03f5 \u2212 2 \u2211qs=1 log (1 + 14\u221a\u2016As \u20162+1n(\u039b+\u2206) ) 3: if \u03f5 \u2032 < \u03f5/2 then\n4: Binary search value of \u2206 such that 2 \u2211qs=1 log(1 + 14\u221a\u2016As \u20162+1n(\u039b+\u2206) ) = \u03f5/2 and set \u03f5 \u2032 \u2190 \u03f5/2 5: Compute t \u2190 \u2211qs=1 \u221a4 + \u2016As \u20162 + maxs \u2208{1, ...,q } \u221a\u20162As \u20162+4 6: Sample a random vector b such that pdf(b) \u221d exp (\u2212\u03f5 \u2032\u2016b\u2016/t) 7: f \u2217 \u2190 arg minf J (f ;D) + 1n \u3008b, f \u3009 + 1 2 \u2206\u2016 f \u20162 8: Output f \u2217\n4.2.1 Proposed Algorithm. Algorithm 2 shows the solution in pseudo-code. At Line 2, we compute \u03f5 \u2032 which is used to calibrate the magnitude of a random variable b. Here, the regularization parameter is equal to \u039b. At Line 3, if \u03f5 \u2032 < \u03f5/2, then it indicates that \u039b is not large enough. In this case, an additional positive regularization parameter \u2206 is picked to set the value of \u03f5 \u2032 equals to \u03f5/2 (Line 4). At Line 5, we compute t which is the sensitivity of \u2207J (f ;D). Line 6 samples a random vector b using the same method described in Subsection 4.1.1. Lines 7-8 return the solution of the noisy objective function using the BFGS algorithm.\n4.2.2 Privacy Guarantee. In this section, we will prove that the probability density of f \u2217 from Algorithm 2 satis es the di erential privacy de nition.\nTheorem 4.4. Algorithm 2 is \u03f5\u2212di erentially private.\nProof. e noisy objective function from Algorithm 2 is:\nf \u2217 = arg min f 1 n n\u2211 i=1 `(f ;di ) + 1 n \u3008b, f \u3009 + 1 2 (\u039b + \u2206) \u2016 f \u20162\nDue to the convexity of `(\u00b7), the gradient is zero at the minimal point f \u2217, equivalently,\nb = \u2212n(\u039b + \u2206)f \u2217 \u2212 n\u2211 i=1 \u2207`(f \u2217;di )\nDue to the strongly convexity of the objective function, there is a bijective (injective and surjective) mapping from f to b (denoted as f \u2192 b). erefore, we can transform the probability density function of random variable f to the probability density function of random variable b by a multiplication factor of the Jacobian determinant (Billingsley 2008). From that, the probability density ratio in di erential privacy can be rewri en as:\npdf(f | D) pdf(f | D \u2032) = pdf(b | D) pdf(b \u2032 | D \u2032) \u00b7\n|det (Jacob (f \u2192 b | D)) |\u22121\n|det (Jacob (f \u2192 b \u2032 | D \u2032)) |\u22121 (6)\nWe rst bound the ratio of the Jacobian determinants. Without loss of generality, we assume that the two data sets D and D \u2032 are di erent at nth record with dn \u2208 D and dn\u2032 \u2208 D \u2032. Let\nA = \u2212Jacob(f \u2192 b | D) = n(\u039b + \u2206)I + n\u2211 i=1 \u22072`(f \u2217;di )\nManuscript submi ed to ACM\nand E = \u22072`(f \u2217;dn ) \u2212 \u22072`(f \u2217;d \u2032n ), then\n|det (Jacob (f \u2192 b | D)) |\u22121\n|det (Jacob (f \u2192 b \u2032 | D \u2032)) |\u22121 = |det (A + E)| |det (A)| = |det (I + A \u22121E)|\nMoreover, E = \u2211qs=1 Esn \u2212\u2211qs=1 Esn\u2032 where Esn =\n (x sn )(x sn )\u2032 (1+exp(f \u2032x sn ))(1+exp(\u2212f \u2032x sn )) , i f s < tn \u2212y2n (x si )(x sn )\u2032 (1+exp(yn f \u2032x sn ))(1+exp(\u2212yn f \u2032x sn )) , i f s = tn 0, i f s > tn\nSimilarly, we can de ne Esn\u2032 by replacing n by n \u2032. From (Seiler and Simon 1975), for any square matrices A and B,\ndet (I + A + B) \u2264 det (I + |A|) \u00b7 det (I + |B |)\nwhere |A|= (A\u2032A) 1 2 . Moreover, A\u22121Esn and A\u22121Esn\u2032 are symmetric, thus\ndet (I + A\u22121E) \u2264 q\u220f s=1 det (I + A\u22121Esn ) \u00b7 det (I + A\u22121Esn\u2032 )\nWe now prove that |det ( I + A\u22121Esn ) |\u2264 1 + 1 4 \u221a \u2016As \u20162+1 n(\u039b+\u2206) . Because \u2212y2n(1+exp(yn f \u2032x sn ))(1+exp(\u2212yn f \u2032x sn )) \u2264 14 , and Esn is either a\nzero matrix or 1-rank matrix. e only non-zero eigenvalue of Esn if exist satis es |\u03bb1(Esn )|\u2264 14 \u2016xsn \u2016\u2264 1 4 \u221a \u2016As \u20162+1. As the objective function is (\u039b + \u2206)\u2212strongly convex, A is a full-rank matrix with each eigenvalue greater than n(\u039b + \u2206).\nerefore, |det ( I + A\u22121Esn ) |\u2264 1 + 1 4 \u221a \u2016As \u20162+1 n(\u039b+\u2206) . Similarly, |det (I + A \u22121Esn\u2032 )|\u2264 1 + 1 4 \u221a \u2016As \u20162+1 n(\u039b+\u2206) . erefore,\n|det (Jacob (f \u2192 b) | D) |\u22121\n|det (Jacob (f \u2192 b \u2032) | D \u2032) |\u22121 \u2264 exp\n( 2\nq\u2211 s=1 log(1 + 1 4 \u221a \u2016As \u20162+1 n\u039b ) ) (7)\nNext, we bound the ratio of the probability density of random vector b with respect to two neighboring data sets. We have:\nb \u2212 b \u2032 = \u2207`(f \u2217;dn ) \u2212 \u2207`(f \u2217;d \u2032n )\nFrom Lemma 4.2,\n\u2016b \u2212 b \u2032\u2016\u2264 q\u2211 s=1 \u221a \u2016As \u20162+4 + max s \u2208{1, ...,q } \u221a \u20162As \u20162+4\nerefore,\npdf(b | D) pdf(b \u2032 | D \u2032) \u2264 exp(\u03f5 \u2032\u2016b \u2212 b \u2032\u2016/t ) \u2264 exp(\u03f5 \u2032) (8)\nFrom (6), (7), (8), and \u03f5 = \u03f5 \u2032 + 2 \u2211qs=1 log (1 + 14\u221a\u2016As \u20162+1n\u039b ) , the theorem follows."}, {"heading": "5 PROPOSED SAMPLING APPROACH", "text": "In this section, we propose a solution which guarantees di erential privacy by directly sampling a random output from a modi ed version of the posterior distribution. In this work, we pick a normal distribution as the prior distribution. is is equivalent to using:\nU(f ;D) = \u22121 2 \u03c3 \u2016 f \u20162\u2212 n\u2211 i=1 `(f ;di )\nManuscript submi ed to ACM\nas the utility function in the exponential mechanism (McSherry and Talwar 2007) where the parameter \u03c3 is used to control the variance of the prior normal distribution. en, the di errentially private output is sampled from the following distribution:\npdf(f ) \u221d exp ( \u03f5U(f ;D)\n2\u2206U ) where \u2206U = maxd (D,D\u2032)=1,f \u2016U(f ;D) \u2212 U(f ;D \u2032)\u2016 is the sensitivity of U . e reason we pick a normal prior distribution instead of a uniform prior distribution is not because our proposed solution required so to guarantee di erential privacy but we observe that with a normal prior distribution the sampling algorithm converges be er and is more stable.\nMoreover, this approach requires the utility functionU(f ;D) has to have a bounded sensitivity. However, the loss function `(\u00b7) is not bounded. erefore, the functionU(f ;D) has unbounded sensitivity. In order to overcome this di culty, we propose a smooth sanitizer functionC(x ) which is used to control the maximum value of the loss function `(\u00b7). e de nition of C(x ) is given as follows:\nCv (x ) = v \u00b7 tanh (x v ) which is illustrated in Figure 2. We now take the composition ofCv (\u00b7) with `(f ;di ) to have a bounded-sensitivity utility function:\nU(f ;D) = \u22121 2 \u03c3 \u2016 f \u20162\u2212 n\u2211 i=1 Cv (`(f ;di ))\nWe intentionally pick the tanh(\u00b7) function as the sanitizer because it nicely keeps the loss function in its original form when the value of the loss function is near 0. Meanwhile, it deforms the loss function at large values to make the function nite. e advantage of this approach is that the sampled parameter can arbitrary large while the objective function is kept almost the same around the optimal parameter which maximizes the posterior probability. We describe the pseudo-code of our approach in Algorithm 3.\nManuscript submi ed to ACM\nAlgorithm 3 ASanitized\u2212EXP: Sanitized Loss Mechanism Input: Data set D = {di }ni=1, loss function `(f ;di ), privacy budget \u03f5 , maximum value v , parameter \u039b Output: f\n1: U(f ;D) = \u2212 12\u03c3 \u2016 f \u20162\u2212 \u2211n i=1Cv (`(f ;di )) 2: Sample a random vector f with the probability density\npdf(f ) \u221d exp ( \u03f5\n2v U(f ;D)\n)\nTheorem 5.1 (Privacy guarantee). Algorithm 3 is \u03f5\u2212di erentially private.\nProof. For two neighboring data sets D and D \u2032, \u2206U = maxf ,d (D,D\u2032)=1 |U(f ;D) \u2212U(f ;D \u2032)|\u2264 v . erefore, at any point f , we have\npdf(f | D) pdf(f | D \u2032) =\nexp ( \u03f5 2vU(f ;D) ) /\u222b exp( \u03f52v U(f ;D))df\nexp ( \u03f5 2vU(f ;D \u2032) ) /\u222b exp( \u03f52v U(f ;D\u2032))df\n\u2264 exp (\n2\u03f5 2v U(f ;D) \u2212U(f ;D \u2032) ) \u2264 exp(\u03f5)\nerefore, Algorithm 3 is \u03f5\u2212di erentially private.\ne problem with Algorithm 3 is that there is no run-time e cient algorithm to sample the distribution of f exactly. Bassily et al. (Bassily et al. 2014) proposed a polynomial run-time sampling algorithm. However, their proposed algorithm is still impractical due to the high degree of the polynomial run-time complexity and only apply for the log-convex function. Recently, there are developments (Ahn et al. 2012; Chen et al. 2014; Ma et al. 2015) in Markov Chain Monte Carlo (MCMC) method which can be applied to machine learning problems with large data sets. e idea is to construct Markov chains to simulate dynamical systems with stochastic gradients. At each step, we compute the gradient at the current location, then add a controlled amount of noise to the gradient and follow the noisy gradient to a new location. Asymptotically, the stationary distribution of this process converges to the true distribution from which the gradient is computed.\nIn this work, we propose to use an MCMC sampling algorithm, namely Preconditioned Stochastic Gradient Langevin Dynamics (pSGLD) (Li et al. 2015), to approximately sample the posterior distribution. pSGLD is good at sampling variables with di erences in scale which is useful for our problem because the parameter \u03b1 is usually much larger in magnitude than the parameter \u03b2 (recall that f = [\u03b1 , \u03b2]\u2032). e pseudo-code of pSGLD is described in Algorithm 4. At Line 1, we initialize the values ofV0 and f1. Line 3 computes the learning rate \u03f5t . It is required that limt\u2192\u221e \u2211 t \u03f5t \u2192\u221e\nand limt\u2192\u221e \u2211 t \u03f5 2 t < \u221e to guarantee the convergence. We sample uniformly k records from D for estimating the average gradient \u0434\u0304t (Line 5). We then compute the variance of the gradient at Line 6 ( is the element-wise product) and convert it to the preconditioned matrixGt at Line 7. We update the parameter at Line 8 with a noise variableN (0, \u03f5tGt ). It is worth to note that there is a permanent bias in pSGLD due to excluding a correction term in the updating step (Line 8). However, this bias is negligible and excluding the correction term helps to speed up the sampling algorithm which then helps to reduce the nite-sample bias as more steps are executed in a nite amount of time.\nManuscript submi ed to ACM\nAlgorithm 4 ApSGLD: pSGLD Sampling Algorithm Input: Data set D = {di }ni=1, loss function `, privacy parameter \u03f5 , \u00b5, k , bounded value v and learning rate \u03c4 Output: f T +1\n1: V0 \u2190 \u00ae0, f1 \u2190 \u00ae0 2: for t = 1 to T do 3: Compute \u03f5t \u2190 t\u2212\u03c4 4: Uniformly sample \u2126tk = { dt1 , . . . ,dtk } \u2282 D 5: Compute \u0434\u0304t = \u03f52v ( \u03c3 f t n + 1 k \u2211k i=1 \u2207Cv (`(f\nt ,dti ))) 6: V t \u2190 \u00b5V t\u22121 + (1 \u2212 \u00b5)(\u0434\u0304t \u0434\u0304t ) 7: Gt \u2190 1/ ( \u03bbI + dia\u0434( \u221a V t ) ) 8: f t+1 \u2190 f t \u2212 \u03f5t ( Gt \u00b7 n\u0434\u0304t ) +N (0, \u03f5tGt ) 9: Output f T+1"}, {"heading": "6 EXPERIMENTAL EVALUATION", "text": "In this section, we present the results of our experiments on four real data sets. We focus on answering the following three important research questions: (1) Does the sampling approach converge to its stationary distribution? (2) What is the trade-o between privacy and accuracy as compared to the non-private estimation? (3) Are the discrete-time regression models good alternatives to the Cox regression model? In the following sections, we address the above research questions accordingly.\n6.1 Data Sets\n6 8 10\n\u03b2(1)\n\u22121\n0\n1\n2\n\u03b2 (2\n)\n(a) FL\n\u22122 \u22121 0 1 2 \u03b2(1)\n\u22124\n\u22122\n0\n2\n\u03b2 (2\n)\n(b) TB\n\u22121.0 \u22120.5 0.0 0.5 1.0 \u03b2(1)\n\u22122\n\u22121\n0\n1\n\u03b2 (2\n)\n(c) WT\n\u22121 0 1 2 3 \u03b2(1)\n\u22122\n\u22121\n0\n1\n2\n\u03b2 (2\n)\n(d) SB\ne survival times in these four data sets are normalized to the interval [0, 1]. We set the number of discrete-time intervals q = 200. All the vectors of the explanatory variables are normalized to have zero mean and ed inside the unit sphere. We use the natural cubic spline with e = 3 knots to model the baseline hazard e ect."}, {"heading": "6.2 Convergence of the Proposed Sampling Approach", "text": "is section reports on the convergence of our proposed sampling approach. e aim is to check whether it converges to the stationary distribution. e loss function is bounded by the value v = 2 log(n) where n is the size of the data set. We set the parameter \u03c3 = 10\u22122 \u00b7 2v/\u03f5 . At each step of the Markov chain, we randomly pick k = 200 records from the data set to compute the gradient. We set the parameters \u03c4 = 0.51, \u03bb = 10\u22125 and \u00b5 = 0.99 in Algorithm 3. In Figure 3, we\nManuscript submi ed to ACM\nplot the estimated probability densities of the two rst parameters (\u03b2 (1) and \u03b2 (2)) a er 250 epochs from the sampling process. We remove the rst 104 steps as the Markov chain does not reach the stationary distribution at the beginning. We can observe that the probability densities of the samples are very similar to the normal distributions which are actually what we expect when sampling from the posterior distributions.\nFor a more formal test, we use the mean relative error (MRE) as a statistical test of convergence. MRE is de ned as follows:\nMRE = 1 t t\u2211 i=1 \u2016 fi \u2212 f \u2217\u2016 \u2016 f \u2217\u2016 (9)\nwhere fi is the parameter vector from the sampling process, f \u2217 is the optimal parameter vector which maximizes the likelihood in non-private se ing and t is the number of samples. We plot the MRE as the function of epochs with three di erent privacy budgets in Figure 4. Each epoch is a bundle of n steps. We observe that a er 250 epochs, MRE becomes stable which indicates that the sampling procedure converges to its stationary distribution."}, {"heading": "6.3 Trade-o between Privacy and Accuracy", "text": "In this section, we investigate the trade-o between privacy and accuracy in our proposed approaches. We rst need to pick the value of regularization terms for the perturbation approaches (Ext-Obj-Pert and Ext-Out-Pert) as the accuracy of these approaches are very much depend on the regularization parameter \u039b. We report in Table 2 the MREs of Ext-Out-Pert with di erent values of \u039b and privacy budget \u03f5 = 6.4. For consistency in performance comparison, we will use the best values of \u039b, which lead to the smallest relative error per data set.\nTo measure the accuracy of the proposed approaches at di erent privacy levels, the privacy budget is varied from 0.1 to 6.4. We also use MRE for the measurement. e results are shown in Figure 5. Overall, pSGLD outperforms both Manuscript submi ed to ACM\nExt-Out-Pert and Ext-Obj-Pert approaches. Moreover, we observe that the accuracy of Ext-Out-Pert and Ext-Obj-Pert does not improve much at high privacy budgets. It is due to the large regularization parameter that causes the output parameter moving towards the zero vector instead of the optimal parameter as the regularization term is the dominant factor of the objective function. Meanwhile, our proposed sampling approach (pSGLD) does not su er from this e ect which leads to much be er results at high privacy budgets."}, {"heading": "6.4 Comparison with Cox regression", "text": "Here, we want to con rm that the discrete-time regression models are good alternatives to the Cox regression model. We compare the results obtained from the non-private discrete-time regression models without regularization term to the results obtained from Cox regression. We use the relative error (RE) which is de ned as:\nRE = \u2016\u03b2 \u2212 \u03b2\u2217\u2016 \u2016\u03b2\u2217\u2016\nwhere \u03b2 is from the discrete-time regression with logit link and \u03b2\u2217 is from Cox regression. e results are shown in Table 3. We observe that the results obtained from the discrete-time regressions are very similar to the results obtained from the Cox regression with relative errors ranging from 2% \u2212 9%. At the worse case of the data set TB, the parameter obtained from the discrete-time model \u03b2 = [0.0122443,\u22120.849823,\u22120.239539]\u2032 is still a good approximation of the parameter obtained from the Cox model \u03b2\u2217 = [0.0585478,\u22120.790977,\u22120.23906]\u2032. As such, these results con rm that the discrete-time regression models are good alternatives to the Cox regression in practice."}, {"heading": "7 CONCLUSION", "text": "In this work, we propose solutions for the problem of protecting di erential privacy for discrete-time regression models used in survival analysis. In particular, we extend the perturbation approaches to a generalized form in which the loss function is a sum of logistic loss functions. In addition, we propose a sampling approach to practically protect di erential privacy by sampling a scaled posterior distribution with the pSGLD sampling algorithm. Even though we focus our work on discrete-time survival regression, our proposed approaches can be applied to other problems with similar loss functions as well. Moreover, our proposed approaches can be easily extended to discrete-time regression models in which the explanatory variables are changed over time. For further work, a di erentially private version of Cox regression would be a good complement to our work."}, {"heading": "ACKNOWLEDGMENTS", "text": "e authors would like to thank Dr. Xiaokui Xiao for his insightful comments on the privacy problem of Cox regression. Manuscript submi ed to ACM"}], "references": [{"title": "Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring", "author": ["Sungjin Ahn", "Anoop Kora\u008aikara Balan", "Max Welling"], "venue": "In ICML. Paul D Allison", "citeRegEx": "Ahn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2012}, {"title": "Private empirical risk minimization: E\u0081cient algorithms and tight error bounds", "author": ["1100\u20131120. Raef Bassily", "Adam Smith", "Abhradeep \u008cakurta"], "venue": null, "citeRegEx": "Bassily et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bassily et al\\.", "year": 2014}, {"title": "\u008ce \u201cmeaningful use\u201d regulation for electronic health records", "author": ["Patrick Billingsley"], "venue": "Science (FOCS),", "citeRegEx": "Billingsley.,? \\Q2014\\E", "shortCiteRegEx": "Billingsley.", "year": 2014}, {"title": "Privacy-preserving logistic regression", "author": ["Kamalika Chaudhuri", "Claire Monteleoni"], "venue": "In Advances in Neural Information Processing Systems. 289\u2013296", "citeRegEx": "Chaudhuri and Monteleoni.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri and Monteleoni.", "year": 2009}, {"title": "Stochastic gradient hamiltonian monte carlo", "author": ["Tianqi Chen", "Emily Fox", "Carlos Guestrin"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Regression models and life-tables", "author": ["1683\u20131691. David R Cox"], "venue": "David Roxbee Cox and David Oakes", "citeRegEx": "Cox.,? \\Q1992\\E", "shortCiteRegEx": "Cox.", "year": 1992}, {"title": "Electronic health records in ambulatory care-a national survey of physicians", "author": ["Alexandra E Shields"], "venue": "New England Journal of Medicine", "citeRegEx": "Shields,? \\Q2008\\E", "shortCiteRegEx": "Shields", "year": 2008}, {"title": "Bayesian sampling using stochastic gradient", "author": ["Nan Ding", "Youhan Fang", "Ryan Babbush", "Changyou Chen", "Robert D Skeel", "Hartmut Neven"], "venue": null, "citeRegEx": "Ding et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2014}, {"title": "Use of nonclonal serum immunoglobulin free light chains to predict overall survival in the general population", "author": ["L Joseph Melton"], "venue": null, "citeRegEx": "Melton,? \\Q2012\\E", "shortCiteRegEx": "Melton", "year": 2012}, {"title": "Di\u0082erential privacy: A survey of results", "author": ["Cynthia Dwork"], "venue": "Clinic Proceedings,", "citeRegEx": "Dwork.,? \\Q2008\\E", "shortCiteRegEx": "Dwork.", "year": 2008}, {"title": "\u008ce di\u0082erential privacy frontier. In \u008aeory of Cryptography Conference. Springer, 496\u2013502", "author": ["1\u201319. Cynthia Dwork"], "venue": "Cynthia Dwork", "citeRegEx": "Dwork.,? \\Q2009\\E", "shortCiteRegEx": "Dwork.", "year": 2009}, {"title": "Practical methods of optimization", "author": ["Roger Fletcher"], "venue": "Empirical Economics", "citeRegEx": "Fletcher.,? \\Q2014\\E", "shortCiteRegEx": "Fletcher.", "year": 2014}, {"title": "Use of electronic health records in US hospitals", "author": ["David Blumenthal."], "venue": "New England Journal of Medicine 360, 16 (2009), 1628\u20131638. Shiva Prasad Kasiviswanathan, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. 2013. Analyzing graphs with node di\u0082erential privacy. In \u008aeory of", "citeRegEx": "Blumenthal.,? 2009", "shortCiteRegEx": "Blumenthal.", "year": 2009}, {"title": "Private convex empirical risk minimization and high-dimensional regression", "author": ["Cryptography. Springer", "457\u2013476. Daniel Kifer", "Adam Smith", "Abhradeep \u008cakurta"], "venue": null, "citeRegEx": "Springer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Springer et al\\.", "year": 2012}, {"title": "Prevalence of monoclonal gammopathy of undetermined signi\u0080cance", "author": ["L Joseph Melton III."], "venue": "New England Journal of Medicine 354, 13 (2006),", "citeRegEx": "III.,? 2006", "shortCiteRegEx": "III.", "year": 2006}, {"title": "Preconditioned stochastic gradient Langevin dynamics for deep neural", "author": ["1362\u20131369. Chunyuan Li", "Changyou Chen", "David Carlson", "Lawrence Carin"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Optimizing linear counting queries under di\u0082erential privacy", "author": ["Chao Li", "Michael Hay", "Vibhor Rastogi", "Gerome Miklau", "Andrew McGregor"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Knowledge discovery and data mining", "author": ["Yi-An Ma", "Tianqi Chen", "Emily Fox"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Personalized social recommendations: accurate or private", "author": ["Ashwin Machanavajjhala", "Aleksandra Korolova", "Atish Das Sarma"], "venue": null, "citeRegEx": "Machanavajjhala et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Machanavajjhala et al\\.", "year": 2011}, {"title": "Di\u0082erentially private recommender systems: building privacy into the net", "author": ["McSherry", "Ilya Mironov"], "venue": "VLDB Endowment 4,", "citeRegEx": "McSherry and Mironov.,? \\Q2011\\E", "shortCiteRegEx": "McSherry and Mironov.", "year": 2011}, {"title": "Discrete-time survival mixture analysis", "author": ["Katherine Masyn"], "venue": "Journal of Educational and Behavioral statistics 30,", "citeRegEx": "Muth\u00e9n and Masyn.,? \\Q2005\\E", "shortCiteRegEx": "Muth\u00e9n and Masyn.", "year": 2005}, {"title": "Privacy for free: Posterior sampling and stochastic gradient monte carlo", "author": ["3277. Yu-Xiang Wang", "Stephen Fienberg", "Alex Smola"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Privacy-preserving cox", "author": ["Shipeng Yu", "Glenn Fung", "Romer Rosales", "Sriram Krishnan", "R Bharat Rao", "Cary Dehing-Oberije", "Philippe Lambin"], "venue": "Series C (Applied Statistics)", "citeRegEx": "Yu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2008}, {"title": "Towards accurate histogram publication under di\u0082erential privacy", "author": ["1034\u20131042. Xiaojian Zhang", "Rui Chen", "Jianliang Xu", "Xiaofeng Meng", "Yingtao Xie"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "functions to provide di\u0082erential privacy are also proposed in (Bassily et al. 2014; Kifer et al. 2012; Wang et al. 2015) for the ERM problems.", "startOffset": 62, "endOffset": 120}, {"referenceID": 21, "context": "functions to provide di\u0082erential privacy are also proposed in (Bassily et al. 2014; Kifer et al. 2012; Wang et al. 2015) for the ERM problems.", "startOffset": 62, "endOffset": 120}, {"referenceID": 1, "context": "(Bassily et al. 2014) proposed a polynomial run-time algorithm to sampling the log-concave objective function but their algorithm is still impractical due to the high degree of its polynomial run-time complexity.", "startOffset": 0, "endOffset": 21}, {"referenceID": 21, "context": "(Wang et al. 2015) proposed to use a stochastic gradient Nos\u00e9-Hoover thermostat algorithm (Ding et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 7, "context": "2015) proposed to use a stochastic gradient Nos\u00e9-Hoover thermostat algorithm (Ding et al. 2014) to sample the posterior distribution.", "startOffset": 77, "endOffset": 95}, {"referenceID": 15, "context": "In this work, we propose to use Preconditioned Stochastic Gradient Langevin Dynamics (pSGLD) sampling algorithm (Li et al. 2015) to sample the objective function due to its advantages in sampling multi-dimensional parameters with di\u0082erent scales.", "startOffset": 112, "endOffset": 128}, {"referenceID": 22, "context": "(Yu et al. 2008) is the only work on privacy protection for Cox regression.", "startOffset": 0, "endOffset": 16}, {"referenceID": 16, "context": "Di\u0082erential privacy is used in many applications such as histogram publication (Li et al. 2010; Zhang et al. 2014), graph analysis (Borgs et al.", "startOffset": 79, "endOffset": 114}, {"referenceID": 23, "context": "Di\u0082erential privacy is used in many applications such as histogram publication (Li et al. 2010; Zhang et al. 2014), graph analysis (Borgs et al.", "startOffset": 79, "endOffset": 114}, {"referenceID": 1, "context": "2013; Lu and Miklau 2014), regression and classi\u0080cation (Bassily et al. 2014; Chaudhuri and Monteleoni 2009; Kifer et al. 2012; Wang et al. 2015), recommender systems (Machanavajjhala et al.", "startOffset": 56, "endOffset": 145}, {"referenceID": 21, "context": "2013; Lu and Miklau 2014), regression and classi\u0080cation (Bassily et al. 2014; Chaudhuri and Monteleoni 2009; Kifer et al. 2012; Wang et al. 2015), recommender systems (Machanavajjhala et al.", "startOffset": 56, "endOffset": 145}, {"referenceID": 18, "context": "2015), recommender systems (Machanavajjhala et al. 2011; McSherry and Mironov 2009), etc.", "startOffset": 27, "endOffset": 83}, {"referenceID": 1, "context": "(Bassily et al. 2014) proposed a polynomial run-time sampling algorithm.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Recently, there are developments (Ahn et al. 2012; Chen et al. 2014; Ma et al. 2015) in Markov Chain Monte Carlo (MCMC) method which can be applied to machine learning problems with large data sets.", "startOffset": 33, "endOffset": 84}, {"referenceID": 4, "context": "Recently, there are developments (Ahn et al. 2012; Chen et al. 2014; Ma et al. 2015) in Markov Chain Monte Carlo (MCMC) method which can be applied to machine learning problems with large data sets.", "startOffset": 33, "endOffset": 84}, {"referenceID": 17, "context": "Recently, there are developments (Ahn et al. 2012; Chen et al. 2014; Ma et al. 2015) in Markov Chain Monte Carlo (MCMC) method which can be applied to machine learning problems with large data sets.", "startOffset": 33, "endOffset": 84}, {"referenceID": 15, "context": "In this work, we propose to use an MCMC sampling algorithm, namely Preconditioned Stochastic Gradient Langevin Dynamics (pSGLD) (Li et al. 2015), to approximately sample the posterior distribution.", "startOffset": 128, "endOffset": 144}], "year": 2017, "abstractText": "In survival analysis, regression models are used to understand the e\u0082ects of explanatory variables (e.g., age, sex, weight, etc.) to the survival probability. However, for sensitive survival data such as medical data, there are serious concerns about the privacy of individuals in the data set when medical data is used to \u0080t the regression models. \u008ce closest work addressing such privacy concerns is the work on Cox regression which linearly projects the original data to a lower dimensional space. However, the weakness of this approach is that there is no formal privacy guarantee for such projection. In this work, we aim to propose solutions for the regression problem in survival analysis with the protection of di\u0082erential privacy which is a golden standard of privacy protection in data privacy research. To this end, we extend the Output Perturbation and Objective Perturbation approaches which are originally proposed to protect di\u0082erential privacy for the Empirical Risk Minimization (ERM) problems. In addition, we also propose a novel sampling approach based on the Markov Chain Monte Carlo (MCMC) method to practically guarantee di\u0082erential privacy with be\u008aer accuracy. We show that our proposed approaches achieve good accuracy as compared to the non-private results while guaranteeing di\u0082erential privacy for individuals in the private data set.", "creator": "LaTeX with hyperref package"}}}