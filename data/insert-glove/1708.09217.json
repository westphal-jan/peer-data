{"id": "1708.09217", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2017", "title": "Look-ahead Attention for Generation in Neural Machine Translation", "abstract": "phil The disembarking attention model has zapotocny become passers-by a standard craftiest component in neural machine translation (NMT) and cockett it ramson guides luman translation lightner process 1,400-mile by selectively 2590 focusing miserables on parts of the vacationed source sentence 230-mile when mbongo predicting braids each target gazeteer word. pa\u00eds However, we jari find that the couriered generation kaptan of a crape target wissowa word does not only sub-montane depend zord on the source beyazid sentence, boswell but also rely mid-nite heavily on the slathers previous yohanes generated crull target stemberg words, especially the distant words which 95.11 are difficult oklahomans to ibovespa model zhenzong by kester using recurrent neural moonwalker networks. prosection To standers solve euro77 this interdicted problem, we gpv propose floorspace in this paper a georgian-style novel agarkar look - ahead large-bodied attention mechanism tendency for hivos generation champagne-ardenne in NMT, which aims at directly najarian capturing hessle the dependency penser relationship unburden between target paygo words. kroy We 109.26 further cakici design walkovers three patterns lleno to kamadhenu integrate anzack our bogdanos look - manejar ahead zago attention gabbai into the defecate conventional 72-hole attention banglalink model. 'em Experiments paestum on rengong NIST Chinese - to - 300-millimeter English mysticism and concurrency WMT goede English - to - selat German translation serbsky tasks decelerated show 500-horsepower that our mam\u00e9s proposed look - kendry ahead amnesiacs attention alliances mechanism achieves substantial hihnz improvements bound over state - of - the - art baselines.", "histories": [["v1", "Wed, 30 Aug 2017 11:27:02 GMT  (360kb,D)", "http://arxiv.org/abs/1708.09217v1", "12 pages, 5 figures"]], "COMMENTS": "12 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["long zhou", "jiajun zhang", "chengqing zong"], "accepted": false, "id": "1708.09217"}, "pdf": {"name": "1708.09217.pdf", "metadata": {"source": "CRF", "title": "Look-ahead Attention for Generation in Neural Machine Translation", "authors": ["Long Zhou", "Jiajun Zhang", "Chengqing Zong"], "emails": ["long.zhou@nlpr.ia.ac.cn", "jjzhang@nlpr.ia.ac.cn", "cqzong@nlpr.ia.ac.cn"], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation (NMT) has significantly improved the quality of machine translation in recent several years [10,26,1,9], in which the attention model increasingly plays an important role. Unlike traditional statistical machine translation (SMT) [13,4,32] which contains multiple separately tuned components, NMT builds upon a single and large neural network to directly map source sentence to associated target sentence.\nTypically, NMT adopts the encoder-decoder architecture which consists of two recurrent neural networks. The encoder network models the semantics of the source sentence and transforms the source sentence into context vector representation, from which the decoder network generates the target translation word by word. Attention mechanism has become an indispensable component in NMT, which enables the model to dynamically compose source representation for each timestep during decoding, instead of a single and static representation. Specifically, the attention model shows which source words the model should focus on in order to predict the next target word.\nHowever, previous attention models are mainly designed to predict the alignment of a target word with respect to source words, which take no account of the\nar X\niv :1\n70 8.\n09 21\n7v 1\n[ cs\n.C L\n] 3\n0 A\nug 2\n2 !\"#$%\n&'#(%)*\nfact that the generation of a target word may have a stronger correlation with the previous generated target words. Recurrent neural networks, such as gated recurrent units (GRU) [5] and long short term memory (LSTM) [8], still suffer from long-distance dependency problems, according to pioneering studies [1,12] that the performance of NMT is getting worse as source sentences get longer. Figure 1 illustrates an example of Chinese-English translation. The dependency relationship of target sentence determines whether the predicate of the sentence should be singular (is) or plural (are). While the conventional attention model does not have a specific mechanism to learn the dependency relationship between target words.\nTo address this problem, we propose in this paper a novel look-ahead attention mechanism for generation in NMT, which can directly model the longdistance dependency relationship between target words. The look-ahead attention model does not only align to source words, but also refer to the previous generated target words when generating a target word. Furthermore, we present and investigate three patterns for the look-ahead attention, which can be integrated into any attention-based NMT. To show the effectiveness of our look-ahead attention, we have conducted experiments on NIST Chinese-to-English translation tasks and WMT14 English-to-German translation tasks. Experiments show that our proposed model obtains significant BLEU score improvements over strong SMT baselines and a state-of-the-art NMT baseline."}, {"heading": "2 Neural Machine Translation", "text": "Our framework integrating the look-ahead attention mechanism into NMT can be applied in any conventional attention model. Without loss of generality, we use the improved attention-based NMT proposed by Luong et al. [16], which utilizes stacked LSTM layers for both encoder and decoder as illustrated in Figure 2.\nThe NMT first encodes the source sentence X = (x1, x2, ..., xm) into a sequence of context vector representation C = (h1, h2, ..., hm) whose size varies\n1 http://nlp.stanford.edu:8080/parser/index.jsp.\nwith respect to the source sentence length. Then, the NMT decodes from the context vector representation C and generates target translation Y = (y1, y2, ..., yn) one word each time by maximizing the probability of p(yj |y<j , C). Next, we briefly review the encoder introducing how to obtain C and the decoder addressing how to calculate p(yj |y<j , C).\nEncoder: The context vector representation C = (hl1, h l 2, ..., h l m) are generated by the encoder using l stacked LSTM layers. Bi-directional connections are used for the bottom encoder layer, and h1i is a concatenation of a left-to-right\u2212\u2192 h 1i and a right-to-left \u2190\u2212 h 1i ,\nh1i = [\u2212\u2192 h 1i\u2190\u2212 h 1i ] = [ LSTM( \u2212\u2192 h 1i\u22121, xi) LSTM( \u2190\u2212 h 1i\u22121, xi) ] (1)\nAll other encoder layers are unidirectional, and hki is calculated as follows:\nhki = LSTM(h k i\u22121, h k\u22121 i ) (2)\nDecoder: The conditional probability p(yj |y<j , C) is formulated as\np(yj |Y<j , C) = p(yj |Y<j , cj) = softmax(Wstj) (3)\nSpecifically, we employ a simple concatenation layer to produce an attentional hidden state tj :\ntj = tanh(Wc[s l j ; cj ] + b) = tanh(W 1 c s l j +W 2 c cj + b) (4)\nwhere slj denotes the target hidden state at the top layer of a stacking LSTM. The attention model calculates cj as the weighted sum of the source-side context\n4 vector representation, just as illustrated in the upper left corner of Figure 2.\ncj = m\u2211 i=1 ATT (slj , h l i) \u00b7 hli = m\u2211 i=1 \u03b1jih l i (5)\nwhere \u03b1ji is a normalized item calculated as follows:\n\u03b1ji = exp(hli \u00b7 slj)\u2211 i\u2032 exp(h l i\u2032 \u00b7 slj)\n(6)\nskj is computed by using the following formula:\nskj = LSTM(s k j\u22121, s k\u22121 j ) (7)\nIf k = 1, s1j will be calculated by combining tj\u22121 as feed input [16]:\ns1j = LSTM(s 1 j\u22121, yj\u22121, tj\u22121) (8)\nGiven the bilingual training data D = {(X(z), Y (z))}Zz=1, all parameters of the attention-based NMT are optimized to maximize the following conditional log-likelihood:\nL(\u03b8) = 1\nZ Z\u2211 z=1 n\u2211 j=1 logp(y (z) j |y (z) <j , X (z), \u03b8) (9)"}, {"heading": "3 Model Description", "text": "Learning long-distance dependencies is a key challenge in machine translation. Although the attention model introduced above has shown its effectiveness in NMT, it takes no account of the dependency relationship between target words. Hence, in order to relieve the burden of LSTM or GRU to carry on the target-side long-distance dependencies, we design a novel look-ahead attention mechanism, which directly establishes a connection between the current target word and the previous generated target words. In this section, we will elaborate on three proposed approaches about integrating the look-ahead attention into the generation of attention-based NMT."}, {"heading": "3.1 Concatenation Pattern", "text": "Figure 3(b) illustrates concatenation pattern of the look-ahead attention mechanism. We not only compute the attention between current target hidden state and source hidden states, but also calculate the attention between current target hidden state and previous target hidden states. The look-ahead attention output at timestep j is computed as:\ncdj = j\u22121\u2211 i=1 ATT (slj , s l i) \u00b7 sli (10)\nwhere ATT (slj , s l i) is a normalized item.\nSpecifically, given the target hidden state slj , the source-side context vector representation cj , and the target-side context vector representation c d j , we employ a concatenation layer to combine the information to produce an attentional hidden state as follows:\ntfinalj = tanh(Wc[s l j ; cj ; c d j ] + b) (11)\nAfter getting the attentional hidden state tfinalj , we can calculate the conditional probability p(yj |y<j , C) as formulated in Eq. 3."}, {"heading": "3.2 Enc-Dec Pattern", "text": "Concatenation pattern is a simple method to achieve look-ahead attention, which regards source-side context vector representation and target-side context vector representation as the same importance. Different from concatenation pattern, Enc-Dec pattern utilizes a hierarchical architecture to integrate look-ahead attention as shown in Figure 3(c).\nOnce we get the attentional hidden state of conventional attention-based NMT, we can employ look-ahead attention mechanism to update the previous attentional hidden state. In detail, the model first computes the attentional hidden state tej of conventional attention-based NMT as Eq. 4. Second, the model\n6 calculates the attention between the attentional hidden state tej and previous target hidden states:\ncdj = j\u22121\u2211 i=1 ATT (tej , s l i) \u00b7 sli (12)\nThen, the final attentional hidden state is calculated as followed:\ntfinalj = tanh(Wc2[t e j ; c d j ] + b2) (13)"}, {"heading": "3.3 Dec-Enc Pattern", "text": "Dec-Enc pattern is the opposite of the Enc-Dec pattern, and it uses look-ahead attention mechanism to help the model align to source words. Figure 3(d) shows this pattern. We compute look-ahead attention output firstly as Eq. 10, and attentional hidden state is computed by:\ntdj = tanh(Wc1[s l j ; c d j ] + b) (14)\nFinally, we can calculate the attention between the attentional hidden state tdj and source hidden states to get final attentional hidden state:\ntfinalj = tanh(Wc2[t d j ; c e j ] + b2) (15)\ncej = m\u2211 i=1 ATT (tdj , h l i) \u00b7 hli (16)\nwhere hli is source-side hidden state at the top layer."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Dataset", "text": "We perform our experiments on the NIST Chinese-English translation tasks and WMT14 English-German translation tasks. The evaluation metric is BLEU [21] as calculated by the multi-blue.perl script.\nFor Chinese-English, our training data consists of 630K sentence pairs extracted from LDC corpus2. We use NIST 2003(MT03) Chinese-English dataset as the validation set, NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06) datasets as our test sets. Besides, 10M Xinhua portion of Gigaword corpus is used in training language model for SMT.\nFor English-German, to compare with the results reported by previous work [16,25,34], we used the same subset of the WMT 2014 training corpus3 that contains 4.5M sentence pairs with 116M English words and 110M German words. The concatenation of news-test 2012 and news-test 2013 is used as the validation set and news-test 2014 as the test set. 2 The corpora include LDC2000T50, LDC2002T01, LDC2002E18, LDC2003E07,\nLDC2003E14, LDC2003T17 and LDC2004T07. 3 http://www.statmt.org/wmt14/translation-task.html\n7"}, {"heading": "4.2 Training Details", "text": "We build the described models modified from the Zoph RNN4 toolkit which is written in C++/CUDA and provides efficient training across multiple GPUs. Our training procedure and hyper parameter choices are similar to those used by Luong et al. [16]. In the NMT architecture as illustrated in Figure 2, the encoder has three stacked LSTM layers including a bidirectional layer, followed by a global attention layer, and the decoder contains two stacked LSTM layers followed by the softmax layer.\nIn more details, we limit the source and target vocabularies to the most frequent 30K words for Chinese-English and 50K words for English-German. The word embedding dimension and the size of hidden layers are all set to 1000. Parameter optimization is performed using stochastic gradient descent(SGD), and we set learning rate to 0.1 at the beginning and halve the threshold while the perplexity go up on the development set. Each SGD is a mini-batch of 128 examples. Dropout was also applied on each layer to avoid over-fitting, and the dropout rate is set to 0.2. At test time, we employ beam search with beam size b = 12."}, {"heading": "4.3 Results on Chinese-English Translation", "text": "We list the BLEU scores of our proposed model in Table 1. Moses-1 [11] is the state-of-the-art phrase-based SMT system with the default configuration and a 4-gram language model trained on the target portion of training data. Moses2 is the same as Moses-1 except that the language model is trained using the target data plus 10M Xinhua portion of Gigaword corpus. The BLEU score of our NMT baseline, which is an attention-based NMT as introduced in Section 2, is about 4.5 higher than the state-of-the-art SMT system Moses-2.\nFor the last three lines in Table 1, Enc-Dec pattern outperforms concatenation pattern and even Dec-Enc pattern, which shows Enc-Dec pattern is best\n4 https://github.com/isi-nlp/Zoph RNN\n8 approach to take advantage of look-ahead attention. Moreover, our Enc-Dec pattern gets an improvement of +0.93 BLEU points over the state-of-the-art NMT baseline, which demonstrates that the look-ahead attention mechanism is effective for generation in conventional attention-based NMT.\nEffects of Translating Long Sentences A well-known flaw of NMT model is the inability to properly translate long sentences. One of the goals that we integrate the look-ahead attention into the generation of NMT decoder is boosting the performance in translating long sentence. We follow Bahdanau et al. [1] to group sentences of similar lengths together and compute a BLEU score per group, as demonstrated in Figure 4.\nAlthough the performance of both the NMT baseline and our proposed model drops rapidly when the length of source sentence increases, our Enc-Dec model is more effective than the NMT Baseline in handling long sentences. Specifically, our proposed model gets an improvement of 1.88 BLEU points over the baseline from 50 to 60 words in source language. Furthermore, when the length of input sentence is greater than 60, our model still outperforms the baseline by 1.04 BLEU points. Experiments show that the look-ahead attention can relieve the burden of LSTM to carry on the target-side long-distance dependencies.\nTarget Alignment of Look-ahead Attention The conventional attention models always refer to some source words when generating a target word. We propose a look-ahead attention for generation in NMT, which also focuses on previous generated words in order to predict the next target word.\nWe provide two real translation examples to show the target alignment of look-ahead attention in Figure 5. The first line is blank because it does not have look-ahead attention when generating the first word. Every line represents the weight distribution for previous generated words when predicting current target word. More specifically, we find some interesting phenomena. First, target words often refer to verb or predicate which has been generated previously, such as the word \u201cwas\u201d in Figure 5(a).\nSecond, the heat map shows that the word \u201cwe\u201d and the word \u201clooking\u201d have a stronger correlation when translating the Chinese sentence as demonstrated in Figure 5(b). Intuitively, the look-ahead attention mechanism establishes a bridge to capture the dependency relationship between target words. Third, most target words mainly focus on the word immediately before the current target word, which may be due to the fact that the last generated word contains more information in recurrent neural networks. We can control the influence of the look-ahead attention like Tu et al. [27] to improve translation quality and instead we remain it as our future work."}, {"heading": "4.4 Results on English-German Translation", "text": "We evaluate our model on the WMT14 translation tasks for English to German, whose results are presented in Table 2. We find that our proposed look-ahead attention NMT model also obtains significant accuracy improvements on largescale English-German translation.\nIn addition, we compare our NMT systems with various other systems including Zhou et al. [34] which use a much deeper neural network. Luong et al. [16] achieves BLEU score of 19.00 with 4 layers deep Encoder-Decoder model. Shen et al. [25] obtained the BLEU score of 18.02 with MRT techniques. For this work,\n10\nour Enc-Dec look-ahead attention NMT model with two layers achieves 20.36 BLEU scores, which is on par with Zhou et al. [34] in term of BLEU. Note that Zhou et al. [34] employ a much larger depth as well as vocabulary size to obtain their best results."}, {"heading": "5 Related Work", "text": "The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].\nOur goal in this work is to design a smart attention mechanism to model the dependency relationship between target words. Tu et al. [28] and Mi et al. [19] proposed to extend attention models with a coverage vector in order to attack the problem of repeating and dropping translations. Cohn et al. [6] augmented the attention model with well-known features in traditional SMT. Unlike previous works that attention models are mainly designed to predict the alignment of a target word with respect to source words, we focus on establishing a direct bridge to capture the long-distance dependency relationship between target words. In addition, Wu et al. [31] lately proposed a sequence-to-dependency NMT method, in which the target word sequence and its corresponding dependency structure are jointly constructed and modeled. However, the target dependency tree references are needed for training in this model and our proposed model does not need extra resources.\nVery Recently, Vaswani et al. [29] proposed a new simple network architecture, Transformer, based solely on attention mechanisms with multi-headed self attention. Besides, Lin et al. [15] presented a self-attention mechanism which extracts different aspects of the sentence into multiple vector representations. And the self-attention model has been used successfully in some tasks including abstractive summarization and reading comprehension[22,2]. Here, in order to alleviate the burden of LSTM to carry on the target-side long-distance dependencies of NMT, we propose to integrate the look-ahead attention mechanism\n11\ninto the conventional attention-based NMT which is used in conjunction with a recurrent network."}, {"heading": "6 Conclusion", "text": "In this work, we propose a novel look-ahead attention mechanism for generation in NMT, which aims at directly capturing the long-distance dependency relationship between target words. The look-ahead attention model not only aligns to source words, but also refers to the previous generated words when generating the next target word. Furthermore, we present and investigate three patterns to integrate our proposed look-ahead attention into the conventional attention model. Experiments on Chinese-to-English and English-to-German translation tasks show that our proposed model obtains significant BLEU score gains over strong SMT baselines and a state-of-the-art NMT baseline."}, {"heading": "Acknowledgments", "text": "The research work has been funded by the Natural Science Foundation of China under Grant No. 61673380, No. 61402478 and No. 61403379."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In Proceedings of ICLR 2015", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory-networks for machine reading", "author": ["J. Cheng", "L. Dong", "M. Lapata"], "venue": "arXiv preprint arXiv:1601.06733", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-supervised learning for neural machine translation", "author": ["Y. Cheng", "W. Xu", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "In Proceedings of ACL 2016", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["D. Chiang"], "venue": "In Proceedings of ACL 2005", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning phrase representations using RNN encoderCdecoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proceedings of EMNLP 2014", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Incorporating structural alignment biases into an attentional neural translation model", "author": ["T. Cohn", "C.D.V. Hoang", "E. Vymolova", "K. Yao", "C. Dyer", "G. Haffari"], "venue": "arXiv preprint arXiv:1601.01085", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Improved neural machine translation with SMT features", "author": ["W. He", "Z. He", "H. Wu", "H. Wang"], "venue": "In Proceedings of AAAI 2016", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory, vol", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "9. MIT Press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Is neural machine translation ready for deployment? A case study on 30 translation directions", "author": ["M. Junczys-Dowmunt", "T. Dwojak", "H. Hoang"], "venue": "In Proceedings of IWSLT 2016", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "In Proceedings of EMNLP 2013", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R Zens"], "venue": "Association for Computational Linguistics", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Six challanges for neural machine translation", "author": ["P. Koehn", "R. Knowles"], "venue": "arXiv preprint arXiv:1706.03872", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "In Proceedings of ACL-NAACL 2013", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Towards zero unknown word in neural machine translation", "author": ["X. Li", "J. Zhang", "C. Zong"], "venue": "In Proceedings of IJCAI 2016", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "A structured self-attentive sentence embedding", "author": ["Z. Lin", "M. Feng", "Santos", "C.N.d.", "M. Yu", "B. Xiang", "B. Zhou", "Y. Bengio"], "venue": "arXiv preprint arXiv:1703.03130", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2017}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M.T. Luong", "H. Pham", "C.D. Manning"], "venue": "In Proceedings of EMNLP 2015", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["M.T. Luong", "I. Sutskever", "Q.V. Le", "O. Vinyals", "W. Zaremba"], "venue": "In Proceedings of ACL 2015", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Interactive attention for neural machine translation", "author": ["F. Meng", "Z. Lu", "H. Li", "Q. Liu"], "venue": "In Proceedings of COLING 2016", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "A coverage embedding model for neural machine translation", "author": ["H. Mi", "B. Sankaran", "Z. Wang", "N. Ge", "A. Ittycheriah"], "venue": "In Proceedings of EMNLP 2016", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Supervised attentions for neural machine translation", "author": ["H. Mi", "Z. Wang", "N. Ge", "A. Ittycheriah"], "venue": "In Proceedings of EMNLP 2016", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Bleu: a methof for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. Zhu"], "venue": "In Proceedings of ACL 2002", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "A deep reinforced model for abstractive summarization", "author": ["R. Paulus", "C. Xiong", "R. Socher"], "venue": "arXiv preprint arXiv:1705.04304", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2017}, {"title": "Improving neural machine translation models with monolingual data", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "In Proceedings of ACL 2016", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "In Proceedings of ACL 2016", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Minimum risk training for neural machine translation", "author": ["S. Shen", "Y. Cheng", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "In Proceedings of ACL 2016", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Proceedings of NIPS 2014", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Context gates for neural machine translation", "author": ["Z. Tu", "Y. Liu", "Z. Lu", "X. Liu", "H. Li"], "venue": "arXiv preprint arXiv:1608.06043", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Modeling coverage for neural machine translation", "author": ["Z. Tu", "Z. Lu", "Y. Liu", "X. Liu", "H. Li"], "venue": "In Proceedings of ACL 2016", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Attention is all you need", "author": ["A. Vawani", "N. Shazeer", "N. Parmar", "J. Uszkoreit", "L. Jones", "A.N.Gomez", "L. Kaiser", "I. Polosukhin"], "venue": "arXiv preprint arXiv:1706.03762", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation advised by statistical machine translation", "author": ["X. Wang", "Z. Lu", "Z. Tu", "H. Li", "D. Xiong", "M. Zhang"], "venue": "In Proceedings of AAAI 2017", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2017}, {"title": "Sequence-to-dependency neural machine translation", "author": ["S. Wu", "D. Zhang", "N. Yang", "M. Li", "M. Zhou"], "venue": "In Proceedings of ACL 2017", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}, {"title": "Tree-based translation without using parse trees", "author": ["F. Zhai", "J. Zhang", "Y. Zhou", "C Zong"], "venue": "In Proceedings of COLING 2012", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploiting source-side monolingual data in neural machine translation", "author": ["J. Zhang", "C. Zong"], "venue": "In Proceedings of EMNLP 2016", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep recurrent models with fast-forward connections for neural machine translation", "author": ["J. Zhou", "Y. Cao", "X. Wang", "P. Li", "W. Xu"], "venue": "arXiv preprint arXiv:1606.04199", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural system combination for machine translation", "author": ["L. Zhou", "W. Hu", "J. Zhang", "C. Zong"], "venue": "In Proceedings of ACL 2017", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 9, "context": "Neural machine translation (NMT) has significantly improved the quality of machine translation in recent several years [10,26,1,9], in which the attention model increasingly plays an important role.", "startOffset": 119, "endOffset": 130}, {"referenceID": 25, "context": "Neural machine translation (NMT) has significantly improved the quality of machine translation in recent several years [10,26,1,9], in which the attention model increasingly plays an important role.", "startOffset": 119, "endOffset": 130}, {"referenceID": 0, "context": "Neural machine translation (NMT) has significantly improved the quality of machine translation in recent several years [10,26,1,9], in which the attention model increasingly plays an important role.", "startOffset": 119, "endOffset": 130}, {"referenceID": 8, "context": "Neural machine translation (NMT) has significantly improved the quality of machine translation in recent several years [10,26,1,9], in which the attention model increasingly plays an important role.", "startOffset": 119, "endOffset": 130}, {"referenceID": 12, "context": "Unlike traditional statistical machine translation (SMT) [13,4,32] which contains multiple separately tuned components, NMT builds upon a single and large neural network to directly map source sentence to associated target sentence.", "startOffset": 57, "endOffset": 66}, {"referenceID": 3, "context": "Unlike traditional statistical machine translation (SMT) [13,4,32] which contains multiple separately tuned components, NMT builds upon a single and large neural network to directly map source sentence to associated target sentence.", "startOffset": 57, "endOffset": 66}, {"referenceID": 31, "context": "Unlike traditional statistical machine translation (SMT) [13,4,32] which contains multiple separately tuned components, NMT builds upon a single and large neural network to directly map source sentence to associated target sentence.", "startOffset": 57, "endOffset": 66}, {"referenceID": 4, "context": "Recurrent neural networks, such as gated recurrent units (GRU) [5] and long short term memory (LSTM) [8], still suffer from long-distance dependency problems, according to pioneering studies [1,12] that the performance of NMT is getting worse as source sentences get longer.", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "Recurrent neural networks, such as gated recurrent units (GRU) [5] and long short term memory (LSTM) [8], still suffer from long-distance dependency problems, according to pioneering studies [1,12] that the performance of NMT is getting worse as source sentences get longer.", "startOffset": 101, "endOffset": 104}, {"referenceID": 0, "context": "Recurrent neural networks, such as gated recurrent units (GRU) [5] and long short term memory (LSTM) [8], still suffer from long-distance dependency problems, according to pioneering studies [1,12] that the performance of NMT is getting worse as source sentences get longer.", "startOffset": 191, "endOffset": 197}, {"referenceID": 11, "context": "Recurrent neural networks, such as gated recurrent units (GRU) [5] and long short term memory (LSTM) [8], still suffer from long-distance dependency problems, according to pioneering studies [1,12] that the performance of NMT is getting worse as source sentences get longer.", "startOffset": 191, "endOffset": 197}, {"referenceID": 15, "context": "[16], which utilizes stacked LSTM layers for both encoder and decoder as illustrated in Figure 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "If k = 1, sj will be calculated by combining tj\u22121 as feed input [16]: sj = LSTM(s 1 j\u22121, yj\u22121, tj\u22121) (8)", "startOffset": 64, "endOffset": 68}, {"referenceID": 20, "context": "The evaluation metric is BLEU [21] as calculated by the multi-blue.", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "For English-German, to compare with the results reported by previous work [16,25,34], we used the same subset of the WMT 2014 training corpus that contains 4.", "startOffset": 74, "endOffset": 84}, {"referenceID": 24, "context": "For English-German, to compare with the results reported by previous work [16,25,34], we used the same subset of the WMT 2014 training corpus that contains 4.", "startOffset": 74, "endOffset": 84}, {"referenceID": 33, "context": "For English-German, to compare with the results reported by previous work [16,25,34], we used the same subset of the WMT 2014 training corpus that contains 4.", "startOffset": 74, "endOffset": 84}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Moses-1 [11] is the state-of-the-art phrase-based SMT system with the default configuration and a 4-gram language model trained on the target portion of training data.", "startOffset": 8, "endOffset": 12}, {"referenceID": 0, "context": "[1] to group sentences of similar lengths together and compute a BLEU score per group, as demonstrated in Figure 4.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "[27] to improve translation quality and instead we remain it as our future work.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] which use a much deeper neural network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] achieves BLEU score of 19.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] obtained the BLEU score of 18.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] LSTM with 4 layers+dropout+local att.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Gated RNN with search + MRT 50K 18.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] LSTM with 16 layers + F-F connections 160K 20.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] in term of BLEU.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] employ a much larger depth as well as vocabulary size to obtain their best results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 93, "endOffset": 109}, {"referenceID": 18, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 93, "endOffset": 109}, {"referenceID": 19, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 93, "endOffset": 109}, {"referenceID": 27, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 93, "endOffset": 109}, {"referenceID": 17, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 93, "endOffset": 109}, {"referenceID": 16, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 165, "endOffset": 175}, {"referenceID": 13, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 165, "endOffset": 175}, {"referenceID": 23, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 165, "endOffset": 175}, {"referenceID": 2, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 217, "endOffset": 226}, {"referenceID": 22, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 217, "endOffset": 226}, {"referenceID": 32, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 217, "endOffset": 226}, {"referenceID": 24, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 259, "endOffset": 271}, {"referenceID": 6, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 259, "endOffset": 271}, {"referenceID": 34, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 259, "endOffset": 271}, {"referenceID": 29, "context": "Most of the existing approaches and models mainly focus on designing better attention models [16,19,20,28,18], better strategies for handling rare and unknown words [17,14,24], exploiting large-scale monolingual data [3,23,33], and integrating SMT techniques [25,7,35,30].", "startOffset": 259, "endOffset": 271}, {"referenceID": 27, "context": "[28] and Mi et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] proposed to extend attention models with a coverage vector in order to attack the problem of repeating and dropping translations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] augmented the attention model with well-known features in traditional SMT.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[31] lately proposed a sequence-to-dependency NMT method, in which the target word sequence and its corresponding dependency structure are jointly constructed and modeled.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] proposed a new simple network architecture, Transformer, based solely on attention mechanisms with multi-headed self attention.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] presented a self-attention mechanism which extracts different aspects of the sentence into multiple vector representations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "And the self-attention model has been used successfully in some tasks including abstractive summarization and reading comprehension[22,2].", "startOffset": 131, "endOffset": 137}, {"referenceID": 1, "context": "And the self-attention model has been used successfully in some tasks including abstractive summarization and reading comprehension[22,2].", "startOffset": 131, "endOffset": 137}], "year": 2017, "abstractText": "The attention model has become a standard component in neural machine translation (NMT) and it guides translation process by selectively focusing on parts of the source sentence when predicting each target word. However, we find that the generation of a target word does not only depend on the source sentence, but also rely heavily on the previous generated target words, especially the distant words which are difficult to model by using recurrent neural networks. To solve this problem, we propose in this paper a novel look-ahead attention mechanism for generation in NMT, which aims at directly capturing the dependency relationship between target words. We further design three patterns to integrate our look-ahead attention into the conventional attention model. Experiments on NIST Chinese-to-English and WMT English-to-German translation tasks show that our proposed look-ahead attention mechanism achieves substantial improvements over state-of-the-art baselines.", "creator": "LaTeX with hyperref package"}}}