{"id": "1709.02555", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Causality-Aided Falsification", "abstract": "64-member Falsification 2,503 is drawing 138.2 attention in kreisau quality assurance vel\u00e1zquez of heterogeneous systems abrasive whose complexities are kirke beyond brahms most verification 2,213 techniques ' matlock scalability. In 3,859 this compagnie paper belloy we salavan introduce 881,000 the idea of appreciated causality aid byng in falsification: stoa by isoenzymes providing a falsification solver - - savinova that relies sabaot on stochastic optimization demmer of ozkan a axiomatic certain cost function - - labor-progressive with paymer suitable sulfite causal information d'un expressed jindong by sutan a shtahd Bayesian dualtone network, search trosky for a josephthal falsifying unconsidered input value can be 1,952 efficient. Our 24.87 experiment bassam results show the branzburg idea ' meffert s duclos viability.", "histories": [["v1", "Fri, 8 Sep 2017 06:34:21 GMT  (1096kb,D)", "http://arxiv.org/abs/1709.02555v1", "In Proceedings FVAV 2017,arXiv:1709.02126"]], "COMMENTS": "In Proceedings FVAV 2017,arXiv:1709.02126", "reviews": [], "SUBJECTS": "cs.SY cs.AI cs.LG cs.LO", "authors": ["takumi akazaki", "yoshihiro kumazawa", "ichiro hasuo university of tokyo", "national institute of informatics)"], "accepted": false, "id": "1709.02555"}, "pdf": {"name": "1709.02555.pdf", "metadata": {"source": "CRF", "title": "Causality-Aided Falsification", "authors": ["Takumi Akazaki", "Yoshihiro Kumazawa", "Ichiro Hasuo"], "emails": ["akazaki@ms.k.u-tokyo.ac.jp", "kumazawa@is.s.u-tokyo.ac.jp", "hasuo@nii.ac.jp"], "sections": [{"heading": null, "text": "L. Bulwahn, M. Kamali, S. Linker (Eds.): First Workshop on Formal Verification of Autonomous Vehicles (FVAV 2017). EPTCS 257, 2017, pp. 3\u201318, doi:10.4204/EPTCS.257.2\nc\u00a9 T. Akazaki, Y. Kumazawa, and I. Hasuo This work is licensed under the Creative Commons Attribution License.\nCausality-Aided Falsification\nTakumi Akazaki \u2217\nThe University of Tokyo, Japan JSPS Research Fellow\nakazaki@ms.k.u-tokyo.ac.jp\nYoshihiro Kumazawa The University of Tokyo, Japan\nkumazawa@is.s.u-tokyo.ac.jp\nIchiro Hasuo \u2020\nNational Institute of Informatics, Tokyo, Japan\nhasuo@nii.ac.jp\nFalsification is drawing attention in quality assurance of heterogeneous systems whose complexities are beyond most verification techniques\u2019 scalability. In this paper we introduce the idea of causality aid in falsification: by providing a falsification solver\u2014that relies on stochastic optimization of a certain cost function\u2014with suitable causal information expressed by a Bayesian network, search for a falsifying input value can be efficient. Our experiment results show the idea\u2019s viability."}, {"heading": "1 Introduction", "text": "Falsification In computer science, verification refers to the task of giving a mathematical proof to the claim that the behavior of a system M satisfies a desired property \u03d5 (called a specification), under any circumstances (such as choices of input to the system M ). A mathematical proof thus obtained gives a level of confidence that is fundamentally different from empirical guarantees given by testing.\nExtensive research efforts have yielded a number of effective verification techniques and they have seen successful real-world applications. At the same time, however, it is also recognized that large-scale heterogeneous systems are still beyond the scalability of most of these verification techniques. Notable among such are cyber-physical systems (CPSs) that exhibit not only discrete digital dynamics but also continuous physical dynamics. Imagine a car today: it contains not only dozens of chips (like ECUs) but also continuous dynamics (wheels, suspensions, internal combustion, etc.).\nIt is in this CPS context that the idea of falsification is found its use [17].\nThe falsification problem \u2022 Given: a model M (a function from an input signal to an output signal), and a specifi-\ncation \u03d5 (a temporal formula) \u2022 Answer: a critical path, that is, an input signal \u03c3in such that the corresponding output\nM (\u03c3in) does not satisfy \u03d5\nTwo benefits of falsification are particularly appealing. For one, a system model M can be totally a black box: once M as a function \u03c3in 7\u2192M (\u03c3in) is given as an oracle, we can check if an educated guess \u03c3in is a solution or not\u2014without knowing M \u2019s internal working. This is an advantage given that many CPSs do have black-box components: they can come from external suppliers, or they can be physical dynamics too complex to mathematically model (typically one uses look-up tables to describe such).\n\u2217Supported by Grants-in-Aid for JSPS Fellows No. 15J09877. \u2020Supported by JST ERATO HASUO Metamathematics for Systems Design Project (No. JPMJER1603), and JSPS Grant-\nin-Aid No. 15KT0012.\nAlgorithm 1 Falsification by optimization, with a cost function f\u03d5 Input: \u03c30 . The initial guess\n1: v0 := f\u03d5 (M (\u03c30)); . vi = f\u03d5 (M (\u03c3i)) is the score of the input vi 2: for i = 1 . . .N do . N is the greatest number of iteration\n3: \u03c3i := argmin\u03c3 ( f\u03d5 (M (\u03c3)) \u2223\u2223\u2223\u2223 under the previous observations(\u03c30,v0),(\u03c31,v1), . . . ,(\u03c3i\u22121,vi\u22121) ) ; 4: vi := f\u03d5 (M (\u03c3i)); 5: if vi < 0 then return \u03c3i; 6: end if . Falsification succeeded, because we assume f\u03d5 (\u03c4)< 0 implies \u03c4 6|= \u03d5 7: i := i+1; 8: end for\nAnother appealing feature of falsification is its affinity with machine learning (ML) and optimization techniques. In automatic verification techniques the greatest challenge is state-space explosion: the size of the input space for \u03c3in grows exponentially with respect to its dimension, often to the extent that exhaustive search in it is no longer possible. Recent surges in ML and optimization algorithms can offer potent countermeasures against this curse of dimensionality: specifically, after observing output M (\u03c31), . . . ,M (\u03c3n) for input \u03c31, . . . ,\u03c3n, those algorithms can \u201clearn\u201d from these previous attempts and suggest an input signal \u03c3n+1 with which M (\u03c3n+1) 6|= \u03d5 is likely.\nOne can say that falsification is after all adaptive testing: most falsification solvers rely on stochastic guess; hence their failure do not prove \u201cM (\u03c3in) |= \u03d5 for every \u03c3in.\u201d However in many real-world scenarios falsification is as good as one gets, because of systems\u2019 complexity and their black-box components within. Existing stochastic optimization-based solvers (such as S-TaLiRo [8] and BREACH [13]) have shown striking performance, too, scaling up to various Simulink diagrams from automotive applications. Moreover, falsification has special appeal to real-world engineers: while it takes certain familiarity to come to appreciate correctness proofs, counterexamples discovered by falsification easily convince engineers that there are issues to be resolved. Search of Cost Functions A technical cornerstone that set off the study of falsification is robust semantics of temporal formulas [14, 15]. With CPS application in mind we assume that input and output of our system model M are given by (time-variant) signals. For them it is standard to specify properties using some temporal logic, such as metric interval temporal logic (MITL) [7] and signal temporal logic (STL) [20]. In robust semantics [14, 15] a signal \u03c3 and a formula \u03d5 are assigned a continuous truth value J\u03c3 , \u03d5K \u2208 R that designates how robustly the formula is satisfied. This departure from the conventional Boolean semantics (where J\u03c3 , \u03d5K \u2208 {tt, ff}) allows one to adopt a hill climbing-style optimization algorithm to look for a falsifying input signal.\nAlgorithm 1 is a high-level description of falsification by optimization. Here a cost function f\u03d5 carries a signal (output of the system M ) to a real; we assume that its value is linked with satisfaction of \u03d5 , that is specifically, f\u03d5(\u03c4)< 0 implies \u03c4 6|= \u03d5 . We assume that the value of f\u03d5 for a given input can be effectively computed; we assume the same for the function M . Still in Line 3 the true solution may not be available since the global structure of M is unknown\u2014this reflects our black-box view on M . Therefore in Line 3 we make a guess based on the previous trials.\nThe robust semantics of temporal formulas in [14, 15] is a prototype of such a cost function (Algorithm 1). Subsequently in the study of falsification, search of better cost functions has been an important topic. For example, sometimes time robustness [14]\u2014as opposed to space robustness in the original work [15]\u2014yields smoother hills to climb down, aiding optimization. Combination of space and time robustness is pursued in [5], where they enrich logics with averaged modalities to systematically enhance expressivity. Additional bias is put on cost functions in [12] so that search for falsifying input\ncovers a greater number of discrete modes of a system M . After all, the question here is how to enrich cost functions, extracting additional information from a system M and/or a specification \u03d5 .\nContribution: Causality Aid in Falsification In this paper we build on the observations in [4] and propose to aid falsification using causal information. We lay out the idea using a simple example.\nExample 1 (incremental counter) Consider the pseudocode shown on the right. We think of: i0, i1, . . . iN \u2208 [\u22121,1] as the values of a time-variant input signal i at time t = 0,1, . . . ,N, respectively; Lines 1\u201310 as a system M that takes such input and returns the value of cnt as output; and the assertion cnt\u2264N as a specification \u03d5 , that is 2[N,N](cnt\u2264N) in temporal logic. It is clear that, to falsify \u03d5 , all the input values i0, i1, . . . , iN must lie in (\u22120.2,0.2); otherwise the counter is reset and never reaches cnt= N +1.\nInput: i0, i1, . . . , iN \u2208 [\u22121,1] Output: cnt\n1: t := 0;cnt := 0 2: while t \u2264 N do 3: flag := (|it |< 0.2); 4: if flag then 5: cnt := cnt+1 6: else 7: cnt := 0; 8: end if 9: t := t +1;\n10: end while 11: assert(cnt\u2264 N)\nNow consider solving the falsification problem here. It turns out that existing falsification solvers have hard time in doing so: besides the apparent hardness of the problem (following the uniform distributions the success probability would be 0.2N), there is the following \u201ccausal\u201d reason for the difficulty.\nAssume iN 6\u2208 (\u22120.2,0.2), meaning that cnt is reset to 0 at the last moment. In this case the earlier input values i0, i1, . . . , iN\u22121 have no effect in the final output cnt, nor in the robust semantics of the specification 2[N,N](cnt \u2264 N). Therefore there is no incentive for stochastic optimization solvers to choose values i0, . . . , iN\u22121 from (\u22120.2,0.2). More generally, desired bias is imposed on earlier input values i0, . . . , ik only after later input values ik+1, . . . , iN have been suitably fixed. Given the system (the above program) as a black box and the specification 2[N,N](cnt \u2264 N) alone, there is no way for optimization solvers to know such causal dependency.\nOur enhancement of falsification algorithms consists of leveraging causal information expressed as Bayesian networks. See Fig. 1, where we fix N = 5 for presentation. The Bayesian network expresses causal dependence of the original specification \u03d5 = \u03d55 on other specifications \u03d50, . . . ,\u03d54. The newly introduced specifications \u03d5i = [i,i](cnt \u2264 i), for each i = 0, . . . ,4, express that the counter cnt has already been reset by time i. Therefore in order to falsify \u03d55, i.e. to keep incrementing cnt, these additional specifications must be falsified, too. The last observation is expressed in the Bayesian network in Fig. 1, specifically in the conditional probabilities Pr ( J\u03d5i+1K = ff | J\u03d5iK = tt ) = 0.\nNow our falsification algorithm looks not only at \u03d55 but\nalso at the other predicates \u03d50, . . . ,\u03d54. This way we successfully impose bias on earlier input values i0, . . . , i4 to lie in (\u22120.2,0.2)\u2014as demonstrated by our experimental results later.\nFollowing the idea illustrated in the last example, our main contribution in this paper is a causalityaided falsification algorithm that uses Bayesian networks of temporal formulas as input on the specification side. Such a Bayesian network can be derived from an original specification \u03d5 alone; they can also be derived through inspection of a system model M ; or we can use both \u03d5 and M . In order to efficiently leverage the causal information expressed by a Bayesian network, we follow [9,10] and use variations of Gaussian process optimization as our optimization algorithms (Line 3 of Algorithm 1). The feature that they allow to guess both average and variance (see \u00a72.2) turns out to be particularly useful. We imple-\nmented the algorithm; our experimental results, although preliminary, seem to support the effectiveness of our approach.\nGeneral methodologies of deriving such Bayesian networks are outside the paper\u2019s focus, although we do have some preliminary ideas and we exploited them for our current examples. One is the use of probabilistic predicate transformers that are a classic topic in semantics [18, 19, 22] and are shed fresh light on in the context of probabilistic programming languages (see e.g. [23]). This idea follows the earlier observations in [6]; it successfully generates the Bayesian network in Fig. 1 for Example 1. Another idea is parse tree-like decomposition of an original temporal formula \u03d5; we decorate the resulting tree with conditional probabilities that we learn through sampling. These methods will be described in our forthcoming papers.\nRelated Work Besides search of better cost functions, an important direction in the study of falsification is improving optimization algorithms (that are used in Line 3 of Algorithm 1). In the falsification literature many different algorithms have been used and studied: they include simulated annealing, antcolony optimization, the cross-entropy method, the Nelder-Mead algorithm, and so on [8,13,25] . In [11] a discrete algorithm of Tabu search is employed for enhanced coverage.\nYet another important direction is multiple shooting falsification [27, 28] where, unlike single shooting approaches like in this paper, a bunch of trajectories are investigated in a single iteration relying on suitable abstraction of a system model and/or a specification. We believe our idea of causality aid in falsification is orthogonal to the choice between single and multiple shooting; we will study as future work the effect of causality in multiple shooting falsification."}, {"heading": "2 Backgrounds", "text": ""}, {"heading": "2.1 STL and Robust Semantics", "text": "Here we present signal temporal logic (STL) [20] as our formalism for expressing (original, without causal information) specifications. We also present its robust semantics [14] that give the prototype of the cost function f\u03d5 in Algorithm 1. Our cost function will be derived from the robust semantics of the formulas in a Bayesian network. At the same time we emphasize that our methodology of causality-aided falsification does not depend on the specific underlying specification formalism of STL.\nDefinition 2.1 (syntax of STL) The set of STL formulas are recursively defined as follows.\n\u03d5 ::= g(y)> 0 | \u00ac\u03d5 | \u03d51\u2228\u03d52 | \u03d51 UI \u03d5\nHere g(y) is some real-value function over the set of variables y = {y1, . . . ,yn}, and I is a closed nonsingular interval in R\u22650.\nWe also introduce the following standard temporal operators as abbreviations: the eventually operator 3I\u03d5 , (\u221e > 0)UI \u03d5 and the always operator I\u03d5 , \u00ac3I\u00ac\u03d5 .\nDefinition 2.2 (Boolean semantics of STL) Let \u03c3y : R\u22650\u2192Rn be a signal, that is, a function that maps time \u03c4 to the values \u03c3y(\u03c4) of the variables y at time \u03c4 . We define the (Boolean) validity of an STL formula\nover a signal \u03c3y, as follows. Here \u03c3 \u03c4y stands for the time-shifted signal such that \u03c3 \u03c4y (\u03c4 \u2032), \u03c3y(\u03c4 + \u03c4 \u2032).\n\u03c3y g(y)> 0 def.\u21d0\u21d2 the inequality g(\u03c3y(0))> 0 holds \u03c3y \u00ac\u03d5 def.\u21d0\u21d2 \u03c3y 2 \u03d5 \u03c3y \u03d51\u2228\u03d52 def.\u21d0\u21d2 \u03c3y \u03d51 or \u03c3y \u03d52 \u03c3y \u03d51 UI \u03d52 def.\u21d0\u21d2 \u2203\u03c4 \u2208 I. ( \u03c3 \u03c4y \u03d52 and \u2200\u03c4 \u2032 \u2208 [0,\u03c4].\u03c3 \u03c4 \u2032 y \u03d51\n) The following \u201cquantitative refinement\u201d of the semantics of STL initiated the research program of\nfalsification by optimization [14, 15].\nDefinition 2.3 (robust semantics of STL) For a signal \u03c3y and an STL formula \u03d5 , we define the robustness J\u03c3y, \u03d5K \u2208 R\u222a{\u221e,\u2212\u221e} inductively as follows. Here u and t denote infimums and supremums of real numbers, respectively.\nJ\u03c3y, g(y)> 0K , g(\u03c3y(0)) J\u03c3y, \u00ac\u03d5K , \u2212J\u03c3y, \u03d5K J\u03c3y, \u03d51\u2228\u03d52K , J\u03c3y, \u03d51Kt J\u03c3y, \u03d52K J\u03c3y, \u03d51 UI \u03d52K , \u2294 \u03c4\u2208I ( J\u03c3 \u03c4y , \u03d52Ku d \u03c4 \u2032\u2208[0,t]J\u03c3 \u03c4 \u2032 y , \u03d51K\n) Note that the sign of robustness coincides with the Boolean semantics. That is, J\u03c3y, \u03d5K > 0 implies \u03c3y \u03d5 , and J\u03c3y, \u03d5K < 0 implies \u03c3y 2 \u03d5 . Conversely, \u03c3y \u03d5 implies J\u03c3y, \u03d5K \u2265 0, and \u03c3y 2 \u03d5 implies J\u03c3y, \u03d5K\u2264 0."}, {"heading": "2.2 Gaussian Process Optimization", "text": "In this paper we follow the workflow in Algorithm 1, deriving the cost function f\u03d5 in it from a Bayesian network. For the optimization step (Line 3 of Algorithm 1) we use Gaussian process optimization\u2014 we follow [4, 9, 10] about this choice. It has a feature that it suggests the global shape of an unknown function; this feature turns out to be convenient for our purpose of integrating causal information in falsification. We present a brief review of the topic; see e.g. [24] for details."}, {"heading": "2.2.1 Gaussian Process Regression", "text": "Let f be an unknown function, from a certain input domain to the set of real numbers, about which we wish to infer certain properties. (For Algorithm 1 we would take f = f\u03d5(M ( ))). In Gaussian process regression the shape of f is estimated assuming that f is a probabilistic process called a Gaussian process.\nWe start with some formal definitions. For more detail, see e.g. [24].\nNotation 2.4 We let N (\u00b5,k) stand for the probability density function of the multivariate Gaussian distribution whose mean vector is \u00b5 and covariance matrix is k.\nDefinition 2.5 (Gaussian process) A Gaussian process is a family of probabilistic variables (zx)x\u2208X such that each of its finite subset (zx1 , . . . ,zxt ) has a joint Gaussian distribution. A Gaussian process is determined by a pair (\u00b5,k) of its mean function \u00b5 : X\u2192 R and its covariance function k : X\u00d7X\u2192 R; this Gaussian process is denoted by GP(\u00b5,k). For this we have\n(zx1 , . . . ,zxt ) > \u223cN (\u00b5,k) where \u00b5 i = \u00b5(xi) and ki j = k(xi,x j)\nfor each finite subset {x1, . . . ,xt} of X. We write GP(\u00b5,k)(x1, . . . ,xt) for the above multivariate Gaussian distribution N (\u00b5,k).\nIn Fig. 2 is how an unknown function f can be guessed by Gaussian processes. The blue pipe designates the estimated values of the unknown function f : the farther input x is from the observed points, the thicker the pipe is (that means bigger uncertainty).\nIn the regression of f using Gaussian processes, a choice of a covariance function k : X\u00d7X\u2192 R determines smoothness of f . A common template for covariance functions is the squared-exponential kernel function kl(x,x\u2032) , exp(\u2212l \u00b7 \u2016x\u2212 x\u2032\u20162/2), where l is so-called the length scale parameter. In practice, we pick a good length scale parameter by cross validation. As we see in Fig. 2, the choice of a covariance function yields the following tendencies in Gaussian process regression:\n\u2022 The bigger the distance \u2016x\u2212x\u2032\u2016 is, the smaller the covariance is, thus the harder it gets to estimate the value f\u03d5(x) from the observation of the value f\u03d5(x\u2032).\n\u2022 Covariance is smaller too when the length scale parameter l is bigger.\nOne advantage of Gaussian process regression is that, given a set of observations, the posterior process is described analytically. Let random variables (zx)x\u2208X obey a prior Gaussian process GP(\u00b5,k); and D = { (x1,z1), . . . ,(xt ,zt) } be a set of observations. Then the posterior distribution, denoted by GP(\u00b5,k;D), is given by the Gaussian process GP(\u00b5 \u2032,k\u2032), where\n\u00b5 \u2032(x) = \u00b5(x)+kD(x)kDD\u22121 ( [z1 . . .zt ]>\u2212 [\u00b5(x1) . . .\u00b5(xt)]> ) ,\nk\u2032(x,x\u2032) = k\u2032(x,x\u2032)\u2212kD(x)kDD\u22121kD(x\u2032)>.\nHere kD(x) = [k(x1,x) . . .k(xt ,x)], and kDD is a t\u00d7t matrix whose i, j-component is k(xi,x j). In practice, given observed data D = { (x1, f (x1)), . . . ,(xt , f (xt)) } and a covariance kernel function k, we estimate the function f as GP(0,k;D) where 0 denotes the function constantly zero."}, {"heading": "2.2.2 Gaussian Process Optimization and Acquisition Function", "text": "Gaussian process regression allows us to predict, based on observations in D, the value f (x) for each input x as a normal distribution GP(\u00b5,k)(x). To complete an optimization scenario, we wish to pick a candidate x \u2208 X for which f (x) is small.\nIt is well-known that, for such choice, a balance is important between exploration (i.e. bias toward a bigger variance k(x,x)) and exploitation (bias toward a smaller expected value \u00b5(x)). A criterion for this balance is called an acquisition function\u2014we pick x at which the acquisition function is minimum. Assuming that an acquisition function \u03c8(x;GP(\u00b5,k)) has been fixed, the whole procedure for Gaussian process optimization can be described as in Algorithm 2. Note that, in Line 3 of Algorithm 2, we usually employ another optimization solving method (such as simulated annealing).\nIn falsification, our goal would be to find x such that f (x) < 0. As a natural choice of acquisition functions, we focus on the following probability in this paper.\nDefinition 2.6 (Probability of Satisfaction)\n\u03c8(x;GP(\u00b5,k)) , PrGP(\u00b5,k)( f (x)> 0) (1)\nHere PrGP(\u00b5,k)( f (x)> c) is an abbreviation of Pr( f (x)> c | f (x)\u223cGP(\u00b5,k)(x)).\nWe write GP-PSat for Algorithm 2 under \u03c8 as an acquisition function; Fig. 3 illustrates how it works.\nThe acquisition functions we will use are extension of this \u03c8 . We note, however, that this acquisition function is commonly known as \u201cpure and impractical\u201d in the field of Gaussian process optimization. More sophisticated acquisition functions that are known include probability improvement, expected improvement [21], upper confidence bound [26] and so on. At the time of writing it is not clear how these acquisition functions can be used as part of our framework in \u00a74.\nAlgorithm 2 Gaussian process optimization GPOptimization(D,k,\u03c8) Input: a covariance function k : X\u00d7X\u2192R; an acquisition function \u03c8; and an initial data set D= {(x\u20321, f (x\u20321)), . . . ,(x\u2032s, f (x\u2032s))} Output: input x \u2208 X for which f (x) is small\n1: for t = 1,2, . . . do 2: GP(\u00b5 \u2032,k\u2032) = GP(0,k;D); . Estimate the unknown function f 3: xt = argminx\u2208X \u03c8(x;GP(\u00b5 \u2032,k\u2032)); . Choose new sample input 4: D = D\u222a{(xt , f (xt))}; . Observe the corresponding output 5: end for 6: return xt"}, {"heading": "3 Causality in Falsification: Further Examples", "text": "In addition to Example 1, we shall exhibit two more examples of falsification problems; for each, we introduce a Bayesian network that encodes suitable causal information, too. The latter will be exploited in our causality-aided algorithm in \u00a74."}, {"heading": "3.1 Example Model 2: Coincidental Sine Waves", "text": "Let us consider the model in Fig. 4. In this simple model there are four sine waves x1(t), . . . ,x4(t) of different frequency, and we pick their initial phases i1, . . . , i4 as input of the system.\nAs a specification, we pick the following formula\u2014it is falsified when the peaks of four sine waves correspond.\n\u03d5 \u2261 [0,10]( \u2228\ni=1,...,4\nxi < 0.99) (2)\nWe see that falsifying \u03d5 with pure random sampling is difficult because \u03d5 is false only in rare cases. What is worth, the (conventional) robustness of \u03d5 does not always guide us to the counterexamples. Example 2 Let us consider the subformula \u2228\ni=1,...,4 xi < 0.99. When we compare the values (x1,x2,x3,x4)= (1,1,1,0) and (0,0,0,1), we could say the former is \u201ccloser\u201d to falsifying the subformula\u2014the three out of four sine waves simultaneously at a peak. However, these robustness values are the same 0.99 in both cases.\nIn this case, we sometimes divide the difficulty into small pieces\u2014first get x1 and x2 simultaneously at a peak; then get x3 and x4 at a peak; finally, try to make them synchronize. Let us introduce formulas \u03d512 and \u03d534 such that falsifying them means matching the peak of x1,x2, and x3,x4 respectively. Decomposing \u03d5 into \u03d512 and \u03d534 might help us in falsification for the following reasons.\n\u2022 The small formulas \u03d512 and \u03d534 are much easier to falsify compared to \u03d5 .\n\u2022 Moreover, the robustness mapping f\u03d512(M ( )) and f\u03d534(M ( )) have much simpler dynamics than the one of the original specification \u03d5 , so the Gaussian process regression for the small formulas tend to work better than the one for \u03d5 .\nThe Bayesian network B in Fig. 5 is devised to express this intuition. For example, the formula \u03d5 is true with probability 1 when either \u03d512 or \u03d534, otherwise \u03d5 becomes false with small probability 0.1. As shown in Fig. 6, the conditional joint distribution PrB(\u2212 | J\u03d5K = ff) tells us the fact that \u03d5 is false only if both \u03d512 and \u03d534 are false.\nInput: i1, . . . i4 \u2208 [0,1] Output: x1(t), . . . ,x4(t)\nfor each t \u2208 R\u22650 x1(t) = sin(1.1t + i1); x2(t) = sin(1.2t + i2); x3(t) = sin(1.3t + i3); x4(t) = sin(1.4t + i4);\nFigure 4: System model for \u00a73.1\n\u03d5: (p12\u2228 p34)\n\u03d512: p12\n\u03d534: p34\ntt ff 0.9 0.1\ntt ff 0.9 0.1\n\u03d512 \u03d534 tt ff tt tt 1 0 tt ff 1 0 ff tt 1 0 ff ff 0.9 0.1\nwhere( p12 \u2261 x1 < 0.99\u2228 x2 < 0.99 p34 \u2261 x3 < 0.99\u2228 x4 < 0.99 )\nFigure 5: Bayesian network for \u00a73.1\n\u03d5 \u223c= \u03d5v\u2227\u03d5\u03c9\n\u03d5v: v < 120\n\u03d5\u03c9 : \u03c9 < 4780\ntt ff 0.99 0.01\ntt ff 0.9 0.1\n\u03d5v \u03d5\u03c9 tt ff tt tt 1 0 tt ff 0 1 ff tt 0 1 ff ff 0 1\nFigure 7: Bayesian network for \u00a73.2\nPrB(\u2212) \u03d5v \u03d5w \u03d5 tt tt tt 0.891 tt tt ff 0 tt ff tt 0 tt ff ff 0.099 ff tt tt 0 ff tt ff 0.009 ff ff tt 0 ff ff ff 0.001\nPrB(\u2212 | J\u03d5K = ff) \u03d5v \u03d5w \u03d5 tt tt tt 0 tt tt ff 0 tt ff tt 0 tt ff ff 0.908 ff tt tt 0 ff tt ff 0.083 ff ff tt 0 ff ff ff 0.009\nPrB(\u2212) \u03d5v = tt 0.99 \u03d5v = ff 0.01 \u03d5w = tt 0.9 \u03d5w = ff 0.1 \u03d5 = tt 0.891 \u03d5 = ff 0.109\nPrB(\u2212 | J\u03d5K = ff) \u03d5v = tt 0.908 \u03d5v = ff 0.092 \u03d5w = tt 0.083 \u03d5w = ff 0.917 \u03d5 = tt 0 \u03d5 = ff 1\nFigure 8: Unconditional/conditional joint distributions in the Bayesian network of Fig. 7"}, {"heading": "3.2 Example Model 3: Automatic Transmission", "text": "The last example is the automatic transmission model from the benchmark of temporal logic verification [16]. This model is still miniature, but an elaborate mimicry of the systems in the real world hence suitable for our purpose.\nAs a specification \u03d5 to falsify, we use the following formula. It is taken from [16] (it is \u03d5AT2 there).\n(v < 120\u2227\u03c9 < 4780)\nHere the variable v and \u03c9 stand for the speed of the vehicle and the angular velocity of the engine rotation respectively.\nWe know that we can falsify \u03d5 either by violating the speed limit (v < 120) or the engine rotation limit (\u03c9 < 4780). In this model, \u03c9 takes the values in the range around [0,4800] while v does around [0,120]. Note that their scales are very different: hence in the most of the cases, the robustness of the \u03c9-component is likely to be shadowed by the one of the v-component. As a consequence, we expect that conventional falsification solver only try to falsify by the violation of the speed limit v < 120.\nThe Bayesian network annotation is also effective in such a situation. That is, we can add the information about \u201cwhich is more likely to happen, the violation of the speed and the rotation limit.\u201d (In actual deployment such insights will be provided by engineers\u2019 domain knowledge.) Let assume that the probabilities of the violation of the speed and the rotation limit are 0.01 and 0.1 respectively. This information is expressed in Fig. 7, where the conditional probabilities for \u03d5 simply encode logical relationship (note that \u03d5 is semantically equivalent to \u03d5v\u2227\u03d5\u03c9 ) however, the probabilities at leaves reflect the above insight.\nRemark 3.1 In \u00a73.1 and \u00a73.2, as an indicator of robustness, we employed the (space) robust semantics of STL in [14] and shown that it is not sensitive enough for some falsification scenarios. In contrast to [14], the metric-based robustness of MITL in [15] has a degree of freedom to capture the lacked notions. For example in \u00a73.2, we could solve the falsification problem more efficiently if we could re-scale v and \u03c9 appropriately, and this re-scaling is nothing but the defining the metric space in [15]. However, defining such a metric space itself is challenging and needs expert\u2019s domain knowledge\u2014similarly as our framework needs suitable causal information. We expect that our causality-aided framework is a viable option compare to finding a suitable metric."}, {"heading": "4 Falsification with Causality Annotation", "text": "Given the backgrounds in \u00a72 and the examples in \u00a73, we are now ready to ask the question: given a falsification problem and a Bayesian network annotation about causality, what cost function should we optimize? In this section, we will give some answers to the question by lifting up the conventional notion of acquisition functions which we reviewed in \u00a72.2 to the multi-formula setting.\nConsider one of the Bayesian networks that we have seen in the paper. Let B denote the Bayesian network; and let \u03a6 = {\u03d51, . . . ,\u03d5N} be the set of formulas that appear there. Now assume that we are running the Gaussian regression not only for f\u03d5 = JM ( ),\u03d5K but also f\u03d5i = JM ( ),\u03d5iK for all the formulas \u03d5i in the Bayesian network.\nThe regression result for f\u03d5i gives us the probabilistic \u201cforecast\u201d of the truth values assignment of the formulas \u0398 \u2208 2\u03a6 as follows.\nNotation 4.1 Let GP(\u00b5i,ki)\u223c f\u03d5i be our estimate for f\u03d5i ; we can use this data to estimate the probability of obtaining \u0398 as the truth assignment, under an input value x. Precisely: let \u0398 be the assignment (\u03d51 = \u03b81, . . . ,\u03d5N = \u03b8N) where \u03b8i \u2208 {tt, ff}; then\nPrGP(x)(\u0398) , PrGP(\u00b51,k1) ( f\u03d51(x) R1 0 ) \u00b7 \u00b7 \u00b7PrGP(\u00b5N ,kN) ( f\u03d5N (x) RN 0 ) , (3)\nwhere Ri is > if \u03b81 = tt, and < otherwise."}, {"heading": "4.1 KL Divergence based acquisition function", "text": "Recall the scenario in \u00a73.1\u2014from the conditional joint distribution PrB(\u2212 | J\u03d5K = ff), we see that the both small formulas \u03d512 and \u03d534 also should be false to synchronize all the peaks of the sine waves.\nInspired from the above example, we propose the following criteria to choose the next candidate x as falsifying input.\nDefinition 4.2 (An acquitision function \u03c8B(x))\nx = argmin x\n\u03c8B(x) where \u03c8B(x) = DKL ( PrB(\u0398 | J\u03d5K = ff) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223PrGP(x)(\u0398))\nHere DKL is the Kullback Leibler divergence\u2014a measure of the difference between two probabilistic distributions. Intuitively, with this criteria, we pick the next input x with which the probabilistic forecast PrGP(x)(\u0398) by regression becomes \u201ccloser to the conditional joint distribution PrB(\u0398 | J\u03d5K = ff)\u201d.\nExample 3 Let us consider the sine waves model in \u00a73.1. From simple calculation, we see that the acquisition function \u03c8B is as follows.\n\u03c8B(x) =\u2212 logPrGP( f\u03d5(x)< 0)\u2212 logPrGP( f\u03d512(x)< 0)\u2212 logPrGP( f\u03d534(x)< 0)\nHence minimizing \u03c8B(x) means trying to falsify all the formulas \u03d5 , \u03d512, and \u03d534.\nRemark 4.3 In this paper, we assume that PrB(J\u03d5K = ff) is not 0 nor 1 on the given Bayesian network B. In the former case, \u03c8B(x) is undefined because PrB(J\u03d5K = ff) is 0, and the latter case, \u03c8B(x) is constantly 0 because PrB(\u0398 | J\u03d5K = ff) = PrB(\u0398). We believe this is reasonable if we believe the given annotation B is correct\u2014in case PrB(J\u03d5K = ff) is 0 (or 1) falsification never succeeds (or always succeeds, respectively).\nThe resulting extension of the GP-PSat algorithm (\u00a72.2) with Bayesian networks is presented in Algorithm 3."}, {"heading": "4.2 Another acquisition function based on the difference of KL divergence", "text": "Aside from the acquisition function \u03c8B in Def. 4.2, we propose another criteria.\nDefinition 4.4 (Another acquitision function \u03c8 \u2032B(x))\nx = argmin x\n\u03c8 \u2032B(x) where\n\u03c8 \u2032B(x) = DKL ( PrB(\u0398 | J\u03d5K = ff) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223PrGP(x)(\u0398))\u2212DKL(PrB(\u0398) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223PrGP(x)(\u0398)).\nAlgorithm 3 Extension of the GP-PSat algorithm with Bayesian network annotation, for falsification Input: an input space X; a system M ; a specification \u03d5 to falsity; a Bayesian network B whose nodes are labeled with formu-\nlas \u03a6 = {\u03d51, . . . ,\u03d5N}; a covariance function k : X\u00d7X\u2192 R; and an initial data set Di = {(x\u20321, f\u03d5i(x\u20321)), . . . ,(x\u2032s, f\u03d5i(x\u2032s))} for each i = 1, . . . ,N 1: for t = 1 . . .T do 2: GP(\u00b5i,ki) = GP(0,k;Di) for each i = 1, . . . ,N; 3: . Estimate the cost functions f\u03d51 , . . . , f\u03d5N by Gaussian process regression 4: xt = argminx\u2208X \u03c8B(x); . Choose a new sample input by the acquisition function 5: Di = Di\u222a{(xt ,JM (xt), \u03d5iK)} for each i = 1, . . . ,N; . Observe the robustness 6: if JM (xt), \u03d5K < 0 then return xt ; . The specification \u03d5 is falsified 7: end if 8: end for\nOne of the advantages of this acquisition function \u03c8 \u2032B(x) is that we can extract it to a simpler form as follows.\n\u03c8 \u2032B(x) = \u2211 \u03d5i\u2208\u03a6\n( ( PrB(J\u03d5iK = tt)\u2212PrB(J\u03d5iK = tt | J\u03d5K = ff) ) log PrGP( f\u03d5i(x)> 0)\n+ ( PrB(J\u03d5iK = ff)\u2212PrB(J\u03d5iK = ff | J\u03d5K = ff) ) log PrGP( f\u03d5i(x)< 0)\n) .\nExample 4 Consider the incremental counter in Example 1. From the Bayesian network in Fig. 1 we extract the following acquisition function \u03c8 \u2032B.\n\u03c8 \u2032B(x) = \u2211 t\u2208[0,5]\n(1\u22120.2t+1) ( logPrGP( f\u03d5t (x)> 0)\u2212 logPrGP( f\u03d5t (x)< 0) )\nFor each formula \u03d5t , when PrGP( f\u03d5t (x) > 0) becomes bigger, so is the value \u03c8 \u2032B(x). Therefore the algorithm tries to make all the formulas to be false. This matches our intuition in \u00a71.\nExample 5 Let us consider the automatic transmission problem in \u00a73.2. The Bayesian network in Fig. 7 tells that most of the failure of \u03d5 is caused by that of \u03d5\u03c9 . The acquisition function \u03c8 \u2032B is as follows.\n\u03c8 \u2032B(x) = logPrGP( f\u03d5(x)> 0)\u2212 logPrGP( f\u03d5(x)< 0) +0.082 ( logPrGP( f\u03d5v(x)> 0)\u2212 logPrGP( f\u03d5v(x)< 0) ) +0.817 ( logPrGP( f\u03d5\u03c9 (x)> 0)\u2212 logPrGP( f\u03d5\u03c9 (x)< 0)\n) Hence as we expected, the satisfaction of \u03d5\u03c9 is a bigger factor than that of \u03d5v.\nWe note that extension of other (more sophisticated) acquisition functions (e.g. GP-UCB) is not straightforward. It is one direction of our future work."}, {"heading": "5 Implementation and Experimental Results", "text": ""}, {"heading": "5.1 Implementation", "text": "Our implementation of Algorithm 3 consists of the following three open source libraries and one new part. They are mostly written in MATLAB.\nComputing the robustness We employ BREACH [2] to compute the simulation output of the system M (x) and the robustness JM (x), \u03d5K as defined in Def. 2.3.\nGaussian process regression Line 2 in Algorithm 3 is done by GPML MATLAB Code version 4.0 [3], a widely used library for computation about Gaussian processes.\nInference on Bayesian networks We employ Bayes Net Toolbox for Matlab [1] for inference on Bayesian networks.\nThe algorithms GP-PSat and GP-PI aided by Bayesian networks This part is new. Optimization of an acquisition function \u03c8 is done by the following two steps: 1) we randomly pick initial samples x1, . . . ,x100 and compute the corresponding values of \u03c8; and 2) from the minimum xi of the one hundred, we further do greedy hill-climbing search."}, {"heading": "5.2 Experiments", "text": "Using our implementation we conducted the following experiments. We do experiments for the three falsification problems; Problem 1 is from Examples 1, Problem 2 from \u00a73.1 and Problem 3 from \u00a73.2. For the automatic transmission example (in \u00a73.2) we used two different parameters; Problem 3-1 is with the specification \u03d5 = (v >\u22121\u2227\u03c9 < 4780); and Problem 3-2 is with \u03d5 = (v < 120\u2227\u03c9 < 4780).\nThe experiments were done on a ThinkPad T530 with Intel Core i7-3520M 2.90GHz CPU with 3.7GB memory. The OS was Ubuntu14.04 LTS (64-bit). A single falsification trial consists of a number of iterations\u2014iterations of for-loop in line 2 in Algorithm 1\u2014before it succeeds or times out (after 100 seconds). For each problem we made ten falsification trials. We made multiple trials because of the stochastic nature of the optimization algorithm. We measured the performance by the following criteria:\n\u2022 Success rate: The number of successful trials (out of ten).\n\u2022 The number of iteration loops: The average number of iteration loops to find the counterexample.\n\u2022 The computational time: The average time to find the counterexample.\nBesides our two extended algorithms with the acquisition functions (in Def. 4.2 and 4.4), we measured the performance of the conventional Gaussian process optimization algorithms GP-PSat and compare them.\nThe experimental results are in Table 1. We see that our causality-aided approach (GP-PSat with \u03c8B and \u03c8 \u2032B) significantly outperformed others for Example 1. This suggests promising potential of the proposed approach in the context of probabilistic programs\u2014all the more because Bayesian networks like in Fig. 1 could be systematically derived using probabilistic predicate transformers.\nOur algorithms performed at least as well as the conventional GP-PSat, for the other examples (Problem 2, 3-1 and 3-2). In Problem 3-1 and 3-2 we observe that our algorithms took fewer iterations before successful falsification. This is potentially an advantage when we wish to deal with bigger Simulink models as system models M (their numerical simulation, i.e. computation of M (\u03c3), is computationally expensive). That said, we believe the idea of causality aid in falsification can be a breaking one, with a potential of accelerating falsification by magnitudes. Its current performance for Problem 3 (that is from cyber-physical systems, a main application domain of falsification) is therefore not satisfactory. We will therefore pursue further improvement of our algorithm (Algorithm 3)."}, {"heading": "6 Future Work", "text": "In this paper, we show that the causality information given in the form of a Bayesian network helps us to solve falsification problems efficiently. However, we still have many challenges in constructing\nsuch helpful Bayesian networks. As we discussed in \u00a71, we expect that the theory of probabilistic programming languages will shed light on the problem, but at any rate we need more practical example scenarios to evaluate the viability of our approach.\nMoreover, we conceive that our proposed algorithm in \u00a74 contains the potential for many improvements. As we note in \u00a72.2.2, the acquisition function in GP-PSat is simple, but not the state-of-the-art in the field of Gaussian process optimization. Extending our approach to other type of the acquisition function is not straightforward, but we think it is within possibility."}], "references": [{"title": "Falsification of conditional safety properties for cyber-physical systems with gaussian process regression", "author": ["Takumi Akazaki"], "venue": "Runtime Verification - 16th International Conference, RV 2016,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Time robustness in MTL and expressivity in hybrid system falsification", "author": ["Takumi Akazaki", "Ichiro Hasuo"], "venue": "Computer Aided Verification - 27th International Conference,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Input synthesis for sampled data systems by program logic", "author": ["Takumi Akazaki", "Ichiro Hasuo", "Kohei Suenaga"], "venue": "Proceedings 4th Workshop on Hybrid Autonomous Systems, HAS 2014,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "The benefits of relaxing punctuality", "author": ["Rajeev Alur", "Tom\u00e1s Feder", "Thomas A. Henzinger"], "venue": "J. ACM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "S-TaLiRo: A tool for temporal logic falsification for hybrid systems", "author": ["Yashwanth Annpureddy", "Che Liu", "Georgios E. Fainekos", "Sriram Sankaranarayanan"], "venue": "editors, TACAS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Active learning based requirement mining for cyberphysical systems. In 55th IEEE Conference on Decision and Control, CDC 2016", "author": ["Gang Chen", "Zachary Sabato", "Zhaodan Kong"], "venue": "Las Vegas, NV, USA, December 12-14,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Active requirement mining of bounded-time temporal properties of cyber-physical systems", "author": ["Gang Chen", "Zachary Sabato", "Zhaodan Kong"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Stochastic local search for falsification of hybrid systems", "author": ["Jyotirmoy V. Deshmukh", "Xiaoqing Jin", "James Kapinski", "Oded Maler"], "venue": "ATVA", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Requirements driven falsification with coverage metrics", "author": ["Adel Dokhanchi", "Aditya Zutshi", "Rahul T. Sriniva", "Sriram Sankaranarayanan", "Georgios E. Fainekos"], "venue": "editors, 2015 International Conference on Embedded Software,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "A toolbox for verification and parameter synthesis of hybrid systems", "author": ["Alexandre Donz\u00e9. Breach"], "venue": "Computer Aided Verification, 22nd International Conference,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Robust satisfaction of temporal logic over real-valued signals", "author": ["Alexandre Donz\u00e9", "Oded Maler"], "venue": "Formal Modeling and Analysis of Timed Systems - 8th International Conference,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Robustness of temporal logic specifications for continuous-time signals", "author": ["Georgios E. Fainekos", "George J. Pappas"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Benchmarks for temporal logic requirements for automotive systems", "author": ["Bardh Hoxha", "Houssam Abbas", "Georgios Fainekos"], "venue": "1st and 2nd International Workshop on Applied veRification for Continuous and Hybrid Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Using S-TaLiRo on industrial size automotive models", "author": ["Bardh Hoxha", "Houssam Abbas", "Georgios E. Fainekos"], "venue": "1st and 2nd International Workshop on Applied veRification for Continuous and Hybrid Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Probabilistic Non-Determinism", "author": ["Claire Jones"], "venue": "PhD thesis, Univ. Edinburgh,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1990}, {"title": "Semantics of probabilistic programs", "author": ["Dexter Kozen"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1981}, {"title": "Monitoring temporal properties of continuous signals", "author": ["Oded Maler", "Dejan Nickovic"], "venue": "Formal Techniques, Modelling and Analysis of Timed and Fault-Tolerant Systems, Joint International Conferences on Formal Modelling and Analysis of Timed Systems, FORMATS 2004 and Formal Techniques in Real-Time and Fault-Tolerant Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Bayesian approach to global optimization: theory and applications. Mathematics and its applications (Kluwer Academic Publishers).", "author": ["Jonas Mockus"], "venue": "Soviet series. Kluwer Academic,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1989}, {"title": "Probabilistic predicate transformers", "author": ["Carroll Morgan", "Annabelle McIver", "Karen Seidel"], "venue": "ACM Trans. Program. Lang. Syst.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1996}, {"title": "Reasoning about recursive probabilistic programs", "author": ["Federico Olmedo", "Benjamin Lucien Kaminski", "Joost-Pieter Katoen", "Christoph Matheja"], "venue": "Proceedings of the 31st Annual ACM/IEEE Symposium on Logic in Computer Science, LICS \u201916,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["Carl Edward Rasmussen", "Christopher K.I. Williams"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "Falsification of temporal properties of hybrid systems using the cross-entropy method", "author": ["Sriram Sankaranarayanan", "Georgios Fainekos"], "venue": "In Proceedings of the 15th ACM International Conference on Hybrid Systems: Computation and Control,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "author": ["Niranjan Srinivas", "Andreas Krause", "Sham Kakade", "Matthias W. Seeger"], "venue": "Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Multiple shooting, cegar-based falsification for hybrid systems", "author": ["Aditya Zutshi", "Jyotirmoy V. Deshmukh", "Sriram Sankaranarayanan", "James Kapinski"], "venue": "In Proceedings of the 14th International Conference on Embedded Software,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "A trajectory splicing approach to concretizing counterexamples for hybrid systems", "author": ["Aditya Zutshi", "Sriram Sankaranarayanan", "Jyotirmoy V. Deshmukh", "James Kapinski"], "venue": "In Proceedings of the 52nd IEEE Conference on Decision and Control,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}], "referenceMentions": [{"referenceID": 13, "context": "It is in this CPS context that the idea of falsification is found its use [17].", "startOffset": 74, "endOffset": 78}, {"referenceID": 4, "context": "Existing stochastic optimization-based solvers (such as S-TaLiRo [8] and BREACH [13]) have shown striking performance, too, scaling up to various Simulink diagrams from automotive applications.", "startOffset": 65, "endOffset": 68}, {"referenceID": 9, "context": "Existing stochastic optimization-based solvers (such as S-TaLiRo [8] and BREACH [13]) have shown striking performance, too, scaling up to various Simulink diagrams from automotive applications.", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "Search of Cost Functions A technical cornerstone that set off the study of falsification is robust semantics of temporal formulas [14, 15].", "startOffset": 130, "endOffset": 138}, {"referenceID": 11, "context": "Search of Cost Functions A technical cornerstone that set off the study of falsification is robust semantics of temporal formulas [14, 15].", "startOffset": 130, "endOffset": 138}, {"referenceID": 3, "context": "For them it is standard to specify properties using some temporal logic, such as metric interval temporal logic (MITL) [7] and signal temporal logic (STL) [20].", "startOffset": 119, "endOffset": 122}, {"referenceID": 16, "context": "For them it is standard to specify properties using some temporal logic, such as metric interval temporal logic (MITL) [7] and signal temporal logic (STL) [20].", "startOffset": 155, "endOffset": 159}, {"referenceID": 10, "context": "In robust semantics [14, 15] a signal \u03c3 and a formula \u03c6 are assigned a continuous truth value J\u03c3 , \u03c6K \u2208 R that designates how robustly the formula is satisfied.", "startOffset": 20, "endOffset": 28}, {"referenceID": 11, "context": "In robust semantics [14, 15] a signal \u03c3 and a formula \u03c6 are assigned a continuous truth value J\u03c3 , \u03c6K \u2208 R that designates how robustly the formula is satisfied.", "startOffset": 20, "endOffset": 28}, {"referenceID": 10, "context": "The robust semantics of temporal formulas in [14, 15] is a prototype of such a cost function (Algorithm 1).", "startOffset": 45, "endOffset": 53}, {"referenceID": 11, "context": "The robust semantics of temporal formulas in [14, 15] is a prototype of such a cost function (Algorithm 1).", "startOffset": 45, "endOffset": 53}, {"referenceID": 10, "context": "For example, sometimes time robustness [14]\u2014as opposed to space robustness in the original work [15]\u2014yields smoother hills to climb down, aiding optimization.", "startOffset": 39, "endOffset": 43}, {"referenceID": 11, "context": "For example, sometimes time robustness [14]\u2014as opposed to space robustness in the original work [15]\u2014yields smoother hills to climb down, aiding optimization.", "startOffset": 96, "endOffset": 100}, {"referenceID": 1, "context": "Combination of space and time robustness is pursued in [5], where they enrich logics with averaged modalities to systematically enhance expressivity.", "startOffset": 55, "endOffset": 58}, {"referenceID": 8, "context": "Additional bias is put on cost functions in [12] so that search for falsifying input", "startOffset": 44, "endOffset": 48}, {"referenceID": 0, "context": "Contribution: Causality Aid in Falsification In this paper we build on the observations in [4] and propose to aid falsification using causal information.", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "\u03c65 = [5,5](cnt\u2264 5) \u03c64 = [4,4](cnt\u2264 4) \u03c60 = [0,0](cnt\u2264 0) tt ff 0.", "startOffset": 5, "endOffset": 10}, {"referenceID": 1, "context": "\u03c65 = [5,5](cnt\u2264 5) \u03c64 = [4,4](cnt\u2264 4) \u03c60 = [0,0](cnt\u2264 0) tt ff 0.", "startOffset": 5, "endOffset": 10}, {"referenceID": 0, "context": "\u03c65 = [5,5](cnt\u2264 5) \u03c64 = [4,4](cnt\u2264 4) \u03c60 = [0,0](cnt\u2264 0) tt ff 0.", "startOffset": 24, "endOffset": 29}, {"referenceID": 0, "context": "\u03c65 = [5,5](cnt\u2264 5) \u03c64 = [4,4](cnt\u2264 4) \u03c60 = [0,0](cnt\u2264 0) tt ff 0.", "startOffset": 24, "endOffset": 29}, {"referenceID": 5, "context": "In order to efficiently leverage the causal information expressed by a Bayesian network, we follow [9,10] and use variations of Gaussian process optimization as our optimization algorithms (Line 3 of Algorithm 1).", "startOffset": 99, "endOffset": 105}, {"referenceID": 6, "context": "In order to efficiently leverage the causal information expressed by a Bayesian network, we follow [9,10] and use variations of Gaussian process optimization as our optimization algorithms (Line 3 of Algorithm 1).", "startOffset": 99, "endOffset": 105}, {"referenceID": 14, "context": "One is the use of probabilistic predicate transformers that are a classic topic in semantics [18, 19, 22] and are shed fresh light on in the context of probabilistic programming languages (see e.", "startOffset": 93, "endOffset": 105}, {"referenceID": 15, "context": "One is the use of probabilistic predicate transformers that are a classic topic in semantics [18, 19, 22] and are shed fresh light on in the context of probabilistic programming languages (see e.", "startOffset": 93, "endOffset": 105}, {"referenceID": 18, "context": "One is the use of probabilistic predicate transformers that are a classic topic in semantics [18, 19, 22] and are shed fresh light on in the context of probabilistic programming languages (see e.", "startOffset": 93, "endOffset": 105}, {"referenceID": 19, "context": "[23]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "This idea follows the earlier observations in [6]; it successfully generates the Bayesian network in Fig.", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "In the falsification literature many different algorithms have been used and studied: they include simulated annealing, antcolony optimization, the cross-entropy method, the Nelder-Mead algorithm, and so on [8,13,25] .", "startOffset": 207, "endOffset": 216}, {"referenceID": 9, "context": "In the falsification literature many different algorithms have been used and studied: they include simulated annealing, antcolony optimization, the cross-entropy method, the Nelder-Mead algorithm, and so on [8,13,25] .", "startOffset": 207, "endOffset": 216}, {"referenceID": 21, "context": "In the falsification literature many different algorithms have been used and studied: they include simulated annealing, antcolony optimization, the cross-entropy method, the Nelder-Mead algorithm, and so on [8,13,25] .", "startOffset": 207, "endOffset": 216}, {"referenceID": 7, "context": "In [11] a discrete algorithm of Tabu search is employed for enhanced coverage.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "Yet another important direction is multiple shooting falsification [27, 28] where, unlike single shooting approaches like in this paper, a bunch of trajectories are investigated in a single iteration relying on suitable abstraction of a system model and/or a specification.", "startOffset": 67, "endOffset": 75}, {"referenceID": 24, "context": "Yet another important direction is multiple shooting falsification [27, 28] where, unlike single shooting approaches like in this paper, a bunch of trajectories are investigated in a single iteration relying on suitable abstraction of a system model and/or a specification.", "startOffset": 67, "endOffset": 75}, {"referenceID": 16, "context": "Here we present signal temporal logic (STL) [20] as our formalism for expressing (original, without causal information) specifications.", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "We also present its robust semantics [14] that give the prototype of the cost function f\u03c6 in Algorithm 1.", "startOffset": 37, "endOffset": 41}, {"referenceID": 10, "context": "The following \u201cquantitative refinement\u201d of the semantics of STL initiated the research program of falsification by optimization [14, 15].", "startOffset": 128, "endOffset": 136}, {"referenceID": 11, "context": "The following \u201cquantitative refinement\u201d of the semantics of STL initiated the research program of falsification by optimization [14, 15].", "startOffset": 128, "endOffset": 136}, {"referenceID": 0, "context": "For the optimization step (Line 3 of Algorithm 1) we use Gaussian process optimization\u2014 we follow [4, 9, 10] about this choice.", "startOffset": 98, "endOffset": 108}, {"referenceID": 5, "context": "For the optimization step (Line 3 of Algorithm 1) we use Gaussian process optimization\u2014 we follow [4, 9, 10] about this choice.", "startOffset": 98, "endOffset": 108}, {"referenceID": 6, "context": "For the optimization step (Line 3 of Algorithm 1) we use Gaussian process optimization\u2014 we follow [4, 9, 10] about this choice.", "startOffset": 98, "endOffset": 108}, {"referenceID": 20, "context": "[24] for details.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "More sophisticated acquisition functions that are known include probability improvement, expected improvement [21], upper confidence bound [26] and so on.", "startOffset": 110, "endOffset": 114}, {"referenceID": 22, "context": "More sophisticated acquisition functions that are known include probability improvement, expected improvement [21], upper confidence bound [26] and so on.", "startOffset": 139, "endOffset": 143}, {"referenceID": 6, "context": "\u03c6 \u2261 [0,10]( \u2228", "startOffset": 4, "endOffset": 10}, {"referenceID": 12, "context": "The last example is the automatic transmission model from the benchmark of temporal logic verification [16].", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "It is taken from [16] (it is \u03c6AT 2 there).", "startOffset": 17, "endOffset": 21}, {"referenceID": 10, "context": "2, as an indicator of robustness, we employed the (space) robust semantics of STL in [14] and shown that it is not sensitive enough for some falsification scenarios.", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "In contrast to [14], the metric-based robustness of MITL in [15] has a degree of freedom to capture the lacked notions.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "In contrast to [14], the metric-based robustness of MITL in [15] has a degree of freedom to capture the lacked notions.", "startOffset": 60, "endOffset": 64}, {"referenceID": 11, "context": "2, we could solve the falsification problem more efficiently if we could re-scale v and \u03c9 appropriately, and this re-scaling is nothing but the defining the metric space in [15].", "startOffset": 173, "endOffset": 177}, {"referenceID": 1, "context": "\u03c8 \u2032 B(x) = \u2211 t\u2208[0,5] (1\u22120.", "startOffset": 15, "endOffset": 20}], "year": 2017, "abstractText": "Falsification is drawing attention in quality assurance of heterogeneous systems whose complexities are beyond most verification techniques\u2019 scalability. In this paper we introduce the idea of causality aid in falsification: by providing a falsification solver\u2014that relies on stochastic optimization of a certain cost function\u2014with suitable causal information expressed by a Bayesian network, search for a falsifying input value can be efficient. Our experiment results show the idea\u2019s viability.", "creator": "LaTeX with hyperref package"}}}