{"id": "1709.05820", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2017", "title": "Toward a full-scale neural machine translation in production: the Booking.com use case", "abstract": "malta While drega some rogo remarkable dorinda progress roads has been laucala made in hennequin neural machine translation (85-yard NMT) research, there tardiness have not rituparno been many enoki reports on bottollier its development and panhandlers evaluation cablevision in practice. This bombed-out paper viagens tries to barred fill krasny this comcast gap gimcrack by starcher presenting some kimber of 439,000 our findings wccs from owais building micon an in - proof-of-concept house backscattered travel durkee domain NMT system browbeating in a large horos scale woden E - dec/24/96 commerce butler setting. nigrescens The three major multi-stemmed topics that monitor we capeman cover 127.75 are op - okpara timization and training (including different 147 optimization strategies spu and corpus g\u00fcne\u015f sizes ), handling 2,090 real - 2.525 world brylcreem content and evaluating mizuna results.", "histories": [["v1", "Mon, 18 Sep 2017 08:46:20 GMT  (55kb)", "http://arxiv.org/abs/1709.05820v1", "11 pages, 4 figures, presented at the Commercial MT Users and Translators Track"], ["v2", "Mon, 25 Sep 2017 10:26:44 GMT  (55kb)", "http://arxiv.org/abs/1709.05820v2", "11 pages, 4 figures, presented at MT Summit XVI, Commercial MT Users and Translators Track"]], "COMMENTS": "11 pages, 4 figures, presented at the Commercial MT Users and Translators Track", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["pavel levin", "nishikant dhanuka", "talaat khalil", "fedor kovalev", "maxim khalilov"], "accepted": false, "id": "1709.05820"}, "pdf": {"name": "1709.05820.pdf", "metadata": {"source": "CRF", "title": "Toward a full-scale neural machine translation in production: the Booking.com use case", "authors": ["Pavel Levin", "Nishikant Dhanuka", "Fedor Kovalev", "Maxim Khalilov"], "emails": ["pavel.levin@booking.com", "nishikant.dhanuka@booking.com", "talaat.khalil@booking.com", "fedor.kovalev@booking.com", "maxim.khalilov@booking.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 9.\n05 82\n0v 1\n[ cs\n.C L\n] 1\nWhile some remarkable progress has been made in neural machine translation (NMT) research, there have not been many reports on its development and evaluation in practice. This paper tries to fill this gap by presenting some of our findings from building an in-house travel domain NMT system in a large scale E-commerce setting. The three major topics that we cover are optimization and training (including different optimization strategies and corpus sizes), handling real-world content and evaluating results."}, {"heading": "1 Introduction", "text": "Booking.com is one of the largest online companies in the world operating in 43 different languages, connecting millions of daily visitors to 1.4 million bookable accommodations while offering both parties multilingual support and information every step of the way. Given the company\u2019s fast growth and a rising need for more high quality translated content, machine translation (MT) is becoming an increasingly attractive option to automate this difficult task.\nOur experiments [9] consistently show the superiority of neural machine translation (NMT) systems over the more traditional statistical ones, even when we benchmark them against the well-established and tested general purpose systems. Therefore our recent focus has been on tailoring and improving our own in-house NMT systems to make them practical and effective for us. This work highlights some of the main learnings on our journey and should be of interest to anyone looking to deploy a custom NMT system.\nIn particular we focus on the following three major topics:\n\u2022 Optimization and training\nAt Booking.com we have collected tens of millions of travel domain specific humantranslated parallel sentences, which in theory allows us to train very flexible models with hundreds of millions of parameters. However learning such system can be computationally expensive which often translates to unacceptably long product development iteration cycles. To address this we first analyze how convergence is affected by different optimization techniques (Section 2.2), including in a multi-GPU environment. Second, we look at how the quality of a trained system improves as a function of the training corpus size (Section 2.3).\n\u2022 Handling real-world content\nReal world text comes with many challenges which have to be addressed. Section 3 presents some practical considerations for dealing with named entities and rare words.\n\u2022 Quality evaluations\nWhen building an MT system with customer-facing output, setting up a good quality evaluation loop can be one of the most important aspects. In this part we show how in addition to the BLEU metric [12], the de facto standard for automatic MT scoring, we employ human evaluation of translation adequacy and fluency. We take a close look at how the two approaches correlate. Further, we share our experience developing our business sensitivity framework, which helps us proofread the final translation identifying particularly pernicious errors."}, {"heading": "2 Optimization and training", "text": ""}, {"heading": "2.1 Model architecture", "text": "The core of our translation pipeline is based on OpenNMT [7], which is a Lua written framework for training encoder-decoder neural architectures. Usually, both the encoder and the decoder recurrent neural networks (RNNs), in our case typically long short-termmemory (LSTM) units [5], each with 4 layers. We always use (global) attention layer with input feeding to help the model learn faster by keeping a \u201cmemory\u201d of past alignment decisions [10]. For European languages we use \u201ccase features\u201d (see Section 3.1) as additional input variables from the \u201ccases\u201d embedding space [14]. The main word embeddings are concatenated with the case embeddings to form the inputs to the encoder. At each layer of the encoder the RNNs are bi-directional [13]. Both the encoder and the decoder use residual connections between layers [4] as well as the dropout rate of 0.3 [16]."}, {"heading": "2.2 Optimization and model fitting", "text": ""}, {"heading": "2.2.1 Single-GPU environment", "text": "To optimize the training of our NMT system in single-GPU environments, we evaluated different algorithms primarily based on their speed of convergence and translation output quality. The dataset used was English-German property descriptions with one million parallel sentences. We conducted experimentswith four well-known optimizers: stochastic gradient descent (SGD) with learning rate decay, Adam [6], Adagrad [3] and Adadelta [18]. Our SGD decay strategy is based on a combination of the perplexity score and epoch number, meaning we decay current learning rate by a multiplicative factor of 0.7 if current epoch\u2019s validation perplexity does not decrease, and after each epoch after the 9th epoch. Our initial learning parameters for SGD, Adam, Adagrad and Adadelta are 1.0, 0.0002, 0.1, and 1.0 respectively. We ran the model for 20 epochs and used both perplexity per epoch and BLEU score after every five epochs on the validation set of 10,000 sentences to measure the performance. Our results are summarized in Table 1 and Figure 1.\nAs can be seen in Table 1 and Figure 1, we observed that initially Adam converged faster as expected because it applies momentum on a per parameter basis, but SGD took over as soon as decay started and outperformed Adam thereafter. The perplexity reached by SGD in the 9th epoch was already achieved by Adam in the 6th. But from the 10th epoch onward, as soon as SGD learning rate starts decaying indefinitely, Adam\u2019s perplexity is consistently worse than that of SGD. However, there was no decrease in perplexity from 15th till 20th epoch, so SGD already converged by epoch 15. We also observed that Adagrad performed very poorly on our model. Adadelta was much better than Adagrad but still slightly behind Adam and SGD.\nWe further validated our results using BLEU scores every 5 epochs. The results were mostly consistent with what we observe by looking at perplexity. In terms of time taken per epoch, SGD was the fastest. Adam was about 10% slower in comparison."}, {"heading": "2.2.2 Multi-GPU environment", "text": "Next we experimentedwith the use of multiple GPUs by using data parallelism technique which trains batches in parallel on different GPUs. On a single GPU our model takes 6h11m per epoch on average, and we usually see it converging around 15th epoch, which means training a model on only 1 million sentences takes about 4 days. 15 epochs on a corpus of size 10M could easily translate to around 40 days1. In an attempt to speed up our development cycle, we ran some experiments with synchronous and asynchronous SGD (with decay) on a cluster of 2, 4, 6 and 8 GPUs. The main difference between these two approaches is that in synchronous mode all gradients are accumulated and parameter updates are synchronized, while in asynchronous each GPU calculates its own gradient and communicates with the \u201cmaster copy\u201d of parameters independently and asynchronously. This mater copy of parameters is stored on a single dedicated GPU which is not used for training. To achieve a faster convergence through better parameter initialization, only one GPU works for the first 6,000 iterations in async SGD.\nAs can be seen in Figure 2, average time per epoch came down as we addedmore hardware:\n1Reported estimates do not account for any time related to model checkpointing.\nfrom 6h11m to 1h23m for sync and to only 56 minutes for async. Note that with 2 GPUs, async takes almost the same time as non-parallel SGD (around 6 hours) while sync is much faster at 3h31m. The reason for that is that 2-GPU async is almost equivalent to a single GPU model as async blocks one GPU completely to store the master copy of parameters and is not used for training. Because async mode skips the overhead of parameter synchronization, it was expected that it would be faster than sync, so we also looked at the quality as measured by perplexity. During the first epoch sync perplexity is much worse than that of async due to only 1 GPU working in async for first 6,000 iterations resulting in better parameter initialization (this cannot be seen in Figure 1 which has been cropped for better visibility; sync has first epoch perplexity of 9.61, compared to 5.61 for async and 3.68 for single-GPU SGD). However, for all remaining epochs their scores are very similar. Single-GPU SGD, on the other hand, performed noticeably better in the first half of the training, but gets quite similar to multi-GPU models eventually (although still marginally better). Overall we are very happy with async\u2019s performance as it is able to reduce the training time by about 85%."}, {"heading": "2.3 The importance of corpus size", "text": "In order see how much benefit we get from an increased corpus size, we compared models trained on 1M, 2.5M, 5M, 7.5M and 10M sentences. For fair comparison we report the learning curves as a function of number of iterations (training time) and not the epoch number. Figure 3 shows our findings.\nEssentially there were no major surprises. It appears that given enough iteration the model with more distinct sentences will have a higher BLEU score. Notice how in the beginning smaller datasets are actually winning, but given enough training time the model is starting to take full advantage of more data. The largest corpus size of 10M does not have the best performance at the end of 90M iterations, however as we shall see in Section 4.3 this is in fact not true and according to human evaluations 10M gives the best results which are simply not captured by the BLEU metric."}, {"heading": "3 Handling real-world content", "text": ""}, {"heading": "3.1 Tokenization and case features", "text": "In our final models we use byte-pair encoding (BPE) tokenization procedure [15]. BPE is a compression technique which was recently adapted to find optimal tokens for sequence composition in sequence-to-sequence learning tasks. In theory the technique should find a perfect compromise between using word-level translation (and dealing with out-of-vocabulary entities)\nand character-level translation (and dealing with much longer sequences of tokens). The procedure is very straightforward. We start with a set of tokens which is the list of acceptable characters and iteratively grow it, at each step adding a concatenation of two items already in the list which is the most frequent in our corpus. The number of iterations can be viewed as the algorithm\u2019s only hyperparameter. We can either apply BPE to the source and the target sentences separately, or we can apply them to the combined corpus. Based on our experiment (see Table 2) we decided ended up with the joint version.\nApart from applying BPE tokenization we also use case features preprocessing. This allows us to map the same words and word pieces spelled with different cases to the same embeddings while also passing the casing information separately. For example raw terms book, Book and BOOK would all be mapped to the same token book, but would have different accompanying case feature values. Case features get their own embeddings which get combined with token embeddings during the translation [14]. In theory this greatly increases the encoding and decoding efficiency of the system, which we also observed in practice through much better\nperformance over not using case features."}, {"heading": "3.2 Handling named entities", "text": "Text in the travel domain contains a large amount of entities. There is almost always some destination involved, a property name, distances, times, etc. Although many NMT researchers report results on end-to-end neural networks [1, 2, 17], we often found RNN encoder-decoder architecture insufficient to produce acceptable results, mainly due to mishandled named entities. This section outlines our approach to processing such entities which drastically improves the translation output quality.\nAs an example, mistranslated distances constitute one of the most common error types when NMT is applied naively on raw text, even with very large corpus sizes (over 10M parallel sentences). Interestingly NMT often correctly converts between kilometers and miles for commonly occurring distances (e.g. 5km, 10 miles); however, the number of distance-related mistakes in our validation set is too large to be left untreated. Another common type of error is related to times and dates (12 vs 24 hour clock times, different date formats).\nIn most such cases we used a set of manually created templates to search for entities and replace them with special placeholders. As our team does not understand most of the languages\nthat we build MT systems for, we get some help from our in-house language specialists (translators). The template refinement cycle goes as follows. We come up with a set of reasonable regular expressions to identify named entities of a certain type in both languages and run them on our parallel corpus. Then we take the set of sentences where the numbers of recognized entities differs between the source and the target. We then look at the breakdown of most common entities in either language which did not have corresponding parallel counterparts, and refine our regular expressions accordingly. At translation (prediction) stage, we preprocess the input to replace all named entities with corresponding placeholders, run the translation, then substitute back the named entities parsed according to the target language format. This simple approach dramatically improves the translation output quality for sentences which involve problematic named entities."}, {"heading": "4 Quality evaluation", "text": "Unlike simple classification or regression tasks, sequence learning problems are much more difficult to evaluate. The problem comes from the fact that there can be many possible solutions and it is hard (and often impossible) to compare the model output to all valid \u201ctrue values\u201d. To assess the quality of translations automatically, a useful heuristic is the so-called BLEU score [12] which roughly measures the degree of word overlap between the model translation and a human translation. BLEU score is attractive because it is completely automatic given translated sentences and correspondingmodel predicted sentences. However, multiple problems have been noted in using BLEU score alone. As a purely counting-based metric, BLEU will favor translation which have more common words and n-grams with the reference translation, regardless of the sentence grammar. It would also penalize models which rephrase the sentence in a way which uses different words from the reference sentence, while preserving its meaning.\nIn this section we first describe how we leverage our in-house linguistic expertise to score our models in a relevant way (Section 4.1). Then we analyze how BLEU score correlates with human metrics (Section 4.3)."}, {"heading": "4.1 Human evaluation loop", "text": "Our main human evaluation is based on adequacy/fluency methodology2 which, as the name suggests, is based on two criteria: adequacy and fluency. Adequacy shows to what degree the meaning of the source sentence is preserved, while fluency scores how grammatically wellformed (from the native speaker\u2019s perspective) the translated segment sounds. Each sentence is scored by two independent professional translators from English to German (native German speakers). For the experiments in Section 4.3 we chose 200 randomly selected sentences and translators with at least one year of experience professionally translating Booking.com content.\nAdditionally we use human evaluators to score the quality of entity handling (as described in Section 3.2). For that task each sentence known to contain a specific entity type is given a binary score of whether or not the entity is translated correctly. We found having a separate evaluation specific to entities in addition to adequacy and fluency is important as it helps us to decide on tokenization procedure, entity handling procedures, etc."}, {"heading": "4.2 Business sensitivity analysis", "text": "One important shortcoming of the BLEU score is that it says nothing about the so-called \u201cbusiness sensitive\u201d errors. For example, the cost of mistranslating \u201cParking is available\u201d to mean \u201cThere is free parking\u201d is much greater than a minor grammatical error in the output. Typically it is very difficult to detect such errors because doing so requires some understanding of the sen-\n2https://www.taus.net/academy/best-practices/evaluate-best-practices\nPrecision Recall F1 Score\nEN DE EN DE EN DE\nFree parking 0.97 0.96 0.93 0.94 0.95 0.95\nNon-free parkinga 0.79 0.83 0.89 0.85 0.84 0.84 Not about parking 1.00 0.99 1.00 1.00 1.00 0.99\nAverage 0.97 0.96 0.97 0.96 0.97 0.96\n(a) Performance of English and German components of our BSF framework measured with a hold-out set of 500 examples.\nGerman prediction\nFree parking Non-free parkinga Not about parking\nE n g li sh p re d ic ti o n Free parking 99.4% 0.5% 0.1% Non-free parkinga 5.1% 94.6% 0.3% Not about parking < 0.1% < 0.1% 99.9%\n(b) The result of applying BSF to our English/German corpus, expressed in matches normalized by the total English volumes. For example out of all English sentences which BSF annotated as \u201cFree parking\u201d, 99.4% also get predicted as \u201cFree parking\u201d in German, while 0.5% of those get identified as \u201cNon-free parking\u201da and 0.1% as not about parking at all.\ntence meaning. Even so, given the potentially huge cost of such mistakes, we have developed a basic \u201cbusiness sensitivity framework\u201d (BSF) layer to detect certain specific types of errors.\nThe way it works is rather straightforward. It is a two-stage system, where we first identify the sentences with a particular sensitive aspect (e.g. parking availability, pet policy, etc.) then we apply two classifiers (one to the source sentence, the other to the translation) to identify the predicted values of this aspect (e.g. \u201cfree parking\u201d, \u201cpets not allowed\u201d etc.) Finally, BSF flags the sentence as problematic if the predicted aspect values differ between the source and the translation. For the first layer of finding relevant sentences, we learn word and phrase embeddings by training word2vec [11] on our full (monolingual) corpora. Then we pick a few \u201cseed\u201d words or phrases (e.g. \u201cpet\u201d, \u201ddog\u201d, \u201ccat\u201d for the pet policy aspects) and expand the list by looking at those words\u2019 word2vec cosine distance neighborhoods. After our language specialists proofread the list, it is used to identify the relevant sentences via simple keyword matching. For the classification task we use a bag-of-ngrams linear model approach [8].\nAs an example, Table 5 shows the BSF performance for \u201cparking availability\u201d aspect in\nEnglish\u2192 German translation."}, {"heading": "4.3 BLEU score vs human-based metrics", "text": "While BLEU score is very convenient to use because it can be computed automatically, the main metrics we really trust are human-based (see Section 4.1). Here we look at how the BLEU scores from our English-German corpus size experiment of Section 2.3 are correlated with adequacy/fluency metrics.\nThe results are shown in Figure 4. The training with the corpus size of 10M clearly gives\nthe best performance according to human evaluation, however this is not reflected in the BLEU score. As we can see the correlation between human metrics and BLEU score is rather tenuous. In particular, had we only looked at BLEU, we could have easily made the wrong conclusion about our experiment from Section 2.3."}, {"heading": "5 Conclusion", "text": "We have presented our approach to developing a large scale NMT system, specifically focusing on practical considerations. We presented the performance of different optimization strategies for model training in single- and multi-GPU environments. We found that a combination of Adam and SGD with learning rate decay works the best on a single GPU, and asynchronous SGD parallelization is a great strategy to dramatically speed up the training. We presented the advantages of BPE tokenization for machine translation and argued in favor of preprocessing named entities for better quality translation. Finally, we presented our approach of dealing with critical translation mistakes through our business sensitivity framework and argue that despite being the main metric in research, BLEU score alone can be a poor way of tracing MT system improvement.\nIn the future we are going to continue running optimization related experiments, particularly around better strategies for taking advantage of multiple GPUs. In order to leverage our massive monolingual corpora that are not translated, we are also focusing more on the research topics of model pre-training and similar techniques. Other important research topics to us are domain adaptation and user-generated content."}, {"heading": "Acknowledgements", "text": "We would like to thank our Language Specialists for providing invaluable human feedback and to Darina Kozlova for her important advice on human evaluation and for patiently coordinating all that work."}], "references": [{"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. Merrienboer", "D. Bahdanau", "B. Yoshua"], "venue": "In Proceedings of SSST-8,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Systran\u2019s pure neural machine translation systems. arXiv preprint arXiv:1610.05540", "author": ["J. Crego", "J. Kim", "G. Klein", "A. Rebollo", "K. Yang", "J. Senellart", "E. Akhanov", "P. Brunelle", "A. Coquard", "Y Deng"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation", "author": ["G. Klein", "Y. Kim", "Y. Deng", "J. Senellart", "A.M. Rush"], "venue": "arXiv preprint arXiv:1701.02810", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Vowpal wabbit online learning project", "author": ["J. Langford", "L. Li", "A. Strehl"], "venue": "Technical report, http://hunch. net", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Machine translation at booking.com: Journey and lessons learned", "author": ["P. Levin", "N. Dhanuka", "M. Khalilov"], "venue": "In Proceedings of the 20th International Conference of the European Association for Machine Translation (EAMT),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong", "M.-T", "H. Pham", "C.D. Manning"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Linguistic input features improve neural machine translation. arXiv preprint arXiv:1606.02892", "author": ["R. Sennrich", "B. Haddow"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "arXiv preprint arXiv:1508.07909", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of machine learning research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Neural machine translation advised by statistical machine translation", "author": ["W. Xing", "Z. Lu", "Z. Tu", "H. Li", "D. Xiong", "M. Zhang"], "venue": "In Proceedings of AAAI 2017,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2017}, {"title": "Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701", "author": ["M.D. Zeiler"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}], "referenceMentions": [{"referenceID": 8, "context": "Our experiments [9] consistently show the superiority of neural machine translation (NMT) systems over the more traditional statistical ones, even when we benchmark them against the well-established and tested general purpose systems.", "startOffset": 16, "endOffset": 19}, {"referenceID": 11, "context": "In this part we show how in addition to the BLEU metric [12], the de facto standard for automatic MT scoring, we employ human evaluation of translation adequacy and fluency.", "startOffset": 56, "endOffset": 60}, {"referenceID": 6, "context": "1 Model architecture The core of our translation pipeline is based on OpenNMT [7], which is a Lua written framework for training encoder-decoder neural architectures.", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "Usually, both the encoder and the decoder recurrent neural networks (RNNs), in our case typically long short-termmemory (LSTM) units [5], each with 4 layers.", "startOffset": 133, "endOffset": 136}, {"referenceID": 9, "context": "We always use (global) attention layer with input feeding to help the model learn faster by keeping a \u201cmemory\u201d of past alignment decisions [10].", "startOffset": 139, "endOffset": 143}, {"referenceID": 13, "context": "1) as additional input variables from the \u201ccases\u201d embedding space [14].", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "At each layer of the encoder the RNNs are bi-directional [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "Both the encoder and the decoder use residual connections between layers [4] as well as the dropout rate of 0.", "startOffset": 73, "endOffset": 76}, {"referenceID": 15, "context": "3 [16].", "startOffset": 2, "endOffset": 6}, {"referenceID": 5, "context": "We conducted experimentswith four well-known optimizers: stochastic gradient descent (SGD) with learning rate decay, Adam [6], Adagrad [3] and Adadelta [18].", "startOffset": 122, "endOffset": 125}, {"referenceID": 2, "context": "We conducted experimentswith four well-known optimizers: stochastic gradient descent (SGD) with learning rate decay, Adam [6], Adagrad [3] and Adadelta [18].", "startOffset": 135, "endOffset": 138}, {"referenceID": 17, "context": "We conducted experimentswith four well-known optimizers: stochastic gradient descent (SGD) with learning rate decay, Adam [6], Adagrad [3] and Adadelta [18].", "startOffset": 152, "endOffset": 156}, {"referenceID": 14, "context": "1 Tokenization and case features In our final models we use byte-pair encoding (BPE) tokenization procedure [15].", "startOffset": 108, "endOffset": 112}, {"referenceID": 8, "context": "Table 2: Comparison of the BLEU scores of identically trained models with different BPE configurations, as well as the baseline with a vocabulary of 50,000 most common words (see [9] for more details on the baseline model).", "startOffset": 179, "endOffset": 182}, {"referenceID": 13, "context": "Case features get their own embeddings which get combined with token embeddings during the translation [14].", "startOffset": 103, "endOffset": 107}, {"referenceID": 0, "context": "Although many NMT researchers report results on end-to-end neural networks [1, 2, 17], we often found RNN encoder-decoder architecture insufficient to produce acceptable results, mainly due to mishandled named entities.", "startOffset": 75, "endOffset": 85}, {"referenceID": 1, "context": "Although many NMT researchers report results on end-to-end neural networks [1, 2, 17], we often found RNN encoder-decoder architecture insufficient to produce acceptable results, mainly due to mishandled named entities.", "startOffset": 75, "endOffset": 85}, {"referenceID": 16, "context": "Although many NMT researchers report results on end-to-end neural networks [1, 2, 17], we often found RNN encoder-decoder architecture insufficient to produce acceptable results, mainly due to mishandled named entities.", "startOffset": 75, "endOffset": 85}, {"referenceID": 11, "context": "To assess the quality of translations automatically, a useful heuristic is the so-called BLEU score [12] which roughly measures the degree of word overlap between the model translation and a human translation.", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "For the first layer of finding relevant sentences, we learn word and phrase embeddings by training word2vec [11] on our full (monolingual) corpora.", "startOffset": 108, "endOffset": 112}, {"referenceID": 7, "context": "For the classification task we use a bag-of-ngrams linear model approach [8].", "startOffset": 73, "endOffset": 76}], "year": 2017, "abstractText": "While some remarkable progress has been made in neural machine translation (NMT) research, there have not been many reports on its development and evaluation in practice. This paper tries to fill this gap by presenting some of our findings from building an in-house travel domain NMT system in a large scale E-commerce setting. The three major topics that we cover are optimization and training (including different optimization strategies and corpus sizes), handling real-world content and evaluating results.", "creator": "LaTeX with hyperref package"}}}