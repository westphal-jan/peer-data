{"id": "0705.4485", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2007", "title": "Mixed Membership Stochastic Blockmodels", "abstract": "Observations consisting of measurements on relationships for pairs of objects arise in many settings, such as protein interaction and gene regulatory networks, collections of author-recipient email, and social networks. Analyzing such data with probabilisic models can be delicate because the simple exchangeability assumptions underlying many boilerplate models no longer hold. In this paper, we describe a latent variable model of such data called the mixed membership stochastic blockmodel. This model extends blockmodels for relational data to ones which capture mixed membership latent relational structure, thus providing an object-specific low-dimensional representation. We develop a general variational inference algorithm for fast approximate posterior inference. We explore applications to social and protein interaction networks.", "histories": [["v1", "Wed, 30 May 2007 23:22:59 GMT  (540kb,D)", "http://arxiv.org/abs/0705.4485v1", "46 pages, 14 figures, 3 tables"]], "COMMENTS": "46 pages, 14 figures, 3 tables", "reviews": [], "SUBJECTS": "stat.ME cs.LG math.ST physics.soc-ph stat.ML stat.TH", "authors": ["edoardo m airoldi", "david m blei", "stephen e fienberg", "eric p xing"], "accepted": true, "id": "0705.4485"}, "pdf": {"name": "0705.4485.pdf", "metadata": {"source": "CRF", "title": "Mixed Membership Stochastic Blockmodels", "authors": ["Edoardo M. Airoldi", "David M. Blei"], "emails": ["(eairoldi@princeton.edu)", "(blei@cs.princeton.edu)", "(fienberg@stat.cmu.edu)", "(epxing@cs.cmu.edu)"], "sections": [{"heading": null, "text": "Observations consisting of measurements on relationships for pairs of objects arise in many settings, such as protein interaction and gene regulatory networks, collections of author-recipient email, and social networks. Analyzing such data with probabilisic models can be delicate because the simple exchangeability assumptions underlying many boilerplate models no longer hold. In this paper, we describe a latent variable model of such data called the mixed membership stochastic blockmodel. This model extends blockmodels for relational data to ones which capture mixed membership latent relational structure, thus providing an object-specific low-dimensional representation. We develop a general variational inference algorithm for fast approximate posterior inference. We explore applications to social and protein interaction networks.\nKeywords: Hierarchical Bayes, Latent Variables, Mean-Field Approximation, Statistical Network Analysis, Social Networks, Protein Interaction Networks."}, {"heading": "1 Introduction", "text": "Modeling relational information among objects, such as pairwise relations represented as graphs, is becoming an important problem in modern data analysis and machine learning. Many data sets contain interrelated observations. For example, scientific literature connects papers by citation, the Web connects pages by links, and protein-protein interaction data connects proteins by physical interaction records. In these settings, we often wish to infer hidden attributes of the objects from the observed measurements on pairwise properties. For example, we might want to compute a clustering of the web-pages, predict the functions of a protein, or\nar X\niv :0\n70 5.\n44 85\nv1 [\nst at\n.M E\n] 3\n0 M\nay 2\nassess the degree of relevance of a scientific abstract to a scholar\u2019s query.\nUnlike traditional attribute data collected over individual objects, relational data violate the classical independence or exchangeability assumptions that are typically made in machine learning and statistics. In fact, the observations are interdependent by their very nature, and this interdependence necessitates developing special-purpose statistical machinery for analysis.\nThere is a history of research devoted to this end. One problem that has been heavily studied is that of clustering the objects to uncover a group structure based on the observed patterns of interactions. Standard model-based clustering methods, e.g., mixture models, are not immediately applicable to relational data because they assume that the objects are conditionally independent given their cluster assignments. The latent stochastic blockmodel (Snijders and Nowicki, 1997) represents an adaptation of mixture modeling to dyadic data. In that model, each object belongs to a cluster and the relationships between objects are governed by the corresponding pair of clusters. Via posterior inference on such a model one can identify latent roles that objects possibly play, which govern their relationships with each other. This model originates from the stochastic blockmodel, where the roles of objects are known in advance (Wang and Wong, 1987). A recent extension of this model relaxed the finite-cardinality assumption on the latent clusters, via a nonparametric hierarchical Bayesian formalism based on the Dirichlet process prior (Kemp et al., 2004, 2006).\nThe latent stochastic blockmodel suffers from a limitation that each object can only belong to one cluster, or in other words, play a single latent role. In real life, it is not uncommon to encounter more intriguing data on entities that are multi-facet. For example, when a protein or a social actor interacts with different partners, different functional or social contexts may apply and thus the protein or the actor may be acting according to different latent roles they can possible play. In this paper, we relax the assumption of single-latent-role for actors, and develop a mixed membership model for relational data. Mixed membership models, such as latent Dirichlet allocation (Blei et al., 2003), have emerged in recent years as a flexible modeling tool for data where the single cluster assumption is violated by the heterogeneity within of a data point. They have been successfully applied in many domains, such as document analysis (Minka and Lafferty, 2002; Blei et al., 2003; Buntine and Jakulin, 2006), surveys (Berkman et al., 1989; Erosheva, 2002), image processing (Li and Perona, 2005), transcriptional regulation (Airoldi et al., 2006b), and population genetics (Pritchard\net al., 2000).\nThe mixed membership model associates each unit of observation with multiple clusters rather than a single cluster, via a membership probability-like vector. The concurrent membership of a data in different clusters can capture its different aspects, such as different underlying topics for words constituting each document. The mixed membership formalism is a particularly natural idea for relational data, where the objects can bear multiple latent roles or cluster-memberships that influence their relationships to others. As we will demonstrate, a mixed membership approach to relational data lets us describe the interaction between objects playing multiple roles. For example, some of a protein\u2019s interactions may be governed by one function; other interactions may be governed by another function.\nExisting mixed membership models are not appropriate for relational data because they assume that the data are conditionally independent given their latent membership vectors. In relational data, where each object is described by its relationships to others, we would like to assume that the ensemble of mixed membership vectors help govern the relationships of each object. The conditional independence assumptions of modern mixed membership models do not apply.\nIn this paper, we develop mixed membership models for relational data, develop a fast variational inference algorithm for inference and estimation, and demonstrate the application of our technique to large scale protein interaction networks and social networks. Our model captures the multiple roles that objects exhibit in interaction with others, and the relationships between those roles in determining the observed interaction matrix.\nMixed membership and the latent block structure can be reliably recovered from relational data (Section 4.1). The application to a friendship network among students tests the model on a real data set where a welldefined latent block structure exists (Section 4.2). The application to a protein interaction network tests to what extent our model can reduce the dimensionality of the data, while revealing substantive information about the functionality of proteins that can be used to inform subsequent analyses (Section 4.3)."}, {"heading": "2 The mixed membership stochastic blockmodel", "text": "In this section, we describe the modeling assumptions behind our proposed mixed membership model of relational data. We represent observed relational data as a graph G = (N , R), where R(p, q) maps pairs of nodes to values, i.e., edge weights. In this work, we consider binary matrices, where R(p, q) \u2208 {0, 1}. Thus, the data can be thought of as a directed graph.\nAs a running example, we will reanalyze the monk data of Sampson (1968). Sampson measured a collection of sociometric relations among a group of monks by repeatedly asking questions such as \u201cDo you like X?\u201d or \u201cDo you trust X?\u201d to determine asymmetric social relationships within the group. The questionnaire was repeated at four subsequent epochs. Information about these repeated, asymmetric relations can be collapsed into a square binary table that encodes the directed connections between monks (Breiger et al., 1975). In analyzing this data, the goal is to determine the underlying social groups within the monastary.\nIn the context of the monastery example, we assume K latent groups over actors, and the observed\nnetwork is generated according to latent distributions of group-membership for each monk and a matrix of group-group interaction strength. The latent per-monk distributions are specified by simplicial vectors. Each monk is associated with a randomly drawn vector, say ~\u03c0i for monk i, where \u03c0i,g denotes the probability of monk i belonging to group g. That is, each monk can simultaneously belong to multiple groups with different degrees of affiliation strength. The probabilities of interactions between different groups are defined by a matrix of Bernoulli ratesB(K\u00d7K), whereB(g, h) represents the probability of having a link between a monk from group g and a monk from group h.\nFor each network node (i.e., monk), the indicator vector ~zp\u2192q denotes the group membership of node p when it is approached by node q and ~zp\u2190q denotes the group membership of node q when it is approached by node p 1. N denotes the number of nodes in the graph, and recall that K denotes the number of distinct groups a node can belong to. Now putting everything together, we have a mixed membership stochastic blockmodel (MMSB), which posits that a graph G = (N , R) is drawn from the following procedure.\n\u2022 For each node p \u2208 N :\n\u2013 Draw a K dimensional mixed membership vector ~\u03c0p \u223c Dirichlet ( ~\u03b1 ) .\n\u2022 For each pair of nodes (p, q) \u2208 N \u00d7N :\n\u2013 Draw membership indicator for the initiator, ~zp\u2192q \u223c Multinomial ( ~\u03c0p ) .\n\u2013 Draw membership indicator for the receiver, ~zq\u2192p \u223c Multinomial ( ~\u03c0q ) .\n\u2013 Sample the value of their interaction, R(p, q) \u223c Bernoulli ( ~z >p\u2192qB ~zp\u2190q ) .\nThis process is illustrated as a graphical model in Figure 1. Note that the group membership of each node is context dependent. That is, each node may assume different membership when interacting to or being interacted by different peers. Statistically, each node is an admixture of group-specific interactions. The two sets of latent group indicators are denoted by {~zp\u2192q : p, q \u2208 N} =: Z\u2192 and {~zp\u2190q : p, q \u2208 N} =: Z\u2190. Also note that the pairs of group memberships that underlie interactions, for example, (~zp\u2192q, ~zp\u2190q) for\n1An indicator vector is used to denote membership in one of the K groups. Such a membership-indicator vector is specified as a K-dimensional vector of which only one element equals to one, whose index corresponds to the group to be indicated, and all other elements equal to zero.\nR(p, q), need not be equal; this fact is useful for characterizing asymmetric interaction networks. Equality may be enforced when modeling symmetric interactions.\nUnder the MMSB, the joint probability of the data R and the latent variables {~\u03c01:N , Z\u2192, Z\u2190} can be\nwritten in the following factored form,\np(R,~\u03c01:N , Z\u2192, Z\u2190|~\u03b1,B) = \u220f p,q P (R(p, q)|~zp\u2192q, ~zp\u2190q, B)P (~zp\u2192q|~\u03c0p)P (~zp\u2190q|~\u03c0q) \u220f p P (~\u03c0p|~\u03b1). (1)\nThis model easily generalizes to two important cases. (Appendix A develops this intuition more formally.) First, multiple networks among the same actors can be generated by the same latent vectors. This might be useful, for example, for analyzing simultaneously the relational measurements about esteem and disesteem, liking and disliking, positive influence and negative influence, praise and blame, e.g., see Sampson (1968), or those about the collection of 17 relations measured by Bradley (1987). Second, in the MMSB the data generating distribution is a Bernoulli, but B can be a matrix that parameterizes any kind of distribution. For example, technologies for measuring interactions between pairs of proteins such as mass spectrometry (Ho et al., 2002) and tandem affinity purification (Gavin et al., 2002) return a probabilistic assessment about the presence of interactions, thus setting the range of R(p, q) to [0, 1]. This is not the case for the manually curated collection of interactions we analyze in Section 4.3.\nThe central computational problem for this model is computing the posterior distribution of per-node mixed membership vectors and per-pair roles that generated the data. The membership vectors in particular provide a low dimensional representation of the underlying objects in the matrix of observations, which can be used in the data analysis task at hand. A related problem is parameter estimation, which is to find maximum likelihood estimates of the Dirichlet parameters ~\u03b1 and Bernoulli rates B.\nFor both of these tasks, we need to compute the probability of the observed data. This amounts to marginalizing out the latent variables from Equation 1. This is intractable for even small graphs. In Section 3, we develop a fast variational algorithm to approximate this marginal likelihood for parameter estimation and posterior inference."}, {"heading": "2.1 Modeling sparsity", "text": "Many real-world networks are sparse, meaning that most pairs of nodes do not have edges connecting them. For many pairs, the absence of an interaction is a result of the rarity of any interaction, rather than an indication that the underlying latent groups of the objects do not tend to interact. In the MMSB, however, all observations (both interactions and non-interactions) contribute equally to our inferences about group memberships and group to group interaction patterns. It is thus useful, in practical applications, to account for sparsity.\nWe introduce a sparsity parameter \u03c1 \u2208 [0, 1] to calibrate the importance of non-interaction. This models how often a non-interaction is due to sparsity rather than carrying information about the group memberships of the nodes. Specifically, instead of drawing an edge directly from the Bernoulli specified above, we downweight it to (1 \u2212 \u03c1) \u00b7 ~z >p\u2192qB ~zp\u2190q. The probability of having no interaction is thus 1 \u2212 \u03c3pq = (1 \u2212 \u03c1) \u00b7 ~z >p\u2192q(1 \u2212 B) ~zp\u2190q + \u03c1. (This is equivalent to re-parameterizing the interaction matrix B.) In posterior inference and parameter estimation, a large value of \u03c1 will cause the interactions in the matrix to be weighted more than non-interactions in determining the estimates of {~\u03b1,B, ~\u03c01:N}."}, {"heading": "2.2 A case study of the Monastery network via MMSB: crisis in a Cloister", "text": "Before turning to the details of posterior inference and parameter estimation, we illustrate the MMSB with an analysis of the monk data described above. In more detail, Sampson (1968) surveyed 18 novice monks in a monastery and asked them to rank the other novices in terms of four sociometric relations: like/dislike, esteem, personal influence, and alignment with the monastic credo. We consider Breiger\u2019s collation of Sampson\u2019s data (Breiger et al., 1975). The original graph of monk-monk interaction is illustrated in Figure 2 (left).\nSampson spent several months in a monastery in New England, where novices (the monks) were preparing to join a monastic order. Sampson\u2019s original analysis was rooted in direct anthropological observations. He strongly suggested the existence of tight factions among the novices: the loyal opposition (whose members joined the monastery first), the young turks (who joined later on), the outcasts (who were not accepted\nin the two main factions), and the waverers (who did not take sides). The events that took place during Sampson\u2019s stay at the monastery supported his observations\u2014members of the young turks resigned after their leaders were expelled over religious differences (John Bosco and Gregory). We shall refer to the labels assigned by Sampson to the novices in the analysis below. For more analyses, we refer to Fienberg et al. (1985), Davis and Carley (2006) and Handcock et al. (2007).\nUsing the techniques outlined below in Section 3, we fit the monks to MMSB models for different numbers of groups, providing model estimates {\u03b1\u0302, B\u0302} and posterior mixed membership vectors ~\u03c0n for each monk. Here, we use the following approximation to BIC to choose the number of groups in the MMSB:\nBIC = 2 \u00b7 log p(R) \u2248 2 \u00b7 log p(R|~\u0302\u03c0, Z\u0302, ~\u0302\u03b1, B\u0302)\u2212 |~\u03b1,B| \u00b7 log |R|,\nwhich selects three groups, where |~\u03b1,B| is the number of hyper-parameters in the model, and |R| is the number of positive relations observed (Volinsky and Raftery, 2000; Handcock et al., 2007). Note that this is the same number of groups that Sampson identified. We illustrate the fit of model fit via the predicted network in Figure 2 (Right).\nThe MMSB can provide interesting descriptive statistics about the actors in the observed graph. In Figure 3 we illustrate the the posterior means of the mixed membership scores, E[~\u03c0|R], for the 18 monks in\nthe monastery. Note that the monks cluster according to Sampson\u2019s classification, with Young Turks, Loyal Opposition, and Outcasts dominating each corner respectively. We can see the central role played by John Bosco and Gregory, who exhibit relations in all three groups, as well as the uncertain affiliations of Ramuald and Victor; Amand\u2019s uncertain affiliation, however, is not captured.\nLater, we considered six graphs encoding specific relations\u2014positive and negative influence, positive and negative praise, esteem and disesteem\u2014and we performed independent analyses using MMSB. This allowed us to look for signal about the mixed membership of monks to factions that may have been lost in the data set prepared by Breiger et al. (1975) because of averaging. Figure 4 shows the projections in the simplex of the posterior mixed membership vectors for each of the six relations above. For instance, we can see how Victor, Amand, and Ramuald\u2014the three waverers\u2014display mixed membership in terms of positive and negative influence, positive praise, and disesteem. The mixed membership of Amand, in particular, is\nexpressed in terms of these relations, but not in terms of negative praise or esteem. This finding is supported Sampson\u2019s anthropological observations, and it suggests that relevant substantive information has been lost when the graphs corresponding to multiple sociometric relations have been collapsed into a single social network (Breiger et al., 1975). Methods for the analysis of multiple sociometric relations are thus to be preferred. In Appendices A and B extend the mixed membership stochastic blockmodel to deal with the case of multivariate relations, and we solve estimation and inference in the general case."}, {"heading": "3 Parameter Estimation and Posterior Inference", "text": "In this section, we tackle the two computational problems for the MMSB: posterior inference of the pernode mixed membership vectors and per-pair roles, and parameter estimation of the Dirichlet parameters and Bernoulli rate matrix. We use empirical Bayes to estimate the parameters (~\u03b1,B), and employ a meanfield approximation scheme (Jordan et al., 1999) for posterior inference."}, {"heading": "3.1 Posterior inference", "text": "In posterior inference, we would like to compute the posterior distribution of the latent variables given a collection of observations. As for other mixed membership models, this is intractable to compute. The normalizing constant of the posterior is the marginal probability of the data, which requires an intractable integral over the simplicial vectors ~\u03c0p,\np(R|~\u03b1,B) = \u222b ~\u03c01:N \u220f p,q \u2211 zp\u2190q ,zp\u2192q P (R(p, q)|~zp\u2192q, ~zp\u2190q, B)P (~zp\u2192q|~\u03c0p)P (~zp\u2190q|~\u03c0q) \u220f p P (~\u03c0p|~\u03b1). (2)\nA number of approxiate inference algorithms for mixed membership models have appeared in recent years, including mean-field variational methods (Blei et al., 2003; Teh et al., 2007), expectation propagation (Minka and Lafferty, 2002), and Monte Carlo Markov chain sampling (MCMC) (Erosheva and Fienberg, 2005; Griffiths and Steyvers, 2004).\nWe appeal to mean-field variational methods to approximate the posterior of interest. Mean-field vari-\national methods provide a practical deterministic alternative to MCMC. MCMC is not practical for the MMSB due to the large number of latent variables needed to be sampled. The main idea behind variational methods is to posit a simple distribution of the latent variables with free parameters. These parameters are fit to be close in Kullback-Leibler divergence to the true posterior of interest. Good reviews of variational methods method can be found in a number of papers (Jordan et al., 1999; Wainwright and Jordan, 2003; Xing et al., 2003; Bishop et al., 2003)\nThe log of the marginal probability in Equation 2 can be bound with Jensen\u2019s inequality as follows,\nlog p(R |\u03b1,B) \u2265 Eq [ log p(R,~\u03c01:N , Z\u2192, Z\u2190|\u03b1,B) ] \u2212Eq [ log q(~\u03c01:N , Z\u2192, Z\u2190) ] , (3)\nby introducing a distribution of the latent variables q that depends on a set of free parameters We specify q as the mean-field fully-factorized family,\nq(~\u03c01:N , Z\u2192, Z\u2190|~\u03b31:N ,\u03a6\u2192,\u03a6\u2190) = \u220f p q1(~\u03c0p|~\u03b3p) \u220f p,q ( q2(~zp\u2192q|~\u03c6p\u2192q) q2(~zp\u2190q|~\u03c6p\u2190q) ) , (4)\nwhere q1 is a Dirichlet, q2 is a multinomial, and {~\u03b31:N ,\u03a6\u2192,\u03a6\u2190} are the set of free variational parameters that can be set to tighten the bound.\nTightening the bound with respect to the variational parameters is equivalent to minimizing the KL divergence between q and the true posterior. When all the nodes in the graphical model are conjugate pairs or mixtures of conjugate pairs, we can directly write down a coordinate ascent algorithm for this optimization (Xing et al., 2003; Bishop et al., 2003). The update for the variational multinomial parameters is\n\u03c6\u0302p\u2192q,g \u221d e Eq [ log \u03c0p,g ] \u00b7 \u220f h ( B(g, h)R(p,q)\u00b7 ( 1\u2212B(g, h) )1\u2212R(p,q))\u03c6p\u2190q,h (5) \u03c6\u0302p\u2190q,h \u221d e Eq [ log \u03c0q,h\n] \u00b7 \u220f g ( B(g, h)R(p,q)\u00b7 ( 1\u2212B(g, h) )1\u2212R(p,q))\u03c6p\u2192q,g , (6)\nfor g, h = 1, . . . ,K. The update for the variational Dirichlet parameters \u03b3p,k is\n\u03b3\u0302p,k = \u03b1k + \u2211 q \u03c6p\u2192q,k + \u2211 q \u03c6p\u2190q,k, (7)\nfor all nodes p = 1, . . . , N and k = 1, . . . ,K. An analytical expression for Eq [ log \u03c0q,h ] is derived in Appendix B.3. The complete coordinate ascent algorithm to perform variational inference is described in Figure 5.\nTo improve convergence in the relational data setting, we introduce a nested variational inference scheme\nbased on an alternative schedule of updates to the traditional ordering. In a na\u0131\u0308ve iteration scheme for variational inference, one initializes the variational Dirichlet parameters ~\u03b31:N and the variational multinomial parameters (~\u03c6p\u2192q, ~\u03c6p\u2190q) to non-informative values, and then iterates until convergence the following two steps: (i) update ~\u03c6p\u2192q and \u03c6p\u2190q for all edges (p, q), and (ii) update ~\u03b3p for all nodes p \u2208 N . In such algorithm, at each variational inference cycle we need to allocate NK + 2N2K scalars.\nIn our experiments, the na\u0131\u0308ve variational algorithm often failed to converge, or converged only after many iterations. We attribute this behavior to the dependence between ~\u03b31:N and B, which is not satisfied by the na\u0131\u0308ve algorithm. Some intuition about why this may happen follows. From a purely algorithmic perspective, the na\u0131\u0308ve variational EM algorithm instantiates a large coordinate ascent algorithm, where the parameters can be semantically divided into coherent blocks. Blocks are processed in a specific order, and the parameters within each block get all updated each time.2 At every new iteration the na\u0131\u0308ve algorithm sets all the elements of ~\u03b3t+11:N equal to the same constant. This dampens the likelihood by suddenly breaking the dependence between the estimates of parameters in ~\u0302\u03b3 t\n1:N and in B\u0302 t that was being inferred from the data\nduring the previous iteration.\nInstead, the nested variational inference algorithm maintains some of this dependence that is being inferred from the data across the various iterations. This is achieved mainly through a different scheduling of the parameter updates in the various blocks. To a minor extent, the dependence is maintained by always keeping the block of free parameters, (~\u03c6p\u2192q, ~\u03c6p\u2190q), optimized given the other variational parameters. Note that these parameters are involved in the updates of parameters in ~\u03b31:N and in B, thus providing us with a channel to maintain some of the dependence among them, i.e., by keeping them at their optimal value given the data.\nFurthermore, the nested algorithm has the advantage that it trades time for space thus allowing us to deal with large graphs; at each variational cycle we need to allocate NK + 2K scalars only. The increased running time is partially offset by the fact that the algorithm can be parallelized and leads to empirically observed faster convergence rates. This algorithm is also better than MCMC variations (i.e., blocked and collapsed Gibbs samplers) in terms of memory requirements and convergence rates\u2014an empirical compar2Within a block, the order according to which (scalar) parameters get updated is not expected to affect convergence.\nison between the na\u0131\u0308ve and nested variational inference schemes in presented in Figure 7, left panel."}, {"heading": "3.2 Parameter estimation", "text": "We compute the empirical Bayes estimates of the model hyper-parameters {~\u03b1,B}with a variational expectationmaximization (EM) algorithm. Alternatives to empirical Bayes have been proposed to fix the hyper-parameters and reduce the computation. The results, however, are not always satisfactory and often times cause of concern, since the inference is sensitive to the choice of the hyper-parameters (Airoldi et al., 2006a). Empirical Bayes, on the other hand, guides the posterior inference towards a region of the hyper-parameter space that is supported by the data.\nVariational EM uses the lower bound in Equation 3 as a surrogate for the likelihood. To find a local optimum of the bound, we iterate between: fitting the variational distribution q to approximate the posterior, and maximizing the corresponding lower bound for the likelihood with respect to the parameters. The latter M-step is equivalent to finding the MLE using expected sufficient statistics under the variational distribution. We consider the maximization step for each parameter in turn.\nA closed form solution for the approximate maximum likelihood estimate of ~\u03b1 does not exist (Minka,\n2003). We use a linear-time Newton-Raphson method, where the gradient and Hessian are\n\u2202L~\u03b1 \u2202\u03b1k\n= N ( \u03c8 (\u2211\nk\n\u03b1k ) \u2212\u03c8(\u03b1k) ) + \u2211 p ( \u03c8(\u03b3p,k)\u2212 \u03c8 (\u2211 k \u03b3p,k )) ,\n\u2202L~\u03b1 \u2202\u03b1k1\u03b1k2\n= N (\nI(k1=k2) \u00b7 \u03c8 \u2032(\u03b1k1)\u2212 \u03c8\u2032 (\u2211 k \u03b1k )) .\nThe approximate MLE of B is\nB\u0302(g, h) = \u2211 p,q R(p, q) \u00b7 \u03c6p\u2192qg \u03c6p\u2190qh\u2211\np,q \u03c6p\u2192qg \u03c6p\u2190qh , (8)\nfor every index pair (g, h) \u2208 [1,K]\u00d7 [1,K]. Finally, the approximate MLE of the sparsity parameter \u03c1 is\n\u03c1\u0302 =\n\u2211 p,q ( 1\u2212R(p, q) ) \u00b7 ( \u2211 g,h \u03c6p\u2192qg \u03c6p\u2190qh )\u2211\np,q \u2211 g,h \u03c6p\u2192qg \u03c6p\u2190qh\n. (9)\nAlternatively, we can fix \u03c1 prior to the analysis; the density of the interaction matrix is estimated with d\u0302 = \u2211\np,q R(p, q)/N 2, and the sparsity parameter is set to \u03c1\u0303 = (1 \u2212 d\u0302). This latter estimator attributes all\nthe information in the non-interactions to the point mass, i.e., to latent sources other than the block model B or the mixed membership vectors ~\u03c01:N . It does however provide a quick recipe to reduce the computational burden during exploratory analyses.3\nSeveral model selection strategies are available for complex hierarchical models. In our setting, model selection translates into the determination of a plausible value of the number of groups K. In the various analyses presented, we selected the optimal value of K with the highest averaged held-out likelihood in a cross-validation experiment, on large networks, and using an approximation to BIC, on small networks."}, {"heading": "4 Experiments and Results", "text": "Here, we present experiments on simulated data, and we develop two applications to social and protein interaction networks. The three problem settings serve different purposes.\nSimulations are performed in Section 4.1 to show that both mixed membership, ~\u03c01:N , and the latent block structure, B, can be recovered from data, when they exist, and that the nested variational inference algorithm is as fast as the na\u0131\u0308ve implementation while reaching a higher peak in the likelihood\u2014coeteris paribus.\nThe application to a friendship network among students in Section 4.2 tests the model on a real data set where we expect a well-defined latent block structure to inform the observed connectivity patterns in the network. In this application, the blocks are interpretable in terms of grades. We compare our results with those that were recently published with a simple mixture of blocks (Doreian et al., 2007) and with a latent space model (Handcock et al., 2007) on the same data.\nThe application to a protein interaction network in Section 4.3 tests the model on a real data set where we expect a noisy, vague latent block structure to inform the observed connectivity patterns in the network 3Note that \u03c1\u0303 = \u03c1\u0302 in the case of single membership. In fact, that implies \u03c6mp\u2192qg = \u03c6mp\u2190qh = 1 for some (g, h) pair, for any (p, q) pair.\nto some degree. In this application, the blocks are interpretable in terms functional biological contexts. This application tests to what extent our model can reduce the dimensionality of the data, while revealing substantive information about the functionality of proteins that can be used to inform subsequent analyses."}, {"heading": "4.1 Exploring Expected Model Behavior with Simulations", "text": "In developing the MMSB and the corresponding computation, our hope is the the model can recover both the mixed membership of nodes to clusters and the latent block structure among clusters in situations where a block structure exists and the relations are measured with some error. To substantiate this claim, we sampled graphs of 100, 300, and 600 nodes from blockmodels with 4, 10, and 20 clusters, respectively, using the MMSB. We used different values of \u03b1 to simulate a range of settings in terms of membership of nodes to clusters\u2014from almost unique (\u03b1 = 0.05) to mixed (\u03b1 = 0.25)."}, {"heading": "4.1.1 Recovering the Truth", "text": "The variational EM algorithm successfully recovers both the latent block model B and the latent mixed membership vectors ~\u03c01:N . In Figure 6 we show the adjacency matrices of binary interactions where rows, i.e., nodes, are reordered according to their most likely membership. The nine panels are organized in to a three-by-three grid; panels in the same row correspond to the same combinations of (# nodes and # groups), whereas panels in the same columns correspond to the same value of \u03b1 that was used to generate the data. In each panel, the estimated reordering of the nodes (i.e., the reordering of rows and columns in the interaction matrix) reveals the block model that was originally used to simulate the interactions. As \u03b1 increases, each node is likely to belong to more clusters. As a consequence, they express interaction patterns of clusters. This phenomenon reflects in the reordered interaction matrices as the block structure is less evident."}, {"heading": "4.1.2 Nested Variational Inference", "text": "The nested variational algorithm drives the log-likelihood to converge as fast as the na\u0131\u0308ve variational inference algorithm does, but reaches a significantly higher plateau. In the left panel of Figure 7, we compare the\nrunning times of the nested variational-EM algorithm versus the na\u0131\u0308ve implementation on a graph with 100 nodes and 4 clusters. We measure the number of seconds on theX axis and the log-likelihood on the Y axis. The two curves are averages over 26 experiments, and the error bars are at three standard deviations. Each of the 26 pairs of experiments was initialized with the same values for the parameters. The nested algorithm, which is more efficient in terms of space, converged faster. Furthermore, the nested variational algorithm can be parallelized given that the updates for each interaction (i, j) are independent of one another."}, {"heading": "4.1.3 Choosing the Number of Groups", "text": "Figure 7 (right panel) shows an example where cross-validation is sufficient to perform model selection for the MMSB. The example shown corresponds to a network among 300 nodes with K = 10 clusters. We measure the number of latent clusters on the X axis and the average held-out log-likelihood, corresponding to five-fold cross-validation experiments, on the Y axis. A peak in this curve identifies the optimal number of clusters, to the extend of describing the data. The nested variational EM algorithm was run till convergence, for each value of K we tested, with a tolerance of = 10\u22125. In the example shown, our estimate for K occurs at the peak in the average held-out log-likelihood, and equals the correct number of clusters,K\u2217 = 10"}, {"heading": "4.2 Application to Social Network Analysis", "text": "The National Longitudinal Study of Adolescent Health is nationally representative study that explores the how social contexts such as families, friends, peers, schools, neighborhoods, and communities influence health and risk behaviors of adolescents, and their outcomes in young adulthood (Harris et al., 2003; Udry, 2003). Here, we analyze a friendship network among the students, at the same school that was considered by Handcock et al. (2007) and discussants.\nA questionnaire was administered to a sample of students who were allowed to nominate up to 10 friends. At the school we picked, friendship nominations were collected among 71 students in grades 7 to 12. Two students did not nominate any friends so we analyzed the network of binary, asymmetric friendship relations among the remaining 69 students. The left panel of Figure 8 shows the raw friendship relations, and we contrast this to the estimated networks in the central and right panels based on our model estimates.\nGiven the size of the network we used BIC to perform model selection, as in the monks example of Section 2.2. The results suggest a model with K\u2217 = 6 groups. (We fix K\u2217 = 6 in the analyses that follow.) The hyper-parameters were estimated with the nested variational EM. They are \u03b1\u0302 = 0.0487, \u03c1\u0302 = 0.936, and a fairly diagonal block-to-block connectivity matrix,\nB\u0302 =  0.3235 0.0 0.0 0.0 0.0 0.0 0.0 0.3614 0.0002 0.0 0.0 0.0 0.0 0.0 0.2607 0.0 0.0 0.0002 0.0 0.0 0.0 0.3751 0.0009 0.0 0.0 0.0 0.0 0.0002 0.3795 0.0\n0.0 0.0 0.0 0.0 0.0 0.3719\n .\nFigure 9 shows the expected posterior mixed membership scores for the 69 students in the sample; few students display mixed membership. The rarity of mixed membership in this context is expected. Mixed membership, instead, may signal unexpected social situations for further investigation. For instance, it may\nsignal a family bond such as brotherhood, or a kid that is repeating a grade and is thus part of a broader social clique. In this data set we can successfully attempt an interpretation of the clusters in terms of grades. Table\n1 shows the correspondence between clusters and grades in terms of students, for three alternative models. The three models are our mixed membership stochastic blockmodel (MMSB), a simpler stochastic block mixture model (Doreian et al., 2007) (MSB), and the latent space cluster model (Handcock et al., 2007) (LSCM).\nConcluding this example, we note how the model decouples the observed friendship patterns into two complementary sources of variability. On the one hand, the connectivity matrixB is a global, unconstrained set of hyper-parameters. On the other hand, the mixed membership vectors ~\u03c01:N provide a collection of node-specific latent vectors, which inform the directed connections in the graph in a symmetric fashion\u2014 and can be used to produce node-specific predictions.\n4.3 Application to Protein Interactions in Saccharomyces Cerevisiae\nProtein-protein interactions (PPI) form the physical basis for the formation of complexes and pathways that carry out different biological processes. A number of high-throughput experimental approaches have been applied to determine the set of interacting proteins on a proteome-wide scale in yeast. These include the two-hybrid (Y2H) screens and mass spectrometry methods. Mass spectrometry can be used to identify components of protein complexes (Gavin et al., 2002; Ho et al., 2002).\nHigh-throughput methods, though, may miss complexes that are not present under the given conditions. For example, tagging may disturb complex formation and weakly associated components may dissociate and escape detection. Statistical models that encode information about functional processes with high precision are an essential tool for carrying out probabilistic de-noising of biological signals from high-throughput experiments.\nOur goal is to identify the proteins\u2019 diverse functional roles by analyzing their local and global patterns of interaction via MMSB. The biochemical composition of individual proteins make them suitable for carrying out a specific set of cellular operations, or functions. Proteins typically carry out these functions as part of stable protein complexes (Krogan et al., 2006). There are many situations in which proteins are believed to interact (Alberts et al., 2002). The main intuition behind our methodology is that pairs of protein interact because they are part of the same stable protein complex, i.e., co-location, or because they are part of\ninteracting protein complexes as they carry out compatible cellular operations."}, {"heading": "4.3.1 Gold Standards for Functional Annotations", "text": "The Munich Institute for Protein Sequencing (MIPS) database was created in 1998 based on evidence derived from a variety of experimental techniques, but does not include information from high-throughput data sets (Mewes et al., 2004). It contains about 8000 protein complex associations in yeast. We analyze a subset of this collection containing 871 proteins, the interactions amongst which were hand-curated. The institute also provides a set of functional annotations, alternative to the gene ontology (GO). These annotations are organized in a tree, with 15 general functions at the first level, 72 more specific functions at an intermediate level, and 255 annotations at the the leaf level. In Table 2 we map the 871 proteins in our collections to the main functions of the MIPS annotation tree; proteins in our sub-collection have about 2.4 functional annotations on average.4\nBy mapping proteins to the 15 general functions, we obtain a 15-dimensional representation for each protein. In Figure 10 each panel corresponds to a protein; the 15 functional categories are ordered as in Table 2 on the X axis, whereas the presence or absence of the corresponding functional annotation is displayed on the Y axis. 4We note that the relative importance of functional categories in our sub-collection, in terms of the number of proteins involved, is different from the relative importance of functional categories over the entire MIPS collection."}, {"heading": "4.3.2 Brief Summary of Previous Findings", "text": "In previous work, we established the usefulness of an admixture of latent blockmodels for analyzing proteinprotein interaction data (Airoldi et al., 2005). For example, we used the MMSB for testing functional interaction hypotheses (by setting a null hypothesis for B), and unsupervised estimation experiments. In the next Section, we assess whether, and how much, functionally relevant biological signal can be captured in by the MMSB.\nIn summary, the results in Airoldi et al. (2005) show that the MMSB identifies protein complexes whose member proteins are tightly interacting with one another. The identifiable protein complexes correlate with the following four categories of Table 2: cell cycle & DNA processing, transcription, protein synthesis, and sub-cellular activities. The high correlation of inferred protein complexes can be leveraged for predicting the presence of absence of functional annotations, for example, by using a logistic regression. However, there is not enough signal in the data to independently predict annotations in other functional categories. The empirical Bayes estimates of the hyper-parameters that support these conclusions in the various types of analyses are consistent; \u03b1\u0302 < 1 and small; and B\u0302 nearly block diagonal with two positive blocks comprising the four identifiable protein complexes. In these previous analyses, we fixed the number of latent protein complexes to 15; the number of broad functional categories in Table 2.\nThe latent protein complexes are not a-priori identifiable in our model. To resolve this, we estimated a mapping between latent complexes and functions by minimizing the divergence between true and predicted marginal frequencies of membership, where the truth was evaluated on a small fraction of the interactions. We used this mapping to compare predicted versus known functional annotations for all proteins. The best estimated mapping is shown in the left panel of Figure 11, along with the marginal latent category membership, and it is compared to the 15 broad functional categories Table 2, along with the known category\nmembership (in the MIPS database), in the right panel. Figure 12 displays a few examples of predicted mixed membership probabilities against the true annotations, given the estimated mapping of latent protein complexes to functional categories."}, {"heading": "4.3.3 Measuring the Functional Content in the Posterior", "text": "In a follow-up study we considered the gene ontology (GO) (Ashburner et al., 2000) as the source of functional annotations to consider as ground truth in our analyses. GO is a broader and finer grained functional annotation scheme if compared to that produced by the Munich Institute for Protein Sequencing. Furthermore, we explored a much larger model space than in the previous study, in order to tests to what extent MMSB can reduce the dimensionality of the data while revealing substantive information about the functionality of proteins that can be used to inform subsequent analyses. We fit models with a number blocks up to K = 225. Thanks to our nested variational inference algorithm, we were able to perform five-fold cross-validation for each value ofK. We determined that a fairly parsimonious model (K\u2217 = 50) provides a good description of the observed protein interaction network. This fact is (qualitatively) consistent with the quality of the predictions that were obtained with a parsimonious model (K = 15) in the previous section, in a different setting. This finding supports the hypothesis that groups of interacting proteins in the MIPS data set encode biological signal at a scale of aggregation that is higher than that of protein complexes.5\n5It has been recently suggested that stable protein complexes average five proteins in size (Krogan et al., 2006). Thus, if MMSB captured biological signal at the protein-complex resolution, we would expect the optimal number of groups to be much higher (Disregarding mixed membership, 871/5 \u2248 175.)\nWe settled on a model with K\u2217 = 50 blocks. To evaluate the functional content of the interactions predicted by such model, we first computed the posterior probabilities of interactions by thresholding the posterior expectations\nE [ R(p, q) = 1 ] \u2248 ~\u0302\u03c0p \u2032 B\u0302 ~\u0302\u03c0q and E [ R(p, q) = 1 ] \u2248 ~\u0302\u03c6p\u2192q \u2032 B\u0302 ~\u0302\u03c6p\u2190q,\nand we then computed the precision-recall curves corresponding to these predictions (Myers et al., 2006). These curves are shown in Figure 13 as the light blue (\u2212\u00d7) line and the the dark blue (\u2212+) line. In Figure 13 we also plotted the functional content of the original MIPS collection. This plot confirms that\nthe MIPS collection of interactions, our data, is one of the most precise (the Y axis measures precision) and most extensive (the X axis measures the amount of functional annotations predicted, a measure of recall) source of biologically relevant interactions available to date\u2014the yellow diamond, point # 2. The posterior means of (~\u03c01:N ) and the estimates of (\u03b1,B) provide a parsimonious representation for the MIPS collection, and lead to precise interaction estimates, in moderate amount (the light blue, \u2212\u00d7 line). The posterior means of (Z\u2192, Z\u2190) provide a richer representation for the data, and describe most of the functional content of the MIPS collection with high precision (the dark blue, \u2212+ line). Most importantly, notice the estimated protein interaction networks, i.e., pluses and crosses, corresponding to lower levels of recall feature a more precise functional content than the original. This means that the proposed latent block structure is helpful in summarizing the collection of interactions\u2014by ranking them properly. (It also happens that dense blocks of predicted interactions contain known functional predictions that were not in the MIPS collection.) Table 3 provides more information about three instances of predicted interaction networks displayed in Figure 13; namely, those corresponding the points annotated with the numbers 1 (a collection of interactions predicted with the ~\u03c0\u2019s), 2 (the original MIPS collection of interactions), and 3 (a collection of interactions predicted with the ~\u03c6\u2019s). Specifically, the table shows a breakdown of the predicted (posterior) collections of interactions in each example network into the gene ontology categories. A count in the second-to-last column of Table 3 corresponds to the fact that both proteins are annotated with the same GO functional category.6 Figure 14 investigates the correlations between the data sets (in rows) we considered in Figure 13 and few gene ontology categories (in columns). The intensity of the square (red is high) measures the area under the precision-recall curve (Myers et al., 2006).\nIn this application, the MMSB learned information about (i) the mixed membership of objects to latent groups, and (ii) the connectivity patterns among latent groups. These quantities were useful in describing and summarizing the functional content of the MIPS collection of protein interactions. This suggests the use of MMSB as a dimensionality reduction approach that may be useful for performing model-driven denoising of new collections of interactions, such as those measured via high-throughput experiments. 6Note that, in GO, proteins are typically annotated to multiple functional categories.\nFi gu\nre 14\n: W\ne in\nve st\nig at\ne th\ne co\nrr el\nat io\nns be\ntw ee\nn da\nta co\nlle ct\nio ns\n(r ow\ns) an\nd a\nsa m\npl e\nof ge\nne on\nto lo\ngy ca\nte go\nri es\n(c ol\num ns\n). T\nhe in\nte ns ity of th e sq ua re (r ed is hi gh )m ea su re s th e ar ea un de rt he pr ec is io nre ca ll cu rv e."}, {"heading": "5 Discussion", "text": "Below we place our research in a larger modeling context, offer some insights into the inner workings of the model, and briefly comment on limitations and extensions.\nModern probabilistic models for relational data analysis are rooted in the stochastic blockmodels for psychometric and sociological analysis, pioneered by Lorrain and White (1971) and by Holland and Leinhardt (1975). In statistics, this line of research has been extended in various contexts over the years (Fienberg et al., 1985; Wasserman and Pattison, 1996; Snijders, 2002; Hoff et al., 2002; Doreian et al., 2004). In machine learning, the related technique of Markov random networks (Frank and Strauss, 1986) have been used for link prediction (Taskar et al., 2003) and the traditional blockmodels have been extended to include nonparametric Bayesian priors (Kemp et al., 2004, 2006) and to integrate relations and text (McCallum et al., 2007).\nThere is a particularly close relationship between the MMSB and the latent space models (Hoff et al., 2002; Handcock et al., 2007). In the latent space models, the latent vectors are drawn from Gaussian distributions and the interaction data is drawn from a Gaussian with mean ~\u03c0p \u2032I~\u03c0q. In the MMSB, the marginal probability of an interaction takes a similar form, ~\u03c0p \u2032B~\u03c0q, whereB is the matrix of probabilities of interactions for each pair of latent groups. Two major differences exist between these approaches. In MMSB, the distribution over the latent vectors is a Dirichlet and the underlying data distribution is arbitrary\u2014 we have chosen Bernoulli. The posterior inference in latent space models (Hoff et al., 2002; Handcock et al., 2007) is carried out via MCMC sampling, while we have developed a scalable variational inference algorithm to analyze large network structures. (It would be interesting to develop a variational algorithm for the latent space models as well.)\nWe note how the model decouples the observed friendship patterns into two complementary sources of variability. On the one hand, the connectivity matrix B is a global, unconstrained set of hyper-parameters. On the other hand, the mixed membership vectors ~\u03c01:N provide a collection of node-specific latent vectors, which inform the directed connections in the graph in a symmetric fashion. Last, the single membership indicators (~zp\u2192q, ~zp\u2190q) provide a collection interaction-specific latent variables.\nA recurring question, which bears relevance to mixed membership models in general, is why we do not integrate out the single membership indicators\u2014(~zp\u2192q, ~zp\u2190q). While this may lead to computational efficiencies we would often lose interpretable quantities that are useful for making predictions, for de-noising new measurements, or for performing other tasks. In fact, the posterior distributions of such quantities typically carry substantive information about elements of the application at hand. In the application to protein interaction networks of Section 4.3, for example, they encode the interaction-specific memberships of individual proteins to protein complexes.\nA limitation of our model can be best appreciated in a simulation setting. If we consider structural properties of the network MMSB is capable of generating, we count a wide array of local and global connectivity patterns. But the model does not readily generate hubs, that is, nodes connected with a large number of directed or undirected connections, or networks with skewed degree distributions.\nFrom a data analysis perspective, we speculate that the value of MMSB in capturing substantive information about a problem will increase in semi-supervised setting\u2014where, for example, information about the membership of genes to functional contexts is included in the form of prior distributions. In such a setting we may be interested in looking at the change between prior and posterior membership; a sharp change may signal biological phenomena worth investigating.\nWe need not assume that the number of groups/blocks, K, is finite. It is possible, for example, to posit that the mixed-membership vectors are sampled form a stochastic process D\u03b1, in the nonparametric setting. In particular, in order to maintain mixed membership of nodes to groups/blocks we need to sample them from a hierarchical Dirichlet process (Teh et al., 2006), rather than from a Diriclet Process (Escobar and West, 1995)."}, {"heading": "6 Conclusions", "text": "In this paper we introduced mixed membership stochastic blockmodels, a novel class of latent variable models for relational data. These models provide exploratory tools for scientific analyses in applications where the observations can be represented as a collection of unipartite graphs. The nested variational inference\nalgorithm is parallelizable and allows fast approximate inference on large graphs.\nThe relational nature of such data as well as the multiple goals of the analysis in the applications we considered motivated our technical choices. Latent variables in our models are introduced to capture application-specific substantive elements of interest, e.g., monks and factions in the monastery. The applications to social and biological networks we considered share considerable similarities in the way such elements relate. This allowed us to identify a general formulation of the model that we present in Appendix A. Approximate variational inference for the general model is presented in Appendix B."}, {"heading": "Acknowledgments", "text": "This work was partially supported by National Institutes of Health under Grant No. R01 AG023141-01, by the Office of Naval Research under Contract No. N00014-02-1-0973, by the National Science Foundation under Grants No. DMS-0240019, IIS-0218466, and DBI-0546594, by the Pennsylvania Department of Health\u2019s Health Research Program under Grant No. 2001NF-Cancer Health Research Grant ME-01-739, and by the Department of Defense, all to Carnegie Mellon University."}, {"heading": "A General Model Formulation", "text": "In general, mixed membership stochastic blockmodels can be specified in terms of assumptions at four levels: population, node, latent variable, and sampling scheme level.\nA1\u2013Population Level Assume that there areK classes or sub-populations in the population of interest. We denote by f ( R(p, q) |\nB(g, h) ) the probability distribution of the relation measured on the pair of nodes (p, q), where the p-th node is in the h-th sub-population, the q-th node is in the h-th sub-population, and B(g, h) contains the relevant parameters. The indices i, j run in 1, . . . , N , and the indices g, h run in 1, . . . ,K.\nA2\u2014Node Level\nThe components of the membership vector ~\u03c0p = [~\u03c0p(1), . . . , ~\u03c0p(k)]\u2032 encodes the mixed membership of the n-th node to the various sub-populations. The distribution of the observed response R(p, q) given the relevant, node-specific memberships, (~\u03c0p, ~\u03c0q), is then\nPr ( R(p, q) | ~\u03c0p, ~\u03c0q, B ) = K\u2211 g,h=1 ~\u03c0p(g) f(R(p, q) | B(g, h)) ~\u03c0q(h). (10)\nConditional on the mixed memberships, the response edges yjnm are independent of one another, both across distinct graphs and pairs of nodes.\nA3\u2014Latent Variable Level\nAssume that the mixed membership vectors ~\u03c01:N are realizations of a latent variable with distribution D~\u03b1, with parameter vector ~\u03b1. The probability of observing R(p, q), given the parameters, is then\nPr ( R(p, q) | ~\u03b1,B ) = \u222b Pr ( R(p, q) | ~\u03c0p, ~\u03c0q, B ) D~\u03b1(d~\u03c0). (11)\nA4\u2014Sampling Scheme Level\nAssume that the M independent replications of the relations measured on the population of nodes are independent of one another. The probability of observing the whole collection of graphs, R1:M , given the\nparameters, is then given by the following equation.\nPr ( R1:M | ~\u03b1,B ) = M\u220f m=1 N\u220f p,q=1 Pr ( Rm(p, q) | ~\u03b1,B ) . (12)\nFull model specifications immediately adapt to the different kinds of data, e.g., multiple data types through the choice of f , or parametric or semi-parametric specifications of the prior on the number of clusters through the choice of D\u03b1."}, {"heading": "B Details of the Variational Approximation", "text": "Here we present more details about the derivation of the variational EM algorithm presented in Section 3. Furthermore, we address a setting where M replicates are available about the paired measurements,\nG1:M = (N,R1:M ), and relations Rm(p, q) take values into an arbitrary metric space according to f (\nRm(p, q) | .. ) . An extension of the inference algorithm to address the case or multivariate relations, say J-dimensional, and multiple blockmodels B1:J each corresponding to a distinct relational response, can be derived with minor modifications of the derivations that follow.\nB.1 Variational Expectation-Maximization\nWe begin by briefly summarizing the general strategy we intend to use. The approximate variant of EM we describe here is often referred to as Variational EM (Beal and Ghahramani, 2003). We begin by rewriting Y = R for the data, X = (~\u03c01:N , Z\u2192, Z\u2190) for the latent variables, and \u0398 = (~\u03b1,B) for the model\u2019s parameters. Briefly, it is possible to lower bound the likelihood, p(Y |\u0398), making use of Jensen\u2019s inequality\nand of any distribution on the latent variables q(X),\np(Y |\u0398) = log \u222b X p(Y,X|\u0398) dX\n= log \u222b X q(X) p(Y,X|\u0398) q(X) dX (for any q)\n\u2265 \u222b X q(X) log p(Y,X|\u0398) q(X) dX (Jensen\u2019s)\n= Eq [ log p(Y,X|\u0398)\u2212 log q(X) ] =: L(q,\u0398) (13)\nIn EM, the lower bound L(q,\u0398) is then iteratively maximized with respect to \u0398, in the M step, and q in the E step (Dempster et al., 1977). In particular, at the t-th iteration of the E step we set\nq(t) = p(X|Y,\u0398(t\u22121)), (14)\nthat is, equal to the posterior distribution of the latent variables given the data and the estimates of the parameters at the previous iteration.\nUnfortunately, we cannot compute the posterior in Equation 14 for the admixture of latent blocks model. Rather, we define a direct parametric approximation to it, q\u0303 = q\u2206(X), which involves an extra set of variational parameters, \u2206, and entails an approximate lower bound for the likelihood L\u2206(q,\u0398). At the t-th iteration of the E step, we then minimize the Kullback-Leibler divergence between q(t) and q(t)\u2206 , with respect to \u2206, using the data.7 The optimal parametric approximation is, in fact, a proper posterior as it depends on the data Y , although indirectly, q(t) \u2248 q(t)\u2206\u2217(Y )(X) = p(X|Y ).\nB.2 Lower Bound for the Likelihood\nAccording to the mean-field theory (Jordan et al., 1999; Xing et al., 2003), one can approximate an intractable distribution such as the one defined by Equation (1) by a fully factored distribution q(~\u03c01:N , Z\u21921:M , Z \u2190 1:M )\n7This is equivalent to maximizing the approximate lower bound for the likelihood, L\u2206(q,\u0398), with respect to \u2206.\ndefined as follows:\nq(~\u03c01:N , Z\u21921:M , Z \u2190 1:M |~\u03b31:N ,\u03a6\u21921:M ,\u03a6\u21901:M )\n= \u220f p q1(~\u03c0p|~\u03b3p) \u220f m \u220f p,q ( q2(~zmp\u2192q|~\u03c6mp\u2192q, 1) q2(~zmp\u2190q|~\u03c6mp\u2190q, 1) ) , (15)\nwhere q1 is a Dirichlet, q2 is a multinomial, and \u2206 = (~\u03b31:N ,\u03a6\u21921:M ,\u03a6 \u2190 1:M ) represent the set of free variational parameters need to be estimated in the approximate distribution.\nMinimizing the Kulback-Leibler divergence between this q(~\u03c01:N , Z\u21921:M , Z \u2190 1:M |\u2206) and the original p(~\u03c01:N , Z\u21921:M , Z\u21901:M\ndefined by Equation (1) leads to the following approximate lower bound for the likelihood.\nL\u2206(q,\u0398) = Eq [ log \u220f m \u220f p,q p1(Rm(p, q)|~zmp\u2192q, ~zmp\u2190q, B) ]\n+ Eq [ log \u220f m \u220f p,q p2(~zmp\u2192q|~\u03c0p, 1) ] +Eq [ log \u220f m \u220f p,q p2(~zmp\u2190q|~\u03c0q, 1) ]\n+ Eq [ log \u220f p p3(~\u03c0p|~\u03b1) ] \u2212Eq [ \u220f p q1(~\u03c0p|~\u03b3p) ]\n\u2212 Eq [ log \u220f m \u220f p,q q2(~zmp\u2192q|~\u03c6mp\u2192q, 1) ] \u2212Eq [ log \u220f m \u220f p,q q2(~zmp\u2190q|~\u03c6mp\u2190q, 1) ] .\nWorking on the single expectations leads to\nL\u2206(q,\u0398) = \u2211 m \u2211 p,q \u2211 g,h \u03c6mp\u2192q,g\u03c6 m p\u2190q,h \u00b7 f ( Rm(p, q), B(g, h) ) +\n\u2211 m \u2211 p,q \u2211 g \u03c6mp\u2192q,g [ \u03c8(\u03b3p,g)\u2212 \u03c8( \u2211 g \u03b3p,g) ]\n+ \u2211 m \u2211 p,q \u2211 h \u03c6mp\u2190q,h [ \u03c8(\u03b3p,h)\u2212 \u03c8( \u2211 h \u03b3p,h) ]\n+ \u2211\np\nlog \u0393( \u2211\nk \u03b1k)\u2212 \u2211 p,k log \u0393(\u03b1k) + \u2211 p,k (\u03b1k \u2212 1) [ \u03c8(\u03b3p,k)\u2212 \u03c8( \u2211 k \u03b3p,k) ]\n\u2212 \u2211\np\nlog \u0393( \u2211\nk \u03b3p,k) + \u2211 p,k log \u0393(\u03b3p,k)\u2212 \u2211 p,k (\u03b3p,k \u2212 1) [ \u03c8(\u03b3p,k)\u2212 \u03c8( \u2211 k \u03b3p,k) ]\n\u2212 \u2211 m \u2211 p,q \u2211 g \u03c6mp\u2192q,g log \u03c6 m p\u2192q,g \u2212 \u2211 m \u2211 p,q \u2211 h \u03c6mp\u2190q,h log \u03c6 m p\u2190q,h\nwhere\nf ( Rm(p, q), B(g, h) ) = Rm(p, q) logB(g, h)+ ( 1\u2212Rm(p, q) ) log ( 1\u2212B(g, h) ) ;\nm runs over 1, . . . ,M ; p, q run over 1, . . . , N ; g, h, k run over 1, . . . ,K; and \u03c8(x) is the derivative of the log-gamma function, d log \u0393(x)dx .\nB.3 The Expected Value of the Log of a Dirichlet Random Vector The computation of the lower bound for the likelihood requires us to evaluate Eq [ log ~\u03c0p ] for p = 1, . . . , N . Recall that the density of an exponential family distribution with natural parameter ~\u03b8 can be written as\np(x|\u03b1) = h(x) \u00b7 c(\u03b1) \u00b7 exp {\u2211\nk\n\u03b8k(\u03b1) \u00b7 tk(x) }\n= h(x) \u00b7 exp {\u2211\nk\n\u03b8k(\u03b1) \u00b7 tk(x)\u2212 log c(\u03b1) } .\nOmitting the node index p for convenience, we can rewrite the density of the Dirichlet distribution p3 as an exponential family distribution,\np3(~\u03c0|~\u03b1) = exp {\u2211\nk\n(\u03b1k \u2212 1) log(\u03c0k)\u2212 log \u220f k \u0393(\u03b1k) \u0393( \u2211\nk \u03b1k)\n} ,\nwith natural parameters \u03b8k(~\u03b1) = (\u03b1k \u2212 1) and natural sufficient statistics tk(~\u03c0) = log(\u03c0k). Let c\u2032(~\u03b8) = c(\u03b11(~\u03b8), . . . , \u03b1K(~\u03b8)); using a well known property of the exponential family distributions Schervish (1995) we find that\nEq [ log \u03c0k ] = E~\u03b8 [ log tk(x) ] = \u03c8 ( \u03b1k ) \u2212\u03c8 (\u2211 k \u03b1k ) ,\nwhere \u03c8(x) is the derivative of the log-gamma function, d log \u0393(x)dx .\nB.4 Variational E Step\nThe approximate lower bound for the likelihood L\u2206(q,\u0398) can be maximized using exponential family arguments and coordinate ascent Wainwright and Jordan (2003).\nIsolating terms containing \u03c6mp\u2192q,g and \u03c6 m p\u2190q,h we obtain L\u03c6mp\u2192q,g(q,\u0398) and L\u03c6mp\u2192q,g(q,\u0398). The natural\nparameters ~gmp\u2192q and ~g m p\u2190q corresponding to the natural sufficient statistics log(~z m p\u2192q) and log(~z m p\u2190q) are functions of the other latent variables and the observations. We find that\ngmp\u2192q,g = log \u03c0p,g + \u2211 h zmp\u2190q,h \u00b7 f ( Rm(p, q), B(g, h) ) ,\ngmp\u2190q,h = log \u03c0q,h + \u2211 g zmp\u2192q,g \u00b7 f ( Rm(p, q), B(g, h) ) ,\nfor all pairs of nodes (p, q) in the m-th network; where g, h = 1, . . . ,K, and\nf ( Rm(p, q), B(g, h) ) = Rm(p, q) logB(g, h)+ ( 1\u2212Rm(p, q) ) log ( 1\u2212B(g, h) ) .\nThis leads to the following updates for the variational parameters (~\u03c6mp\u2192q, ~\u03c6 m p\u2190q), for a pair of nodes (p, q) in the m-th network:\n\u03c6\u0302mp\u2192q,g \u221d e Eq [ gmp\u2192q,g ] (16)\n= e Eq [ log \u03c0p,g ] \u00b7 e P h \u03c6 m p\u2190q,h\u00b7 Eq [ f ( Rm(p,q),B(g,h) )] = e Eq [ log \u03c0p,g\n] \u00b7 \u220f h ( B(g, h)Rm(p,q)\u00b7 ( 1\u2212B(g, h) )1\u2212Rm(p,q))\u03c6mp\u2190q,h , \u03c6\u0302mp\u2190q,h \u221d e Eq [ gmp\u2190q,h ] (17)\n= e Eq [ log \u03c0q,h ] \u00b7 e P g \u03c6 m p\u2192q,g \u00b7 Eq [ f ( Rm(p,q),B(g,h) )] = e Eq [ log \u03c0q,h\n] \u00b7 \u220f g ( B(g, h)Rm(p,q)\u00b7 ( 1\u2212B(g, h) )1\u2212Rm(p,q))\u03c6mp\u2192q,g , for g, h = 1, . . . ,K. These estimates of the parameters underlying the distribution of the nodes\u2019 group indicators ~\u03c6mp\u2192q and ~\u03c6 m p\u2190q need be normalized, to make sure \u2211 k \u03c6 m p\u2192q,k = \u2211 k \u03c6 m p\u2190q,k = 1.\nIsolating terms containing \u03b3p,k we obtain L\u03b3p,k(q,\u0398). Setting \u2202L\u03b3p,k \u2202\u03b3p,k equal to zero and solving for \u03b3p,k\nyields:\n\u03b3\u0302p,k = \u03b1k + \u2211 m \u2211 q \u03c6mp\u2192q,k + \u2211 m \u2211 q \u03c6mp\u2190q,k, (18)\nfor all nodes p \u2208 P and k = 1, . . . ,K.\nThe t-th iteration of the variational E step is carried out for fixed values of \u0398(t\u22121) = (~\u03b1(t\u22121), B(t\u22121)),\nand finds the optimal approximate lower bound for the likelihood L\u2206\u2217(q,\u0398(t\u22121)).\nB.5 Variational M Step\nThe optimal lower bound L\u2206\u2217(q(t\u22121),\u0398) provides a tractable surrogate for the likelihood at the t-th iteration of the variational M step. We derive empirical Bayes estimates for the hyper-parameters \u0398 that are based upon it.8 That is, we maximize L\u2206\u2217(q(t\u22121),\u0398) with respect to \u0398, given expected sufficient statistics computed using L\u2206\u2217(q(t\u22121),\u0398(t\u22121)).\nIsolating terms containing ~\u03b1 we obtain L~\u03b1(q,\u0398). Unfortunately, a closed form solution for the approximate maximum likelihood estimate of ~\u03b1 does not exist Blei et al. (2003). We can produce a Newton-Raphson method that is linear in time, where the gradient and Hessian for the bound L~\u03b1 are\n\u2202L~\u03b1 \u2202\u03b1k\n= N ( \u03c8 (\u2211\nk\n\u03b1k ) \u2212\u03c8(\u03b1k) ) + \u2211 p ( \u03c8(\u03b3p,k)\u2212 \u03c8 (\u2211 k \u03b3p,k )) ,\n\u2202L~\u03b1 \u2202\u03b1k1\u03b1k2\n= N (\nI(k1=k2) \u00b7 \u03c8 \u2032(\u03b1k1)\u2212 \u03c8\u2032 (\u2211 k \u03b1k )) .\nIsolating terms containing B we obtain LB , whose approximate maximum is\nB\u0302(g, h) = 1 M \u2211 m ( \u2211 p,q Rm(p, q) \u00b7 \u03c6mp\u2192qg \u03c6mp\u2190qh\u2211 p,q \u03c6 m p\u2192qg \u03c6 m p\u2190qh ) , (19)\nfor every index pair (g, h) \u2208 [1,K]\u00d7 [1,K].\nIn Section 2.1 we introduced an extra parameter, \u03c1, to control the relative importance of presence and absence of interactions in likelihood, i.e., the score that informs inference and estimation. Isolating terms 8We could term these estimates pseudo empirical Bayes estimates, since they maximize an approximate lower bound for the likelihood, L\u2206\u2217 .\ncontaining \u03c1 we obtain L\u03c1. We may then estimate the sparsity parameter \u03c1 by\n\u03c1\u0302 = 1 M \u2211 m\n( \u2211 p,q ( 1\u2212Rm(p, q) ) \u00b7 ( \u2211 g,h \u03c6 m p\u2192qg \u03c6 m p\u2190qh )\u2211 p,q \u2211 g,h \u03c6 m p\u2192qg \u03c6 m p\u2190qh ) . (20)\nAlternatively, we can fix \u03c1 prior to the analysis; the density of the interaction matrix is estimated with d\u0302 = \u2211\nm,p,q Rm(p, q)/(N 2M), and the sparsity parameter is set to \u03c1\u0303 = (1 \u2212 d\u0302). This latter estimator\nattributes all the information in the non-interactions to the point mass, i.e., to latent sources other than the block model B or the mixed membership vectors ~\u03c01:N . It does, however, provide a quick recipe to reduce the computational burden during exploratory analyses.9\n9Note that \u03c1\u0303 = \u03c1\u0302 in the case of single membership. In fact, that implies \u03c6mp\u2192qg = \u03c6mp\u2190qh = 1 for some (g, h) pair, for any (p, q) pair."}], "references": [{"title": "A latent mixed-membership model for relational data", "author": ["E.M. Airoldi", "D.M. Blei", "E.P. Xing", "S.E. Fienberg"], "venue": "In ACM SIGKDD Workshop on Link Discovery: Issues, Approaches and Applications,", "citeRegEx": "Airoldi et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Airoldi et al\\.", "year": 2005}, {"title": "Discovering latent patterns with hierarchical Bayesian mixed-membership models and the issue of model choice", "author": ["E.M. Airoldi", "S.E. Fienberg", "C. Joutard", "T.M. Love"], "venue": "Technical Report CMU-ML-06-101,", "citeRegEx": "Airoldi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Airoldi et al\\.", "year": 2006}, {"title": "Biological context analysis of gene expression data", "author": ["E.M. Airoldi", "S.E. Fienberg", "E.P. Xing"], "venue": null, "citeRegEx": "Airoldi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Airoldi et al\\.", "year": 2006}, {"title": "Gene ontology: Tool for the unification of biology. The gene ontology consortium", "author": ["M. Ashburner", "C.A. Ball", "J.A. Blake", "D. Botstein", "H. Butler", "J.M. Cherry", "A.P. Davis", "K. Dolinski", "S.S. Dwight", "J.T. Eppig", "M.A. Harris", "D.P. Hill", "L. Issel-Tarver", "A. Kasarskis", "S. Lewis", "J.C. Matese", "J.E. Richardson", "M. Ringwald", "G.M. Rubinand", "G. Sherlock"], "venue": "Nature Genetics,", "citeRegEx": "Ashburner et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ashburner et al\\.", "year": 2000}, {"title": "The variational Bayesian EM algorithm for incomplete data: With application to scoring graphical model structures", "author": ["M.J. Beal", "Z. Ghahramani"], "venue": "Bayesian Statistics,", "citeRegEx": "Beal and Ghahramani.,? \\Q2003\\E", "shortCiteRegEx": "Beal and Ghahramani.", "year": 2003}, {"title": "Black/white differences in health status and mortality among the elderly", "author": ["L. Berkman", "B.H. Singer", "K. Manton"], "venue": "Demography,", "citeRegEx": "Berkman et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Berkman et al\\.", "year": 1989}, {"title": "VIBES: A variational inference engine for Bayesian networks", "author": ["C. Bishop", "D. Spiegelhalter", "J. Winn"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Bishop et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bishop et al\\.", "year": 2003}, {"title": "Latent Dirichlet allocation", "author": ["D.M. Blei", "A. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Charisma and Social Structure", "author": ["R.T. Bradley"], "venue": "Paragon House,", "citeRegEx": "Bradley.,? \\Q1987\\E", "shortCiteRegEx": "Bradley.", "year": 1987}, {"title": "An algorithm for clustering relational data with applications to social network analysis and comparison to multidimensional scaling", "author": ["R.L. Breiger", "S.A. Boorman", "P. Arabie"], "venue": "Journal of Mathematical Psychology,", "citeRegEx": "Breiger et al\\.,? \\Q1975\\E", "shortCiteRegEx": "Breiger et al\\.", "year": 1975}, {"title": "Discrete components analysis", "author": ["W.L. Buntine", "A. Jakulin"], "venue": "Latent Structure and Feature Selection Techniques. Springer-Verlag,", "citeRegEx": "Buntine and Jakulin.,? \\Q2006\\E", "shortCiteRegEx": "Buntine and Jakulin.", "year": 2006}, {"title": "Clearing the FOG: Fuzzy, overlapping groups for social networks", "author": ["G.B. Davis", "K.M. Carley"], "venue": null, "citeRegEx": "Davis and Carley.,? \\Q2006\\E", "shortCiteRegEx": "Davis and Carley.", "year": 2006}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Generalized Blockmodeling", "author": ["P. Doreian", "V. Batagelj", "A. Ferligoj"], "venue": null, "citeRegEx": "Doreian et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Doreian et al\\.", "year": 2004}, {"title": "Discussion of \u201cModel-based clustering for social networks", "author": ["P. Doreian", "V. Batagelj", "A. Ferligoj"], "venue": "Journal of the Royal Statistical Society, Series A,", "citeRegEx": "Doreian et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Doreian et al\\.", "year": 2007}, {"title": "Grade of membership and latent structure models with application to disability survey data", "author": ["E.A. Erosheva"], "venue": "PhD thesis,", "citeRegEx": "Erosheva.,? \\Q2002\\E", "shortCiteRegEx": "Erosheva.", "year": 2002}, {"title": "Bayesian mixed membership models for soft clustering and classification", "author": ["E.A. Erosheva", "S.E. Fienberg"], "venue": "In C. Weihs and W. Gaul, editors, Classification\u2014The Ubiquitous Challenge,", "citeRegEx": "Erosheva and Fienberg.,? \\Q2005\\E", "shortCiteRegEx": "Erosheva and Fienberg.", "year": 2005}, {"title": "Bayesian density estimation and inference using mixtures", "author": ["M. Escobar", "M. West"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Escobar and West.,? \\Q1995\\E", "shortCiteRegEx": "Escobar and West.", "year": 1995}, {"title": "Statistical analysis of multiple sociometric relations", "author": ["S.E. Fienberg", "M.M. Meyer", "S. Wasserman"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Fienberg et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Fienberg et al\\.", "year": 1985}, {"title": "Functional organization of the yeast proteome by systematic analysis of protein", "author": ["A.C. Gavin", "M. Bosche", "R. Krause", "P. Grandi", "M. Marzioch", "A. Bauer", "J. Schultz", "et. al"], "venue": "complexes. Nature,", "citeRegEx": "Gavin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gavin et al\\.", "year": 2002}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Griffiths and Steyvers.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths and Steyvers.", "year": 2004}, {"title": "Model-based clustering for social networks", "author": ["M.S. Handcock", "A.E. Raftery", "J.M. Tantrum"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Handcock et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Handcock et al\\.", "year": 2007}, {"title": "The national longitudinal study of adolescent health: research design", "author": ["K.M. Harris", "F. Florey", "J. Tabor", "P.S. Bearman", "J. Jones", "R.J. Udry"], "venue": "Technical report,", "citeRegEx": "Harris et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Harris et al\\.", "year": 2003}, {"title": "Systematic identification of protein complexes in saccharomyces cerevisiae by mass spectrometry", "author": ["Y. Ho", "A. Gruhler", "A. Heilbut", "G.D. Bader", "L. Moore", "S.L. Adams", "A. Millar", "P. Taylor", "K. Bennett", "K. Boutilier et. al"], "venue": "Nature, 415:180\u2013183,", "citeRegEx": "Ho et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2002}, {"title": "Latent space approaches to social network analysis", "author": ["P.D. Hoff", "A.E. Raftery", "M.S. Handcock"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoff et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Hoff et al\\.", "year": 2002}, {"title": "Local structure in social networks", "author": ["P.W. Holland", "S. Leinhardt"], "venue": "Sociological Methodology,", "citeRegEx": "Holland and Leinhardt.,? \\Q1975\\E", "shortCiteRegEx": "Holland and Leinhardt.", "year": 1975}, {"title": "Introduction to variational methods for graphical models", "author": ["M. Jordan", "Z. Ghahramani", "T. Jaakkola", "L. Saul"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Discovering latent classes in relational data", "author": ["C. Kemp", "T.L. Griffiths", "J.B. Tenenbaum"], "venue": "Technical Report AI Memo 2004-019,", "citeRegEx": "Kemp et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kemp et al\\.", "year": 2004}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["C. Kemp", "J.B. Tenenbaum", "T.L. Griffiths", "T. Yamada", "N. Ueda"], "venue": "In Proceedings of the 21st National Conference on Artificial Intelligence,", "citeRegEx": "Kemp et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kemp et al\\.", "year": 2006}, {"title": "Global landscape of protein complexes in the yeast", "author": ["A. Shilatifard", "E. O\u2019Shea", "J.S. Weissman", "C.J. Ingles", "T.R. Hughes", "J. Parkinson", "M. Gerstein", "S.J. Wodak", "A. Emili", "J.F. Greenblatt"], "venue": "Saccharomyces Cerevisiae. Nature,", "citeRegEx": "Shilatifard et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shilatifard et al\\.", "year": 2006}, {"title": "A Bayesian hierarchical model for learning natural scene categories", "author": ["F.-F. Li", "P. Perona"], "venue": "IEEE Computer Vision and Pattern Recognition,", "citeRegEx": "Li and Perona.,? \\Q2005\\E", "shortCiteRegEx": "Li and Perona.", "year": 2005}, {"title": "Joint group and topic discovery from relations and text", "author": ["A. McCallum", "X. Wang", "N. Mohanty"], "venue": null, "citeRegEx": "McCallum et al\\.,? \\Q1971\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 1971}, {"title": "Mips: analysis and annotation", "author": ["H.W. Mewes", "C. Amid", "R. Arnold", "D. Frishman", "U. Guldener", "et. al"], "venue": null, "citeRegEx": "Mewes et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mewes et al\\.", "year": 2007}, {"title": "Bayesian information criterion for censored survival models", "author": ["Hill", "2003. C.T. Volinsky", "A.E. Raftery"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2003}, {"title": "Logit models and logistic regression for social networks: I", "author": ["S. Wasserman", "P. Pattison"], "venue": null, "citeRegEx": "Wasserman and Pattison.,? \\Q1987\\E", "shortCiteRegEx": "Wasserman and Pattison.", "year": 1987}, {"title": "We can produce a Newton-Raphson method that is linear in time, where the gradient and Hessian for the bound", "author": ["Blei"], "venue": null, "citeRegEx": "Blei,? \\Q2003\\E", "shortCiteRegEx": "Blei", "year": 2003}], "referenceMentions": [{"referenceID": 7, "context": "Mixed membership models, such as latent Dirichlet allocation (Blei et al., 2003), have emerged in recent years as a flexible modeling tool for data where the single cluster assumption is violated by the heterogeneity within of a data point.", "startOffset": 61, "endOffset": 80}, {"referenceID": 7, "context": "They have been successfully applied in many domains, such as document analysis (Minka and Lafferty, 2002; Blei et al., 2003; Buntine and Jakulin, 2006), surveys (Berkman et al.", "startOffset": 79, "endOffset": 151}, {"referenceID": 10, "context": "They have been successfully applied in many domains, such as document analysis (Minka and Lafferty, 2002; Blei et al., 2003; Buntine and Jakulin, 2006), surveys (Berkman et al.", "startOffset": 79, "endOffset": 151}, {"referenceID": 5, "context": ", 2003; Buntine and Jakulin, 2006), surveys (Berkman et al., 1989; Erosheva, 2002), image processing (Li and Perona, 2005), transcriptional regulation (Airoldi et al.", "startOffset": 44, "endOffset": 82}, {"referenceID": 15, "context": ", 2003; Buntine and Jakulin, 2006), surveys (Berkman et al., 1989; Erosheva, 2002), image processing (Li and Perona, 2005), transcriptional regulation (Airoldi et al.", "startOffset": 44, "endOffset": 82}, {"referenceID": 30, "context": ", 1989; Erosheva, 2002), image processing (Li and Perona, 2005), transcriptional regulation (Airoldi et al.", "startOffset": 42, "endOffset": 63}, {"referenceID": 9, "context": "Information about these repeated, asymmetric relations can be collapsed into a square binary table that encodes the directed connections between monks (Breiger et al., 1975).", "startOffset": 151, "endOffset": 173}, {"referenceID": 23, "context": "For example, technologies for measuring interactions between pairs of proteins such as mass spectrometry (Ho et al., 2002) and tandem affinity purification (Gavin et al.", "startOffset": 105, "endOffset": 122}, {"referenceID": 19, "context": ", 2002) and tandem affinity purification (Gavin et al., 2002) return a probabilistic assessment about the presence of interactions, thus setting the range of R(p, q) to [0, 1].", "startOffset": 41, "endOffset": 61}, {"referenceID": 8, "context": ", see Sampson (1968), or those about the collection of 17 relations measured by Bradley (1987). Second, in the MMSB the data generating distribution is a Bernoulli, but B can be a matrix that parameterizes any kind of distribution.", "startOffset": 80, "endOffset": 95}, {"referenceID": 9, "context": "We consider Breiger\u2019s collation of Sampson\u2019s data (Breiger et al., 1975).", "startOffset": 50, "endOffset": 72}, {"referenceID": 21, "context": "which selects three groups, where |~ \u03b1,B| is the number of hyper-parameters in the model, and |R| is the number of positive relations observed (Volinsky and Raftery, 2000; Handcock et al., 2007).", "startOffset": 143, "endOffset": 194}, {"referenceID": 9, "context": "This allowed us to look for signal about the mixed membership of monks to factions that may have been lost in the data set prepared by Breiger et al. (1975) because of averaging.", "startOffset": 135, "endOffset": 157}, {"referenceID": 9, "context": "This finding is supported Sampson\u2019s anthropological observations, and it suggests that relevant substantive information has been lost when the graphs corresponding to multiple sociometric relations have been collapsed into a single social network (Breiger et al., 1975).", "startOffset": 247, "endOffset": 269}, {"referenceID": 26, "context": "We use empirical Bayes to estimate the parameters (~ \u03b1,B), and employ a meanfield approximation scheme (Jordan et al., 1999) for posterior inference.", "startOffset": 103, "endOffset": 124}, {"referenceID": 7, "context": "A number of approxiate inference algorithms for mixed membership models have appeared in recent years, including mean-field variational methods (Blei et al., 2003; Teh et al., 2007), expectation propagation (Minka and Lafferty, 2002), and Monte Carlo Markov chain sampling (MCMC) (Erosheva and Fienberg, 2005; Griffiths and Steyvers, 2004).", "startOffset": 144, "endOffset": 181}, {"referenceID": 16, "context": ", 2007), expectation propagation (Minka and Lafferty, 2002), and Monte Carlo Markov chain sampling (MCMC) (Erosheva and Fienberg, 2005; Griffiths and Steyvers, 2004).", "startOffset": 106, "endOffset": 165}, {"referenceID": 20, "context": ", 2007), expectation propagation (Minka and Lafferty, 2002), and Monte Carlo Markov chain sampling (MCMC) (Erosheva and Fienberg, 2005; Griffiths and Steyvers, 2004).", "startOffset": 106, "endOffset": 165}, {"referenceID": 26, "context": "Good reviews of variational methods method can be found in a number of papers (Jordan et al., 1999; Wainwright and Jordan, 2003; Xing et al., 2003; Bishop et al., 2003) The log of the marginal probability in Equation 2 can be bound with Jensen\u2019s inequality as follows,", "startOffset": 78, "endOffset": 168}, {"referenceID": 6, "context": "Good reviews of variational methods method can be found in a number of papers (Jordan et al., 1999; Wainwright and Jordan, 2003; Xing et al., 2003; Bishop et al., 2003) The log of the marginal probability in Equation 2 can be bound with Jensen\u2019s inequality as follows,", "startOffset": 78, "endOffset": 168}, {"referenceID": 6, "context": "When all the nodes in the graphical model are conjugate pairs or mixtures of conjugate pairs, we can directly write down a coordinate ascent algorithm for this optimization (Xing et al., 2003; Bishop et al., 2003).", "startOffset": 173, "endOffset": 213}, {"referenceID": 14, "context": "We compare our results with those that were recently published with a simple mixture of blocks (Doreian et al., 2007) and with a latent space model (Handcock et al.", "startOffset": 95, "endOffset": 117}, {"referenceID": 21, "context": ", 2007) and with a latent space model (Handcock et al., 2007) on the same data.", "startOffset": 38, "endOffset": 61}, {"referenceID": 22, "context": "The National Longitudinal Study of Adolescent Health is nationally representative study that explores the how social contexts such as families, friends, peers, schools, neighborhoods, and communities influence health and risk behaviors of adolescents, and their outcomes in young adulthood (Harris et al., 2003; Udry, 2003).", "startOffset": 290, "endOffset": 323}, {"referenceID": 21, "context": "Here, we analyze a friendship network among the students, at the same school that was considered by Handcock et al. (2007) and discussants.", "startOffset": 100, "endOffset": 123}, {"referenceID": 14, "context": "MMSB is the proposed mixed membership stochastic blockmodel, MSB is a simpler stochastic block mixture model (Doreian et al., 2007), and LSCM is the latent space cluster model (Handcock et al.", "startOffset": 109, "endOffset": 131}, {"referenceID": 21, "context": ", 2007), and LSCM is the latent space cluster model (Handcock et al., 2007).", "startOffset": 52, "endOffset": 75}, {"referenceID": 14, "context": "The three models are our mixed membership stochastic blockmodel (MMSB), a simpler stochastic block mixture model (Doreian et al., 2007) (MSB), and the latent space cluster model (Handcock et al.", "startOffset": 113, "endOffset": 135}, {"referenceID": 21, "context": ", 2007) (MSB), and the latent space cluster model (Handcock et al., 2007) (LSCM).", "startOffset": 50, "endOffset": 73}, {"referenceID": 19, "context": "Mass spectrometry can be used to identify components of protein complexes (Gavin et al., 2002; Ho et al., 2002).", "startOffset": 74, "endOffset": 111}, {"referenceID": 23, "context": "Mass spectrometry can be used to identify components of protein complexes (Gavin et al., 2002; Ho et al., 2002).", "startOffset": 74, "endOffset": 111}, {"referenceID": 0, "context": "In previous work, we established the usefulness of an admixture of latent blockmodels for analyzing proteinprotein interaction data (Airoldi et al., 2005).", "startOffset": 132, "endOffset": 154}, {"referenceID": 3, "context": "In a follow-up study we considered the gene ontology (GO) (Ashburner et al., 2000) as the source of functional annotations to consider as ground truth in our analyses.", "startOffset": 58, "endOffset": 82}, {"referenceID": 18, "context": "In statistics, this line of research has been extended in various contexts over the years (Fienberg et al., 1985; Wasserman and Pattison, 1996; Snijders, 2002; Hoff et al., 2002; Doreian et al., 2004).", "startOffset": 90, "endOffset": 200}, {"referenceID": 24, "context": "In statistics, this line of research has been extended in various contexts over the years (Fienberg et al., 1985; Wasserman and Pattison, 1996; Snijders, 2002; Hoff et al., 2002; Doreian et al., 2004).", "startOffset": 90, "endOffset": 200}, {"referenceID": 13, "context": "In statistics, this line of research has been extended in various contexts over the years (Fienberg et al., 1985; Wasserman and Pattison, 1996; Snijders, 2002; Hoff et al., 2002; Doreian et al., 2004).", "startOffset": 90, "endOffset": 200}, {"referenceID": 24, "context": "There is a particularly close relationship between the MMSB and the latent space models (Hoff et al., 2002; Handcock et al., 2007).", "startOffset": 88, "endOffset": 130}, {"referenceID": 21, "context": "There is a particularly close relationship between the MMSB and the latent space models (Hoff et al., 2002; Handcock et al., 2007).", "startOffset": 88, "endOffset": 130}, {"referenceID": 24, "context": "The posterior inference in latent space models (Hoff et al., 2002; Handcock et al., 2007) is carried out via MCMC sampling, while we have developed a scalable variational inference algorithm to analyze large network structures.", "startOffset": 47, "endOffset": 89}, {"referenceID": 21, "context": "The posterior inference in latent space models (Hoff et al., 2002; Handcock et al., 2007) is carried out via MCMC sampling, while we have developed a scalable variational inference algorithm to analyze large network structures.", "startOffset": 47, "endOffset": 89}, {"referenceID": 20, "context": "Modern probabilistic models for relational data analysis are rooted in the stochastic blockmodels for psychometric and sociological analysis, pioneered by Lorrain and White (1971) and by Holland and Leinhardt (1975). In statistics, this line of research has been extended in various contexts over the years (Fienberg et al.", "startOffset": 187, "endOffset": 216}, {"referenceID": 17, "context": ", 2006), rather than from a Diriclet Process (Escobar and West, 1995).", "startOffset": 45, "endOffset": 69}], "year": 2013, "abstractText": "Observations consisting of measurements on relationships for pairs of objects arise in many settings, such as protein interaction and gene regulatory networks, collections of author-recipient email, and social networks. Analyzing such data with probabilisic models can be delicate because the simple exchangeability assumptions underlying many boilerplate models no longer hold. In this paper, we describe a latent variable model of such data called the mixed membership stochastic blockmodel. This model extends blockmodels for relational data to ones which capture mixed membership latent relational structure, thus providing an object-specific low-dimensional representation. We develop a general variational inference algorithm for fast approximate posterior inference. We explore applications to social and protein interaction networks.", "creator": "LaTeX with hyperref package"}}}