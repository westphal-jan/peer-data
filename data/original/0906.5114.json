{"id": "0906.5114", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2009", "title": "Non-Parametric Bayesian Areal Linguistics", "abstract": "We describe a statistical model over linguistic areas and phylogeny.", "histories": [["v1", "Sun, 28 Jun 2009 02:32:53 GMT  (41kb,D)", "http://arxiv.org/abs/0906.5114v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hal daum\u00e9 iii"], "accepted": true, "id": "0906.5114"}, "pdf": {"name": "0906.5114.pdf", "metadata": {"source": "CRF", "title": "Non-Parametric Bayesian Areal Linguistics", "authors": ["Hal Daum\u00e9 III"], "emails": ["me@hal3.name"], "sections": [{"heading": "1 Introduction", "text": "Why are some languages more alike than others? This question is one of the most central issues in historical linguistics. Typically, one of three answers is given (Aikhenvald and Dixon, 2001; Campbell, 2006). First, the languages may be related \u201cgenetically.\u201d That is, they may have all derived from a common ancestor language. Second, the similarities may be due to chance. Some language properties are simply more common than others, which is often attributed to be mostly due to linguistic universals (Greenberg, 1963). Third, the languages may be related areally. Languages that occupy the same geographic area often exhibit similar characteristics, not due to genetic relatedness, but due to sharing. Regions (and the languages contained within them) that exhibit sharing are called linguistic areas and the features that are shared are called areal features.\nMuch is not understood or agreed upon in the field of areal linguistics. Different linguists favor different defintions of what it means to be a linguistic area (are two languages sufficient to describe an area or do you need three (Thomason, 2001; Katz, 1975)?),\nwhat areal features are (is there a linear ordering of \u201cborrowability\u201d (Katz, 1975; Curnow, 2001) or is that too prescriptive?), and what causes sharing to take place (does social status or number of speakers play a role (Thomason, 2001)?).\nIn this paper, we attempt to provide a statistical answer to some of these questions. In particular, we develop a Bayesian model of typology that allows for, but does not force, the existence of linguistic areas. Our model also allows for, but does not force, preference for some feature to be shared areally. When applied to a large typological database of linguistic features (Haspelmath et al., 2005), we find that it discovers linguistic areas that are well documented in the literature (see Campbell (2005) for an overview), and a small preference for certain features to be shared areally. This latter agrees, to a lesser degree, with some of the published hierarchies of borrowability (Curnow, 2001). Finally, we show that reconstructing language family trees is significantly aided by knowledge of areal features. We note that Warnow et al. (2005) have independently proposed a model for phonological change in Indo-European (based on the Dyen dataset (Dyen et al., 1992)) that includes notions of borrowing. Our model is different in that we (a) base our model on typological features rather than just lexical patterns and (b) we explicitly represent language areas, not just one-time borrowing phenomena."}, {"heading": "2 Background", "text": "We describe (in Section 3) a non-parametric, hierarchical Bayesian model for finding linguistic areas and areal features. In this section, we provide necessary background\u2014both linguistic and statistical\u2014 ar X iv :0 90 6. 51 14 v1 [ cs .C\nL ]\n2 8\nJu n\n20 09\nfor understanding our model."}, {"heading": "2.1 Areal Linguistics", "text": "Areal effects on linguistic typology have been studied since, at least, the late 1920s by Trubetzkoy, though the idea of tracing family trees for languages goes back to the mid 1800s and the comparative study of historical linguistics dates back, perhaps to Giraldus Cambrenis in 1194 (Campbell, In press). A recent article provides a short introduction to both the issues that surround areal linguistics, as well as an enumeration of many of the known language areas (Campbell, 2005). A fairly wide, modern treatment of the issues surrounding areal diffusion is also given by essays in a recent book edited by Aikhenvald and Dixon (2001). The essays in this book provide a good introduction to the issues in the field. Campbell (2006) provides a critical survey of these and other hypotheses relating to areal linguistics.\nThere are several issues which are basic to the study of areal linguistics (these are copied almost directly from Campbell (2006)). Must a linguistic area comprise more than two languages? Must it comprise more than one language family? Is a single trait sufficient to define an area? How \u201cnearby\u201d must languages in an area be to one another? Are some feature more easily borrowed that others?\nDespite these formal definitional issues of what constitutes a language area and areal features, most historical linguists seem to believe that areal effects play some role in the change of languages."}, {"heading": "2.1.1 Established Linguistic Areas", "text": "Below, we list some of the well-known linguistic areas; Campbell (2005) provides are more complete listing together with example areal features for these areas. For each area, we list associated languages: The Balkans: Albanian, Bulgarian, Greek, Macedonian, Rumanian and Serbo-Croatian. (Sometimes: Romani and Turkish) South Asian: Languages belonging to the Dravidian, Indo-Aryan, Munda, Tibeto-Burman families. Meso-America: Cuitlatec, Huave, Mayan, MixeZoquean, Nahua, Otomanguean, Tarascan, Tequistlatecan, Totonacan and Xincan. North-west America: Alsea, Chimakuan, Coosan, Eyak, Haida, Kalapuyan, Lower Chinook, Salishan, Takelman, Tlingit, Tsimshian and Wakashan. The Baltic: Baltic languages, Baltic German, and\nFinnic languages (especially Estonian and Livonian). (Sometimes many more are included, such as: Belorussian, Lavian, Lithuanian, Norwegian, Old Prussian, Polish, Romani, Russian, Ukranian.) Ethiopia: Afar, Amharic, Anyuak, Awngi, Beja, Ge\u2019ez, Gumuz, Janjero, Kefa, Sidamo, Somali, Tigre, Tigrinya and Wellamo.\nNeedless to say, the exact definition and extent of the actual areas is up to significant debate. Moreover, claims have been made in favor of many linguistic areas not defined above. For instance, Dixon (2001) presents arguments for several Australian linguistic areas and Matisoff (2001) defines a SouthEast Asian language area. Finally, although \u201cfolk lore\u201d is in favor of identifying a linguistic area including English, French and certain Norse languages (Norwegian, Swedish, Low Dutch, High German, etc.), there are counter-arguments to this position (Thomason, 2001) (see especially Case Study 9.8)."}, {"heading": "2.1.2 Linguistic Features", "text": "Identifying which linguistic features are most easily shared \u201careally\u201d is a long standing problem in contact linguistics. Here we briefly review some of the major claims. Much of this overview is adoped from the summary given by Curnow (2001).\nHaugen (1950) considers only borrowability as far as the lexicon is concerned. He provided evidence that nouns are the easiest, followed by verbs, adjectives, adverbs, prepositions, etc. Ross (1988) corroborates Haugen\u2019s analysis and deepens it to cover morphology, syntax and phonology. He proposes the following hierarchy of borrowability (easiest items coming first): nouns > verbs > adjectives > syntax > non-bound function words > bound morphemes > phonemes. Coming from a \u201cconstraints\u201d perspective, Moravcsik (1978) suggests that: lexical items must be borrowed before lexical properties; inflected words before bound morphemes; verbal items can never be borrowed; etc.\nCurnow (2001) argues that coming up with a reasonable hierarchy of borrowability is that \u201cwe may never be able to develop such constraints.\u201d Nevertheless, he divides the space of borrowable features into 15 categories and discusses the evidence supporting each of these categories, including: phonetics (rare), phonology (common), lexical (very common), interjections and discourse markers (com-\nmon), free grammatical forms (occasional), bound grammatical forms (rare), position of morphology (rare), syntactic frames (rare), clause-internal syntax (common), between-clause syntax (occasional)."}, {"heading": "2.2 Non-parametric Bayesian Models", "text": "We treat the problem of understanding areal linguistics as a statistical question, based on a database of typological information. Due to the issues raised in the previous section, we do not want to commit to the existence of a particular number of linguistic areas, or particular sizes thereof. (Indeed, we do not even want to commit to the existence of any linguistic areas.) However, we will need to \u201cunify\u201d the languages that fall into a linguistic area (if such a thing exists) by means of some statistical parameter. Such problems have been studied under the name non-parametric models. The idea behind nonparametric models is that one does not commit a priori to a particularly number of parameters. Instead, we allow the data to dictate how many parameters there are. In Bayesian modeling, non-parametric distributions are typically used as priors; see Jordan (2005) or Ghahramani (2005) for overviews. In our model, we use two different non-parametric priors: the Pitman-Yor process (for modeling linguistic areas) and Kingman\u2019s coalescent (for modeling linguistic phylogeny), both described below."}, {"heading": "2.2.1 The Pitman-Yor Process", "text": "One particular example of a non-parametric prior is the Pitman-Yor process (Pitman and Yor, 1997), which can be seen as an extension to the betterknown Dirichlet process (Ferguson, 1974). The Pitman-Yor process can be understood as a particular example of a Chinese Restaurant process (CRP) (Pitman, 2002). The idea in all CRPs is that there exists a restaurant with an infinite number of tables. Customers come into the restaurant and have to choose a table at which to sit.\nThe Pitman-Yor process is described by three parameters: a base rate \u03b1, a discount parameter d and a mean distribution G0. These combine to describe a process denoted by PY(\u03b1, d,G0). The parameters \u03b1 and d must satisfy: 0 \u2264 d < 1 and \u03b1 > \u2212d. In the CRP analogy, the model works as follows. The first customer comes in and sits at any table. After N customers have come in and seated themselves (at a total of K tables), the N th customer arrives. In\nthe Pitman-Yor process, the N th customer sits at a new table with probability proportional to \u03b1 + Kd and sits at a previously occupied table k with probability proportional to #k \u2212 d, where #k is the number of customers already seated at table k. Finally, with each table k we associate a parameter \u03b8k, with each \u03b8k drawn independently from G0. An important property of the Pitman-Yor process is that draws from it are exchangable: perhaps counterintuitively, the distribution does not care about customer order.\nThe Pitman-Yor process induces a power-law distribution on the number of singleton tables (i.e., the number of tables that have only one customer). This can be seen by noticing two things. In general, the number of singleton tables grows as O(\u03b1Nd). When d = 0, we obtain a Dirichlet process with the number of singleton tables growing as O(\u03b1 logN)."}, {"heading": "2.2.2 Kingman\u2019s Coalescent", "text": "Kingman\u2019s coalescent is a standard model in population genetics describing the common genealogy (ancestral tree) of a set of individuals (Kingman, 1982b; Kingman, 1982a). In its full form it is a distribution over the genealogy of a countable set.\nConsider the genealogy of n individuals alive at the present time t = 0. We can trace their ancestry backwards in time to the distant past t = \u2212\u221e. Assume each individual has one parent (in genetics, haploid organisms), and therefore genealogies of [n] = {1, . . . , n} form a directed forest. Kingman\u2019s n-coalescent is simply a distribution over genealogies of n individuals. To describe the Markov process in its entirety, it is sufficient to describe the jump process (i.e. the embedded, discrete-time, Markov chain over partitions) and the distribution over coalescent times. In the n-coalescent, every pair of lineages merges independently with rate 1, with parents chosen uniformly at random from the set of possible parents at the previous time step.\nThe n-coalescent has some interesting statistical properties (Kingman, 1982b; Kingman, 1982a). The marginal distribution over tree topologies is uniform and independent of the coalescent times. Secondly, it is infinitely exchangeable: given a genealogy drawn from an n-coalescent, the genealogy of any m contemporary individuals alive at time t\u2264 0 embedded within the genealogy is a draw from the m-coalescent. Thus, taking n\u2192\u221e, there is a distri-\nbution over genealogies of a countably infinite population for which the marginal distribution of the genealogy of any n individuals gives the n-coalescent. Kingman called this the coalescent.\nTeh et al. (2007) recently described efficient inference algorithms for Kingman\u2019s coalescent. They applied the coalescent to the problem of recovering linguistic phylogenies. The application was largely successful\u2014at least in comparison to alternative algorithms that use the same data-. Unfortunately, even in the results they present, one can see significant areal effects. For instance, in their Figure(3a), Romanian is very near Albanian and Bulgarian. This is likely an areal effect: specifically, an effect due to the Balkan langauge area. We will revisit this issue in our own experiments."}, {"heading": "3 A Bayesian Model for Areal Linguistics", "text": "We will consider a data set consisting of N languages and F typological features. We denote the value of feature f in language n as Xn,f . For simplicity of exposition, we will assume two things: (1) there is no unobserved data and (2) all features are binary. In practice, for the data we use (described in Section 4), neither of these is true. However, both extensions are straightforward.\nWhen we construct our model, we attempt to be as neutral to the \u201careal linguistics\u201d questions defined in Section 2.1 as possible. We allow areas with only two languages (though for brevity we do not present them in the results). We allow areas with only one family (though, again, do not present them). We are generous with our notion of locality, allowing a radius of 1000 kilometers (though see Section 5.4 for an analysis of the effect of radius).1 And we allow, but do not enforce trait weights. All of this is accomplished through the construction of the model and the choice of the model hyperparameters.\nAt a high-level, our model works as follows. Values Xn,f appear for one of two reasons: they are either areally derived or genetically derived. A latent variable Zn,f determines this. If it is derived areally, then the value Xn,f is drawn from a latent variable\n1An reader might worry about exchangeability: Our method of making language centers and locations part of the Pitman-Yor distribution ensures this is not an issue. An alternative would be to use a location-sensitive process such as the kernel stickbreaking process (Dunson and Park, 2007), though we do not explore that here.\ncorresponding to the value preferences in the langauge area to which language n belongs. If it is derived genetically, thenXn,f is drawn from a variable corresponding to value preferences for the genetic substrate to which language n belongs. The set of areas, and the area to which a language belongs are given by yet more latent variables. It is this aspect of the model for which we use the Pitman-Yor process: languages are customers, areas are tables and area value preferences are the parameters of the tables."}, {"heading": "3.1 The formal model", "text": "We assume that the value a feature takes for a particular language (i.e., the value of Xn,f ) can be explained either genetically or areally.2 We denote this by a binary indicator variable Zn,f , where a value 1 means \u201careal\u201d and a value 0 means \u201cgenetic.\u201d We assume that each Zn,f is drawn from a feature-specific binomial parameter \u03c0f . By having the parameter feature-specific, we express the fact that some features may be more or less likely to be shared than others. In other words, a high value of \u03c0f would mean that feature f is easily shared areally, while a low value would mean that feature f is hard to share. Each language n has a known latitude/longitude `n.\nWe further assume that there are K linguistic areas, whereK is treated non-parametrically by means of the Pitman-Yor process. Note that in our context, a linguistic area may contain only one language, which would technically not be allowed according to the linguistic definition. When a language belongs to a singleton area, we interpret this to mean that it does not belong to any language area.\nEach language area k (including the singleton areas) has a set of F associated parameters \u03c6k,f , where \u03c6k,f is the probability that feature f is \u201con\u201d in area k. It also has a \u201ccentral location\u201d given by a longitude and latitude denoted ck. We only allow languages to belong to areas that fall within a given radius R of them (distances computed according to geodesic distance). This accounts for the \u201cgeographical\u201d constraints on language areas. We denote the area to which language n belongs as an.\nWe assume that each language belongs to a \u201cfamily tree.\u201d We denote the parent of language n in the\n2As mentioned in the introduction, (at least) one more option is possible: chance. We treat \u201cchance\u201d as noise and model it in the data generation process, not as an alternative \u201csource.\u201d\nfamily tree by pn. We associate with each node i in the family tree and each feature f a parameter \u03b8i,f . As in the areal case, \u03b8i,f is the probability that feature f is on for languages that descend from node i in the family tree. We model genetic trees by Kingman\u2019s coalescent with binomial mutation.\nFinally, we put non-informative priors on all the hyperparameters. Written hierarchically, our model has the following shown in Figure 1. There, by (p, \u03b8) \u223c Coalescent(\u03c00,m0), we mean that the tree and parameters are given by a coalescent."}, {"heading": "3.2 Inference", "text": "Inference in our model is mostly by Gibbs sampling. Most of the distributions used are conjugate, so Gibbs sampling can be implemented efficiently. The only exceptions are: (1) the coalescent for which we use the GreedyRate1 algorithm described by Teh et al. (2007); (2) the area centers c, for which we using a Metropolis-Hastings step. Our proposal distribution is a Gaussian centered at the previous center, with standard deviation of 5. Experimentally, this resulted in an acceptance rate of about 50%.\nIn our implementation, we analytically integrate out \u03c0 and \u03c6 and sample only over Z, the coalescent tree, and the area assignments. In some of our experiments, we treat the family tree as given. In this case, we also analytically integrate out the \u03b8 parameters and sample only over Z and area assignments."}, {"heading": "4 Typological Data", "text": "The database on which we perform our analysis is the World Atlas of Language Structures (henceforth, WALS) (Haspelmath et al., 2005). The database contains information about 2150 languages (sampled from across the world). There are 139 typological features in this database. The database is sparse: only 16% of the possible language/feature pairs are known. We use the version extracted and prepro-\ncessed by Daume\u0301 III and Campbell (2007). In WALS, languages a grouped into 38 language families (including Indo-European, Afro-Asiatic, Austronesian, Niger-Congo, etc.). Each of these language families is grouped into a number of language geni. The Indo-European family includes ten geni, including: Germanic, Romance, Indic and Slavic. The Austronesian family includes seventeen geni, including: Borneo, Oceanic, Palauan and Sundic. Overall, there are 275 geni represented in WALS.\nWe further preprocess the data as follows. For the Indo-European subset (hence-forth, \u201cIE\u201d), we remove all languages with \u2264 10 known features and then remove all features that appear in at most 1/4 of the languages. This leads to 73 languages and 87 features. For the whole-world subset, we remove languages with \u2264 25 known features and then features that appear in at most 1/10 of the languages. This leads to 349 languages and 129 features. 5 Experiments"}, {"heading": "5.1 Identifying Language Areas", "text": "Our first experiment is aimed at discovering language areas. We first focus on the IE family, and then extend the analysis to all languages. In both cases, we use a known family tree (for the IE experiment, we use a tree given by the language genus structure; for the whole-world experiment, we use a tree given by the language family structure). We run each experiment with five random restarts and 2000 iterations. We select the MAP configuration from the combination of these runs.\nIn the IE experiment, the model identified the areas shown in Figure 5.1. The best area identified by our model is the second one listed, which clearly correlates highly with the Balkans. There are two areas identified by our model (the first and last) that include only Indic and Iranian languages. While we are not aware of previous studies of these as linguistic areas, they are not implausible given\n(Indic) Bhojpuri, Darai, Gujarati, Hindi, Kalami, Kashmiri, Kumauni, Nepali, Panjabi, Shekhawati, Sindhi (Iranian) Ormuri, Pashto (Albanian) Albanian (Greek) Greek (Modern) (Indic) Romani (Kalderash) (Romance) Romanian, Romansch (Scharans), Romansch (Sursilvan), Sardinian (Slavic) Bulgarian, Macedonian, Serbian-Croatian, Slovak, Slovene, Sorbian (Baltic) Latvian, Lithuanian (Germanic) Danish, Swedish (Slavic) Polish, Russian (Celtic) Irish (Germanic) English, German, Norwegian (Romance) French (Indic) Prasuni, Urdu (Iranian) Persian, Tajik Plus 46 non-areal languages\nFigure 2: IE areas identified. Areas that consist of just one genus are not listed, nor are areas with two languages.\n(Mayan) Huastec, Jakaltek, Mam, Tzutujil (Mixe-Zoque) Zoque (Copainala\u0301) (Oto-Manguean) Mixtec (Chalcatongo), Otom\u0131\u0301 (Mezquital) (Uto-Aztecan) Nahualtl (Tetelcingo), Pipil (Baltic) Latvian, Lithuanian (Finnic) Estonian, Finnish (Slavic) Polish, Russian, Ukranian (Austro-Asiatic) Khasi (Dravidian) Telugu (IE) Bengali (Sino-Tibetan) Bawm, Garo, Newari (Kathmandu)\nFigure 3: A small subset of the world areas identified.\nthe history of the region. The fourth area identified by our model corresponds roughly to the debated \u201cEnglish\u201d area. Our area includes the requisite French/English/German/Norwegian group, as well as the somewhat surprising Irish. However, in addition to being intuitively plausible, it is not hard to find evidence in the literature for the contact relationship between English and Irish (Sommerfelt, 1960).\nIn the whole-world experiment, the model identified too many linguistic areas to fit (39 in total that contained at least two languages, and contained at least two language families). In Figure 5.1, we depict the areas found by our model that best correspond to the areas described in Section 2.1.1. We acknowledge that this gives a warped sense of the quality of our model. Nevertheless, our model is able to identify large parts of the the Meso-American area, the Baltic area and the South Asian area. (It also finds the Balkans, but since these languages are all IE, we do not consider it a linguistic area in this evaluation.) While our model does find areas that match Meso-American and North-west American areas, neither is represented in its entirety (according to the definition of these areas given in Sec-\ntion 2.1.1). Despite the difficulty humans have in assigning linguistic areas, In Table 1, we explicitly compare the quality of the areal clusters found on the IE subset. We compare against the most inclusive areal lists from Section 2.1.1 for IE: the Balkans and the Baltic. When there is overlap (eg., Romani appears in both lists), we assigned it to the Balkans.\nWe compare our model with a flat Pitman-Yor model that does not use the hierarchy. We also compare to a baseline K-means algorithm. For Kmeans, we ran with K \u2208 {5, 10, 15, . . . , 80, 85} and chose the value of K for each metric that did best (giving an unfair advantage). Clustering performance is measured on the Indo-European task according to the Rand Index, F-score, Normalized Edit Score (Pantel, 2003) and Normalized Variation of Information (Meila, 2003). In these results, we see that the Pitman-Yor process model dominates the K-means model and the Areal model dominates the Pitman-Yor model."}, {"heading": "5.2 Identifying Areal Features", "text": "Our second experiment is an analysis of the features that tend to be shared areally (as opposed to genetically). For this experiment, we make use of the whole-world version of the data, again with known language family structure. We initialize a Gibbs sampler from the MAP configuration found in Section 5.1. We run the sampler for 1000 iterations and take samples every ten steps.\nFrom one particular sample, we can estimate a posterior distribution over each \u03c0f . Due to conjugacy, we obtain a posterior distribution of \u03c0f \u223c Bet(1 + \u2211 n Zn,f , 1 + \u2211 n[1\u2212Zn,f ]). The 1s come from the prior. From this Beta distribution, we can ask the question: what is the probability that a value of \u03c0f drawn from this distribution will have value < 0.5? If this value is high, then the feature is likely\nto be a \u201cgenetic feature\u201d; if it is low, then the feature is likely to be an \u201careal feature.\u201d We average these probabilities across all 100 samples.\nThe features that are most likely to be areal according to our model are summaries in Table 2. In this table, we list the categories to which each feature belongs, together with the number of features in that category, and the average probability that a feature in that category is genetically transmitted. Apparently, the vast majority of features are not areal.\nWe can treat the results presented in Table 2 as a hierarchy of borrowability. In doing so, we see that our hierarchy agrees to a large degree with the hierarchies summarized in Section 2.1.2. Indeed, (aside from \u201cTea\u201d, which we will ignore) the two most easily shared categories according to our model are phonology and the lexicon; this is in total agreement with the agreed state of affairs in linguistics.\nLower in our list, we see that noun-related categories tend to precede their verb-related counterparts (nominal categories before verbal categores, nominal syntax before complex sentences). According to Curnow (2001), the most difficult features to borrow are phonetics (for which we have no data), bound grammatical forms (which appear low on our list), morphology (which is 99% genetic, according to our model) and syntactic frames (which would roughly correspond to \u201ccomplex sentences\u201d, another\nitem which is 99% genetic in our model)."}, {"heading": "5.3 Genetic Reconstruction", "text": "In this section, we investigate whether the use of areal knowledge can improve the automatic reconstruction of language family trees. We use Kingman\u2019s coalescent (see Section 2.2.2) as a probabilistic model of trees, endowed with a binomial mutation process on the language features.\nOur baseline model is to run the vanilla coalescent on the WALS data, effective reproducing the results presented by Teh et al. (2007). This method was already shown to outperform competing hierarchical clustering algorithms such as average-link agglomerative clustering (see, eg., Duda and Hart (1973)) and the Bayesian Hierarchical Clustering algorithm (Heller and Ghahramani, 2005).\nWe run the same experiment both on the IE subset of data and on the whole-world subset. We evaluate the results qualitatively, by observing the trees found (on the IE subset) and quantitatively (below). For the qualitative analysis, we show the subset of IE that does not contain Indic languages or Iranian languages (just to keep the figures small). The tree derived from the original data is on the left in Figure 4, below: The tree based on areal information is on the right in Figure 4, below. As we can see, the use of areal information qualitatively improves the structure of the tree. Where the original tree had a number of errors with respect to Romance and Germanic languages, these are sorted out in the areally-aware tree. Moreover, Greek now appears in a more appropriate part of the tree and English appears on a branch that is further out from the Norse languages.\nWe perform two varieties of quantitative analysis. In the first, we attempt to predict unknown feature values. In particular, we hide an addition 10% of the feature values in the WALS data and fit a model\nto the remaining 90%. We then use that model to predict the hidden 10%. The baseline model is to make predictions according to the family tree. The augmented model is to make predictions according to the family tree for those features identified as genetic and according to the linguistic area for those features identified as areal. For both settings, we compute both the absolute accuracy as well as the log probability of the hidden data under the model (the latter is less noisy). We repeat this experiment 10 times with a different random 10% hidden. The results are shown in Table 3, below. The differences are not large, but are outside one standard deviation.\nFor the second quantitative analysis, we use present purity scores (Heller and Ghahramani, 2005), subtree scores (the number of interior nodes with pure leaf labels, normalized) and leave-one-out log accuracies (all scores are between 0 and 1, and higher scores are better). These scores are computed against both language family and language genus as the \u201cclasses.\u201d The results are in Table 4, below. As we can see, the results are generally in favor of the Areal model (LOO Acc on IE versus Genus nonwithstanding), depending on the evaluation metric."}, {"heading": "5.4 Effect of Radius", "text": "Finally, we evaluate the effect of the radius hyperparameter on performance. Table 5 shows performance for models built with varying radii. As can be seen by purity and subtree scores, there is a \u201csweet spot\u201d around 500 to 1000 kilometers where the model seems optimal. LOO (strangely) seems to continue to improve as we allow areas to grow arbitrarily large. This is perhaps overfitting. Nevertheless, performance is robust for a range of radii."}, {"heading": "6 Discussion", "text": "We presented a model that is able to recover wellknown linguistic areas. Using this areas, we have shown improvement in the ability to recover phylogenetic trees of languages. It is important to note that despite our successes, there is much at our model does not account for: borrowing is known to be assymetric; contact is temporal; borrowing must obey univeral implications. Despite the failure of our model to account for these issues, however, it appears largely successful. Moreover, like any \u201cdata mining\u201d expedition, our model suggests new linguistic areas (particularly in the \u201cwhole world\u201d experiments) that deserve consideration."}, {"heading": "Acknowledgments", "text": "Deep thanks to Lyle Campbell, Yee Whye Teh and Eric Xing for discussions; comments from the three anonymous reviewers were very helpful. This work was partially supported by NSF grant IIS0712764."}], "references": [{"title": "Areal diffusion and genetic inheritance: problems in comparative linguistics", "author": ["Alexandra Aikhenvald", "R.M.W. Dixon", "editors"], "venue": null, "citeRegEx": "Aikhenvald et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Aikhenvald et al\\.", "year": 2001}, {"title": "Areal linguistics", "author": ["Lyle Campbell."], "venue": "Keith Brown, editor, Encyclopedia of Language and Linguistics. Elsevier, 2 edition.", "citeRegEx": "Campbell.,? 2005", "shortCiteRegEx": "Campbell.", "year": 2005}, {"title": "Areal linguistics: the problem to the answer", "author": ["Lyle Campbell."], "venue": "April McMahon, Nigel Vincent, and Yaron Matras, editors, Language contact and areal linguistics.", "citeRegEx": "Campbell.,? 2006", "shortCiteRegEx": "Campbell.", "year": 2006}, {"title": "What language features can be \u201dborrowed\u201d? In Aikhenvald and Dixon, editors, Areal diffusion and genetic inheritance: problems in comparative linguistics, pages 412\u2013436", "author": ["Timothy Curnow."], "venue": "Oxford University Press.", "citeRegEx": "Curnow.,? 2001", "shortCiteRegEx": "Curnow.", "year": 2001}, {"title": "A Bayesian model for discovering typological implications", "author": ["Hal Daum\u00e9 III", "Lyle Campbell."], "venue": "Proceedings of the Conference of the Association for Computational Linguistics (ACL).", "citeRegEx": "III and Campbell.,? 2007", "shortCiteRegEx": "III and Campbell.", "year": 2007}, {"title": "The Australian linguistic area", "author": ["R.M.W. Dixon."], "venue": "Aikhenvald and Dixon, editors, Areal diffusion and genetic inheritance: problems in comparative linguistics, pages 64\u2013104. Oxford University Press.", "citeRegEx": "Dixon.,? 2001", "shortCiteRegEx": "Dixon.", "year": 2001}, {"title": "Pattern Classification And Scene Analysis", "author": ["R.O. Duda", "P.E. Hart."], "venue": "Wiley and Sons, New York.", "citeRegEx": "Duda and Hart.,? 1973", "shortCiteRegEx": "Duda and Hart.", "year": 1973}, {"title": "Kernel stick breaking processes", "author": ["David Dunson", "Ju-Hyun Park."], "venue": "Biometrika, 95:307\u2013323.", "citeRegEx": "Dunson and Park.,? 2007", "shortCiteRegEx": "Dunson and Park.", "year": 2007}, {"title": "An Indoeuropean classification: A lexicostatistical experiment", "author": ["Isidore Dyen", "Joseph Kurskal", "Paul Black."], "venue": "Transactions of the American Philosophical Society, 82(5). American Philosophical Society.", "citeRegEx": "Dyen et al\\.,? 1992", "shortCiteRegEx": "Dyen et al\\.", "year": 1992}, {"title": "Prior distributions on spaces of probability measures", "author": ["Thomas S. Ferguson."], "venue": "The Annals of Statistics, 2(4):615\u2013629, July.", "citeRegEx": "Ferguson.,? 1974", "shortCiteRegEx": "Ferguson.", "year": 1974}, {"title": "Nonparametric Bayesian methods", "author": ["Zoubin Ghahramani."], "venue": "Tutorial presented at UAI conference.", "citeRegEx": "Ghahramani.,? 2005", "shortCiteRegEx": "Ghahramani.", "year": 2005}, {"title": "The analysis of linguistic borrowing", "author": ["E. Haugen."], "venue": "Language, 26:210\u2013231.", "citeRegEx": "Haugen.,? 1950", "shortCiteRegEx": "Haugen.", "year": 1950}, {"title": "Bayesian hierarchical clustering", "author": ["Katherine Heller", "Zoubin Ghahramani."], "venue": "Proceedings of the International Conference on Machine Learning (ICML), volume 22.", "citeRegEx": "Heller and Ghahramani.,? 2005", "shortCiteRegEx": "Heller and Ghahramani.", "year": 2005}, {"title": "Dirichlet processes, Chinese restaurant processes and all that", "author": ["Michael I. Jordan."], "venue": "Tutorial presented at NIPS conference.", "citeRegEx": "Jordan.,? 2005", "shortCiteRegEx": "Jordan.", "year": 2005}, {"title": "Generative Phonologie und phonologische Sprachb\u00fcnde des Ostjakischen un Samojedischen", "author": ["Harmut Katz."], "venue": "Wilhelm Fink.", "citeRegEx": "Katz.,? 1975", "shortCiteRegEx": "Katz.", "year": 1975}, {"title": "The coalescent", "author": ["J.F.C. Kingman."], "venue": "Stochastic Processes and their Applications, 13:235\u2013248.", "citeRegEx": "Kingman.,? 1982a", "shortCiteRegEx": "Kingman.", "year": 1982}, {"title": "On the genealogy of large populations", "author": ["J.F.C. Kingman."], "venue": "Journal of Applied Probability, 19:27\u2013", "citeRegEx": "Kingman.,? 1982b", "shortCiteRegEx": "Kingman.", "year": 1982}, {"title": "Genetic versus contact relationship: prosodic diffusibility in South-East Asian languages", "author": ["James Matisoff."], "venue": "Aikhenvald and Dixon, editors, Areal diffusion and genetic inheritance: problems in comparative linguistics, pages 291\u2013327. Oxford University Press.", "citeRegEx": "Matisoff.,? 2001", "shortCiteRegEx": "Matisoff.", "year": 2001}, {"title": "Comparing clusterings", "author": ["Marina Meila."], "venue": "Proceedings of the Conference on Computational Learning Theory (COLT).", "citeRegEx": "Meila.,? 2003", "shortCiteRegEx": "Meila.", "year": 2003}, {"title": "Language contact", "author": ["E. Moravcsik."], "venue": "J.H. Greenberg, C. Ferguson, and E. Moravcsik, editors, Universals of Human Language, volume 1; Method and Theory, pages 3\u2013123. Stanford University Press.", "citeRegEx": "Moravcsik.,? 1978", "shortCiteRegEx": "Moravcsik.", "year": 1978}, {"title": "Clustering by Committee", "author": ["Patrick Pantel."], "venue": "Ph.D. thesis, University of Alberta.", "citeRegEx": "Pantel.,? 2003", "shortCiteRegEx": "Pantel.", "year": 2003}, {"title": "The two-parameter PoissonDirichlet distribution derived from a stable subordinator", "author": ["J. Pitman", "M. Yor."], "venue": "Annals of Probability, 25:855\u2013900.", "citeRegEx": "Pitman and Yor.,? 1997", "shortCiteRegEx": "Pitman and Yor.", "year": 1997}, {"title": "Combinatorial stochastic processes", "author": ["Jim Pitman."], "venue": "Technical Report 621, University of California at Berkeley. Lecture notes for St. Flour Summer School.", "citeRegEx": "Pitman.,? 2002", "shortCiteRegEx": "Pitman.", "year": 2002}, {"title": "Proto Oceanic and the Austronesian languages of western melanesia", "author": ["M.D. Ross."], "venue": "Canberra: Pacific Linguitics, Australian National University.", "citeRegEx": "Ross.,? 1988", "shortCiteRegEx": "Ross.", "year": 1988}, {"title": "External versus internal factors in the development of language", "author": ["Alf Sommerfelt."], "venue": "Norsk Tidsskrift for Sprogvidenskap, 19:296\u2013315.", "citeRegEx": "Sommerfelt.,? 1960", "shortCiteRegEx": "Sommerfelt.", "year": 1960}, {"title": "Bayesian agglomerative clustering with coalescents", "author": ["Yee Whye Teh", "Hal Daum\u00e9 III", "Daniel Roy."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Teh et al\\.,? 2007", "shortCiteRegEx": "Teh et al\\.", "year": 2007}, {"title": "Language contact: an introduction", "author": ["Sarah Thomason."], "venue": "Edinburgh University Press.", "citeRegEx": "Thomason.,? 2001", "shortCiteRegEx": "Thomason.", "year": 2001}, {"title": "A stochastic model of language evolution that incorporates homoplasy and borrowing", "author": ["T. Warnow", "S.N. Evans", "D. Ringe", "L. Nakhleh."], "venue": "Phylogenetic Methods and the Prehistory of Language. Cambridge University Press. Invited paper.", "citeRegEx": "Warnow et al\\.,? 2005", "shortCiteRegEx": "Warnow et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "Typically, one of three answers is given (Aikhenvald and Dixon, 2001; Campbell, 2006).", "startOffset": 41, "endOffset": 85}, {"referenceID": 26, "context": "Different linguists favor different defintions of what it means to be a linguistic area (are two languages sufficient to describe an area or do you need three (Thomason, 2001; Katz, 1975)?), what areal features are (is there a linear ordering of \u201cborrowability\u201d (Katz, 1975; Curnow, 2001) or is that too prescriptive?), and what causes sharing to take place (does social status or number of speakers play a role (Thomason, 2001)?).", "startOffset": 159, "endOffset": 187}, {"referenceID": 14, "context": "Different linguists favor different defintions of what it means to be a linguistic area (are two languages sufficient to describe an area or do you need three (Thomason, 2001; Katz, 1975)?), what areal features are (is there a linear ordering of \u201cborrowability\u201d (Katz, 1975; Curnow, 2001) or is that too prescriptive?), and what causes sharing to take place (does social status or number of speakers play a role (Thomason, 2001)?).", "startOffset": 159, "endOffset": 187}, {"referenceID": 14, "context": "Different linguists favor different defintions of what it means to be a linguistic area (are two languages sufficient to describe an area or do you need three (Thomason, 2001; Katz, 1975)?), what areal features are (is there a linear ordering of \u201cborrowability\u201d (Katz, 1975; Curnow, 2001) or is that too prescriptive?), and what causes sharing to take place (does social status or number of speakers play a role (Thomason, 2001)?).", "startOffset": 262, "endOffset": 288}, {"referenceID": 3, "context": "Different linguists favor different defintions of what it means to be a linguistic area (are two languages sufficient to describe an area or do you need three (Thomason, 2001; Katz, 1975)?), what areal features are (is there a linear ordering of \u201cborrowability\u201d (Katz, 1975; Curnow, 2001) or is that too prescriptive?), and what causes sharing to take place (does social status or number of speakers play a role (Thomason, 2001)?).", "startOffset": 262, "endOffset": 288}, {"referenceID": 26, "context": "Different linguists favor different defintions of what it means to be a linguistic area (are two languages sufficient to describe an area or do you need three (Thomason, 2001; Katz, 1975)?), what areal features are (is there a linear ordering of \u201cborrowability\u201d (Katz, 1975; Curnow, 2001) or is that too prescriptive?), and what causes sharing to take place (does social status or number of speakers play a role (Thomason, 2001)?).", "startOffset": 412, "endOffset": 428}, {"referenceID": 3, "context": "This latter agrees, to a lesser degree, with some of the published hierarchies of borrowability (Curnow, 2001).", "startOffset": 96, "endOffset": 110}, {"referenceID": 8, "context": "(2005) have independently proposed a model for phonological change in Indo-European (based on the Dyen dataset (Dyen et al., 1992)) that includes notions of borrowing.", "startOffset": 111, "endOffset": 130}, {"referenceID": 1, "context": "Typically, one of three answers is given (Aikhenvald and Dixon, 2001; Campbell, 2006). First, the languages may be related \u201cgenetically.\u201d That is, they may have all derived from a common ancestor language. Second, the similarities may be due to chance. Some language properties are simply more common than others, which is often attributed to be mostly due to linguistic universals (Greenberg, 1963). Third, the languages may be related areally. Languages that occupy the same geographic area often exhibit similar characteristics, not due to genetic relatedness, but due to sharing. Regions (and the languages contained within them) that exhibit sharing are called linguistic areas and the features that are shared are called areal features. Much is not understood or agreed upon in the field of areal linguistics. Different linguists favor different defintions of what it means to be a linguistic area (are two languages sufficient to describe an area or do you need three (Thomason, 2001; Katz, 1975)?), what areal features are (is there a linear ordering of \u201cborrowability\u201d (Katz, 1975; Curnow, 2001) or is that too prescriptive?), and what causes sharing to take place (does social status or number of speakers play a role (Thomason, 2001)?). In this paper, we attempt to provide a statistical answer to some of these questions. In particular, we develop a Bayesian model of typology that allows for, but does not force, the existence of linguistic areas. Our model also allows for, but does not force, preference for some feature to be shared areally. When applied to a large typological database of linguistic features (Haspelmath et al., 2005), we find that it discovers linguistic areas that are well documented in the literature (see Campbell (2005) for an overview), and a small preference for certain features to be shared areally.", "startOffset": 70, "endOffset": 1760}, {"referenceID": 1, "context": "Typically, one of three answers is given (Aikhenvald and Dixon, 2001; Campbell, 2006). First, the languages may be related \u201cgenetically.\u201d That is, they may have all derived from a common ancestor language. Second, the similarities may be due to chance. Some language properties are simply more common than others, which is often attributed to be mostly due to linguistic universals (Greenberg, 1963). Third, the languages may be related areally. Languages that occupy the same geographic area often exhibit similar characteristics, not due to genetic relatedness, but due to sharing. Regions (and the languages contained within them) that exhibit sharing are called linguistic areas and the features that are shared are called areal features. Much is not understood or agreed upon in the field of areal linguistics. Different linguists favor different defintions of what it means to be a linguistic area (are two languages sufficient to describe an area or do you need three (Thomason, 2001; Katz, 1975)?), what areal features are (is there a linear ordering of \u201cborrowability\u201d (Katz, 1975; Curnow, 2001) or is that too prescriptive?), and what causes sharing to take place (does social status or number of speakers play a role (Thomason, 2001)?). In this paper, we attempt to provide a statistical answer to some of these questions. In particular, we develop a Bayesian model of typology that allows for, but does not force, the existence of linguistic areas. Our model also allows for, but does not force, preference for some feature to be shared areally. When applied to a large typological database of linguistic features (Haspelmath et al., 2005), we find that it discovers linguistic areas that are well documented in the literature (see Campbell (2005) for an overview), and a small preference for certain features to be shared areally. This latter agrees, to a lesser degree, with some of the published hierarchies of borrowability (Curnow, 2001). Finally, we show that reconstructing language family trees is significantly aided by knowledge of areal features. We note that Warnow et al. (2005) have independently proposed a model for phonological change in Indo-European (based on the Dyen dataset (Dyen et al.", "startOffset": 70, "endOffset": 2104}, {"referenceID": 1, "context": "A recent article provides a short introduction to both the issues that surround areal linguistics, as well as an enumeration of many of the known language areas (Campbell, 2005).", "startOffset": 161, "endOffset": 177}, {"referenceID": 1, "context": "Areal effects on linguistic typology have been studied since, at least, the late 1920s by Trubetzkoy, though the idea of tracing family trees for languages goes back to the mid 1800s and the comparative study of historical linguistics dates back, perhaps to Giraldus Cambrenis in 1194 (Campbell, In press). A recent article provides a short introduction to both the issues that surround areal linguistics, as well as an enumeration of many of the known language areas (Campbell, 2005). A fairly wide, modern treatment of the issues surrounding areal diffusion is also given by essays in a recent book edited by Aikhenvald and Dixon (2001). The essays in this book provide a good introduction to the issues in the field.", "startOffset": 286, "endOffset": 639}, {"referenceID": 1, "context": "Areal effects on linguistic typology have been studied since, at least, the late 1920s by Trubetzkoy, though the idea of tracing family trees for languages goes back to the mid 1800s and the comparative study of historical linguistics dates back, perhaps to Giraldus Cambrenis in 1194 (Campbell, In press). A recent article provides a short introduction to both the issues that surround areal linguistics, as well as an enumeration of many of the known language areas (Campbell, 2005). A fairly wide, modern treatment of the issues surrounding areal diffusion is also given by essays in a recent book edited by Aikhenvald and Dixon (2001). The essays in this book provide a good introduction to the issues in the field. Campbell (2006) provides a critical survey of these and other hypotheses relating to areal linguistics.", "startOffset": 286, "endOffset": 736}, {"referenceID": 1, "context": "Areal effects on linguistic typology have been studied since, at least, the late 1920s by Trubetzkoy, though the idea of tracing family trees for languages goes back to the mid 1800s and the comparative study of historical linguistics dates back, perhaps to Giraldus Cambrenis in 1194 (Campbell, In press). A recent article provides a short introduction to both the issues that surround areal linguistics, as well as an enumeration of many of the known language areas (Campbell, 2005). A fairly wide, modern treatment of the issues surrounding areal diffusion is also given by essays in a recent book edited by Aikhenvald and Dixon (2001). The essays in this book provide a good introduction to the issues in the field. Campbell (2006) provides a critical survey of these and other hypotheses relating to areal linguistics. There are several issues which are basic to the study of areal linguistics (these are copied almost directly from Campbell (2006)).", "startOffset": 286, "endOffset": 954}, {"referenceID": 1, "context": "1 Established Linguistic Areas Below, we list some of the well-known linguistic areas; Campbell (2005) provides are more complete listing together with example areal features for these areas.", "startOffset": 87, "endOffset": 103}, {"referenceID": 26, "context": "), there are counter-arguments to this position (Thomason, 2001) (see especially Case Study 9.", "startOffset": 48, "endOffset": 64}, {"referenceID": 5, "context": "For instance, Dixon (2001) presents arguments for several Australian linguistic areas and Matisoff (2001) defines a SouthEast Asian language area.", "startOffset": 14, "endOffset": 27}, {"referenceID": 5, "context": "For instance, Dixon (2001) presents arguments for several Australian linguistic areas and Matisoff (2001) defines a SouthEast Asian language area.", "startOffset": 14, "endOffset": 106}, {"referenceID": 3, "context": "Much of this overview is adoped from the summary given by Curnow (2001).", "startOffset": 58, "endOffset": 72}, {"referenceID": 12, "context": "In Bayesian modeling, non-parametric distributions are typically used as priors; see Jordan (2005) or Ghahramani (2005) for overviews.", "startOffset": 85, "endOffset": 99}, {"referenceID": 10, "context": "In Bayesian modeling, non-parametric distributions are typically used as priors; see Jordan (2005) or Ghahramani (2005) for overviews.", "startOffset": 102, "endOffset": 120}, {"referenceID": 21, "context": "One particular example of a non-parametric prior is the Pitman-Yor process (Pitman and Yor, 1997), which can be seen as an extension to the betterknown Dirichlet process (Ferguson, 1974).", "startOffset": 75, "endOffset": 97}, {"referenceID": 9, "context": "One particular example of a non-parametric prior is the Pitman-Yor process (Pitman and Yor, 1997), which can be seen as an extension to the betterknown Dirichlet process (Ferguson, 1974).", "startOffset": 170, "endOffset": 186}, {"referenceID": 22, "context": "The Pitman-Yor process can be understood as a particular example of a Chinese Restaurant process (CRP) (Pitman, 2002).", "startOffset": 103, "endOffset": 117}, {"referenceID": 16, "context": "Kingman\u2019s coalescent is a standard model in population genetics describing the common genealogy (ancestral tree) of a set of individuals (Kingman, 1982b; Kingman, 1982a).", "startOffset": 137, "endOffset": 169}, {"referenceID": 15, "context": "Kingman\u2019s coalescent is a standard model in population genetics describing the common genealogy (ancestral tree) of a set of individuals (Kingman, 1982b; Kingman, 1982a).", "startOffset": 137, "endOffset": 169}, {"referenceID": 16, "context": "The n-coalescent has some interesting statistical properties (Kingman, 1982b; Kingman, 1982a).", "startOffset": 61, "endOffset": 93}, {"referenceID": 15, "context": "The n-coalescent has some interesting statistical properties (Kingman, 1982b; Kingman, 1982a).", "startOffset": 61, "endOffset": 93}, {"referenceID": 15, "context": "Kingman called this the coalescent. Teh et al. (2007) recently described efficient inference algorithms for Kingman\u2019s coalescent.", "startOffset": 0, "endOffset": 54}, {"referenceID": 7, "context": "An alternative would be to use a location-sensitive process such as the kernel stickbreaking process (Dunson and Park, 2007), though we do not explore that here.", "startOffset": 101, "endOffset": 124}, {"referenceID": 25, "context": "The only exceptions are: (1) the coalescent for which we use the GreedyRate1 algorithm described by Teh et al. (2007); (2) the area centers c, for which we using a Metropolis-Hastings step.", "startOffset": 100, "endOffset": 118}, {"referenceID": 1, "context": "We use the version extracted and preprocessed by Daum\u00e9 III and Campbell (2007). In WALS, languages a grouped into 38 language families (including Indo-European, Afro-Asiatic, Austronesian, Niger-Congo, etc.", "startOffset": 63, "endOffset": 79}, {"referenceID": 24, "context": "However, in addition to being intuitively plausible, it is not hard to find evidence in the literature for the contact relationship between English and Irish (Sommerfelt, 1960).", "startOffset": 158, "endOffset": 176}, {"referenceID": 20, "context": "Clustering performance is measured on the Indo-European task according to the Rand Index, F-score, Normalized Edit Score (Pantel, 2003) and Normalized Variation of Information (Meila, 2003).", "startOffset": 121, "endOffset": 135}, {"referenceID": 18, "context": "Clustering performance is measured on the Indo-European task according to the Rand Index, F-score, Normalized Edit Score (Pantel, 2003) and Normalized Variation of Information (Meila, 2003).", "startOffset": 176, "endOffset": 189}, {"referenceID": 3, "context": "According to Curnow (2001), the most difficult features to borrow are phonetics (for which we have no data), bound grammatical forms (which appear low on our list), morphology (which is 99% genetic, according to our model) and syntactic frames (which would roughly correspond to \u201ccomplex sentences\u201d, another Indo-European Model Accuracy Log Prob Baseline 0.", "startOffset": 13, "endOffset": 27}, {"referenceID": 12, "context": ", Duda and Hart (1973)) and the Bayesian Hierarchical Clustering algorithm (Heller and Ghahramani, 2005).", "startOffset": 75, "endOffset": 104}, {"referenceID": 12, "context": "We use Kingman\u2019s coalescent (see Section 2.2.2) as a probabilistic model of trees, endowed with a binomial mutation process on the language features. Our baseline model is to run the vanilla coalescent on the WALS data, effective reproducing the results presented by Teh et al. (2007). This method was already shown to outperform competing hierarchical clustering algorithms such as average-link agglomerative clustering (see, eg.", "startOffset": 7, "endOffset": 285}, {"referenceID": 6, "context": ", Duda and Hart (1973)) and the Bayesian Hierarchical Clustering algorithm (Heller and Ghahramani, 2005).", "startOffset": 2, "endOffset": 23}, {"referenceID": 12, "context": "For the second quantitative analysis, we use present purity scores (Heller and Ghahramani, 2005), subtree scores (the number of interior nodes with pure leaf labels, normalized) and leave-one-out log accuracies (all scores are between 0 and 1, and higher scores are better).", "startOffset": 67, "endOffset": 96}], "year": 2009, "abstractText": "We describe a statistical model over linguistic areas and phylogeny. Our model recovers known areas and identifies a plausible hierarchy of areal features. The use of areas improves genetic reconstruction of languages both qualitatively and quantitatively according to a variety of metrics. We model linguistic areas by a Pitman-Yor process and linguistic phylogeny by Kingman\u2019s coalescent.", "creator": "TeX"}}}