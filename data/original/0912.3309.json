{"id": "0912.3309", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2009", "title": "New Generalization Bounds for Learning Kernels", "abstract": "This paper presents several novel generalization bounds for the problem of learning kernels based on the analysis of the Rademacher complexity of the corresponding hypothesis sets. Our bound for learning kernels with a convex combination of p base kernels has only a log(p) dependency on the number of kernels, p, which is considerably more favorable than the previous best bound given for the same problem. We also give a novel bound for learning with a linear combination of p base kernels with an L_2 regularization whose dependency on p is only in p^{1/4}.", "histories": [["v1", "Thu, 17 Dec 2009 02:29:41 GMT  (11kb)", "http://arxiv.org/abs/0912.3309v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["corinna cortes", "mehryar mohri", "afshin rostamizadeh"], "accepted": true, "id": "0912.3309"}, "pdf": {"name": "0912.3309.pdf", "metadata": {"source": "CRF", "title": "New Generalization Bounds for Learning Kernels", "authors": ["Corinna Cortes", "Mehryar Mohri", "Afshin Rostamizadeh"], "emails": ["corinna@google.com", "mohri@cims.nyu.edu", "rostami@cs.nyu.edu"], "sections": [{"heading": null, "text": "ar X\niv :0\n91 2.\n33 09\nv1 [\ncs .A\nI] 1\n7 D\nec 2\n00 9"}, {"heading": "1 Introduction", "text": "Kernel methods are widely used in statistical learning [17, 18]. Positive definite symmetric (PDS) kernels specify an inner product in an implicit Hilbert space where large-margin methods are used for learning and estimation. They can be combined with algorithms such as support vector machines (SVMs) [5, 10, 20] or other kernel-based algorithms to form powerful learning techniques.\nBut, the choice of the kernel, which is critical to the success of the algorithm, is typically left to the user. Rather than requesting the user to commit to a specific kernel, which may not be optimal for the task, especially if the user\u2019s prior knowledge about the task is poor, learning kernel methods require him only to specify a family of kernels. The learning algorithm then selects both the specific kernel out of that family, and the hypothesis defined with respect to that kernel.\nThere is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9]. Some of this previous work considers families of Gaussian kernels [15] or hyperkernels [16]. Non-linear combinations of kernels have been recently considered by [21, 3, 9]. But, the most common family of kernels examined is that of non-negative combinations of some fixed kernels constrained by a trace condition, which can be viewed as an L1 regularization [13], or by an L2 regularization [8].\nThis paper presents several novel generalization bounds for the problem of learning kernels for the family of convex combinations of base kernels or linear combinations with an L2 constraint. One of the first learning bounds given by Lanckriet et al. [13] for the family of convex combinations of p base kernels is similar to that of Bousquet and Herrmann [6] and has the following form: R(h) \u2264 R\u0302\u03c1(h) + O (\n1\u221a m \u221a maxpk=1 Tr(Kk)max p i=1(\u2016Kk\u2016/Tr(Kk))/\u03c12 ) where R(h) is the generalization error of a hypothesis h, R\u03c1(h) is the fraction of training points with margin less than or equal to \u03c1 and Kk is the kernel matrix associated to the kth base kernel. This bound was later shown by Srebro and Ben-David [19] to be always larger than one. Another bound by Lanckriet et al. [13] for the family of linear combinations of base kernels was also shown by the same authors to be always larger than one.\nBut Lanckriet et al. [13] also presented a multiplicative bound for convex combinations of base kernels\nthat is of the form R(h) \u2264 R\u0302\u03c1(h) + O (\u221a p/\u03c12\nm\n) . This bound converges and can perhaps be viewed as\nthe first informative generalization bound for this family of kernels. However, the dependence of the bound on the number of kernels p is multiplicative which therefore does not encourage the use of too many base kernels. Srebro and Ben-David [19] presented a generalization bound based on the pseudo-dimension of the family of kernels that significantly improved on this bound. Their bound has the form R(h) \u2264 R\u0302\u03c1(h) + O\u0303 (\u221a p+R2/\u03c12\nm\n) , where the notation O\u0303(\u00b7) hides logarithmic terms and whereR is an upper bound onKk(x, x)\nfor all points x and base kernels kk , k \u2208 [1, p]. Thus, disregarding logarithmic terms, their bound is only additive in p. Their analysis also applies to other families of kernels. Ying and Campbell [22] also give generalization bounds for learning kernels based on the notion of Rademacher chaos complexity and the pseudo-dimension of the family of kernels used. It is not clear however how their bound compares to that of Srebro and Ben-David. We present new generalization bounds for the family of convex combinations of base kernels that have only a logarithmic dependency on p. Our learning bound is based on a careful analysis of the Rademacher complexity of the hypothesis set considered and has the form: R(h) \u2264 R\u0302\u03c1(h) + O (\u221a (log p)R2/\u03c12\nm\n) . Our bound is simpler and contains no other extra logarithmic term. Thus, this represents\na substantial improvement over the previous best bounds for this problem. Our bound is also valid for a very large number of kernels, in particular for p \u226b m, while the previous bounds were not informative in that case.\nWe also present new generalization bounds for the family of linear combinations of base kernels with an L2 regularization. We had previously given a stability bound for an algorithm extending kernel ridge regression to learning kernels that had an additive dependency with respect to p [8] assuming a technical condition of orthogonality on the base kernels. The complexity term of our bound was of the formO(1/\n\u221a m+\u221a\np/m). Our new learning bound admits only a mild dependency of p1/4 on the number of base kernels. The next section (Section 2) defines the family of kernels and hypothesis sets we examine. Section 3 presents a bound on the Rademacher complexity of the class of convex combinations of base kernels with an L1 constraint and a generalization bond for binary classification directly derived from that result. Similarly, Section 4 presents first a bound on the Rademacher complexity, then a generalization bound for the case of an of L2 regularization."}, {"heading": "2 Preliminaries", "text": "Most learning kernel algorithms are based on a hypothesis set derived from convex combinations of a fixed set of kernels K1, . . . ,Kp:\nHp = { m\u2211\ni=1\n\u03b1iK(xi, \u00b7) : K = p\u2211\nk=1\n\u00b5kKk, \u00b5k \u2265 0, p\u2211\nk=1\n\u00b5k = 1,\u03b1 \u22a4 K\u03b1 \u2264 1/\u03c12 } . (1)\nNote that linear combinations with possibly negative mixture weights have also been considered in the literature, e.g., [13], however these combinations do not ensure that the combined kernel is PDS.\nWe also consider the hypothesis set H \u2032p based on a L2 condition on the vector \u00b5 and defined as follows:\nH \u2032p = { m\u2211\ni=1\n\u03b1iK(xi, \u00b7) : K = p\u2211\nk=1\n\u00b5kKk, \u00b5k \u2265 0, p\u2211\nk=1\n\u00b52k = 1,\u03b1 \u22a4 K\u03b1 \u2264 1/\u03c12\n} . (2)\nWe bound the empirical Rademacher complexity R\u0302S(Hp) or R\u0302S(H \u2032p) of these families for an arbitrary sample S of size m, which immediately yields a generalization bound for learning kernels based on this family of hypotheses. For a fixed sample S = (x1, . . . , xm), the empirical Rademacher complexity of a hypothesis set H is defined as\nR\u0302S(H) = 1\nm E \u03c3 [ sup h\u2208H m\u2211\ni=1\n\u03c3ih(xi) ] . (3)\nThe expectation is taken over \u03c3 = (\u03c31, . . . , \u03c3n) where \u03c3is are independent uniform random variables taking values in {\u22121,+1}.\nLet h \u2208 Hp, then\nh(x) =\nm\u2211\ni=1\n\u03b1iK(xi, x) =\np\u2211\nk=1\nm\u2211\ni=1\n\u00b5k\u03b1iKk(xi, x) = w \u00b7\u03a6(x), (4)\nwhere w =\n[ w1\n... wp\n] with wk = \u00b5k \u2211m i=1 \u03b1i\u03a6k(xi) and \u03a6(x) = [ \u03a61(x)\n... \u03a6p(x)\n] with \u03a6k = Kk(x, \u00b7), for all\nk \u2208 [1, p]."}, {"heading": "3 Rademacher complexity bound for Hp", "text": "Theorem 1 For any sample S of size m, the Rademacher complexity of the hypothesis set Hp can be bounded as follows:\nR\u0302S(Hp) \u2264 \u2016\u03c4\u2016r m\u03c1 with \u03c4 = ( \u221a rTr[K1], . . . , \u221a rTr[Kp]) \u22a4, (5)\nfor any even integer r > 0. If additionally, Kk(x, x) \u2264 R2 for all x \u2208 X and k \u2208 [1, p], then, for p > 1,\nR\u0302S(Hp) \u2264 \u221a\n2e\u2308log p\u2309R2/\u03c12 m .\nProof: Fix a sample S, then R\u0302S(Hp) can be bounded as follows for the hypothesis set of kernel learning algorithms for any q, r > 1 with 1/q + 1/r = 1:\nR\u0302S(Hp) = 1\nm E \u03c3 [ sup h\u2208Hp m\u2211\ni=1\n\u03c3ih(xi) ]\n= 1\nm E \u03c3 [ sup w w \u00b7 m\u2211\ni=1\n\u03c3i\u03a6(xi) ]\n\u2264 1 m E \u03c3 [ sup w ( p\u2211\nk=1\n\u2016wk\u2016q )1/q( p\u2211\nk=1\n\u2225\u2225\u2225 m\u2211\ni=1\n\u03c3i\u03a6k(xi) \u2225\u2225\u2225 r)1/r] (Lemma 5)\n= 1\nm [ sup w ( p\u2211\nk=1\n\u2016wk\u2016q )1/q]\nE \u03c3\n[( p\u2211\nk=1\n\u2225\u2225\u2225 m\u2211\ni=1\n\u03c3i\u03a6k(xi) \u2225\u2225\u2225 r)1/r] .\nWe bound each of these two factors separately. The first term can be bounded as follows.\n( p\u2211\nk=1\n\u2016wk\u2016q )1/q \u2264 p\u2211\nk=1\n(\u2016wk\u2016q)1/q (sub-additivity of x 7\u2192 x1/q, (1/q) < 1)\n=\np\u2211\nk=1\n\u00b5k\u2016 m\u2211\ni=1\n\u03b1i\u03a6k(xi)\u2016\n\u2264\n\u221a\u221a\u221a\u221a p\u2211\nk=1\n\u00b5k\u2016 m\u2211\ni=1\n\u03b1i\u03a6k(xi)\u20162 (convexity)\n=\n\u221a\u221a\u221a\u221a p\u2211\nk=1\n\u00b5k\u03b1\u22a4Kk\u03b1 = \u221a \u03b1\u22a4K\u03b1 \u2264 1/\u03c1.\nWe bound the second term as follows:\nE \u03c3\n[( p\u2211\nk=1\n\u2225\u2225 m\u2211\ni=1\n\u03c3i\u03a6k(xi) \u2225\u2225r )1/r] \u2264 ( E \u03c3 [ p\u2211\nk=1\n\u2225\u2225 m\u2211\ni=1\n\u03c3i\u03a6k(xi) \u2225\u2225r ])1/r (Jensen\u2019s inequality)\n=\n( p\u2211\nk=1\nE \u03c3\n[\u2225\u2225 m\u2211\ni=1\n\u03c3i\u03a6k(xi) \u2225\u2225r ])1/r\nSuppose that r is an even integer, r = 2r\u2032. Then, we can bound the expectation as follows:\nE \u03c3\n[\u2225\u2225 m\u2211\ni=1\n\u03c3i\u03a6k(xi) \u2225\u2225r ] = E\n\u03c3\n[( m\u2211\ni,j=1\n\u03c3i\u03c3jKk(xi, xj) )r\u2032]\n\u2264 \u2211\n1\u2264i1,...,ir\u2032\u2264m 1\u2264j1,...,jr\u2032\u2264m\n\u2223\u2223\u2223E \u03c3 [ \u03c3i1\u03c3j1 \u00b7 \u00b7 \u00b7\u03c3ir\u2032\u03c3jr\u2032 ]\u2223\u2223\u2223 \u2223\u2223\u2223Kk(xi1 , xj1 ) \u00b7 \u00b7 \u00b7Kk(xir\u2032 , xjr\u2032 ) \u2223\u2223\u2223\n\u2264 \u2211\n1\u2264i1,...,ir\u2032\u2264m 1\u2264j1,...,jr\u2032\u2264m\n\u2223\u2223\u2223E \u03c3 [ \u03c3i1\u03c3j1 \u00b7 \u00b7 \u00b7\u03c3ir\u2032\u03c3jr\u2032 ]\u2223\u2223\u2223(Kk(xi1 , xi1) \u00b7 \u00b7 \u00b7Kk(xir\u2032 , xir\u2032 ))1/2\n(Kk(xj1 , xj1) \u00b7 \u00b7 \u00b7Kk(xjr\u2032 , xjr\u2032 ))1/2 (Cauchy-Schwarz)\n= \u2211\ns1+...+sm=2r\u2032\n( 2r\u2032\ns1,...,sm )\u2223\u2223\u2223E \u03c3 [ \u03c3s11 \u00b7 \u00b7 \u00b7\u03c3smm ]\u2223\u2223\u2223Kk(x1, x1)s1/2 \u00b7 \u00b7 \u00b7Kk(xm, xm)sm/2.\nSince E[\u03c3i] = 0 for all i and since the Rademacher variables are independent, we can write E[\u03c3i1 . . . \u03c3il ] =\nE[\u03c3i1 ] \u00b7 \u00b7 \u00b7E[\u03c3il ] = 0 for any l distinct variables \u03c3i1 , . . . , \u03c3il . Thus, E\u03c3 [ \u03c3s11 \u00b7 \u00b7 \u00b7\u03c3sm1 ] = 0 unless all sis are even, in which case E\u03c3 [ \u03c3s11 \u00b7 \u00b7 \u00b7\u03c3smm ] = 1. Therefore, the following inequality holds:1\nE \u03c3\n[\u2225\u2225 m\u2211\ni=1\n\u03a6k(xi) \u2225\u2225r ] \u2264\n\u2211\n2t1+...+2tm=2r\u2032\n( 2r\u2032\n2t1,...,2tm\n) Kk(x1, x1) t1 \u00b7 \u00b7 \u00b7Kk(xm, xm)tm\n\u2264 (2r\u2032)r\u2032 \u2211\nt1+...+tm=r\u2032\n( r\u2032\nt1,...,tm\n) Kk(x1, x1) t1 \u00b7 \u00b7 \u00b7Kk(xm, xm)tm\n= (2r\u2032 Tr[Kk]) r\u2032 = (rTr[Kk]) r/2.\nThus, the Rademacher complexity is bounded by\nR\u0302S(Hp) \u2264 \u2016\u03c4\u2016r m\u03c1 with \u03c4 = ( \u221a rTr[K1], . . . , \u221a rTr[Kp]) \u22a4, (6)\nfor any even integer r. Assume that Kk(x, x) \u2264 R2 for all x \u2208 X and k \u2208 [1, p]. Then, Tr[Kk] \u2264 mR2 for any k \u2208 [1, p], thus the Rademacher complexity can be bounded as follows\nR\u0302S(Hp) \u2264 1\nm\u03c1 (p(\n\u221a rmR2)r)1/r = p1/rr1/2\n\u221a R2/\u03c12\nm .\nFor p > 1, the function r 7\u2192 p1/rr1/2 reaches its minimum at r0 = 2 log p. This gives\nR\u0302S(Hp) \u2264 \u221a\n2e\u2308log p\u2309R2/\u03c12 m .\nIt is likely that the constants in the bound of theorem can be further improved. We used a very rough upper bound for the multinomial coefficients. A finer bound using Sterling\u2019s approximation should provide a better result. Remarkably, the bound of the theorem has a very mild dependency with respect to p.\nThe theorem can be used to derive generalization bounds for learning kernels in classification, regression, and other tasks. We briefly illustrate its application to binary classification where the labels y are in {\u22121,+1}. Let R(h) denote the generalization error of h \u2208 Hp, that is R(h) = Pr[yh(x) < 0]. For a training sample S = ((x1, y1), . . . , (xm, ym)) and any \u03c1 > 0, let R\u0302\u03c1(h) denote the fraction of the training points with margin less than or equal to \u03c1, that is R\u0302\u03c1(h) = 1m \u2211m i=1 1yih(xi)\u2264\u03c1. Then, the following result holds.\nCorollary 2 For any \u03b4 > 0, with probability at least 1\u2212 \u03b4, the following bound holds for any h \u2208 Hp:\nR(h) \u2264 R\u0302\u03c1(h) + 2\u2016\u03c4\u2016r m\u03c1 + 2 \u221a log 2\u03b4 2m . (7)\nwith \u03c4 = ( \u221a rTr[K1], . . . , \u221a rTr[Kp])\n\u22a4, for any even integer r > 0. If additionally, Kk(x, x) \u2264 R2 for all x \u2208 X and k \u2208 [1, p], then, for p > 1,\nR(h) \u2264 R\u0302\u03c1(h) + 2 \u221a\n2e\u2308log p\u2309R2/\u03c12 m + 2 \u221a log 2\u03b4 2m .\nProof: With our definition of the Rademacher complexity, for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4, the following bound holds for any h \u2208 Hp [12, 4]:\nR(h) \u2264 R\u0302\u03c1(h) + 2R\u0302S(Hp) + 2 \u221a log 2\u03b4 2m . (8)\n1We use the following rather rough inequality:\n` 2r \u2032\n2t1,...,2tm\n\u00b4 = (2r\u2032)!\n(2t1)! \u00b7 \u00b7 \u00b7 (2tm)! \u2264\n(2r\u2032)!\n(t1)! \u00b7 \u00b7 \u00b7 (tm)! \u2264\n(2r\u2032) \u00b7 \u00b7 \u00b7 (r\u2032 + 1) \u00b7 r\u2032!\n(t1)! \u00b7 \u00b7 \u00b7 (tm)! \u2264\n(2r\u2032)r \u2032 \u00b7 r \u2032!\n(t1)! \u00b7 \u00b7 \u00b7 (tm)! = (2r\u2032)r\n\u2032` r \u2032\nt1,...,tm\n\u00b4\n.\nPlugging in the bound on the empirical Rademacher complexity given by Theorem 1 yields the statement of the corollary.\nThe corollary gives a generalization bound for learning kernels with Hp that is in\nO\n(\u221a (log p) R2/\u03c12\nm\n) . (9)\nIn comparison, the bound for this problem given by Srebro and Ben-David [19] using the pseudo-dimension has a stronger dependency with respect to p and is more complex:\nO\n(\u221a\n8 2 + p log 128em\n3R2 \u03c12p + 256 R2 \u03c12 log \u03c1em 8R log 128mR2 \u03c12\nm\n) . (10)\nThis bound is also not informative for p > m.\n4 Rademacher complexity bound for H \u2032 p\nTheorem 3 For any sample S of size m, the Rademacher complexity of the hypothesis set H \u2032p can be bounded as follows:\nR\u0302S(H \u2032 p) \u2264 \u2016\u03c4\u2016r m\u03c1 with \u03c4 = ( \u221a rTr[K1], . . . , \u221a rTr[Kp]) \u22a4, (11)\nfor any even integer 0 < r \u2264 4. If additionally, Kk(x, x) \u2264 R2 for all x \u2208 X and k \u2208 [1, p], then, for any p \u2265 1,\nR\u0302S(H \u2032 p) \u2264 2p1/4\n\u221a R2/\u03c12\nm .\nThis bound also hold without the condition \u00b5k \u2265 0, k \u2208 [1, p], on the hypothesis set H \u2032p. Proof: We can proceed as in the proof for bounding the Rademacher complexity of Hp, except for bounding the following term:\n( p\u2211\nk=1\n\u2016wk\u2016q )1/q = [ p\u2211\nk=1\n\u00b5qk(\u03b1 \u22a4 Kk\u03b1)\nq/2 ]1/q\n= [ p\u2211\nk=1\n\u00b52k(\u00b5 2(q\u22122) q k \u03b1 \u22a4 Kk\u03b1)\nq/2 ]1/q\n\u2264 [ ( p\u2211\nk=1\n\u00b52k \u00b5 2(q\u22122) q k \u03b1 \u22a4 Kk\u03b1)\nq/2 ]1/q\n(convexity)\n=\n\u221a\u221a\u221a\u221a p\u2211\nk=1\n\u00b5 4(q\u22121) q\nk \u03b1 \u22a4Kk\u03b1.\nAssume now that q > 4/3, which implies 4(q\u22121)q < 1. Then, since \u00b5k \u2208 [0, 1], this implies \u00b5 4(q\u22121) q\nk \u2264 \u00b5k. Thus, for any q > 4/3, we can write:\n( p\u2211\nk=1\n\u2016wk\u2016q )1/q \u2264\n\u221a\u221a\u221a\u221a p\u2211\nk=1\n\u00b5k\u03b1\u22a4Kk\u03b1 = \u221a \u03b1\u22a4K\u03b1 \u2264 1/\u03c12.\nTaking the limit q \u2192 4/3 shows that the inequality is also verified for q = 4/3. Thus, as in the proof for Hp, the Rademacher complexity can be bounded as follows\nR\u0302S(H \u2032 p) \u2264 \u2016\u03c4\u2016r m\u03c1 with \u03c4 = ( \u221a rTr[K1], . . . , \u221a rTr[Kp]) \u22a4, (12)\nbut here r is an even integer such that 1/r = 1 \u2212 1/q \u2265 1 \u2212 3/4 = 1/4, that is r \u2264 4. Assume that Kk(x, x) \u2264 R2 for all x \u2208 X and k \u2208 [1, p]. Then, Tr[Kk] \u2264 mR2 for any k \u2208 [1, p], thus, for r = 4, the Rademacher complexity can be bounded as follows\nR\u0302S(H \u2032 p) \u2264\n1\nm\u03c1 (p(\n\u221a 4mR2)4)1/4 = 2p1/4\n\u221a R2/\u03c12\nm .\nThus, in this case, the bound has a mild dependence ( 4 \u221a\u00b7) on the number of kernels p. Proceeding as in\nthe L1 case leads to the following margin bound in binary classification.\nCorollary 4 For any \u03b4 > 0, with probability at least 1\u2212 \u03b4, the following bound holds for any h \u2208 Hp:\nR(h) \u2264 R\u0302\u03c1(h) + 2\u2016\u03c4\u2016r m\u03c1 + 2 \u221a log 2\u03b4 2m . (13)\nwith \u03c4 = ( \u221a rTr[K1], . . . , \u221a rTr[Kp])\n\u22a4, for any even integer r \u2208 {2, 4}. If additionally, Kk(x, x) \u2264 R2 for all x \u2208 X and k \u2208 [1, p], then, for any p \u2265 1,\nR(h) \u2264 R\u0302\u03c1(h) + 4p1/4 \u221a R2/\u03c12\nm + 2 \u221a log 2\u03b4 2m ."}, {"heading": "5 Conclusion", "text": "We presented several new generalization bounds for the problem of learning kernels with non-negative combinations of base kernels. Our bounds are simpler and significantly improve over previous bounds. Their very mild dependency on the number of kernels seems to suggest the use of a large number of kernels for this problem. Our experiments with this problem in regression using a large number of kernels seems to corroborate this idea [8]. Much needs to be done however to combine these theoretical findings with the somewhat disappointing performance observed in practice in most learning kernel experiments [7]."}, {"heading": "A Lemma 5", "text": "The following lemma is a straightforward version of Ho\u0308lder\u2019s inequality.\nLemma 5 Let q, r > 1 with 1/q + 1/r = 1. Then, the following result similar to Ho\u0308lder\u2019s inequality holds:\n|w \u00b7\u03a6(x)| \u2264 ( p\u2211\nk=1\n\u2016wk\u2016q )1/q( p\u2211\nk=1\n\u2225\u2225\u2225\u03a6(x) \u2225\u2225\u2225 r)1/r . (14)\nProof: Let \u03a8q(w) = ( \u2211p k=1 \u2016wk\u2016q)1/q and \u03a8r(\u03a6(x)) = ( \u2211p k=1 \u2016\u03a6k(x)\u2016r)1/r, then\n|w \u00b7\u03a6(x)| \u03a8q(w)\u03a8r(\u03a6(x))\n= \u2223\u2223\u2223 p\u2211\nk=1\nwk \u03a8q(w) \u00b7 \u03a6k(x) \u03a8r(\u03a6(x))\n\u2223\u2223\u2223\n\u2264 p\u2211\nk=1\n\u2223\u2223\u2223 wk\n\u03a8q(w) \u00b7 \u03a6k(x) \u03a8r(\u03a6(x))\n\u2223\u2223\u2223\n\u2264 p\u2211\nk=1\n\u2016wk\u2016 \u03a8q(w) \u00b7 \u2016\u03a6k(x)\u2016 \u03a8r(\u03a6(x))\n(Cauchy-Schwarz)\n\u2264 p\u2211\nk=1\n1\nq \u2016wk\u2016q \u03a8q(w)q + 1 r \u2016\u03a6k(x)\u2016r \u03a8r(\u03a6(x))r\n(Young\u2019s inequality)\n= 1\nq +\n1 r = 1."}], "references": [{"title": "A DC-programming algorithm for kernel selection", "author": ["Andreas Argyriou", "Raphael Hauser", "Charles Micchelli", "Massimiliano Pontil"], "venue": "In ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Learning convex combinations of continuously parameterized basic kernels", "author": ["Andreas Argyriou", "Charles Micchelli", "Massimiliano Pontil"], "venue": "In COLT,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Exploring large feature spaces with hierarchical multiple kernel learning", "author": ["F. Bach"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["Peter L. Bartlett", "Shahar Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "A training algorithm for optimal margin classifiers", "author": ["Bernhard Boser", "Isabelle Guyon", "Vladimir Vapnik"], "venue": "In COLT,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1992}, {"title": "On the complexity of learning the kernel matrix", "author": ["Olivier Bousquet", "Daniel J.L. Herrmann"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Invited talk: Can learning kernels help performance", "author": ["Corinna Cortes"], "venue": "In ICML, page 161,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "L2 regularization for learning kernels", "author": ["Corinna Cortes", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Learning non-linear combinations of kernels", "author": ["Corinna Cortes", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "In Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Multi-task feature and kernel selection for SVMs", "author": ["Tony Jebara"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Rademacher processes and bounding the risk of function learning", "author": ["V. Koltchinskii", "D. Panchenko"], "venue": "In High Dimensional Probability II,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["Gert Lanckriet", "Nello Cristianini", "Peter Bartlett", "Laurent El Ghaoui", "Michael Jordan"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Nonstationary kernel combination", "author": ["Darrin P. Lewis", "Tony Jebara", "William Stafford Noble"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Learning the kernel function via regularization", "author": ["Charles Micchelli", "Massimiliano Pontil"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Learning the kernel with hyperkernels", "author": ["Cheng Soon Ong", "Alexander Smola", "Robert Williamson"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Learning with Kernels", "author": ["Bernhard Sch\u00f6lkopf", "Alex Smola"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Kernel Methods for Pattern Analysis", "author": ["John Shawe-Taylor", "Nello Cristianini"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Learning bounds for support vector machines with learned kernels", "author": ["Nathan Srebro", "Shai Ben-David"], "venue": "In COLT,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Statistical Learning Theory", "author": ["Vladimir N. Vapnik"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "More generality in efficient multiple kernel learning", "author": ["Manik Varma", "Bodla Rakesh Babu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Generalization bounds for learning the kernel problem", "author": ["Yiming Ying", "Colin Campbell"], "venue": "In COLT,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Multiclass multiple kernel learning", "author": ["Alexander Zien", "Cheng Soon Ong"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}], "referenceMentions": [{"referenceID": 15, "context": "1 Introduction Kernel methods are widely used in statistical learning [17, 18].", "startOffset": 70, "endOffset": 78}, {"referenceID": 16, "context": "1 Introduction Kernel methods are widely used in statistical learning [17, 18].", "startOffset": 70, "endOffset": 78}, {"referenceID": 4, "context": "They can be combined with algorithms such as support vector machines (SVMs) [5, 10, 20] or other kernel-based algorithms to form powerful learning techniques.", "startOffset": 76, "endOffset": 87}, {"referenceID": 18, "context": "They can be combined with algorithms such as support vector machines (SVMs) [5, 10, 20] or other kernel-based algorithms to form powerful learning techniques.", "startOffset": 76, "endOffset": 87}, {"referenceID": 11, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 13, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 1, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 0, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 17, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 14, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 12, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 21, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 9, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 2, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 7, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 20, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 8, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 13, "context": "Some of this previous work considers families of Gaussian kernels [15] or hyperkernels [16].", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "Some of this previous work considers families of Gaussian kernels [15] or hyperkernels [16].", "startOffset": 87, "endOffset": 91}, {"referenceID": 19, "context": "Non-linear combinations of kernels have been recently considered by [21, 3, 9].", "startOffset": 68, "endOffset": 78}, {"referenceID": 2, "context": "Non-linear combinations of kernels have been recently considered by [21, 3, 9].", "startOffset": 68, "endOffset": 78}, {"referenceID": 8, "context": "Non-linear combinations of kernels have been recently considered by [21, 3, 9].", "startOffset": 68, "endOffset": 78}, {"referenceID": 11, "context": "But, the most common family of kernels examined is that of non-negative combinations of some fixed kernels constrained by a trace condition, which can be viewed as an L1 regularization [13], or by an L2 regularization [8].", "startOffset": 185, "endOffset": 189}, {"referenceID": 7, "context": "But, the most common family of kernels examined is that of non-negative combinations of some fixed kernels constrained by a trace condition, which can be viewed as an L1 regularization [13], or by an L2 regularization [8].", "startOffset": 218, "endOffset": 221}, {"referenceID": 11, "context": "[13] for the family of convex combinations of p base kernels is similar to that of Bousquet and Herrmann [6] and has the following form: R(h) \u2264 R\u0302\u03c1(h) + O ( 1 \u221a m \u221a maxpk=1 Tr(Kk)max p i=1(\u2016Kk\u2016/Tr(Kk))/\u03c1 ) where R(h) is the generalization error of a hypothesis h, R\u03c1(h) is the fraction of training points with margin less than or equal to \u03c1 and Kk is the kernel matrix associated to the kth base kernel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[13] for the family of convex combinations of p base kernels is similar to that of Bousquet and Herrmann [6] and has the following form: R(h) \u2264 R\u0302\u03c1(h) + O ( 1 \u221a m \u221a maxpk=1 Tr(Kk)max p i=1(\u2016Kk\u2016/Tr(Kk))/\u03c1 ) where R(h) is the generalization error of a hypothesis h, R\u03c1(h) is the fraction of training points with margin less than or equal to \u03c1 and Kk is the kernel matrix associated to the kth base kernel.", "startOffset": 105, "endOffset": 108}, {"referenceID": 17, "context": "This bound was later shown by Srebro and Ben-David [19] to be always larger than one.", "startOffset": 51, "endOffset": 55}, {"referenceID": 11, "context": "[13] for the family of linear combinations of base kernels was also shown by the same authors to be always larger than one.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] also presented a multiplicative bound for convex combinations of base kernels that is of the form R(h) \u2264 R\u0302\u03c1(h) + O (\u221a p/\u03c12 m ) .", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Srebro and Ben-David [19] presented a generalization bound based on the pseudo-dimension of the family of kernels that significantly improved on this bound.", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": "Ying and Campbell [22] also give generalization bounds for learning kernels based on the notion of Rademacher chaos complexity and the pseudo-dimension of the family of kernels used.", "startOffset": 18, "endOffset": 22}, {"referenceID": 7, "context": "We had previously given a stability bound for an algorithm extending kernel ridge regression to learning kernels that had an additive dependency with respect to p [8] assuming a technical condition of orthogonality on the base kernels.", "startOffset": 163, "endOffset": 166}, {"referenceID": 11, "context": ", [13], however these combinations do not ensure that the combined kernel is PDS.", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": "Proof: With our definition of the Rademacher complexity, for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4, the following bound holds for any h \u2208 Hp [12, 4]: R(h) \u2264 R\u0302\u03c1(h) + 2R\u0302S(Hp) + 2 \u221a log 2\u03b4 2m .", "startOffset": 146, "endOffset": 153}, {"referenceID": 3, "context": "Proof: With our definition of the Rademacher complexity, for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4, the following bound holds for any h \u2208 Hp [12, 4]: R(h) \u2264 R\u0302\u03c1(h) + 2R\u0302S(Hp) + 2 \u221a log 2\u03b4 2m .", "startOffset": 146, "endOffset": 153}, {"referenceID": 17, "context": "(9) In comparison, the bound for this problem given by Srebro and Ben-David [19] using the pseudo-dimension has a stronger dependency with respect to p and is more complex:", "startOffset": 76, "endOffset": 80}, {"referenceID": 0, "context": "Then, since \u03bck \u2208 [0, 1], this implies \u03bc 4(q\u22121) q k \u2264 \u03bck.", "startOffset": 17, "endOffset": 23}], "year": 2009, "abstractText": "This paper presents several novel generalization bounds for the problem of learning kernels based on the analysis of the Rademacher complexity of the corresponding hypothesis sets. Our bound for learning kernels with a convex combination of p base kernels has only a log p dependency on the number of kernels, p, which is considerably more favorable than the previous best bound given for the same problem. We also give a novel bound for learning with a linear combination of p base kernels with an L2 regularization whose dependency on p is only in p.", "creator": "LaTeX with hyperref package"}}}