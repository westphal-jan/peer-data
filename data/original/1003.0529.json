{"id": "1003.0529", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2010", "title": "A Unified Algorithmic Framework for Multi-Dimensional Scaling", "abstract": "In this paper, we propose a unified algorithmic framework for solving many known variants of \\mds. Our algorithm is a simple iterative scheme with guaranteed convergence, and is \\emph{modular}; by changing the internals of a single subroutine in the algorithm, we can switch cost functions and target spaces easily. In addition to the formal guarantees of convergence, our algorithms are accurate; in most cases, they converge to better quality solutions than existing methods, in comparable time. We expect that this framework will be useful for a number of \\mds variants that have not yet been studied.", "histories": [["v1", "Tue, 2 Mar 2010 09:11:44 GMT  (147kb,D)", "https://arxiv.org/abs/1003.0529v1", "18 pages, 7 figures"], ["v2", "Tue, 30 Mar 2010 17:21:53 GMT  (145kb,D)", "http://arxiv.org/abs/1003.0529v2", "18 pages, 7 figures. This version fixes a bug in the proof of Theorem 6.1 (dimensionality reduction for spherical data). The statement of the result remains the same."]], "COMMENTS": "18 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CG cs.CV", "authors": ["arvind agarwal", "jeff m phillips", "suresh venkatasubramanian"], "accepted": false, "id": "1003.0529"}, "pdf": {"name": "1003.0529.pdf", "metadata": {"source": "CRF", "title": "A Unified Algorithmic Framework for Multi-Dimensional Scaling", "authors": ["Arvind Agarwal", "Jeff M. Phillips", "Suresh Venkatasubramanian"], "emails": [], "sections": [{"heading": null, "text": "Our framework extends to embedding high-dimensional points lying on a sphere to points on a lower dimensional sphere, preserving geodesic distances. As a compliment to this result, we also extend the JohnsonLindenstrauss Lemma to this spherical setting, where projecting to a random O((1/\"2) log n)-dimensional sphere causes \"-distortion."}, {"heading": "1 Introduction", "text": "Multidimensional scaling (MDS) [23, 10, 3] is a widely used method for embedding a general distance matrix into a low dimensional Euclidean space, used both as a preprocessing step for many problems, as well as a visualization tool in its own right. MDS has been studied and used in psychology since the 1930s [35, 33, 22] to help visualize and analyze data sets where the only input is a distance matrix. More recently MDS has become a standard dimensionality reduction and embedding technique to manage the complexity of dealing with large high dimensional data sets [8, 9, 31, 6].\nIn general, the problem of embedding an arbitrary distance matrix into a fixed dimensional Euclidean space with minimum error is nonconvex (because of the dimensionality constraint). Thus, in addition to the standard formulation [12], many variants of MDS have been proposed, based on changing the underlying error function [35, 8]. There are also applications where the target space, rather than being a Euclidean space, is a manifold (e.g. a low dimensional sphere), and various heuristics for MDS in this setting have also been proposed [13, 6].\nEach such variant is typically addressed by a different heuristic, including majorization, the singular value decomposition, semidefinite programming, subgradient methods, and standard Lagrange-multipler-based methods (in both primal and dual settings). Some of these heuristics are efficient, and others are not; in general, every new variant of MDS seems to require different ideas for efficient heuristics."}, {"heading": "1.1 Our Work", "text": "In this paper, we present a unified algorithmic framework for solving many variants of MDS. Our approach is based on an iterative local improvement method, and can be summarized as follows: \u201cPick a point and move it so that the cost function is locally optimal. Repeat this process until convergence.\u201d The improvement step reduces to a well-studied and efficient family of iterative minimization techniques, where the specific algorithm depends on the variant of MDS.\nA central result of this paper is a single general convergence result for all variants of MDS that we examine. This single result is a direct consequence of the way in which we break down the general problem into an\n\u2217Partially supported by NSF IIS-0712764 \u2020Supported by a subaward to the University of Utah under NSF award 0937060 to the Computing Research Association \u2021Partially supported by NSF CCF-0953066\nar X\niv :1\n00 3.\n05 29\nv2 [\ncs .L\nG ]\n3 0\nM ar\n2 01\n0\niterative algorithm combined with a point-wise optimization scheme. Our approach is generic, efficient, and simple. The high level framework can be written in 10-12 lines of MATLAB code, with individual functionspecific subroutines needing only a few more lines each. Further, our approach compares well with the best methods for all the variants of MDS. In each case our method is consistently either the best performer or is close to the best, regardless of the data profile or cost function used, while other approaches have much more variable performance. Another useful feature of our method is that it is parameter-free, requiring no tuning parameters or Lagrange multipliers in order to perform at its best.\nSpherical MDS. An important application of our approach is the problem of performing spherical MDS. Spherical MDS is the problem of embedding a matrix of distances onto a (low-dimensional) sphere. Spherical MDS has applications in texture mapping and image analysis [6], and is a generalization of the spherical dimensionality reduction problem, where the goal is to map points from a high dimensional sphere onto a lowdimensional sphere. This latter problem is closely related to dimensionality reduction for finite dimensional distributions. A well-known isometric embedding takes a distribution represented as a point on the d-dimensional simplex to the d-dimensional sphere while preserving the Hellinger distance between distributions. A spherical dimensionality reduction result is an important step to representing high dimensional distributions in a lowerdimensional space of distributions, and will have considerable impact in domains that represent data natively as histograms or distributions, such as in document processing [29, 18, 2], image analysis [25, 11] and speech recognition [16].\nOur above framework applies directly to this setting, where for the local improvement step we adapt a technique first developed by Karcher for finding geodesic means on a manifold. In addition, we prove a Johnson-Lindenstrauss-type result for the sphere; namely, that n points lying on a d-dimensional sphere can be embedded on a O((1/\"2) log n)-dimensional sphere while approximately preserving the geodesic distances between pairs of points, that is, no distance changes by more than a relative (1+ \")-factor. This latter result can be seen as complementary to the local improvement scheme; the formal embedding result guarantees the error while being forced to use log n dimensions, while the local improvement strategy generates a mapping into any k dimensional hypersphere but provides no formal guarantees on the error.\nSummary of contributions. The main contributions of this paper can be summarized as follows:\n\u2022 In Section 4 we present our iterative framework, illustrate how it is applied to specific MDS variants and prove a convergence result.\n\u2022 In Section 5 we present a comprehensive experimental study that compares our approach to the prior best known methods for different MDS variants.\n\u2022 In Section 6 we prove a formal dimensionality reduction result that embeds a set of n points on a highdimensional sphere into a sphere of dimension O((1/\"2) log n) while preserving all distances to within relative error of (1+ \") for any \" > 0."}, {"heading": "2 Background and Existing Methods", "text": "Multidimensional scaling is a family of methods for embedding a distance matrix into a low-dimensional Euclidean space. There is a general taxonomy of MDS methods [10]; in this paper we will focus primarily the metric and generalized MDS problems.\nThe traditional formulation of MDS [23] assumes that the distance matrix D arises from points in some ddimensional Euclidean space. Under this assumption, a simple transformation takes D to a matrix of similarities S, where si j = \u2329x i , x j\u232a. These similarities also arise from many psychology data sets directly [33, 35]. The\nproblem then reduces to finding a set of points X in k-dimensional space such that X X T approximates S. This can be done optimally using the top k singular values and vectors from the singular value decomposition of S.\nA more general approach called SMACOF that drops the Euclidean assumption uses a technique known as stress majorization [27, 12, 13]. It has been adapted to many other MDS variants as well including restrictions of data to lie on quadratic surfaces and specifically spheres [13].\nSince the sum-of-squares error metric is sensitive to outliers, Cayton and Dasgupta [8] proposed a robust variant based on an `1 error metric. They separate the rank and cost constraints, solving the latter using either semidefinite programming or a subgradient heuristic, followed by a singular value decomposition to enforce the rank constraints.\nMany techniques have been proposed for performing spherical MDS. Among them are majorization methods ([30] and SMACOF-Q [13]), a multiresolution approach due to Elad, Keller, and Kimmel [14] and an approach based on computing the classical MDS and renormalizing [31].\nEmbeddings that guarantee bounded error. A complementary line of work in dimensionality reduction fixes an error bound for every pair of distances (rather than computing an average error), and asks for the minimum dimension a data set can be embedded in while maintaining this error. The Johnson-Lindenstrauss Lemma [19] states that any collection of n points in a Euclidean space can be embedded in a O((1/\"2) log n) dimensional Euclidean space that preserves all distances within a relative error of \". If the points instead define an abstract metric space, then the best possible result is an embedding into O(log n)-dimensional Euclidean space that preserves distances up to a factor of O(log n). An exhaustive survey of the different methods for dimensionality reduction is beyond the scope of this paper - the reader is directed to the survey by Indyk and Matousek for more information [17].\nThe Johnson-Lindenstrauss Lemma can be extended to data lying on manifolds. Any manifold M with \u201clinearization dimension\u201d k (a measure of its complexity) can be embedded into a O((1/\"2)k log(kn)) dimensional space so that all pairwise Euclidean distances between points on M are distorted by at most a relative (1+ \")- factor [1, 32, 26]. A k-dimensional sphere has linearization dimension O(k), so this bound applies directly for preserving the chordal (i.e. Euclidean) distance between points on a sphere. The geodesic distance between points on a sphere can be interpreted as the angle between the points in radians, and a result by Magen [26] show that O((1/\"2) log n) dimensions preserve angles to within a relative factor of 1+ p \" (which is weaker than our result preserving the geodesic distance to within a relative factor of (1+ \"))."}, {"heading": "3 Definitions", "text": "Let D = (di j) be an n\u00d7 n matrix representing distances between all pairs of points in a set Y = {y1, . . . yn}. In general, we assume that D is symmetric di j = d ji , although our method does not formally require this. The multidimensional scaling problem takes as input Y , D and k, and asks for a mapping \u00b5 : Y \u2192 X from Y to a set of points X in a k-dimensional space T such that the difference between the original and resulting distances is minimized.\nThere are many different ways to measure the difference between the sets of distances, and these can be captured by the following general function\nC(X , D) = \u2211\ni\n\u2211\nj\nErr( f (x i , x j)\u2212 di j)\nwhere Err measures the discrepancy between the source and target distances, and f denotes the function that measures distance in the target space.\n\u2022 T = Rk,Err(\u03b4) = \u03b42, f (x , x \u2032) = \u2016x \u2212 x \u2032\u20162: This is a general form of the MDS problem, which we refer to as fMDS.\n\u2022 T = Rk,Err(\u03b4) = |\u03b4|, f (x , x \u2032) = \u2016x \u2212 x \u2032\u20162: This is a robust variant of MDS called rMDS, first suggested by Cayton and Dasgupta [8].\n\u2022 T = S \u00af k,Err(\u03b4) = |\u03b4| or \u03b42, f (x , x \u2032) is either chordal (c) or geodesic distance (g) on S \u00af k. We refer to this family of problems as {c,g}-{1,2}-sMDS.\nIt will be convenient to split the expression into component terms. We define\nCi(X , D, x i) = \u2211\nj\nErr( f (x i , x j)\u2212 di j)\nwhich allows us to write C(X , D) = \u2211\ni Ci(X , D, x i).\nNotes. The actual measure studied by Cayton and Dasgupta [8] is not rMDS. It is a variant which takes the absolute difference of the squared distance matrices. We call this measure r2MDS. Also, classical MDS does not appear in this list since it tries to minimize the error between similarities rather than distances. We refer to this measure as cMDS."}, {"heading": "4 Algorithm", "text": "We now present our algorithm PLACECENTER(X , D) that finds a mapping Y \u2192 X minimizing C(X , D). For now we assume that we are given an initial embedding X1 \u2208 R\u00af\nk to seed our algorithm. Our experiments indicate the SVD-based approach [35] is almost always the optimal way to seed the algorithm, and we use it unless specifically indicated otherwise.\nAlgorithm 1 PLACECENTER (D) Run any MDS strategy to obtain initial seed X . repeat \"\u2190 C(X , D) for i = 1 to n do\nx i \u2190 PLACEi(X , D) {this updates x i \u2208 X } end for\nuntil (\"\u2212 C(X , D)< t) {for a fixed threshold t} return X\nPLACECENTER operates by employing a technique from the block-relaxation class of heuristics. The cost function can be expressed as a sum of costs for each point x i , and so in each step of the inner loop we find the best placement for x i while keeping all other points fixed, using the algorithm PLACEi(X , D). A key insight driving our approach is that PLACEi(X , D) can be implemented either iteratively or exactly for a wide class of distance functions. The process terminates when over all i, invoking PLACEi(X , D) does not reduce the cost C(X , D) by more than a threshold t. The algorithm takes O(n2) for each iteration, since PLACEi(X , D) will take O(n) time and computing C(X , D) takes O(n2) time.\n4.1 A Geometric Perspective On Placei(X , D)\nThe routine PLACEi(X , D) is the heart of our algorithm. This routine finds the optimal placement of a fixed point x i with respect to the cost function Ci(X , D, x i) = \u2211\nj Err( f (x i , x j)\u2212 di j). Set r j = di j . Then the optimal placement of x i is given by the point x \u2217 minimizing the function\ng(x) = \u2211\nj\nErr( f (x , x j)\u2212 r j).\nNote that the terms f (x , x i) and ri = dii are zero, so we can ignore their presence in the summation for ease of notation.\nThere is a natural geometric interpretation of g(x), illustrated in Figure 1. Consider a sphere around the point x j of radius r j . Let x\u0302 j be the point on this sphere that intersects the ray from x j towards x . Then the distance f (x , x\u0302 j) = | f (x , x j)\u2212 r j|. Thus, we can rewrite g(x) as\ng(x) = \u2211\nj\nErr( f (x , x\u0302 j)).\nThis function is well-known in combinatorial optimization as the min-sum problem. For Err(\u03b4) = \u03b42, g(x) finds the point minimizing the sum-of-squared distances from a collection of fixed points (the 1-mean), which is the centroid x\u2217 = 1\nn\n\u2211\nj x\u0302 j . For Err(\u03b4) = |\u03b4|, g(x) finds the 1-median, the point minimizing the sum of distances from a collection of fixed points. Although there is no closed form expression for the 1-median, there are numerous algorithms for solving this problem both exactly [34] and approximately [4]. Methods that converge to the global optimum exist for any Err(\u03b4) = |\u03b4|p, p \u2264 2; it is known that if p is sufficiently larger than 2, then convergent methods may not exist [5].\nWhile g(x) can be minimized optimally for error functions Err of interest, the location of the points x\u0302 j depends on the location of the solution x\u2217, which is itself unknown! This motivates an alternating optimization procedure, where the current iterate x is used to compute x\u0302 j , and then these x\u0302 j are used as input to the min-sum problem to solve for the next value of x .\nAlgorithm 2 PLACEi(X , D) repeat \"\u2190 g(x i) for j = 1 to n do\nx\u0302 j \u2190 intersection of sphere of radius r j around x j with ray from x j towards x i end for x i \u2190 RECENTER({ x\u03021, x\u03022, . . . , x\u0302n})\nuntil (\"\u2212 g(x i)< t) {for a fixed threshold t} return x i"}, {"heading": "4.2 Implementing Recenter", "text": "Up to this point, the description of PLACECENTER and PLACE has been generic, requiring no specification of Err and f . In fact, all the domain-specificity of the method appears in RECENTER, which solves the min-sum\nproblem. We now demonstrate how different implementations of RECENTER allow us to solve the different variants of MDS discussed above.\n4.2.1 The original MDS: fMDS\nRecall from Section 3 that the fMDS problem is defined by Err(\u03b4) = \u03b42 and f (x , x \u2032) = \u2016x \u2212 x \u2032\u20162. Thus, g(x) = \u2211\nj \u2016x\u2212 x\u0302 j\u2016 2. As mentioned earlier, the minimum of this function is attained at x\u2217 = (1/n)\n\u2211\nj x\u0302 j . Thus, RECENTER({ x\u03021, x\u03022, . . . , x\u0302n}) merely outputs (1/n) \u2211 j x\u0302 j , and takes O(n) time per invocation.\n4.2.2 Robust MDS: rMDS\nThe robust MDS problem rMDS is defined by Err(\u03b4) = |\u03b4| and f (x , x \u2032) = \u2016x \u2212 x \u2032\u20162. Minimizing the resulting function g(x) yields the famous Fermat-Weber problem, or the 1-median problem as it is commonly known. An exact iterative algorithm for solving this problem was given by Weiszfeld [34], and works as follows. At each step of PLACEi the value x i is updated by\nx i \u2190 \u2211\nj\nx\u0302 j \u2016x i \u2212 x\u0302 j\u2016\n,\n\u2211\nj\n1\n\u2016x i \u2212 x\u0302 j\u2016 .\nThis algorithm is guaranteed to converge to the optimal solution [24, 28], and in most settings converges quadratically [21].\nOther norms and distances. If Err(\u03b4) = |\u03b4|p, 1 < p < 2, then an iterative algorithm along the same lines as the Weiszfeld algorithm can be used to minimize g(x) optimally [5]. In practice, this is the most interesting range of values for p. It is also known that for p sufficiently larger than 2, this iterative scheme may not converge.\nWe also can tune PLACECENTER to the r2MDS problem (using squared distances) by setting r j = d2i j ."}, {"heading": "4.2.3 Spherical MDS", "text": "Spherical MDS poses special challenges for the implementation of RECENTER. Firstly, it is no longer obvious what the definition of x\u0302 j should be, since the \u201cspheres\u201d surrounding points must also lie on the sphere. Secondly, consider the case where Err(\u03b4) = \u03b42, and f (x , x \u2032) is given by geodesic distance on the sphere. Unlike in the case of Rk, we no longer can solve for the minimizer of g(x) by computing the centroid of the given points, because this centroid will not in general lie on the sphere, and even computing the centroid followed by a projection onto the sphere will not guarantee optimality.\nThe first problem can be solved easily. Rather than draw spheres around each x j , we draw geodesic spheres, which are the set of points at a fixed geodesic distance from x j . On the sphere, this set of points can be easily described as the intersection of an appropriately chosen halfplane with the sphere. Next, instead of computing the intersection of this geodesic sphere with the ray from x j towards the current estimate of x i , we compute the intersection with a geodesic ray from x j towards x i .\nThe second problem can be addressed by prior work on computing min-sums on manifolds. Karcher [20] proposed an iterative scheme for the geodesic sum-of-squares problem that always converges as long as the points do not span the entire sphere. His work extends (for the same functions Err, f ) to points defined on more general Riemannian manifolds satisfying certain technical conditions. It runs in O(n) time per iteration.\nFor the robust case (Err(\u03b4) = |\u03b4|), the Karcher scheme no longer works. For this case, we make use of a Weiszfeld-like adaption [15] that again works on general Riemannian manifolds, and on the sphere in particular. Like the Weiszfeld scheme, this approach takes O(n) time per iteration."}, {"heading": "4.3 Convergence Proofs", "text": "Here we prove that each step of PLACECENTER converges as long as the recursively called procedures reduce the relevant cost functions. Convergence is defined with respect to a cost function \u03ba, so that an algorithm converges if at each step \u03ba decreases until the algorithm terminates.\nTheorem 4.1. If each call to x\u0303 i \u2190 PLACEi(X , D) decreases the cost Ci(X , D, x i), then PLACECENTER(D) converges with respect to C(\u00b7, D).\nProof. Let X\u0303 \u2190 PLACEi(X , D) result from running an iteration of PLACEi(X , D). Let X\u0303 = {x1, . . . , x i\u22121, x\u0303 i , x i+1, . . . , xn}. Then we can argue\nC(X , D)\u2212 C(X\u0303 , D) = 2 \u2211\nj=1\nErr( f (x i , x j)\u2212 di, j)\u2212 2 \u2211\nj=1\nErr( f ( x\u0303 i , x j)\u2212 di, j)\n= 2Ci(X , D, x i)\u2212 2Ci(X\u0303 , D, x\u0303 i)> 0.\nThe last line follows because X and X\u0303 only differ at x i versus x\u0303 i , and by assumption on PLACEi(X , D), this sub-cost function must otherwise decrease.\nTheorem 4.2. If each call x i \u2190 RECENTER(X\u0302 ) reduces \u2211n j=1 f (x i , x\u0302 j) p, then PLACEi(X , D, x i) converges with respect to Ci(X , D, \u00b7).\nProof. First we can rewrite\nCi(X , D, x i) = n \u2211\nj=1\nErr( f (x i , x j)\u2212 di, j) = n \u2211\nj=1\nErr(( f (x i , x\u0302 j) + di, j)\u2212 di, j) = n \u2211\nj=1\nErr( f (x i , x\u0302 j)).\nSince Err( f (x i , x\u0302 j))measures the distance to the sphere \u25e6 j , choosing x \u2032i to minimize (or decrease) \u2211n j=1 Err( f (x \u2032 i , x\u0302 j)) must decrease the sum of distances to each point x\u0302 j on each sphere \u25e6 j . Now let x\u0302 \u2032j be the closest point to x \u2032 i on \u25e6 j . Err( f (x \u2032i , x\u0302 \u2032 j))\u2264 Err( f (x \u2032 i , x j)) and thus\nCi(X , D, x \u2032 i) =\nn \u2211\nj=1\nErr( f (x \u2032i , x\u0302 \u2032 j))\u2264\nn \u2211\nj=1\nErr( f (x \u2032i , x\u0302 \u2032 j))\u2264\nn \u2211\nj=1\nErr( f (x i , x\u0302 j)) = Ci(X , D, x i)\nwhere equality only holds if x i = x \u2032i , in which case the algorithm terminates."}, {"heading": "5 Experiments", "text": "In this section we evaluate the performance of PLACECENTER (PC). Since PC generalizes to many different cost functions, we compare it with the best known algorithm for each cost function, if one exists. For the fMDS problem the leading algorithm is SMACOF [13]; for the r2MDS problem the leading algorithm is by Cayton and Dasgupta (CD) [8]. We know of no previous scalable algorithm designed for rMDS. We note that the Cayton-Dasgupta algorithm REE does not exactly solve the r2MDS problem. Instead, it takes a non-Euclidean distance matrix and finds a Euclidean distance matrix that minimizes the error without any rank restrictions. Thus, as suggested by the authors [8], to properly compare the algorithms, we let CD refer to running REE and then projecting the result to a k-dimensional subspace using the SVD technique [35] (our plots show this projection after each step). With regards to each of these Euclidean measures we compare our algorithm with SMACOF and CD. We also compare with the popular SVD-based method [35], which solves the related cMDS problem based on similarities, by seeding all three iterative techniques with the results of the closed-form SVD-based solution.\nThen we consider the family of spherical MDS problems {c,g}-{1,2}-sMDS. We compare against a version of SMACOF-Q [13] that is designed for data restricted to a low dimensional sphere, specifically for the c-2SMDS measure. We compare this algorithm to ours under the c-2-SMDS measure (for a fair comparison with SMACOF-Q) and under the g-1-SMDS measure which is the most robust to noise.\nThe subsections that follow focus on individual cost measures. We then discuss the overall behavior of our algorithm in Section 5.5.\nData sets, code, and setup. Test inputs for the algorithms are generated as follows. We start with input consisting of a random point set with n = 300 points in R\n\u00af d for d = 200, with the target space T = R \u00af k with\nk = 10. Many data sets in practice have much larger parameters n and d, but we limit ourselves to this range because for larger values CD becomes prohibitively slow. The data is generated to first lie on a k-dimensional subspace, and then (full-dimensional) Poisson noise is applied to all points up to a magnitude of 30% of the variation in any dimension. Finally, we construct the Euclidean distance matrix D which is provided as input to the algorithms.\nThese data sets are Euclidean, but \u201cclose\u201d to k-dimensional. To examine the behavior of the algorithms on distance matrices that are non-Euclidean, we generate data as before in a k-dimensional subspace and generate the resulting distance matrix D. Then we perturb a fraction of the elements of D (rather than perturbing the points) with Poisson noise. The fraction perturbed varies in the set (2%, 10%,30%, 90%).\nAll algorithms were implemented in MATLAB. For SMACOF, we used the implementation provided by Bronstein [7], and built our own implementation of SMACOF-Q around it. For all other algorithms, we used our own implementation1. In all cases, we compare performance in terms of the error function Err as a function of clock time.\n5.1 The rMDS Problem\nFigure 2(a) shows the cost function Err associated with rMDS plotted with respect to runtime. PLACECENTER always reaches the best local minimum, partially because only PLACECENTER can be adjusted for the rMDS problem. We also observe that the runtime is comparable to SMACOF and much faster than CD in order to get to the same Err value. Although SMACOF initially reaches a smaller cost that PC, it later converges to a larger cost because it optimizes a different cost function (fMDS).\nWe repeat this experiment in Figure 2(b) for different values of k (equal to {2,20, 50,150}) to analyze the performance as a function of k. Note that PC performs even better for lower k in relation to CD. This is likely as a result of CD\u2019s reliance on the SVD technique to reduce the dimension. At smaller k, the SVD technique has a tougher job to do, and optimizes the wrong metric. Also for k = 150 note that CD oscillates in its cost; this is again because the REE part finds a nearby Euclidean distance matrix which may be inherently very high dimensional and the SVD projection is very susceptible to changes in this matrix for such large k. We observe that SMACOF is the fastest method to reach a low cost, but does not converge to the lowest cost value. The reason it achieves a cost close to that of PC is that for this type of data the rMDS and fMDS cost functions are fairly similar.\nIn Figure 3 we evaluate the effect of changing the amount of noise added to the input distance matrix D, as described above. We consider two variants of the CD algorithm, one where it is seeded with an SVD-based seed (marked CD+SVD) and one where it is seeded with a random projection to a k-dimensional subspace (marked CD+rand). In both cases the plots show the results of the REE algorithm after SVD-type projections back to a k-dimensional space.\nThe CD+SVD technique consistently behaves poorly and does not improve with further iterations. This probably is because the REE component finds the closest Euclidean distance matrix which may correspond to points in a much high dimensional space, after which it is difficult for the SVD to help. The CD+rand\n1All of our code may be found at http://www.cs.utah.edu/~arvind/smds.html.\napproach does much better, likely because the random projection initializes the procedure in a reasonably low dimensional space so REE can find a relatively low dimension Euclidean distance matrix that is nearby. SMACOF is again the fastest algorithm, but with more noise, the difference between fMDS and rMDS is larger, and thus SMACOF converges to a configuration with much higher cost than PC. We reiterate that PC consistently converges to the lowest cost solution among the different methods, and consistently is either the fastest or is comparable to the fastest algorithm. We will see this trend repeated with other cost measures as well.\n5.2 The fMDS Problem\nWe next evaluate the algorithms PC, SMACOF, and CD under the fMDS distance measure. The results are very similar to the rMDS case except now both SMACOF and PC are optimizing the correct distance measure and converge to the same local minimum. SMACOF is still slightly faster that PC, but since they both run very fast, the difference is of the order of less than a second even in the very worst part of the cost/time tradeoff curve shown in Figure 4(a). Note that CD performs poorly under this cost function here except when k = 50. For smaller values of k, the SVD step does not optimize the correct distance and for larger k the REE part is likely finding an inherently very high dimensional Euclidean distance matrix, making the SVD projection very noisy.\nFor the fMDS measure, SMACOF and PC perform very similarly under different levels of noise, both converging to similar cost functions with SMACOF running a bit faster, as seen in Figure 4(b). CD consistently runs slower and converges to a higher cost solution.\n5.3 The r2MDS Problem\nIn this setting we would expect CD to perform consistently as well as PC because both minimize the same cost function. However, this is not always the case because CD requires the SVD step to generate a point set in R\n\u00af k.\nAs seen in Figure 5 this becomes a problem when k is small (k = 2,10). For medium values of k, CD converges slightly faster than PC and sometimes to a slightly lower cost solution, but again for large k (= 150), the REE\npart has trouble handling the amount of error and the solution cost oscillates. SMACOF is again consistently the fastest to converge, but unless k is very large (i.e. k = 150) then it converges to a significantly worse solution because the fMDS and r2MDS error functions are different."}, {"heading": "5.4 The Spherical MDS Problem", "text": "For the spherical MDS problem we compare PC against SMACOF-Q, an adaptation of SMACOF to restrict data points to a low-dimensional sphere, and a technique of Elad, Keller and Kimmel [14]. It turns out that the Elad et.al. approach consistently performs poorly compared to both other techniques, and so we do not display it in our reported results. SMACOF-Q basically runs SMACOF on the original data set, but also adds one additional point p0 at the center of the sphere. The distance d0,i between any other point pi and p0 is set to be 1 thus encouraging all other points to be on a sphere, and this constraint is controlled by a weight factor \u03ba, a larger \u03ba implying a stronger emphasis on satisfying this constraint. Since the solution produced via this procedure may not lie on the sphere, we normalize all points to the sphere after each step for a fair comparison.\nHere we compare PC against SMACOF-Q in the g-1-SMDS (Figure 6(a)) and the c-2-SMDS (Figure 6(b)) problem. For g-1-SMDS, PC does not converge as quickly as SMACOF-Q with small \u03ba, but it reaches a better cost value. However, when SMACOF-Q is run with a larger \u03ba, then PC runs faster and reaches nearly the same cost value. For our input data, the solution has similar g-1-MDS and c-1-MDS cost. When we compare SMACOF-Q with PC under c-2-MDS (Figure 6(b)) then for an optimal choice of \u03ba in SMACOF-Q, both PC and SMACOF-Q perform very similarly, converging to the same cost function and in about the same time. But for larger choices of \u03ba SMACOF-Q does much worse than PC.\nIn both cases, it is possible to find a value of \u03ba that allows SMACOF-Q to match PC. However, this value is different for different settings, and varies from input to input. The key observation here is that since PC is parameter-free, it can be run regardless of the choice of input or cost function, and consistently performs well."}, {"heading": "5.5 Summary Of Results", "text": "In summary, here are the main conclusions that can be drawn from this experimental study. Firstly, PC is consistently among the top performing methods, regardless of the choice of cost function, the nature of the input, or the level of noise in the problem. Occasionally, other methods will converge faster, but will not in general return a better quality answer, and different methods have much more variable behavior with changing\ninputs and noise levels."}, {"heading": "6 A JL Lemma for Spherical Data", "text": "In this section we present a Johnson-Lindenstrauss-style bound for mapping data from a high dimensional sphere to a low-dimensional sphere while preserving the distances to within a multiplicative error of (1+ \").\nConsider a set Y \u2282 S \u00af d \u2282 R \u00af d+1 of n points, defining a distance matrix D where the element di, j represents the geodesic distance between yi and y j on S\u00af k. We seek an embedding of Y into S \u00af\nd that preserves pairwise distances as much as possible. For a set Y \u2208 S\n\u00af d and a projection \u03c0(Y ) = X \u2282 S \u00af k we say the X has \u03b3-distortion\nfrom Y if these exists a constant c such that for all x i , x j \u2208 X\n(1\u2212 \u03b3) f (yi , y j)\u2264 c f (x i , x j)\u2264 (1+ \u03b3) f (yi , y j).\nFor a subspace H = R \u00af k, let \u03c0H(Y ) be the projection of Y \u2208 R\u00af d onto H and then scaled by d/k. For X \u2208 R \u00af k,\nlet S(X ) be the projection to S \u00af k\u22121, that is for all x \u2208 X , the corresponding point in S(X ) is x/||x ||. When f (yi , y j) = ||yi \u2212 y j||, and Y \u2208 R\u00af\nd , then the Johnson-Lindenstrauss (JL) Lemma [19] says that if H \u2282 R\n\u00af d is a random k-dimensional linear subspace with k = O((1/\"2) log(n/\u03b4)), then X = \u03c0H(Y ) has \"-\ndistortion from Y with probability at least 1\u2212\u03b4. We now present the main result of this section. We note that recent results [1] have shown similar results for point on a variety of manifolds (including spheres) where projections preserve Euclidean distances. We reiterate that our results extend this to geodesic distances on spheres which can be seen as angle \u2220x ,y between the vectors to points x , y \u2208 S\n\u00af k. Another recent result [26] shows that k = O((1/\"2) log(n/\u03b4)) dimensions\npreserves p \"-distortion in angles, which is weaker than the following result.\nTheorem 6.1. Let Y \u2282 S \u00af d \u2282 R \u00af d+1, and let H = R \u00af k+1 be a random subspace of R \u00af d with k = O((1/\"2) log(n/\u03b4)) with \" \u2208 (0, 1/4]. Let f (yi , y j) measure the geodesic distance on S\u00af d (or S \u00af\nk as appropriate). Then S(\u03c0H(Y )) has \"-distortion from Y with probability at least 1\u2212\u03b4.\nThis implies that if we project n data points that lie on any high-dimensional sphere to a low-dimensional sphere S\n\u00af k with k \u223c log n, then the pairwise distances are each individually preserved. Before we proceed with\nthe proof, we require a key technical lemma.\nLemma 6.1. For \" \u2208 [0,0.5] and x \u2208 [0,0.7],\n(1) sin((1\u2212 2\")x)\u2264 (1\u2212 \") sin(x), and\n(2) sin((1+ 2\")x)\u2265 (1+ \")sin(x).\ni || is as small as possible (lies on inner circle) and ||x\nmax\ni \u2212 x\nmax\nj || is as large as possible (on\nthe outer edges of the disks of diameter \"/8 shifted down from dashed line of length ||yi \u2212 y j||. Bounds for xmini and x min j are derived symmetrically.\nProof. Let g\"(x) = (1\u2212 \") sin x \u2212 sin((1\u2212 2\")x). We will show that for x \u2208 [0, 1] and \" \u2208 [0, 0.5], g\"(x) is concave. This implies that it achieves its minimum value at the boundary. Now g\"(0) = 0 for all \", and it can be easily shown that g\"(0.7)\u2265 0 for \" \u2208 [0, 0.5]. This will therefore imply that g\"(x)\u2265 0 in the specified range.\nIt remains to show that g\"(x) is concave in [0,0.7].\ng \u2032\u2032\" (x) = (1\u2212 2\") 2 sin((1\u2212 2\")x)\u2212 (1\u2212 \") sin x\n\u2264 (1\u2212 \")(sin((1\u2212 2\")x)\u2212 sin x)\nwhich is always negative for \" \u2208 [0, 0.5] and since sin x is increasing in the range [0, 0.7]. This proves the first part of the lemma. For the second part, observe that h\"(x) = sin((1+2\")x)\u2212(1+\")sin(x) can be rewritten as h\"(x) = g\u2212\"(\u2212x). The rest of the argument follows along the same lines, by showing that h\"(x) is concave in the desired range using that h\u2032\u2032\" (x) = g \u2032\u2032 \u2212\"(\u2212x).\nWhile the upper bound of 0.7 on x is not tight, it is close. The actual bound (evaluated by direct calculation) is slightly over 0.72.\nProof of Theorem 6.1. Let X = \u03c0H(Y ). We consider two cases, (Short Case) when \u2016yi \u2212 y j\u2016 \u2264 1/2 and (Long Case) when \u2016yi \u2212 y j\u2016 \u2208 (1/2,2].\nShort Case: First consider points yi , y j \u2208 S\u00af d such that ||yi\u2212 y j|| \u2264 1/2. Note that ||yi\u2212 y j||= 2 sin(\u2220yi ,y j/2),\nsince ||yi||= ||y j||= 1. By JL, we know that there exists a constant c such that\n(1\u2212 \"/8)||yi \u2212 y j|| \u2264 c||x i \u2212 x j|| \u2264 (1+ \"/8)||yi \u2212 y j||.\nWe need to compare the angle \u2220x i ,x j with that of \u2220yi ,y j . The largest \u2220x i ,x j can be is when c||x i|| = c||x j|| = (1\u2212 \"/8) is as small as possible, and so ||cx i \u2212 cx j|| = (1+ \"/8)||yi \u2212 y j|| is as large as possible. See Figure 7. In this case, we have\n(||cx i||+ ||cx j||) sin(\u2220x i ,x j/2) \u2264 ||cx i \u2212 cx j|| 2(1\u2212 \"/8) sin(\u2220x i ,x j/2) \u2264 (1+ \"/8)||yi \u2212 y j|| 2(1\u2212 \"/8) sin(\u2220x i ,x j/2) \u2264 (1+ \"/8)2sin(\u2220yi ,y j/2)\nsin(\u2220x i ,x j/2) \u2264 1+ \"/8 1\u2212 \"/8 sin(\u2220yi ,y j/2),\nwhich for \" < 4 implies sin(\u2220x i ,x j/2)\u2264 (1+ \"/2) sin(\u2220yi ,y j/2).\nSimilarly, we can show when \u2220x i ,x j is as small as possible (when c||x i|| = c||x j|| = (1+ \") and ||cx i \u2212 cx j|| = (1\u2212 \")||yi \u2212 y j||), then\n(1\u2212 \"/2) sin(\u2220yi ,y j/2)\u2264 sin(\u2220x i ,x j/2).\nWe can also show (via Lemma 6.1) that since ||yi \u2212 y j|| \u2264 1/2 implies \u2220yi ,y j < 0.7 we have\nsin((1\u2212 \")\u2220yi ,y j )\u2264 (1\u2212 \"/2) sin(\u2220yi ,y j )\nand (1+ \"/2) sin(\u2220yi ,y j )\u2264 sin((1+ \")\u2220yi ,y j ).\nThus, we have sin((1\u2212 \")\u2220yi ,y j/2) \u2264 sin(\u2220x i ,x j/2) \u2264 sin((1+ \")\u2220yi ,y j/2)\n(1\u2212 \")\u2220yi ,y j/2 \u2264 \u2220x i ,x j/2 \u2264 (1+ \")\u2220yi ,y j/2 (1\u2212 \")\u2220yi ,y j \u2264 \u2220x i ,x j \u2264 (1+ \")\u2220yi ,y j .\nLong Case: For ||yi \u2212 y j|| \u2208 (1/2,2], we consider 6 additional points y (h) i, j \u2208 S\u00af d+1 (for h \u2208 [1 : 6]) equally spaced between yi and y j on the shortest great circle connecting them. Let Y\u0302 be the set Y plus all added points {y(h)i, j }h=[1:6]. Note that |Y\u0302 |= O(n 2), so by JL we have that\n(1\u2212 \"/8)||yi \u2212 y\u0302i, j|| \u2264 c||x i \u2212 x\u0302 i, j|| \u2264 (1+ \"/8)||yi \u2212 y\u0302i, j||.\nFor notational convenience let yi = y (0) i, j and y j = y (7) i, j . Since for \u2016yi\u2212 y j\u2016 \u2208 (1/2, 2] then \u2016y (h) i, j \u2212 y (h+1) i, j \u2016 \u2264 1/2, for h \u2208 [0 : 6]. This follows since the geodesic length of the great circular arc through yi and y j is at most \u03c0, and \u03c0/7 < 1/2. Then the chordal distance for each pair \u2016y(h)i, j \u2212 y (h+1) i, j \u2016 is upper bounded by the geodesic distance. Furthermore, by invoking the short case, for any pair\n(1\u2212 \")\u2220y(h)i, j ,y(h+1)i, j \u2264 \u2220x (h)i, j ,x (h+1)i, j \u2264 (1+ \")\u2220y(h)i, j ,y(h)i, j .\nThen since projections preserve coplanarity (specifically, the points 0 and y(h)i, j for h \u2208 [0 : 7] are coplanar, hence 0 and x (h)i, j for h \u2208 [0 : 7] are coplanar), we can add together the bounds on angles which all lie on a single great circle.\n(1\u2212 \")\u2220yi ,y j \u2264 (1\u2212 \") 6 \u2211\nh=0\n\u2220y(h)i, j ,y(h+1)i, j \u2264 \u22116 h=0\u2220x (h)i, j ,x (h+1)i, j \u2264 (1+ \")\n6 \u2211\nh=0\n\u2220y(h)i, j ,y(h+1)i, j \u2264min{\u03c0, (1+ \")\u2220yi ,y j}\n(1\u2212 \")\u2220yi ,y j \u2264 \u2220x i ,x j \u2264min{\u03c0, (1+ \")\u2220yi ,y j}."}], "references": [{"title": "On embeddings of moving points in Euclidean space", "author": ["P.K. Agarwal", "S. Har-Peled", "H. Yu"], "venue": "Proceedings 23rd Symposium on Computational Geometry,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Latent Dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res., 3:993\u20131022,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Modern Multidimensional Scaling", "author": ["I. Borg", "P.J.F. Groenen"], "venue": "Springer,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast approximations for sums of distances, clustering and the Fermat\u2013Weber problem", "author": ["P. Bose", "A. Maheshwari", "P. Morin"], "venue": "Comput. Geom. Theory Appl., 24(3):135\u2013146,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Global convergence of a generalized iterative procedure for the minisum location problem with `p distances", "author": ["J. Brimberg", "R.F. Love"], "venue": "Operations Research, 41(6):1153\u20131163,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1993}, {"title": "Numerical Geometry of Non-Rigid Shapes", "author": ["A.M. Bronstein", "M.M. Bronstein", "R. Kimmel"], "venue": "Springer,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust Euclidean embedding", "author": ["L. Cayton", "S. Dasgupta"], "venue": "ICML \u201906: Proceedings of the 23rd International Conference on Machine Learning, pages 169\u2013176,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Local multidimensional scaling for nonlinear dimension reduction, graph drawing, and proximity analysis", "author": ["L. Chen", "A. Buja"], "venue": "Journal of the Americal Statistical Association, 104:209\u2013219,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Multidimensional Scaling, Second Edition", "author": ["T.F. Cox", "M.A.A. Cox"], "venue": "Chapman & Hall/CRC, September", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "CVPR \u201905: Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 886\u2013893,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Applications of convex analysis to multidimensional scaling", "author": ["J. de Leeuw"], "venue": "Recent Developments in Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1977}, {"title": "Multidimensional scaling using majorization: SMACOF in R", "author": ["J. de Leeuw", "P. Mair"], "venue": "Technical Report 537, UCLA Statistics Preprints Series,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Texture mapping via spherical multi-dimensional scaling", "author": ["A.E. Elad", "Y. Keller", "R. Kimmel"], "venue": "R. Kimmel, N. A. Sochen, and J. Weickert, editors, Scale-Space, volume 3459 of Lecture Notes in Computer Science, pages 443\u2013455. Springer,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "The Geometric Median on Riemannian Manifolds with Application to Robust Atlas Estimation", "author": ["P.T. Fletcher", "S. Venkatasubramanian", "S. Joshi"], "venue": "Neuroimage (invited to special issue), 45(1):S143\u2013S152, March", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Distortion measures for speech processing", "author": ["R. Gray", "A. Buzo", "A. Gray Jr", "Y. Matsuyama"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on, 28(4):367\u2013376, Aug", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1980}, {"title": "Low-distortion embeddings of finite metric spaces", "author": ["P. Indyk", "J. Matousek"], "venue": "Handbook of Discrete and Computational Geometry, pages 177\u2013196. CRC Press,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning to Classify Text Using Support Vector Machines \u2013 Methods, Theory, and Algorithms", "author": ["T. Joachims"], "venue": "Kluwer/Springer,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Extensions of Lipschitz mappings into Hilbert space", "author": ["W.B. Johnson", "J. Lindenstrauss"], "venue": "Contemporary Mathematics, 26:189\u2013206,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1984}, {"title": "Riemannian center of mass and mollifier smoothing", "author": ["H. Karcher"], "venue": "Comm. on Pure and Appl. Math., 30:509\u2013 541,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1977}, {"title": "Local convergence in Fermat\u2019s problem", "author": ["I.N. Katz"], "venue": "Mathematical Programming, 6:89\u2013104,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1974}, {"title": "Multidimensional scaling by optimizing goodness of fit to nonmetric hypothesis", "author": ["J.B. Kruskal"], "venue": "Psychometrika, 29:1\u201327,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1964}, {"title": "Multidimensional scaling", "author": ["J.B. Kruskal", "M. Wish"], "venue": "E. M. Uslander, editor, Quantitative Applications in the Social Sciences, volume 11. Sage Publications,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1978}, {"title": "A note on Fermat\u2019s problem", "author": ["H.W. Kuhn"], "venue": "Mathematical Programming, 4:98\u2013107,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1973}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "Int. J. Comput. Vision, 60(2),", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Dimensionality reductions that preserve volumes and distance to affine spaces, and their algorithmic applications", "author": ["A. Magen"], "venue": "Proceedings of the 6th International Workshop on Randomization and Approximation Techniques, Lecture Notes In Computer Science; Vol. 2483,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}, {"title": "Inequalities: Theory of Majorization and Its Applications", "author": ["A.W. Marshall", "I. Olkin"], "venue": "Academic Press,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1979}, {"title": "On the convergence of a class of iterative methods for solving the Weber location problem", "author": ["L.M. Ostresh"], "venue": "Operations Research, 26:597\u2013609,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1978}, {"title": "Distributional clustering of English words", "author": ["F. Pereira", "N. Tishby", "L. Lee"], "venue": "Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 183\u2013190,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1993}, {"title": "Rank reduction of correlation matrices by majorization", "author": ["R. Pietersz", "P.J.F. Groenen"], "venue": "Technical Report 519086, SSRN,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Embedding images in non-flat spaces", "author": ["R. Pless", "I. Simon"], "venue": "Proc. of the International Conference on Imaging Science, Systems, and Technology,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2002}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["T. Sarl\u00f3s"], "venue": "Proceedings 47th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Multidimensional scaling: I", "author": ["W.S. Torgerson"], "venue": "theory and method. Psychometrika, 17:401\u2013419,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1952}, {"title": "Sur le point pour lequel la somme des distances de n points donn\u00e9s est minimum", "author": ["E. Weiszfeld"], "venue": "Tohoku Math. J., 43:355\u2013386,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1937}, {"title": "Discussion of a set of points in terms of their mutual distances", "author": ["G. Young", "A.S. Householder"], "venue": "Psychometrika, 3:19\u201322,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1938}], "referenceMentions": [{"referenceID": 21, "context": "1 Introduction Multidimensional scaling (MDS) [23, 10, 3] is a widely used method for embedding a general distance matrix into a low dimensional Euclidean space, used both as a preprocessing step for many problems, as well as a visualization tool in its own right.", "startOffset": 46, "endOffset": 57}, {"referenceID": 8, "context": "1 Introduction Multidimensional scaling (MDS) [23, 10, 3] is a widely used method for embedding a general distance matrix into a low dimensional Euclidean space, used both as a preprocessing step for many problems, as well as a visualization tool in its own right.", "startOffset": 46, "endOffset": 57}, {"referenceID": 2, "context": "1 Introduction Multidimensional scaling (MDS) [23, 10, 3] is a widely used method for embedding a general distance matrix into a low dimensional Euclidean space, used both as a preprocessing step for many problems, as well as a visualization tool in its own right.", "startOffset": 46, "endOffset": 57}, {"referenceID": 33, "context": "MDS has been studied and used in psychology since the 1930s [35, 33, 22] to help visualize and analyze data sets where the only input is a distance matrix.", "startOffset": 60, "endOffset": 72}, {"referenceID": 31, "context": "MDS has been studied and used in psychology since the 1930s [35, 33, 22] to help visualize and analyze data sets where the only input is a distance matrix.", "startOffset": 60, "endOffset": 72}, {"referenceID": 20, "context": "MDS has been studied and used in psychology since the 1930s [35, 33, 22] to help visualize and analyze data sets where the only input is a distance matrix.", "startOffset": 60, "endOffset": 72}, {"referenceID": 6, "context": "More recently MDS has become a standard dimensionality reduction and embedding technique to manage the complexity of dealing with large high dimensional data sets [8, 9, 31, 6].", "startOffset": 163, "endOffset": 176}, {"referenceID": 7, "context": "More recently MDS has become a standard dimensionality reduction and embedding technique to manage the complexity of dealing with large high dimensional data sets [8, 9, 31, 6].", "startOffset": 163, "endOffset": 176}, {"referenceID": 29, "context": "More recently MDS has become a standard dimensionality reduction and embedding technique to manage the complexity of dealing with large high dimensional data sets [8, 9, 31, 6].", "startOffset": 163, "endOffset": 176}, {"referenceID": 5, "context": "More recently MDS has become a standard dimensionality reduction and embedding technique to manage the complexity of dealing with large high dimensional data sets [8, 9, 31, 6].", "startOffset": 163, "endOffset": 176}, {"referenceID": 10, "context": "Thus, in addition to the standard formulation [12], many variants of MDS have been proposed, based on changing the underlying error function [35, 8].", "startOffset": 46, "endOffset": 50}, {"referenceID": 33, "context": "Thus, in addition to the standard formulation [12], many variants of MDS have been proposed, based on changing the underlying error function [35, 8].", "startOffset": 141, "endOffset": 148}, {"referenceID": 6, "context": "Thus, in addition to the standard formulation [12], many variants of MDS have been proposed, based on changing the underlying error function [35, 8].", "startOffset": 141, "endOffset": 148}, {"referenceID": 11, "context": "a low dimensional sphere), and various heuristics for MDS in this setting have also been proposed [13, 6].", "startOffset": 98, "endOffset": 105}, {"referenceID": 5, "context": "a low dimensional sphere), and various heuristics for MDS in this setting have also been proposed [13, 6].", "startOffset": 98, "endOffset": 105}, {"referenceID": 5, "context": "Spherical MDS has applications in texture mapping and image analysis [6], and is a generalization of the spherical dimensionality reduction problem, where the goal is to map points from a high dimensional sphere onto a lowdimensional sphere.", "startOffset": 69, "endOffset": 72}, {"referenceID": 27, "context": "A spherical dimensionality reduction result is an important step to representing high dimensional distributions in a lowerdimensional space of distributions, and will have considerable impact in domains that represent data natively as histograms or distributions, such as in document processing [29, 18, 2], image analysis [25, 11] and speech recognition [16].", "startOffset": 295, "endOffset": 306}, {"referenceID": 16, "context": "A spherical dimensionality reduction result is an important step to representing high dimensional distributions in a lowerdimensional space of distributions, and will have considerable impact in domains that represent data natively as histograms or distributions, such as in document processing [29, 18, 2], image analysis [25, 11] and speech recognition [16].", "startOffset": 295, "endOffset": 306}, {"referenceID": 1, "context": "A spherical dimensionality reduction result is an important step to representing high dimensional distributions in a lowerdimensional space of distributions, and will have considerable impact in domains that represent data natively as histograms or distributions, such as in document processing [29, 18, 2], image analysis [25, 11] and speech recognition [16].", "startOffset": 295, "endOffset": 306}, {"referenceID": 23, "context": "A spherical dimensionality reduction result is an important step to representing high dimensional distributions in a lowerdimensional space of distributions, and will have considerable impact in domains that represent data natively as histograms or distributions, such as in document processing [29, 18, 2], image analysis [25, 11] and speech recognition [16].", "startOffset": 323, "endOffset": 331}, {"referenceID": 9, "context": "A spherical dimensionality reduction result is an important step to representing high dimensional distributions in a lowerdimensional space of distributions, and will have considerable impact in domains that represent data natively as histograms or distributions, such as in document processing [29, 18, 2], image analysis [25, 11] and speech recognition [16].", "startOffset": 323, "endOffset": 331}, {"referenceID": 14, "context": "A spherical dimensionality reduction result is an important step to representing high dimensional distributions in a lowerdimensional space of distributions, and will have considerable impact in domains that represent data natively as histograms or distributions, such as in document processing [29, 18, 2], image analysis [25, 11] and speech recognition [16].", "startOffset": 355, "endOffset": 359}, {"referenceID": 8, "context": "There is a general taxonomy of MDS methods [10]; in this paper we will focus primarily the metric and generalized MDS problems.", "startOffset": 43, "endOffset": 47}, {"referenceID": 21, "context": "The traditional formulation of MDS [23] assumes that the distance matrix D arises from points in some ddimensional Euclidean space.", "startOffset": 35, "endOffset": 39}, {"referenceID": 31, "context": "These similarities also arise from many psychology data sets directly [33, 35].", "startOffset": 70, "endOffset": 78}, {"referenceID": 33, "context": "These similarities also arise from many psychology data sets directly [33, 35].", "startOffset": 70, "endOffset": 78}, {"referenceID": 25, "context": "A more general approach called SMACOF that drops the Euclidean assumption uses a technique known as stress majorization [27, 12, 13].", "startOffset": 120, "endOffset": 132}, {"referenceID": 10, "context": "A more general approach called SMACOF that drops the Euclidean assumption uses a technique known as stress majorization [27, 12, 13].", "startOffset": 120, "endOffset": 132}, {"referenceID": 11, "context": "A more general approach called SMACOF that drops the Euclidean assumption uses a technique known as stress majorization [27, 12, 13].", "startOffset": 120, "endOffset": 132}, {"referenceID": 11, "context": "It has been adapted to many other MDS variants as well including restrictions of data to lie on quadratic surfaces and specifically spheres [13].", "startOffset": 140, "endOffset": 144}, {"referenceID": 6, "context": "Since the sum-of-squares error metric is sensitive to outliers, Cayton and Dasgupta [8] proposed a robust variant based on an `1 error metric.", "startOffset": 84, "endOffset": 87}, {"referenceID": 28, "context": "Among them are majorization methods ([30] and SMACOF-Q [13]), a multiresolution approach due to Elad, Keller, and Kimmel [14] and an approach based on computing the classical MDS and renormalizing [31].", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "Among them are majorization methods ([30] and SMACOF-Q [13]), a multiresolution approach due to Elad, Keller, and Kimmel [14] and an approach based on computing the classical MDS and renormalizing [31].", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "Among them are majorization methods ([30] and SMACOF-Q [13]), a multiresolution approach due to Elad, Keller, and Kimmel [14] and an approach based on computing the classical MDS and renormalizing [31].", "startOffset": 121, "endOffset": 125}, {"referenceID": 29, "context": "Among them are majorization methods ([30] and SMACOF-Q [13]), a multiresolution approach due to Elad, Keller, and Kimmel [14] and an approach based on computing the classical MDS and renormalizing [31].", "startOffset": 197, "endOffset": 201}, {"referenceID": 17, "context": "The Johnson-Lindenstrauss Lemma [19] states that any collection of n points in a Euclidean space can be embedded in a O((1/\"2) log n) dimensional Euclidean space that preserves all distances within a relative error of \".", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "An exhaustive survey of the different methods for dimensionality reduction is beyond the scope of this paper - the reader is directed to the survey by Indyk and Matousek for more information [17].", "startOffset": 191, "endOffset": 195}, {"referenceID": 0, "context": "Any manifold M with \u201clinearization dimension\u201d k (a measure of its complexity) can be embedded into a O((1/\"2)k log(kn)) dimensional space so that all pairwise Euclidean distances between points on M are distorted by at most a relative (1+ \")factor [1, 32, 26].", "startOffset": 248, "endOffset": 259}, {"referenceID": 30, "context": "Any manifold M with \u201clinearization dimension\u201d k (a measure of its complexity) can be embedded into a O((1/\"2)k log(kn)) dimensional space so that all pairwise Euclidean distances between points on M are distorted by at most a relative (1+ \")factor [1, 32, 26].", "startOffset": 248, "endOffset": 259}, {"referenceID": 24, "context": "Any manifold M with \u201clinearization dimension\u201d k (a measure of its complexity) can be embedded into a O((1/\"2)k log(kn)) dimensional space so that all pairwise Euclidean distances between points on M are distorted by at most a relative (1+ \")factor [1, 32, 26].", "startOffset": 248, "endOffset": 259}, {"referenceID": 24, "context": "The geodesic distance between points on a sphere can be interpreted as the angle between the points in radians, and a result by Magen [26] show that O((1/\"2) log n) dimensions preserve angles to within a relative factor of 1+ p \" (which is weaker than our result preserving the geodesic distance to within a relative factor of (1+ \")).", "startOffset": 134, "endOffset": 138}, {"referenceID": 6, "context": "\u2022 T = Rk,Err(\u03b4) = |\u03b4|, f (x , x \u2032) = \u2016x \u2212 x \u20162: This is a robust variant of MDS called rMDS, first suggested by Cayton and Dasgupta [8].", "startOffset": 132, "endOffset": 135}, {"referenceID": 6, "context": "The actual measure studied by Cayton and Dasgupta [8] is not rMDS.", "startOffset": 50, "endOffset": 53}, {"referenceID": 33, "context": "Our experiments indicate the SVD-based approach [35] is almost always the optimal way to seed the algorithm, and we use it unless specifically indicated otherwise.", "startOffset": 48, "endOffset": 52}, {"referenceID": 32, "context": "Although there is no closed form expression for the 1-median, there are numerous algorithms for solving this problem both exactly [34] and approximately [4].", "startOffset": 130, "endOffset": 134}, {"referenceID": 3, "context": "Although there is no closed form expression for the 1-median, there are numerous algorithms for solving this problem both exactly [34] and approximately [4].", "startOffset": 153, "endOffset": 156}, {"referenceID": 4, "context": "Methods that converge to the global optimum exist for any Err(\u03b4) = |\u03b4|p, p \u2264 2; it is known that if p is sufficiently larger than 2, then convergent methods may not exist [5].", "startOffset": 171, "endOffset": 174}, {"referenceID": 32, "context": "An exact iterative algorithm for solving this problem was given by Weiszfeld [34], and works as follows.", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "This algorithm is guaranteed to converge to the optimal solution [24, 28], and in most settings converges quadratically [21].", "startOffset": 65, "endOffset": 73}, {"referenceID": 26, "context": "This algorithm is guaranteed to converge to the optimal solution [24, 28], and in most settings converges quadratically [21].", "startOffset": 65, "endOffset": 73}, {"referenceID": 19, "context": "This algorithm is guaranteed to converge to the optimal solution [24, 28], and in most settings converges quadratically [21].", "startOffset": 120, "endOffset": 124}, {"referenceID": 4, "context": "If Err(\u03b4) = |\u03b4|p, 1 < p < 2, then an iterative algorithm along the same lines as the Weiszfeld algorithm can be used to minimize g(x) optimally [5].", "startOffset": 144, "endOffset": 147}, {"referenceID": 18, "context": "Karcher [20] proposed an iterative scheme for the geodesic sum-of-squares problem that always converges as long as the points do not span the entire sphere.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "For this case, we make use of a Weiszfeld-like adaption [15] that again works on general Riemannian manifolds, and on the sphere in particular.", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "For the fMDS problem the leading algorithm is SMACOF [13]; for the r2MDS problem the leading algorithm is by Cayton and Dasgupta (CD) [8].", "startOffset": 53, "endOffset": 57}, {"referenceID": 6, "context": "For the fMDS problem the leading algorithm is SMACOF [13]; for the r2MDS problem the leading algorithm is by Cayton and Dasgupta (CD) [8].", "startOffset": 134, "endOffset": 137}, {"referenceID": 6, "context": "Thus, as suggested by the authors [8], to properly compare the algorithms, we let CD refer to running REE and then projecting the result to a k-dimensional subspace using the SVD technique [35] (our plots show this projection after each step).", "startOffset": 34, "endOffset": 37}, {"referenceID": 33, "context": "Thus, as suggested by the authors [8], to properly compare the algorithms, we let CD refer to running REE and then projecting the result to a k-dimensional subspace using the SVD technique [35] (our plots show this projection after each step).", "startOffset": 189, "endOffset": 193}, {"referenceID": 33, "context": "We also compare with the popular SVD-based method [35], which solves the related cMDS problem based on similarities, by seeding all three iterative techniques with the results of the closed-form SVD-based solution.", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "We compare against a version of SMACOF-Q [13] that is designed for data restricted to a low dimensional sphere, specifically for the c-2SMDS measure.", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "4 The Spherical MDS Problem For the spherical MDS problem we compare PC against SMACOF-Q, an adaptation of SMACOF to restrict data points to a low-dimensional sphere, and a technique of Elad, Keller and Kimmel [14].", "startOffset": 210, "endOffset": 214}, {"referenceID": 17, "context": "When f (yi , y j) = ||yi \u2212 y j||, and Y \u2208  \u0304 d , then the Johnson-Lindenstrauss (JL) Lemma [19] says that if H \u2282 R \u0304 d is a random k-dimensional linear subspace with k = O((1/\"2) log(n/\u03b4)), then X = \u03c0H(Y ) has \"distortion from Y with probability at least 1\u2212\u03b4.", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "We note that recent results [1] have shown similar results for point on a variety of manifolds (including spheres) where projections preserve Euclidean distances.", "startOffset": 28, "endOffset": 31}, {"referenceID": 24, "context": "Another recent result [26] shows that k = O((1/\"2) log(n/\u03b4)) dimensions preserves p \"-distortion in angles, which is weaker than the following result.", "startOffset": 22, "endOffset": 26}, {"referenceID": 0, "context": "We will show that for x \u2208 [0, 1] and \" \u2208 [0, 0.", "startOffset": 26, "endOffset": 32}], "year": 2010, "abstractText": "In this paper, we propose a unified algorithmic framework for solving many known variants of MDS. Our algorithm is a simple iterative scheme with guaranteed convergence, and is modular; by changing the internals of a single subroutine in the algorithm, we can switch cost functions and target spaces easily. In addition to the formal guarantees of convergence, our algorithms are accurate; in most cases, they converge to better quality solutions than existing methods, in comparable time. We expect that this framework will be useful for a number of MDS variants that have not yet been studied. Our framework extends to embedding high-dimensional points lying on a sphere to points on a lower dimensional sphere, preserving geodesic distances. As a compliment to this result, we also extend the JohnsonLindenstrauss Lemma to this spherical setting, where projecting to a random O((1/\"2) log n)-dimensional sphere causes \"-distortion.", "creator": "LaTeX with hyperref package"}}}