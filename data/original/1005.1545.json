{"id": "1005.1545", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2010", "title": "Improving Semi-Supervised Support Vector Machines Through Unlabeled Instances Selection", "abstract": "Semi-supervised learning tries to improve performance by using unlabeled data. In some situations, however, its performance may become inferior to that of without using unlabeled data. It is desired to have safe semi-supervised methods which often improve the performance while rarely degenerate the performance. In this paper, we focus on semi-supervised support vector machine and propose the S4VM (Safe Semi-Supervised Support Vector Machine) approach. Our intuition is that we shall use only the unlabeled examples which are very likely to help improve the performance while keeping the unlabeled data which are with high risk to be unexploited. Experimental results on a broad range of data sets over 120 different settings show that our proposed S4VM is highly competitive with TSVM. More important, contrasting to TSVM which degenerates performance in many cases when using unlabeled data, our S4VM never degenerates performance.", "histories": [["v1", "Mon, 10 May 2010 13:49:01 GMT  (664kb)", "http://arxiv.org/abs/1005.1545v1", "20 pages, 4 figures"], ["v2", "Mon, 9 May 2011 13:08:45 GMT  (834kb)", "http://arxiv.org/abs/1005.1545v2", "14 pages, 11 figures"]], "COMMENTS": "20 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yu-feng li", "zhi-hua zhou"], "accepted": true, "id": "1005.1545"}, "pdf": {"name": "1005.1545.pdf", "metadata": {"source": "CRF", "title": "S4VM: Safe Semi-Supervised Support Vector Machine", "authors": ["Yu-Feng Li", "Zhi-Hua Zhou"], "emails": ["zhouzh@nju.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 5.\n15 45\nv1 [\ncs .L\nG ]\n1 0\nM ay\nSemi-supervised learning tries to improve performance by using unlabeled data. In some situations, however, its performance may become inferior to that of without using unlabeled data. It is desired to have safe semi-supervised methods which often improve the performance while rarely degenerate the performance. In this paper, we focus on semi-supervised support vector machine and propose the S4VM (Safe Semi-Supervised Support Vector Machine) approach. Our intuition is that we shall use only the unlabeled examples which are very likely to help improve the performance while keeping the unlabeled data which are with high risk to be unexploited. Experimental results on a broad range of data sets over 120 different settings show that our proposed S4VM is highly competitive with TSVM. More important, contrasting to TSVM which degenerates performance in many cases when using unlabeled data, our S4VM never degenerates performance. Key words: safe semi-supervised method, semi-supervised support vector machine"}, {"heading": "1. Introduction", "text": "Semi-supervised learning (SSL) is proposed to deal with many real scenarios where a large amount of training data are unlabeled while the acquisition of class labels is costly and difficult. Its goal is to improve the generalization performance by appropriately exploiting unlabeled instances. Over the past decade, SSL has received much attention and many algorithms have been proposed [9, 31]. Examples mainly include generative methods [23]; graph-based methods [32, 29, 3]; co-training [8] and semi-supervised support vector machines [5, 19].\n\u2217Corresponding author. Email: zhouzh@nju.edu.cn\nPreprint submitted for review February 17, 2017\nWhile SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24]. Such a phenomenon highly affects the applicability and reliability of SSL for real-world applications. It is desired to have safe semi-supervised methods which often improves the performance while in the worst case it rarely degenerates performance.\nIn this paper, we focus on semi-supervised support vector machines which are well known as one popular type of semi-supervised method and propose a safe semi-supervised support vector machine (S4VM). Our intuition is that given a set of unlabeled data, it is not the best to use all of them to tune the classification boundary without any sanity check. Instead, we shall use only the unlabeled examples which are very likely to help improve the performance while keeping the unlabeled data which are with high risk to be unexploited. To exclude the high risky unlabeled data, we first study the use of standard clustering technique motivated by the discernibility of density set [24] and label propagation technique motivated by confidence estimation, and then propose our S4VM method. Comprehensive experiments on a broad range of data sets on 120 different settings show that the performance of S4VM is highly competitive with TSVM [19]. More importantly, contrasting to TSVM which has more than 40 times of performance degradation, our S4VM never degenerates performance.\nThe rest of this paper is organized as follows. Section 2 introduces related work. Section 3 studies two simple approaches with standard clustering technique and label propagation technique. Section 4 proposes our S4VM. Experimental results are reported in Section 5. The last section concludes this paper."}, {"heading": "2. Related Work", "text": ""}, {"heading": "2.1. Semi-Supervised SVMs (S3VMs)", "text": "Semi-Supervised SVMs (S3VMs) [5, 19] are known as a popular type of semi-supervised method which extends supervised SVM for the unlabeled data. Its goal is to assign class label to the unlabeled data such that the margin of the resultant supervised SVM is maximized. Intuitively, S3VMs are built on cluster assumption and favor the decision boundaries that cross the low density regions [12].\nThe correctness of the objective of S3VM has been well studied on small data sets [11] and many work have been devoted to cope with the high complexity in solving S3VM, examples include local search [19], semi-definite programming relaxation [6], convex-concave procedure [14, 26], label mean estimation\n[21] and many other optimization techniques [10]. However, it has been reported that S3VMs may not work well in many situations [28, 10] and there is no result on how to make S3VMs rarely degenerate performance."}, {"heading": "2.2. Other SSL Approaches", "text": "Roughly, existing semi-supervised methods mainly fall into three categories besides S3VMs. The first is generative methods which extend supervised generative models to SSL by additionally estimating the label of unlabeled data such that the fitness of the model is maximized, like utilizing EM algorithm [23]. The second is graph-based methods which encode both the labeled and unlabeled data by a connected graph and then find class labels for the unlabeled data such that their inconsistencies with both the supervised data and the underlying graph structure are minimized. Examples mainly include Mincut [7], harmonic function [32], local and global consistency [29], manifold regularization [3], etc. The third is co-training [8] which employs multiple learners and improves each learner by labeling the unlabeled data based on the exploitation of disagreement of these learners.\nThe issues of using the unlabeled data carefully have been discussed in a number of literatures. For generative methods, Cozman et al.,(2003) indicate that unlabeled data can increase classification error even in situations where additional labeled data would decrease classification error. The main conjecture on the performance degeneration is attributed to the difficulties of making a right model assumption which prevents the performance from degenerated by fitting with unlabeled data. For graph-based methods, researchers have realized that graph construction is the crucial problem for label propagation strategies which is more important than the method. By using domain knowledge, it is possible to construct a good graph which leads to good performance [2]. However, how to develop a method which works\nwell in general cases remains an open problem. One interesting observation by [33] is weighted kNN graphs with a small k tend to perform well empirically. Recently, Jebara et.al.,(2009) indicate that bmatching graph, where each node owns the same number of edges, performs robustly better than kNN graph. As for co-training, the generalization ability has been studied with plentiful theoretical results based on different assumptions [8, 16, 27]. However, how to relax and verify those assumptions in practice is still an unsolved issue.\nIt is also notable that recently, several work have been devoted to discuss the usefulness of unlabeled data theoretically [20, 4, 24] or empirically [13]. However, to the best of our knowledge, there is no proposal on safe semi-supervised learning methods which rarely degenerate performance by using unlabeled data."}, {"heading": "3. Two Baseline Approaches", "text": "Recalled that our intuition is to use only the unlabeled data which are very likely to help improve the performance and keep the unlabeled data which are with high risk to be unexploited. In this way, the chance of degenerating the performance may be significantly reduced. Current S3VM can be regarded as an extreme case which believes that all unlabeled data are with low risk and therefore all of them should be used; while SVM using labeled data only can be regarded as another extreme case which believes that all the unlabeled data are high risky and therefore only labeled data are used.\nFormally, we consider the following problem in transductive setting: once we obtain the predicted results of S3VM, how to remove risky predictions of S3VM such that the final performance could often be better than that of supervised SVM using labeled data only while in the worse case, the performance is rarely degenerated?\nSuppose we are given a training data set D = L \u22c3 U where L = {(x1, y1), . . . , (xl, yl)} denotes the set of labeled data and U = {xl+1, . . . ,xl+u} denotes the set of unlabeled data. Here x \u2208 X is training data and y \u2208 {+1,\u22121} is the label. We further let ySVM (x) and yS3VM (x) be the predicted label of x by SVM and S3VM respectively.\nTo address our problem, there are two simple ideas that can easy to be worked out, leading to two simple approaches, namely S3VM-c and S3VM-p respectively.\n3.1. S3VM-c\nFigure 2(d) illustrates the intuition of S3VM-c algorithm. Once we obtain decision boundaries of both SVM and S3VM, as can be seen, the disagreement between SVM and S3VM occurs in some clusters, i.e., C1,C2,3,C4,C5, C6,C7,8,C9,10 where Ca denotes the cluster including data points of group a and Ca,b denotes the cluster including data points of groups a and b. As for the unlabeled data in C1 and C4, SVM and S3VM achieve the same bias to assign the label while S3VM strengthens such a bias. For these cases, we should use the prediction of S3VM because according to low density separation, these unlabeled data should belong to one class. While for the unlabeled data in clusters C5, C6 and C7,8 where S3VM weakens the bias of SVM and clusters C2,3, C9,10 where S3VM obtains opposite bias to SVM, it may be risky if we totally trust S3VM. A conservative strategy is to keep the class assignment of these clusters unchanged. The S3VM-c method is shown in Algorithm 1. In fact, such an idea could be interpreted by the analysis in [24] where they show that unlabeled data helps when the component density sets are discernable. Specifically, S3VM-c simulates component density sets as clusters and discernibility as the condition in step 4 of Algorithm 1. Overall, when there is no gap between cluster assumption and clustering algorithm and the label bias of SVM is correct, S3VM-c will be a safe S3VM method.\nAlgorithm 1 S3VM-c Input: D and parameter k\n1: Train SVM & S3VM 2: Perform partitional clustering method, e.g., kmeans for D. Denote C1, . . . , Ck as the indices of data\nfor each cluster respectively.\n3: For each cluster i, i = 1, . . . , k, calculate the label bias lb and confidence cf of SVM and S3VM as\nfollows,\nlbS(3)VM = sgn( \u2211\nj\u2208Ci\nyS(3)VM (xj))\ncfS(3)VM = | \u2211\nj\u2208Ci\nyS(3)VM (xj)|.\n4: If lbSVM = lbS3VM & cfS3VM > cfSVM , use the prediction of S3VM; else, use the prediction of\nSVM.\n3.2. S3VM-p\nIn contrast to S3VM-c which works in a local manner, S3VM-p is proposed in a global way. A simple choice to use graph-based label propagation method [32] to estimate the label of unlabeled data and their confidences. Formally, let Fl = [(yl + 1)/2, (1 \u2212 yl)/2] \u2208 {0, 1}l\u00d72 be the label matrix for labeled data where yl = [y1, . . . , yl]\u2032 \u2208 {\u00b11}l\u00d71 is the label vector. Further let W = [wij] \u2208 R(l+u)\u00d7(l+u) be the weight matrix and \u039b is the laplacian of W, i.e., \u039b = D\u2212W where D = diag(di) is the diagonal matrix with entries di = \u2211 j wij . Therefore, the prediction of unlabeled data can be obtained by\nFu = \u039b\u22121u,uWu,uF l,\nwhere \u039bu,u (Wu,u) is the sub-matrix of \u039b (W) with respective to the part of unlabeled data only. Then, labels each point xi as a label yLabPo(xi) = sgn(Fui\u2212l,1\u2212F u i\u2212l,2) and a confidence hi = |F u i\u2212l,1\u2212F u i\u2212l,2|, i = l + 1, . . . , l + u.\nWith the estimated confidence, we can keep some unlabeled data with low confidence to be unexploited. Figure 2(e) illustrates the intuition of S3VM-p approach. For the data points lying on the uppermost moon, they are all predicted as \u2019Green-color\u2019 with high confidences while S3VM also achieves the same predictions. For this case, we should use the S3VM\u2019s predictions for those unlabeled data points in groups\n1-3. For the data points lying on the downmost moon, although they are all predicted as \u2019Blue-color\u2019 with high confidence, it may be risky to trust S3VM since the decision boundary of S3VM crosses through the manifold structure and we should keep those unlabeled data in groups 7-10 to be unexploited. Finally, for the data points lying on the middle moon, as stated in [25], label propagation methods does not obtain high confidence of the unlabeled points in groups 4 and 5 due to the unbalanced initial labeled points between the middle and downmost moons, and thus S3VM-p does not exploit these unlabeled points as well. Algorithm 2 describes the S3VM-p approach. In short, if the unlabeled instances with high confidence were reliable, S3VM-p would be safe.\nAlgorithm 2 S3VM-p Input: D, weight matrix W and parameter \u03b7\n1: Train SVM & S3VM 2: Perform label propagation method [32] with W, obtain the predicted label yLabPo(xi) and confidence\nhi for each unlabeled data xi, i = l + 1, . . . , l + u.\n3: Update the h as follows,\nhi = yS3VM (xi)yLabPo(xi)hi, i = l + 1, . . . , l + u.\nLet c be the number of non-negative entries in h.\n4: Sort h and pick up the top min{\u03b7u, c} highest h values and their corresponding unlabeled instances\nwhich are predicted by S3VM. Else are predicted by SVM."}, {"heading": "4. The Proposed S4VM Method", "text": "4.1. Deficiencies of S3VM-c and S3VM-p\nBoth S3VM-c and S3VM-p methods are capable to reduce the chances of degenerating performance, however, they both suffer from some deficiencies.\nFor S3VM-c, it needs to determine an appropriate number k of cluster which works well in general cases, this remains an open problem. Besides, S3VM-c may loss some useful unlabeled instances as it does not consider any relation between clusters, e.g., the unlabeled instances in groups 2 and 3. By contrast, S3VM-p does not suffer from the problems of S3VM-c by introducing a graph. However, S3VM-p needs to determine a type of graph and the distance measurement. Moreover, as stated in [25], it is sensitive to\nthe initialization of label data while S3VM-c does not have such a problem. Besides, both S3VM-c and S3VM-p heavily rely on S3VM\u2019s predictions which may become a crucial issue especially when S3VM obtains degenerated performance. Figure 3(b) and 3(c) illustrate the behaviors of S3VM-c and S3VM-p when S3VM degenerates the performances. Specifically, both S3VM-c and S3VM-p erroneously inherit the wrong predictions of the unlabeled instances in group 1 by S3VM.\nTable 1 summarizes the deficiencies of S3VM-c and S3VM-p."}, {"heading": "4.2. S4VM", "text": "To propose a safe semi-supervised support vector machine, the key is to avoid all the above issues in table 1 and present a meaningful way to exploit reliable unlabeled instances. Our solution is benefited from hierarchical clustering [17] which leads to our S4VM method.\nHierarchical clustering works in a greedy and iterative manner where it first initials each singe instance as a cluster and then at each step, it merges two of the clusters which have the shortest distance among\nall pairs of clusters. To avoid sensitive distance parameters, single linkage method, i.e., the minimum Euclidean distance between elements of each cluster is performed as the distance between two clusters. Now we show that hierarchical clustering does not suffer from the issues (a), (b), (d) and (e) in table 1.\n1. Hierarchical clustering creates a hierarchy of clusters, thus hierarchical clustering doesn\u2019t need to\ndetermine the cluster number k in advanced, i.e., issue (a) in table 1.\n2. Recalled that hierarchical clustering creates a hierarchy of clusters, thus it naturally considers the\nrelations between clusters, i.e., issue (b) in table 1.\n3. Since hierarchical clustering greedily merges two clusters via the minimum Euclidean distance, it\ndoesn\u2019t need to determine any type of graph and the distance measurement, i.e., issue (d) in table 1.\n4. Since hierarchical clustering works in an unsupervised scenario, it does not have label data initial-\nization problem in S3VM-p, i.e., issue (e) in table 1.\nTo exploit the reliable unlabeled instances, let lij be the merging-length for an instance xi to another instance xj where merging-length is the number of times invoking the merging subroutine of hierarchical clustering algorithm until the two instances are in the same cluster. Further denote Dt as the minimum Euclidean distance between elements of each cluster in the t-th step of hierarchical clustering algorithm and maxDij = max{D1, . . . ,Dlij}. Then one can have the following proposition.\nProposition 1 Suppose that data points are sampled from multiple disjoint clusters and denote I1, . . . ,IK as the indices of data for each cluster respectively. Let\nwk = max i,j\u2208Ik maxDij, k = 1, . . . ,K\nbk,k\u2032 = min i\u2208Ik,j\u2208Ik\u2032\n\u2016xi \u2212 xj\u2016 2, 1 \u2264 k, k\u2032 \u2264 K, k 6= k\u2032.\nIf \u22001 \u2264 k, k\u2032 \u2264 K, k 6= k\u2032, wk < bk,k\u2032, then lij < lij\u2032 , \u2200i, j \u2208 Ik, j\u2032 \u2208 Ik\u2032 ,.\nProof. The proof is a simply consequence of the definitions and the procedure of hierarchical clustering algorithm, so will be omitted here.\nProposition 1 implies that during the process of hierarchical clustering algorithm, every data point will first merge the data points in the same cluster before that of other clusters when there is sufficient high density of data points within each cluster1. Now let pi and ni be the minimal merging-length for unlabeled instance xi to a positive and negative labeled instance respectively. Then we present our main theoretical result.\nTheorem 1 Based on cluster assumption [9], i.e., if points are in the same cluster, they are likely to be of the same class and assumed that each cluster has at least one labeled instances, if wk < bk,k\u2032, 1 \u2264 k, k\u2032 \u2264 K, k 6= k\u2032, then every unlabeled instance xi can be perfectly predicted by sgn(ni \u2212 pi).\nProof. For each unlabeled instance xi, suppose it belongs to the k-th cluster, i.e., i \u2208 Ik. Due to cluster assumption, all the data points in the k-th cluster have the same class label. Without loss of generality, suppose they are positive instances and x is a positive labeled instance in this cluster. According to the proposition 1, xi will merge x before other data points which aren\u2019t in the k-th cluster. In other words, xi will merge x before any other negative labeled instances and thus pi < ni. Therefore, sgn(ni \u2212 pi) is the same as what we suppose. Due to the arbitrary of unlabeled instance, theorem 1 is hold.\nTheorem 1 shows that ideally, sgn(ni \u2212 pi) can be used as the prediction of unlabeled instance xi. However, in practice, the assumptions in proposition 1 may not be exactly hold and thus we need to restrict sgn(ni \u2212 pi) to some extent such that some risky unlabeled instances could be removed. Intuitively, the larger |ni \u2212 pi|, the higher confidence that xi is close to one certain class. Further note that both pi and ni scale [1, l + u], we trust those unlabeled instances whose corresponding |ni \u2212 pi|s are larger than a threshold \u01eb|l + u| while keeping the remaining unlabeled instances to be unexploited.\nFinally, to further alleviate the cases where S3VM achieves degenerated performance, i.e., issue (c) or (f) in table 1, the exploited unlabeled instances are used as a validate set, if S3VM does not perform better on the validate set, we simply degenerate S4VM to SVM. Figure 3 illustrates the effect of S4VM and algorithm 3 shows the S4VM method.\n1as the density of data points within each cluster tends to infinity, wk \u2192 0\nAlgorithm 3 S4VM Input: D and parameter \u01eb\n1: Train two classifiers SVM & S3VM and let S be a set of the unlabeled data x such that ySVM (x) 6=\nyS3VM (x).\n2: Perform hierarchical clustering, e.g., singe linkage method [17]. 3: For each unlabeled data xi \u2208 S , calculate pi and ni which are the minimal merging-lengths from xi\nto its nearest positive label data and negative label data respectively. Let ti = (ni \u2212 pi).\n4: Let B be the set of unlabeled data xi in S satisfying |ti| \u2265 \u01eb|l + u|. 5: If \u2211\nxi\u2208B yS3VM (xi)ti \u2265 \u2211 xi\u2208B ySVM (xi)ti, predict the unlabeled data in B by S3VM and else by\nSVM.\n6: Predict the unlabeled data x 6\u2208 B by SVM."}, {"heading": "4.3. Time Complexity Analysis", "text": "The time complexity of hierarchical clustering is O(N2 lnN) using priority-queue algorithm, where N = l + u is the size of data set. For single-link method, the complexity could further reduce to O(N2) with use of next-best-merge array [22]. Computing all the pi and ni for unlabeled instances costs at most O(N2). While the time complexity of S3VM implementation, e.g., TSVM [19], scales at least O(TN2) for non-linear kernel where T is the number of time invoking standard SVM solver [14]. So, it is clear that the time complexity of S4VM is close to that of S3VM."}, {"heading": "5. Empirical Study", "text": ""}, {"heading": "5.1. Settings", "text": "We evaluate S4VM a broad range of data sets including the SSL benchmark data sets used in [9] and 16 UCI data sets [1]. Information of these data sets are summarized in Table 2.\nThe benchmark data sets are g241c, g241d, Digit1, USPS,TEXT and BCI. For each data, the archival 2 provides two data sets, with one using 10 labeled examples and the other using 100 labeled examples. As for UCI data sets, we randomly select 10, 50 and 100 instances to be used as labeled examples and use\n2http://www.kyb.tuebingen.mpg.de/ssl-book/\nthe remaining data as unlabeled data. The experiments are repeated for 30 times and the average accuracy and standard deviations are recorded. Considering that all previous semi-supervised studies used paired t-test with multiple hold-out repetitions as the statistical significant testing, here we also take this process.\nThe settings of the proposed methods as follows. S3VM is implemented by TSVM algorithm [19]3. Both the linear and Gaussian kernels are used. For the benchmark data sets, we follow the setup in [9]. Specifically, for the case of 10 labeled examples, the parameter C for SVM is fixed to N/ \u2211m\ni=1 \u2016xi\u2016 2 and\nthe the Gaussian kernel width is set to \u03b4, i.e., the average distance between patterns. For the case of 100 labeled examples, C is fixed to 100 and Gaussian kernel width is selected from {0.25\u03b4, 0.5\u03b4, \u03b4, 2\u03b4, 4\u03b4} by cross-validation. On UCI data sets, parameter C is fixed to 1 and the Gaussian kernel width is set to \u03b4 for 10 labeled examples. For the other cases, parameter C is selected from {0.1, 1, 10, 100} and the parameter of gaussian kernel is selected from {0.25\u03b4, 0.5\u03b4, \u03b4, 2\u03b4, 4\u03b4} by cross-validation. For S3VM-c, cluster number k is set to 50; for S3VM-p, the weighted graph is constructed via gaussian distance to avoid the numerical problem and parameter \u03b7 is fixed to 0.1; for S4VM, parameter \u01eb is fixed to 0.1. In addition, we compare with SVM using labeled data only and TSVM.\n3The implementation of TSVM is from http://svmlight. joachims.org/"}, {"heading": "5.2. Comparison Results", "text": "Table 3 summarizes the results with 10 labeled examples. All semi-supervised methods obtain improvement against SVM with labeled data only. Specifically, according to the average accuracy over all the 22 data sets, S4VM is comparable to TSVM while outperforms both S3VM-c and S3VM-p. Paired t-tests at 95% significance level show that SVM achieves 8 wins, 18 ties and 18 losses when compared to TSVM; and 1 wins, 29 ties and 14 losses when compared to S3VM-c; and 12 wins, 25 ties and 7 losses when\ncompared to S3VM-p; and 0 wins, 32 ties and 12 losses when compared to S4VM. It can be observed that S4VM achieves competitive win times with TSVM and what is more important, S4VM never degenerates the performance while TSVM degrades the performance for 8 times. It is worth noting that there are multiple cases where S4VM achieves the best performance than that of TSVM and SVM, e.g., vehicle, house-vote, optdigits with both linear and RBF kernel. These experimental results indicate although S4VM is directly performed on the predictions of TSVM and SVM, it effectively selects useful unlabeled data to improve the performance while more important, it successfully excludes high risky unlabeled data such that the performance would never been degenerated.\nTable 4 summarizes the results with 50 labeled examples. As can be seen, the paired t-tests results show that TSVM obtains 22 losses and only 3 wins when compared with SVM while S4VM achieves 0 loss and 7 wins respectively. These results emphasize that S4VM algorithm is useful when TSVM directly will\nlead to a significant degeneration of the performance when using the unlabeled data. When the risk of degenerating performance is low, it would be worthwhile to take the risk of using TSVM. However, how to explicitly judge the risk remains an open problem. Both S3VM-c and S3VM-p are capable to reduce the chances of degenerating performance and achieve more win times than that of TSVM, however, they both are not good enough to be a safe semi-supervised method.\nTable 5 summarizes the result with 100 labeled examples. As can be seen, TSVM suffers 19 losses and obtains 7 wins when compared with SVM while similar to the cases in Table 4, S4VM never degenerates\nthe performance and achieves more win times than that of TSVM.\nOverall, S4VM achieves highly competitive performance with TSVM, no matter with respective to average accuracy and the win times compared to SVM. More important, unlike TSVM which degenerates the performances in many cases, S4VM never degenerates the performance.\n5.3. Parameter Influence\nS4VM has a parameter to \u01eb to set. To study the influence of \u01eb, we perform more experiments on setting \u01eb to different values (0.1,0.2 and 0.3). The results are summarized in Figure 4 (10 labeled data are used). It can be seen that either for linear or rbf kernel, the performance of S4VM is never significantly worse than SVM using labeled data only, whatever the value of \u01eb is used. It can be observed that, whatever linear kernel or RBF kernel is used, the larger the value of \u01eb, the more close the performance of S4VM to SVM; while the smaller the value of \u01eb, the larger the improvement of performance. However, we need to note that this also implies that the bias introduced becomes larger, and the risk of performance degeneration also increases. Nevertheless, S4VM is more safe than existing S3VMs since it rarely degenerates performance."}, {"heading": "6. Conclusion", "text": "Semi-supervised learning attempts to improve performance by using unlabeled data. In some situations using unlabeled data may hurt the performance. This limits the reliability and applicability of semisupervised learning for many real scenarios. In this paper, we propose a safe semi-supervised support\nvector machine (S4VM). Our main intuition is that we should use those unlabeled data which are likely help improve the performance while keeping the unlabeled data with high risk to be unexploited to tune the classification boundary of SVM. Comprehensive experimental results show that S4VM achieves highly competitive performance with TSVM. More importantly, contrasting to TSVM degenerates performance in many cases, S4VM never degenerates performance.\nIn this paper, the effectiveness of S4VM is validated by empirical study in the transductive setting. In the future, we want to analyze the inductive setting expected that important insights of this paper may be help design more powerful semi-supervised methods."}], "references": [{"title": "UCI machine learning repository", "author": ["A. Asuncion", "D.J. Newman"], "venue": "School of Information and Computer Sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Person identification in webcam images: An application of semi-supervised learning", "author": ["M.F. Balcan", "A. Blum", "P.P. Choi", "J. Lafferty", "B. Pantano", "M.R. Rwebangira", "X. Zhu"], "venue": "In Proceeding of the 22nd International Conference on Machine Learning Workshop on Learning with Partially Classified Training Data,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Does unlabeled data provably help? worst-case analysis of the sample complexity of semi-supervised learning", "author": ["S. Ben-David", "T. Lu", "D. P\u00e1l"], "venue": "In Proceedings of the 21th Annual Conference on Learning Theory,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Semi-supervised support vector machines", "author": ["K. Bennett", "A. Demiriz"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Convex methods for transduction", "author": ["T. De Bie", "N. Cristianini"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Learning from Labeled and Unlabeled Data using Graph Mincuts", "author": ["A. Blum", "S. Chawla"], "venue": "In Proceedings of the 8th International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Combining Labeled and Unlabeled Data with Co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "In Proceedings of the 7th annual Conference on Computational Learning Theory,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien", "editors"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Optimization techniques for semi-supervised support vector machines", "author": ["O. Chapelle", "V. Sindhwani", "S.S. Keerthi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Branch and Bound for Semi-Supervised Support Vector Machines", "author": ["O. Chapelle", "G. Tubingen", "V. Sindhwani", "S.S. Keerthi"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Semi-supervised learning by low density separation", "author": ["O. Chapelle", "A. Zien"], "venue": "In Proceeding of the 8th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Learning from labeled and unlabeled data: An empirical study across techniques and domains", "author": ["N.V. Chawla", "G. Karakoulas"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Large Scale Transductive SVMs", "author": ["R. Collobert", "F. Sinz", "J. Weston", "L. Bottou"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Semi-supervised learning of mixture models", "author": ["F.G. Cozman", "I. Cohen", "M.C. Cirelo"], "venue": "In Proceeding of the 20th International Conference on Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Pac generalization bounds for co-training", "author": ["S. Dasgupta", "M.L. Littman", "D. McAllester"], "venue": "In Advances in Neural information processing systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Algorithms for clustering data", "author": ["A.K. Jain", "R.C. Dubes"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1988}, {"title": "Graph construction and b-matching for semi-supervised learning", "author": ["T. Jebara", "J. Wang", "S.F. Chang"], "venue": "In Proceeding of the 26th International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "In Proceeding of the 16th International Conference on Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Statistical analysis of semi-supervised regression", "author": ["J. Lafferty", "L. Wasserman"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Semi-supervised learning using label mean", "author": ["Y.-F. Li", "J.T. Kwok", "Z.-H. Zhou"], "venue": "In Proceeding of the 26th International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Schtze"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Text classification from labeled and unlabeled documents using EM", "author": ["K. Nigam", "A.K. McCallum", "S. Thrun", "T. Mitchell"], "venue": "Machine learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2000}, {"title": "Unlabeled data: Now it helps, now it doesn\u2019t", "author": ["A. Singh", "R. Nowak", "X. Zhu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Graph transduction via alternating minimization", "author": ["J. Wang", "T. Jebara", "S.F. Chang"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Large margin semi-supervised learning", "author": ["J. Wang", "X. Shen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Analyzing co-training style algorithms", "author": ["W. Wang", "Z.-H. Zhou"], "venue": "In Proceeding of the 18th European Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "The Value of Unlabeled Data for Classification Problems", "author": ["T. Zhang", "F. Oles"], "venue": "In Proceedings of the 17th International Conference on Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2000}, {"title": "Scholkopf. Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Semi-supervised learning by disagreement", "author": ["Z.-H. Zhou", "M. Li"], "venue": "Knowledge and Information Systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Computer Science, University of Wisconsin- Madison,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J.D. Lafferty"], "venue": "In Proceeding of the 20th International Conference on Machine Learning,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "Semi-supervised learning with graphs", "author": ["X. Zhu", "J. Lafferty", "R. Rosenfeld"], "venue": "PhD Thesis,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}], "referenceMentions": [{"referenceID": 8, "context": "Over the past decade, SSL has received much attention and many algorithms have been proposed [9, 31].", "startOffset": 93, "endOffset": 100}, {"referenceID": 30, "context": "Over the past decade, SSL has received much attention and many algorithms have been proposed [9, 31].", "startOffset": 93, "endOffset": 100}, {"referenceID": 22, "context": "Examples mainly include generative methods [23]; graph-based methods [32, 29, 3]; co-training [8] and semi-supervised support vector machines [5, 19].", "startOffset": 43, "endOffset": 47}, {"referenceID": 31, "context": "Examples mainly include generative methods [23]; graph-based methods [32, 29, 3]; co-training [8] and semi-supervised support vector machines [5, 19].", "startOffset": 69, "endOffset": 80}, {"referenceID": 28, "context": "Examples mainly include generative methods [23]; graph-based methods [32, 29, 3]; co-training [8] and semi-supervised support vector machines [5, 19].", "startOffset": 69, "endOffset": 80}, {"referenceID": 2, "context": "Examples mainly include generative methods [23]; graph-based methods [32, 29, 3]; co-training [8] and semi-supervised support vector machines [5, 19].", "startOffset": 69, "endOffset": 80}, {"referenceID": 7, "context": "Examples mainly include generative methods [23]; graph-based methods [32, 29, 3]; co-training [8] and semi-supervised support vector machines [5, 19].", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "Examples mainly include generative methods [23]; graph-based methods [32, 29, 3]; co-training [8] and semi-supervised support vector machines [5, 19].", "startOffset": 142, "endOffset": 149}, {"referenceID": 18, "context": "Examples mainly include generative methods [23]; graph-based methods [32, 29, 3]; co-training [8] and semi-supervised support vector machines [5, 19].", "startOffset": 142, "endOffset": 149}, {"referenceID": 14, "context": "While SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24].", "startOffset": 132, "endOffset": 167}, {"referenceID": 27, "context": "While SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24].", "startOffset": 132, "endOffset": 167}, {"referenceID": 12, "context": "While SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24].", "startOffset": 132, "endOffset": 167}, {"referenceID": 30, "context": "While SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24].", "startOffset": 132, "endOffset": 167}, {"referenceID": 19, "context": "While SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24].", "startOffset": 132, "endOffset": 167}, {"referenceID": 3, "context": "While SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24].", "startOffset": 132, "endOffset": 167}, {"referenceID": 17, "context": "While SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24].", "startOffset": 132, "endOffset": 167}, {"referenceID": 29, "context": "While SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24].", "startOffset": 132, "endOffset": 167}, {"referenceID": 23, "context": "While SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24].", "startOffset": 132, "endOffset": 167}, {"referenceID": 23, "context": "To exclude the high risky unlabeled data, we first study the use of standard clustering technique motivated by the discernibility of density set [24] and label propagation technique motivated by confidence estimation, and then propose our S4VM method.", "startOffset": 145, "endOffset": 149}, {"referenceID": 18, "context": "Comprehensive experiments on a broad range of data sets on 120 different settings show that the performance of S4VM is highly competitive with TSVM [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 4, "context": "Semi-Supervised SVMs (S3VMs) [5, 19] are known as a popular type of semi-supervised method which extends supervised SVM for the unlabeled data.", "startOffset": 29, "endOffset": 36}, {"referenceID": 18, "context": "Semi-Supervised SVMs (S3VMs) [5, 19] are known as a popular type of semi-supervised method which extends supervised SVM for the unlabeled data.", "startOffset": 29, "endOffset": 36}, {"referenceID": 11, "context": "Intuitively, S3VMs are built on cluster assumption and favor the decision boundaries that cross the low density regions [12].", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "The correctness of the objective of S3VM has been well studied on small data sets [11] and many work have been devoted to cope with the high complexity in solving S3VM, examples include local search [19], semi-definite programming relaxation [6], convex-concave procedure [14, 26], label mean estimation", "startOffset": 82, "endOffset": 86}, {"referenceID": 18, "context": "The correctness of the objective of S3VM has been well studied on small data sets [11] and many work have been devoted to cope with the high complexity in solving S3VM, examples include local search [19], semi-definite programming relaxation [6], convex-concave procedure [14, 26], label mean estimation", "startOffset": 199, "endOffset": 203}, {"referenceID": 5, "context": "The correctness of the objective of S3VM has been well studied on small data sets [11] and many work have been devoted to cope with the high complexity in solving S3VM, examples include local search [19], semi-definite programming relaxation [6], convex-concave procedure [14, 26], label mean estimation", "startOffset": 242, "endOffset": 245}, {"referenceID": 13, "context": "The correctness of the objective of S3VM has been well studied on small data sets [11] and many work have been devoted to cope with the high complexity in solving S3VM, examples include local search [19], semi-definite programming relaxation [6], convex-concave procedure [14, 26], label mean estimation", "startOffset": 272, "endOffset": 280}, {"referenceID": 25, "context": "The correctness of the objective of S3VM has been well studied on small data sets [11] and many work have been devoted to cope with the high complexity in solving S3VM, examples include local search [19], semi-definite programming relaxation [6], convex-concave procedure [14, 26], label mean estimation", "startOffset": 272, "endOffset": 280}, {"referenceID": 20, "context": "[21] and many other optimization techniques [10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[21] and many other optimization techniques [10].", "startOffset": 44, "endOffset": 48}, {"referenceID": 27, "context": "However, it has been reported that S3VMs may not work well in many situations [28, 10] and there is no result on how to make S3VMs rarely degenerate performance.", "startOffset": 78, "endOffset": 86}, {"referenceID": 9, "context": "However, it has been reported that S3VMs may not work well in many situations [28, 10] and there is no result on how to make S3VMs rarely degenerate performance.", "startOffset": 78, "endOffset": 86}, {"referenceID": 22, "context": "The first is generative methods which extend supervised generative models to SSL by additionally estimating the label of unlabeled data such that the fitness of the model is maximized, like utilizing EM algorithm [23].", "startOffset": 213, "endOffset": 217}, {"referenceID": 6, "context": "Examples mainly include Mincut [7], harmonic function [32], local and global consistency [29], manifold regularization [3], etc.", "startOffset": 31, "endOffset": 34}, {"referenceID": 31, "context": "Examples mainly include Mincut [7], harmonic function [32], local and global consistency [29], manifold regularization [3], etc.", "startOffset": 54, "endOffset": 58}, {"referenceID": 28, "context": "Examples mainly include Mincut [7], harmonic function [32], local and global consistency [29], manifold regularization [3], etc.", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "Examples mainly include Mincut [7], harmonic function [32], local and global consistency [29], manifold regularization [3], etc.", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "The third is co-training [8] which employs multiple learners and improves each learner by labeling the unlabeled data based on the exploitation of disagreement of these learners.", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "By using domain knowledge, it is possible to construct a good graph which leads to good performance [2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 32, "context": "One interesting observation by [33] is weighted kNN graphs with a small k tend to perform well empirically.", "startOffset": 31, "endOffset": 35}, {"referenceID": 7, "context": "As for co-training, the generalization ability has been studied with plentiful theoretical results based on different assumptions [8, 16, 27].", "startOffset": 130, "endOffset": 141}, {"referenceID": 15, "context": "As for co-training, the generalization ability has been studied with plentiful theoretical results based on different assumptions [8, 16, 27].", "startOffset": 130, "endOffset": 141}, {"referenceID": 26, "context": "As for co-training, the generalization ability has been studied with plentiful theoretical results based on different assumptions [8, 16, 27].", "startOffset": 130, "endOffset": 141}, {"referenceID": 19, "context": "It is also notable that recently, several work have been devoted to discuss the usefulness of unlabeled data theoretically [20, 4, 24] or empirically [13].", "startOffset": 123, "endOffset": 134}, {"referenceID": 3, "context": "It is also notable that recently, several work have been devoted to discuss the usefulness of unlabeled data theoretically [20, 4, 24] or empirically [13].", "startOffset": 123, "endOffset": 134}, {"referenceID": 23, "context": "It is also notable that recently, several work have been devoted to discuss the usefulness of unlabeled data theoretically [20, 4, 24] or empirically [13].", "startOffset": 123, "endOffset": 134}, {"referenceID": 12, "context": "It is also notable that recently, several work have been devoted to discuss the usefulness of unlabeled data theoretically [20, 4, 24] or empirically [13].", "startOffset": 150, "endOffset": 154}, {"referenceID": 23, "context": "In fact, such an idea could be interpreted by the analysis in [24] where they show that unlabeled data helps when the component density sets are discernable.", "startOffset": 62, "endOffset": 66}, {"referenceID": 31, "context": "A simple choice to use graph-based label propagation method [32] to estimate the label of unlabeled data and their confidences.", "startOffset": 60, "endOffset": 64}, {"referenceID": 24, "context": "Finally, for the data points lying on the middle moon, as stated in [25], label propagation methods does not obtain high confidence of the unlabeled points in groups 4 and 5 due to the unbalanced initial labeled points between the middle and downmost moons, and thus S3VM-p does not exploit these unlabeled points as well.", "startOffset": 68, "endOffset": 72}, {"referenceID": 31, "context": "Algorithm 2 S3VM-p Input: D, weight matrix W and parameter \u03b7 1: Train SVM & S3VM 2: Perform label propagation method [32] with W, obtain the predicted label yLabPo(xi) and confidence hi for each unlabeled data xi, i = l + 1, .", "startOffset": 117, "endOffset": 121}, {"referenceID": 24, "context": "Moreover, as stated in [25], it is sensitive to", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "Our solution is benefited from hierarchical clustering [17] which leads to our S4VM method.", "startOffset": 55, "endOffset": 59}, {"referenceID": 8, "context": "Theorem 1 Based on cluster assumption [9], i.", "startOffset": 38, "endOffset": 41}, {"referenceID": 16, "context": ", singe linkage method [17].", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "For single-link method, the complexity could further reduce to O(N2) with use of next-best-merge array [22].", "startOffset": 103, "endOffset": 107}, {"referenceID": 18, "context": ", TSVM [19], scales at least O(TN2) for non-linear kernel where T is the number of time invoking standard SVM solver [14].", "startOffset": 7, "endOffset": 11}, {"referenceID": 13, "context": ", TSVM [19], scales at least O(TN2) for non-linear kernel where T is the number of time invoking standard SVM solver [14].", "startOffset": 117, "endOffset": 121}, {"referenceID": 8, "context": "We evaluate S4VM a broad range of data sets including the SSL benchmark data sets used in [9] and 16 UCI data sets [1].", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "We evaluate S4VM a broad range of data sets including the SSL benchmark data sets used in [9] and 16 UCI data sets [1].", "startOffset": 115, "endOffset": 118}, {"referenceID": 18, "context": "S3VM is implemented by TSVM algorithm [19]3.", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "For the benchmark data sets, we follow the setup in [9].", "startOffset": 52, "endOffset": 55}], "year": 2017, "abstractText": "Semi-supervised learning tries to improve performance by using unlabeled data. In some situations, however, its performance may become inferior to that of without using unlabeled data. It is desired to have safe semi-supervised methods which often improve the performance while rarely degenerate the performance. In this paper, we focus on semi-supervised support vector machine and propose the S4VM (Safe Semi-Supervised Support Vector Machine) approach. Our intuition is that we shall use only the unlabeled examples which are very likely to help improve the performance while keeping the unlabeled data which are with high risk to be unexploited. Experimental results on a broad range of data sets over 120 different settings show that our proposed S4VM is highly competitive with TSVM. More important, contrasting to TSVM which degenerates performance in many cases when using unlabeled data, our S4VM never degenerates performance.", "creator": "LaTeX with hyperref package"}}}