{"id": "1006.2588", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2010", "title": "Agnostic Active Learning Without Constraints", "abstract": "We present and analyze an agnostic active learning algorithm that works without keeping a version space. This is unlike all previous approaches where a restricted set of candidate hypotheses is maintained throughout learning, and only hypotheses from this set are ever returned. By avoiding this version space approach, our algorithm sheds the computational burden and brittleness associated with maintaining version spaces, yet still allows for substantial improvements over supervised learning for classification.", "histories": [["v1", "Mon, 14 Jun 2010 02:03:12 GMT  (31kb)", "http://arxiv.org/abs/1006.2588v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alina beygelzimer", "daniel j hsu", "john langford", "tong zhang 0001"], "accepted": true, "id": "1006.2588"}, "pdf": {"name": "1006.2588.pdf", "metadata": {"source": "CRF", "title": "Agnostic Active Learning Without Constraints", "authors": ["Alina Beygelzimer", "Daniel Hsu", "John Langford", "Tong Zhang"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n00 6.\n25 88\nv1 [\ncs .L\nG ]\n1 4\nJu n\n20 10"}, {"heading": "1 Introduction", "text": "In active learning, a learner is given access to unlabeled data and is allowed to adaptively choose which ones to label. This learning model is motivated by applications in which the cost of labeling data is high relative to that of collecting the unlabeled data itself. Therefore, the hope is that the active learner only needs to query the labels of a small number of the unlabeled data, and otherwise perform as well as a fully supervised learner. In this work, we are interested in agnostic active learning algorithms for binary classification that are provably consistent, i.e. that converge to an optimal hypothesis in a given hypothesis class.\nOne technique that has proved theoretically profitable is to maintain a candidate set of hypotheses (sometimes called a version space), and to query the label of a point only if there is disagreement within this set about how to label the point. The criteria for membership in this candidate set needs to be carefully defined so that an optimal hypothesis is always included, but otherwise this set can be quickly whittled down as more labels are queried. This technique is perhaps most readily understood in the noise-free setting [CAL94, Das05], and it can be extended to noisy settings by using empirical confidence bounds [BBL06, DHM07, BDL09, Han09, Kol09].\nThe version space approach unfortunately has its share of significant drawbacks. The first is computational intractability: maintaining a version space and guaranteeing that only hypotheses from this set are returned is difficult for linear predictors and appears intractable for interesting nonlinear predictors such as neural nets and decision trees [CAL94]. Another drawback of the approach is its brittleness: a single mishap (due to, say, modeling failures or computational approximations) might cause the learner to exclude the best hypothesis from the version space forever; this is an ungraceful failure mode that is not easy to correct. A third drawback is related to sample re-usability: if (labeled) data is collected using a version space-based active learning algorithm, and we later decide to use a different algorithm or hypothesis class, then the earlier data may not be freely re-used because its collection process is inherently biased.\nHere, we develop a new strategy addressing all of the above problems given an oracle that returns an empirical risk minimizing (ERM) hypothesis. As this oracle matches our abstraction of many supervised learning algorithms, we believe active learning algorithms built in this way are immediately and widely applicable.\n\u2217IBM Research, beygel@us.ibm.com \u2020UC San Diego, djhsu@cs.ucsd.edu \u2021Yahoo! Research, jl@yahoo-inc.com \u00a7Rutgers University, tongz@rci.rutgers.edu\nOur approach instantiates the importance weighted active learning framework of [BDL09] using a rejection threshold similar to the algorithm of [DHM07] which only accesses hypotheses via a supervised learning oracle. However, the oracle we require is simpler and avoids strict adherence to a candidate set of hypotheses. Moreover, our algorithm creates an importance weighted sample that allows for unbiased risk estimation, even for hypotheses from a class different from the one employed by the active learner. This is in sharp contrast to many previous algorithms (e.g., [CAL94, BBL06, BBZ07, DHM07, Han09, Kol09]) that create heavily biased data sets. We prove that our algorithm is always consistent and has an improved label complexity over passive learning in cases previously studied in the literature. We also describe a practical instantiation of our algorithm and report on some experimental results."}, {"heading": "1.1 Related Work", "text": "As already mentioned, our work is closely related to the previous works of [DHM07] and [BDL09], both of which in turn draw heavily on the work of [CAL94] and [BBL06]. The algorithm from [DHM07] extends the selective sampling method of [CAL94] to the agnostic setting using generalization bounds in a manner similar to that first suggested in [BBL06]. It accesses hypotheses only through a special ERM oracle that can enforce an arbitrary number of example-based constraints; these constraints define a version space, and the algorithm only ever returns hypotheses from this space, which can be undesirable as we previously argued. Other previous algorithms with comparable performance guarantees also require similar examplebased constraints (e.g., [BBL06, BDL09, Han09, Kol09]). Our algorithm differs from these in that (i) it never restricts its attention to a version space when selecting a hypothesis to return, and (ii) it only requires an ERM oracle that enforces at most one example-based constraint, and this constraint is only used for selective sampling. Our label complexity bounds are comparable to those proved in [BDL09] (though somewhat worse that those in [BBL06, DHM07, Han09, Kol09]).\nThe use of importance weights to correct for sampling bias is a standard technique for many machine learning problems (e.g., [SB98, ACBFS02, SKM07]) including active learning [Sug05, Bac06, BDL09]. Our algorithm is based on the importance weighted active learning (IWAL) framework introduced by [BDL09]. In that work, a rejection threshold procedure called loss-weighting is rigorously analyzed and shown to yield improved label complexity bounds in certain cases. Loss-weighting is more general than our technique in that it extends beyond zero-one loss to a certain subclass of loss functions such as logistic loss. On the other hand, the loss-weighting rejection threshold requires optimizing over a restricted version space, which is computationally undesirable. Moreover, the label complexity bound given in [BDL09] only applies to hypotheses selected from this version space, and not when selected from the entire hypothesis class (as the general IWAL framework suggests). We avoid these deficiencies using a new rejection threshold procedure and a more subtle martingale analysis.\nMany of the previously mentioned algorithms are analyzed in the agnostic learning model, where no assumption is made about the noise distribution (see also [Han07]). In this setting, the label complexity of active learning algorithms cannot generally improve over supervised learners by more than a constant factor [Ka\u0308a\u030806, BDL09]. However, under a parameterization of the noise distribution related to Tsybakov\u2019s low-noise condition [Tsy04], active learning algorithms have been shown to have improved label complexity bounds over what is achievable in the purely agnostic setting [CN06, BBZ07, CN07, Han09, Kol09]. We also consider this parameterization to obtain a tighter label complexity analysis."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Learning Model", "text": "Let D be a distribution over X \u00d7 Y where X is the input space and Y = {\u00b11} are the labels. Let (X,Y ) \u2208 X \u00d7 Y be a pair of random variables with joint distribution D. An active learner receives a sequence (X1, Y1), (X2, Y2), . . . of i.i.d. copies of (X,Y ), with the label Yi hidden unless it is explicitly queried. We use the shorthand a1:k to denote a sequence (a1, a2, . . . , ak) (so k = 0 correspond to the empty sequence).\nLet H be a set of hypotheses mapping from X to Y. For simplicity, we assume H is finite but does not completely agree on any single x \u2208 X (i.e., \u2200x \u2208 X , \u2203h, h\u2032 \u2208 H such that h(x) 6= h\u2032(x)). This keeps the focus on the relevant aspects of active learning that differ from passive learning. The error of a hypothesis h : X \u2192 Y is err(h) := Pr(h(X) 6= Y ). Let h\u2217 := argmin{err(h) : h \u2208 H} be a hypothesis of minimum error in H. The goal of the active learner is to return a hypothesis h \u2208 H with error err(h) not much more than err(h\u2217), using as few label queries as possible."}, {"heading": "2.2 Importance Weighted Active Learning", "text": "In the importance weighted active learning (IWAL) framework of [BDL09], an active learner looks at the unlabeled data X1, X2, . . . one at a time. After each new point Xi, the learner determines a probability Pi \u2208 [0, 1]. Then a coin with bias Pi is flipped, and the label Yi is queried if and only if the coin comes up heads. The query probability Pi can depend on all previous unlabeled examples X1:i\u22121, any previously queried labels, any past coin flips, and the current unlabeled point Xi.\nFormally, an IWAL algorithm specifies a rejection threshold function p : (X \u00d7 Y \u00d7 {0, 1})\u2217 \u00d7 X \u2192 [0, 1] for determining these query probabilities. Let Qi \u2208 {0, 1} be a random variable conditionally independent of the current label Yi, Qi \u22a5 Yi | X1:i, Y1:i\u22121, Q1:i\u22121 and with conditional expectation\nE[Qi|Z1:i\u22121, Xi] = Pi := p(Z1:i\u22121, Xi). where Zj := (Xj , Yj , Qj). That is, Qi indicates if the label Yi is queried (the outcome of the coin toss). Although the notation does not explicitly suggest this, the query probability Pi = p(Z1:i\u22121, Xi) is allowed to explicitly depend on a label Yj (j < i) if and only if it has been queried (Qj = 1)."}, {"heading": "2.3 Importance Weighted Estimators", "text": "We first review some standard facts about the importance weighting technique. For a function f : X\u00d7Y \u2192 R, define the importance weighted estimator of E[f(X,Y )] from Z1:n \u2208 (X \u00d7 Y \u00d7 {0, 1})n to be\nf\u0302(Z1:n) := 1\nn\nn\u2211\ni=1\nQi Pi \u00b7 f(Xi, Yi).\nNote that this quantity depends on a label Yi only if it has been queried (i.e., only if Qi = 1; it also depends on Xi only if Qi = 1). Our rejection threshold will be based on a specialization of this estimator, specifically the importance weighted empirical error of a hypothesis h\nerr(h, Z1:n) := 1\nn\nn\u2211\ni=1\nQi Pi \u00b7 1[h(Xi) 6= Yi].\nIn the notation of Algorithm 1, this is equivalent to\nerr(h, Sn) := 1\nn\n\u2211\n(Xi,Yi,1/Pi)\u2208Sn\n(1/Pi) \u00b7 1[h(Xi) 6= Yi] (1)\nwhere Sn \u2286 X \u00d7 Y \u00d7 R is the importance weighted sample collected by the algorithm. A basic property of these estimators is unbiasedness :\nE[f\u0302(Z1:n)] = 1\nn\nn\u2211\ni=1\nE[E[(Qi/Pi) \u00b7 f(Xi, Yi) | X1:i, Y1:i, Q1:i\u22121]]\n= 1\nn\nn\u2211\ni=1\nE[(Pi/Pi) \u00b7 f(Xi, Yi)]\n= E[f(X,Y )].\nSo, for example, the importance weighted empirical error of a hypothesis h is an unbiased estimator of its true error err(h). This holds for any choice of the rejection threshold that guarantees Pi > 0."}, {"heading": "3 A Deviation Bound for Importance Weighted Estimators", "text": "As mentioned before, the rejection threshold used by our algorithm is based on importance weighted error estimates err(h, Z1:n). Even though these estimates are unbiased, they are only reliable when the variance is not too large. To get a handle on this, we need a deviation bound for importance weighted estimators. This is complicated by two factors that rules out straightforward applications of some standard bounds:\n1. The importance weighted samples (Xi, Yi, 1/Pi) (or equivalently, the Zi = (Xi, Yi, Qi)) are not i.i.d. This is because the query probability Pi (and thus the importance weight 1/Pi) generally depends on Z1:i\u22121 and Xi.\n2. The effective range and variance of each term in the estimator are, themselves, random variables.\nTo address these issues, we develop a deviation bound using a martingale technique from [Zha05]. Let f : X \u00d7 Y \u2192 [\u22121, 1] be a bounded function. Consider any rejection threshold function p : (X \u00d7 Y \u00d7 {0, 1})\u2217 \u00d7X \u2192 (0, 1] for which Pn = p(Z1:n\u22121, Xn) is bounded below by some positive quantity (which may depend on n). Equivalently, the query probabilities Pn should have inverses 1/Pn bounded above by some deterministic quantity rmax (which, again, may depend on n). The a priori upper bound rmax on 1/Pn can be pessimistic, as the dependence on rmax in the final deviation bound will be very mild\u2014it enters in as log log rmax. Our goal is to prove a bound on |f\u0302(Z1:n) \u2212 E[f(X,Y )]| that holds with high probability over the joint distribution of Z1:n.\nTo start, we establish bounds on the range and variance of each term Wi := (Qi/Pi) \u00b7 f(Xi, Yi) in the estimator, conditioned on (X1:i, Y1:i, Q1:i\u22121). Let Ei[ \u00b7 ] denote E[ \u00b7 |X1:i, Y1:i, Q1:i\u22121]. Note that Ei[Wi] = (Ei[Qi]/Pi) \u00b7 f(Xi, Yi) = f(Xi, Yi), so if Ei[Wi] = 0, then Wi = 0. Therefore, the (conditional) range and variance are non-zero only if Ei[Wi] 6= 0. For the range, we have |Wi| = (Qi/Pi)\u00b7|f(Xi, Yi)| \u2264 1/Pi, and for the variance, Ei[(Wi\u2212Ei[Wi])2] \u2264 (Ei[Q2i ]/P 2i ) \u00b7f(Xi, Yi)2 \u2264 1/Pi. These range and variance bounds indicate the form of the deviations we can expect, similar to that of other classical deviation bounds.\nTheorem 1. Pick any t \u2265 0 and n \u2265 1. Assume 1 \u2264 1/Pi \u2264 rmax for all 1 \u2264 i \u2264 n, and let Rn := 1/min({Pi : 1 \u2264 i \u2264 n \u2227 f(Xi, Yi) 6= 0} \u222a {1}). With probability at least 1\u2212 2(3 + log2 rmax)e\u2212t/2,\n\u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\nQi Pi \u00b7 f(Xi, Yi)\u2212 E[f(X,Y )] \u2223\u2223\u2223\u2223\u2223 \u2264 \u221a 2Rnt n + \u221a 2t n + Rnt 3n .\nWe defer all proofs to the appendices."}, {"heading": "4 Algorithm", "text": "First, we state a deviation bound for the importance weighted error of hypotheses in a finite hypothesis class H that holds for all n \u2265 1. It is a simple consequence of Theorem 1 and union bounds; the form of the bound motivates certain algorithmic choices to be described below.\nLemma 1. Pick any \u03b4 \u2208 (0, 1). For all n \u2265 1, let\n\u03b5n := 16 log(2(3 + n log2 n)n(n+ 1)|H|/\u03b4)\nn = O\n( log(n|H|/\u03b4)\nn\n) . (3)\nLet (Z1, Z2, . . .) \u2208 (X \u00d7 Y \u00d7 {0, 1})\u2217 be the sequence of random variables specified in Section 2.2 using a rejection threshold p : (X \u00d7 Y \u00d7 {0, 1})\u2217 \u00d7 X \u2192 [0, 1] that satisfies p(z1:n, x) \u2265 1/nn for all (z1:n, x) \u2208 (X \u00d7 Y \u00d7 {0, 1})n \u00d7X and all n \u2265 1.\nThe following holds with probability at least 1\u2212 \u03b4. For all n \u2265 1 and all h \u2208 H,\n|(err(h, Z1:n)\u2212 err(h\u2217, Z1:n))\u2212 (err(h)\u2212 err(h\u2217))| \u2264 \u221a\n\u03b5n Pmin,n(h) + \u03b5n Pmin,n(h) (4)\nwhere Pmin,n(h) = min{Pi : 1 \u2264 i \u2264 n \u2227 h(Xi) 6= h\u2217(Xi)} \u222a {1} .\nWe let C0 = O(log(|H|/\u03b4)) \u2265 2 be a quantity such that \u03b5n (as defined in Eq. (3)) is bounded as \u03b5n \u2264 C0 \u00b7 log(n+1)/n. The following absolute constants are used in the description of the rejection threshold and the subsequent analysis: c1 := 5+2 \u221a 2, c2 := 5, c3 := ((c1+ \u221a 2)/(c1\u22122))2, c4 := (c1+ \u221a c3)\n2, c5 := c2+c3 .\nOur proposed algorithm is shown in Figure 1. The rejection threshold (Step 2) is based on the deviation bound from Lemma 1. First, the importance weighted error minimizing hypothesis hk and the \u201calternative\u201d hypothesis h\u2032k are found. Note that both optimizations are over the entire hypothesis class H (with h\u2032k only being required to disagree with hk on xk)\u2014this is a key aspect where our algorithm differs from previous approaches. The difference in importance weighted errors Gk of the two hypotheses is then computed. If Gk \u2264 \u221a (C0 log k)/(k \u2212 1) + (C0 log k)/(k \u2212 1), then the query probability Pk is set to 1. Otherwise, Pk is set to the positive solution s to the quadratic equation in Eq. (2). The functional form of Pk is roughly\nmin { 1, O ( 1\nG2k +\n1\nGk\n) \u00b7 C0 log k\nk \u2212 1\n} .\nIt can be checked that Pk \u2208 (0, 1] and that Pk is non-increasing with Gk. It is also useful to note that (log k)/(k \u2212 1) is monotonically decreasing with k \u2265 1 (we use the convention log(1)/0 = \u221e).\nIn order to apply Lemma 1 with our rejection threshold, we need to establish the (very crude) bound Pk \u2265 1/kk for all k.\nLemma 2. The rejection threshold of Algorithm 1 satisfies p(z1:n\u22121, x) \u2265 1/nn for all n \u2265 1 and all (z1:n\u22121, x) \u2208 (X \u00d7 Y \u00d7 {0, 1})n\u22121 \u00d7X .\nNote that this is a worst-case bound; our analysis shows that the probabilities Pk are more like 1/poly(k) in the typical case."}, {"heading": "5 Analysis", "text": ""}, {"heading": "5.1 Correctness", "text": "We first prove a consistency guarantee for Algorithm 1 that bounds the generalization error of the importance weighted empirical error minimizer. The proof actually establishes a lower bound on the query probabilities Pi \u2265 1/2 for Xi such that hn(Xi) 6= h\u2217(Xi). This offers an intuitive characterization of the weighting landscape induced by the importance weights 1/Pi.\nTheorem 2. The following holds with probability at least 1\u2212 \u03b4. For any n \u2265 1,\n0 \u2264 err(hn)\u2212 err(h\u2217) \u2264 err(hn, Z1:n\u22121)\u2212 err(h\u2217, Z1:n\u22121) + \u221a 2C0 logn\nn\u2212 1 + 2C0 logn n\u2212 1 .\nThis implies, for all n \u2265 1,\nerr(hn) \u2264 err(h\u2217) + \u221a 2C0 logn\nn\u2212 1 + 2C0 logn n\u2212 1 .\nTherefore, the final hypothesis returned by Algorithm 1 after seeing n unlabeled data has roughly the same error bound as a hypothesis returned by a standard passive learner with n labeled data. A variant of this result under certain noise conditions is given in the appendix."}, {"heading": "5.2 Label Complexity Analysis", "text": "We now bound the number of labels requested by Algorithm 1 after n iterations. The following lemma bounds the probability of querying the label Yn; this is subsequently used to establish the final bound on the expected number of labels queried. The key to the proof is in relating empirical error differences and their deviations to the probability of querying a label. This is mediated through the disagreement coefficient, a quantity first used by [Han07] for analyzing the label complexity of the A2 algorithm of [BBL06]. The disagreement coefficient \u03b8 := \u03b8(h\u2217,H,D) is defined as\n\u03b8(h\u2217,H,D) := sup { Pr(X \u2208 DIS(h\u2217, r))\nr : r > 0\n}\nwhere DIS(h\u2217, r) := {x \u2208 X : \u2203h\u2032 \u2208 H such that Pr(h\u2217(X) 6= h\u2032(X)) \u2264 r and h\u2217(x) 6= h\u2032(x)}\n(the disagreement region around h\u2217 at radius r). This quantity is bounded for many learning problems studied in the literature; see [Han07, Han09, Fri09, Wan09] for more discussion. Note that the supremum can instead be taken over r > \u01eb if the target excess error is \u01eb, which allows for a more detailed analysis.\nLemma 3. Assume the bounds from Eq. (4) holds for all h \u2208 H and n \u2265 1. For any n \u2265 1,\nE[Qn] \u2264 \u03b8 \u00b7 2 err(h\u2217) + O ( \u03b8 \u00b7 \u221a C0 logn\nn\u2212 1 + \u03b8 \u00b7 C0 log\n2 n\nn\u2212 1\n) .\nTheorem 3. With probability at least 1 \u2212 \u03b4, the expected number of labels queried by Algorithm 1 after n iterations is at most\n1 + \u03b8 \u00b7 2 err(h\u2217) \u00b7 (n\u2212 1) +O ( \u03b8 \u00b7 \u221a C0n logn+ \u03b8 \u00b7 C0 log3 n ) .\nProof. Follows from assuming Y1 is always queried; applying Lemmas 1, 2, 3, and linearity of expectation.\nThe bound is dominated by a linear term scaled by err(h\u2217), plus a sublinear term. The linear term err(h\u2217) \u00b7 n is unavoidable in the worst case, as evident from label complexity lower bounds [Ka\u0308a\u030806, BDL09]. When err(h\u2217) is negligible (e.g., the data is separable) and \u03b8 is bounded (as is the case for many problems studied in the literature [Han07]), then the bound represents a polynomial label complexity improvement over supervised learning, similar to that achieved by the version space algorithm from [BDL09]."}, {"heading": "5.3 Analysis under Low Noise Conditions", "text": "Some recent work on active learning has focused on improved label complexity under certain noise conditions [CN06, BBZ07, CN07, Han09, Kol09]. Specifically, it is assumed that there exists constants \u03ba > 0 and 0 < \u03b1 \u2264 1 such that Pr(h(X) 6= h\u2217(X)) \u2264 \u03ba \u00b7 (err(h)\u2212 err(h\u2217))\u03b1 (5) for all h \u2208 H. This is related to Tsybakov\u2019s low noise condition [Tsy04]. Essentially, this condition requires that low error hypotheses not be too far from the optimal hypothesis h\u2217 under the disagreement metric Pr(h\u2217(X) 6= h(X)). Under this condition, Lemma 3 can be improved, which in turn yields the following theorem.\nTheorem 4. Assume that for some value of \u03ba > 0 and 0 < \u03b1 \u2264 1, the condition in Eq. (5) holds for all h \u2208 H. There is a constant c\u03b1 > 0 depending only on \u03b1 such that the following holds. With probability at least 1\u2212 \u03b4, the expected number of labels queried by Algorithm 1 after n iterations is at most\n\u03b8 \u00b7 \u03ba \u00b7 c\u03b1 \u00b7 (C0 logn)\u03b1/2 \u00b7 n1\u2212\u03b1/2.\nNote that the bound is sublinear in n for all 0 < \u03b1 \u2264 1, which implies label complexity improvements whenever \u03b8 is bounded (an improved analogue of Theorem 2 under these conditions can be established using similar techniques). The previous algorithms of [Han09, Kol09] obtain even better rates under these noise conditions using specialized data dependent generalization bounds, but these algorithms also required optimizations over restricted version spaces, even for the bound computation."}, {"heading": "6 Experiments", "text": "Although agnostic learning is typically intractable in the worst case, empirical risk minimization can serve as a useful abstraction for many practical supervised learning algorithms in non-worst case scenarios. With this in mind, we conducted a preliminary experimental evaluation of Algorithm 1, implemented using a popular algorithm for learning decision trees in place of the required ERM oracle. Specifically, we use the J48 algorithm from Weka v3.6.2 (with default parameters) to select the hypothesis hk in each round k; to produce the \u201calternative\u201d hypothesis h\u2032k, we just modify the decision tree hk by changing the label of the node used for predicting on xk. Both of these procedures are clearly heuristic, but they are similar in spirit to the required optimizations. We set C0 = 8 and c1 = c2 = 1\u2014these can be regarded as tuning parameters, with C0 controlling the aggressiveness of the rejection threshold. We did not perform parameter tuning with active learning although the importance weighting approach developed here could potentially be used for that. Rather, the goal of these experiments is to assess the compatibility of Algorithm 1 with an existing, practical supervised learning procedure."}, {"heading": "6.1 Data Sets", "text": "We constructed two binary classification tasks using MNIST and KDDCUP99 data sets. For MNIST, we randomly chose 4000 training 3s and 5s for training (using the 3s as the positive class), and used all of the 1902 testing 3s and 5s for testing. For KDDCUP99, we randomly chose 5000 examples for training, and another 5000 for testing. In both cases, we reduced the dimension of the data to 25 using PCA.\nTo demonstrate the versatility of our algorithm, we also conducted a multi-class classification experiment using the entire MNIST data set (all ten digits, so 60000 training data and 10000 testing data). This required modifying how h\u2032k is selected: we force h \u2032\nk(xk) 6= hk(xk) by changing the label of the prediction node for xk to the next best label. We used PCA to reduce the dimension to 40."}, {"heading": "6.2 Results", "text": "We examined the test error as a function of (i) the number of unlabeled data seen, and (ii) the number of labels queried. We compared the performance of the active learner described above to a passive learner (one that queries every label, so (i) and (ii) are the same) using J48 with default parameters.\nIn all three cases, the test errors as a function of the number of unlabeled data were roughly the same for both the active and passive learners. This agrees with the consistency guarantee from Theorem 2. We note that this is a basic property not satisfied by many active learning algorithms (this issue is discussed further in [DH08]).\nIn terms of test error as a function of the number of labels queried (Figure 2), the active learner had minimal improvement over the passive learner on the binary MNIST task, but a substantial improvement over the passive learner on the KDDCUP99 task (even at small numbers of label queries). For the multiclass MNIST task, the active learner had a moderate improvement over the passive learner. Note that KDDCUP99 is far less noisy (more separable) than MNIST 3s vs 5s task, so the results are in line with the label complexity behavior suggested by Theorem 3, which states that the label complexity improvement may scale with the error of the optimal hypothesis. Also, the results from MNIST tasks suggest that the active learner may require an initial random sampling phase during which it is equivalent to the passive learner, and the advantage manifests itself after this phase. This again is consistent with the analysis (also\nsee [Han07]), as the disagreement coefficient can be large at initial scales, yet much smaller as the number of (unlabeled) data increases and the scale becomes finer."}, {"heading": "7 Conclusion", "text": "This paper provides a new active learning algorithm based on error minimization oracles, a departure from the version space approach adopted by previous works. The algorithm we introduce here motivates computationally tractable and effective methods for active learning with many classifier training algorithms. The overall algorithmic template applies to any training algorithm that (i) operates by approximate error minimization and (ii) for which the cost of switching a class prediction (as measured by example errors) can be estimated. Furthermore, although these properties might only hold in an approximate or heuristic sense, the created active learning algorithm will be \u201csafe\u201d in the sense that it will eventually converge to the same solution as a passive supervised learning algorithm. Consequently, we believe this approach can be widely used to reduce the cost of labeling in situations where labeling is expensive.\nRecent theoretical work on active learning has focused on improving rates of convergence. However, in some applications, it may be desirable to improve performance at much smaller sample sizes, perhaps even at the cost of improved rates as long as consistency is ensured. Importance sampling and weighting techniques like those analyzed in this work may be useful for developing more aggressive strategies with such properties."}, {"heading": "A Proof of Deviation Bound for Importance Weighted Estimators", "text": "The techniques here are mostly developed in [Zha05]; for completeness, we detail the proofs for our particular application. The first two lemmas establish a basic bound in terms of conditional moment generating functions.\nLemma 4. For all n \u2265 1 and all functionals \u039ei := \u03bei(Z1:i),\nE [ exp ( n\u2211\ni=1\n\u039ei \u2212 n\u2211\ni=1\nlnEi[exp(\u039ei)]\n)] = 1.\nProof. A straightforward induction on n.\nLemma 5. For all t \u2265 0, \u03bb \u2208 R, n \u2265 1, and functionals \u039ei := \u03bei(Z1:i),\nPr ( \u03bb n\u2211\ni=1\n\u039ei \u2212 n\u2211\ni=1\nlnEi[exp(\u03bb\u039ei)] \u2265 t ) \u2264 e\u2212t.\nProof. The claim follows by Markov\u2019s inequality and Lemma 4 (replacing \u039ei with \u03bb\u039ei).\nIn order to specialize Lemma 5 for our purposes, we first analyze the conditional moment generating function of Wi \u2212 Ei[Wi]. Lemma 6. If 0 < \u03bb < 3Pi, then\nlnEi[exp(\u03bb(Wi \u2212 Ei[Wi]))] \u2264 1 Pi \u00b7 \u03bb\n2\n2(1\u2212 \u03bb/(3Pi)) .\nIf Ei[Wi] = 0, then lnEi[exp(\u03bb(Wi \u2212 Ei[Wi]))] = 0.\nProof. Let g(x) := (exp(x)\u2212x\u22121)/x2 for x 6= 0, so exp(x) = 1+x+x2\u00b7g(x). Note that g(x) is non-decreasing. Thus,\nEi [exp(\u03bb(Wi \u2212 Ei[Wi]))] = Ei [ 1 + \u03bb(Wi \u2212 Ei[Wi]) + \u03bb2(Wi \u2212 Ei[Wi])2 \u00b7 g(\u03bb(Wi \u2212 Ei[Wi])) ]\n= 1 + \u03bb2 \u00b7 Ei [ (Wi \u2212 Ei[Wi])2 \u00b7 g(\u03bb(Wi \u2212 Ei[Wi])) ] \u2264 1 + \u03bb2 \u00b7 Ei [ (Wi \u2212 Ei[Wi])2 \u00b7 g(\u03bb/Pi) ] = 1 + \u03bb2 \u00b7 Ei [ (Wi \u2212 Ei[Wi])2 ] \u00b7 g(\u03bb/Pi)\n\u2264 1 + (\u03bb2/Pi) \u00b7 g(\u03bb/Pi) where the first inequality follows from the range bound |Wi| \u2264 1/Pi and the second follows from variance bound Ei[(Wi \u2212Ei[Wi])2] \u2264 1/Pi. Now the first claim follows from the definition of g(x), the facts exp(x)\u2212 x\u2212 1 \u2264 x2/(2(1\u2212 x/3)) for 0 \u2264 x < 3 and ln(1 + x) \u2264 x.\nThe second claim is immediate from the definition of Wi and the fact Ei[Wi] = f(Xi, Yi).\nWe now combine Lemma 6 and Lemma 5 to bound the deviation of the importance weighted estimator f\u0302(Z1:n) from (1/n) \u2211n i=1 Ei[Wi]. Lemma 7. Pick any t \u2265 0, n \u2265 1, and pmin > 0, and let E be the (joint) event\n1\nn\nn\u2211\ni=1\nWi \u2212 1\nn\nn\u2211\ni=1\nEi[Wi] \u2265 \u221a 1\npmin \u00b7 2t n + 1 pmin \u00b7 t 3n\nand min{Pi : 1 \u2264 i \u2264 n \u2227 Ei[Wi] 6= 0} \u2265 pmin. Then Pr(E) \u2264 e\u2212t.\nProof. With foresight, let\n\u03bb := 3pmin \u00b7\n\u221a 1\n3pmin \u00b7 2t3n\n1 + \u221a\n1 3pmin \u00b7 2t3n .\nNote that 0 < \u03bb < 3pmin. By Lemma 6 and the choice of \u03bb, we have that if min{Pi : 1 \u2264 i \u2264 n \u2227 Ei[Wi] 6= 0} \u2265 pmin, then\n1\nn\u03bb \u00b7\nn\u2211\ni=1\nlnEi[exp(\u03bb(Wi \u2212 Ei[Wi]))] \u2264 1 pmin \u00b7 \u03bb 2(1\u2212 \u03bb/(3pmin)) =\n\u221a 1\npmin \u00b7 t 2n\n(6)\nand t\nn\u03bb =\n\u221a 1\npmin \u00b7 t 2n + 1 pmin \u00b7 t 3n . (7)\nLet E\u2032 be the event that\n1 n \u00b7\nn\u2211\ni=1\n(Wi \u2212 Ei[Wi])\u2212 1\nn\u03bb \u00b7\nn\u2211\ni=1\nlnEi[exp(\u03bb(Wi \u2212 Ei[Wi]))] \u2265 t\nn\u03bb\nand let E\u2032\u2032 be the event min{Pi : 1 \u2264 i \u2264 n \u2227 Ei[Wi] 6= 0} \u2265 pmin. Together, Eq. (6) and Eq. (7) imply E \u2286 E\u2032 \u2229E\u2032\u2032. And of course, E\u2032 \u2229 E\u2032\u2032 \u2286 E\u2032, so Pr(E) \u2264 Pr(E\u2032 \u2229 E\u2032\u2032) \u2264 Pr(E\u2032) \u2264 e\u2212t by Lemma 5.\nTo do away with the joint event in Lemma 7, we use the standard trick of taking a union bound over a geometric sequence of possible values for pmin.\nLemma 8. Pick any t \u2265 0 and n \u2265 1. Assume 1 \u2264 1/Pi \u2264 rmax for all 1 \u2264 i \u2264 n, and let Rn := 1/min{Pi : 1 \u2264 i \u2264 n \u2227 Ei[Wi] 6= 0} \u222a {1}. We have\nPr (\u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\nWi \u2212 1\nn\nn\u2211\ni=1\nEi[Wi] \u2223\u2223\u2223\u2223\u2223 \u2265 \u221a 2Rnt n + Rnt 3n ) \u2264 2(2 + log2 rmax)e\u2212t/2.\nProof. The assumption on Pi implies 1 \u2264 Rn \u2264 rmax. Let rj := 2j for \u22121 \u2264 j \u2264 m := \u2308log2 rmax\u2309. Then\nPr\n( 1\nn\nn\u2211\ni=1\nWi \u2212 1\nn\nn\u2211\ni=1\nEi[Wi] \u2265 \u221a 2Rnt\nn +\nRnt\n3n\n)\n=\nm\u2211\nj=0\nPr\n( 1\nn\nn\u2211\ni=1\nWi \u2212 1\nn\nn\u2211\ni=1\nEi[Wi] \u2265 \u221a 2Rnt\nn +\nRnt\n3n \u2227 rj\u22121 < Rn \u2264 rj\n)\n\u2264 m\u2211\nj=0\nPr\n( 1\nn\nn\u2211\ni=1\nWi \u2212 1\nn\nn\u2211\ni=1\nEi[Wi] \u2265 \u221a 2rj\u22121t\nn +\nrj\u22121t\n3n \u2227 Rn \u2264 rj\n)\n=\nm\u2211\nj=0\nPr\n( 1\nn\nn\u2211\ni=1\nWi \u2212 1\nn\nn\u2211\ni=1\nEi[Wi] \u2265 \u221a 2rj(t/2)\nn +\nrj(t/2)\n3n \u2227 Rn \u2264 rj\n)\n\u2264 (2 + log2 rmax)e\u2212t/2\nwhere the last inequality follows from Lemma 7. ReplacingWi with \u2212Wi bounds the probability of deviations in the other direction in exactly the same way. The claim then follows by the union bound.\nProof of Theorem 1. By Hoeffding\u2019s inequality and the fact |f(Xi, Yi)| \u2264 1, we have\nPr (\u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\nf(Xi, Yi)\u2212 E[f(X,Y )] \u2223\u2223\u2223\u2223\u2223 \u2265 \u221a 2t n ) \u2264 2e\u2212t/2.\nSince Ei[Wi] = f(Xi, Yi), the claim follows by combining this and Lemma 8 with the triangle inequality and the union bound."}, {"heading": "B Remaining Proofs", "text": "In this section, we use the notation \u03b5k := C0 log(k + 1)/k.\nB.1 Proof of Lemma 2\nBy induction on n. Trivial for n = 1 (since p(empty sequence, x) = 1 for all x \u2208 X ), so now fix any n \u2265 2 and assume as the inductive hypothesis pn\u22121 = p(z1:n\u22122, x) \u2265 1/(n \u2212 1)n\u22121 for all (z1:n\u22122, x) \u2208 (X \u00d7 Y \u00d7 {0, 1})n\u22122 \u00d7 X . Fix any (z1:n\u22121, x) \u2208 (X \u00d7 Y \u00d7 {0, 1})n\u22121 \u00d7 X , and consider the error difference gn := err(h \u2032\nn, z1:n\u22121) \u2212 err(hn, z1:n\u22121) used to determine pn := p(z1:n\u22121, x). We only have to consider the case gn > \u221a \u03b5n\u22121 + \u03b5n\u22121. By the inductive hypothesis and triangle inequality, we have gn \u2264 2(n \u2212 1)n\u22121. Solving the quadratic in Eq. (2) implies\n\u221a pn =\nc1 \u00b7 \u221a\u03b5n\u22121 + \u221a c21 \u00b7 \u03b5n\u22121 + 4 \u00b7 ( gn + (c1 \u2212 1) \u00b7 \u221a\u03b5n\u22121 + (c2 \u2212 1) \u00b7 \u03b5n\u22121 ) \u00b7 c2 \u00b7 \u03b5n\u22121\n2 ( gn + (c1 \u2212 1) \u00b7 \u221a\u03b5n\u22121 + (c2 \u2212 1) \u00b7 \u03b5n\u22121 )\n>\n\u221a 4 \u00b7 ( gn + (c1 \u2212 1) \u00b7 \u221a\u03b5n\u22121 + (c2 \u2212 1) \u00b7 \u03b5n\u22121 ) \u00b7 c2 \u00b7 \u03b5n\u22121\n2 ( gn + (c1 \u2212 1) \u00b7 \u221a\u03b5n\u22121 + (c2 \u2212 1) \u00b7 \u03b5n\u22121 ) (dropping terms)\n=\n\u221a c2 \u00b7 \u03b5n\u22121\ngn + (c1 \u2212 1) \u00b7 \u221a\u03b5n\u22121 + (c2 \u2212 1) \u00b7 \u03b5n\u22121\n\u2265 \u221a\nc2 \u00b7 \u03b5n\u22121 gn + (c1 \u2212 1) \u00b7 \u221a\u03b5n\u22121 + (c1 \u2212 1) \u00b7 \u03b5n\u22121 (since c2 \u2264 c1)\n\u2265 \u221a\nc2 \u00b7 \u03b5n\u22121 c1 \u00b7 gn (since gn > \u221a \u03b5n\u22121 + \u03b5n\u22121)\n=\n\u221a c2 \u00b7 C0 logn\nc1 \u00b7 (n\u2212 1) \u00b7 gn\n\u2265 \u221a\nc2 \u00b7 C0 logn 2c1 \u00b7 (n\u2212 1) \u00b7 (n\u2212 1)n\u22121 (inductive hypothesis)\n>\n\u221a 1\ne(n\u2212 1)n (since C0 \u2265 2, n \u2265 2, and (c2 \u00b7 C0 log 2)/(2c1) > 1/e)\n\u2265 \u221a 1\nnn (since (n/(n\u2212 1))n \u2265 e)\nas required.\nB.2 Proof of Theorem 2\nWe condition on the 1 \u2212 \u03b4 probability event that the deviation bounds from Lemma 1 hold (also using Lemma 2). The proof now proceeds by induction on n. The claim is trivially true for n = 1. Now pick any n \u2265 2 and assume as the (strong) inductive hypothesis that\n0 \u2264 err(hk)\u2212 err(h\u2217) \u2264 err(hk, Z1:k\u22121)\u2212 err(h\u2217, Z1:k\u22121) + \u221a 2\u03b5k\u22121 + 2\u03b5k\u22121 (8)\nfor all 1 \u2264 k \u2264 n\u2212 1. We need to show Eq. (8) holds for k = n. Let Pmin := min{Pi : 1 \u2264 i \u2264 n \u2212 1 \u2227 hn(Xi) 6= h\u2217(Xi)} \u222a {1}. If Pmin \u2265 1/2, then Eq. (4) implies that Eq. (8) holds for k = n as needed. So assume for sake of contradiction that Pmin < 1/2, and let n0 := max{i \u2264 n\u2212 1 : Pi = Pmin \u2227 hn(Xi) 6= h\u2217(Xi)}. By definition of Pn0 , we have\nerr(h\u2032n0 , Z1:n0\u22121)\u2212 err(hn0 , Z1:n0\u22121) = (\nc1\u221a Pmin\n\u2212 c1 + 1 )\u221a \u03b5n0\u22121 + ( c2\nPmin \u2212 c2 + 1\n) \u03b5n0\u22121.\nUsing this fact together with the inductive hypothesis, we have\nerr(h\u2032n0 , Z1:n0\u22121)\u2212 err(h\u2217, Z1:n0\u22121) = err(h\u2032n0 , Z1:n0\u22121)\u2212 err(hn0 , Z1:n0\u22121) + err(hn0 , Z1:n0\u22121)\u2212 err(h\u2217, Z1:n0\u22121)\n\u2265 (\nc1\u221a Pmin\n\u2212 c1 + 1 ) \u00b7 \u221a\u03b5n0\u22121 + ( c2\nPmin \u2212 c2 + 1\n) \u00b7 \u03b5n0\u22121 \u2212 \u221a 2\u03b5n0\u22121 \u2212 2\u03b5n0\u22121\n= ( c1\u221a Pmin \u2212 c1 + 1\u2212 \u221a 2 ) \u00b7 \u221a\u03b5n0\u22121 + ( c2 Pmin \u2212 c2 \u2212 1 ) \u00b7 \u03b5n0\u22121 . (9)\nWe use the assumption Pmin < 1/2 to lower bound the righthand side to get the inequality\nerr(h\u2032n0 , Z1:n0\u22121)\u2212 err(h\u2217, Z1:n0\u22121) > (c1 \u2212 1) \u00b7 ( \u221a 2\u2212 1) \u00b7 \u221a\u03b5n0\u22121 + (c2 \u2212 1) \u00b7 \u03b5n0\u22121 > 0.\nwhich implies err(h\u2032n0 , Z1:n0\u22121) > err(h \u2217, Z1:n0\u22121). Since h \u2032 n0 minimizes err(h, Z1:n0\u22121) among hypotheses h \u2208 H that disagree with hn0 on Xn0 , it must be that h\u2217 agrees with hn0 on Xn0 . By transitivity and the definition of n0, we conclude that hn(Xn0) = h \u2032\nn0(Xn0); so err(hn, Z1:n0\u22121) \u2265 err(h\u2032n0 , Z1:n0\u22121). Then err(hn, Z1:n\u22121)\u2212 err(h\u2217, Z1:n\u22121)\n\u2265 err(hn)\u2212 err(h\u2217)\u2212 \u221a 1\nPmin \u00b7 \u03b5n\u22121 \u2212\n1\nPmin \u00b7 \u03b5n\u22121\n\u2265 err(hn, Z1:n0\u22121)\u2212 err(h\u2217, Z1:n0\u22121)\u2212 2 \u00b7 \u221a 1\nPmin \u00b7 \u03b5n0\u22121 \u2212 2 \u00b7\n1\nPmin \u00b7 \u03b5n0\u22121\n\u2265 (\nc1 \u2212 2\u221a Pmin \u2212 c1 + 1\u2212 \u221a 2\n) \u00b7 \u221a\u03b5n0\u22121 + ( c2 \u2212 2 Pmin \u2212 c2 \u2212 1 ) \u00b7 \u03b5n0\u22121\n> ( (c1 \u2212 1) \u00b7 ( \u221a 2\u2212 1)\u2212 2 \u221a 2 ) \u00b7 \u221a\u03b5n0\u22121 + (c2 \u2212 5) \u00b7 \u03b5n0\u22121\nwhere Eq. (4) is used in the first two inequalities, Eq. (9) and the fact err(hn, Z1:n0\u22121) \u2265 err(h\u2032n0 , Z1:n0\u22121) are used in the third inequality, and the fact Pmin < 1/2 is used in the last inequality. This final quantity is non-negative, so we have the contradiction err(hn, Z1:n\u22121) > err(h \u2217, Z1:n\u22121).\nB.3 Proof of Lemma 3\nFirst, we establish a property of the query probabilities that relates error deviations (via Pmin) to empirical error differences (via Pn). Both quantities play essential roles in bounding the label complexity through the disagreement metric structure around h\u2217.\nLemma 9. Assume the bounds from Eq. (4) hold for all h \u2208 H and n \u2265 1. For any n \u2265 1, we have Pn \u2264 c3 \u00b7 Pmin, where Pmin := min({Pi : 1 \u2264 i \u2264 n\u2212 1 \u2227 h(Xi) 6= h\u2217(Xi)} \u222a {1}) and\nh :=\n{ hn if hn disagrees with h\n\u2217 on Xn h\u2032n if h \u2032 n disagrees with h \u2217 on Xn.\n(10)\nProof. We can assume Pmin < 1/c3, since otherwise the claim is trivial. Pick any n0 \u2264 n \u2212 1 such that h(Xn0) 6= h\u2217(Xn0) and Pn0 = Pmin (such an n0 is guaranteed to exist given the above assumption). We now proceed as in the proof of Theorem 2. We first show a lower bound on err(h, Z1:n0\u22121) \u2212 err(h\u2217, Z1:n0\u22121). Note that\nerr(h\u2032n0 , Z1:n0\u22121)\u2212 err(h\u2217, Z1:n0\u22121) = err(h\u2032n0 , Z1:n0\u22121)\u2212 err(hn0 , Z1:n0\u22121) + err(hn0 , Z1:n0\u22121)\u2212 err(h\u2217, Z1:n0\u22121)\n\u2265 (\nc1\u221a Pmin\n\u2212 c1 + 1 ) \u00b7 \u221a\u03b5n0\u22121 + ( c2\nPmin \u2212 c2 + 1\n) \u00b7 \u03b5n0\u22121 \u2212 \u221a 2\u03b5n0\u22121 \u2212 2\u03b5n0\u22121\n= ( c1\u221a Pmin \u2212 c1 + 1\u2212 \u221a 2 ) \u00b7 \u221a\u03b5n0\u22121 + ( c2 Pmin \u2212 c2 \u2212 1 ) \u00b7 \u03b5n0\u22121 (11)\nwhere the inequality follows from Theorem 2. The righthand side is positive, so h\u2217 must disagree with h\u2032n0 on Xn0 . By transitivity (recalling that h(Xn0) 6= h\u2217(Xn0)), h must agree with h\u2032n0 on Xn0 . Therefore err(h, Z1:n0\u22121) \u2212 err(h\u2032n0 , Z1:n0\u22121) \u2265 0, so the inequality in Eq. (11) holds with h in place of h\u2032n0 on the lefthand side.\nNow err(h, Z1:n\u22121)\u2212err(h\u2217, Z1:n\u22121) is related to err(h, Z1:n0\u22121)\u2212err(h\u2217, Z1:n0\u22121) through err(h)\u2212err(h\u2217) using the deviation bound from Eq. (4) (as well as the fact \u03b5n0\u22121 \u2265 \u03b5n\u22121):\nerr(h, Z1:n\u22121)\u2212 err(h\u2217, Z1:n\u22121)\n\u2265 err(h, Z1:n0\u22121)\u2212 err(h\u2217, Z1:n0\u22121)\u2212 2 \u00b7 \u221a 1\nPmin \u00b7 \u03b5n0\u22121 \u2212 2 \u00b7\n1\nPmin \u00b7 \u03b5n0\u22121\n\u2265 (\nc1 \u2212 2\u221a Pmin \u2212 c1 + 1\u2212 \u221a 2\n) \u00b7 \u221a\u03b5n\u22121 + ( c2 \u2212 2 Pmin \u2212 c2 \u2212 1 ) \u00b7 \u03b5n\u22121 > 0. (12)\nIf h = hn, then err(h, Z1:n\u22121)\u2212 err(h\u2217, Z1:n\u22121) = err(hn, Z1:n\u22121)\u2212 err(h\u2217, Z1:n\u22121) \u2264 0 by the minimality of err(hn, Z1:n\u22121); this contradicts Eq. (12). Therefore it must be that h = h \u2032 n. In this case,\nerr(h, Z1:n\u22121)\u2212 err(h\u2217, Z1:n\u22121) \u2264 err(h\u2032n, Z1:n\u22121)\u2212 err(hn, Z1:n\u22121)\n= ( c1\u221a Pn \u2212 c1 + 1 ) \u00b7 \u221a\u03b5n\u22121 + ( c2 Pn \u2212 c2 + 1 ) \u00b7 \u03b5n\u22121 (13)\nwhere the inequality follows from the minimality of err(hn, Z1:n\u22121), and the subsequent step follows from the definition of Pn. Combining the lower bound in Eq. (12) and the upper bound in Eq. (13) implies that\nc1\u221a Pn \u00b7 \u221a\u03b5n\u22121 + c2 Pn\n\u00b7 \u03b5n\u22121 \u2265 (\nc1 \u2212 2\u221a Pmin \u2212 \u221a 2\n) \u00b7 \u221a\u03b5n\u22121 + ( c2 \u2212 2 Pmin \u2212 2 ) \u00b7 \u03b5n\u22121.\nIt is easily checked that this implies Pn \u2264 c3 \u00b7 Pmin.\nProof of Lemma 3. Define h as in Eq. (10). By Lemma 9, we have min({Pi : 1 \u2264 i \u2264 n \u2212 1 \u2227 h(Xi) 6= h\u2217(Xi)} \u222a {1}) \u2265 Pn/c3. We first show that\nerr(h)\u2212 err(h\u2217) \u2264 err(h, Z1:n\u22121)\u2212 err(h\u2217, Z1:n\u22121) + \u221a\nc3 Pn \u00b7 \u03b5n\u22121 + c3 Pn \u00b7 \u03b5n\u22121\n\u2264 \u221a\nc4 Pn \u00b7 \u221a\u03b5n\u22121 + c5 Pn \u00b7 \u03b5n\u22121. (14)\nThe first inequality follows from Eq. (4) and Lemma 9. For the second inequality, we consider two cases depending on h. If h = h\u2032n, then we bound err(h, Z1:n\u22121) \u2212 err(h\u2217, Z1:n\u22121) from above by err(h\u2032n, Z1:n\u22121)\u2212 err(hn, Z1:n\u22121) (by definition of h and minimality of err(hn, Z1:n\u22121)), and then simplify\nerr(h\u2032n, , Z1:n\u22121)\u2212 err(hn, Z1:n\u22121) + \u221a\nc3 Pn \u00b7 \u03b5n\u22121 + c3 Pn \u00b7 \u03b5n\u22121\n\u2264 ( c1 + \u221a c3\u221a\nPn \u2212 c1 + 1\n) \u00b7 \u221a\u03b5n\u22121 + ( c2 + c3 Pn \u2212 c2 + 1 ) \u00b7 \u03b5n\u22121 \u2264 \u221a c4 Pn \u00b7 \u221a\u03b5n\u22121 + c5 Pn \u00b7 \u03b5n\u22121\nusing the definition of Pn and the facts c1 \u2265 1 and c2 \u2265 1. If instead h = hn, then we use the facts err(h, Z1:n\u22121)\u2212 err(h\u2217, Z1:n\u22121) = err(hn, Z1:n\u22121)\u2212 err(h\u2217, Z1:n\u22121) \u2264 0 and c3 \u2264 min{c4, c5}.\nIf err(h)\u2212 err(h\u2217) = \u03b3 > 0, then solving the quadratic inequality in Eq. (14) for Pn gives the bound\nPn \u2264 min { 1, 3 2 \u00b7 ( c4 \u03b32 + c5 \u03b3 ) \u00b7 \u03b5n\u22121 } .\nIf err(h)\u2212 err(h\u2217) \u2264 \u03b3\u0304, then by the triangle inequality we have\nPr(h\u2217(X) 6= h(X)) \u2264 err(h\u2217) + err(h) \u2264 2 err(h\u2217) + \u03b3\u0304\nwhich in turn implies Xn \u2208 DIS(h\u2217, 2 err(h\u2217)+ \u03b3\u0304). Note that Pr(Xn \u2208 DIS(h\u2217, 2 err(h\u2217)+ \u03b3\u0304)) \u2264 \u03b8 \u00b7(2 err(h\u2217)+ \u03b3\u0304) by definition of \u03b8, so Pr(err(h)\u2212 err(h\u2217) \u2264 \u03b3\u0304) \u2264 \u03b8 \u00b7 (2 err(h\u2217) + \u03b3\u0304).\nLet f(\u03b3) := \u2202 Pr(err(h)\u2212err(h\u2217) \u2264 \u03b3)/\u2202\u03b3 be the probability density (mass) function of the error difference err(h)\u2212 err(h\u2217); note that this error difference is a function of (Z1:n\u22121, Xn). We compute the expected value of Qn by conditioning on err(h) \u2212 err(h\u2217) and integrating (an upper bound on) E[Qn| err(h)\u2212 err(h\u2217) = \u03b3] with respect to f(\u03b3).\nLet \u03b30 > 0 be the positive solution to 1.5(c4/\u03b3 2+c5/\u03b3)\u03b5n\u22121 = 1. It can be checked that \u03b30 > \u221a 1.5c4\u03b5n\u22121.\nWe have\nE[Qn] = E[E[Qn|Z1:n\u22121, Xn]] (the outer expectation is over (Z1:n\u22121, Xn))\n=\n\u222b 1\n0\n( \u2202\n\u2202\u03b3 Pr(err(h)\u2212 err(h\u2217) \u2264 \u03b3)\n) \u00b7 E[Qn| err(h)\u2212 err(h\u2217) = \u03b3] \u00b7 d\u03b3\n\u2264 \u222b 1\n0\n( \u2202\n\u2202\u03b3 Pr(err(h)\u2212 err(h\u2217) \u2264 \u03b3)\n) \u00b7min { 1, 3 2 \u00b7 ( c4 \u03b32 + c5 \u03b3 ) \u00b7 \u03b5n\u22121 } \u00b7 d\u03b3\n\u2264 3 2 \u00b7 (c4 + c5) \u00b7 \u03b5n\u22121 \u00b7 Pr(err(h)\u2212 err(h\u2217) \u2264 1)\n\u2212 \u222b 1\n0\n( \u2202\n\u2202\u03b3 min\n{ 1, 3 2 \u00b7 ( c4 \u03b32 + c5 \u03b3 ) \u00b7 \u03b5n\u22121 }) \u00b7 Pr(err(h)\u2212 err(h\u2217) \u2264 \u03b3) \u00b7 d\u03b3\n\u2264 3 2 \u00b7 (c4 + c5) \u00b7 \u03b5n\u22121 +\n\u222b 1\n\u03b30\n3 2 \u00b7 ( 2c4 \u03b33 + c5 \u03b32 ) \u00b7 \u03b5n\u22121 \u00b7 \u03b8 \u00b7 (2 err(h\u2217) + \u03b3) \u00b7 d\u03b3\n= 3\n2 \u00b7 (c4 + c5) \u00b7 \u03b5n\u22121 + \u03b8 \u00b7 2 err(h\u2217) \u00b7\n3 2 \u00b7 ( c4 ( 1 \u03b320 \u2212 1 ) + c5 ( 1 \u03b30 \u2212 1 )) \u00b7 \u03b5n\u22121\n+ \u03b8 \u00b7 3 2 \u00b7 ( 2c4 ( 1 \u03b30 \u2212 1 ) + c5 ln 1 \u03b30 ) \u00b7 \u03b5n\u22121\n\u2264 3 2 \u00b7 (c4 + c5) \u00b7 \u03b5n\u22121 + \u03b8 \u00b7 2 err(h\u2217) + \u03b8 \u00b7\n\u221a 6c4\u03b5n\u22121 + \u03b8 \u00b7\n3c5 4 \u00b7 \u03b5n\u22121 \u00b7 ln 1\n1.5c4\u03b5n\u22121\nwhere the first inequality uses the bound on E[Qn| err(h)\u2212err(h\u2217) = \u03b3]; the second inequality uses integrationby-parts; the third inequality uses the fact that the integrand from the previous line is 0 for 0 \u2264 \u03b3 \u2264 \u03b30, as well as the bound on Pr(err(h)\u2212 err(h\u2217) \u2264 \u03b3); and the fourth inequality uses the definition of \u03b30.\nB.4 Proof of Theorem 4\nThe theorem is a simple consequence of the following analogue of Lemma 3.\nLemma 10. Assume that for some value of \u03ba > 0 and 0 < \u03b1 \u2264 1, the condition in Eq. (5) holds for all h \u2208 H. Assume the bounds from Eq. (4) holds for all h \u2208 H and n \u2265 1. There is a constant c\u03b1 > 0 such that the following holds. For any n \u2265 1,\nE[Qn] \u2264 \u03b8 \u00b7 \u03ba \u00b7 c\u03b1 \u00b7 ( C0 logn\nn\u2212 1\n)\u03b1/2 .\nProof. For the most part, the proof is the same as that of Lemma 3. The key difference is to use the noise condition in Eq. (5) to directly bound Pr(h(X) 6= h\u2217(X)) \u2264 \u03ba \u00b7 (err(h) \u2212 err(h\u2217))\u03b1, which in turn implies the bound Pr(err(h) \u2212 err(h\u2217) \u2264 \u03b3) \u2264 \u03b8\u03ba\u03b3\u03b1. As before, let \u03b30 > \u221a 1.5c4\u03b5n\u22121 be the solution to\n1.5(c4/\u03b3 2 + c5/\u03b3)\u03b5n\u22121 = 1. First consider the case \u03b1 < 1. Then, the expectation of Qn can be bounded as\nE[Qn] \u2264 3\n2 \u00b7 (c4 + c5) \u00b7 \u03b5n\u22121 +\n\u222b 1\n\u03b30\n3 2 \u00b7 ( 2c4 \u03b33 + c5 \u03b32 ) \u00b7 \u03b5n\u22121 \u00b7 Pr(err(h)\u2212 err(h\u2217) \u2264 \u03b3) \u00b7 d\u03b3\n\u2264 3 2 \u00b7 (c4 + c5) \u00b7 \u03b5n\u22121 +\n\u222b 1\n\u03b30\n3 2 \u00b7 ( 2c4 \u03b33 + c5 \u03b32 ) \u00b7 \u03b5n\u22121 \u00b7 \u03b8\u03ba\u03b3\u03b1 \u00b7 d\u03b3\n\u2264 3 2 \u00b7 (c4 + c5) \u00b7 \u03b5n\u22121 + \u03b8 \u00b7 \u03ba \u00b7 3 2 \u00b7 ( 2c4 2\u2212 \u03b1 \u00b7 1\n\u03b32\u2212\u03b10 +\nc5 1\u2212 \u03b1 \u00b7 1\n\u03b31\u2212\u03b10\n) \u00b7 \u03b5n\u22121.\nThe case \u03b1 = 1 is handled similarly.\nB.5 Analogue of Theorem 2 under Low Noise Conditions\nWe first state a variant of Lemma 1 that takes into account the probability of disagreement between a hypothesis h and the optimal hypothesis h\u2217.\nLemma 11. There exists an absolute constant c > 0 such that the following holds. Pick any \u03b4 \u2208 (0, 1). For all n \u2265 1, let\n\u03b5n := c \u00b7 log((n+ 1)|H|/\u03b4)\nn .\nLet (Z1, Z2, . . .) \u2208 (X \u00d7 Y \u00d7 {0, 1})\u2217 be the sequence of random variables specified in Section 2.2 using a rejection threshold p : (X \u00d7 Y \u00d7 {0, 1})\u2217 \u00d7 X \u2192 [0, 1] that satisfies p(z1:n, x) \u2265 1/nn for all (z1:n, x) \u2208 (X \u00d7 Y \u00d7 {0, 1})n \u00d7X and all n \u2265 1.\nThe following holds with probability at least 1\u2212 \u03b4. For all n \u2265 1 and all h \u2208 H,\n|(err(h, Z1:n)\u2212 err(h\u2217, Z1:n))\u2212 (err(h)\u2212 err(h\u2217))| \u2264 \u221a\nPr(h(X) 6= h\u2217(X)) Pmin,n(h) \u00b7 \u03b5n + \u03b5n Pmin,n(h)\nwhere Pmin,n(h) = min{Pi : 1 \u2264 i \u2264 n \u2227 h(Xi) 6= h\u2217(Xi)} \u222a {1}.\nProof sketch. The proof of this lemma follows along the same lines as that of Lemma 1. A key difference comes in Lemma 7: the joint event is modified to also conjoin with\n1\nn\nn\u2211\ni=1\n1(Ei[f(Xi, Yi)] \u2264 0) \u2264 a\nfor some fixed a > 0. In the proof, the parameter \u03bb should be chosen as\n\u03bb := 3pmin \u00b7\n\u221a 1\n3pmin \u00b7 2at3n\na+ \u221a\n1 3pmin \u00b7 2at3n .\nLemma 8 is modified to also take a union bound over a sequence of possible values for a (in fact, only n + 1 different values need to be considered). Finally, instead of combining with Hoeffding\u2019s inequality, we use Bernstein\u2019s inequality (or a multiplicative form of Chernoff\u2019s bound) so the resulting bound (an analogue of Theorem 1) involves an empirical average inside the square-root term: with probability at least 1\u2212O(n \u00b7 log2 rmax)e\u2212t/2,\n\u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\nQi Pi \u00b7 f(Xi, Yi)\u2212 E[f(X,Y )] \u2223\u2223\u2223\u2223\u2223 \u2264 O (\u221a RnAnt n + Rnt 3n )\nwhere\nAn := 1\nn\nn\u2211\ni=1\n1(f(Xi, Yi) 6= 0).\nFinally, we apply this deviation bound to obtain uniform error bounds over all hypotheses H (a few extra steps are required to replace the empirical quantity An in the bound with a distributional quantity).\nUsing the previous lemma, a modified version of Theorem 2 follows from essentially the same proof. We note that the quantity C1 := O(log(|H|/\u03b4)) used here may differ from C0 by constant factors.\nLemma 12. The following holds with probability at least 1\u2212 \u03b4. For any n \u2265 1,\n0 \u2264 err(hn)\u2212 err(h\u2217) \u2264 err(hn, Z1:n\u22121)\u2212 err(h\u2217, Z1:n\u22121)\n+\n\u221a 2Pr(hn(X) 6= h\u2217(X))C1 logn\nn\u2212 1 + 2C1 logn n\u2212 1 .\nThis implies, for all n \u2265 1,\nerr(hn) \u2264 err(h\u2217) + \u221a\n2Pr(hn(X) 6= h\u2217(X))C1 logn n\u2212 1 + 2C0 logn n\u2212 1 .\nFinally, using the noise condition to bound Pr(hn(X) 6= h\u2217(X)) \u2264 \u03ba \u00b7 (err(hn)\u2212 err(h\u2217))\u03b1, we obtain the final error bound.\nTheorem 5. The following holds with probability at least 1\u2212 \u03b4. For any n \u2265 1,\nerr(hn) \u2264 err(h\u2217) + c\u03ba \u00b7 ( C1 logn\nn\u2212 1\n) 1 2\u2212\u03b1\nwhere c\u03ba is a constant that depends only on \u03ba."}], "references": [{"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM Journal of Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Active learning for misspecified generalized linear models", "author": ["F. Bach"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bach.,? \\Q2006\\E", "shortCiteRegEx": "Bach.", "year": 2006}, {"title": "Agnostic active learning", "author": ["M.-F. Balcan", "A. Beygelzimer", "J. Langford"], "venue": "In Twenty-Third International Conference on Machine Learning,", "citeRegEx": "Balcan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2006}, {"title": "Margin based active learning", "author": ["M.-F. Balcan", "A. Broder", "T. Zhang"], "venue": "In Twentieth Annual Conference on Learning Theory,", "citeRegEx": "Balcan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2007}, {"title": "Importance weighted active learning", "author": ["A. Beygelzimer", "S. Dasgupta", "J. Langford"], "venue": "In Twenty-Sixth International Conference on Machine Learning,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2009}, {"title": "Improving generalization with active learning", "author": ["D. Cohn", "L. Atlas", "R. Ladner"], "venue": "Machine Learning,", "citeRegEx": "Cohn et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 1994}, {"title": "Upper and lower bounds for active learning", "author": ["R. Castro", "R. Nowak"], "venue": "In Allerton Conference on Communication, Control and Computing,", "citeRegEx": "Castro and Nowak.,? \\Q2006\\E", "shortCiteRegEx": "Castro and Nowak.", "year": 2006}, {"title": "Minimax bounds for active learning", "author": ["R. Castro", "R. Nowak"], "venue": "In Twentieth Annual Conference on Learning Theory,", "citeRegEx": "Castro and Nowak.,? \\Q2007\\E", "shortCiteRegEx": "Castro and Nowak.", "year": 2007}, {"title": "Coarse sample complexity bounds for active learning", "author": ["S. Dasgupta"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Dasgupta.,? \\Q2005\\E", "shortCiteRegEx": "Dasgupta.", "year": 2005}, {"title": "Hierarchical sampling for active learning", "author": ["S. Dasgupta", "D. Hsu"], "venue": "In Twenty-Fifth International Conference on Machine Learning,", "citeRegEx": "Dasgupta and Hsu.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta and Hsu.", "year": 2008}, {"title": "A general agnostic active learning algorithm", "author": ["S. Dasgupta", "D. Hsu", "C. Monteleoni"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Dasgupta et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2007}, {"title": "Active learning for smooth problems", "author": ["E. Friedman"], "venue": "In Twenty-Second Annual Conference on Learning Theory,", "citeRegEx": "Friedman.,? \\Q2009\\E", "shortCiteRegEx": "Friedman.", "year": 2009}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["S. Hanneke"], "venue": "In Twenty-Fourth International Conference on Machine Learning,", "citeRegEx": "Hanneke.,? \\Q2007\\E", "shortCiteRegEx": "Hanneke.", "year": 2007}, {"title": "Adaptive rates of convergence in active learning", "author": ["S. Hanneke"], "venue": "In Twenty-Second Annual Conference on Learning Theory,", "citeRegEx": "Hanneke.,? \\Q2009\\E", "shortCiteRegEx": "Hanneke.", "year": 2009}, {"title": "Active learning in the non-realizable case", "author": ["M. K\u00e4\u00e4ri\u00e4inen"], "venue": "In Seventeenth International Conference on Algorithmic Learning Theory,", "citeRegEx": "K\u00e4\u00e4ri\u00e4inen.,? \\Q2006\\E", "shortCiteRegEx": "K\u00e4\u00e4ri\u00e4inen.", "year": 2006}, {"title": "Rademacher complexities and bounding the excess risk in active learning", "author": ["V. Koltchinskii"], "venue": null, "citeRegEx": "Koltchinskii.,? \\Q2009\\E", "shortCiteRegEx": "Koltchinskii.", "year": 2009}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. .S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Covariate shift adaptation by importance weighted cross validation", "author": ["M. Sugiyama", "M. Krauledat", "K.-R. M\u00fcller"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sugiyama et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2007}, {"title": "Active learning for misspecified models", "author": ["M. Sugiyama"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sugiyama.,? \\Q2005\\E", "shortCiteRegEx": "Sugiyama.", "year": 2005}, {"title": "Optimal aggregation of classifiers in statistical learning", "author": ["A.B. Tsybakov"], "venue": "Annals of Statistics,", "citeRegEx": "Tsybakov.,? \\Q2004\\E", "shortCiteRegEx": "Tsybakov.", "year": 2004}, {"title": "Sufficient conditions for agnostic active learnable", "author": ["L. Wang"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Wang.,? \\Q2009\\E", "shortCiteRegEx": "Wang.", "year": 2009}, {"title": "Data dependent concentration bounds for sequential prediction algorithms", "author": ["T. Zhang"], "venue": "In Eighteenth Annual Conference on Learning Theory,", "citeRegEx": "Zhang.,? \\Q2005\\E", "shortCiteRegEx": "Zhang.", "year": 2005}], "referenceMentions": [], "year": 2010, "abstractText": "We present and analyze an agnostic active learning algorithm that works without keeping a version space. This is unlike all previous approaches where a restricted set of candidate hypotheses is maintained throughout learning, and only hypotheses from this set are ever returned. By avoiding this version space approach, our algorithm sheds the computational burden and brittleness associated with maintaining version spaces, yet still allows for substantial improvements over supervised learning for classification.", "creator": "LaTeX with hyperref package"}}}