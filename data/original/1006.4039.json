{"id": "1006.4039", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2010", "title": "Distributed Autonomous Online Learning: Regrets and Intrinsic Privacy-Preserving Properties", "abstract": "Online learning is becoming increasingly popular for training on large datasets. However, the sequential nature of online learning requires a centralized learner to store data and update parameters. In this paper, we consider a fully decentralized setting, cooperative autonomous online learning, with a distributed data source. The learners perform learning with local parameters while periodically communicating with a small subset of neighbors to exchange information. We define the regret in terms of an implicit aggregated parameter of the learners for such a setting and prove regret bounds similar to the classical sequential online learning.", "histories": [["v1", "Mon, 21 Jun 2010 11:30:06 GMT  (24kb)", "http://arxiv.org/abs/1006.4039v1", null], ["v2", "Tue, 1 Feb 2011 03:16:12 GMT  (74kb,D)", "http://arxiv.org/abs/1006.4039v2", "24 pages, 2 figures"], ["v3", "Fri, 4 Feb 2011 16:06:35 GMT  (69kb,D)", "http://arxiv.org/abs/1006.4039v3", "25 pages, 2 figures"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["feng yan", "shreyas sundaram", "s v n vishwanathan", "yuan qi"], "accepted": false, "id": "1006.4039"}, "pdf": {"name": "1006.4039.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n00 6.\n40 39\nv1 [\ncs .L\nG ]\n2 1\nJu n\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n050\n051\n052\n053"}, {"heading": "1 Introduction", "text": "Online learning has emerged as an attractive and dominant paradigm in machine learning given the ever increasing amounts of data that is being increasing collected everyday. However, the key underlying assumption here is that all the training data is available at a central location. For many applications this is not the case. For instance, sensor networks may be deployed in rain forests and collect data autonomously. The cost of transmitting the data to a central server can be prohibitively high. Similarly, banks collect credit information about their customers but might not share the information with other financial institutions. Similar privacy concerns might prevent sharing of patient records across hospitals. Our aim in this paper is to devise an online learning algorithm which can work in the distributed data setting.\nSome research effort has been devoted to this problem recently. For instance, [1] cast many machine learning problems as a regularized risk minimization problem. Since the empirical risk is computed by averaging the loss over the entire data set, one can design a decentralized algorithm by letting individual slave processors take charge of a portion of the data. At every iteration, the master communicates the current parameter vector to all the slaves, and they in turn compute the loss and its gradient on the part of the data they own and communicate it back to the master. However, as [1] observe, when the number of processors increases Amdahls law [2] kicks in and the cost of communicating and synchronizing becomes prohibitively expensive.\nAnother notable effort is the work by [3], where they show that one can avoid the expensive synchronization step above. The key idea here is that the slaves periodically poll the master node to receive the latest parameter vector. This is used to compute stochastic gradients which are then fed back to the master node. Even though the gradients received by the master may be out of sync, that is, it may not be computed using the current parameter vector, [3] show that their algorithm suffers small regret.\nUnlike the above case where the processors have to communicate with a master node, we will work in a fully decentralized setting. In our case the individual processors act as autonomous agents. They maintain their individual parameter vectors and periodically communicate with each other and pass information. We call this the cooperative autonomous online learning algorithm. Using tools from convex analysis and a theorem from Markov chain theory, we conduct a theoretical study of\n055\n056\n057\n058\n059\n060\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n100\n101\n102\n103\n104\n105\n106\n107\nthis algorithm, and derive regret bounds. If we assume that each local learner processes data at the same rate, then in spite of not having complete information, the new regret bounds guarantee that the predictive performance of autonomous online learners will converge to that of the best parameter vector chosen in hindsight, as fast as a centralized online learner.\nOur paper is structured as follows: In section 2 we introduce notation and some key results from the analysis of finite-state Markov chains. We present our algorithm in section 3 and its analysis in section 4. The paper concludes with a outlook and discussion in section 5."}, {"heading": "2 Preliminaries", "text": "Notation: Lower case letters (e.g., w) denote (column) vectors while upper case letters (e.g., A) denote matrices. We will denote the (j, i)-th element of A by Aji and the i-th column of A by Ai. Subscripts with t, t + 1 etc are used for indexing the parameter vector with respect to time while superscripts are used for indexing with respect to a processor. For instance,wit denotes the parameter vector of the i-th processor at time t. We use ei to denote the i-th basis vector (the vector of all zeros except one on the ith position), and e to denote the vector of all ones. Unless specified otherwise, \u2016\u00b7\u2016 refers to the Euclidean norm \u2016x\u2016 := (\u2211\ni x 2 i\n)1/2 , and \u3008\u00b7, \u00b7\u3009 denotes the Euclidean dot product\n\u3008x, x\u2032\u3009 = \u2211\ni xix \u2032 i.\nSubgradients and Strongly Convex Functions: The subgradient (set) \u2202xf(\u00b7) of a convex function f(x) at x0 is defined as [4]\ng \u2208 \u2202f(x0) \u21d0\u21d2 \u2200y, f(y)\u2212 f(x0) \u2265 \u3008y \u2212 x0, g\u3009 . (1) A convex function f(\u00b7) defined on domain \u2126 is said to be strongly convex with modulus \u03bb > 0 if and only if [4]\n\u2200x, y \u2208 \u2126, f(y)\u2212 f(x)\u2212 \u3008y \u2212 x, \u2202xf(x)\u3009 \u2265 \u03bb \u2016y \u2212 x\u20162 . (2)\nProjection and its Properties: The Euclidean projection operator onto a set \u2126 \u2286 Rn is defined as P\u2126(w\n\u2032) = argmin w\u2208\u2126 \u2016w \u2212 w\u2032\u2016 . (3)\nThe projection operator satisfies the following property (see e.g. equation (59) in [5]):\n\u3008P\u2126 (w\u0302)\u2212 w\u0302, w\u0302 \u2212 w\u3009 \u2264 \u2212\u2016P\u2126 (w\u0302)\u2212 w\u0302\u20162 \u2264 0, \u2200w \u2208 \u2126. (4)\nA Result from Analysis of Markov Chains: We shall see that our autonomous learners exchange information with their neighbors. The communication pattern is defined by a weighted directed graph whose adjacency matrix, A, is doubly stochastic. Recall that a matrix is said to be doubly stochastic if and only if all elements of A are non-negative and both rows and columns sum to one.\nA doubly stochastic matrix also occurs in the analysis of Markov chains, where the entryAji defines the probability of transitioning from the ith state to the jth state. A is said to be irreducible if and only if the weighted directed graph whose adjacency matrix is A\u22a4 is strongly connected. A is called aperiodic if and only if\n\u2200i, gcd{k \u2265 1 : Akii 6= 0} = 1, where gcd denotes the greatest common divisor. A distribution vector \u03c0 \u2208 Rm is called a stationary distribution if and only if A\u03c0 = \u03c0. Whenever A is doubly stochastic its stationary distribution \u03c0 = 1me. We will use the following standard result from the analysis of finite-state Markov chains that characterizes the limiting behaviors of Ak as k \u2192 \u221e in our analysis (see e.g. Chapter 12 of [6]):\nTheorem 1 Let A be the transition matrix of a Markov chain. If A is irreducible, aperiodic, and \u03c0 is the stationary distribution, then there exists a positive real number \u03b2 < 1 such that\n\u2200i, \u2211\nj\n|Akji \u2212 \u03c0j | \u2264 2\u03b2k. (5)\nThe connectivity of G affects \u03b2. The intuition is with stronger connectivity, transiting from one state to another state will bypass fewer intermediate states, so mixing to the stationary distribution is easier. Therefore, the higher the connectivity, the lower the \u03b2.\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161"}, {"heading": "3 Cooperative Autonomous Online Learning", "text": "Sequential Online Learning: Online learning usually proceeds in trials. At each trial a data point xt is given to the learner which produces a parameter vectorwt from a convex set \u2126 \u2286 Rn. One then computes some function of the inner product \u3008wt, xt\u3009 in order to produce a label y\u0302t. The true label yt is revealed to the learner, which then incurs a convex (but not necessarily smooth) loss l(wt, xt, yt) and adjusts its parameter vector. If we succinctly denote ft(w) := l(w, xt, yt), then online learning is equivalent to solving the following optimization problem in a stochastic fashion:\nmin w\u2208\u2126\nJ(w), where J(w) = T\u2211\nt=1\nft(w) and \u2126 \u2286 Rn, (6)\nand the goal is to minimize the regret\nRS = T\u2211\nt=1\nft(wt)\u2212min w\u2208\u2126 J(w). (7)\nCooperative Autonomous Online Learning: Sequential online learning assumes a centralized learner with all the data available to it sequentially. However, for cooperative autonomous online learning, we assume we have m local online learners using only data contained at local sites. At each trial m data points xit with i \u2208 {1, 2, . . . ,m} are given and the i-th learner updates model parameters based on the i-th point. The learner produces a parameter vector wit which is used to compute the prediction \u2329 wit, x i t \u232a and the corresponding loss f it (w) = l(w, x i t, y i t). The learners then exchange information with a selected set of their neighbors before updating wit to w i t+1. In order to reduce network traffic, the communication pattern amongst processors is assumed to form a strongly (but not necessarily fully) connected graph. In particular, we will assume a directed weighted graph whose adjacency matrix A is doubly stochastic. One can interpret the entry Aji as the importance that learner i places on the parameter vector communicated by learner j. Of course, if Aji = 0 then learners i and j do not communicate.\nThe optimization problem in this case can be written as\nmin w\u2208\u2126\nJ(w), where J(w) = T\u2211\nt=1\nm\u2211\ni=1\nf it (w) and \u2126 \u2286 Rn, (8)\nand regret is measured with respect to the vector wt which is computed by averaging over the wit:\nRCA = T\u2211\nt=1\nm\u2211\ni=1\nf it (wt)\u2212min w\u2208\u2126\nJ(w), where wt = 1\nm\nm\u2211\ni=1\nwit. (9)\nIf we denote ft = \u2211m i=1 f i t\n1, our definition of the regret has the same form of the regret in sequential online learning. However, our definition of RCA is distinguished from the sequential regret RS by the following two points\n\u2022 Given N data points, there are T = N iterations or trial in sequential online learning. In our case, this number reduces down to T = Nm . \u2022 The regret of cooperative autonomous online learning is defined through the average parameters wt which can be regarded as summaries of the learners\u2019 local parameters wit. We shall see in Algorithm 1 that wt need not even be calculated explicitly.\nWe will show the convergence of wt by bounding the regret RCA. In particular, we are interested in generalizing the celebrated \u221a T and log T bounds [7, 8] of sequential online learning to cooperative autonomous online learning.\nWe present a general online learning algorithm for solving (8) here. Specifically, a local learner propagates the parameter to other learners. After receiving the parameters from other learners, each learner updates its local parameter through a linear combination of the received and its own old parameter. Then the local learner updates the local model parameter based on the data collected and the local subgradient. Via this cooperation, the learners learn a model from distributed data sequentially. The algorithm is summarized in Algorithm 1.\n1We abuse the notation ft hereinafter.\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\nAlgorithm 1 Cooperative Autonomous Online Learning\n1: Input: The number of learners m; initial points w11 , . . . w m 1 ; double stochastic matrix A =\n(Aji) \u2208 Rm\u00d7m; and maximum iterations T . 2: for t = 1, . . . , T do 3: for each learner i = 1, . . . ,m do 4: git \u2190 \u2202wf it (wit). 5: Communicate wit with neighbors (as defined by A) and obtain their parameters. 6: w\u0302it+1 \u2190 \u2211 j Ajiw j t \u2212 \u03b7tgit (Local subgradient descent) 7: wit+1 \u2190 P\u2126 ( w\u0302it+1 ) (Projection) 8: end for 9: end for\n10: Return: wT+1 \u2190 1m \u2211 i w i T+1"}, {"heading": "4 Regret Bounds", "text": "For our analysis we make the following standard assumptions, which are assumed to hold for all the proofs and theorems presented below.\n\u2022 Each f it is strongly convex with modulus \u03bb \u2265 02. \u2022 Aji 6= 0 if and only if the ith learner communicates with the jth learner. We further assume A is irreducible, aperiodic, and there exists \u03b2 < 1 as defined in Theorem 1.\n\u2022 \u2126 is a closed convex subset of Rn with non-empty interior. The subgradient \u2202wf it (w) can be computed for every w \u2208 \u2126. \u2022 The diameter diam(\u2126) = supx,x\u2032\u2208\u2126 \u2016x\u2212 x\u2032\u2016 of \u2126 is bounded by F <\u221e. \u2022 The set of optimal solutions of (8) denoted by \u2126\u2217 is non-empty. \u2022 The norm of the subgradients of f it is bounded by L, and wi1 are identically initialized.\nWe start from a key result concerning the decomposition of regret is Lemma 2 given below (also see Lemma 5 of [9]). It extends a result that can be found in various guises in many places most notably Lemma 2.1 and 2.2 in [10], Theorem 4.1 and Eq. (4.21) and (4.15) in [11], in the proof of Theorem 1 of [7], as well as Lemma 3 of [12].\nLemma 2 Let wit denote the sequences generated by Algorithm 1. Denote wt = 1\nm \u2211m i=1 w i t, and\ng\u0304it = \u2202wf i t (wt). For any w \u2208 \u2126 we have\n\u2016wt+1 \u2212 w\u20162 \u2264 (1\u2212 2\u03b7t\u03bb) \u2016wt \u2212 w\u20162 + 4\u03b72t m2\n( m\u2211\ni=1\n\u2225 \u2225git \u2225 \u2225\n)2\n\u2212 2\u03b7t m (ft(wt)\u2212 ft(w))\n+ 2\u03b7t m\nm\u2211\ni=1\n( \u2225 \u2225git \u2225 \u2225+ \u2225 \u2225g\u0304it \u2225 \u2225) \u2225 \u2225wt \u2212 wit \u2225 \u2225+ 2\u03b7t m\nm\u2211\ni=1\n\u2225 \u2225git \u2225 \u2225 \u2225 \u2225wt \u2212 w\u0302it+1 \u2225 \u2225 (10)"}, {"heading": "Proof", "text": "Define\nrit := w i t \u2212 w\u0302it = P\u2126 ( w\u0302it ) \u2212 w\u0302it. (11)\nRecall that \u2126 is assumed to be convex, A is a doubly stochastic matrix, and wjt \u2208 \u2126 for all j. Therefore, Aji \u2265 0, \u2211 j Aji = 1, and \u2211 j Ajiw j t \u2208 \u2126 for all i. By this observation, the definition of the projection operator (3), and the definition of w\u0302it+1 in Line 6 of Algorithm 1 we have the following estimate for the norm of rit+1\n\u2225 \u2225rit+1 \u2225 \u2225 = \u2225 \u2225P\u2126 ( w\u0302it+1 ) \u2212 w\u0302it+1 \u2225 \u2225 \u2264 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nj\nAjiw j t \u2212 w\u0302it+1 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 = \u03b7t \u2225 \u2225git \u2225 \u2225 (12)\n2Note that we allow for \u03bb = 0, in which case f it is just convex, but not strongly convex.\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\nThen, we define the following matrices to simplify the notations.\nWt = [w 1 t , . . . , w m t ], W\u0302t = [w\u0302 1 t , . . . , w\u0302 m t ]\nGt = [g 1 t , . . . , g m t ], Rt = [r 1 t , . . . , r m t ]\nSince A is doubly stochastic Ae = 1. Therefore, by using (11) and the update in step 6 of Algorithm 1, we have the relation\nwt+1 = 1\nm Wt+1e =\n1 m (WtA\u2212 \u03b7tGt +Rt+1)e (13)\n= 1\nm Wte\u2212 \u03b7t m Gte+ 1 m Rt+1e = wt \u2212 \u03b7t m\nm\u2211\ni=1\ngit + 1\nm\n\u2211\ni=1\nrit+1.\nUsing the above relation we unroll \u2016wt+1 \u2212 w\u20162 by\n\u2016wt+1 \u2212 w\u20162 = \u2016wt \u2212 w\u20162 + 1\nm2 \u2225 \u2225 \u2225 \u2225 \u2225 m\u2211\ni=1\n( rit+1 + \u03b7tg i t ) \u2225 \u2225 \u2225 \u2225 \u2225 2\n\u2212 2\u03b7t m\nm\u2211\ni=1\n\u2329 git, wt \u2212 w \u232a + 2\nm\nm\u2211\ni=1\n\u2329 rit+1, wt \u2212 w \u232a . (14)\nIn view of (12)\n1\nm2 \u2225 \u2225 \u2225 \u2225 \u2225 m\u2211\ni=1\n( rit+1 + \u03b7tg i t ) \u2225 \u2225 \u2225 \u2225 \u2225 2 \u2264 1 m2 ( m\u2211\ni=1\n\u2225 \u2225rit+1 \u2225 \u2225+ \u03b7t \u2225 \u2225git \u2225 \u2225\n)2\n= 4\u03b72t m2\n( m\u2211\ni=1\n\u2225 \u2225git \u2225 \u2225\n)2\n. (15)\nNext we turn our attention to the \u2212\u2211i \u2329 git, wt \u2212 w \u232a term which we bound using (1) and (2) as follows: \u2212 \u2329 git, wt \u2212 w \u232a = \u2212 \u2329 git, wt \u2212 wit \u232a \u2212 \u2329 git, w i t \u2212 w \u232a\n\u2264 \u2225 \u2225git \u2225 \u2225 \u2225 \u2225wt \u2212 wit \u2225 \u2225+ f it (w) \u2212 f it (wit)\u2212 \u03bb \u2225 \u2225wit \u2212 w \u2225 \u2225 = \u2225 \u2225git \u2225 \u2225 \u2225 \u2225wt \u2212 wit \u2225 \u2225+ f it (wt)\u2212 f it (wit)\u2212 \u03bb \u2225 \u2225wit \u2212 w \u2225 \u2225+ f it (w) \u2212 f it (wt) \u2264 \u2225 \u2225git \u2225 \u2225 \u2225 \u2225wt \u2212 wit \u2225 \u2225+ \u2329 g\u0304it, wt \u2212 wit \u232a \u2212 \u03bb \u2225 \u2225wit \u2212 wt \u2225 \u2225\u2212 \u03bb \u2225 \u2225wit \u2212 w \u2225 \u2225+ f it (w) \u2212 f it (wt) \u2264 (\u2225 \u2225git \u2225 \u2225+ \u2225 \u2225g\u0304it \u2225 \u2225 ) \u2225 \u2225wt \u2212 wit \u2225 \u2225\u2212 \u03bb \u2016wt \u2212 w\u2016+ f it (w)\u2212 f it (wt).\nThe last inequality is by using \u2329 g\u0304it, wt \u2212 wit \u232a \u2264 \u2225 \u2225g\u0304it \u2225 \u2225 \u2225 \u2225wt \u2212 wit \u2225 \u2225 and \u2225 \u2225wit \u2212 wt \u2225 \u2225 + \u2225 \u2225wit \u2212 w \u2225 \u2225 \u2265 \u2016wt \u2212 w\u2016. Summing up over i = 1, . . . ,m, obtains\n\u2212 m\u2211\ni=1\n\u2329 git, wt \u2212 w \u232a \u2264\nm\u2211\ni=1\n(\u2225 \u2225git \u2225 \u2225+ \u2225 \u2225g\u0304it \u2225 \u2225 ) \u2225 \u2225wt \u2212 wit \u2225 \u2225\u2212 \u03bbm \u2016wt \u2212 w\u2016 \u2212 (ft(wt)\u2212 ft(w)) (16)\nIn order to estimate \u2329 rit+1, wt \u2212 w \u232a , we use (11), (4), and (12) to write\n\u2329 rit+1, wt \u2212 w \u232a = \u2329 rit+1, wt \u2212 w\u0302it+1 \u232a + \u2329 P\u2126 ( w\u0302it+1 ) \u2212 w\u0302it+1, w\u0302it+1 \u2212 w \u232a\n\u2264 \u2329 rit+1, wt \u2212 w\u0302it+1 \u232a \u2264 \u03b7t \u2225 \u2225git \u2225 \u2225 \u2225 \u2225wt \u2212 w\u0302it+1 \u2225 \u2225 . (17)\nCombining (15), (16) and (17) with (14) completes the proof.\nNext we upper bound the terms \u2225 \u2225wt \u2212 wit \u2225 \u2225 and \u2225 \u2225wt \u2212 w\u0302it+1 \u2225 \u2225 in (10). The convergence rate in Theorem 1 plays a central role in this lemma.\nLemma 3 If the assumptions in section 3 hold, and let \u03b2 be as in Theorem 1, then\n\u2225 \u2225wt \u2212 wit \u2225 \u2225 \u2264 4L\nt\u22121\u2211\nk=1\n\u03b7t\u2212k\u03b2 k\u22121 (18)\n\u2225 \u2225wt \u2212 w\u0302it+1 \u2225 \u2225 \u2264 4L\nt\u22121\u2211\nk=0\n\u03b7t\u2212k\u03b2 k. (19)\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323"}, {"heading": "Proof", "text": "Using the notations defined in the proof of Lemma 2, we unroll the relation Wt =Wt\u22121A\u2212 \u03b7tGt\u22121 +Rt\nwhich is defined through Algorithm 1 yields\nWt =W1A t\u22121 \u2212\nt\u22121\u2211\nk=1\n\u03b7t\u2212kGt\u2212kA k\u22121 +\nt\u22121\u2211\nk=1\nRt\u2212k+1A k\u22121. (20)\nUsing Ake = 1 for all k, (5), (12), and the above relation we can write\n\u2225 \u2225wt \u2212 wit \u2225 \u2225 = \u2225 \u2225 \u2225 \u2225 Wt ( 1\nm e\u2212 ei\n)\u2225 \u2225 \u2225 \u2225 = \u2225 \u2225 \u2225 \u2225 \u2225 ( W1A t\u22121 \u2212 t\u22121\u2211\nk=1\n\u03b7t\u2212kGt\u2212kA k\u22121 +\nt\u22122\u2211\nk=0\nRt\u2212kA k\n)( 1\nm e\u2212 ei ) \u2225 \u2225 \u2225 \u2225 \u2225\n\u2264 \u2225 \u2225w1 \u2212 wi1 \u2225 \u2225+\nt\u22121\u2211\nk=1\n\u03b7t\u2212k \u2225 \u2225 \u2225 \u2225 Gt\u2212k ( 1\nm e \u2212Ak\u22121i\n)\u2225 \u2225 \u2225 \u2225 + t\u22121\u2211\nk=1\n\u2225 \u2225 \u2225 \u2225 Rt\u2212k+1 ( 1\nm e\u2212Ak\u22121i\n)\u2225 \u2225 \u2225 \u2225\n\u2264 4L t\u22121\u2211\nk=1\n\u03b7t\u2212k\u03b2 k\u22121\nWe omit the proof for (19) which follows along similar lines.\nUsing Lemma 2 and 3, we are now ready to bound the regret for our algorithm.\nLemma 4 Let w\u2217 \u2208 \u2126\u2217 denote the best parameter chosen in hindsight. Then the regret of Algorithm 1 can be bounded via\nT\u2211\nt=1\nft(wt)\u2212 ft(w\u2217) \u2264 mF ( 1\n2\u03b7T \u2212 T\u03bb\n)\n+mCL2 T\u2211\nt=1\n\u03b7t, (21)\nwhere C is an absolute constant defined as\nC = (4 + 12\n1\u2212 \u03b2 ). (22)\nProof The proof is straight-forward, we put it in the supplementary material.\nNow with Lemma 4, we can obtain the following regret bounds.\nTheorem 5 If \u03bb > 0 and we set \u03b7t = 12\u03bbt then T\u2211\nt=1\nft(wt)\u2212 ft(w\u2217) \u2264 CL2m\n2\u03bb (1 + log(T )), (23)\nOn the other hand, when \u03bb = 0, if we set \u03b7t = 1 2 \u221a t then\nT\u2211\nt=1\nft(wt)\u2212 ft(w\u2217) \u2264 m ( F + CL2 )\u221a T . (24)\nProof First consider \u03bb > 0 with \u03b7t = 12\u03bbt . In this case 1 2\u03b7T = T\u03bb, and consequently (21) in Lemma 4 specializes to T\u2211\nt=1\nft(wt)\u2212 ft(w\u2217) \u2264 CL2m\n2\u03bb\nT\u2211\nt=1\n1 t \u2264 CL\n2m\n2\u03bb (1 + log(T )).\nWhen \u03bb = 0, and we set \u03b7t = 1 2 \u221a t and to rewrite (21) as\nT\u2211\nt=1\nft(wt)\u2212 ft(w\u2217) \u2264 mF \u221a T + CL2m T\u2211\nt=1\n1\n2 \u221a t \u2264 mF\n\u221a T + CL2m \u221a T .\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377"}, {"heading": "4.1 Interpreting the Bounds", "text": "Whenm = 1, then algorithm 1 reduces to the familiar sequential online learning, and unsurprisingly (24) and (23) recover the classical square root regret of [7] and logarithmic regret of [8]. Whenm > 1 then one has to exercise care when interpreting the bounds. Recall that every time instance t them processors simultaneously processm data points. Therefore in T steps our learners processmT data points. If we letN = mT , then our bounds can be rewritten asO( \u221a mN) andO(m+m log(N/m)) respectively. Contrast these bounds with the ones obtained by [3] which are of the form O( \u221a \u03c4N ) and O(\u03c4 + \u03c4 log(N/m)), where \u03c4 is the delay in the subgradient calculation.\nAnother interesting exercise is to compare our bounds with that of a sequential online learning algorithm which processes N = mT data points. In this case, the regret of the sequential algorithm is O( \u221a N) (resp.O(log(N))), while our algorithm suffers a O( \u221a mN) (resp.O(m+m log(N/m)) regret. It must be borne in mind, however, that our algorithm is handicapped by two factors. First, there is only limited information sharing between different learners and second, by our definition of regret, our algorithm is forced to predict on m data points in one shot with a single parameter vector wt. This is in contrast with the sequential online learner which has access to the full data set and can use different parameter vectors for each of the m data points.\nOn the other end of the spectrum let us consider the most optimistic scenario for our algorithm. If instead of working on m separate data points at every time instance, all the learners receive the same data at every iteration, then the updates in algorithm 1 recover the familiar sequential online learning updates and the corresponding regret bounds. This is in contrast to the algorithm of [3] where the optimistic scenario is achieved for gradients which are completely de-correlated, which in turn means that the data points are de-correlated. These scenarios represent two ends of the spectrum and it is hard to argue which one is more favorable than the other. Moreover, Langford et al.\u2019s optimistic bound O(\u03c42 + logT ) may not suitable for large delay/number of learners, because the O(\u03c42) dominates. In such a scenario, the optimistic bound is no better than theO(\u03c4 logT ) since logT is a slowly growing function."}, {"heading": "4.2 Generalization to Bregman Divergences", "text": "Our proof techniques can be generalized to Bregman divergences. We assume access to a function \u03c8 : \u2126 \u2192 R which is continuously differentiable and strongly convex with modulus of strong convexity \u03c3 > 0, and use it to define a Bregman divergence [13, 14]:\n\u2206\u03c8(w,w \u2032) = \u03c8(w)\u2212 \u03c8(w\u2032)\u2212 \u3008w \u2212 w\u2032,\u2207\u03c8(w\u2032)\u3009 . (25)\nThe diameter of \u2126 as measured by \u2206\u03c8 is given by\ndiam\u03c8(\u2126) = max w,w\u2032\u2208\u2126\n\u2206\u03c8(w,w \u2032). (26)\nIn addition to the assumptions made for the analysis of Algorithm 1, we further need to assume that the gradient \u2207\u03c8, and its inverse (\u2207\u03c8)\u22121 = \u2207\u03c8\u2217 can be computed. Algorithm 2 summarizes the generalization of Algorithm 1. Note that the projection step can be omitted now because the domain of \u03c8 is restricted to \u2126. The bounds we prove in Lemma 4 and Theorem 5 can be recovered with minor modifications. We omit these proofs due to lack of space."}, {"heading": "5 Outlook and Discussion", "text": "We presented a decentralized online learning algorithm3 and proved regret bounds which are similar to the traditional sequential online learning bounds. Stochastic optimization with autonomous agents is also gaining research attention in optimization [9]. Even though our analysis shares some similarities with these methods, the focus is very different. While we work with projections, Bregman divergences, and focus on regret bounds in the presence and absence of strong convexity they mainly focus on the rates of convergence of stochastic (sub)gradient descent with a fixed step size. These differences are somewhat similar to the key differences between the analysis of stochastic\n3Using very different techniques, Langford, Smola, and Zinkevich independently study an autonomous setting somewhat similar to ours in a paper submitted to NIPS 2010 (personal communication).\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\nAlgorithm 2 Cooperative Autonomous Online Learning with Bregman Divergence\n1: Input: The number of learners m; initial points w11 , . . . w m 1 ; double stochastic matrix A =\n(Aji) \u2208 Rm\u00d7m; and maximum iterations T . 2: for t = 1, . . . , T do 3: for each learner i = 1, . . . ,m do 4: git \u2190 \u2202wf it (wit). 5: Communicate wit with neighbors and obtain their parameters. 6: wit+1 \u2190 \u2207\u03c8\u2217( \u2211 j Aji\u2207\u03c8(w j t )\u2212 \u03b7tgit) (Local gradient descent) 7: end for 8: end for 9: Return: wT+1 \u2190 \u2207\u03c8\u2217( 1m \u2211 i\u2207\u03c8(wiT+1))\n(sub)gradient descent methods (see e.g. [15]) and regret bounds in online learning (e.g. [8]). There is also literature in the area of sensor networks which deals with learning in decentralized settings with limited communication. However, the tasks here are rather specific such as probabilistic inference [16] or evolving consensus [17]. A common form of f it (w) is l(y i t, \u2329 w, xit \u232a ), where l(yit, z) is the loss function. So the subgradient w.r.t. to wit is g i t = \u2202zl(y i t, \u2329 wit, x i t \u232a ) xit, which is proportional to x i t. Thus algorithms that transmit subgradients (e.g.the first variant of Langford et al.\u2019s algorithm [3]) may disclose information about data points. In the privacy sensitive applications mentioned in section 1, such as mining patient information across hospitals, we would like to keep the data at local sites and collaboratively complete data mining tasks. Our decentralized algorithm transmits only local parameters between the online learners, which makes it safer against network attacks.\nFor simplicity, we only analyzed the case where the communication pattern matrix A is fixed, and does not evolve over time. Our proofs can be extended to the setting where the communication graph evolves over time, or is selected from a small list of possible candidates at every iteration. This might be of importance in the case of privacy preserving data mining, where a fixed communication graph may be vulnerable to attack by network sniffing. Also, for simplicity, we assumed that nodes communicate with each other at every iteration. This can be relaxed to the case where the nodes only communicate periodically, that is, after having performed several local parameter updates.\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485"}, {"heading": "Appendix", "text": ""}, {"heading": "Proof of Lemma 4", "text": "Proof Set w = w\u2217, divide both sides of (10) by 2\u03b7tm and rearrange to obtain\nft(wt)\u2212 ft(w\u2217) \u2264 m\n2\u03b7t [(1\u2212 2\u03b7t\u03bb) \u2016wt \u2212 w\u2217\u2016 \u2212 \u2016wt+1 \u2212 w\u2217\u2016] + 2 \u03b7t m\n( m\u2211\ni=1\n\u2225 \u2225git \u2225 \u2225\n)2\n+ 2L\nm\u2211\ni=1\n\u2225 \u2225wt \u2212 wit \u2225 \u2225+ L\nm\u2211\ni=1\n\u2225 \u2225wt \u2212 w\u0302it+1 \u2225 \u2225\nPlug in the estimate of the subgradients and the bounds (18) and (19).\nft(wt)\u2212 ft(w\u2217) \u2264 m\n2\u03b7t [(1\u2212 2\u03b7t\u03bb) \u2016wt \u2212 w\u2217\u2016 \u2212 \u2016wt+1 \u2212 w\u2217\u2016]\n+ 2mL2\u03b7t + 8L 2m\nt\u22121\u2211\nk=1\n\u03b7t\u2212k\u03b2 k\u22121 + 4L2m\nt\u22121\u2211\nk=0\n\u03b7t\u2212k\u03b2 k\n\u2264 m 2\u03b7t [(1\u2212 2\u03b7t\u03bb) \u2016wt \u2212 w\u2217\u2016 \u2212 \u2016wt+1 \u2212 w\u2217\u2016]\n+ 4mL2\u03b7t + 12L 2m\nt\u22121\u2211\nk=1\n\u03b7t\u2212k\u03b2 k\u22121\nSumming over t = 1, . . . , T\nT\u2211\nt=1\nft(w \u2212 t)\u2212 ft(w\u2217) \u2264 m T\u2211\nt=1\n1\n2\u03b7t [(1 \u2212 2\u03b7t\u03bb) \u2016wt \u2212 w\u2217\u2016 \u2212 \u2016wt+1 \u2212 w\u2217\u2016] \ufe38 \ufe37\ufe37 \ufe38\n:=C1\n+ 4mL2 T\u2211\nt=1\n\u03b7t + 12L 2m\nT\u2211\nt=1\nt\u22121\u2211\nk=1\n\u03b7t\u2212k\u03b2 k\u22121\n\ufe38 \ufe37\ufe37 \ufe38\n:=C2\nSince the diameter of \u2126 is bounded by F\nC1 =\n( 1 2\u03b71 \u2212 \u03bb ) \u2016w1 \u2212 w\u2217\u2016 \u2212 1 2\u03b7T \u2016wT+1 \u2212 w\u2217\u2016+ T\u2211\nt=2\n\u2016wt \u2212 w\u2217\u2016 ( 1\n2\u03b7t \u2212 1 2\u03b7t\u22121 \u2212 \u03bb )\n\u2264 ( 1 2\u03b71 \u2212 \u03bb ) F + T\u2211\nt=2\nF\n( 1\n2\u03b7t \u2212 1 2\u03b7t\u22121 \u2212 \u03bb ) = F ( 1 2\u03b7T \u2212 T\u03bb )\nLet I(t > k) be the indicator function which is 1 when t > k and 0 otherwise. Then\nC2 =\nT\u2211\nt=1\nT\u2211\nk=1\n\u03b7t\u2212k\u03b2 k\u22121I(t > k) =\nT\u2211\nk=1\n\u03b2k\u22121 T\u2211\nt=k+1\n\u03b7t\u2212k\n\u2264 T\u2211\nk=1\n\u03b2k\u22121 T\u2211\nt=1\n\u03b7t \u2264 1\n1\u2212 \u03b2\nT\u2211\nt=1\n\u03b7t\nPlug in the estimate for C1 and C2, to obtain (21)."}], "references": [{"title": "Bundle methods for regularized risk minimization", "author": ["Choon Hui Teo", "S.V.N. Vishwanthan", "Alex J. Smola", "Quoc V. Le"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Validity of the single processor approach to achieving large-scale computing capabilities", "author": ["Gene Amdahl"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1967}, {"title": "Slow learners are fast", "author": ["J. Langford", "A.J. Smola", "M. Zinkevich"], "venue": "arXiv:0911.0491", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Convex Analysis and Minimization Algorithms", "author": ["J.B. Hiriart-Urruty", "C. Lemar\u00e9chal"], "venue": "I and II, volume 305 and 306. Springer-Verlag", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1993}, {"title": "Tracking the best linear predictor", "author": ["M. Herbster", "M.K. Warmuth"], "venue": "Journal of Machine Learning Research, 1:281\u2013309", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Monte Carlo strategies in scientific computing", "author": ["Jun S. Liu"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Online convex programming and generalised infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "Proc. Intl. Conf. Machine Learning, pages 928\u2013936", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Distributed subgradient methods for multi-agent optimization", "author": ["Angelia Nedic", "Asu Ozdaglar"], "venue": "IEEE Trans. on Automatic Control,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Subgradient Methods for Convex Minimization", "author": ["Angelia Nedic"], "venue": "PhD thesis,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Mirror descent and nonlinear projected subgradient methods for convex optimization", "author": ["Amir Beck", "Marc Teboulle"], "venue": "Operations Research Letters,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Logarithmic regret algorithms for strongly convex repeated games", "author": ["S. Shalev-Shwartz", "Y. Singer"], "venue": "Technical report, School of Computer Science, Hebrew University", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Relative loss bounds for on-line density estimation with the exponential family of distributions", "author": ["K. Azoury", "M.K. Warmuth"], "venue": "Machine Learning, 43(3):211\u2013246", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Parallel Optimization", "author": ["Y. Censor", "S.A. Zenios"], "venue": "Oxford, New York", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Convergence rate of incremental subgradient algorithms", "author": ["A. Nedich", "D. P Bertsekas"], "venue": "S. Uryasev and P. M. Pardalos, editors, Stochastic Optimization: Algorithms and Applications, pages 263\u2013304. Kluwer Academic Publishers", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Robust probabilistic inference in distributed systems", "author": ["Mark Paskin", "Carlos Guestrin"], "venue": "In Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Constrained consensus and optimization in multiagent networks", "author": ["A. Nedic", "A. Ozdaglar", "P.A. Parrilo"], "venue": "Automatic Control, IEEE Transactions on, 55", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "For instance, [1] cast many machine learning problems as a regularized risk minimization problem.", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "However, as [1] observe, when the number of processors increases Amdahls law [2] kicks in and the cost of communicating and synchronizing becomes prohibitively expensive.", "startOffset": 12, "endOffset": 15}, {"referenceID": 1, "context": "However, as [1] observe, when the number of processors increases Amdahls law [2] kicks in and the cost of communicating and synchronizing becomes prohibitively expensive.", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "Another notable effort is the work by [3], where they show that one can avoid the expensive synchronization step above.", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "Even though the gradients received by the master may be out of sync, that is, it may not be computed using the current parameter vector, [3] show that their algorithm suffers small regret.", "startOffset": 137, "endOffset": 140}, {"referenceID": 3, "context": "Subgradients and Strongly Convex Functions: The subgradient (set) \u2202xf(\u00b7) of a convex function f(x) at x0 is defined as [4] g \u2208 \u2202f(x0) \u21d0\u21d2 \u2200y, f(y)\u2212 f(x0) \u2265 \u3008y \u2212 x0, g\u3009 .", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "(1) A convex function f(\u00b7) defined on domain \u03a9 is said to be strongly convex with modulus \u03bb > 0 if and only if [4]", "startOffset": 111, "endOffset": 114}, {"referenceID": 4, "context": "equation (59) in [5]):", "startOffset": 17, "endOffset": 20}, {"referenceID": 5, "context": "Chapter 12 of [6]):", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "In particular, we are interested in generalizing the celebrated \u221a T and log T bounds [7, 8] of sequential online learning to cooperative autonomous online learning.", "startOffset": 85, "endOffset": 91}, {"referenceID": 7, "context": "In particular, we are interested in generalizing the celebrated \u221a T and log T bounds [7, 8] of sequential online learning to cooperative autonomous online learning.", "startOffset": 85, "endOffset": 91}, {"referenceID": 8, "context": "We start from a key result concerning the decomposition of regret is Lemma 2 given below (also see Lemma 5 of [9]).", "startOffset": 110, "endOffset": 113}, {"referenceID": 9, "context": "2 in [10], Theorem 4.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "15) in [11], in the proof of Theorem 1 of [7], as well as Lemma 3 of [12].", "startOffset": 7, "endOffset": 11}, {"referenceID": 6, "context": "15) in [11], in the proof of Theorem 1 of [7], as well as Lemma 3 of [12].", "startOffset": 42, "endOffset": 45}, {"referenceID": 11, "context": "15) in [11], in the proof of Theorem 1 of [7], as well as Lemma 3 of [12].", "startOffset": 69, "endOffset": 73}, {"referenceID": 6, "context": "Whenm = 1, then algorithm 1 reduces to the familiar sequential online learning, and unsurprisingly (24) and (23) recover the classical square root regret of [7] and logarithmic regret of [8].", "startOffset": 157, "endOffset": 160}, {"referenceID": 7, "context": "Whenm = 1, then algorithm 1 reduces to the familiar sequential online learning, and unsurprisingly (24) and (23) recover the classical square root regret of [7] and logarithmic regret of [8].", "startOffset": 187, "endOffset": 190}, {"referenceID": 2, "context": "Contrast these bounds with the ones obtained by [3] which are of the form O( \u221a \u03c4N ) and O(\u03c4 + \u03c4 log(N/m)), where \u03c4 is the delay in the subgradient calculation.", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "This is in contrast to the algorithm of [3] where the optimistic scenario is achieved for gradients which are completely de-correlated, which in turn means that the data points are de-correlated.", "startOffset": 40, "endOffset": 43}, {"referenceID": 12, "context": "We assume access to a function \u03c8 : \u03a9 \u2192 R which is continuously differentiable and strongly convex with modulus of strong convexity \u03c3 > 0, and use it to define a Bregman divergence [13, 14]:", "startOffset": 180, "endOffset": 188}, {"referenceID": 13, "context": "We assume access to a function \u03c8 : \u03a9 \u2192 R which is continuously differentiable and strongly convex with modulus of strong convexity \u03c3 > 0, and use it to define a Bregman divergence [13, 14]:", "startOffset": 180, "endOffset": 188}, {"referenceID": 8, "context": "Stochastic optimization with autonomous agents is also gaining research attention in optimization [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 14, "context": "[15]) and regret bounds in online learning (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "However, the tasks here are rather specific such as probabilistic inference [16] or evolving consensus [17].", "startOffset": 76, "endOffset": 80}, {"referenceID": 16, "context": "However, the tasks here are rather specific such as probabilistic inference [16] or evolving consensus [17].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "\u2019s algorithm [3]) may disclose information about data points.", "startOffset": 13, "endOffset": 16}], "year": 2017, "abstractText": "Online learning is becoming increasingly popular for training on large datasets. However, the sequential nature of online learning requires a centralized learner to store data and update parameters. In this paper, we consider a fully decentralized setting, cooperative autonomous online learning, with a distributed data source. The learners perform learning with local parameters while periodically communicating with a small subset of neighbors to exchange information. We define the regret in terms of an implicit aggregated parameter of the learners for such a setting and prove regret bounds similar to the classical sequential online learning.", "creator": "LaTeX with hyperref package"}}}