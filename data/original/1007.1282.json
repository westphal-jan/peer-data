{"id": "1007.1282", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jul-2010", "title": "A note on sample complexity of learning binary output neural networks under fixed input distributions", "abstract": "We show that the learning sample complexity of a sigmoidal neural network constructed by Sontag (1992) required to achieve a given misclassification error under a fixed purely atomic distribution can grow arbitrarily fast: for any prescribed rate of growth there is an input distribution having this rate as the sample complexity, and the bound is asymptotically tight. The rate can be superexponential, a non-recursive function, etc. We further observe that Sontag's ANN is not Glivenko-Cantelli under any input distribution having a non-atomic part.", "histories": [["v1", "Thu, 8 Jul 2010 03:58:25 GMT  (72kb)", "http://arxiv.org/abs/1007.1282v1", "6 pages, latex in IEEE conference proceedings format"]], "COMMENTS": "6 pages, latex in IEEE conference proceedings format", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vladimir pestov"], "accepted": false, "id": "1007.1282"}, "pdf": {"name": "1007.1282.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["vpest283@uottawa.ca"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 7.\n12 82\nv1 [\ncs .L\nG ]\n8 J\nul 2\n01 0\nKeywords-PAC learnability, fixed distribution learning, sample complexity, infinite VC dimension, witness of irregularity, Sontag\u2019s ANN, precompactness.\nI. INTRODUCTION\nWe begin with a quote of the first part of the open problem 12.6 from Vidyasagar\u2019s book [11] (this problem appears already in the original 1997 version).\n\u201cHow can one reconcile the fact that in distribution-free learning, every learnable concept class is also \u201cpolynomially\u201d learnable, whereas this might not be so in fixeddistribution learning?\nIn the case of distribution-free learning of concept classes (...) there are only two possibilities: 1. C has infinite VC-dimension, in which case C is not PAC learnable at all. 2. C has finite VC-dimension, in which case C is not only PAC learnable, but the sample complexity m0(\u03b5, \u03b4) is O(1/\u03b5 + log(1/\u03b4)). Let us call such a concept class \u201cpolynomially learnable\u201d.\nIn other words, there is no \u201cintermediate\u201d possibility of a concept class being learnable, but having a sample complexity that is superpolynomial in 1/\u03b5.\nIn the case of fixed-distribution learning, the situation is not so clear. (...) Is there a concept class for which every algorithm would require a superpolynomial number of samples? The only known way of consructing such a concept class would be to (...) attempt to construct a concept class whose \u03b5-covering number grows faster than any exponential in 1/\u03b5. It would be interesting to know whether such a concept class exists.\u201d\nIn fact, the existence of a concept class whose sample complexity grows exponentially in 1/\u03b5 under a given fixed input distribution was already shown in 1991 by Benedek and Itai [2] (Theorem 3.5). Their example consisted of all finite subsets of a domain. Later and independently, a rather more natural concept class with such properties (generated by a neural network) was constructed by Barbara Hammer in her Ph.D. thesis [5] (Example 4.4.3 on page 77), cf. also [6].\nHere we somewhat strengthen the above results and at the same time show that the phenomenon is quite common. Suppose that a concept class C satisfies a slightly stronger property than having an infinite VC dimension, namely: C shatters every finite subset of an infinite set. Fix a sequence \u03b5k of desired values of learning precision, converning to zero, and let f be an increasing real function on [0,+\u221e). Then one can find a probability measure \u00b5 on the domain \u2126 of C with the property that C is PAC learnable under \u00b5, but the sample complexity of learning to precision \u03b5k, k = 1, 2, 3, . . ., is growing as \u2126(f(\u03b5\u22121k )). The prescribed rate of growth can be ridiculouly high, for instance, a non-recursive function. The bound is essentially tight. For example, a wellknown sigmoidal feed-forward neural network of infinite VC dimension constructed by Sontag [8] has this property.\nThis naturally brings up a question of behaviour of Sontag\u2019s network N under non-atomic input distributions. It follows from Talagrand\u2019s theory of witness of irregularity [9], [10] that N is not Glivenko\u2013Cantelli with regard to any measure having a non-atomic part. We do not know if a similar property holds for PAC learnability, although it is easy to see non-learnability of N for some common measures (the uniform distribution on the interval, the gaussian measure). While discussing a relationship between Glivenko\u2013Cantelli property, PAC learnability, and precompactness, we give an answer to another (minor) question of Vidyasagar.\nNote that we find it instructive to present the above observations in the reverse order. In Conclusion, we suggest a few open problems and a conjecture supported by the results of this note which might shed light on Vidyasagar\u2019s problem."}, {"heading": "II. GLIVENKO\u2013CANTELLI CLASSES AND LEARNABILITY", "text": "A. PAC learnability and total boundedness\nBenedek and Itai [2] had proved that a concept class C is PAC learnable under a single probability distribution \u00b5 if and only if C is totally bounded in the L1(\u00b5)-distance. Here we remind their results.\nTheorem 2.1 (Theorem 4.8 in [2]; Theorem 6.3 in [11]): Suppose C is a concept class, \u03b5 > 0, and that B1, . . . , Bk is an \u03b5/2-cover for C . Then the minimal empirical risk algorithm is PAC to accuracy \u03b5. In particular, the sample complexity of PAC learning C to accuracy \u03b5 with confidence 1\u2212 \u03b4 is\nm \u2264 32 \u03b5 log k \u03b4 .\nRecall that a subset A of a metric space X is \u03b5-separated, or \u03b5-discrete, if, whenever a, b \u2208 A and a 6= b, one has d(a, b) \u2265 \u03b5 > 0. The largest cardinality of an \u03b5-discrete subset of X is the \u03b5-packing number of X . For example, the following lemma estimates from below the packing number of the Hamming cube.\nLemma 2.2 ([11], Lemma 7.2 on p. 279): Let 0 < \u03b5 \u2264 1/4. The Hamming cube {0, 1}n, equipped with the normalized Hamming distance\ndh(x, y) = 1\nn |{i : xi 6= yi}| ,\nadmits a family of elements which are pairwise at a distance of at least 2\u03b5 from each other of cardinality at least exp[2(0.5\u2212 2\u03b5)2n].\nThe following is a source of lower bounds on the sample complexity.\nTheorem 2.3 (Lemma 4.8 in [2]; Theorem 6.6 in [11]): Suppose C is a given concept class, and let \u03b5 > 0 be specified. Then any algorithm that is PAC to accuracy \u03b5 requires at least lgM(2\u03b5,C , L1(\u00b5)) samples, where M(2\u03b5,C , L1(\u00b5)) denotes the 2\u03b5-packing number of the concept class C with regard to the L1(\u00b5)-distance.\nFor the most comprehensive presentation of PAC learnability under a single distribution, see [11], Ch. 6.\nB. Glivenko\u2013Cantelli classes\nA function class F on a domain (a standard Borel space) \u2126 is Glivenko\u2013Cantelli with regard to a probability distribution \u00b5 ([3], Ch. 3), or else has the property of uniform convergence of empirical means (UCEM property) [11], if for each \u03b5 > 0\nsup \u00b5\u2208P\n\u00b5\u2297n\n{\nsup f\u2208F\n|E\u00b5(f)\u2212 E\u00b5n(f)| \u2265 \u03b5 }\n\u2192 0 as n \u2192 \u221e. (1)\nHere \u00b5\u2297n is the product measure on \u2126n, and \u00b5n stands for the empirical (uniform) measure on n points, sampled from the domain in an i.i.d. fashion. We assume F to assume values in an interval (i.e., to be uniformly bounded). The\nnotion applies to neural networks as well, if F denotes the family of output functions corresponding to all possible values of learning parameters.\nEvery Glivenko\u2013Cantelli class F is PAC learnable, which explains the important role of this notion. In fact, every consistent learning rule L will learn F . We find it instructive to give a different proof, replying in passing to a remark of Vidyasagar [11], p. 241. After proving that every Glivenko\u2013 Cantelli concept class C with regard to a fixed measure \u00b5 is precompact with regard to the L1(\u00b5)-distance, the author remarks that his proof is both indirect (Glivenko\u2013Cantelli \u21d2 PAC learnable \u21d2 precompact), and does not extend to function classes, so it is not known to the author whether the result holds if C is replaced with a function class F .\nThe answer is yes, as is (implicitely) stated in [10] (p. 379, the beginning of the proof of Proposition 2.5), but a deduction is also rather roundabout (proving first the absence of a witness of irregularity). In fact, the result is really very simple.\nObservation 2.4: Every (uniformly bounded) Glivenko\u2013 Cantelli function class F with regard to a fixed probabillty measure \u00b5 is precompact in the L1(\u00b5)-distance.\nProof: If F is not precompact, then for some \u03b50 > 0 it contains an infinite \u03b50-discrete subfamily F \u2032. For every finite sample \u03c3 \u2208 \u2126n there is a further infinite subfamily F \u2032\u2032 \u2286 F \u2032 of functions whose restrictions to \u03c3 are at a pairwise L1(\u00b5n)-distance < \u03b50/2 from each other (the pigeonhole principle coupled with the fact that the restriction of F to \u03c3 is L1(\u00b5n)-precompact). This means that \u00b5- and \u00b5n-expectations of some function of the form |f1 \u2212 f2|, fi \u2208 F , i = 1, 2, differ between themselves by at least \u03b50/2, and for at least one of i \u2208 {1, 2},\n|E\u00b5(fi)\u2212 E\u00b5n(fi)| \u2265 \u03b50/4 (an application of the triangle inequality in R). Since the latter is true for every sample, no matter the size, F is not Glivenko\u2013Cantelli.\nIn fact, the same proof works in a slightly more general case when F is uniformly bounded by a single function (not necessarily integrable).\nThis gives an alternative deduction of the implication Glivenko\u2013Cantelli \u21d2 PAC learnability. Admittedly, the result obtained is somewhat weaker, as this way we do not get consistent learnability.\nC. Talagrand\u2019s witness of irregularity\nTalagrand [9], [10] had characterized uniform Glivenko\u2013 Cantelli function classes with regard to a single distribution in terms of shattering. We will remind his main result for concept classes only. Let \u2126 be a measurable space, let C be a concept class on \u2126, and let \u00b5 be a probability measure on \u2126. A measurable subset A \u2286 \u2126 is a witness of irregularity of C , if \u00b5(A) > 0 and for every n the set of all n-tuples of elements of A shattered by C has full measure in An.\nIn other words, \u00b5-almost all n-tuples of elements of A are shattered by C .\nTheorem 2.5 (Talagrand [9], Th. 2): A concept class C is Glivenko\u2013Cantelli with regard to the probability measure \u00b5 if and only if C admits no witness of irregulaity.\nLet \u00b5 be a probability measure on \u2126. Recall that a set A is an atom if for every measurable B \u2286 A one has either \u00b5(B) = 0 or \u00b5(B) = \u00b5(A). The measure \u00b5 is non-atomic if it contains no atoms, and purely atomic if the measures of atoms add up to one. The restriction of \u00b5 to the union of atoms is the atomic part of \u00b5.\nSince a witness of irregularity can contain no atoms, the following is an immediate corollary of Talagrand\u2019s 1987 result.\nCorollary 2.6: If a measure \u00b5 is purely atomic, then every concept class C is uniform Glivenko\u2013Cantelli with regard to \u00b5, and in particular PAC learnable.\nThe corollary is easy to prove directly, without using subtle results of Talagrand, and the result was observed (independently) in 1991 and investigated in detail by Benedek and Itai ([2], Theorem 3.2). Notice that the result does not assert polynomial PAC learnability of C , and we will see shortly that the required sample complexity of C can grow arbitrarily fast.\nD. The neural network of Sontag\nFigure 1 recalls a well-known example of a sigmoidal neural network N constructed by Sontag [8], pp. 34\u201336. (Cf. also [11], page 389, where the top diagram in Figure 1 is borrowed from.) The activation sigmoid is of the form\n\u03c6(x) = 1\n\u03c0 tan\u22121 x+\ncosx\n\u03b1(1 + x2) +\n1 2 ,\nwhere \u03b1 \u2265 2\u03c0 is fixed, e.g. \u03b1 = 100. and the outputlayer perceptron has both input weights equal to one and a threshold of one. The input-output function of the network is given by\ny = \u03b7[\u03c1(x)],\nwhere\n\u03c1(x) = 2 coswx\n\u03b1(1 + w2x2) .\nThe input space of N is the space R of real numbers. Recall that a collection x1, x2, . . . , xn of real numbers is rationally independent if no non-trivial linear combination of 1, x1, x2, . . . , xn with rational coefficients vanishes.\nTheorem 2.7 ([8], pp. 42-43): The Sontag network N shatters every rationally independent n-tuple of real inputs x1, x2, . . . , xn.\nIn particular, the VC dimension of Sontag\u2019s network is infinite. Besides, it is easy to find an infinite rationally independent set, and so every finite subset of such a set is shattered by N . We will need this fact later.\nHere is another extreme property of Sontag\u2019s network.\nTheorem 2.8: The neural network of Sontag N is Glivenko\u2013Cantelli under a probability distribution \u00b5 on the inputs if and only if \u00b5 is purely atomic.\nProof: Sufficiency (\u21d0) follows from Corollary 2.6. Let us prove necessity (\u21d2). By splitting \u00b5 into a purely atomic part \u00b5a and a continuous part \u00b5c, in view of Theorems 2.5 of Talagrand and 2.7 of Sontag, it suffices to prove that for every non-atomic probability measure \u03bd on R the set of rationally independent n-tuples has a full \u03bd\u2297n measure in R\nn: the support of \u00b5c will then be a witness of irregularity. In its turn, this reduces to a proof that for a fixed collection (\u03bb1, . . . , \u03bbn+1) of rationals not all of which are zero, the affine hyperplane\nH\u03bb = {x \u2208 Rn : \u3008x, \u03bb\u3009 = \u03bbn+1}, where \u03bb = (\u03bb1, . . . , \u03bbn), has \u03bd\u2297n-measure null. This is a consequence of Eggleston\u2019s theorem [4]: If A is a measurable, Lebesgue-positive subset of the unit square, then there is a measurable positive set B and a perfect set C such\nthat B \u00d7 C is included in A. \u201cLebesgue measure on the unit square\u201d here is not a loss of generality, as every two non-atomic standard Borel probability measure spaces are isomorphic, and we obtain by induction that if A \u2286 Rn and \u03bd\u2297n(A) > 0, then A contains a product of n sets one of which is \u03bd\u2297n-measure positive and all the rest are perfect (contain no isolated points). Clearly, no (n\u2212 1)-hyperplane in Rn can have this property.\nExample 2.9: Sontag\u2019s ANN is not PAC learnable under the uniform distribution on an interval.\nIndeed, for the sequence of learning parameters wk = 2k the corresponding output binary functions are at a pairwise L1(\u03bb)-distance 1/2 from each other, where \u03bb is a uniform distribution on some interval.\nA similar argument works for the gaussian distribution on the inputs.\nHowever, we do not know if there exists a non-atomic measure under which Sontag\u2019s ANN is PAC learnable.\nE. Glivenko\u2013Cantelli versus learnability\nNot every PAC learnable function, or even concept, class is Glivenko\u2013Cantelli. Examples of such concept classes exist trivially, e.g. the concept class consisting of all finite and all cofinite subsets of the unit intervals is PAC learnable under every non-atomic distribution, yet clearly not uniform Glivenko\u2013Cantelli, cf. [2], p. 385, note (2), or [11], p. 230, Example 6.4. A more interesting example, though based on the same idea, is Example 6.6 in [11], p. 232. Here we present such an example of a countable concept class.\nExample 2.10: For n \u2208 N, say that intervals [i/n, (i + 1)/n], i = 0, 1, . . . , n \u2212 1, are of order n. Let Cn consist of all unions of less than \u221a n intervals of order n, and set C = \u222a\u221ei=1Cn. If now k \u2208 N is any and x1 < x2 < . . . < xk are points of the unit interval, choose n > k2 so that 1/n is smaller than any of the half-distances between neighbouring points (xi+1 \u2212 xi)/2, i = 1, 2, . . . , n. Clearly, elements of Cn shatter the sample {x1, x2, . . . , xk}, and so the entire interval is a witness of irregularity for the concept class C . By Talagrand\u2019s result, the class C is not Glivenko\u2013Cantelli. At the same time, for every n, Cn forms an n\u22121/2-net for C with regard to the L1(\u03bb)-distance, and so C is PAC learnable under the Lebesgue measure \u03bb (the uniform measure on the interval).\nObserve that, in fact, C fails the Glivenko\u2013Cantelli property with regard to every measure having a non-atomic part. As we have seen, there exist non-atomic measures under which C is PAC learnable. There are also measures under which C is not PAC learnable. for example the Haar measure \u03bd on the Cantor set.\nRecall the construction of the Cantor \u201cmiddle third\u201d set C (Figure 3). This is the set left of the closed unit interval [0, 1] after first deleting the middle third (1/3, 2/3), then deleting the middle thirds of the two remaining intervals, (1/9, 2/9) and (7/9, 8/9), and continuing to delete the middle thirds ad infimum. The elements of the Cantor set are exactly those real numbers between 0 and 1 admitting a ternary expansion not containing 1. Sometimes C is called Cantor dust. The complement to the Cantor set is a union of countably many open intervals, all the middle thirds left out. The set Cn left after the first n steps of removing the middle thirds is the union of 2n closed intervals of equal length 3\u2212n each. The Haar measure of every such interval is set to be equal to 2\u2212n, and this condition defines a non-atomic measure \u03bd supported on C in a unique way.\nIt is easy to see now that the closed intervals I1, I2, . . . , I2n at the level n are shattered with concept classes from CN if N is large enough (\u2265 22n), in the following sense: for every set of indices J \u2286 {1, 2, . . . , 2n} there is a C \u2208 CN which contains every interval Ij , j \u2208 J , and is disjoint from every interval Ik , where k /\u2208 J . Now\nsecond step\ntwo middle thirds removed at the\none can modify the proof of Lemma 2.2 exactly as it was done in [7], proof of Theorem 3, in order to conclude that C is not totally bounded in the L1(\u03bd)-distance."}, {"heading": "III. ALL RATES OF SAMPLE COMPLEXITY ARE POSSIBLE", "text": "Theorem 3.1: Let C be a concept class which shatters every finite subset of some infinite set. Let (\u03b5k), \u03b5k \u2193 0 be a sequence of positive reals converging to zero, and let f : R+ \u2192 R+ be a non-decreasing function growing at least linearly: f(x) = \u2126(x). Then there is a probability measure \u00b5 = \u00b5((\u03b5k), f) on the input domain \u2126 with the property that for every \u03b4 > 0 and k \u2208 N the class C is PAC learnable under the distribution \u00b5 to accuracy \u03b5k, and the rate of required sample complexity is at least\nn(\u03b5k, \u03b4) = \u2126\n(\nf\n(\n1\n\u03b5k\n))\n. (2)\nMoreover, the above estimate is essentially tight in the sense that the sample complexity\nn(\u03b5k, \u03b4) = O\n(\nf\n(\n1\n\u03b5k\n)\n+ log\n(\n1\n\u03b4\n))\n. (3)\nsuffices to learn C to accuracy 4\u03b5k with confidence 1\u2212 \u03b4. Proof: We can assume without loss in generality that \u03b51 = 1/5. For every k, set mk = 5(\u03b5k+1 \u2212 \u03b5k). Then mk form a sequence of non-negative reals which sums up to one. Denote, for simplicity, fk = f(\u03b5 \u22121\nk ). Further, choose pairwise disjoint finite sets Fk of cardinality |Fk| = fk \u2212 fk\u22121 (where f0 = 0) in a way that every union of finitely many of Fk\u2019s is shattered by C (this is possible due to the assumption on the class C ). Let \u00b5k denote a uniform measure supported on Fk of total mass mk. Now set \u00b5 = \u2211\u221e\ni=1 \u00b5k. Since \u2211\u221e\ni=1 mk = 1, \u00b5 is a probability Borel measure.\nLet k be arbitrary. Select any subset of C shattering \u222aki=1Fi and containing\nk \u220f\ni=1\n|Fi| = 2fk\nelements. This set forms a finite \u03b5k-net in C with regard to the L1(\u00b5)-distance. Since \u03b5k \u2193 0, we use Theorem 2.1 to conclude: the class C is PAC learnable under \u00b5, and the sample complexity of learning C to accuracy \u03b5k and confidence 1\u2212 \u03b4, \u03b4 > 0 is\nm \u2265 8 \u03b52 log 2fk \u03b4 = 8\n\u03b52k\n(\nfk + log(\u03b4 \u22121)\n)\n.\nFor every k, Lemma 2.2, applied with \u03b5 = 0.2, guarantees the existence of a subset \u03a6k of C every two elements of which are at a L1(\u00b5i)-distance \u2265 0.42mi from each other, and containing \u2265 exp[0.0128(fk \u2212 fk\u22121)] elements. Let N be so large that\n\u2211N k=1 mk \u2265 (1.05)\u22121. Fix k. Since\n\u222aNk=1Fk is shattered by C , one can find elements of C which correspond to elements of the product\n\u220fN i=k \u03a6i, and every\ntwo of which are at a distance \u2265 0.42\u2211Nk=1 mk\u03b5k \u2265 0.4\u03b5k from each other. According to Theorem 2.3, this means that the computational complexity of learning C under \u00b5 to accuracy \u03b5k with confidence 1 \u2212 \u03b4 is at least 0.0128fk samples.\nRemark 3.2: The measure \u00b5 constructed in the proof is purely atomic. However, by replacing the domain \u2126 with \u2126 \u00d7 [0, 1], every concept C \u2208 C with C \u00d7 [0, 1], and \u00b5 with the product \u00b5\u2297 \u03bb, where \u03bb is the uniform (Lebesgue) measure on the interval, one can \u201ctranslate\u201d every example as above into an example of learning under a non-atomic probability distribution.\nCorollary 3.3: Let \u03bd be a probability distribution on a domain \u2126 having infinite support. Then there exist concept classes C which are PAC learnable under \u03bd and whose required sample complexity is arbitrarily high.\nProof: The measure space (\u2126, \u03bd) admits a measurepreserving map \u03c6 to the measure space constructed in the proof of Theorem 3.1 in such a way that \u03bd\u03c6\u22121 = \u00b5 (here one uses the fact that \u00b5 is purely atomic). Now the concept class C\u03c6\u22121, consisting of all sets \u03c6\u22121(C), has the same learning properties under the distribution \u03bd as the class C has under \u00b5.\nCorollary 3.4: Let \u03b5k \u2193 0 be a sequence of positive values converging to zero, and let fk be a real function on [0,+\u221e) growing at least linearly. Then there is a probability distribution \u00b5 on the real numbers under which Sontag\u2019s network N is PAC learnable to accuracy \u03b5k with confidence 1\u2212\u03b4, requiring the sample of size \u2126(f(\u03b5\u22121k )). This estimate is essentially tight, because the sample size\nn(\u03b5k, \u03b4) = O\n(\nf\n(\n1\n\u03b5k\n)\n+ log\n(\n1\n\u03b4\n))\n. (4)\nalready suffices to train N to accuracy 4\u03b5k with confidence 1\u2212 \u03b4.\nRemark 3.5: It is easy to construct concept classes which are PAC learnable under every input distribution, and yet exhibit all possible rates of learning sample complexity. These are the classes C which, speaking informally, cannot\ntell a difference between a given probability distribution \u00b5 and some purely atomic measure \u03bd. More precisely, if the sigma-algebra of sets generated by C is purely atomic and C shatters every finite subset of an infinite set, then C will have the above property.\nAn example is a class C that consists of all finite unions of middle thirds of the Cantor set C. The atoms of the sigma-algebra of sets generated by this class are precisely the middle thirds, and so C has the desired property."}, {"heading": "IV. CONCLUSION", "text": "Stimulated by a question embedded into the Problem 12.6 of Vidyasagar [11], we have shown that all rates of sample compleixity growth are possible for distributiondependent learning, in particular all are realized by binary output feed-forward sigmoidal neural network of Sontag. Now Vidyasagar continues thus:\n\u201cI would like to have an \u201cintrinsic\u201d explanation as to why in distribution-free learning, every learnable concept class is also forced to be polynomially learnable. Next, how far can one \u201cpush\u201d this line of argument? Suppose P is a family of probabilities that contains a ball in the total variation metric \u03c1. From Theorem 8.8 it follows that every concept class that is learnable with respect to P must also be polynomially learnable (because C must have finite VCdimension). Is it possible to identify other such classes of probabilities?\u201d\nWe suggest the following conjecture, which, in our view, is the right framework in which to address Vidyasagar\u2019s question.\nConjecture (\u201cthe sample complexity alternative\u201d). Let P be a family of probability distributions on the domain \u2126. Then either every class learnable under P is learnable with sample complexity O(\u03b5\u22121), or else there exist PAC learnable classes under P whose required sample complexity grows arbitrarily fast.\nThe classical VC theory tells that the conjecture is true if P is the family of all probability measures: namely, the first alternative holds always. In view of Corollary 3.3, the conjecture is also true in the other extreme case, where P = {\u00b5} contains a single distribution: unless \u00b5 is finitelysupported, we have the second alternative.\nProblem 1. Does the above alternative hold for every family P of probability distributions on the inputs?\nProblem 2. Does there exist a non-atomic probability measure on R under which the Sontag ANN is PAC learnable?\nProblem 3. Give a criterion for a concept class to be PAC learnable under a fixed probability distribution in terms of shattering.\nSome sufficient conditions can be found in [2], [1], but none of them is also necessary. The \u201cright\u201d condition will be strictly intermediate between the witness of irregularity [9], [10] and the VC dimension modulo countable sets [7]."}, {"heading": "ACKNOWLEDGMENTS", "text": "The author is grateful to the anonymous referees, in particular for pointing out the references [5], [6], and to Ilijas Farah for pointing out the reference [4]."}], "references": [{"title": "A sufficient condition for polynomial distribution-dependent learnability", "author": ["M. Anthony", "J. Shawe-Taylor"], "venue": "Discrete Applied Math. 77 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Learnability with respect to fixed distributions", "author": ["G.M. Benedek", "A. Itai"], "venue": "Theor. Comp. Sci. 86 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1991}, {"title": "Uniform Central Limit Theorems", "author": ["R.M. Dudley"], "venue": "Cambridge Studies in Advanced Mathematics, 63, Cambridge University Press, Cambridge ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Two measure properties of Cartesian product sets", "author": ["H.G. Eggleston"], "venue": "Quart. J. Math. Oxford (2) 5 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1954}, {"title": "Learning with Recurrent Neural Networks", "author": ["B. Hammer"], "venue": "Dissertation, Universit\u00e4t Osnabr\u00fcck", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "On the learnability of recursive data", "author": ["B. Hammer"], "venue": "Mathematics of Control Signals and Systems, 12 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "PAC learnability of a concept class under nonatomic measures: a problem by Vidyasagar", "author": ["V. Pestov"], "venue": "to appear in Proc. 21st Conf. on Algorithmic Learning Theory ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Feedforward nets for interpolation and classification", "author": ["E.D. Sontag"], "venue": "J. Comp. Systems Sci 45(1) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1992}, {"title": "The Glivenko\u2013Cantelli problem", "author": ["M. Talagrand"], "venue": "Ann. Probab. 15 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1987}, {"title": "The Glivenko-Cantelli problem", "author": ["M. Talagrand"], "venue": "ten years later, J. Theoret. Probab. 9 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Learning and Generalization", "author": ["M. Vidyasagar"], "venue": "with Applications to Neural Networks, 2nd Ed., Springer-Verlag", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 10, "context": "6 from Vidyasagar\u2019s book [11] (this problem appears already in the original 1997 version).", "startOffset": 25, "endOffset": 29}, {"referenceID": 1, "context": "\u201d In fact, the existence of a concept class whose sample complexity grows exponentially in 1/\u03b5 under a given fixed input distribution was already shown in 1991 by Benedek and Itai [2] (Theorem 3.", "startOffset": 180, "endOffset": 183}, {"referenceID": 4, "context": "thesis [5] (Example 4.", "startOffset": 7, "endOffset": 10}, {"referenceID": 5, "context": "also [6].", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "For example, a wellknown sigmoidal feed-forward neural network of infinite VC dimension constructed by Sontag [8] has this property.", "startOffset": 110, "endOffset": 113}, {"referenceID": 8, "context": "It follows from Talagrand\u2019s theory of witness of irregularity [9], [10] that N is not Glivenko\u2013Cantelli with regard to any measure having a non-atomic part.", "startOffset": 62, "endOffset": 65}, {"referenceID": 9, "context": "It follows from Talagrand\u2019s theory of witness of irregularity [9], [10] that N is not Glivenko\u2013Cantelli with regard to any measure having a non-atomic part.", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "Benedek and Itai [2] had proved that a concept class C is PAC learnable under a single probability distribution \u03bc if and only if C is totally bounded in the L(\u03bc)-distance.", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": "8 in [2]; Theorem 6.", "startOffset": 5, "endOffset": 8}, {"referenceID": 10, "context": "3 in [11]): Suppose C is a concept class, \u03b5 > 0, and that B1, .", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "2 ([11], Lemma 7.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "8 in [2]; Theorem 6.", "startOffset": 5, "endOffset": 8}, {"referenceID": 10, "context": "6 in [11]): Suppose C is a given concept class, and let \u03b5 > 0 be specified.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "For the most comprehensive presentation of PAC learnability under a single distribution, see [11], Ch.", "startOffset": 93, "endOffset": 97}, {"referenceID": 2, "context": "A function class F on a domain (a standard Borel space) \u03a9 is Glivenko\u2013Cantelli with regard to a probability distribution \u03bc ([3], Ch.", "startOffset": 124, "endOffset": 127}, {"referenceID": 10, "context": "3), or else has the property of uniform convergence of empirical means (UCEM property) [11], if for each \u03b5 > 0", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "We find it instructive to give a different proof, replying in passing to a remark of Vidyasagar [11], p.", "startOffset": 96, "endOffset": 100}, {"referenceID": 9, "context": "The answer is yes, as is (implicitely) stated in [10] (p.", "startOffset": 49, "endOffset": 53}, {"referenceID": 8, "context": "Talagrand [9], [10] had characterized uniform Glivenko\u2013 Cantelli function classes with regard to a single distribution in terms of shattering.", "startOffset": 10, "endOffset": 13}, {"referenceID": 9, "context": "Talagrand [9], [10] had characterized uniform Glivenko\u2013 Cantelli function classes with regard to a single distribution in terms of shattering.", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "5 (Talagrand [9], Th.", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "The corollary is easy to prove directly, without using subtle results of Talagrand, and the result was observed (independently) in 1991 and investigated in detail by Benedek and Itai ([2], Theorem 3.", "startOffset": 184, "endOffset": 187}, {"referenceID": 7, "context": "Figure 1 recalls a well-known example of a sigmoidal neural network N constructed by Sontag [8], pp.", "startOffset": 92, "endOffset": 95}, {"referenceID": 10, "context": "also [11], page 389, where the top diagram in Figure 1 is borrowed from.", "startOffset": 5, "endOffset": 9}, {"referenceID": 7, "context": "7 ([8], pp.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "This is a consequence of Eggleston\u2019s theorem [4]: If A is a measurable, Lebesgue-positive subset of the unit square, then there is a measurable positive set B and a perfect set C such", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "[2], p.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "385, note (2), or [11], p.", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "6 in [11], p.", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "This is the set left of the closed unit interval [0, 1] after first deleting the middle third (1/3, 2/3), then deleting the middle thirds of the two remaining intervals, (1/9, 2/9) and (7/9, 8/9), and continuing to delete the middle thirds ad infimum.", "startOffset": 49, "endOffset": 55}, {"referenceID": 6, "context": "2 exactly as it was done in [7], proof of Theorem 3, in order to conclude that C is not totally bounded in the L(\u03bd)-distance.", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "However, by replacing the domain \u03a9 with \u03a9 \u00d7 [0, 1], every concept C \u2208 C with C \u00d7 [0, 1], and \u03bc with the product \u03bc\u2297 \u03bb, where \u03bb is the uniform (Lebesgue) measure on the interval, one can \u201ctranslate\u201d every example as above into an example of learning under a non-atomic probability distribution.", "startOffset": 44, "endOffset": 50}, {"referenceID": 0, "context": "However, by replacing the domain \u03a9 with \u03a9 \u00d7 [0, 1], every concept C \u2208 C with C \u00d7 [0, 1], and \u03bc with the product \u03bc\u2297 \u03bb, where \u03bb is the uniform (Lebesgue) measure on the interval, one can \u201ctranslate\u201d every example as above into an example of learning under a non-atomic probability distribution.", "startOffset": 81, "endOffset": 87}, {"referenceID": 10, "context": "6 of Vidyasagar [11], we have shown that all rates of sample compleixity growth are possible for distributiondependent learning, in particular all are realized by binary output feed-forward sigmoidal neural network of Sontag.", "startOffset": 16, "endOffset": 20}, {"referenceID": 1, "context": "Some sufficient conditions can be found in [2], [1], but none of them is also necessary.", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "Some sufficient conditions can be found in [2], [1], but none of them is also necessary.", "startOffset": 48, "endOffset": 51}, {"referenceID": 8, "context": "The \u201cright\u201d condition will be strictly intermediate between the witness of irregularity [9], [10] and the VC dimension modulo countable sets [7].", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "The \u201cright\u201d condition will be strictly intermediate between the witness of irregularity [9], [10] and the VC dimension modulo countable sets [7].", "startOffset": 93, "endOffset": 97}, {"referenceID": 6, "context": "The \u201cright\u201d condition will be strictly intermediate between the witness of irregularity [9], [10] and the VC dimension modulo countable sets [7].", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "The author is grateful to the anonymous referees, in particular for pointing out the references [5], [6], and to Ilijas Farah for pointing out the reference [4].", "startOffset": 96, "endOffset": 99}, {"referenceID": 5, "context": "The author is grateful to the anonymous referees, in particular for pointing out the references [5], [6], and to Ilijas Farah for pointing out the reference [4].", "startOffset": 101, "endOffset": 104}, {"referenceID": 3, "context": "The author is grateful to the anonymous referees, in particular for pointing out the references [5], [6], and to Ilijas Farah for pointing out the reference [4].", "startOffset": 157, "endOffset": 160}], "year": 2013, "abstractText": "We show that the learning sample complexity of a sigmoidal neural network constructed by Sontag (1992) required to achieve a given misclassification error under a fixed purely atomic distribution can grow arbitrarily fast: for any prescribed rate of growth there is an input distribution having this rate as the sample complexity, and the bound is asymptotically tight. The rate can be superexponential, a nonrecursive function, etc. We further observe that Sontag\u2019s ANN is not Glivenko\u2013Cantelli under any input distribution having a non-atomic part. Keywords-PAC learnability, fixed distribution learning, sample complexity, infinite VC dimension, witness of irregularity, Sontag\u2019s ANN, precompactness.", "creator": "LaTeX with hyperref package"}}}