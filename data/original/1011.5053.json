{"id": "1011.5053", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2010", "title": "Tight Sample Complexity of Large-Margin Learning", "abstract": "We obtain a tight distribution-specific characterization of the sample complexity of large-margin classification with L_2 regularization: We introduce the \\gamma-adapted-dimension, which is a simple function of the spectrum of a distribution's covariance matrix, and show distribution-specific upper and lower bounds on the sample complexity, both governed by the \\gamma-adapted-dimension of the source distribution. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classification. The bounds hold for a rich family of sub-Gaussian distributions.", "histories": [["v1", "Tue, 23 Nov 2010 10:44:21 GMT  (89kb,S)", "https://arxiv.org/abs/1011.5053v1", "Appearing in Neural Information Processing Systems (NIPS) 2010; This is the full version, including appendix with proofs"], ["v2", "Thu, 5 Apr 2012 16:40:03 GMT  (29kb)", "http://arxiv.org/abs/1011.5053v2", "Appearing in Neural Information Processing Systems (NIPS) 2010; This is the full version, including appendix with proofs; Also with some corrections"]], "COMMENTS": "Appearing in Neural Information Processing Systems (NIPS) 2010; This is the full version, including appendix with proofs", "reviews": [], "SUBJECTS": "cs.LG math.PR math.ST stat.ML stat.TH", "authors": ["sivan sabato", "nathan srebro", "naftali tishby"], "accepted": true, "id": "1011.5053"}, "pdf": {"name": "1011.5053.pdf", "metadata": {"source": "CRF", "title": "Tight Sample Complexity of Large-Margin Learning", "authors": ["Sivan Sabato", "Nathan Srebro", "Naftali Tishby"], "emails": ["sabato@cs.huji.ac.il,", "tishby@cs.huji.ac.il,", "nati@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n01 1.\n50 53\nv2 [\ncs .L\nG ]\n5 A\npr 2\nWe obtain a tight distribution-specific characterization of the sample complexity of large-margin classification with L2 regularization: We introduce the \u03b3-adapted-dimension, which is a simple function of the spectrum of a distribution\u2019s covariance matrix, and show distribution-specific upper and lower bounds on the sample complexity, both governed by the \u03b3-adapted-dimension of the source distribution. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classification. The bounds hold for a rich family of sub-Gaussian distributions."}, {"heading": "1 Introduction", "text": "In this paper we tackle the problem of obtaining a tight characterization of the sample complexity which a particular learning rule requires, in order to learn a particular source distribution. Specifically, we obtain a tight characterization of the sample complexity required for large (Euclidean) margin learning to obtain low error for a distribution D(X,Y ), for X \u2208 Rd, Y \u2208 {\u00b11}. Most learning theory work focuses on upper-bounding the sample complexity. That is, on providing a bound m(D, \u01eb) and proving that when using some specific learning rule, if the sample size is at least m(D, \u01eb), an excess error of at most \u01eb (in expectation or with high probability) can be ensured. For instance, for large-margin classification we know that if PD[\u2016X\u2016 \u2264 B] = 1, then m(D, \u01eb) can be set to O(B2/(\u03b32\u01eb2)) to get true error of no more than \u2113\u2217\u03b3 + \u01eb, where \u2113\u2217\u03b3 = min\u2016w\u2016\u22641 PD(Y \u3008w,X\u3009 \u2264 \u03b3) is the optimal margin error at margin \u03b3. Such upper bounds can be useful for understanding positive aspects of a learning rule. But it is difficult to understand deficiencies of a learning rule, or to compare between different rules, based on upper bounds alone. After all, it is possible, and often the case, that the true sample complexity, i.e. the actual number of samples required to get low error, is much lower than the bound.\nOf course, some sample complexity upper bounds are known to be \u201ctight\u201d or to have an almostmatching lower bound. This usually means that the bound is tight as a worst-case upper bound for a specific class of distributions (e.g. all those with PD[\u2016X\u2016 \u2264 B] = 1). That is, there exists some source distribution for which the bound is tight. In other words, the bound concerns some quantity of the distribution (e.g. the radius of the support), and is the lowest possible bound in terms of this quantity. But this is not to say that for any specific distribution this quantity tightly characterizes the sample complexity. For instance, we know that the sample complexity can be much smaller than the radius of the support of X , if the average norm \u221a E[\u2016X\u20162] is small. However, E[\u2016X\u20162] is also not a precise characterization of the sample complexity, for instance in low dimensions.\nThe goal of this paper is to identify a simple quantity determined by the distribution that does precisely characterize the sample complexity. That is, such that the actual sample complexity for the learning rule on this specific distribution is governed, up to polylogarithmic factors, by this quantity.\nIn particular, we present the \u03b3-adapted-dimension k\u03b3(D). This measure refines both the dimension and the average norm ofX , and it can be easily calculated from the covariance matrix ofX . We show that for a rich family of \u201clight tailed\u201d distributions (specifically, sub-Gaussian distributions with independent uncorrelated directions \u2013 see Section 2), the number of samples required for learning by minimizing the \u03b3-margin-violations is both lower-bounded and upper-bounded by \u0398\u0303(k\u03b3). More precisely, we show that the sample complexity m(\u01eb, \u03b3,D) required for achieving excess error of no more than \u01eb can be bounded from above and from below by:\n\u2126(k\u03b3(D)) \u2264 m(\u01eb, \u03b3,D) \u2264 O\u0303( k\u03b3(D)\n\u01eb2 ).\nAs can be seen in this bound, we are not concerned about tightly characterizing the dependence of the sample complexity on the desired error [as done e.g. in 1], nor with obtaining tight bounds for very small error levels. In fact, our results can be interpreted as studying the sample complexity needed to obtain error well below random, but bounded away from zero. This is in contrast to classical statistics asymptotic that are also typically tight, but are valid only for very small \u01eb. As was recently shown by Liang and Srebro [2], the quantities on which the sample complexity depends on for very small \u01eb (in the classical statistics asymptotic regime) can be very different from those for moderate error rates, which are more relevant for machine learning.\nOur tight characterization, and in particular the distribution-specific lower bound on the sample complexity that we establish, can be used to compare large-margin (L2 regularized) learning to other learning rules. In Section 7 we provide two such examples: we use our lower bound to rigorously establish a sample complexity gap betweenL1 andL2 regularization previously studied in [3], and to show a large gap between discriminative and generative learning on a Gaussian-mixture distribution.\nIn this paper we focus only on large L2 margin classification. But in order to obtain the distributionspecific lower bound, we develop novel tools that we believe can be useful for obtaining lower bounds also for other learning rules."}, {"heading": "Related work", "text": "Most work on \u201csample complexity lower bounds\u201d is directed at proving that under some set of assumptions, there exists a source distribution for which one needs at least a certain number of examples to learn with required error and confidence [4, 5, 6]. This type of a lower bound does not, however, indicate much on the sample complexity of other distributions under the same set of assumptions.\nAs for distribution-specific lower bounds, the classical analysis of Vapnik [7, Theorem 16.6] provides not only sufficient but also necessary conditions for the learnability of a hypothesis class with respect to a specific distribution. The essential condition is that the \u01eb-entropy of the hypothesis class with respect to the distribution be sub-linear in the limit of an infinite sample size. In some sense, this criterion can be seen as providing a \u201clower bound\u201d on learnability for a specific distribution. However, we are interested in finite-sample convergence rates, and would like those to depend on simple properties of the distribution. The asymptotic arguments involved in Vapnik\u2019s general learnability claim do not lend themselves easily to such analysis.\nBenedek and Itai [8] show that if the distribution is known to the learner, a specific hypothesis class is learnable if and only if there is a finite \u01eb-cover of this hypothesis class with respect to the distribution. Ben-David et al. [9] consider a similar setting, and prove sample complexity lower bounds for learning with any data distribution, for some binary hypothesis classes on the real line. In both of these works, the lower bounds hold for any algorithm, but only for a worst-case target hypothesis. Vayatis and Azencott [10] provide distribution-specific sample complexity upper bounds for hypothesis classes with a limited VC-dimension, as a function of how balanced the hypotheses are with respect to the considered distributions. These bounds are not tight for all distributions, thus this work also does not provide true distribution-specific sample complexity."}, {"heading": "2 Problem setting and definitions", "text": "Let D be a distribution over Rd \u00d7 {\u00b11}. DX will denote the restriction of D to Rd. We are interested in linear separators, parametrized by unit-norm vectors in Bd1 , {w \u2208 Rd | \u2016w\u20162 \u2264 1}.\nFor a predictor w denote its misclassification error with respect to distribution D by \u2113(w,D) , P(X,Y )\u223cD[Y \u3008w,X\u3009 \u2264 0]. For \u03b3 > 0, denote the \u03b3-margin loss of w with respect to D by \u2113\u03b3(w,D) , P(X,Y )\u223cD[Y \u3008w,X\u3009 \u2264 \u03b3]. The minimal margin loss with respect to D is denoted by \u2113\u2217\u03b3(D) , minw\u2208Bd 1 \u2113\u03b3(w,D). For a sample S = {(xi, yi)}mi=1 such that (xi, yi) \u2208 Rd \u00d7 {\u00b11}, the margin loss with respect to S is denoted by \u2113\u0302\u03b3(w, S) , 1m |{i | yi\u3008xi, w\u3009 \u2264 \u03b3}| and the misclassification error is \u2113\u0302(w, S) , 1m |{i | yi\u3008xi, w\u3009 \u2264 0}|. In this paper we are concerned with learning by minimizing the margin loss. It will be convenient for us to discuss transductive learning algorithms. Since many predictors minimize the margin loss, we define:\nDefinition 2.1. A margin-error minimization algorithm A is an algorithm whose input is a margin \u03b3, a training sample S = {(xi, yi)}mi=1 and an unlabeled test sample S\u0303X = {x\u0303i}mi=1, which outputs a predictor w\u0303 \u2208 argminw\u2208Bd\n1 \u2113\u0302\u03b3(w, S). We denote the output of the algorithm by\nw\u0303 = A\u03b3(S, S\u0303X).\nWe will be concerned with the expected test loss of the algorithm given a random training sample and a random test sample, each of size m, and define \u2113m(A\u03b3 , D) , ES,S\u0303\u223cDm [\u2113\u0302(A(S, S\u0303X), S\u0303)], where S, S\u0303 \u223c Dm independently. For \u03b3 > 0, \u01eb \u2208 [0, 1], and a distribution D, we denote the distributionspecific sample complexity by m(\u01eb, \u03b3,D): this is the minimal sample size such that for any marginerror minimization algorithm A, and for any m \u2265 m(\u01eb, \u03b3,D), \u2113m(A\u03b3 , D)\u2212 \u2113\u2217\u03b3(D) \u2264 \u01eb."}, {"heading": "Sub-Gaussian distributions", "text": "We will characterize the distribution-specific sample complexity in terms of the covariance of X \u223c DX . But in order to do so, we must assume that X is not too heavy-tailed. Otherwise, X can have even infinite covariance but still be learnable, for instance if it has a tiny probability of having an exponentially large norm. We will thus restrict ourselves to sub-Gaussian distributions. This ensures light tails in all directions, while allowing a sufficiently rich family of distributions, as we presently see. We also require a more restrictive condition \u2013 namely that DX can be rotated to a product distribution over the axes of Rd. A distribution can always be rotated so that its coordinates are uncorrelated. Here we further require that they are independent, as of course holds for any multivariate Gaussian distribution.\nDefinition 2.2 (See e.g. [11, 12]). A random variable X is sub-Gaussian with moment B (or B-sub-Gaussian) for B \u2265 0 if\n\u2200t \u2208 R, E[exp(tX)] \u2264 exp(B2t2/2). (1)\nWe further say that X is sub-Gaussian with relative moment \u03c1 = B/ \u221a E[X2].\nThe sub-Gaussian family is quite extensive: For instance, any bounded, Gaussian, or Gaussianmixture random variable with mean zero is included in this family.\nDefinition 2.3. A distribution DX over X \u2208 Rd is independently sub-Gaussian with relative moment \u03c1 if there exists some orthonormal basis a1, . . . , ad \u2208 Rd, such that \u3008X, ai\u3009 are independent sub-Gaussian random variables, each with a relative moment \u03c1.\nWe will focus on the family Dsg\u03c1 of all independently \u03c1-sub-Gaussian distributions in arbitrary dimension, for a small fixed constant \u03c1. For instance, the family Dsg3/2 includes all Gaussian distributions, all distributions which are uniform over a (hyper)box, and all multi-Bernoulli distributions, in addition to other less structured distributions. Our upper bounds and lower bounds will be tight up to quantities which depend on \u03c1, which we will regard as a constant, but the tightness will not depend on the dimensionality of the space or the variance of the distribution."}, {"heading": "3 The \u03b3-adapted-dimension", "text": "As mentioned in the introduction, the sample complexity of margin-error minimization can be upperbounded in terms of the average norm E[\u2016X\u20162] by m(\u01eb, \u03b3,D) \u2264 O(E[\u2016X\u20162]/(\u03b32\u01eb2)) [13]. Alternatively, we can rely only on the dimensionality and conclude m(\u01eb, \u03b3,D) \u2264 O\u0303(d/\u01eb2) [7]. Thus,\nalthough both of these bounds are tight in the worst-case sense, i.e. they are the best bounds that rely only on the norm or only on the dimensionality respectively, neither is tight in a distributionspecific sense: If the average norm is unbounded while the dimensionality is small, an arbitrarily large gap is created between the true m(\u01eb, \u03b3,D) and the average-norm upper bound. The converse happens if the dimensionality is arbitrarily high while the average-norm is bounded.\nSeeking a distribution-specific tight analysis, one simple option to try to tighten these bounds is to consider their minimum, min(d,E[\u2016X\u20162]/\u03b32)/\u01eb2, which, trivially, is also an upper bound on the sample complexity. However, this simple combination is also not tight: Consider a distribution in which there are a few directions with very high variance, but the combined variance in all other directions is small. We will show that in such situations the sample complexity is characterized not by the minimum of dimension and norm, but by the sum of the number of high-variance dimensions and the average norm in the other directions. This behavior is captured by the \u03b3-adapted-dimension:\nDefinition 3.1. Let b > 0 and k a positive integer.\n(a). A subset X \u2286 Rd is (b, k)-limited if there exists a sub-space V \u2286 Rd of dimension d \u2212 k such that X \u2286 {x \u2208 Rd | \u2016x\u2032P\u20162 \u2264 b}, where P is an orthogonal projection onto V .\n(b). A distribution DX over Rd is (b, k)-limited if there exists a sub-space V \u2286 Rd of dimension d\u2212 k such that EX\u223cDX [\u2016X \u2032P\u20162] \u2264 b, with P an orthogonal projection onto V .\nDefinition 3.2. The \u03b3-adapted-dimension of a distribution or a set, denoted by k\u03b3 , is the minimum k such that the distribution or set is (\u03b32k, k) limited.\nIt is easy to see that k\u03b3(DX) is upper-bounded by min(d,E[\u2016X\u20162]/\u03b32). Moreover, it can be much smaller. For example, for X \u2208 R1001 with independent coordinates such that the variance of the first coordinate is 1000, but the variance in each remaining coordinate is 0.001 we have k1 = 1 but d = E[\u2016X\u20162] = 1001. More generally, if \u03bb1 \u2265 \u03bb2 \u2265 \u00b7 \u00b7 \u00b7\u03bbd are the eigenvalues of the covariance matrix of X , then k\u03b3 = min{k | \u2211d i=k+1 \u03bbi \u2264 \u03b32k}. A quantity similar to k\u03b3 was studied previously in [14]. k\u03b3 is different in nature from some other quantities used for providing sample complexity bounds in terms of eigenvalues, as in [15], since it is defined based on the eigenvalues of the distribution and not of the sample. In Section 6 we will see that these can be quite different.\nIn order to relate our upper and lower bounds, it will be useful to relate the \u03b3-adapted-dimension for different margins. The relationship is established in the following Lemma , proved in the appendix:\nLemma 3.3. For 0 < \u03b1 < 1, \u03b3 > 0 and a distribution DX , k\u03b3(DX) \u2264 k\u03b1\u03b3(DX) \u2264 2k\u03b3(DX )\u03b12 + 1.\nWe proceed to provide a sample complexity upper bound based on the \u03b3-adapted-dimension."}, {"heading": "4 A sample complexity upper bound using \u03b3-adapted-dimension", "text": "In order to establish an upper bound on the sample complexity, we will bound the fat-shattering dimension of the linear functions over a set in terms of the \u03b3-adapted-dimension of the set. Recall that the fat-shattering dimension is a classic quantity for proving sample complexity upper bounds:\nDefinition 4.1. Let F be a set of functions f : X \u2192 R, and let \u03b3 > 0. The set {x1, . . . , xm} \u2286 X is \u03b3-shattered by F if there exist r1, . . . , rm \u2208 R such that for all y \u2208 {\u00b11}m there is an f \u2208 F such that \u2200i \u2208 [m], yi(f(xi) \u2212 ri) \u2265 \u03b3. The \u03b3-fat-shattering dimension of F is the size of the largest set in X that is \u03b3-shattered by F .\nThe sample complexity of \u03b3-loss minimization is bounded by O\u0303(d\u03b3/8/\u01eb2) were d\u03b3/8 is the \u03b3/8fat-shattering dimension of the function class [16, Theorem 13.4]. Let W(X ) be the class of linear functions restricted to the domain X . For any set we show: Theorem 4.2. If a set X is (B2, k)-limited, then the \u03b3-fat-shattering dimension of W(X ) is at most 3 2 (B 2/\u03b32 + k + 1). Consequently, it is also at most 3k\u03b3(X ) + 1.\nProof. Let X be a m \u00d7 d matrix whose rows are a set of m points in Rd which is \u03b3-shattered. For any \u01eb > 0 we can augment X with an additional column to form the matrix X\u0303 of dimensions m\u00d7 (d+1), such that for all y \u2208 {\u2212\u03b3,+\u03b3}m, there is a wy \u2208 Bd+11+\u01eb such that X\u0303wy = y (the details\ncan be found in the appendix). Since X is (B2, k)-limited, there is an orthogonal projection matrix P\u0303 of size (d + 1) \u00d7 (d + 1) such that \u2200i \u2208 [m], \u2016X\u0303 \u2032iP\u20162 \u2264 B2 where X\u0303i is the vector in row i of X\u0303 . Let V\u0303 be the sub-space of dimension d\u2212 k spanned by the columns of P\u0303 . To bound the size of the shattered set, we show that the projected rows of X\u0303 on V are \u2018shattered\u2019 using projected labels. We then proceed similarly to the proof of the norm-only fat-shattering bound [17].\nWe have X\u0303 = X\u0303P\u0303 + X\u0303(I \u2212 P\u0303 ). In addition, X\u0303wy = y. Thus y \u2212 X\u0303P\u0303wy = X\u0303(I \u2212 P\u0303 )wy . I \u2212 P\u0303 is a projection onto a k + 1-dimensional space, thus the rank of X\u0303(I \u2212 P\u0303 ) is at most k + 1. Let T be an m \u00d7 m orthogonal projection matrix onto the subspace orthogonal to the columns of X\u0303(I \u2212 P\u0303 ). This sub-space is of dimension at most l = m \u2212 (k + 1), thus trace(T ) = l. T (y \u2212 X\u0303P\u0303wy) = T X\u0303(I \u2212 P\u0303 )wy = 0(d+1)\u00d71. Thus Ty = T X\u0303P\u0303wy for every y \u2208 {\u2212\u03b3,+\u03b3}m.\nDenote row i of T by ti and row i of T X\u0303P\u0303 by zi. We have \u2200i \u2264 m, \u3008zi, w1y\u3009 = tiy =\u2211 j\u2264m ti[j]y[j]. Therefore \u3008 \u2211 i ziy[i], w 1 y\u3009 = \u2211 i\u2264m \u2211 j\u2264(l+k) ti[j]y[i]y[j]. Since \u2016w1y\u2016 \u2264 1 + \u01eb, \u2200x \u2208 Rd+1, (1 + \u01eb)\u2016x\u2016 \u2265 \u2016x\u2016\u2016w1y\u2016 \u2265 \u3008x,w1y\u3009. Thus \u2200y \u2208 {\u2212\u03b3,+\u03b3}m, (1 + \u01eb)\u2016 \u2211\ni ziy[i]\u2016 \u2265\u2211 i\u2264m \u2211 j\u2264m ti[j]y[i]y[j]. Taking the expectation of y chosen uniformly at random, we have\n(1 + \u01eb)E[\u2016 \u2211\ni\nziy[i]\u2016] \u2265 \u2211\ni,j\nE[ti[j]y[i]y[j]] = \u03b3 2 \u2211\ni\nti[i] = \u03b3 2trace(T ) = \u03b32l.\nIn addition, 1\u03b32E[\u2016 \u2211 i ziy[i]\u20162] = \u2211l\ni=1 \u2016zi\u20162 = trace(P\u0303 \u2032X\u0303 \u2032T 2X\u0303P\u0303 ) \u2264 trace(P\u0303 \u2032X\u0303 \u2032X\u0303P\u0303 ) \u2264 B2m. From the inequality E[X2] \u2264 E[X ]2, it follows that l2 \u2264 (1 + \u01eb)2B2\u03b32 m. Since this holds for any \u01eb > 0, we can set \u01eb = 0 and solve for m. Thus m \u2264 (k + 1) + B22\u03b32 + \u221a B4 4\u03b34 + B2 \u03b32 (k + 1) \u2264\n(k + 1) + B 2 \u03b32 + \u221a B2 \u03b32 (k + 1) \u2264 32 (B 2 \u03b32 + k + 1).\nCorollary 4.3. Let D be a distribution over X \u00d7 {\u00b11}, X \u2286 Rd. Then\nm(\u01eb, \u03b3,D) \u2264 O\u0303 ( k\u03b3/8(X )\n\u01eb2\n) .\nThe corollary above holds only for distributions with bounded support. However, since sub-Gaussian variables have an exponentially decaying tail, we can use this corollary to provide a bound for independently sub-Gaussian distributions as well (see appendix for proof): Theorem 4.4 (Upper Bound for Distributions in Dsg\u03c1 ). For any distribution D over Rd\u00d7{\u00b11} such that DX \u2208 Dsg\u03c1 ,\nm(\u01eb, \u03b3,D) = O\u0303( \u03c12k\u03b3(DX)\n\u01eb2 ).\nThis new upper bound is tighter than norm-only and dimension-only upper bounds. But does the \u03b3-adapted-dimension characterize the true sample complexity of the distribution, or is it just another upper bound? To answer this question, we need to be able to derive sample complexity lower bounds as well. We consider this problem in following section."}, {"heading": "5 Sample complexity lower bounds using Gram-matrix eigenvalues", "text": "We wish to find a distribution-specific lower bound that depends on the \u03b3-adapted-dimension, and matches our upper bound as closely as possible. To do that, we will link the ability to learn with a margin, with properties of the data distribution. The ability to learn is closely related to the probability of a sample to be shattered, as evident from Vapnik\u2019s formulations of learnability as a function of the \u01eb-entropy. In the preceding section we used the fact that non-shattering (as captured by the fat-shattering dimension) implies learnability. For the lower bound we use the converse fact, presented below in Theorem 5.1: If a sample can be fat-shattered with a reasonably high probability, then learning is impossible. We then relate the fat-shattering of a sample to the minimal eigenvalue of its Gram matrix. This allows us to present a lower-bound on the sample complexity using a lower bound on the smallest eigenvalue of the Gram-matrix of a sample drawn from the data distribution. We use the term \u2018\u03b3-shattered at the origin\u2019 to indicate that a set is \u03b3-shattered by setting the bias r \u2208 Rm (see Def. 4.1) to the zero vector.\nTheorem 5.1. Let D be a distribution over Rd \u00d7 {\u00b11}. If the probability of a sample of size m drawn from DmX to be \u03b3-shattered at the origin is at least \u03b7, then there is a margin-error minimization algorithm A, such that \u2113m/2(A\u03b3 , D) \u2265 \u03b7/2.\nProof. For a given distribution D, let A be an algorithm which, for every two input samples S and S\u0303X , labels S\u0303X using the separator w \u2208 argminw\u2208Bd\n1 \u2113\u0302\u03b3(w, S) that maximizes ES\u0303Y \u2208DmY [\u2113\u0302\u03b3(w, S\u0303)].\nFor every x \u2208 Rd there is a label y \u2208 {\u00b11} such that P(X,Y )\u223cD[Y 6= y | X = x] \u2265 12 . If the set of examples in SX and S\u0303X together is \u03b3-shattered at the origin, then A chooses a separator with zero margin loss on S, but loss of at least 12 on S\u0303. Therefore \u2113m/2(A\u03b3 , D) \u2265 \u03b7/2.\nThe notion of shattering involves checking the existence of a unit-norm separator w for each labelvector y \u2208 {\u00b11}m. In general, there is no closed form for the minimum-norm separator. However, the following Theorem provides an equivalent and simple characterization for fat-shattering:\nTheorem 5.2. Let S = (X1, . . . , Xm) be a sample in Rd, denoteX the m\u00d7dmatrix whose rows are the elements of S. Then S is 1-shattered iff X is invertible and \u2200y \u2208 {\u00b11}m, y\u2032(XX \u2032)\u22121y \u2264 1.\nThe proof of this theorem is in the appendix. The main issue in the proof is showing that if a set is shattered, it is also shattered with exact margins, since the set of exact margins {\u00b11}m lies in the convex hull of any set of non-exact margins that correspond to all the possible labelings. We can now use the minimum eigenvalue of the Gram matrix to obtain a sufficient condition for fat-shattering, after which we present the theorem linking eigenvalues and learnability. For a matrix X , \u03bbn(X) denotes the n\u2019th largest eigenvalue of X .\nLemma 5.3. Let S = (X1, . . . , Xm) be a sample in Rd, with X as above. If \u03bbm(XX \u2032) \u2265 m then S is 1-shattered at the origin.\nProof. If \u03bbm(XX \u2032) \u2265 m then XX \u2032 is invertible and \u03bb1((XX \u2032)\u22121) \u2264 1/m. For any y \u2208 {\u00b11}m we have \u2016y\u2016 = \u221am and y\u2032(XX \u2032)\u22121y \u2264 \u2016y\u20162\u03bb1((XX \u2032)\u22121) \u2264 m(1/m) = 1. By Theorem 5.2 the sample is 1-shattered at the origin.\nTheorem 5.4. Let D be a distribution over Rd\u00d7{\u00b11},S be an i.i.d. sample of size m drawn from D, and denote XS the m\u00d7 d matrix whose rows are the points from S. If P[\u03bbm(XSX \u2032S) \u2265 m\u03b32] \u2265 \u03b7, then there exists a margin-error minimization algorithm A such that \u2113m/2(A\u03b3 , D) \u2265 \u03b7/2.\nTheorem 5.4 follows by scaling XS by \u03b3, applying Lemma 5.3 to establish \u03b3-fat shattering with probability at least \u03b7, then applying Theorem 5.1. Lemma 5.3 generalizes the requirement for linear independence when shattering using hyperplanes with no margin (i.e. no regularization). For unregularized (homogeneous) linear separation, a sample is shattered iff it is linearly independent, i.e. if \u03bbm > 0. Requiring \u03bbm > m\u03b32 is enough for \u03b3-fat-shattering. Theorem 5.4 then generalizes the simple observation, that if samples of size m are linearly independent with high probability, there is no hope of generalizing from m/2 points to the other m/2 using unregularized linear predictors. Theorem 5.4 can thus be used to derive a distribution-specific lower bound. Define:\nm\u03b3(D) , 1\n2 min\n{ m \u2223\u2223\u2223\u2223 PS\u223cDm [\u03bbm(XSX \u2032 S) \u2265 m\u03b32] < 1\n2\n}\nThen for any \u01eb < 1/4\u2212 \u2113\u2217\u03b3(D), we can conclude that m(\u01eb, \u03b3,D) \u2265 m\u03b3(D), that is, we cannot learn within reasonable error with less than m\u03b3 examples. Recall that our upper-bound on the sample\ncomplexity from Section 4 was O\u0303(k\u03b3). The remaining question is whether we can relate m\u03b3 and k\u03b3 , to establish that the our lower bound and upper bound tightly specify the sample complexity."}, {"heading": "6 A lower bound for independently sub-Gaussian distributions", "text": "As discussed in the previous section, to obtain sample complexity lower bound we require a bound on the value of the smallest eigenvalue of a random Gram-matrix. The distribution of this eigenvalue has been investigated under various assumptions. The cleanest results are in the case where m, d \u2192 \u221e and md \u2192 \u03b2 < 1, and the coordinates of each example are identically distributed:\nTheorem 6.1 (Theorem 5.11 in [18]). Let Xi be a series of mi\u00d7di matrices whose entries are i.i.d. random variables with mean zero, variance \u03c32 and finite fourth moments. If limi\u2192\u221e\nmi di = \u03b2 < 1,\nthen limi\u2192\u221e \u03bbm( 1dXiX \u2032 i) = \u03c3\n2(1\u2212\u221a\u03b2)2.\nThis asymptotic limit can be used to calculate m\u03b3 and thus provide a lower bound on the sample complexity: Let the coordinates of X \u2208 Rd be i.i.d. with variance \u03c32 and consider a sample of size m. If d,m are large enough, we have by Theorem 6.1:\n\u03bbm(XX \u2032) \u2248 d\u03c32(1\u2212 \u221a m/d)2 = \u03c32( \u221a d\u2212\u221am)2\nSolving \u03c32( \u221a d \u2212\u221a2m\u03b3)2 = 2m\u03b3\u03b32 we get m\u03b3 \u2248 12d/(1 + \u03b3/\u03c3)2. We can also calculate the \u03b3adapted-dimension for this distribution to get k\u03b3 \u2248 d/(1+ \u03b32/\u03c32), and conclude that 14k\u03b3 \u2264 m\u03b3 \u2264 1 2k\u03b3 . In this case, then, we are indeed able to relate the sample complexity lower bound with k\u03b3 , the same quantity that controls our upper bound. This conclusion is easy to derive from known results, however it holds only asymptotically, and only for a highly limited set of distributions. Moreover, since Theorem 6.1 holds asymptotically for each distribution separately, we cannot deduce from it any finite-sample lower bounds for families of distributions.\nFor our analysis we require finite-sample bounds for the smallest eigenvalue of a random Grammatrix. Rudelson and Vershynin [19, 20] provide such finite-sample lower bounds for distributions with identically distributed sub-Gaussian coordinates. In the following Theorem we generalize results of Rudelson and Vershynin to encompass also non-identically distributed coordinates. The proof of Theorem 6.2 can be found in the appendix. Based on this theorem we conclude with Theorem 6.3, stated below, which constitutes our final sample complexity lower bound.\nTheorem 6.2. Let \u03c1 > 0. There is a constant \u03b2 > 0 which depends only on B, such that for any \u03b4 \u2208 (0, 1) there exists a number L0, such that for any independently sub-Gaussian distribution with covariance matrix \u03a3 \u2264 I and trace(\u03a3) \u2265 L0, if each of its independent sub-Gaussian coordinates has relative moment \u03c1, then for any m \u2264 \u03b2 \u00b7 trace(\u03a3)\nP[\u03bbm(XmX \u2032 m) \u2265 m] \u2265 1\u2212 \u03b4,\nWhere Xm is an m\u00d7 d matrix whose rows are independent draws from DX . Theorem 6.3 (Lower bound for distributions in Dsg\u03c1 ). For any \u03c1 > 0, there are a constant \u03b2 > 0 and an integer L0 such that for any D such that DX \u2208 Dsg\u03c1 and k\u03b3(DX) > L0, for any margin \u03b3 > 0 and any \u01eb < 14 \u2212 \u2113\u2217\u03b3(D),\nm(\u01eb, \u03b3,D) \u2265 \u03b2k\u03b3(DX).\nProof. The covariance matrix of DX is clearly diagonal. We assume w.l.o.g. that \u03a3 = diag(\u03bb1, . . . , \u03bbd) where \u03bb1 \u2265 . . . \u2265 \u03bbd > 0. Let S be an i.i.d. sample of size m drawn from D. Let X be the m\u00d7 d matrix whose rows are the unlabeled examples from S. Let \u03b4 be fixed, and set \u03b2 and L0 as defined in Theorem 6.2 for \u03b4. Assume m \u2264 \u03b2(k\u03b3 \u2212 1). We would like to use Theorem 6.2 to bound the smallest eigenvalue of XX \u2032 with high probability, so that we can then apply Theorem 5.4 to get the desired lower bound. However, Theorem 6.2 holds only if all the coordinate variances are bounded by 1. Thus we divide the problem to two cases, based on the value of \u03bbk\u03b3+1, and apply Theorem 6.2 separately to each case.\nCase I: Assume \u03bbk\u03b3+1 \u2265 \u03b32. Then \u2200i \u2208 [k\u03b3 ], \u03bbi \u2265 \u03b32. Let \u03a31 = diag(1/\u03bb1, . . . , 1/\u03bbk\u03b3 , 0, . . . , 0). The random matrix X \u221a \u03a31 is drawn from an independently sub-Gaussian distribution, such that each of its coordinates has sub-Gaussian relative moment \u03c1 and covariance matrix \u03a3 \u00b7 \u03a31 \u2264 Id. In addition, trace(\u03a3 \u00b7\u03a31) = k\u03b3 \u2265 L0. Therefore Theorem 6.2 holds for X \u221a \u03a31, and P[\u03bbm(X\u03a31X \u2032) \u2265 m] \u2265 1\u2212\u03b4. Clearly, for any X , \u03bbm( 1\u03b32XX \u2032) \u2265 \u03bbm(X\u03a31X \u2032). Thus P[\u03bbm( 1\u03b32XX \u2032) \u2265 m] \u2265 1\u2212\u03b4. Case II: Assume \u03bbk\u03b3+1 < \u03b3\n2. Then \u03bbi < \u03b32 for all i \u2208 {k\u03b3 + 1, . . . , d}. Let \u03a32 = diag(0, . . . , 0, 1/\u03b32, . . . , 1/\u03b32), with k\u03b3 zeros on the diagonal. Then the random matrix X \u221a \u03a32 is drawn from an independently sub-Gaussian distribution with covariance matrix \u03a3 \u00b7\u03a32 \u2264 Id, such that all its coordinates have sub-Gaussian relative moment \u03c1. In addition, from the properties of k\u03b3 (see discussion in Section 2), trace(\u03a3\u00b7\u03a32) = 1\u03b32 \u2211d i=k\u03b3+1\n\u03bbi \u2265 k\u03b3\u22121 \u2265 L0\u22121. Thus Theorem 6.2 holds for X \u221a \u03a32, and so P[\u03bbm( 1\u03b32XX \u2032) \u2265 m] \u2265 P[\u03bbm(X\u03a32X \u2032) \u2265 m] \u2265 1\u2212 \u03b4.\nIn both cases P[\u03bbm( 1\u03b32XX \u2032) \u2265 m] \u2265 1\u2212 \u03b4 for any m \u2264 \u03b2(k\u03b3 \u2212 1). By Theorem 5.4, there exists an algorithm A such that for any m \u2264 \u03b2(k\u03b3 \u2212 1) \u2212 1, \u2113m(A\u03b3 , D) \u2265 12 \u2212 \u03b4/2. Therefore, for any \u01eb < 12 \u2212 \u03b4/2\u2212 \u2113\u2217\u03b3(D), we have m(\u01eb, \u03b3,D) \u2265 \u03b2(k\u03b3 \u2212 1). We get the theorem by setting \u03b4 = 14 ."}, {"heading": "7 Summary and consequences", "text": "Theorem 4.4 and Theorem 6.3 provide an upper bound and a lower bound for the sample complexity of any distribution D whose data distribution is in Dsg\u03c1 for some fixed \u03c1 > 0. We can thus draw the following bound, which holds for any \u03b3 > 0 and \u01eb \u2208 (0, 14 \u2212 \u2113\u2217\u03b3(D)):\n\u2126(k\u03b3(DX)) \u2264 m(\u01eb, \u03b3,D) \u2264 O\u0303( k\u03b3(DX)\n\u01eb2 ). (2)\nIn both sides of the bound, the hidden constants depend only on the constant \u03c1. This result shows that the true sample complexity of learning each of these distributions is characterized by the \u03b3adapted-dimension. An interesting conclusion can be drawn as to the influence of the conditional distribution of labels DY |X : Since Eq. (2) holds for any DY |X , the effect of the direction of the best separator on the sample complexity is bounded, even for highly non-spherical distributions. We can use Eq. (2) to easily characterize the sample complexity behavior for interesting distributions, and to compare L2 margin minimization to learning methods.\nGaps between L1 and L2 regularization in the presence of irrelevant features. Ng [3] considers learning a single relevant feature in the presence of many irrelevant features, and compares using L1 regularization and L2 regularization. When \u2016X\u2016\u221e \u2264 1, upper bounds on learning with L1 regularization guarantee a sample complexity of O(log(d)) for an L1-based learning rule [21]. In order to compare this with the sample complexity of L2 regularized learning and establish a gap, one must use a lower bound on the L2 sample complexity. The argument provided by Ng actually assumes scale-invariance of the learning rule, and is therefore valid only for unregularized linear learning. However, using our results we can easily establish a lower bound of \u2126(d) for many specific distributions with \u2016X\u2016\u221e \u2264 1 and Y = X [1] \u2208 {\u00b11}. For instance, when each coordinate is an independent Bernoulli variable, the distribution is sub-Gaussian with \u03c1 = 1, and k1 = \u2308d/2\u2309. Gaps between generative and discriminative learning for a Gaussian mixture. Consider two classes, each drawn from a unit-variance spherical Gaussian in a high dimension Rd and with a large distance 2v >> 1 between the class means, such that d >> v4. Then PD[X |Y = y] = N (yv \u00b7 e1, Id), where e1 is a unit vector in Rd. For any v and d, we have DX \u2208 Dsg1 . For large values of v, we have extremely low margin error at \u03b3 = v/2, and so we can hope to learn the classes by looking for a large-margin separator. Indeed, we can calculate k\u03b3 = \u2308d/(1 + v 2\n4 )\u2309, and conclude that the sample complexity required is \u0398\u0303(d/v2). Now consider a generative approach: fitting a spherical Gaussian model for each class. This amounts to estimating each class center as the empirical average of the points in the class, and classifying based on the nearest estimated class center. It is possible to show that for any constant \u01eb > 0, and for large enough v and d, O(d/v4) samples are enough in order to ensure an error of \u01eb. This establishes a rather large gap of \u2126(v2) between the sample complexity of the discriminative approach and that of the generative one.\nTo summarize, we have shown that the true sample complexity of large-margin learning of a rich family of specific distributions is characterized by the \u03b3-adapted-dimension. This result allows true comparison between this learning algorithm and other algorithms, and has various applications, such as semi-supervised learning and feature construction. The challenge of characterizing true sample complexity extends to any distribution and any learning algorithm. We believe that obtaining answers to these questions is of great importance, both to learning theory and to learning applications."}, {"heading": "Acknowledgments", "text": "The authors thank Boaz Nadler for many insightful discussions, and Karthik Sridharan for pointing out [14] to us. Sivan Sabato is supported by the Adams Fellowship Program of the Israel Academy of Sciences and Humanities. This work was supported by the NATO SfP grant 982480."}, {"heading": "A Proofs for \u201cTight Sample Complexity of Large-Margin Learning\u201d (S. Sabato, N. Srebro and N. Tishby)", "text": ""}, {"heading": "A.1 Proof of Lemma 3.3", "text": "Proof. The inequality k\u03b3 \u2264 k\u03b1\u03b3 is trivial from the definition of k\u03b3 . For the other inequality, note first that we can always let EX\u223cDX [XX\n\u2032] be diagonal by rotating the axes w.l.o.g. . Therefore k\u03b3 = min{k | \u2211di=k+1 \u03bbi \u2264 \u03b32k}. Since k\u03b3 \u2264 k\u03b1\u03b3 , we have \u03b32k\u03b3 \u2265 \u2211d i=k\u03b3+1 \u03bbi \u2265 \u2211d i=k\u03b1\u03b3+1 \u03bbi. In addition, by the minimality of k\u03b1\u03b3 , \u2211d\nk\u03b1\u03b3 \u03bbi > \u03b1\n2\u03b32(k\u03b1\u03b3 \u2212 1). Thus \u2211d\ni=k\u03b1\u03b3+1 \u03bbi > \u03b1 2\u03b32(k\u03b1\u03b3 \u2212 1)\u2212\u03bbk\u03b1\u03b3 . Combining the inequalities we get \u03b32k\u03b3 > \u03b12\u03b32(k\u03b1\u03b3\u22121)\u2212\u03bbk\u03b1\u03b3 . In addition, if k\u03b3 < k\u03b1\u03b3 then \u03b32k\u03b3 \u2265 \u2211d i=k\u03b1\u03b3 \u03bbi \u2265 \u03bbk\u03b1\u03b3 . Thus, either k\u03b3 = k\u03b1\u03b3 or 2\u03b32k\u03b3 > \u03b12\u03b32(k\u03b1\u03b3 \u2212 1).\nA.2 Details omitted from the proof of Theorem 4.2\nThe proof of Theorem 4.2 is complete except for the construction of X\u0303 and P\u0303 in the first paragraph, which is disclosed here in full, using the following lemma:\nLemma A.1. Let S = (X1, . . . , Xm) be a sequence of elements in Rd, and let X be a m \u00d7 d matrix whose rows are the elements of S. If S is \u03b3-shattered, then for every \u01eb > 0 there is a column vector r \u2208 Rd such that for every y \u2208 {\u00b1\u03b3}m there is a wy \u2208 Bd+11+\u01eb such that X\u0303wy = y, where X\u0303 = (X r).\nProof. if S is \u03b3-shattered then there exists a vector r \u2208 Rd, such that for all y \u2208 {\u00b11}m there exists wy \u2208 Bd1 such that for all i \u2208 [m], yi(\u3008Xi, wy\u3009\u2212ri) \u2265 \u03b3. For \u01eb > 0 define w\u0303y = (wy , \u221a \u01eb) \u2208 B1+\u01eb, and r\u0303 = r/ \u221a \u01eb, and let X\u0303 = (X r\u0303). For every y \u2208 {\u00b11}m there is a vector ty \u2208 Rm such that \u2200i \u2208 [m], 1\u03b3 ty[i]y[i] \u2265 1, and 1\u03b3 X\u0303w\u0303y = 1\u03b3 ty . As in the proof of necessity in Theorem 5.2, it follows that there exists w\u0302y \u2208 B1+\u01eb such that 1\u03b3 X\u0303w\u0302y = y. Scaling y by \u03b3, we get the claim of the theorem.\nNow, Let X be a m \u00d7 d matrix whose rows are a set of m points in Rd which is \u03b3-shattered. By Lemma A.1, for any \u01eb > 0 there exists matrix X\u0303 of dimensions m \u00d7 (d + 1) such that the first d columns of X\u0303 are the respective columns of X , and for all y \u2208 {pm\u03b3}m, there is a wy \u2208 Bd+11+\u01eb such that X\u0303wy = y. Since X is (B2, k)-limited, there exists an orthogonal projection matrix P of size d \u00d7 d and rank d \u2212 k such that \u2200i \u2208 [m], \u2016X \u2032iP\u20162 \u2264 B2. Let P\u0303 be the embedding of P in a (d + 1) \u00d7 (d + 1) zero matrix, so that P\u0303 is of the same rank and projects onto the same subspace. The rest of the proof follows as in the body of the paper."}, {"heading": "A.3 Proof of Theorem 4.4", "text": "Proof of Theorem 4.4. Let \u03a3 = diag(\u03bb1, . . . , \u03bbd) be the covariance matrix of DX , where \u2200i \u2208 [d\u2212 1], \u03bbi \u2265 \u03bbi\u22121. Define X\u03b1 = {x \u2208 Rd | \u2211d i=k\u03b3 (DX)+1 x[i]2 \u2264 \u03b1}.\nLet {xi}mi=1 be an i.i.d. sample of size m drawn from DX . We will select \u03b1 such that the probability that the whole sample is contained in X\u03b1 is large. P[\u2200i \u2208 [m], xi \u2208 X\u03b1] = (1 \u2212 P[xi /\u2208 X\u03b1])m. Let X \u223c DX . Then for all t > 0, P[X /\u2208 X\u03b1] = P[ \u2211d i=k\u03b3+1\nX [i]2 \u2265 \u03b1] \u2264 E[exp(t\n\u2211d i=k\u03b3+1 X [i]2)] exp(\u2212t\u03b1).\nLet \u03bbmax = \u03bbk\u03b3+1. Define Y \u2208 Rd such that Y [i] = X [i] \u221a \u03bbmax \u03bbi . Then \u2211d i=k\u03b3+1 X [i]2 = \u2211d i=k\u03b3+1 \u03bbi \u03bbmax Y [i]2, and by the definition of k\u03b3 , \u2211d i=k\u03b3+1 \u03bbi \u03bbmax \u2264 k\u03b3\u03bbmax . Thus, by Lemma A.2\nE[exp(t\nd\u2211\ni=k\u03b3+1\nX [i]2)] \u2264 max i (E[exp(3tY [i]2)])\u2308k\u03b3/\u03bbmax\u2309.\nFor every i, Y [i] is a sub-Gaussian random variable with moment B = \u03c1 \u221a \u03bbmax. By [12], Lemma 1.1.6, E[exp(3tY [i]2)] \u2264 (1\u2212 6\u03c12\u03bbmaxt)\u2212 12 , for t \u2208 (0, (6\u03c12\u03bbmax)\u22121). Setting t = 112\u03c12\u03bbmax ,\nP[X /\u2208 X\u03b1] \u2264 2k\u03b3/\u03bbmax exp(\u2212 \u03b1\n12\u03c12\u03bbmax ).\nThus there is a constant C such that for \u03b1(\u03b3) , C \u00b7 \u03c12(k\u03b3(DX) + \u03bbmax ln m\u03b4 ), P[X /\u2208 X\u03b1(\u03b3)] \u2264 1 \u2212 \u03b42m . Clearly, \u03bbmax \u2264 k\u03b3(DX), and k\u03b3(X\u03b1(\u03b3)) \u2264 \u03b1(\u03b3). Therefore, from Theorem 4.2, the \u03b3-fat-shattering dimension of W(X\u03b1(\u03b3)) is O(\u03c12k\u03b3(DX) ln m\u03b4 ). Define D\u03b3 to be the distribution such that PD\u03b3 [(X,Y )] = PDX [(X,Y ) | X \u2208 X\u03b1(\u03b3)]. By standard sample complexity bounds [16], for any distribution D over Rd \u00d7 {\u00b11}, with probability at least 1 \u2212 \u03b42 over samples, \u2113m(A, D) \u2264 O\u0303( \u221a F (\u03b3/8,D) ln 1\u03b4\nm ), where F (\u03b3,D) is the \u03b3-fat-shattering dimension of the class of linear functions\nwith domain restricted to the support of D in Rd. Consider D\u03b3/8. Since the support of D\u03b3/8 is X\u03b1(\u03b3/8), F (\u03b3/8, D\u03b3/8) \u2264 O(\u03c12k\u03b3/8(DX) ln m\u03b4 ). With probability 1 \u2212 \u03b4 over samples from DX , the sample is drawn from D\u03b3/8. In addition, the probability of the unlabeled example to be drawn from X\u03b1(\u03b3/8) is larger than 1\u2212 1m . Therefore \u2113m(A, D) \u2264 O\u0303( \u221a \u03c12k\u03b3/8(DX ) ln m \u03b4 m ). Setting \u03b4 = \u01eb/2 and bounding the expected error, we get m(\u01eb, \u03b3,D) \u2264 O\u0303(\u03c1 2k\u03b3/8(DX )\n\u01eb2 ). Lemma 3.3 allows replacing k\u03b3/8 with O(k\u03b3).\nLemma A.2. Let T1, . . . , Td be independent random variables such that all the moments E[T ni ] for all i are non-negative. Let \u03bb1, . . . , \u03bbd be real coefficients such that \u2211d i=1 \u03bbi = L, and \u03bbi \u2208 [0, 1] for all i \u2208 [d]. Then for all t \u2265 0\nE[exp(t\nd\u2211\ni=1\n\u03bbiTi)] \u2264 max i\u2208[d] (E[exp(3tTi)]) \u2308L\u2309.\nProof. Let Ti be independent random variables. Then, by Jensen\u2019s inequality,\nE[exp(t\nd\u2211\ni=1\n\u03bbiTi)] =\nd\u220f\ni=1\nE[exp(t\u03bbiTi)] \u2264 d\u220f\ni=1\nE[exp(tTi\nd\u2211\nj=1\n\u03bbj)]\n\u03bbi \u2211d\nj=1 \u03bbj \u2264 max\ni\u2208[d] E[exp(tTi\nd\u2211\nj=1\n\u03bbj)].\nNow, consider a partition Z1, . . . , Zk of [d], and denote Lj = \u2211\ni\u2208Zj \u03bbi. Then by the inequality\nabove,\nE[exp(t d\u2211\ni=1\n\u03bbiTi)] = k\u220f\nj=1\nE[exp(t \u2211\ni\u2208Zj\n\u03bbiTi)] \u2264 k\u220f\nj=1\nmax i\u2208Zj E[exp(tTiLj)].\nLet the partition be such that for all j \u2208 [k], Lj \u2264 1. There exists such a partition such that Lj < 12 for no more than one j. Therefore, for this partition L = \u2211d i=1 \u03bbi = \u2211 j\u2208[k] Lj \u2265 12 (k \u2212 1). Thus k \u2264 2L+ 1. Now, consider E[exp(tTiLj)] for some i and j. For any random variable X\nE[exp(tX)] =\n\u221e\u2211\nn=0\ntnE[Xn]\nn! .\nTherefore, E[exp(tTiLj)] = \u2211\u221e\nn=0\ntnLnj E[T n i ]\n(n)! . Since E[T n i ] \u2265 0 for all n, and Lj \u2264 1, it follows\nthat E[exp(tTiLj)] \u2264 E[exp(tTi)]. Thus\nE[exp(t\nd\u2211\ni=1\n\u03bbiTi)] \u2264 k\u220f\nj=1\nmax i\u2208Zj E[exp(tTi)] \u2264 max i\u2208[d] E[exp(t\nk\u2211\nj=1\nTi[j])],\nwhere Ti[j] are independent copies of Ti.\nIt is easy to see that E[exp[ 1a \u2211a i=1 Xi]] \u2264 E[exp[ 1b \u2211b\ni=1 Xi]], for a \u2265 b and X1, . . . , Xa i.i.d. random variables. Since k \u2265 \u2308L\u2309 it follows that\nE[exp(t d\u2211\ni=1\n\u03bbiTi)] \u2264 max i\u2208[d]\nE[exp(t k\u2211\nj=1\nTi[j])] \u2264 max i\u2208[d]\nE[exp(t k\n\u2308L\u2309\n\u2308L\u2309\u2211\nj=1\nTi[j])].\nSince k \u2264 2L+ 1 and all the moments of Ti[j] are non-negative, it follows that\nE[exp(t d\u2211\ni=1\n\u03bbiTi)] \u2264 max i\u2208[d]\nE[exp(t(2 + 1 \u2308L\u2309 ) \u2308L\u2309\u2211\nj=1\nTi[j])]."}, {"heading": "A.4 Proof of Theorem 5.2", "text": "the following lemma, which allows converting the representation of the Gram-matrix to a different feature space while keeping the separation properties intact. For a matrix M , M+ denotes its pseudo-inverse. If (M \u2032M) is invertible then M+ = (B\u2032B)\u22121B\u2032.\nLemma A.3. Let X be an m\u00d7d matrix such that XX \u2032 is invertible, and Y such that XX \u2032 = Y Y \u2032. Let r \u2208 Rm be some real vector. If there exists a vector w\u0303 such that Y w\u0303 = r, then there exists a vector w such that Xw = r and \u2016w\u2016 = \u2016Pw\u0303\u2016, where P = Y \u2032Y \u2032+ = Y \u2032(Y Y \u2032)\u22121Y is the projection matrix onto the sub-space spanned by the rows of Y .\nProof. Denote K = XX \u2032 = Y Y \u2032. Set T = Y \u2032X \u2032+ = Y \u2032K\u22121X . Set w = T \u2032w\u0303. We have Xw = XT \u2032w\u0303 = XX \u2032K\u22121Y w\u0303 = Y w\u0303 = r. In addition, \u2016w\u2016 = w\u2032w = w\u0303\u2032TT \u2032w\u0303. By definition of T , TT \u2032 = Y \u2032X \u2032+X+Y = Y \u2032K+Y = Y \u2032K\u22121Y = Y \u2032(Y Y \u2032)\u22121Y = Y \u2032Y \u2032+ = P. Since P is a projection matrix, we have P 2 = P . In addition, P = P \u2032. Therefore TT \u2032 = PP \u2032, and so \u2016w\u2016 = w\u0303\u2032PP \u2032w\u0303 = \u2016Pw\u0303\u2016.\nThe next lemma will allow us to prove that if a set is shattered at the origin, it can be separated with the exact margin.\nLemma A.4. Let R = {ry \u2208 Rm | y \u2208 {\u00b11}m} such that for all y \u2208 {\u00b11}m and for all i \u2208 [m], ry[i]y[i] \u2265 1. Then \u2200y \u2208 {\u00b11}m, y \u2208 conv(R).\nProof. We will prove the claim by induction on the dimension m.\nInduction base: For m = 1, we have R = {(a), (b)} where a \u2264 \u22121 and b \u2265 1. Clearly, convR = [a, b], and the two one-dimensional vectors (+1) and (\u22121) are in [a, b]. Induction step: For a vector t = (t[1], . . . , t[m]) \u2208 Rm, denote by t\u0304 its projection (t[1], . . . , t[m\u2212 1]) on Rm\u22121. Similarly, for a set of vectors S \u2286 Rm, let S\u0304 = {s\u0304 | s \u2208 S} \u2286 Rm\u22121. Define\nY+ = {y \u2208 {\u00b11}m | y[m] = +1} Y\u2212 = {y \u2208 {\u00b11}m | y[m] = \u22121}.\nLet R+ = {ry | y \u2208 Y+}, and similarly for R\u2212. Then R\u0304+ and R\u0304\u2212 satisfy the assumptions for R when m\u2212 1 is substituted for m. Let y\u2217 \u2208 {\u00b11}m. We wish to prove y\u2217 \u2208 conv(R). From the induction hypothesis we have y\u0304\u2217 \u2208 conv(R\u0304+) and y\u0304\u2217 \u2208 conv(R\u0304\u2212). Thus\ny\u0304\u2217 = \u2211\ny\u2208Y+\n\u03b1y r\u0304y = \u2211\ny\u2208Y \u2212\n\u03b2y r\u0304y ,\nwhere \u03b1y, \u03b2y \u2265 0, \u2211\ny\u2208Y+ \u03b1y = 1, and \u2211 y\u2208Y \u2212 \u03b2y = 1. Let y\u2217a = \u2211 y\u2208Y+ \u03b1yry and y\u2217b =\u2211\ny\u2208Y \u2212 \u03b1yry . We have that \u2200y \u2208 Y+, ry[m] \u2265 1, and \u2200y \u2208 Y\u2212, ry[m] \u2264 \u22121. Therefore, y\u2217a[m] \u2265 1 and y\u2217b [m] \u2264 \u22121. In addition, y\u0304\u2217a = y\u0304\u2217b = y\u0304. Hence there is \u03b3 \u2208 [0, 1] such that y\u2217 = \u03b3y\u2217a+(1\u2212\u03b3)y\u2217b . Since y\u2217a \u2208 conv(R+) and y\u2217b \u2208 conv(R\u2212), we have y\u2217 \u2208 conv(R).\nProof of Theorem 5.2. Let XX \u2032 = U\u039bU \u2032 be the SVD of XX \u2032, where U is an orthogonal matrix and \u039b is a diagonal matrix. Let Y = U\u039b 1 2 . We have XX \u2032 = Y Y \u2032. We show that the conditions are sufficient and necessary for the shattering of S.\nSufficient: Assume XX \u2032 is invertible. Then \u039b is invertible, thus Y is invertible. For any y \u2208 {\u00b11}m, Let w\u0303 = Y \u22121y. We have Y w\u0303 = y. In addition, \u2016w\u0303\u20162 = y\u2032(Y Y \u2032)\u22121y = y\u2032(XX \u2032)\u22121y \u2264 1. Therefore, by Lemma A.3, there exists a separator w such that Xw = y and \u2016w\u2016 = \u2016Pw\u0303\u2016 = \u2016w\u0303\u2016. Necessary: If XX \u2032 is not invertible then the vectors in S are linearly dependent, thus by standard VC-theory [16] S cannot be shattered using linear separators. The first condition is therefore necessary. We assume S is 1-shattered at the origin and show that the second condition necessarily holds. Let L = {r | \u2203w \u2208 Bd1, Xw = r}. Since S is shattered, For any y \u2208 {\u00b11}m there exists ry \u2208 L such that \u2200i \u2208 [m], ry[i]y[i] \u2265 1. By Lemma A.4, \u2200y \u2208 {\u00b11}m, y \u2208 conv(R) where R = {ry | y \u2208 {\u00b11}m}. Since L is convex and R \u2286 L, conv(R) \u2286 L. Thus for all y \u2208 {\u00b11}m, y \u2208 L, that is there exists wy \u2208 Rm such that Xwy = y and \u2016wy\u2016 \u2264 1. From Lemma A.3 we thus have w\u0303y such that Y w\u0303y = y and \u2016w\u0303y\u2016 = \u2016Pwy\u2016 \u2264 \u2016wy\u2016 \u2264 1. Y is invertible, hence w\u0303y = Y \u22121y. Thus y\u2032(XX \u2032)\u22121y = y\u2032(Y Y \u2032)\u22121y = \u2016w\u0303y\u2016 \u2264 1."}, {"heading": "A.5 Proof of Theorem 6.2", "text": "In the proof of Theorem 6.2 we use the fact \u03bbm(XX \u2032) = inf\u2016x\u20162=1 \u2016X \u2032x\u20162 and bound the righthand side via an \u01eb-net of the unit sphere in Rm, denoted by Sm\u22121 , {x \u2208 Rm | \u2016x\u20162 = 1}. An \u01eb-net of the unit sphere is a set C \u2286 Sm\u22121 such that \u2200x \u2208 Sm\u22121, \u2203x\u2032 \u2208 C, \u2016x \u2212 x\u2032\u2016 \u2264 \u01eb. Denote the minimal size of an \u01eb-net for Sm\u22121 by Nm(\u01eb), and by Cm(\u01eb) a minimal \u01eb-net of Sm\u22121, so that Cm(\u01eb) \u2286 Sm\u22121 and |Cm(\u01eb)| = Nm(\u01eb). The proof of Theorem 6.2 requires several lemmas. First we prove a concentration result for the norm of a matrix defined by sub-Gaussian variables. Then we bound the probability that the squared norm of a vector is small.\nLemma A.5. Let Y be a d \u00d7 m matrix with m \u2264 d, such that Yij are independent sub-Gaussian variables with moment B. Let \u03a3 be a diagonal d \u00d7 d PSD matrix such that \u03a3 \u2264 I . Then for all t \u2265 0 and \u01eb \u2208 (0, 1),\nP[\u2016 \u221a \u03a3Y \u2016 \u2265 t] \u2264 Nm(\u01eb) exp(\ntr(\u03a3) 2 \u2212 t 2(1\u2212 \u01eb)2 4B2 ).\nProof. We have \u2016 \u221a \u03a3Y \u2016 \u2264 maxx\u2208Cm(\u01eb) \u2016 \u221a \u03a3Y x\u2016/(1\u2212 \u01eb), see for instance in [22]. Therefore,\nP[\u2016 \u221a \u03a3Y \u2016 \u2265 t] \u2264 \u2211\nx\u2208Cm(\u01eb)\nP[\u2016 \u221a \u03a3Y x\u2016 \u2265 (1\u2212 \u01eb)t]. (3)\nFix x \u2208 Cm(\u01eb). Let V = \u221a \u03a3Y x, and assume \u03a3 = diag(\u03bb1, . . . , \u03bbd). For u \u2208 Rd,\nE[exp(\u3008u, V \u3009)] = E[exp( \u2211\ni\u2208[d]\nui \u221a \u03bbi \u2211\nj\u2208[m]\nYijxj)] = \u220f\nj,i\nE[exp(ui \u221a \u03bbiYijxj)]\n\u2264 \u220f\nj,i\nexp(u2i\u03bbiB 2x2j/2) = exp(\nB2\n2\n\u2211\ni\u2208[d]\nu2i\u03bbi \u2211\nj\u2208[m]\nx2j )\n= exp( B2\n2\n\u2211\ni\u2208[d]\nu2i\u03bbi) = exp(\u3008B2\u03a3u, u\u3009/2).\nLet s = 1/(4B2). Since \u03a3 \u2264 I , we have s \u2264 1/(4B2maxi\u2208[d] \u03bbi). Therefore, by Lemma A.9 (see Section A.6), E[exp(s\u2016V \u20162)] \u2264 exp(2sB2tr(\u03a3)). By Chernoff\u2019s method, P[\u2016V \u20162 \u2265 z2] \u2264 E[exp(s\u2016V \u20162)]/ exp(sz2). Thus\nP[\u2016V \u20162 \u2265 z2] \u2264 exp(2sB2tr(\u03a3)\u2212 sz2) = exp( tr(\u03a3) 2 \u2212 z 2 4B2 ).\nSet z = t(1\u2212 \u01eb). Then for all x \u2208 Sm\u22121\nP[\u2016 \u221a \u03a3Y x\u2016 \u2265 t(1\u2212 \u01eb)] = P[\u2016V \u2016 \u2265 t(1\u2212 \u01eb)] \u2264 exp( tr(\u03a3)\n2 \u2212 t 2(1\u2212 \u01eb)2 4B2 ).\nTherefore, by Eq. (3),\nP[\u2016 \u221a \u03a3Y \u2016 \u2265 t] \u2264 Nm(\u01eb) exp(\ntr(\u03a3) 2 \u2212 t 2(1\u2212 \u01eb)2 4B2 ).\nLemma A.6. Let Y be a d\u00d7m matrix with m \u2264 d, such that Yij are independent centered random variables with variance 1 and fourth moments at most B. Let \u03a3 be a diagonal d\u00d7d PSD matrix such that \u03a3 \u2264 I . There exist \u03b1 > 0 and \u03b7 \u2208 (0, 1) that depend only on B such that for any x \u2208 Sm\u22121\nP[\u2016 \u221a \u03a3Y x\u20162 \u2264 \u03b1 \u00b7 (tr(\u03a3)\u2212 1)] \u2264 \u03b7tr(\u03a3).\nTo prove Lemma A.6 we require Lemma A.7 [20, Lemma 2.2] and Lemma A.8, which extends Lemma 2.6 in the same work.\nLemma A.7. Let T1, . . . , Tn be independent non-negative random variables. Assume that there are \u03b8 > 0 and \u00b5 \u2208 (0, 1) such that for any i, P[Ti \u2264 \u03b8] \u2264 \u00b5. There are \u03b1 > 0 and \u03b7 \u2208 (0, 1) that depend only on \u03b8 and \u00b5 such that P[ \u2211n i=1 Ti < \u03b1n] \u2264 \u03b7n. Lemma A.8. Let Y be a d \u00d7m matrix with m \u2264 d, such that the columns of Y are i.i.d. random vectors. Assume further that Yij are centered, and have a variance of 1 and a fourth moment at most B. Let \u03a3 be a diagonal d \u00d7 d PSD matrix. Then for all x \u2208 Sm\u22121, P[\u2016 \u221a \u03a3Y x\u2016 \u2264 \u221a tr(\u03a3)/2] \u2264 1\u2212 1/(196B).\nProof. Let x \u2208 Sm\u22121, and Ti = ( \u2211m j=1 Yijxj) 2. Let \u03bb1, . . . , \u03bbd be the values on the diagonal of \u03a3, and let T\u03a3 = \u2016 \u221a \u03a3Y x\u20162 = \u2211di=1 \u03bbiTi. First, since E[Yij ] = 0 and E[Yij ] = 1 for all i, j, we\nhave E[Ti] = \u2211 i\u2208[m] x 2 jE[Y 2 ij ] = \u2016x\u20162 = 1. Therefore E[T\u03a3] = tr(\u03a3). Second, since Yi1, . . . , Yim are independent and centered, we have [23, Lemma 6.3]\nE[T 2i ] = E[( \u2211\nj\u2208[m]\nYijxj) 4] \u2264 16E\u03c3[(\n\u2211\nj\u2208[m]\n\u03c3jYijxj) 4],\nwhere \u03c31, . . . , \u03c3m are independent uniform {\u00b11} variables. Now, by Khinchine\u2019s inequality [24], E\u03c3[( \u2211\nj\u2208[m]\n\u03c3jYijxj) 4] \u2264 3E[(\n\u2211\nj\u2208[m]\nY 2ijx 2 j )\n2] = 3 \u2211\nj,k\u2208[m]\nx2jx 2 kE[Y 2 ij ]E[Y 2 ik].\nNow E[Y 2ij ]E[Y 2 ik] \u2264 \u221a E[Y 4ij ]E[Y 4 ik] \u2264 B. Thus E[T 2i ] \u2264 48B \u2211 j,k\u2208[m] x 2 jx 2 k = 48B\u2016x\u20164 = 48B. Thus,\nE[T 2\u03a3] = E[(\nd\u2211\ni=1\n\u03bbiTi) 2] =\nd\u2211\ni,j=1\n\u03bbi\u03bbjE[TiTj]\n\u2264 d\u2211\ni,j=1\n\u03bbi\u03bbj \u221a E[T 2i ]E[T 2 j ] \u2264 48B( d\u2211\ni=1\n\u03bbi) 2 = 48B \u00b7 tr(\u03a3)2.\nBy the Paley-Zigmund inequality [25], for \u03b8 \u2208 [0, 1]\nP[T\u03a3 \u2265 \u03b8E[T\u03a3]] \u2265 (1 \u2212 \u03b8)2 E[T\u03a3]\n2 E[T 2\u03a3] \u2265 (1\u2212 \u03b8) 2 48B .\nTherefore, setting \u03b8 = 1/2, we get P[T\u03a3 \u2264 tr(\u03a3)/2] \u2264 1\u2212 1/(196B).\nProof of Lemma A.6. Let \u03bb1, . . . , \u03bbd \u2208 [0, 1] be the values on the diagonal of\u03a3. Consider a partition Z1, . . . , Zk of [d], and denote Lj = \u2211 i\u2208Zj\n\u03bbi. There exists such a partition such that for all j \u2208 [k], Lj \u2264 1, and for all j \u2208 [k \u2212 1], Lj > 12 . Let \u03a3[j] be the sub-matrix of \u03a3 that includes the rows and columns whose indexes are in Zj . Let Y [j] be the sub-matrix of Y that includes the rows in Zj . Denote Tj = \u2016 \u221a \u03a3[j]Y [j]x\u20162. Then\n\u2016 \u221a \u03a3Y x\u20162 = \u2211\nj\u2208[k]\n\u2211\ni\u2208Zj\n\u03bbi(\nm\u2211\nj=1\nYijxj) 2 =\n\u2211\nj\u2208[k]\nTj .\nWe have tr(\u03a3) = \u2211d i=1 \u03bbi \u2265 \u2211\nj\u2208[k\u22121] Lj \u2265 12 (k \u2212 1). In addition, Lj \u2264 1 for all j \u2208 [k]. Thus tr(\u03a3) \u2264 k \u2264 2tr(\u03a3) + 1. For all j \u2208 [k \u2212 1], Lj \u2265 12 , thus by Lemma A.8, P[Tj \u2264 1/4] \u2264 1 \u2212 1/(196B). Therefore, by Lemma A.7 there are \u03b1 > 0 and \u03b7 \u2208 (0, 1) that depend only on B such that\nP[\u2016 \u221a \u03a3Y x\u20162 < \u03b1 \u00b7 (tr(\u03a3)\u2212 1)] \u2264 P[\u2016 \u221a \u03a3Y x\u20162 < \u03b1(k \u2212 1)]\n= P[ \u2211\nj\u2208[k]\nTj < \u03b1(k \u2212 1)] \u2264 P[ \u2211\nj\u2208[k\u22121]\nTj < \u03b1(k \u2212 1)] \u2264 \u03b7k\u22121 \u2264 \u03b72tr(\u03a3).\nThe lemma follows by substituting \u03b7 for \u03b72.\nProof of Theorem 6.2. We have \u221a \u03bbm(XX \u2032) = inf\nx\u2208Sm\u22121 \u2016X \u2032x\u2016 \u2265 min x\u2208Cm(\u01eb) \u2016X \u2032x\u2016 \u2212 \u01eb\u2016X \u2032\u2016. (4)\nFor brevity, denote L = tr(\u03a3). Assume L \u2265 2. Let m \u2264 L \u00b7min(1, (c \u2212K\u01eb)2) where c,K, \u01eb are constants that will be set later such that c\u2212K\u01eb > 0. By Eq. (4)\nP[\u03bbm(XX \u2032) \u2264 m] \u2264 P[\u03bbm(XX \u2032) \u2264 (c\u2212K\u01eb)2L]\n\u2264 P[ min x\u2208Cm(\u01eb)\n\u2016X \u2032x\u2016 \u2212 \u01eb\u2016X \u2032\u2016 \u2264 (c\u2212K\u01eb) \u221a L] (5)\n\u2264 P[\u2016X \u2032\u2016 \u2265 K \u221a L] + P[ min\nx\u2208Cm(\u01eb) \u2016X \u2032x\u2016 \u2264 c\n\u221a L]. (6)\nThe last inequality holds since the inequality in line (5) implies at least one of the inequalities in line (6). We will now upper-bound each of the terms in line (6). We assume w.l.o.g. that \u03a3 is not singular (since zero rows and columns can be removed fromX without changing\u03bbm(XX \u2032)). Define Y , \u221a \u03a3\u22121X \u2032. Note that Yij are independent sub-Gaussian variables with (absolute) moment \u03c1. To bound the first term in line (6), note that by Lemma A.5, for any K > 0,\nP[\u2016X \u2032\u2016 \u2265 K \u221a L] = P[\u2016 \u221a \u03a3Y \u2016 \u2265 K \u221a L] \u2264 Nm( 1\n2 ) exp(L(\n1 2 \u2212 K\n2\n16\u03c12 )).\nBy [19], Proposition 2.1, for all \u01eb \u2208 [0, 1], Nn(\u01eb) \u2264 2m(1 + 2\u01eb )m\u22121. Therefore\nP[\u2016X \u2032\u2016 \u2265 K \u221a L] \u2264 2m5m\u22121 exp(L(1\n2 \u2212 K\n2\n16\u03c12 )).\nLet K2 = 16\u03c12(32 + ln(5) + ln(2/\u03b4)). Recall that by assumption m \u2264 L, and L \u2265 2. Therefore\nP[\u2016X \u2032\u2016 \u2265 K \u221a L] \u2264 2m5m\u22121 exp(\u2212L(1 + ln(5) + ln(2/\u03b4)))\n\u2264 2L5L\u22121 exp(\u2212L(1 + ln(5) + ln(2/\u03b4))). Since L \u2265 2, we have 2L exp(\u2212L) \u2264 1. Therefore\nP[\u2016X \u2032\u2016 \u2265 K \u221a L] \u2264 2L exp(\u2212L\u2212 ln(2/\u03b4)) \u2264 exp(\u2212 ln(2/\u03b4)) = \u03b4\n2 . (7)\nTo bound the second term in line (6), since Yij are sub-Gaussian with moment \u03c1, E[Y 4ij ] \u2264 5\u03c14 [12, Lemma 1.4]. Thus, by Lemma A.6, there are \u03b1 > 0 and \u03b7 \u2208 (0, 1) that depend only on \u03c1 such that for all x \u2208 Sm\u22121, P[\u2016 \u221a \u03a3Y x\u20162 \u2264 \u03b1(L \u2212 1)] \u2264 \u03b7L. Set c = \u221a \u03b1/2. Since L \u2265 2, we have c \u221a L \u2264 \u221a \u03b1(L \u2212 1). Thus\nP[ min x\u2208Cm(\u01eb)\n\u2016X \u2032x\u2016 \u2264 c \u221a L] \u2264 \u2211\nx\u2208Cm(\u01eb)\nP[\u2016X \u2032x\u2016 \u2264 c \u221a L]\n\u2264 \u2211\nx\u2208Cm(\u01eb)\nP[\u2016 \u221a \u03a3Y x\u2016 \u2264 \u221a \u03b1(L \u2212 1)] \u2264 Nm(\u01eb)\u03b7L.\nLet \u01eb = c/(2K), so that c \u2212 K\u01eb > 0. Let \u03b8 = min(12 , ln(1/\u03b7) 2 ln(1+2/\u01eb) ). Set L\u25e6 such that \u2200L \u2265 L\u25e6, L \u2265 2 ln(2/\u03b4)+2 ln(L)ln(1/\u03b7) . For L \u2265 L\u25e6 and m \u2264 \u03b8L \u2264 L/2,\nNm(\u01eb)\u03b7L \u2264 2m(1 + 2/\u01eb)m\u22121\u03b7L \u2264 L exp(L(\u03b8 ln(1 + 2/\u01eb)\u2212 ln(1/\u03b7))) = exp(ln(L) + L(\u03b8 ln(1 + 2/\u01eb)\u2212 ln(1/\u03b7)/2)\u2212 L ln(1/\u03b7)/2) \u2264 exp(L(\u03b8 ln(1 + 2/\u01eb)\u2212 ln(1/\u03b7)/2) + ln(\u03b4/2)) (8)\n\u2264 exp(ln(\u03b4/2)) = \u03b4 2 . (9)\nLine (8) follows from L \u2265 L\u25e6, and line (9) follows from \u03b8 ln(1 + 2/\u01eb) \u2212 ln(1/\u03b7)/2 \u2264 0. Set \u03b2 = min{(c \u2212 K\u01eb)2, 1, \u03b8}. Combining Eq. (6), Eq. (7) and Eq. (9) we have that if L \u2265 L\u0304 , max(L\u25e6, 2), then P[\u03bbm(XX \u2032) \u2264 m] \u2264 \u03b4 for all m \u2264 \u03b2L. Specifically, this holds for all L \u2265 0 and for all m \u2264 \u03b2(L \u2212 L\u0304). Letting C = \u03b2L\u0304 and substituting \u03b4 for 1 \u2212 \u03b4 we get the statement of the theorem."}, {"heading": "A.6 Lemma A.9", "text": "Lemma A.9. Let X \u2208 Rd be a random vector and let B be a PSD matrix such that for all u \u2208 Rd, E[exp(\u3008u, V \u3009)] \u2264 exp(\u3008Bu, u\u3009/2).\nThen for all t \u2208 (0, 14\u03bb1(B) ], E[exp(t\u2016X\u2016 2)] \u2264 exp(2t \u00b7 trace(B)).\nProof of Lemma A.9. It suffices to consider diagonal moment matrices: If B is not diagonal, let V \u2208 Rd\u00d7d be an orthogonal matrix such that V BV \u2032 is diagonal, and let Y = V X . We have E[exp(t\u2016Y \u20162)] = E[exp(t\u2016X\u20162)] and tr(V BV \u2032) = tr(B). In addition, for all u \u2208 Rd,\nE[exp(\u3008u, Y \u3009)] = E[exp(\u3008V \u2032u,X\u3009)] \u2264\nexp( 1 2 \u3008BV \u2032u, V \u2032u\u3009) = exp(1 2 \u3008V BV \u2032u, u\u3009).\nThus assume w.l.o.g. that B = diag(\u03bb1, . . . , \u03bbd) where \u03bb1 \u2265 . . . \u2265 \u03bbd \u2265 0. We have exp(t\u2016X\u20162) = \u220fi\u2208[d] exp(tX [i]2). In addition, for any t > 0 and x \u2208 R, 2 \u221a \u03a0t \u00b7 exp(tx2) = \u222b\u221e \u2212\u221e exp(sx\u2212 s24t )ds. Therefore, for any u \u2208 Rd,\n(2 \u221a \u03a0t)d \u00b7 E[exp(t\u2016X\u20162)] = E  \u220f\ni\u2208[d]\n\u222b \u221e\n\u2212\u221e\nexp(u[i]X [i]\u2212 u[i] 2\n4t )du[i]\n \n= E\n  \u222b \u221e\n\u2212\u221e\n. . .\n\u222b \u221e\n\u2212\u221e\n\u220f\ni\u2208[d]\nexp(u[i]X [i]\u2212 u[i] 2\n4t )du[i]\n \n= E\n  \u222b \u221e\n\u2212\u221e\n. . .\n\u222b \u221e\n\u2212\u221e\nexp(\u3008u,X\u3009 \u2212 \u2016u\u2016 2 4t ) \u220f\ni\u2208[d]\ndu[i]\n \n=\n\u222b \u221e\n\u2212\u221e\n. . .\n\u222b \u221e\n\u2212\u221e\nE[exp(\u3008u,X\u3009)] exp(\u2212\u2016u\u2016 2 4t ) \u220f\ni\u2208[d]\ndu[i]\nBy the sub-Gaussianity of X , the last expression is bounded by\n\u2264 \u222b \u221e\n\u2212\u221e\n. . .\n\u222b \u221e\n\u2212\u221e\nexp( 1 2 \u3008Bu, u\u3009 \u2212 \u2016u\u2016 2 4t ) \u220f\ni\u2208[d]\ndu[i]\n=\n\u222b \u221e\n\u2212\u221e\n. . .\n\u222b \u221e\n\u2212\u221e\n\u220f\ni\u2208[d]\nexp( \u03bbiu[i]\n2\n2 \u2212 u[i]\n2\n4t )du[i]\n= \u220f\ni\u2208[d]\n\u222b \u221e\n\u2212\u221e\nexp(u[i]2( \u03bbi 2 \u2212 1 4t ))du[i] = \u03a0d/2\n( \u220f\ni\u2208[d]\n( 1 4t \u2212 \u03bbi 2 ) )\u2212 1 2 .\nThe last equality follows from the fact that for any a > 0, \u222b\u221e \u2212\u221e exp(\u2212a \u00b7 s2)ds = \u221a \u03a0/a, and from\nthe assumption t \u2264 14\u03bb1 . We conclude that\nE[exp(t\u2016X\u20162)] \u2264 ( \u220f\ni\u2208[d]\n(1\u2212 2\u03bbit))\u2212 1 2 \u2264 exp(2t \u00b7\nd\u2211\ni=1\n\u03bbi) = exp(2t \u00b7 tr(B)),\nwhere the second inequality holds since \u2200x \u2208 [0, 1], (1\u2212 x/2)\u22121 \u2264 exp(x)."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We obtain a tight distribution-specific characterization of the sample complex-<lb>ity of large-margin classification with L2 regularization: We introduce the<lb>\u03b3-adapted-dimension, which is a simple function of the spectrum of a distribu-<lb>tion\u2019s covariance matrix, and show distribution-specific upper and lower bounds<lb>on the sample complexity, both governed by the \u03b3-adapted-dimension of the<lb>source distribution. We conclude that this new quantity tightly characterizes the<lb>true sample complexity of large-margin classification. The bounds hold for a rich<lb>family of sub-Gaussian distributions.", "creator": "LaTeX with hyperref package"}}}