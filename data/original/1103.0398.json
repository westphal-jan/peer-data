{"id": "1103.0398", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2011", "title": "Natural Language Processing (almost) from Scratch", "abstract": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.", "histories": [["v1", "Wed, 2 Mar 2011 11:34:50 GMT  (338kb,D)", "http://arxiv.org/abs/1103.0398v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["ronan collobert", "jason weston", "leon bottou", "michael karlen", "koray kavukcuoglu", "pavel kuksa"], "accepted": false, "id": "1103.0398"}, "pdf": {"name": "1103.0398.pdf", "metadata": {"source": "CRF", "title": "Natural Language Processing (almost) from Scratch", "authors": ["Ronan Collobert", "Jason Weston", "Michael Karlen", "Koray Kavukcuoglu"], "emails": ["ronan@collobert.com", "jweston@google.com", "leon@bottou.org", "michael.karlen@gmail.com", "koray@cs.nyu.edu", "pkuksa@cs.rutgers.edu"], "sections": [{"heading": null, "text": "ar Xi v\nKeywords: Natural Language Processing, Neural Networks"}, {"heading": "1. Introduction", "text": "Will a computer program ever be able to convert a piece of English text into a data structure that unambiguously and completely describes the meaning of the natural language text? Among numerous problems, no consensus has emerged about the form of such a data structure. Until such fundamental Artificial Intelligence problems are resolved, computer scientists must settle for reduced objectives: extracting simpler representations describing restricted aspects of the textual information.\nThese simpler representations are often motivated by specific applications, for instance, bag-of-words variants for information retrieval. These representations can also be motivated by our belief that they capture something more general about natural language. They can describe syntactic information (e.g. part-of-speech tagging, chunking, and parsing) or semantic information (e.g. word-sense disambiguation, semantic role labeling, named entity extraction, and anaphora resolution). Text corpora have been manually annotated with such data structures in order to compare the performance of various systems. The availability of standard benchmarks has stimulated research in Natural Language Processing (NLP) and\n\u2020. Koray Kavukcuoglu is also with New York University, New York, NY. \u2021. Pavel Kuksa is also with Rutgers University, New Brunswick, NJ.\nc\u00a92009 Ronan Collobert, Jason Weston, Le\u0301on Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuksa.\nar X\niv :1\n10 3.\n03 98\nv1 [\ncs .L\nG ]\nar Xi v\neffective systems have been designed for all these tasks. Such systems are often viewed as software components for constructing real-world NLP solutions.\nThe overwhelming majority of these state-of-the-art systems address a benchmark task by applying linear statistical models to ad-hoc features. In other words, the researchers themselves discover intermediate representations by engineering task-specific features. These features are often derived from the output of preexisting systems, leading to complex runtime dependencies. This approach is effective because researchers leverage a large body of linguistic knowledge. On the other hand, there is a great temptation to optimize the performance of a system for a specific benchmark. Although such performance improvements can be very useful in practice, they teach us little about the means to progress toward the broader goals of natural language understanding and the elusive goals of Artificial Intelligence.\nIn this contribution, we try to excel on multiple benchmarks while avoiding task-specific enginering. Instead we use a single learning system able to discover adequate internal representations. In fact we view the benchmarks as indirect measurements of the relevance of the internal representations discovered by the learning procedure, and we posit that these intermediate representations are more general than any of the benchmarks. Our desire to avoid task-specific engineered features led us to ignore a large body of linguistic knowledge. Instead we reach good performance levels in most of the tasks by transferring intermediate representations discovered on large unlabeled datasets. We call this approach \u201calmost from scratch\u201d to emphasize the reduced (but still important) reliance on a priori NLP knowledge.\nThe paper is organized as follows. Section 2 describes the benchmark tasks of interest. Section 3 describes the unified model and reports benchmark results obtained with supervised training. Section 4 leverages large unlabeled datasets (\u223c 852 million words) to train the model on a language modeling task. Performance improvements are then demonstrated by transferring the unsupervised internal representations into the supervised benchmark models. Section 5 investigates multitask supervised training. Section 6 then evaluates how much further improvement can be achieved by incorporating standard NLP task-specific engineering into our systems. Drifting away from our initial goals gives us the opportunity to construct an all-purpose tagger that is simultaneously accurate, practical, and fast. We then conclude with a short discussion section."}, {"heading": "2. The Benchmark Tasks", "text": "In this section, we briefly introduce four standard NLP tasks on which we will benchmark our architectures within this paper: Part-Of-Speech tagging (POS), chunking (CHUNK), Named Entity Recognition (NER) and Semantic Role Labeling (SRL). For each of them, we consider a standard experimental setup and give an overview of state-of-the-art systems on this setup. The experimental setups are summarized in Table 1, while state-of-the-art systems are reported in Table 2."}, {"heading": "2.1 Part-Of-Speech Tagging", "text": "POS aims at labeling each word with a unique tag that indicates its syntactic role, e.g. plural noun, adverb, . . . A standard benchmark setup is described in detail by Toutanova\nar\nXi\nv\nSystem Accuracy Shen et al. (2007) 97.33% Toutanova et al. (2003) 97.24% Gime\u0301nez and Ma\u0300rquez (2004) 97.16%\n(a) POS\nSystem F1 Shen and Sarkar (2005) 95.23% Sha and Pereira (2003) 94.29% Kudo and Matsumoto (2001) 93.91%\n(b) CHUNK\nSystem F1 Ando and Zhang (2005) 89.31% Florian et al. (2003) 88.76% Kudo and Matsumoto (2001) 88.31%\n(c) NER\nSystem F1 Koomen et al. (2005) 77.92% Pradhan et al. (2005) 77.30% Haghighi et al. (2005) 77.04%\n(d) SRL\nTable 2: State-of-the-art systems on four NLP tasks. Performance is reported in per-word accuracy for POS, and F1 score for CHUNK, NER and SRL. Systems in bold will be referred as benchmark systems in the rest of the paper (see text).\net al. (2003). Sections 0\u201318 of Wall Street Journal (WSJ) data are used for training, while sections 19\u201321 are for validation and sections 22\u201324 for testing.\nThe best POS classifiers are based on classifiers trained on windows of text, which are then fed to a bidirectional decoding algorithm during inference. Features include preceding and following tag context as well as multiple words (bigrams, trigrams. . . ) context, and handcrafted features to deal with unknown words. Toutanova et al. (2003), who use maximum entropy classifiers, and a bidirectional dependency network (Heckerman et al., 2001) at inference, reach 97.24% per-word accuracy. Gime\u0301nez and Ma\u0300rquez (2004) proposed a SVM approach also trained on text windows, with bidirectional inference achieved with two Viterbi decoders (left-to-right and right-to-left). They obtained 97.16% per-word accuracy. More recently, Shen et al. (2007) pushed the state-of-the-art up to 97.33%, with a new learning algorithm they call guided learning, also for bidirectional sequence classification.\nar Xi v"}, {"heading": "2.2 Chunking", "text": "Also called shallow parsing, chunking aims at labeling segments of a sentence with syntactic constituents such as noun or verb phrases (NP or VP). Each word is assigned only one unique tag, often encoded as a begin-chunk (e.g. B-NP) or inside-chunk tag (e.g. I-NP). Chunking is often evaluated using the CoNLL 2000 shared task1. Sections 15\u201318 of WSJ data are used for training and section 20 for testing. Validation is achieved by splitting the training set.\nKudoh and Matsumoto (2000) won the CoNLL 2000 challenge on chunking with a F1score of 93.48%. Their system was based on Support Vector Machines (SVMs). Each SVM was trained in a pairwise classification manner, and fed with a window around the word of interest containing POS and words as features, as well as surrounding tags. They perform dynamic programming at test time. Later, they improved their results up to 93.91% (Kudo and Matsumoto, 2001) using an ensemble of classifiers trained with different tagging conventions (see Section 3.2.3).\nSince then, a certain number of systems based on second-order random fields were reported (Sha and Pereira, 2003; McDonald et al., 2005; Sun et al., 2008), all reporting around 94.3% F1 score. These systems use features composed of words, POS tags, and tags.\nMore recently, Shen and Sarkar (2005) obtained 95.23% using a voting classifier scheme, where each classifier is trained on different tag representations2 (IOB, IOE, . . . ). They use POS features coming from an external tagger, as well carefully hand-crafted specialization features which again change the data representation by concatenating some (carefully chosen) chunk tags or some words with their POS representation. They then build trigrams over these features, which are finally passed through a Viterbi decoder a test time."}, {"heading": "2.3 Named Entity Recognition", "text": "NER labels atomic elements in the sentence into categories such as \u201cPERSON\u201d or \u201cLOCATION\u201d. As in the chunking task, each word is assigned a tag prefixed by an indicator of the beginning or the inside of an entity. The CoNLL 2003 setup3 is a NER benchmark dataset based on Reuters data. The contest provides training, validation and testing sets.\nFlorian et al. (2003) presented the best system at the NER CoNLL 2003 challenge, with 88.76% F1 score. They used a combination of various machine-learning classifiers. Features they picked included words, POS tags, CHUNK tags, prefixes and suffixes, a large gazetteer (not provided by the challenge), as well as the output of two other NER classifiers trained on richer datasets. Chieu (2003), the second best performer of CoNLL 2003 (88.31% F1), also used an external gazetteer (their performance goes down to 86.84% with no gazetteer) and several hand-chosen features.\nLater, Ando and Zhang (2005) reached 89.31% F1 with a semi-supervised approach. They trained jointly a linear model on NER with a linear model on two auxiliary unsupervised tasks. They also performed Viterbi decoding at test time. The unlabeled\n1. See http://www.cnts.ua.ac.be/conll2000/chunking. 2. See Table 3 for tagging scheme details. 3. See http://www.cnts.ua.ac.be/conll2003/ner.\nar Xi v\ncorpus was 27M words taken from Reuters. Features included words, POS tags, suffixes and prefixes or CHUNK tags, but overall were less specialized than CoNLL 2003 challengers."}, {"heading": "2.4 Semantic Role Labeling", "text": "SRL aims at giving a semantic role to a syntactic constituent of a sentence. In the PropBank (Palmer et al., 2005) formalism one assigns roles ARG0-5 to words that are arguments of a verb (or more technically, a predicate) in the sentence, e.g. the following sentence might be tagged \u201c[John]ARG0 [ate]REL [the apple]ARG1 \u201d, where \u201cate\u201d is the predicate. The precise arguments depend on a verb\u2019s frame and if there are multiple verbs in a sentence some words might have multiple tags. In addition to the ARG0-5 tags, there there are several modifier tags such as ARGM-LOC (locational) and ARGM-TMP (temporal) that operate in a similar way for all verbs. We picked CoNLL 20054 as our SRL benchmark. It takes sections 2\u201321 of WSJ data as training set, and section 24 as validation set. A test set composed of section 23 of WSJ concatenated with 3 sections from the Brown corpus is also provided by the challenge.\nState-of-the-art SRL systems consist of several stages: producing a parse tree, identifying which parse tree nodes represent the arguments of a given verb, and finally classifying these nodes to compute the corresponding SRL tags. This entails extracting numerous base features from the parse tree and feeding them into statistical models. Feature categories commonly used by these system include (Gildea and Jurafsky, 2002; Pradhan et al., 2004):\n\u2022 the parts of speech and syntactic labels of words and nodes in the tree; \u2022 the node\u2019s position (left or right) in relation to the verb; \u2022 the syntactic path to the verb in the parse tree; \u2022 whether a node in the parse tree is part of a noun or verb phrase; \u2022 the voice of the sentence: active or passive; \u2022 the node\u2019s head word; and \u2022 the verb sub-categorization.\nPradhan et al. (2004) take these base features and define additional features, notably the part-of-speech tag of the head word, the predicted named entity class of the argument, features providing word sense disambiguation for the verb (they add 25 variants of 12 new feature types overall). This system is close to the state-of-the-art in performance. Pradhan et al. (2005) obtain 77.30% F1 with a system based on SVM classifiers and simultaneously using the two parse trees provided for the SRL task. In the same spirit, Haghighi et al. (2005) use log-linear models on each tree node, re-ranked globally with a dynamic algorithm. Their system reaches 77.04% using the five top Charniak parse trees.\nKoomen et al. (2005) hold the state-of-the-art with Winnow-like (Littlestone, 1988) classifiers, followed by a decoding stage based on an integer program that enforces specific constraints on SRL tags. They reach 77.92% F1 on CoNLL 2005, thanks to the five top parse trees produced by the Charniak (2000) parser (only the first one was provided by the contest) as well as the Collins (1999) parse tree.\n4. See http://www.lsi.upc.edu/~srlconll.\nar Xi v"}, {"heading": "2.5 Evaluation", "text": "In our experiments, we strictly followed the standard evaluation procedure of each CoNLL challenges for NER, CHUNK and SRL. All these three tasks are evaluated by computing the F1 scores over chunks produced by our models. The POS task is evaluated by computing the per-word accuracy, as it is the case for the standard benchmark we refer to (Toutanova et al., 2003). We picked the conlleval script5 for evaluating POS6, NER and CHUNK. For SRL, we used the srl-eval.pl script included in the srlconll package7."}, {"heading": "2.6 Discussion", "text": "When participating in an (open) challenge, it is legitimate to increase generalization by all means. It is thus not surprising to see many top CoNLL systems using external labeled data, like additional NER classifiers for the NER architecture of Florian et al. (2003) or additional parse trees for SRL systems (Koomen et al., 2005). Combining multiple systems or tweaking carefully features is also a common approach, like in the chunking top system (Shen and Sarkar, 2005).\nHowever, when comparing systems, we do not learn anything of the quality of each system if they were trained with different labeled data. For that reason, we will refer to benchmark systems, that is, top existing systems which avoid usage of external data and have been well-established in the NLP field: (Toutanova et al., 2003) for POS and (Sha and Pereira, 2003) for chunking. For NER we consider (Ando and Zhang, 2005) as they were using additional unlabeled data only. We picked (Koomen et al., 2005) for SRL, keeping in mind they use 4 additional parse trees not provided by the challenge. These benchmark systems will serve as baseline references in our experiments. We marked them in bold in Table 2.\nWe note that for the four tasks we are considering in this work, it can be seen that for the more complex tasks (with corresponding lower accuracies), the best systems proposed have more engineered features relative to the best systems on the simpler tasks. That is, the POS task is one of the simplest of our four tasks, and only has relatively few engineered features, whereas SRL is the most complex, and many kinds of features have been designed for it. This clearly has implications for as yet unsolved NLP tasks requiring more sophisticated semantic understanding than the ones considered here."}, {"heading": "3. The Networks", "text": "All the NLP tasks above can be seen as tasks assigning labels to words. The traditional NLP approach is: extract from the sentence a rich set of hand-designed features which are then fed to a standard classification algorithm, e.g. a Support Vector Machine (SVM), often with a linear kernel. The choice of features is a completely empirical process, mainly based first on linguistic intuition, and then trial and error, and the feature selection is task dependent, implying additional research for each new NLP task. Complex tasks like SRL then require a large number of possibly complex features (e.g., extracted from a parse tree) which can\n5. Available at http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt. 6. We used the \u201c-r\u201d option of the conlleval script to get the per-word accuracy, for POS only. 7. Available at http://www.lsi.upc.es/~srlconll/srlconll-1.1.tgz.\nar\nXi\nv\nInput Window\nLookup Table\nLinear\nHardTanh\nLinear\nText cat sat on the mat\nFeature 1 w11 w 1 2 . . . w 1 N\n...\nFeature K wK1 w K 2 . . . w K N\nLTW 1\n...\nLTWK\nM1 \u00d7 \u00b7\nM2 \u00d7 \u00b7\nword of interest\nd\nconcat\nn1hu\nn2hu = #tags\nFigure 1: Window approach network.\nimpact the computational cost which might be important for large-scale applications or applications requiring real-time response.\nInstead, we advocate a radically different approach: as input we will try to pre-process our features as little as possible and then use a multilayer neural network (NN) architecture, trained in an end-to-end fashion. The architecture takes the input sentence and learns several layers of feature extraction that process the inputs. The features computed by the deep layers of the network are automatically trained by backpropagation to be relevant to the task. We describe in this section a general multilayer architecture suitable for all our NLP tasks, which is generalizable to other NLP tasks as well.\nOur architecture is summarized in Figure 1 and Figure 2. The first layer extracts features for each word. The second layer extracts features from a window of words or from the whole sentence, treating it as a sequence with local and global structure (i.e., it is not treated like a bag of words). The following layers are standard NN layers.\nNotations We consider a neural network f\u03b8(\u00b7), with parameters \u03b8. Any feed-forward neural network with L layers, can be seen as a composition of functions f l\u03b8(\u00b7), corresponding to each layer l: f\u03b8(\u00b7) = fL\u03b8 (fL\u22121\u03b8 (. . . f1\u03b8 (\u00b7) . . .)) . In the following, we will describe each layer we use in our networks shown in Figure 1 and Figure 2. We adopt few notations. Given a matrix A we denote [A]i, j the coefficient\nat row i and column j in the matrix. We also denote \u3008A\u3009dwini the vector obtained by concatenating the dwin column vectors around the i\nth column vector of matrix A \u2208 Rd1\u00d7d2 :[ \u3008A\u3009dwini ]T = ( [A]1, i\u2212dwin/2 . . . [A]d1, i\u2212dwin/2 , . . . , [A]1, i+dwin/2 . . . [A]d1, i+dwin/2 ) .\nAs a special case, \u3008A\u30091i represents the ith column of matrix A. For a vector v, we denote [v]i the scalar at index i in the vector. Finally, a sequence of element {x1, x2, . . . , xT } is written [x]T1 . The i th element of the sequence is [x]i.\nar Xi v"}, {"heading": "3.1 Transforming Words into Feature Vectors", "text": "One of the essential key points of our architecture is its ability to perform well with the use of (almost8) raw words. The ability for our method to learn good word representations is thus crucial to our approach. For efficiency, words are fed to our architecture as indices taken from a finite dictionary D. Obviously, a simple index does not carry much useful information about the word. However, the first layer of our network maps each of these word indices into a feature vector, by a lookup table operation. Given a task of interest, a relevant representation of each word is then given by the corresponding lookup table feature vector, which is trained by backpropagation.\nMore formally, for each word w \u2208 D, an internal dwrd-dimensional feature vector representation is given by the lookup table layer LTW (\u00b7):\nLTW (w) = \u3008W \u30091w ,\nwhere W \u2208 Rdwrd\u00d7|D| is a matrix of parameters to be learnt, \u3008W \u30091w \u2208 Rdwrd is the wth column of W and dwrd is the word vector size (a hyper-parameter to be chosen by the user). Given a sentence or any sequence of T words [w]T1 in D, the lookup table layer applies the same operation for each word in the sequence, producing the following output matrix:\nLTW ([w] T 1 ) = ( \u3008W \u30091[w]1 \u3008W \u3009 1 [w]2 . . . \u3008W \u30091[w]T ) . (1)\nThis matrix can then be fed to further neural network layers, as we will see below."}, {"heading": "3.1.1 Extending to Any Discrete Features", "text": "One might want to provide features other than words if one suspects that these features are helpful for the task of interest. For example, for the NER task, one could provide a feature which says if a word is in a gazetteer or not. Another common practice is to introduce some basic pre-processing, such as word-stemming or dealing with upper and lower case. In this latter option, the word would be then represented by three discrete features: its lower case stemmed root, its lower case ending, and a capitalization feature.\nGenerally speaking, we can consider a word as represented by K discrete features w \u2208 D1\u00d7\u00b7 \u00b7 \u00b7\u00d7DK , where Dk is the dictionary for the kth feature. We associate to each feature a lookup table LTWk(\u00b7), with parameters W k \u2208 Rd k wrd\u00d7|D\nk| where dkwrd \u2208 N is a user-specified vector size. Given a word w, a feature vector of dimension dwrd = \u2211 k d k wrd is then obtained by concatenating all lookup table outputs:\nLTW 1,...,WK (w) =  LTW 1(w1)... LTWK (wK)  =  \u3008W 1\u30091w1 ... \u3008WK\u30091wK  . 8. We did some pre-processing, namely lowercasing and encoding capitalization as another feature. With\nenough (unlabeled) training data, presumably we could learn a model without this processing. Ideally, an even more raw input would be to learn from letter sequences rather than words, however we felt that this was beyond the scope of this work.\nar Xi v\nThe matrix output of the lookup table layer for a sequence of words [w]T1 is then similar to (1), but where extra rows have been added for each discrete feature:\nLTW 1,...,WK ([w] T 1 ) =  \u3008W 1\u30091[w1]1 . . . \u3008W 1\u30091[w1]T ...\n... \u3008WK\u30091[wK ]1 . . . \u3008W K\u30091[wK ]T  . (2) These vector features in the lookup table effectively learn features for words in the dictionary. Now, we want to use these trainable features as input to further layers of trainable feature extractors, that can represent groups of words and then finally sentences."}, {"heading": "3.2 Extracting Higher Level Features from Word Feature Vectors", "text": "Feature vectors produced by the lookup table layer need to be combined in subsequent layers of the neural network to produce a tag decision for each word in the sentence. Producing tags for each element in variable length sequences (here, a sentence is a sequence of words) is a standard problem in machine-learning. We consider two common approaches which tag one word at the time: a window approach, and a (convolutional) sentence approach."}, {"heading": "3.2.1 Window Approach", "text": "A window approach assumes the tag of a word depends mainly on its neighboring words. Given a word to tag, we consider a fixed size ksz (a hyper-parameter) window of words around this word. Each word in the window is first passed through the lookup table layer (1) or (2), producing a matrix of word features of fixed size dwrd \u00d7 ksz. This matrix can be viewed as a dwrd ksz-dimensional vector by concatenating each column vector, which can be fed to further neural network layers. More formally, the word feature window given by the first network layer can be written as:\nf1\u03b8 = \u3008LTW ([w]T1 )\u3009dwint =  \u3008W \u30091[w] t\u2212dwin/2 ... \u3008W \u30091[w]t ...\n\u3008W \u30091[w] t+dwin/2\n . (3)\nLinear Layer The fixed size vector f1\u03b8 can be fed to one or several standard neural network layers which perform affine transformations over their inputs:\nf l\u03b8 = W l f l\u22121\u03b8 + b l , (4)\nwhere W l \u2208 Rnlhu\u00d7nl\u22121hu and bl \u2208 Rnlhu are the parameters to be trained. The hyper-parameter nlhu is usually called the number of hidden units of the l th layer.\nHardTanh Layer Several linear layers are often stacked, interleaved with a non-linearity function, to extract highly non-linear features. If no non-linearity is introduced, our network\nar Xi v\nwould be a simple linear model. We chose a \u201chard\u201d version of the hyperbolic tangent as nonlinearity. It has the advantage of being slightly cheaper to compute (compared to the exact hyperbolic tangent), while leaving the generalization performance unchanged (Collobert, 2004). The corresponding layer l applies a HardTanh over its input vector:[\nf l\u03b8 ] i = HardTanh( [ f l\u22121\u03b8 ] i ) ,\nwhere\nHardTanh(x) =  \u22121 if x < \u22121 x if \u2212 1 <= x <= 1 1 if x > 1 . (5)\nScoring Finally, the output size of the last layer L of our network is equal to the number of possible tags for the task of interest. Each output can be then interpreted as a score of the corresponding tag (given the input of the network), thanks to a carefully chosen cost function that we will describe later in this section.\nRemark 1 (Border Effects) The feature window (3) is not well defined for words near the beginning or the end of a sentence. To circumvent this problem, we augment the sentence with a special \u201cPADDING\u201d word replicated dwin/2 times at the beginning and the end. This is akin to the use of \u201cstart\u201d and \u201cstop\u201d symbols in sequence models."}, {"heading": "3.2.2 Sentence Approach", "text": "We will see in the experimental section that a window approach performs well for most natural language processing tasks we are interested in. However this approach fails with SRL, where the tag of a word depends on a verb (or, more correctly, predicate) chosen beforehand in the sentence. If the verb falls outside the window, one cannot expect this word to be tagged correctly. In this particular case, tagging a word requires the consideration of the whole sentence. When using neural networks, the natural choice to tackle this problem becomes a convolutional approach, first introduced by Waibel et al. (1989) and also called Time Delay Neural Networks (TDNNs) in the literature.\nWe describe in detail our convolutional network below. It successively takes the complete sentence, passes it through the lookup table layer (1), produces local features around each word of the sentence thanks to convolutional layers, combines these feature into a global feature vector which can then be fed to standard affine layers (4). In the semantic role labeling case, this operation is performed for each word in the sentence, and for each verb in the sentence. It is thus necessary to encode in the network architecture which verb we are considering in the sentence, and which word we want to tag. For that purpose, each word at position i in the sentence is augmented with two features in the way described in Section 3.1.1. These features encode the relative distances i \u2212 posv and i \u2212 posw with respect to the chosen verb at position posv, and the word to tag at position posw respectively.\nConvolutional Layer A convolutional layer can be seen as a generalization of a window approach: given a sequence represented by columns in a matrix f l\u22121\u03b8 (in our lookup table matrix (1)), a matrix-vector operation as in (4) is applied to each window of successive\nar Xi v\nwindows in the sequence. Using previous notations, the tth output column of the lth layer can be computed as:\n\u3008f l\u03b8\u30091t = W l \u3008f l\u22121\u03b8 \u3009 dwin t + b l \u2200t , (6) where the weight matrix W l is the same across all windows t in the sequence. Convolutional layers extract local features around each window of the given sequence. As for standard affine layers (4), convolutional layers are often stacked to extract higher level features. In this case, each layer must be followed by a non-linearity (5) or the network would be equivalent to one convolutional layer.\nMax Layer The size of the output (6) depends on the number of words in the sentence fed to the network. Local feature vectors extracted by the convolutional layers have to be combined to obtain a global feature vector, with a fixed size independent of the sentence length, in order to apply subsequent standard affine layers. Traditional convolutional networks often apply an average (possibly weighted) or a max operation over the \u201ctime\u201d t of the sequence (6). (Here, \u201ctime\u201d just means the position in the sentence, this term stems from the use of convolutional layers in e.g. speech data where the sequence occurs over time.) The average operation does not make much sense in our case, as in general most words in the sentence do not have any influence on the semantic role of a given word to tag. Instead, we used a max approach, which forces the network to capture the most useful local features produced by the convolutional layers (see Figure 3), for the task at hand. Given a matrix f l\u22121\u03b8 output by a convolutional layer l \u2212 1, the Max layer l outputs a vector f l\u03b8:[\nf l\u03b8 ] i = max t [ f l\u22121\u03b8 ] i, t\n1 \u2264 i \u2264 nl\u22121hu . (7)\nThis fixed sized global feature vector can be then fed to standard affine network layers (4). As in the window approach, we then finally produce one score per possible tag for the given task.\nar Xi v\nRemark 2 The same border effects arise in the convolution operation (6) as in the window approach (3). We again work around this problem by padding the sentences with a special word."}, {"heading": "3.2.3 Tagging Schemes", "text": "As explained earlier, the network output layers compute scores for all the possible tags for the task of interest. In the window approach, these tags apply to the word located in the center of the window. In the (convolutional) sentence approach, these tags apply to the word designated by additional markers in the network input.\nThe POS task indeed consists of marking the syntactic role of each word. However, the remaining three tasks associate labels with segments of a sentence. This is usually achieved by using special tagging schemes to identify the segment boundaries, as shown in Table 3. Several such schemes have been defined (IOB, IOE, IOBES, . . . ) without clear conclusion as to which scheme is better in general. State-of-the-art performance is sometimes obtained by combining classifiers trained with different tagging schemes (e.g. Kudo and Matsumoto, 2001).\nThe ground truth for the NER, CHUNK, and SRL tasks is provided using two different tagging schemes. In order to eliminate this additional source of variations, we have decided to use the most expressive IOBES tagging scheme for all tasks. For instance, in the CHUNK task, we describe noun phrases using four different tags. Tag \u201cS-NP\u201d is used to mark a noun phrase containing a single word. Otherwise tags \u201cB-NP\u201d, \u201cI-NP\u201d, and \u201cE-NP\u201d are used to mark the first, intermediate and last words of the noun phrase. An additional tag \u201cO\u201d marks words that are not members of a chunk. During testing, these tags are then converted to the original IOB tagging scheme and fed to the standard performance evaluation scripts mentioned in Section 2.5."}, {"heading": "3.3 Training", "text": "All our neural networks are trained by maximizing a likelihood over the training data, using stochastic gradient ascent. If we denote \u03b8 to be all the trainable parameters of the network, which are trained using a training set T we want to maximize the following log-likelihood\nar Xi v\nwith respect to \u03b8:\n\u03b8 7\u2192 \u2211\n(x, y)\u2208T\nlog p(y |x, \u03b8) , (8)\nwhere x corresponds to either a training word window or a sentence and its associated features, and y represents the corresponding tag. The probability p(\u00b7) is computed from the outputs of the neural network. We will see in this section two ways of interpreting neural network outputs as probabilities."}, {"heading": "3.3.1 Word-Level Log-Likelihood", "text": "In this approach, each word in a sentence is considered independently. Given an input example x, the network with parameters \u03b8 outputs a score [f\u03b8(x)]i, for the i\nth tag with respect to the task of interest. To simplify the notation, we drop x from now, and we write instead [f\u03b8]i. This score can be interpreted as a conditional tag probability p(i |x, \u03b8) by applying a softmax (Bridle, 1990) operation over all the tags:\np(i |x, \u03b8) = e [f\u03b8]i\u2211 j e [f\u03b8]j . (9)\nDefining the log-add operation as\nlogadd i zi = log( \u2211 i ezi) , (10)\nwe can express the log-likelihood for one training example (x, y) as follows:\nlog p(y |x, \u03b8) = [f\u03b8]y \u2212 logadd j [f\u03b8]j . (11)\nWhile this training criterion, often referred as cross-entropy is widely used for classification problems, it might not be ideal in our case, where there is often a correlation between the tag of a word in a sentence and its neighboring tags. We now describe another common approach for neural networks which enforces dependencies between the predicted tags in a sentence."}, {"heading": "3.3.2 Sentence-Level Log-Likelihood", "text": "In tasks like chunking, NER or SRL we know that there are dependencies between word tags in a sentence: not only are tags organized in chunks, but some tags cannot follow other tags. Training using a word-level approach discards this kind of labeling information. We consider a training scheme which takes into account the sentence structure: given the predictions of all tags by our network for all words in a sentence, and given a score for going from one tag to another tag, we want to encourage valid paths of tags during training, while discouraging all other paths.\nWe consider the matrix of scores f\u03b8([x] T 1 ) output by the network. As before, we drop the input [x]T1 for notation simplification. The element [f\u03b8]i, t of the matrix is the score output by the network with parameters \u03b8, for the sentence [x]T1 and for the i th tag, at the tth word.\nar Xi v\nWe introduce a transition score [A]i, j for jumping from i to j tags in successive words, and an initial score [A]i, 0 for starting from the i th tag. As the transition scores are going to be trained (as are all network parameters \u03b8), we define \u03b8\u0303 = \u03b8 \u222a {[A]i, j \u2200i, j}. The score of a sentence [x]T1 along a path of tags [i] T 1 is then given by the sum of transition scores and network scores:\ns([x]T1 , [i] T 1 , \u03b8\u0303) = T\u2211 t=1 ( [A][i]t\u22121, [i]t + [f\u03b8][i]t, t ) . (12)\nExactly as for the word-level likelihood (11), where we were normalizing with respect to all tags using a softmax (9), we normalize this score over all possible tag paths [j]T1 using a softmax, and we interpret the resulting ratio as a conditional tag path probability. Taking the log, the conditional probability of the true path [y]T1 is therefore given by:\nlog p([y]T1 | [x]T1 , \u03b8\u0303) = s([x]T1 , [y]T1 , \u03b8\u0303)\u2212 logadd \u2200[j]T1 s([x]T1 , [j] T 1 , \u03b8\u0303) . (13)\nWhile the number of terms in the logadd operation (11) was equal to the number of tags, it grows exponentially with the length of the sentence in (13). Fortunately, one can compute it in linear time with the following standard recursion over t, taking advantage of the associativity and distributivity on the semi-ring9 (R \u222a {\u2212\u221e}, logadd, +):\n\u03b4t(k) \u2206 = logadd {[j]t1 \u2229 [j]t=k} s([x]t1, [j] t 1, \u03b8\u0303)\n= logadd i logadd {[j]t1 \u2229 [j]t\u22121=i\u2229 [j]t=k}\ns([x]t1, [j] t\u22121 1 , \u03b8\u0303) + [A][j]t\u22121, k + [f\u03b8]k, t\n= logadd i \u03b4t\u22121(i) + [A]i, k + [f\u03b8]k, t\n= [f\u03b8]k, t + logadd i\n( \u03b4t\u22121(i) + [A]i, k ) \u2200k ,\n(14)\nfollowed by the termination\nlogadd \u2200[j]T1\ns([x]T1 , [j] T 1 , \u03b8\u0303) = logadd\ni \u03b4T (i) . (15)\nWe can now maximize in (8) the log-likelihood (13) over all the training pairs ([x]T1 , [y] T 1 ).\nAt inference time, given a sentence [x]T1 to tag, we have to find the best tag path which minimizes the sentence score (12). In other words, we must find\nargmax [j]T1\ns([x]T1 , [j] T 1 , \u03b8\u0303) . (16)\nThe Viterbi algorithm is the natural choice for this inference. It corresponds to performing the recursion (14) and (15), but where the logadd is replaced by a max, and then tracking back the optimal path through each max.\n9. In other words, read logadd as \u2295 and + as \u2297.\nar Xi v\nRemark 3 (Graph Transformer Networks) Our approach is a particular case of the discriminative forward training for graph transformer networks (GTNs) (Bottou et al., 1997; Le Cun et al., 1998). The log-likelihood (13) can be viewed as the difference between the forward score constrained over the valid paths (in our case there is only the labeled path) and the unconstrained forward score (15).\nRemark 4 (Conditional Random Fields) An important feature of equation (12) is the absence of normalization. Summing the exponentials e [f\u03b8]i, t over all possible tags does not necessarily yield the unity. If this was the case, the scores could be viewed as the logarithms of conditional transition probabilities, and our model would be subject to the label-bias problem that motivates Conditional Random Fields (CRFs) (Lafferty et al., 2001). The denormalized scores should instead be likened to the potential functions of a CRF. In fact, a CRF maximizes the same likelihood (13) using a linear model instead of a nonlinear neural network. CRFs have been widely used in the NLP world, such as for POS tagging (Lafferty et al., 2001), chunking (Sha and Pereira, 2003), NER (McCallum and Li, 2003) or SRL (Cohn and Blunsom, 2005). Compared to such CRFs, we take advantage of the nonlinear network to learn appropriate features for each task of interest."}, {"heading": "3.3.3 Stochastic Gradient", "text": "Maximizing (8) with stochastic gradient (Bottou, 1991) is achieved by iteratively selecting a random example (x, y) and making a gradient step:\n\u03b8 \u2190\u2212 \u03b8 + \u03bb \u2202 log p(y |x, \u03b8) \u2202\u03b8 , (17)\nwhere \u03bb is a chosen learning rate. Our neural networks described in Figure 1 and Figure 2 are a succession of layers that correspond to successive composition of functions. The neural network is finally composed with the word-level log-likelihood (11), or successively composed in the recursion (14) if using the sentence-level log-likelihood (13). Thus, an analytical formulation of the derivative (17) can be computed, by applying the differentiation chain rule through the network, and through the word-level log-likelihood (11) or through the recurrence (14).\nRemark 5 (Differentiability) Our cost functions are differentiable almost everywhere. Non-differentiable points arise because we use a \u201chard\u201d transfer function (5) and because we use a \u201cmax\u201d layer (7) in the sentence approach network. Fortunately, stochastic gradient still converges to a meaningful local minimum despite such minor differentiability problems (Bottou, 1991, 1998). Stochastic gradient iterations that hit a non-differentiability are simply skipped.\nRemark 6 (Modular Approach) The well known \u201cback-propagation\u201d algorithm (LeCun, 1985; Rumelhart et al., 1986) computes gradients using the chain rule. The chain rule can also be used in a modular implementation.10 Our modules correspond to the boxes in Figure 1 and Figure 2. Given derivatives with respect to its outputs, each module can independently\n10. See http://torch5.sf.net.\nar\nXi\nv\nTask Window/Conv. size Word dim. Caps dim. Hidden units Learning rate\nPOS dwin = 5 d 0 = 50 d1 = 5 n1hu = 300 \u03bb = 0.01\nCHUNK \u201d \u201d \u201d \u201d \u201d\nNER \u201d \u201d \u201d \u201d \u201d\nSRL \u201d \u201d \u201d n1hu = 300\nn2hu = 500 \u201d\nTable 5: Hyper-parameters of our networks. We report for each task the window size (or convolution size), word feature dimension, capital feature dimension, number of hidden units and learning rate.\ncompute derivatives with respect to its inputs and with respect to its trainable parameters, as proposed by Bottou and Gallinari (1991). This allows us to easily build variants of our networks. For details about gradient computations, see Appendix A.\nRemark 7 (Tricks) Many tricks have been reported for training neural networks (LeCun et al., 1998). Which ones to choose is often confusing. We employed only two of them: the initialization and update of the parameters of each network layer were done according to the \u201cfan-in\u201d of the layer, that is the number of inputs used to compute each output of this layer (Plaut and Hinton, 1987). The fan-in for the lookup table (1), the lth linear layer (4) and the convolution layer (6) are respectively 1, nl\u22121hu and dwin\u00d7nl\u22121hu . The initial parameters of the network were drawn from a centered uniform distribution, with a variance equal to the inverse of the square-root of the fan-in. The learning rate in (17) was divided by the fan-in, but stays fixed during the training."}, {"heading": "3.4 Supervised Benchmark Results", "text": "For POS, chunking and NER tasks, we report results with the window architecture described in Section 3.2.1. The SRL task was trained using the sentence approach (Section 3.2.2). Results are reported in Table 4, in per-word accuracy (PWA) for POS, and F1 score for all\nar Xi v\nthe other tasks. We performed experiments both with the word-level log-likelihood (WLL) and with the sentence-level log-likelihood (SLL). The hyper-parameters of our networks are reported in Table 5. All our networks were fed with two raw text features: lower case words, and a capital letter feature. We chose to consider lower case words to limit the number of words in the dictionary. However, to keep some upper case information lost by this transformation, we added a \u201ccaps\u201d feature which tells if each word was in low caps, was all caps, had first letter capital, or had one capital. Additionally, all occurrences of sequences of numbers within a word are replaced with the string \u201cNUMBER\u201d, so for example both the words \u201cPS1\u201d and \u201cPS2\u201d would map to the single word \u201cpsNUMBER\u201d. We used a dictionary containing the 100,000 most common words in WSJ (case insensitive). Words outside this dictionary were replaced by a single special \u201cRARE\u201d word.\nResults show that neural networks \u201cout-of-the-box\u201d are behind baseline benchmark systems. Looking at all submitted systems reported on each CoNLL challenge website showed us our networks performance are nevertheless in the performance ballpark of existing approaches. The training criterion which takes into account the sentence structure (SLL) seems to boost the performance for the Chunking, NER and SRL tasks, with little advantage for POS. This result is in line with existing NLP studies comparing sentence-level and wordlevel likelihoods (Liang et al., 2008). The capacity of our network architectures lies mainly in the word lookup table, which contains 50\u00d7100, 000 parameters to train. In the WSJ data, 15% of the most common words appear about 90% of the time. Many words appear only a few times. It is thus very difficult to train properly their corresponding 50 dimensional feature vectors in the lookup table. Ideally, we would like semantically similar words to be close in the embedding space represented by the word lookup table: by continuity of the neural network function, tags produced on semantically similar sentences would be similar. We show in Table 6 that it is not the case: neighboring words in the embedding space do not seem to be semantically related.\nar Xi v\nWe will focus in the next section on improving these word embeddings by leveraging unlabeled data. We will see our approach results in a performance boost for all tasks.\nRemark 8 (Architectures) In all our experiments in this paper, we tuned the hyperparameters by trying only a few different architectures by validation. In practice, the choice of hyperparameters such as the number of hidden units, provided they are large enough, has a limited impact on the generalization performance. In Figure 4, we report the F1 score for each task on the validation set, with respect to the number of hidden units. Considering the variance related to the network initialization, we chose the smallest network achieving \u201creasonable\u201d performance, rather than picking the network achieving the top performance obtained on a single run.\nRemark 9 (Training Time) Training our network is quite computationally expensive. Chunking and NER take about one hour to train, POS takes few hours, and SRL takes about three days. Training could be faster with a larger learning rate, but we prefered to stick to a small one which works, rather than finding the optimal one for speed. Second order methods (LeCun et al., 1998) could be another speedup technique."}, {"heading": "4. Lots of Unlabeled Data", "text": "We would like to obtain word embeddings carrying more syntactic and semantic information than shown in Table 6. Since most of the trainable parameters of our system are associated with the word embeddings, these poor results suggest that we should use considerably more training data. Following our NLP from scratch philosophy, we now describe how to dramatically improve these embeddings using large unlabeled datasets. We then use these improved embeddings to initialize the word lookup tables of the networks described in Section 3.4.\nar Xi v"}, {"heading": "4.1 Datasets", "text": "Our first English corpus is the entire English Wikipedia.11 We have removed all paragraphs containing non-roman characters and all MediaWiki markups. The resulting text was tokenized using the Penn Treebank tokenizer script.12 The resulting dataset contains about 631 million words. As in our previous experiments, we use a dictionary containing the 100,000 most common words in WSJ, with the same processing of capitals and numbers. Again, words outside the dictionary were replaced by the special \u201cRARE\u201d word.\nOur second English corpus is composed by adding an extra 221 million words extracted from the Reuters RCV1 (Lewis et al., 2004) dataset.13 We also extended the dictionary to 130, 000 words by adding the 30, 000 most common words in Reuters. This is useful in order to determine whether improvements can be achieved by further increasing the unlabeled dataset size."}, {"heading": "4.2 Ranking Criterion versus Entropy Criterion", "text": "We used these unlabeled datasets to train language models that compute scores describing the acceptability of a piece of text. These language models are again large neural networks using the window approach described in Section 3.2.1 and in Figure 1. As in the previous section, most of the trainable parameters are located in the lookup tables.\nSimilar language models were already proposed by Bengio and Ducharme (2001) and Schwenk and Gauvain (2002). Their goal was to estimate the probability of a word given the previous words in a sentence. Estimating conditional probabilities suggests a crossentropy criterion similar to those described in Section 3.3.1. Because the dictionary size is large, computing the normalization term can be extremely demanding, and sophisticated approximations are required. More importantly for us, neither work leads to significant word embeddings being reported.\nShannon (1951) has estimated the entropy of the English language between 0.6 and 1.3 bits per character by asking human subjects to guess upcoming characters. Cover and King (1978) give a lower bound of 1.25 bits per character using a subtle gambling approach. Meanwhile, using a simple word trigram model, Brown et al. (1992b) reach 1.75 bits per character. Teahan and Cleary (1996) obtain entropies as low as 1.46 bits per character using variable length character n-grams. The human subjects rely of course on all their knowledge of the language and of the world. Can we learn the grammatical structure of the English language and the nature of the world by leveraging the 0.2 bits per character that separate human subjects from simple n-gram models? Since such tasks certainly require high capacity models, obtaining sufficiently small confidence intervals on the test set entropy may require prohibitively large training sets.14 The entropy criterion lacks dynamical range because its numerical value is largely determined by the most frequent phrases. In order to learn syntax, rare but legal phrases are no less significant than common phrases.\n11. Available at http://download.wikimedia.org. We took the November 2007 version. 12. Available at http://www.cis.upenn.edu/~treebank/tokenization.html. 13. Now available at http://trec.nist.gov/data/reuters/reuters.html. 14. However, Klein and Manning (2002) describe a rare example of realistic unsupervised grammar induction\nusing a cross-entropy approach on binary-branching parsing trees, that is, by forcing the system to generate a hierarchical representation.\nar Xi v\nIt is therefore desirable to define alternative training criteria. We propose here to use a pairwise ranking approach (Cohen et al., 1998). We seek a network that computes a higher score when given a legal phrase than when given an incorrect phrase. Because the ranking literature often deals with information retrieval applications, many authors define complex ranking criteria that give more weight to the ordering of the best ranking instances (see Burges et al., 2007; Cle\u0301menc\u0327on and Vayatis, 2007). However, in our case, we do not want to emphasize the most common phrase over the rare but legal phrases. Therefore we use a simple pairwise criterion.\nWe consider a window approach network, as described in Section 3.2.1 and Figure 1, with parameters \u03b8 which outputs a score f\u03b8(x) given a window of text x = [w] dwin 1 . We minimize the ranking criterion with respect to \u03b8:\n\u03b8 7\u2192 \u2211 x\u2208X \u2211 w\u2208D max { 0 , 1\u2212 f\u03b8(x) + f\u03b8(x(w)) } , (18)\nwhere X is the set of all possible text windows with dwin words coming from our training corpus, D is the dictionary of words, and x(w) denotes the text window obtained by replacing the central word of text window [w]dwin1 by the word w.\nOkanohara and Tsujii (2007) use a related approach to avoiding the entropy criteria using a binary classification approach (correct/incorrect phrase). Their work focuses on using a kernel classifier, and not on learning word embeddings as we do here. Smith and Eisner (2005) also propose a contrastive criterion which estimates the likelihood of the data conditioned to a \u201cnegative\u201d neighborhood. They consider various data neighborhoods, including sentences of length dwin drawn from Ddwin . Their goal was however to perform well on some tagging task on fully unsupervised data, rather than obtaining generic word embeddings useful for other tasks."}, {"heading": "4.3 Training Language Models", "text": "The language model network was trained by stochastic gradient minimization of the ranking criterion (18), sampling a sentence-word pair (s, w) at each iteration.\nSince training times for such large scale systems are counted in weeks, it is not feasible to try many combinations of hyperparameters. It also makes sense to speed up the training time by initializing new networks with the embeddings computed by earlier networks. In particular, we found it expedient to train a succession of networks using increasingly large dictionaries, each network being initialized with the embeddings of the previous network. Successive dictionary sizes and switching times are chosen arbitrarily. (Bengio et al., 2009) provides a more detailed discussion of this, the (as yet, poorly understood) \u201ccurriculum\u201d process.\nFor the purposes of model selection we use the process of \u201cbreeding\u201d. The idea of breeding is instead of trying a full grid search of possible values (which we did not have enough computing power for) to search for the parameters in anology to breeding biological cell lines. Within each line, child networks are initialized with the embeddings of their parents and trained on increasingly rich datasets with sometimes different parameters. That is, suppose we have k processors, which is much less than the possible set of parameters one would like to try. One chooses k initial parameter choices from the large set, and trains\nar Xi v\nthese on the k processors. In our case, possible parameters to adjust are: the learning rate \u03bb, the word embedding dimensions d, number of hidden units n1hu and input window size dwin. One then trains each of these models in an online fashion for a certain amount of time (i.e. a few days), and then selects the best ones using the validation set error rate. That is, breeding decisions were made on the basis of the value of the ranking criterion (18) estimated on a validation set composed of one million words held out from the Wikipedia corpus. In the next breeding iteration, one then chooses another set of k parameters from the possible grid of values that permute slightly the most successful candidates from the previous round. As many of these parameter choices can share weights, we can effectively continue online training retaining some of the learning from the previous iterations.\nVery long training times make such strategies necessary for the foreseeable future: if we had been given computers ten times faster, we probably would have found uses for datasets ten times bigger. However, we should say we believe that although we ended up with a particular choice of parameters, many other choices are almost equally as good, although perhaps there are others that are better as we could not do a full grid search.\nIn the following subsections, we report results obtained with two trained language models. The results achieved by these two models are representative of those achieved by networks trained on the full corpuses.\n\u2022 Language model LM1 has a window size dwin = 11 and a hidden layer with n1hu = 100 units. The embedding layers were dimensioned like those of the supervised networks (Table 5). Model LM1 was trained on our first English corpus (Wikipedia) using successive dictionaries composed of the 5000, 10, 000, 30, 000, 50, 000 and finally 100, 000 most common WSJ words. The total training time was about four weeks.\n\u2022 Language model LM2 has the same dimensions. It was initialized with the embeddings of LM1, and trained for an additional three weeks on our second English corpus (Wikipedia+Reuters) using a dictionary size of 130,000 words."}, {"heading": "4.4 Embeddings", "text": "Both networks produce much more appealing word embeddings than in Section 3.4. Table 7 shows the ten nearest neighbors of a few randomly chosen query words for the LM1 model. The syntactic and semantic properties of the neighbors are clearly related to those of the query word. These results are far more satisfactory than those reported in Table 7 for embeddings obtained using purely supervised training of the benchmark NLP tasks."}, {"heading": "4.5 Semi-supervised Benchmark Results", "text": "Semi-supervised learning has been the object of much attention during the last few years (see Chapelle et al., 2006). Previous semi-supervised approaches for NLP can be roughly categorized as follows:\n\u2022 Ad-hoc approaches such as (Rosenfeld and Feldman, 2007) for relation extraction.\n\u2022 Self-training approaches, such as (Ueffing et al., 2007) for machine translation, and (McClosky et al., 2006) for parsing. These methods augment the labeled training\nar Xi v\nset with examples from the unlabeled dataset using the labels predicted by the model itself. Transductive approaches, such as (Joachims, 1999) for text classification can be viewed as a refined form of self-training.\n\u2022 Parameter sharing approaches such as (Ando and Zhang, 2005; Suzuki and Isozaki, 2008). Ando and Zhang propose a multi-task approach where they jointly train models sharing certain parameters. They train POS and NER models together with a language model (trained on 15 million words) consisting of predicting words given the surrounding tokens. Suzuki and Isozaki embed a generative model (Hidden Markov Model) inside a CRF for POS, Chunking and NER. The generative model is trained on one billion words. These approaches should be seen as a linear counterpart of our work. Using multilayer models vastly expands the parameter sharing opportunities (see Section 5).\nOur approach simply consists of initializing the word lookup tables of the supervised networks with the embeddings computed by the language models. Supervised training is then performed as in Section 3.4. In particular the supervised training stage is free to modify the lookup tables. This sequential approach is computationally convenient because it separates the lengthy training of the language models from the relatively fast training of the supervised networks. Once the language models are trained, we can perform multiple experiments on the supervised networks in a relatively short time. Note that our procedure is clearly linked to the (semi-supervised) deep learning procedures of (Hinton et al., 2006; Bengio et al., 2007; Weston et al., 2008).\nTable 8 clearly shows that this simple initialization significantly boosts the generalization performance of the supervised networks for each task. It is worth mentioning the larger language model led to even better performance. This suggests that we could still take advantage of even bigger unlabeled datasets.\nar Xi v"}, {"heading": "4.6 Ranking and Language", "text": "There is a large agreement in the NLP community that syntax is a necessary prerequisite for semantic role labeling (Gildea and Palmer, 2002). This is why state-of-the-art semantic role labeling systems thoroughly exploit multiple parse trees. The parsers themselves (Charniak, 2000; Collins, 1999) contain considerable prior information about syntax (one can think of this as a kind of informed pre-processing).\nOur system does not use such parse trees because we attempt to learn this information from the unlabeled data set. It is therefore legitimate to question whether our ranking criterion (18) has the conceptual capability to capture such a rich hierarchical information. At first glance, the ranking task appears unrelated to the induction of probabilistic grammars that underly standard parsing algorithms. The lack of hierarchical representation seems a fatal flaw (Chomsky, 1956).\nHowever, ranking is closely related to an alternative description of the language structure: operator grammars (Harris, 1968). Instead of directly studying the structure of a sentence, Harris defines an algebraic structure on the space of all sentences. Starting from a couple of elementary sentence forms, sentences are described by the successive application of sentence transformation operators. The sentence structure is revealed as a side effect of the successive transformations. Sentence transformations can also have a semantic interpretation.\nIn the spirit of structural linguistics, Harris describes procedures to discover sentence transformation operators by leveraging the statistical regularities of the language. Such procedures are obviously useful for machine learning approaches. In particular, he proposes a test to decide whether two sentences forms are semantically related by a transformation operator. He first defines a ranking criterion (Harris, 1968, section 4.1):\n\u201cStarting for convenience with very short sentence forms, say ABC, we choose a particular word choice for all the classes, say BqCq, except one, in\nar Xi v\nthis case A; for every pair of members Ai, Aj of that word class we ask how the sentence formed with one of the members, i.e. AiBqCq compares as to acceptability with the sentence formed with the other member, i.e. AjBqCq.\u201d\nThese gradings are then used to compare sentence forms:\n\u201cIt now turns out that, given the graded n-tuples of words for a particular sentence form, we can find other sentences forms of the same word classes in which the same n-tuples of words produce the same grading of sentences.\u201d\nThis is an indication that these two sentence forms exploit common words with the same syntactic function and possibly the same meaning. This observation forms the empirical basis for the construction of operator grammars that describe real-world natural languages such as English.\nTherefore there are solid reasons to believe that the ranking criterion (18) has the conceptual potential to capture strong syntactic and semantic information. On the other hand, the structure of our language models is probably too restrictive for such goals, and our current approach only exploits the word embeddings discovered during training."}, {"heading": "5. Multi-Task Learning", "text": "It is generally accepted that features trained for one task can be useful for related tasks. This idea was already exploited in the previous section when certain language model features, namely the word embeddings, were used to initialize the supervised networks.\nMulti-task learning (MTL) leverages this idea in a more systematic way. Models for all tasks of interests are jointly trained with an additional linkage between their trainable parameters in the hope of improving the generalization error. This linkage can take the form of a regularization term in the joint cost function that biases the models towards common representations. A much simpler approach consists in having the models share certain parameters defined a priori. Multi-task learning has a long history in machine learning and neural networks. Caruana (1997) gives a good overview of these past efforts."}, {"heading": "5.1 Joint Decoding versus Joint Training", "text": "Multitask approaches do not necessarily involve joint training. For instance, modern speech recognition systems use Bayes rule to combine the outputs of an acoustic model trained on speech data and a language model trained on phonetic or textual corpora (Jelinek, 1976). This joint decoding approach has been successfully applied to structurally more complex NLP tasks. Sutton and McCallum (2005b) obtains improved results by combining the predictions of independently trained CRF models using a joint decoding process at test time that requires more sophisticated probabilistic inference techniques. On the other hand, Sutton and McCallum (2005a) obtain results somewhat below the state-of-the-art using joint decoding for SRL and syntactic parsing. Musillo and Merlo (2006) also describe a negative result at the same joint task.\nJoint decoding invariably works by considering additional probabilistic dependency paths between the models. Therefore it defines an implicit supermodel that describes all the tasks in the same probabilistic framework. Separately training a submodel only\nar Xi v\nmakes sense when the training data blocks these additional dependency paths (in the sense of d-separation, Pearl, 1988). This implies that, without joint training, the additional dependency paths cannot directly involve unobserved variables. Therefore, the natural idea of discovering common internal representations across tasks requires joint training.\nJoint training is relatively straightforward when the training sets for the individual tasks contain the same patterns with different labels. It is then sufficient to train a model that computes multiple outputs for each pattern (Suddarth and Holden, 1991). Using this scheme, Sutton et al. (2007) demonstrates improvements on POS tagging and nounphrase chunking using jointly trained CRFs. However the joint labeling requirement is a limitation because such data is not often available. Miller et al. (2000) achieves performance improvements by jointly training NER, parsing, and relation extraction in a statistical parsing model. The joint labeling requirement problem was weakened using a predictor to fill in the missing annotations.\nAndo and Zhang (2005) propose a setup that works around the joint labeling requirements. They define linear models of the form fi(x) = w > i \u03a6(x) + v > i \u0398\u03a8(x) where fi is the classifier for the i-th task with parameters wi and vi. Notations \u03a6(x) and \u03a8(x) represent engineered features for the pattern x. Matrix \u0398 maps the \u03a8(x) features into a low dimensional subspace common across all tasks. Each task is trained using its own examples without a joint labeling requirement. The learning procedure alternates the optimization of wi and vi for each task, and the optimization of \u0398 to minimize the average loss for all examples in all tasks. The authors also consider auxiliary unsupervised tasks for predicting substructures. They report excellent results on several tasks, including POS and NER.\nar\nXi"}, {"heading": "5.2 Multi-Task Benchmark Results", "text": "Table 9 reports results obtained by jointly trained models for the POS, CHUNK, NER and SRL tasks using the same setup as Section 4.5. We trained jointly POS, CHUNK and NER using the window approach network. As we mentioned earlier, SRL can be trained only with the sentence approach network, due to long-range dependencies related to the verb predicate. We thus also trained all four tasks using the sentence approach network. In both cases, all models share the lookup table parameters (2). The parameters of the first linear layers (4) were shared in the window approach case (see Figure 5), and the first the convolution layer parameters (6) were shared in the sentence approach networks.\nFor the window approach, best results were obtained by enlarging the first hidden layer size to n1hu = 500 (chosen by validation) in order to account for its shared responsibilities. We used the same architecture than SRL for the sentence approach network. The word embedding dimension was kept constant d0 = 50 in order to reuse the language models of Section 4.5.\nTraining was achieved by minimizing the loss averaged across all tasks. This is easily achieved with stochastic gradient by alternatively picking examples for each task and applying (17) to all the parameters of the corresponding model, including the shared parameters. Note that this gives each task equal weight. Since each task uses the training sets described in Table 1, it is worth noticing that examples can come from quite different\nar Xi v\ndatasets. The generalization performance for each task was measured using the traditional testing data specified in Table 1. Fortunately, none of the training and test sets overlap across tasks.\nWhile we find worth mentioning that MTL can produce a single unified architecture that performs well for all these tasks, no (or only marginal) improvements were obtained with this approach compared to training separate architectures per task (which still use semisupervised learning, which is somehow the most important MTL task). The next section shows we can leverage known correlations between tasks in more direct manner."}, {"heading": "6. The Temptation", "text": "Results so far have been obtained by staying (almost15) true to our from scratch philosophy. We have so far avoided specializing our architecture for any task, disregarding a lot of useful a priori NLP knowledge. We have shown that, thanks to large unlabeled datasets, our generic neural networks can still achieve close to state-of-the-art performance by discovering useful features. This section explores what happens when we increase the level of taskspecific engineering in our systems by incorporating some common techniques from the NLP literature. We often obtain further improvements. These figures are useful to quantify how far we went by leveraging large datasets instead of relying on a priori knowledge."}, {"heading": "6.1 Suffix Features", "text": "Word suffixes in many western languages are strong predictors of the syntactic function of the word and therefore can benefit the POS system. For instance, Ratnaparkhi (1996)\n15. We did some basic preprocessing of the raw input words as described in Section 3.4, hence the \u201calmost\u201d in the title of this article. A completely from scratch approach would presumably not know anything about words at all and would work from letters only (or, taken to a further extreme, from speech or optical character recognition, as humans do).\nar Xi v\nuses inputs representing word suffixes and prefixes up to four characters. We achieve this in the POS task by adding discrete word features (Section 3.1.1) representing the last two characters of every word. The size of the suffix dictionary was 455. This led to a small improvement of the POS performance (Table 10, row NN+SLL+LM2+Suffix2). We also tried suffixes obtained with the Porter (1980) stemmer and obtained the same performance as when using two character suffixes."}, {"heading": "6.2 Gazetteers", "text": "State-of-the-art NER systems often use a large dictionary containing well known named entities (e.g. Florian et al., 2003). We restricted ourselves to the gazetteer provided by the CoNLL challenge, containing 8, 000 locations, person names, organizations, and miscellaneous entities. We trained a NER network with 4 additional word features indicating (feature \u201con\u201d or \u201coff\u201d) whether the word is found in the gazetteer under one of these four categories. The gazetteer includes not only words, but also chunks of words. If a sentence chunk is found in the gazetteer, then all words in the chunk have their corresponding gazetteer feature turned to \u201con\u201d. The resulting system displays a clear performance improvement (Table 10, row NN+SLL+LM2+Gazetteer), slightly outperforming the baseline. A plausible explanation of this large boost over the network using only the language model is that gazeetters include word chunks, while we use only the word representation of our language model. For example, \u201cunited\u201d and \u201cbicycle\u201d seen separately are likely to be nonentities, while \u201cunited bicycle\u201d might be an entity, but catching it would require higher level representations of our language model."}, {"heading": "6.3 Cascading", "text": "When one considers related tasks, it is reasonable to assume that tags obtained for one task can be useful for taking decisions in the other tasks. Conventional NLP systems often use features obtained from the output of other preexisting NLP systems. For instance, Shen and Sarkar (2005) describe a chunking system that uses POS tags as input; Florian et al. (2003) describes a NER system whose inputs include POS and CHUNK tags, as well as the output of two other NER classifiers. State-of-the-art SRL systems exploit parse trees (Gildea and Palmer, 2002; Punyakanok et al., 2005), related to CHUNK tags, and built using POS tags (Charniak, 2000; Collins, 1999).\nTable 10 reports results obtained for the CHUNK and NER tasks by adding discrete word features (Section 3.1.1) representing the POS tags. In order to facilitate comparisons, instead of using the more accurate tags from our POS network, we use for each task the POS tags provided by the corresponding CoNLL challenge. We also report results obtained for the SRL task by adding word features representing the CHUNK tags (also provided by the CoNLL challenge). We consistently obtain moderate improvements."}, {"heading": "6.4 Ensembles", "text": "Constructing ensembles of classifiers is a proven way to trade computational efficiency for generalization performance (Bell et al., 2007). Therefore it is not surprising that many NLP systems achieve state-of-the-art performance by combining the outputs of multiple\nar Xi v\nclassifiers. For instance, Kudo and Matsumoto (2001) use an ensemble of classifiers trained with different tagging conventions (see Section 3.2.3). Winning a challenge is of course a legitimate objective. Yet it is often difficult to figure out which ideas are most responsible for the state-of-the-art performance of a large ensemble.\nBecause neural networks are nonconvex, training runs with different initial parameters usually give different solutions. Table 11 reports results obtained for the CHUNK and NER task after ten training runs with random initial parameters. Voting the ten network outputs on a per tag basis (\u201cvoting ensemble\u201d) leads to a small improvement over the average network performance. We have also tried a more sophisticated ensemble approach: the ten network output scores (before sentence-level likelihood) were combined with an additional linear layer (4) and then fed to a new sentence-level likelihood (13). The parameters of the combining layers were then trained on the existing training set, while keeping the ten networks fixed (\u201cjoined ensemble\u201d). This approach did not improve on simple voting.\nThese ensembles come of course at the expense of a ten fold increase of the running time. On the other hand, multiple training times could be improved using smart sampling strategies (Neal, 1996).\nWe can also observe that the performance variability among the ten networks is not very large. The local minima found by the training algorithm are usually good local minima, thanks to the oversized parameter space and to the noise induced by the stochastic gradient procedure (LeCun et al., 1998). In order to reduce the variance in our experimental results, we always use the same initial parameters for networks trained on the same task (except of course for the results reported in Table 11.)"}, {"heading": "6.5 Parsing", "text": "Gildea and Palmer (2002) offer several arguments suggesting that syntactic parsing is a necessary prerequisite for the SRL task. The CoNLL 2005 SRL benchmark task provides parse trees computed using both the Charniak (2000) and Collins (1999) parsers. State-ofthe-art systems often exploit additional parse trees such as the k top ranking parse trees (Koomen et al., 2005; Haghighi et al., 2005).\nIn contrast our SRL networks so far do not use parse trees at all. They rely instead on internal representations transferred from a language model trained with an objective\nar\nXi\nv\nfunction that captures a lot of syntactic information (see Section 4.6). It is therefore legitimate to question whether this approach is an acceptable lightweight replacement for parse trees.\nWe answer this question by providing parse tree information as additional input features to our system. We have limited ourselves to the Charniak parse tree provided with the CoNLL 2005 data. Considering that a node in a syntactic parse tree assigns a label to a segment of the parsed sentence, we propose a way to feed (partially) this labeled segmentation to our network, through additional lookup tables. Each of these lookup tables encode labeled segments of each parse tree level (up to a certain depth). The labeled segments are fed to the network following a IOBES tagging scheme (see Sections 3.2.3 and 3.1.1). As there are 40 different phrase labels in WSJ, each additional tree-related lookup tables has 161 entries (40\u00d7 4 + 1) corresponding to the IBES segment tags, plus the extra O tag.\nWe call level 0 the information associated with the leaves of the original Charniak parse tree. The lookup table for level 0 encodes the corresponding IOBES phrase tags for each words. We obtain levels 1 to 4 by repeatedly trimming the leaves as shown in Figure 6. We labeled \u201cO\u201d words belonging to the root node \u201cS\u201d, or all words of the sentence if the root itself has been trimmed.\nExperiments were performed using the LM2 language model using the same network architectures (see Table 5) and using additional lookup tables of dimension 5 for each\nar Xi\nv\nparse tree level. Table 12 reports the performance improvements obtained by providing increasing levels of parse tree information. Level 0 alone increases the F1 score by almost 1.5%. Additional levels yield diminishing returns. The top performance reaches 76.06% F1 score. This is not too far from the state-of-the-art system which we note uses six parse trees instead of one. Koomen et al. (2005) also report a 74.76% F1 score on the validation set using only the Charniak parse tree. Using the first three parse tree levels, we reach this performance on the validation set.\nWe also reported in Table 12 our previous performance obtained with the CHUNK feature (see Table 10). It is surprising to observe that adding chunking features into the semantic role labeling network performs significantly worse than adding features describing the level 0 of the Charniak parse tree (Table 12). Indeed, if we ignore the label prefixes \u201cBIES\u201d defining the segmentation, the parse tree leaves (at level 0) and the chunking have identical labeling. However, the parse trees identify leaf sentence segments that are often smaller than those identified by the chunking tags, as shown by Hollingshead et al. (2005).16 Instead of relying on Charniak parser, we chose to train a second chunking network to identify the segments delimited by the leaves of the Penn Treebank parse trees (level 0). Our network achieved 92.25% F1 score on this task (we call it PT0), while we\n16. As in (Hollingshead et al., 2005), consider the sentence and chunk labels \u201c(NP They) (VP are starting to buy) (NP growth stocks)\u201d. The parse tree can be written as \u201c(S (NP They) (VP are (VP starting (S (VP to (VP buy (NP growth stocks)))))))\u201d. The tree leaves segmentation is thus given by \u201c(NP They) (VP are) (VP starting) (VP to) (VP buy) (NP growth stocks)\u201d.\nar Xi v\nevaluated Charniak performance as 91.94% on the same task. As shown in Table 12, feeding our own PT0 prediction into the SRL system gives similar performance to using Charniak predictions, and is consistently better than the CHUNK feature."}, {"heading": "6.6 Word Representations", "text": "In Section 4, we adapted our neural network architecture for training a language model task. By leveraging a large amount of unlabeled text data, we induced word embeddings which were shown to boost generalization performance on all tasks. While we chose to stick with one single architecture, other ways to induce word representations exist. Mnih and Hinton (2007) proposed a related language model approach inspired from Restricted Boltzmann Machines. However, word representations are perhaps more commonly infered from n-gram language modelling rather than smoothed language models. One popular approach is the Brown clustering algorithm (Brown et al., 1992a), which builds hierachical word clusters by maximizing the bigram\u2019s mutual information. The induced word representation has been used with success in a wide variety of NLP tasks, including POS (Schu\u0308tze, 1995), NER (Miller et al., 2004; Ratinov and Roth, 2009), or parsing (Koo et al., 2008). Other related approaches exist, like phrase clustering (Lin and Wu, 2009) which has been shown to work well for NER. Finally, Huang and Yates (2009) have recently proposed a smoothed language modelling approach based on a Hidden Markov Model, with success on POS and Chunking tasks.\nWhile a comparison of all these word representations is beyond the scope of this paper, it is rather fair to question the quality of our word embeddings compared to a popular NLP approach. In this section, we report a comparison of our word embeddings against Brown clusters, when used as features into our neural network architecture. We report as baseline previous results where our word embeddings are fine-tuned for each task. We also report performance when our embeddings are kept fixed during task-specific training. Since convex machine learning algorithms are common practice in NLP, we finally report performances for the convex version of our architecture.\nFor the convex experiments, we considered the linear version of our neural networks (instead of having several linear layers interleaved with a non-linearity). While we always picked the sentence approach for SRL, we had to consider the window approach in this particular convex setup, as the sentence approach network (see Figure 2) includes a Max layer. Having only one linear layer in our neural network is not enough to make our architecture convex: all lookup-tables (for each discrete feature) must also be fixed. The word-lookup table is simply fixed to the embeddings obtained from our language model LM2. All other discrete feature lookup-tables (caps, POS, Brown Clusters...) are fixed to a standard sparse representation. Using the notation introduced in Section 3.1.1, if LTWk is the lookup-table of the kth discrete feature, we have W k \u2208 R|Dk|\u00d7|Dk| and the representation of the discrete input w is obtained with:\nLTWk(w) = \u3008W k\u30091w = (\n0, \u00b7 \u00b7 \u00b7 0, 1 at index w\n, 0, \u00b7 \u00b7 \u00b7 0 )T . (19)\nTraining our architecture in this convex setup with the sentence-level likelihood (13) corresponds to training a CRF. In that respect, these convex experiments show the performance of our word embeddings in a classical NLP framework.\nar Xi\nv\nFollowing the Ratinov and Roth (2009) and Koo et al. (2008) setups, we generated 1, 000 Brown clusters using the implementation17 from Liang (2005). To make the comparison fair, the clusters were first induced on the concatenation of Wikipedia and Reuters datasets, as we did in Section 4 for training our largest language model LM2, using a 130K word dictionary. This dictionary covers about 99% of the words in the training set of each task. To cover the last 1%, we augmented the dictionary with the missing words (reaching about 140K words) and induced Brown Clusters using the concatenation of WSJ, Wikipedia, and Reuters.\nThe Brown clustering approach is hierarchical and generates a binary tree of clusters. Each word in the vocabulary is assigned to a node in the tree. Features are extracted from this tree by considering the path from the root to the node containing the word of interest. Following Ratinov & Roth, we picked as features the path prefixes of size 4, 6, 10 and 20. In the non-convex experiments, we fed these four Brown Cluster features to our architecture using four different lookup tables, replacing our word lookup table. The size of the lookup tables was chosen to be 12 by validation. In the convex case, we used the classical sparse representation (19), as for any other discrete feature.\nWe first report in Table 13 generalization performance of our best non-convex networks trained with our LM2 language model and with Brown Cluster features. Our embeddings perform at least as well as Brown Clusters. Results are more mitigated in a convex setup. For most task, going non-convex is better for both word representation types. In general,\n17. Available at http://www.eecs.berkeley.edu/~pliang/software.\nar Xi v\n\u201cfine-tuning\u201d our embeddings for each task also gives an extra boost. Finally, using a better word coverage with Brown Clusters (\u201call words\u201d instead of \u201c130K words\u201d in Table 13) did not help.\nMore complex features could be possibly combined instead of using a non-linear model. For instance, Turian et al. (2010) performed a comparison of Brown Clusters and embeddings trained in the same spirit as ours18, with additional features combining labels and tokens. We believe this type of comparison should be taken with care, as combining a given feature with different word representations might not have the same effect on each word representation."}, {"heading": "6.7 Engineering a Sweet Spot", "text": "We implemented a standalone version of our architecture, written in the C language. We gave the name \u201cSENNA\u201d (Semantic/syntactic Extraction using a Neural Network Architecture) to the resulting system. The parameters of each architecture are the ones described in Table 5. All the networks were trained separately on each task using the sentence-level likelihood (SLL). The word embeddings were initialized to LM2 embeddings, and then fine-tuned for each task. We summarize features used by our implementation in Table 14, and we report performance achieved on each task in Table 15. The runtime version19 contains about 2500 lines of C code, runs in less than 150MB of memory, and needs less than a millisecond per word to compute all the tags. Table 16 compares the tagging speeds for our system and for the few available state-of-the-art systems: the Toutanova et al. (2003) POS tagger20, the Shen et al. (2007) POS tagger21 and the Koomen et al. (2005) SRL system.22 All programs were run on a single 3GHz Intel core. The POS taggers were run with Sun Java 1.6 with a large enough memory allocation to reach their top tagging speed.\n18. However they did not reach our embedding performance. There are several differences in how they trained their models that might explain this. Firstly, they may have experienced difficulties because they train 50-dimensional embeddings for 269K distinct words using a comparatively small training set (RCV1, 37M words), unlikely to contain enough instances of the rare words. Secondly, they predict the correctness of the final word of each window instead of the center word (Turian et al., 2010), effectively restricting the model to unidirectional prediction. Finally, they do not fine tune their embeddings after unsupervised training.\n19. Available at http://ml.nec-labs.com/senna. 20. Available at http://nlp.stanford.edu/software/tagger.shtml. We picked the 3.0 version (May 2010). 21. Available at http://www.cis.upenn.edu/~xtag/spinal. 22. Available at http://l2r.cs.uiuc.edu/~cogcomp/asoftware.php?skey=SRL.\nar\nXi\nv\nPOS System RAM (MB) Time (s)\nToutanova et al. (2003) 800 64 Shen et al. (2007) 2200 833\nSENNA 32 4\nSRL System RAM (MB) Time (s) Koomen et al. (2005) 3400 6253\nSENNA 124 51\nTable 16: Runtime speed and memory consumption comparison between state-of-the-art systems and our approach (SENNA). We give the runtime in seconds for running both the POS and SRL taggers on their respective testing sets. Memory usage is reported in megabytes.\nThe beam size of the Shen tagger was set to 3 as recommended in the paper. Regardless of implementation differences, it is clear that our neural networks run considerably faster. They also require much less memory. Our POS and SRL tagger runs in 32MB and 120MB of RAM respectively. The Shen and Toutanova taggers slow down significantly when the Java machine is given less than 2.2GB and 800MB of RAM respectively, while the Koomen tagger requires at least 3GB of RAM.\nWe believe that a number of reasons explain the speed advantage of our system. First, our system only uses rather simple input features and therefore avoids the nonnegligible computation time associated with complex handcrafted features. Secondly, most network computations are dense matrix-vector operations. In contrast, systems that rely on a great number of sparse features experience memory latencies when traversing the sparse data structures. Finally, our compact implementation is self-contained. Since it does not rely on the outputs of disparate NLP system, it does not suffer from communication latency issues."}, {"heading": "7. Critical Discussion", "text": "Although we believe that this contribution represents a step towards the \u201cNLP from scratch\u201d objective, we are keenly aware that both our goal and our means can be criticized.\nar Xi v\nThe main criticism of our goal can be summarized as follows. Over the years, the NLP community has developed a considerable expertise in engineering effective NLP features. Why should they forget this painfully acquired expertise and instead painfully acquire the skills required to train large neural networks? As mentioned in our introduction, we observe that no single NLP task really covers the goals of NLP. Therefore we believe that task-specific engineering (i.e. that does not generalize to other tasks) is not desirable. But we also recognize how much our neural networks owe to previous NLP task-specific research.\nThe main criticism of our means is easier to address. Why did we choose to rely on a twenty year old technology, namely multilayer neural networks? We were simply attracted by their ability to discover hidden representations using a stochastic learning algorithm that scales linearly with the number of examples. Most of the neural network technology necessary for our work has been described ten years ago (e.g. Le Cun et al., 1998). However, if we had decided ten years ago to train the language model network LM2 using a vintage computer, training would only be nearing completion today. Training algorithms that scale linearly are most able to benefit from such tremendous progress in computer hardware."}, {"heading": "8. Conclusion", "text": "We have presented a multilayer neural network architecture that can handle a number of NLP tasks with both speed and accuracy. The design of this system was determined by our desire to avoid task-specific engineering as much as possible. Instead we rely on large unlabeled datasets and let the training algorithm discover internal representations that prove useful for all the tasks of interest. Using this strong basis, we have engineered a fast and efficient \u201call purpose\u201d NLP tagger that we hope will prove useful to the community."}, {"heading": "Acknowledgments", "text": "We acknowledge the persistent support of NEC for this research effort. We thank Yoshua Bengio, Samy Bengio, Eric Cosatto, Vincent Etter, Hans-Peter Graf, Ralph Grishman, and Vladimir Vapnik for their useful feedback and comments.\nar Xi v"}, {"heading": "Appendix A. Neural Network Gradients", "text": "We consider a neural network f\u03b8(\u00b7), with parameters \u03b8. We maximize the likelihood (8), or minimize ranking criterion (18), with respect to the parameters \u03b8, using stochastic gradient. By negating the likelihood, we now assume it all corresponds to minimize a cost C(f\u03b8(\u00b7)), with respect to \u03b8.\nFollowing the classical \u201cback-propagation\u201d derivations (LeCun, 1985; Rumelhart et al., 1986) and the modular approach shown in (Bottou, 1991), any feed-forward neural network with L layers, like the ones shown in Figure 1 and Figure 2, can be seen as a composition of functions f l\u03b8(\u00b7), corresponding to each layer l:\nf\u03b8(\u00b7) = fL\u03b8 (fL\u22121\u03b8 (. . . f1\u03b8 (\u00b7) . . .))\nPartionning the parameters of the network with respect to each layers 1 \u2264 l \u2264 L, we write:\n\u03b8 = (\u03b81, . . . , \u03b8l, . . . , \u03b8L) .\nWe are now interested in computing the gradients of the cost with respect to each \u03b8l. Applying the chain rule (generalized to vectors) we obtain the classical backpropagation recursion:\n\u2202C \u2202\u03b8l = \u2202f l\u03b8 \u2202\u03b8l \u2202C\n\u2202f l\u03b8 (20)\n\u2202C\n\u2202f l\u22121\u03b8 = \u2202f l\u03b8 \u2202f l\u22121\u03b8 \u2202C \u2202f l\u03b8 . (21)\nIn other words, we first initialize the recursion by computing the gradient of the cost with respect to the last layer output \u2202C/\u2202fL\u03b8 . Then each layer l computes the gradient respect to its own parameters with (20), given the gradient coming from its output \u2202C/\u2202f l\u03b8. To perform the backpropagation, it also computes the gradient with respect to its own inputs, as shown in (21). We now derive the gradients for each layer we used in this paper.\nLookup Table Layer Given a mat ix of parameters \u03b81 = W 1 and word (or discrete feature) indices [w]T1 , the layer outputs the matrix:\nf l\u03b8([w] T l ) = ( \u3008W \u30091[w]1 \u3008W \u3009 1 [w]2 . . . \u3008W \u30091[w]T ) .\nThe gradients of the weights \u3008W \u30091i are given by:\n\u2202C\n\u2202\u3008W \u30091i = \u2211 {1\u2264t\u2264T / [w]t=i} \u3008 \u2202C \u2202f l\u03b8 \u30091i\nThis sum equals zero if the index i in the lookup table does not corresponds to a word in the sequence. In this case, the ith column of W does not need to be updated. As a Lookup Table Layer is always the first layer, we do not need to compute its gradients with respect to the inputs.\nar Xi v\nLinear Layer Given parameters \u03b8l = (W l, bl), and an input vector f l\u22121\u03b8 the output is given by:\nf l\u03b8 = W lf l\u22121\u03b8 + b l . (22)\nThe gradients with respect to the parameters are then obtained with:\n\u2202C\n\u2202W l =\n[ \u2202C\n\u2202f l\u03b8\n] [ f l\u22121\u03b8 ]T and \u2202C \u2202bl = \u2202C\n\u2202f l\u03b8 , (23)\nand the gradients with respect to the inputs are computed with:\n\u2202C \u2202f l\u22121\u03b8 = [ W l ]T \u2202C \u2202f l\u03b8 . (24)\nConvolution Layer Given a input matrix f l\u22121\u03b8 , a Convolution Layer f l \u03b8(\u00b7) applies a Linear Layer operation (22) successively on each window \u3008f l\u22121\u03b8 \u3009 dwin t (1 \u2264 t \u2264 T ) of size dwin. Using (23), the gradients of the parameters are thus given by summing over all windows:\n\u2202C\n\u2202W l = T\u2211 t=1 [ \u3008 \u2202C \u2202f l\u03b8 \u30091t ] [ \u3008f l\u22121\u03b8 \u3009 dwin t ]T and \u2202C \u2202bl = T\u2211 t=1 \u3008 \u2202C \u2202f l\u03b8 \u30091t .\nAfter initializing the input gradients \u2202C/\u2202f l\u22121\u03b8 to zero, we iterate (24) over all windows for 1 \u2264 t \u2264 T , leading the accumulation23:\n\u3008 \u2202C \u2202f l\u22121\u03b8 \u3009dwint += [ W l ]T \u3008 \u2202C \u2202f l\u03b8 \u30091t .\nMax Layer Given a matrix f l\u22121\u03b8 , the Max Layer computes[ f l\u03b8 ] i = max t [ \u3008f l\u22121\u03b8 \u30091t ] i and ai = argmax t [ \u3008f l\u22121\u03b8 \u30091t ] i \u2200i ,\nwhere ai stores the index of the largest value. We only need to compute the gradient with respect to the inputs, as this layer has no parameters. The gradient is given by[\n\u3008 \u2202C \u2202f l\u22121\u03b8 \u30091t ] i = { [ \u3008 \u2202C \u2202f l\u03b8 \u30091t ] i if t = ai 0 otherwise .\nHardTanh Layer Given a vector f l\u22121\u03b8 , and the definition of the HardTanh (5) we get\n[ \u2202C\n\u2202f l\u22121\u03b8 ] i =  0 if [ f l\u22121\u03b8 ] i < \u22121[ \u2202C \u2202f l\u03b8 ] i if \u2212 1 <= [ f l\u22121\u03b8 ] i <= 1 0 if [ f l\u22121\u03b8 ] i > 1 ,\nif we ignore non-differentiability points.\n23. We denote \u201c+=\u201d any accumulation operation.\nar Xi v\nWord-Level Log-Likelihood The network outputs a score [f\u03b8]i for each tag indexed by i. Following (11), if y is the true tag for a given example, the stochastic score to minimize can be written as\nC(f\u03b8) = logadd j\n[f\u03b8]j \u2212 [f\u03b8]y\nConsidering the definition of the logadd (10), the gradient with respect to f\u03b8 is given by\n\u2202C\n\u2202 [f\u03b8]i = e[f\u03b8]i\u2211 k e [f\u03b8]k \u2212 1i=y \u2200i.\nSentence-Level Log-Likelihood The network outputs a matrix where each element [f\u03b8]i, t gives a score for tag i at word t. Given a tag sequence [y] T 1 and a input sequence [x] T 1 , we maximize the likelihood (13), which corresponds to minimizing the score\nC(f\u03b8, A) = logadd \u2200[j]T1 s([x]T1 , [j] T 1 , \u03b8\u0303)\ufe38 \ufe37\ufe37 \ufe38\nClogadd\n\u2212s([x]T1 , [y]T1 , \u03b8\u0303) ,\nwith\ns([x]T1 , [y] T 1 , \u03b8\u0303) = T\u2211 t=1 ( [A][y]t\u22121, [y]t + [f\u03b8][y]t, t ) .\nWe first initialize all gradients to zero\n\u2202C \u2202 [ f\u03b8 ] i, t = 0 \u2200i, t and \u2202C \u2202 [A]i, j = 0 \u2200i, j .\nWe then accumulate gradients over the second part of the cost \u2212s([x]T1 , [y]T1 , \u03b8\u0303), which gives:\n\u2202C \u2202 [ f\u03b8 ] [y]t, t += 1\n\u2202C\n\u2202 [A][y]t\u22121, [y]t += 1\n\u2200t .\nWe now need to accumulate the gradients over the first part of the cost, that is Clogadd. We differentiate Clogadd by applying the chain rule through the recursion (14). First we initialize our recursion with\n\u2202Clogadd \u2202\u03b4T (i) = e\u03b4T (i)\u2211 k e \u03b4T (k) \u2200i .\nWe then compute iteratively:\n\u2202Clogadd \u2202\u03b4t\u22121(i) = \u2211 j \u2202Clogadd \u2202\u03b4t(j) e\u03b4t\u22121(i)+[A]i, j\u2211 k e \u03b4t\u22121(k)+[A]k, j , (25)\nar Xi v\nwhere at each step t of the recursion we accumulate of the gradients with respect to the inputs f\u03b8, and the transition scores [A]i, j :\n\u2202C \u2202 [ f\u03b8 ] i, t += \u2202Clogadd \u2202\u03b4t(i) \u2202\u03b4t(i) \u2202 [ f\u03b8 ] i, t\n= \u2202Clogadd \u2202\u03b4t(i)\n\u2202C\n\u2202 [A]i, j += \u2202Clogadd \u2202\u03b4t(j) \u2202\u03b4t(j) \u2202 [A]i, j = \u2202Clogadd \u2202\u03b4t(j) e\u03b4t\u22121(i)+[A]i, j\u2211 k e \u03b4t\u22121(k)+[A]k, j .\nRanking Criterion We use the ranking criterion (18) for training our language model. In this case, given a \u201cpositive\u201d example x and a \u201cnegative\u201d example x(w), we want to minimize:\nC(f\u03b8(x), f\u03b8(x w)) = max { 0 , 1\u2212 f\u03b8(x) + f\u03b8(x(w)) } . (26)\nIgnoring the non-differentiability of max(0, \u00b7) in zero, the gradient is simply given by:\n( \u2202C \u2202f\u03b8(x)\n\u2202C \u2202f\u03b8(xw)\n) =  ( \u22121 1 ) if 1\u2212 f\u03b8(x) + f\u03b8(x(w)) > 0( 0\n0\n) otherwise\n."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "JMLR, 6:1817\u20131953,", "citeRegEx": "Ando and Zhang.,? \\Q2005\\E", "shortCiteRegEx": "Ando and Zhang.", "year": 2005}, {"title": "The BellKor solution to the Netflix Prize", "author": ["R.M. Bell", "Y. Koren", "C. Volinsky"], "venue": "Technical report, AT&T Labs,", "citeRegEx": "Bell et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bell et al\\.", "year": 2007}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Stochastic gradient learning in neural networks", "author": ["L. Bottou"], "venue": "In Proceedings of Neuro-N\u0131\u0302mes 91, Nimes,", "citeRegEx": "Bottou.,? \\Q1991\\E", "shortCiteRegEx": "Bottou.", "year": 1991}, {"title": "Online algorithms and stochastic approximations", "author": ["L. Bottou"], "venue": "Online Learning and Neural Networks", "citeRegEx": "Bottou.,? \\Q1998\\E", "shortCiteRegEx": "Bottou.", "year": 1998}, {"title": "A framework for the cooperation of learning algorithms", "author": ["L. Bottou", "P. Gallinari"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Bottou and Gallinari.,? \\Q1991\\E", "shortCiteRegEx": "Bottou and Gallinari.", "year": 1991}, {"title": "Global training of document processing systems using graph transformer networks", "author": ["L. Bottou", "Y. LeCun", "Yoshua Bengio"], "venue": "In Proc. of Computer Vision and Pattern Recognition,", "citeRegEx": "Bottou et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 1997}, {"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition", "author": ["J.S. Bridle"], "venue": "In F. Fogelman Soulie\u0301 and J. He\u0301rault, editors, Neurocomputing: Algorithms, Architectures and Applications,", "citeRegEx": "Bridle.,? \\Q1990\\E", "shortCiteRegEx": "Bridle.", "year": 1990}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "P.V. deSouza", "R.L. Mercer", "V.J.D. Pietra", "J C. Lai"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "An estimate of an upper bound for the entropy of english", "author": ["P.F. Brown", "V.J. Della Pietra", "R.L. Mercer", "S.A. Della Pietra", "J.C. Lai"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Learning to rank with nonsmooth cost functions", "author": ["C.J.C. Burges", "R. Ragno", "Quoc Viet Le"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Burges et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Burges et al\\.", "year": 2007}, {"title": "Multitask Learning", "author": ["R. Caruana"], "venue": "Machine Learning,", "citeRegEx": "Caruana.,? \\Q1997\\E", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Semi-Supervised Learning. Adaptive computation and machine learning", "author": ["O. Chapelle", "B. Schlkopf", "A. Zien"], "venue": null, "citeRegEx": "Chapelle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2006}, {"title": "A maximum-entropy-inspired parser", "author": ["E. Charniak"], "venue": "Proceedings of the first conference on North American chapter of the Association for Computational Linguistics,", "citeRegEx": "Charniak.,? \\Q2000\\E", "shortCiteRegEx": "Charniak.", "year": 2000}, {"title": "Named entity recognition with a maximum entropy approach", "author": ["H.L. Chieu"], "venue": "Proceedings of the Seventh Conference on Natural Language Learning (CoNLL-2003,", "citeRegEx": "Chieu.,? \\Q2003\\E", "shortCiteRegEx": "Chieu.", "year": 2003}, {"title": "Three models for the description of language", "author": ["N. Chomsky"], "venue": "IRE Transactions on Information Theory,", "citeRegEx": "Chomsky.,? \\Q1956\\E", "shortCiteRegEx": "Chomsky.", "year": 1956}, {"title": "Ranking the best instances", "author": ["S. Cl\u00e9men\u00e7on", "N. Vayatis"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cl\u00e9men\u00e7on and Vayatis.,? \\Q2007\\E", "shortCiteRegEx": "Cl\u00e9men\u00e7on and Vayatis.", "year": 2007}, {"title": "Learning to order things", "author": ["W.W. Cohen", "R.E. Schapire", "Y. Singer"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Cohen et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 1998}, {"title": "Semantic role labelling with tree conditional random fields", "author": ["T. Cohn", "P. Blunsom"], "venue": "In Ninth Conference on Computational Natural Language (CoNLL),", "citeRegEx": "Cohn and Blunsom.,? \\Q2005\\E", "shortCiteRegEx": "Cohn and Blunsom.", "year": 2005}, {"title": "Head-Driven Statistical Models for Natural Language Parsing", "author": ["M. Collins"], "venue": "PhD thesis, University of Pennsylvania,", "citeRegEx": "Collins.,? \\Q1999\\E", "shortCiteRegEx": "Collins.", "year": 1999}, {"title": "Large Scale Machine Learning", "author": ["R. Collobert"], "venue": "PhD thesis, Universite\u0301 Paris VI,", "citeRegEx": "Collobert.,? \\Q2004\\E", "shortCiteRegEx": "Collobert.", "year": 2004}, {"title": "A convergent gambling estimate of the entropy of english", "author": ["T. Cover", "R. King"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cover and King.,? \\Q1978\\E", "shortCiteRegEx": "Cover and King.", "year": 1978}, {"title": "Named entity recognition through classifier combination", "author": ["R. Florian", "A. Ittycheriah", "H. Jing", "T. Zhang"], "venue": "In Proceedings of the seventh conference on Natural language learning at HLT-NAACL", "citeRegEx": "Florian et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2003}, {"title": "Automatic labeling of semantic roles", "author": ["D. Gildea", "D. Jurafsky"], "venue": "Computational Linguistics,", "citeRegEx": "Gildea and Jurafsky.,? \\Q2002\\E", "shortCiteRegEx": "Gildea and Jurafsky.", "year": 2002}, {"title": "The necessity of parsing for predicate argument recognition", "author": ["D. Gildea", "M. Palmer"], "venue": "Proceedings of the 40th Annual Meeting of the ACL,", "citeRegEx": "Gildea and Palmer.,? \\Q2002\\E", "shortCiteRegEx": "Gildea and Palmer.", "year": 2002}, {"title": "SVMTool: A general POS tagger generator based on support vector machines", "author": ["J. Gim\u00e9nez", "L. M\u00e0rquez"], "venue": "In Proceedings of the 4th International Conference on Language Resources and Evaluation", "citeRegEx": "Gim\u00e9nez and M\u00e0rquez.,? \\Q2004\\E", "shortCiteRegEx": "Gim\u00e9nez and M\u00e0rquez.", "year": 2004}, {"title": "A joint model for semantic role labeling", "author": ["A. Haghighi", "K. Toutanova", "C.D. Manning"], "venue": "In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005). Association for Computational Linguistics,", "citeRegEx": "Haghighi et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2005}, {"title": "Mathematical Structures of Language", "author": ["Z.S. Harris"], "venue": null, "citeRegEx": "Harris.,? \\Q1968\\E", "shortCiteRegEx": "Harris.", "year": 1968}, {"title": "Dependency networks for inference, collaborative filtering, and data visualization", "author": ["D. Heckerman", "D.M. Chickering", "C. Meek", "R. Rounthwaite", "C. Kadie"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Heckerman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Heckerman et al\\.", "year": 2001}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Comp.,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Comparing and combining finite-state and context-free parsers", "author": ["K. Hollingshead", "S. Fisher", "B. Roark"], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "Hollingshead et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hollingshead et al\\.", "year": 2005}, {"title": "Distributional representations for handling sparsity in supervised sequence-labeling. In Proceedings of the Association for Computational Linguistics (ACL), pages 495\u2013503", "author": ["F. Huang", "A. Yates"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Huang and Yates.,? \\Q2009\\E", "shortCiteRegEx": "Huang and Yates.", "year": 2009}, {"title": "Continuous speech recognition by statistical methods", "author": ["F. Jelinek"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Jelinek.,? \\Q1976\\E", "shortCiteRegEx": "Jelinek.", "year": 1976}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "In ICML,", "citeRegEx": "Joachims.,? \\Q1999\\E", "shortCiteRegEx": "Joachims.", "year": 1999}, {"title": "Natural language grammar induction using a constituentcontext model", "author": ["D. Klein", "C.D. Manning"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Klein and Manning.,? \\Q2002\\E", "shortCiteRegEx": "Klein and Manning.", "year": 2002}, {"title": "Simple semi-supervised dependency parsing", "author": ["T. Koo", "X. Carreras", "M. Collins"], "venue": "In Proceedings of the Association for Computational Linguistics (ACL),", "citeRegEx": "Koo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Generalized inference with multiple semantic role labeling systems (shared task paper)", "author": ["P. Koomen", "V. Punyakanok", "D. Roth", "W. Yih"], "venue": "Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL),", "citeRegEx": "Koomen et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Koomen et al\\.", "year": 2005}, {"title": "Chunking with support vector machines", "author": ["T. Kudo", "Y. Matsumoto"], "venue": "Proceedings of the 2nd Meeting of the North American Association for Computational Linguistics: NAACL", "citeRegEx": "Kudo and Matsumoto.,? \\Q2001\\E", "shortCiteRegEx": "Kudo and Matsumoto.", "year": 2001}, {"title": "Use of support vector learning for chunk identification", "author": ["T. Kudoh", "Y. Matsumoto"], "venue": "In Proceedings of CoNLL-2000 and LLL-2000,", "citeRegEx": "Kudoh and Matsumoto.,? \\Q2000\\E", "shortCiteRegEx": "Kudoh and Matsumoto.", "year": 2000}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In Eighteenth International Conference on Machine Learning,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Gradient based learning applied to document recognition", "author": ["Y. Le Cun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of IEEE,", "citeRegEx": "Cun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Cun et al\\.", "year": 1998}, {"title": "A learning scheme for asymmetric threshold networks", "author": ["Y. LeCun"], "venue": "In Proceedings of Cognitiva", "citeRegEx": "LeCun.,? \\Q1985\\E", "shortCiteRegEx": "LeCun.", "year": 1985}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "Neural Networks: Tricks of the Trade,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Semi-supervised learning for natural language", "author": ["P. Liang"], "venue": "Master\u2019s thesis, Massachusetts Institute of Technology,", "citeRegEx": "Liang.,? \\Q2005\\E", "shortCiteRegEx": "Liang.", "year": 2005}, {"title": "Structure compilation: trading structure for features", "author": ["P. Liang", "III H. Daum\u00e9", "D. Klein"], "venue": "In International conference on Machine learning (ICML),", "citeRegEx": "Liang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2008}, {"title": "Phrase clustering for discriminative learning. In Proceedings of the Association for Computational Linguistics (ACL), pages 1030\u20131038", "author": ["D. Lin", "X. Wu"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Lin and Wu.,? \\Q2009\\E", "shortCiteRegEx": "Lin and Wu.", "year": 2009}, {"title": "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm", "author": ["N. Littlestone"], "venue": "In Machine Learning,", "citeRegEx": "Littlestone.,? \\Q1988\\E", "shortCiteRegEx": "Littlestone.", "year": 1988}, {"title": "Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons", "author": ["A. McCallum", "Wei Li"], "venue": "In Proceedings of the seventh conference on Natural language learning at HLT-NAACL", "citeRegEx": "McCallum and Li.,? \\Q2003\\E", "shortCiteRegEx": "McCallum and Li.", "year": 2003}, {"title": "Effective self-training for parsing", "author": ["D. McClosky", "E. Charniak", "M. Johnson"], "venue": "Proceedings of HLT-NAACL", "citeRegEx": "McClosky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Flexible text segmentation with structured multilabel classification", "author": ["R. McDonald", "K. Crammer", "F. Pereira"], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "A novel use of statistical parsing to extract information from text", "author": ["S. Miller", "H. Fox", "L. Ramshaw", "R. Weischedel"], "venue": "6th Applied Natural Language Processing Conference,", "citeRegEx": "Miller et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2000}, {"title": "Name tagging with word clusters and discriminative training", "author": ["S. Miller", "J. Guinness", "A. Zamanian"], "venue": "In Proceedings of HLT-NAACL,", "citeRegEx": "Miller et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2004}, {"title": "Three new graphical models for statistical language modelling", "author": ["A Mnih", "G.E. Hinton"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih and Hinton.,? \\Q2007\\E", "shortCiteRegEx": "Mnih and Hinton.", "year": 2007}, {"title": "Robust Parsing of the Proposition Bank", "author": ["G. Musillo", "P. Merlo"], "venue": "ROMAND 2006: Robust Methods in Analysis of Natural language Data,", "citeRegEx": "Musillo and Merlo.,? \\Q2006\\E", "shortCiteRegEx": "Musillo and Merlo.", "year": 2006}, {"title": "Bayesian Learning for Neural Networks", "author": ["R.M. Neal"], "venue": "Number 118 in Lecture Notes in Statistics. Springer-Verlag,", "citeRegEx": "Neal.,? \\Q1996\\E", "shortCiteRegEx": "Neal.", "year": 1996}, {"title": "A discriminative language model with pseudo-negative samples", "author": ["D. Okanohara", "J. Tsujii"], "venue": "Proceedings of the 45th Annual Meeting of the ACL,", "citeRegEx": "Okanohara and Tsujii.,? \\Q2007\\E", "shortCiteRegEx": "Okanohara and Tsujii.", "year": 2007}, {"title": "The proposition bank: An annotated corpus of semantic roles", "author": ["M. Palmer", "D. Gildea", "P. Kingsbury"], "venue": "Comput. Linguist.,", "citeRegEx": "Palmer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Palmer et al\\.", "year": 2005}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl.,? \\Q1988\\E", "shortCiteRegEx": "Pearl.", "year": 1988}, {"title": "Learning sets of filters using back-propagation", "author": ["D.C. Plaut", "G.E. Hinton"], "venue": "Computer Speech and Language,", "citeRegEx": "Plaut and Hinton.,? \\Q1987\\E", "shortCiteRegEx": "Plaut and Hinton.", "year": 1987}, {"title": "An algorithm for suffix stripping", "author": ["M.F. Porter"], "venue": null, "citeRegEx": "Porter.,? \\Q1980\\E", "shortCiteRegEx": "Porter.", "year": 1980}, {"title": "Shallow semantic parsing using support vector machines", "author": ["S. Pradhan", "W. Ward", "K. Hacioglu", "J. Martin", "D. Jurafsky"], "venue": "Proceedings of HLT/NAACL-2004,", "citeRegEx": "Pradhan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2004}, {"title": "Semantic role chunking combining complementary syntactic views", "author": ["S. Pradhan", "K. Hacioglu", "W. Ward", "J.H. Martin", "D. Jurafsky"], "venue": "In Proceedings of the Ninth Conference on Computational Natural Language Learning", "citeRegEx": "Pradhan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2005}, {"title": "The necessity of syntactic parsing for semantic role labeling", "author": ["V. Punyakanok", "D. Roth", "W. Yih"], "venue": "In IJCAI,", "citeRegEx": "Punyakanok et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2005}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["L. Ratinov", "D. Roth"], "venue": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL),", "citeRegEx": "Ratinov and Roth.,? \\Q2009\\E", "shortCiteRegEx": "Ratinov and Roth.", "year": 2009}, {"title": "A maximum entropy model for part-of-speech tagging", "author": ["A. Ratnaparkhi"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ratnaparkhi.,? \\Q1996\\E", "shortCiteRegEx": "Ratnaparkhi.", "year": 1996}, {"title": "Using Corpus Statistics on Entities to Improve Semisupervised Relation Extraction from the Web", "author": ["B. Rosenfeld", "R. Feldman"], "venue": "Proceedings of the 45th Annual Meeting of the ACL,", "citeRegEx": "Rosenfeld and Feldman.,? \\Q2007\\E", "shortCiteRegEx": "Rosenfeld and Feldman.", "year": 2007}, {"title": "Learning internal representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Distributional part-of-speech tagging. In Proceedings of the Association for Computational Linguistics (ACL), pages 141\u2013148", "author": ["H. Sch\u00fctze"], "venue": null, "citeRegEx": "Sch\u00fctze.,? \\Q1995\\E", "shortCiteRegEx": "Sch\u00fctze.", "year": 1995}, {"title": "Connectionist language modeling for large vocabulary continuous speech recognition", "author": ["H. Schwenk", "J.L. Gauvain"], "venue": "In IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Schwenk and Gauvain.,? \\Q2002\\E", "shortCiteRegEx": "Schwenk and Gauvain.", "year": 2002}, {"title": "Shallow parsing with conditional random fields", "author": ["F. Sha", "F. Pereira"], "venue": "In NAACL \u201903: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,", "citeRegEx": "Sha and Pereira.,? \\Q2003\\E", "shortCiteRegEx": "Sha and Pereira.", "year": 2003}, {"title": "Prediction and entropy of printed english", "author": ["C.E. Shannon"], "venue": "Bell Systems Technical Journal,", "citeRegEx": "Shannon.,? \\Q1951\\E", "shortCiteRegEx": "Shannon.", "year": 1951}, {"title": "Voting between multiple data representations for text chunking", "author": ["H. Shen", "A. Sarkar"], "venue": "Advances in Artificial Intelligence,", "citeRegEx": "Shen and Sarkar.,? \\Q2005\\E", "shortCiteRegEx": "Shen and Sarkar.", "year": 2005}, {"title": "Guided learning for bidirectional sequence classification", "author": ["L. Shen", "G. Satta", "A.K. Joshi"], "venue": "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Shen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2007}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 354\u2013362", "author": ["N.A. Smith", "J. Eisner"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Smith and Eisner.,? \\Q2005\\E", "shortCiteRegEx": "Smith and Eisner.", "year": 2005}, {"title": "Symbolic-neural systems and the use of hints for developing complex systems", "author": ["S.C. Suddarth", "A.D.C. Holden"], "venue": "International Journal of Man-Machine Studies,", "citeRegEx": "Suddarth and Holden.,? \\Q1991\\E", "shortCiteRegEx": "Suddarth and Holden.", "year": 1991}, {"title": "Modeling latent-dynamic in shallow parsing: a latent conditional model with improved inference", "author": ["X. Sun", "L.-P. Morency", "D. Okanohara", "J. Tsujii"], "venue": "In COLING \u201908: Proceedings of the 22nd International Conference on Computational Linguistics,", "citeRegEx": "Sun et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2008}, {"title": "Joint parsing and semantic role labeling", "author": ["C. Sutton", "A. McCallum"], "venue": "In Proceedings of CoNLL-2005,", "citeRegEx": "Sutton and McCallum.,? \\Q2005\\E", "shortCiteRegEx": "Sutton and McCallum.", "year": 2005}, {"title": "Composition of conditional random fields for transfer learning", "author": ["C. Sutton", "A. McCallum"], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "Sutton and McCallum.,? \\Q2005\\E", "shortCiteRegEx": "Sutton and McCallum.", "year": 2005}, {"title": "Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence", "author": ["C. Sutton", "A. McCallum", "K. Rohanimanesh"], "venue": "Data. JMLR,", "citeRegEx": "Sutton et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2007}, {"title": "Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data", "author": ["J. Suzuki", "H. Isozaki"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "Suzuki and Isozaki.,? \\Q2008\\E", "shortCiteRegEx": "Suzuki and Isozaki.", "year": 2008}, {"title": "The entropy of english using ppm-based models", "author": ["W.J. Teahan", "J.G. Cleary"], "venue": "Data Compression Conference", "citeRegEx": "Teahan and Cleary.,? \\Q1996\\E", "shortCiteRegEx": "Teahan and Cleary.", "year": 1996}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C.D. Manning", "Y. Singer"], "venue": "In HLT-NAACL,", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Word representations: A simple and general method for semi-supervised learning. In Proceedings of the Association for Computational Linguistics (ACL), pages 384\u2013392", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Transductive learning for statistical machine translation", "author": ["N. Ueffing", "G. Haffari", "A. Sarkar"], "venue": "Proceedings of the 45th Annual Meeting of the ACL,", "citeRegEx": "Ueffing et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ueffing et al\\.", "year": 2007}, {"title": "Phoneme recognition using time-delay neural networks", "author": ["A. Waibel", "T. Hanazawa", "G. Hinton", "K. Shikano", "K.J. Lang"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Waibel et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Waibel et al\\.", "year": 1989}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. Weston", "F. Ratle", "R. Collobert"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Weston et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 83, "context": "Task Benchmark Dataset Training set Test set (#tokens) (#tokens) (#tags) POS Toutanova et al. (2003) WSJ sections 0\u201318 sections 22\u201324 ( 45 ) ( 912,344 ) ( 129,654 ) Chunking CoNLL 2000 WSJ sections 15\u201318 section 20 ( 42 ) ( 211,727 ) ( 47,377 ) (IOBES) NER CoNLL 2003 Reuters \u201ceng.", "startOffset": 77, "endOffset": 101}, {"referenceID": 70, "context": "System Accuracy Shen et al. (2007) 97.", "startOffset": 16, "endOffset": 35}, {"referenceID": 70, "context": "System Accuracy Shen et al. (2007) 97.33% Toutanova et al. (2003) 97.", "startOffset": 16, "endOffset": 66}, {"referenceID": 26, "context": "24% Gim\u00e9nez and M\u00e0rquez (2004) 97.", "startOffset": 4, "endOffset": 31}, {"referenceID": 26, "context": "24% Gim\u00e9nez and M\u00e0rquez (2004) 97.16% (a) POS System F1 Shen and Sarkar (2005) 95.", "startOffset": 4, "endOffset": 79}, {"referenceID": 26, "context": "24% Gim\u00e9nez and M\u00e0rquez (2004) 97.16% (a) POS System F1 Shen and Sarkar (2005) 95.23% Sha and Pereira (2003) 94.", "startOffset": 4, "endOffset": 109}, {"referenceID": 26, "context": "24% Gim\u00e9nez and M\u00e0rquez (2004) 97.16% (a) POS System F1 Shen and Sarkar (2005) 95.23% Sha and Pereira (2003) 94.29% Kudo and Matsumoto (2001) 93.", "startOffset": 4, "endOffset": 142}, {"referenceID": 0, "context": "System F1 Ando and Zhang (2005) 89.", "startOffset": 10, "endOffset": 32}, {"referenceID": 0, "context": "System F1 Ando and Zhang (2005) 89.31% Florian et al. (2003) 88.", "startOffset": 10, "endOffset": 61}, {"referenceID": 0, "context": "System F1 Ando and Zhang (2005) 89.31% Florian et al. (2003) 88.76% Kudo and Matsumoto (2001) 88.", "startOffset": 10, "endOffset": 94}, {"referenceID": 0, "context": "System F1 Ando and Zhang (2005) 89.31% Florian et al. (2003) 88.76% Kudo and Matsumoto (2001) 88.31% (c) NER System F1 Koomen et al. (2005) 77.", "startOffset": 10, "endOffset": 140}, {"referenceID": 0, "context": "System F1 Ando and Zhang (2005) 89.31% Florian et al. (2003) 88.76% Kudo and Matsumoto (2001) 88.31% (c) NER System F1 Koomen et al. (2005) 77.92% Pradhan et al. (2005) 77.", "startOffset": 10, "endOffset": 169}, {"referenceID": 0, "context": "System F1 Ando and Zhang (2005) 89.31% Florian et al. (2003) 88.76% Kudo and Matsumoto (2001) 88.31% (c) NER System F1 Koomen et al. (2005) 77.92% Pradhan et al. (2005) 77.30% Haghighi et al. (2005) 77.", "startOffset": 10, "endOffset": 199}, {"referenceID": 29, "context": "(2003), who use maximum entropy classifiers, and a bidirectional dependency network (Heckerman et al., 2001) at inference, reach 97.", "startOffset": 84, "endOffset": 108}, {"referenceID": 80, "context": "Toutanova et al. (2003), who use maximum entropy classifiers, and a bidirectional dependency network (Heckerman et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 26, "context": "Gim\u00e9nez and M\u00e0rquez (2004) proposed a SVM approach also trained on text windows, with bidirectional inference achieved with two Viterbi decoders (left-to-right and right-to-left).", "startOffset": 0, "endOffset": 27}, {"referenceID": 26, "context": "Gim\u00e9nez and M\u00e0rquez (2004) proposed a SVM approach also trained on text windows, with bidirectional inference achieved with two Viterbi decoders (left-to-right and right-to-left). They obtained 97.16% per-word accuracy. More recently, Shen et al. (2007) pushed the state-of-the-art up to 97.", "startOffset": 0, "endOffset": 254}, {"referenceID": 38, "context": "91% (Kudo and Matsumoto, 2001) using an ensemble of classifiers trained with different tagging conventions (see Section 3.", "startOffset": 4, "endOffset": 30}, {"referenceID": 71, "context": "Since then, a certain number of systems based on second-order random fields were reported (Sha and Pereira, 2003; McDonald et al., 2005; Sun et al., 2008), all reporting around 94.", "startOffset": 90, "endOffset": 154}, {"referenceID": 51, "context": "Since then, a certain number of systems based on second-order random fields were reported (Sha and Pereira, 2003; McDonald et al., 2005; Sun et al., 2008), all reporting around 94.", "startOffset": 90, "endOffset": 154}, {"referenceID": 77, "context": "Since then, a certain number of systems based on second-order random fields were reported (Sha and Pereira, 2003; McDonald et al., 2005; Sun et al., 2008), all reporting around 94.", "startOffset": 90, "endOffset": 154}, {"referenceID": 38, "context": "Kudoh and Matsumoto (2000) won the CoNLL 2000 challenge on chunking with a F1score of 93.", "startOffset": 0, "endOffset": 27}, {"referenceID": 38, "context": "91% (Kudo and Matsumoto, 2001) using an ensemble of classifiers trained with different tagging conventions (see Section 3.2.3). Since then, a certain number of systems based on second-order random fields were reported (Sha and Pereira, 2003; McDonald et al., 2005; Sun et al., 2008), all reporting around 94.3% F1 score. These systems use features composed of words, POS tags, and tags. More recently, Shen and Sarkar (2005) obtained 95.", "startOffset": 5, "endOffset": 425}, {"referenceID": 21, "context": "Florian et al. (2003) presented the best system at the NER CoNLL 2003 challenge, with 88.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "Chieu (2003), the second best performer of CoNLL 2003 (88.", "startOffset": 0, "endOffset": 13}, {"referenceID": 0, "context": "Later, Ando and Zhang (2005) reached 89.", "startOffset": 7, "endOffset": 29}, {"referenceID": 58, "context": "In the PropBank (Palmer et al., 2005) formalism one assigns roles ARG0-5 to words that are arguments of a verb (or more technically, a predicate) in the sentence, e.", "startOffset": 16, "endOffset": 37}, {"referenceID": 24, "context": "Feature categories commonly used by these system include (Gildea and Jurafsky, 2002; Pradhan et al., 2004): \u2022 the parts of speech and syntactic labels of words and nodes in the tree; \u2022 the node\u2019s position (left or right) in relation to the verb; \u2022 the syntactic path to the verb in the parse tree; \u2022 whether a node in the parse tree is part of a noun or verb phrase; \u2022 the voice of the sentence: active or passive; \u2022 the node\u2019s head word; and \u2022 the verb sub-categorization.", "startOffset": 57, "endOffset": 106}, {"referenceID": 62, "context": "Feature categories commonly used by these system include (Gildea and Jurafsky, 2002; Pradhan et al., 2004): \u2022 the parts of speech and syntactic labels of words and nodes in the tree; \u2022 the node\u2019s position (left or right) in relation to the verb; \u2022 the syntactic path to the verb in the parse tree; \u2022 whether a node in the parse tree is part of a noun or verb phrase; \u2022 the voice of the sentence: active or passive; \u2022 the node\u2019s head word; and \u2022 the verb sub-categorization.", "startOffset": 57, "endOffset": 106}, {"referenceID": 48, "context": "(2005) hold the state-of-the-art with Winnow-like (Littlestone, 1988) classifiers, followed by a decoding stage based on an integer program that enforces specific constraints on SRL tags.", "startOffset": 50, "endOffset": 69}, {"referenceID": 25, "context": "In the same spirit, Haghighi et al. (2005) use log-linear models on each tree node, re-ranked globally with a dynamic algorithm.", "startOffset": 20, "endOffset": 43}, {"referenceID": 14, "context": "04% using the five top Charniak parse trees. Koomen et al. (2005) hold the state-of-the-art with Winnow-like (Littlestone, 1988) classifiers, followed by a decoding stage based on an integer program that enforces specific constraints on SRL tags.", "startOffset": 23, "endOffset": 66}, {"referenceID": 14, "context": "04% using the five top Charniak parse trees. Koomen et al. (2005) hold the state-of-the-art with Winnow-like (Littlestone, 1988) classifiers, followed by a decoding stage based on an integer program that enforces specific constraints on SRL tags. They reach 77.92% F1 on CoNLL 2005, thanks to the five top parse trees produced by the Charniak (2000) parser (only the first one was provided by the contest) as well as the Collins (1999) parse tree.", "startOffset": 23, "endOffset": 350}, {"referenceID": 14, "context": "04% using the five top Charniak parse trees. Koomen et al. (2005) hold the state-of-the-art with Winnow-like (Littlestone, 1988) classifiers, followed by a decoding stage based on an integer program that enforces specific constraints on SRL tags. They reach 77.92% F1 on CoNLL 2005, thanks to the five top parse trees produced by the Charniak (2000) parser (only the first one was provided by the contest) as well as the Collins (1999) parse tree.", "startOffset": 23, "endOffset": 436}, {"referenceID": 83, "context": "The POS task is evaluated by computing the per-word accuracy, as it is the case for the standard benchmark we refer to (Toutanova et al., 2003).", "startOffset": 119, "endOffset": 143}, {"referenceID": 37, "context": "(2003) or additional parse trees for SRL systems (Koomen et al., 2005).", "startOffset": 49, "endOffset": 70}, {"referenceID": 73, "context": "Combining multiple systems or tweaking carefully features is also a common approach, like in the chunking top system (Shen and Sarkar, 2005).", "startOffset": 117, "endOffset": 140}, {"referenceID": 83, "context": "For that reason, we will refer to benchmark systems, that is, top existing systems which avoid usage of external data and have been well-established in the NLP field: (Toutanova et al., 2003) for POS and (Sha and Pereira, 2003) for chunking.", "startOffset": 167, "endOffset": 191}, {"referenceID": 71, "context": ", 2003) for POS and (Sha and Pereira, 2003) for chunking.", "startOffset": 20, "endOffset": 43}, {"referenceID": 0, "context": "For NER we consider (Ando and Zhang, 2005) as they were using additional unlabeled data only.", "startOffset": 20, "endOffset": 42}, {"referenceID": 37, "context": "We picked (Koomen et al., 2005) for SRL, keeping in mind they use 4 additional parse trees not provided by the challenge.", "startOffset": 10, "endOffset": 31}, {"referenceID": 22, "context": "It is thus not surprising to see many top CoNLL systems using external labeled data, like additional NER classifiers for the NER architecture of Florian et al. (2003) or additional parse trees for SRL systems (Koomen et al.", "startOffset": 145, "endOffset": 167}, {"referenceID": 21, "context": "It has the advantage of being slightly cheaper to compute (compared to the exact hyperbolic tangent), while leaving the generalization performance unchanged (Collobert, 2004).", "startOffset": 157, "endOffset": 174}, {"referenceID": 86, "context": "When using neural networks, the natural choice to tackle this problem becomes a convolutional approach, first introduced by Waibel et al. (1989) and also called Time Delay Neural Networks (TDNNs) in the literature.", "startOffset": 124, "endOffset": 145}, {"referenceID": 8, "context": "This score can be interpreted as a conditional tag probability p(i |x, \u03b8) by applying a softmax (Bridle, 1990) operation over all the tags:", "startOffset": 96, "endOffset": 110}, {"referenceID": 7, "context": "Remark 3 (Graph Transformer Networks) Our approach is a particular case of the discriminative forward training for graph transformer networks (GTNs) (Bottou et al., 1997; Le Cun et al., 1998).", "startOffset": 149, "endOffset": 191}, {"referenceID": 40, "context": "If this was the case, the scores could be viewed as the logarithms of conditional transition probabilities, and our model would be subject to the label-bias problem that motivates Conditional Random Fields (CRFs) (Lafferty et al., 2001).", "startOffset": 213, "endOffset": 236}, {"referenceID": 40, "context": "CRFs have been widely used in the NLP world, such as for POS tagging (Lafferty et al., 2001), chunking (Sha and Pereira, 2003), NER (McCallum and Li, 2003) or SRL (Cohn and Blunsom, 2005).", "startOffset": 69, "endOffset": 92}, {"referenceID": 71, "context": ", 2001), chunking (Sha and Pereira, 2003), NER (McCallum and Li, 2003) or SRL (Cohn and Blunsom, 2005).", "startOffset": 18, "endOffset": 41}, {"referenceID": 49, "context": ", 2001), chunking (Sha and Pereira, 2003), NER (McCallum and Li, 2003) or SRL (Cohn and Blunsom, 2005).", "startOffset": 47, "endOffset": 70}, {"referenceID": 19, "context": ", 2001), chunking (Sha and Pereira, 2003), NER (McCallum and Li, 2003) or SRL (Cohn and Blunsom, 2005).", "startOffset": 78, "endOffset": 102}, {"referenceID": 4, "context": "3 Stochastic Gradient Maximizing (8) with stochastic gradient (Bottou, 1991) is achieved by iteratively selecting a random example (x, y) and making a gradient step:", "startOffset": 62, "endOffset": 76}, {"referenceID": 42, "context": "Remark 6 (Modular Approach) The well known \u201cback-propagation\u201d algorithm (LeCun, 1985; Rumelhart et al., 1986) computes gradients using the chain rule.", "startOffset": 72, "endOffset": 109}, {"referenceID": 68, "context": "Remark 6 (Modular Approach) The well known \u201cback-propagation\u201d algorithm (LeCun, 1985; Rumelhart et al., 1986) computes gradients using the chain rule.", "startOffset": 72, "endOffset": 109}, {"referenceID": 4, "context": "compute derivatives with respect to its inputs and with respect to its trainable parameters, as proposed by Bottou and Gallinari (1991). This allows us to easily build variants of our networks.", "startOffset": 108, "endOffset": 136}, {"referenceID": 43, "context": "Remark 7 (Tricks) Many tricks have been reported for training neural networks (LeCun et al., 1998).", "startOffset": 78, "endOffset": 98}, {"referenceID": 60, "context": "We employed only two of them: the initialization and update of the parameters of each network layer were done according to the \u201cfan-in\u201d of the layer, that is the number of inputs used to compute each output of this layer (Plaut and Hinton, 1987).", "startOffset": 221, "endOffset": 245}, {"referenceID": 46, "context": "This result is in line with existing NLP studies comparing sentence-level and wordlevel likelihoods (Liang et al., 2008).", "startOffset": 100, "endOffset": 120}, {"referenceID": 43, "context": "Second order methods (LeCun et al., 1998) could be another speedup technique.", "startOffset": 21, "endOffset": 41}, {"referenceID": 44, "context": "Our second English corpus is composed by adding an extra 221 million words extracted from the Reuters RCV1 (Lewis et al., 2004) dataset.", "startOffset": 107, "endOffset": 127}, {"referenceID": 67, "context": "Similar language models were already proposed by Bengio and Ducharme (2001) and Schwenk and Gauvain (2002). Their goal was to estimate the probability of a word given the previous words in a sentence.", "startOffset": 80, "endOffset": 107}, {"referenceID": 67, "context": "Similar language models were already proposed by Bengio and Ducharme (2001) and Schwenk and Gauvain (2002). Their goal was to estimate the probability of a word given the previous words in a sentence. Estimating conditional probabilities suggests a crossentropy criterion similar to those described in Section 3.3.1. Because the dictionary size is large, computing the normalization term can be extremely demanding, and sophisticated approximations are required. More importantly for us, neither work leads to significant word embeddings being reported. Shannon (1951) has estimated the entropy of the English language between 0.", "startOffset": 80, "endOffset": 569}, {"referenceID": 20, "context": "Cover and King (1978) give a lower bound of 1.", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "Meanwhile, using a simple word trigram model, Brown et al. (1992b) reach 1.", "startOffset": 46, "endOffset": 67}, {"referenceID": 9, "context": "Meanwhile, using a simple word trigram model, Brown et al. (1992b) reach 1.75 bits per character. Teahan and Cleary (1996) obtain entropies as low as 1.", "startOffset": 46, "endOffset": 123}, {"referenceID": 35, "context": "However, Klein and Manning (2002) describe a rare example of realistic unsupervised grammar induction using a cross-entropy approach on binary-branching parsing trees, that is, by forcing the system to generate a hierarchical representation.", "startOffset": 9, "endOffset": 34}, {"referenceID": 18, "context": "We propose here to use a pairwise ranking approach (Cohen et al., 1998).", "startOffset": 51, "endOffset": 71}, {"referenceID": 17, "context": "Because the ranking literature often deals with information retrieval applications, many authors define complex ranking criteria that give more weight to the ordering of the best ranking instances (see Burges et al., 2007; Cl\u00e9men\u00e7on and Vayatis, 2007).", "startOffset": 197, "endOffset": 251}, {"referenceID": 57, "context": "Okanohara and Tsujii (2007) use a related approach to avoiding the entropy criteria using a binary classification approach (correct/incorrect phrase).", "startOffset": 0, "endOffset": 28}, {"referenceID": 57, "context": "Okanohara and Tsujii (2007) use a related approach to avoiding the entropy criteria using a binary classification approach (correct/incorrect phrase). Their work focuses on using a kernel classifier, and not on learning word embeddings as we do here. Smith and Eisner (2005) also propose a contrastive criterion which estimates the likelihood of the data conditioned to a \u201cnegative\u201d neighborhood.", "startOffset": 0, "endOffset": 275}, {"referenceID": 3, "context": "(Bengio et al., 2009) provides a more detailed discussion of this, the (as yet, poorly understood) \u201ccurriculum\u201d process.", "startOffset": 0, "endOffset": 21}, {"referenceID": 67, "context": "Previous semi-supervised approaches for NLP can be roughly categorized as follows: \u2022 Ad-hoc approaches such as (Rosenfeld and Feldman, 2007) for relation extraction.", "startOffset": 111, "endOffset": 140}, {"referenceID": 85, "context": "\u2022 Self-training approaches, such as (Ueffing et al., 2007) for machine translation, and (McClosky et al.", "startOffset": 36, "endOffset": 58}, {"referenceID": 50, "context": ", 2007) for machine translation, and (McClosky et al., 2006) for parsing.", "startOffset": 37, "endOffset": 60}, {"referenceID": 34, "context": "Transductive approaches, such as (Joachims, 1999) for text classification can be viewed as a refined form of self-training.", "startOffset": 33, "endOffset": 49}, {"referenceID": 0, "context": "\u2022 Parameter sharing approaches such as (Ando and Zhang, 2005; Suzuki and Isozaki, 2008).", "startOffset": 39, "endOffset": 87}, {"referenceID": 81, "context": "\u2022 Parameter sharing approaches such as (Ando and Zhang, 2005; Suzuki and Isozaki, 2008).", "startOffset": 39, "endOffset": 87}, {"referenceID": 30, "context": "Note that our procedure is clearly linked to the (semi-supervised) deep learning procedures of (Hinton et al., 2006; Bengio et al., 2007; Weston et al., 2008).", "startOffset": 95, "endOffset": 158}, {"referenceID": 2, "context": "Note that our procedure is clearly linked to the (semi-supervised) deep learning procedures of (Hinton et al., 2006; Bengio et al., 2007; Weston et al., 2008).", "startOffset": 95, "endOffset": 158}, {"referenceID": 87, "context": "Note that our procedure is clearly linked to the (semi-supervised) deep learning procedures of (Hinton et al., 2006; Bengio et al., 2007; Weston et al., 2008).", "startOffset": 95, "endOffset": 158}, {"referenceID": 25, "context": "6 Ranking and Language There is a large agreement in the NLP community that syntax is a necessary prerequisite for semantic role labeling (Gildea and Palmer, 2002).", "startOffset": 138, "endOffset": 163}, {"referenceID": 14, "context": "The parsers themselves (Charniak, 2000; Collins, 1999) contain considerable prior information about syntax (one can think of this as a kind of informed pre-processing).", "startOffset": 23, "endOffset": 54}, {"referenceID": 20, "context": "The parsers themselves (Charniak, 2000; Collins, 1999) contain considerable prior information about syntax (one can think of this as a kind of informed pre-processing).", "startOffset": 23, "endOffset": 54}, {"referenceID": 16, "context": "The lack of hierarchical representation seems a fatal flaw (Chomsky, 1956).", "startOffset": 59, "endOffset": 74}, {"referenceID": 28, "context": "However, ranking is closely related to an alternative description of the language structure: operator grammars (Harris, 1968).", "startOffset": 111, "endOffset": 125}, {"referenceID": 12, "context": "Caruana (1997) gives a good overview of these past efforts.", "startOffset": 0, "endOffset": 15}, {"referenceID": 33, "context": "For instance, modern speech recognition systems use Bayes rule to combine the outputs of an acoustic model trained on speech data and a language model trained on phonetic or textual corpora (Jelinek, 1976).", "startOffset": 190, "endOffset": 205}, {"referenceID": 33, "context": "For instance, modern speech recognition systems use Bayes rule to combine the outputs of an acoustic model trained on speech data and a language model trained on phonetic or textual corpora (Jelinek, 1976). This joint decoding approach has been successfully applied to structurally more complex NLP tasks. Sutton and McCallum (2005b) obtains improved results by combining the predictions of independently trained CRF models using a joint decoding process at test time that requires more sophisticated probabilistic inference techniques.", "startOffset": 191, "endOffset": 334}, {"referenceID": 33, "context": "For instance, modern speech recognition systems use Bayes rule to combine the outputs of an acoustic model trained on speech data and a language model trained on phonetic or textual corpora (Jelinek, 1976). This joint decoding approach has been successfully applied to structurally more complex NLP tasks. Sutton and McCallum (2005b) obtains improved results by combining the predictions of independently trained CRF models using a joint decoding process at test time that requires more sophisticated probabilistic inference techniques. On the other hand, Sutton and McCallum (2005a) obtain results somewhat below the state-of-the-art using joint decoding for SRL and syntactic parsing.", "startOffset": 191, "endOffset": 584}, {"referenceID": 33, "context": "For instance, modern speech recognition systems use Bayes rule to combine the outputs of an acoustic model trained on speech data and a language model trained on phonetic or textual corpora (Jelinek, 1976). This joint decoding approach has been successfully applied to structurally more complex NLP tasks. Sutton and McCallum (2005b) obtains improved results by combining the predictions of independently trained CRF models using a joint decoding process at test time that requires more sophisticated probabilistic inference techniques. On the other hand, Sutton and McCallum (2005a) obtain results somewhat below the state-of-the-art using joint decoding for SRL and syntactic parsing. Musillo and Merlo (2006) also describe a negative result at the same joint task.", "startOffset": 191, "endOffset": 712}, {"referenceID": 76, "context": "It is then sufficient to train a model that computes multiple outputs for each pattern (Suddarth and Holden, 1991).", "startOffset": 87, "endOffset": 114}, {"referenceID": 56, "context": "makes sense when the training data blocks these additional dependency paths (in the sense of d-separation, Pearl, 1988). This implies that, without joint training, the additional dependency paths cannot directly involve unobserved variables. Therefore, the natural idea of discovering common internal representations across tasks requires joint training. Joint training is relatively straightforward when the training sets for the individual tasks contain the same patterns with different labels. It is then sufficient to train a model that computes multiple outputs for each pattern (Suddarth and Holden, 1991). Using this scheme, Sutton et al. (2007) demonstrates improvements on POS tagging and nounphrase chunking using jointly trained CRFs.", "startOffset": 107, "endOffset": 653}, {"referenceID": 51, "context": "Miller et al. (2000) achieves performance improvements by jointly training NER, parsing, and relation extraction in a statistical parsing model.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Ando and Zhang (2005) propose a setup that works around the joint labeling requirements.", "startOffset": 0, "endOffset": 22}, {"referenceID": 66, "context": "For instance, Ratnaparkhi (1996)", "startOffset": 14, "endOffset": 33}, {"referenceID": 61, "context": "We also tried suffixes obtained with the Porter (1980) stemmer and obtained the same performance as when using two character suffixes.", "startOffset": 41, "endOffset": 55}, {"referenceID": 25, "context": "State-of-the-art SRL systems exploit parse trees (Gildea and Palmer, 2002; Punyakanok et al., 2005), related to CHUNK tags, and built using POS tags (Charniak, 2000; Collins, 1999).", "startOffset": 49, "endOffset": 99}, {"referenceID": 64, "context": "State-of-the-art SRL systems exploit parse trees (Gildea and Palmer, 2002; Punyakanok et al., 2005), related to CHUNK tags, and built using POS tags (Charniak, 2000; Collins, 1999).", "startOffset": 49, "endOffset": 99}, {"referenceID": 14, "context": ", 2005), related to CHUNK tags, and built using POS tags (Charniak, 2000; Collins, 1999).", "startOffset": 57, "endOffset": 88}, {"referenceID": 20, "context": ", 2005), related to CHUNK tags, and built using POS tags (Charniak, 2000; Collins, 1999).", "startOffset": 57, "endOffset": 88}, {"referenceID": 68, "context": "For instance, Shen and Sarkar (2005) describe a chunking system that uses POS tags as input; Florian et al.", "startOffset": 14, "endOffset": 37}, {"referenceID": 21, "context": "For instance, Shen and Sarkar (2005) describe a chunking system that uses POS tags as input; Florian et al. (2003) describes a NER system whose inputs include POS and CHUNK tags, as well as the output of two other NER classifiers.", "startOffset": 93, "endOffset": 115}, {"referenceID": 1, "context": "4 Ensembles Constructing ensembles of classifiers is a proven way to trade computational efficiency for generalization performance (Bell et al., 2007).", "startOffset": 131, "endOffset": 150}, {"referenceID": 56, "context": "On the other hand, multiple training times could be improved using smart sampling strategies (Neal, 1996).", "startOffset": 93, "endOffset": 105}, {"referenceID": 43, "context": "The local minima found by the training algorithm are usually good local minima, thanks to the oversized parameter space and to the noise induced by the stochastic gradient procedure (LeCun et al., 1998).", "startOffset": 182, "endOffset": 202}, {"referenceID": 38, "context": "For instance, Kudo and Matsumoto (2001) use an ensemble of classifiers trained with different tagging conventions (see Section 3.", "startOffset": 14, "endOffset": 40}, {"referenceID": 37, "context": "State-ofthe-art systems often exploit additional parse trees such as the k top ranking parse trees (Koomen et al., 2005; Haghighi et al., 2005).", "startOffset": 99, "endOffset": 143}, {"referenceID": 27, "context": "State-ofthe-art systems often exploit additional parse trees such as the k top ranking parse trees (Koomen et al., 2005; Haghighi et al., 2005).", "startOffset": 99, "endOffset": 143}, {"referenceID": 23, "context": "5 Parsing Gildea and Palmer (2002) offer several arguments suggesting that syntactic parsing is a necessary prerequisite for the SRL task.", "startOffset": 10, "endOffset": 35}, {"referenceID": 14, "context": "The CoNLL 2005 SRL benchmark task provides parse trees computed using both the Charniak (2000) and Collins (1999) parsers.", "startOffset": 79, "endOffset": 95}, {"referenceID": 14, "context": "The CoNLL 2005 SRL benchmark task provides parse trees computed using both the Charniak (2000) and Collins (1999) parsers.", "startOffset": 79, "endOffset": 114}, {"referenceID": 14, "context": "We show performance of our system fed with different levels of depth of the Charniak parse tree. We report previous results of our architecture with no parse tree as a baseline. Koomen et al. (2005) report test and validation performance using six parse trees, as well as validation performance using only the top Charniak parse tree.", "startOffset": 76, "endOffset": 199}, {"referenceID": 35, "context": "Koomen et al. (2005) also report a 74.", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "76% F1 score on the validation set using only the Charniak parse tree. Using the first three parse tree levels, we reach this performance on the validation set. We also reported in Table 12 our previous performance obtained with the CHUNK feature (see Table 10). It is surprising to observe that adding chunking features into the semantic role labeling network performs significantly worse than adding features describing the level 0 of the Charniak parse tree (Table 12). Indeed, if we ignore the label prefixes \u201cBIES\u201d defining the segmentation, the parse tree leaves (at level 0) and the chunking have identical labeling. However, the parse trees identify leaf sentence segments that are often smaller than those identified by the chunking tags, as shown by Hollingshead et al. (2005).16 Instead of relying on Charniak parser, we chose to train a second chunking network to identify the segments delimited by the leaves of the Penn Treebank parse trees (level 0).", "startOffset": 50, "endOffset": 787}, {"referenceID": 31, "context": "As in (Hollingshead et al., 2005), consider the sentence and chunk labels \u201c(NP They) (VP are starting to buy) (NP growth stocks)\u201d.", "startOffset": 6, "endOffset": 33}, {"referenceID": 69, "context": "The induced word representation has been used with success in a wide variety of NLP tasks, including POS (Sch\u00fctze, 1995), NER (Miller et al.", "startOffset": 105, "endOffset": 120}, {"referenceID": 53, "context": "The induced word representation has been used with success in a wide variety of NLP tasks, including POS (Sch\u00fctze, 1995), NER (Miller et al., 2004; Ratinov and Roth, 2009), or parsing (Koo et al.", "startOffset": 126, "endOffset": 171}, {"referenceID": 65, "context": "The induced word representation has been used with success in a wide variety of NLP tasks, including POS (Sch\u00fctze, 1995), NER (Miller et al., 2004; Ratinov and Roth, 2009), or parsing (Koo et al.", "startOffset": 126, "endOffset": 171}, {"referenceID": 36, "context": ", 2004; Ratinov and Roth, 2009), or parsing (Koo et al., 2008).", "startOffset": 44, "endOffset": 62}, {"referenceID": 47, "context": "Other related approaches exist, like phrase clustering (Lin and Wu, 2009) which has been shown to work well for NER.", "startOffset": 55, "endOffset": 73}, {"referenceID": 47, "context": "Mnih and Hinton (2007) proposed a related language model approach inspired from Restricted Boltzmann Machines.", "startOffset": 0, "endOffset": 23}, {"referenceID": 9, "context": "One popular approach is the Brown clustering algorithm (Brown et al., 1992a), which builds hierachical word clusters by maximizing the bigram\u2019s mutual information. The induced word representation has been used with success in a wide variety of NLP tasks, including POS (Sch\u00fctze, 1995), NER (Miller et al., 2004; Ratinov and Roth, 2009), or parsing (Koo et al., 2008). Other related approaches exist, like phrase clustering (Lin and Wu, 2009) which has been shown to work well for NER. Finally, Huang and Yates (2009) have recently proposed a smoothed language modelling approach based on a Hidden Markov Model, with success on POS and Chunking tasks.", "startOffset": 56, "endOffset": 517}, {"referenceID": 63, "context": "Following the Ratinov and Roth (2009) and Koo et al.", "startOffset": 14, "endOffset": 38}, {"referenceID": 36, "context": "Following the Ratinov and Roth (2009) and Koo et al. (2008) setups, we generated 1, 000 Brown clusters using the implementation17 from Liang (2005).", "startOffset": 42, "endOffset": 60}, {"referenceID": 36, "context": "Following the Ratinov and Roth (2009) and Koo et al. (2008) setups, we generated 1, 000 Brown clusters using the implementation17 from Liang (2005). To make the comparison fair, the clusters were first induced on the concatenation of Wikipedia and Reuters datasets, as we did in Section 4 for training our largest language model LM2, using a 130K word dictionary.", "startOffset": 42, "endOffset": 148}, {"referenceID": 84, "context": "For instance, Turian et al. (2010) performed a comparison of Brown Clusters and embeddings trained in the same spirit as ours18, with additional features combining labels and tokens.", "startOffset": 14, "endOffset": 35}, {"referenceID": 81, "context": "Table 16 compares the tagging speeds for our system and for the few available state-of-the-art systems: the Toutanova et al. (2003) POS tagger20, the Shen et al.", "startOffset": 108, "endOffset": 132}, {"referenceID": 73, "context": "(2003) POS tagger20, the Shen et al. (2007) POS tagger21 and the Koomen et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 37, "context": "(2007) POS tagger21 and the Koomen et al. (2005) SRL system.", "startOffset": 28, "endOffset": 49}, {"referenceID": 84, "context": "Secondly, they predict the correctness of the final word of each window instead of the center word (Turian et al., 2010), effectively restricting the model to unidirectional prediction.", "startOffset": 99, "endOffset": 120}, {"referenceID": 82, "context": "POS System RAM (MB) Time (s) Toutanova et al. (2003) 800 64 Shen et al.", "startOffset": 29, "endOffset": 53}, {"referenceID": 74, "context": "(2003) 800 64 Shen et al. (2007) 2200 833 SENNA 32 4", "startOffset": 14, "endOffset": 33}, {"referenceID": 37, "context": "SRL System RAM (MB) Time (s) Koomen et al. (2005) 3400 6253 SENNA 124 51", "startOffset": 29, "endOffset": 50}, {"referenceID": 42, "context": "Following the classical \u201cback-propagation\u201d derivations (LeCun, 1985; Rumelhart et al., 1986) and the modular approach shown in (Bottou, 1991), any feed-forward neural network with L layers, like the ones shown in Figure 1 and Figure 2, can be seen as a composition of functions f l \u03b8(\u00b7), corresponding to each layer l: f\u03b8(\u00b7) = f \u03b8 (fL\u22121 \u03b8 (.", "startOffset": 55, "endOffset": 92}, {"referenceID": 68, "context": "Following the classical \u201cback-propagation\u201d derivations (LeCun, 1985; Rumelhart et al., 1986) and the modular approach shown in (Bottou, 1991), any feed-forward neural network with L layers, like the ones shown in Figure 1 and Figure 2, can be seen as a composition of functions f l \u03b8(\u00b7), corresponding to each layer l: f\u03b8(\u00b7) = f \u03b8 (fL\u22121 \u03b8 (.", "startOffset": 55, "endOffset": 92}, {"referenceID": 4, "context": ", 1986) and the modular approach shown in (Bottou, 1991), any feed-forward neural network with L layers, like the ones shown in Figure 1 and Figure 2, can be seen as a composition of functions f l \u03b8(\u00b7), corresponding to each layer l: f\u03b8(\u00b7) = f \u03b8 (fL\u22121 \u03b8 (.", "startOffset": 42, "endOffset": 56}], "year": 2011, "abstractText": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.", "creator": "LaTeX with hyperref package"}}}