{"id": "1104.4681", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2011", "title": "Performance Evaluation of Statistical Approaches for Text Independent Speaker Recognition Using Source Feature", "abstract": "This paper introduces the performance evaluation of statistical approaches for TextIndependent speaker recognition system using source feature. Linear prediction LP residual is used as a representation of excitation information in speech. The speaker-specific information in the excitation of voiced speech is captured using statistical approaches such as Gaussian Mixture Models GMMs and Hidden Markov Models HMMs. The decrease in the error during training and recognizing speakers during testing phase close to 100 percent accuracy demonstrates that the excitation component of speech contains speaker-specific information and is indeed being effectively captured by continuous Ergodic HMM than GMM. The performance of the speaker recognition system is evaluated on GMM and 2 state ergodic HMM with different mixture components and test speech duration. We demonstrate the speaker recognition studies on TIMIT database for both GMM and Ergodic HMM.", "histories": [["v1", "Mon, 25 Apr 2011 05:00:25 GMT  (233kb)", "http://arxiv.org/abs/1104.4681v1", "8 pages, 7 figures, International Journal"]], "COMMENTS": "8 pages, 7 figures, International Journal", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["r rajeswara rao", "v kamakshi prasad", "a nagesh"], "accepted": false, "id": "1104.4681"}, "pdf": {"name": "1104.4681.pdf", "metadata": {"source": "CRF", "title": "Performance Evaluation of Statistical Approaches for Text- Independent Speaker Recognition Using Source Feature", "authors": ["Rajeswara Rao", "Kamakshi Prasad"], "emails": ["raob4u@yahoo.com", "akknagesh@rediffmail.com"], "sections": [{"heading": null, "text": "information in speech. The speaker-specific information in the excitation of voiced speech is captured using statistical approaches such as Gaussian Mixture Models (GMMs) and Hidden Markov Models (HMMs). The decrease in the error during training and recognizing speakers during testing phase close to 100% accuracy demonstrates that the excitation component of speech contains speaker-specific information and is indeed being effectively captured by continuous Ergodic HMM than GMM. The performance of the speaker recognition system is evaluated on GMM and 2-state ergodic HMM with different mixture components and test speech duration. We demonstrate the speaker recognition studies on TIMIT database for both GMM and Ergodic HMM.\nIndex Terms: Ergodic, LP residual, MFCC, Speaker."}, {"heading": "1. INTRODUCTION", "text": "Within the past decade, technological advances such as telebanking and remote collaborative data processing over large computer networks have increased the demand for improved methods of information security. For personal information including medical records, bank accounts and credit history, the ability to verify the identity of individuals attempting to access such data is critical. To date, low-cost methods such as passwords, personal identification numbers and magnetic cards have been widely used. More advanced security measures have also been developed (e.g., face recognizers, retinal scanners, as well as automatic finger print analyzers). The uses of these procedures have been limited by both cost and ease of use. In recent years, speaker recognition (recognizing a person from his/her voice by a machine) and verification algorithms have received considerable attention. There are several reasons for this interest. In particular, speech provides a convenient and natural form of input, conveys a significant amount of speaker dependent information.\nSpeech is a composite signal which carries information about the message, the speaker identity and the language identity [1], [2]. It is difficult to isolate the speaker specific features alone from the signal. The speaker characteristics present in the signal can be attributed to the anatomical and the behavioural aspects of the speech production mechanism. The representation of the behavioural characteristics is a difficult task, and usually requires large amount of data.\nAutomatic speaker recognition systems rely mainly on features derived from the physiological characteristics of the speaker.\nSpeech is produced as sequence of sounds. Hence the state of the vocal folds, shape and size of various articulators, change over time to reflect the sound being produced. To produce a particular sound the articulators have to be positioned in a particular way. When different speakers try to produce same\nsound, though their vocal tracts are positioned in a similar manner, the actual vocal tract shapes will be different due to differences in the anatomical structure of the vocal tract. System features represent the structure of vocal tract. The movements of vocal folds vary from one speaker to another, the manner and speed in which the vocal folds close also varies across speakers. As a result different voices are produced by different speakers. The variations in the vibrations of the vocal folds represent the source features.\nThe theory of Linear Prediction (LP) is closely linked to modelling of the vocal tract system, and relies upon the fact that a particular speech sample may be predicted by a linear combination of previous samples. The number of previous samples used for prediction is known as the order of the prediction. The weights applied to each of the previous speech samples are known as Linear Prediction Coefficients (LPC). They are calculated so as to minimize the prediction error [4].\nA study into the use of LPC for speaker recognition was carried out by [3]. These coefficients are highly correlated, and the use of all prediction coefficients may not be necessary for speaker recognition task [6] [7] used a method called orthogonal linear prediction. It is shown that only a small subset of the resulting orthogonal coefficients exhibits significant variation over the duration of an utterance. It is also shown that reflection coefficients are as good as the other feature sets. [8] Used principal spectral components derived from linear prediction coefficients for speaker verification task. Hence a detailed exploration to know the speaker- specific excitation information present in the residual of speech is needed and hence the motivation for the present work.\nIt has been shown that humans can recognize people by listening to the LP residual signal [9]. This may be attributed to the speaker-specific excitation information present at the segmental (10\u201330 ms) and suprasegmental levels (1\u20133 s). The\npresence of speaker-specific information at the segmental and suprasegmental levels can be established by generating signals that retain specific features at these levels. For instance, speakerspecific suprasegmental information like intonation and duration can be perceived in the signal which has impulses of appropriate strength at each pitch epoch in the voiced region, and at random instances in the unvoiced regions. Instants of significant excitation correspond to pitch epochs in case of voiced speech and some random excitation instants like onset of burst events in case of unvoiced speech. The LP residual has the additional information of the glottal pulse characteristics in the samples between two pitch epochs. Perceptually the signals will be different if these samples (related to the glottal pulse characteristics) are replaced by synthetic model signals [10] [11] [12], [13] or by random noise. It appears that significant speaker specific excitation information is present in the segmental and suprasegmental features of the residual. The present work focuses on extracting speaker-specific excitation information present at the segmental level of the residual.\nAt the segmental level, each short segment of the LP residual can be considered to belong to one of the five broad categories. They are voiced, unvoiced, plosive, silence and mixed excitation. The voiced excitation is the dominant mode of excitation during speech production. Further, if voiced excitation is replaced by random noise excitation, it is difficult to perceive the speaker\u2019s identity [13]. In this paper we demonstrate that the speaker specific characteristics are indeed present at the segmental level of the LP residual, and they can be reliably extracted using hidden Markov models.\nThe rest of the paper is organized as follows: In Section 2 we examine the speaker specific characteristics of the LP residual, and demonstrate the approach to extract the speakerspecific information from the residual signal. Finally we discuss feature extraction using Melceptral coefficients to capture the speaker specific information from the residual. Section 3 describes parametric approaches such as GMM and HMM based implementation for speaker recognition. Section 4 describes the database used in the study and the performance of speaker recognition systems based on the speaker specific features from the LP residual. The proposed speaker recognition system, based on the LP residual, may not require large amounts of data. Summary and conclusions of this study is presented at the end of the paper."}, {"heading": "2. SPEAKER CHARACTERISTICS IN THE LP RESIDUAL", "text": "Speech signals, as any other real world signals, are produced by exciting a system with source. A simple block diagram representation of the speech production mechanism is shown in the Figure 1. Vibrations of the vocal folds, powered by air coming from the lungs during exhalation, are the sound source for speech. As shown in the Figure 1, the glottal excitation forms the source, and the vocal tract forms the system. The philosophy of linear prediction is intimately related to the basic speech production model. The Linear Predictive Coding (LPC) analysis performs spectral analysis on short segments of speech with an all-pole modelling constraint [14]. Since speech\ncan be modelled as the output of linear, time-varying system excited by a source, LPC analysis captures the vocal tract system information in terms of coefficients of the filter representing the vocal tract mechanism. Hence, analysis of speech signal by linear prediction results in two components, namely the synthesis filter on one hand and the residual on the other hand. In brief, the LP residual signal is generated as a by product of the LPC analysis, and the computation of the residual signal is given below.\nIf the input signal is represented by ( )nu and the output signal by ( )ns , then the transfer function of the system can be expressed as,\n( ) ( ) ( )zU zSzH = (1)\nWhere ( )zS and ( )zU are z-transforms of ( )ns and ( )nu respectively.\nConsider the case where we have the output signal and the system and have to compute the input signal. The above equation can be expressed as\n( ) ( ) ( )zUzHzS = (2) ( ) ( ) ( )zH zSzU = (3)\n( ) ( ) ( )zSzH1zU = (4)\n( ) ( ) ( )zSzAzU = (5) Where A (z) = 1/ H(z) is the inverse filter representation of the vocal tract system."}, {"heading": "2.1 Computing LP Residual from Speech Signal", "text": "Linear prediction models the output ( )ns as the linear function of past outputs and present and past inputs. Since prediction is done by a linear function, the name linear prediction. Assuming an all-pole model for the vocal tract, the signal ( )ns can be expressed as a linear combination of past values and some input ( )nu as shown below.\n( ) ( ) ( )nGuknsns p\n1k +\u2212\u2212= \u2211 =\n(6)\nWhere G is a gain factor\nNow assuming that the input ( )nU is unknown, the signal ( )nS can be predicted only approximately from a linear\nweighted sum of past samples. Let this approximation of\n( )nS be ( )nS\u0302 , where\n\u2211 = \u2212\u2212= p 1k k )kn(sa)n(s\u0302 (7)\nThen the error between the actual value ( )ns and the predicted value ( )ns\u0302 is given by\n)n(Gu)n(s\u0302)n(s)n(e =\u2212= (8)\nThis error ( )ne is nothing but the LP residual of the signal shown in Figure 2.\nFigure 2. Actual signal and its LP residual"}, {"heading": "2.2 Feature extraction of LP residual signal", "text": "MFCC features have been used for extracting features from the source signal. MFCC is based on the known variation of the human ear\u2019s critical bandwidths with frequency. The MFCC technique makes use of two types of filters namely, linearly spaced filters and logarithmically spaced filters. To capture the phonetically important characteristics of speech, signal is expressed in the Mel frequency scale. This scale has a linear frequency spacing below 1000 Hz and a logarithmic spacing above 1000 Hz. Normal speech waveform may vary from time to time depending on the physical condition of speaker\u2019s vocal cords. MFFCs are less susceptible to the said variations [15]."}, {"heading": "2.3 Motivation to use Mel Frequency Cepstral Coefficients (MFCCs):", "text": "Since our interest is in capturing global features which correspond to glottal excitation, the low frequency components are to be emphasized. To fulfil this requirement it is felt that MFCC is most suitable as it emphasize low frequency and de-emphasize high frequencies."}, {"heading": "3. PARAMETRIC APPROACHES", "text": "Parametric approaches are model-based approaches. The parameters of the model are estimated using the training feature vectors. It is assumed that the model is adequate to represent the distribution. The most widely used parametric approaches are GMM and HMM based approaches."}, {"heading": "3.1 Gaussian Mixture Models", "text": "GMM is a classic parametric method best used to model speaker identities due to the fact that Gaussian components have the capability of representing some general speaker dependent spectral shapes. Gaussian classifier has been successfully\nemployed in the several text-independent speaker identification applications since the approach used by this classifier is similar to that used by the long term average of spectral features for representing a speaker\u2019s average vocal tract shape [16].\nAs shown in Figure 3 in a GMM model, the probability distribution of the observed data takes the form given by the following equation [17].\n\u2211 =\n= M\n1i ii )x(bp)|x(p \u03bb\nWhere M is the number of component densities x is a D\ndimensional observed data (random vector), )x(bi are the\ncomponent densities and Pi are the mixture weights for i = 1... M.\n      \u2212\u2212\u2212= \u2212 \u2211\u2211 )()( 2 1 exp ||)2( 1 )( 1 2/12/ ii T i\ni\nDi xxxb \u00b5\u00b5\n\u03c0\nEach component density )x(bi denotes a D-dimensional\nnormal distribution with mean vector i\u00b5 and covariance matrix i\u2211 . The mixture weights satisfy the condition\n1p M\n1i i =\u2211 = and therefore represent positive scalar values. These parameters can be collectively represented as\n{ }\u2211= iii ,,p \u00b5\u03bb for i = 1 \u2026 M. Each speaker in a speaker identification system can be represented by a GMM and is referred to by the speaker\u2019s respective models\u03bb .\nThe parameters of a GMM model can be estimated using Maximum Likelihood (ML) [19] estimation. The main objective of the ML estimation is to derive the optimum model parameters that can maximize the likelihood of GMM. Unfortunately direct maximization using ML estimation is not possible and therefore a special case of ML estimation known as Expectation-\nMaximization (EM) [19] algorithm is used to extract the model parameters. The GMM likelihood of a sequence of T training vectors { }T1 x,...xX = can be given as [17]\n)|x(p)|X(p t T\n1t \u03bb\u03bb \u220f = = . The EM algorithm begins with\nan initial model \u03bb and tends to estimate a new model \u03bb such that )|X(p)|X(p \u03bb\u03bb \u2265 [17]. This is an iterative process where the new model is considered to be an initial model in the next iteration and the entire process is repeated until a certain convergence threshold is obtained."}, {"heading": "3.2 Continuous Ergodic Hidden Markov Model for Speaker Recognition", "text": "As shown in the Figure 4 in the training phase, one HMM for each speaker is obtained (i.e., parameters of model are estimated) using training feature vectors. The parameters of HMM are [8] State-transition probability distribution: It is represented by [ ]ijaA = Where Nj,i1)iq|jq(Pa t1tij \u2264\u2264=== + (9) The above equation defines the probability of transition from state i to j at time t .\nFor a three state left-right model the state transition matrix is\ngiven as { }   \n\n\n   \n\n==\n33\n2322\n131211\n00\n0\na\naa\naaa\naA ij (10)\nThe state transition matrix of three state ergodic model is given by\n{ }   \n\n\n  \n\n ==\n3332a31a\n232221\n131211\nij\na\naaa\naaa\naA (11)\nObservation symbol probability distribution: It is given by\n( )[ ]kbB j= in which\nMk1)jq|VO(P)k(b tktj \u2264\u2264=== (12)\nThe above equation defines the symbol distribution in state Nj .....3,2,1= . The initial state distribution is given by ( )iqP == 1\u03c0 where\nNi1)iq(P 1i \u2264\u2264==\u03c0 (13)\nHere, N is the total number of states, and tq is the state at\ntime t , M is the number of distinct observation symbols per\nstate, and tO is the observation symbol at time t . In testing phase, ( )\u03bbOP for each model is calculated, where ( )TOOOOO ....321= . Here the goal is to find out the probability for a given model to which the test utterance belongs to. The speaker whose model gives the highest score is declared as the identified speaker. GMM corresponds to a single-state continuous ergodic HMM.\nThe model parameters can be collectively represented as ( )iii BA \u03c0\u03bb ,,= for Mi ........1= . Each speaker in a speaker identification system can be represented by a HMM and is referred to by the speaker\u2019s respective models\u03bb .\nIn the testing phase, p (O/\u03bb) for each model is calculated [19]. Where O= (o1o2o3\u2026OT) is the sequence of the test feature vectors. The goal is to find the probability, given the model that the test utterance belongs to that particular model. The speaker model that gives the highest score is declared as the indent."}, {"heading": "4. EXPERIMENTAL EVALUATION", "text": ""}, {"heading": "4.1 Database Used for the Study", "text": "In general, speaker recognition refers to both speaker identification and speaker verification. Speaker identification is the task of identifying a given speaker from a set of speakers. In the closed-set speaker identification no speaker outside the given set is used for testing. Speaker verification is the task of verifying the identity claim of a given speaker. The result of speaker verification is either to accept or reject the claim of the speaker. In this paper we consider identification task for TIMIT Speaker database.\nThe TIMIT corpus of read speech has been designed to provide speaker data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speaker recognition systems. TIMIT contains a total of 6300 sentences, 10 sentences spoken by each of 630 speakers from 8 major dialect regions of the United States. We consider 200 speakers out of 630 speakers for speaker recognition. Maximum of 30 seconds of speech data is used for training and minimum of 1 second of data for testing. In all the cases the speech signal was sampled at 16 kHz sampling frequency. Throughout this study, closed set identification experiments are done to demonstrate the feasibility of capturing the speaker-specific information from the source features. Requirement of number of mixtures duration of test data to get better accuracy is demonstrated."}, {"heading": "4.2 Experimental Setup", "text": "The system has been implemented in Matlab7 on Windows XP platform. We have used LP order of 12 for all experiments. We have trained the models GMM and HMM using total Gaussian components as 4, 8, 16 and 32 for any training, speech duration of 30 seconds testing is performed using different test speech durations such as 1 second, 3 seconds, and 5 seconds. The same setup has been implemented for both GMM and Ergodic HMM. Here, recognition rate is defined as the ratio of the number of speakers identified to the total number of speakers tested."}, {"heading": "5. PERFORMANCE EVALAUATION", "text": "There is no theoretical way to evaluate the performance of the statistical approaches. To evaluate the speaker recognition system the experiment is carried out for a GMM and 2-state HMM for varying number of Gaussian components such as 4, 8, 16 and 32. Here the model is trained with 30 seconds of speech duration, LP order of 12 and tested with 1 second, 3 seconds and 5 seconds as shown in the Figure 5, 6 and 7 respectively, the ergodic HMM for speaker recognition system outperformed GMM. The experimental results are tabulated in Table 1. The percentage recognition of 2-state ergodic HMM for different Gaussian components such as 4, 8, 16 and 32 seems to uniformly increasing. The minimum number of Gaussian components to achieve good recognition performance seems to be 16 and thereafter the recognition performance is minimal. The recognition performance of the Ergodic HMM drastically increases for the test speech duration of 1 second to 3 seconds Increasing the test speech duration from 3 seconds to 5 seconds improves the recognition performance with small improvement."}, {"heading": "6. CONCLUSION", "text": "In this work we have demonstrated the importance of information in the excitation component of speech for speaker recognition task. Linear prediction residual was used to represent the excitation information. Performance of the recognition experiments shows that 2-state Ergodic Hidden Markov Model can capture speaker-specific excitation information e from the LP residual effectively than GMM. Performance of the system for different HMM states shows that it could capture the speaker-specific excitation information effectively.\nThe objective in this paper was mainly to demonstrate the capture the speaker-specific excitation information present in the linear prediction residual for speaker recognition effectively than GMM. We have not made any attempt to optimize the parameters of the model used for feature extraction, and also the decision making stage. Therefore the performance of speaker recognition may be improved by optimizing the various design parameters."}], "references": [{"title": "Acoustic Phonetics", "author": ["K.N. Stevens"], "venue": "Cambridge, England: The MIT Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Speech Communication: Human and Machine", "author": ["D. O\u2019Shaughnessy"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1987}, {"title": "Automatic recognition of speakers from their voices", "author": ["B.S. Atal"], "venue": "Proc. IEEE", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1976}, {"title": "Linear prediction: a tutorial review", "author": ["J. Makhoul"], "venue": "Proc. IEEE", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1975}, {"title": "New techniques for automatic speaker verification", "author": ["A.E. Rosenberg", "M. Sambur"], "venue": "vol. 23, no.2, pp.169-175", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1975}, {"title": "Speaker recognition using orthogonal linear prediction", "author": ["M.R. Sambur"], "venue": "IEEE Trans. Acoust. Speech, Signal Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1976}, {"title": " high performance speaker verification using principal spectral componentsQ", "author": ["J. Naik", "G.R. Doddington"], "venue": "proc. IEEE Int. Conf. Acoust. Speech, Signal Processing, pp. 881- 884", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1986}, {"title": "Human and machine performance on speaker identity verification", "author": ["T.C. Feustel", "G.A. Velius", "R.J. Logan"], "venue": "Speech Technology,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "Effect of glottal pulse shape on the quality of natural vowels", "author": ["A.E. Rosenberg"], "venue": "J. Acoust. Soc. Amer", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1971}, {"title": "Epoch extraction from linear prediction residual for identification of closed glottis interval", "author": ["T.V. Ananthapadmanabha G", "B. Yegnanarayana"], "venue": "IEEE Trans. Acoust. Speech Signal Process. ASSP-", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1979}, {"title": "Artificial Neural Networks", "author": ["B. Yegnanarayana"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Speaker-specific information from residual phase. In: Inter nat", "author": ["K.S.R. Murthy", "S.R.M. Prasanna", "B. Yegnanarayana"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Recent advances in speaker recognition", "author": ["S. Furui"], "venue": "Pattern Recognition Lett", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Fundamentals of Speech Recognition", "author": ["L.R. Rabiner", "B.H. Juang"], "venue": "Prentice-Hall", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1993}, {"title": "Methods and experiments for text-independent speaker recognition over telephone channels,", "author": ["H. Gish", "M. Krasner", "W. Russell", "J. Wolf"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1986}, {"title": "Robust Text-Independent Speaker Identification using Gaussian Mixture Models,\u2019", "author": ["D.A. Reynolds", "R.C. Rose"], "venue": "IEEE- Transactions on Speech and Audio Processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "J. Royal Statist. Soc. Ser. B. (methodological),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1977}, {"title": "Discriminating semi-continuous HMM for speaker verification,Q in proc", "author": ["M. Forsyth", "M. Jack"], "venue": "IEEE Int. Conf. Acoust. Speech, Signal Processing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1994}, {"title": "Discriminating observation probability (DOP) HMM for speaker verification", "author": ["M. Forsyth"], "venue": "Speech Communicaiton, vol. 17, pp.117-129", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}, {"title": "Automatic Text-Independent Speaker Recognition using source feature", "author": ["R. Rajeshwara Rao"], "venue": "Ph.D Thesis ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Speech is a composite signal which carries information about the message, the speaker identity and the language identity [1], [2].", "startOffset": 121, "endOffset": 124}, {"referenceID": 1, "context": "Speech is a composite signal which carries information about the message, the speaker identity and the language identity [1], [2].", "startOffset": 126, "endOffset": 129}, {"referenceID": 3, "context": "They are calculated so as to minimize the prediction error [4].", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "A study into the use of LPC for speaker recognition was carried out by [3].", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "These coefficients are highly correlated, and the use of all prediction coefficients may not be necessary for speaker recognition task [6] [7] used a method called orthogonal linear prediction.", "startOffset": 135, "endOffset": 138}, {"referenceID": 6, "context": "These coefficients are highly correlated, and the use of all prediction coefficients may not be necessary for speaker recognition task [6] [7] used a method called orthogonal linear prediction.", "startOffset": 139, "endOffset": 142}, {"referenceID": 7, "context": "[8] Used principal spectral components derived from linear prediction coefficients for speaker verification task.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "It has been shown that humans can recognize people by listening to the LP residual signal [9].", "startOffset": 90, "endOffset": 93}, {"referenceID": 9, "context": "Perceptually the signals will be different if these samples (related to the glottal pulse characteristics) are replaced by synthetic model signals [10] [11] [12], [13] or by random noise.", "startOffset": 147, "endOffset": 151}, {"referenceID": 10, "context": "Perceptually the signals will be different if these samples (related to the glottal pulse characteristics) are replaced by synthetic model signals [10] [11] [12], [13] or by random noise.", "startOffset": 152, "endOffset": 156}, {"referenceID": 11, "context": "Perceptually the signals will be different if these samples (related to the glottal pulse characteristics) are replaced by synthetic model signals [10] [11] [12], [13] or by random noise.", "startOffset": 157, "endOffset": 161}, {"referenceID": 12, "context": "Perceptually the signals will be different if these samples (related to the glottal pulse characteristics) are replaced by synthetic model signals [10] [11] [12], [13] or by random noise.", "startOffset": 163, "endOffset": 167}, {"referenceID": 12, "context": "Further, if voiced excitation is replaced by random noise excitation, it is difficult to perceive the speaker\u2019s identity [13].", "startOffset": 121, "endOffset": 125}, {"referenceID": 13, "context": "The Linear Predictive Coding (LPC) analysis performs spectral analysis on short segments of speech with an all-pole modelling constraint [14].", "startOffset": 137, "endOffset": 141}, {"referenceID": 14, "context": "MFFCs are less susceptible to the said variations [15].", "startOffset": 50, "endOffset": 54}, {"referenceID": 15, "context": "Gaussian classifier has been successfully employed in the several text-independent speaker identification applications since the approach used by this classifier is similar to that used by the long term average of spectral features for representing a speaker\u2019s average vocal tract shape [16].", "startOffset": 287, "endOffset": 291}, {"referenceID": 16, "context": "As shown in Figure 3 in a GMM model, the probability distribution of the observed data takes the form given by the following equation [17].", "startOffset": 134, "endOffset": 138}, {"referenceID": 18, "context": "The parameters of a GMM model can be estimated using Maximum Likelihood (ML) [19] estimation.", "startOffset": 77, "endOffset": 81}, {"referenceID": 18, "context": "Maximization (EM) [19] algorithm is used to extract the model parameters.", "startOffset": 18, "endOffset": 22}, {"referenceID": 16, "context": "x X = can be given as [17]", "startOffset": 22, "endOffset": 26}, {"referenceID": 16, "context": "that ) | X ( p ) | X ( p \u03bb \u03bb \u2265 [17].", "startOffset": 31, "endOffset": 35}, {"referenceID": 19, "context": "In a variety of ways, HMMs can be used as probabilistic speaker models for both text-dependent and textindependent speaker recognition [20, 21].", "startOffset": 135, "endOffset": 143}, {"referenceID": 7, "context": "The parameters of HMM are [8] State-transition probability distribution: It is represented by [ ] ij a A =", "startOffset": 26, "endOffset": 29}, {"referenceID": 18, "context": "In the testing phase, p (O/\u03bb) for each model is calculated [19].", "startOffset": 59, "endOffset": 63}], "year": 2010, "abstractText": "This paper introduces the performance evaluation of statistical approaches for Text-Independent speaker recognition system using source feature. Linear prediction (LP) residual is used as a representation of excitation information in speech. The speaker-specific information in the excitation of voiced speech is captured using statistical approaches such as Gaussian Mixture Models (GMMs) and Hidden Markov Models (HMMs). The decrease in the error during training and recognizing speakers during testing phase close to 100% accuracy demonstrates that the excitation component of speech contains speaker-specific information and is indeed being effectively captured by continuous Ergodic HMM than GMM. The performance of the speaker recognition system is evaluated on GMM and 2-state ergodic HMM with different mixture components and test speech duration. We demonstrate the speaker recognition studies on TIMIT database for both GMM and Ergodic HMM.", "creator": "PrimoPDF http://www.primopdf.com"}}}