{"id": "1105.0974", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2011", "title": "GANC: Greedy Agglomerative Normalized Cut", "abstract": "This paper describes a graph clustering algorithm that aims to minimize the normalized cut criterion and has a model order selection procedure. The performance of the proposed algorithm is comparable to spectral approaches in terms of minimizing normalized cut. However, unlike spectral approaches, the proposed algorithm scales to graphs with millions of nodes and edges. The algorithm consists of three components that are processed sequentially: a greedy agglomerative hierarchical clustering procedure, model order selection, and a local refinement.", "histories": [["v1", "Thu, 5 May 2011 04:55:53 GMT  (659kb)", "http://arxiv.org/abs/1105.0974v1", "Submitted to Pattern Recognition. 27 pages, 5 figures"]], "COMMENTS": "Submitted to Pattern Recognition. 27 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["seyed salim tabatabaei", "mark coates", "michael rabbat"], "accepted": false, "id": "1105.0974"}, "pdf": {"name": "1105.0974.pdf", "metadata": {"source": "CRF", "title": "GANC: Greedy Agglomerative Normalized Cut", "authors": ["Seyed Salim Tabatabaei", "Michael Rabbat"], "emails": ["seyed.s.tabatabaei@mail.mcgill.ca.", "mark.coates@mcgill.ca", "michael.rabbat@mcgill.ca"], "sections": [{"heading": null, "text": "ar X\niv :1\n10 5.\n09 74\nv1 [\ncs .A\nI] 5\nM ay\n2 01\n1\nThis paper describes a graph clustering algorithm that aims to minimize the normalized cut criterion and has a model order selection procedure. The performance of the proposed algorithm is comparable to spectral approaches in terms of minimizing normalized cut. However, unlike spectral approaches, the proposed algorithm scales to graphs with millions of nodes and edges. The algorithm consists of three components that are processed sequentially: a greedy agglomerative hierarchical clustering procedure, model order selection, and a local refinement.\nFor a graph of n nodes and O(n) edges, the computational complexity of the algorithm is O(n log2 n), a major improvement over the O(n3) complexity of spectral methods. Experiments are performed on real and synthetic networks to demonstrate the scalability of the proposed approach, the effectiveness of the model order selection procedure, and the performance of the proposed algorithm in terms of minimizing the normalized cut metric.\nKeywords: Graph Clustering, Normalized Cut, Model Order Selection, Large Scale"}, {"heading": "1. Introduction", "text": "Clustering or partitioning of nodes in networks or graphs is an important task that has many applications in a diverse range of fields. It has been used for many years to study social networks [1] and continues to be employed in the field of sociology to explore social interactions. More recently it has been employed in the study of biochemical networks [2, 3], biological neural networks [4, 5], and transport and communication networks.\nThere are three components to the clustering task: (i) choosing the number of clusters; (ii) selecting a criterion that measures the merit of each candidate cluster allocation; and (iii) identifying an algorithm that searches for the optimal clustering. Some performance criteria can be used both to select the number of clusters and choose the clustering, e.g., modularity [6] or information-theoretic criteria based on the minimum description length [7].\nThere is no universally-accepted performance criterion and indeed the most appropriate criterion can vary depending on the application domain and the goal of the clustering. Modularity, for example, focuses on network structure and places primary value on direct\n1Corresponding author. Tel.: +1 514 677 0056; fax: +1 514 398 4470; E-mail: seyed.s.tabatabaei@mail.mcgill.ca.\n2E-mail: mark.coates@mcgill.ca 3E-mail: michael.rabbat@mcgill.ca\nconnections between nodes. On the other hand, clustering algorithms based on Markov random walks [7, 8] also value indirect connections and network flow. In this paper we select the normalized cut criterion [9], which simultaneously encourages intra-cluster similarity while penalizing inter-cluster similarities. Methods based on this criterion have been employed successfully in a wide range of applications [5, 8\u201312]. The normalized cut criterion is related to the conductance of the underlying graph [13]. Furthermore an implicit duality between normalized cut and normalized association exists; the former encourages clusters that are less connected to each other and the latter encourages clusters whose nodes are well-connected.\nWhen partitioning a graph into clusters, the cut associated with cluster i is the sum of the weights of the edges between nodes in i and nodes in other clusters. Intuitively, minimizing the maximum cut (or minimizing the average cut) identifies clusters that have the weakest ties between them. The problem is that the minimum cut will often be achieved by identifying many very small clusters, which provides little insight into the underlying structure of the graph. The normalized cut metric was introduced by Shi and Malik in [9] to address this shortcoming. It normalizes each cut by dividing it by the total weight of the associated cluster. This has the effect of penalizing very small clusters because they generally have low total weight.\nMinimizing normalized cut is an NP-complete problem [9]. Shi and Malik illustrated that the minimization could be relaxed to form a generalized eigenvalue system, whose (discretized) solution corresponds to the minimum normalized cut. This has led to the development of a spectral clustering; it involves calculating eigenvectors of the identified system. In order to identify a partitioning of n nodes to k clusters, some techniques perform recursive bipartitioning [9], and thus require the repeated identification of two eigenvectors. Other approaches strive to identify k clusters directly by calculating the k smallest eigenvectors of the underlying graph Laplacian. One of the concerns about these methods is computational complexity; even when employing fast eigendecomposition methods such as the Lanczos iterative technique, complexity grows rapidly with n.\nIn this paper, we propose an agglomerative clustering algorithm that strives to minimize the normalized cut (or equivalently, maximize the normalized association). It is a fast, scalable algorithm, with almost linear complexity in the number of nodes for relatively sparse graphs. Performance evaluation using a range of benchmark and observed graphs indicates that the algorithm identifies partitionings that have average normalized association metrics as large as those of the partitions identified by the spectral clustering techniques. We also propose a method for identifying important partitioning scales which can be used to automatically select the number of clusters. To the best of our knowledge, this is the first model order selection method applicable to the normalized association maximization that is scalable. Through our experiments, we will show how effective our model order selection criterion is for both synthetic and real networks.\nThe rest of the paper is organized as follows. In Section 2 the major approaches for minimization of normalized cut and model order selection are reviewed. The problem formulation of normalized association is re-stated in Section 3. Our proposed algorithm is detailed in Section 4. The proposed algorithm is then compared to the state-of-the-art clustering algorithms in Section 5. The concluding remarks are provided in Section 6."}, {"heading": "2. Related Work", "text": "The identification of clusters in graphs and networks has received significant attention. In our review we focus on a representative set of algorithms that minimize the normalized cut with or without spectral decomposition. We also review the existing methods for selecting the number of clusters."}, {"heading": "2.1. Optimization of Normalized Cut", "text": "Spectral partitioning of graphs was first proposed by Donath and Hoffman in the 1970\u2019s [14]. Interest in the techniques was renewed in the 1990\u2019s when Pothen et al. described an algorithm for bi-partitioning using the Fiedler vector [15]. Hendrickson et al. and Karypis et al. contributed with multilevel algorithms for more efficient spectral partitioning [16, 17].\nThe normalized cut metric was introduced by Shi and Malik in [9]. They demonstrated how the bipartitioning task, with the objective of minimizing the normalized cut, could be relaxed to construct a generalized eigenvalue problem, and was thus related to spectral partitioning. The eigenvector corresponding to the second-smallest eigenvalue of the graph Laplacian identifies the bipartitioning (the real values in the eigenvector must be mapped to two discrete values for partitioning). This established the connection between minimization of normalized cut and spectral partitioning. Shi and Malik proposed a recursive bipartitioning scheme in order to partition a graph into k clusters. In [18], Meila and Shi proposed an algorithm that calculates k eigenvectors (thereby associating k real values with each node in the graph) and then uses a clustering algorithm, such as k-means, to do the partitioning in \u211ck. Ng et al. observed in [19] that the algorithm in [18] is susceptible to failure when there is substantial variation in the degree of connectivity between clusters. They proposed an alternative algorithm that uses a different normalization in both the eigenvalue problem and the construction of feature vectors prior to the application of k-means.\nAll of these algorithms involve a computationally expensive eigendecomposition. To address the computational difficulties for large graphs, Fowlkes et al. described a procedure that uses the Nystro\u0308m method to reduce the complexity of the eigenvalue problem [10]. This does significantly reduce the computational overhead, but it is not enough to make the eigendecomposition algorithms scalable to large graphs. Yan et al. recently described an algorithm for fast approximate spectral clustering [20], but the focus is not on clustering for graphs (rather it addresses real-valued feature vectors).\nDhillon et al. introduced a much faster algorithm for minimizing normalized cut in [11]; the graph is first greedily coarsened, then the coarsened graph is partitioned using a region growing procedure [17] and finally weighted kernel k-means clustering is applied to each partition to refine the clustering.\nOther methods also exist that strive to optimize related cost functions; for example Sharon et al. proposed a scalable hierarchical clustering using ratio association (the normalization term is the number of nodes in clusters) [21]. Other examples include the work by Ding et al. [22], Sharon et al. [23], Akselrod-Ballin et al. [24], and Corso et al. [25] that use the sum of internal weights of clusters as the normalization term."}, {"heading": "2.2. Model Order Selection", "text": "The problem of selecting the number of clusters is almost as old as clustering itself. In the following discussion, k\u2217 denotes the number of clusters that a given approach identifies\nas the true number of clusters. Numerous authors have used some notion of quality of clustering, usually based on some definition of the inter- and intra-cluster distances, to identify the number of clusters. Let F (k) denote this notion for a given partitioning to k clusters. F (k) is then examined for various values of k. Partitionings for different values of k can be obtained from a hierarchical clustering, or alternatively, by several flat partitionings.\nIf F is not a monotonic function of k, the approach is usually to take k\u2217 = argmaxk F (k) or k\u2217 = argmink F (k) (depending on the definition of F ). The majority of the methods discussed in [26] and [27] are of this type; Calinski and Harabasz [28], C-index [29], Baker and Hubert [30], and cubic clustering criterion [31] are among the most effective ones [26]. Tibshirani et al. [32] define F (k) as the gap, that is the difference of the average pairwise distance of the data points of the clustering at kth level and the expected value of the same measure of some reference model. This is similar to modularity [6] (discussed later) in the sense that it compares the clustering results to a reference model. None of these methods are directly applicable to graph clustering algorithms; the calculations of the defined metrics require pairwise distances which are not immediately available from a graph representation. Possible distance metrics include the shortest path [33] or the diffusion distance [8]; however the shortest path is very sensitive to noise and the calculation of the diffusion distance requires eigendecomposition.\nIn the case where F is a monotonic function of k, the only extremal values of F (k) correspond to trivial values of k. Hence, the value of F (k) corresponding to two or more choices of k are examined to quantify the significance of a given level of a hierarchical clustering [34\u201336]. In order to assess the clustering at level k of the hierarchy, Gnanadesikan et al. [34] propose the fraction F (k)/F (k\u2212 1), Arnold [37] use the value of F (k)\u2212F (k\u2212 1), Krzanowski and Lai [35] employ the fraction |F (k) \u2212 F (k \u2212 1)|/|F (k + 1) \u2212 F (k)|, and Pederson and Kulkarni [36] suggest using the fraction 2\u00d7F (k)/ (F (k \u2212 1) + F (k + 1)). The latter two approaches are the most similar to what we propose in Section 4 in the sense that they use the preceding and succeeding levels of the hierarchy to obtain the significance of the clustering at level k. However the approaches in [35, 36] are potentially susceptible to noise due to the division; small perturbations in the weights could lead to dramatic changes in the selected model order.\nOther approaches to selecting the number of clusters are based on the adoption of (semi)parametric models for the structure of the graph. This allows the application of model selection techniques based on concepts such as the Bayesian InformationCriterion (BIC) [38], and Akaike Information Criterion [39]. Despite the strong theoretical support for these methods, the adoption of a parametric model for the graph structure is undesirable. The models are often overly restrictive and do not adequately capture the properties of many real-life networks. An example is the requirement in [40, 41] that the input data are normally distributed (after projection of the graph into a real space). The more general methods based on mixture models do not scale well to very large graphs; even the recent approaches have only been applied to graphs with a few thousand nodes [42\u201344].\nSome heuristics are based on the sizes of the clusters that are merged at different levels of the clustering hierarchy [45, 46]. The authors in [46] suggest that when two clusters with large number of nodes are merged, a significant amount of detail is lost; hence such an instance is potentially where a hierarchical clustering algorithm should stop. The authors\nof [45] propose a similar approach. The main drawback of these approaches is that only the granularity of the clusters are taken into account and the number and the weights of the edges are simply ignored.\nA well-known and effective method of selecting the number of clusters is to examine the eigenvalues of the Laplacian of the graph that is to be clustered [19, 47, 48]. For a graph with isolated connected components the multiplicity of the eigenvalue zero is equal to the number of clusters. Any other graph with well-separated clusters can be considered as a perturbation of this ideal case. Matrix perturbation theory states that the stability of the eigenvectors of a matrix is proportional to the eigengap (the difference between two successive eigenvalues). Von Luxburg [48] suggests using k\u2217 = argmaxk (\u03bbk \u2212 \u03bbk+1). A large eigengap at k\n\u2217 is the case in which spectral algorithms using the Laplacian perform most successfully [19]. A more robust criterion is proposed by Zelnik-Manor and Perona [49] that uses eigenvectors instead. Despite the solid theoretical support behind these approaches, the requirement of eigendecomposition makes them impractical when large graphs are being clustered.\nThere have been attempts to identify the number of clusters using stability analysis. \u201cStability\u201d has been defined differently by different authors, but they all based on the same intuition; with respect to some algorithm, a stable clustering is one that behaves relatively consistently in the presence of some controlled perturbation. The perturbation could be in terms of noise [50], sampling subsets from the input [51, 52], or random projection of a high dimensional data into a lower dimensional space [53]. These approaches require several runs of clustering for every value of k which makes them computationally expensive. Furthermore, Ben-David et al. [52] warn against using stability analysis in this context, and they suggest that this family of model order selection techniques is not suitable for selecting the number of clusters in general. The intuition behind their work is that when an underlying objective function, F (k), has several local optima of relatively similar values, a clustering algorithm might be trapped by any of them, even when k is the true number of clusters. The difference in clustering solutions is interpreted as instability and the model order rejected, but the behavior is caused by the imperfect nature of the clustering algorithm.\nTwo more recently proposed methods to select the number of clusters are based on optimization of the quality metrics of modularity [6] and description length [7]. Both methods simultaneously address both clustering and the selection of the number of clusters; they strive to optimize the objective function over all possible partitionings. These methods are discussed in more detail in Section 5."}, {"heading": "3. Problem Formulation", "text": "Let G = (V,E, w) be a weighted graph having n = |V | nodes and m = |E| edges4. We assume that edge weights w(u, v) = w(v, u) \u2265 0 are non-negative and symmetric, that w(u, v) = 0 if (u, v) /\u2208 E, and that for (u, v) \u2208 E, the weight w(u, v) \u2265 0 is indicative of the similarity between nodes u and v; that is, the larger the weight, the more similar the nodes. We also allow for self-weights, w(u, u) \u2265 0.\n4In this work, we assume we are given the graph on which we wish to perform clustering. We do not address the problem of forming a graph from data, which arises when one applies graph clustering methods to general data sets; see, e.g., [48, 54, 55].\nFor a fixed number of clusters, k, we measure the quality of the partition via the normalized cut metric [9], defined as follows. For a node u, denote the degree of u by d(u) = \u2211\nv\u2208V w(u, v), and for a subset U \u2282 V of nodes, let d(U) = \u2211\nu\u2208U d(u) denote the cumulative degree of the subset. Similarly, for two disjoint subsets of nodes, V1 and V2, let w(V1, V2) = \u2211\nu\u2208V1\n\u2211\nv\u2208V2 w(u, v) denote the sum of the weights of edges with one end in V1\nand the other end in V2. Let Ck denote the partition of nodes to k clusters, Ck = {C1, . . . , Ck} where Ci is the subset of nodes affiliated to cluster i. The normalized cut metric is defined as\nNCut(Ck) = k \u2211\ni=1\nw(Ci, V \\ Ci)\nd(Ci) . (1)\nMinimizing the normalized cut can be interpreted as minimizing the similarity of nodes in different clusters, relative to the degree of each cluster. Alternatively, maximizing the intra-cluster similarity can be achieved by maximizing the normalized association, defined as\nNAssoc(Ck) = k \u2211\ni=1\nw(Ci, Ci)\nd(Ci) . (2)\nMoreover, maximizing normalized association is equivalent to minimizing normalized cut since w(Ci, Ci)+w(Ci, V \\Ci) = d(Ci), and hence NAssoc(Ck) = k\u2212NCut(Ck). For the sake of readability, we adopt the maximization of normalized association as our goal in developing a clustering procedure."}, {"heading": "4. GANC: Greedy Agglomerative Normalized Cut", "text": "In this section, we describe a greedy algorithm for building an agglomerative clustering on a graph. Although we do not make guarantees about its accuracy, the algorithm is fast on sparse graphs and yields excellent performance on a variety of examples, as illustrated in Section 5. Moreover, GANC has a model order selection criterion and does not have to be provided with the number of clusters a priori. The algorithm consists of three steps: agglomerative clustering, model order selection, and refinement."}, {"heading": "4.1. Agglomerative Clustering", "text": "We propose greedy maximization of normalized association via an agglomerative hierarchical clustering. In the following discussion, note that the number of clusters at stage k of the hierarchy is k. At stage k of the hierarchy, using the given partition, Ck, two clusters are merged to form a new partition, Ck\u22121. The two clusters to be merged are chosen greedily so that normalized association of stage k \u2212 1 is maximized.5\nInitially, Cn is a function that maps nodes to unique clusters, 1, . . . , n. The degree of every node u, d(u), is calculated. Furthermore, for every edge, (u, v) \u2208 E, the improvement in normalized association by its contraction to obtain Cn\u22121 is stored in \u2206(u, v) =\n5We note that a similar greedy merging algorithm is alluded to by Shi and Malik in [9], however they also suggest first projecting each node into \u211ck using the first k eigenvectors of the graph Laplacian, and running an algorithm such as k-means to obtain an initial clustering. Our approach requires no eigendecomposition and performs no such initialization. We also note that, although Shi and Malik mention having experimented with greedy merging, results for this approach have not been reported in the literature.\n2w(u, v)/ (d(u) + d(v)), so that a matrix of the improvements is constructed. Here we assume that initially there are no self-loops, but if there are, the value of improvements are computed by (4) presented shortly. Throughout the agglomerative clustering procedure, d(\u00b7), w(\u00b7, \u00b7), and \u2206(\u00b7, \u00b7) are updated at each stage as described below.\nAt each iteration, the pair (u\u2217, v\u2217) = argmax(u,v)\u2206(u, v) is selected for merging to create a larger cluster, uv\u2217. The degree is computed for the newly constructed cluster, d(uv\u2217) = d(u\u2217)+d(v\u2217). The weights and the improvement matrices are updated by removing the rows and columns corresponding to u\u2217 and v\u2217, and inserting a new row and column corresponding to uv\u2217 (rows and columns are not removed or added in our implementation, but this is the practical effect). The weight matrix is updated as follows:\nw(uv\u2217, x) =w(x, uv\u2217) = w(u\u2217, x) + w(v\u2217, x), (3)\nand the improvement matrix update is:\n\u2206(uv\u2217, x) = w(uv\u2217, uv\u2217) + w(x, x) + 2w(uv\u2217, x)\nd(uv\u2217) + d(x) \u2212\nw(uv\u2217, uv\u2217)\nd(uv\u2217) \u2212\nw(x, x)\nd(x) , (4)\nfor all the clusters, x, adjacent to either u\u2217 or v\u2217. The self-weights are also calculated by:\nw(uv\u2217, uv\u2217) = w(u\u2217, u\u2217) + w(v\u2217, v\u2217) + 2w(u\u2217, v\u2217). (5)\nFor all pairs of nodes (u, v) not adjacent to u\u2217 and v\u2217, the weights w(u, v) and improvements \u2206(u, v) remain unchanged. The above sequence of steps is repeated n \u2212 1 times to form the clustering hierarchy."}, {"heading": "4.2. Model Order Selection", "text": "Many of the clustering algorithms require the number of clusters to be provided to the algorithm a priori. However such information is often not available in practical situations making the decision about the number of clusters an issue in itself.\nIt is worth noting that the stage number (k) that maximizes NAssoc(Ck) does not necessarily correspond to a meaningful number of clusters.6 Here we propose a simple but effective approach to model order selection. Let C\u2217k = argmaxCkNAssoc(Ck) denote the partition that maximizes normalized assocation over all partitions of V into k clusters. To carry out model order selection, we examine the curvature7 of NAssoc(C\u2217k) which we define as\nCurv(k) = (\nNAssoc(C\u2217k)\u2212NAssoc(C \u2217 k\u22121) ) \u2212 ( NAssoc(C\u2217k+1)\u2212 NAssoc(C \u2217 k) )\n=2NAssoc(C\u2217k)\u2212 NAssoc(C \u2217 k\u22121)\u2212 NAssoc(C \u2217 k+1). (6)\n6To see this, consider an unweighted graph that consists of two isolated chains, each having 4 nodes. The true number of clusters is trivially 2 resulting in NAssoc(C2) = 2; however if one groups the adjacent nodes to obtain 4 groups of node pairs, the resulting value of normalized association would be NAssoc(C4) = 2.66.\n7The reason we call this metric the curvature is its similarity to the central approximation of the second order derivative which is defined for a continuous function, f(\u00b7) as f \u2032\u2032(x) \u2248 \u22022h[f ](x)/h\n2 = [f(x+ h)\u2212 2f(x) + f(x\u2212 h)] /h2. By substituting h = 1, we get the negative of our curvature equation (the negation is to have positive peaks).\nThe first term of the above addition is the improvement of normalized association moving from stage k to k\u2212 1 of the hierarchy and the second term is the improvement moving from stage k + 1 to k.\nThe function Curv(k) captures the notion that a particular number of clusters, k, identifies meaningful structural similarities embodied in the graph if it provides a normalized association which is significantly larger than the best partition with one additional cluster (k + 1) and little can be gained by reducing the number of clusters to k \u2212 1.\nIn practice we do not have access to the optimal partitions, so we cannot evaluate the exact value of the curvature function. Instead, we approximate the curvature by using the normalized association values for the partitions returned by the agglomerative step of our algorithm.\nNote that the model order selection step of the algorithm could be used as a model order selection step for any other algorithm that maximizes the normalized association and generates a clustering hierarchy. Furthermore, this step of the algorithm can be considered optional if there is a prior knowledge about the true number of clusters."}, {"heading": "4.3. Refinement", "text": "Greedy algorithms can get trapped by local optima. This is also the case with the agglomerative step of our algorithm, especially when the clusters are not clearly separated. After selecting the number of clusters either using the model order selection rule described previously or prior knowledge, a refinement step is invoked in order to improve the initial clustering results. The nodes are moved across the clusters to further improve the value of normalized association. A similar approach is taken in [56], but groups of nodes are moved from a cluster to another instead of individual nodes. If we try all possible moves, we end up performing an exhaustive search of dividing n nodes into k clusters. This defeats the purpose of developing the fast agglomerative clustering step. Instead, we look only at the boundary nodes, i.e., the nodes that have at least one neighbor in another cluster.\nInitially the set of all the boundary nodes is identified. We use an n\u00d7 k matrix to keep track of the neighborhood information of the boundary nodes. If B denotes this matrix, and I denotes an n-dimensional vector:\nB(u, i) =\n{\n\u2211\nv\u2208Ci w(u, v) if u 6\u2208 Ci\n0 otherwise , I(u) =\n\u2211\nv\u2208Ci\nw(u, v) if u \u2208 Ci. (7)\nFor each of the boundary nodes, improvements in normalized association by moving from their current cluster to each of their neighbors are calculated. This improvement for node u moving from cluster i to cluster j is\n\u03b4(u, i, j) = w(Ci, Ci)\u2212 2I(u)\nd(Ci)\u2212 d(u) +\nw(Cj, Cj) + 2B(u, j)\nd(Cj) + d(u)\n\u2212\n(\nw(Ci, Ci)\nd(Ci) +\nw(Cj, Cj)\nd(Cj)\n)\n. (8)\nIf no move leads to improvement, the node stays in the cluster to which it belongs. If one or more moves result in improvement, then the node is moved to the cluster that results\nin maximum improvement. If u is moved from Ci to Cj , the corresponding cluster degrees and associations are updated:\nd(Ci) new = d(Ci) old \u2212 d(u) (9) d(Cj) new = d(Cj) old + d(u) (10)\nw(Ci, Ci) new = w(Ci, Ci) old \u2212 2I(u, i)old (11) w(Cj, Cj) new = w(Cj, Cj) old + 2B(u, j)old, (12)\nand for all the neighboring nodes of u denoted by v, entries of B and I are updated as follows:\nI(v)new = I(v)old + w(u, v)\nB(v, i)new = B(v, i)old \u2212 w(u, v)\n}\nif v \u2208 Cj , (13)\nI(v)new = I(v)old \u2212 w(u, v)\nB(v, j)new = B(v, j)old + w(u, v)\n}\nif v \u2208 Ci, (14)\nB(v, i)new = B(v, i)old \u2212 w(u, v)\nB(v, j)new = B(v, j)old + w(u, v)\n}\nif v /\u2208 Ci and v /\u2208 Cj, (15)\nand finally\nB(u, i)new = I(u)old, (16)\nI(u)new = B(u, j)old, (17) B(u, j)new = 0. (18)\nWhile performing the updates (9) to (18), the set of boundary nodes is also updated. Whenever a node is moved from cluster i to cluster j, its neighbors in cluster i are added to the set of boundary nodes. For some node v neighboring u, moving u might result in B(v, i)new becoming zero in (15) for all i; i.e., some nodes might be removed from the set of boundary nodes.\nOne pass through all the boundary nodes is considered a single refinement iteration. When an iteration causes no positive improvement, the refinement procedure is stopped. Alternatively, one could specify the maximum number of refinement iterations. Note that although we prohibit the refinement step from emptying any cluster, we have not observed such an attempt in our experiments."}, {"heading": "4.4. Implementation and Computational Complexity of GANC", "text": "We take a similar approach to [57] when implementing the agglomerative clustering procedure of GANC. Max-heaps and balanced binary trees are used to store the rows of the \u2206 matrix and the adjacency matrix. A separate heap is also used to store the maximum of each row of \u2206. This leads to the complexity of O(mh log(n)) for the agglomerative clustering procedure, where h is the height of the generated dendrogram and m is the number of edges with non-zero weight (see [57] for details).\nThe model order selection step of the algorithm requires O(n) computations. The computational requirements are much less than those of the methods analyzing the eigenvalue\nAlgorithm 1 GANC: Greedy Agglomerative Normalized Cut\nGREEDY AGGLOMERATION\n1: Build the initial \u2206 matrix, and NAssoc(Cn) \u2190 \u2211\nu\u2208V w(u, u)/d(u) 2: for k = n\u2212 1 to 1 do 3: (i\u2217, j\u2217) \u2190 argmaxi,j \u2206(i, j) 4: NAssoc(Ck) \u2190 NAssoc(Ck+1) + \u2206(i\n\u2217, j\u2217) 5: Update rows and columns of \u2206, the weights, and the degrees corresponding to clusters i\u2217 and j\u2217 (3 - 5) 6: end for\nMODEL ORDER SELECTION\n1: for k = n\u2212 1 to 2 do 2: Calculate Curv(k) (6) 3: end for 4: k\u2217 \u2190 argmaxk Curv(k) or provided by the user\nREFINEMENT\n1: Obtain flat partitioning for level k\u2217 2: Build the initial B and I (7), and refine \u2190 true 3: while refine do 4: \u03b4\u0304 \u2190 0 5: for u = 1 to n do 6: if u \u2208 Ci is on the boundary and maxj \u03b4(u, i, j) > 0 then 7: Move u from Ci to Cj\u2217, where j\n\u2217 = argmaxj \u03b4(u, i, j) 8: Update B, I, the weights, and the degrees (9 - 18) 9: \u03b4\u0304 \u2190 \u03b4\u0304 + \u03b4(u, i, j\u2217) 10: end if\n11: end for 12: if \u03b4\u0304 = 0 then 13: refine \u2190 false 14: end if\n15: end while\nfall-off which require eigendecomposition (e.g., [49] and [47]). Performing an eigendecomposition to study the eigenvalue fall-off for all of the eigenvalues requires O(n3) operations.\nTo implement the refinement step, we use the map data structure to store and update B. Every row of B is stored in a map (C++ STL implementation of the map data structure is used [58]). Updates (9) to (12) are performed in constant time. To access each map entry of a row of B to insert, update, or delete, no more than O(log k) operations are required, because the maximum number of clusters connected to a boundary node is strictly less than k. Updates (13), (14), and (15) each take no more than O(log k) operations and are repeated for all neighbors of a node that is moved. We have observed through experiments that nodes with larger degrees do not tend to be on the boundaries and hence the average number of neighbors to be updated is practically smaller than the average node degree. (16), (18), and any insertion to or deletion from the set of boundary nodes are performed in O(log k) time.\n(17) is trivial and is performed in a constant time. Not more than n nodes can be moved in a given iteration. When node i is moved, O(di log k) operations are required, where di is the degree of node i. Summing over all boundary nodes, the complexity is O(m log k) because \u2211n\ni=1 di = 2m. Hence the number of operations in a single iteration is at most O(m log k). Throughout our experiments we have observed very small number of refinement iterations even when the refinement is applied to the networks with millions of nodes and edges. Furthermore the algorithm is capable of limiting the number of iterations. Hence the complexity of the average number of operations of the whole refinement step does not exceed O(m log k).\nThe total computational complexity of GANC is dominated by the agglomerative clustering procedure. In practice, the height of the resulting dendrogram is often O(logn). Also, the graphs that are studied are usually sparse and hence m = O(n). Therefore the complexity of the agglomerative clustering step of our algorithm is O(n log2 n) for many real-life graphs.\nThe worst case runtime happens when the graph is complete and the dendrogram is totally unbalanced. There are n \u2212 1 agglomerations to construct the whole hierarchy each of which requires at most O(n logn) operations. Hence the worst case complexity for the agglomerative clustering step of the algorithm is O(n2 logn) which is not expected to occur in practical situations.\nThe memory requirement of GANC is O(m) which is equivalent to O(n) for sparse graphs. In summary, in many practical cases (relatively sparse graphs and balanced dendrograms), the computational complexity is O(n log2 n) and the memory requirement is O(n). GANC can be downloaded from http://www.ece.mcgill.ca/~coates/software/."}, {"heading": "5. Experimental Results", "text": "In this section we provide a comparison of the performance and the runtime of our proposed algorithm GANC with a selection of the state-of-the-art graph clustering algorithms from the literature whose implementations were downloaded from the corresponding authors\u2019 websites. Experiments were performed on an Intel 3.0 GHz Core 2 Quad CPU with 8 GB of RAM and Ubuntu 9.10 operating system."}, {"heading": "5.1. Comparing Algorithms", "text": "We compare to the following algorithms: Shi and Malik (recursive NCut) [9]; Meila and Shi (k-way NCut) [18]; Ng, Jordan, and Weiss [19]; Dhillon, Gaun, and Kulis [11]; Rosvall and Bergstrom [7]; and Blondel et al. [56]. The first four algorithms focus on maximizing NAssoc. Despite not addressing our criterion of interest directly, the other algorithms are included because they are scalable and represent the state-of-the-art in graph clustering.\nThe discussed clustering algorithms ([9],[18],[19]) are not scalable as they include eigendecomposition. Dhillon, Guan, and Kulis proposed an algorithm that strives to maximize normalized association without requiring any eigendecomposition [11]. Similar to the other algorithms that address the maximization of normalized association, the Dhillon-Guan-Kulis algorithm requires the user to provide the number of clusters. After specifying the number of clusters, three steps are performed: a coarsening phase, a base clustering step using region growing, and finally a refinement step using weighted kernel k-means with appropriate choices of the kernel and the node weights to maximize the normalized association.\nMinimization of the criterion proposed by Rosvall and Bergstrom [7] results in a coarsegrained representation of the information flow through the network. Their proposed clustering objective aims to compress the underlying random walk on a graph without losing important network structures. To facilitate this, a coding is designed as follows: every cluster is assigned to a unique code; in a given cluster, every node is also assigned to a unique code. However two nodes in different clusters are allowed to be assigned to the same code. The algorithm then strives to minimize the average description length of a single step of a random walk.\nThe algorithm by Blondel et al. targets maximizing modularity [56]. Modularity (Q) is defined as the total fraction of intra-cluster sum-weight minus the expected fraction if the edges (and weights) were distributed randomly while the node degrees were preserved:\nQ = k \u2211\ni=1\n(\nw(Ci, Ci)\nM \u2212\nd(Ci) 2\nM2\n)\n, (19)\nwhere M = \u2211\ni\n\u2211\nj w(i, j). Modularity provides a valuable metric of the connectedness of clusters, but a number of authors have demonstrated that it suffers from a resolution limit when used to select the number of clusters [59]. The partitioning that maximizes modularity will generally not isolate clusters if the number of edges in the cluster is a small fraction of the total number of edges in the graph. The reason behind the resolution limit of modularity is the second term in the summation of (19); when the network gets larger, M increases monotonically. However the cluster degrees are not necessarily a function of the network size and are often bounded, regardless of the number of nodes [60]. This results in d(Ci)\n2/M2 \u226a 1 and hence the value of modularity becomes dependent only on w(Ci)/M . By normalizing the summation by d(Ci) as suggested in [12], the resolution limit phenomena is resolved; but we have\nQnormalized =\nk \u2211\ni=1\n1\nd(Ci)\n(\nw(Ci, Ci)\nM \u2212\nd(Ci) 2\nM2\n)\n= 1\nM [NAssoc(Ck)\u2212 1] . (20)\ni.e., maximization of Qnormalized and normalized association are equivalent."}, {"heading": "5.2. Synthetic Graphs", "text": "We first analyze the performance on synthetic graphs, for which the true clustering behavior is known. We use benchmark graphs developed by Lancichinetti, Fortunato, and Radicchi [61] (LFR graphs). These random graphs are designed based on the planted partition model [62]. Each node is assigned to one of k clusters. When edges are added to the graph, the probability that the edge is between nodes in the same cluster is 1 \u2212 \u00b5 and the probability that the edge joins nodes from different clusters is \u00b5. The LFR benchmarks have heterogeneous cluster sizes with user-specified lower bound and upper bound, cmin and cmax, respectively. Furthermore the node degrees are upper-bounded by dmax and the average node degree is denoted by davg . As \u00b5 decreases, edges are increasingly likely to be intra-cluster, making the partitioning task easier.\nThe Meila-Shi algorithm performs at least as well as the other spectral clustering algorithms (Ng-Jordan-Weiss [19] and Shi-Malik [9]) and hence we only display the Meila-Shi results. The local search of Dhillon-Guan-Kulis is also excluded often, because it results in negligible improvement while it introduces a very large computational overhead."}, {"heading": "5.2.1. Maximizing normalized association", "text": "Figure 1 examines how the algorithms perform with respect to maximizing NAssoc for 1,000-node LFR graphs. In order to observe how the algorithms perform on graphs with heterogeneous clusters, we let the cluster sizes range from 20 to 50 nodes.8 For each value of \u00b5, 100 graph realizations are generated. The value of NAssoc is divided by k to obtain a value between 0 and 1 which represents the average NAssoc per cluster.\nThe algorithms perform almost identically with the exception of the Dhillon et al. algorithm [11]. The refinement step of GANC results in a significant improvement in the value of NAssoc. However, the local search of Dhillon\u2019s algorithm does not improve NAssoc significantly. Note that the Blondel et al. algorithm results in higher values of NAssoc due to selecting smaller number of clusters than the true ones (NAssoc(Ck)/k is a monotonically decreasing function of k); a fair comparison between algorithms is possible only when the number of clusters are equal."}, {"heading": "5.2.2. Comparing to the planted partitions", "text": "The advantage of exploring performance on synthetic graphs is that a ground-truth partitioning is available. Because the LFR benchmarks are based on the planted partition model, there is the additional advantage that NAssoc is an appropriate criterion to adopt. Apart from some possible small errors due to the randomness inherent in the construction of the benchmark graphs, the ground truth partitioning will correspond to a maximum of NAssoc\n8In the real-life networks that we consider in this paper, the clusters have been observed to be of a limited size, regardless of the network size [60]; for example the Dunbar number suggests an upper limit of 150 nodes for clusters in a social network [63].\nfor a given value of k. For comparing two partitions on the same graph, we use the Jaccard index [64] which for two partitions, X and Y , is defined as\nJI = a\n(a+ b+ c) , (21)\nwhere a, b, and c are, respectively, the total pair of nodes that are assigned to the same cluster in both X and Y , the same cluster in X and different clusters in Y , and the same cluster in Y and different clusters in X . When two partitions are identical, the Jaccard index evaluates to one, and it decreases as the partitions deviate from each other.\nFigures 1b show the closeness of the partitionings identified by the algorithms and the ground truth in terms of the Jaccard index. The algorithm by Blondel et al. deviates dramatically from the true clustering (Figure 1b). The reason is that the minimum and maximum cluster sizes are fixed for all the networks in addition to the average node degree. Hence the average number of edges per cluster remains the same making the proportion of the intra-cluster edges decrease. Due to the resolution limit of modularity, as the proportion of intra-cluster edges is decreased, modularity maximization algorithms tend to group several clusters into a single cluster. If we reduce n to 1000, the Blondel et al. algorithm leads to closer results to the ground truth. The algorithm by Dhillon et al. does not perform as well as other algorithms that strive to maximize NAssoc and the local search results in negligible improvement in terms of the Jaccard index."}, {"heading": "5.3. Model order selection: the curvature metric", "text": "We first provide an illustrative example of the use of the curvature metric for selecting the number of clusters in the partitioning. This example highlights the difference in behavior compared to the modularity metric used by modularity maximization algorithms [56].\nAn example used by Good et al. [59], to illustrate the resolution limit is the ring of 24 cliques9, each of which has 5 nodes and is connected to its neighboring clique by a single edge. The value of modularity is maximized by a 12-cluster partition that merges pairs of cliques together. A more natural clustering is to consider each clique as an individual cluster. Figure 2b shows the curvature plot for the case of the ring of cliques. In addition to the peak of the curvature that is correctly located at 24 clusters, it is interesting to note that the other local peaks of the curvature are also meaningful; e.g., the peak at 12 clusters corresponds to the clustering that maximizes modularity.\nTo see how the curvature indicates the true number of clusters for networks with more complex structures, we consider the LFR benchmarks. We fix the LFR parameters for a 1,000-node graph and explore how the curvature changes as \u00b5 is varied. Figure 3 indicates that the peak of the curvature plot correctly identifies the true number of clusters for \u00b5 in the range 0.1 \u2013 0.5. As \u00b5 increases, the main peak becomes less distinct; for \u00b5 = 0.5, the second highest peak is almost as large as the primary peak. As the clusters become increasingly interconnected, the curvature plot provides a less clear indication of the true\n9A clique is a subset of nodes in a graph which are fully connected.\nnumber of clusters. Note that Figure 3 corresponds to a single realization of the LFR benchmarks. The empirical probabilities that the highest peak of the curvature corresponds to the true number of clusters for 100 realizations of the LFR benchmark with parameters as in Figure 3 are p1 = [1.0 1.0 0.96 0.60 0.42 0.06], for \u00b5 = [0.1 0.2 0.3 0.4 0.5 0.6] when no refinement step is applied; furthermore the empirical probabilities that one of the two highest peaks corresponds to the true number of clusters are p2 = [1.0 1.0 1.0 0.98 0.75 0.13].\nThese probabilities are obtained using the approximate curvatures derived from the agglomerative clustering procedure. To achieve a better insight into the model selection capabilities of the curvature metric, we derive better approximations by applying the refinement step to every level of the hierarchy and then recomputing curvature estimates. With this new procedure, we obtain p1 = [1.0 0.99 0.99 0.95 0.87 0.77] and p2 = [1.0 0.99 0.99 0.96 0.88 0.78]. This indicates that curvature can provide a good indication of model order even for \u00b5 = 0.6.\nThe model order selection algorithm of Rosvall-Bergstrom algorithm quite successfully indicates the true number of clusters in the case of LFR benchmarks; the empirical probabilities for the Rosvall-Bergstrom algorithm is p1 = [1.0 1.0 1.0 0.98 0.85 0.83]. The Blondel et al. algorithm does not successfully identify the true number of clusters when \u00b5 increases: p1 = [1.0 0.99 0.98 0.79 0.07 0.0]. Note that when n = 5, 000, the failure of the Blondel et al. algorithm becomes more evident and p1 turns to a 0% success for any \u00b5 (this is reflected in Figure 1). However the refined GANC and Rosvall-Bergstrom algorithms obtain the same success ratios when n = 5, 000."}, {"heading": "5.4. Real networks", "text": "In this section, we examine the behavior the GANC and compare it to the other algorithms for graphs representing real networks. We first examine the behavior for small networks where there is knowledge of the ground truth partition. We then experiment with large networks, which allows us to assess the scalability of GANC."}, {"heading": "5.4.1. Small networks", "text": "We conduct experiments using the Zachary karate club network [65], the football network [6], and the political books network10. The Zachary karate club network portrays friendships in a karate club. During the polling period, there was a dispute between the manager and the instructor which led to the establishment of a new club by the instructor. The karate students then split into two groups, either staying with the original club or following the instructor to the new club. The two clusters associated with the network correspond to these two groups. Each node in the football networks corresponds to a team in US college football. There are 11 conferences and 5 independent teams. Each edge corresponds to the existence of a match between the connected nodes. The independent teams can be interpreted as outliers. Nodes of the political book network correspond to the political books sold by Amazon.com and are categorized as neutral, conservative, and liberal. The edges between pairs of nodes correspond to frequent co-buying of the pairs of books by the same buyers. The three networks are unweighted.\n10Collected by V. Krebs, http://www.orgnet.com\nTables 1 and 2 compare the performance of GANC with other clustering algorithms for these small, well-known datasets. The tables provide the average NAssoc per cluster (NAssoc/k) and the Jaccard index. The \u201ctrue\u201d number of clusters is indicated within parentheses beside the name of each network11. In Table 1, all algorithms (including GANC) are informed of the true number of clusters and instructed to generate a partitioning with that number of clusters. Table 1 indicates that the spectral clustering algorithms (Ng-JordanWeiss, Meila-Shi, and Shi-Malik) achieve similar performance in terms of NAssoc. We note that when k is specified for these three networks, GANC identifies partitionings with average NAssoc as large or larger than those of the partitionings identified by the spectral clustering techniques, with the exception of Shi-Malik in the case of the football network.\nTable 2 compares the performance of algorithms that select the number of clusters based on some aspect of the data. The number of clusters selected by each algorithm is shown within parentheses after the Jaccard index. Rosvall\u2019s algorithm uses a description length metric to select the number of clusters [7]; Blondel\u2019s algorithm chooses the partitioning that maximizes the modularity [56]. A direct comparison of NAssoc is not valid when k is not fixed. The first row of this table shows the performance of GANC when the curvature plot\nis used to select the number of clusters. Although the selected number of clusters does not correspond to the true number of clusters for any of the networks, all three values can be explained. In the karate club network there is a group of students who have weak ties to other members of the network and these are identified as a third cluster; in the football network, GANC isolates the independent teams; in the political books network, GANC does not identify a neutral cluster and assigns each of the neutral books to either the liberal or conservative cluster.\n11Note that the \u201ctrue\u201d number of clusters is something of an artificial social construct and does not necessarily correspond to a partitioning that maximizes any meaningful graph clustering metric."}, {"heading": "5.4.2. Cortical Networks", "text": "Here we study the networks presented in [66] which are weighted graphs, each with 998 nodes. The nodes correspond to small regions of the human cerebral cortex and the edges correspond to cortico-cortical axonal pathways. The networks are developed for five patients (the extraction is performed twice for patient A). We first perform a comparison of the competing algorithms, then we discuss the clustering results of GANC.\nWhen applying the Rosvall-Bergstrom and Blondel et al. algorithms the used does not have the freedom to choose the number of clusters. Hence we repeat the experiment twice to make a meaningful objective comparison in terms of NAssoc. Table 3 lists the average NAssoc of every clustering algorithm and patient. In the first subtable, k is set to the model order selected by the Rosvall-Bergstrom algorithm. Then, in the second subtable, k is set to the model order selected by the Blondel et al. algorithm, and in the third subtable, peaks of the model order selected by curvature are used. Note that the Shi-Malik algorithm outperforms the other two spectral algorithms for these datasets; hence we only include the Shi-Malik results.\nThe clustering results corresponding to the curvature peaks include cluster(s) that contain nodes from both of the brain hemispheres and clusters that include nodes from only one of the hemispheres. In the following discussion, we call the clusters that contain nodes from both of the hemispheres, the central clusters. Graphs A1, B, D, and E include only one central cluster. Graphs A2 and C each include two central clusters.\nThe regions of the brain that are commonly grouped in the central clusters are posterior cingulate cortex, precuneus, cuneus, paracentral lobule, pericalcarine cortex, caudal anterior cingulate cortex, isthmuscingulate, isthmus of the cingulate cortex, and lingual gyrus, provided that graph B is excluded. If the second largest peak of the curvature is selected for B (k = 5), the same regions are assigned to its central cluster. The first five of the mentioned regions are also classified as part of the structural core proposed by Hagmann et al. [66]; Hagmann et al. used cluster strengths, cluster degrees, k-Core, s-Core, betweenness centrality,\nTable 4: Larger real networks: Partition metrics ( (Number of clusters) NAssoc per cluster/execution time).\nNetwork Cond-Mat Googleweb Amazon AS-Skitter\n# of Nodes/Edges 36458/171736 342408/1142134 403364/2443311 1694616/11094209\nAvg/Max Degree 28.5/278 6.7/1367 12.1/2752 13.1/35455\nGANC (2121) 0.784/0m2s (12213) 0.869/2m30s (11237) 0.768/3m4s (26399) 0.769/46m33s\nDhillon-Guan-Kulis (2121) 0.669/0m3s (12213) \u2014 (11237) 0.679/6m0s (26399) \u2014\nRosvall-Bergstrom (2121) 0.746/0m10s (12213) 0.844/1m17s (11237) 0.712/11m26s (26399) 0.720/99m37s\nGANC (82) 0.957/0m2s (239) 0.993/2m30s (230) 0.984/3m4s (1776) 0.933/46m33s\nDhillon-Guan-Kulis (82) 0.784/<1s (239) 0.935/0m2s (230) 0.872/0m4s (1776) 0.709/8m0s\nBlondel et al. (82) 0.849/0m1s (239) 0.990/0m5s (230) 0.952/0m14s (1776) 0.859/0m4s\nGANC (37) 0.970/0m2s (7) 0.999/2m30s (22) 0.997/3m4s (32) 0.994/46m33s\nGANC (526) 0.902/0m2s (855) 0.984/2m30s (206) 0.996/3m4s \u2014\nand efficiency12 to propose a structural core of the brain consisting of eight regions. The authors of [66] also used modularity maximization and found six modules, two of which included nodes from both of the hemispheres (central clusters). The regions that are assigned to the central clusters by modularity maximization are similar to the ones extracted by GANC."}, {"heading": "5.4.3. Larger networks", "text": "Here we illustrate the performance of our algorithm on large graphs. We apply our algorithm to four networks with different natures: Cond-Mat (a collaboration network) [67], Googleweb (a web graph) [60]13, Amazon (a product co-purchasing network)[68], and ASSkitter (an autonomous system graph)[69]. The nodes in Cond-Mat represent scientists that submit their preprints to the condensed matter archive at www.arxiv.org; the edges represent co-authorships. The nodes in Googleweb represent websites and the directed edges represent the existence of hyperlinks. Each node in Amazon network corresponds to a product purchased at www.amazon.com. Each directed edge from a node to another means that when the former is purchased, the latter is frequently also purchased. AS-Skitter is a network of autonomous systems extracted by traceroute analysis.14\nThe Cond-Mat graph is weighted and the rest of the graphs are unweighted. In Table 4, the average and maximum degrees do not take the weights into account; the number of edges affects the run-time of GANC, not the weight values. None of the above graphs are originally connected. However each has a very large connected component that includes the majority of the nodes and the edges. Here we conduct clustering analysis of the largest connected component. Some of these graphs are directed, but we construct undirected graphs by adding the adjacency matrix to its transpose.\n12Cluster strength and degree are the weighted and unweighted degrees, respectively. k-Core (s-Core) is the largest subgraph that includes nodes of degree (respectively, strength) at least k (respectively, s). Betweenness centrality of a region is a measure of the proportion of the shortest paths passing through it. Efficiency of a region indicates how short the region\u2019s average path lengths to other regions are.\n13Google programming contest: http://www.google.com/programming-contest/ 14http://www.caida.org/tools/measurement/skitter\nTable 4 compares the performance of the algorithms that are scalable to such large networks. The table shows the average NAssoc per cluster of the identified partitioning and the time required for completion of the algorithm. The spectral clustering algorithms cannot be executed on our test machine when either the number of nodes or the number of clusters is very large. Both the computational time and the memory requirements are excessive. We therefore compare the performance of GANC, Dhillon-Guan-Kulis (no local search), Rosvall-Bergstrom, and the Blondel et al. algorithms.\nThe Rosvall-Bergstrom and Blondel et al. algorithms automatically select the number of clusters. A meaningful comparison of the average NAssoc is only possible when the number of clusters are equal; to facilitate comparisons, we therefore construct multiple partitionings for GANC and the Dhillon-Guan-Kulis algorithms, each with a different number of clusters. We also construct a partitioning corresponding to one or more of the peaks in the curvature plot (for some of the networks, there are two peaks that are very similar in value, so we consider it useful to examine the partitionings corresponding to each). The last two rows of Table 4 correspond to those peaks.\nGANC is superior to other algorithms with respect to maximizing NAssoc in all of the cases. GANC also significantly outperforms the Dhillon-Guan-Kulis algorithm, which is also striving to maximize NAssoc. The latter is also outperformed by Rosvall-Bergstrom and Blondel et al. algorithms.\nIn terms of the computation time, GANC is often faster than Rosvall-Bergstrom, but the scaling behavior is different. To illustrate this, in addition to the graphs listed in Table 1, we have applied the algorithms to the road network graph of California [60] which is extremely sparse (the average degree is 2.8). The graph contains around 2 million nodes. While GANC performs the clustering in 32 seconds, in takes 197 minutes for the Rosvall-Bergstrom algorithm to converge to a solution. The complexity of GANC is dominated by the agglomerative clustering procedure which requires O(mh logn) operations. Hence the speed of GANC depends on the number of edges in the graph, and the height of the dendrogram. However the graph sparsity does not show an impact on Rosvall-Bergstrom\u2019s execution time. Blondel\u2019s algorithm is much faster than GANC, but as mentioned previously, it suffers from resolution limit associated with clustering algorithms that maximize modularity. The Dhillon-GuanKulis algorithm is also very fast for small values of k, but it gets slower and its memory requirements become excessive if k becomes large.\nTo illustrate the properties of the algorithm outputs in terms of the cluster sizes and the connectivity of individual clusters, we have examined the plots of NAssoc of individual clusters versus the number of nodes in them. Each algorithm behaves similarly on different graphs presented in Table 4. Figure 4a compares GANC and Rosvall-Bergstrom when applying these algorithms on Amazon co-purchasing graph. For the same number of clusters, GANC produces clusters with higher values of NAssoc than Rosvall-Bergstrom. The latter produces many small clusters with very low values of NAssoc. The behavior of the Dhillon-Guan-Kulis and the Blondel et al. algorithms are discussed in our case study."}, {"heading": "5.5. Case Study: US Patent Citation Graph", "text": "As a case study, we consider the undirected version of the citation graph released by the National Bureau of Economic Research [70, 71]. The patents are classified into 6 broad technological categories. A more refined classification leads to 36 sub-categories. We use\neach patent\u2019s label (category or sub-category) in addition to NAssoc and runtime to perform a comparison.\nThe original graph is not connected; however the largest connected component contains more than 99.7% of the nodes and all of the edges. Hence we focus on the largest connected component which contains 3,764,117 nodes and 16,511,740 edges. The maximum node degree is 793."}, {"heading": "5.5.1. Clustering Runtime", "text": "The Rosvall-Bergstrom algorithm [7] was terminated after more than 30 hours without converging to a solution. GANC takes 77 minutes to construct the full hierarchy. After the hierarchy has been generated, each flat partitioning including the refinement takes less than 35 seconds. The algorithm by Blondel et al. [56] takes 8 minutes. The Dhillon-Guan-Kulis algorithm [11] takes 72 seconds for k = 57 and increases as k is increased."}, {"heading": "5.5.2. Maximization of Normalized Association", "text": "In order to have a fair comparison in terms of NAssoc, we fix k = 57 to match the number of clusters of the Blondel et al. result. The values of NAssoc/k for GANC, Dhillon-GuanKulis, and Blondel et al. are 0.964, 0.859, and 0.855, respectively. The values of NAssoc of the individual clusters are shown in Figure 4b which illustrates the clear superiority of GANC."}, {"heading": "5.5.3. Extraction of True Clusters and Absence of Large Well-Defined Clusters", "text": "We use the categories and sub-categories to classify the nodes in the patent citation graph. We denote the maximum proportion of nodes from the same class in a given cluster as the homogeneity proportion of that cluster. For the Blondel et al. and Dhillon-GuanKulis algorithms, we use k = 57 and for GANC we use k = 52 (the closest peak of the curvature plot). The clusters are sorted according to their homogeneity proportion in Figure\n5a. The figure shows the superiority of GANC in extracting clusters of nodes from the same categories. When sub-categories are employed for the assessment, the superiority of GANC becomes more pronounced.\nFigure 5a alone does not provide a fair comparison as any singleton would have homogeneity proportion of 1. In Figure 5b, the homogeneity proportion is plotted versus the size of the extracted clusters for the three algorithms.\nThe Blondel et al. algorithm produces clusters as small as 14 to as large as 250,000 nodes. However the largest homogeneous cluster that is extracted has 38 nodes. The quality of clusters degrade as the size of the clusters increase. This trend is very likely to be due to the resolution limit of modularity.\nBoth GANC and Dhillon-Guan-Kulis produce a very large cluster (the core cluster [60]). Excluding the core, Dhillon-Guan-Kulis produces clusters of an average size of n/k which is around 37,000 nodes in this example (See Figure 5b). This behavior of the Dhillon-GuanKulis algorithm is due to the underlying region-growing procedure it adopts from Metis [17].\nThe clusters extracted by GANC are much smaller than the other two algorithms. Excluding the core, the cluster sizes are between 7 and 225. Similar to previous observations of Leskovec et al. [60], the strongly-connected clusters of the large graphs listed in Table 4 diminish in number as we move towards the center of the graph. Having a small homogeneity proportion is then expected for the core as it includes several clusters that are not very well-connected and are lumped together. If we study a clustering that is in a lower level in the hierarchy, further clusters would be extracted from the core and the core shrinks. The resulting clusters are not as well-connected in terms of NAssoc though. In other words, the most isolated and well-connected clusters are extracted at the higher levels of the hierarchy."}, {"heading": "6. Conclusion", "text": "We proposed a novel algorithm to maximize normalized association that consists of three steps: the agglomerative hierarchical clustering procedure, the model order selection step, and the refinement step.\nThe agglomerative clustering procedure requires O(n log2 n) operations for many real-life graphs, where n is the number of nodes in the graph. This procedure dominates the computational complexity of the other steps of the algorithm. The second step of our algorithm, the model order selection, is based on the relative improvement of normalized association when changing the number of clusters; the curvature plot is used to select one or several model orders for the final clustering. Unlike modularity, the curvature metric does not exhibit an intrinsic resolution limit. For a multi-resolution analysis, a user can specify a range on the allowable number of clusters, and the algorithm will select the number of clusters with the maximum curvature in that range. After selecting the number of clusters, the clusters are passed to the refinement step. This step of our algorithm iterates over the boundary nodes in the clusters and explores possible improvements in normalized association by moving each of the boundary nodes to their neighboring clusters. Using the map data structure, the overhead added by the refinement becomes negligible. Experiments show that despite the negligible runtime of the refinement step, it significantly improves the initial results with respect to the normalized association maximization.\nOur experimental analysis on relatively small networks indicated that the proposed algorithm identified partitions that have normalized association values comparable to the spectral algorithms that involve an eigendecomposition. These algorithms are too computationally complex to be applied to very large graphs. We demonstrated that our proposed algorithm can be applied to large graphs (millions of nodes and edges). For these large graphs, the proposed algorithm identifies partitions that have larger values of normalized association than those identified by the only comparable algorithm that directly addresses the normalized cut metric.\nA clustering algorithm can by no means be suitable for every application. For example, despite the failure of Dhillon-Guan-Kulis algorithm to extract clusters with nodes of the same categories in the patent citation network, it generates clusters of very similar sizes. This is more suitable for VLSI applications for instance [72]. On the other hand, when clustering is meant to extract \u201ccommunities\u201d (group of nodes with strong intra-connection), GANC and Rosvall-Bergstrom are clearly preferred.\nThe Blondel et al. and Rosvall-Bergstrom algorithms are not able to generate a partitioning to an arbitrary number of clusters15; even though they are agglomerative, they do not generate the complete hierarchy because they merge several nodes/clusters in each of their iterations. Hence if one is interested in several arbitrary clustering levels, GANC fits one\u2019s requirement the best out of the competing algorithms discussed.\nIn this paper we only considered undirected graphs while the edge directions could carry valuable information about the structure of a graph. An extension of GANC can be developed by adopting the generalized normalized cut criterion [73].\n15The available implementation of the Rosvall-Bergstrom algorithm is based on the Blondel et al. algorithm. See http://www.tp.umu.se/~rosvall/algorithm.pdf."}], "references": [{"title": "Social network analysis: Methods and applications", "author": ["S. Wasserman", "K. Faust"], "venue": "Cambridge Univ. Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Functional cartography of complex metabolic networks", "author": ["R. Guimera", "L. Amaral"], "venue": "Nature 433 (7028) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Efficient algorithms for accurate hierarchical clustering of huge datasets: tackling the entire protein space", "author": ["Y. Loewenstein", "E. Portugaly", "M. Fromer", "M. Linial"], "venue": "Bioinformatics 24 (13) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Revealing modular architecture of human brain structural networks by using cortical thickness from MRI", "author": ["Z. Chen", "Y. He", "P. Rosa-Neto", "J. Germann", "A. Evans"], "venue": "Cerebral Cortex 18 (10) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Spectral Clustering and Label Fusion For 3D Tissue Classification: Sensitivity and Consistency Analysis", "author": ["W. Crum"], "venue": "in: Proc. Med. Im. Underst. Anal., Dundee, Scotland", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Community structure in social and biological networks", "author": ["M. Girvan", "M.E.J. Newman"], "venue": "Proc. Natl. Acad. Sci. 99 (12) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Maps of random walks on complex networks reveal community structure", "author": ["M. Rosvall", "C.T. Bergstrom"], "venue": "Proc. Natl. Acad. Sci. 105 (4) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Diffusion maps and coarse-graining: a unified framework for dimensionality reduction", "author": ["S. Lafon", "A. Lee"], "venue": "graph partitioning, and data set parameterization, IEEE Trans. Patt. Anal. Mach. Intel. 28 (9) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Trans. Patt. Anal. Mach. Intel. 22 (8) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Spectral grouping using the Nystr\u00f6m method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "IEEE Trans. Patt. Anal. Mach. Intel. 26 (2) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Weighted graph cuts without eigenvectors: a multilevel approach", "author": ["I. Dhillon", "Y. Guan", "B. Kulis"], "venue": "IEEE Trans. Patt. Anal. Mach. Intel. 29 (11) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Network community discovery: solving modularity clustering via normalized cut", "author": ["L. Yu", "C. Ding"], "venue": "in: Proc. ACM Wkshp Mining and Learn. with Graphs", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Approximate counting", "author": ["A. Sinclair", "M. Jerrum"], "venue": "uniform generation and rapidly mixing Markov chains, Info. Comp. 82 (1) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1989}, {"title": "Algorithms for partitioning of graphs and computer logic based on eigenvectors of connection matrices", "author": ["W. Donath", "A. Hoffman"], "venue": "IBM Tech. Disc. Bull. 15 (3) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1972}, {"title": "Partitioning sparse matrices with eigenvectors of graphs", "author": ["A. Pothen", "H. Simon", "K.-P. Liou"], "venue": "SIAM J. Matrix Anal. App. 11 (1) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1990}, {"title": "A multilevel algorithm for partitioning graphs", "author": ["B. Hendrickson", "R. Leland"], "venue": "in: Proc. ACM Int. Conf. Supercomp., Barcelona, Spain", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1995}, {"title": "A fast and high quality multilevel scheme for partitioning irregular graphs", "author": ["G. Karypis", "V. Kumar"], "venue": "SIAM J. Sci. Comp. 20 (1) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "A random walks view of spectral segmentation", "author": ["M. Meila", "J. Shi"], "venue": "in: Proc. Int. Wkshp Art. Intel. Stat., Key West, FL, USA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A. Ng", "M. Jordan", "Y. Weiss"], "venue": "in: Proc. Adv. Neur. Inf. Proc. Sys., Vancouver, BC, Canada", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Fast approximate spectral clustering", "author": ["D. Yan", "L. Huang", "M.I. Jordan"], "venue": "in: Proc. ACM Int. Conf. Knowl. Disc. Data Mining, Paris, France", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast multiscale image segmentation", "author": ["E. Sharon", "A. Brandt", "R. Basri"], "venue": "in: Proc. IEEE Conf. Comp. Vis. Patt. Recog., Hilton Head, SC, USA", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "A min-max cut algorithm for graph partitioning and data clustering", "author": ["C. Ding", "X. He", "H. Zha", "M. Gu", "H. Simon"], "venue": "in: Proc. IEEE Int. Conf. Data Mining, San Jose, CA, USA", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Segmentation and boundary detection using multiscale intensity measurements", "author": ["E. Sharon", "A. Brandt", "R. Basri"], "venue": "in: Proc. IEEE Conf. Comp. Vis. Patt. Recog., Kauai, HI, USA", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "An integrated segmentation and classification approach applied to multiple sclerosis analysis", "author": ["A. Akselrod-Ballin", "M. Galun", "R. Basri", "A. Brandt", "M. Gomori", "M. Filippi", "P. Valsasina"], "venue": "in: Proc. IEEE Conf. Comp. Vis. Patt. Recog., New York, NY, USA", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Efficient multilevel brain tumor segmentation with integrated bayesian model classification", "author": ["J. Corso", "E. Sharon", "S. Dube", "S. El-Saden", "U. Sinha", "A. Yuille"], "venue": "IEEE Trans. Med. Im. 27 (5) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "An examination of procedures for determining the number of clusters in a data set", "author": ["G. Milligan", "M. Cooper"], "venue": "Psychometrika 50 (2) ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1985}, {"title": "On clustering validation techniques", "author": ["M. Halkidi", "Y. Batistakis", "M. Vazirgiannis"], "venue": "J. Intel. Inf. Sys. 17 (2) ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2001}, {"title": "A dendrite method for cluster analysis", "author": ["T. Cali\u0144ski", "J. Harabasz"], "venue": "Comm. Stat. Theo. Meth. 3 (1) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1974}, {"title": "A general statistical framework for assessing categorical clustering in free recall", "author": ["L. Hubert", "J. Levin"], "venue": "Psych. Bull. 83 (6) ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1976}, {"title": "Measuring the power of hierarchical cluster analysis", "author": ["F. Baker", "L. Hubert"], "venue": "J. American Stat. Assoc. ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1975}, {"title": "Cubic Clustering Criterion", "author": ["W. Sarle"], "venue": "Tech. rep., Cary, NC: SAS Inst. Inc ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1983}, {"title": "Estimating the number of clusters in a data set via the gap statistic", "author": ["R. Tibshirani", "G. Walther", "T. Hastie"], "venue": "J. Royal Stat. Soc.: Series B (Stat. Meth.) 63 (2) ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2001}, {"title": "Graph clustering using distance-k cliques", "author": ["J. Edachery", "A. Sen", "F. Brandenburg"], "venue": "in: Graph Drawing, Vol. 1731 of Lecture Notes in Comp. Science, Springer Berlin / Heidelberg", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1999}, {"title": "Interpreting and assessing the results of cluster analyses", "author": ["R. Gnanadesikan", "J. Kettenring", "J. Landwehr"], "venue": "Bull. Int. Stat. Inst. 47 (2) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1977}, {"title": "A criterion for determining the number of groups in a data set using sum-of-squares clustering", "author": ["W. Krzanowski", "Y. Lai"], "venue": "Biometrics 44 (1) ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1988}, {"title": "Automatic cluster stopping with criterion functions and the gap statistic", "author": ["T. Pedersen", "A. Kulkarni"], "venue": "in: Proc. Conf. North American Chap. Assoc. Comp. Ling. Human Lang. Tech., New York City, NY, USA", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "A test for clusters", "author": ["S. Arnold"], "venue": "J. Market. Res. 16 (4) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1979}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "Ann. Stat. 6 (2) ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1978}, {"title": "A new look at the statistical model identification", "author": ["H. Akaike"], "venue": "IEEE Trans. Auto. Control 19 (6) ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1974}, {"title": "Pattern clustering by multivariate mixture analysis", "author": ["J. Wolfe"], "venue": "Multivar. Behav. Res. 5 (3) ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1970}, {"title": "X-means: Extending K-means with Effcient Estimation of the Number of Clusters", "author": ["D. Pelleg", "A. Moore"], "venue": "in: Proc. Int. Conf. Mach. Learn., Stanford, CA, USA", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2000}, {"title": "Uncovering latent structure in valued graphs: a variational approach", "author": ["M. Mariadassou", "S. Robin", "C. Vacher"], "venue": "Ann. App. Stat. 4 (2) ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian methods for graph clustering", "author": ["P. Latouche", "E. Birmel", "C. Ambroise"], "venue": "Tech. rep., Laboratoire Statistique et Gnome, UMR CNRS 8071-INRA 1152-UEVE, 91000 Evry, France ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Bayesian methods for graph clustering", "author": ["P. Latouche", "E. Birmel", "C. Ambroise"], "venue": "in: Adv. Data Anal. Data Hand. Bus. Intel.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic image segmentation by typical cuts", "author": ["Y. Gdalyahu", "D. Weinshall", "M. Werman"], "venue": "in: Proc. IEEE Comp. Soc. Conf. Comp. Vis. Patt. Recog., Fort Collins, CO, USA", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1999}, {"title": "On clustering using random walks", "author": ["D. Harel", "Y. Koren"], "venue": "in: Proc. Conf. Found. Soft. Tech. Theor. Comp. Sci., Bangalore, India", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2001}, {"title": "Spectral methods for automatic multiscale data clustering", "author": ["A. Azran", "Z. Ghaharamani"], "venue": "in: Proc. IEEE Conf. Comp. Vis. Patt. Recog., New York, NY, USA", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2006}, {"title": "Self-tuning spectral clustering", "author": ["L. Zelnik-Manor", "P. Perona"], "venue": "in: Proc. Adv. Neur. Inf. Proc. Sys., Vancouver, BC, Canada", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2004}, {"title": "A stability based method for discovering structure in clustered data", "author": ["A. Ben-Hur", "A. Elisseeff", "I. Guyon"], "venue": "in: Pac. Symp. Biocomp., Lihue, HI, USA", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2002}, {"title": "Stability-based validation of clustering solutions", "author": ["T. Lange", "V. Roth", "M. Braun", "J. Buhmann"], "venue": "Neural Comp. 16 (6) ", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2004}, {"title": "A sober look at clustering stability", "author": ["S. Ben-David", "U. Von Luxburg", "D. P\u00e1l"], "venue": "Learning Theory ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2006}, {"title": "Cluster stability scores for microarray data in cancer studies", "author": ["M. Smolkin", "D. Ghosh"], "venue": "BMC Bioinformatics 4 (1) ", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2003}, {"title": "Fitting a graph to vector data", "author": ["S.I. Daitch", "J.A. Kelner", "D.A. Spielman"], "venue": "in: Proc. ACM Int. Conf. Mach. Learn., Montreal, QC, Canada", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2009}, {"title": "Influence of graph construction on graph-based clustering measures", "author": ["M. Maier", "U. Von Luxburg", "M. Hein"], "venue": "in: Proc. Adv. Neur. Inf. Proc. Sys., Vancouver, BC, Canada", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast unfolding of communities in large networks", "author": ["V. Blondel", "J. Guillaume", "R. Lambiotte", "E. Lefebvre"], "venue": "J. Stat. Mech.: Theor. Exp. 2008 ", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2008}, {"title": "Finding community structure in very large networks", "author": ["A. Clauset", "M. Newman", "C. Moore"], "venue": "Phys. Rev. E 70 (6) ", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2004}, {"title": "C++ Standard Template Library", "author": ["P. Plauger", "M. Lee", "D. Musser", "A. Stepanov"], "venue": "Prentice Hall PTR Upper Saddle River, NJ, USA", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2000}, {"title": "Performance of modularity maximization in practical contexts", "author": ["B.H. Good", "Y.-A. de Montjoye", "A. Clauset"], "venue": "Phys. Rev. E", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2010}, {"title": "Benchmarks for testing community detection algorithms on directed and weighted graphs with overlapping communities", "author": ["A. Lancichinetti", "S. Fortunato"], "venue": "Phys. Rev. E 80 (1) ", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2009}, {"title": "Algorithms for graph partitioning on the planted partition model", "author": ["A. Condon", "R.M. Karp"], "venue": "Rand. Struct. Alg. 18 (2) ", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2001}, {"title": "Grooming", "author": ["R. Dunbar"], "venue": "gossip, and the evolution of language, Harvard Univ. Press, MA, USA", "citeRegEx": "63", "shortCiteRegEx": null, "year": 1998}, {"title": "Comparing classifications: an evaluation of several coefficients of partition agreement", "author": ["M. Downton", "T. Brennan"], "venue": "in: Proc. Meet. Class. Soc., Boulder, CO, USA", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1980}, {"title": "An information flow model for conflict and fission in small groups", "author": ["W. Zachary"], "venue": "J. Anthrop. Res. 33 (4) ", "citeRegEx": "65", "shortCiteRegEx": null, "year": 1977}, {"title": "Mapping the structural core of human cerebral cortex", "author": ["P. Hagmann", "L. Cammoun", "X. Gigandet", "R. Meuli", "C. Honey", "V. Wedeen", "O. Sporns"], "venue": "PLoS Biol. 6 (7) ", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2008}, {"title": "The structure of scientific collaboration networks", "author": ["M. Newman"], "venue": "Proc. Natl. Acad. Sci. 98 (2) ", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2001}, {"title": "The dynamics of viral marketing", "author": ["J. Leskovec", "L.A. Adamic", "B.A. Huberman"], "venue": "in: Proc. Int. Conf. Elect. Commerce, Ann Arbor, MI, USA", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2006}, {"title": "Graphs over time: densification laws", "author": ["J. Leskovec", "J. Kleinberg", "C. Faloutsos"], "venue": "shrinking diameters and possible explanations, in: Proc. ACM SIGKDD Int. Conf. Knowl. Disc. Data Mining, 26  ACM, Chicago, Illinois, USA", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2005}, {"title": "A connectivity based clustering algorithm with application to VLSI circuit partitioning", "author": ["J. Li", "L. Behjat"], "venue": "IEEE Trans. Circ. Sys. II: Exp. Briefs 53 (5) ", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2006}, {"title": "Clustering by weighted cuts in directed graphs", "author": ["M. Meila", "W. Pentney"], "venue": "in: Proc. SIAM Int. Conf. Data Mining, Minneapolis, MN, USA", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "It has been used for many years to study social networks [1] and continues to be employed in the field of sociology to explore social interactions.", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "More recently it has been employed in the study of biochemical networks [2, 3], biological neural networks [4, 5], and transport and communication networks.", "startOffset": 72, "endOffset": 78}, {"referenceID": 2, "context": "More recently it has been employed in the study of biochemical networks [2, 3], biological neural networks [4, 5], and transport and communication networks.", "startOffset": 72, "endOffset": 78}, {"referenceID": 3, "context": "More recently it has been employed in the study of biochemical networks [2, 3], biological neural networks [4, 5], and transport and communication networks.", "startOffset": 107, "endOffset": 113}, {"referenceID": 4, "context": "More recently it has been employed in the study of biochemical networks [2, 3], biological neural networks [4, 5], and transport and communication networks.", "startOffset": 107, "endOffset": 113}, {"referenceID": 5, "context": ", modularity [6] or information-theoretic criteria based on the minimum description length [7].", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": ", modularity [6] or information-theoretic criteria based on the minimum description length [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "On the other hand, clustering algorithms based on Markov random walks [7, 8] also value indirect connections and network flow.", "startOffset": 70, "endOffset": 76}, {"referenceID": 7, "context": "On the other hand, clustering algorithms based on Markov random walks [7, 8] also value indirect connections and network flow.", "startOffset": 70, "endOffset": 76}, {"referenceID": 8, "context": "In this paper we select the normalized cut criterion [9], which simultaneously encourages intra-cluster similarity while penalizing inter-cluster similarities.", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "Methods based on this criterion have been employed successfully in a wide range of applications [5, 8\u201312].", "startOffset": 96, "endOffset": 105}, {"referenceID": 7, "context": "Methods based on this criterion have been employed successfully in a wide range of applications [5, 8\u201312].", "startOffset": 96, "endOffset": 105}, {"referenceID": 8, "context": "Methods based on this criterion have been employed successfully in a wide range of applications [5, 8\u201312].", "startOffset": 96, "endOffset": 105}, {"referenceID": 9, "context": "Methods based on this criterion have been employed successfully in a wide range of applications [5, 8\u201312].", "startOffset": 96, "endOffset": 105}, {"referenceID": 10, "context": "Methods based on this criterion have been employed successfully in a wide range of applications [5, 8\u201312].", "startOffset": 96, "endOffset": 105}, {"referenceID": 11, "context": "Methods based on this criterion have been employed successfully in a wide range of applications [5, 8\u201312].", "startOffset": 96, "endOffset": 105}, {"referenceID": 12, "context": "The normalized cut criterion is related to the conductance of the underlying graph [13].", "startOffset": 83, "endOffset": 87}, {"referenceID": 8, "context": "The normalized cut metric was introduced by Shi and Malik in [9] to address this shortcoming.", "startOffset": 61, "endOffset": 64}, {"referenceID": 8, "context": "Minimizing normalized cut is an NP-complete problem [9].", "startOffset": 52, "endOffset": 55}, {"referenceID": 8, "context": "In order to identify a partitioning of n nodes to k clusters, some techniques perform recursive bipartitioning [9], and thus require the repeated identification of two eigenvectors.", "startOffset": 111, "endOffset": 114}, {"referenceID": 13, "context": "Spectral partitioning of graphs was first proposed by Donath and Hoffman in the 1970\u2019s [14].", "startOffset": 87, "endOffset": 91}, {"referenceID": 14, "context": "described an algorithm for bi-partitioning using the Fiedler vector [15].", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "contributed with multilevel algorithms for more efficient spectral partitioning [16, 17].", "startOffset": 80, "endOffset": 88}, {"referenceID": 16, "context": "contributed with multilevel algorithms for more efficient spectral partitioning [16, 17].", "startOffset": 80, "endOffset": 88}, {"referenceID": 8, "context": "The normalized cut metric was introduced by Shi and Malik in [9].", "startOffset": 61, "endOffset": 64}, {"referenceID": 17, "context": "In [18], Meila and Shi proposed an algorithm that calculates k eigenvectors (thereby associating k real values with each node in the graph) and then uses a clustering algorithm, such as k-means, to do the partitioning in R.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "observed in [19] that the algorithm in [18] is susceptible to failure when there is substantial variation in the degree of connectivity between clusters.", "startOffset": 12, "endOffset": 16}, {"referenceID": 17, "context": "observed in [19] that the algorithm in [18] is susceptible to failure when there is substantial variation in the degree of connectivity between clusters.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "described a procedure that uses the Nystr\u00f6m method to reduce the complexity of the eigenvalue problem [10].", "startOffset": 102, "endOffset": 106}, {"referenceID": 19, "context": "recently described an algorithm for fast approximate spectral clustering [20], but the focus is not on clustering for graphs (rather it addresses real-valued feature vectors).", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "introduced a much faster algorithm for minimizing normalized cut in [11]; the graph is first greedily coarsened, then the coarsened graph is partitioned using a region growing procedure [17] and finally weighted kernel k-means clustering is applied to each partition to refine the clustering.", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "introduced a much faster algorithm for minimizing normalized cut in [11]; the graph is first greedily coarsened, then the coarsened graph is partitioned using a region growing procedure [17] and finally weighted kernel k-means clustering is applied to each partition to refine the clustering.", "startOffset": 186, "endOffset": 190}, {"referenceID": 20, "context": "proposed a scalable hierarchical clustering using ratio association (the normalization term is the number of nodes in clusters) [21].", "startOffset": 128, "endOffset": 132}, {"referenceID": 21, "context": "[22], Sharon et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23], Akselrod-Ballin et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24], and Corso et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] that use the sum of internal weights of clusters as the normalization term.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "The majority of the methods discussed in [26] and [27] are of this type; Calinski and Harabasz [28], C-index [29], Baker and Hubert [30], and cubic clustering criterion [31] are among the most effective ones [26].", "startOffset": 41, "endOffset": 45}, {"referenceID": 26, "context": "The majority of the methods discussed in [26] and [27] are of this type; Calinski and Harabasz [28], C-index [29], Baker and Hubert [30], and cubic clustering criterion [31] are among the most effective ones [26].", "startOffset": 50, "endOffset": 54}, {"referenceID": 27, "context": "The majority of the methods discussed in [26] and [27] are of this type; Calinski and Harabasz [28], C-index [29], Baker and Hubert [30], and cubic clustering criterion [31] are among the most effective ones [26].", "startOffset": 95, "endOffset": 99}, {"referenceID": 28, "context": "The majority of the methods discussed in [26] and [27] are of this type; Calinski and Harabasz [28], C-index [29], Baker and Hubert [30], and cubic clustering criterion [31] are among the most effective ones [26].", "startOffset": 109, "endOffset": 113}, {"referenceID": 29, "context": "The majority of the methods discussed in [26] and [27] are of this type; Calinski and Harabasz [28], C-index [29], Baker and Hubert [30], and cubic clustering criterion [31] are among the most effective ones [26].", "startOffset": 132, "endOffset": 136}, {"referenceID": 30, "context": "The majority of the methods discussed in [26] and [27] are of this type; Calinski and Harabasz [28], C-index [29], Baker and Hubert [30], and cubic clustering criterion [31] are among the most effective ones [26].", "startOffset": 169, "endOffset": 173}, {"referenceID": 25, "context": "The majority of the methods discussed in [26] and [27] are of this type; Calinski and Harabasz [28], C-index [29], Baker and Hubert [30], and cubic clustering criterion [31] are among the most effective ones [26].", "startOffset": 208, "endOffset": 212}, {"referenceID": 31, "context": "[32] define F (k) as the gap, that is the difference of the average pairwise distance of the data points of the clustering at k level and the expected value of the same measure of some reference model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "This is similar to modularity [6] (discussed later) in the sense that it compares the clustering results to a reference model.", "startOffset": 30, "endOffset": 33}, {"referenceID": 32, "context": "Possible distance metrics include the shortest path [33] or the diffusion distance [8]; however the shortest path is very sensitive to noise and the calculation of the diffusion distance requires eigendecomposition.", "startOffset": 52, "endOffset": 56}, {"referenceID": 7, "context": "Possible distance metrics include the shortest path [33] or the diffusion distance [8]; however the shortest path is very sensitive to noise and the calculation of the diffusion distance requires eigendecomposition.", "startOffset": 83, "endOffset": 86}, {"referenceID": 33, "context": "Hence, the value of F (k) corresponding to two or more choices of k are examined to quantify the significance of a given level of a hierarchical clustering [34\u201336].", "startOffset": 156, "endOffset": 163}, {"referenceID": 34, "context": "Hence, the value of F (k) corresponding to two or more choices of k are examined to quantify the significance of a given level of a hierarchical clustering [34\u201336].", "startOffset": 156, "endOffset": 163}, {"referenceID": 35, "context": "Hence, the value of F (k) corresponding to two or more choices of k are examined to quantify the significance of a given level of a hierarchical clustering [34\u201336].", "startOffset": 156, "endOffset": 163}, {"referenceID": 33, "context": "[34] propose the fraction F (k)/F (k\u2212 1), Arnold [37] use the value of F (k)\u2212F (k\u2212 1), Krzanowski and Lai [35] employ the fraction |F (k) \u2212 F (k \u2212 1)|/|F (k + 1) \u2212 F (k)|, and Pederson and Kulkarni [36] suggest using the fraction 2\u00d7F (k)/ (F (k \u2212 1) + F (k + 1)).", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[34] propose the fraction F (k)/F (k\u2212 1), Arnold [37] use the value of F (k)\u2212F (k\u2212 1), Krzanowski and Lai [35] employ the fraction |F (k) \u2212 F (k \u2212 1)|/|F (k + 1) \u2212 F (k)|, and Pederson and Kulkarni [36] suggest using the fraction 2\u00d7F (k)/ (F (k \u2212 1) + F (k + 1)).", "startOffset": 49, "endOffset": 53}, {"referenceID": 34, "context": "[34] propose the fraction F (k)/F (k\u2212 1), Arnold [37] use the value of F (k)\u2212F (k\u2212 1), Krzanowski and Lai [35] employ the fraction |F (k) \u2212 F (k \u2212 1)|/|F (k + 1) \u2212 F (k)|, and Pederson and Kulkarni [36] suggest using the fraction 2\u00d7F (k)/ (F (k \u2212 1) + F (k + 1)).", "startOffset": 106, "endOffset": 110}, {"referenceID": 35, "context": "[34] propose the fraction F (k)/F (k\u2212 1), Arnold [37] use the value of F (k)\u2212F (k\u2212 1), Krzanowski and Lai [35] employ the fraction |F (k) \u2212 F (k \u2212 1)|/|F (k + 1) \u2212 F (k)|, and Pederson and Kulkarni [36] suggest using the fraction 2\u00d7F (k)/ (F (k \u2212 1) + F (k + 1)).", "startOffset": 198, "endOffset": 202}, {"referenceID": 34, "context": "However the approaches in [35, 36] are potentially susceptible to noise due to the division; small perturbations in the weights could lead to dramatic changes in the selected model order.", "startOffset": 26, "endOffset": 34}, {"referenceID": 35, "context": "However the approaches in [35, 36] are potentially susceptible to noise due to the division; small perturbations in the weights could lead to dramatic changes in the selected model order.", "startOffset": 26, "endOffset": 34}, {"referenceID": 37, "context": "This allows the application of model selection techniques based on concepts such as the Bayesian InformationCriterion (BIC) [38], and Akaike Information Criterion [39].", "startOffset": 124, "endOffset": 128}, {"referenceID": 38, "context": "This allows the application of model selection techniques based on concepts such as the Bayesian InformationCriterion (BIC) [38], and Akaike Information Criterion [39].", "startOffset": 163, "endOffset": 167}, {"referenceID": 39, "context": "An example is the requirement in [40, 41] that the input data are normally distributed (after projection of the graph into a real space).", "startOffset": 33, "endOffset": 41}, {"referenceID": 40, "context": "An example is the requirement in [40, 41] that the input data are normally distributed (after projection of the graph into a real space).", "startOffset": 33, "endOffset": 41}, {"referenceID": 41, "context": "The more general methods based on mixture models do not scale well to very large graphs; even the recent approaches have only been applied to graphs with a few thousand nodes [42\u201344].", "startOffset": 175, "endOffset": 182}, {"referenceID": 42, "context": "The more general methods based on mixture models do not scale well to very large graphs; even the recent approaches have only been applied to graphs with a few thousand nodes [42\u201344].", "startOffset": 175, "endOffset": 182}, {"referenceID": 43, "context": "The more general methods based on mixture models do not scale well to very large graphs; even the recent approaches have only been applied to graphs with a few thousand nodes [42\u201344].", "startOffset": 175, "endOffset": 182}, {"referenceID": 44, "context": "Some heuristics are based on the sizes of the clusters that are merged at different levels of the clustering hierarchy [45, 46].", "startOffset": 119, "endOffset": 127}, {"referenceID": 45, "context": "Some heuristics are based on the sizes of the clusters that are merged at different levels of the clustering hierarchy [45, 46].", "startOffset": 119, "endOffset": 127}, {"referenceID": 45, "context": "The authors in [46] suggest that when two clusters with large number of nodes are merged, a significant amount of detail is lost; hence such an instance is potentially where a hierarchical clustering algorithm should stop.", "startOffset": 15, "endOffset": 19}, {"referenceID": 44, "context": "of [45] propose a similar approach.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "A well-known and effective method of selecting the number of clusters is to examine the eigenvalues of the Laplacian of the graph that is to be clustered [19, 47, 48].", "startOffset": 154, "endOffset": 166}, {"referenceID": 46, "context": "A well-known and effective method of selecting the number of clusters is to examine the eigenvalues of the Laplacian of the graph that is to be clustered [19, 47, 48].", "startOffset": 154, "endOffset": 166}, {"referenceID": 18, "context": "A large eigengap at k \u2217 is the case in which spectral algorithms using the Laplacian perform most successfully [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 47, "context": "A more robust criterion is proposed by Zelnik-Manor and Perona [49] that uses eigenvectors instead.", "startOffset": 63, "endOffset": 67}, {"referenceID": 48, "context": "The perturbation could be in terms of noise [50], sampling subsets from the input [51, 52], or random projection of a high dimensional data into a lower dimensional space [53].", "startOffset": 44, "endOffset": 48}, {"referenceID": 49, "context": "The perturbation could be in terms of noise [50], sampling subsets from the input [51, 52], or random projection of a high dimensional data into a lower dimensional space [53].", "startOffset": 82, "endOffset": 90}, {"referenceID": 50, "context": "The perturbation could be in terms of noise [50], sampling subsets from the input [51, 52], or random projection of a high dimensional data into a lower dimensional space [53].", "startOffset": 82, "endOffset": 90}, {"referenceID": 51, "context": "The perturbation could be in terms of noise [50], sampling subsets from the input [51, 52], or random projection of a high dimensional data into a lower dimensional space [53].", "startOffset": 171, "endOffset": 175}, {"referenceID": 50, "context": "[52] warn against using stability analysis in this context, and they suggest that this family of model order selection techniques is not suitable for selecting the number of clusters in general.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Two more recently proposed methods to select the number of clusters are based on optimization of the quality metrics of modularity [6] and description length [7].", "startOffset": 131, "endOffset": 134}, {"referenceID": 6, "context": "Two more recently proposed methods to select the number of clusters are based on optimization of the quality metrics of modularity [6] and description length [7].", "startOffset": 158, "endOffset": 161}, {"referenceID": 52, "context": ", [48, 54, 55].", "startOffset": 2, "endOffset": 14}, {"referenceID": 53, "context": ", [48, 54, 55].", "startOffset": 2, "endOffset": 14}, {"referenceID": 8, "context": "For a fixed number of clusters, k, we measure the quality of the partition via the normalized cut metric [9], defined as follows.", "startOffset": 105, "endOffset": 108}, {"referenceID": 8, "context": "We note that a similar greedy merging algorithm is alluded to by Shi and Malik in [9], however they also suggest first projecting each node into R using the first k eigenvectors of the graph Laplacian, and running an algorithm such as k-means to obtain an initial clustering.", "startOffset": 82, "endOffset": 85}, {"referenceID": 54, "context": "A similar approach is taken in [56], but groups of nodes are moved from a cluster to another instead of individual nodes.", "startOffset": 31, "endOffset": 35}, {"referenceID": 55, "context": "We take a similar approach to [57] when implementing the agglomerative clustering procedure of GANC.", "startOffset": 30, "endOffset": 34}, {"referenceID": 55, "context": "This leads to the complexity of O(mh log(n)) for the agglomerative clustering procedure, where h is the height of the generated dendrogram and m is the number of edges with non-zero weight (see [57] for details).", "startOffset": 194, "endOffset": 198}, {"referenceID": 47, "context": ", [49] and [47]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 46, "context": ", [49] and [47]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 56, "context": "Every row of B is stored in a map (C++ STL implementation of the map data structure is used [58]).", "startOffset": 92, "endOffset": 96}, {"referenceID": 8, "context": "We compare to the following algorithms: Shi and Malik (recursive NCut) [9]; Meila and Shi (k-way NCut) [18]; Ng, Jordan, and Weiss [19]; Dhillon, Gaun, and Kulis [11]; Rosvall and Bergstrom [7]; and Blondel et al.", "startOffset": 71, "endOffset": 74}, {"referenceID": 17, "context": "We compare to the following algorithms: Shi and Malik (recursive NCut) [9]; Meila and Shi (k-way NCut) [18]; Ng, Jordan, and Weiss [19]; Dhillon, Gaun, and Kulis [11]; Rosvall and Bergstrom [7]; and Blondel et al.", "startOffset": 103, "endOffset": 107}, {"referenceID": 18, "context": "We compare to the following algorithms: Shi and Malik (recursive NCut) [9]; Meila and Shi (k-way NCut) [18]; Ng, Jordan, and Weiss [19]; Dhillon, Gaun, and Kulis [11]; Rosvall and Bergstrom [7]; and Blondel et al.", "startOffset": 131, "endOffset": 135}, {"referenceID": 10, "context": "We compare to the following algorithms: Shi and Malik (recursive NCut) [9]; Meila and Shi (k-way NCut) [18]; Ng, Jordan, and Weiss [19]; Dhillon, Gaun, and Kulis [11]; Rosvall and Bergstrom [7]; and Blondel et al.", "startOffset": 162, "endOffset": 166}, {"referenceID": 6, "context": "We compare to the following algorithms: Shi and Malik (recursive NCut) [9]; Meila and Shi (k-way NCut) [18]; Ng, Jordan, and Weiss [19]; Dhillon, Gaun, and Kulis [11]; Rosvall and Bergstrom [7]; and Blondel et al.", "startOffset": 190, "endOffset": 193}, {"referenceID": 54, "context": "[56].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "The discussed clustering algorithms ([9],[18],[19]) are not scalable as they include eigendecomposition.", "startOffset": 37, "endOffset": 40}, {"referenceID": 17, "context": "The discussed clustering algorithms ([9],[18],[19]) are not scalable as they include eigendecomposition.", "startOffset": 41, "endOffset": 45}, {"referenceID": 18, "context": "The discussed clustering algorithms ([9],[18],[19]) are not scalable as they include eigendecomposition.", "startOffset": 46, "endOffset": 50}, {"referenceID": 10, "context": "Dhillon, Guan, and Kulis proposed an algorithm that strives to maximize normalized association without requiring any eigendecomposition [11].", "startOffset": 136, "endOffset": 140}, {"referenceID": 6, "context": "Minimization of the criterion proposed by Rosvall and Bergstrom [7] results in a coarsegrained representation of the information flow through the network.", "startOffset": 64, "endOffset": 67}, {"referenceID": 54, "context": "targets maximizing modularity [56].", "startOffset": 30, "endOffset": 34}, {"referenceID": 57, "context": "Modularity provides a valuable metric of the connectedness of clusters, but a number of authors have demonstrated that it suffers from a resolution limit when used to select the number of clusters [59].", "startOffset": 197, "endOffset": 201}, {"referenceID": 11, "context": "By normalizing the summation by d(Ci) as suggested in [12], the resolution limit phenomena is resolved; but we have", "startOffset": 54, "endOffset": 58}, {"referenceID": 58, "context": "We use benchmark graphs developed by Lancichinetti, Fortunato, and Radicchi [61] (LFR graphs).", "startOffset": 76, "endOffset": 80}, {"referenceID": 59, "context": "These random graphs are designed based on the planted partition model [62].", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "The Meila-Shi algorithm performs at least as well as the other spectral clustering algorithms (Ng-Jordan-Weiss [19] and Shi-Malik [9]) and hence we only display the Meila-Shi results.", "startOffset": 111, "endOffset": 115}, {"referenceID": 8, "context": "The Meila-Shi algorithm performs at least as well as the other spectral clustering algorithms (Ng-Jordan-Weiss [19] and Shi-Malik [9]) and hence we only display the Meila-Shi results.", "startOffset": 130, "endOffset": 133}, {"referenceID": 10, "context": "algorithm [11].", "startOffset": 10, "endOffset": 14}, {"referenceID": 60, "context": "In the real-life networks that we consider in this paper, the clusters have been observed to be of a limited size, regardless of the network size [60]; for example the Dunbar number suggests an upper limit of 150 nodes for clusters in a social network [63].", "startOffset": 252, "endOffset": 256}, {"referenceID": 61, "context": "For comparing two partitions on the same graph, we use the Jaccard index [64] which for two partitions, X and Y , is defined as", "startOffset": 73, "endOffset": 77}, {"referenceID": 54, "context": "This example highlights the difference in behavior compared to the modularity metric used by modularity maximization algorithms [56].", "startOffset": 128, "endOffset": 132}, {"referenceID": 57, "context": "[59], to illustrate the resolution limit is the ring of 24 cliques, each of which has 5 nodes and is connected to its neighboring clique by a single edge.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "We conduct experiments using the Zachary karate club network [65], the football network [6], and the political books network.", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "We conduct experiments using the Zachary karate club network [65], the football network [6], and the political books network.", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": "Rosvall\u2019s algorithm uses a description length metric to select the number of clusters [7]; Blondel\u2019s algorithm chooses the partitioning that maximizes the modularity [56].", "startOffset": 86, "endOffset": 89}, {"referenceID": 54, "context": "Rosvall\u2019s algorithm uses a description length metric to select the number of clusters [7]; Blondel\u2019s algorithm chooses the partitioning that maximizes the modularity [56].", "startOffset": 166, "endOffset": 170}, {"referenceID": 63, "context": "Here we study the networks presented in [66] which are weighted graphs, each with 998 nodes.", "startOffset": 40, "endOffset": 44}, {"referenceID": 63, "context": "[66]; Hagmann et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "The authors of [66] also used modularity maximization and found six modules, two of which included nodes from both of the hemispheres (central clusters).", "startOffset": 15, "endOffset": 19}, {"referenceID": 64, "context": "We apply our algorithm to four networks with different natures: Cond-Mat (a collaboration network) [67], Googleweb (a web graph) [60], Amazon (a product co-purchasing network)[68], and ASSkitter (an autonomous system graph)[69].", "startOffset": 99, "endOffset": 103}, {"referenceID": 65, "context": "We apply our algorithm to four networks with different natures: Cond-Mat (a collaboration network) [67], Googleweb (a web graph) [60], Amazon (a product co-purchasing network)[68], and ASSkitter (an autonomous system graph)[69].", "startOffset": 175, "endOffset": 179}, {"referenceID": 66, "context": "We apply our algorithm to four networks with different natures: Cond-Mat (a collaboration network) [67], Googleweb (a web graph) [60], Amazon (a product co-purchasing network)[68], and ASSkitter (an autonomous system graph)[69].", "startOffset": 223, "endOffset": 227}, {"referenceID": 6, "context": "The Rosvall-Bergstrom algorithm [7] was terminated after more than 30 hours without converging to a solution.", "startOffset": 32, "endOffset": 35}, {"referenceID": 54, "context": "[56] takes 8 minutes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The Dhillon-Guan-Kulis algorithm [11] takes 72 seconds for k = 57 and increases as k is increased.", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "This behavior of the Dhillon-GuanKulis algorithm is due to the underlying region-growing procedure it adopts from Metis [17].", "startOffset": 120, "endOffset": 124}, {"referenceID": 67, "context": "This is more suitable for VLSI applications for instance [72].", "startOffset": 57, "endOffset": 61}, {"referenceID": 68, "context": "An extension of GANC can be developed by adopting the generalized normalized cut criterion [73].", "startOffset": 91, "endOffset": 95}], "year": 2011, "abstractText": "This paper describes a graph clustering algorithm that aims to minimize the normalized cut criterion and has a model order selection procedure. The performance of the proposed algorithm is comparable to spectral approaches in terms of minimizing normalized cut. However, unlike spectral approaches, the proposed algorithm scales to graphs with millions of nodes and edges. The algorithm consists of three components that are processed sequentially: a greedy agglomerative hierarchical clustering procedure, model order selection, and a local refinement. For a graph of n nodes and O(n) edges, the computational complexity of the algorithm is O(n log n), a major improvement over the O(n) complexity of spectral methods. Experiments are performed on real and synthetic networks to demonstrate the scalability of the proposed approach, the effectiveness of the model order selection procedure, and the performance of the proposed algorithm in terms of minimizing the normalized cut metric.", "creator": "LaTeX with hyperref package"}}}