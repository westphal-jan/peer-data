{"id": "1105.5444", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2011", "title": "Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language", "abstract": "This article presents a measure of semantic similarity in an IS-A taxonomy based on the notion of shared information content. Experimental evaluation against a benchmark set of human similarity judgments demonstrates that the measure performs better than the traditional edge-counting approach. The article presents algorithms that take advantage of taxonomic similarity in resolving syntactic and semantic ambiguity, along with experimental results demonstrating their effectiveness.", "histories": [["v1", "Fri, 27 May 2011 01:46:05 GMT  (108kb)", "http://arxiv.org/abs/1105.5444v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["p resnik"], "accepted": false, "id": "1105.5444"}, "pdf": {"name": "1105.5444.pdf", "metadata": {"source": "CRF", "title": "Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language", "authors": ["Philip Resnik"], "emails": ["resnik@umiacs.umd.edu"], "sections": [{"heading": null, "text": "Journal of Arti cial Intelligence Research 11 (1999) 95-130 Submitted 3/98; published 7/99Semantic Similarity in a Taxonomy: An Information-BasedMeasure and its Application to Problems of Ambiguity inNatural LanguagePhilip Resnik resnik@umiacs.umd.eduDepartment of Linguistics andInstitute for Advanced Computer StudiesUniversity of MarylandCollege Park, MD 20742 USA AbstractThis article presents a measure of semantic similarity in an is-a taxonomy based onthe notion of shared information content. Experimental evaluation against a benchmarkset of human similarity judgments demonstrates that the measure performs better thanthe traditional edge-counting approach. The article presents algorithms that take advan-tage of taxonomic similarity in resolving syntactic and semantic ambiguity, along withexperimental results demonstrating their e ectiveness.1. IntroductionEvaluating semantic relatedness using network representations is a problem with a longhistory in arti cial intelligence and psychology, dating back to the spreading activationapproach of Quillian (1968) and Collins and Loftus (1975). Semantic similarity represents aspecial case of semantic relatedness: for example, cars and gasoline would seem to be moreclosely related than, say, cars and bicycles, but the latter pair are certainly more similar.Rada et al. (Rada, Mili, Bicknell, & Blettner, 1989) suggest that the assessment of similarityin semantic networks can in fact be thought of as involving just taxonomic (is-a) links, tothe exclusion of other link types; that view will also be taken here, although admittedlylinks such as part-of can also be viewed as attributes that contribute to similarity (cf.Richardson, Smeaton, & Murphy, 1994; Sussna, 1993).Although many measures of similarity are de ned in the literature, they are seldomaccompanied by an independent characterization of the phenomenon they are measuring,particularly when the measure is proposed in service of a computational application (e.g.,similarity of documents in information retrieval, similarity of cases in case-based reasoning).Rather, the worth of a similarity measure is in its utility for the given task. In the cognitivedomain, similarity is treated as a property characterized by human perception and intuition,in much the same way as notions like \\plausibility\" and \\typicality.\" As such, the worthof a similarity measure is in its delity to human behavior, as measured by predictionsof human performance on experimental tasks. The latter view underlies the work in thisarticle, although the results presented comprise not only direct comparison with humanperformance but also practical application to problems in natural language processing.A natural, time-honored way to evaluate semantic similarity in a taxonomy is to measurethe distance between the nodes corresponding to the items being compared | the shorterc 1999 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.\nResnikthe path from one node to another, the more similar they are. Given multiple paths, onetakes the length of the shortest one (Lee, Kim, & Lee, 1993; Rada & Bicknell, 1989; Radaet al., 1989).A widely acknowledged problem with this approach, however, is that it relies on thenotion that links in the taxonomy represent uniform distances. Unfortunately, uniformlink distance is di cult to de ne, much less to control. In real taxonomies, there is widevariability in the \\distance\" covered by a single taxonomic link, particularly when certainsub-taxonomies (e.g., biological categories) are much denser than others. For example, inWordNet (Miller, 1990; Fellbaum, 1998), a widely used, broad-coverage semantic networkfor English, it is not at all di cult to nd links that cover an intuitively narrow distance(rabbit ears is-a television antenna) or an intuitively wide one (phytoplanktonis-a living thing). The same kinds of examples can be found in the Collins COBUILDDictionary (Sinclair, ed., 1987), which identi es superordinate terms for many words (e.g.,safety valve is-a valve seems much narrower than knitting machine is-a machine).In the rst part of this article, I describe an alternative way to evaluate semantic sim-ilarity in a taxonomy, based on the notion of information content. Like the edge-countingmethod, it is conceptually quite simple. However, it is not sensitive to the problem ofvarying link distances. In addition, by combining a taxonomic structure with empiricalprobability estimates, it provides a way of adapting a static knowledge structure to mul-tiple contexts. Section 2 sets up the probabilistic framework and de nes the measure ofsemantic similarity in information-theoretic terms, and Section 3 presents an evaluation ofthe similarity measure against human similarity judgments, using the simple edge-countingmethod as a baseline.In the second part of the article, Sections 4 and 5, I describe two applications of semanticsimilarity to problems of ambiguity in natural language. The rst concerns a particularcase of syntactic ambiguity that involves both coordination and nominal compounds, eachof which is a pernicious source of structural ambiguity in English. Consider the phrase foodhandling and storage procedures: does it represent a conjunction of food handling and storageprocedures, or does it refer to the handling and storage of food? The second applicationconcerns the resolution of word sense ambiguity | not for words in running text, which is alarge open problem (though cf. Wilks & Stevenson, 1996), but for groups of related wordsas are often discovered by distributional analysis of text corpora or found in dictionariesand thesauri. Finally, Section 6 discusses related work.2. Similarity and Information ContentLet C be the set of concepts in an is-a taxonomy, permitting multiple inheritance. Intuitively,one key to the similarity of two concepts is the extent to which they share information, in-dicated in an is-a taxonomy by a highly speci c concept that subsumes them both. Theedge-counting method captures this indirectly, since if the minimal path of is-a links be-tween two nodes is long, that means it is necessary to go high in the taxonomy, to moreabstract concepts, in order to nd a least upper bound. For example, in WordNet, nickeland dime are both subsumed by coin, whereas the most speci c superclass that nickeland credit card share is medium of exchange (see Figure 1). In a feature-based setting(e.g., Tversky, 1977), this would be re ected by explicit shared features: nickels and dimes96"}, {"heading": "Information-Based Semantic Similarity", "text": "COIN CASH MONEY\nNICKEL DIME\nMEDIUM OF EXCHANGE\nCREDIT CARD CREDITFigure 1: Fragment of the WordNet taxonomy. Solid lines represent is-a links; dashed linesindicate that some intervening nodes were omitted to save space.are both small, round, metallic, and so on. These features are captured implicitly by thetaxonomy in categorizing nickel and dime as subordinates of coin.By associating probabilities with concepts in the taxonomy, it is possible to capturethe same idea as edge-counting, but avoiding the unreliability of edge distances. Let thetaxonomy be augmented with a function p : C ! [0; 1], such that for any c 2 C, p(c) is theprobability of encountering an instance of concept c. This implies that p is monotonicallynondecreasing as one moves up the taxonomy: if c1 is-a c2, then p(c1) p(c2). Moreover,if the taxonomy has a unique top node then its probability is 1.Following the standard argumentation of information theory (Ross, 1976), the infor-mation content of a concept c can be quanti ed as negative the log likelihood, log p(c).Notice that quantifying information content in this way makes intuitive sense in this set-ting: as probability increases, informativeness decreases; so the more abstract a concept,the lower its information content. Moreover, if there is a unique top concept, its informationcontent is 0.This quantitative characterization of information provides a new way to measure seman-tic similarity. The more information two concepts share, the more similar they are, and theinformation shared by two concepts is indicated by the information content of the conceptsthat subsume them in the taxonomy. Formally, de nesim(c1; c2) = maxc 2 S(c1; c2) [ log p(c)] ; (1)where S(c1; c2) is the set of concepts that subsume both c1 and c2. A class that achieves themaximum value in Equation 1 will be termed a most informative subsumer; most often thereis a unique most informative subsumer, although this need not be true in the general case.Taking the maximum with respect to information content is analogous to taking the rstintersection in semantic network marker-passing or the shortest path with respect to edgedistance (cf. Quillian, 1968; Rada et al., 1989); a generalization from taking the maximumto taking a weighted average is introduced in Section 3.4.Notice that although similarity is computed by considering all upper bounds for the twoconcepts, the information measure has the e ect of identifying minimal upper bounds, sinceno class is less informative than its superordinates. For example, in Figure 1, coin, cash,etc. are all members of S(nickel;dime), but the concept that is structurally the minimal97"}, {"heading": "Resnik", "text": "NURSE2 p=.0001 info=13.17\nDOCTOR1 p=.0018 info=9.093\nHEALTH_PROFESSIONAL p=.0022 info=8.844\nNURSE1 p=.0001 info=12.94\nPROFESSIONAL p=.0079 info=6.993\nLAWYER p=.0007 info=10.39\nADULT p=.0208 info=5.584\nPERSON p=.2491 info=2.005 FEMALE_PERSON p=.0188 info=5.736 GUARDIAN p=.0058 info=7.434 p=.0027 info=8.522 ACTOR1 INTELLECTUAL p=.0113 info=6.471\nDOCTOR2 p=.0005 info=10.84\nFigure 2: Another fragment of the WordNet taxonomyupper bound, coin, will also be the most informative. This can make a di erence in casesof multiple inheritance: two distinct ancestor nodes may both be minimal upper bounds,as measured using distance in the graph, but those two nodes might have very di erentvalues for information content. Also notice that in is-a taxonomies such as WordNet,where there are multiple sub-taxonomies but no unique top node, asserting zero similarityfor concepts in separate sub-taxonomies (e.g., liberty, aorta) is equivalent to unifyingthe sub-taxonomies by creating a virtual topmost concept.In practice, one often needs to measure word similarity , rather than concept similarity.Using s(w) to represent the set of concepts in the taxonomy that are senses of word w,de ne wsim(w1; w2) = maxc1; c2 [sim(c1; c2)] ; (2)where c1 ranges over s(w1) and c2 ranges over s(w2). This is consistent with Rada et al.'s(1989) treatment of \\disjunctive concepts\" using edge-counting: they de ne the distancebetween two disjunctive sets of concepts as the minimum path length from any element ofthe rst set to any element of the second. Here, the word similarity is judged by takingthe maximal information content over all concepts of which both words could be an in-stance. To take an example, consider how the word similarity wsim(doctor, nurse) wouldbe computed, using the taxonomic information in Figure 2. (Note that only noun sensesare considered here.) By Equation 2, we must consider all pairs of concepts hc1; c2i, wherec1 2 fdoctor1;doctor2g and c2 2 fnurse1;nurse2g, and for each such pair we mustcompute the semantic similarity sim(c1,c2) according to Equation 1. Table 1 illustrates thecomputation. 98\nInformation-Based Semantic Similarityc1 (description) c2 (description) subsumer sim(c1,c2)doctor1 (medical) nurse1 (medical) health professional 8.844doctor1 (medical) nurse2 (nanny) person 2.005doctor2 (Ph.D.) nurse1 (medical) person 2.005doctor2 (Ph.D.) nurse2 (nanny) person 2.005Table 1: Computation of similarity for doctor and nurseAs the table shows, when all the senses for doctor are considered against all the sensesfor nurse, the maximum value is 8.844, via health professional as a most informativesubsumer; this is, therefore, the value of word similarity for doctor and nurse.13. EvaluationThis section describes a simple, direct method for evaluating semantic similarity, usinghuman judgments as the basis for comparison.3.1 ImplementationThe work reported here used WordNet's taxonomy of concepts represented by nouns (andcompound nominals) in English.2 Frequencies of concepts in the taxonomy were estimatedusing noun frequencies from the Brown Corpus of American English (Francis & Ku cera,1982), a large (1,000,000 word) collection of text across genres ranging from news articlesto science ction. Each noun that occurred in the corpus was counted as an occurrence ofeach taxonomic class containing it.3 For example, in Figure 1, an occurrence of the noundime would be counted toward the frequency of dime, coin, cash, and so forth. Formally,freq(c) = Xn2words(c) count(n); (3)where words(c) is the set of words subsumed by concept c. Concept probabilities werecomputed simply as relative frequency:p\u0302(c) = freq(c)N ; (4)where N was the total number of nouns observed (excluding those not subsumed by anyWordNet class, of course). Naturally the frequency estimates in Equation 3 would be1. The taxonomy in Figure 2 is a fragment of WordNet version 1.6, showing real quantitative informationcomputed using the method described below. The \\nanny\" sense of nurse (nursemaid, a woman who isthe custodian of children) is primarily a British usage. The example omits two other senses of doctor inWordNet: a theologian in the Roman Catholic Church, and a game played by children. WordNet doesnot use node labels like doctor1, but I have created such labels here for the sake of readability.2. Concept as used here refers to what Miller et al. (1990) call a synset, essentially a node in the taxonomy.The experiment reported in this section used the noun taxonomy from WordNet version 1.4, which hasapproximately 50,000 nodes.3. Plural nouns counted as instances of their singular forms.99\nResnikimproved by taking into account the intended sense of each noun in the corpus | forexample, an instance of crane can be a bird or a machine, but not both. Sense-taggedcorpora are generally not available, however, and so the frequency estimates are done usingthis weaker but more generally applicable technique.It should be noted that the present method of associating probabilities with concepts ina taxonomy is not based on the notion of a single random variable ranging over all concepts| were that the case, the \\credit\" for each noun occurrence would be distributed over allconcepts for the noun, and the counts normalized across the entire taxonomy to sum to 1.(That is the approach taken in Resnik, 1993a, also see Resnik, 1998b for discussion.) Inassigning taxonomic probabilities for purposes of measuring semantic similarity, the presentmodel associates a separate, binomially distributed random variable with each concept.4That is, from the perspective of any given concept c, an observed noun either is or isnot an instance of that concept, with probabilities p(c) and 1 p(c), respectively. Unlike amodel in which there is a single multinomial variable ranging over the entire set of concepts,this formulation assigns probability 1 to the top concept of the taxonomy, leading to thedesirable consequence that its information content is zero.3.2 TaskAlthough there is no standard way to evaluate computational measures of semantic similar-ity, one reasonable way to judge would seem to be agreement with human similarity ratings.This can be assessed by using a computational similarity measure to rate the similarity ofa set of word pairs, and looking at how well its ratings correlate with human ratings of thesame pairs.An experiment by Miller and Charles (1991) provided appropriate human subject datafor the task. In their study, 38 undergraduate subjects were given 30 pairs of nouns thatwere chosen to cover high, intermediate, and low levels of similarity (as determined usinga previous study, Rubenstein & Goodenough, 1965), and those subjects were asked torate \\similarity of meaning\" for each pair on a scale from 0 (no similarity) to 4 (perfectsynonymy). The average rating for each pair thus represents a good estimate of how similarthe two words are, according to human judgments.5In order to get a baseline for comparison, I replicated Miller and Charles's experiment,giving ten subjects the same 30 noun pairs. The subjects were all computer science graduatestudents or postdoctoral researchers at the University of Pennsylvania, and the instructionswere exactly the same as used by Miller and Charles, the main di erence being that inthis replication the subjects completed the questionnaire by electronic mail (though theywere instructed to complete the whole task in a single uninterrupted sitting). Five subjectsreceived the list of word pairs in a random order, and the other ve received the list in thereverse order. The correlation between the Miller and Charles mean ratings and the meanratings in my replication was .96, quite close to the .97 correlation that Miller and Charlesobtained between their results and the ratings determined by the earlier study.4. This is similar in spirit to the way probabilities are used in a Bayesian network.5. An anonymous reviewer points out that human judgments on this task may be in uenced by prototypi-cality, e.g., the pair bird/robin would likely yield higher ratings than bird/crane. Issues of this kind arebrie y touched on in Section 6, but for the most part they are ignored here since prototypicality, liketopical relatedness, is not captured in most is-a taxonomies.100\nInformation-Based Semantic SimilarityFor each subject in my replication, I computed how well his or her ratings correlatedwith the Miller and Charles ratings. The average correlation over the 10 subjects wasr = 0:88, with a standard deviation of 0.08.6 This value represents an upper bound onwhat one should expect from a computational attempt to perform the same task.For purposes of evaluation, three computational similarity measures were used. The rst is the similarity measurement using information content proposed in the previous sec-tion. The second is a variant on the edge-counting method, converting it from distance tosimilarity by subtracting the path length from the maximum possible path length:wsimedge(w1; w2) = (2 max) minc1; c2len(c1; c2) (5)where c1 ranges over s(w1), c2 ranges over s(w2), max is the maximum depth of the tax-onomy, and len(c1; c2) is the length of the shortest path from c1 to c2 . (Recall that s(w)denotes the set of concepts in the taxonomy that represent senses of word w.) If all sensesof w1 and w2 are in separate sub-taxonomies of WordNet their similarity is taken to bezero. Note that because correlation is used as the evaluation metric, the conversion from adistance to a similarity can be viewed as an expository convenience, and does not a ect theresults: although the sign of the correlation coe cient changes from positive to negative,its magnitude turns out to be just the same regardless of whether or not the minimum pathlength is subtracted from (2 max).The third point of comparison is a measure that simply uses the probability of a concept,rather than the information content, to de ne semantic similarity of conceptssimp(c)(c1; c2) = maxc 2 S(c1; c2) [1 p(c)] (6)and the corresponding measure of word similarity:wsimp(c)(w1; w2) = maxc1; c2 hsimp(c)(c1; c2)i ; (7)where c1 ranges over s(w1) and c2 ranges over s(w2) in Equation 7. The probability-basedsimilarity score is included in order to assess the extent to which similarity judgments mightbe sensitive to frequency per se rather than information content. Again, the di erencebetween maximizing 1 p(c) and minimizing p(c) turns out not to a ect the magnitude ofthe correlation. It simply ensures that the value can be interpreted as a similarity value,with high values indicating similar words.3.3 ResultsTable 2 summarizes the experimental results, giving the correlation between the similarityratings and the mean ratings reported by Miller and Charles. Note that, owing to a nounmissing from the WordNet 1.4 taxonomy, it was only possible to obtain computationalsimilarity ratings for 28 of the 30 noun pairs; hence the proper point of comparison forhuman judgments is not the correlation over all 30 items (r = :88), but rather the correlationover the 28 included pairs (r = :90). The similarity ratings by item are given in Table 3.6. Inter-subject correlation in the replication, estimated using leaving-one-out resampling (Weiss & Ku-likowski, 1991), was r = :90; stdev = 0:07. 101\nResnikSimilarity method CorrelationHuman judgments (replication) r = :9015Information content r = :7911Probability r = :6671Edge-counting r = :6645Table 2: Summary of experimental results.Word Pair Miller and Charles Replication wsim wsimedge wsimp(c)means meanscar automobile 3.92 3.9 8.0411 30 0.9962gem jewel 3.84 3.5 14.9286 30 1.0000journey voyage 3.84 3.5 6.7537 29 0.9907boy lad 3.76 3.5 8.4240 29 0.9971coast shore 3.70 3.5 10.8076 29 0.9994asylum madhouse 3.61 3.6 15.6656 29 1.0000magician wizard 3.50 3.5 13.6656 30 0.9999midday noon 3.42 3.6 12.3925 30 0.9998furnace stove 3.11 2.6 1.7135 23 0.6951food fruit 3.08 2.1 5.0076 27 0.9689bird cock 3.05 2.2 9.3139 29 0.9984bird crane 2.97 2.1 9.3139 27 0.9984tool implement 2.95 3.4 6.0787 29 0.9852brother monk 2.82 2.4 2.9683 24 0.8722crane implement 1.68 0.3 2.9683 24 0.8722lad brother 1.66 1.2 2.9355 26 0.8693journey car 1.16 0.7 0.0000 0 0.0000monk oracle 1.10 0.8 2.9683 24 0.8722food rooster 0.89 1.1 1.0105 18 0.5036coast hill 0.87 0.7 6.2344 26 0.9867forest graveyard 0.84 0.6 0.0000 0 0.0000monk slave 0.55 0.7 2.9683 27 0.8722coast forest 0.42 0.6 0.0000 0 0.0000lad wizard 0.42 0.7 2.9683 26 0.8722chord smile 0.13 0.1 2.3544 20 0.8044glass magician 0.11 0.1 1.0105 22 0.5036noon string 0.08 0.0 0.0000 0 0.0000rooster voyage 0.08 0.0 0.0000 0 0.0000Table 3: Semantic similarity by item.102\nInformation-Based Semantic Similarityn1 n2 wsim(n1,n2) subsumertobacco alcohol 7.63 drugtobacco sugar 3.56 substancetobacco horse 8.26 narcoticTable 4: Similarity with tobacco computed by maximizing information content3.4 DiscussionThe experimental results in the previous section suggest that measuring semantic similarityusing information content provides results that are better than the traditional method ofsimply counting the number of intervening is-a links.The measure is not without its problems, however. Like simple edge-counting, themeasure sometimes produces spuriously high similarity measures for words on the basis ofinappropriate word senses. For example, Table 4 shows the word similarity for several wordswith tobacco. Tobacco and alcohol are similar, both being drugs, and tobacco and sugar areless similar, though not entirely dissimilar, since both can be classi ed as substances. Theproblem arises, however, in the similarity rating for tobacco with horse: the word horse canbe used as a slang term for heroin, and as a result information-based similarity is maximized,and path length minimized, when the two words are both categorized as narcotics. This iscontrary to intuition.Cases like this are probably relatively rare. However, the example illustrates a moregeneral concern: in measuring similarity between words, it is really the relationship amongword senses that matters, and a similarity measure should be able to take this into account.In the absence of a reliable algorithm for choosing the appropriate word senses, the moststraightforward way to do so in the information-based setting is to consider all conceptsto which both nouns belong rather than taking just the single maximally informative class.This suggests de ning a measure of weighted word similarity as follows:wsim (w1; w2) = Xi (ci)[ log p(ci)]; (8)where fcig is the set of concepts dominating both w1 and w2 in any sense of either word, and is a weighting function over concepts such that Pi (ci) = 1. This measure of similaritytakes more information into account than the previous one: rather than relying on thesingle concept with maximum information content, it allows each class representing sharedproperties to contribute information content according to the value of (ci). Intuitively,these values measure relevance. For example, in computing wsim (tobacco,horse), theci would range over all concepts of which tobacco and horse are both instances, includingnarcotic, drug, artifact, life form, etc. In an everyday context one might expect lowvalues for (narcotic) and (drug), but in the context of, say, a newspaper article aboutdrug dealers, the weights of these concepts might be quite high. Although it is not possibleto include weighted word similarity in the comparison of Section 3, since the noun pairs arejudged without context, Section 4 provides further discussion and a weighting function designed for a particular natural language processing task.103\nResnik4. Using Taxonomic Similarity in Resolving Syntactic AmbiguityHaving considered a direct evaluation of the information-based semantic similarity measure,I now turn to the application of the measure in resolving syntactic ambiguity.4.1 Clues for Resolving Coordination AmbiguitySyntactic ambiguity is a pervasive problem in natural language. As Church and Patil(1982) point out, the class of \\every way ambiguous\" syntactic constructions | those forwhich the number of analyses is the number of binary trees over the terminal elements |includes such frequent constructions as prepositional phrases, coordination, and nominalcompounds. In the last several years, researchers in natural language have made a greatdeal of progress in using quantitative information from text corpora to provide the neededconstraints. Progress on broad-coverage prepositional phrase attachment ambiguity hasbeen particularly notable, now that the dominant approach has shifted from structuralstrategies to quantitative analysis of lexical relationships (Whittemore, Ferrara, & Brunner,1990; Hindle & Rooth, 1993; Brill & Resnik, 1994; Ratnaparkhi & Roukos, 1994; Li & Abe,1995; Collins & Brooks, 1995; Merlo, Crocker, & Berthouzoz, 1997). Noun compounds havereceived comparatively less attention (Kobayasi, Takunaga, & Tanaka, 1994; Lauer, 1994,1995), as has the problem of coordination ambiguity (Agarwal & Boggess, 1992; Kurohashi& Nagao, 1992).In this section, I investigate the role of semantic similarity in resolving coordinationambiguities involving nominal compounds. I began with noun phrase coordinations of theform n1 and n2 n3, which admit two structural analyses, one in which n1 and n2 are thetwo noun phrase heads being conjoined (1a) and one in which the conjoined heads are n1and n3 (1b).(1) a. a (bank and warehouse) guardb. a (policeman) and (park guard)Identifying which two head nouns are conjoined is necessary in order to arrive at a correctinterpretation of the phrase's content. For example, analyzing (1b) according to the struc-ture of (1a) could lead a machine translation system to produce a noun phrase describingsomebody who guards both policemen and parks. Analyzing (1a) according to the struc-ture of (1b) could lead an information retrieval system to miss this phrase when looking forqueries involving the term bank guard.Kurohashi and Nagao (1992) point out that similarity of form and similarity of meaningare important cues to conjoinability. In English, similarity of form is to a great extentcaptured by agreement in number (singular vs. plural):(2) a. several business and university groupsb. several businesses and university groupsSimilarity of form between candidate conjoined heads can thus be thought of as a Booleanvariable: number agreement is either satis ed by the candidate heads or it is not.Similarity of meaning of the conjoined heads also appears to play an important role:(3) a. a television and radio personality104\nInformation-Based Semantic Similarityb. a psychologist and sex researcherClearly television and radio are more similar than television and personality; correspond-ingly for psychologist and researcher. This similarity of meaning is captured well by semanticsimilarity in a taxonomy, and thus a second variable to consider when evaluating a coordina-tion structure is semantic similarity as measured by overlap in information content betweenthe two head nouns.In addition, for the constructions considered here, the appropriateness of noun-nounmodi cation is relevant:(4) a. mail and securities fraudb. corn and peanut butterOne reason we prefer to conjoin mail and securities is thatmail fraud is a salient compoundnominal phrase. On the other hand, corn butter is not a familiar concept; compare to thechange in perceived structure if the phrase were corn and peanut crops. In order to measurethe appropriateness of noun-noun modi cation, I use a quantitative measure of selectional t called selectional association (Resnik, 1996), which takes into account both lexical co-occurrence frequencies and semantic class membership in the WordNet taxonomy. Brie y,the selectional association of a word w with a WordNet class c is given byA(w; c) = p(cjw) log p(cjw)p(c)D(p(Cjw) k p(C)) (9)where D(p1 k p2) is the Kullback-Leibler distance (relative entropy) between probabilitydistributions p1 and p2. Intuitively, A(w, c) is measuring the extent to which class c ispredicted by word w; for example, A(wool, clothing) would have a higher value than,say, A(wool, person). The selectional association A(w1; w2) of two words is de ned asthe maximum of A(w1; c) taken over all classes c to which w2 belongs. For example,A(wool, glove) would most likely be equal to A(wool, clothing), as compared to, say,A(wool, sports equipment) | the latter value corresponding to the sense of glove assomething used in baseball or in boxing. (See Li & Abe, 1995, for an approach in whichselectional relationships are modeled using conditional probability.) A simple way to treatselectional association as a variable in resolving coordination ambiguities is to prefer analy-ses that include noun-noun modi cations with very strong a nities (e.g., bank as a modi erof guard) and to disprefer very weak noun-noun relationships (e.g., corn as a modi er ofbutter). Thresholds de ning \\strong\" and \\weak\" are parameters of the algorithm, de nedbelow.4.2 Resolving Coordination Ambiguity: First ExperimentI investigated the roles of these sources of evidence by conducting a straightforward dis-ambiguation experiment using naturally occurring linguistic data. Two sets of 100 nounphrases of the form [NP n1 and n2 n3] were extracted from the parsed Wall Street Journal(WSJ) corpus, as found in the Penn Treebank (Marcus, Santorini, & Marcinkiewicz, 1993).These were disambiguated by hand, with one set used for development and the other for105\nResnikSource of evidence Conjoined ConditionNumber agreement n1 and n2 number(n1) = number(n2) and number(n1) 6= number(n3)n1 and n3 number(n1) = number(n3) and number(n1) 6= number(n2)undecided otherwiseSemantic similarity n1 and n2 wsim(n1,n2) > wsim(n1,n3)n1 and n3 wsim(n1,n3) > wsim(n1,n2)undecided otherwiseNoun-noun n1 and n2 A(n1,n3) > or A(n3,n1) > modi cation n1 and n3 A(n1,n3) < or A(n3,n1) < undecided otherwiseTable 5: Rules for number agreement, semantic similarity, and noun-noun modi cation inresolving syntactic ambiguity of noun phrases n1 and n2 n3testing.7 A set of simple transformations were applied to all WSJ data, including the map-ping of all proper names to the token someone, the expansion of month abbreviations, andthe reduction of all nouns to their root forms.Number agreement was determined using a simple analysis of su xes in combinationwith WordNet's lists of root nouns and irregular plurals.8 Semantic similarity was deter-mined using the information-based measure of Equation (2) | the noun class probabilitiesof Equation (1) were estimated using a sample of approximately 800,000 noun occurrencesin Associated Press newswire stories.9 For the purpose of determining semantic similarity,nouns not in WordNet were treated as instances of the class hthingi. Appropriateness ofnoun-noun modi cation was determined by computing selectional association (Equation 9),using co-occurrence frequencies taken from a sample of approximately 15,000 noun-nouncompounds extracted from the WSJ corpus. (This sample did not include the test data.)Both selection of the modi er for the head and selection of the head for the modi er wereconsidered by the disambiguation algorithm. Table 5 provides details of the decision rulefor each source of evidence when used independently.10In addition, I investigated several methods for combining the three sources of informa-tion. These included: (a) a simple form of \\backing o \" (speci cally, given the numberagreement, noun-noun modi cation, and semantic similarity strategies in that order, usethe choice given by the rst strategy that isn't undecided); (b) taking a vote among thethree strategies and choosing the majority; (c) classifying using the results of a linear re-7. Hand disambiguation was necessary because the Penn Treebank does not encode NP-internal structure.These phrases were disambiguated using the full sentence in which they occurred, plus the previous andfollowing sentence, as context.8. The experiments in this section used WordNet version 1.2.9. I am grateful to Donald Hindle for making these data available.10. Thresholds = 2:0 and = 0:0 were xed manually based on experience with the development setbefore evaluating the test data. 106\nInformation-Based Semantic SimilarityStrategy Coverage (%) Accuracy (%)Default 100.0 66.0Number agreement 53.0 90.6Noun-noun modi cation 75.0 69.3Semantic similarity 66.0 71.2Backing o 95.0 81.1Voting 89.0 78.7Number agreement + default 100.0 82.0Noun-noun modi cation + default 100.0 65.0Semantic similarity + default 100.0 72.0Backing o + default 100.0 81.0Voting + default 100.0 76.0Regression 100.0 79.0ID3 Tree 100.0 80.0Table 6: Syntactic disambiguation for items of the form n1 and n2 n3gression; and (d) constructing a decision tree classi er. The latter two methods are forms ofsupervised learning; in this experiment the development set was used as the training data.11The results are shown in Table 6. The development set contained a bias in favor ofconjoining n1 and n2; therefore a \\default\" strategy, always choosing that bracketing, wasused as a baseline for comparison. The default was also used for resolving undecided casesin order to make comparisons of individual strategies at 100% coverage. For example,\\Number agreement + default\" shows the gures obtained when number agreement isused to make the choice and the default is selected if that choice is undecided.Not surprisingly, the individual strategies perform reasonably well on the instances theycan classify, but coverage is poor; the strategy based on similarity of form is the most highlyaccurate, but arrives at an answer only half the time. However, the heavy a priori bias makesup the di erence | to such an extent that even though the other forms of evidence havesome value, no combination beats the number-agreement-plus-default combination. On thepositive side, this shows that the ambiguity can be resolved reasonably well using a verysimple algorithm: viewed in terms of how many errors are made, number agreement makesit possible to cut the baseline 34% error rate nearly in half to 18% incorrect analyses (a44% reduction). On the negative side, the results fail to make a strong case that semanticsimilarity can add something useful.Before taking up this issue, let us assess the contributions of the individual strategies tothe results when evidence is combined, by further analyzing the behavior of the unsupervisedevidence combination strategies. When combining evidence by voting, a choice was madein 89 cases. The number agreement strategy agreed with the majority vote in 57 cases,of which 43 (75.4%) were correct; the noun-noun modi cation strategy agreed with themajority in 73 cases, of which 50 (68.5%) were correct; and the semantic similarity strategy11. What I am calling \\backing o \" is related in spirit to Katz's well known smoothing technique (Katz,1987), but the \\backing o \" strategy used here is not quantitative. I retain the double quotes in orderto highlight the distinction. 107\nResnikagreed with the majority in 58 cases, of which 43 (74.1%) were correct. In the \\backing o \"form of evidence combination, number agreement makes a choice for 53 cases and is correctfor 48 (90.6%); then, of those remaining undecided, noun-noun modi cation makes a choicefor 35 cases and is correct for 24 (68.6%); then, of those still undecided, semantic similaritymakes a choice for 7 cases of which 5 are correct (71.4%); and the remaining 5 cases areundecided.This analysis and the above-baseline performance of the semantic-similarity-plus-defaultstrategy show that semantic similarity does contain information about the correct answer:it agrees with the majority vote a substantial portion of the time, and it selects correctanswers more often than one would expect by default for the cases it receives through\\backing o .\" However, because the default is correct two thirds of the time, and becausethe number agreement strategy is correct nine out of ten times for the cases it can decide,the potential contribution of semantic similarity remains suggestive rather than conclusive.In a second experiment, therefore, I investigated a more di cult formulation of the problemin order to obtain a better assessment.4.3 Resolving Coordination Ambiguity: Second ExperimentIn the second experiment using the same data sources, I investigated a more complex setof coordinations, looking at noun phrases of the form n0 n1 and n2 n3. The syntacticanalyses of such phrases are characterized by the same top-level binary choice as the datain the previous experiment, either conjoining heads n1 and n2 as in (5) or conjoining n1and n3 as in (6).12(5) a. freshman ((business and marketing) major)b. (food (handling and storage)) proceduresc. ((mail fraud) and bribery) charges(6) a. Clorets (gum and (breath mints))b. (baby food) and (puppy chow)For this experiment, one set of 89 items was extracted from the Penn Treebank WSJ datafor development, and another set of 89 items was set aside for testing. The developmentset showed signi cantly less bias than the data in the previous experiment, with 53.9% ofitems conjoining n1 and n2.The disambiguation strategies in this experiment were a more re ned version of thoseused in the previous experiment, as illustrated in Table 7. Number agreement was used justas before. However, rather than employing semantic similarity and noun-noun modi cationas independent strategies | something not clearly warranted given the lackluster perfor-mance of the modi cation strategy | the two were combined in a measure of weightedsemantic similarity as de ned in Equation (8). Selectional association was used as the basisfor . In particular, 1;2(c) was the greater of A(n0,c) and A(n3,c), capturing the fact thatwhen n1 and n2 are conjoined, the combined phrase potentially stands in a head-modi errelationship with n0 and a modi er-head relationship with n3 . Correspondingly, 1;3(c)was the greater of A(n0,c) and A(n2,c), capturing the fact that the coordination of n112. The full 5-way classi cation problem for the structures in (5) and (6) was not investigated.108\nInformation-Based Semantic SimilaritySource of evidence Conjoined ConditionNumber agreement n1 and n2 number(n1) = number(n2) and number(n1) 6= number(n3)n1 and n3 number(n1) = number(n3) and number(n1) 6= number(n2)undecided otherwiseWeighted semantic n1 and n2 wsim 1;2(n1,n2) > wsim 1;3(n1,n3)similarity n1 and n3 wsim 1;3(n1,n3) > wsim 1;2(n1,n2)undecided otherwiseTable 7: Rules for number agreement and weighted semantic similarity in resolving syntac-tic ambiguity of noun phrases n0 n1 and n2 n3Strategy Coverage (%) Accuracy (%)Default 100.0 44.9Number agreement 40.4 80.6Weighted semantic similarity 69.7 77.4Backing o 85.4 81.6Table 8: Syntactic disambiguation for items of the form n0 n1 and n2 n3and n3 takes place in the context of n2 modifying n3 and of n1 (or a coordinated phrasecontaining it) being modi ed by n0.For example, consider an instance of the ambiguous phrase:(7) telecommunications products and services units.It so happens that a high-information-content connection exists between product in itssense as \\a quantity obtained by multiplication\" and unit in its sense as \\a single undividedwhole.\" As a result, although neither of these senses is relevant for this example, nouns n1and n3 would be assigned a high value for (unweighted) semantic similarity and be chosenincorrectly as the conjoined heads for this example. However, the unweighted similarity com-putation misses an important piece of context: in any syntactic analysis conjoining productand unit (cf. examples 6a and 6b), the word telecommunications is necessarily a modi erof the concept identi ed by products. But the selectional association between telecommu-nications and products in its \\multiplication\" sense is weak or nonexistent. Weighting byselection association, therefore, provides a way to reduce the impact of the spurious senseson the similarity computation.In order to combine sources of evidence, I used \\backing o \" (from number agreementto weighted semantic similarity) to combine the two individual strategies. As a baseline,results were evaluated against a simple default strategy of always choosing the group thatwas more common in the development set. The results are shown in Table 8.In this case, the default strategy de ned using the development set was misleading,yielding worse than chance accuracy. For this reason, strategy-plus-default gures are notreported. However, even if default choices were made using the bias found in the test set,109\nResnikaccuracy would be only 55.1%. In contrast to the equivocal results in the rst experiment,this experiment demonstrates a clear contribution of semantic similarity: by employingsemantic similarity in those cases where the more accurate number-agreement strategycannot apply, it is possible to obtain equivalent or even somewhat better accuracy thannumber agreement alone while at the same time more than doubling the coverage.Comparison with previous algorithms is unfortunately not possible, since researchers oncoordination ambiguity have not established a common data set for evaluation or even acommon characterization of the problem, in contrast to the now-standard (v, n1, prep, n2)contexts used in work on propositional phrase attachment. With that crucial caveat, itis nonetheless interesting to note that the results obtained here are broadly consistentwith Kurohashi and Nagao (1992), who report accuracy results in the range of 80-83% at100% coverage when analyzing a broad range of conjunctive structures in Japanese usinga combination of string matching, syntactic similarity, and thesaurus-based similarity, andwith Agarwal and Boggess (1992), who use syntactic types and structure, along with partlydomain-dependent semantic labels, to obtain accuracies in a similar range for identifyingconjuncts in English.5. Using Taxonomic Similarity in Word Sense SelectionThis section considers the application of the semantic similarity measure in resolving anotherform of ambiguity: selecting the appropriate sense of a noun when it appears in the contextof other nouns that are related in meaning.5.1 Associating Word Senses with Noun GroupingsKnowledge about groups of related words plays a role in many natural language applications.As examples, query expansion using related words is a well studied technique in informationretrieval (e.g., Harman, 1992; Grefenstette, 1992), clusters of similar words can play arole in smoothing stochastic language models for speech recognition (Brown, Della Pietra,deSouza, Lai, & Mercer, 1992), classes of verbs that share semantic structure form thebasis for an approach to interlingual machine translation (Dorr, 1997), and clusterings ofrelated words can be used in characterizing subgroupings of retrieved documents in large-scale Web searches (e.g., Digital Equipment Corporation, 1998). There is a wide bodyof research on the use of distributional methods for measuring word similarity in order toobtain groups of related words (e.g., Bensch & Savitch, 1992; Brill, 1991; Brown et al., 1992;Grefenstette, 1992, 1994; McKeown & Hatzivassiloglou, 1993; Pereira, Tishby, & Lee, 1993;Sch utze, 1993), and thesauri such as WordNet are another source of word relationships (e.g.,Voorhees, 1994).Distributional techniques can sometimes do a good job of identifying groups of relatedwords (see Resnik, 1998b, for an overview and critical discussion), but for some tasks therelevant relationships are not among words, but among word senses. For example, Brownet al. (1992) illustrate the notion of a distributionally derived, \\semantically sticky\" clusterusing an automatically derived word group containing attorney, counsel, trial, court, andjudge. Although the semantic coherence of this cluster \\pops out\" for a human reader, anaive computational system has no defense against word sense ambiguity: using this cluster110\nInformation-Based Semantic Similarityfor query expansion could result in retrieving documents involving advice (one sense ofcounsel) and royalty (as one sense of court).13Resnik (1998a) introduces an algorithm that uses taxonomically-de ned semantic simi-larity in order to derive grouping relationships among word senses from grouping relation-ships among words. Formally, the problem can be stated as follows. Consider a set of wordsW = fw1; : : : ; wng, with each word wi having an associated set Si = fsi;1; : : : ; si;mg of pos-sible senses. Assume that there exists some set W 0 SSi, representing the set of wordsenses that an ideal human judge would conclude belong to the group of senses correspond-ing to the word grouping W . (It follows that W 0 must contain at least one representativefrom each Si.) The goal is then to de ne a membership function ' that takes si;j , wi, andW as its arguments and computes a value in [0; 1], representing the con dence with whichone can state that sense si;j belongs in sense grouping W 0. Note that, in principle, nothingprecludes the possibility that multiple senses of a word are included in W 0.For example, consider again the groupattorney, counsel, trial, court, judge.Restricting attention to noun senses in WordNet, every word but attorney is polysemous.Treating this word group as W , a good algorithm for computing ' should assign a valueof 1 to the unique sense of attorney, and it should assign a high value to the sense of counselas a lawyer who pleads cases in court.Similarly, it should assign high values to the senses of trial aslegal proceedings consisting of the judicial examination of issues by a competent tri-bunalthe determination of a person's innocence or guilt by due process of law.It should also assign high values to the senses of court asan assembly to conduct judicial businessa room in which a law court sits.And it should assign a high value to the sense of judge asa public o cial authorized to decide questions brought before a court of justice.It should assign low values of ' to the various word senses of words in this cluster that areassociated with the group to a lesser extent or not at all. These would include the sense ofcounsel asdirection or advice as to a decision or course of action;similarly, a low value of ' should be assigned to other senses of court such as13. See Krovetz and Kroft, 1992 and Voorhees, 1993 for experimentation and discussion of the e ects ofword sense ambiguity in information retrieval. 111\nResnikAlgorithm (Resnik, 1998a). Given W = fw1; : : : ; wng, a set of nouns:for i and j = 1 to n, with i < jf vi;j = wsim(wi, wj)ci;j = the most informative subsumer for wi and wjfor k = 1 to num senses(wi)if ci;j is an ancestor of sensei;kincrement support[i, k] by vi;jfor k0 = 1 to num senses(wj)if ci;j is an ancestor of sensej;k0increment support[j, k'] by vi;jincrement normalization[i] by vi;jincrement normalization[j] by vi;jgfor i = 1 to nfor k = 1 to num senses(wi)f if (normalization[i] > 0.0)'i;k = support[i, k] / normalization[i]else'i;k = 1 / num senses(wi)g Figure 3: Disambiguation algorithm for noun groupingsa yard wholly or partly surrounded by walls or buildings.The disambiguation algorithm for noun groups is given in Figure 3. Intuitively, when twopolysemous words are similar, their most informative subsumer provides information aboutwhich sense of each word is the relevant one. This observation is similar in spirit to otherapproaches to word sense disambiguation based on maximizing relatedness of meaning (e.g.,Lesk, 1986; Sussna, 1993). The key idea behind the algorithm is to consider the nouns in aword group pairwise. For each pair the algorithm goes through all possible combinations ofthe words' senses, and assigns \\credit\" to senses on the basis of shared information content,as measured using the information content of the most informative subsumer.14As an example, WordNet lists doctor as meaning either a medical doctor or someoneholding a Ph.D., and lists nurse as meaning either a health professional or a nanny, butwhen the two words are considered together, the medical sense of each word is obvious tothe human reader. This e ect nds its parallel in the operation of the algorithm. Given ataxonomy like that of Figure 2, consider a case in which the set W of words contains w1 =doctor, w2 = nurse, and w3 = actor. In the rst pairwise comparison, for doctor and nurse,14. In Figure 3, the square bracket notation highlights the fact that support is a matrix and normalizationis an array. Conceptually v and c are (triangular) matrices also; however, I use subscripts rather thansquare brackets because at implementation time there is no need to implement them as such since thevalues vi;j and ci;j are used and discarded on each pass through the double loop.112\nInformation-Based Semantic Similaritythe most informative subsumer is c1;2 = health professional, which has informationcontent v1;2 = 8.844. Therefore the support for doctor1 and nurse1 is incrementedby 8.844. Neither doctor2 nor nurse2 receives any increment in support based on thiscomparison, since neither has health professional as an ancestor. In the second pairwisecomparison, the most informative subsumer for doctor and actor is c1;3 = person, withinformation content v1;3 = 2.005, and so there is an increment by that amount to thesupport for doctor1, doctor2, and actor1, all of which have person as an ancestor.Similarly, in the third pairwise comparison, the most informative subsumer for nurse andactor is also person, so nurse1, nurse2, and actor1 all have their support incrementedby 2.005. In the end, therefore, doctor1 has received support 8:884 + 2:005 out of apossible 8:884 + 2:005 for all the pairwise comparisons in which it participated, so for thatword sense ' = 1. In contrast, doctor2 received support in the amount of 2.005 out of apossible 8:884 + 2:005 for the comparisons in which it was involved, so the value of ' fordoctor2 is 2:0058:884+2:005 = 0:185.Resnik (1998a) illustrates the algorithm of Figure 3 using word groupings from a varietyof sources, including several of the sources on distributional clustering cited above, andevaluates the algorithm more rigorously on the task of associating WordNet senses withnouns in Roget's thesaurus, based on their thesaurus category membership. On average, thealgorithm achieved approximately 89% of the performance of human annotators performingthe same task.15 In the remainder of this section I describe a new application of thealgorithm, and evaluate its performance.5.2 Linking to WordNet using a Bilingual DictionaryMultilingual resources for natural language processing can be di cult to obtain, althoughsome promising e orts are underway in projects like EuroWordNet (Vossen, 1998). Formany languages, however, such large-scale resources are unlikely to be available in thenear future, and individual research e orts will have to continue to build from scratch orto adapt existing resources such as bilingual dictionaries (e.g., Klavans & Tzoukermann,1995). In this section I describe an application of the algorithm of Figure 3 to the Englishde nitions in the CETA Chinese-English dictionary (CETA, 1982). The ultimate task,being undertaken in the context of a Chinese-English machine translation project, will beto associate Chinese vocabulary items with nodes in WordNet, much in the same way thatvocabulary in Spanish, Dutch, and Italian are associated with interlingual taxonomy nodesderived from the American WordNet, in the EuroWordNet project; the task is also similarto attempts to relate dictionaries and thesauri monolingually (e.g., see Section 5.3 andJi, Gong, & Huang, 1998). The present study investigates the extent to which semanticsimilarity might be useful in partially automating the process.15. The task was performed independently by two human judges. Treating Judge 1 as the benchmark theaccuracies achieved by Judge 2, the algorithm, and random selection were respectively 65.7%, 58.6%,and 34.8%; treating Judge 2 as the benchmark the accuracies achieved by Judge 1, the algorithm, andrandom selection were respectively 68.6%, 60.5%, and 33.3%. As the relatively low accuracies for humanjudges demonstrate, disambiguation using WordNet's ne-grained senses is quite a bit more di cult thandisambiguation to the level of homographs (Hearst, 1991; Cowie, Guthrie, & Guthrie, 1992). Resnik andYarowsky (1997, 1999) discuss the implications of WordNet's ne-grainedness for evaluation of wordsense disambiguation, and consider alternative evaluation methods.113\nResnikFor example, consider the following dictionary entries:(a) : 1. hliti brother-in-law (husband's elder brother) 2. hregi father 3.hregi uncle (father's elder brother) 4. uncle (form of address to an olderman)(b) : actress, player of female roles.In order to associate Chinese terms such as these with the WordNet noun taxonomy, itis important to avoid associations with inappropriate senses | for example, the word inentry (a), , should clearly not be associated with father in its WordNet senses as ChurchFather, priest, God-the-Father, or founding father.16Although one traditional approach to using dictionary entries has been to compute wordoverlap with respect to dictionary de nitions (e.g., Lesk, 1986), the English glosses in theCETA dictionary are generally too short to take advantage of word overlap in this fashion.However, many of the de nitions do have a useful property: they possess multiple sub-de nitions that are similar in meaning, as in the cases illustrated above. Although onecannot always assume that this is so, e.g.,(c) : 1. case (i.e., upper case or lower case) 2. dial (of a watch, etc.),inspection of the dictionary con rms that when multiple de nitions are present they tendmore toward polysemy than homonymy.Based on this observation, I conducted an experiment to assess the extent to which theword sense disambiguation algorithm of Figure 3 can be used to identify relevant noun sensesin WordNet for Chinese words in the CETA dictionary, using the English de nitions as thesource of similar nouns to disambiguate. Nouns heading de nitional noun phrases wereextracted automatically via simple heuristic methods, for a randomly-selected sample of100 dictionary entries containing multiple de nitions to be used as a test set. For example,the noun groups associated with the de nitions above would be(a') uncle, brother-in-law, father(b') actress, player.WordNet's noun database was used to automatically identify compound nominals wherepossible. So, for example, a word de ned as \\record player\" would have the compoundrecord player rather than player as its head noun because record player is a compoundnoun known to WordNet.17It should be noted that no attempt was made to exclude dictionary entries like (c) whencreating the test set. Since in general there is no way to automatically identify alternativede nitions distinguished by synonymy from those distinguished by homonymy, such entriesmust be faced by any disambiguation algorithm for this task.Two independent judges were recruited for assistance in annotating the test set, one anative Chinese speaker, and the second a Chinese language expert for the United Statesgovernment. These judges independently annotated the 100 test items. For each item,16. Annotations within the dictionary entries such as <lit> (literary), <reg> (regional), and the like areignored by the algorithm described in this section.17. WordNet version 1.5 was used for this experiment.114\nInformation-Based Semantic SimilarityFor each WordNet de nition, you will see 6 boxes: 1, 2, 3, 4, 5, and is-a. For each de nition: if you think the Chinese word can have that meaning, select the number corresponding to yourcon dence in that choice, where 1 is lowest con dence and 5 is highest con dence. If the Chinese word cannot have that meaning, but can have a more speci c meaning, select is-a.For example, if the Chinese word means \\truck\" and the WordNet de nition is \\automotive vehicle:self-propelled wheeled vehicle\", you would select this option. (That is, it makes sense to say thatthis Chinese word describes a concept that IS A KIND OF automotive vehicle.) Then pick 1,2, 3, 4, or 5 as your con dence in this decision, again with 1 as lowest con dence and 5 as highestcon dence. If neither of the above cases apply for this WordNet de nition, don't check o anything for thisde nition.Figure 4: Instructions for human judges selecting senses associated with Chinese wordsthe judge was given the Chinese word, its full CETA dictionary de nition (as in examplesa{c), and a list of all the WordNet sense descriptions associated with any sense of any headnoun in the associated noun group. For example, the list corresponding to the followingdictionary de nition(d) : urgent message, urgent dispatchwould contain the following WordNet sense descriptions, as generated via the head nounsmessage and dispatch: message, content, subject matter, substance: what a communication thatis about something is about dispatch, expedition, expeditiousness, fastness: subconcept of celerity, quick-ness, rapidity dispatch, despatch, communique: an o cial report (usually sent in haste) message: a communication (usually brief) that is written or spoken orsignaled; \\he sent a three-word message\" dispatch, despatch, shipment: the act of sending o something dispatch, despatch: the murder or execution of someoneFor each item, the judge was rst asked whether he knew that Chinese word in that meaning;if the response was negative, he was instructed to proceed to the next item. For items withknown words, the judges were instructed as in Figure 4.Although the use of the is-a selection was not used in the analysis of the results, itwas important to include it because it provided the judges with a way to indicate where aChinese word could best be classi ed in the WordNet noun taxonomy, without having toassert translational equivalence between the Chinese concept and a close WordNet (English)concept. So, for example, a judge could classify the word (the spring festival, lunarnew year, Chinese new year) as belonging under the WordNet sense glossed asfestival: a day or period of time set aside for feasting and celebration,115\nResnikthe most sensible choice given that \\Chinese New Year\" does not appear as a WordNetconcept. Annotating the is-a relationship for the set was also important because the al-gorithm being evaluated was working on groups of head nouns, thereby potentially losinginformation pointing to a more speci c concept reading. For example, the de nition(e) : steel tube, steel pipewould be given to the algorithm as a group containing head nouns tube and pipe.Once the test set was annotated, evaluation was done according to two paradigms:selection and ltering. In both paradigms we assume that for each entry in the test set, anannotator has correctly speci ed which WordNet senses are to be considered correct, andwhich are incorrect. An algorithm being tested against this set must identify, for each listedsense, whether that sense should be included for that item or whether it should be excluded.For example, the WordNet sense corresponding to \\the murder or execution of someone\"would be identi ed by an annotator as incorrect for (d), and so an algorithm marking it as\\included\" should be penalized.For the selection paradigm, the goal is to identify WordNet senses to include. We cantherefore de ne precision in that paradigm asPselection = number of correctly included sensesnumber of included senses (10)and recall as Rselection = number of correctly included sensesnumber of correct senses : (11)These correspond directly to the use of precision and recall in information retrieval. Preci-sion begins with the set of senses included by some method, and computes the proportion ofthese that are correct. Recall begins with the set of senses that should have been included,and computes the proportion of these that the method actually managed to choose.Since the number of potential WordNet senses for an item can be quite large, an equallyvalid alternative to the selection paradigm is what I will call the ltering paradigm, accordingto which the goal is to identify WordNet senses to exclude. One can easily imagine thisbeing the more relevant paradigm | for example, in a semi-automated setting where onewishes to reduce the burden of a user selecting among alternatives. In the ltering paradigmone can de ne ltering precision asP ltering = number of correctly excluded sensesnumber of excluded senses (12)and ltering recall asR ltering = number of correctly excluded sensesnumber of senses labeled incorrect : (13)In the ltering paradigm, precision begins with the set of senses that the method lteredout and computes the proportion that were correctly ltered out. And recall in lteringbegins with the set of senses that should have been excluded (i.e. the incorrect ones) andcomputes the proportion of these that the method actually managed to exclude.116\nInformation-Based Semantic SimilaritySense Selection Sense FilteringPrecision (%) Recall (%) Precision (%) Recall (%)Random 29.5 31.2 88.0 87.1Algorithm 36.9 69.9 93.8 79.3Judge 2 54.8 55.6 91.9 91.7Table 9: Evaluation using Judge 1 as the reference standard, considering items selectedwith con dence 3 and above.Judge 2 Algorithm RandomInclude Exclude Include Exclude Include ExcludeJudge 1 Include 40 32 58 25 26 57Exclude 33 363 99 380 61 418Table 10: Agreement and disagreement with Judge 1Table 9 shows the precision/recall gures using the judgments of Judge 1, the nativeChinese speaker, as a reference standard, considering only known items selected with con- dence 3 and above.18 The algorithm recorded all 100 items as known, and its con dencevalues were scaled linearly from continuous values in range [0,1] to discrete values from 1to 5. The table shows the algorithm's results with its choice thresholded at con dence 3,and Figure 5 shows how recall and precision vary as the con dence threshold changes. Asa lower bound for comparison, an algorithm was implemented that considered each wordsense for each item, selecting that sense probabilistically (with complete con dence) in sucha way as to make the average number of senses per item as close as possible to the averagenumber of senses per item in the reference standard (1.3 senses). Figures for the randombaseline are the average over 10 runs. Table 10 illustrates the choices underlying those gures; for example, there were 26 senses that the random procedure chose to include thatwere also included by Judge 1.The fact that Judge 2 has such low precision and recall for selection indicates thatmatching the choices of an independent judge is indeed a di cult task. This is unsurpris-ing, given previous experience with the problem of selecting among WordNet's ne-grainedsenses (Resnik, 1998a; Resnik & Yarowsky, 1997). The results clearly show that the algo-rithm is better than the baseline, but also indicate that it is overgenerating senses, whichhurts selection precision. In terms of ltering, when the algorithm chooses to lter out asense it tends to do so reliably ( ltering precision). However, its propensity toward overgen-eration is re ected in its below-baseline performance on ltering recall; that is, the algorithmis choosing to allow in senses that it should be ltering out.18. Judge 1, the native speaker of Chinese, identi ed 65 of the words as known to him; Judge 2 identi ed69. This on-line dictionary was constructed from a large variety of lexical resources, and includes a greatmany uncommon words, archaic usages, regionalisms, and the like.117\nResnik 0.2 0.4 0.6 0.8 1\n0.2 0.4 0.6 0.8 1\nPr ec\nis io\nn\nRecall\nFiltering: Algorithm Human Random Selection: Algorithm\nHuman Random\nFigure 5: Precision/recall curves using Judge 1 as the reference standard, varying the con- dence thresholdThis pattern of results suggests that the best use of this algorithm at its present levelof performance would be as a lter for a lexical acquisition process with a human in theloop, dividing candidate WordNet senses for dictionary entries according to higher andlower priority. For Chinese-English dictionary entries that serve as appropriate input to thealgorithm (of which there are approximately 37000 in the CETA dictionary), if a WordNetsense is not selected by the algorithm with a con dence at least equal to 3 it should bedemoted to the lower priority group in the presentation of alternatives, since the algorithm'schoice to exclude a sense is correct approximately 93% of the time. Those senses that areselected by the algorithm are not necessarily to be included | the human judge is stillneeded to make the selection, since selection precision is low | but the algorithm tends toerr on the side of caution, and so correct senses will be found in the higher priority groupsome 70% of the time.5.3 Linking to WordNet from an English Dictionary/ThesaurusThe results on WordNet sense selection using a bilingual dictionary demonstrate that thealgorithm of Figure 3 does a good job of assigning low scores to WordNet senses that shouldbe ltered out, even if it should probably not be trusted to make categorical decisions. Oneapplication proposed as suitable, therefore, was helping to identify which senses should be ltered out within a semi-automated process of lexical acquisition. Here I describe a closelyrelated, real-world application for which the algorithm has been deployed: adding pointersinto WordNet from an on-line dictionary/thesaurus on the Web.The context of this application is the Wordsmyth English Dictionary-Thesaurus (WEDT,http://www.wordsmyth.net/), an on-line educational dictionary a liated with the ARTFLtext database project (http://humanities.uchicago.edu/ARTFL/; Morrissey, 1993). Ithas been designed to be useful in educational contexts, and, as part of that design, itintegrates a thesaurus within the structure of the dictionary. As illustrated in Figure 6,118\nInformation-Based Semantic SimilaritybarSYL: bar1PRO: barPOS: nounDEF: 1. a length of solid material, usu. rectangular or cylindrical:EXA: a bar of soap;EXA: a candy bar;EXA: an iron bar.SYN: rod (1), stick1 (1,2,3)SIM: pole1 , shaft, stake1, ingot, block, rail1 , railing,crowbar, jimmy, leverDEF: 2. anything that acts as a restraint or hindrance.SYN: block (10), hindrance (1), obstruction (1), impediment (1),obstacle, barrier (1,3), stop (5)SIM: barricade, blockade, deterrent, hurdle, curb, stumblingblock, snag, jam1, shoal1 , reef1, sandbar...Figure 6: Example from the Wordsmyth English Dictionary-Thesaurus (WEDT)WEDT contains traditional dictionary information, such as part of speech, pronunciation,and de nitional information, but in many cases also includes pointers to synonyms (SYN)or similar words (SIM). Within the on-line dictionary, these thesaurus items are hyperlinks| for example, stake1 is a link to the rst WEDT entry for stake | and parentheticalnumbers refer to speci c de nitions within an entry.The thesaurus-like grouping of similar words provides an opportunity to exploit thealgorithm for disambiguating noun groupings by automatically linking WEDT entries toWordNet. The value in linking these two resources comes from their compatability, in thatboth have properties of both a thesaurus and a dictionary, as well as from their complemen-tarity: beyond being an alternative source of de nitional information and lists of synonyms,WordNet provides ordering of word senses by frequency, estimates of word familiarity, part-of relationships, and of course the overall taxonomic organization illustrated in Figures 1and 2. Figure 7 shows how taxonomic information is presented using the WordNet Webserver (http://www.cogsci.princeton.edu/cgi-bin/webwn/).In a collaboration with WEDT and ARTFL, I have taken the noun entries from theWEDT dictionary and, for each grouping of similar words, added a set of experimentalhyperlinks to WordNet entries on the WordNet Web server. Figure 8 shows how the exper-imental WordNet links (XWN) look to the WEDT user. Links to WordNet senses, such aspole1, appear together with the con dence level assigned by the sense disambiguation al-gorithm; senses with con dence less than a threshold are not presented.19 When an XWNhyperlink is selected by the user, WordNet taxonomic information for the selected senseappears in a parallel browser window, as in Figure 7.From this window, the user has an entry point into the other capabilities of the WordNetweb server. For example, one might choose to look at all the WordNet senses for pole as19. The current threshold, 0.1, was chosen manually. It may be sub-optimal but I have found that it workswell in practice. 119\nResnikSense 1pole(a long (usually round) rod of wood or metal or plastic)=> rod(a long thin implement made of metal or wood)=> implement(a piece of equipment or tool used to effect an end)=> instrumentality, instrumentation(an artifact (or system of artifacts) that isinstrumental in accomplishing some end)=> artifact, artefact(a man-made object)=> object, physical object(a physical (tangible and visible) entity; ``it wasfull of rackets, balls and other objects'')=> entity, something(anything having existence (living or nonliving))Figure 7: WordNet entry (hypernyms) for pole1barSYL: bar1PRO: barPOS: nounDEF: 1. a length of solid material, usu. rectangular or cylindrical:EXA: a bar of soap;EXA: a candy bar;EXA: an iron bar.SYN: rod (1), stick1 (1,2,3)SIM: pole1 , shaft, stake1, ingot, block, rail1 , railing,crowbar, jimmy, leverXWN: pole1(0.82) ingot1(1.00) block1(0.16) rail1(0.39)railing1 (1.00) crowbar1(1.00)jimmy1(1.00) lever1(0.67) lever2(0.23) lever3(0.15)DEF: 2. anything that acts as a restraint or hindrance.SYN: block (10), hindrance (1), obstruction (1), impediment (1),obstacle, barrier (1,3), stop (5)SIM: barricade, blockade, deterrent, hurdle, curb, stumblingblock, snag, jam1, shoal1 , reef1, sandbarXWN: barricade1(1.00) barricade2(1.00) blockade1(0.25)blockade2(0.75) deterrent1(1.00) hurdle1(0.50)hurdle2(0.43) curb1(0.56) curb2(0.56)curb3(0.29) curb4(0.44) stumbling block1(1.00)snag1(1.00) jam1(0.27) shoal1(0.23) shoal2(0.91)reef1(1.00) sandbar1(1.00)...Figure 8: Example from WEDT with experimental WordNet links120\nInformation-Based Semantic Similarity1. pole { (a long (usually round) rod of wood or metal or plastic)2. Pole { (a native or inhabitant of Poland)3. pole { (one of two divergent or mutually exclusive opinions; \\they are at opposite poles\" or \\they arepoles apart\")4. perch, rod, pole { ((British) a linear measure of 16.5 feet)5. perch, rod, pole { (a square rod of land)6. pole, celestial pole { (one of two points of intersection of the Earth's axis and the celestial sphere)7. pole { (one of two antipodal points where the Earth's axis of rotation intersects the Earth's surface)8. terminal, pole { (a point on an electrical device (such as a battery) at which electric current entersor leaves)9. pole { (a long berglass implement used for pole vaulting)10. pole, magnetic pole { (one of the two ends of a magnet where the magnetism seems to be concentrated)Figure 9: List of WordNet senses for polea noun, displayed as in Figure 9. Notice that if a user of WEDT had simply gone directlyto the WordNet server to look up pole, the full list of 10 senses would have appearedwith no indication of which are most potentially related to the WEDT dictionary entryunder consideration. In contrast, the WEDT hyperlinks, introduced via the sense selectionalgorithm, lter out the majority of the irrelevant senses and provide the user a measure ofcon dence in selecting among those that remain.Although no formal evaluation of the WEDT/WordNet connection has been attempted,the results of the bilingual dictionary experiment suggest that this application of wordsense disambiguation | ltering out the least relevant senses, and then leaving the userin the loop | is a task for which the sense disambiguation algorithm is well suited. Thisis supported by user feedback on the XWN feature of WEDT, which has been favorable(Robert Parks, personal communication). The site has been growing in popularity, with acurrent estimate of 1000-1500 hits per day.6. Related WorkThere is an extensive literature on measuring similarity in general, and on word similarityin particular; for a classic paper see Tversky (1977). Recent work in information retrievaland computational linguistics has emphasized a distributional approach, in which wordsare represented as vectors in a space of features and similarity measures are de ned interms of those vectors; see Resnik (1998b) for discussion, and Lee (1997) for a good recentexample. Common to the traditional and the distributional approaches is the idea that wordor concept representations include explicit features, whether those features are speci ed ina knowledge-based fashion (e.g., dog might have features like mammal, loyal) or de nedin terms of distributional context (e.g., dog might have features like \\observed within 5words of howl). This representational assumption contrasts with the assumptions embodiedin a taxonomic representation, where most often the is-a relation stands between non-decomposed concepts. The two are not inconsistent, of course, since concepts in a taxonomy121\nResniksometimes can be decomposed into explicit features, and the is-a relation, as it is usuallyinterpreted, implies inheritance of features whether they are explicit or implicit. In thatrespect, the traditional approach of counting edges can be viewed as a particularly simpleapproximation to a similarity measure based on counting feature di erences, under theassumption that an edge exists to indicate a di erence of at least one feature.Information-theoretic concepts and techniques have, in recent years, emerged from thespeech recognition community to nd wide application in natural language processing; e.g.,see Church and Mercer (1993). The information of an event is a fundamental notion instochastic language modeling for speech recognition, where the contribution of a correctword prediction based on its conditional probability, p(wordjcontext), is measured as theinformation conveyed by that prediction, log p(wordjcontext). This forms the basis forstandard measures of language model performance, such as cross entropy. Frequency ofshared and unshared features has also long been a factor in computing similarity over vec-tor representations. The inverse document frequency (idf) for term weighting in informationretrieval makes use of logarithmic scaling, and serves to identify terms that do not discrim-inate well among di erent documents, a concept very similar in spirit to the idea that suchterms have low information content (Salton, 1989).Although the counting of edges in is-a taxonomies seems to be something many peoplehave tried, there seem to be few published descriptions of attempts to directly evaluatethe e ectiveness of this method. A number of researchers have attempted to make use ofconceptual distance in information retrieval. For example, Rada et al. (1989, 1989) and Leeet al. (1993) report experiments using conceptual distance, implemented using the edge-counting metric, as the basis for ranking documents by their similarity to a query. Sussna(1993) uses semantic relatedness measured with WordNet in word sense disambiguation,de ning a measure of distance that weights di erent types of links and also explicitly takesdepth in the taxonomy into account.Following the original proposal to measure semantic similarity in a taxonomy usinginformation content (Resnik, 1993b, 1993a), a number of related proposals have been ex-plored. Leacock and Chodorow (1994) de ne a measure resembling information content,but using the normalized path length between the two concepts being compared rather thanthe probability of a subsuming concept. Speci cally, they de newsimndist(w1; w2) = log24 minc1; c2len(c1; c2)(2 max) 35 : (14)(The notation above is the same as for Equation (5).) In addition to this de nition, theyalso include several special cases, most notably to avoid in nite similarity when c1 andc2 are exact synonyms and thus have a path length of 0. Leacock and Chodorow haveexperimented with this measure and the information content measure described here in thecontext of word sense disambiguation, and found that they yield roughly similar results.Implementing their method and testing it on the task reported in Section 3, I found thatit actually outperformed the information-based measure slightly on that data set; however,in a follow-up experiment using a di erent and larger set of noun pairs (100 items), theinformation-based measure performed signi cantly better (Table 11).Analyzing the di erences between the two studies is illuminating. In the follow-up ex-periment, I used netnews archives to gather highly frequent nouns within related topic areas122\nInformation-Based Semantic SimilaritySimilarity method CorrelationInformation content r = :6894Leacock and Chodorow r = :4320Edge-counting r = :4101Table 11: Summary of experimental results in follow-up study.(to ensure that similar noun pairs occurred) and then selected noun pairings at random (inorder to avoid biasing the follow-up study in favor of either algorithm). There is, therefore,a predominance of low-similarity noun pairs in the test data. Looking at the distributionof ratings for the noun pairs, as given by the two measures, it is evident that the Leacockand Chodorow measure is overestimating semantic similarity for many of the predominantlynon-similar pairs. This stands to reason since the measure is identical whenever the edgedistance is identical, regardless of whether the pair is high or low in the taxonomy (e.g., thedistance between plant and animal is the same as the distance between white oak and redoak). In contrast, the information-based measure is sensitive to the di erence, and betterat avoiding spuriously high similarity values for non-similar pairs. On a related note, theedge-counting measure used in the follow-up study was a variant that computes path lengththrough a virtual top node, rather than asserting zero similarity between words with no pathconnecting them in the existing WordNet taxonomy, as was done previously. Using the dataset in the follow-up study, the information-based measure, at r = :6894, does signi cantlybetter than either of the edge-counting variants (r = :4101 and r = :2777); but going backto the original Miller and Charles data, the virtual-top-node variant does signi cantly betterthan the assert-zero edge distance measure, with its correlation of r = :7786 approachingthat of the measure based on information content. This comparison between the follow-upstudy and the original Miller and Charles data illustrates quite clearly how the utility of asimilarity measure can depend upon the distribution of items given by the task.Lin (1997, 1998) has recently proposed an alternative information-theoretic similaritymeasure, derived from a set of basic assumptions about similarity in a style reminiscent ofthe way in which entropy/information itself has a formal de nition derivable from a set ofbasic properties (Khinchin, 1957). Formally, Lin de nes similarity in a taxonomy as:simLin(c1; c2) = 2 log p(Ti Ci)log p(c1) + log p(c2) (15)where the Ci are the \\maximally speci c superclasses\" of both c1 and c2. Although thepossibility of multiple inheritance makes the intersection TiCi necessary in principle, mul-tiple inheritance is in fact so rare in WordNet that in practice one computes Equation (15)separately for each common ancestor Ci, using p(Ci) in the numerator, and then takesthe maximum (Dekang Lin, p.c.). Other than the multiplicative constant of 2, therefore,Lin's method for determining similarity in a taxonomy is essentially the information-basedsimilarity measure of Equation 1, but normalized by the combined information content ofthe two concepts assuming their independence. Put another way, Lin's measure is taking123\nResnikSimilarity method CorrelationInformation content r = :7947simWu&Palmer r = :8027simLin r = :8339Table 12: Summary of Lin's results comparing alternative similarity measuresinto account not only commonalities but di erences between the items being compared,expressing both in information-theoretic terms.Lin's measure is theoretically well motivated and elegantly derived. Moreover, Lin pointsout that his measure will by de nition yield the same value for simLin(x; x) regardless ofthe identity of x | unlike information content, which has been criticized on the groundsthat the value of self-similarity depends on how speci c a concept x is, and that two non-identical items x and y can be rated more similar to each other than a third item z is to itself(Richardson et al., 1994). From a cognitive perspective, however, similarity comparisonsinvolving self-similarity (\\Robins are similar to robins\"), as well as subclass relationships(\\Robins are similar to birds\"), have themselves been criticized by psychologists as anoma-lous (Medin, Goldstone, & Gentner, 1993). Moreover, experimental evidence with humanjudgments suggests that not all identical objects are judged equally similar, consistent withthe information-content measure proposed here but contrary to Lin's measure. For exam-ple, objects that are identical and complex, such as twins, can seem more similar to eachother than objects that are identical and simple, such as two instances of a simple geo-metric shape (Goldstone, 1999; Tversky, 1977). It would appear, therefore, that insofar as delity to human judgments is relevant, further experimentation is needed to evaluate thecompeting predictions of alternative similarity measures.Wu and Palmer (1994) propose a similarity measure that is based on edge distances, butrelated to Lin's measure in the way it takes into account the most speci c node dominatingc1 and c2, characterizing their commonalities, while normalizing in a way that accounts fortheir di erences. Revising Wu and Palmer's notation slightly, their measure is:simWu&Palmer(c1; c2) = 2 d(c3)d(c1) + d(c2) (16)where c3 is the maximally speci c superclass of c1 and c2, d(c3) is its depth, i.e. distancefrom the root of the taxonomy, and d(c1) and d(c2) are the depths of c1 and c2 on the paththrough c3.Lin (1998) repeats the experiment of Section 3 for the information content measure,simLin, and simWu&Palmer, reporting the results that appear in Table 12. Lin uses a sense-tagged corpus to estimate frequencies, and smoothed probabilities rather than simple rel-ative frequency. His results show a somewhat higher correlation for simLin than the othermeasures. Further experimentation is needed in order to assess the alternative measures,particularly with respect to their competing predictions and the variability of performanceacross data sets. What seems clear, however, is that all these measures perform better thanthe traditional edge-counting measure. 124\nInformation-Based Semantic Similarity7. ConclusionsThis article has presented a measure of semantic similarity in an is-a taxonomy, based onthe notion of information content. Experimental evaluation was performed using a large,independently constructed corpus, an independently constructed taxonomy, and previouslyexisting and new human subject data, and the results suggest that the measure performsencouragingly well and can be signi cantly better than the traditional edge-counting ap-proach. Semantic similarity, as measured using information content, was shown to be usefulin resolving cases of two pervasive kinds of linguistic ambiguity. In resolving coordinationambiguity, the measure was employed to capture the intuition that similarity of meaning isone indicator that two words are being conjoined; suggestive results of a rst experimentwere bolstered by unequivocal results in a second study, demonstrating signi cant improve-ments over a disambiguation strategy based only on syntactic agreement. In resolving wordsense ambiguity, the semantic similarity measure was used to assign con dence values toword senses of nouns within thesaurus-like groupings. A formal evaluation provided evi-dence that the technique can produce useful results but is better suited for semi-automatedsense ltering than categorical sense selection. Application of the technique to a dictio-nary/thesaurus on the World Wide Web provides a demonstration of the method in actionin a real-world setting.AcknowledgementsSections 1-3 of this article comprise a revised and extended version of Resnik (1995).Section 4 describes previously presented algorithms and data (Resnik, 1993b, 1993a), ex-tended by further discussion and analysis. Section 5 summarizes an algorithm describedin Resnik (1998a), and then extends previous results by presenting new applications of thealgorithm, with Section 5.2 containing a formal evaluation in a new setting and Section 5.3giving a real-world illustration where the approach has been put into practice. Section 6adds a substantial discussion of related work by other authors that has taken place sincethe information-based similarity measure was originally proposed.Parts of this research were done at the University of Pennsylvania with the partialsupport of an IBM Graduate Fellowship and grants ARO DAAL 03-89-C-0031, DARPAN00014-90-J-1863, NSF IRI 90-16592, and Ben Franklin 91S.3078C-1; parts of this researchwere also done at Sun Microsystems Laboratories in Chelmsford, Massachusetts; and partsof this work were supported at the University of Maryland by Department of Defensecontract MDA90496C1250, DARPA/ITO Contract N66001-97-C-8540, Army ResearchLaboratory contract DAAL03-91-C-0034 through Battelle, and a research grant from SunMicrosystems Laboratories. The author gratefully acknowledges the comments of threeanonymous JAIR reviewers and helpful discussions with John Kovarik, Claudia Leacock,Dekang Lin, Johanna Moore, Mari Broman Olsen, and Jin Tong, as well as comments andcriticism received during various presentations of this work.ReferencesAgarwal, R., & Boggess, L. (1992). A simple but useful approach to conjunct identi ca-125\nResniktion. In Proceedings of the 30th Annual Meeting of the Association for ComputationalLinguistics, pp. 15{21. Association for Computational Linguistics.Bensch, P. A., & Savitch, W. J. (1992). An occurrence-based model of word categorization.Presented at 3rd Meeting on Mathematics of Language (MOL3).Brill, E. (1991). Discovering the lexical features of a language. In Proceedings of the 29thAnnual Meeting of the Association for Computational Linguistics, Berkeley, CA.Brill, E., & Resnik, P. (1994). A rule-based approach to prepositional phrase attachment dis-ambiguation. In Proceedings of the 15th International Conference on ComputationalLinguistics (COLING-94).Brown, P. F., Della Pietra, V. J., deSouza, P. V., Lai, J. C., & Mercer, R. L. (1992).Class-based n-gram models of natural language. Computational Linguistics, 18 (4),467{480.CETA (1982). Chinese Dictionaries: an Extensive Bibliography of Dictionaries in Chineseand Other Languages. Chinese-English Translation Assistance Group, GreenwoodPublishing.Church, K. W., & Mercer, R. (1993). Introduction to the special issue on computationallinguistics using large corpora. Computational Linguistics, 19 (1), 1{24.Church, K. W., & Patil, R. (1982). Coping with syntactic ambiguity or how to put theblock in the box on the table. American Journal of Computational Linguistics, 8 (3-4),139{149.Collins, A., & Loftus, E. (1975). A spreading activation theory of semantic processing.Psychological Review, 82, 407{428.Collins, M., & Brooks, J. (1995). Prepositional phrase attachment through a backed-o model. In Third Workshop on Very Large Corpora. Association for ComputationalLinguistics. cmp-lg/9506021.Cowie, J., Guthrie, J., & Guthrie, L. (1992). Lexical disambiguation using simulated anneal-ing. In Proceedings of the 14th International Conference on Computational Linguistics(COLING-92), pp. 359{365 Nantes, France.Digital Equipment Corporation (1998). AltaVista web page: Re ne, or cow9?..http://altavista.digital.com/av/content/about_our_technology_cow9.htm.Dorr, B. J. (1997). Large-Scale Dictionary Construction for Foreign Language Tutoring andInterlingual Machine Translation. Machine Translation, 12 (4), 271{322.Fellbaum, C. (Ed.). (1998). WordNet: An Electronic Lexical Database. MIT Press.Francis, W. N., & Ku cera, H. (1982). Frequency Analysis of English Usage: Lexicon andGrammar. Houghton Mi in, Boston. 126\nInformation-Based Semantic SimilarityGoldstone, R. L. (1999). Similarity. In MIT Encyclopedia of the Cognitive Sciences. MITPress, Cambridge, MA.Grefenstette, G. (1992). Use of syntactic context to produce term association lists for textretrieval. In Proceedings of the Fifteenth Annual International ACM SIGIR Confer-ence on Research and Development in Information Retrieval, pp. 89{97.Grefenstette, G. (1994). Explorations in Automatic Thesaurus Discovery. Kluwer, Boston.Harman, D. (1992). Relevance feedback revisited. In Proceedings of the Fifteenth AnnualInternational ACM SIGIR Conference on Research and Development in InformationRetrieval, pp. 1{10.Hearst, M. (1991). Noun homograph disambiguation using local context in large corpora.In Proceedings of the 7th Annual Conference of the University of Waterloo Centre forthe New OED and Text Research Oxford.Hindle, D., & Rooth, M. (1993). Structural ambiguity and lexical relations. ComputationalLinguistics, 19 (1), 103{120.Ji, D., Gong, J., & Huang, C. (1998). Combining a Chinese thesaurus with a Chinesedictionary. In COLING-ACL '98, pp. 600{606. Universit e de Montreal.Katz, S. M. (1987). Estimation of probabilities from sparse data for the language modelcomponent of a speech recognizer. IEEE Transactions on Acoustics, Speech and SignalProcessing, ASSP-35 (3), 400{401.Khinchin, A. I. (1957). Mathematical Foundations of Information Theory. New York: DoverPublications. Translated by R. A. Silverman and M. D. Friedman.Klavans, J. L., & Tzoukermann, E. (1995). Dictionaries and Corpora: Combining Corpusand Machine-Readable Dictionary Data for Building Bilingual Lexicons. MachineTranslation, 10, 185{218.Kobayasi, Y., Takunaga, T., & Tanaka, H. (1994). Analysis of Japanese compound nounsusing collocational information. In Proceedings of the 15th International Conferenceon Computational Linguistics (COLING-94).Krovetz, R., & Croft, W. B. (1992). Lexical ambiguity and information retrieval. ACMTransactions on Information Systems, 10 (2), 115{141.Kurohashi, S., & Nagao, M. (1992). Dynamic programming method for analyzing conjunc-tive structures in Japanese. In Proceedings of the 14th International Conference onComputational Linguistics (COLING-92) Nantes, France.Lauer, M. (1994). Conceptual association for compound noun analysis. In Proceedings of the32nd Annual Meeting of the Association for Computational Linguistics Las Cruces,New Mexico. Student Session.Lauer, M. (1995). Designing Statistical Language Learners: Experiments on Noun Com-pounds. Ph.D. thesis, Macquarie University, Sydney, Australia.127\nResnikLeacock, C., & Chodorow, M. (1994). Filling in a sparse training space for word senseidenti cation. ms.Lee, J. H., Kim, M. H., & Lee, Y. J. (1993). Information retrieval based on conceptualdistance in IS-A hierarchies. Journal of Documentation, 49 (2), 188{207.Lee, L. (1997). Similarity-based approaches to natural language processing. Tech. rep.TR-11-97, Harvard University. Doctoral dissertation. cmp-lg/9708011.Lesk, M. (1986). Automatic sense disambiguation using machine readable dictionaries:how to tell a pine cone from an ice cream cone. In Proceedings of the 1986 SIGDOCConference, pp. 24{26.Li, H., & Abe, N. (1995). Generalizing case frames using a thesaurus and the MDL principle.In Proceedings of the International Conference on Recent Advances in NLP Velingrad,Bulgaria.Lin, D. (1997). Using syntactic dependency as local context to resolve word sense ambigu-ity. In Proceedings of the 35th Annual Meeting of the Association for ComputationalLinguistics and 8th Conference of the European Chapter of the Association for Com-putational Linguistics Madrid, Spain.Lin, D. (1998). An information-theoretic de nition of similarity. In Proceedings of theFifteenth International Conference on Machine Learning (ICML-98) Madison, Wis-consin.Marcus, M. P., Santorini, B., & Marcinkiewicz, M. (1993). Building a large annotatedcorpus of English: the Penn Treebank. Computational Linguistics, 19, 313{330.McKeown, K., & Hatzivassiloglou, V. (1993). Augmenting lexicons automatically: Cluster-ing semantically related adjectives. In Bates, M. (Ed.), ARPA Workshop on HumanLanguage Technology. Morgan Kaufmann.Medin, D., Goldstone, R., & Gentner, D. (1993). Respects for similarity. PsychologicalReview, 100 (2), 254{278.Merlo, P., Crocker, M., & Berthouzoz, C. (1997). Attaching multiple prepositional phrases:Generalized backed-o estimation. In Proceedings of the Second Conference on Em-pirical Methods in Natural Language Processing (EMNLP-2). cmp-lg/9710005.Miller, G. (1990). WordNet: An on-line lexical database. International Journal of Lexicog-raphy, 3 (4). (Special Issue).Miller, G. A., & Charles, W. G. (1991). Contextual correlates of semantic similarity. Lan-guage and Cognitive Processes, 6 (1), 1{28.Morrissey, R.(1993). Texts and contexts: The ARTFL database in French studies. Profession 93,27{33. http://humanities.uchicago.edu/homes/publications/romoart.html.128\nInformation-Based Semantic SimilarityPereira, F., Tishby, N., & Lee, L. (1993). Distributional clustering of English words. In Pro-ceedings of the 31st Annual Meeting of the Association for Computational Linguistics(ACL-93) Morristown, New Jersey. Association for Computational Linguistics.Quillian, M. R. (1968). Semantic memory. In Minsky, M. (Ed.), Semantic InformationProcessing. MIT Press, Cambridge, MA.Rada, R., & Bicknell, E. (1989). Ranking documents with a thesaurus. JASIS, 40 (5),304{310.Rada, R., Mili, H., Bicknell, E., & Blettner, M. (1989). Development and application ofa metric on semantic nets. IEEE Transaction on Systems, Man, and Cybernetics,19 (1), 17{30.Ratnaparkhi, A., & Roukos, S. (1994). A maximum entropy model for prepositional phraseattachment. In Proceddings of the ARPA Workshop on Human Language TechnologyPlainsboro, NJ.Resnik, P. (1993a). Selection and Information: A Class-Based Approach to Lexical Re-lationships. Ph.D. thesis, University of Pennsylvania.(ftp://ftp.cis.upenn.edu/pub/ircs/tr/93-42.ps.Z).Resnik, P. (1993b). Semantic classes and syntactic ambiguity. In Proceedings of the 1993ARPA Human Language Technology Workshop. Morgan Kaufmann.Resnik, P. (1995). Using information content to evaluate semantic similarity in a taxonomy.In Proceedings of the 14th International Joint Conference on Arti cial Intelligence(IJCAI-95). (cmp-lg/9511007).Resnik, P. (1996). Selectional constraints: An information-theoretic model and its compu-tational realization. Cognition, 61, 127{159.Resnik, P. (1998a). Disambiguating noun groupings with respect to Wordnet senses. InArmstrong, S., Church, K., Isabelle, P., Tzoukermann, E., & Yarowsky, D. (Eds.),Natural Language Processing Using Very Large Corpora. Kluwer.Resnik, P. (1998b). WordNet and class-based probabilities. In Fellbaum, C. (Ed.),WordNet:An Electronic Lexical Database. MIT Press.Resnik, P., & Yarowsky, D. (1997). A perspective on word sense disambiguation methodsand their evaluation. In ANLP Workshop on Tagging Text with Lexical SemanticsWashington, D.C.Resnik, P., & Yarowsky, D. (1999). Distinguishing systems and distinguishing senses: Newevaluation methods for word sense disambiguation. Natural Language Engineering.(to appear).Richardson, R., Smeaton, A. F., & Murphy, J. (1994). Using WordNet as a knowl-edge base for measuring semantic similarity between words. Working paper CA-1294, Dublin City University, School of Computer Applications, Dublin, Ireland.ftp://ftp.compapp.dcu.ie/pub/w-papers/1994/CA1294.ps.Z.129\nResnikRoss, S. (1976). A First Course in Probability. Macmillan.Rubenstein, H., & Goodenough, J. (1965). Contextual correlates of synonymy. CACM,8 (10), 627{633.Salton, G. (1989). Automatic Text Processing. Addison-Wesley.Sch utze, H. (1993). Word space. In Hanson, S. J., Cowan, J. D., & Giles, C. L. (Eds.), Ad-vances in Neural Information Processing Systems 5, pp. 895{902. Morgan KaufmannPublishers, San Mateo CA.Sinclair (ed.), J. (1987). Collins COBUILD English Language Dictionary. Collins: London.Sussna, M. (1993). Word sense disambiguation for free-text indexing using a massive seman-tic network. In Proceedings of the Second International Conference on Informationand Knowledge Management (CIKM-93) Arlington, Virginia.Tversky, A. (1977). Features of similarity. Psychological Review, 84, 327{352.Voorhees, E. M. (1993). Using WordNet to disambiguate word senses for text retrieval. InKorfhage, R., Rasmussen, E., & Willett, P. (Eds.), Proceedings of the Sixteenth AnnualInternational ACM SIGIR Conference on Research and Development in InformationRetrieval, pp. 171{180.Voorhees, E. M. (1994). Query expansion using lexical-semantic relations. In 17th Inter-national Conference on Research and Development in Information Retrieval (SIGIR'94) Dublin, Ireland.Vossen, P. (1998). Special issue on EuroWordNet. Computers and the Humanities, 32 (2/3).Weiss, S. M., & Kulikowski, C. A. (1991). Computer systems that learn: classi cation andprediction methods from statistics, neural nets, machine learning, and expert systems.Morgan Kaufmann, San Mateo, CA.Whittemore, G., Ferrara, K., & Brunner, H. (1990). Empirical study of predictive powers ofsimple attachment schemes for post-modi er prepositional phrases. In Proceedings ofthe 28th Annual Meeting of the Association for Computational Linguistics, pp. 23{30.Pittsburgh, Pennsylvania.Wilks, Y., & Stevenson, M. (1996). The grammar of sense: Is word-sense tagging muchmore than part-of-speech tagging?.. Technical Report CS-96-05, cmp-lg/9607028.Wu, Z., & Palmer, M. (1994). Verb Semantics and Lexical Selection. In Proceedings of the32nd Annual Meeting of the Association for Computational Linguistics Las Cruces,New Mexico. 130"}], "references": [{"title": "A simple but useful approach to conjunct", "author": ["R. Agarwal", "L. Boggess"], "venue": null, "citeRegEx": "Agarwal and Boggess,? \\Q1992\\E", "shortCiteRegEx": "Agarwal and Boggess", "year": 1992}, {"title": "An occurrence-based model of word categorization", "author": ["P.A. Bensch", "W.J. Savitch"], "venue": null, "citeRegEx": "Bensch and Savitch,? \\Q1992\\E", "shortCiteRegEx": "Bensch and Savitch", "year": 1992}, {"title": "Discovering the lexical features of a language", "author": ["E. Brill"], "venue": "Proceedings of the 29th", "citeRegEx": "Brill,? 1991", "shortCiteRegEx": "Brill", "year": 1991}, {"title": "A rule-based approach to prepositional phrase attachment", "author": ["E. Brill", "P. Resnik"], "venue": null, "citeRegEx": "Brill and Resnik,? \\Q1994\\E", "shortCiteRegEx": "Brill and Resnik", "year": 1994}, {"title": "Introduction to the special issue", "author": ["K.W. Church", "R. Mercer"], "venue": null, "citeRegEx": "Church and Mercer,? \\Q1993\\E", "shortCiteRegEx": "Church and Mercer", "year": 1993}, {"title": "Coping with syntactic ambiguity or how to put", "author": ["K.W. Church", "R. Patil"], "venue": null, "citeRegEx": "Church and Patil,? \\Q1982\\E", "shortCiteRegEx": "Church and Patil", "year": 1982}, {"title": "A spreading activation theory of semantic processing", "author": ["A. Collins", "E. Loftus"], "venue": null, "citeRegEx": "Collins and Loftus,? \\Q1975\\E", "shortCiteRegEx": "Collins and Loftus", "year": 1975}, {"title": "Prepositional phrase attachment through a backed-o", "author": ["M. Collins", "J. Brooks"], "venue": null, "citeRegEx": "Collins and Brooks,? \\Q1995\\E", "shortCiteRegEx": "Collins and Brooks", "year": 1995}, {"title": "Lexical disambiguation using simulated anneal", "author": ["J. Cowie", "J. Guthrie", "L. Guthrie"], "venue": null, "citeRegEx": "Cowie et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Cowie et al\\.", "year": 1992}, {"title": "Large-Scale Dictionary Construction for Foreign Language Tutoring", "author": ["B.J. Dorr"], "venue": null, "citeRegEx": "Dorr,? \\Q1997\\E", "shortCiteRegEx": "Dorr", "year": 1997}, {"title": "WordNet: An Electronic Lexical Database", "author": ["C. Fellbaum"], "venue": null, "citeRegEx": "Fellbaum,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum", "year": 1998}, {"title": "Frequency Analysis of English Usage: Lexicon", "author": ["W.N. Francis", "H. Ku cera"], "venue": null, "citeRegEx": "Francis and cera,? \\Q1982\\E", "shortCiteRegEx": "Francis and cera", "year": 1982}, {"title": "Similarity", "author": ["R.L. Goldstone"], "venue": "MIT Encyclopedia of the Cognitive Sciences. MIT", "citeRegEx": "Goldstone,? 1999", "shortCiteRegEx": "Goldstone", "year": 1999}, {"title": "Use of syntactic context to produce term association lists for text", "author": ["G. Grefenstette"], "venue": null, "citeRegEx": "Grefenstette,? \\Q1992\\E", "shortCiteRegEx": "Grefenstette", "year": 1992}, {"title": "Explorations in Automatic Thesaurus Discovery", "author": ["G. Grefenstette"], "venue": "Kluwer, Boston.", "citeRegEx": "Grefenstette,? 1994", "shortCiteRegEx": "Grefenstette", "year": 1994}, {"title": "Relevance feedback revisited", "author": ["D. Harman"], "venue": "Proceedings of the Fifteenth Annual", "citeRegEx": "Harman,? 1992", "shortCiteRegEx": "Harman", "year": 1992}, {"title": "Noun homograph disambiguation using local context in large corpora", "author": ["M. Hearst"], "venue": null, "citeRegEx": "Hearst,? \\Q1991\\E", "shortCiteRegEx": "Hearst", "year": 1991}, {"title": "Structural ambiguity and lexical relations", "author": ["D. Hindle", "M. Rooth"], "venue": null, "citeRegEx": "Hindle and Rooth,? \\Q1993\\E", "shortCiteRegEx": "Hindle and Rooth", "year": 1993}, {"title": "Combining a Chinese thesaurus with a Chinese", "author": ["D. Ji", "J. Gong", "C. Huang"], "venue": null, "citeRegEx": "Ji et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Ji et al\\.", "year": 1998}, {"title": "Estimation of probabilities from sparse data for the language model", "author": ["S.M. Katz"], "venue": null, "citeRegEx": "Katz,? \\Q1987\\E", "shortCiteRegEx": "Katz", "year": 1987}, {"title": "Mathematical Foundations of Information Theory", "author": ["A.I. Khinchin"], "venue": "New York: Dover", "citeRegEx": "Khinchin,? 1957", "shortCiteRegEx": "Khinchin", "year": 1957}, {"title": "Dictionaries and Corpora: Combining Corpus", "author": ["J.L. Klavans", "E. Tzoukermann"], "venue": null, "citeRegEx": "Klavans and Tzoukermann,? \\Q1995\\E", "shortCiteRegEx": "Klavans and Tzoukermann", "year": 1995}, {"title": "Analysis of Japanese compound nouns", "author": ["Y. Kobayasi", "T. Takunaga", "H. Tanaka"], "venue": null, "citeRegEx": "Kobayasi et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Kobayasi et al\\.", "year": 1994}, {"title": "Lexical ambiguity and information retrieval", "author": ["R. Krovetz", "W.B. Croft"], "venue": null, "citeRegEx": "Krovetz and Croft,? \\Q1992\\E", "shortCiteRegEx": "Krovetz and Croft", "year": 1992}, {"title": "Dynamic programming method for analyzing conjunc", "author": ["S. Kurohashi", "M. Nagao"], "venue": null, "citeRegEx": "Kurohashi and Nagao,? \\Q1992\\E", "shortCiteRegEx": "Kurohashi and Nagao", "year": 1992}, {"title": "Conceptual association for compound noun analysis", "author": ["M. Lauer"], "venue": "Proceedings of the", "citeRegEx": "Lauer,? 1994", "shortCiteRegEx": "Lauer", "year": 1994}, {"title": "Designing Statistical Language Learners: Experiments on", "author": ["M. Lauer"], "venue": null, "citeRegEx": "Lauer,? \\Q1995\\E", "shortCiteRegEx": "Lauer", "year": 1995}, {"title": "Filling in a sparse training space for word sense", "author": ["C. Leacock", "M. Chodorow"], "venue": null, "citeRegEx": "Leacock and Chodorow,? \\Q1994\\E", "shortCiteRegEx": "Leacock and Chodorow", "year": 1994}, {"title": "Similarity-based approaches to natural language processing", "author": ["L. Lee"], "venue": "Tech. rep.", "citeRegEx": "Lee,? 1997", "shortCiteRegEx": "Lee", "year": 1997}, {"title": "Automatic sense disambiguation using machine readable dictionaries", "author": ["M. Lesk"], "venue": null, "citeRegEx": "Lesk,? \\Q1986\\E", "shortCiteRegEx": "Lesk", "year": 1986}, {"title": "Generalizing case frames using a thesaurus and the MDL principle", "author": ["H. Li", "N. Abe"], "venue": null, "citeRegEx": "Li and Abe,? \\Q1995\\E", "shortCiteRegEx": "Li and Abe", "year": 1995}, {"title": "Using syntactic dependency as local context to resolve word sense ambigu", "author": ["D. Lin"], "venue": null, "citeRegEx": "Lin,? \\Q1997\\E", "shortCiteRegEx": "Lin", "year": 1997}, {"title": "An information-theoretic de nition of similarity", "author": ["D. Lin"], "venue": "Proceedings of the", "citeRegEx": "Lin,? 1998", "shortCiteRegEx": "Lin", "year": 1998}, {"title": "Building a large annotated", "author": ["M.P. Marcus", "B. Santorini", "M. Marcinkiewicz"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Augmenting lexicons automatically", "author": ["K. McKeown", "V. Hatzivassiloglou"], "venue": null, "citeRegEx": "McKeown and Hatzivassiloglou,? \\Q1993\\E", "shortCiteRegEx": "McKeown and Hatzivassiloglou", "year": 1993}, {"title": "Respects for similarity", "author": ["D. Medin", "R. Goldstone", "D. Gentner"], "venue": null, "citeRegEx": "Medin et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Medin et al\\.", "year": 1993}, {"title": "WordNet: An on-line lexical database", "author": ["G. Miller"], "venue": "International Journal of Lexicog-", "citeRegEx": "Miller,? 1990", "shortCiteRegEx": "Miller", "year": 1990}, {"title": "Contextual correlates of semantic similarity", "author": ["G.A. Miller", "W.G. Charles"], "venue": null, "citeRegEx": "Miller and Charles,? \\Q1991\\E", "shortCiteRegEx": "Miller and Charles", "year": 1991}, {"title": "Distributional clustering of English words", "author": ["F. Pereira", "N. Tishby", "L. Lee"], "venue": null, "citeRegEx": "Pereira et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pereira et al\\.", "year": 1993}, {"title": "Semantic memory", "author": ["M.R. Quillian"], "venue": "Minsky, M. (Ed.), Semantic Information", "citeRegEx": "Quillian,? 1968", "shortCiteRegEx": "Quillian", "year": 1968}, {"title": "Ranking documents with a thesaurus. JASIS", "author": ["R. Rada", "E. Bicknell"], "venue": null, "citeRegEx": "Rada and Bicknell,? \\Q1989\\E", "shortCiteRegEx": "Rada and Bicknell", "year": 1989}, {"title": "Development and application", "author": ["R. Rada", "H. Mili", "E. Bicknell", "M. Blettner"], "venue": null, "citeRegEx": "Rada et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Rada et al\\.", "year": 1989}, {"title": "A maximum entropy model for prepositional phrase", "author": ["A. Ratnaparkhi", "S. Roukos"], "venue": null, "citeRegEx": "Ratnaparkhi and Roukos,? \\Q1994\\E", "shortCiteRegEx": "Ratnaparkhi and Roukos", "year": 1994}, {"title": "Selection and Information: A Class-Based Approach to", "author": ["P. Resnik"], "venue": null, "citeRegEx": "Resnik,? \\Q1993\\E", "shortCiteRegEx": "Resnik", "year": 1993}, {"title": "Semantic classes and syntactic ambiguity", "author": ["P. Resnik"], "venue": "Proceedings of the 1993", "citeRegEx": "Resnik,? 1993b", "shortCiteRegEx": "Resnik", "year": 1993}, {"title": "Using information content to evaluate semantic similarity in a taxonomy", "author": ["P. Resnik"], "venue": null, "citeRegEx": "Resnik,? \\Q1995\\E", "shortCiteRegEx": "Resnik", "year": 1995}, {"title": "Selectional constraints: An information-theoretic model and its compu", "author": ["P. Resnik"], "venue": null, "citeRegEx": "Resnik,? \\Q1996\\E", "shortCiteRegEx": "Resnik", "year": 1996}, {"title": "Disambiguating noun groupings with respect to Wordnet senses", "author": ["P. Resnik"], "venue": "In", "citeRegEx": "Resnik,? 1998a", "shortCiteRegEx": "Resnik", "year": 1998}, {"title": "WordNet and class-based probabilities", "author": ["P. Resnik"], "venue": "Fellbaum, C. (Ed.),WordNet:", "citeRegEx": "Resnik,? 1998b", "shortCiteRegEx": "Resnik", "year": 1998}, {"title": "A perspective on word sense disambiguation methods", "author": ["P. Resnik", "D. Yarowsky"], "venue": null, "citeRegEx": "Resnik and Yarowsky,? \\Q1997\\E", "shortCiteRegEx": "Resnik and Yarowsky", "year": 1997}, {"title": "Distinguishing systems and distinguishing senses", "author": ["P. Resnik", "D. Yarowsky"], "venue": null, "citeRegEx": "Resnik and Yarowsky,? \\Q1999\\E", "shortCiteRegEx": "Resnik and Yarowsky", "year": 1999}, {"title": "A First Course in Probability", "author": ["S. Ross"], "venue": "Macmillan.", "citeRegEx": "Ross,? 1976", "shortCiteRegEx": "Ross", "year": 1976}, {"title": "Contextual correlates of synonymy", "author": ["H. Rubenstein", "J. Goodenough"], "venue": null, "citeRegEx": "Rubenstein and Goodenough,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein and Goodenough", "year": 1965}, {"title": "Automatic Text Processing", "author": ["G. Salton"], "venue": "Addison-Wesley.", "citeRegEx": "Salton,? 1989", "shortCiteRegEx": "Salton", "year": 1989}, {"title": "Word sense disambiguation for free-text indexing using a massive", "author": ["M. Sussna"], "venue": null, "citeRegEx": "Sussna,? \\Q1993\\E", "shortCiteRegEx": "Sussna", "year": 1993}, {"title": "Features of similarity", "author": ["A. Tversky"], "venue": "Psychological Review, 84, 327{352.", "citeRegEx": "Tversky,? 1977", "shortCiteRegEx": "Tversky", "year": 1977}, {"title": "Using WordNet to disambiguate word senses for text retrieval", "author": ["E.M. Voorhees"], "venue": "In", "citeRegEx": "Voorhees,? 1993", "shortCiteRegEx": "Voorhees", "year": 1993}, {"title": "Query expansion using lexical-semantic relations", "author": ["E.M. Voorhees"], "venue": "17th Inter-", "citeRegEx": "Voorhees,? 1994", "shortCiteRegEx": "Voorhees", "year": 1994}, {"title": "Special issue on EuroWordNet", "author": ["P. Vossen"], "venue": "Computers and the Humanities, 32 (2/3).", "citeRegEx": "Vossen,? 1998", "shortCiteRegEx": "Vossen", "year": 1998}, {"title": "Empirical study of predictive powers", "author": ["G. Whittemore", "K. Ferrara", "H. Brunner"], "venue": null, "citeRegEx": "Whittemore et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Whittemore et al\\.", "year": 1990}, {"title": "The grammar of sense: Is word-sense tagging", "author": ["Y. Wilks", "M. Stevenson"], "venue": null, "citeRegEx": "Wilks and Stevenson,? \\Q1996\\E", "shortCiteRegEx": "Wilks and Stevenson", "year": 1996}, {"title": "Verb Semantics and Lexical Selection", "author": ["Z. Wu", "M. Palmer"], "venue": null, "citeRegEx": "Wu and Palmer,? \\Q1994\\E", "shortCiteRegEx": "Wu and Palmer", "year": 1994}], "referenceMentions": [{"referenceID": 54, "context": "(Rada, Mili, Bicknell, & Blettner, 1989) suggest that the assessment of similarity in semantic networks can in fact be thought of as involving just taxonomic (is-a) links, to the exclusion of other link types; that view will also be taken here, although admittedly links such as part-of can also be viewed as attributes that contribute to similarity (cf. Richardson, Smeaton, & Murphy, 1994; Sussna, 1993).", "startOffset": 350, "endOffset": 405}, {"referenceID": 30, "context": "edu Department of Linguistics and Institute for Advanced Computer Studies University of Maryland College Park, MD 20742 USA Abstract This article presents a measure of semantic similarity in an is-a taxonomy based on the notion of shared information content. Experimental evaluation against a benchmark set of human similarity judgments demonstrates that the measure performs better than the traditional edge-counting approach. The article presents algorithms that take advantage of taxonomic similarity in resolving syntactic and semantic ambiguity, along with experimental results demonstrating their e ectiveness. 1. Introduction Evaluating semantic relatedness using network representations is a problem with a long history in arti cial intelligence and psychology, dating back to the spreading activation approach of Quillian (1968) and Collins and Loftus (1975).", "startOffset": 18, "endOffset": 838}, {"referenceID": 6, "context": "Introduction Evaluating semantic relatedness using network representations is a problem with a long history in arti cial intelligence and psychology, dating back to the spreading activation approach of Quillian (1968) and Collins and Loftus (1975). Semantic similarity represents a special case of semantic relatedness: for example, cars and gasoline would seem to be more closely related than, say, cars and bicycles, but the latter pair are certainly more similar.", "startOffset": 222, "endOffset": 248}, {"referenceID": 41, "context": "Given multiple paths, one takes the length of the shortest one (Lee, Kim, & Lee, 1993; Rada & Bicknell, 1989; Rada et al., 1989).", "startOffset": 63, "endOffset": 128}, {"referenceID": 36, "context": "For example, in WordNet (Miller, 1990; Fellbaum, 1998), a widely used, broad-coverage semantic network for English, it is not at all di cult to nd links that cover an intuitively narrow distance (rabbit ears is-a television antenna) or an intuitively wide one (phytoplankton is-a living thing).", "startOffset": 24, "endOffset": 54}, {"referenceID": 10, "context": "For example, in WordNet (Miller, 1990; Fellbaum, 1998), a widely used, broad-coverage semantic network for English, it is not at all di cult to nd links that cover an intuitively narrow distance (rabbit ears is-a television antenna) or an intuitively wide one (phytoplankton is-a living thing).", "startOffset": 24, "endOffset": 54}, {"referenceID": 51, "context": "Following the standard argumentation of information theory (Ross, 1976), the information content of a concept c can be quanti ed as negative the log likelihood, log p(c).", "startOffset": 59, "endOffset": 71}, {"referenceID": 41, "context": "Taking the maximum with respect to information content is analogous to taking the rst intersection in semantic network marker-passing or the shortest path with respect to edge distance (cf. Quillian, 1968; Rada et al., 1989); a generalization from taking the maximum to taking a weighted average is introduced in Section 3.", "startOffset": 185, "endOffset": 224}, {"referenceID": 41, "context": "This is consistent with Rada et al.'s (1989) treatment of \\disjunctive concepts\" using edge-counting: they de ne the distance between two disjunctive sets of concepts as the minimum path length from any element of the rst set to any element of the second.", "startOffset": 24, "endOffset": 45}, {"referenceID": 36, "context": "Concept as used here refers to what Miller et al. (1990) call a synset, essentially a node in the taxonomy.", "startOffset": 36, "endOffset": 57}, {"referenceID": 35, "context": "An experiment by Miller and Charles (1991) provided appropriate human subject data for the task.", "startOffset": 17, "endOffset": 43}, {"referenceID": 4, "context": "As Church and Patil (1982) point out, the class of \\every way ambiguous\" syntactic constructions | those for which the number of analyses is the number of binary trees over the terminal elements | includes such frequent constructions as prepositional phrases, coordination, and nominal compounds.", "startOffset": 3, "endOffset": 27}, {"referenceID": 2, "context": "Progress on broad-coverage prepositional phrase attachment ambiguity has been particularly notable, now that the dominant approach has shifted from structural strategies to quantitative analysis of lexical relationships (Whittemore, Ferrara, & Brunner, 1990; Hindle & Rooth, 1993; Brill & Resnik, 1994; Ratnaparkhi & Roukos, 1994; Li & Abe, 1995; Collins & Brooks, 1995; Merlo, Crocker, & Berthouzoz, 1997). Noun compounds have received comparatively less attention (Kobayasi, Takunaga, & Tanaka, 1994; Lauer, 1994, 1995), as has the problem of coordination ambiguity (Agarwal & Boggess, 1992; Kurohashi & Nagao, 1992). In this section, I investigate the role of semantic similarity in resolving coordination ambiguities involving nominal compounds. I began with noun phrase coordinations of the form n1 and n2 n3, which admit two structural analyses, one in which n1 and n2 are the two noun phrase heads being conjoined (1a) and one in which the conjoined heads are n1 and n3 (1b). (1) a. a (bank and warehouse) guard b. a (policeman) and (park guard) Identifying which two head nouns are conjoined is necessary in order to arrive at a correct interpretation of the phrase's content. For example, analyzing (1b) according to the structure of (1a) could lead a machine translation system to produce a noun phrase describing somebody who guards both policemen and parks. Analyzing (1a) according to the structure of (1b) could lead an information retrieval system to miss this phrase when looking for queries involving the term bank guard. Kurohashi and Nagao (1992) point out that similarity of form and similarity of meaning are important cues to conjoinability.", "startOffset": 281, "endOffset": 1566}, {"referenceID": 46, "context": "In order to measure the appropriateness of noun-noun modi cation, I use a quantitative measure of selectional t called selectional association (Resnik, 1996), which takes into account both lexical cooccurrence frequencies and semantic class membership in the WordNet taxonomy.", "startOffset": 143, "endOffset": 157}, {"referenceID": 19, "context": "What I am calling \\backing o \" is related in spirit to Katz's well known smoothing technique (Katz, 1987), but the \\backing o \" strategy used here is not quantitative.", "startOffset": 93, "endOffset": 105}, {"referenceID": 13, "context": "As examples, query expansion using related words is a well studied technique in information retrieval (e.g., Harman, 1992; Grefenstette, 1992), clusters of similar words can play a role in smoothing stochastic language models for speech recognition (Brown, Della Pietra, deSouza, Lai, & Mercer, 1992), classes of verbs that share semantic structure form the basis for an approach to interlingual machine translation (Dorr, 1997), and clusterings of related words can be used in characterizing subgroupings of retrieved documents in largescale Web searches (e.", "startOffset": 102, "endOffset": 142}, {"referenceID": 9, "context": ", Harman, 1992; Grefenstette, 1992), clusters of similar words can play a role in smoothing stochastic language models for speech recognition (Brown, Della Pietra, deSouza, Lai, & Mercer, 1992), classes of verbs that share semantic structure form the basis for an approach to interlingual machine translation (Dorr, 1997), and clusterings of related words can be used in characterizing subgroupings of retrieved documents in largescale Web searches (e.", "startOffset": 309, "endOffset": 321}, {"referenceID": 2, "context": "There is a wide body of research on the use of distributional methods for measuring word similarity in order to obtain groups of related words (e.g., Bensch & Savitch, 1992; Brill, 1991; Brown et al., 1992; Grefenstette, 1992, 1994; McKeown & Hatzivassiloglou, 1993; Pereira, Tishby, & Lee, 1993; Sch\u007f utze, 1993), and thesauri such as WordNet are another source of word relationships (e.", "startOffset": 143, "endOffset": 313}, {"referenceID": 18, "context": "With that crucial caveat, it is nonetheless interesting to note that the results obtained here are broadly consistent with Kurohashi and Nagao (1992), who report accuracy results in the range of 80-83% at 100% coverage when analyzing a broad range of conjunctive structures in Japanese using a combination of string matching, syntactic similarity, and thesaurus-based similarity, and with Agarwal and Boggess (1992), who use syntactic types and structure, along with partly domain-dependent semantic labels, to obtain accuracies in a similar range for identifying conjuncts in English.", "startOffset": 123, "endOffset": 150}, {"referenceID": 0, "context": "With that crucial caveat, it is nonetheless interesting to note that the results obtained here are broadly consistent with Kurohashi and Nagao (1992), who report accuracy results in the range of 80-83% at 100% coverage when analyzing a broad range of conjunctive structures in Japanese using a combination of string matching, syntactic similarity, and thesaurus-based similarity, and with Agarwal and Boggess (1992), who use syntactic types and structure, along with partly domain-dependent semantic labels, to obtain accuracies in a similar range for identifying conjuncts in English.", "startOffset": 389, "endOffset": 416}, {"referenceID": 0, "context": "With that crucial caveat, it is nonetheless interesting to note that the results obtained here are broadly consistent with Kurohashi and Nagao (1992), who report accuracy results in the range of 80-83% at 100% coverage when analyzing a broad range of conjunctive structures in Japanese using a combination of string matching, syntactic similarity, and thesaurus-based similarity, and with Agarwal and Boggess (1992), who use syntactic types and structure, along with partly domain-dependent semantic labels, to obtain accuracies in a similar range for identifying conjuncts in English. 5. Using Taxonomic Similarity in Word Sense Selection This section considers the application of the semantic similarity measure in resolving another form of ambiguity: selecting the appropriate sense of a noun when it appears in the context of other nouns that are related in meaning. 5.1 Associating Word Senses with Noun Groupings Knowledge about groups of related words plays a role in many natural language applications. As examples, query expansion using related words is a well studied technique in information retrieval (e.g., Harman, 1992; Grefenstette, 1992), clusters of similar words can play a role in smoothing stochastic language models for speech recognition (Brown, Della Pietra, deSouza, Lai, & Mercer, 1992), classes of verbs that share semantic structure form the basis for an approach to interlingual machine translation (Dorr, 1997), and clusterings of related words can be used in characterizing subgroupings of retrieved documents in largescale Web searches (e.g., Digital Equipment Corporation, 1998). There is a wide body of research on the use of distributional methods for measuring word similarity in order to obtain groups of related words (e.g., Bensch & Savitch, 1992; Brill, 1991; Brown et al., 1992; Grefenstette, 1992, 1994; McKeown & Hatzivassiloglou, 1993; Pereira, Tishby, & Lee, 1993; Sch\u007f utze, 1993), and thesauri such as WordNet are another source of word relationships (e.g., Voorhees, 1994). Distributional techniques can sometimes do a good job of identifying groups of related words (see Resnik, 1998b, for an overview and critical discussion), but for some tasks the relevant relationships are not among words, but among word senses. For example, Brown et al. (1992) illustrate the notion of a distributionally derived, \\semantically sticky\" cluster using an automatically derived word group containing attorney, counsel, trial, court, and judge.", "startOffset": 389, "endOffset": 2299}, {"referenceID": 43, "context": "13 Resnik (1998a) introduces an algorithm that uses taxonomically-de ned semantic similarity in order to derive grouping relationships among word senses from grouping relationships among words.", "startOffset": 3, "endOffset": 18}, {"referenceID": 47, "context": "Resnik Algorithm (Resnik, 1998a).", "startOffset": 17, "endOffset": 32}, {"referenceID": 54, "context": "This observation is similar in spirit to other approaches to word sense disambiguation based on maximizing relatedness of meaning (e.g., Lesk, 1986; Sussna, 1993).", "startOffset": 130, "endOffset": 162}, {"referenceID": 58, "context": "2 Linking to WordNet using a Bilingual Dictionary Multilingual resources for natural language processing can be di cult to obtain, although some promising e orts are underway in projects like EuroWordNet (Vossen, 1998).", "startOffset": 204, "endOffset": 218}, {"referenceID": 16, "context": "As the relatively low accuracies for human judges demonstrate, disambiguation using WordNet's ne-grained senses is quite a bit more di cult than disambiguation to the level of homographs (Hearst, 1991; Cowie, Guthrie, & Guthrie, 1992).", "startOffset": 187, "endOffset": 234}, {"referenceID": 40, "context": "Resnik (1998a) illustrates the algorithm of Figure 3 using word groupings from a variety of sources, including several of the sources on distributional clustering cited above, and evaluates the algorithm more rigorously on the task of associating WordNet senses with nouns in Roget's thesaurus, based on their thesaurus category membership.", "startOffset": 0, "endOffset": 15}, {"referenceID": 47, "context": "This is unsurprising, given previous experience with the problem of selecting among WordNet's ne-grained senses (Resnik, 1998a; Resnik & Yarowsky, 1997).", "startOffset": 112, "endOffset": 152}, {"referenceID": 31, "context": "perch, rod, pole { ((British) a linear measure of 16.5 feet) 5. perch, rod, pole { (a square rod of land) 6. pole, celestial pole { (one of two points of intersection of the Earth's axis and the celestial sphere) 7. pole { (one of two antipodal points where the Earth's axis of rotation intersects the Earth's surface) 8. terminal, pole { (a point on an electrical device (such as a battery) at which electric current enters or leaves) 9. pole { (a long berglass implement used for pole vaulting) 10. pole, magnetic pole { (one of the two ends of a magnet where the magnetism seems to be concentrated) Figure 9: List of WordNet senses for pole a noun, displayed as in Figure 9. Notice that if a user of WEDT had simply gone directly to the WordNet server to look up pole, the full list of 10 senses would have appeared with no indication of which are most potentially related to the WEDT dictionary entry under consideration. In contrast, the WEDT hyperlinks, introduced via the sense selection algorithm, lter out the majority of the irrelevant senses and provide the user a measure of con dence in selecting among those that remain. Although no formal evaluation of the WEDT/WordNet connection has been attempted, the results of the bilingual dictionary experiment suggest that this application of word sense disambiguation | ltering out the least relevant senses, and then leaving the user in the loop | is a task for which the sense disambiguation algorithm is well suited. This is supported by user feedback on the XWN feature of WEDT, which has been favorable (Robert Parks, personal communication). The site has been growing in popularity, with a current estimate of 1000-1500 hits per day. 6. Related Work There is an extensive literature on measuring similarity in general, and on word similarity in particular; for a classic paper see Tversky (1977). Recent work in information retrieval and computational linguistics has emphasized a distributional approach, in which words are represented as vectors in a space of features and similarity measures are de ned in terms of those vectors; see Resnik (1998b) for discussion, and Lee (1997) for a good recent example.", "startOffset": 32, "endOffset": 1860}, {"referenceID": 31, "context": "perch, rod, pole { ((British) a linear measure of 16.5 feet) 5. perch, rod, pole { (a square rod of land) 6. pole, celestial pole { (one of two points of intersection of the Earth's axis and the celestial sphere) 7. pole { (one of two antipodal points where the Earth's axis of rotation intersects the Earth's surface) 8. terminal, pole { (a point on an electrical device (such as a battery) at which electric current enters or leaves) 9. pole { (a long berglass implement used for pole vaulting) 10. pole, magnetic pole { (one of the two ends of a magnet where the magnetism seems to be concentrated) Figure 9: List of WordNet senses for pole a noun, displayed as in Figure 9. Notice that if a user of WEDT had simply gone directly to the WordNet server to look up pole, the full list of 10 senses would have appeared with no indication of which are most potentially related to the WEDT dictionary entry under consideration. In contrast, the WEDT hyperlinks, introduced via the sense selection algorithm, lter out the majority of the irrelevant senses and provide the user a measure of con dence in selecting among those that remain. Although no formal evaluation of the WEDT/WordNet connection has been attempted, the results of the bilingual dictionary experiment suggest that this application of word sense disambiguation | ltering out the least relevant senses, and then leaving the user in the loop | is a task for which the sense disambiguation algorithm is well suited. This is supported by user feedback on the XWN feature of WEDT, which has been favorable (Robert Parks, personal communication). The site has been growing in popularity, with a current estimate of 1000-1500 hits per day. 6. Related Work There is an extensive literature on measuring similarity in general, and on word similarity in particular; for a classic paper see Tversky (1977). Recent work in information retrieval and computational linguistics has emphasized a distributional approach, in which words are represented as vectors in a space of features and similarity measures are de ned in terms of those vectors; see Resnik (1998b) for discussion, and Lee (1997) for a good recent example.", "startOffset": 32, "endOffset": 2116}, {"referenceID": 28, "context": "Recent work in information retrieval and computational linguistics has emphasized a distributional approach, in which words are represented as vectors in a space of features and similarity measures are de ned in terms of those vectors; see Resnik (1998b) for discussion, and Lee (1997) for a good recent example.", "startOffset": 275, "endOffset": 286}, {"referenceID": 53, "context": "The inverse document frequency (idf) for term weighting in information retrieval makes use of logarithmic scaling, and serves to identify terms that do not discriminate well among di erent documents, a concept very similar in spirit to the idea that such terms have low information content (Salton, 1989).", "startOffset": 290, "endOffset": 304}, {"referenceID": 4, "context": ", see Church and Mercer (1993). The information of an event is a fundamental notion in stochastic language modeling for speech recognition, where the contribution of a correct word prediction based on its conditional probability, p(wordjcontext), is measured as the information conveyed by that prediction, log p(wordjcontext).", "startOffset": 6, "endOffset": 31}, {"referenceID": 4, "context": ", see Church and Mercer (1993). The information of an event is a fundamental notion in stochastic language modeling for speech recognition, where the contribution of a correct word prediction based on its conditional probability, p(wordjcontext), is measured as the information conveyed by that prediction, log p(wordjcontext). This forms the basis for standard measures of language model performance, such as cross entropy. Frequency of shared and unshared features has also long been a factor in computing similarity over vector representations. The inverse document frequency (idf) for term weighting in information retrieval makes use of logarithmic scaling, and serves to identify terms that do not discriminate well among di erent documents, a concept very similar in spirit to the idea that such terms have low information content (Salton, 1989). Although the counting of edges in is-a taxonomies seems to be something many people have tried, there seem to be few published descriptions of attempts to directly evaluate the e ectiveness of this method. A number of researchers have attempted to make use of conceptual distance in information retrieval. For example, Rada et al. (1989, 1989) and Lee et al. (1993) report experiments using conceptual distance, implemented using the edgecounting metric, as the basis for ranking documents by their similarity to a query.", "startOffset": 6, "endOffset": 1220}, {"referenceID": 4, "context": ", see Church and Mercer (1993). The information of an event is a fundamental notion in stochastic language modeling for speech recognition, where the contribution of a correct word prediction based on its conditional probability, p(wordjcontext), is measured as the information conveyed by that prediction, log p(wordjcontext). This forms the basis for standard measures of language model performance, such as cross entropy. Frequency of shared and unshared features has also long been a factor in computing similarity over vector representations. The inverse document frequency (idf) for term weighting in information retrieval makes use of logarithmic scaling, and serves to identify terms that do not discriminate well among di erent documents, a concept very similar in spirit to the idea that such terms have low information content (Salton, 1989). Although the counting of edges in is-a taxonomies seems to be something many people have tried, there seem to be few published descriptions of attempts to directly evaluate the e ectiveness of this method. A number of researchers have attempted to make use of conceptual distance in information retrieval. For example, Rada et al. (1989, 1989) and Lee et al. (1993) report experiments using conceptual distance, implemented using the edgecounting metric, as the basis for ranking documents by their similarity to a query. Sussna (1993) uses semantic relatedness measured with WordNet in word sense disambiguation, de ning a measure of distance that weights di erent types of links and also explicitly takes depth in the taxonomy into account.", "startOffset": 6, "endOffset": 1390}, {"referenceID": 4, "context": ", see Church and Mercer (1993). The information of an event is a fundamental notion in stochastic language modeling for speech recognition, where the contribution of a correct word prediction based on its conditional probability, p(wordjcontext), is measured as the information conveyed by that prediction, log p(wordjcontext). This forms the basis for standard measures of language model performance, such as cross entropy. Frequency of shared and unshared features has also long been a factor in computing similarity over vector representations. The inverse document frequency (idf) for term weighting in information retrieval makes use of logarithmic scaling, and serves to identify terms that do not discriminate well among di erent documents, a concept very similar in spirit to the idea that such terms have low information content (Salton, 1989). Although the counting of edges in is-a taxonomies seems to be something many people have tried, there seem to be few published descriptions of attempts to directly evaluate the e ectiveness of this method. A number of researchers have attempted to make use of conceptual distance in information retrieval. For example, Rada et al. (1989, 1989) and Lee et al. (1993) report experiments using conceptual distance, implemented using the edgecounting metric, as the basis for ranking documents by their similarity to a query. Sussna (1993) uses semantic relatedness measured with WordNet in word sense disambiguation, de ning a measure of distance that weights di erent types of links and also explicitly takes depth in the taxonomy into account. Following the original proposal to measure semantic similarity in a taxonomy using information content (Resnik, 1993b, 1993a), a number of related proposals have been explored. Leacock and Chodorow (1994) de ne a measure resembling information content, but using the normalized path length between the two concepts being compared rather than the probability of a subsuming concept.", "startOffset": 6, "endOffset": 1802}, {"referenceID": 20, "context": "Lin (1997, 1998) has recently proposed an alternative information-theoretic similarity measure, derived from a set of basic assumptions about similarity in a style reminiscent of the way in which entropy/information itself has a formal de nition derivable from a set of basic properties (Khinchin, 1957).", "startOffset": 287, "endOffset": 303}], "year": 2011, "abstractText": null, "creator": "dvips 5.55 Copyright 1986, 1994 Radical Eye Software"}}}