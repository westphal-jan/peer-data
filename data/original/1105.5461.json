{"id": "1105.5461", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2011", "title": "Probabilistic Deduction with Conditional Constraints over Basic Events", "abstract": "We study the problem of probabilistic deduction with conditional constraints over basic events. We show that globally complete probabilistic deduction with conditional constraints over basic events is NP-hard. We then concentrate on the special case of probabilistic deduction in conditional constraint trees. We elaborate very efficient techniques for globally complete probabilistic deduction. In detail, for conditional constraint trees with point probabilities, we present a local approach to globally complete probabilistic deduction, which runs in linear time in the size of the conditional constraint trees. For conditional constraint trees with interval probabilities, we show that globally complete probabilistic deduction can be done in a global approach by solving nonlinear programs. We show how these nonlinear programs can be transformed into equivalent linear programs, which are solvable in polynomial time in the size of the conditional constraint trees.", "histories": [["v1", "Fri, 27 May 2011 01:53:20 GMT  (205kb)", "http://arxiv.org/abs/1105.5461v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["t lukasiewicz"], "accepted": false, "id": "1105.5461"}, "pdf": {"name": "1105.5461.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["lukasiewicz@informatik.uni-giessen.de"], "sections": [{"heading": null, "text": "Journal of Arti cial Intelligence Research 10 (1999) 199-241 Submitted 9/98; published 4/99 Probabilistic Deduction withConditional Constraints over Basic EventsThomas Lukasiewicz lukasiewicz@informatik.uni-giessen.deInstitut f ur Informatik, Universit at Gie enArndtstra e 2, D-35392 Gie en, Germany AbstractWe study the problem of probabilistic deduction with conditional constraints over basicevents. We show that globally complete probabilistic deduction with conditional constraintsover basic events is NP-hard. We then concentrate on the special case of probabilisticdeduction in conditional constraint trees. We elaborate very e cient techniques for globallycomplete probabilistic deduction. In detail, for conditional constraint trees with pointprobabilities, we present a local approach to globally complete probabilistic deduction,which runs in linear time in the size of the conditional constraint trees. For conditionalconstraint trees with interval probabilities, we show that globally complete probabilisticdeduction can be done in a global approach by solving nonlinear programs. We show howthese nonlinear programs can be transformed into equivalent linear programs, which aresolvable in polynomial time in the size of the conditional constraint trees.1. IntroductionDealing with uncertain knowledge plays an important role in knowledge representation andreasoning. There are many di erent formalisms and methodologies for handling uncertainty.Most of them are directly or indirectly based on probability theory.In this paper, we focus on probabilistic deduction with conditional constraints over basicevents (that is, interval restrictions for conditional probabilities of elementary events). Theconsidered probabilistic deduction problems consist of a probabilistic knowledge base anda probabilistic query. We give a classical example. As a probabilistic knowledge base, wemay take the probabilistic knowledge that all ostriches are birds, that the probability ofTweety being a bird is greater than 0.90, and that the probability of Tweety being an ostrichprovided she is a bird is greater than 0.80. As a probabilistic query, we may now wonderabout the entailed greatest lower and least upper bound for the probability that Tweetyis an ostrich. The solution to this probabilistic deduction problem is 0.72 for the entailedgreatest lower bound and 1.00 for the entailed least upper bound.More generally, probabilistic deduction with conditional constraints over propositionalevents can be done in a global approach by linear programming or in a local approach bythe iterative application of inference rules. Note that it is immediately NP-hard, since itgeneralizes the satis ability problem for classical propositional logic (see Section 2.2).Research on the global approach spread in particular after the important work on prob-abilistic logic by Nilsson (1986) (see also the work by Paa , 1988). The main focus wason analyzing the computational complexity of satis ability and entailment in probabilis-tic logic and on developing e cient linear programming algorithms for these problems.c 1999 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.\nLukasiewiczGeorgakopoulos et al. (1988) show that the satis ability problem in probabilistic logic isNP-complete and propose to apply column generation techniques for its processing. Thisapproach was further developed by Kavvadias and Papadimitriou (1990), Jaumard et al.(1991), Andersen and Hooker (1994), and Hansen et al. (1995). In particular, Jaumard etal. (1991) report promising experimental results on the e ciency in special cases of prob-abilistic satis ability and entailment. Moreover, Kavvadias and Papadimitriou (1990) andJaumard et al. (1991) identify special cases of probabilistic satis ability that can be solvedin polynomial time. Other work on the global approach concentrates on reducing the num-ber of linear constraints (Luo et al. 1996) and the number of variables (Lukasiewicz, 1997).Finally, Fagin et al. (1992) present a sound and complete axiom system for reasoning aboutprobabilities that are expressed by linear inequalities over propositional events. They showthat the satis ability problem in this quite expressive framework is still NP-complete.In early work, Dubois and Prade (1988) use inference rules to model default reason-ing with imprecise numerical and fuzzy quanti ers. For this reason, subsequent researchon inference rules especially aims at analyzing patterns of human commonsense reasoning(Dubois et al. 1990, 1993; Amarger et al. 1991; Th one, 1994; Th one et al. 1995). Frischand Haddawy (1994) discuss the use of inference rules for deduction in probabilistic logic.Recent work on inference rules concentrates on integrating probabilistic knowledge into de-scription logics (Heinsohn, 1994) and on analyzing the interplay between taxonomic andprobabilistic deduction (Lukasiewicz 1998a, 1999a).We now summarize the main characteristics of the global and the local approach.The global approach can be performed within quite rich probabilistic languages (Faginet al., 1992). Crucially, probabilistic deduction by linear programming is globally complete(that is, it really provides the requested tightest bounds entailed by the whole probabilisticknowledge base). However, a main drawback of the global approach is that it generally doesnot provide useful explanatory information on the deduction process. Finally, results onthe special-case tractability of global approaches are driven by the technical possibilities oflinear programming techniques and not by the needs of arti cial intelligence applications.Hence, they do not seem to be very useful in the arti cial intelligence context.A main advantage of the local approach is that the deduced results can be explainedin a natural way by the sequence of applied inference rules (Amarger et al. 1991; Frisch& Haddawy, 1994). However, the iterative application of inference rules is generally re-stricted to quite narrow probabilistic languages. Moreover, it is very rarely and only withinvery restricted languages globally complete (Frisch and Haddawy, 1994, give an exampleof globally complete local probabilistic deduction in a very restricted framework). Finally,as far as the computational complexity is concerned, there are very few experimental andtheoretical results on the special-case tractability of local approaches.The main motivating idea of this paper is to elaborate e cient local techniques forglobally complete probabilistic deduction. Inspired by previous work on inference rules, wefocus our research on the language of conditional constraints over basic events:Dubois and Prade (1988) study the chaining of two bidirectional conditional constraintsover basic events (\\quanti ed syllogism rule\") and some of its special cases. Dubois etal. (1990) additionally discuss probabilistic deductions about conjunctions of basic events.Furthermore, they describe the open problem of probabilistic deduction along a chain ofmore than two bidirectional conditional constraints over basic events. In later work, Dubois200\nProbabilistic Deduction with Conditional Constraints over Basic Eventset al. (1993) use a qualitative version of the \\quanti ed syllogism rule\" in an approach toreasoning with linguistic quanti ers. Amarger et al. (1991) propose to apply the \\quan-ti ed syllogism rule\" and the \\generalized Bayes' rule\" to sets of bidirectional conditionalconstraints over basic events. They report promising experimental results on the global com-pleteness and the computational complexity of the presented deduction technique. However,this deduction technique is generally not globally complete. Th one (1994) examines trees ofbidirectional conditional constraints over basic events. He presents a linear-time deductiontechnique that is based on a system of inference rules and that computes certain logicallyentailed greatest lower bounds (in the technical notions of this paper, which will be de nedbelow, tight lower answers to conclusion-restricted queries are computed).As a rst contribution of this paper, we show that globally complete probabilistic de-duction with conditional constraints over basic events is NP-hard. It is surprising that thisquite restricted class of probabilistic deduction problems is still computationally so di -cult. Hence, it is unlikely that there is an algorithm that e ciently solves all probabilisticdeduction problems with conditional constraints over basic events. However, we can stillhope that there are e cient special-case, average-case, or approximation algorithms.In this paper, we then elaborate e cient special-case algorithms. In detail, we concen-trate on probabilistic deduction in conditional constraint trees. It is an interesting sub-class of all probabilistic deduction problems with conditional constraints over basic events.Conditional constraint trees are undirected trees with basic events as nodes and with bidi-rectional conditional constraints over basic events as edges between the nodes (that is,deduction in conditional constraint trees is a generalization of deduction along a chain ofbidirectional conditional constraints over basic events). Like Bayesian networks, conditionalconstraint trees represent a well-structured probabilistic knowledge base. Di erently fromBayesian networks, they do not encode any probabilistic independencies.As a main contribution of this paper, we have the following results. For conditional con-straint trees with point probabilities, we present functions for deducing greatest lower andleast upper bounds in linear time in the size of the conditional constraint trees. Moreover, forconditional constraint trees with interval probabilities, we show that greatest lower boundscan be deduced in the same way, in linear time in the size of the conditional constraint trees.However, computing least upper bounds turns out to be computationally more di cult. Itcan be done by solving special nonlinear programs. We show how these nonlinear programscan be transformed into equivalent linear programs. The resulting linear programs have anumber of variables and inequalities linear and polynomial, respectively, in the size of theconditional constraint trees. Thus, our way of deducing least upper bounds still runs inpolynomial time in the size of the conditional constraint trees, since linear programmingruns in polynomial time in the size of the linear programs.Another important contribution of this paper is related to the question whether toperform probabilistic deduction with conditional constraints by the iterative application ofinference rules or by linear programming. On the one hand, the idea of inference rules carriesus to very e cient techniques for globally complete probabilistic deduction in conditionalconstraint trees. In particular, the considered deduction problems generalize patterns ofcommonsense reasoning. However, on the other hand, the corresponding proofs of soundnessand global completeness are technically quite complex. Hence, it seems unlikely that theresults of this work can be extended to signi cantly more general probabilistic deduction201\nLukasiewiczproblems. Note that a companion paper (1998a, 1999a) reports similar limits of the localapproach in probabilistic deduction under taxonomic knowledge.The rest of this paper is organized as follows. In Section 2, we formulate the proba-bilistic deduction problems considered in this work. Section 3 focuses on the probabilisticsatis ability of conditional constraint trees. Section 4 deals with globally complete proba-bilistic deduction in exact and general conditional constraint trees. In Section 5, we give acomparison with Bayesian networks. Section 6 summarizes the main results of this work.2. Formulating the Probabilistic Deduction ProblemIn this section, we introduce the syntactic and semantic notions related to probabilisticknowledge in general and to conditional constraint trees in particular.2.1 Probabilistic KnowledgeBefore focusing on the details of conditional constraint trees, we give a general introductionto the kind of probabilistic knowledge considered in this work. We deal with conditionalconstraints over propositional events. They represent interval restrictions for conditionalprobabilities of propositional events. Note that the formal background introduced in thissection is commonly accepted in the literature (see especially the work by Frisch and Had-dawy, 1994, for other work in the same spirit).We assume a nonempty and nite set of basic events B = fB1; B2; : : : ; Bng. The set ofconjunctive events CB is the closure of B under the Boolean operation ^. We abbreviate theconjunctive event C^D by CD . The set of propositional events GB is the closure of B underthe Boolean operations ^ and :. We abbreviate the propositional events G ^H and :Gby GH and G, respectively. The false event B1 ^ :B1 and the true event :(B1 ^ :B1) areabbreviated by ? and >, respectively. Conditional constraints are expressions of the form(HjG)[u1; u2] with real numbers u1; u2 2 [0; 1] and propositional events G and H. In theconditional constraint (HjG)[u1; u2], we call G the premise and H the conclusion.To de ne probabilistic interpretations of propositional events and of conditional con-straints, we introduce atomic events and the binary relation ) between atomic and propo-sitional events. The set of atomic events AB is de ned by AB = fE1E2 En jEi = Bior Ei = Bi for all i 2 [1 :n]g. Note that each atomic event can be interpreted as a possibleworld (which corresponds to a mapping from B to ftrue; falseg). For all atomic events Aand all propositional events G, let A) G i AG is a propositional contradiction.A probabilistic interpretation Pr is a mapping from AB to [0; 1] such that all Pr(A) withA 2 AB sum up to 1. Pr is extended in a well-de ned way to propositional events G by:Pr(G) is the sum of all Pr(A) with A 2 AB and A ) G. Pr is extended to conditionalconstraints by: Pr j= (HjG)[u1; u2] i u1 Pr(G) Pr(GH ) u2 Pr(G).Note that conditional constraints characterize conditional probabilities of events, ratherthan probabilities of conditional events (Coletti, 1994; Gilio & Scozzafava, 1994). Note alsothat Pr(G) = 0 always entails Pr j= (HjG)[u1; u2]. This semantics of conditional probabilitystatements is also assumed by Halpern (1990) and by Frisch and Haddawy (1994).The notions of models, satis ability, and logical consequence for conditional constraintsare de ned in the classical way. A probabilistic interpretation Pr is a model of a conditionalconstraint (HjG)[u1; u2] i Pr j= (HjG)[u1; u2]. Pr is a model of a set of conditional202\nProbabilistic Deduction with Conditional Constraints over Basic Eventsconstraints KB , denoted Pr j= KB , i Pr is a model of all (HjG)[u1; u2] 2 KB . KB issatis able i a model of KB exists. (HjG)[u1; u2] is a logical consequence of KB; denotedKB j= (HjG)[u1; u2], i each model of KB is also a model of (HjG)[u1; u2].For a conditional constraint (HjG)[u1; u2] and a set of conditional constraints KB , letu denote the set of all real numbers u 2 [0; 1] for which there exists a model Pr of KB withu Pr(G) = Pr(GH ) and Pr(G) > 0. Now, we easily verify that (HjG)[u1; u2] is a logicalconsequence of KB i u1 inf u and u2 supu.This observation yields a canonical notion of tightness for logical consequences of con-ditional constraints. The conditional constraint (HjG)[u1; u2] is a tight logical consequenceof KB; denoted KB j=tight (HjG)[u1; u2], i u1 = inf u and u2 = supu.The set u is a closed interval in the real line (Frisch & Haddawy, 1994). Note that foru = ;, we canonically de ne inf u = max [0; 1] = 1 and supu = min [0; 1] = 0. Thus, u = ;i KB j= (Gj>)[0; 0] i KB j=tight (HjG)[1; 0] i KB j= (HjG)[u1; u2] for all u1; u2 2 [0; 1].Based on the just introduced notion of tight logical consequence, probabilistic deductionproblems and their solutions are more formally speci ed as follows.A probabilistic knowledge base (B;KB) consists of a set of basic events B and a set ofconditional constraints KB over GB with u1 u2 for all (HjG)[u1; u2] 2 KB . A probabilisticquery to a probabilistic knowledge base (B;KB) is an expression of the form 9(F jE)[x1; x2]with E;F 2 GB and two di erent variables x1 and x2. Its tight answer is the substitution = fx1=u1; x2=u2g with u1; u2 2 [0; 1] such that KB j=tight (F jE)[u1; u2] (we call 1 =fx1=u1g the tight lower answer and 2 = fx2=u2g the tight upper answer). A correct answeris a substitution = fx1=u1; x2=u2g with u1; u2 2 [0; 1] such that KB j= (F jE)[u1; u2].Finally, we de ne the notions of soundness and of completeness related to inferencerules and to techniques for probabilistic deduction. An inference rule KB ` (HjG)[u1; u2] issound i KB j= (HjG)[u1; u2], where (HjG)[u1; u2] is a conditional constraint and KB is aset of conditional constraints. It is sound and locally complete i KB j=tight (HjG)[u1; u2].A technique for probabilistic deduction is sound for a set of probabilistic queriesQ i it com-putes a correct answer to any given query from Q. It is sound and globally complete for Qi it computes the tight answer to any given query from Q.2.2 Computational ComplexityIn the framework of conditional constraints over propositional events, the optimization prob-lem of computing the tight answer to a probabilistic query is immediately NP-hard, since itgeneralizes the satis ability problem for classical propositional logic (the NP-complete prob-lem of deciding whether a propositional formula in conjunctive normal form is satis able;see especially the survey by Garey and Johnson, 1979).Surprisingly, the optimization problem of computing the tight answer to a probabilisticquery remains NP-hard even if we just consider conditional constraints over basic events:Theorem 2.1 The optimization problem of computing the tight answer to a probabilisticquery over basic events that is directed to a probabilistic knowledge base over basic eventsis NP-hard.Proof. The NP-complete decision problem of graph 3-colorability (Garey & Johnson, 1979)can be polynomially-reduced to the optimization problem of computing the tight answer203\nLukasiewiczto a probabilistic query over basic events that is directed to a probabilistic knowledge baseover basic events. The proof follows similar lines to the proof of NP-hardness of 2PSATgiven by Georgakopoulos et al. (1988).Let (V;E) be a nite undirected graph. We construct a probabilistic knowledge base(B;KB) as follows. We initialize (B;KB) with (fBg; ;). For each node v 2 V , we increaseB by the new basic events B1v , B2v , and B3v . For each node v 2 V and for each i 2 f1; 2; 3g,we increase KB by (BjBiv)[1; 1] and (BivjB)[1=3; 1=3]. For each node v 2 V and for eachi; j 2 f1; 2; 3g with i < j, we increase KB by (BjvjBiv)[0; 0]. For each edge fu; vg 2 E and foreach i 2 f1; 2; 3g, we increase KB by (BivjBiu)[0; 0]. It is easy to see that the probabilisticknowledge base (B;KB) can be constructed in polynomial time in the size of (V;E).Now, we show that (V;E) is 3-colorable i fx1=1; x2=1g is the tight answer to theprobabilistic query 9(BjB)[x1; x2] to (B;KB), or equivalently, i KB is satis able:If (V;E) is 3-colorable, then there exists a mapping c1 from V to f1; 2; 3g with c1(u) 6=c1(v) for all edges fu; vg 2 E. Thus, if is a cyclic permutation of the members in f1; 2; 3gand if c2; c3 : V ! f1; 2; 3g are de ned by c2(v) = (c1(v)) and c3(v) = (c2(v)) for allnodes v 2 V , then also c2(u) 6= c2(v) and c3(u) 6= c3(v) for all edges fu; vg 2 E. Forj 2 f1; 2; 3g, let Aj 2 AB such that Aj ) B and Aj ) Biv i cj(v) = i for all nodes v 2 Vand i 2 f1; 2; 3g. If Pr : AB ! [0; 1] is de ned by Pr(A) = 1=3 for all A 2 fA1; A2; A3g andby Pr(A) = 0 for all A 2 AB n fA1; A2; A3g, then Pr is a model of KB .Conversely, if there is a model Pr of KB , then there is an atomic event A 2 AB withPr(A) > 0. Thus, if c : V ! f1; 2; 3g is de ned by c(v) = i i A) Biv for all nodes v 2 V ,then c(u) 6= c(v) for all edges fu; vg 2 E. Hence, (V;E) is 3-colorable. 2Hence, it is unlikely that there is an e cient algorithm for computing the tight answerto all probabilistic queries over basic events that are directed to any given probabilisticknowledge base over basic events. However, there may still be e cient algorithms forsolving more specialized probabilistic deduction problems.The rest of this work deals with probabilistic deduction in conditional constraint trees.The next section provides a motivating example, which gives evidence of the practicalimportance of this kind of probabilistic deduction problems.2.3 Motivating ExampleA senior student in mathematics describes her experience about being successful at the uni-versity as follows. The success of a student (su) is in uenced by how well-informed (wi) andhow well-prepared (wp) the student is. Well-informedness can be reached by interviewingprofessors (pr) or by asking senior students (st). Being well-prepared is in uenced by howmuch time is invested in books (bo), exercises (ex), and hobbies (ho).It is estimated that the probability of a student being successful given she is well-informed lies between 0.60 and 0.70, that the probability of a student being well-informedgiven she is successful is greater than 0.85, that the probability of a student being successfulgiven she is well-prepared is greater than 0.95, and that the probability of a student beingwell-prepared given she is successful is greater than 0.95.This probabilistic knowledge completed by further probabilistic estimations is given bythe probabilistic knowledge base (B;KB) in Fig. 1, where B is the set of nodes fsu;wi;wp; pr;204\nProbabilistic Deduction with Conditional Constraints over Basic Eventsst; bo; ex; hog and KB is the least set of conditional constraints that contains (Y jX)[u1; u2]for each arrow from X to Y labeled with u1; u2. su wp ex st\n.85,1\n.6,.7 .95,1 .85,.9\n.85,.9\nho\n.6,.7.95,1 .35,.4\npr bo\n.95,1\n.95,1\n.05,.1.95,1 .95,1\n.35,.4\nwi\nFigure 1: A Conditional Constraint TreeWe may wonder whether it is useful for being successful at the university to interview theprofessors, to study on books, to spend the time on one's hobbies, or to do both studyingon books and spending the time on one's hobbies. This can be expressed by the prob-abilistic queries 9(sujpr)[x1; x2], 9(sujbo)[x1; x2], 9(sujho)[x1; x2], and 9(sujbo ho)[x1; x2],which yield the tight answers fx1=0:00, x2=1:00g, fx1=0:90; x2=1:00g, fx1=0:30; x2=0:46g,and fx1=0:71; x2=1:00g, respectively.We may wonder whether successful students at the university interviewed their profes-sors, whether they studied on books, whether they spent their time with their hobbies, orwhether they both studied on books and spent their time with their hobbies. This can beexpressed by the probabilistic queries 9(prjsu)[x1; x2], 9(bojsu)[x1; x2], 9(hojsu)[x1; x2], and9(bo hojsu)[x1; x2], which yield the tight answers fx1=0:00, x2=0:17g, fx1=0:90; x2=1:00g,fx1=0:30; x2=0:45g, and fx1=0:25; x2=0:45g, respectively.2.4 Conditional Constraint TreesWe formally de ne conditional constraint trees and queries to conditional constraint trees.We provide some additional examples, which are subsequently used as running examples.A (general) conditional constraint tree is a probabilistic knowledge base (B;KB) forwhich an undirected tree (a singly connected undirected graph) (B;$) exists such thatKB contains exactly one pair of conditional constraints (BjA)[u1; u2] and (AjB)[v1; v2] withu1; v1 > 0 for each pair of adjacent nodes A and B (note that B = fBg implies KB = ;).A basic event B 2 B is called a leaf in (B;KB) i it has exactly one neighbor in (B;$).A conditional constraint tree is exact i u1 = u2 for all (BjA)[u1; u2] 2 KB .A query to a conditional constraint tree is a probabilistic query 9(F jE)[x1; x2] with twoconjunctive events E and F that are disjoint in their basic events and such that all pathsfrom a basic event in E to a basic event in F have at least one basic event in common.A query 9(F jE)[x1; x2] to a conditional constraint tree is premise-restricted i E is a basicevent. It is conclusion-restricted i F is a basic event. It is strongly conclusion-restrictedi F is the only basic event that is contained in all paths from a basic event in E to F .It is complete i EF contains exactly the leaves of (B;$).205\nLukasiewiczFig. 2 shows two conditional constraint trees of which the one on the left side is exact.9(STUjMNQR)[x1; x2] is a query, while 9(MSjQU)[x1; x2] is not a query to the conditionalconstraint trees of Fig. 2. Furthermore, 9(STUjM)[x1; x2] is a premise-restricted query,9(OjQRSTU)[x1; x2] a strongly conclusion-restricted query, and 9(QRSTUjM)[x1; x2] a prem-ise-restricted complete query to the conditional constraint trees of Fig. 2. 1)\n1\n1\n.8,.9\n.3,.4 .9,1\n.9,1\n.8,.9.9,1 .8,.9\n.9,1.8,.9\n1\n1\n.85\n.95 .95\n.95 .85\n.85\n.85 .95\n.95\n.15 .95\n.55\n.85\n.5,.6\n.1,.2\n.8,.9 .9,1\n.9,1\nM O\nP\nQ\nR\nS\nT\nU\nM N O\nP\nQ\nR\nS\nT\nU\n2)\n.35\nN\nFigure 2: Two Conditional Constraint TreesFor conditional constraint trees (B;KB), conjunctive events C, and basic events B, wewrite C)B i there exists a path G1; G2; : : : ; Gk from a basic event G1 in C to the basicevent Gk=B such that (Gi+1jGi)[1; 1]2KB for all i 2 [1 : k 1]. We write B)C i forall paths G1; G2; : : : ; Gk from the basic event G1=B to a basic event Gk in C, it holds(Gi+1jGi)[1; 1]2KB for all i 2 [1 : k 1]. That is, the conditions C)B and B)Cimmediately entail KB j= (BjC)[1; 1] and KB j= (CjB)[1; 1], respectively.Note that the restriction u1; v1 > 0 for all (BjA)[u1; u2], (AjB)[v1; v2] 2 KB is just madefor technical convenience. The deduction technique of Section 4 can easily be generalizedto conditional constraint trees (B;KB) that satisfy only the restriction u1> 0 i v1> 0 forall (BjA)[u1; u2]; (AjB)[v1; v2] 2 KB (Lukasiewicz, 1996).The restriction that for each query 9(F jE)[x1; x2], all paths from a basic event in Eto a basic event in F have at least one basic event in common is crucial for the deductiontechnique of Section 4. It assures that the problem of computing the tight answer to acomplete query can be reduced to the problems of computing the tight answer to a premise-restricted complete query and the tight answer to a strongly conclusion-restricted completequery. Note that this restriction is trivially satis ed by all premise- and conclusion-restrictedqueries (for example, by all the queries in Section 2.3).Especially tight answers to conclusion-restricted queries seem to be quite important inpractice. They may be used to characterize the probability of uncertain basic events givena collection of basic events that are known with certainty.206\nProbabilistic Deduction with Conditional Constraints over Basic Events3. Probabilistic Satis abilityIn this section, we show that conditional constraint trees have the nice property that theyare always satis able. That is, within conditional constraint trees, the user is preventedfrom specifying inconsistent probabilistic knowledge.First, note that conditional constraint trees always have a trivial model in which theprobability of the conjunction of all negated basic events is one and in which the probabilityof all the other atomic events is zero.The next lemma shows that, given a model Pr of a conditional constraint tree and areal number s from [0; 1], we can construct a new model Pr s by setting Pr s(A) = s Pr(A)for all atomic events A that are di erent from the conjunction of all negated basic events.Note that Pr0 coincides with the trivial model and that Pr1 is identical to Pr . This lemmais crucial for inductively constructing models of conditional constraint trees.Lemma 3.1 Let (B;KB) be a conditional constraint tree with B = fB1; B2; : : : ; Bng. LetPr be a model of KB and let s be a real number from [0; 1].The mapping Prs : AB ! [0; 1] with Prs(A) = s Pr(A) for all A 2 AB n fB1B2 Bngand Prs(B1B2 Bn) = s Pr(B1B2 Bn) s+ 1 is a model of KB.Proof. We easily verify that Prs is a probabilistic interpretation. It remains to show thatPrs is also a model of KB . Let (HjG)[u1; u2] 2 KB . Since Pr is a model of KB , we havePr j= (HjG)[u1; u2], hence u1 Pr(G) Pr(GH ) u2 Pr(G), and thus also u1s Pr(G) s Pr(GH ) u2s Pr(G). Since neither B1B2 Bn ) G nor B1B2 Bn ) GH , we getu1 Prs(G) Prs(GH ) u2 Prs(G) and thus Prs j= (HjG)[u1; u2]. 2Finally, the following theorem shows that conditional constraint trees always have anontrivial model in which all the basic events have a probability greater than zero.Theorem 3.2 Let (B;KB) be a conditional constraint tree with B = fB1; B2; : : : ; Bng.There is a model Pr of KB with Pr(B1B2 Bn) > 0.Proof. It is su cient to show the claim for exact conditional constraint trees. The claimis proved by induction on the number of basic events.Basis: for (B;KB) = (fBg; ;), a model Pr of KB with Pr(B) > 0 is given by B;B 7! 0; 1(note that B;B 7! 0; 1 is an abbreviation for Pr(B) = 0 and Pr(B) = 1).Induction: let (B;KB) = (B1 [B2;KB1 [KB2) with two exact conditional constraint trees(B1;KB1) = (fB;Cg; f(CjB)[u; u]; (BjC)[v; v]g) and (B2;KB2) = (fC;D1; : : : ;Dkg;KB2)such that B1 \\ B2 = fCg. A model Pr1 of KB1 with Pr1(BC ) > 0 is given by:BC;BC;BC;BC 7! uvu+v ; v uvu+v ; u uvu+v ; uvu+v :By the induction hypothesis, there is a model Pr2 of KB2 (that is de ned on the atomicevents over B2) with Pr2(CD1 Dk)> 0. By Lemma 3.1, we can assume Pr2(C) = Pr1(C).A probabilistic interpretation Pr on the atomic events over B is now de ned by:Pr(AbAcA2) = Pr1(AbAc) Pr2(AcA2)Pr2(Ac)207\nLukasiewiczfor all atomic events Ab, Ac, and A2 over fBg, fCg, and B2 n fCg, respectively. We easilyverify that Pr(AbAc) = Pr1(AbAc) and Pr(AcA2 ) = Pr2(AcA2 ) for all atomic eventsAb, Ac, and A2 over fBg, fCg, and B2 n fCg, respectively. Hence, Pr is a model of KB .Moreover, Pr1(BC ) > 0 and Pr2(CD1 Dk) > 0 entails Pr(BCD1 Dk) > 0. 24. Probabilistic DeductionIn this section, we present techniques for computing tight answers to queries directed toexact and general conditional constraint trees, and we analyze their computational com-plexity. More precisely, the problem of computing the tight answer to a query is reduced tothe problem of computing the tight answer to a complete query. The latter problem is thenreduced to the problems of computing the tight answer to a premise-restricted completequery and the tight answer to a strongly conclusion-restricted complete query.4.1 Premise-Restricted Complete Queries4.1.1 Exact Conditional Constraint TreesWe now focus on the problem of computing tight answers to premise-restricted completequeries that are directed to exact conditional constraint trees.Let (B;KB) be an exact conditional constraint tree and let 9(F jE)[x1; x2] be a premise-restricted complete query. To compute the tight answer to 9(F jE)[x1; x2], we start byde ning a directed tree (that is, a directed acyclic graph in which each node has exactlyone parent, except for the root, which does not have any):A! B i A$ B and A is closer to E than B.This directed tree (B;!) is uniquely determined by the conditional constraint tree and thepremise-restricted complete query. Fig. 3 shows (B;!) for the premise-restricted completequery 9(QRSTUjM)[x1; x2] to the exact conditional constraint tree in Fig. 2, left side.Now, the set of nodes B is partitioned into several strata. The lowest stratum containsonly nodes with no children in (B;!), the highest stratum contains the nodes with noparents in (B;!) (that is, exactly the node of the premise E of the query). Fig. 3 alsoshows the di erent strata in our example.At each node of (B;!), we compute certain tightest bounds that are logically entailedby KB . More precisely, the tightest bounds at a node B are computed locally, by exploitingthe tightest bounds that have previously been computed at the children of B. Hence, weiteratively compute the tightest bounds at the nodes of each stratum, starting with thenodes of the lowest stratum and terminating with the nodes of the highest stratum. Wedistinguish three di erent ways of computing tightest bounds at a node: initialization of a leaf (Leaf), chaining of an arrow and a subtree via a common node (Chaining), fusion of subtrees via a common node (Fusion).Let us consider again the premise-restricted complete query 9(QRSTUjM)[x1; x2] to theexact conditional constraint tree in Fig. 2, left side. Fig. 4 illustrates the three di erent ways208\nProbabilistic Deduction with Conditional Constraints over Basic Events\nM N\nO\nQ R\nS T U P\n3 2 1 0 strata4 Figure 3: Directed Tree (B;!)of computing tightest bounds at a node (the common nodes for Chaining and Fusion are lled black). Table 1 shows the greatest lower and the least upper bounds that are computedat each node B of each stratum. More precisely, these bounds are 1 = inf Pr(BD)=Pr (B), 2 = supPr(BD)=Pr (B), 2 = supPr(BD)=Pr (B), and 2 = supPr(D)=Pr(B) subject toPr j= KB and Pr(B) > 0. Table 1 also shows the requested tight answer fx1=0:02; x2=0:17g,which is given by the tightest bounds 1 and 2 that are computed at the premise M.strata B D 1 2 2 2S S 1:0000 1:0000 0:0000 1:0000 (Leaf)0 T T 1:0000 1:0000 0:0000 1:0000 (Leaf)U U 1:0000 1:0000 0:0000 1:0000 (Leaf)P S 0:8500 0:8500 0:0447 0:8947 (Chaining)1 P T 0:8500 0:8500 0:0447 0:8947 (Chaining)P U 0:8500 0:8500 0:0000 0:8500 (Chaining)P STU 0:5500 0:8500 0:0000 0:8500 (Fusion)1 Q Q 1:0000 1:0000 0:0000 1:0000 (Leaf)R R 1:0000 1:0000 0:0000 1:0000 (Leaf)O STU 0:4474 0:7605 0:0447 0:7605 (Chaining)2 O Q 0:9500 0:9500 0:0500 1:0000 (Chaining)O R 0:9500 0:9500 5:3833 6:3333 (Chaining)2 O QRSTU 0:3474 0:7605 0:0447 0:7605 (Fusion)3 N QRSTU 0:1911 0:4183 0:0246 0:4183 (Chaining)4 M QRSTU 0:0169 0:1722 0:0719 0:1722 (Chaining)Table 1: Locally Computed Tightest Bounds209\nLukasiewicz\nProbabilistic Deduction with Conditional Constraints over Basic EventsWe now focus on the technical details. We present the functions H 1 , H 2 , H 2 , and H 2 ,which compute the described greatest lower and least upper bounds. For this purpose, weneed the following de nitions. Let Pr(C jB) denote u for all (CjB)[u; u] 2 KB .A node B is a leaf if it does not have any children. For all leaves B, let B\" = B. For allthe other nodes B, let B\" be the conjunction of all the children of B. For all leaves C, letL(C) = C. For all the other conjunctive events C, let L(C) be the conjunction of all theleaves that are in C or that are descendants of a node in C.In the sequel, let B be a node and let C = B\". The case C = B refers to the initializationof the leaf B, the case C = B1 with a node B1 6= B to the chaining of the arrow B ! B1and a subtree via the common node B1, and the case C = B1B2 : : : Bk with k > 1 nodesB1; B2; : : : ; Bk to the fusion of k subtrees via the common node B.We de ne the function H 1 for computing greatest lower bounds: let H 1 (B;C) = 1(note that 1 will coincide with the greatest lower bound of Pr(BL(C )) =Pr (B) subject toPr j= KB and Pr(B) > 0), where 1 in Leaf (C = B), Chaining (C = B1), and Fusion(C = B1B2 : : : Bk with k > 1) is given as follows:Leaf: 1 = 1Chaining: 1 = max(0;Pr (C jB) (1 + H 1 (C;C\") 1Pr(B jC) ))Fusion: 1 = max(0; 1 k + kPi=1H 1 (B;Bi))To express that H 1 computes greatest lower bounds, we need the following de nitions.Let B(B;C) comprise B, all nodes in C and all descendants of a node in C. Let KB(B;C)be the set of all conditional constraints of KB over B(B;C). Let Mo(B;C) be the set of allmodels of KB(B;C) that are de ned on the atomic events over B(B;C).Now, the function H 1 is sound and globally complete with respect to B and C i H 1 (B;C) = 1 is the greatest lower bound of Pr(BL(C )) =Pr (B) subject to Pr 2 Mo(B;C)and Pr(B) > 0. Thus, the next theorem shows soundness and global completeness of H 1 .Theorem 4.1a) For all probabilistic interpretations Pr 2 Mo(B;C), it holds 1 Pr(B) Pr(BL(C)).b) There exists a probabilistic interpretation Pr 2 Mo(B;C) with Pr(B) > 0, 1 Pr(B) =Pr(BL(C )), and Pr(BL(C)) = 0 i L(C))B.Proof. The proof is given in full detail in Appendix B. 2Next, we present the functions H 2 , H 2 , and H 2 for computing least upper bounds. Notethat H 2 , H 2 , and H 2 show the crucial result that for exact conditional constraint trees,there are local probabilistic deduction techniques that are sound and globally complete.211\nLukasiewiczIn detail, let H 2 (B;C) = 2, H 2 (B;C) = 2, and H 2 (B;C) = 2 (note that 2, 2,and 2 will coincide with the least upper bound of Pr(BL(C )) =Pr (B), Pr(BL(C )) =Pr (B),and Pr(L(C )) =Pr (B), respectively, subject to Pr j= KB and Pr(B) > 0), where 2, 2,and 2 in Leaf (C = B), Chaining (C = B1), and Fusion (C = B1B2 : : : Bk with k > 1)are given as follows:Leaf: 2 = 1 2 = 0 2 = 1Chaining: 2 = min(1;Pr (C jB) H 2 (C;C\")Pr(BjC) ; 1 Pr(C jB) (1 H 2 (C;C\")Pr(BjC) );Pr(C jB) (1 + H 2 (C;C\")Pr(BjC) )) 2 = min(Pr(C jB) (H 2 (C;C\")+1Pr(BjC) 1); Pr(C jB) H 2 (C;C\")Pr(BjC) ) 2 = Pr(C jB) H 2 (C;C\")Pr(BjC)Fusion: 2 = mini2[1:k]H 2 (B;Bi) 2 = mini2[1:k]H 2 (B;Bi) 2 = min( mini2[1:k]H 2 (B;Bi); mini;j2[1:k];i 6=j(H 2 (B;Bi) +H 2 (B;Bj)))The functions H 2 , H 2 , and H 2 are sound and globally complete with respect to Band C i H 2 (B;C) = 2, H 2 (B;C) = 2, and H 2 (B;C) = 2 are the least upper boundsof Pr(BL(C )) =Pr (B), Pr(BL(C )) =Pr(B), and Pr(L(C )) =Pr (B), respectively, subjectto Pr 2 Mo(B;C) and Pr(B) > 0. Hence, the following theorem shows soundness andglobal completeness of H 2 , H 2 , and H 2 (actually, it shows even more to enable a proof byinduction on the recursive de nition of H 2 , H 2 , and H 2 ).Theorem 4.2a) For all probabilistic interpretations Pr 2 Mo(B;C), it holds Pr(BL(C)) 2 Pr(B),Pr(BL(C)) 2 Pr(B), and Pr(L(C)) 2 Pr(B).b) There exists a probabilistic interpretation Pr 2 Mo(B;C) with Pr(B) > 0, Pr(BL(C)) = 2 Pr(B), and Pr(L(C)) = 2 Pr(B).c) There exists a probabilistic interpretation Pr 2 Mo(B;C) with Pr(B) > 0, Pr(BL(C)) = 2 Pr(B), and Pr(L(C)) = 2 Pr(B). 212\nProbabilistic Deduction with Conditional Constraints over Basic EventsProof. The proof is given in full detail in Appendix B. 2Note that Theorem 4.2 also shows that H 2 (B;Bi) H 2 (B;Bi) + H 2 (B;Bi) for alli 2 [1 :k]. Thus, the expression mini;j2[1:k];i 6=j(H 2 (B;Bi) + H 2 (B;Bj)) in the de nition of 2 in Fusion can be replaced by 2 + 2 for an increased e ciency in computing 2 byexploiting the already computed values of 2 and 2.Brie y, by Theorems 4.1 and 4.2, the tight answer to the premise-restricted completequery 9(F jE)[x1; x2] is given by fx1=H 1 (E;E\"); x2=H 2 (E;E\")g.4.1.2 Conditional Constraint TreesWe now focus on computing the tight answer to premise-restricted complete queries togeneral conditional constraint trees. In the sequel, let (B;KB) be a conditional constrainttree and let 9(F jE)[x1; x2] be a premise-restricted complete query.We may think that the local deduction technique for exact conditional constraint treesof Section 4.1.1 can easily be generalized to conditional constraint trees. In fact, this is trueas far as the computation of greatest lower bounds is concerned. However, the computationof least upper bounds cannot be generalized that easily from exact conditional constrainttrees to conditional constraint trees. More precisely, generalizing the computation of leastupper bounds results in solving nonlinear programs. These nonlinear programs and our wayto solve them are illustrated by the following chaining example.Let the conditional constraint tree (B;KB) be given by B=fM;N;O;Pg and KB =f(NjM)[u1; u2]; (MjN)[v1; v2], (OjN)[x1; x2], (NjO)[y1; y2], (PjO)[r1; r2], (OjP)[s1; s2]g. Let usconsider the premise-restricted complete query 9(PjM)[z1; z2].By Theorem 4.2 and some straightforward arithmetic transformations, the requestedleast upper bound is the maximum of z subject to u 2 [u1; u2], v 2 [v1; v2], x 2 [x1; x2],y 2 [y1; y2], r 2 [r1; r2], s 2 [s1; s2], and the nonlinear inequalities in (1) to (5):z 1(1) z 1 u+ uv uxv + uxrvy(2) z 1 u+ uxv uxrvy + uxrvys(3) z u uxv + uxvy uxrvy + uxrvys(4) z uxrvys(5)In this system of nonlinear inequalities, all upper bounds of z are monotonically decreasingin v, y, and s. Hence, we can equivalently maximize z subject to u 2 [u1; u2], x 2 [x1; x2],r 2 [r1; r2], and the nonlinear inequalities in (6) to (10):z 1(6) z 1 u+ uv1 uxv1 + uxrv1y1(7) z 1 u+ uxv1 uxrv1y1 + uxrv1y1s1(8) z u uxv1 + uxv1y1 uxrv1y1 + uxrv1y1s1(9) z uxrv1y1s1(10)For example, the requested least upper bound for u1 = u2 = u and x1 = x2 = x is shown inFig. 5 for u; x 2 [0; 1], r1 = r2 = 0:15, v1 = 0:8, y1 = 0:8, and s1 2 f0:05; 0:1g. The requestedleast upper bound for u1 < u2 or x1 < x2 is the maximum value over [u1; u2] [x1; x2].213\nProbabilistic Deduction with Conditional Constraints over Basic EventsWe now transform this nonlinear program into an equivalent linear program (by re-placing 1, u, ux, and uxr by the new variables xM, xN, xO, and xP, respectively). Moreprecisely, the maximum of z subject to u 2 [u1; u2], x 2 [x1; x2], r 2 [r1; r2], and the non-linear inequalities in (6) to (10) coincides with the maximum of z subject to the followingsystem of linear inequalities over z and xB 0 (B 2 B):z xMz xM + 1 v1v1 xN y1v1y1 xO + s1v1y1s1 xPz xM v1v1 xN + y1v1y1 xO + 1 s1v1y1s1 xPz v1v1 xN + 1 y1v1y1 xO + 1 s1v1y1s1 xPz 1v1y1s1 xP 1 xM 1u1 xM xN u2 xMx1 xN xO x2 xNr1 xO xP r2 xOMore generally, tight upper answers to premise-restricted complete queries to conditionalconstraint trees can be computed by solving similar nonlinear programs, which can similarlybe transformed into linear programs.For example, let us consider the premise-restricted complete query 9(QRSTUjM)[x1; x2]to the conditional constraint tree in Fig. 2, right side. The requested least upper boundis the maximum of x subject to the system of linear inequalities in Fig. 6 (we actuallygenerated 72 linear inequalities of which 31 were trivially subsumed by others). Note thatthe nine variables xM to xU correspond to the nine nodes M to U.x xMx 2518xQx 252 xRx 2518xSx 2518xTx 2518xUx xN + 19xPx xN + 19xQx xN + 19xRx 54xO + 536xPx 54xO + 536xQx 54xO + 454 xR x 54xP + 536xQx 536xP + 54xQx 536xP + 54xRx 536xQ + 54xRx xM xN + 54xO + 536xRx xM + 14xN 54xO + 54xPx xM + 14xN 54xO + 54xQx xM + 14xN 54xO + 54xRx xM + 14xN 54xP + 2518xSx xM + 14xN 54xP + 2518xTx xM + 14xN 54xP + 2518xU\n1 xM 1310xM xN 25xM12xN xO 35xN910xO xR xO910xO xQ xO45xO xP 910xO45xP xS 910xP45xP xT 910xP45xP xU 910xPFigure 6: Generated Linear Inequalities in the Chaining ExampleThus, in this example, the tight upper answer is computed by solving a linear programthat has 10 variables and 72 linear inequalities. Note that computing the tight upper answerby the classical linear programming approach would result in solving a linear program thathas 29 = 512 variables and 4 9 2 = 34 linear inequalities (see Section 4.6).215\nLukasiewiczLet us now focus on the technical details. We subsequently generalize the function H 1of Section 4.1.1 in a straightforward way to compute greatest lower bounds in conditionalconstraint trees. Moreover, we present a linear program for computing the requested leastupper bound in conditional constraint trees.Let Pr1(C jB) denote u1 for all (CjB)[u1; u2] 2 KB . In the sequel, let B be a node andlet C = B\". Again, the cases C = B, C = B1 with a node B1 6= B, and C = B1B2 : : : Bkwith k > 1 nodes B1; B2; : : : ; Bk refer to Leaf, Chaining, and Fusion, respectively.We de ne the generalized function H 1 for computing greatest lower bounds in con-ditional constraint trees: let H 1 (B;C) = 1 (note that 1 will coincide with the greatestlower bound of Pr(BL(C )) =Pr (B) subject to Pr j= KB and Pr(B) > 0), where 1 in Leaf(C = B), Chaining (C = B1), and Fusion (C = B1B2 : : : Bk with k > 1) is given by:Leaf: 1 = 1Chaining: 1 = max(0;Pr 1(C jB) (1 + H 1 (C;C\") 1Pr1(B jC) ))Fusion: 1 = max(0; 1 k + kPi=1H 1 (B;Bi))H 1 is sound and globally complete with respect to B and C i H 1 (B;C) = 1 is thegreatest lower bound of Pr(BL(C )) =Pr (B) subject to Pr 2 Mo(B;C) and Pr(B) > 0.Thus, the next theorem shows soundness and global completeness of H 1 .Theorem 4.3a) For all probabilistic interpretations Pr 2 Mo(B;C), it holds 1 Pr(B) Pr(BL(C)).b) There exists a probabilistic interpretation Pr 2 Mo(B;C) with Pr(B) > 0, 1 Pr(B) =Pr(BL(C )), and Pr(BL(C)) = 0 i L(C))B.Proof. The claims follow from Theorem 4.1. 2Next, we focus on the requested least upper bound, which is computed by solving alinear program as described in the two examples.We start by de ning the functions I , I , and I over the variables xB (B 2 B). LetI (B;C) = 2, I (B;C) = 2, and I (B;C) = 2, where 2, 2, and 2 in Leaf (C = B),Chaining (C = B1), and Fusion (C = B1B2 : : : Bk with k > 1) are given as follows:Leaf: 2 = xB 2 = 0 2 = xB 216\nProbabilistic Deduction with Conditional Constraints over Basic EventsChaining: 2 = min(xB; I (C;C\")Pr1(BjC) ; xC + I (C;C\")Pr1(BjC) ; xB xC + I (C;C\")Pr1(BjC) ) 2 = min(1 Pr1(BjC)Pr1(BjC) xC + I (C;C\")Pr1(BjC) ; I (C;C\")Pr1(BjC) ) 2 = I (C;C\")Pr1(BjC)Fusion: 2 = mini2[1:k] I (B;Bi) 2 = mini2[1:k] I (B;Bi) 2 = min( mini2[1:k] I (B;Bi); mini;j2[1:k];i 6=j(I (B;Bi) + I (B;Bj)))The system of linear inequalities J(B;C) is de ned as the least set of linear inequalitiesover xG 0 (G 2 B(B;C)) that contains 1 xB 1 and u1 xG xH u2 xG for all(HjG)[u1; u2] 2 KB(B;C) with G! H (that is, G is the parent of H).The intuition behind these de nitions can now be described as follows.Each xG (G 2 B(B;C)) that satis es J(B;C) corresponds to the exact conditional con-straint tree (B(B;C);KB 0(B;C)), whereKB 0(B;C) contains the pair (HjG)[xH=xG; xH=xG]and (GjH)[v1; v1] for each pair (HjG)[u1; u2]; (GjH)[v1; v2] 2 KB(B;C) with G! H.We will show that the least upper bound of Pr(BL(C))=Pr (B), Pr(BL(C))=Pr (B),and Pr(L(C))=Pr (B) subject to Pr j= KB 0(B;C) and Pr(B) > 0 is given by I (B;C),I (B;C), and I (B;C), respectively. It will then follow that the least upper bound ofPr(BL(C))=Pr(B), Pr(BL(C))=Pr(B), and Pr(L(C))=Pr (B) subject to Pr j= KB(B;C)and Pr(B) > 0 is given by the maximum of I (B;C), I (B;C), and I (B;C), respectively,subject to all xG (G 2 B(B;C)) satisfying J(B;C).That is, we implicitly performed the variable transformation described in the two ex-amples. This transformation is indeed correct for conditional constraint trees:Lemma 4.4a) If xG (G 2 B(B;C)) satis es J(B;C), then for all conditional constraints (HjG)[u1; u2] 2KB(B;C) such that G! H, there exists uH 2 [u1; u2] with xH = uH xG .b) Let uH 2 [u1; u2] for all (HjG)[u1; u2] 2 KB(B;C) such that G ! H. There exists xG(G 2 B(B;C)) with J(B;C) and xH = uH xG for all nodes H with parent G.Proof. a) For all nodes H with parent G, let uH be de ned by uH = xH = xG.b) Let xB = 1, and for all nodes H with parent G, let xH be de ned by xH = uH xG. 2We are now ready to formulate an optimization problem for computing the requestedleast upper bound.Theorem 4.5 Let X2 be the maximum of x subject to x I (E;E\") and J(E;E\").a) Pr(EL(E\")) X2 Pr(E) for all Pr 2 Mo(E;E\").b) There exists Pr 2 Mo(E;E\") with Pr(E) > 0 and Pr(EL(E \")) = X2 Pr(E).217\nLukasiewiczProof. Let Pr(BjC) = v1 for all (BjC)[v1; v2] 2 KB such that B ! C. By Theorem 4.2,the requested least upper bound is the maximum of x subject to x H 2 (E;E\") andPr(CjB) = uC 2 [u1; u2] for all (CjB)[u1; u2] 2 KB such that B ! C. By Lemma 4.4, wecan equivalently maximize x subject to x I (E;E\") and J(E;E\"). 2We now wonder how to solve the generated optimization problem, since I (E;E\") maystill contain min-operations that cannot be tackled by linear programming. Moreover, givena method for solving this optimization problem, we are also interested in a rough idea onthe overall time complexity of computing the requested least upper bound this way. Finally,we are interested in possible improvements to increase e ciency. These topics are discussedin the rest of this section.If I (E;E\") does not contain any min-operations at all, then the generated optimizationproblem is already a linear program. Otherwise, it can easily be transformed into a linearprogram. In a rst transformation step, all inner min-operations are eliminated. This caneasily be done due to the well-structuredness of I (E;E\"). In a second step, the onlyremaining outer min-operation is eliminated by introducing exactly one linear inequalityfor each contained operand. In these linear inequalities, the operands of the outer min-operation are upper bounds of x.To get a rough idea on the time complexity of computing the requested least upperbound this way, we must analyze the size of the generated linear programs. It is givenby the number of variables, the number of linear inequalities in J(E;E\"), and the num-ber of linear inequalities extracted from x I (E;E\"). The latter is quite worrying,since I (B;C) in Fusion seems to produce many min-operands. Moreover, I (B;C) inFusion contains I (B;Bi), and I (B;C) in Chaining contains I (C;C\"). So, due tothis crossed dependency, the overall number of generated linear inequalities is likely to`explode' for trees that branch very often.To avoid these problems, we introduce the auxiliary functions J , J , and J over thevariables xB (B 2 B). Let J (B;C) = 02, J (B;C) = 02, and J (B;C) = 02, where 02, 02,and 02 in Leaf (C = B), Chaining (C = B1), and Fusion (C = B1B2 : : : Bk with k > 1)are given as follows:Leaf: 02 = xB 02 = 0 02 = xBChaining: 02 = min(xB; xC + J (C;C\")Pr1(BjC) ; xB xC + J (C;C\")Pr1(BjC) ) 02 = 1 Pr1(BjC)Pr1(BjC) xC + J (C;C\")Pr1(BjC) 02 = J (C;C\")Pr1(BjC) 218\nProbabilistic Deduction with Conditional Constraints over Basic EventsFusion: 02 = mini2[1:k]J (B;Bi) 02 = mini2[1:k]J (B;Bi) 02 = min( mini2[1:k]J (B;Bi); mini;j2[1:k];i 6=j(J (B;Bi) + J (B;Bj)))Note that 02 in Chaining can be separated into the cases C\" = C and C\" 6= C. Sincesimply 02 = xC for C\" = C, we reduce the number of generated linear inequalities this way.The next lemma shows that the functions I , I , and I can be expressed in terms ofthe auxiliary functions J , J , and J .Lemma 4.6 For all xB (B 2 B) that satisfy J(E;E\"): 2 = min( 02; 02), 2 = min( 02; 02), and 2 = 02 :Proof sketch. The claim can be proved by induction on the recursive de nition of thefunctions I , I , and I . 2Brie y, by Theorem 4.3, Theorem 4.5, and Lemma 4.6, the tight answer to the premise-restricted complete query 9(F jE)[x1; x2] is given by fx1=H 1 (E;E\"); x2=X2g, where X2 isthe maximum of x subject to x J (E;E\"), x J (E;E\"), and J(E;E\").In our example, we get fx1=0:00; x2=0:27g as the tight answer to the premise-restrictedcomplete query 9(QRSTUjM)[x1; x2] to the conditional constraint tree in Fig. 2, right side.The time complexity of computing the requested greatest lower bound and especiallythe requested least upper bound this way is analyzed in Section 4.5.4.2 Strongly Conclusion-Restricted Complete QueriesWe now focus on computing the tight answer to strongly conclusion-restricted completequeries to general conditional constraint trees. In the sequel, let (B;KB) be a conditionalconstraint tree and let 9(F jE)[x1; x2] be a strongly conclusion-restricted complete query.The tight upper answer to 9(F jE)[x1; x2] is always given by fx2=1g. To compute thetight lower answer to 9(F jE)[x1; x2], we rst compute the tight lower answer fy1=u1g to thepremise-restricted complete query 9(EjF )[y1; y2]. We then distinguish the following cases:If u1 > 0, then the tight lower answer to 9(F jE)[x1; x2] is computed locally by a functionH 1 (like the tight lower answer to premise-restricted complete queries in Section 4.1.2).If u1 = 0 and E ) F , then the tight lower answer to 9(F jE)[x1; x2] is given by fx1=1g.Otherwise, the tight lower answer to 9(F jE)[x1; x2] is given by fx1=0g.We now focus on the technical details. Let (B;!) be the directed graph that belongsto the premise-restricted complete query 9(EjF )[y1; y2] (see Section 4.1.1). Let Pr1(BjC)denote v1 for all (BjC)[v1; v2] 2 KB . In the sequel, let B be a node and let C = B\". Again,the cases C = B, C = B1 with a node B1 6= B, and C = B1B2 : : : Bk with k > 1 nodesB1; B2; : : : ; Bk refer to Leaf, Chaining, and Fusion, respectively.We de ne the functionH 1 for computing greatest lower bounds in the case H 1 (B;C) > 0as follows. Let H 1 (B;C) = 1 (note that 1 will coincide with the greatest lower boundof Pr(BL(C )) =Pr (L(C)) subject to Pr j= KB and Pr(L(C)) > 0), where 1 in Leaf219\nLukasiewicz(C = B), Chaining (C = B1), and Fusion (C = B1B2 : : : Bk with k > 1) is given asfollows (note that H 1 (C;C\") and H 1 (B;Bi) are de ned like in Section 4.1.2):Leaf: 1 = 1Chaining: 1 = H 1 (C;C\") (1 + Pr1(BjC) 1H 1 (C;C\") )Fusion: 1 = 0BB@1 + mini2[1:k]H 1 (B;Bi) (1=H 1 (B;Bi) 1)1 k+ kPi=1H 1 (B;Bi) 1CCA 1By induction on the de nition of H 1 , it is easy to see that H 1 (B;C) > 0 entails that 1is de ned and that 1 > 0 (note that H 1 (B;C) = 1 in Leaf, Chaining, and Fusion isde ned like in Section 4.1.2). In this case, H 1 is sound and globally complete with respectto B and C i H 1 (B;C) = 1 is the greatest lower bound of Pr(BL(C )) =Pr (L(C)) subjectto Pr 2 Mo(B;C) and Pr(L(C)) > 0. Thus, the next theorem shows soundness and globalcompleteness of H 1 . It also shows that, for C = B1B2 : : : Bk with k > 1, the least upperbound of Pr(BL(C )) =Pr (L(C)) subject to Pr 2 Mo(B;C) and Pr(L(C)) > 0 is given by 1.Theorem 4.7a) If 1 > 0, then for all Pr 2 Mo(B;C), it holds 1 Pr(L(C)) Pr(BL(C)).b) If 1 > 0, then there is a probabilistic interpretation Pr 2 Mo(B;C) with Pr(B) > 0,Pr(L(C)) > 0, 1 Pr(L(C)) = Pr(BL(C)), and 1 Pr(B) = Pr(BL(C )).c) If 1 > 0 and C = B1B2 : : : Bk with k > 1, then there is some Pr 2 Mo(B;C) withPr(B) > 0, Pr(L(C)) > 0, 1 Pr(L(C)) = Pr(BL(C)), and 1 Pr(B) = Pr(BL(C )).d) If 1=0 and C =B1B2 : : : Bk with k > 1, then for each \"> 0 there is some Pr 2Mo(B;C)with Pr(B) > 0, Pr(L(C)) > 0, 1 Pr(L(C)) = Pr(BL(C)), and \" Pr(B) Pr(BL(C )).Proof. The proof is given in full detail in Appendix C. 2We are now ready to give the following characterization of tight answers to stronglyconclusion-restricted complete queries to conditional constraint trees.Theorem 4.8 Let (B;KB) be a conditional constraint tree and let 9(F jE)[x1; x2] be astrongly conclusion-restricted complete query. Let the tight lower answer to the premise-restricted complete query 9(EjF )[y1; y2] be given by fy1=u1g.(1) If u1 > 0, then the tight answer to 9(F jE)[x1; x2] is given by fx1=H 1 (F; F \"); x2=1g.(2) If u1 = 0 and E ) F , then the tight answer to 9(F jE)[x1; x2] is given by fx1=1; x2=1g.(3) Otherwise, the tight answer to 9(F jE)[x1; x2] is given by fx1=0; x2=1g.Proof. The proof is given in full detail in Appendix C. 2220\nProbabilistic Deduction with Conditional Constraints over Basic Events4.3 Complete QueriesWe now show that the problem of computing tight answers to complete queries can bereduced to the problems of computing tight answers to premise-restricted complete queriesand of computing tight answers to strongly conclusion-restricted complete queries.In detail, a complete query is premise-restricted, it is strongly conclusion-restricted,or it can be reduced to premise-restricted complete queries and to strongly conclusion-restricted complete queries. For example, given the complete query 9(STUjMQR)[x1; x2]to the conditional constraint tree in Fig. 2, right side, we rst compute the tight answerfy1=u1; y2=u2g to the premise-restricted complete query 9(MQRjO)[y1; y2] (directed to thecorresponding subtree) and the tight answer fz1=v1; z2=v2g to the strongly conclusion-re-stricted complete query 9(OjMQR)[z1; z2] (directed to the corresponding subtree). We thengenerate a new conditional constraint tree by replacing the subtree over the nodes M, N,O, Q, and R by the pair of conditional constraints (BjO)[u1; u2] and (OjB)[v1; v2] over thenodes B and O (note that B represents MQR). Finally, we compute the tight answer to thepremise-restricted complete query 9(STUjB)[x1; x2] to the new conditional constraint tree.Note that this reduction can always be done, since for each query 9(F jE)[x1; x2], allpaths from a basic event in E to a basic event in F have at least one basic event in common.Theorem 4.9 Let (B;KB) be a conditional constraint tree and let 9(F jE)[x1; x2] be a com-plete query that is not premise-restricted and not strongly conclusion-restricted.a) There exists a basic event G 2 B and two conditional constraint trees (B1;KB1) and(B2;KB2) such that B1 \\ B2 = fGg, B1 [ B2 = B, and 9(GjE)[z1; z2] is a strongly conclu-sion-restricted complete query to (B1;KB1).b) Let the tight answer to the premise-restricted complete query 9(EjG)[y1; y2] to (B1;KB1)be given by fy1=u1; y2=u2g and let the tight answer to the strongly conclusion-restrictedcomplete query 9(GjE)[z1; z2] to (B1;KB1) be given by fz1=v1; z2=v2g.(1) If u1 > 0, then also v1 > 0 and the tight answer to the complete query 9(F jE)[x1; x2]to (B;KB) coincides with the tight answer to the premise-restricted complete query9(F jB)[x1; x2] to (B2 [ fBg;KB2 [ f(BjG)[u1; u2]; (GjB)[v1; v2]g), where B is a newbasic event with B 62 B2. In particular, for exact conditional constraint trees (B;KB),the tight answer to the complete query 9(F jE)[x1; x2] is given by:fx1=max(0; v1 v1u1 + v1s1u1 ); x2=min(1; 1 v1 + v1s2u1 ; t2t2 s2+u1 )g ;where s1 = H 1 (G;G\"), s2 = H 2 (G;G\"), and t2 = H 2 (G;G\") (note that H 1 , H 2 , andH 2 are de ned like in Section 4.1.1).(2) If u1=0, v1=1, and G)F , then the tight answer to the complete query 9(F jE)[x1; x2]to (B;KB) is given by fx1=1; x2=1g.(3) Otherwise, the tight answer to the complete query 9(F jE)[x1; x2] to (B;KB) is givenby fx1=0; x2=1g.Proof. The proof is given in full detail in Appendix D. 2221\nLukasiewicz4.4 QueriesThe problem of computing tight answers to queries can be reduced to the more specializedproblem of calculating tight answers to complete queries.More precisely, given a query 9(F jE)[x1; x2] to a conditional constraint tree (B;KB),a complete query 9(F 0jE0)[x1; x2] to a conditional constraint tree (B0;KB 0) is generated by:1. While (B;KB) contains a leaf B that is not contained in EF : remove B from B andremove the corresponding pair (CjB)[u1; u2]; (BjC)[v1; v2] 2 KB from KB .2. While EF contains a basic event B that is not a leaf in (B;KB): increase B by a newbasic event B0, increase KB by the pair (B0jB)[1; 1] and (BjB0)[1; 1], and replace eachoccurrence of B in 9(F jE)[x1; x2] by the new basic event B0.It remains to show that the generated probabilistic deduction problem has the samesolution as the original probabilistic deduction problem:Theorem 4.10 The tight answer to the query 9(F jE)[x1; x2] to (B;KB) coincides with thetight answer to the complete query 9(F 0jE0)[x1; x2] to (B0;KB 0).Proof. Let (B00;KB 00) be the conditional constraint tree that is generated in step 1 and let(F jE)[u1; u2] be a tight logical consequence of KB 00. We now show that (F jE)[u1; u2] is alsoa tight logical consequence of KB . First, (F jE)[u1; u2] is a logical consequence of KB , sinceKB 00 is a subset of KB . Moreover, each model Pr 00 of KB 00 (that is de ned on all atomicevents over B00) can be extended to a model Pr of KB (that is de ned on all atomic eventsover B) with Pr(A) = s Pr 00(A) for all atomic events A over B00 that are di erent from theconjunction of all negated basic events in B00, where s is a real number from (0; 1]. Thismodel can be constructed inductively like in the proof of Theorem 3.2. Thus, for u 2 [u1; u2],Pr 00(E) > 0 and u Pr 00(E) = Pr 00(EF ) entails Pr(E) > 0 and u Pr(E) = Pr(EF ).Finally, (F jE)[u1; u2] is a tight logical consequence of KB 00 i (F 0jE0)[u1; u2] is a tightlogical consequence of KB 0, since we just introduce synonyms for basic events in step 2. 24.5 Computational Complexity4.5.1 Exact Conditional Constraint TreesWe now show that for exact conditional constraint trees, our technique to compute the tightanswer to queries runs in linear time in the number of nodes of the tree. In the sequel, let(B;KB) be an exact conditional constraint tree and let n denote its number of nodes.Lemma 4.11 The tight answer to a premise-restricted or strongly conclusion-restrictedcomplete query can be computed in linear time in n.Proof. For exact conditional constraint trees, our approach to compute the tight upperanswer to premise-restricted complete queries by H 2 , H 2 , and H 2 runs in time O(n):The directed tree can be computed in time O(n). An initialization of a leaf with aconstant number of assignments is performed exactly for each leaf of the directed tree, achaining with a constant number of arithmetic operations is performed exactly for eacharrow of the directed tree. Hence, initializing all leaves and performing all chainings runs222\nProbabilistic Deduction with Conditional Constraints over Basic Eventsin time O(n). A fusion is done for each branching of the directed tree, using linear time inthe number of branches. Thus, all fusions together run in time O(n).Even for general conditional constraint trees, the tight lower answer to premise-restrictedcomplete queries, and hence also the tight answer to strongly conclusion-restricted completequeries, is analogously computed in time O(n). 2Theorem 4.12 The tight answer to a query can be computed in linear time in n.Proof. We assume that the set of basic events B is totally ordered and that the basic eventsin the conjunctive events E and F of the query 9(F jE)[x1; x2] are written in this order.First, the query is reduced to a complete query according to Section 4.4. This reductioncan be done in time O(n). Now, if the generated complete query is premise-restricted orstrongly conclusion-restricted, then the claim follows immediately from Lemma 4.11.Otherwise, the generated complete query is reduced to premise-restricted and stronglyconclusion-restricted complete queries according to Section 4.3. Also this reduction can bedone in time O(n), since the basic event G in Theorem 4.9 a) is computable in time O(n).Hence, the claim follows from Theorem 4.9 and Lemma 4.11. Note that t2 = H 2 (G;G\") inTheorem 4.9 b) (1) can also be computed in time O(n). 24.5.2 Conditional Constraint TreesFor general conditional constraint trees, our technique to compute the tight lower answerto queries runs still in linear time, while our technique to compute the tight upper answerto queries runs in polynomial time in the number of nodes of the tree. In the sequel, let(B;KB) be a general conditional constraint tree and let n denote its number of nodes.Lemma 4.13a) The tight lower answer to a premise-restricted complete query and the tight answer to astrongly conclusion-restricted complete query can be computed in linear time in n.b) The tight upper answer to a premise-restricted complete query can be computed in poly-nomial time in n.Proof. a) The claim is already shown in the proof of Lemma 4.11.b) Our linear programming technique to compute the tight upper answer to premise-restricted complete queries runs in polynomial time in n:Linear programming runs in polynomial time in the size of the linear programs (Pa-padimitriou & Steiglitz, 1982; Schrijver, 1986), where the size of a linear program is givenby its number of variables and its number of linear inequalities.We now show that the size of our linear programs in Section 4.1.2 is polynomial in n.The number of variables is n + 1. The number of linear inequalities in J(E;E\") is 2n.By induction on the recursive de nition of J , J , and J , it can be shown that thenumber of min-operands in J (B;C), J (B;C), and J (B;C) is limited by jB(B;C)j2,jB(B;C)j, and jB(B;C)j4, respectively. Hence, the number of linear inequalities extractedfrom x J (E;E\") and x J (E;E\") is limited by jB(E;E\")j2 + jB(E;E\")j4 = n2 + n4.Thus, the overall number of generated linear inequalities l is limited by lu = 2n+ n2 + n4.223\nLukasiewiczFinally, note that lu is a very rough upper bound for l, in many conditional constrainttrees (especially in those that branch very rarely), l is much lower than lu. For example,taking a complete binary tree with n = 127 nodes, we get only l = 19 964 compared tolu = 260 161 024. In the example of Section 4.1.2 with n = 9 nodes, we get only l = 72compared to lu = 6660. Another example is a tree that is degenerated to a chain of basicevents. In this case, we even get l = 5n+ 1, that is, the overall number of generated linearinequalities is linear in n. 2Theorem 4.14a) The tight lower answer to a query can be computed in linear time in n.b) The tight upper answer to a query can be computed in polynomial time in n.Proof. We assume that the set of basic events B is totally ordered and that the basic eventsin the conjunctive events E and F of the query 9(F jE)[x1; x2] are written in this order.Like in the proof of Theorem 4.12, the query is reduced to a complete query accordingto Section 4.4. This reduction can be done in time O(n). Now, if the generated com-plete query is premise-restricted or strongly conclusion-restricted, then the claims followimmediately from Lemma 4.13.Otherwise, the generated complete query is reduced to premise-restricted and stronglyconclusion-restricted complete queries according to Section 4.3. Again, this reduction canbe done in time O(n), since the basic event G in Theorem 4.9 a) is computable in time O(n).Thus, the claims follow from Theorem 4.9 and Lemma 4.13. Note that in Theorem 4.9 b) (1),the tight lower answer to 9(F jB)[x1; x2] can be computed without u2 and v2. 24.6 Comparison with the Classical Linear Programming ApproachAs a comparison, we now brie y describe how probabilistic deduction in conditional con-straint trees can be done by the classical linear programming approach (Paa , 1988; vander Gaag, 1991; Amarger et al. 1991; Hansen et al. 1995). In the sequel, let 9(F jE)[x1; x2]be a query to an exact or general conditional constraint tree (B;KB) over n nodes.The tight answer to 9(F jE)[x1; x2] can be computed by solving two linear programs. Indetail, the requested greatest lower and least upper bound are given by the optimal valuesof the following two linear programs with xA 0 (A 2 AB) and opt 2 fmin;maxg:opt PA2AB; A)EF xA subject toPA2AB; A)E xA = 1PA2AB; A)GH xA u1 PA2AB; A)G xA for all (HjG)[u1; u2] 2 KBPA2AB; A)GH xA u2 PA2AB; A)G xA for all (HjG)[u1; u2] 2 KBThat is, the tight answer is computed by solving two linear programs with 2n variablesand 4n 2 linear inequalities. For example, the tight answer to the premise-restrictedcomplete query 9(QRSTUjM)[x1; x2] to the conditional constraint trees in Fig. 2 yields twolinear programs with 29 = 512 variables and 4 9 2 = 34 linear inequalities.224\nProbabilistic Deduction with Conditional Constraints over Basic EventsHence, if we now solve these two linear programs by the standard simplex method orthe standard interior-point technique, then we need immediately exponential time in n. Itis still an open question whether column generation techniques can help to solve the twolinear programs in less than exponential time in n in the worst case.5. Comparison with Bayesian NetworksIn this section, we brie y discuss the relationship between conditional constraint trees andBayesian networks (Pearl, 1988).A Bayesian network is de ned by a directed acyclic graph G over discrete random vari-ables X1;X2; : : : ;Xn as nodes and by a conditional probability distribution Pr(Xijpa(Xi))for each random variable Xi and each instantiation pa(Xi) of its parents pa(Xi). It speci esa unique joint probability distribution Pr over X1;X2; : : : ;Xn by:Pr(X1;X2; : : : ;Xn) = nYi=1Pr(Xijpa(Xi)) :That is, the joint probability distribution Pr is uniquely determined by the conditionaldistributions Pr(Xijpa(Xi)) and certain conditional independencies encoded in G.Hence, Bayesian trees (that is, Bayesian networks that have a directed tree as associateddirected acyclic graph) with only binary random variables seem to be very close to exactconditional constraint trees. However, exact and general conditional constraint trees areassociated with an undirected tree that does not encode any independencies! For this rea-son, exact and general conditional constraint trees describe convex sets of joint probabilitydistributions rather than single joint probability distributions.But, would it be possible to additionally assume certain independencies? Of course, witheach exact or general conditional constraint tree (B;KB), we can associate all probabilisticinterpretations Pr that are models of KB and that have additionally the undirected tree(B;$) as an I-map (Pearl, 1988). That is, we would have independencies without causaldirectionality like in Markov trees (Pearl, 1988). However, this idea does not carry usto a single probabilistic interpretation (neither for exact conditional constraint trees, norfor general conditional constraint trees), and it is an interesting topic of future research toinvestigate how the computation of tight answers in exact and general conditional constrainttrees changes under this kind of independencies (which yield tighter bounds, since theyreduce the number of models of exact and general conditional constraint trees).Finally, if we additionally x the probability of exactly one node, then an exact condi-tional constraint tree under the described independencies speci es exactly one probabilisticinterpretation (note that, to keep satis ability, the probability of a node must respect certainupper bounds, which are entailed by the exact conditional constraint tree). But, such exactconditional constraint trees are in fact Bayesian trees with only binary random variables.6. Summary and ConclusionsWe showed that globally complete probabilistic deduction with conditional constraints overbasic events is NP-hard. We then concentrated on the special case of probabilistic deduction225\nLukasiewiczin exact and general conditional constraint trees. We presented very e cient techniques forglobally complete probabilistic deduction. More precisely, for exact conditional constrainttrees, we presented a local approach that runs in linear time in the size of the conditionalconstraint trees. For general conditional constraint trees, we introduced a global approachthat runs in polynomial time in the size of the conditional constraint trees.Probabilistic deduction in conditional constraint trees is motivated by previous workin the literature on inference rules. It generalizes patterns of commonsense reasoning thathave been thoroughly studied in this work. Hence, we presented a new class of tractableprobabilistic deduction problems, which are driven by arti cial intelligence applications.It is also important to note that the deduction process in exact and general conditionalconstraint trees can easily be elucidated in a graphical way. For example, the computationof the tight answer to the premise-restricted complete query 9(QRSTUjM)[x1; x2] to theexact conditional constraint tree in Fig. 2, left side, can be illustrated by labeling each nodeof the directed tree in Fig. 3 with the corresponding tightest bounds of Table 1.Like Bayesian networks, conditional constraint trees are well-structured probabilisticknowledge bases that have an intuitive graphical representation. Di erently from Bayesiannetworks, conditional constraint trees do not encode any probabilistic independencies. Thus,they can also be understood as a complement to Bayesian networks, useful for restrictedapplications in which well-structured independencies do not hold or are di cult to access.Conditional constraint trees are quite restricted in their expressive power. However, inmore general probabilistic knowledge bases, probabilistic deduction in conditional contrainttrees may always act as local inference rules. For example, in case we desire explanatoryinformation on some speci c local deductions from a subset of the whole knowledge base(which could especially be useful in the design phase of a probabilistic knowledge base).An important conclusion of this paper concerns the question whether to perform prob-abilistic deduction by the iterative application of inference rules or by linear programming.The techniques of this paper have been elaborated by following the idea of inference rules inprobabilistic deduction. Hence, on the one hand, this paper shows that the idea of inferencerules can indeed bring us to e cient techniques for globally complete probabilistic deductionin restricted settings. However, on the other hand, given the technical complexity of thecorresponding proofs, it seems unlikely that these results can be extended to probabilisticknowledge bases that are signi cantly more general than conditional constraint trees.That is, as far as signi cantly more general probabilistic deduction problems with con-ditional constraints are concerned, the iterative application of inference rules does not seemto be very promising for globally complete probabilistic deduction. Note that a similarconclusion is drawn in a companion paper (1998a, 1999a), which shows the limits of locallycomplete inference rules for probabilistic deduction under taxonomic knowledge.For example, probabilistic deduction from probabilistic logic programs that do not as-sume probabilistic independencies (Ng & Subrahmanian 1993, 1994; Lukasiewicz, 1998d)should better not be done by the iterative application of inference rules. Note that muchmore promising techniques are, for example, global techniques by linear programming(Lukasiewicz, 1998d) and in particular approximation techniques based on truth-functionalmany-valued logics (Lukasiewicz 1998b, 1999b).226\nProbabilistic Deduction with Conditional Constraints over Basic Events AcknowledgementsI am very grateful to Michael Wellman and the referees for their useful comments. I alsowant to thank Thomas Eiter for valuable comments on an earlier version of this paper. Thispaper is an extended and revised version of a paper that appeared in Principles of KnowledgeRepresentation and Reasoning: Proceedings of the 6th International Conference, pp. 380{391.Appendix A. Preliminaries of the Proofs for Sections 4.1 to 4.3In this section, we make some technical preparations for the proofs of Theorems 4.1, 4.2,4.7, 4.8, and 4.9. In the sequel, we use the notationx1;1 x1;2 r1x2;1 x2;2 r2c1 c2as an abbreviation of the following system of equations:x1;1 + x1;2 = r1x2;1 + x2;2 = r2 x1;1 + x2;1 = c1x1;2 + x2;2 = c2 :(11)The next lemma provides the optimal values of two linear programs to be solved in theproofs of Theorems 4.1, 4.2, 4.7, 4.8, and 4.9.Lemma A.1 Let r1; r2; c1; c2 0 with r1 + r2 = c1 + c2. For all i; j 2 f1; 2g:a) min(ri; cj) = max xi;j subject to (11) and xn;m 0 for all n;m 2 f1; 2g.b) max(0; ri c3 j) = min xi;j subject to (11) and xn;m 0 for all n;m 2 f1; 2g.Proof. The claims can easily be veri ed (Lukasiewicz, 1996). 2Let us assume that a conditional constraint tree is the union of two subtrees that havejust one node in common. A model of each subtree and a third model related to thecommon node can now be combined to a model of the whole conditional constraint tree.This important result follows from the next lemma.Lemma A.2 Let B1 and B2 be sets of basic events with B1\\B2 = ;. Let B0 be a new basicevent that is not contained in B1 [ B2. Let Pr1 and Pr2 be probabilistic interpretations onthe atomic events over B1[fB0g and B2[fB0g, respectively. Let B1 and B2 be conjunctiveevents over B1 and B2, respectively. Let Pr0 be a probabilistic interpretation on the atomicevents over fB0; B1; B2g with Pr0(H0H1 ) = Pr1(H0H1 ) and Pr0(H0H2 ) = Pr2(H0H2 ) forall atomic events H0, H1, and H2 over fB0g, fB1g, and fB2g, respectively.There is a probabilistic interpretation Pr on the atomic events over B1[B2[fB0g with:Pr(H0H1H2 ) = Pr0(H0H1H2 );Pr(H0A1 ) = Pr1(H0A1 ); and Pr(H0A2 ) = Pr2(H0A2 )(12) 227\nLukasiewiczfor all atomic events H0, H1, H2, A1, and A2 over the sets of basic events fB0g, fB1g,fB2g, B1, and B2, respectively.Proof. Let the probabilistic interpretation Pr on the atomic events over B1 [ B2 [ fB0gbe de ned as follows:Pr(H0A1A2 ) = 8<:Pr0(H0H1H2 ) Pr1(H0A1 )Pr1(H0H1 ) Pr2(H0A2 )Pr2(H0H2 ) if Pr1(H0H1 ) Pr2(H0H2 ) > 00 if Pr1(H0H1 ) Pr2(H0H2 ) = 0for all atomic events H0, A1, and A2 over fB0g, B1, and B2, respectively, with atomic eventsH1 over fB1g and H2 over fB2g such that A1 ) H1 and A2 ) H2.Now, we must show that Pr satis es (12). Let H0, H1, and H2 be atomic events overfB0g, fB1g, and fB2g, respectively. For Pr1(H0H1 ) > 0 and Pr2(H0H2 ) > 0, we get:Pr(H0H1H2 ) = PA12AB1 ; A1)H1A22AB2 ; A2)H2 Pr0(H0H1H2 ) Pr1(H0A1 )Pr1(H0H1 ) Pr2(H0A2 )Pr2(H0H2 ) = Pr0(H0H1H2 ) :For Pr1(H0H1 ) = 0 or Pr2(H0H2 ) = 0, we get Pr(H0H1H2 ) = 0 = Pr0(H0H1H2 ).Let H0, H1, and A1 be atomic events over fB0g, fB1g, and B1, respectively, withA1 ) H1. For Pr1(H0H1 ) > 0, Pr2(H0B2 ) > 0, and Pr2(H0B2 ) > 0, it holds:Pr(H0A1 ) = PA22AB2 ; A2)B2 Pr0(H0H1B2 ) Pr1(H0A1 )Pr1(H0H1 ) Pr2(H0A2 )Pr2(H0B2 )+ PA22AB2 ; A2)B2 Pr0(H0H1B2 ) Pr1(H0A1 )Pr1(H0H1 ) Pr2(H0A2 )Pr2(H0B2 )= Pr0(H0H1 ) Pr1(H0A1 )Pr1(H0H1 ) = Pr1(H0A1 ) :For Pr1(H0H1 ) > 0, Pr2(H0B2 ) > 0, and Pr2(H0B2 ) = 0, we get:Pr(H0A1 ) = PA22AB2 ; A2)B2 Pr0(H0H1B2 ) Pr1(H0A1 )Pr1(H0H1 ) Pr2(H0A2 )Pr2(H0B2 )= Pr0(H0H1 ) Pr1(H0A1 )Pr1(H0H1 ) = Pr1(H0A1 ) :The proof is similar for Pr1(H0H1 ) > 0, Pr2(H0B2 ) = 0, and Pr2(H0B2 ) > 0.For Pr1(H0H1 ) = 0, we get Pr(H0A1 ) = 0 = Pr1(H0A1 ).Finally, the proof of Pr(H0A2 ) = Pr2(H0A2 ) for all atomic events H0 over fB0g andA2 over B2 can be done analogously. 2Appendix B. Proofs for Section 4.1In this section, we give the proofs of Theorems 4.1 and 4.2. That is, we show the globalsoundness and the global completeness of the functions H 1 , H 2 , H 2 , and H 2 . The proofsare done by induction on the recursive de nition of H 1 , H 2 , H 2 , and H 2 .228\nProbabilistic Deduction with Conditional Constraints over Basic EventsTo prove global soundness, we just have to show the local soundness of the computationsin Leaf, Chaining, and Fusion. To prove global completeness, we construct two modelsof the conditional constraint tree, one related to the greatest lower bound and another onerelated to the least upper bound computed in Leaf, Chaining, and Fusion.For Leaf, such a model is trivially given. For Chaining, we combine a model of thearrow, a model of the subtree, and a model connected to the common node to a model of theextended conditional constraint tree. For Fusion, we combine models of the subtrees anda model connected to the common node to a model of the extended conditional constrainttree. More precisely, for Chaining and Fusion, the models of the subtrees are related topreviously computed tightest bounds, while the model connected to the common node isrelated to the tightest bounds computed in the running Chaining or Fusion.We need the following technical preparations. The next lemma helps us to show theglobal completeness of the functions H 2 , H 2 , and H 2 in Chaining and Fusion.Lemma B.3 a) For all real numbers u; v 2 (0; 1], x2 2 [0; 1], and x2; z2 2 [0;1) withx2; x2 z2 and z2 x2 + x2, there is some x 2 [z2 x2; x2] with:min(1; uz2v ; 1 u+ uxv ; u uxv + uz2v ) = min(1; uz2v ; 1 u+ ux2v ; u+ ux2v ) :(13)b) For v2; x2 2 [0; 1] and v2; x2; w2; z2 2 [0;1) with v2 w2, v2 w2, x2 z2, x2 z2,w2 v2 + v2, and z2 x2 + x2, there is v 2 [w2 v2; v2] and x 2 [z2 x2; x2] with:min(w2; z2; v + z2 x; x+ w2 v) = min(w2; z2; v2 + x2; x2 + v2)min(v; x) = min(v2; x2) :(14)Proof. The claims can easily be veri ed (Lukasiewicz, 1996). 2The following lemma helps us to prove the local soundness and the local completenessof the functions H 1 , H 2 , H 2 , and H 2 in Chaining and Fusion.Lemma B.4 a) Let u; v 2 (0; 1], x 2 [0; 1], and x 2 [0;1). For all probabilistic inter-pretations Pr with Pr(B) > 0, the conditions u Pr(B) = Pr(BC ), v Pr(C) = Pr(BC ),x Pr(C) = Pr(CL(C \")), and x Pr(C) = Pr(CL(C \")) are equivalent to:Pr(B C L(C\"))Pr(B) Pr(B CL(C\"))Pr(B) Pr(B C)Pr(B)Pr(BC L(C\"))Pr(B) Pr(BCL(C \"))Pr(B) 1 uPr(C L(C\"))Pr(B) uxv Pr(BCL(C\"))Pr(B) Pr(BCL(C \"))Pr(B) uv uPr(BCL(C\"))Pr(B) Pr(BCL(C \"))Pr(B) uuv uxv uxvb) Let v; x 2 [0; 1] and v; x 2 [0;1). For all probabilistic interpretations Pr with Pr(B) > 0,the conditions v Pr(B) = Pr(BL(G)), v Pr(B) = Pr(BL(G)), x Pr(B) = Pr(BL(H )),and x Pr(B) = Pr(BL(H )) are equivalent to:Pr(B L(G)L(H))Pr(B) Pr(B L(G)L(H))Pr(B) Pr(B L(G))Pr(B)Pr(BL(G)L(H))Pr(B) Pr(BL(G)L(H ))Pr(B) vPr(B L(H))Pr(B) x Pr(BL(G)L(H))Pr(B) Pr(BL(G)L(H ))Pr(B) 1 vPr(BL(G)L(H))Pr(B) Pr(BL(G)L(H ))Pr(B) v1 x x229\nLukasiewiczProof. The claims can be veri ed by straightforward arithmetic transformations based onthe properties of probabilistic interpretations. 2After these preparations, we are now ready to prove the global soundness and the globalcompleteness of the functions H 1 , H 2 , H 2 , and H 2 .Proof of Theorem 4.1. The claims are proved by induction on the recursive de nitionof H 1 . The case C = B1 : : : Bk is tackled by iteratively splitting C into two conjunctiveevents. Thus, it is reduced to C = GH with conjunctive events G and H that are disjoint intheir basic events. For C=B1, we de ne u = Pr(CjB), v = Pr(BjC), and x1 = H 1 (C;C\").For C = B1 : : : Bk, hence C = GH , let v1 = H 1 (B;G) and x1 = H 1 (B;H).a) All models Pr 2 Mo(B;C) with Pr(B) = 0 satisfy the indicated condition. In the sequel,let Pr 2 Mo(B;C) with Pr(B) > 0.Basis: Let C = B. Since C = L(C), we get: 1 Pr(B) = 1 Pr(B) = Pr(BC ) = Pr(BL(C)) :Induction: Let C = B1. For all models Pr2 2 Mo(C;C\"), we get by the induction hypothesisx1 Pr2(C) Pr2(CL(C\")). Thus, Pr satis es the same conditions. Since L(C\") = L(C)and by Lemmata A.1 and B.4 a), we then get: 1 Pr(B) = max(0; u uv + ux1v ) Pr(B) Pr(BL(C\")) = Pr(BL(C)) :Let C=GH . For all Pr1 2Mo(B;G) and Pr2 2Mo(B;H), we get by the inductionhypothesis v1 Pr1(B) Pr1(BL(G)) and x1 Pr2(B) Pr2(BL(H)). Thus, Pr satis esthe same conditions. Since L(G)L(H) = L(GH) = L(C) and by Lemmata A.1 and B.4 b):max(0; v1 + x1 1) Pr(B) Pr(BL(G)L(H)) = Pr(BL(C)) :b)Basis: Let C =B. A model Pr 2Mo(B;C) such that Pr(B)> 0, 1 Pr(B)= 1 Pr(B) =Pr(BL(C)), and Pr(BL(C)) = 0 is given by B;B 7! 0; 1.Induction: Let C = B1. Let the model Pr1 of f(CjB)[u; u]; (BjC)[v; v]g with Pr1(B)> 0and Pr1(C) > 0 be de ned like in the proof of Theorem 3.2.We now choose an appropriate model Pr2 2 Mo(C;C\"). Let us rst consider thecase x1> 0, v=1, or not L(C\"))C. By the induction hypothesis, there exists a modelPr2 2Mo(C;C\") with Pr2(C) > 0, x1 Pr2(C) = Pr2(CL(C\")), and Pr2(CL(C\")) = 0i L(C\"))C. Let us next assume x1=0, v < 1, and L(C\"))C. By Theorem 3.2, thereexists a model Pr 002 2 Mo(C;C\") with Pr 002(CL(C\")) > 0. By the induction hypothesis,there exists a model Pr 02 2Mo(C;C\") with Pr 02(C) > 0 and 0 Pr 02(C) = Pr 02(CL(C\")).Hence, there exists a model Pr2 2 Mo(C;C\") with Pr2(C) > 0 andmin(1 v;Pr 002(CL(C\")) =Pr 002(C)) Pr2(C) = Pr2(CL(C\")) :By Lemma 3.1, we can choose Pr1 and Pr2 with Pr1(C) = Pr2(C) and Pr1(BC) Pr2(CL(C\")). By Lemmata A.1 and B.4 a), we can choose the probabilistic interpretation230\nProbabilistic Deduction with Conditional Constraints over Basic EventsPr0 over fB;C;L(C\")g with Pr0(A1) = Pr1(A1) and Pr0(A2) = Pr2(A2) for all atomicevents A1 and A2 over fB;Cg and fC;L(C\")g, respectively, such that:Pr0(BCL(C\")) = max(0;Pr 2(CL(C\")) Pr1(BC)) = 0Pr0(BCL(C \")) = max(0;Pr 2(CL(C\")) Pr1(BC)) :By Lemma A.2 with B1 = fBg, B2 = B(C;C\")nfCg, B0 = C, B1 = B, and B2 = L(C\"),there exists a probabilistic interpretation Pr over B(B;C) with (12). Hence, it holds Pr 2Mo(B;C) and Pr(B) > 0. By Lemma B.4 a), we get: 1 Pr(B) = max(0; u uv + ux1v ) Pr(B) = Pr(BL(C\")) = Pr(BL(C)) :Moreover, it is easy to see that Pr(BL(C)) = 0 i L(C))B.Let C = GH . By the induction hypothesis, there are models Pr1 2 Mo(B;G) andPr2 2 Mo(B;H) with Pr1(B) > 0, Pr2(B) > 0, v1 Pr1(B) = Pr1(BL(G)), x1 Pr2(B) =Pr2(BL(H)), Pr1(BL(G)) = 0 i L(G))B, and Pr2(BL(H)) = 0 i L(H))B.By Lemma 3.1, we can choose Pr1 and Pr2 with Pr1(B) = Pr2(B) and Pr1(B L(G)) Pr2(BL(H). By Lemmata A.1 and B.4 b), we can choose the probabilistic interpretationPr0 over fB;L(G); L(H)g with Pr0(A1) = Pr1(A1) and Pr0(A2) = Pr2(A2) for all atomicevents A1 and A2 over fB;L(G)g and fB;L(H)g, respectively, such that:Pr0(BL(G)L(H)) = min(Pr2(BL(H));Pr 1(BL(G))Pr0(BL(G)L(H)) = max(0;Pr 2(BL(H)) Pr1(BL(G))) :By Lemma A.2 with B1 = B(B;G)nfBg, B2 = B(B;H)nfBg, B0 = B, B1 = L(G), andB2 = L(H), there exists a probabilistic interpretation Pr over B(B;C) with (12). Hence,it holds Pr 2 Mo(B;C) and Pr(B) > 0. By Lemma B.4 b), we get:max(0; v1 + x1 1) Pr(B) = Pr(BL(G)L(H)) = Pr(BL(C)) :Moreover, it is easy to see that Pr(BL(C)) = 0 i L(C))B. 2Proof of Theorem 4.2. The claims are proved by induction on the recursive de nition ofH 2 , H 2 , and H 2 . Again, the case C = B1 : : : Bk is tackled by iteratively splitting C intotwo conjunctive events. Thus, it is reduced to C = GH with conjunctive events G and Hthat are disjoint in their basic events. For C = B1 let u = Pr(CjB), v = Pr(BjC), andx2 = H 2 (C;C\"); x2 = H 2 (C;C\"); z2 = H 2 (C;C\") :For C = B1 : : : Bk, hence C = GH , we de ne:v2 = H 2 (B;G); v2 = H 2 (B;G); w2 = H 2 (B;G)x2 = H 2 (B;H); x2 = H 2 (B;H); z2 = H 2 (B;H) :a) For Pr 2 Mo(B;C) with Pr(B) = 0, we get Pr(N) = 0 for all N 2 B(B;C). Thus, Prsatis es the indicated conditions. Next, let Pr 2 Mo(B;C) with Pr(B) > 0.231\nLukasiewiczBasis: Let C = B. Since L(C) = C, we get:Pr(BL(C)) = Pr(BC ) = 1 Pr(B) = 2 Pr(B)Pr(BL(C)) = Pr(BC) = 0 Pr(B) = 2 Pr(B)Pr(L(C)) = Pr(C) = 1 Pr(B) = 2 Pr(B) :Induction: Let C = B1. For all models Pr2 2 Mo(C;C\"), we get by the induction hypothesisPr2(CL(C\")) x2 Pr2(C), Pr2(CL(C\")) x2 Pr2(C), and Pr2(L(C\")) z2 Pr2(C).Hence, Pr satis es the same conditions. Since L(C) = L(C\") and by Lemmata A.1 andB.4 a), we then get:Pr(BL(C)) = Pr(BL(C\")) min(1; uz2v ; 1 u+ ux2v ; u+ ux2v ) Pr(B) = 2 Pr(B)Pr(BL(C)) = Pr(BL(C\")) min(ux2v + uv u; uz2v ) Pr(B) = 2 Pr(B)Pr(L(C)) = Pr(L(C\")) uz2v Pr(B) = 2 Pr(B) :Let C = GH . For all models Pr1 2 Mo(B;G) and all models Pr2 2 Mo(B;H), we getby the induction hypothesis:Pr1(BL(G)) v2 Pr1(B); Pr2(BL(H)) x2 Pr2(B)Pr1(BL(G)) v2 Pr1(B); Pr2(BL(H)) x2 Pr2(B)Pr1(L(G)) w2 Pr1(B); Pr2(L(H)) z2 Pr2(B) :Thus, Pr satis es the same conditions. Since L(C) = L(GH) = L(G)L(H) and by Lem-mata A.1 and B.4 b), we get:Pr(BL(C)) = Pr(BL(G)L(H)) min(v2; x2) Pr(B)Pr(BL(C)) = Pr(BL(G)L(H)) min(v2; x2) Pr(B)Pr(L(C)) = Pr(L(G)L(H)) min(w2; z2; v2 + x2; x2 + v2) Pr(B) :b) and c)Basis: Let C = B. A model Pr 2 Mo(B;C) with Pr(B) > 0 satisfying Pr(BL(C)) =1 Pr(B) = 2 Pr(B), Pr(BL(C)) = 0 Pr(B) = 2 Pr(B), and Pr(L(C)) = 1 Pr(B) = 2 Pr(B) is given by B;B 7! 0; 1.Induction: Let C = B1. Let the model Pr1 of f(CjB)[u; u]; (BjC)[v; v]g with Pr1(B) > 0and Pr1(C) > 0 be de ned like in the proof of Theorem 3.2.For the proof of c), by the induction hypothesis, there is some Pr2 2Mo(C;C\") withPr2(C)> 0, Pr2(CL(C\")) = x2 Pr2(C), and Pr2(L(C\")) = z2 Pr2(C).By Lemma 3.1, we can choose Pr1 and Pr2 with Pr1(C) = Pr2(C) and Pr1(BC) Pr2(CL(C\")). By Lemma A.1, we can choose the probabilistic interpretation Pr0 overfB;C;L(C\")g with Pr0(A1) = Pr1(A1) and Pr0(A2) = Pr2(A2) for all atomic events A1and A2 over fB;Cg and fC;L(C\")g, respectively, such that:Pr0(BCL(C\")) = min(Pr1(BC);Pr 2(CL(C\"))) = Pr2(CL(C\"))Pr0(BCL(C \")) = min(Pr1(BC);Pr2(CL(C\"))) :232\nProbabilistic Deduction with Conditional Constraints over Basic EventsBy Lemma A.2 with B1 = fBg, B2 = B(C;C\") n fCg, B0 = C, B1 = B, and B2 =L(C\"), there is a probabilistic interpretation Pr over B(B;C) with (12). Hence, it holdsPr 2 Mo(B;C) and Pr(B) > 0. By Lemma B.4 a), we get:Pr(BL(C)) = Pr(BL(C\")) = min(ux2v + uv u; uz2v ) Pr(B) = 2 Pr(B)Pr(L(C)) = Pr(L(C\")) = uz2v Pr(B) = 2 Pr(B) :For the proof of b), by the induction hypothesis, there are models Pr1;2 ;Pr 2;2 2Mo(C;C\") with Pr1;2(C) > 0, Pr2;2(C) > 0, andPr1;2(CL(C\")) = x2 Pr1;2(C); Pr1;2(L(C\")) = z2 Pr1;2(C)Pr2;2(CL(C\")) = x2 Pr2;2(C); Pr2;2(L(C\")) = z2 Pr2;2(C) :(15)These conditions already entail x2 z2 and x2 z2. With the results from a), we addi-tionally get z2 x2 + x2. By Lemma B.3 a), there is x 2 [z2 x2; x2] with (13). By (15),there is Pr2 2 Mo(C;C\") with Pr2(C) > 0 andPr2(CL(C\")) = x Pr2(C); Pr2(L(C\")) = z2 Pr2(C) :By Lemma 3.1, we can choose Pr1 and Pr2 with Pr1(C) = Pr2(C). By Lemma A.1, wecan choose the probabilistic interpretation Pr0 over fB;C;L(C\")g with Pr0(A1) = Pr1(A1)and Pr0(A2) = Pr2(A2) for all atomic events A1 and A2 over fB;Cg and fC;L(C\")g,respectively, such that:Pr0(BCL(C\")) = min(Pr1(BC);Pr2(CL(C\")))Pr0(BCL(C\")) = min(Pr1(BC );Pr2(CL(C \"))) :By Lemma A.2 with B1 = fBg, B2 = B(C;C\") n fCg, B0 = C, B1 = B, and B2 =L(C\"), there is a probabilistic interpretation Pr over B(B;C) with (12). Hence, it holdsPr 2 Mo(B;C) and Pr(B) > 0. By Lemma B.4 a), we get:Pr(BL(C)) = Pr(BL(C\")) = min(1; uz2v ; 1 u+ ux2v ; u+ ux2v ) Pr(B) = 2 Pr(B)Pr(L(C)) = Pr(L(C\")) = uz2v Pr(B) = 2 Pr(B) :Let C = GH . We just show b), the claim in c) can be proved analogously. By the in-duction hypothesis, there are models Pr1;1 ;Pr2;1 2 Mo(B;G) and Pr1;2 ;Pr2;2 2 Mo(B;H)with Pr1;1(B) > 0, Pr2;1(B) > 0, Pr1;2(B) > 0, Pr2;2(B) > 0, andPr1;1(BL(G)) = v2 Pr1;1(B); Pr1;1(L(G)) = w2 Pr1;1(B)Pr2;1(BL(G)) = v2 Pr2;1(B); Pr2;1(L(G)) = w2 Pr2;1(B)Pr1;2(BL(H)) = x2 Pr1;2(B); Pr1;2(L(H)) = z2 Pr1;2(B)Pr2;2(BL(H)) = x2 Pr2;2(B); Pr2;2(L(H)) = z2 Pr2;2(B) :(16)These conditions already entail v2 w2, v2 w2, x2 z2, and x2 z2. With the resultsfrom a), we additionally get w2 v2 + v2 and z2 x2 + x2. By Lemma B.3 b), there is233\nLukasiewiczv 2 [w2 v2; v2] and x 2 [z2 x2; x2] with (14). By (16), there is Pr1 2 Mo(B;G) andPr2 2 Mo(B;H) with Pr1(B) > 0, Pr2(B) > 0, andPr1(BL(G)) = v Pr1(B); Pr1(L(G)) = w2 Pr1(B)Pr2(BL(H)) = x Pr2(B); Pr2(L(H)) = z2 Pr2(B) :By Lemma 3.1, we can choose Pr1 and Pr2 with Pr1(B) = Pr2(B). By Lemma A.1,we can choose the probabilistic interpretation Pr0 over fB;L(G); L(H)g with Pr0(A1) =Pr1(A1) and Pr0(A2) = Pr2(A2) for all atomic events A1 and A2 over fB;L(G)g andfB;L(H)g, respectively, such that:Pr0(BL(G)L(H )) = min(Pr1(BL(G));Pr2(BL(H)))Pr0(BL(G)L(H )) = min(Pr1(BL(G));Pr2(BL(H))) :By Lemma A.2 with B1 = B(B;G) n fBg, B2 = B(B;H) n fBg, B0 = B, B1 = L(G),and B2 = L(H), there is a probabilistic interpretation Pr over B(B;C) with (12). Hence,it holds Pr 2 Mo(B;C) and Pr(B) > 0. By Lemma B.4 b), we get:Pr(BL(C)) = Pr(BL(G)L(H)) = min(v2; x2) Pr(B)Pr(L(C)) = Pr(L(G)L(H)) = min(w2; z2; v2 + x2; x2 + v2) Pr(B) : 2Finally, note that computing least upper bounds is more di cult than computing great-est lower bounds, since for each edge B ! C, by Lemmata 3.1 and B.4 a), the greatest lowerbound of Pr(BCL(C\"))=Pr (B) subject to Pr 2Mo(B;C) and Pr(B)> 0 is always 0, butthe least upper bound of Pr(BCL(C\"))=Pr(B) subject to Pr 2Mo(B;C) and Pr(B)> 0 isgenerally not 1.Appendix C. Proofs for Section 4.2In this section, we give the proofs of Theorems 4.7 and 4.8.We need some technical preparations as follows. The next lemma helps us to show thelocal soundness of the function H 1 in Fusion.Lemma C.5 For all real numbers u1; u; v1; v; x1; x; y1; y 2 (0; 1] with u1 u, v1 v,x1 x, y1 y, and u1 + x1 > 1, it holds:min(u=v u; x=y x) = (u + x 1) min(u1=v1 u1; x1=y1 x1) = (u1 + x1 1) :Proof. The claim can easily be veri ed (Lukasiewicz, 1996). 2The following lemma helps us to show the local soundness and the local completenessof the function H 1 in Chaining and Fusion.Lemma C.6 a) Let u, v, x, and y be real numbers from (0; 1]. For all probabilistic inter-pretations Pr with Pr(L(C\")) > 0, the conditions u Pr (B) = Pr(BC ), v Pr (C) = Pr(BC ),x Pr(C) = Pr(CL(C \")), and y Pr(L(C\")) = Pr(CL(C \")) are equivalent to:234\nProbabilistic Deduction with Conditional Constraints over Basic EventsPr(B C L(C\"))Pr(L(C\")) Pr(BCL(C\"))Pr(L(C\")) Pr(BC)Pr(L(C\"))Pr(BC L(C\"))Pr(L(C\")) Pr(BCL(C \"))Pr(L(C\")) yvxu yvxPr(C L(C\"))Pr(L(C\")) 1 y Pr(BCL(C\"))Pr(L(C\")) Pr(BCL(C \"))Pr(L(C\")) yx yvxPr(BCL(C\"))Pr(L(C\")) Pr(BCL(C \"))Pr(L(C\")) yvxyx y yb) Let u, v, x, and y be real numbers from (0; 1]. For all probabilistic interpretations Prwith Pr(B) > 0, the conditions u Pr(B) = Pr(BL(G)), v Pr(L(G)) = Pr(BL(G)),x Pr(B) = Pr(BL(H )), and y Pr(L(H)) = Pr(BL(H )) are equivalent to:Pr(BL(G)L(H))Pr(B) Pr(B L(G)L(H))Pr(B) Pr(BL(G))Pr(B)Pr(BL(G)L(H))Pr(B) Pr(BL(G)L(H ))Pr(B) uv uPr(B L(H))Pr(B) xy x Pr(BL(G)L(H))Pr(B) Pr(BL(G)L(H ))Pr(B) 1 uPr(BL(G)L(H))Pr(B) Pr(BL(G)L(H ))Pr(B) u1 x xProof. The claims can be veri ed by straightforward arithmetic transformations based onthe properties of probabilistic interpretations. 2We are now ready to prove Theorems 4.7 and 4.8.Proof of Theorem 4.7. The claims are proved by induction on the recursive de nition ofH 1 . The proof for C = B1B2 : : : Bk with k > 1 is done for k = 2. It can easily be generalizedto k 2. For C =B1, we de ne u1 = Pr1(CjB), v1 = Pr1(BjC), x1 = H 1 (C;C\"), andy1 = H 1 (C;C\"). Note that 1 > 0 entails x1; y1 > 0 and v1 + x1 > 1. For C = B1B2,we de ne G = B1, H = B2, u1 = H 1 (B;G), v1 = H 1 (B;G), x1 = H 1 (B;H), andy1 = H 1 (B;H). Note that 1 > 0 entails u1; v1; x1; y1 > 0 and u1 + x1 > 1.a) All models Pr 2 Mo(B;C) with Pr(L(C)) = 0 satisfy the indicated condition. In thesequel, let Pr 2 Mo(B;C) with Pr(L(C)) > 0 and thus also Pr(B) > 0.Basis: Let C = B. Since C = L(C), we get: 1 Pr(L(C)) = 1 Pr(L(C)) = Pr(CL(C)) = Pr(BL(C)) :Induction: Let C = B1. For all models Pr2 2 Mo(C;C\"), we get x1 Pr2(C) Pr2(CL(C\"))by Theorem 4.3 a), and y1 Pr2(L(C\")) Pr2(CL(C\")) by the induction hypothesis. Thus,Pr satis es the same conditions. Since L(C\") = L(C) and by Lemmata A.1 and C.6 a): 1 = y1 y1x1 + y1v1x1 Pr(CL(C\")) =Pr (L(C\")) = Pr(CL(C)) =Pr (L(C)) :Let C = GH . For all models Pr1 2 Mo(B;G) and Pr2 2 Mo(B;H), we get byTheorem 4.3 a) and by the induction hypothesis, respectively:u1 Pr1(B) Pr1(BL(G)); x1 Pr2(B) Pr2(BL(H))v1 Pr1(L(G)) Pr1(BL(G)); y1 Pr2(L(H)) Pr2(BL(H)) :235\nLukasiewiczHence, Pr satis es the same conditions. Since L(G)L(H) = L(GH) = L(C) and by Lem-mata A.1, C.5, and C.6 b), we then get: 1 = 1 = (1 + min(u1=v1 u1; x1=y1 x1)u1+x1 1 ) 1 = (1 + Pr(BL(G)L(H))=Pr (B)Pr(BL(G)L(H))=Pr (B) ) = Pr(BL(C))Pr(L(C)) :b)Basis: Let C = B. A model Pr 2 Mo(B;C) such that Pr(B) > 0, Pr(L(C)) > 0,1 Pr(L(C)) = Pr(BL(C)), and 1 Pr(B) = Pr(BL(C)) is given by B;B 7! 0; 1.Induction: Let C =B1. Let the model Pr1 of f(CjB)[u1; u1]; (BjC)[v1; v1]g with Pr1(B)> 0and Pr1(C) > 0 be de ned like in the proof of Theorem 3.2.By the induction hypothesis, there is Pr2 2Mo(C;C\") with Pr2(C)> 0, Pr2(L(C\"))> 0,y1 Pr2(L(C\")) = Pr2(CL(C\")), and x1 Pr2(C) = Pr2(CL(C\")). By Lemma 3.1, we canchoose Pr1 and Pr2 such that Pr1(C) = Pr2(C) and Pr1(BC) Pr2(CL(C\")). By Lem-mata A.1 and C.6 a), we can choose the probabilistic interpretation Pr0 over fB;C;L(C\")gwith Pr0(A1) = Pr1(A1) and Pr0(A2) = Pr2(A2) for all atomic events A1 and A2 overfB;Cg and fC;L(C\")g, respectively, such that:Pr0(BCL(C\")) = max(0;Pr 2(CL(C\")) Pr1(BC)) = 0Pr0(BCL(C \")) = max(0;Pr 2(CL(C\")) Pr1(BC)) :By Lemma A.2 with B1 = fBg, B2 = B(C;C\")nfCg, B0 = C, B1 = B, and B2 = L(C\"),there exists a probabilistic interpretation Pr over B(B;C) with (12). Hence, it holds Pr 2Mo(B;C), Pr(B) > 0, and Pr(L(C)) > 0. Moreover, by Lemma C.6 a), we get: 1 = y1 y1x1 + y1v1x1 = Pr(CL(C\")) =Pr (L(C\")) = Pr(CL(C)) =Pr(L(C)) 1 = u1 u1v1 + u1x1v1 = Pr(CL(C\")) =Pr (C) = Pr(CL(C)) =Pr(C) :Let C = GH . By the induction hypothesis, there are models Pr1 2 Mo(B;G) andPr2 2 Mo(B;H) with Pr1(B) > 0, Pr2(B) > 0, Pr1(L(G)) > 0, Pr2(L(H)) > 0, andu1 Pr1(B) = Pr1(BL(G)); x1 Pr2(B) = Pr2(BL(H))v1 Pr1(L(G)) = Pr1(BL(G)); y1 Pr2(L(H)) = Pr2(BL(H)) :By Lemma 3.1, we can choose Pr1 and Pr2 with Pr1(B) = Pr2(B) and Pr1(B L(G)) Pr2(BL(H). By Lemmata A.1 and C.6 b), we can choose the probabilistic interpretationPr0 over fB;L(G); L(H)g with Pr0(A1) = Pr1(A1) and Pr0(A2) = Pr2(A2) for all atomicevents A1 and A2 over fB;L(G)g and fB;L(H)g, respectively, such that:Pr0(BL(G)L(H)) = min(Pr2(BL(H));Pr 1(BL(G)))Pr0(BL(G)L(H)) = max(0;Pr 2(BL(H)) Pr1(BL(G))) :By Lemma A.2 with B1 = B(B;G)nfBg, B2 = B(B;H)nfBg, B0 = B, B1 = L(G), andB2 = L(H), there exists a probabilistic interpretation Pr over B(B;C) with (12). Hence,236\nProbabilistic Deduction with Conditional Constraints over Basic Eventsit holds Pr 2 Mo(B;C) and Pr(B) > 0. By Lemma C.6 b), we get Pr(L(C)) > 0 and 1 = 1 = (1 + min(u1=v1 u1; x1=y1 x1)u1+x1 1 ) = 1 = (1 + Pr(BL(G)L(H))=Pr (B)Pr(BL(G)L(H))=Pr (B) ) = Pr(BL(C))Pr(L(C)) 1 = u1 + x1 1 = Pr(BL(G)L(H))Pr(B) = Pr(BL(C))Pr(B) :c) Let C = GH . By Theorem 4.3 b), there exist Pr1 2 Mo(B;G) and Pr2 2 Mo(B;H) withPr1(B) > 0, Pr2(B) > 0, u1 Pr1(B) = Pr1(BL(G)), and x1 Pr2(B) = Pr2(BL(H)). ByLemma 3.1, we can choose Pr1 and Pr2 such that Pr1(B) = Pr2(B) and Pr1(B L(G)) Pr2(BL(H). By Lemmata A.1 and C.6 b), we can choose the probabilistic interpretationPr0 over fB;L(G); L(H)g with Pr0(A1) = Pr1(A1) and Pr0(A2) = Pr2(A2) for all atomicevents A1 and A2 over fB;L(G)g and fB;L(H)g, respectively, such that:Pr0(BL(G)L(H)) = max(0;Pr 2(BL(H) Pr1(B L(G))) = 0Pr0(BL(G)L(H)) = max(0;Pr 2(BL(H)) Pr1(BL(G))) :By Lemma A.2 with B1 = B(B;G)nfBg, B2 = B(B;H)nfBg, B0 = B, B1 = L(G), andB2 = L(H), there exists a probabilistic interpretation Pr over B(B;C) with (12). Hence,it holds Pr 2 Mo(B;C) and Pr(B) > 0. By Lemma C.6 b), we get Pr(L(C)) > 0 and1 = Pr(BL(G)L(H)) =Pr (L(G)L(H)) = Pr(BL(C)) =Pr (L(C)) 1 = Pr(BL(G)L(H)) =Pr (B)) = Pr(BL(C)) =Pr (B) :d) Let C = GH . By Theorem 3.2, there is a model Pr 000 2 Mo(B;C) with Pr 000(BL(C)) > 0.By Theorem 4.3 b), there is a model Pr 00 2Mo(B;C) with Pr 00(B)> 0 and 0 Pr 00(B) = 1 Pr 00(B) = Pr 00(BL(C)). Hence, there is a model Pr 0 2 Mo(B;C) with Pr 0(B) > 0 andmin(\";Pr 000(BL(C)) =Pr 000(B)) Pr 0(B) = Pr 0(BL(C)) :Let the models Pr1 2Mo(B;G) and Pr2 2 Mo(B;H) be de ned by Pr1(A1) = Pr 0(A1) andPr2(A2) = Pr 0(A2) for all atomic events A1 and A2 over B(B;G) and B(B;H), respectively.By Lemma 3.1, we can choose Pr1 and Pr2 such that Pr1(B) = Pr2(B) and Pr1(B L(G)) Pr2(BL(H). By Lemmata A.1 and C.6 b), we can choose the probabilistic interpretationPr0 over fB;L(G); L(H)g with Pr0(A1) = Pr1(A1) and Pr0(A2) = Pr2(A2) for all atomicevents A1 and A2 over fB;L(G)g and fB;L(H)g, respectively, such that:Pr0(BL(G)L(H)) = max(0;Pr 2(BL(H) Pr1(B L(G))) = 0Pr0(BL(G)L(H)) = min(\";Pr 000(BL(C)) =Pr 000(B)) Pr0(B) :By Lemma A.2 with B1 = B(B;G) n fBg, B2 = B(B;H) n fBg, B0 = B, B1 = L(G), andB2 = L(H), there is a probabilistic interpretation Pr over B(B;C) with (12). Hence, it holdsPr 2Mo(B;C), Pr(B)> 0, Pr(L(C))> 0, Pr(BL(C))= 0, and \" Pr(B) Pr(BL(C )). 2Proof of Theorem 4.8. For u1 > 0, the claim is immediate by Theorem 4.7 a) to c).Let u1 = 0 and E ) F . It holds 1 Pr(E) = Pr(EF ) for all models Pr of KB . Moreover,by Theorem 3.2, there exists a model Pr of KB with Pr(E) > 0.Let u1 = 0 and not E ) F . By Theorem 4.3 b), there exists a model Pr of KBwith Pr(E)> 0 and Pr(EF )= 0. By Theorem 4.7 d), there exists a model Pr of KB withPr(E)> 0 and 1 Pr(E) = Pr(EF ). 2 237\nLukasiewiczAppendix D. Proofs for Section 4.3In this section, we give the proof of Theorem 4.9.The next lemma will help us to show the global tightness of the computed lower boundin the case (3) of Theorem 4.9 b).Lemma D.7 Let x 2 [0; 1] and v; x 2 [0;1). For all probabilistic interpretations Pr withPr(G) > 0, the conditions Pr(EG) = 0, v Pr(G) = Pr(EG), x Pr(G) = Pr(GF ), andx Pr(G) = Pr(GF ) are equivalent to:Pr(EGF )Pr(G) Pr(EGF )Pr(G) Pr(EG)Pr(G)Pr(EGF )Pr(G) Pr(EGF )Pr(G) vPr(GF )Pr(G) x Pr(EGF )Pr(G) Pr(EGF )Pr(G) 1Pr(EGF )Pr(G) Pr(EGF )Pr(G) 01 x xProof. The claim can be veri ed by straightforward arithmetic transformations based onthe properties of probabilistic interpretations. 2We are now ready to prove Theorem 4.9.Proof of Theorem 4.9. a) By the de nition of queries to conditional constraint trees, allpaths from a basic event in E to a basic event in F have at least one basic event in common.Hence, we can choose the basic event G from all such basic events in common such that9(GjE)[z1; z2] is a strongly conclusion-restricted complete query to a subtree.b) For u1 > 0, the claim follows from Theorem 4.7 a) to c). For the special case of exactconditional constraint trees (B;KB), the claim then follows from Theorems 4.3 and 4:5.Let u1 = 0, v1 = 1, and G) F . It holds 1 Pr(E) = Pr(EF ) for all models Pr of KB .Moreover, by Theorem 3.2, there exists a model Pr of KB with Pr(E) > 0.Let u1 = 0, v1 = 0, and G ) F . It is easy to see that by (1) and Theorem 4.7 d), thetight upper answer is given by fx2=1g. We now show that the tight lower answer is given byfx1=0g. By Theorem 4.3 b), there exists a model Pr1 of KB1 with Pr1(E) > 0, Pr1(G) > 0,and Pr1(EG) = 0. By Theorem 3.2, there exists a model Pr2 of KB2 with Pr2(G) > 0. ByLemma 3.1, we can choose Pr1 and Pr2 with Pr1(G) = Pr2(G) and Pr1(E G) Pr2(GF ).By Lemmata A.1 and D.7, we can choose the probabilistic interpretation Pr0 over fE;G;Fgwith Pr0(A1) = Pr1(A1) and Pr0(A2) = Pr2(A2) for all atomic events A1 and A2 overfE;Gg and fG;Fg, respectively, such that:Pr0(EGF ) = max(0;Pr 2(GF ) Pr1(E G)) = 0Pr0(EGF ) = 0 :By Lemma A.2, there exists a probabilistic interpretation Pr over B with (12) for all atomicevents H0, H1, H2, A1, and A2 over the sets of basic events fGg, fEg, fFg, B1 n fGg, andB2 n fGg, respectively. Hence, Pr is a model of KB with Pr(E) > 0 and Pr(EF ) = 0.For u1 = 0 and not G) F , the claim follows from (1) and Theorem 4.7 d). 2238\nProbabilistic Deduction with Conditional Constraints over Basic EventsReferencesAdams, E. W. (1975). The Logic of Conditionals, Vol. 86 of Synthese Library. D. Reidel,Dordrecht, Netherlands.Amarger, S., Dubois, D., & Prade, H. (1991). Constraint propagation with impreciseconditional probabilities. In Proceedings of the 7th Conference on Uncertainty inArti cial Intelligence, pp. 26{34. Morgan Kaufmann.Andersen, K. A., & Hooker, J. N. (1994). Bayesian logic. Decision Support Systems, 11,191{210.Bacchus, F. (1990). Representing and Reasoning with Probabilistic Knowledge: A LogicalApproach to Probabilities. MIT Press, Cambridge, USA.Bacchus, F., Grove, A., Halpern, J. Y., & Koller, D. (1996). From statistical knowledgebases to degrees of beliefs. Arti cial Intelligence, 87, 75{143.Carnap, R. (1950). Logical Foundations of Probability. University of Chicago Press, Chicago.Coletti, G. (1994). Coherent numerical and ordinal probabilistic assessments. IEEE Trans-actions on Systems, Man, and Cybernetics, 24 (12), 1747{1754.de Finetti, B. (1974). Theory of Probability. Wiley, New York.Dubois, D., & Prade, H. (1988). On fuzzy syllogisms. Computational Intelligence, 4 (2),171{179.Dubois, D., Prade, H., Godo, L., & de M antaras, R. L. (1993). Qualitative reasoning withimprecise probabilities. Journal of Intelligent Information Systems, 2, 319{363.Dubois, D., Prade, H., & Touscas, J.-M. (1990). Inference with imprecise numerical quanti- ers. In Ras, Z. W., & Zemankova, M. (Eds.), Intelligent Systems, chap. 3, pp. 53{72.Ellis Horwood.Fagin, R., Halpern, J. Y., & Megiddo, N. (1990). A logic for reasoning about probabilities.Information and Computation, 87, 78{128.Frisch, A. M., & Haddawy, P. (1994). Anytime deduction for probabilistic logic. Arti cialIntelligence, 69, 93{122.Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to theTheory of NP-Completeness. Freeman, New York.Georgakopoulos, G., Kavvadias, D., & Papadimitriou, C. H. (1988). Probabilistic satis a-bility. Journal of Complexity, 4 (1), 1{11.Gilio, A., & Scozzafava, R. (1994). Conditional events in probability assessment and revi-sion. IEEE Transactions on Systems, Man, and Cybernetics, 24 (12), 1741{1746.Halpern, J. Y. (1990). An analysis of rst-order logics of probability. Arti cal Intelligence,46, 311{350. 239\nLukasiewiczHansen, P., Jaumard, B., Nguets e, G.-B. D., & de Arag~ao, M. P. (1995). Models and algo-rithms for probabilistic and Bayesian logic. In Proceedings of the 14th InternationalJoint Conference on Arti cial Intelligence, pp. 1862{1868.Heinsohn, J. (1994). Probabilistic description logics. In Proceedings of the 10th Conferenceon Uncertainty in Arti cial Intelligence. Morgan Kaufmann.Jaumard, B., Hansen, P., & de Arag~ao, M. P. (1991). Column generation methods forprobabilistic logic. ORSA Journal of Computing, 3, 135{147.Kavvadias, D., & Papadimitriou, C. H. (1990). A linear programming approach to reasoningabout probabilities. Annals of Mathematics and Arti cial Intelligence, 1, 189{205.Lukasiewicz, T. (1996). Precision of Probabilistic Deduction under Taxonomic Knowledge.Doctoral Dissertation, Universit at Augsburg.Lukasiewicz, T. (1997). E cient global probabilistic deduction from taxonomic and prob-abilistic knowledge-bases over conjunctive events. In Proceedings of the 6th Inter-national Conference on Information and Knowledge Management, pp. 75{82. ACMPress.Lukasiewicz, T. (1998a). Magic inference rules for probabilistic deduction under taxonomicknowledge. In Proceedings of the 14th Conference on Uncertainty in Arti cial Intel-ligence, pp. 354{361. Morgan Kaufmann.Lukasiewicz, T. (1998b). Many-valued rst-order logics with probabilistic semantics. In Pro-ceedings of the Annual Conference of the European Association for Computer ScienceLogic. To appear.Lukasiewicz, T. (1998c). Probabilistic deduction with conditional constraints over basicevents. In Principles of Knowledge Representation and Reasoning: Proceedings of the6th International Conference, pp. 380{391. Morgan Kaufmann.Lukasiewicz, T. (1998d). Probabilistic logic programming. In Proceedings of the 13th Eu-ropean Conference on Arti cial Intelligence, pp. 388{392. J. Wiley & Sons.Lukasiewicz, T. (1999a). Local probabilistic deduction from taxonomic and probabilisticknowledge-bases over conjunctive events. International Journal of Approximate Rea-soning. To appear.Lukasiewicz, T. (1999b). Probabilistic and truth-functional many-valued logic program-ming. In Proceedings of the 29th IEEE International Symposium on Multiple-ValuedLogic. To appear.Luo, C., Yu, C., Lobo, J., Wang, G., & Pham, T. (1996). Computation of best bounds ofprobabilities from uncertain data. Computational Intelligence, 12 (4), 541{566.Ng, R. T., & Subrahmanian, V. S. (1993). A semantical framework for supporting subjectiveand conditional probabilities in deductive databases. Journal of Automated Reasoning,10 (2), 191{235. 240\nProbabilistic Deduction with Conditional Constraints over Basic EventsNg, R. T., & Subrahmanian, V. S. (1994). Stable semantics for probabilistic deductivedatabases. Information and Computation, 110, 42{83.Nilsson, N. J. (1986). Probabilistic logic. Arti cial Intelligence, 28, 71{88.Nilsson, N. J. (1993). Probabilistic logic revisited. Arti cial Intelligence, 59, 39{42.Paa , G. (1988). Probabilistic Logic. In Dubois, D., Smets, P., Mamdani, A., & Prade,H. (Eds.), Non-Standard Logics for Automated Reasoning, chap. 8, pp. 213{251. Aca-demic Press.Papadimitriou, C. H., & Steiglitz, K. (1982). Combinatorial Optimization, Algorithms andComplexity. Prentice-Hall, Englewood Cli s, NJ.Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of PlausibleInference. Morgan Kaufmann, San Mateo, CA.Pittarelli, M. (1994). Anytime decision making with imprecise probabilities. In Proceedingsof the 10th Conference on Uncertainty in Arti cial Intelligence, pp. 470{477. MorganKaufmann.Schrijver, A. (1986). Theory of Linear and Integer Programming. Wiley, New York.Th one, H. (1994). Precise Conclusion under Uncertainty and Incompleteness in DeductiveDatabase Systems. Doctoral Dissertation, Universit at T ubingen.Th one, H., Kie ling, W., & G untzer, U. (1995). On cautious probabilistic inference anddefault detachment. Annals of Operations Research, 55, 195{224.van der Gaag, L. (1991). Computing probability intervals under independency constraints.In Uncertainty in Arti cial Intelligence 6, pp. 457{466. North-Holland, Amsterdam.Walley, P. (1991). Statistical Reasoning with Imprecise Probabilities. Chapman and Hall,New York.\n241"}], "references": [{"title": "The Logic of Conditionals, Vol", "author": ["E.W. Adams"], "venue": "86 of Synthese Library. D. Reidel,", "citeRegEx": "Adams,? 1975", "shortCiteRegEx": "Adams", "year": 1975}, {"title": "Constraint propagation with imprecise", "author": ["S. Amarger", "D. Dubois", "H. Prade"], "venue": null, "citeRegEx": "Amarger et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Amarger et al\\.", "year": 1991}, {"title": "Representing and Reasoning with Probabilistic Knowledge: A Logical", "author": ["F. Bacchus"], "venue": null, "citeRegEx": "Bacchus,? \\Q1990\\E", "shortCiteRegEx": "Bacchus", "year": 1990}, {"title": "Logical Foundations of Probability", "author": ["R. Carnap"], "venue": "University of Chicago Press, Chicago.", "citeRegEx": "Carnap,? 1950", "shortCiteRegEx": "Carnap", "year": 1950}, {"title": "Coherent numerical and ordinal probabilistic assessments", "author": ["G. Coletti"], "venue": "IEEE Trans-", "citeRegEx": "Coletti,? 1994", "shortCiteRegEx": "Coletti", "year": 1994}, {"title": "On fuzzy syllogisms", "author": ["D. Dubois", "H. Prade"], "venue": "Computational Intelligence,", "citeRegEx": "Dubois and Prade,? \\Q1988\\E", "shortCiteRegEx": "Dubois and Prade", "year": 1988}, {"title": "Qualitative reasoning with", "author": ["R.L. antaras"], "venue": null, "citeRegEx": "antaras,? \\Q1993\\E", "shortCiteRegEx": "antaras", "year": 1993}, {"title": "Inference with imprecise numerical quanti", "author": ["D. Dubois", "H. Prade", "Touscas", "J.-M"], "venue": null, "citeRegEx": "Dubois et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Dubois et al\\.", "year": 1990}, {"title": "A logic for reasoning about probabilities", "author": ["R. Fagin", "J.Y. Halpern", "N. Megiddo"], "venue": null, "citeRegEx": "Fagin et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Fagin et al\\.", "year": 1990}, {"title": "Anytime deduction for probabilistic logic", "author": ["A.M. Frisch", "P. Haddawy"], "venue": null, "citeRegEx": "Frisch and Haddawy,? \\Q1994\\E", "shortCiteRegEx": "Frisch and Haddawy", "year": 1994}, {"title": "Computers and Intractability: A Guide", "author": ["M.R. Garey", "D.S. Johnson"], "venue": null, "citeRegEx": "Garey and Johnson,? \\Q1979\\E", "shortCiteRegEx": "Garey and Johnson", "year": 1979}, {"title": "Conditional events in probability assessment and revi", "author": ["A. Gilio", "R. Scozzafava"], "venue": null, "citeRegEx": "Gilio and Scozzafava,? \\Q1994\\E", "shortCiteRegEx": "Gilio and Scozzafava", "year": 1994}, {"title": "An analysis of rst-order logics of probability", "author": ["J.Y. Halpern"], "venue": "Arti cal Intelligence,", "citeRegEx": "Halpern,? 1990", "shortCiteRegEx": "Halpern", "year": 1990}, {"title": "Probabilistic description logics", "author": ["J. Heinsohn"], "venue": "Proceedings of the 10th Conference", "citeRegEx": "Heinsohn,? 1994", "shortCiteRegEx": "Heinsohn", "year": 1994}, {"title": "Column generation methods", "author": ["B. Jaumard", "P. Hansen", "M.P. de Arag~ao"], "venue": null, "citeRegEx": "Jaumard et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Jaumard et al\\.", "year": 1991}, {"title": "A linear programming approach to reasoning", "author": ["D. Kavvadias", "C.H. Papadimitriou"], "venue": null, "citeRegEx": "Kavvadias and Papadimitriou,? \\Q1990\\E", "shortCiteRegEx": "Kavvadias and Papadimitriou", "year": 1990}, {"title": "Precision of Probabilistic Deduction under Taxonomic Knowledge", "author": ["T. Lukasiewicz"], "venue": null, "citeRegEx": "Lukasiewicz,? \\Q1996\\E", "shortCiteRegEx": "Lukasiewicz", "year": 1996}, {"title": "E cient global probabilistic deduction from taxonomic and prob", "author": ["T. Lukasiewicz"], "venue": null, "citeRegEx": "Lukasiewicz,? \\Q1997\\E", "shortCiteRegEx": "Lukasiewicz", "year": 1997}, {"title": "Magic inference rules for probabilistic deduction under taxonomic", "author": ["T. Lukasiewicz"], "venue": null, "citeRegEx": "Lukasiewicz,? \\Q1998\\E", "shortCiteRegEx": "Lukasiewicz", "year": 1998}, {"title": "Many-valued rst-order logics with probabilistic semantics", "author": ["T. Lukasiewicz"], "venue": "Pro-", "citeRegEx": "Lukasiewicz,? 1998b", "shortCiteRegEx": "Lukasiewicz", "year": 1998}, {"title": "Probabilistic deduction with conditional constraints over basic", "author": ["T. Lukasiewicz"], "venue": null, "citeRegEx": "Lukasiewicz,? \\Q1998\\E", "shortCiteRegEx": "Lukasiewicz", "year": 1998}, {"title": "Probabilistic logic programming", "author": ["T. Lukasiewicz"], "venue": "Proceedings of the 13th Eu-", "citeRegEx": "Lukasiewicz,? 1998d", "shortCiteRegEx": "Lukasiewicz", "year": 1998}, {"title": "Local probabilistic deduction from taxonomic and probabilistic", "author": ["T. Lukasiewicz"], "venue": null, "citeRegEx": "Lukasiewicz,? \\Q1999\\E", "shortCiteRegEx": "Lukasiewicz", "year": 1999}, {"title": "Probabilistic and truth-functional many-valued logic program", "author": ["T. Lukasiewicz"], "venue": null, "citeRegEx": "Lukasiewicz,? \\Q1999\\E", "shortCiteRegEx": "Lukasiewicz", "year": 1999}, {"title": "Computation of best bounds", "author": ["C. Luo", "C. Yu", "J. Lobo", "G. Wang", "T. Pham"], "venue": null, "citeRegEx": "Luo et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Luo et al\\.", "year": 1996}, {"title": "A semantical framework for supporting subjective", "author": ["R.T. Ng", "V.S. Subrahmanian"], "venue": null, "citeRegEx": "Ng and Subrahmanian,? \\Q1993\\E", "shortCiteRegEx": "Ng and Subrahmanian", "year": 1993}, {"title": "Stable semantics for probabilistic deductive", "author": ["R.T. Ng", "V.S. Subrahmanian"], "venue": null, "citeRegEx": "Ng and Subrahmanian,? \\Q1994\\E", "shortCiteRegEx": "Ng and Subrahmanian", "year": 1994}, {"title": "Probabilistic logic", "author": ["N.J. Nilsson"], "venue": "Arti cial Intelligence, 28, 71{88.", "citeRegEx": "Nilsson,? 1986", "shortCiteRegEx": "Nilsson", "year": 1986}, {"title": "Probabilistic logic revisited", "author": ["N.J. Nilsson"], "venue": "Arti cial Intelligence, 59, 39{42.", "citeRegEx": "Nilsson,? 1993", "shortCiteRegEx": "Nilsson", "year": 1993}, {"title": "Probabilistic Logic", "author": ["G. Paa"], "venue": "Dubois, D., Smets, P., Mamdani, A., & Prade,", "citeRegEx": "Paa,? 1988", "shortCiteRegEx": "Paa", "year": 1988}, {"title": "Combinatorial Optimization, Algorithms", "author": ["C.H. Papadimitriou", "K. Steiglitz"], "venue": null, "citeRegEx": "Papadimitriou and Steiglitz,? \\Q1982\\E", "shortCiteRegEx": "Papadimitriou and Steiglitz", "year": 1982}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Anytime decision making with imprecise probabilities", "author": ["M. Pittarelli"], "venue": "Proceedings", "citeRegEx": "Pittarelli,? 1994", "shortCiteRegEx": "Pittarelli", "year": 1994}, {"title": "Theory of Linear and Integer Programming", "author": ["A. Schrijver"], "venue": "Wiley, New York.", "citeRegEx": "Schrijver,? 1986", "shortCiteRegEx": "Schrijver", "year": 1986}, {"title": "Precise Conclusion under Uncertainty and Incompleteness in Deductive", "author": ["H. one"], "venue": null, "citeRegEx": "one,? \\Q1994\\E", "shortCiteRegEx": "one", "year": 1994}, {"title": "On cautious probabilistic inference and", "author": ["U. untzer"], "venue": null, "citeRegEx": "untzer,? \\Q1995\\E", "shortCiteRegEx": "untzer", "year": 1995}, {"title": "Computing probability intervals under independency constraints", "author": ["L. van der Gaag"], "venue": null, "citeRegEx": "Gaag,? \\Q1991\\E", "shortCiteRegEx": "Gaag", "year": 1991}, {"title": "Statistical Reasoning with Imprecise Probabilities", "author": ["P. Walley"], "venue": "Chapman and Hall,", "citeRegEx": "Walley,? 1991", "shortCiteRegEx": "Walley", "year": 1991}], "referenceMentions": [{"referenceID": 16, "context": "Journal of Arti cial Intelligence Research 10 (1999) 199-241 Submitted 9/98; published 4/99 Probabilistic Deduction with Conditional Constraints over Basic Events Thomas Lukasiewicz lukasiewicz@informatik.uni-giessen.de Institut f\u007f ur Informatik, Universit\u007fat Gie en Arndtstra e 2, D-35392 Gie en, Germany Abstract We study the problem of probabilistic deduction with conditional constraints over basic events. We show that globally complete probabilistic deduction with conditional constraints over basic events is NP-hard. We then concentrate on the special case of probabilistic deduction in conditional constraint trees. We elaborate very e cient techniques for globally complete probabilistic deduction. In detail, for conditional constraint trees with point probabilities, we present a local approach to globally complete probabilistic deduction, which runs in linear time in the size of the conditional constraint trees. For conditional constraint trees with interval probabilities, we show that globally complete probabilistic deduction can be done in a global approach by solving nonlinear programs. We show how these nonlinear programs can be transformed into equivalent linear programs, which are solvable in polynomial time in the size of the conditional constraint trees. 1. Introduction Dealing with uncertain knowledge plays an important role in knowledge representation and reasoning. There are many di erent formalisms and methodologies for handling uncertainty. Most of them are directly or indirectly based on probability theory. In this paper, we focus on probabilistic deduction with conditional constraints over basic events (that is, interval restrictions for conditional probabilities of elementary events). The considered probabilistic deduction problems consist of a probabilistic knowledge base and a probabilistic query. We give a classical example. As a probabilistic knowledge base, we may take the probabilistic knowledge that all ostriches are birds, that the probability of Tweety being a bird is greater than 0.90, and that the probability of Tweety being an ostrich provided she is a bird is greater than 0.80. As a probabilistic query, we may now wonder about the entailed greatest lower and least upper bound for the probability that Tweety is an ostrich. The solution to this probabilistic deduction problem is 0.72 for the entailed greatest lower bound and 1.00 for the entailed least upper bound. More generally, probabilistic deduction with conditional constraints over propositional events can be done in a global approach by linear programming or in a local approach by the iterative application of inference rules. Note that it is immediately NP-hard, since it generalizes the satis ability problem for classical propositional logic (see Section 2.2). Research on the global approach spread in particular after the important work on probabilistic logic by Nilsson (1986) (see also the work by Paa , 1988).", "startOffset": 170, "endOffset": 2915}, {"referenceID": 24, "context": "Other work on the global approach concentrates on reducing the number of linear constraints (Luo et al. 1996) and the number of variables (Lukasiewicz, 1997).", "startOffset": 92, "endOffset": 109}, {"referenceID": 17, "context": "1996) and the number of variables (Lukasiewicz, 1997).", "startOffset": 34, "endOffset": 53}, {"referenceID": 1, "context": "For this reason, subsequent research on inference rules especially aims at analyzing patterns of human commonsense reasoning (Dubois et al. 1990, 1993; Amarger et al. 1991; Th\u007fone, 1994; Th\u007f one et al. 1995).", "startOffset": 125, "endOffset": 207}, {"referenceID": 13, "context": "Recent work on inference rules concentrates on integrating probabilistic knowledge into description logics (Heinsohn, 1994) and on analyzing the interplay between taxonomic and probabilistic deduction (Lukasiewicz 1998a, 1999a).", "startOffset": 107, "endOffset": 123}, {"referenceID": 1, "context": "A main advantage of the local approach is that the deduced results can be explained in a natural way by the sequence of applied inference rules (Amarger et al. 1991; Frisch & Haddawy, 1994).", "startOffset": 144, "endOffset": 189}, {"referenceID": 8, "context": "This approach was further developed by Kavvadias and Papadimitriou (1990), Jaumard et al.", "startOffset": 39, "endOffset": 74}, {"referenceID": 8, "context": "This approach was further developed by Kavvadias and Papadimitriou (1990), Jaumard et al. (1991), Andersen and Hooker (1994), and Hansen et al.", "startOffset": 75, "endOffset": 97}, {"referenceID": 8, "context": "This approach was further developed by Kavvadias and Papadimitriou (1990), Jaumard et al. (1991), Andersen and Hooker (1994), and Hansen et al.", "startOffset": 75, "endOffset": 125}, {"referenceID": 8, "context": "This approach was further developed by Kavvadias and Papadimitriou (1990), Jaumard et al. (1991), Andersen and Hooker (1994), and Hansen et al. (1995). In particular, Jaumard et al.", "startOffset": 75, "endOffset": 151}, {"referenceID": 8, "context": "This approach was further developed by Kavvadias and Papadimitriou (1990), Jaumard et al. (1991), Andersen and Hooker (1994), and Hansen et al. (1995). In particular, Jaumard et al. (1991) report promising experimental results on the e ciency in special cases of probabilistic satis ability and entailment.", "startOffset": 75, "endOffset": 189}, {"referenceID": 8, "context": "This approach was further developed by Kavvadias and Papadimitriou (1990), Jaumard et al. (1991), Andersen and Hooker (1994), and Hansen et al. (1995). In particular, Jaumard et al. (1991) report promising experimental results on the e ciency in special cases of probabilistic satis ability and entailment. Moreover, Kavvadias and Papadimitriou (1990) and Jaumard et al.", "startOffset": 75, "endOffset": 352}, {"referenceID": 8, "context": "This approach was further developed by Kavvadias and Papadimitriou (1990), Jaumard et al. (1991), Andersen and Hooker (1994), and Hansen et al. (1995). In particular, Jaumard et al. (1991) report promising experimental results on the e ciency in special cases of probabilistic satis ability and entailment. Moreover, Kavvadias and Papadimitriou (1990) and Jaumard et al. (1991) identify special cases of probabilistic satis ability that can be solved in polynomial time.", "startOffset": 75, "endOffset": 378}, {"referenceID": 5, "context": "Finally, Fagin et al. (1992) present a sound and complete axiom system for reasoning about probabilities that are expressed by linear inequalities over propositional events.", "startOffset": 9, "endOffset": 29}, {"referenceID": 4, "context": "In early work, Dubois and Prade (1988) use inference rules to model default reasoning with imprecise numerical and fuzzy quanti ers.", "startOffset": 15, "endOffset": 39}, {"referenceID": 1, "context": "1990, 1993; Amarger et al. 1991; Th\u007fone, 1994; Th\u007f one et al. 1995). Frisch and Haddawy (1994) discuss the use of inference rules for deduction in probabilistic logic.", "startOffset": 12, "endOffset": 95}, {"referenceID": 1, "context": "1990, 1993; Amarger et al. 1991; Th\u007fone, 1994; Th\u007f one et al. 1995). Frisch and Haddawy (1994) discuss the use of inference rules for deduction in probabilistic logic. Recent work on inference rules concentrates on integrating probabilistic knowledge into description logics (Heinsohn, 1994) and on analyzing the interplay between taxonomic and probabilistic deduction (Lukasiewicz 1998a, 1999a). We now summarize the main characteristics of the global and the local approach. The global approach can be performed within quite rich probabilistic languages (Fagin et al., 1992). Crucially, probabilistic deduction by linear programming is globally complete (that is, it really provides the requested tightest bounds entailed by the whole probabilistic knowledge base). However, a main drawback of the global approach is that it generally does not provide useful explanatory information on the deduction process. Finally, results on the special-case tractability of global approaches are driven by the technical possibilities of linear programming techniques and not by the needs of arti cial intelligence applications. Hence, they do not seem to be very useful in the arti cial intelligence context. A main advantage of the local approach is that the deduced results can be explained in a natural way by the sequence of applied inference rules (Amarger et al. 1991; Frisch & Haddawy, 1994). However, the iterative application of inference rules is generally restricted to quite narrow probabilistic languages. Moreover, it is very rarely and only within very restricted languages globally complete (Frisch and Haddawy, 1994, give an example of globally complete local probabilistic deduction in a very restricted framework). Finally, as far as the computational complexity is concerned, there are very few experimental and theoretical results on the special-case tractability of local approaches. The main motivating idea of this paper is to elaborate e cient local techniques for globally complete probabilistic deduction. Inspired by previous work on inference rules, we focus our research on the language of conditional constraints over basic events: Dubois and Prade (1988) study the chaining of two bidirectional conditional constraints over basic events (\\quanti ed syllogism rule\") and some of its special cases.", "startOffset": 12, "endOffset": 2177}, {"referenceID": 1, "context": "1990, 1993; Amarger et al. 1991; Th\u007fone, 1994; Th\u007f one et al. 1995). Frisch and Haddawy (1994) discuss the use of inference rules for deduction in probabilistic logic. Recent work on inference rules concentrates on integrating probabilistic knowledge into description logics (Heinsohn, 1994) and on analyzing the interplay between taxonomic and probabilistic deduction (Lukasiewicz 1998a, 1999a). We now summarize the main characteristics of the global and the local approach. The global approach can be performed within quite rich probabilistic languages (Fagin et al., 1992). Crucially, probabilistic deduction by linear programming is globally complete (that is, it really provides the requested tightest bounds entailed by the whole probabilistic knowledge base). However, a main drawback of the global approach is that it generally does not provide useful explanatory information on the deduction process. Finally, results on the special-case tractability of global approaches are driven by the technical possibilities of linear programming techniques and not by the needs of arti cial intelligence applications. Hence, they do not seem to be very useful in the arti cial intelligence context. A main advantage of the local approach is that the deduced results can be explained in a natural way by the sequence of applied inference rules (Amarger et al. 1991; Frisch & Haddawy, 1994). However, the iterative application of inference rules is generally restricted to quite narrow probabilistic languages. Moreover, it is very rarely and only within very restricted languages globally complete (Frisch and Haddawy, 1994, give an example of globally complete local probabilistic deduction in a very restricted framework). Finally, as far as the computational complexity is concerned, there are very few experimental and theoretical results on the special-case tractability of local approaches. The main motivating idea of this paper is to elaborate e cient local techniques for globally complete probabilistic deduction. Inspired by previous work on inference rules, we focus our research on the language of conditional constraints over basic events: Dubois and Prade (1988) study the chaining of two bidirectional conditional constraints over basic events (\\quanti ed syllogism rule\") and some of its special cases. Dubois et al. (1990) additionally discuss probabilistic deductions about conjunctions of basic events.", "startOffset": 12, "endOffset": 2340}, {"referenceID": 1, "context": "Amarger et al. (1991) propose to apply the \\quanti ed syllogism rule\" and the \\generalized Bayes' rule\" to sets of bidirectional conditional constraints over basic events.", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "Amarger et al. (1991) propose to apply the \\quanti ed syllogism rule\" and the \\generalized Bayes' rule\" to sets of bidirectional conditional constraints over basic events. They report promising experimental results on the global completeness and the computational complexity of the presented deduction technique. However, this deduction technique is generally not globally complete. Th\u007f one (1994) examines trees of bidirectional conditional constraints over basic events.", "startOffset": 0, "endOffset": 398}, {"referenceID": 4, "context": "Note that conditional constraints characterize conditional probabilities of events, rather than probabilities of conditional events (Coletti, 1994; Gilio & Scozzafava, 1994).", "startOffset": 132, "endOffset": 173}, {"referenceID": 4, "context": "Note that conditional constraints characterize conditional probabilities of events, rather than probabilities of conditional events (Coletti, 1994; Gilio & Scozzafava, 1994). Note also that Pr(G) = 0 always entails Pr j= (HjG)[u1; u2]. This semantics of conditional probability statements is also assumed by Halpern (1990) and by Frisch and Haddawy (1994).", "startOffset": 133, "endOffset": 323}, {"referenceID": 4, "context": "Note that conditional constraints characterize conditional probabilities of events, rather than probabilities of conditional events (Coletti, 1994; Gilio & Scozzafava, 1994). Note also that Pr(G) = 0 always entails Pr j= (HjG)[u1; u2]. This semantics of conditional probability statements is also assumed by Halpern (1990) and by Frisch and Haddawy (1994). The notions of models, satis ability, and logical consequence for conditional constraints are de ned in the classical way.", "startOffset": 133, "endOffset": 356}, {"referenceID": 16, "context": "The deduction technique of Section 4 can easily be generalized to conditional constraint trees (B;KB) that satisfy only the restriction u1> 0 i v1> 0 for all (BjA)[u1; u2]; (AjB)[v1; v2] 2 KB (Lukasiewicz, 1996).", "startOffset": 192, "endOffset": 211}, {"referenceID": 33, "context": "b) Our linear programming technique to compute the tight upper answer to premiserestricted complete queries runs in polynomial time in n: Linear programming runs in polynomial time in the size of the linear programs (Papadimitriou & Steiglitz, 1982; Schrijver, 1986), where the size of a linear program is given by its number of variables and its number of linear inequalities.", "startOffset": 216, "endOffset": 266}, {"referenceID": 1, "context": "6 Comparison with the Classical Linear Programming Approach As a comparison, we now brie y describe how probabilistic deduction in conditional constraint trees can be done by the classical linear programming approach (Paa , 1988; van der Gaag, 1991; Amarger et al. 1991; Hansen et al. 1995).", "startOffset": 217, "endOffset": 290}, {"referenceID": 31, "context": "Comparison with Bayesian Networks In this section, we brie y discuss the relationship between conditional constraint trees and Bayesian networks (Pearl, 1988).", "startOffset": 145, "endOffset": 158}, {"referenceID": 31, "context": "But, would it be possible to additionally assume certain independencies? Of course, with each exact or general conditional constraint tree (B;KB), we can associate all probabilistic interpretations Pr that are models of KB and that have additionally the undirected tree (B;$) as an I-map (Pearl, 1988).", "startOffset": 288, "endOffset": 301}, {"referenceID": 31, "context": "That is, we would have independencies without causal directionality like in Markov trees (Pearl, 1988).", "startOffset": 89, "endOffset": 102}, {"referenceID": 21, "context": "For example, probabilistic deduction from probabilistic logic programs that do not assume probabilistic independencies (Ng & Subrahmanian 1993, 1994; Lukasiewicz, 1998d) should better not be done by the iterative application of inference rules.", "startOffset": 119, "endOffset": 169}, {"referenceID": 21, "context": "Note that much more promising techniques are, for example, global techniques by linear programming (Lukasiewicz, 1998d) and in particular approximation techniques based on truth-functional many-valued logics (Lukasiewicz 1998b, 1999b).", "startOffset": 99, "endOffset": 119}, {"referenceID": 16, "context": "The claims can easily be veri ed (Lukasiewicz, 1996).", "startOffset": 33, "endOffset": 52}, {"referenceID": 16, "context": "The claims can easily be veri ed (Lukasiewicz, 1996).", "startOffset": 33, "endOffset": 52}, {"referenceID": 16, "context": "The claim can easily be veri ed (Lukasiewicz, 1996).", "startOffset": 32, "endOffset": 51}], "year": 2011, "abstractText": null, "creator": "dvipsk 5.58f Copyright 1986, 1994 Radical Eye Software"}}}