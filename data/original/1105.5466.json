{"id": "1105.5466", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2011", "title": "Issues in Stacked Generalization", "abstract": "Stacked generalization is a general method of using a high-level model to combine lower-level models to achieve greater predictive accuracy. In this paper we address two crucial issues which have been considered to be a `black art' in classification tasks ever since the introduction of stacked generalization in 1992 by Wolpert: the type of generalizer that is suitable to derive the higher-level model, and the kind of attributes that should be used as its input. We find that best results are obtained when the higher-level model combines the confidence (and not just the predictions) of the lower-level ones. We demonstrate the effectiveness of stacked generalization for combining three different types of learning algorithms for classification tasks. We also compare the performance of stacked generalization with majority vote and published results of arcing and bagging.", "histories": [["v1", "Fri, 27 May 2011 01:54:47 GMT  (104kb)", "http://arxiv.org/abs/1105.5466v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["k m ting", "i h witten"], "accepted": false, "id": "1105.5466"}, "pdf": {"name": "1105.5466.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Kai Ming Ting", "Ian H. Witten"], "emails": ["kmting@deakin.edu.au", "ihw@cs.waikato.ac.nz"], "sections": [{"heading": null, "text": "Journal of Arti cial Intelligence Research 10 (1999) 271-289 Submitted 11/98; published 5/99 Issues in Stacked GeneralizationKai Ming Ting kmting@deakin.edu.auSchool of Computing and MathematicsDeakin University, Australia.Ian H. Witten ihw@cs.waikato.ac.nzDepartment of Computer ScienceUniversity of Waikato, New Zealand. AbstractStacked generalization is a general method of using a high-level model to combine lower-level models to achieve greater predictive accuracy. In this paper we address two crucialissues which have been considered to be a `black art' in classi cation tasks ever since theintroduction of stacked generalization in 1992 by Wolpert: the type of generalizer that issuitable to derive the higher-level model, and the kind of attributes that should be used asits input. We nd that best results are obtained when the higher-level model combines thecon dence (and not just the predictions) of the lower-level ones.We demonstrate the e ectiveness of stacked generalization for combining three di erenttypes of learning algorithms for classi cation tasks. We also compare the performance ofstacked generalization with majority vote and published results of arcing and bagging.1. IntroductionStacked generalization is a way of combining multiple models that have been learned for aclassi cation task (Wolpert, 1992), which has also been used for regression (Breiman, 1996a)and even unsupervised learning (Smyth & Wolpert, 1997). Typically, di erent learningalgorithms learn di erent models for the task at hand, and in the most common form ofstacking the rst step is to collect the output of each model into a new set of data. For eachinstance in the original training set, this data set represents every model's prediction of thatinstance's class, along with its true classi cation. During this step, care is taken to ensurethat the models are formed from a batch of training data that does not include the instancein question, in just the same way as ordinary cross-validation. The new data are treatedas the data for another learning problem, and in the second step a learning algorithm isemployed to solve this problem. In Wolpert's terminology, the original data and the modelsconstructed for them in the rst step are referred to as level-0 data and level-0 models,respectively, while the set of cross-validated data and the second-stage learning algorithmare referred to as level-1 data and the level-1 generalizer.In this paper, we show how to make stacked generalization work for classi cation tasksby addressing two crucial issues which Wolpert (1992) originally described as `black art'and have not been resolved since. The two issues are (i) the type of attributes that shouldbe used to form level-1 data, and (ii) the type of level-1 generalizer in order to get improvedaccuracy using the stacked generalization method.Breiman (1996a) demonstrated the success of stacked generalization in the setting ofordinary regression. The level-0 models are regression trees of di erent sizes or linearc 1999 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.\nTing & Wittenregressions using di erent number of variables. But instead of selecting the single modelthat works best as judged by (for example) cross-validation, Breiman used the di erent level-0 regressors' output values for each member of the training set to form level-1 data. Thenhe used least-squares linear regression, under the constraint that all regression coe cientsbe non-negative, as the level-1 generalizer. The non-negativity constraint turned out to becrucial to guarantee that the predictive accuracy would be better than that achieved byselecting the single best predictor.Here we show how stacked generalization can be made to work reliably in classi cationtasks. We do this by using the output class probabilities generated by level-0 models toform level-1 data. Then for the level-1 generalizer we use a version of least squares linearregression adapted for classi cation tasks. We nd the use of class probabilities to be crucialfor the successful application of stacked generalization in classi cation tasks. However,the non-negativity constraints found necessary by Breiman in regression are found to beirrelevant to improved predictive accuracy in our classi cation situation.In Section 2, we formally introduce the technique of stacked generalization and describepertinent details of each learning algorithm used in our experiments. Section 3 describesthe results of stacking three di erent types of learning algorithms. Section 4 comparesstacked generalization with arcing and bagging, two recent methods that employ samplingtechniques to modify the data distribution in order to produce multiple models from a singlelearning algorithm. The following section describes related work, and the paper ends witha summary of our conclusions.2. Stacked GeneralizationGiven a data set L = f(yn; xn); n = 1; : : : ; Ng, where yn is the class value and xn is a vectorrepresenting the attribute values of the nth instance, randomly split the data into J almostequal parts L1; : : : ;LJ . De ne Lj and L( j) = L Lj to be the test and training sets forthe jth fold of a J -fold cross-validation. Given K learning algorithms, which we call level-0generalizers, invoke the kth algorithm on the data in the training set L( j) to induce amodel M( j)k , for k = 1; : : : ;K. These are called level-0 models.For each instance xn in Lj, the test set for the jth cross-validation fold, let zkn denotethe prediction of the model M( j)k on xn. At the end of the entire cross-validation process,the data set assembled from the outputs of the K models isLCV = f(yn; z1n; : : : ; zKn); n = 1; : : : ; Ng:These are the level-1 data. Use some learning algorithm that we call the level-1 generalizerto derive from these data a model ~M for y as a function of (z1; : : : ; zK). This is the level-1model. Figure 1 illustrates the cross-validation process. To complete the training process,the nal level-0 models Mk, k = 1; : : : ;K, are derived using all the data in L.Now let us consider the classi cation process, which uses the modelsMk, k = 1; : : : ;K,in conjunction with ~M. Given a new instance, models Mk produce a vector (z1; : : : ; zK).This vector is input to the level-1 model ~M, whose output is the nal classi cation result forthat instance. This completes the stacked generalization method as proposed by Wolpert(1992), and also used by Breiman (1996a) and LeBlanc & Tibshirani (1993).272\nIssues in Stacked Generalization\nM MM1 k K\nM ~ L\nL\nCV\n(-j)\n(-j) (-j) (-j)\nLevel-1 Level-0\nFigure 1: This gure illustrates the j-fold cross-validation process in level-0; and the level-1data set LCV at the end of this process is used to produce level-1 model ~M.As well as the situation described above, which results in the level-1 model ~M, thepresent paper also considers a further situation where the output from the level-0 models isa set of class probabilities rather than a single class prediction. If model M( j)k is used toclassify an instance x in Lj , let Pki(x) denote the probability of the ith output class, andthe vector Pkn = (Pk1(xn); : : : ; Pki(xn); : : : ; PkI(xn))gives the model's class probabilities for the nth instance, assuming that there are I classes.As the level-1 data, assemble together the class probability vector from all K models, alongwith the actual class:L0CV = f(yn;P1n; : : : ;Pkn; : : : ;PKn); n = 1; : : : ; Ng:Denote the level-1 model derived from this as ~M0 to contrast it with ~M.The following two subsections describe the algorithms used as level-0 and level-1 gener-alizers in the experiments reported in Section 3.2.1 Level-0 GeneralizersThree learning algorithms are used as the level-0 generalizers: C4.5, a decision tree learningalgorithm (Quinlan, 1993); NB, a re-implementation of a Naive Bayesian classi er (Cestnik,1990); and IB1, a variant of a lazy learning algorithm (Aha, Kibler & Albert, 1991) whichemploys the p-nearest-neighbor method using a modi ed value-di erence metric for nominaland binary attributes (Cost & Salzberg, 1993). For each of these learning algorithms wenow show the formula that we use for the estimated output class probabilities Pi(x) for aninstance x (where, in all cases, Pi Pi(x) = 1).C4.5: Consider the leaf of the decision tree at which the instance x falls. Let mi be thenumber of (training) instances with class i at this leaf, and suppose the majority class273\nTing & Wittenat the leaf is I\u0302. Let E =Pi 6=I\u0302 mi. Then, using a Laplace estimator,PI\u0302(x) = 1 E + 1Pimi + 2 ;Pi(x) = (1 PI\u0302(x)) miE ; for i 6= I\u0302 :Note that only pruned trees and default settings of C4.5 are used in our experiments.NB: Let P (ijx) be the posterior probability of class i, given instance x. ThenPi(x) = P (ijx)Pi P (ijx) :Note that NB uses a Laplacian estimate for estimating the conditional probabilitiesfor each nominal attribute to compute P (ijx). For each continuous-valued attribute,a normal distribution is assumed in which case the conditional probabilities can beconveniently represented entirely in terms of the mean and variance of the observedvalues for each class.IB1: Suppose p nearest neighbors are used; denote them by f(ys; xs); s = 1; : : : ; pg forinstance x. (We use p = 3 in the experiments.) ThenPi(x) = Pps=1 f(ys)=d(x; xs)Pps=1 1=d(x; xs) ;where f(ys) = 1 if i = ys and 0 otherwise, and d is the Euclidean distance function.In all three learning algorithms, the predicted class of the level-0 model, given an instancex, is that I\u0302 for which PI\u0302(x) > Pi(x) for all i 6= I\u0302 :2.2 Level-1 GeneralizersWe compare the e ect of four di erent learning algorithms as the level-1 generalizer: C4.5,IB1(using p = 21 nearest neighbors),1 NB, and a multi-response linear regression algorithm,MLR. Only the last needs further explanation.MLR is an adaptation of a least-squares linear regression algorithm that Breiman (1996a)used in regression settings. Any classi cation problem with real-valued attributes can betransformed into a multi-response regression problem. If the original classi cation problemhas I classes, it is converted into I separate regression problems, where the problem forclass ` has instances with responses equal to one when they have class ` and zero otherwise.The input to MLR is level-1 data, and we need to consider the situation for the model~M0, where the attributes are probabilities, separately from that for the model ~M, where1. A large p value is used following Wolpert's (1992) advice that \\: : : it is reasonable that `relatively global,smooth : : : ' level-1 generalizers should perform well.\"274\nIssues in Stacked Generalizationthey are classes. In the former case, where the attributes are already real-valued, the linearregression for class ` is simply LR`(x) = KXk k`Pk`(x):In the latter case, the classes are unordered nominal attributes. We map them into binaryvalues in the obvious way, setting Pk`(x) to 1 if the class of instance x is ` and zero otherwise;and then use the above linear regression.Choose the linear regression coe cients f k`g to minimizeXj X(yn;xn)2Lj(yn Xk k`P ( j)k` (xn))2:The coe cients f k`g are constrained to be non-negative, following Breiman's (1996a) dis-covery that this is necessary for the successful application of stacked generalization to regres-sion problems. The non-negative-coe cient least-squares algorithm described by Lawson& Hanson (1995) is employed here to derive the linear regression for each class. We showlater that, in fact, the non-negative constraint is unnecessary in classi cation tasks.With this in place, we can now describe the working of MLR. To classify a new instancex, compute LR`(x) for all I classes and assign the instance to that class ` which has thegreatest value:2 LR`(x) > LR`0(x) for all `0 6= `:In the next section we investigate the stacking of C4.5, NB and IB1.3. Stacking C4.5, NB and IB13.1 When Does Stacked Generalization Work?The experiments in this section show that for successful stacked generalization it is necessary to use output class prob-abilities rather than class predictions|that is, ~M0 rather than ~M; only the MLR algorithm is suitable for the level-1 generalizer, among the fouralgorithms used.We use two arti cial datasets and eight real-world datasets from the UCI Repository ofmachine learning databases (Blake, Keogh & Merz, 1998). Details of these are given inTable 1.For the arti cial datasets|Led24 and Waveform|each training dataset L of size 200and 300, respectively, is generated using a di erent seed. The algorithms used for theexperiments are then tested on a separate dataset of 5000 instances. Results are expressedas the average error rate of ten repetitions of this entire procedure.For the real-world datasets, W -fold cross-validation is performed. In each fold of thiscross-validation, the training dataset is used as L, and the models derived are evaluated2. The pattern recognition community calls this type of classi er a linear machine (Duda & Hart, 1973).275\nTing & WittenDatasets # Samples # Classes # Attr & TypeLed24 200/5000 10 10NWaveform 300/5000 3 40CHorse 368 2 3B+12N+7CCredit 690 2 4B+5N+6CVowel 990 11 10CEuthyroid 3163 2 18B+7CSplice 3177 3 60NAbalone 4177 3 1N+7CNettalk(s) 5438 5 7NCoding 20000 2 15NN-nominal; B-binary; C-Continuous.Table 1: Details of the datasets used in the experiment.on the test dataset. The result is expressed as the average error rate of the W -fold cross-validation. Note that this cross-validation is used for evaluation of the entire procedure,whereas the J -fold cross-validation mentioned in Section 2 is the internal operation ofstacked generalization. However, both W and J are set to 10 in the experiments.In this section, we present results of model combination using level-1 models ~M and~M0, as well as a model selection method, employing the same J -fold cross-validation pro-cedure. Note that the only di erence between model combination and model selection hereis whether the level-1 learning is employed or not.Table 2 shows the average error rates, obtained using W -fold cross-validation, of C4.5,NB and IB1, and BestCV, which is the best of the three, selected using J -fold cross-validation. As expected, BestCV is almost always the classi er with the lowest error rate.3Table 3 shows the result of stacked generalization using the level-1 model ~M, for whichthe level-1 data comprise the classi cations generated by the level-0 models, and ~M0, forwhich the level-1 data comprise the probabilities generated by the level-0 models. Resultsare shown for all four level-1 generalizers in each case, along with BestCV. The lowest errorrate for each dataset is given in bold.Table 4 summarizes the results in Table 3 in terms of a comparison of each level-1model with BestCV totaled over all datasets. Clearly, the best level-1 model is ~M0 derivedusing MLR. It performs better than BestCV in nine datasets and equally well in the tenth.The best performing ~M is derived from NB, which performs better than BestCV in sevendatasets but signi cantly worse in two (Waveform and Vowel). We regard a di erence ofmore than two standard errors as signi cant (95% con dence). The standard error guresare omitted in this table to increase readability.The datasets are shown in the order of increasing size. MLR performs signi cantlybetter than BestCV in the four largest datasets. This indicates that stacked generalizationis more likely to give signi cant improvements in predictive accuracy if the volume of datais large|a direct consequence of more accurate estimation using cross-validation.3. Note that BestCV does not always select the same classi er in all W folds. That is why its error rate isnot always equal to the lowest error rate among the three classi ers.276\nIssues in Stacked Generalization Datasets Level-0 GeneralizersC4.5 NB IB1 BestCVLed24 35.4 35.4 32.2 32.8 0.6Waveform 31.8 17.1 26.2 17.1 0.3Horse 15.8 17.9 15.8 17.1 1.6Credit 17.4 17.3 28.1 17.4 1.2Vowel 22.7 51.0 2.6 2.6 0.2Euthyroid 1.9 9.8 8.6 1.9 0.3Splice 5.5 4.5 4.7 4.5 0.4Abalone 41.4 42.1 40.5 40.1 0.6Nettalk(s) 17.0 15.9 12.7 12.7 0.4Coding 27.6 28.8 25.0 25.0 0.3Table 2: Average error rates of C4.5, NB and IB1, and BestCV|the best among themselected using J -fold cross-validation. The standard errors are shown in the lastcolumn.Datasets Level-1 model, ~M Level-1 model, ~M0BestCV C4.5 NB IB1 MLR C4.5 NB IB1 MLRLed24 32.8 34.0 32.4 35.0 33.3 41.7 35.7 32.1 31.3Waveform 17.1 17.7 19.2 18.7 17.2 20.6 17.6 17.8 16.8Horse 17.1 16.9 14.9 17.6 16.3 18.0 18.5 17.7 15.2Credit 17.4 18.4 16.1 16.9 17.4 15.4 15.9 14.3 16.2Vowel 2.6 2.6 3.8 3.6 2.6 2.7 7.2 3.3 2.5Euthyroid 1.9 1.9 1.9 1.9 1.9 2.2 4.3 2.0 1.9Splice 4.5 3.9 3.9 3.8 3.8 4.0 3.9 3.8 3.8Abalone 40.1 38.5 38.5 38.2 38.1 43.3 37.1 39.2 38.3Nettalk(s) 12.7 12.4 11.9 12.4 12.6 14.0 14.6 12.0 11.5Coding 25.0 23.2 23.1 23.2 23.2 22.3 21.2 21.2 20.7Table 3: Average error rates for stacking C4.5, NB and IB1.Level-1 model, ~M Level-1 model, ~M0C4.5 NB IB1 MLR C4.5 NB IB1 MLR#Win vs. #Loss 3-5 2-7 4-5 2-5 7-3 6-4 4-6 0-9Table 4: Summary of Table 3|Comparison of BestCV with ~M and ~M0.\n277\nTing & WittenWhen one of the level-0 models performs signi cantly much better than the rest, like inthe Euthyroid and Vowel datasets, MLR performs either as good as BestCV by selectingthe best performing level-0 model, or better than BestCV.MLR has an advantage over the other three level-1 generalizers in that its model caneasily be interpreted. Examples of the combination weights it derives (for the probability-based model ~M0) appear in Table 5 for the Horse, Credit, Splice, Abalone, Waveform, Led24and Vowel datasets. The weights indicate the relative importance of the level-0 generalizersfor each prediction class. For example, in the Splice dataset (in Table 5(b)), NB is thedominant generalizer for predicting class 2, NB and IB1 are both good at predicting class3, and all three generalizers make a worthwhile contribution to the prediction of class 1.In contrast, in the Abalone dataset all three generalizers contribute substantially to theprediction of all three classes. Note that the weights for each class do not sum to onebecause no such constraint is imposed on MLR.3.2 Are Non-negativity Constraints Necessary?Both Breiman (1996a) and LeBlanc & Tibshirani (1993) use the stacked generalizationmethod in a regression setting and report that it is necessary to constrain the regressioncoe cients to be non-negative in order to guarantee that stacked regression improves pre-dictive accuracy. Here we investigate this nding in the domain of classi cation tasks.To assess the e ect of the non-negativity constraint on performance, three versions ofMLR are employed to derive the level-1 model ~M0:i. each linear regression in MLR is calculated with an intercept constant (that is,I + 1 weights for the I classes) but without any constraints;ii. each linear regression is derived with neither an intercept constant (I weightsfor I classes) nor constraints;iii. each linear regression is derived without an intercept constant, but with non-negativity constraints (I non-negative weights for I classes).The third version is the one used for the results presented earlier. Table 6 shows theresults of all three versions. They all have almost indistinguishable error rates. We concludethat in classi cation tasks, non-negativity constraints are not necessary to guarantee thatstacked generalization improves predictive accuracy.However, there is another reason why it is a good idea to employ non-negativity con-straints. Table 7 shows an example of the weights derived by these three versions of MLR onthe Led24 dataset. The third version, shown in column (iii), supports a more perspicuousinterpretation of each level-0 generalizer's contribution to the class predictions than do theother two. In this dataset, IB1 is the dominant generalizer in predicting classes 4, 5 and 8,and both NB and IB1 make a worthwhile contribution in predicting class 2, as evidencedby their high weights. However, the negative weights used in predicting these classes renderthe interpretation of the other two versions much less clear.278\nIssues in Stacked Generalization Horse CreditClass C4.5 NB IB1 C4.5 NB IB11 0.36 0.20 0.42 0.63 0.30 0.042 0.39 0.19 0.41 0.65 0.28 0.07C4.5 for 1; NB for 2; IB1 for 3.Table 5: (a) Weights generated by MLR (model ~M0) for the Horse and Credit datasets.Splice Abalone WaveformClass C4.5 NB IB1 C4.5 NB IB1 C4.5 NB IB11 0.23 0.43 0.36 0.25 0.25 0.39 0.16 0.59 0.342 0.15 0.72 0.12 0.27 0.20 0.25 0.14 0.72 0.073 0.08 0.52 0.40 0.30 0.18 0.39 0.04 0.65 0.23Table 5: (b) Weights generated by MLR (model ~M0) for the Splice, Abalone and Waveformdatasets. Led24 VowelClass C4.5 NB IB1 C4.5 NB IB11 0.46 0.65 0.00 0.04 0.00 0.962 0.00 0.37 0.43 0.03 0.00 0.973 0.47 0.00 0.54 0.01 0.00 1.004 0.00 0.13 0.65 0.05 0.25 0.865 0.00 0.19 0.64 0.01 0.08 0.976 0.35 0.14 0.35 0.15 0.00 0.927 0.15 0.43 0.36 0.03 0.01 1.028 0.00 0.00 0.68 0.04 0.01 0.969 0.00 0.38 0.29 0.03 0.00 1.0210 0.00 0.50 0.24 0.08 0.01 0.9311 { { { 0.00 0.04 0.96Table 5: (c) Weights generated by MLR (model ~M0) for the Led24 and Vowel datasets.\n279\nTing & Witten Datasets MLR withNo Constraints No Intercept Non-NegativityLed24 31.4 31.4 31.3Waveform 16.8 16.8 16.8Horse 15.8 15.8 15.2Credit 16.2 16.2 16.2Vowel 2.4 2.4 2.5Euthyroid 1.9 1.9 1.9Splice 3.7 3.8 3.8Abalone 38.3 38.3 38.3Nettalk(s) 11.6 11.5 11.5Coding 20.7 20.7 20.7Table 6: Average error rates of three versions of MLR.(i) (ii) (iii)Class 0 1 2 3 1 2 3 1 2 31 0.00 0.45 0.65 0.00 0.46 0.65 0.00 0.46 0.65 0.002 0.02 {0.42 0.47 0.56 {0.40 0.49 0.56 0.00 0.37 0.433 0.00 0.46 {0.01 0.54 0.47 {0.01 0.54 0.47 0.00 0.544 0.04 {0.33 0.15 0.84 {0.29 0.21 0.81 0.00 0.13 0.655 0.03 {0.37 0.26 0.84 {0.32 0.26 0.84 0.00 0.19 0.646 0.01 0.35 0.12 0.35 0.36 0.14 0.35 0.35 0.14 0.357 0.01 0.15 0.43 0.36 0.15 0.43 0.36 0.15 0.43 0.368 0.02 {0.05 {0.25 0.72 {0.03 {0.19 0.72 0.00 0.00 0.689 0.04 {0.08 0.32 0.32 {0.05 0.40 0.30 0.00 0.38 0.2910 0.04 {0.06 0.43 0.25 {0.01 0.50 0.24 0.00 0.50 0.24Table 7: Weights generated by three versions of MLR: (i) no constraints, (ii) no intercept,and (iii) non-negativity constraints, for the LED24 dataset.\n280\nIssues in Stacked GeneralizationDataset #SE BestCV Majority MLRHorse 0.5 17.1 15.0 15.2Splice 2.5 4.5 4.0 3.8Abalone 3.3 40.1 39.0 38.3Led24 8.7 32.8 31.8 31.3Credit 8.9 17.4 16.1 16.2Nettalk(s) 10.8 12.7 12.2 11.5Coding 12.7 25.0 23.1 20.7Waveform 18.7 17.1 19.5 16.8Euthyroid 26.3 1.9 8.1 1.9Vowel 242.0 2.6 13.0 2.5Table 8: Average error rates of BestCV, Majority Vote and MLR (model ~M0), along withthe number of standard error (#SE) between BestCV and the worst performinglevel-0 generalizers. 3.3 How Does Stacked Generalization Compare To Majority Vote?Let us now compare the error rate of ~M0, derived from MLR, to that of majority vote,a simple decision combination method which requires neither cross-validation nor level-1 learning. Table 8 shows the average error rates of BestCV, majority vote and MLR.In order to see whether the relative performances of level-0 generalizers have any e ecton these methods, the number of standard errors (#SE) between the error rates of theworst performing level-0 generalizer and BestCV is given, and the datasets are re-orderedaccording to this measure. Since BestCV almost always selects the best performing level-0generalizer, small values of #SE indicate that the level-0 generalizers perform comparablyto one another, and vice versa.MLR compares favorably to majority vote, with eight wins versus two losses. Out ofthe eight wins, six have signi cant di erences (the two exceptions are for the Splice andLed24 datasets); whereas both losses (for the Horse and Credit datasets) have insigni cantdi erences. Thus the extra computation for cross-validation and level-1 learning seems tohave paid o .It is interesting to note that the performance of majority vote is related to the size of#SE. Majority vote compares favorably to BestCV in the rst seven datasets, where thevalues of #SE are small. In the last three, where #SE is large, majority vote performsworse. This indicates that if the level-0 generalizers perform comparably, it is not worthusing cross-validation to determine the best one, because the result of majority vote|whichis far cheaper|is not signi cantly di erent. Although small values of #SE are a necessarycondition for majority vote to rival BestCV, they are not a su cient condition|see Matan(1996) for an example. The same applies when majority vote is compared with MLR. MLRperforms signi cantly better in the ve datasets that have large #SE values, but in onlyone of the other cases. 281\nTing & Witten~M versus ~M0C4.5 NB IB1 MLR#Win vs. #Loss 8-2 5-4 3-6 1-7Table 9: ~M versus ~M0 for each generalizer|summarized results from Table 3.It is worth mentioning a method that averages Pi(x) for each i over all level-0 models,yielding Pi(x), and then predicts class I\u0302 for which PI\u0302(x) > Pi(x) for all i 6= I\u0302 : According toBreiman (1996b), this method produces an error rate almost identical to that of majorityvote.3.4 Why Does Stacked Generalization Work Best With ~M0 Generated FromMLR?We have shown that stacked generalization works best when output class probabilities(rather than class predictions) are used with the MLR algorithm (rather than C4.5, IB1,NB). In retrospect, this is not surprising, and can be explained intuitively as follows. Thelevel-1 model should provide a simple way of combining all the evidence available. Thisevidence includes not just the predictions, but the con dence of each level-0 model inits predictions. A linear combination is the simplest way of pooling the level-0 models'con dence, and MLR provides just that.The alternative methods of NB, C4.5, and IB1 each have shortcomings. A Bayesian ap-proach could form the basis for a suitable alternative way of pooling the level-0 models' con -dence, but the independence assumption central to Naive Bayes hampers its performance insome datasets because the evidence provided by the individual level-0 models is certainly notindependent. C4.5 builds trees that can model interaction amongst attributes|particularlywhen the tree is large|but this is not desirable for combining con dences. Nearest neigh-bor methods do not really give a way of combining con dences; also, the similarity metricemployed could misleadingly assume that two di erent sets of con dence levels are similar.Table 9 summarizes the results in Table 3 by comparing ~M with ~M0 for each level-1generalizer, across all datasets. C4.5 is clearly better o with a label-based representation,because discretizing continuous-valued attributes creates intra-attribute interaction in ad-dition to interactions between di erent attributes. The evidence from Table 9 is that NBis indi erent to the use of labels or con dences: the normal distribution assumption thatit embodies in the latter case could be another reason why it is unsuitable for combiningcon dence measures. Both MLR and IB1 handle continuous-valued attributes better thanlabel-based ones, since this is the domain in which they are designed to work.SummaryWe summarize our ndings in this section as follows. None of the four learning algorithms used to obtain model ~M perform satisfactorily.282\nIssues in Stacked Generalization MLR is the best of the four learning algorithms to use as the level-1 generalizer forobtaining the model ~M0. When obtained using MLR, ~M0 has lower predictive error rate than the best modelselected by J -fold cross-validation, for almost all datasets used in the experiments. Another advantage of MLR over the other three level-1 generalizers is its interpretability.The weights k` indicate the di erent contributions that each level-0 model k makesto the prediction classes `. Model ~M0 can be derived by MLR with or without non-negativity constraints. Suchconstraints make little di erence to the model's predictive accuracy. The use of non-negativity constraints in MLR has the advantage of interpretability. Non-negative weights k` support easier interpretation of the extent to which each modelcontributes to each prediction class. When derived using MLR, model ~M0 compares favorably with majority vote. MLR provides a method of combining the con dence generated by the level-0 models intoa nal decision. For various reasons, NB, C4.5, and IB1 are not suitable for this task.4. Comparison With Arcing And BaggingThis section compares the results of stacking C4.5, NB and IB1 with the results of arcing(called boosting by its originator, Schapire, 1990) and bagging that are reported by Breiman(1996b; 1996c). Both arcing and bagging employ sampling techniques to modify the datadistribution in order to produce multiple models from a single learning algorithm. Tocombine the decisions of the individual models, arcing uses a weighted majority vote andbagging uses an unweighted majority vote. Breiman reports that both arcing and baggingcan substantially improve the predictive accuracy of a single model derived using a baselearning algorithm.4.1 Experimental ResultsFirst we describe the di erences between the experimental procedures. Our results forstacking are averaged over ten-fold cross-validation for all datasets except Waveform, whichis averaged over ten repeated trials. Standard errors are also shown. Results for arcing andbagging are those obtained by Breiman (1996b; 1996c), which are averaged over 100 trials.In Breiman's experiments, each trial uses a random 9:1 split to form the training and testsets for all datasets except Waveform. Also note that the Waveform dataset we used has 19irrelevant attributes, but Breiman used a version without irrelevant attributes (which wouldbe expected to degrade the performance of level-0 generalizers in our experiments). In bothcases 300 training instances were used for this dataset, but we used 5000 test instanceswhereas Breiman used 1800. Arcing and bagging are done with 50 decision tree modelsderived from CART (Breiman et al., 1984) in each trial.283\nTing & WittenDataset #Samples stacking arcing baggingWaveform 300 16.8 0.2 17.8 19.3Glass 214 28.4 2.9 22.0 23.2Ionosphere 351 9.7 1.5 6.4 7.9Soybean 683 4.3 1.1 5.8 6.8Breast Cancer 699 2.7 0.8 3.2 3.7Diabetes 768 24.2 1.2 26.6 23.9Table 10: Comparing stacking with arcing and bagging classi ers.The results on six datasets are given in Table 10, and indicate that the three methodsare very competitive.4 Stacking performs better than both arcing and bagging in threedatasets (Waveform, Soybean and Breast Cancer), and is better than arcing but worse thanbagging in the Diabetes dataset. Note that stacking performs very poorly on Glass andIonosphere, two small real-world datasets. This is not surprising, because cross-validationinevitably produces poor estimates for small datasets.4.2 DiscussionLike bagging, stacking is ideal for parallel computation. The construction of each level-0model proceeds independently, no communication with the other modeling processes beingnecessary.Arcing and bagging require a considerable number of member models because theyrely on varying the data distribution to get a diverse set of models from a single learningalgorithm. Using a level-1 generalizer, stacking can work with only two or three level-0models.Suppose the computation time required for a learning algorithm is C, and arcing orbagging needs h models. The learning time required is Ta = hC. Suppose stacking requiresg models and each model employs J -fold cross-validation. Assuming that time C is neededto derive each of the g level-0 models and the level-1 model, the learning time for stackingis Ts = (g(J + 1) + 1)C. For the results given in Table 10, h = 50, J = 10, and g = 3; thusTa = 50C and Ts = 34C. However, in practice the learning time required for the level-0and level-1 generalizers may be di erent.Users of stacking have a free choice of level-0 models. They may either be derived from asingle learning algorithm, or from a variety of di erent algorithms. The example in Section3 uses di erent types of learning algorithms, while bag-stacking|stacking bagged models(Ting & Witten, 1997)|uses data variation to obtain a diverse set of models from a singlelearning algorithm. In the former case, performance may vary substantially between thelevel-0 models|for example NB performs very poorly in the Vowel and Euthyroid datasetscompared to the other two models (see Table 2). Stacking copes well with this situation.The performance variation among the member models in bagging is rather small becausethey are derived from the same learning algorithm using bootstrap samples. Section 3.34. The heart dataset used by Breiman (1996b; 1996c) is omitted because it was very much modi ed fromthe original one. 284\nIssues in Stacked Generalizationshows that a small performance variation among member models is a necessary conditionfor majority vote (as employed by bagging) to work well.It is worth noting that arcing and bagging can be incorporated into the framework ofstacked generalization by using arced or bagged models as level-0 models. Ting & Witten(1997) show one possible way of incorporating bagged models with level-1 learning, em-ploying MLR instead of voting. In this implementation, L is used as a test set for eachof the bagged models to derive level-1 data rather than the cross-validated data. This isviable because each bootstrap sample leaves out about 37% of the examples. Ting & Witten(1997) show that bag-stacking almost always has higher predictive accuracy than baggingmodels derived from either C4.5 or NB. Note that the only di erence here is whether anadaptive level-1 model or a simple majority vote is employedAccording to Breiman (1996b; 1996c), arcing and bagging can only improve the predic-tive accuracy of learning algorithms that are `unstable.'5 An unstable learning algorithmis one for which small perturbations in the training set can produce large changes in thederived model. Decision trees and neural networks are unstable; NB and IB1 are stable.Stacking works with both.While MLR is the most successful candidate for level-1 learning that we have found,other algorithms might work equally well. One candidate is neural networks. However,we have experimented with back-propagation neural networks for this purpose and foundthat they have a much slower learning rate than MLR. For example, MLR only took 2.9seconds as compare to 4790 seconds for the neural network in the nettalk dataset; whileboth have the same error rate. Other possible candidates are the multinomial logit model(Jordan & Jacobs, 1994), which is a special case of generalized linear models (McCullagh& Nelder, 1983), and the supra Bayesian procedure (Jacobs, 1995) which treats the level-0models' con dence as data that may be combined with prior distribution of level-0 modelsvia Bayes' rule.5. Related WorkOur analysis of stacked generalization was motivated by that of Breiman (1996a), discussedearlier, and LeBlanc & Tibshirani (1993). LeBlanc & Tibshirani (1993) examine the stackingof a linear discriminant and a nearest neighbor classi er and show that, for one arti cialdataset, a method similar to MLR performs better with non-negativity constraints thanwithout. Our results in Section 3.2 show that these constraints are irrelevant to MLR'spredictive accuracy in the classi cation situation.LeBlanc & Tibshirani (1993) and Ting & Witten (1997) use a version of MLR thatemploys all class probabilities from each level-0 model to induce each linear regression. Inthis case, the linear regression for class ` isLR`(x) = KXk IXi ki`Pki(x):This implementation requires the tting of KI parameters, as compared to K parametersfor the version used in this paper (see the corresponding formula in Section 2.2). Both5. Schapire, R.E., Y. Freund, P. Bartlett, & W.S. Lee (1997) provide an alternative explanation for thee ectiveness of arcing and bagging. 285\nTing & Wittenversions give comparable results in terms of predictive accuracy, but the version used inthis paper runs considerably faster because it needs to t fewer parameters.The limitations of MLR are well-known (Duda & Hart, 1973). For a I-class problem, itdivides the description space into I convex decision regions. Every region must be singlyconnected, and the decision boundaries are linear hyperplanes. This means that MLR ismost suitable for problems with unimodal probability densities. Despite these limitations,MLR still performs better as a level-1 generalizer than IB1, its nearest competitor in deriving~M0. These limitations may hold the key to a fuller understanding of the behavior of stackedgeneralization. Jacobs (1995) reviews linear combination methods like that used in MLR.Previous work on stacked generalization, especially as applied to classi cation tasks,has been limited in several ways. Some only applies to a particular dataset (e.g., Zhang,Mesirov & Waltz, 1992). Others report results that are less than convincing (Merz, 1995).Still others have a di erent focus and evaluate the results on just a few datasets (LeBlanc& Tibshirani, 1993; Chan & Stolfo, 1995; Kim & Bartlett, 1995; Fan et al., 1996).One might consider a degenerate form of stacked generalization that does not use cross-validation to produce data for level-1 learning. Then, level-1 learning can be done `on the y' during the training process (Jacobs et al., 1991). In another approach, level-1 learningtakes place in batch mode, after all level-0 models are derived (Ho et al., 1994).Several researchers have worked on a still more degenerate form of stacked generalizationwithout any cross-validation or learning at level 1. Examples are neural network ensembles(Hansen & Salamon, 1990; Perrone & Cooper, 1993; Krogh & Vedelsby, 1995), multipledecision tree combination (Kwok & Carter, 1990; Buntine, 1991; Oliver & Hand, 1995), andmultiple rule combination (Kononenko & Kova ci c, 1992). The methods used at level 1 aremajority voting, weighted averaging and Bayesian combination. Other possible methods aredistribution summation and likelihood combination. There are various forms of re-orderingclass rank, and Ali & Pazzani (1996) study some of these methods for a rule learner. Ting(1996) uses the con dence of each prediction to combine a nearest neighbor classi er and aNaive Bayesian classi er.6. ConclusionsWe have addressed two crucial issues for the successful implementation of stacked general-ization in classi cation tasks. First, class probabilities should be used instead of the singlepredicted class as input attributes for higher-level learning. The class probabilities serve asthe con dence measure for the prediction made. Second, the multi-response least squareslinear regression technique should be employed as the high-level generalizer. This techniqueprovides a method of combining level-0 models' con dence. The other three learning algo-rithms have either algorithmic limitations or are not suitable for aggregating con dences.When combining three di erent types of learning algorithms, this implementation ofstacked generalization was found to achieve better predictive accuracy than both modelselection based on cross-validation and majority vote; it was also found to be competi-tive with arcing and bagging. Unlike stacked regression, non-negativity constraints in theleast-squares regression are not necessary to guarantee improved predictive accuracy inclassi cation tasks. However, these constraints are still preferred because they increase theinterpretability of the level-1 model. 286\nIssues in Stacked GeneralizationThe implication of our successful implementation of stacked generalization is that earliermodel combination methods employing (weighted) majority vote, averaging, or other com-putations that do not make use of level-1 learning, can now apply this learning to improvetheir predictive accuracy.AcknowledgmentThe authors are grateful to the New Zealand Marsden Fund for nancial support for thisresearch. This work was conducted when the rst author was in Department of ComputerScience, University of Waikato. The authors are grateful to J. Ross Quinlan for providingC4.5 and David W. Aha for providing IB1. The anonymous reviewers and the editor haveprovided many helpful comments.ReferencesAha, D.W., D. Kibler & M.K. Albert (1991). Instance-Based Learning Algorithms. Ma-chine Learning, 6, pp. 37-66.Ali, K.M. & M.J. Pazzani (1996). Error Reduction through Learning Multiple Descrip-tions. Machine Learning, Vol. 24, No. 3, pp. 173-206.Blake, C., E. Keogh & C.J. Merz (1998). UCI Repository of machine learning databases[http:// www.ics.uci.edu/ mlearn/MLRepository.html]. Irvine, CA: University of Cal-ifornia, Department of Information and Computer Science.Breiman, L. (1996a). Stacked Regressions. Machine Learning, Vol. 24, pp. 49-64.Breiman, L. (1996b). Bagging Predictors. Machine Learning, Vol. 24, No. 2, pp. 123-140.Breiman, L. (1996c). Bias, Variance, and Arcing Classi ers. Technical Report 460. De-partment of Statistics, University of California, Berkeley, CA.Breiman, L., J.H. Friedman, R.A. Olshen & C.J. Stone (1984). Classi cation And Regres-sion Trees. Belmont, CA: Wadsworth.Cestnik, B. (1990). Estimating Probabilities: A Crucial Task in Machine Learning. InProceedings of the European Conference on Arti cial Intelligence, pp. 147-149.Chan, P.K. & S.J. Stolfo (1995). A Comparative Evaluation of Voting and Meta-learningon Partitioned Data. In Proceedings of the Twelfth International Conference on Ma-chine Learning, pp. 90-98, Morgan Kaufmann.Cost, S & S. Salzberg (1993). A Weighted Nearest Neighbor Algorithm for Learning withSymbolic Features. Machine Learning, 10, pp. 57-78.Fan, D.W., P.K. Chan, S.J. Stolfo (1996). A Comparative Evaluation of Combiner andStacked Generalization. In Proceedings of AAAI-96 workshop on Integrating MultipleLearned Models, pp. 40-46.Hansen, L.K. & P. Salamon (1990). Neural Network Ensembles. IEEE Transactions ofPattern Analysis and Machine Intelligence, 12, pp. 993-1001.287\nTing & WittenHo, T.K., J.J. Hull & S.N. Srihari (1994). Decision Combination in Multiple Classi erSystems. IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 16,No. 1, pp. 66-75.Jacobs, R.A. (1995). Methods of Combining Experts' Probability Assessments. NeuralComputation 7, pp. 867-888, MIT Press.Jacobs, R.A., M.I. Jordan, S.J. Nowlan & G.E. Hinton (1991). Adaptive Mixtures of LocalExperts. Neural Computation 3, pp. 79-87.Jacobs, R.A. & M.I. Jordan (1994). Hierachical Mixtures of Experts and the EM Algo-rithms. Neural Computation 6, pp. 181-214.Kim, K. & E.B. Bartlett (1995). Error Estimation by Series Association for Neural NetworkSystems. Neural Computation 7, pp. 799-808, MIT Press.Kononenko, I. & M. Kova ci c (1992). Learning as Optimization: Stochastic Generationof Multiple Knowledge. In Proceedings of the Ninth International Conference onMachine Learning, pp. 257-262, Morgan Kaufmann.Krogh, A. & J. Vedelsby (1995). Neural Network Ensembles, Cross Validation, and ActiveLearning. Advances in Neural Information Processing Systems 7, G. Tesauro, D.S.Touretsky & T.K. Leen (Editors), pp. 231-238, MIT Press.Kwok, S. & C. Carter (1990). Multiple Decision Trees. Uncertainty in Arti cial Intel-ligence 4, R. Shachter, T. Levitt, L. Kanal and J. Lemmer (Editors), pp. 327-335,North-Holland.Lawson C.L. & R.J. Hanson (1995). Solving Least Squares Problems. SIAM Publications.LeBlanc, M. & R. Tibshirani (1993). Combining Estimates in Regression and Classi ca-tion. Technical Report 9318. Department of Statistics, University of Toronto.Matan, O. (1996). On Voting Ensembles of Classi ers (extended abstract). In Proceedingsof AAAI-96 workshop on Integrating Multiple Learned Models, pp. 84-88.McCullagh, P. & J.A. Nelder (1983). Generalized Linear Models. London: Chapman andHall.Merz, C.J. (1995). Dynamic Learning Bias Selection. In Proceedings of the Fifth In-ternational Workshop on Arti cial Intelligence and Statistics, Ft. Lauderdale, FL:Unpublished, pp. 386-395.Oliver, J.J. & D.J. Hand (1995). On Pruning and Averaging Decision Trees. In Proceedingsof the Twelfth International Conference on Machine Learning, pp. 430-437, MorganKaufmann.Perrone, M.P. & L.N. Cooper (1993). When Networks Disagree: Ensemble Methods forHybrid Neural Networks. Arti cial Neural Networks for Speech and Vision, R.J.Mammone (Editor). Chapman-Hall.Quinlan, J.R. (1993). C4.5: Program for machine learning. Morgan Kaufmann.288\nIssues in Stacked GeneralizationSchapire, R.E. (1990). The Strength of Weak Learnability. Machine Learning, 5, pp.197-227, Kluwer Academic Publishers.Schapire, R.E., Y. Freund, P. Bartlett, & W.S. Lee (1997). Boosting the margin: A newexplanation for the e ectiveness of voting methods. In Proceedings of the FourteenthInternational Conference on Machine Learning, pages 322-330, Morgan Kaufmann.Smyth, P. & D. Wolpert (1997). Stacked Density Estimation. Advances in Neural Infor-mation Processing Systems.Ting, K.M. (1996). The Characterisation of Predictive Accuracy and Decision Combina-tion. In Proceedings of the Thirteenth International Conference on Machine Learning,pp. 498-506, Morgan Kaufmann.Ting, K.M. & I.H. Witten (1997). Stacking Bagged and Dagged Models. In Proceedings ofthe Fourteenth International Conference on Machine Learning, pp. 367-375, MorganKaufmann.Weiss S. M. & C. A. Kulikowski (1991). Computer Systems That Learns. Morgan Kauf-mann.Wolpert, D.H. (1992). Stacked Generalization. Neural Networks, Vol. 5, pp. 241-259,Pergamon Press.Zhang, X., J.P. Mesirov & D.L. Waltz (1992). Hybrid System for Protein SecondaryStructure Prediction. Journal of Molecular Biology, 225, pp. 1049-1063.\n289"}], "references": [{"title": "Instance-Based Learning Algorithms", "author": ["D.W. Aha", "M.K.D. Kibler"], "venue": null, "citeRegEx": "Aha and Kibler,? \\Q1991\\E", "shortCiteRegEx": "Aha and Kibler", "year": 1991}, {"title": "UCI Repository of machine learning databases", "author": ["C. Blake", "E. Keogh", "C.J. Merz"], "venue": null, "citeRegEx": "Blake et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Blake et al\\.", "year": 1998}, {"title": "Stacked Regressions", "author": ["L. Breiman"], "venue": "Machine Learning, Vol. 24, pp. 49-64.", "citeRegEx": "Breiman,? 1996a", "shortCiteRegEx": "Breiman", "year": 1996}, {"title": "Bagging Predictors", "author": ["L. Breiman"], "venue": "Machine Learning, Vol. 24, No. 2, pp. 123-140.", "citeRegEx": "Breiman,? 1996b", "shortCiteRegEx": "Breiman", "year": 1996}, {"title": "Bias, Variance, and Arcing Classi ers", "author": ["L. Breiman"], "venue": "Technical Report 460. De-", "citeRegEx": "Breiman,? 1996c", "shortCiteRegEx": "Breiman", "year": 1996}, {"title": "Estimating Probabilities: A Crucial Task in Machine Learning", "author": ["B. Cestnik"], "venue": "In", "citeRegEx": "Cestnik,? 1990", "shortCiteRegEx": "Cestnik", "year": 1990}, {"title": "A Comparative Evaluation of Voting and Meta-learning", "author": ["P.K. Chan", "S.J"], "venue": "Stolfo", "citeRegEx": "Chan and S.J.,? \\Q1995\\E", "shortCiteRegEx": "Chan and S.J.", "year": 1995}, {"title": "A Weighted Nearest Neighbor Algorithm for Learning", "author": ["S Cost", "S. Salzberg"], "venue": null, "citeRegEx": "Cost and Salzberg,? \\Q1993\\E", "shortCiteRegEx": "Cost and Salzberg", "year": 1993}, {"title": "A Comparative Evaluation of Combiner", "author": ["D.W. Fan", "S.J.P.K. Chan"], "venue": "Stolfo", "citeRegEx": "Fan and Chan,? \\Q1996\\E", "shortCiteRegEx": "Fan and Chan", "year": 1996}, {"title": "Neural Network Ensembles", "author": ["L.K. Hansen", "P. Salamon"], "venue": "IEEE Transactions", "citeRegEx": "Hansen and Salamon,? \\Q1990\\E", "shortCiteRegEx": "Hansen and Salamon", "year": 1990}, {"title": "Decision Combination in Multiple Classi er", "author": ["T.K. Ho", "S.N.J.J. Hull"], "venue": null, "citeRegEx": "Ho and Hull,? \\Q1994\\E", "shortCiteRegEx": "Ho and Hull", "year": 1994}, {"title": "Methods of Combining Experts' Probability Assessments", "author": ["R.A. Jacobs"], "venue": "Neural", "citeRegEx": "Jacobs,? 1995", "shortCiteRegEx": "Jacobs", "year": 1995}, {"title": "Hierachical Mixtures of Experts and the EM", "author": ["R.A. Jacobs"], "venue": "M.I. Jordan", "citeRegEx": "Jacobs,? \\Q1994\\E", "shortCiteRegEx": "Jacobs", "year": 1994}, {"title": "Error Estimation by Series Association for Neural Network", "author": ["K. Kim", "E.B"], "venue": null, "citeRegEx": "Kim and E.B.,? \\Q1995\\E", "shortCiteRegEx": "Kim and E.B.", "year": 1995}, {"title": "Neural Network Ensembles, Cross Validation, and Active", "author": ["A. Krogh"], "venue": "J. Vedelsby", "citeRegEx": "Krogh,? \\Q1995\\E", "shortCiteRegEx": "Krogh", "year": 1995}, {"title": "Multiple Decision Trees", "author": ["S. Kwok", "C. Carter"], "venue": "Uncertainty in Arti cial Intel-", "citeRegEx": "Kwok and Carter,? \\Q1990\\E", "shortCiteRegEx": "Kwok and Carter", "year": 1990}, {"title": "Combining Estimates in Regression and Classi", "author": ["M. LeBlanc", "R. Tibshirani"], "venue": null, "citeRegEx": "LeBlanc and Tibshirani,? \\Q1993\\E", "shortCiteRegEx": "LeBlanc and Tibshirani", "year": 1993}, {"title": "On Voting Ensembles of Classi ers (extended abstract)", "author": ["O. Matan"], "venue": "Proceedings", "citeRegEx": "Matan,? 1996", "shortCiteRegEx": "Matan", "year": 1996}, {"title": "Generalized Linear Models", "author": ["P. McCullagh", "J.A"], "venue": null, "citeRegEx": "McCullagh and J.A.,? \\Q1983\\E", "shortCiteRegEx": "McCullagh and J.A.", "year": 1983}, {"title": "Dynamic Learning Bias Selection", "author": ["C.J. Merz"], "venue": "Proceedings of the Fifth In-", "citeRegEx": "Merz,? 1995", "shortCiteRegEx": "Merz", "year": 1995}, {"title": "On Pruning and Averaging Decision Trees", "author": ["J.J. Oliver", "D.J"], "venue": "Hand", "citeRegEx": "Oliver and D.J.,? \\Q1995\\E", "shortCiteRegEx": "Oliver and D.J.", "year": 1995}, {"title": "When Networks Disagree: Ensemble Methods", "author": ["M.P. Perrone"], "venue": "L.N. Cooper", "citeRegEx": "Perrone,? \\Q1993\\E", "shortCiteRegEx": "Perrone", "year": 1993}, {"title": "Program for machine learning", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "Quinlan,? \\Q1993\\E", "shortCiteRegEx": "Quinlan", "year": 1993}, {"title": "The Strength of Weak Learnability", "author": ["R.E. Schapire"], "venue": "Machine Learning, 5, pp.", "citeRegEx": "Schapire,? 1990", "shortCiteRegEx": "Schapire", "year": 1990}, {"title": "Boosting the margin: A new", "author": ["R.E. Schapire", "Y. Freund", "P. Bartlett", "W.S"], "venue": null, "citeRegEx": "Schapire et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 1997}, {"title": "Stacked Density Estimation", "author": ["P. Smyth", "D. Wolpert"], "venue": "Advances in Neural Infor-", "citeRegEx": "Smyth and Wolpert,? \\Q1997\\E", "shortCiteRegEx": "Smyth and Wolpert", "year": 1997}, {"title": "The Characterisation of Predictive Accuracy and Decision Combina", "author": ["K.M. Ting"], "venue": null, "citeRegEx": "Ting,? \\Q1996\\E", "shortCiteRegEx": "Ting", "year": 1996}, {"title": "Stacking Bagged and Dagged Models", "author": ["K.M. Ting"], "venue": "I.H. Witten", "citeRegEx": "Ting,? \\Q1997\\E", "shortCiteRegEx": "Ting", "year": 1997}, {"title": "Computer Systems That Learns", "author": ["Weiss S. M", "C.A. Kulikowski"], "venue": null, "citeRegEx": "M. and Kulikowski,? \\Q1991\\E", "shortCiteRegEx": "M. and Kulikowski", "year": 1991}, {"title": "Stacked Generalization", "author": ["D.H. Wolpert"], "venue": "Neural Networks, Vol. 5, pp. 241-259,", "citeRegEx": "Wolpert,? 1992", "shortCiteRegEx": "Wolpert", "year": 1992}, {"title": "Hybrid System for Protein Secondary", "author": ["X. Zhang", "D.L.J.P. Mesirov"], "venue": "Waltz", "citeRegEx": "Zhang and Mesirov,? \\Q1992\\E", "shortCiteRegEx": "Zhang and Mesirov", "year": 1992}], "referenceMentions": [{"referenceID": 29, "context": "Introduction Stacked generalization is a way of combining multiple models that have been learned for a classi cation task (Wolpert, 1992), which has also been used for regression (Breiman, 1996a) and even unsupervised learning (Smyth & Wolpert, 1997).", "startOffset": 122, "endOffset": 137}, {"referenceID": 2, "context": "Introduction Stacked generalization is a way of combining multiple models that have been learned for a classi cation task (Wolpert, 1992), which has also been used for regression (Breiman, 1996a) and even unsupervised learning (Smyth & Wolpert, 1997).", "startOffset": 179, "endOffset": 195}, {"referenceID": 2, "context": "Introduction Stacked generalization is a way of combining multiple models that have been learned for a classi cation task (Wolpert, 1992), which has also been used for regression (Breiman, 1996a) and even unsupervised learning (Smyth & Wolpert, 1997). Typically, di erent learning algorithms learn di erent models for the task at hand, and in the most common form of stacking the rst step is to collect the output of each model into a new set of data. For each instance in the original training set, this data set represents every model's prediction of that instance's class, along with its true classi cation. During this step, care is taken to ensure that the models are formed from a batch of training data that does not include the instance in question, in just the same way as ordinary cross-validation. The new data are treated as the data for another learning problem, and in the second step a learning algorithm is employed to solve this problem. In Wolpert's terminology, the original data and the models constructed for them in the rst step are referred to as level-0 data and level-0 models, respectively, while the set of cross-validated data and the second-stage learning algorithm are referred to as level-1 data and the level-1 generalizer. In this paper, we show how to make stacked generalization work for classi cation tasks by addressing two crucial issues which Wolpert (1992) originally described as `black art' and have not been resolved since.", "startOffset": 180, "endOffset": 1397}, {"referenceID": 2, "context": "Introduction Stacked generalization is a way of combining multiple models that have been learned for a classi cation task (Wolpert, 1992), which has also been used for regression (Breiman, 1996a) and even unsupervised learning (Smyth & Wolpert, 1997). Typically, di erent learning algorithms learn di erent models for the task at hand, and in the most common form of stacking the rst step is to collect the output of each model into a new set of data. For each instance in the original training set, this data set represents every model's prediction of that instance's class, along with its true classi cation. During this step, care is taken to ensure that the models are formed from a batch of training data that does not include the instance in question, in just the same way as ordinary cross-validation. The new data are treated as the data for another learning problem, and in the second step a learning algorithm is employed to solve this problem. In Wolpert's terminology, the original data and the models constructed for them in the rst step are referred to as level-0 data and level-0 models, respectively, while the set of cross-validated data and the second-stage learning algorithm are referred to as level-1 data and the level-1 generalizer. In this paper, we show how to make stacked generalization work for classi cation tasks by addressing two crucial issues which Wolpert (1992) originally described as `black art' and have not been resolved since. The two issues are (i) the type of attributes that should be used to form level-1 data, and (ii) the type of level-1 generalizer in order to get improved accuracy using the stacked generalization method. Breiman (1996a) demonstrated the success of stacked generalization in the setting of ordinary regression.", "startOffset": 180, "endOffset": 1687}, {"referenceID": 2, "context": "But instead of selecting the single model that works best as judged by (for example) cross-validation, Breiman used the di erent level0 regressors' output values for each member of the training set to form level-1 data. Then he used least-squares linear regression, under the constraint that all regression coe cients be non-negative, as the level-1 generalizer. The non-negativity constraint turned out to be crucial to guarantee that the predictive accuracy would be better than that achieved by selecting the single best predictor. Here we show how stacked generalization can be made to work reliably in classi cation tasks. We do this by using the output class probabilities generated by level-0 models to form level-1 data. Then for the level-1 generalizer we use a version of least squares linear regression adapted for classi cation tasks. We nd the use of class probabilities to be crucial for the successful application of stacked generalization in classi cation tasks. However, the non-negativity constraints found necessary by Breiman in regression are found to be irrelevant to improved predictive accuracy in our classi cation situation. In Section 2, we formally introduce the technique of stacked generalization and describe pertinent details of each learning algorithm used in our experiments. Section 3 describes the results of stacking three di erent types of learning algorithms. Section 4 compares stacked generalization with arcing and bagging, two recent methods that employ sampling techniques to modify the data distribution in order to produce multiple models from a single learning algorithm. The following section describes related work, and the paper ends with a summary of our conclusions. 2. Stacked Generalization Given a data set L = f(yn; xn); n = 1; : : : ; Ng, where yn is the class value and xn is a vector representing the attribute values of the nth instance, randomly split the data into J almost equal parts L1; : : : ;LJ . De ne Lj and L( j) = L Lj to be the test and training sets for the jth fold of a J -fold cross-validation. Given K learning algorithms, which we call level-0 generalizers, invoke the kth algorithm on the data in the training set L( j) to induce a model M( j) k , for k = 1; : : : ;K. These are called level-0 models. For each instance xn in Lj, the test set for the jth cross-validation fold, let zkn denote the prediction of the model M( j) k on xn. At the end of the entire cross-validation process, the data set assembled from the outputs of the K models is LCV = f(yn; z1n; : : : ; zKn); n = 1; : : : ; Ng: These are the level-1 data. Use some learning algorithm that we call the level-1 generalizer to derive from these data a model ~ M for y as a function of (z1; : : : ; zK). This is the level-1 model. Figure 1 illustrates the cross-validation process. To complete the training process, the nal level-0 models Mk, k = 1; : : : ;K, are derived using all the data in L. Now let us consider the classi cation process, which uses the modelsMk, k = 1; : : : ;K, in conjunction with ~ M. Given a new instance, models Mk produce a vector (z1; : : : ; zK). This vector is input to the level-1 model ~ M, whose output is the nal classi cation result for that instance. This completes the stacked generalization method as proposed by Wolpert (1992), and also used by Breiman (1996a) and LeBlanc & Tibshirani (1993).", "startOffset": 103, "endOffset": 3311}, {"referenceID": 2, "context": "But instead of selecting the single model that works best as judged by (for example) cross-validation, Breiman used the di erent level0 regressors' output values for each member of the training set to form level-1 data. Then he used least-squares linear regression, under the constraint that all regression coe cients be non-negative, as the level-1 generalizer. The non-negativity constraint turned out to be crucial to guarantee that the predictive accuracy would be better than that achieved by selecting the single best predictor. Here we show how stacked generalization can be made to work reliably in classi cation tasks. We do this by using the output class probabilities generated by level-0 models to form level-1 data. Then for the level-1 generalizer we use a version of least squares linear regression adapted for classi cation tasks. We nd the use of class probabilities to be crucial for the successful application of stacked generalization in classi cation tasks. However, the non-negativity constraints found necessary by Breiman in regression are found to be irrelevant to improved predictive accuracy in our classi cation situation. In Section 2, we formally introduce the technique of stacked generalization and describe pertinent details of each learning algorithm used in our experiments. Section 3 describes the results of stacking three di erent types of learning algorithms. Section 4 compares stacked generalization with arcing and bagging, two recent methods that employ sampling techniques to modify the data distribution in order to produce multiple models from a single learning algorithm. The following section describes related work, and the paper ends with a summary of our conclusions. 2. Stacked Generalization Given a data set L = f(yn; xn); n = 1; : : : ; Ng, where yn is the class value and xn is a vector representing the attribute values of the nth instance, randomly split the data into J almost equal parts L1; : : : ;LJ . De ne Lj and L( j) = L Lj to be the test and training sets for the jth fold of a J -fold cross-validation. Given K learning algorithms, which we call level-0 generalizers, invoke the kth algorithm on the data in the training set L( j) to induce a model M( j) k , for k = 1; : : : ;K. These are called level-0 models. For each instance xn in Lj, the test set for the jth cross-validation fold, let zkn denote the prediction of the model M( j) k on xn. At the end of the entire cross-validation process, the data set assembled from the outputs of the K models is LCV = f(yn; z1n; : : : ; zKn); n = 1; : : : ; Ng: These are the level-1 data. Use some learning algorithm that we call the level-1 generalizer to derive from these data a model ~ M for y as a function of (z1; : : : ; zK). This is the level-1 model. Figure 1 illustrates the cross-validation process. To complete the training process, the nal level-0 models Mk, k = 1; : : : ;K, are derived using all the data in L. Now let us consider the classi cation process, which uses the modelsMk, k = 1; : : : ;K, in conjunction with ~ M. Given a new instance, models Mk produce a vector (z1; : : : ; zK). This vector is input to the level-1 model ~ M, whose output is the nal classi cation result for that instance. This completes the stacked generalization method as proposed by Wolpert (1992), and also used by Breiman (1996a) and LeBlanc & Tibshirani (1993).", "startOffset": 103, "endOffset": 3345}, {"referenceID": 2, "context": "But instead of selecting the single model that works best as judged by (for example) cross-validation, Breiman used the di erent level0 regressors' output values for each member of the training set to form level-1 data. Then he used least-squares linear regression, under the constraint that all regression coe cients be non-negative, as the level-1 generalizer. The non-negativity constraint turned out to be crucial to guarantee that the predictive accuracy would be better than that achieved by selecting the single best predictor. Here we show how stacked generalization can be made to work reliably in classi cation tasks. We do this by using the output class probabilities generated by level-0 models to form level-1 data. Then for the level-1 generalizer we use a version of least squares linear regression adapted for classi cation tasks. We nd the use of class probabilities to be crucial for the successful application of stacked generalization in classi cation tasks. However, the non-negativity constraints found necessary by Breiman in regression are found to be irrelevant to improved predictive accuracy in our classi cation situation. In Section 2, we formally introduce the technique of stacked generalization and describe pertinent details of each learning algorithm used in our experiments. Section 3 describes the results of stacking three di erent types of learning algorithms. Section 4 compares stacked generalization with arcing and bagging, two recent methods that employ sampling techniques to modify the data distribution in order to produce multiple models from a single learning algorithm. The following section describes related work, and the paper ends with a summary of our conclusions. 2. Stacked Generalization Given a data set L = f(yn; xn); n = 1; : : : ; Ng, where yn is the class value and xn is a vector representing the attribute values of the nth instance, randomly split the data into J almost equal parts L1; : : : ;LJ . De ne Lj and L( j) = L Lj to be the test and training sets for the jth fold of a J -fold cross-validation. Given K learning algorithms, which we call level-0 generalizers, invoke the kth algorithm on the data in the training set L( j) to induce a model M( j) k , for k = 1; : : : ;K. These are called level-0 models. For each instance xn in Lj, the test set for the jth cross-validation fold, let zkn denote the prediction of the model M( j) k on xn. At the end of the entire cross-validation process, the data set assembled from the outputs of the K models is LCV = f(yn; z1n; : : : ; zKn); n = 1; : : : ; Ng: These are the level-1 data. Use some learning algorithm that we call the level-1 generalizer to derive from these data a model ~ M for y as a function of (z1; : : : ; zK). This is the level-1 model. Figure 1 illustrates the cross-validation process. To complete the training process, the nal level-0 models Mk, k = 1; : : : ;K, are derived using all the data in L. Now let us consider the classi cation process, which uses the modelsMk, k = 1; : : : ;K, in conjunction with ~ M. Given a new instance, models Mk produce a vector (z1; : : : ; zK). This vector is input to the level-1 model ~ M, whose output is the nal classi cation result for that instance. This completes the stacked generalization method as proposed by Wolpert (1992), and also used by Breiman (1996a) and LeBlanc & Tibshirani (1993). 272", "startOffset": 103, "endOffset": 3377}, {"referenceID": 22, "context": "5, a decision tree learning algorithm (Quinlan, 1993); NB, a re-implementation of a Naive Bayesian classi er (Cestnik, 1990); and IB1, a variant of a lazy learning algorithm (Aha, Kibler & Albert, 1991) which employs the p-nearest-neighbor method using a modi ed value-di erence metric for nominal and binary attributes (Cost & Salzberg, 1993).", "startOffset": 38, "endOffset": 53}, {"referenceID": 5, "context": "5, a decision tree learning algorithm (Quinlan, 1993); NB, a re-implementation of a Naive Bayesian classi er (Cestnik, 1990); and IB1, a variant of a lazy learning algorithm (Aha, Kibler & Albert, 1991) which employs the p-nearest-neighbor method using a modi ed value-di erence metric for nominal and binary attributes (Cost & Salzberg, 1993).", "startOffset": 109, "endOffset": 124}, {"referenceID": 2, "context": "MLR is an adaptation of a least-squares linear regression algorithm that Breiman (1996a) used in regression settings.", "startOffset": 73, "endOffset": 89}, {"referenceID": 2, "context": "MLR is an adaptation of a least-squares linear regression algorithm that Breiman (1996a) used in regression settings. Any classi cation problem with real-valued attributes can be transformed into a multi-response regression problem. If the original classi cation problem has I classes, it is converted into I separate regression problems, where the problem for class ` has instances with responses equal to one when they have class ` and zero otherwise. The input to MLR is level-1 data, and we need to consider the situation for the model ~ M0, where the attributes are probabilities, separately from that for the model ~ M, where 1. A large p value is used following Wolpert's (1992) advice that \\: : : it is reasonable that `relatively global, smooth : : : ' level-1 generalizers should perform well.", "startOffset": 73, "endOffset": 686}, {"referenceID": 2, "context": "Choose the linear regression coe cients f k`g to minimize Xj X (yn;xn)2Lj(yn Xk k`P ( j) k` (xn))2: The coe cients f k`g are constrained to be non-negative, following Breiman's (1996a) discovery that this is necessary for the successful application of stacked generalization to regression problems.", "startOffset": 167, "endOffset": 185}, {"referenceID": 2, "context": "Choose the linear regression coe cients f k`g to minimize Xj X (yn;xn)2Lj(yn Xk k`P ( j) k` (xn))2: The coe cients f k`g are constrained to be non-negative, following Breiman's (1996a) discovery that this is necessary for the successful application of stacked generalization to regression problems. The non-negative-coe cient least-squares algorithm described by Lawson & Hanson (1995) is employed here to derive the linear regression for each class.", "startOffset": 167, "endOffset": 386}, {"referenceID": 2, "context": "2 Are Non-negativity Constraints Necessary? Both Breiman (1996a) and LeBlanc & Tibshirani (1993) use the stacked generalization method in a regression setting and report that it is necessary to constrain the regression coe cients to be non-negative in order to guarantee that stacked regression improves predictive accuracy.", "startOffset": 49, "endOffset": 65}, {"referenceID": 2, "context": "2 Are Non-negativity Constraints Necessary? Both Breiman (1996a) and LeBlanc & Tibshirani (1993) use the stacked generalization method in a regression setting and report that it is necessary to constrain the regression coe cients to be non-negative in order to guarantee that stacked regression improves predictive accuracy.", "startOffset": 49, "endOffset": 97}, {"referenceID": 17, "context": "Although small values of #SE are a necessary condition for majority vote to rival BestCV, they are not a su cient condition|see Matan (1996) for an example.", "startOffset": 128, "endOffset": 141}, {"referenceID": 2, "context": "It is worth mentioning a method that averages Pi(x) for each i over all level-0 models, yielding Pi(x), and then predicts class \u00ce for which P\u00ce(x) > Pi(x) for all i 6= \u00ce : According to Breiman (1996b), this method produces an error rate almost identical to that of majority vote.", "startOffset": 184, "endOffset": 200}, {"referenceID": 11, "context": "Other possible candidates are the multinomial logit model (Jordan & Jacobs, 1994), which is a special case of generalized linear models (McCullagh & Nelder, 1983), and the supra Bayesian procedure (Jacobs, 1995) which treats the level-0 models' con dence as data that may be combined with prior distribution of level-0 models via Bayes' rule.", "startOffset": 197, "endOffset": 211}, {"referenceID": 20, "context": "Ting & Witten (1997) show one possible way of incorporating bagged models with level-1 learning, employing MLR instead of voting.", "startOffset": 0, "endOffset": 21}, {"referenceID": 20, "context": "Ting & Witten (1997) show one possible way of incorporating bagged models with level-1 learning, employing MLR instead of voting. In this implementation, L is used as a test set for each of the bagged models to derive level-1 data rather than the cross-validated data. This is viable because each bootstrap sample leaves out about 37% of the examples. Ting & Witten (1997) show that bag-stacking almost always has higher predictive accuracy than bagging models derived from either C4.", "startOffset": 0, "endOffset": 373}, {"referenceID": 2, "context": "Note that the only di erence here is whether an adaptive level-1 model or a simple majority vote is employed According to Breiman (1996b; 1996c), arcing and bagging can only improve the predictive accuracy of learning algorithms that are `unstable.'5 An unstable learning algorithm is one for which small perturbations in the training set can produce large changes in the derived model. Decision trees and neural networks are unstable; NB and IB1 are stable. Stacking works with both. While MLR is the most successful candidate for level-1 learning that we have found, other algorithms might work equally well. One candidate is neural networks. However, we have experimented with back-propagation neural networks for this purpose and found that they have a much slower learning rate than MLR. For example, MLR only took 2.9 seconds as compare to 4790 seconds for the neural network in the nettalk dataset; while both have the same error rate. Other possible candidates are the multinomial logit model (Jordan & Jacobs, 1994), which is a special case of generalized linear models (McCullagh & Nelder, 1983), and the supra Bayesian procedure (Jacobs, 1995) which treats the level-0 models' con dence as data that may be combined with prior distribution of level-0 models via Bayes' rule. 5. Related Work Our analysis of stacked generalization was motivated by that of Breiman (1996a), discussed earlier, and LeBlanc & Tibshirani (1993).", "startOffset": 122, "endOffset": 1382}, {"referenceID": 2, "context": "Note that the only di erence here is whether an adaptive level-1 model or a simple majority vote is employed According to Breiman (1996b; 1996c), arcing and bagging can only improve the predictive accuracy of learning algorithms that are `unstable.'5 An unstable learning algorithm is one for which small perturbations in the training set can produce large changes in the derived model. Decision trees and neural networks are unstable; NB and IB1 are stable. Stacking works with both. While MLR is the most successful candidate for level-1 learning that we have found, other algorithms might work equally well. One candidate is neural networks. However, we have experimented with back-propagation neural networks for this purpose and found that they have a much slower learning rate than MLR. For example, MLR only took 2.9 seconds as compare to 4790 seconds for the neural network in the nettalk dataset; while both have the same error rate. Other possible candidates are the multinomial logit model (Jordan & Jacobs, 1994), which is a special case of generalized linear models (McCullagh & Nelder, 1983), and the supra Bayesian procedure (Jacobs, 1995) which treats the level-0 models' con dence as data that may be combined with prior distribution of level-0 models via Bayes' rule. 5. Related Work Our analysis of stacked generalization was motivated by that of Breiman (1996a), discussed earlier, and LeBlanc & Tibshirani (1993). LeBlanc & Tibshirani (1993) examine the stacking of a linear discriminant and a nearest neighbor classi er and show that, for one arti cial dataset, a method similar to MLR performs better with non-negativity constraints than without.", "startOffset": 122, "endOffset": 1434}, {"referenceID": 2, "context": "Note that the only di erence here is whether an adaptive level-1 model or a simple majority vote is employed According to Breiman (1996b; 1996c), arcing and bagging can only improve the predictive accuracy of learning algorithms that are `unstable.'5 An unstable learning algorithm is one for which small perturbations in the training set can produce large changes in the derived model. Decision trees and neural networks are unstable; NB and IB1 are stable. Stacking works with both. While MLR is the most successful candidate for level-1 learning that we have found, other algorithms might work equally well. One candidate is neural networks. However, we have experimented with back-propagation neural networks for this purpose and found that they have a much slower learning rate than MLR. For example, MLR only took 2.9 seconds as compare to 4790 seconds for the neural network in the nettalk dataset; while both have the same error rate. Other possible candidates are the multinomial logit model (Jordan & Jacobs, 1994), which is a special case of generalized linear models (McCullagh & Nelder, 1983), and the supra Bayesian procedure (Jacobs, 1995) which treats the level-0 models' con dence as data that may be combined with prior distribution of level-0 models via Bayes' rule. 5. Related Work Our analysis of stacked generalization was motivated by that of Breiman (1996a), discussed earlier, and LeBlanc & Tibshirani (1993). LeBlanc & Tibshirani (1993) examine the stacking of a linear discriminant and a nearest neighbor classi er and show that, for one arti cial dataset, a method similar to MLR performs better with non-negativity constraints than without.", "startOffset": 122, "endOffset": 1463}, {"referenceID": 2, "context": "Note that the only di erence here is whether an adaptive level-1 model or a simple majority vote is employed According to Breiman (1996b; 1996c), arcing and bagging can only improve the predictive accuracy of learning algorithms that are `unstable.'5 An unstable learning algorithm is one for which small perturbations in the training set can produce large changes in the derived model. Decision trees and neural networks are unstable; NB and IB1 are stable. Stacking works with both. While MLR is the most successful candidate for level-1 learning that we have found, other algorithms might work equally well. One candidate is neural networks. However, we have experimented with back-propagation neural networks for this purpose and found that they have a much slower learning rate than MLR. For example, MLR only took 2.9 seconds as compare to 4790 seconds for the neural network in the nettalk dataset; while both have the same error rate. Other possible candidates are the multinomial logit model (Jordan & Jacobs, 1994), which is a special case of generalized linear models (McCullagh & Nelder, 1983), and the supra Bayesian procedure (Jacobs, 1995) which treats the level-0 models' con dence as data that may be combined with prior distribution of level-0 models via Bayes' rule. 5. Related Work Our analysis of stacked generalization was motivated by that of Breiman (1996a), discussed earlier, and LeBlanc & Tibshirani (1993). LeBlanc & Tibshirani (1993) examine the stacking of a linear discriminant and a nearest neighbor classi er and show that, for one arti cial dataset, a method similar to MLR performs better with non-negativity constraints than without. Our results in Section 3.2 show that these constraints are irrelevant to MLR's predictive accuracy in the classi cation situation. LeBlanc & Tibshirani (1993) and Ting & Witten (1997) use a version of MLR that employs all class probabilities from each level-0 model to induce each linear regression.", "startOffset": 122, "endOffset": 1829}, {"referenceID": 2, "context": "Note that the only di erence here is whether an adaptive level-1 model or a simple majority vote is employed According to Breiman (1996b; 1996c), arcing and bagging can only improve the predictive accuracy of learning algorithms that are `unstable.'5 An unstable learning algorithm is one for which small perturbations in the training set can produce large changes in the derived model. Decision trees and neural networks are unstable; NB and IB1 are stable. Stacking works with both. While MLR is the most successful candidate for level-1 learning that we have found, other algorithms might work equally well. One candidate is neural networks. However, we have experimented with back-propagation neural networks for this purpose and found that they have a much slower learning rate than MLR. For example, MLR only took 2.9 seconds as compare to 4790 seconds for the neural network in the nettalk dataset; while both have the same error rate. Other possible candidates are the multinomial logit model (Jordan & Jacobs, 1994), which is a special case of generalized linear models (McCullagh & Nelder, 1983), and the supra Bayesian procedure (Jacobs, 1995) which treats the level-0 models' con dence as data that may be combined with prior distribution of level-0 models via Bayes' rule. 5. Related Work Our analysis of stacked generalization was motivated by that of Breiman (1996a), discussed earlier, and LeBlanc & Tibshirani (1993). LeBlanc & Tibshirani (1993) examine the stacking of a linear discriminant and a nearest neighbor classi er and show that, for one arti cial dataset, a method similar to MLR performs better with non-negativity constraints than without. Our results in Section 3.2 show that these constraints are irrelevant to MLR's predictive accuracy in the classi cation situation. LeBlanc & Tibshirani (1993) and Ting & Witten (1997) use a version of MLR that employs all class probabilities from each level-0 model to induce each linear regression.", "startOffset": 122, "endOffset": 1854}, {"referenceID": 2, "context": "Note that the only di erence here is whether an adaptive level-1 model or a simple majority vote is employed According to Breiman (1996b; 1996c), arcing and bagging can only improve the predictive accuracy of learning algorithms that are `unstable.'5 An unstable learning algorithm is one for which small perturbations in the training set can produce large changes in the derived model. Decision trees and neural networks are unstable; NB and IB1 are stable. Stacking works with both. While MLR is the most successful candidate for level-1 learning that we have found, other algorithms might work equally well. One candidate is neural networks. However, we have experimented with back-propagation neural networks for this purpose and found that they have a much slower learning rate than MLR. For example, MLR only took 2.9 seconds as compare to 4790 seconds for the neural network in the nettalk dataset; while both have the same error rate. Other possible candidates are the multinomial logit model (Jordan & Jacobs, 1994), which is a special case of generalized linear models (McCullagh & Nelder, 1983), and the supra Bayesian procedure (Jacobs, 1995) which treats the level-0 models' con dence as data that may be combined with prior distribution of level-0 models via Bayes' rule. 5. Related Work Our analysis of stacked generalization was motivated by that of Breiman (1996a), discussed earlier, and LeBlanc & Tibshirani (1993). LeBlanc & Tibshirani (1993) examine the stacking of a linear discriminant and a nearest neighbor classi er and show that, for one arti cial dataset, a method similar to MLR performs better with non-negativity constraints than without. Our results in Section 3.2 show that these constraints are irrelevant to MLR's predictive accuracy in the classi cation situation. LeBlanc & Tibshirani (1993) and Ting & Witten (1997) use a version of MLR that employs all class probabilities from each level-0 model to induce each linear regression. In this case, the linear regression for class ` is LR`(x) = K Xk I Xi ki`Pki(x): This implementation requires the tting of KI parameters, as compared to K parameters for the version used in this paper (see the corresponding formula in Section 2.2). Both 5. Schapire, R.E., Y. Freund, P. Bartlett, & W.S. Lee (1997) provide an alternative explanation for the e ectiveness of arcing and bagging.", "startOffset": 122, "endOffset": 2285}], "year": 2011, "abstractText": null, "creator": "dvips 5.58 Copyright 1986, 1994 Radical Eye Software"}}}