{"id": "1106.1622", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2011", "title": "Large-Scale Convex Minimization with a Low-Rank Constraint", "abstract": "We address the problem of minimizing a convex function over the space of large matrices with low rank. While this optimization problem is hard in general, we propose an efficient greedy algorithm and derive its formal approximation guarantees. Each iteration of the algorithm involves (approximately) finding the left and right singular vectors corresponding to the largest singular value of a certain matrix, which can be calculated in linear time. This leads to an algorithm which can scale to large matrices arising in several applications such as matrix completion for collaborative filtering and robust low rank matrix approximation.", "histories": [["v1", "Wed, 8 Jun 2011 19:07:09 GMT  (39kb,D)", "http://arxiv.org/abs/1106.1622v1", "ICML 2011"]], "COMMENTS": "ICML 2011", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["shai shalev-shwartz", "alon gonen", "ohad shamir"], "accepted": true, "id": "1106.1622"}, "pdf": {"name": "1106.1622.pdf", "metadata": {"source": "META", "title": "Large-Scale Convex Minimization with a Low-Rank Constraint", "authors": ["Shai Shalev-Shwartz", "Alon Gonen"], "emails": ["shais@cs.huji.ac.il", "alongnn@gmail.com", "ohadsh@microsoft.com"], "sections": [{"heading": "1. Introduction", "text": "Our goal is to approximately solve an optimization problem of the form:\nmin A:rank(A)\u2264r R(A) , (1)\nwhere R : Rm\u00d7n \u2192 R is a convex and smooth function. This problem arises in many machine learning applications such as collaborating filtering (Koren et al., 2009), robust low rank matrix approximation (Ke & Kanade, 2005; Croux & Filzmoser, 1998; A. Baccini & Falguerolles, 1996), and multiclass classification (Amit et al., 2007). The rank constraint on A is non-convex and therefore it is generally NP-hard to solve Equation (1) (this follows from (Natarajan, 1995; Davis et al., 1997)).\nAppearing in Proceedings of the 28 th International Conference on Machine Learning, Bellevue, WA, USA, 2011. Copyright 2011 by the author(s)/owner(s).\nIn this paper we describe and analyze an approximation algorithm for solving Equation (1). Roughly speaking, the proposed algorithm is based on a simple, yet powerful, observation: instead of representing a matrix A using m \u00d7 n numbers, we represent it using an infinite dimensional vector \u03bb, indexed by all pairs (u, v) taken from the unit spheres of Rm and Rn respectively. In this representation, low rank corresponds to sparsity of the vector \u03bb.\nThus, we can reduce the problem given in Equation (1) to the problem of minimizing a vector function f(\u03bb) over the set of sparse vectors, \u2016\u03bb\u20160 \u2264 r. Based on this reduction, we apply a greedy approximation algorithm for minimizing a convex vector function subject to a sparsity constraint. At first glance, a direct application of this reduction seems impossible, since \u03bb is an infinite-dimensional vector, and at each iteration of the greedy algorithm one needs to search over the infinite set of the coordinates of \u03bb. However, we show that this search problem can be cast as the problem of finding the first leading right and left singular vectors of a certain matrix.\nAfter describing and analyzing the general algorithm, we show how to apply it to the problems of matrix completion and robust low-rank matrix approximation. As a side benefit, our general analysis yields a new sample complexity bound for matrix completion. We demonstrate the efficacy of our algorithm by conducting experiments on large-scale movie recommendation data sets."}, {"heading": "1.1. Related work", "text": "As mentioned earlier, the problem defined in Equation (1) has many applications, and therefore it was studied in various contexts. A popular approach is to use the trace norm as a surrogate for the rank (e.g. (Fazel et al., 2002)). This approach is closely related to the idea of using the `1 norm as a surrogate for spar-\nar X\niv :1\n10 6.\n16 22\nv1 [\ncs .L\nG ]\n8 J\nun 2\n01 1\nsity, because low rank corresponds to sparsity of the vector of singular values and the trace norm is the `1 norm of the vector of singular values. This approach has been extensively studied, mainly in the context of collaborating filtering. See for example (Cai et al., 2008; Candes & Plan, 2010; Cande\u0300s & Recht, 2009; Keshavan et al., 2010; Keshavan & Oh, 2009).\nWhile the trace norm encourages low rank solutions, it does not always produce sparse solutions. Generalizing recent studies in compressed sensing, several papers (e.g. (Recht et al., 2007; Cai et al., 2008; Candes & Plan, 2010; Cande\u0300s & Recht, 2009; Recht, to appear)) give recovery guarantees for the trace norm approach. However, these guarantees rely on rather strong assumptions (e.g., it is assumed that the data is indeed generated by a low rank matrix, that certain incoherence assumptions hold, and for matrix completion problems, it requires the entries to be sampled uniformly at random). In addition, trace norm minimization often involves semi-definite programming, which usually does not scale well to large-scale problems.\nIn this paper we tackle the rank minimization directly, using a greedy selection approach, without relying on the trace norm as a convex surrogate. Our approach is similar to forward greedy selection approaches for optimization with sparsity constraint (e.g. the MP (Mallat & Zhang, 1993) and OMP (Pati et al., 2002) algorithms), and in particular we extend the fully corrective forward greedy selection algorithm given in (Shalev-Shwartz et al., 2010)). We also provide formal guarantees on the competitiveness of our algorithm relative to matrices with small trace norm.\nRecently, (Lee & Bresler, 2010) proposed the ADMiRA algorithm, which also follows the greedy approach. However, the ADMiRA algorithm is different, as in each step it first chooses 2r components and then uses SVD to revert back to a r rank matrix. This is more expensive then our algorithm which chooses a single rank 1 matrix at each step. The difference between the two algorithms is somewhat similar to the difference between the OMP (Pati et al., 2002) algorithm for learning sparse vectors, to CoSaMP (Needell & Tropp, 2009) and SP (Dai & Milenkovic, 2008). In addition, the ADMiRA algorithm is specific to the squared loss while our algorithm can handle any smooth loss. Finally, while ADMiRA comes with elegant performance guarantees, these rely on strong assumptions, e.g. that the matrix defining the quadratic loss satisfies a rankrestricted isometry property. In contrast, our analysis only assumes smoothness of the loss function.\nThe algorithm we propose is also related to Hazan\u2019s algorithm (Hazan, 2008) for solving PSD problems,\nwhich in turns relies on Frank-Wolfe algorithm (Frank & Wolfe, 1956) (see Clarkson (Clarkson, 2008)), as well as to the follow-up paper of (Jaggi & Sulovsky\u0300, 2010), which applies Hazan\u2019s algorithm for optimizing with trace-norm constraints. There are several important changes though. First, we tackle the problem directly and do not enforce neither PSDness of the matrix nor a bounded trace-norm. Second, our algorithm is \u201dfully corrective\u201d, that is, it extracts all the information from existing components before adding a new component. These differences between the approaches are analogous to the difference between Frank-Wolfe algorithm and fully corrective greedy selection, for minimizing over sparse vectors, as discussed in (Shalev-Shwartz et al., 2010). Finally, while each iteration of both methods involves approximately finding leading eigenvectors, in (Hazan, 2008) the quality of approximation should improve as the algorithm progresses while our algorithm can always rely on the same constant approximation factor."}, {"heading": "2. The GECO algorithm", "text": "In this section we describe our algorithm, which we call Greedy Efficient Component Optimization (or GECO for short). Let A \u2208 Rm\u00d7n be a matrix, and without loss of generality assume that m \u2264 n. The SVD theorem states that A can be written as A = \u2211m i=1 \u03bbiuiv T i , where u1, . . . , um are members of U = {u \u2208 Rm : \u2016u\u2016 = 1}, v1, . . . , vm comes from V = {v \u2208 Rn : \u2016v\u2016 = 1}, and \u03bb1, . . . , \u03bbm are scalars. To simplify the presentation, we assume that each real number is represented using a finite number of bits, therefore the sets U and V are finite sets.1 It follows that we can also write A as A = \u2211 (u,v)\u2208U\u00d7V \u03bbu,vuv T , where \u03bb \u2208 R|U\u00d7V| and we index the elements of \u03bb using pairs (u, v) \u2208 U\u00d7V. Note that the representation of A using a vector \u03bb is not unique, but from the SVD theorem, there is always a representation of A for which the number of non-zero elements of \u03bb is at most m, i.e. \u2016\u03bb\u20160 \u2264 m where \u2016\u03bb\u20160 = |{(u, v) : \u03bbu,v 6= 0}|. Furthermore, if rank(A) \u2264 r then there is a representation of A using a vector \u03bb for which \u2016\u03bb\u20160 \u2264 r.\nGiven a (sparse) vector \u03bb \u2208 R|U\u00d7V| we define the cor1This assumption greatly simplifies the presentation but is not very limiting since we do not impose any restriction on the amount of bits needed to represent a single real number. We note that the assumption is not necessary and can be waived by writing A = \u222b (u,v)\u2208U\u00d7V uv T d\u03bb(u, v), where \u03bb is a measure on U\u00d7V, and from the SVD theorem, there is always a representation with \u03bb which is non-zero on finitely many points.\nAlgorithm 1 GECO\n1: Input: Convex-smooth function R : Rm\u00d7n \u2192 R ; rank constraint r ; tolerance \u03c4 \u2208 [0, 1/2] 2: Initialize: U = [], V = [] 3: for i=1,. . . ,r do 4: (u, v) = ApproxSV(\u2207R(UV T ), \u03c4) 5: Set U = [U , u] and V = [V , v] 6: Set B = argminB:\u2208Ri\u00d7i R(UBV\nT ) 7: Calculate SVD: B = PDQT 8: Update: U = UPD, V = V Q 9: end for\nresponding matrix to be A(\u03bb) = \u2211\n(u,v)\u2208U\u00d7V\n\u03bbu,vuv T .\nNote that A(\u03bb) is a linear mapping. Given a function R : Rm\u00d7n \u2192 R, we define a function\nf(\u03bb) = R(A(\u03bb)) = R  \u2211 (u,v)\u2208U\u00d7V \u03bbu,vuv T  . It is easy to verify that if R is a convex function over Rm\u00d7n then f is convex over R|U\u00d7V| (since f is a composition of R over a linear mapping). We can therefore reduce the problem given in Equation (1) to the problem\nmin \u03bb\u2208R|U\u00d7V|:\u2016\u03bb\u20160\u2264r f(\u03bb) . (2)\nWhile the optimization problem given in Equation (2) is over an arbitrary large space, we next show that a forward greedy selection procedure can be implemented efficiently. The greedy algorithm starts with \u03bb = (0, . . . , 0). At each iteration, we first find the vectors (u, v) that maximizes the magnitude of the partial derivative of f(\u03bb) with respect to \u03bbu,v. Assuming that R is differentiable, and using the chain rule, we obtain:\n\u2202f(\u03bb) \u2202\u03bbu,v = \u3008\u2207R(A(\u03bb)), uvT \u3009 = uT\u2207R(A(\u03bb))v ,\nwhere \u2207R(A(\u03bb)) is the m\u00d7n matrix of partial derivatives of R with respect to the elements of A(\u03bb). The vectors u, v that maximizes the magnitude of the above expression are the left and right singular vectors corresponding to the maximal singular value of \u2207R(A(\u03bb)). Therefore, even though the number of elements in U \u00d7 V is very large, we can still perform a greedy selection of one pair (u, v) \u2208 U \u00d7 V in an efficient way.\nIn some situations, even the calculation of the leading singular vectors might be too expensive. We therefore allow approximate maximization, and denote by\nApproxSV(\u2207R(A(\u03bb)), \u03c4) a procedure2 which returns vectors for which\nuT\u2207R(A(\u03bb))v \u2265 (1\u2212 \u03c4) max p,q pT\u2207R(A(\u03bb))q .\nLet U and V be matrices whose columns contain the vectors u and v we aggregated so far. The second step of each iteration of the algorithm sets \u03bb to be the solution of the following optimization problem:\nmin \u03bb\u2208R|U\u00d7V|\nf(\u03bb) s.t. supp(\u03bb) \u2286 span(U)\u00d7span(V ), (3)\nwhere supp(\u03bb) = {(u, v) : \u03bbu,v 6= 0}, and span(U), span(V ) are the linear spans of the columns of U, V respectively.\nWe now describe how to solve Equation (3). Let s be the number of columns of U and V . Note that any vector u \u2208 span(U) can be written as Ubu, where bu \u2208 Rs, and similarly, any v \u2208 span(V ) can be written as V bv. Therefore, if the support of \u03bb is in span(U)\u00d7 span(V ) we have that A(\u03bb) can be written as\nA(\u03bb) = \u2211\n(u,v)\u2208supp(\u03bb)\n\u03bbu,v(Ubu)(V bv) T\n= U  \u2211 (u,v)\u2208supp(\u03bb) \u03bbu,vbub T v V T . Thus, any \u03bb whose support is in span(U) \u00d7 span(V ) yields a matrix B(\u03bb) = \u2211 u,v \u03bbu,vbub T v . The SVD theorem tells us that the opposite direction is also true, namely, for any B \u2208 Rs\u00d7s there exists \u03bb whose support is in span(U) \u00d7 span(V ) that generates B (and also UBV T ). Denote R\u0303(B) = R(UBV T ), it follows that Equation (3) is equivalent to the following unconstrained optimization problem minB\u2208Rs\u00d7s R\u0303(B). It is easy to verify that R\u0303 is a convex function, and therefore can be minimized efficiently. Once we obtain the matrix B that minimizes R\u0303(B) we can use its SVD to generate the corresponding \u03bb.\nIn practice, we do not need to maintain \u03bb at all, but only to maintain matrices U, V such thatA(\u03bb) = UV T .\n2 An example of such a procedure is the power iteration method, which can implement ApproxSV in time O(N log(n)/\u03c4), where N is the number of non-zero elements of \u2207R(A(\u03bb)). See Theorem 3.1 in (Kuczyn\u0301ski & Woz\u0301niakowski, 1992). Our analysis shows that the value of \u03c4 has a mild effect on the convergence of GECO, and one can even choose a constant value like \u03c4 = 1/2. This is in contrast to (Hazan, 2008; Jaggi & Sulovsky\u0300, 2010) which require the approximation parameter to decrease when the rank increases. Note also that the ApproxEV procedure described in (Hazan, 2008; Jaggi & Sulovsky\u0300, 2010) requires an additive approximation, while we require a multiplicative approximation.\nA summary of the pseudo-code is given in Algorithm 1. The runtime of the algorithm is as follows. Step 4 can be performed in time O(N log(n)/\u03c4), where N is the number of non zero elements of \u2207R(UV T ), using the power method (see Footnote 2). Since our analysis (given in Section 3) allows \u03c4 to be a constant (e.g. 1/2), this means that the runtime is O(N log(n)). The runtime of Step 6 depends on the structure of the function R. We specify it when describing specific applications of GECO in later sections. Finally, the runtime of Step 7 is at most r3, and step 8 takes O(r2(m+n))."}, {"heading": "2.1. Variants of GECO", "text": "2.1.1. How to choose (u, v)\nGECO chooses (u, v) to be the leading singular vectors, which are the maximizers of uT\u2207R(A) v over unit spheres of Rm and Rn. Our analysis in the next section guarantees that this choice yields a sufficient decrease of the objective function. However, there may be a pair (u, v) which leads to an even larger decrease in the objective value. Choosing such a direction can lead to improved performance. We note that our analysis in the next section still holds, as long as the direction we choose leads to a larger decrease in the objective value, relative to the increase we can get from using the leading singular vectors. In Section 6 we describe a method that finds better directions."}, {"heading": "2.1.2. Additional replacement steps", "text": "Each iteration of GECO increases the rank by 1. In many cases, it is possible to decrease the objective by replacing one of the components without increasing the rank. If we verify that this replacement step indeed decreases the objective (by simply evaluating the objective before and after the change), then the analysis we present in the next section remains valid. We now describe a simple way to perform a replacement. We start with finding a candidate pair (u, v) and perform steps 5\u22127 of GECO. Then, we approximate the matrix B by zeroing its smallest singular value. Let B\u0302 denote this approximation. We next check if R(UB\u0302V T ) is strictly smaller than the previous objective value. If yes, we update U, V based on B\u0302 and obtain that the rank of UV T has not been increased while the objective has been decreased. Otherwise, we update U, V based on B, thus increasing the rank, but our analysis tells us that we are guaranteed to sufficiently decrease the objective. If we restrict the algorithm to perform at most O(1) attempted replacement steps between each rank-increasing iteration, then its runtime guarantee is only increased by an O(1) factor, and all the convergence guarantees remain valid."}, {"heading": "2.1.3. Adding Schatten norm regularization", "text": "In some situations, rank constraint is not enough for obtaining good generalization guarantees and one can consider objective functions R(A) which contains additional regularization of the form h(\u03bb(A)), where \u03bb(A) is the vector of singular values of A and h is a vector function such as h(x) = \u2016x\u20162p. For example, if p = 2, this regularization term is equivalent to Frobenius norm regularization of A. In general, adding a convex regularization term should not pose any problem. A simple trick to do this is to orthonormalize the columns of U and V before Step 6. Therefore, for any B, the singular values of B equal the singular values of UBV T . Thus, we can solve the problem in Step 6 more efficiently while regularizing B instead of the larger matrix UBV T ."}, {"heading": "2.1.4. Optimizing over diagonal matrices B", "text": "Step 6 of GECO involves solving a problem with i2 variables, where i \u2208 {1, . . . , r}. When r is small this is a reasonable computational effort. However, when r is large, Steps 6 \u2212 7 can be expensive. For example, in matrix completion problems, the complexity of Step 6 can scale with r6. If runtime is important, it is possible to restrict B to be a diagonal matrix, or in other words, we only optimize over the coefficients of \u03bb corresponding to U and V without changing the support of \u03bb. Thus, in step 6 we solve a problem with i variables, and Step 7 is not needed. It is possible to verify that the analysis we give in the next section still holds for this variant."}, {"heading": "3. Analysis", "text": "In this section we give a competitive analysis for GECO. The first theorem shows that after performing r iterations of GECO, its solution is not much worse than the solution of all matrices A\u0304, whose trace norm3 is bounded by a function of r. The second theorem shows that with additional assumptions, we can be competitive with matrices whose rank is at most r. The proofs can be found in the long version of this paper.\nTo formally state the theorems we first need to define a smoothness property of the function f .\nDefinition 1 (smoothness) We say that f is \u03b2smooth if for any \u03bb and (u, v) \u2208 U \u00d7 V we have\nf(\u03bb+ \u03b7eu,v) \u2264 f(\u03bb) + \u03b7 \u2202f(\u03bb) \u2202\u03bbu,v + \u03b2 \u03b72 2 ,\n3The trace norm of a matrix is the sum of its singular values.\nwhere eu,v is the all zeros vector except 1 in the coordinate corresponds to (u, v). We say that R is \u03b2-smooth if the function f(\u03bb) = R(A(\u03bb)) is \u03b2-smooth.\nTheorem 1 Fix some > 0. Assume that GECO (or one of its variants) is run with a \u03b2-smooth function R, a rank constraint r, and a tolerance parameter \u03c4 \u2208 [0, 1). Let A be its output matrix. Then, for all matrices A\u0304 with\n\u2016A\u0304\u20162tr \u2264 (r + 1)(1\u2212 \u03c4)2\n2\u03b2\nwe have that R(A) \u2264 R(A\u0304) + .\nThe previous theorem shows competitiveness with matrices of low trace norm. Our second theorem shows that with additional assumptions on the function f we can be competitive with matrices of low rank as well. We need the following definition.\nDefinition 2 (strong convexity) Let I \u2282 U \u00d7 V. We say that f is \u03c3-strongly-convex over I if for any \u03bb1, \u03bb2 whose support 4 is in I we have\nf(\u03bb1)\u2212 f(\u03bb2)\u2212 \u3008\u2207f(\u03bb2), \u03bb1 \u2212 \u03bb2\u3009 \u2265 \u03c3\n2 \u2016\u03bb1 \u2212 \u03bb2\u201622 .\nWe say that R is \u03c3-strongly-convex over I if the function f(\u03bb) = R(A(\u03bb)) is \u03c3-strongly-convex over I.\nTheorem 2 Assume that the conditions of Theorem 1 hold. Then, for any A\u0304 such that\nrank(A\u0304) \u2264 (r + 1)(1\u2212 \u03c4) 2 \u03c3\n4\u03b2R(0) .\nand such that R is \u03c3-strongly-convex over the singular vectors of A\u0304, we have that R(A) \u2264 R(A\u0304) + .\nWe discuss the implications of these theorems for several applications in the next sections."}, {"heading": "4. Application I: Matrix Completion", "text": "Matrix completion is the problem of predicting the entries of some unknown target matrix Y \u2208 Rm\u00d7n based on a random subset of observed entries, E \u2282 [m]\u00d7 [n]. For example, in the famous Netflix problem, m represents the number of users, n represents the number of movies, and Yi,j is a rating user i gives to movie j. One approach for learning the matrix Y is to find a matrix A of low rank which approximately agrees with Y on the entries of E (in mean squared\n4The support of \u03bb is the set of (u, v) for which \u03bbu,v 6= 0.\nerror terms). Using the notation of this paper, we would like to minimize the objective\nR(A) = 1 |E| \u2211\n(i,j)\u2208E\n(Ai,j \u2212 Yi,j)2,\nover low rank matrices A.\nWe now specify GECO for this objective function. It is easy to verify that the (i, j) element of \u2207R(A) is 2(Ai,j \u2212 Yi,j) if (i, j) \u2208 E and 0 otherwise. The number of non-zero elements of \u2207R(A) is at most |E|, and therefore Step 4 of GECO can be implemented using the power method in time O(|E| log(n)). Given matrices U, V , let ui be the i\u2019th row of U and vj be the j\u2019th row of V . We have that the (i, j) element of the matrix UBV T can be written as \u3008vec(uTi vj), vec(B)\u3009, where vec of a matrix is the vector obtained by taking all the elements of the matrix column wise. We can therefore rewrite R(UBV t) as\n1 |E| \u2211\n(i,j)\u2208E(\u3008vec(uTi vj), vec(B)\u3009 \u2212 Yi,j), which makes Step 6 of GECO a vanilla least squares problem over at most r2 variables. The runtime of this step is therefore bounded by O(r6 + |E|r2)."}, {"heading": "4.1. Analysis", "text": "To apply our analysis for matrix completion we first bound the smoothness parameter.\nLemma 1 For matrix completion the smoothness parameter is at most 2/|E|.\nProof For any u, v and i, j we can rewrite (Ai,j + \u03b7uivj \u2212 Yi,j)2 as\n(Ai,j \u2212 Yi,j)2 + 2(Ai,j \u2212 Yi,j) \u03b7uivj + \u03b72u2i v2j .\nTaking expectation over (i, j) \u2208 E we obtain:\nf(\u03bb+ \u03b7eu,v) \u2264 f(\u03bb) + \u03b7\u2207u,vf(\u03bb) + \u03b72 1 |E| \u2211\n(i,j)\u2208E\nu2i v 2 j .\nSince \u2211\n(i,j)\u2208E u 2 i v 2 j \u2264 \u2211 i u 2 i \u2211 j v 2 j = 1, the proof\nfollows.\nOur general analysis therefore implies that for any A\u0304, GECO can find a matrix with rank r \u2264 O(\u2016A\u0304\u20162tr/( |E|)), such that R(A) \u2264 R(A\u0304) + .\nLet us now discuss the implications of this result for the number of observed entries required for predicting the entire entries of Y . Suppose that the entries E are sampled i.i.d. from some unknown distribution D \u2208 Rm\u00d7n, Di,j \u2265 0 for all i, j and \u2211 i,j Di,j = 1.\nDenote the generalization error of a matrix A by F (A) = \u2211 i,j Di,j(Ai,j \u2212 Yi,j)2 .\nUsing generalization bounds for low rank matrices (e.g. (Srebro et al., 2005)), it is possible to show that for any matrix A of rank at most r we have that with high probability5\n|F (A)\u2212R(A)| \u2264 O\u0303( \u221a r(m+ n)/|E|) .\nCombining this with our analysis for GECO, and optimizing , it is easy to derive the following:\nCorollary 1 Fix some matrix A\u0304. Then, GECO can find a matrix A such that with high probability over the choice of the entries in E\nF (A) \u2264 F (A\u0304) + O\u0303\n(( \u2016A\u0304\u2016tr \u221a m+ n\n|E|\n)2/3) .\nWithout loss of generality assume that m \u2264 n. It follows that if \u2016A\u0304\u2016tr is order of \u221a mn then order of n3/2 entries are suffices to learn the matrix Y . This matches recent learning-theoretic guarantees for distributionfree learning with the trace norm (Shalev-Shwartz & Shamir, 2011)."}, {"heading": "5. Application II: Robust Low Rank Matrix Approximation", "text": "A very common problem in data analysis is finding a low-rank matrix A which approximates a given matrix Y , namely solving minA:rank(A)\u2264r d(A, Y ), where d is some discrepancy measure. For simplicity, assume that Y \u2208 Rn\u00d7n. When d(A, V ) is the normalized Frobenius norm d(A, V ) = 1n2 \u2211 i,j(Ai,j\u2212Yi,j)2, this problem can be solved efficiently via SVD. However, due to the use of the Frobenius norm, this procedure is well-known to be sensitive to outliers.\nOne way to make the procedure more robust is to replace the Frobenius norm by a less sensitive norm, such as the l1 norm d(A, V ) = 1 n2 \u2211 i,j |Ai,j\u2212Yi,j | (see for instance (A. Baccini & Falguerolles, 1996),(Croux & Filzmoser, 1998),(Ke & Kanade, 2005)). Unfortunately, there are no known efficient algorithms to obtain the global optimum of this objective function, subject to a rank constraint on A. However, using\n5To be more precise, this bound requires that the elements of A are bounded by a constant. But, since we can assume that the elements of Y are bounded by a constant, it is always possible to clip the elements of A to the range of the elements of Y without increasing F (A).\nour proposed algorithm, we can efficiently find a lowrank matrix which approximately minimizes d(A, V ). In particular, we can apply it to any convex discrepancy measure d, including robust ones such as the l1 norm. The only technicality is that our algorithm requires d to be smooth, which is not true in the case of the l1 norm. However, this can be easily alleviated by working with smoothed versions of the l1 norm, which replace the absolute value by a smooth approximation. One example is a Huber loss, defined as L(x) = x2/2 for |x| \u2264 1, and L(x) = |x| \u2212 1/2 otherwise.\nLemma 2 The smoothness parameter of d(A, Y ) = 1 n2 \u2211 i,j L(Ai,j \u2212 Yi,j), where L is the Huber loss, is at most 1/n2.\nProof It is easy to verify that the smoothness parameter of L(x) is 1, since L(x) is upper bounded by the parabola x2/2, whose smoothness parameter is exactly 1. Therefore,\nL(Ai,j + \u03b7uivj \u2212 Yi,j) \u2264 L(Ai,j \u2212 Yi,j)\n+ \u03b7L\u2032(Ai,j \u2212 Yi,j)uivj + \u03b72\n2 u2i v 2 j .\nTaking the average over all entries, this implies that\nf(\u03bb+ \u03b7eu,v) \u2264 f(\u03bb) + \u03b7\u2207u,vf(\u03bb) + \u03b72\nn2 \u2211 i,j u2i v 2 j .\nSince the last term is at most \u03b72/n2, the result follows.\nWe therefore obtain:\nCorollary 2 Let d(A, Y ) be the Huber loss discrepancy as defined in Lemma 2. Then, for any matrix A\u0304, GECO can find a matrix A with d(A, Y ) \u2264 d(A\u0304, Y )+ and rank(A) = O(\n\u2016A\u0304\u20162tr n2 )."}, {"heading": "6. Experiments", "text": "We evaluated GECO for the problem of matrix completion by conducting experiments on three standard collaborative filtering datasets: MovieLens100K, MovieLens1M, and MovieLens10M6. The different datasets contain 105, 106, 107 ratings of 943, 6040, 69878 users on 1682, 3706, 10677 movies, respectively. All the ranking are integers in 1 \u2212 5. We partitioned each data set into training and testing sets as done in (Jaggi & Sulovsky\u0300, 2010).\nWe implemented GECO while applying two of the variants described in Section 2.1 as we explain in details\n6Available through www.grouplens.org\nbelow. The first variant (see Section 2.1.1) tries to find update vectors (u\u2032, v\u2032) which leads to a larger decrease of the objective function relatively to the leading singular vectors (u, v) of the gradient matrix \u2207R(A). Inspired by the proof of Theorem 1, we observe that the decrease of the objective function inversely depends on the smoothness of the scalar function R(A+\u03b7uvt). We therefore would like to find a pair which on one hand has a large correlation with \u2207R(A) and on the other hand yields a smooth scalar function R(A+\u03b7uvt). The smoothness of R(A + \u03b7uvt) is analyzed in Lemma 1 and is shown to be at most 2|E| . Examining the proof lines more carefully, we see that for balanced vectors, i.e. ui = \u00b1 1\u221am , vj = \u00b1 1\u221a n , we obtain a lower smoothness parameter of 2mn . Thus, a possible good update direction is to choose u, v that maximizes uT\u2207R(A)v over vectors of the form ui = \u00b1 1\u221am , vj = \u00b1 1\u221a n . This is equivalent to maximizing uT\u2207R(A)v over the `\u221e balls of Rm and Rn, which is unfortunately known to be NP-hard. Nevertheless, a simple alternate maximization approach is easy to implement and often works well. That is, fixing some u, we can see that v = sign(uT\u2207(A))/ \u221a n maximizes the objective, and similarly, fixing v we have that u = sign(\u2207R(A)v)/ \u221a m is optimal. We therefore implement this alternate maximization at each step and find a candidate pair (u\u2032, v\u2032). As described in section Section 2.1.1, we compare the decrease of loss as obtained by the leading singular vectors, (u, v), and the candidate pair mentioned previously, (u\u2032, v\u2032), and update using the pair which leads to a larger decrease of the objective. We remind the reader that although (u\u2032, v\u2032) are obtained heuristically, our implementation is still provably correct and our guarantees from Section 3 still hold.\nIn addition we performed the additional replacement steps as described in Section 2.1.2. For that purpose, let q be the number of times we try to perform additional replacement steps for each rank. Each replace-\nment attempt is done using the alternate maximization procedure described previously. After utilizing q attempts of additional replacement steps, we force an increase of the rank. In our experiments, we set q = 20. Finally, we implemented the ApproxSV procedure using 30 iterations of the power iteration method.\nWe compared GECO to a state-of-the-art method, recently proposed in (Jaggi & Sulovsky\u0300, 2010), which we denote as the JS algorithm. JS, similarly to GECO, iteratively increases the rank by computing a direction that maximizes some objective function and performing a step in that direction. See more details in Section 1.1. In Figure 1, we plot the root mean squared error (RMSE) on the test set as a function of the rank. As can be seen, GECO decreases the error much faster than the JS algorithm. This is expected \u2014 see again the discussion in Section 1.1. We observe that GECO achieves slightly larger test error on the small data set, slightly smaller test error on the medium data set, and the same error on the large data set. On the small data set, GECO starts to overfit when the rank increases beyond 4. The JS algorithm avoids this overfitting by constraining the trace-norm, but also starts overfitting after around 30 iterations. On the other hand, on the medium data, the trace-norm constraint employed by the JS algorithm yields a higher estimation error, and GECO, which does not constrain the trace-norm, achieves a smaller error. In any case, GECO achieves very good results while using a rank of at most 10."}, {"heading": "7. Discussion", "text": "GECO is an efficient greedy approach for minimizing a convex function subject to a rank constraint. One of the main advantages of GECO is that each of its iterations involves running few (precisely, O(log(n))) iterations of the power method, and therefore GECO scales to large matrices. In future work we intend to\napply GECO to additional applications such as multiclass classification and learning fast quadratic classifiers."}, {"heading": "Acknowledgements", "text": "This work emerged from fruitful discussions with Tomer Baba, Barak Cohen, Harel Livyatan, and Oded Schwarz. The work is supported by the Israeli Science Foundation grant number 598-10."}, {"heading": "A. Proofs", "text": "A.1. Proof of Theorem 1\nTo prove the theorem we need the following key lemma, which generalizes a result given in (ShalevShwartz et al., 2010).\nLemma 3 Assume that f is \u03b2-smooth. Let I, I\u0304 be two subsets of U \u00d7 V. Let \u03bb be a minimizer of f(\u03bb) over all vectors with support in I and let \u03bb\u0304 be a vector supported on I\u0304. Assume that f(\u03bb) > f(\u03bb\u0304), denote s = \u2016\u03bb\u0304\u20161, and let \u03c4 \u2208 [0, 1). Let (u, v) = ApproxSV(\u2207R(A(\u03bb)), ). Then, there exists \u03b7 such that\nf(\u03bb)\u2212 f(\u03bb+ \u03b7eu,v) \u2265 (f(\u03bb)\u2212 f(\u03bb\u0304)) 2(1\u2212 \u03c4)2\n2\u03b2s2 .\nProof\nWithout loss of generality assume that \u03bb\u0304 \u2265 0 (if \u03bb\u0304p,q < 0 for some (p, q) we can set \u03bb\u0304\u2212p,q = \u2212\u03bb\u0304p,q and \u03bb\u0304p,q = 0 without effecting the objective) and assume that ut\u2207(R(A(\u03bb)))v \u2264 0 (if this does not hold, let u = \u2212u). For any (p, q), let \u2207p,q = pt\u2207R(A(\u03bb))q be the partial derivative of f w.r.t. coordinate (p, q) at \u03bb and denote\nQp,q(\u03b7) = f(\u03bb) + \u03b7\u2207p,q + \u03b2 \u03b72\n2 .\nNote that the definition of (u, v) and our assumption above implies that\n\u2212\u2207u,v = |\u2207u,v| \u2265 (1\u2212 \u03c4) max p,q |\u2207p,q| ,\nwhich gives\n\u2207u,v \u2264 (\u03c4 \u2212 1) max p,q |\u2207p,q| = (1\u2212 \u03c4) min p,q \u2207p,q .\nTherefore, for all \u03b7 \u2265 0 we have\nQu,v(\u03b7) \u2264 f(\u03bb) + (1\u2212 \u03c4)\u03b7min p,q \u2207p,q +\n\u03b2\u03b72\n2 .\nIn addition, the smoothness assumption tells us that for all \u03b7 we have f(\u03bb + \u03b7eu,v) \u2264 Qu,v(\u03b7). Thus, for any \u03b7 \u2265 0 we have\nmin a f(\u03bb+ aeu,v) \u2264 f(\u03bb+ \u03b7eu,v) \u2264 Qu,v(\u03b7)\nCombining the above we get\nmin a f(\u03bb+aeu,v) \u2264 f(\u03bb)+(1\u2212\u03c4)\u03b7 min (p,q)\u2208I\u0304\\I \u2207p,q+\n\u03b2\u03b72\n2 .\nMultiplying both sides by s and noting that\ns min (p,q)\u2208I\u0304\\I\n\u2207p,q \u2264 \u2211\n(p,q)\u2208I\u0304\\I\n\u03bb\u0304p,q\u2207p,q\nwe get that\nsmin a f(\u03bb+ aeu,v)\n\u2264 sf(\u03bb) + (1\u2212 \u03c4)\u03b7 \u2211\n(p,q)\u2208I\u0304\\I\n\u03bb\u0304p,q\u2207p,q + s \u03b2 \u03b72\n2 .\nSince \u03bb is a minimizer of f over I we have that\u2207p,q = 0 for (p, q) \u2208 I. Combining this with the fact that \u03bb is supported on I and \u03bb\u0304 is supported on I\u0304 we obtain that\u2211\n(p,q)\u2208I\u0304\\I\n\u03bb\u0304p,q\u2207p,q = \u3008\u03bb\u0304,\u2207f(\u03bb)\u3009 = \u3008\u03bb\u0304\u2212 \u03bb,\u2207f(\u03bb)\u3009 .\nFrom the convexity of f we know that \u3008\u03bb\u0304\u2212\u03bb,\u2207f(\u03bb)\u3009 \u2264 f(\u03bb\u0304)\u2212 f(\u03bb). Combining all the above we obtain\nsmin a f(\u03bb+aeu,v) \u2264 sf(\u03bb)+(1\u2212\u03c4)\u03b7(f(\u03bb\u0304)\u2212f(\u03bb))+s\u03b2 \u03b7\n2\n2 .\nThis holds for all \u03b7 \u2265 0 and in particular for \u03b7 = (f(\u03bb)\u2212 f(\u03bb\u0304))(1\u2212 \u03c4)/(s\u03b2) (which is positive). Thus,\nsmin a f(\u03bb+ aeu,v) \u2264 sf(\u03bb)\u2212 (f(\u03bb)\u2212 f(\u03bb\u0304)) 2(1\u2212 \u03c4)2 2\u03b2s .\nRearranging the above concludes our proof.\nEquipped with the above lemma we are ready to prove Theorem 1.\nFix some A\u0304 and let \u03bb\u0304 be the vector of its singular values. Thus, \u2016\u03bb\u0304\u20161 = \u2016A\u0304\u2016tr and f(\u03bb\u0304) = R(A\u0304). For each iteration i, denote i = f(\u03bb\n(i)) \u2212 f(\u03bb\u0304), where \u03bb(i) is the value of \u03bb at the beginning of iteration i of GECO, before we increase the rank to be i. Note that all the operations we perform in GECO or one if its variants guarantee that the loss is monotonically non-increasing. Therefore, if i \u2264 we are done. In addition, whenever we increase the rank by 1, the definition of the update implies that f(\u03bb(i+1)) \u2264 min\u03b7 f(\u03bb(i) + \u03b7eu,v), where (u, v) = ApproxSV(R(A(\u03bb(i))), \u03c4). Lemma 3 implies that\ni \u2212 i+1 = f(\u03bb(i))\u2212 f(\u03bb(i+1)) \u2265 2i (1\u2212 \u03c4)2\n2\u03b2 \u2016A\u0304\u20162tr . (4)\nUsing Lemma B.2 from (Shalev-Shwartz et al., 2010), the above implies that for i \u2265 2\u03b2 \u2016A\u0304\u20162tr/( (1\u2212 \u03c4)2) we have that i \u2264 . We obtain that if \u2016A\u0304\u20162tr \u2264 (r + 1)(1 \u2212 \u03c4)2/(2\u03b2) then r+1 \u2264 , which concludes the proof of Theorem 1.\nA.2. Proof of Theorem 2\nLet \u03bb\u0304 be the vector obtained from the SVD of A\u0304, that is, A\u0304 = A(\u03bb\u0304) and \u2016\u03bb\u0304\u20160 = rank(A\u0304). Note that\nf is \u03c3-strongly-convex over the support of \u03bb\u0304. Using Lemma 2.2 of (Shalev-Shwartz et al., 2010) we know that \u2016\u03bb\u0304\u201621 \u2264 2\u2016\u03bb\u0304\u20160 f(0)\n\u03c3 . But, since \u2016A\u0304\u2016tr = \u2016\u03bb\u0304\u20161, rank(A\u0304) = \u2016\u03bb\u0304\u20160, and f(0) = R(0), we get\n\u2016A\u0304\u20162tr \u2264 2rank(A\u0304)R(0)\n\u03c3 .\nThe proof follows from the above using Theorem 1."}], "references": [{"title": "A l1-norm pca and a heuristic approach", "author": ["A. Baccini", "P. Besse", "A. Falguerolles"], "venue": "Ordinal and Symbolic Data Analysis,", "citeRegEx": "Baccini et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Baccini et al\\.", "year": 1996}, {"title": "Uncovering shared structures in multiclass classification", "author": ["Amit", "Yonatan", "Fink", "Michael", "Srebro", "Nathan", "Ullman", "Shimon"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Amit et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Amit et al\\.", "year": 2007}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["J.F. Cai", "E.J. Candes", "Z. Shen"], "venue": null, "citeRegEx": "Cai et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2008}, {"title": "Matrix completion with noise", "author": ["E.J. Candes", "Y. Plan"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Candes and Plan,? \\Q2010\\E", "shortCiteRegEx": "Candes and Plan", "year": 2010}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Cand\u00e8s and Recht,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht", "year": 2009}, {"title": "Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm", "author": ["K.L. Clarkson"], "venue": "In Proceedings of the nineteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "Clarkson,? \\Q2008\\E", "shortCiteRegEx": "Clarkson", "year": 2008}, {"title": "Robust factorization of a data matrix", "author": ["C. Croux", "P. Filzmoser"], "venue": "In COMPASTAT, Proceedings in Computational Statistics,", "citeRegEx": "Croux and Filzmoser,? \\Q1998\\E", "shortCiteRegEx": "Croux and Filzmoser", "year": 1998}, {"title": "Subspace pursuit for compressive sensing: Closing the gap between performance and complexity", "author": ["W. Dai", "O. Milenkovic"], "venue": null, "citeRegEx": "Dai and Milenkovic,? \\Q2008\\E", "shortCiteRegEx": "Dai and Milenkovic", "year": 2008}, {"title": "Greedy adaptive approximation", "author": ["G. Davis", "S. Mallat", "M. Avellaneda"], "venue": "Journal of Constructive Approximation,", "citeRegEx": "Davis et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Davis et al\\.", "year": 1997}, {"title": "A rank minimization heuristic with application to minimum order system approximation", "author": ["M. Fazel", "H. Hindi", "S.P. Boyd"], "venue": "In American Control Conference,", "citeRegEx": "Fazel et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Fazel et al\\.", "year": 2001}, {"title": "An algorithm for quadratic programming", "author": ["M. Frank", "P. Wolfe"], "venue": "Naval Res. Logist. Quart.,", "citeRegEx": "Frank and Wolfe,? \\Q1956\\E", "shortCiteRegEx": "Frank and Wolfe", "year": 1956}, {"title": "Sparse approximate solutions to semidefinite programs", "author": ["Hazan", "Elad"], "venue": "In Proceedings of the 8th Latin American conference on Theoretical informatics,", "citeRegEx": "Hazan and Elad.,? \\Q2008\\E", "shortCiteRegEx": "Hazan and Elad.", "year": 2008}, {"title": "A simple algorithm for nuclear norm regularized problems", "author": ["M. Jaggi", "M. Sulovsk\u1ef3"], "venue": "In ICML,", "citeRegEx": "Jaggi and Sulovsk\u1ef3,? \\Q2010\\E", "shortCiteRegEx": "Jaggi and Sulovsk\u1ef3", "year": 2010}, {"title": "Robust l1 norm factorization in the presence of outliers and missing data by alternative convex programming", "author": ["Q. Ke", "T. Kanade"], "venue": "In CVPR,", "citeRegEx": "Ke and Kanade,? \\Q2005\\E", "shortCiteRegEx": "Ke and Kanade", "year": 2005}, {"title": "Optspace: A gradient descent algorithm on the grassman manifold for matrix completion", "author": ["R.H. Keshavan", "S. Oh"], "venue": "Arxiv preprint arXiv:0910.5260", "citeRegEx": "Keshavan and Oh,? \\Q2009\\E", "shortCiteRegEx": "Keshavan and Oh", "year": 2009}, {"title": "Matrix completion from a few entries", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Keshavan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2010}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Koren", "Yehuda", "Bell", "Robert M", "Volinsky", "Chris"], "venue": "IEEE Computer,", "citeRegEx": "Koren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Estimating the largest eigenvalue by the power and Lanczos algorithms with a random start", "author": ["J. Kuczy\u0144ski", "H. Wo\u017aniakowski"], "venue": "SIAM journal on matrix analysis and applications,", "citeRegEx": "Kuczy\u0144ski and Wo\u017aniakowski,? \\Q1992\\E", "shortCiteRegEx": "Kuczy\u0144ski and Wo\u017aniakowski", "year": 1992}, {"title": "Admira: Atomic decomposition for minimum rank approximation", "author": ["K. Lee", "Y. Bresler"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Lee and Bresler,? \\Q2010\\E", "shortCiteRegEx": "Lee and Bresler", "year": 2010}, {"title": "Matching pursuits with timefrequency dictionaries", "author": ["S. Mallat", "Z. Zhang"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Mallat and Zhang,? \\Q1993\\E", "shortCiteRegEx": "Mallat and Zhang", "year": 1993}, {"title": "Sparse approximate solutions to linear systems", "author": ["B. Natarajan"], "venue": "SIAM J. Computing,", "citeRegEx": "Natarajan,? \\Q1995\\E", "shortCiteRegEx": "Natarajan", "year": 1995}, {"title": "CoSaMP: Iterative signal recovery from incomplete and inaccurate samples", "author": ["D. Needell", "J.A. Tropp"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Needell and Tropp,? \\Q2009\\E", "shortCiteRegEx": "Needell and Tropp", "year": 2009}, {"title": "Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition", "author": ["YC Pati", "R. Rezaiifar", "Krishnaprasad", "PS"], "venue": "In Signals, Systems and Computers,", "citeRegEx": "Pati et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pati et al\\.", "year": 1993}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "author": ["B. Recht", "M. Fazel", "P.A. Parrilo"], "venue": null, "citeRegEx": "Recht et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2007}, {"title": "Collaborative filtering with the trace norm: Learning, bounding, and transducing", "author": ["Shalev-Shwartz", "Shai", "Shamir", "Ohad"], "venue": "In COLT,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Trading accuracy for sparsity in optimization problems with sparsity constraints", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong", "Srebro", "Nathan"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2010}, {"title": "Generalization error bounds for collaborative prediction with low-rank matrices", "author": ["N. Srebro", "N. Alon", "T. Jaakkola"], "venue": "Advances In Neural Information Processing Systems,", "citeRegEx": "Srebro et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2005}, {"title": "Lemma 3 Assume that f is \u03b2-smooth. Let I, \u012a be two subsets of U \u00d7 V. Let \u03bb be a minimizer of f(\u03bb) over all vectors with support in I and let \u03bb\u0304 be a vector supported on \u012a", "author": ["Shwartz"], "venue": null, "citeRegEx": "Shwartz,? \\Q2010\\E", "shortCiteRegEx": "Shwartz", "year": 2010}], "referenceMentions": [{"referenceID": 16, "context": "This problem arises in many machine learning applications such as collaborating filtering (Koren et al., 2009), robust low rank matrix approximation (Ke & Kanade, 2005; Croux & Filzmoser, 1998; A.", "startOffset": 90, "endOffset": 110}, {"referenceID": 1, "context": "Baccini & Falguerolles, 1996), and multiclass classification (Amit et al., 2007).", "startOffset": 61, "endOffset": 80}, {"referenceID": 20, "context": "The rank constraint on A is non-convex and therefore it is generally NP-hard to solve Equation (1) (this follows from (Natarajan, 1995; Davis et al., 1997)).", "startOffset": 118, "endOffset": 155}, {"referenceID": 8, "context": "The rank constraint on A is non-convex and therefore it is generally NP-hard to solve Equation (1) (this follows from (Natarajan, 1995; Davis et al., 1997)).", "startOffset": 118, "endOffset": 155}, {"referenceID": 2, "context": "See for example (Cai et al., 2008; Candes & Plan, 2010; Cand\u00e8s & Recht, 2009; Keshavan et al., 2010; Keshavan & Oh, 2009).", "startOffset": 16, "endOffset": 121}, {"referenceID": 15, "context": "See for example (Cai et al., 2008; Candes & Plan, 2010; Cand\u00e8s & Recht, 2009; Keshavan et al., 2010; Keshavan & Oh, 2009).", "startOffset": 16, "endOffset": 121}, {"referenceID": 25, "context": ", 2002) algorithms), and in particular we extend the fully corrective forward greedy selection algorithm given in (Shalev-Shwartz et al., 2010)).", "startOffset": 114, "endOffset": 143}, {"referenceID": 5, "context": "The algorithm we propose is also related to Hazan\u2019s algorithm (Hazan, 2008) for solving PSD problems, which in turns relies on Frank-Wolfe algorithm (Frank & Wolfe, 1956) (see Clarkson (Clarkson, 2008)), as well as to the follow-up paper of (Jaggi & Sulovsk\u1ef3, 2010), which applies Hazan\u2019s algorithm for optimizing with trace-norm constraints.", "startOffset": 185, "endOffset": 201}, {"referenceID": 25, "context": "These differences between the approaches are analogous to the difference between Frank-Wolfe algorithm and fully corrective greedy selection, for minimizing over sparse vectors, as discussed in (Shalev-Shwartz et al., 2010).", "startOffset": 194, "endOffset": 223}, {"referenceID": 26, "context": "(Srebro et al., 2005)), it is possible to show that for any matrix A of rank at most r we have that with high probability", "startOffset": 0, "endOffset": 21}], "year": 2011, "abstractText": "We address the problem of minimizing a convex function over the space of large matrices with low rank. While this optimization problem is hard in general, we propose an efficient greedy algorithm and derive its formal approximation guarantees. Each iteration of the algorithm involves (approximately) finding the left and right singular vectors corresponding to the largest singular value of a certain matrix, which can be calculated in linear time. This leads to an algorithm which can scale to large matrices arising in several applications such as matrix completion for collaborative filtering and robust low rank matrix approximation.", "creator": "LaTeX with hyperref package"}}}