{"id": "1106.1813", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2011", "title": "SMOTE: Synthetic Minority Over-sampling Technique", "abstract": "An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of \"normal\" examples with only a small percentage of \"abnormal\" or \"interesting\" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.", "histories": [["v1", "Thu, 9 Jun 2011 13:53:42 GMT  (229kb)", "http://arxiv.org/abs/1106.1813v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["n v chawla", "k w bowyer", "l o hall", "w p kegelmeyer"], "accepted": false, "id": "1106.1813"}, "pdf": {"name": "1106.1813.pdf", "metadata": {"source": "CRF", "title": "SMOTE: Synthetic Minority Over-sampling Technique", "authors": ["Nitesh V. Chawla", "Kevin W. Bowyer", "Philip Kegelmeyer"], "emails": ["chawla@csee.usf.edu", "kwb@cse.nd.edu", "hall@csee.usf.edu", "wpk@california.sandia.gov"], "sections": [{"heading": "1. Introduction", "text": "A dataset is imbalanced if the classes are not approximately equally represented. Imbalance on the order of 100 to 1 is prevalent in fraud detection and imbalance of up to 100,000 to\nc\u00a92002 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.\n1 has been reported in other applications (Provost & Fawcett, 2001). There have been attempts to deal with imbalanced datasets in domains such as fraudulent telephone calls (Fawcett & Provost, 1996), telecommunications management (Ezawa, Singh, & Norton, 1996), text classification (Lewis & Catlett, 1994; Dumais, Platt, Heckerman, & Sahami, 1998; Mladenic\u0301 & Grobelnik, 1999; Lewis & Ringuette, 1994; Cohen, 1995a) and detection of oil spills in satellite images (Kubat, Holte, & Matwin, 1998).\nThe performance of machine learning algorithms is typically evaluated using predictive accuracy. However, this is not appropriate when the data is imbalanced and/or the costs of different errors vary markedly. As an example, consider the classification of pixels in mammogram images as possibly cancerous (Woods, Doss, Bowyer, Solka, Priebe, & Kegelmeyer, 1993). A typical mammography dataset might contain 98% normal pixels and 2% abnormal pixels. A simple default strategy of guessing the majority class would give a predictive accuracy of 98%. However, the nature of the application requires a fairly high rate of correct detection in the minority class and allows for a small error rate in the majority class in order to achieve this. Simple predictive accuracy is clearly not appropriate in such situations. The Receiver Operating Characteristic (ROC) curve is a standard technique for summarizing classifier performance over a range of tradeoffs between true positive and false positive error rates (Swets, 1988). The Area Under the Curve (AUC) is an accepted traditional performance metric for a ROC curve (Duda, Hart, & Stork, 2001; Bradley, 1997; Lee, 2000). The ROC convex hull can also be used as a robust method of identifying potentially optimal classifiers (Provost & Fawcett, 2001). If a line passes through a point on the convex hull, then there is no other line with the same slope passing through another point with a larger true positive (TP) intercept. Thus, the classifier at that point is optimal under any distribution assumptions in tandem with that slope.\nThe machine learning community has addressed the issue of class imbalance in two ways. One is to assign distinct costs to training examples (Pazzani, Merz, Murphy, Ali, Hume, & Brunk, 1994; Domingos, 1999). The other is to re-sample the original dataset, either by oversampling the minority class and/or under-sampling the majority class (Kubat & Matwin, 1997; Japkowicz, 2000; Lewis & Catlett, 1994; Ling & Li, 1998). Our approach (Chawla, Bowyer, Hall, & Kegelmeyer, 2000) blends under-sampling of the majority class with a special form of over-sampling the minority class. Experiments with various datasets and the C4.5 decision tree classifier (Quinlan, 1992), Ripper (Cohen, 1995b), and a Naive Bayes Classifier show that our approach improves over other previous re-sampling, modifying loss ratio, and class priors approaches, using either the AUC or ROC convex hull.\nSection 2 gives an overview of performance measures. Section 3 reviews the most closely related work dealing with imbalanced datasets. Section 4 presents the details of our approach. Section 5 presents experimental results comparing our approach to other re-sampling approaches. Section 6 discusses the results and suggests directions for future work."}, {"heading": "2. Performance Measures", "text": "The performance of machine learning algorithms is typically evaluated by a confusion matrix as illustrated in Figure 1 (for a 2 class problem). The columns are the Predicted class and the rows are the Actual class. In the confusion matrix, TN is the number of negative examples\ncorrectly classified (True Negatives), FP is the number of negative examples incorrectly classified as positive (False Positives), FN is the number of positive examples incorrectly classified as negative (False Negatives) and TP is the number of positive examples correctly classified (True Positives).\nPredictive accuracy is the performance measure generally associated with machine learning algorithms and is defined as Accuracy = (TP + TN)/(TP + FP + TN + FN). In the context of balanced datasets and equal error costs, it is reasonable to use error rate as a performance metric. Error rate is 1 \u2212 Accuracy. In the presence of imbalanced datasets with unequal error costs, it is more appropriate to use the ROC curve or other similar techniques (Ling & Li, 1998; Drummond & Holte, 2000; Provost & Fawcett, 2001; Bradley, 1997; Turney, 1996).\nROC curves can be thought of as representing the family of best decision boundaries for relative costs of TP and FP. On an ROC curve the X-axis represents %FP = FP/(TN+FP ) and the Y-axis represents %TP = TP/(TP+FN). The ideal point on the ROC curve would be (0,100), that is all positive examples are classified correctly and no negative examples are misclassified as positive. One way an ROC curve can be swept out is by manipulating the balance of training samples for each class in the training set. Figure 2 shows an illustration. The line y = x represents the scenario of randomly guessing the class. Area Under the ROC Curve (AUC) is a useful metric for classifier performance as it is independent of the decision criterion selected and prior probabilities. The AUC comparison can establish a dominance relationship between classifiers. If the ROC curves are intersecting, the total AUC is an average comparison between models (Lee, 2000). However, for some specific cost and class distributions, the classifier having maximum AUC may in fact be suboptimal. Hence, we also compute the ROC convex hulls, since the points lying on the ROC convex hull are potentially optimal (Provost, Fawcett, & Kohavi, 1998; Provost & Fawcett, 2001)."}, {"heading": "3. Previous Work: Imbalanced datasets", "text": "Kubat and Matwin (1997) selectively under-sampled the majority class while keeping the original population of the minority class. They have used the geometric mean as a performance measure for the classifier, which can be related to a single point on the ROC curve. The minority examples were divided into four categories: some noise overlapping the positive class decision region, borderline samples, redundant samples and safe samples. The borderline examples were detected using the Tomek links concept (Tomek, 1976). Another\nrelated work proposed the SHRINK system that classifies an overlapping region of minority (positive) and majority (negative) classes as positive; it searches for the \u201cbest positive region\u201d (Kubat et al., 1998).\nJapkowicz (2000) discussed the effect of imbalance in a dataset. She evaluated three strategies: under-sampling, resampling and a recognition-based induction scheme. We focus on her sampling approaches. She experimented on artificial 1D data in order to easily measure and construct concept complexity. Two resampling methods were considered. Random resampling consisted of resampling the smaller class at random until it consisted of as many samples as the majority class and \u201cfocused resampling\u201d consisted of resampling only those minority examples that occurred on the boundary between the minority and majority classes. Random under-sampling was considered, which involved under-sampling the majority class samples at random until their numbers matched the number of minority class samples; focused under-sampling involved under-sampling the majority class samples lying further away. She noted that both the sampling approaches were effective, and she also observed that using the sophisticated sampling techniques did not give any clear advantage in the domain considered (Japkowicz, 2000).\nOne approach that is particularly relevant to our work is that of Ling and Li (1998). They combined over-sampling of the minority class with under-sampling of the majority class. They used lift analysis instead of accuracy to measure a classifier\u2019s performance. They proposed that the test examples be ranked by a confidence measure and then lift be used as the evaluation criteria. A lift curve is similar to an ROC curve, but is more tailored for the\nmarketing analysis problem (Ling & Li, 1998). In one experiment, they under-sampled the majority class and noted that the best lift index is obtained when the classes are equally represented (Ling & Li, 1998). In another experiment, they over-sampled the positive (minority) examples with replacement to match the number of negative (majority) examples to the number of positive examples. The over-sampling and under-sampling combination did not provide significant improvement in the lift index. However, our approach to oversampling differs from theirs.\nSolberg and Solberg (1996) considered the problem of imbalanced data sets in oil slick classification from SAR imagery. They used over-sampling and under-sampling techniques to improve the classification of oil slicks. Their training data had a distribution of 42 oil slicks and 2,471 look-alikes, giving a prior probability of 0.98 for look-alikes. This imbalance would lead the learner (without any appropriate loss functions or a methodology to modify priors) to classify almost all look-alikes correctly at the expense of misclassifying many of the oil slick samples (Solberg & Solberg, 1996). To overcome this imbalance problem, they over-sampled (with replacement) 100 samples from the oil slick, and they randomly sampled 100 samples from the non oil slick class to create a new dataset with equal probabilities. They learned a classifier tree on this balanced data set and achieved a 14% error rate on the oil slicks in a leave-one-out method for error estimation; on the look alikes they achieved an error rate of 4% (Solberg & Solberg, 1996).\nAnother approach that is similar to our work is that of Domingos (1999). He compares the \u201cmetacost\u201d approach to each of majority under-sampling and minority over-sampling. He finds that metacost improves over either, and that under-sampling is preferable to minority over-sampling. Error-based classifiers are made cost-sensitive. The probability of each class for each example is estimated, and the examples are relabeled optimally with respect to the misclassification costs. The relabeling of the examples expands the decision space as it creates new samples from which the classifier may learn (Domingos, 1999).\nA feed-forward neural network trained on an imbalanced dataset may not learn to discriminate enough between classes (DeRouin, Brown, Fausett, & Schneider, 1991). The authors proposed that the learning rate of the neural network be adapted to the statistics of class representation in the data. They calculated an attention factor from the proportion of samples presented to the neural network for training. The learning rate of the network elements was adjusted based on the attention factor. They experimented on an artificially generated training set and on a real-world training set, both with multiple (more than two) classes. They compared this to the approach of replicating the minority class samples to balance the data set used for training. The classification accuracy on the minority class was improved.\nLewis and Catlett (1994) examined heterogeneous uncertainty sampling for supervised learning. This method is useful for training samples with uncertain classes. The training samples are labeled incrementally in two phases and the uncertain instances are passed on to the next phase. They modified C4.5 to include a loss ratio for determining the class values at the leaves. The class values were determined by comparison with a probability threshold of LR/(LR+ 1), where LR is the loss ratio (Lewis & Catlett, 1994).\nThe information retrieval (IR) domain (Dumais et al., 1998; Mladenic\u0301 & Grobelnik, 1999; Lewis & Ringuette, 1994; Cohen, 1995a) also faces the problem of class imbalance in the dataset. A document or web page is converted into a bag-of-words representation;\nthat is, a feature vector reflecting occurrences of words in the page is constructed. Usually, there are very few instances of the interesting category in text categorization. This overrepresentation of the negative class in information retrieval problems can cause problems in evaluating classifiers\u2019 performances. Since error rate is not a good metric for skewed datasets, the classification performance of algorithms in information retrieval is usually measured by precision and recall:\nrecall = TP\nTP + FN\nprecision = TP\nTP + FP\nMladenic\u0301 and Grobelnik (1999) proposed a feature subset selection approach to deal with imbalanced class distribution in the IR domain. They experimented with various feature selection methods, and found that the odds ratio (van Rijsbergen, Harper, & Porter, 1981) when combined with a Naive Bayes classifier performs best in their domain. Odds ratio is a probabilistic measure used to rank documents according to their relevance to the positive class (minority class). Information gain for a word, on the other hand, does not pay attention to a particular target class; it is computed per word for each class. In an imbalanced text dataset (assuming 98 to 99% is the negative class), most of the features will be associated with the negative class. Odds ratio incorporates the target class information in its metric giving better results when compared to information gain for text categorization.\nProvost and Fawcett (1997) introduced the ROC convex hull method to estimate the classifier performance for imbalanced datasets. They note that the problems of unequal class distribution and unequal error costs are related and that little work has been done to address either problem (Provost & Fawcett, 2001). In the ROC convex hull method, the ROC space is used to separate classification performance from the class and cost distribution information.\nTo summarize the literature, under-sampling the majority class enables better classifiers to be built than over-sampling the minority class. A combination of the two as done in previous work does not lead to classifiers that outperform those built utilizing only undersampling. However, the over-sampling of the minority class has been done by sampling with replacement from the original data. Our approach uses a different method of over-sampling."}, {"heading": "4. SMOTE: Synthetic Minority Over-sampling TEchnique", "text": ""}, {"heading": "4.1 Minority over-sampling with replacement", "text": "Previous research (Ling & Li, 1998; Japkowicz, 2000) has discussed over-sampling with replacement and has noted that it doesn\u2019t significantly improve minority class recognition. We interpret the underlying effect in terms of decision regions in feature space. Essentially, as the minority class is over-sampled by increasing amounts, the effect is to identify similar but more specific regions in the feature space as the decision region for the minority class. This effect for decision trees can be understood from the plots in Figure 3.\nThe data for the plot in Figure 3 was extracted from a Mammography dataset1 (Woods et al., 1993). The minority class samples are shown by + and the majority class samples are shown by o in the plot. In Figure 3(a), the region indicated by the solid-line rectangle is a majority class decision region. Nevertheless, it contains three minority class samples shown by \u2019+\u2019 as false negatives. If we replicate the minority class, the decision region for the minority class becomes very specific and will cause new splits in the decision tree. This will lead to more terminal nodes (leaves) as the learning algorithm tries to learn more and more specific regions of the minority class; in essence, overfitting. Replication of the minority class does not cause its decision boundary to spread into the majority class region. Thus, in Figure 3(b), the three samples previously in the majority class decision region now have very specific decision regions."}, {"heading": "4.2 SMOTE", "text": "We propose an over-sampling approach in which the minority class is over-sampled by creating \u201csynthetic\u201d examples rather than by over-sampling with replacement. This approach is inspired by a technique that proved successful in handwritten character recognition (Ha & Bunke, 1997). They created extra training data by performing certain operations on real data. In their case, operations like rotation and skew were natural ways to perturb the training data. We generate synthetic examples in a less application-specific manner, by operating in \u201cfeature space\u201d rather than \u201cdata space\u201d. The minority class is over-sampled by taking each minority class sample and introducing synthetic examples along the line segments joining any/all of the k minority class nearest neighbors. Depending upon the amount of over-sampling required, neighbors from the k nearest neighbors are randomly chosen. Our implementation currently uses five nearest neighbors. For instance, if the amount of over-sampling needed is 200%, only two neighbors from the five nearest neighbors are chosen and one sample is generated in the direction of each. Synthetic samples are generated in the following way: Take the difference between the feature vector (sample) under consideration and its nearest neighbor. Multiply this difference by a random number between 0 and 1, and add it to the feature vector under consideration. This causes the selection of a random point along the line segment between two specific features. This approach effectively forces the decision region of the minority class to become more general.\nAlgorithm SMOTE , on the next page, is the pseudo-code for SMOTE. Table 4.2 shows an example of calculation of random synthetic samples. The amount of over-sampling is a parameter of the system, and a series of ROC curves can be generated for different populations and ROC analysis performed.\nThe synthetic examples cause the classifier to create larger and less specific decision regions as shown by the dashed lines in Figure 3(c), rather than smaller and more specific regions. More general regions are now learned for the minority class samples rather than those being subsumed by the majority class samples around them. The effect is that decision trees generalize better. Figures 4 and 5 compare the minority over-sampling with replacement and SMOTE. The experiments were conducted on the mammography dataset. There were 10923 examples in the majority class and 260 examples in the minority class originally. We have approximately 9831 examples in the majority class and 233 examples\n1. The data is available from the USF Intelligent Systems Lab, http://morden.csee.usf.edu/\u02dcchawla.\nin the minority class for the training set used in 10-fold cross-validation. The minority class was over-sampled at 100%, 200%, 300%, 400% and 500% of its original size. The graphs show that the tree sizes for minority over-sampling with replacement at higher degrees of replication are much greater than those for SMOTE, and the minority class recognition of the minority over-sampling with replacement technique at higher degrees of replication isn\u2019t as good as SMOTE.\nAlgorithm SMOTE (T, N, k) Input: Number of minority class samples T ; Amount of SMOTE N%; Number of nearest neighbors k Output: (N/100) * T synthetic minority class samples 1. (\u2217 If N is less than 100%, randomize the minority class samples as only a random percent of them will be SMOTEd. \u2217) 2. if N < 100 3. then Randomize the T minority class samples 4. T = (N/100) \u2217 T 5. N = 100 6. endif 7. N = (int)(N/100) (\u2217 The amount of SMOTE is assumed to be in integral multiples of 100. \u2217) 8. k = Number of nearest neighbors 9. numattrs = Number of attributes 10. Sample[ ][ ]: array for original minority class samples 11. newindex: keeps a count of number of synthetic samples generated, initialized to 0 12. Synthetic[ ][ ]: array for synthetic samples (\u2217 Compute k nearest neighbors for each minority class sample only. \u2217) 13. for i \u2190 1 to T 14. Compute k nearest neighbors for i, and save the indices in the nnarray 15. Populate(N , i, nnarray) 16. endfor\nPopulate(N, i, nnarray) (\u2217 Function to generate the synthetic samples. \u2217) 17. while N 6= 0 18. Choose a random number between 1 and k, call it nn. This step chooses one of the k nearest neighbors of i. 19. for attr \u2190 1 to numattrs 20. Compute: dif = Sample[nnarray[nn]][attr]\u2212 Sample[i][attr] 21. Compute: gap = random number between 0 and 1 22. Synthetic[newindex][attr] = Sample[i][attr] + gap \u2217 dif 23. endfor 24. newindex++ 25. N = N \u2212 1 26. endwhile 27. return (\u2217 End of Populate. \u2217)\nEnd of Pseudo-Code."}, {"heading": "4.3 Under-sampling and SMOTE Combination", "text": "The majority class is under-sampled by randomly removing samples from the majority class population until the minority class becomes some specified percentage of the majority class. This forces the learner to experience varying degrees of under-sampling and at higher degrees of under-sampling the minority class has a larger presence in the training set. In describing our experiments, our terminology will be such that if we under-sample the majority class at 200%, it would mean that the modified dataset will contain twice as many elements from the minority class as from the majority class; that is, if the minority class had 50 samples and the majority class had 200 samples and we under-sample majority at 200%, the majority class would end up having 25 samples. By applying a combination of under-sampling and over-sampling, the initial bias of the learner towards the negative (majority) class is reversed in the favor of the positive (minority) class. Classifiers are learned on the dataset perturbed by \u201cSMOTING\u201d the minority class and under-sampling the majority class."}, {"heading": "5. Experiments", "text": "We used three different machine learning algorithms for our experiments. Figure 6 provides an overview of our experiments.\n1. C4.5: We compared various combinations of SMOTE and under-sampling with plain under-sampling using C4.5 release 8 (Quinlan, 1992) as the base classifier.\n2. Ripper: We compared various combinations of SMOTE and under-sampling with plain under-sampling using Ripper (Cohen, 1995b) as the base classifier. We also varied Ripper\u2019s loss ratio (Cohen & Singer, 1996; Lewis & Catlett, 1994) from 0.9 to 0.001 (as a means of varying misclassification cost) and compared the effect of this variation with the combination of SMOTE and under-sampling. By reducing the loss ratio from 0.9 to 0.001 we were able to build a set of rules for the minority class.\n3. Naive Bayes Classifier: The Naive Bayes Classifier2 can be made cost-sensitive by varying the priors of the minority class. We varied the priors of the minority class from 1 to 50 times the majority class and compared with C4.5\u2019s SMOTE and under-sampling combination.\nThese different learning algorithms allowed SMOTE to be compared to some methods that can handle misclassification costs directly. %FP and %TP were averaged over 10-fold cross-validation runs for each of the data combinations. The minority class examples were over-sampled by calculating the five nearest neighbors and generating synthetic examples. The AUC was calculated using the trapezoidal rule. We extrapolated an extra point of TP = 100% and FP = 100% for each ROC curve. We also computed the ROC convex hull to identify the optimal classifiers, as the points lying on the hull are potentially optimal classifiers (Provost & Fawcett, 2001).\n2. The source code was downloaded from http://fuzzy.cs.uni-magdeburg.de/\u02dcborgelt/software.html."}, {"heading": "5.1 Datasets", "text": "We experimented on nine different datasets. These datasets are summarized in Table 5.2. These datasets vary extensively in their size and class proportions, thus offering different domains for SMOTE. In order of increasing imbalance they are:\n1. The Pima Indian Diabetes (Blake & Merz, 1998) has 2 classes and 768 samples. The data is used to identify the positive diabetes cases in a population near Phoenix, Arizona. The number of positive class samples is only 268. Good sensitivity to detection of diabetes cases will be a desirable attribute of the classifier.\n2. The Phoneme dataset is from the ELENA project3. The aim of the dataset is to distinguish between nasal (class 0) and oral sounds (class 1). There are 5 features. The class distribution is 3,818 samples in class 0 and 1,586 samples in class 1.\n3. The Adult dataset (Blake & Merz, 1998) has 48,842 samples with 11,687 samples belonging to the minority class. This dataset has 6 continuous features and 8 nominal features. SMOTE and SMOTE-NC (see Section 6.1) algorithms were evaluated on this dataset. For SMOTE, we extracted the continuous features and generated a new dataset with only continuous features.\n4. The E-state data4 (Hall, Mohney, & Kier, 1991) consists of electrotopological state descriptors for a series of compounds from the National Cancer Institute\u2019s Yeast AntiCancer drug screen. E-state descriptors from the NCI Yeast AntiCancer Drug Screen were generated by Tripos, Inc. Briefly, a series of about 60,000 compounds were tested against a series of 6 yeast strains at a given concentration. The test was a high-throughput screen at only one concentration so the results are subject to contamination, etc. The growth inhibition of the yeast strain when exposed to the given compound (with respect to growth of the yeast in a neutral solvent) was measured. The activity classes are either active \u2014 at least one single yeast strain was inhibited more than 70%, or inactive \u2014 no yeast strain was inhibited more than 70%. The dataset has 53,220 samples with 6,351 samples of active compounds.\n5. The Satimage dataset (Blake & Merz, 1998) has 6 classes originally. We chose the smallest class as the minority class and collapsed the rest of the classes into one as was done in (Provost et al., 1998). This gave us a skewed 2-class dataset, with 5809 majority class samples and 626 minority class samples.\n6. The Forest Cover dataset is from the UCI repository (Blake & Merz, 1998). This dataset has 7 classes and 581,012 samples. This dataset is for the prediction of forest cover type based on cartographic variables. Since our system currently works for binary classes we extracted data for two classes from this dataset and ignored the rest. Most other approaches only work for only two classes (Ling & Li, 1998; Japkowicz, 2000; Kubat & Matwin, 1997; Provost & Fawcett, 2001). The two classes we considered are Ponderosa Pine with 35,754 samples and Cottonwood/Willow with 2,747\n3. ftp.dice.ucl.ac.be in the directory pub/neural-nets/ELENA/databases. 4. We would like to thank Steven Eschrich for providing the dataset and description to us.\nsamples. Nevertheless, the SMOTE technique can be applied to a multiple class problem as well by specifying what class to SMOTE for. However, in this paper, we have focused on 2-classes problems, to explicitly represent positive and negative classes.\n7. The Oil dataset was provided by Robert Holte and is used in their paper (Kubat et al., 1998). This dataset has 41 oil slick samples and 896 non-oil slick samples.\n8. The Mammography dataset (Woods et al., 1993) has 11,183 samples with 260 calcifications. If we look at predictive accuracy as a measure of goodness of the classifier for this case, the default accuracy would be 97.68% when every sample is labeled noncalcification. But, it is desirable for the classifier to predict most of the calcifications correctly.\n9. The Can dataset was generated from the Can ExodusII data using the AVATAR (Chawla & Hall, 1999) version of the Mustafa Visualization tool5. The portion of the can being crushed was marked as \u201cvery interesting\u201d and the rest of the can was marked as \u201cunknown.\u201d A dataset of size 443,872 samples with 8,360 samples marked as \u201cvery interesting\u201d was generated."}, {"heading": "5.2 ROC Creation", "text": "A ROC curve for SMOTE is produced by using C4.5 or Ripper to create a classifier for each one of a series of modified training datasets. A given ROC curve is produced by first over-sampling the minority class to a specified degree and then under-sampling the majority class at increasing degrees to generate the successive points on the curve. The amount of under-sampling is identical to plain under-sampling. So, each corresponding point on each ROC curve for a dataset represents the same number of majority class samples. Different ROC curves are produced by starting with different levels of minority over-sampling. ROC curves were also generated by varying the loss ratio in Ripper from 0.9 to 0.001 and by varying the priors of the minority class from the original distribution to up to 50 times the majority class for a Naive Bayes Classifier.\n5. The Mustafa visualization tool was developed by Mike Glass of Sandia National Labs.\nFigures 9 through 23 show the experimental ROC curves obtained for the nine datasets with the three classifiers. The ROC curve for plain under-sampling of the majority class (Ling & Li, 1998; Japkowicz, 2000; Kubat & Matwin, 1997; Provost & Fawcett, 2001) is compared with our approach of combining synthetic minority class over-sampling (SMOTE) with majority class under-sampling. The plain under-sampling curve is labeled \u201cUnder\u201d, and the SMOTE and under-sampling combination ROC curve is labeled \u201cSMOTE\u201d. Depending on the size and relative imbalance of the dataset, one to five SMOTE and undersampling curves are created. We only show the best results from SMOTE combined with under-sampling and the plain under-sampling curve in the graphs. The SMOTE ROC curve from C4.5 is also compared with the ROC curve obtained from varying the priors of minority class using a Naive Bayes classifier \u2014 labeled as \u201cNaive Bayes\u201d. \u201cSMOTE\u201d, \u201cUnder\u201d, and \u201cLoss Ratio\u201d ROC curves, generated using Ripper are also compared. For a given family of ROC curves, an ROC convex hull (Provost & Fawcett, 2001) is generated. The ROC convex hull is generated using the Graham\u2019s algorithm (O\u2019Rourke, 1998). For reference, we show the ROC curve that would be obtained using minority over-sampling by replication in Figure 19.\nEach point on the ROC curve is the result of either a classifier (C4.5 or Ripper) learned for a particular combination of under-sampling and SMOTE, a classifier (C4.5 or Ripper) learned with plain under-sampling, or a classifier (Ripper) learned using some loss ratio or a classifier (Naive Bayes) learned for a different prior for the minority class. Each point represents the average (%TP and %FP) 10-fold cross-validation result. The lower leftmost point for a given ROC curve is from the raw dataset, without any majority class under-\nsampling or minority class over-sampling. The minority class was over-sampled at 50%, 100%, 200%, 300%, 400%, 500%. The majority class was under-sampled at 10%, 15%, 25%, 50%, 75%, 100%, 125%, 150%, 175%, 200%, 300%, 400%, 500%, 600%, 700%, 800%, 1000%, and 2000%. The amount of majority class under-sampling and minority class oversampling depended on the dataset size and class proportions. For instance, consider the ROC curves in Figure 17 for the mammography dataset. There are three curves \u2014 one for plain majority class under-sampling in which the range of under-sampling is varied between 5% and 2000% at different intervals, one for a combination of SMOTE and majority class under-sampling, and one for Naive Bayes \u2014 and one ROC convex hull curve. The ROC curve shown in Figure 17 is for the minority class over-sampled at 400%. Each point on the SMOTE ROC curves represents a combination of (synthetic) over-sampling and undersampling, the amount of under-sampling follows the same range as for plain under-sampling. For a better understanding of the ROC graphs, we have shown different sets of ROC curves for one of our datasets in Appendix A.\nFor the Can dataset, we had to SMOTE to a lesser degree than for the other datasets due to the structural nature of the dataset. For the Can dataset there is a structural neighborhood already established in the mesh geometry, so SMOTE can lead to creating neighbors which are under the surface (and hence not interesting), since we are looking at the feature space of physics variables and not the structural information.\nThe ROC curves show a trend that as we increase the amount of under-sampling coupled with over-sampling, our minority classification accuracy increases, of course at the expense of more majority class errors. For almost all the ROC curves, the SMOTE approach dom-\ninates. Adhering to the definition of ROC convex hull, most of the potentially optimal classifiers are the ones generated with SMOTE."}, {"heading": "5.3 AUC Calculation", "text": "The Area Under the ROC curve (AUC) is calculated using a form of the trapezoid rule. The lower leftmost point for a given ROC curve is a classifier\u2019s performance on the raw data. The upper rightmost point is always (100%, 100%). If the curve does not naturally end at this point, the point is added. This is necessary in order for the AUC\u2019s to be compared over the same range of %FP.\nThe AUCs listed in Table 5.3 show that for all datasets the combined synthetic minority over-sampling and majority over-sampling is able to improve over plain majority under-sampling with C4.5 as the base classifier. Thus, our SMOTE approach provides an improvement in correct classification of data in the underrepresented class. The same conclusion holds from an examination of the ROC convex hulls. Some of the entries are missing in the table, as SMOTE was not applied at the same amounts to all datasets. The amount of SMOTE was less for less skewed datasets. Also, we have not included AUC\u2019s for Ripper/Naive Bayes. The ROC convex hull identifies SMOTE classifiers to be potentially optimal as compared to plain under-sampling or other treatments of misclassification costs, generally. Exceptions are as follows: for the Pima dataset, Naive Bayes dominates over SMOTE-C4.5; for the Oil dataset, Under-Ripper dominates over SMOTE-Ripper. For the Can dataset, SMOTE-classifier (classifier = C4.5 or Ripper) and Under-classifier ROC\ncurves overlap in the ROC space. For all the other datasets, SMOTE-classifier has more potentially optimal classifiers than any other approach."}, {"heading": "5.4 Additional comparison to changing the decision thresholds", "text": "Provost (2000) suggested that simply changing the decision threshold should always be considered as an alternative to more sophisticated approaches. In the case of C4.5, this would mean changing the decision threshold at the leaves of the decision trees. For example, a leaf could classify examples as the minority class even if more than 50% of the training examples at the leaf represent the majority class. We experimented by setting the decision thresholds at the leaves for the C4.5 decision tree learner at 0.5, 0.45, 0.42, 0.4, 0.35, 0.32, 0.3, 0.27, 0.25, 0.22, 0.2, 0.17, 0.15, 0.12, 0.1, 0.05, 0.0. We experimented on the Phoneme dataset. Figure 24 shows the comparison of the SMOTE and under-sampling combination against C4.5 learning by tuning the bias towards the minority class. The graph shows that the SMOTE and under-sampling combination ROC curve is dominating over the entire range of values."}, {"heading": "5.5 Additional comparison to one-sided selection and SHRINK", "text": "For the oil dataset, we also followed a slightly different line of experiments to obtain results comparable to (Kubat et al., 1998). To alleviate the problem of imbalanced datasets the authors have proposed (a) one-sided selection for under-sampling the majority class (Kubat & Matwin, 1997) and (b) the SHRINK system (Kubat et al., 1998). Table 5.5 contains the results from (Kubat et al., 1998). Acc+ is the accuracy on positive (minority) examples and Acc\u2212 is the accuracy on the negative (majority) examples. Figure 25 shows the trend for Acc+ and Acc\u2212 for one combination of the SMOTE strategy and varying degrees of undersampling of the majority class. The Y-axis represents the accuracy and the X-axis represents the percentage majority class under-sampled. The graphs indicate that in the band of under-sampling between 50% and 125% the results are comparable to those achieved by SHRINK and better than SHRINK in some cases. Table 5.5 summarizes the results for the SMOTE at 500% and under-sampling combination. We also tried combinations of SMOTE at 100-400% and varying degrees of under-sampling and achieved comparable results. The\nSHRINK approach and our SMOTE approach are not directly comparable, though, as they see different data points. SMOTE offers no clear improvement over one-sided selection."}, {"heading": "800% 90.7% 33.3%", "text": ""}, {"heading": "700% 96.0% 32.8%", "text": ""}, {"heading": "600% 98.0% 40.0%", "text": ""}, {"heading": "500% 98.0% 35.5%", "text": ""}, {"heading": "400% 95.5% 44.2%", "text": ""}, {"heading": "300% 89.0% 55.0%", "text": ""}, {"heading": "200% 81.7% 56.7%", "text": ""}, {"heading": "175% 85.0% 57.8%", "text": ""}, {"heading": "150% 83.3% 57.8%", "text": ""}, {"heading": "125% 84.2% 68.1%", "text": ""}, {"heading": "100% 78.3% 68.7%", "text": ""}, {"heading": "75% 83.7% 73.0%", "text": ""}, {"heading": "50% 89.5% 78.9%", "text": ""}, {"heading": "25% 64.0% 89.1%", "text": ""}, {"heading": "15% 62.8% 91.3%", "text": ""}, {"heading": "10% 64.7% 94.2%", "text": ""}, {"heading": "6. Future Work", "text": "There are several topics to be considered further in this line of research. Automated adaptive selection of the number of nearest neighbors would be valuable. Different strategies for creating the synthetic neighbors may be able to improve the performance. Also, selecting nearest neighbors with a focus on examples that are incorrectly classified may improve performance. A minority class sample could possibly have a majority class sample as its nearest neighbor rather than a minority class sample. This crowding will likely contribute to the redrawing of the decision surfaces in favor of the minority class. In addition to these topics, the following subsections discuss two possible extensions of SMOTE, and an application of SMOTE to information retrieval."}, {"heading": "6.1 SMOTE-NC", "text": "While our SMOTE approach currently does not handle data sets with all nominal features, it was generalized to handle mixed datasets of continuous and nominal features. We call this approach Synthetic Minority Over-sampling TEchnique-Nominal Continuous [SMOTE-NC]. We tested this approach on the Adult dataset from the UCI repository. The SMOTE-NC algorithm is described below.\n3. Populate the synthetic sample: The continuous features of the new synthetic minority class sample are created using the same approach of SMOTE as described earlier. The nominal feature is given the value occuring in the majority of the k-nearest neighbors.\nThe SMOTE-NC experiments reported here are set up the same as those with SMOTE, except for the fact that we examine one dataset only. SMOTE-NC with the Adult dataset differs from our typical result: it performs worse than plain under-sampling based on AUC, as shown in Figures 26 and 27. We extracted only continuous features to separate the effect of SMOTE and SMOTE-NC on this dataset, and to determine whether this oddity was due to our handling of nominal features. As shown in Figure 28, even SMOTE with only continuous features applied to the Adult dataset, does not achieve any better performance than plain under-sampling. Some of the minority class continuous features have a very high variance, so, the synthetic generation of minority class samples could be overlapping with the majority class space, thus leading to more false positives than plain under-sampling. This hypothesis is also supported by the decreased AUC measure as we SMOTE at degrees greater than 50%. The higher degrees of SMOTE lead to more minority class samples in the dataset, and thus a greater overlap with the majority class decision space."}, {"heading": "6.2 SMOTE-N", "text": "Potentially, SMOTE can also be extended for nominal features \u2014 SMOTE-N \u2014 with the nearest neighbors computed using the modified version of Value Difference Metric (Stanfill & Waltz, 1986) proposed by Cost and Salzberg (1993). The Value Difference Metric (VDM) looks at the overlap of feature values over all feature vectors. A matrix defining the distance\nbetween corresponding feature values for all feature vectors is created. The distance \u03b4 between two corresponding feature values is defined as follows.\n\u03b4(V1, V2) = n\u2211\ni=1\n| C1i C1 \u2212 C2i C2 | k\n(1)\nIn the above equation, V1 and V2 are the two corresponding feature values. C1 is the total number of occurrences of feature value V1, and C1i is the number of occurrences of feature value V1 for class i. A similar convention can also be applied to C2i and C2. k is a constant, usually set to 1. This equation is used to compute the matrix of value differences for each nominal feature in the given set of feature vectors. Equation 1 gives a geometric distance on a fixed, finite set of values (Cost & Salzberg, 1993). Cost and Salzberg\u2019s modified VDM omits the weight term waf included in the \u03b4 computation by Stanfill and Waltz, which has an effect of making \u03b4 symmetric. The distance \u2206 between two feature vectors is given by:\n\u2206(X,Y ) = wxwy\nN\u2211\ni=1\n\u03b4(xi, yi) r (2)\nr = 1 yields the Manhattan distance, and r = 2 yields the Euclidean distance (Cost & Salzberg, 1993). wx and wy are the exemplar weights in the modified VDM. wy = 1 for a new example (feature vector), and wx is the bias towards more reliable examples (feature vectors) and is computed as the ratio of the number of uses of a feature vector to the number of correct uses of the feature vector; thus, more accurate feature vectors will have wx \u2248 1. For SMOTE-N we can ignore these weights in equation 2, as SMOTE-N is not used for classification purposes directly. However, we can redefine these weights to give more weight to the minority class feature vectors falling closer to the majority class feature vectors; thus, making those minority class features appear further away from the feature vector under consideration. Since, we are more interested in forming broader but accurate regions of the minority class, the weights might be used to avoid populating along neighbors which fall closer to the majority class. To generate new minority class feature vectors, we can create new set feature values by taking the majority vote of the feature vector in consideration and its k nearest neighbors. Table 6.2 shows an example of creating a synthetic feature vector."}, {"heading": "6.3 Application of SMOTE to Information Retrieval", "text": "We are investigating the application of SMOTE to information retrieval (IR). The IR problems come with a plethora of features and potentially many categories. SMOTE would have to be applied in conjunction with a feature selection algorithm, after transforming the given document or web page in a bag-of-words format.\nAn interesting comparison to SMOTE would be the combination of Naive Bayes and Odds ratio. Odds ratio focuses on a target class, and ranks documents according to their relevance to the target or positive class. SMOTE also focuses on a target class by creating more examples of that class."}, {"heading": "7. Summary", "text": "The results show that the SMOTE approach can improve the accuracy of classifiers for a minority class. SMOTE provides a new approach to over-sampling. The combination of SMOTE and under-sampling performs better than plain under-sampling. SMOTE was tested on a variety of datasets, with varying degrees of imbalance and varying amounts of data in the training set, thus providing a diverse testbed. The combination of SMOTE and under-sampling also performs better, based on domination in the ROC space, than varying loss ratios in Ripper or by varying the class priors in Naive Bayes Classifier: the methods that could directly handle the skewed class distribution. SMOTE forces focused learning and introduces a bias towards the minority class. Only for Pima \u2014 the least skewed dataset \u2014 does the Naive Bayes Classifier perform better than SMOTE-C4.5. Also, only for the Oil dataset does the Under-Ripper perform better than SMOTE-Ripper. For the Can dataset, SMOTE-classifier and Under-classifier ROC curves overlap in the ROC space. For all the rest of the datasets SMOTE-classifier performs better than Under-classifier, Loss Ratio, and Naive Bayes. Out of a total of 48 experiments performed, SMOTE-classifier does not perform the best only for 4 experiments.\nThe interpretation of why synthetic minority over-sampling improves performance where as minority over-sampling with replacement does not is fairly straightforward. Consider the effect on the decision regions in feature space when minority over-sampling is done by replication (sampling with replacement) versus the introduction of synthetic examples. With replication, the decision region that results in a classification decision for the minority class can actually become smaller and more specific as the minority samples in the region are replicated. This is the opposite of the desired effect. Our method of synthetic over-sampling works to cause the classifier to build larger decision regions that contain nearby minority class points. The same reasons may be applicable to why SMOTE performs better than Ripper\u2019s loss ratio and Naive Bayes; these methods, nonetheless, are still learning from the information provided in the dataset, albeit with different cost information. SMOTE provides more related minority class samples to learn from, thus allowing a learner to carve broader decision regions, leading to more coverage of the minority class."}, {"heading": "Acknowledgments", "text": "This research was partially supported by the United States Department of Energy through the Sandia National Laboratories ASCI VIEWS Data Discovery Program, contract number\nDE-AC04-76DO00789. We thank Robert Holte for providing the oil spill dataset used in their paper. We also thank Foster Provost for clarifying his method of using the Satimage dataset. We would also like to thank the anonymous reviewers for their various insightful comments and suggestions."}, {"heading": "Appendix A. ROC graphs for Oil Dataset", "text": "The following figures show different sets of ROC curves for the oil dataset. Figure 29 (a) shows the ROC curves for the Oil dataset, as included in the main text; Figure 29(b) shows the ROC curves without the ROC convex hull; Figure 29(c) shows the two convex hulls, obtained with and without SMOTE. The ROC convex hull shown by dashed lines and stars in Figure 29(c), was computed by including Under-C4.5 and Naive Bayes in the family of ROC curves. The ROC convex hull shown by solid line and small circles in Figure 29(c) was computed by including 500 SMOTE-C4.5, Under-C4.5, and Naive Bayes in the family of ROC curves. The ROC convex hull with SMOTE dominates the ROC convex hull without SMOTE, hence SMOTE-C4.5 contributes more optimal classifiers."}], "references": [{"title": "UCI Repository of Machine Learning Databases http://www.ics.uci.edu/\u223cmlearn/\u223cMLRepository.html", "author": ["C. Blake", "C. Merz"], "venue": "Department of Information and Computer Sciences,", "citeRegEx": "Blake and Merz,? \\Q1998\\E", "shortCiteRegEx": "Blake and Merz", "year": 1998}, {"title": "The Use of the Area Under the ROC Curve in the Evaluation of Machine Learning Algorithms", "author": ["A.P. Bradley"], "venue": "Pattern Recognition, 30(6), 1145\u20131159.", "citeRegEx": "Bradley,? 1997", "shortCiteRegEx": "Bradley", "year": 1997}, {"title": "SMOTE: Synthetic Minority Over-sampling TEchnique", "author": ["N. Chawla", "K. Bowyer", "L. Hall", "P. Kegelmeyer"], "venue": "In International Conference of Knowledge Based Computer Systems,", "citeRegEx": "Chawla et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chawla et al\\.", "year": 2000}, {"title": "Modifying MUSTAFA to capture salient data", "author": ["N. Chawla", "L. Hall"], "venue": "Tech. rep. ISL-99-01,", "citeRegEx": "Chawla and Hall,? \\Q1999\\E", "shortCiteRegEx": "Chawla and Hall", "year": 1999}, {"title": "Learning to Classify English Text with ILP Methods", "author": ["W. Cohen"], "venue": "Proceedings of the 5th International Workshop on Inductive Logic Programming, pp. 3\u201324. Department of Computer Science, Katholieke Universiteit Leuven.", "citeRegEx": "Cohen,? 1995a", "shortCiteRegEx": "Cohen", "year": 1995}, {"title": "Fast Effective Rule Induction", "author": ["W.W. Cohen"], "venue": "Proc. 12th International Conference on Machine Learning, pp. 115\u2013123 Lake Tahoe, CA. Morgan Kaufmann.", "citeRegEx": "Cohen,? 1995b", "shortCiteRegEx": "Cohen", "year": 1995}, {"title": "Context-sensitive Learning Methods for Text Categorization", "author": ["W.W. Cohen", "Y. Singer"], "venue": "Proceedings of SIGIR-96,", "citeRegEx": "Cohen and Singer,? \\Q1996\\E", "shortCiteRegEx": "Cohen and Singer", "year": 1996}, {"title": "A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features", "author": ["S. Cost", "S. Salzberg"], "venue": "Machine Learning,", "citeRegEx": "Cost and Salzberg,? \\Q1993\\E", "shortCiteRegEx": "Cost and Salzberg", "year": 1993}, {"title": "Neural Network Training on Unequally Represented Classes", "author": ["E. DeRouin", "J. Brown", "L. Fausett", "M. Schneider"], "venue": "In Intellligent Engineering Systems Through Artificial Neural Networks,", "citeRegEx": "DeRouin et al\\.,? \\Q1991\\E", "shortCiteRegEx": "DeRouin et al\\.", "year": 1991}, {"title": "Metacost: A General Method for Making Classifiers Cost-sensitive", "author": ["P. Domingos"], "venue": "Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 155\u2013164 San Diego, CA. ACM Press.", "citeRegEx": "Domingos,? 1999", "shortCiteRegEx": "Domingos", "year": 1999}, {"title": "Explicitly Representing Expected Cost: An Alternative to ROC Representation", "author": ["C. Drummond", "R. Holte"], "venue": "In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Drummond and Holte,? \\Q2000\\E", "shortCiteRegEx": "Drummond and Holte", "year": 2000}, {"title": "Inductive Learning Algorithms and Representations for Text Categorization", "author": ["S. Dumais", "J. Platt", "D. Heckerman", "M. Sahami"], "venue": "In Proceedings of the Seventh International Conference on Information and Knowledge Management.,", "citeRegEx": "Dumais et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Dumais et al\\.", "year": 1998}, {"title": "Learning Goal Oriented Bayesian Networks for Telecommunications Risk Management", "author": ["K. Ezawa", "M. Singh", "S. Norton"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Ezawa et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Ezawa et al\\.", "year": 1996}, {"title": "Combining Data Mining and Machine Learning for Effective User Profile", "author": ["T. Fawcett", "F. Provost"], "venue": "In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Fawcett and Provost,? \\Q1996\\E", "shortCiteRegEx": "Fawcett and Provost", "year": 1996}, {"title": "Off-line, Handwritten Numeral Recognition by Perturbation Method", "author": ["T.M. Ha", "H. Bunke"], "venue": "Pattern Analysis and Machine Intelligence,", "citeRegEx": "Ha and Bunke,? \\Q1997\\E", "shortCiteRegEx": "Ha and Bunke", "year": 1997}, {"title": "The Electrotopological State: Structure Information at the Atomic Level for Molecular Graphs", "author": ["L. Hall", "B. Mohney", "L. Kier"], "venue": "Journal of Chemical Information and Computer Science,", "citeRegEx": "Hall et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Hall et al\\.", "year": 1991}, {"title": "The Class Imbalance Problem: Significance and Strategies", "author": ["N. Japkowicz"], "venue": "Proceedings of the 2000 International Conference on Artificial Intelligence (IC-AI\u20192000): Special Track on Inductive Learning Las Vegas, Nevada.", "citeRegEx": "Japkowicz,? 2000", "shortCiteRegEx": "Japkowicz", "year": 2000}, {"title": "Machine Learning for the Detection of Oil Spills in Satellite Radar Images", "author": ["M. Kubat", "R. Holte", "S. Matwin"], "venue": "Machine Learning,", "citeRegEx": "Kubat et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kubat et al\\.", "year": 1998}, {"title": "Addressing the Curse of Imbalanced Training Sets: One Sided Selection", "author": ["M. Kubat", "S. Matwin"], "venue": "In Proceedings of the Fourteenth International Conference on Machine Learning,", "citeRegEx": "Kubat and Matwin,? \\Q1997\\E", "shortCiteRegEx": "Kubat and Matwin", "year": 1997}, {"title": "Noisy Replication in Skewed Binary Classification", "author": ["S. Lee"], "venue": "Computational Statistics and Data Analysis, 34.", "citeRegEx": "Lee,? 2000", "shortCiteRegEx": "Lee", "year": 2000}, {"title": "Heterogeneous Uncertainity Sampling for Supervised Learning", "author": ["D. Lewis", "J. Catlett"], "venue": "In Proceedings of the Eleventh International Conference of Machine Learning,", "citeRegEx": "Lewis and Catlett,? \\Q1994\\E", "shortCiteRegEx": "Lewis and Catlett", "year": 1994}, {"title": "A Comparison of Two Learning Algorithms for Text Categorization", "author": ["D. Lewis", "M. Ringuette"], "venue": "In Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval,", "citeRegEx": "Lewis and Ringuette,? \\Q1994\\E", "shortCiteRegEx": "Lewis and Ringuette", "year": 1994}, {"title": "Data Mining for Direct Marketing Problems and Solutions", "author": ["C. Ling", "C. Li"], "venue": "In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining", "citeRegEx": "Ling and Li,? \\Q1998\\E", "shortCiteRegEx": "Ling and Li", "year": 1998}, {"title": "Feature Selection for Unbalanced Class Distribution and Naive Bayes", "author": ["D. Mladeni\u0107", "M. Grobelnik"], "venue": "In Proceedings of the 16th International Conference on Machine Learning.,", "citeRegEx": "Mladeni\u0107 and Grobelnik,? \\Q1999\\E", "shortCiteRegEx": "Mladeni\u0107 and Grobelnik", "year": 1999}, {"title": "Computational Geometry in C", "author": ["J. O\u2019Rourke"], "venue": null, "citeRegEx": "O.Rourke,? \\Q1998\\E", "shortCiteRegEx": "O.Rourke", "year": 1998}, {"title": "Reducing Misclassification Costs", "author": ["M. Pazzani", "C. Merz", "P. Murphy", "K. Ali", "T. Hume", "C. Brunk"], "venue": "In Proceedings of the Eleventh International Conference on Machine Learning", "citeRegEx": "Pazzani et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Pazzani et al\\.", "year": 1994}, {"title": "Robust Classification for Imprecise Environments", "author": ["F. Provost", "T. Fawcett"], "venue": "Machine Learning,", "citeRegEx": "Provost and Fawcett,? \\Q2001\\E", "shortCiteRegEx": "Provost and Fawcett", "year": 2001}, {"title": "The Case Against Accuracy Estimation for Comparing Induction Algorithms", "author": ["F. Provost", "T. Fawcett", "R. Kohavi"], "venue": "In Proceedings of the Fifteenth International Conference on Machine Learning,", "citeRegEx": "Provost et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Provost et al\\.", "year": 1998}, {"title": "A Large-Scale Evaluation of Features for Automatic Detection of Oil Spills in ERS SAR Images", "author": ["A. Solberg", "R. Solberg"], "venue": "In International Geoscience and Remote Sensing Symposium,", "citeRegEx": "Solberg and Solberg,? \\Q1996\\E", "shortCiteRegEx": "Solberg and Solberg", "year": 1996}, {"title": "Toward Memory-based Reasoning", "author": ["C. Stanfill", "D. Waltz"], "venue": "Communications of the ACM,", "citeRegEx": "Stanfill and Waltz,? \\Q1986\\E", "shortCiteRegEx": "Stanfill and Waltz", "year": 1986}, {"title": "Measuring the Accuracy of Diagnostic Systems", "author": ["J. Swets"], "venue": "Science, 240, 1285\u20131293.", "citeRegEx": "Swets,? 1988", "shortCiteRegEx": "Swets", "year": 1988}, {"title": "Two Modifications of CNN", "author": ["I. Tomek"], "venue": "IEEE Transactions on Systems, Man and Cybernetics, 6, 769\u2013772.", "citeRegEx": "Tomek,? 1976", "shortCiteRegEx": "Tomek", "year": 1976}, {"title": "Cost Sensitive Bibliography", "author": ["P. Turney"], "venue": "http://ai.iit.nrc.ca/bibiliographies/costsensitive.html.", "citeRegEx": "Turney,? 1996", "shortCiteRegEx": "Turney", "year": 1996}, {"title": "The Selection of Good Search Terms", "author": ["C. van Rijsbergen", "D. Harper", "M. Porter"], "venue": "Information Processing and Management,", "citeRegEx": "Rijsbergen et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Rijsbergen et al\\.", "year": 1981}, {"title": "Comparative Evaluation of Pattern Recognition Techniques for Detection of Microcalcifications in Mammography", "author": ["K. Woods", "C. Doss", "K. Bowyer", "J. Solka", "C. Priebe", "P. Kegelmeyer"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Woods et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Woods et al\\.", "year": 1993}], "referenceMentions": [{"referenceID": 4, "context": "There have been attempts to deal with imbalanced datasets in domains such as fraudulent telephone calls (Fawcett & Provost, 1996), telecommunications management (Ezawa, Singh, & Norton, 1996), text classification (Lewis & Catlett, 1994; Dumais, Platt, Heckerman, & Sahami, 1998; Mladeni\u0107 & Grobelnik, 1999; Lewis & Ringuette, 1994; Cohen, 1995a) and detection of oil spills in satellite images (Kubat, Holte, & Matwin, 1998).", "startOffset": 213, "endOffset": 345}, {"referenceID": 30, "context": "The Receiver Operating Characteristic (ROC) curve is a standard technique for summarizing classifier performance over a range of tradeoffs between true positive and false positive error rates (Swets, 1988).", "startOffset": 192, "endOffset": 205}, {"referenceID": 1, "context": "The Area Under the Curve (AUC) is an accepted traditional performance metric for a ROC curve (Duda, Hart, & Stork, 2001; Bradley, 1997; Lee, 2000).", "startOffset": 93, "endOffset": 146}, {"referenceID": 19, "context": "The Area Under the Curve (AUC) is an accepted traditional performance metric for a ROC curve (Duda, Hart, & Stork, 2001; Bradley, 1997; Lee, 2000).", "startOffset": 93, "endOffset": 146}, {"referenceID": 9, "context": "One is to assign distinct costs to training examples (Pazzani, Merz, Murphy, Ali, Hume, & Brunk, 1994; Domingos, 1999).", "startOffset": 53, "endOffset": 118}, {"referenceID": 16, "context": "The other is to re-sample the original dataset, either by oversampling the minority class and/or under-sampling the majority class (Kubat & Matwin, 1997; Japkowicz, 2000; Lewis & Catlett, 1994; Ling & Li, 1998).", "startOffset": 131, "endOffset": 210}, {"referenceID": 5, "context": "5 decision tree classifier (Quinlan, 1992), Ripper (Cohen, 1995b), and a Naive Bayes Classifier show that our approach improves over other previous re-sampling, modifying loss ratio, and class priors approaches, using either the AUC or ROC convex hull.", "startOffset": 51, "endOffset": 65}, {"referenceID": 1, "context": "In the presence of imbalanced datasets with unequal error costs, it is more appropriate to use the ROC curve or other similar techniques (Ling & Li, 1998; Drummond & Holte, 2000; Provost & Fawcett, 2001; Bradley, 1997; Turney, 1996).", "startOffset": 137, "endOffset": 232}, {"referenceID": 32, "context": "In the presence of imbalanced datasets with unequal error costs, it is more appropriate to use the ROC curve or other similar techniques (Ling & Li, 1998; Drummond & Holte, 2000; Provost & Fawcett, 2001; Bradley, 1997; Turney, 1996).", "startOffset": 137, "endOffset": 232}, {"referenceID": 19, "context": "If the ROC curves are intersecting, the total AUC is an average comparison between models (Lee, 2000).", "startOffset": 90, "endOffset": 101}, {"referenceID": 31, "context": "The borderline examples were detected using the Tomek links concept (Tomek, 1976).", "startOffset": 68, "endOffset": 81}, {"referenceID": 18, "context": "Previous Work: Imbalanced datasets Kubat and Matwin (1997) selectively under-sampled the majority class while keeping the original population of the minority class.", "startOffset": 35, "endOffset": 59}, {"referenceID": 17, "context": "related work proposed the SHRINK system that classifies an overlapping region of minority (positive) and majority (negative) classes as positive; it searches for the \u201cbest positive region\u201d (Kubat et al., 1998).", "startOffset": 189, "endOffset": 209}, {"referenceID": 16, "context": "She noted that both the sampling approaches were effective, and she also observed that using the sophisticated sampling techniques did not give any clear advantage in the domain considered (Japkowicz, 2000).", "startOffset": 189, "endOffset": 206}, {"referenceID": 16, "context": "Japkowicz (2000) discussed the effect of imbalance in a dataset.", "startOffset": 0, "endOffset": 17}, {"referenceID": 16, "context": "Japkowicz (2000) discussed the effect of imbalance in a dataset. She evaluated three strategies: under-sampling, resampling and a recognition-based induction scheme. We focus on her sampling approaches. She experimented on artificial 1D data in order to easily measure and construct concept complexity. Two resampling methods were considered. Random resampling consisted of resampling the smaller class at random until it consisted of as many samples as the majority class and \u201cfocused resampling\u201d consisted of resampling only those minority examples that occurred on the boundary between the minority and majority classes. Random under-sampling was considered, which involved under-sampling the majority class samples at random until their numbers matched the number of minority class samples; focused under-sampling involved under-sampling the majority class samples lying further away. She noted that both the sampling approaches were effective, and she also observed that using the sophisticated sampling techniques did not give any clear advantage in the domain considered (Japkowicz, 2000). One approach that is particularly relevant to our work is that of Ling and Li (1998). They combined over-sampling of the minority class with under-sampling of the majority class.", "startOffset": 0, "endOffset": 1182}, {"referenceID": 9, "context": "The relabeling of the examples expands the decision space as it creates new samples from which the classifier may learn (Domingos, 1999).", "startOffset": 120, "endOffset": 136}, {"referenceID": 11, "context": "The information retrieval (IR) domain (Dumais et al., 1998; Mladeni\u0107 & Grobelnik, 1999; Lewis & Ringuette, 1994; Cohen, 1995a) also faces the problem of class imbalance in the dataset.", "startOffset": 38, "endOffset": 126}, {"referenceID": 4, "context": "The information retrieval (IR) domain (Dumais et al., 1998; Mladeni\u0107 & Grobelnik, 1999; Lewis & Ringuette, 1994; Cohen, 1995a) also faces the problem of class imbalance in the dataset.", "startOffset": 38, "endOffset": 126}, {"referenceID": 23, "context": "Solberg and Solberg (1996) considered the problem of imbalanced data sets in oil slick classification from SAR imagery.", "startOffset": 0, "endOffset": 27}, {"referenceID": 7, "context": "Another approach that is similar to our work is that of Domingos (1999). He compares the \u201cmetacost\u201d approach to each of majority under-sampling and minority over-sampling.", "startOffset": 56, "endOffset": 72}, {"referenceID": 7, "context": "Another approach that is similar to our work is that of Domingos (1999). He compares the \u201cmetacost\u201d approach to each of majority under-sampling and minority over-sampling. He finds that metacost improves over either, and that under-sampling is preferable to minority over-sampling. Error-based classifiers are made cost-sensitive. The probability of each class for each example is estimated, and the examples are relabeled optimally with respect to the misclassification costs. The relabeling of the examples expands the decision space as it creates new samples from which the classifier may learn (Domingos, 1999). A feed-forward neural network trained on an imbalanced dataset may not learn to discriminate enough between classes (DeRouin, Brown, Fausett, & Schneider, 1991). The authors proposed that the learning rate of the neural network be adapted to the statistics of class representation in the data. They calculated an attention factor from the proportion of samples presented to the neural network for training. The learning rate of the network elements was adjusted based on the attention factor. They experimented on an artificially generated training set and on a real-world training set, both with multiple (more than two) classes. They compared this to the approach of replicating the minority class samples to balance the data set used for training. The classification accuracy on the minority class was improved. Lewis and Catlett (1994) examined heterogeneous uncertainty sampling for supervised learning.", "startOffset": 56, "endOffset": 1456}, {"referenceID": 16, "context": "1 Minority over-sampling with replacement Previous research (Ling & Li, 1998; Japkowicz, 2000) has discussed over-sampling with replacement and has noted that it doesn\u2019t significantly improve minority class recognition.", "startOffset": 60, "endOffset": 94}, {"referenceID": 34, "context": "The data for the plot in Figure 3 was extracted from a Mammography dataset1 (Woods et al., 1993).", "startOffset": 76, "endOffset": 96}, {"referenceID": 5, "context": "Ripper: We compared various combinations of SMOTE and under-sampling with plain under-sampling using Ripper (Cohen, 1995b) as the base classifier.", "startOffset": 108, "endOffset": 122}, {"referenceID": 27, "context": "We chose the smallest class as the minority class and collapsed the rest of the classes into one as was done in (Provost et al., 1998).", "startOffset": 112, "endOffset": 134}, {"referenceID": 16, "context": "Most other approaches only work for only two classes (Ling & Li, 1998; Japkowicz, 2000; Kubat & Matwin, 1997; Provost & Fawcett, 2001).", "startOffset": 53, "endOffset": 134}, {"referenceID": 17, "context": "The Oil dataset was provided by Robert Holte and is used in their paper (Kubat et al., 1998).", "startOffset": 72, "endOffset": 92}, {"referenceID": 34, "context": "The Mammography dataset (Woods et al., 1993) has 11,183 samples with 260 calcifications.", "startOffset": 24, "endOffset": 44}, {"referenceID": 16, "context": "The ROC curve for plain under-sampling of the majority class (Ling & Li, 1998; Japkowicz, 2000; Kubat & Matwin, 1997; Provost & Fawcett, 2001) is compared with our approach of combining synthetic minority class over-sampling (SMOTE) with majority class under-sampling.", "startOffset": 61, "endOffset": 142}, {"referenceID": 24, "context": "The ROC convex hull is generated using the Graham\u2019s algorithm (O\u2019Rourke, 1998).", "startOffset": 62, "endOffset": 78}, {"referenceID": 17, "context": "For the oil dataset, we also followed a slightly different line of experiments to obtain results comparable to (Kubat et al., 1998).", "startOffset": 111, "endOffset": 131}, {"referenceID": 17, "context": "To alleviate the problem of imbalanced datasets the authors have proposed (a) one-sided selection for under-sampling the majority class (Kubat & Matwin, 1997) and (b) the SHRINK system (Kubat et al., 1998).", "startOffset": 185, "endOffset": 205}, {"referenceID": 17, "context": "5 contains the results from (Kubat et al., 1998).", "startOffset": 28, "endOffset": 48}, {"referenceID": 17, "context": "Table 4: Cross-validation results (Kubat et al., 1998)", "startOffset": 34, "endOffset": 54}, {"referenceID": 7, "context": "2 SMOTE-N Potentially, SMOTE can also be extended for nominal features \u2014 SMOTE-N \u2014 with the nearest neighbors computed using the modified version of Value Difference Metric (Stanfill & Waltz, 1986) proposed by Cost and Salzberg (1993). The Value Difference Metric (VDM) looks at the overlap of feature values over all feature vectors.", "startOffset": 210, "endOffset": 235}], "year": 2011, "abstractText": "An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of \u201cnormal\u201d examples with only a small percentage of \u201cabnormal\u201d or \u201cinteresting\u201d examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.", "creator": "dvips(k) 5.86 Copyright 1999 Radical Eye Software"}}}