{"id": "1106.4251", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2011", "title": "Learning with the weighted trace-norm under arbitrary sampling distributions", "abstract": "We provide rigorous guarantees on learning with the weighted trace-norm under arbitrary sampling distributions. We show that the standard weighted trace-norm might fail when the sampling distribution is not a product distribution (i.e. when row and column indexes are not selected independently), present a corrected variant for which we establish strong learning guarantees, and demonstrate that it works better in practice. We provide guarantees when weighting by either the true or empirical sampling distribution, and suggest that even if the true distribution is known (or is uniform), weighting by the empirical distribution may be beneficial.", "histories": [["v1", "Tue, 21 Jun 2011 16:16:24 GMT  (222kb,D)", "http://arxiv.org/abs/1106.4251v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["rina foygel", "ruslan salakhutdinov", "ohad shamir", "nati srebro"], "accepted": true, "id": "1106.4251"}, "pdf": {"name": "1106.4251.pdf", "metadata": {"source": "CRF", "title": "Learning with the Weighted Trace-norm under Arbitrary Sampling Distributions", "authors": ["Rina Foygel"], "emails": ["rina@uchicago.edu", "rsalakhu@mit.edu", "ohadsh@microsoft.com", "nati@ttic.edu"], "sections": [{"heading": null, "text": "We provide rigorous guarantees on learning with the weighted trace-norm under arbitrary sampling distributions. We show that the standard weighted trace-norm might fail when the sampling distribution is not a product distribution (i.e. when row and column indexes are not selected independently), present a corrected variant for which we establish strong learning guarantees, and demonstrate that it works better in practice. We provide guarantees when weighting by either the true or empirical sampling distribution, and suggest that even if the true distribution is known (or is uniform), weighting by the empirical distribution may be beneficial.\n1 Introduction\nOne of the most common approaches to collaborative filtering and matrix completion is trace-norm regularization [1, 2, 3, 4]. In this approach we attempt to complete an unknown matrix, based on a small subset of revealed entries, by finding a matrix with small trace-norm, which matches those entries as best as possible.\nThis approach has repeatedly shown good performance in practice, and is theoretically well understood for the case where revealed entries are sampled uniformly [5, 6, 7, 8, 9, 10]. Under such uniform sampling, \u0398(n log(n)) entries are sufficient for good completion of an n \u00d7 n matrix\u2014i.e. a nearly constant number of entries per row. However, for arbitrary sampling distributions, the worst-case sample complexity lies between a lower bound of \u2126(n4/3) [11] and an upper bound of O(n3/2) [12], i.e. requiring between n1/3 and n1/2 observations per row, and indicating it is not appropriate for matrix completion in this setting.\nMotivated by these issues, Salakhutdinov and Srebro [11] proposed to use a weighted variant of the trace-norm, which takes the distribution of the entries into account, and showed experimentally that this variant indeed leads to superior performance. However, although this recent paper established that the weighted trace-norm corrects a specific situation where the standard trace-norm fails, no general learning guarantees are provided, and it is not clear if indeed the weighted trace-norm always leads to the desired behavior. The only theoretical analysis of the weighted trace-norm that we are aware of is a recent report by Negahban and Wainwright [9] that provides reconstruction guarantees for a low-rank matrix with i.i.d. noise, but only when the sampling distribution is a product distribution, i.e. the rows index and column index of observed entries are selected independently. A product distribution assumption does not seem realistic in many cases\u2014e.g. for the Netflix data, it would indicate that all users have the same (conditional) distribution over which movies they rate.\nar X\niv :1\n10 6.\n42 51\nv1 [\ncs .L\nG ]\n2 1\nIn this paper we rigorously study learning with a weighted trace-norm under an arbitrary sampling distribution, and show that this situation is indeed more complicated, requiring a correction to the weighting. We show that this correction is necessary, and present empirical results on the Netflix and MovieLens dataset indicating that it is also helpful in practice. We also rigorously consider weighting according to either the true sampling distribution (as in [9]) or the empirical frequencies, as is actually done in practice, and present evidence that weighting by the empirical frequencies might be advantageous. Our setting is also more general then that of [9]\u2014we consider an arbitrary loss and do not rely in i.i.d. noise, instead presenting results in an agnostic learning framework.\nSetup and Notation. We consider an arbitrary unknown n \u00d7m target matrix Y , where a subset of entries {Yit,jt}st=1 indexed by S = {(i1, j1), . . . , (is, js)} is revealed to us. Without loss of generality, we assume n \u2265 m. Throughout most of the paper, we assume S is drawn i.i.d. according to some sampling distribution p(i, j) (with replacement). Based on this subset on entries, we would like to fill in the missing entries and obtain a prediction matrix X\u0302S \u2208 Rn\u00d7m, with low expected loss Lp(X\u0302S) = Eij\u223cp [ `((X\u0302S)ij , Yij) ] , where `(x, y) is some loss function. Note that we measure the loss with respect to\nthe same distribution p(i, j) from which the training set is drawn (this is also the case in [11, 9, 12]). Given some distribution p(i, j) on [n]\u00d7 [m], the weighted trace-norm of a matrix X \u2208 Rn\u00d7m is given by [11]\n\u2016X\u2016tr(pr,pc) = \u2225\u2225\u2225diag (pr)1/2 \u00b7X \u00b7 diag (pc)1/2\u2225\u2225\u2225\ntr ,\nwhere pr \u2208 Rn and pc \u2208 Rm denote vectors of the row- and column-marginals respectively. Note that the weighted trace-norm only depends on these marginals (but not their joint distribution) and that if pr and pc are uniform, then \u2016X\u2016tr(pr,pc) = 1\u221a nm \u2016X\u2016tr. The weighted trace-norm does not generally scale with n and m, and in particular, if X has rank r and entries bounded in [\u22121, 1], then \u2016X\u2016tr(pr,pc) \u2264 \u221a r regardless of which p (i, j) is used. This motivates us to define the class\nWr [p] = {X \u2208 Rn\u00d7m : \u2016X\u2016tr(pr,pc) \u2264 \u221a r},\nalthough we emphasize that our results do not directly depend on the rank, andWr [p] certainly includes full-rank matrices. We analyze here estimators of the form X\u0302S = arg min{L\u0302S(X) : X \u2208 Wr [p]} where L\u0302S(X) = 1 s \u2211s t=1 `(Xit,jt , Yit,jt) is the empirical error on the observed entries.\nAlthough we focus mostly on the standard inductive setting, where the samples are drawn i.i.d. and the guarantee is on generalization for future samples drawn by the same distribution, our results can also be stated in a transductive model, where a training set and a test set are created by splitting a fixed subset of entries uniformly at random (as in [12]). The transductive setting is discussed in Section 4.2, and variants of our Theorems in this setting are found there and in Appendix B.\n2 Learning with the Standard Weighting\nIn this Section, we consider learning using the weighted trace-norm as suggested by Salakhutdinov and Srebro [11], i.e. when the weighting is according to the sampling distribution p(i, j). Following the approach of [5] and [10], we base our results on bounding the Rademacher complexity of Wr [p], as a class of functions mapping index pairs to entry values. However, we modify the analysis for the weighted trace-norm with non-uniform sampling.\nFor a class of matrices X and a sample S = {(i1, j1), . . . , (is, js)} of indexes in [n]\u00d7 [m], the empirical Rademacher complexity of the class (with respect to S) is given by\nR\u0302S(X ) = E\u03c3\u223c{\u00b11}s [\nsup X\u2208X\n1\ns s\u2211 t=1 \u03c3tXitjt\n] ,\nwhere \u03c3 is a vector of signs drawn uniformly at random. Intuitively, R\u0302S(X ) measures the extent to which the class X can \u201coverfit\u201d data, by finding a matrix X which correlates as strongly as possible to a sample from a matrix of random noise. For a loss `(x, y) that is Lipschitz in x, the Rademacher complexity can be used to uniformly bound the deviations |Lp(X)\u2212 L\u0302S(X)| for all X \u2208 X , yielding a learning guarantee on the empirical risk minimizer [13].\n2.1 Guarantees for Special Sampling Distributions\nWe begin by providing guarantees for an arbitrary, possibly unbounded, Lipschitz loss `(x, y), but only under sampling distributions which are either product distributions (i.e. p(i, j) = pr(i)pc(j)) or have uniform marginals (i.e. pr and pc are uniform, but perhaps the rows and columns are not independent). In Section 2.3 below, we will see why this severe restriction on p is needed.\nTheorem 1. For an l-Lipschitz loss `, fix any matrix Y , sample size s, and distribution p, such that p is either a product distribution or has uniform marginals. Let X\u0302S = arg min { L\u0302S(X) : X \u2208 Wr [p] } . Then, in expectation over the training sample S drawn\ni.i.d. from the distribution p,\nLp(X\u0302S) \u2264 inf X\u2208Wr[p] Lp(X) + O\n( l \u00b7 \u221a rn log(n)\ns\n) . (1)\nHere and elsewhere we state learning guarantees in expectation for simplicity, but all guarantees can also be obtained with high probability.\nProof. We will show how to bound the expected Rademacher complexity ES [ R\u0302S(Wr [p]) ] , from which\nthe desired results follows using standard arguments [13]. Following [10] by including the weights, using the duality between spectral norm \u2016\u00b7\u2016sp and trace-norm, we compute:\nES [ R\u0302S(Wr [p]) ] = \u221a r\ns ES,\u03c3 \u2225\u2225\u2225\u2225\u2225 s\u2211 t=1 \u03c3t eit,jt\u221a pr (it) pc (jt) \u2225\u2225\u2225\u2225\u2225 sp  = \u221ar s ES,\u03c3 \u2225\u2225\u2225\u2225\u2225 s\u2211 t=1 Qt \u2225\u2225\u2225\u2225\u2225 sp  , where ei,j = eie T j and Qt = \u03c3t\neit,jt\u221a pr(it)pc(jt) \u2208 Rn\u00d7m. Since the Qt\u2019s are i.i.d. zero-mean matrices, Theorem 6.1 of [14], combined with Remarks 6.4 and 6.5 there, establishes that\nES,\u03c3 \u2225\u2225\u2225\u2225\u2225 s\u2211 t=1 Qt \u2225\u2225\u2225\u2225\u2225 sp  = O(\u03c3\u221alog(n) +R log(n)) , where \u2016Qt\u2016sp \u2264 R (almost surely) and \u03c32 = max\n{\u2225\u2225\u2211E [QTt Qt]\u2225\u2225sp ,\u2225\u2225\u2211E [QtQTt ]\u2225\u2225sp}. Calculating these (see Appendix A ), we get R \u2264 \u221a nm\nmini,j{npr(i)\u00b7mpc(j)} , and\n\u03c3 \u2264 \u221a\u221a\u221a\u221a\u221asmax maxi \u2211\nj\np (i, j)\npr (i) pc (j) ,max j \u2211 i p (i, j) pr (i) pc (j)  \u2264 \u221a\nsn\nmini,j{npr (i) \u00b7mpc (j)} .\nIf p has uniform row- and column-marginals, then for all i, j, npr (i) = mpc (j) = 1. This yields\nES [ R\u0302(Wr [p]) ] \u2264 O\n(\u221a rn log(n)\ns\n) ,\nas desired. (Here we assume s > n log(n), since otherwise we need only establish that excess error is O(l \u221a r), which holds trivially for any matrix in Wr [p].)\nIf p does not have uniform marginals, but instead is a product distribution, then the quantity R defined above is potentially unbounded, so we cannot apply the same simple argument. However, we can consider the \u201cp-truncated\u201d class of matrices\nZ = { Z(X) = ( XijI { p (i, j) \u2265 log(n)\ns \u221a nm }) ij : X \u2208 Wr [p] } .\nBy a similar calculation of the expected spectral norms, we can now bound ES [ R\u0302S(Z) ] \u2264 O (\u221a rn log(n)\ns\n) .\nApplying [13], this bounds ( Lp(Z(X\u0302S))\u2212 L\u0302S(Z(X\u0302S)) ) (in expectation). Since Z(X\u0302S)ij 6= (X\u0302S)ij only on\nthe extremely low-probability entries, we can also bound ( Lp(X\u0302S)\u2212 Lp(Z(X\u0302S)) ) and ( L\u0302S(Z(X\u0302S))\u2212 L\u0302S(X\u0302S) ) .\nCombining these steps, we can bound ( Lp(X\u0302S)\u2212 L\u0302S(X\u0302S) ) . We similarly bound L\u0302S(X\n\u2217)\u2212Lp(X\u2217), where X\u2217 = arg minX\u2208Wr[p] Lp(X). Since L\u0302S(X\u0302S) \u2264 L\u0302S(X\u2217), this yields the desired bound on excess error. The details are given in Appendix A.\nExamining the proof of Theorem 1, we see that we can generalize the result by including distributions p with row- and column-marginals that are lower-bounded. More precisely, if p satisfies pr (i) \u2265 1Cn , pc (j) \u2265 1Cm for all i, j, then the bound (1) holds, up to a factor of C. Note that this result does not require an upper bound on the row- and column-marginals, only a lower bound, i.e. it only requires that no marginals are too low. This is important to note since the examples where the unweighted trace-norm fails under a non-uniform distribution are situations where some marginals are very high (but none are too low) [11]. This suggests that the low-probability marginals could perhaps be \u201csmoothed\u201d to satisfy a lower bound, without removing the advantages of the weighted trace-norm. We will exploit this in Section 3 to give a guarantee that holds more generally for arbitrary p, when smoothing is applied.\n2.2 Guarantees for bounded loss\nIn Theorem 1, we showed a strong bound on excess error, but only for a restricted class of distributions p. We now show that if the loss function ` is bounded, then we can give a non-trivial, but weaker, learning guarantee that holds uniformly over all distributions p. Since we are in any case discussing Lipschitz loss functions, requiring that the loss function be bounded essentially amounts to requiring that the entries of the matrices involved be bounded. That is, we can view this as a guarantee on learning matrices with bounded entries. In Section 2.3 below, we will show that this boundedness assumption is unavoidable if we want to give a guarantee that holds for arbitrary p.\nTheorem 2. For an l-Lipschitz loss ` bounded by b, fix any matrix Y , sample size s, and any distribution p. Let X\u0302S = arg min { L\u0302S(X) : X \u2208 Wr [p] } for r \u2265 1. Then, in expectation over the training sample S\ndrawn i.i.d. from the distribution p,\nLp(X\u0302S) \u2264 inf X\u2208Wr[p] Lp(X) + O\n( (l + b) \u00b7 3 \u221a rn log(n)\ns\n) . (2)\nThe proof is provided in Appendix A, and is again based on analyzing the expected Rademacher\ncomplexity, ES [ R\u0302(` \u25e6Wr [p]) ] \u2264 O ( (l + b) \u00b7 3 \u221a rn log(n)\ns\n) .\n2.3 Problems with the standard weighting\nIn the previous Sections, we showed that for distributions p that are either product distributions or have uniform marginals, we can prove a square-root bound on excess error, as shown in (1). For arbitrary p, the only learning guarantee we obtain is a cube-root bound given in (2), for the special case of bounded loss. We would like to know whether the square-root bound might hold uniformly over all distributions p, and if not, whether the cube-root bound is the strongest result that we can give in this case for the bounded-loss setting, and whether any bound will hold uniformly over all p in the unbounded-loss setting.\nThe examples below demonstrate that we cannot improve the results of Theorems 1 and 2 (up to log factors), by constructing degenerate examples using non-product distributions p with non-uniform marginals. Specifically, in Example 1, we show that in the special case of bounded loss, the cube-root bound in 2 is the best possible bound (up to the log factor) that will hold for all p, by giving a construction for arbitrary n = m and arbitrary s \u2264 nm, such that with 1-bounded loss, excess error is \u2126 ( 3 \u221a\nn s\n) . In\nExample 2, we show that with unbounded (Lipschitz) loss, we cannot bound excess error better than a constant bound, by giving a construction for arbitrary n = m and arbitrary s \u2264 nm in the unboundedloss regime, where excess error is \u2126(1). For both examples we fix r = 1. We note that both examples\ncan be modified to fit the transductive setting, demonstrating that smoothing is necessary also in the transductive setting as well.\nExample 1. Let `(x, y) = min{1, |x\u2212y|} \u2264 1, let a = (2s/n)2/3 < n, and let matrix Y and block-wise constant distribution p be given by\nY =\n( A 0\na\u00d7n2 0\n(n\u2212a)\u00d7n2 0 (n\u2212a)\u00d7n2\n) , (p (i, j)) =  12s \u00b7 1a\u00d7n2 0a\u00d7n2 0\n(n\u2212a)\u00d7n2 1\u2212an4s (n\u2212a) n2 \u00b7 1 (n\u2212a)\u00d7n2  , where A \u2208 {\u00b11}a\u00d7\nn 2 is any sign matrix. Clearly, \u2016Y \u2016tr(pr,pc) \u2264 1, and so infX\u2208Wr[p] Lp(X) = 0. Now\nsuppose we draw a sample S of size s from the matrix Y , according to the distribution p. We will show an ERM Y\u0302 such that in expectation over S, Lp(Y\u0302 ) \u2265 18 3 \u221a n s .\nConsider Y S where Y Sij = YijI {ij \u2208 S}, and note that \u2225\u2225Y S\u2225\u2225 tr(pr,pc) \u2264 1. Since L\u0302S(Y S) = 0, it clearly an ERM. We also have Lp(Y S) = N2s , where N is the number of \u00b11\u2019s in Y which are not observed in the\nsample. Since E [N ] \u2265 an4 , we see that E [ Lp(Y S) ] \u2265 12s \u00b7 an 4 \u2265 1 8 3 \u221a n s .\nExample 2. Let `(x, y) = |x \u2212 y|. Let Y = 0n\u00d7n; trivially, Y \u2208 Wr [p]. Let p (1, 1) = 1s , and p (i, 1) = p (1, j) = 0 for all i, j > 1, yielding pr (1) = pc (1) = 1s . (The other entries of p may be defined arbitrarily.) We will show an ERM Y\u0302 such that, in expectation over S, Lp(Y\u0302 ) \u2265 0.25. Let A be the matrix with X11 = s and zeros elsewhere, and note that \u2016A\u2016tr(pr,pc) = 1. With probability \u2265 0.25, entry (1, 1) will not appear in S, in which case Y\u0302 = A is an ERM, with Lp(Y\u0302 ) = 1.\nThe following table summarizes the learning guarantees that can be established for the (standard) weighted trace-norm. As we saw, these guarantees are tight up to log-factors.\n1-Lipschitz, 1-bounded loss 1-Lipschitz, unbounded loss p = product \u221a\nrn log(n) s\n\u221a rn log(n)\ns pr, pc = uniform \u221a\nrn log(n) s\n\u221a rn log(n)\ns p arbitrary 3 \u221a\nrn log(n) s 1\n3 Smoothing the weighted trace norm\nConsidering Theorem 1 and the degenerate examples in Section 2.3, it seems that in order to be able to generalize for non-product distributions, we need to enforce some sort of uniformity on the weights. The Rademacher complexity computations in the proof of Theorem 1 show that the problem lies not with large entries in the vectors pr and pc (i.e. if pr and/or pc are \u201cspiky\u201d), but with the small entries in these vectors. This suggests the possibility of \u201csmoothing\u201d any overly low row- or column-marginals, in order to improve learning guarantees.\nIn Section 3.1, we present such a smoothing, and provide guarantees for learning with a smoothed weighted trace-norm. The result suggests that there is no strong negative consequence to smoothing, but there might be a large advantage, if confronted with situations as in Examples 1 and 2. In Section 3.2 we check the smoothing correction to the weighted trace-norm on real data, and observe that indeed it can also be beneficial in practice.\n3.1 Learning guarantee for arbitrary distributions\nFix a distribution p and a constant \u03b1 \u2208 (0, 1), and let p\u0303 denote the smoothed marginals:\np\u0303r (i) = \u03b1 \u00b7 pr (i) + (1\u2212 \u03b1) \u00b7 1n , p\u0303 c (j) = \u03b1 \u00b7 pc (j) + (1\u2212 \u03b1) \u00b7 1m . (3)\nIn the theoretical results below, we use \u03b1 = 12 , but up to a constant factor, the same results hold for any fixed choice of \u03b1 \u2208 (0, 1).\nTheorem 3. For an l-Lipschitz loss `, fix any matrix Y , sample size s, and any distribution p. Let X\u0302S = arg min { L\u0302S(X) : X \u2208 Wr [p\u0303] } . Then, in expectation over the training sample S drawn i.i.d.\nfrom the distribution p,\nLp(X\u0302S) \u2264 inf X\u2208Wr[p\u0303] Lp(X) + O\n( l \u00b7 \u221a rn log(n)\ns\n) . (4)\nProof. We bound ES\u223cp [ R\u0302S(Wr [p\u0303]) ] \u2264 O (\u221a rn log(n)\ns\n) , and then apply [13]. The proof of this Rademacher\nbound is essentially identical to the proof in Theorem 1, with the modified definition of Qt = \u03c3t eit,jt\u221a p\u0303r(i)p\u0303c(j) .\nThen \u2016Qt\u2016sp \u2264 maxij 1\u221a p\u0303r(i)p\u0303c(j) \u2264 2 \u221a nm . = R, and E [\u2225\u2225\u2211s t=1QtQ T t \u2225\u2225 sp ] = s \u00b7 maxi \u2211 j p(i,j) p\u0303r(i)p\u0303c(j) \u2264\ns \u00b7maxi \u2211 j\np(i,j) 1 2p r(i)\u00b7 12m \u2264 4sm. Similarly, E [\u2225\u2225\u2211s\nt=1Q T t Qt \u2225\u2225 sp ] \u2264 4sn. Setting \u03c3 .= \u221a 4sn and applying [14], we obtain the result.\nMoving from Theorem 1 to Theorem 3, we are competing with a different class of matrices:\ninf X\u2208Wr[p] Lp(X) inf X\u2208Wr[p\u0303] Lp(X) .\nIn most applications we can think of, this change is not significant. For example, we consider the lowrank matrix reconstruction problem, where the trace-norm bound is used as a surrogate for rank. In order for the (squared) weighted trace-norm to be a lower bound on the rank, we would need to assume\u2225\u2225\u2225diag (pr)1/2Xdiag (pc)1/2\u2225\u2225\u22252\nF \u2264 1 [10]. If we also assume that \u2225\u2225(X\u2217)(i)\u2225\u222522 \u2264 m and \u2225\u2225(X\u2217)(j)\u2225\u222522 \u2264 n for all rows i and columns j \u2014 i.e. the row and column magnitudes are not \u201cspiky\u201d \u2014 then X\u2217 \u2208 Wr [p\u0303]. Note that this condition is much weaker than placing a spikiness condition on X\u2217 itself, e.g. requiring |X\u2217|\u221e \u2264 1.\n3.2 Results on Netflix and MovieLens Datasets\nWe evaluated different models on two publicly-available collaborative filtering datasets: Netflix [15] and MovieLens [16]. The Netflix dataset consists of 100,480,507 ratings from 480,189 users on 17,770 movies. Netflix also provides qualification set containing 1,408,395 ratings, but due to the sampling scheme, ratings from users with few ratings are overrepresented relative to the training set. To avoid dealing with different training and test distributions, we also created our own validation and test sets, each containing 100,000 ratings set aside from the training set. The MovieLens dataset contains 10,000,054 ratings from 71,567 users and 10,681 movies. We again set aside test and validation sets of 100,000 ratings. Ratings were normalized to be zero-mean.\nWhen dealing with large datasets the most practical way to fit trace-norm regularized models is via stochastic gradient descent [17, 2, 11]. For computational reasons, however, we consider rank-truncated trace-norm minimization, by optimizing within the restricted class {X : X \u2208 Wr [p] , rank (X) \u2264 k} for k = 30 and k = 100, and for various values of smoothing parameters \u03b1 (as in (3)). For each value of \u03b1 and k, the regularization parameter was chosen by cross-validation.\nThe following table shows root mean squared error (RMSE) for the experiments. For both k=30 and k=100 the weighted trace-norm with smoothing significantly outperforms the weighted trace-norm without smoothing (\u03b1 = 1), even on the differently-sampled Netflix qualification set. We also note that the proposed weighted trace-norm with smoothing outperforms max-norm regularization [18], and compares favorably with the \u201cgeometric\u201d smoothing used by [11] as a heuristic, without theoretical or conceptual justification. A moderate value of \u03b1 = 0.9 seems consistently good.\nNetflix MovieLens \u03b1 k Test Qual k Test Qual k Test k Test 1 30 0.7604 0.9107 100 0.7404 0.9078 30 0.7852 100 0.7821 0.9 30 0.7589 0.9096 100 0.7391 0.9068 30 0.7831 100 0.7798 0.5 30 0.7601 0.9173 100 0.7419 0.9161 30 0.7836 100 0.7815 0.3 30 0.7712 0.9198 100 0.7528 0.9207 30 0.7864 100 0.7871 0 30 0.7887 0.9249 100 0.7659 0.9236 30 0.7997 100 0.7987\n4 The empirically-weighted trace norm\nIn practice, the sampling distribution p is not known exactly \u2014 it can only be estimated via the locations of the entries which are observed in the sample. Defining the empirical marginals\np\u0302r (i) = #{t : it = i}\ns , p\u0302c (j) = #{t : jt = j} s ,\nwe would like to give a learning guarantee when X\u0302S is estimated via regularization on the p\u0302-weighted trace-norm, rather than the p-weighted trace-norm.\nIn Section 4.1, we give bounds on excess error when learning with smoothed empirical marginals, which show that there is no theoretical disadvantage as compared to learning with the smoothed true marginals. In fact, we provide evidence that suggests there might even be an advantage to using the empirical marginals. To this end, in Section 4.2, we introduce the transductive learning setting, and give a result based on the empirical marginals which implies a sample complexity bound that is better by a factor of log 1/2(n). In Section 4.3, we show that in low-rank matrix reconstruction simulations, using empirical marginals is indeed yields better reconstructions.\n4.1 Guarantee for the standard (inductive) setting\nWe first show that when learning with the smoothed empirical marginals, defined as p\u030cr (i) = 12 ( p\u0302r (i) + 1n ) , p\u030cc (j) = 12 ( p\u0302c (j) + 1m ) ,\nwe can obtain the same guarantee as for learning with the smoothed (true) marginals, given by p\u0303.\nTheorem 4. For an l-Lipschitz loss `, fix any matrix Y , sample size s, and any distribution p. Let X\u0302S = arg min { L\u0302S(X) : X \u2208 Wr [p\u030c] } . Then, in expectation over the training sample S drawn i.i.d.\nfrom the distribution p,\nLp(X\u0302S) \u2264 inf X\u2208Wr[p\u0303] Lp(X) + O\n( l \u00b7 \u221a rmax{n,m} log(n+m)\ns\n) . (5)\nNote that although we regularize using the (smoothed) empirically-weighted trace-norm, we still compare ourselves to the best possible matrix in the class defined by the (smoothed) true marginals.\nThe proof of the Theorem (given in Appendix A) uses Theorem 3 and involves showing that with a sample of size s = \u2126(n log(n)), which is required for all Theorems so far to be meaningful, the true and empirical marginals are the same up to a constant factor. For this to be the case, such a sample size is even necessary. In fact, the log(n) factor in our analysis (e.g. in the proof of Theorem 1) arises from the bound on the expected spectral norm of a matrix, which, for a diagonal matrix, is just a bound on the deviation of empirical frequencies. Might it be possible, then, to avoid this logarithmic factor by using the empirical marginals? Although we could not establish such a result in the inductive setting, we now turn to the transductive setting, where we could indeed obtain a better guarantee.\n4.2 Guarantee for the transductive setting\nIn the transductive model, we fix a set S \u2282 [n]\u00d7 [m] of size 2s, and then randomly split S into a training set S and a test set T of equal size s. The goal is to obtain a good estimator for the entries in T based on the values of the entries in S, as well as the locations (indexes) of all elements on S. We then use the (smoothed or unsmoothed) empirical marginals of S, for the weighted trace-norm.\nWe now show that, for bounded loss, there may be a benefit to weighting with the smoothed empirical marginals \u2014 the sample size requirement can be lowered to s = O ( rn log 1/2(n) ) .\nTheorem 5. For an l-Lipschitz loss ` bounded by b, fix any matrix Y and sample size s. Let S \u2282 [n]\u00d7[m] be a fixed subset of size 2s, split uniformly at random into training and test sets S and T , each of size s. Let p denote the smoothed empirical marginals of S. Let X\u0302S = arg min { L\u0302S(X) : X \u2208 Wr [p] } . Then\nin expectation over the splitting of S into S and T ,\nL\u0302T (X\u0302S) \u2264 inf X\u2208Wr[p] L\u0302T (X) + O\nl \u00b7 \u221a rn log 1/2(n)\ns + b\u221a s\n . (6)\nThis result (proved in Appendix B) is stated in the transductive setting, with a somewhat different sampling procedure and evaluation criteria, but we believe the main difference is in the use of the empirical weights. Although it is usually straightforward to convert a transductive guarantee to an inductive one, the situation here is more complicated, since the hypothesis class depends on the weighting, and hence on the sample S. Nevertheless, we believe such a conversion might be possible, establishing a similar guarantee for learning with the (smoothed) empirically weighted trace-norm also in the inductive setting. Furthermore, by using the fact that a sample of size s = \u0398(n log(n)) is sufficient for the empirical marginals to be close to the true marginals, it might be possible to obtain a learning guarantee for the true (non-empirical) weighting with a sample of size s = O(n(r log 1/2(n) + log(n))).\nTheorem 5 above can be viewed as a transductive analog to Theorem 3 (where weights are based on the combined sample S). In Appendix B we state and prove transductive analogs also to Theorem 1 (for the case where smoothing is not needed) and Theorem 2 (giving a cubic-root rate). As mentioned in Section 2.3, our lower bound examples can also be stated in the transductive setting, and thus all our guarantees and lower bounds can also be obtained in this setting.\n4.3 Simulations with empirical weights\nIn order to numerically investigate the possible advantage of empirical weighting, we performed simulations on low-rank matrix reconstruction under uniform sampling with the unweighted, and the smoothed empirically weighted, trace-norms. We choose to work with uniform sampling in order to emphasize the benefit of empirical weights, even in situations where one might not consider to use any weights at all. In all the experiments, we attempt to reconstruct a possibly noisy, random rank-2 \u201csignal\u201d matrix M with singular values 1\u221a\n2 (n, n, 0, . . . , 0), ensuring \u2016M\u2016F = n, measuring error using the squared loss1.\nSimulations were performed using Matlab, with code adapted from the SoftImpute code developed by [19]. We performed two types of simulations:\nSample complexity comparison in the noiseless setting: We define Y = M , and compute X\u0302S = arg min { \u2016X\u2016 : L\u0302S(X) = 0 } , where \u2016X\u2016 = \u2016X\u2016tr or = \u2016X\u2016tr(p\u0302r,p\u0302c), as appropriate. In Figure 1(a), we\nplot the average number of samples per row needed to get average squared error (over 100 repetitions) of at most 0.1, with both uniform weighting and empirical weighting.\nExcess error comparison in the noiseless and noisy settings: We define Y = M+\u03bdN , where noise N has i.i.d. standard normal entries. We compute X\u0302S = arg min { \u2016X\u2016 : L\u0302S(X) \u2264 \u03bd2 } . In Figure 1(b),\nwe plot the resulting average squared error (over 100 repetitions) over a range of sample sizes s and noise levels \u03bd, with both uniform weighting and empirical weighting.\nThe results from both experiments show a significant benefit to using the empirical marginals.\n1Although the squared loss is Lipschitz in a bounded domain, it is probably possible to improve all our results (removing the square root) in the special case of the squared loss, possibly with the additional assumption of i.i.d. noise , as in [9].\n5 Discussion\nIn this paper, we prove learning guarantees for the weighted trace-norm by analyzing expected Rademacher complexities. We show that weighting with smoothed marginals eliminates degenerate scenarios that can arise in the case of a non-product sampling distribution, and demonstrate in experiments on the Netflix and MovieLens datasets that this correction can be useful in applied settings. We also give results for empirically-weighted trace-norm regularization, and see indications that using the empirical distribution may be better than using the true distribution, even if it is available.\nReferences\n[1] N. Srebro, J. Rennie, and T. Jaakkola. Maximum-margin matrix factorization. Advances in Neural Information Processing Systems, 17, 2004.\n[2] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. Advances in Neural Information Processing Systems, 20, 2007.\n[3] F. Bach. Consistency of trace-norm minimization. Journal of Machine Learning Research, 9:1019\u2013 1048, 2008.\n[4] E. Cande\u0300s and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Trans. Inform. Theory, 56(5):2053\u20132080, 2009.\n[5] N. Srebro and A. Shraibman. Rank, trace-norm and max-norm. 18th Annual Conference on Learning Theory (COLT), pages 545\u2013560, 2005.\n[6] B. Recht. A simpler approach to matrix completion. arXiv:0910.0651, 2009.\n[7] R. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. Journal of Machine Learning Research, 11:2057\u20132078, 2010.\n[8] V. Koltchinskii, A. Tsybakov, and K. Lounici. Nuclear norm penalization and optimal rates for noisy low rank matrix completion. arXiv:1011.6256, 2010.\n[9] S. Negahban and M. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. arXiv:1009.2118, 2010.\n[10] R. Foygel and N. Srebro. Concentration-based guarantees for low-rank matrix reconstruction. 24th Annual Conference on Learning Theory (COLT), 2011.\n[11] R. Salakhutdinov and N. Srebro. Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm. Advances in Neural Information Processing Systems, 23, 2010.\n[12] O. Shamir and S. Shalev-Shwartz. Collaborative filtering with the trace norm: Learning, bounding, and transducing. 24th Annual Conference on Learning Theory (COLT), 2011.\n[13] P. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463\u2013482, 2002.\n[14] J.A. Tropp. User-friendly tail bounds for sums of random matrices. arXiv:1004.4389, 2010.\n[15] J. Bennett and S. Lanning. The netflix prize. In Proceedings of KDD Cup and Workshop, volume 2007, page 35. Citeseer, 2007.\n[16] MovieLens Dataset. Available at http://www.grouplens.org/node/73. 2006.\n[17] Yehuda Koren. Factorization meets the neighborhood: a multifaceted collaborative filtering model. ACM Int. Conference on Knowledge Discovery and Data Mining (KDD\u201908), pages 426\u2013434, 2008.\n[18] J. Lee, B. Recht, R. Salakhutdinov, N. Srebro, and J. Tropp. Practical Large-Scale Optimization for Max-Norm Regularization. Advances in Neural Information Processing Systems, 23, 2010.\n[19] R. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. Journal of Machine Learning Research, 11:2287\u20132322, 2010.\n[20] Y. Seginer. The expected norm of random matrices. Combinatorics, Probability and Computing, 9 (2):149\u2013166, 2000.\nA Proofs for the i.i.d. sampling setting\nA.1 Proof of Theorem 1\nWe first fill in the details for the Rademacher bound in the case that p has uniform row- and columnmarginals. Define\nQt = \u03c3t eit,jt\u221a\npr (it) pc (jt) \u2208 Rn\u00d7m .\nWe need to calculate R and \u03c32 such that \u2016Qt\u2016sp \u2264 R (almost surely) and\n\u03c32 = max {\u2225\u2225\u2225\u2211E [QTt Qt]\u2225\u2225\u2225 sp , \u2225\u2225\u2225\u2211E [QtQTt ]\u2225\u2225\u2225 sp } .\nFor each t, Qt is just a matrix with a single non-zero entry of magnitude 1\u221a\npr(i)pc(j) , for some i, j, and\nso \u2016Qt\u2016sp \u2264 maxij 1\u221a\npr(i)pc(j)\n. = R.\nThe matrix QtQ T t \u2208 Rn\u00d7n is equal to ei,i pr(i)pc(j) with probability p (i, j). Hence E [ QTt Qt ] is a diagonal\nmatrix with entries \u2211 j p(i,j) pr(i)pc(j) . Similar arguments apply to Q T t Qt. Multiplying by s, and recalling the spectral norm of a diagonal matrix is simply the maximal magnitude element, we have:\n\u03c32 = s \u00b7max maxi \u2211 j p (i, j) pr (i) pc (j) ,max j \u2211 i p (i, j) pr (i) pc (j)  . This completes the proof for the case that p has uniform row- and column- marginals.\nNext we turn to the case that p is a product distribution, p = pr \u00d7 pc (with possibly non-uniform marginals). For any X \u2208 Wr [p], define\nZ(X) = ( XijI { p (i, j) \u2265 log(n)\ns \u221a nm }) ij .\nLet Z = {Z(X) : X \u2208 Wr [p]}. We can then follow the proof of the bound in the uniform-marginals case, with a modified definition of Qt:\nQt = \u03c3t eit,jtI\n{ p (it, jt) \u2265 log(n)s\u221anm } \u221a pr (it) pc (jt) .\nProceeding as in the proof for Theorem 1, we obtain R \u2264 \u221a s \u221a nm\nlog(n) and \u03c3 2 \u2264 sn, and thus\nES\u223cp [ R\u0302S(Z) ] = O\n(\u221a rn log(n)\ns\n) .\nTherefore, by [13],\nE [ sup\nX\u2208Wr[p] Lp(Z(X))\u2212 L\u0302S(Z(X))\n] \u2264 O ( l \u00b7 \u221a rn log(n)\ns\n) ,\nE [ sup\nX\u2208Wr[p] L\u0302S(Z(X))\u2212 Lp(Z(X))\n] \u2264 O ( l \u00b7 \u221a rn log(n)\ns\n) .\nNext, let I = (\u221a p (i, j)I { p (i, j) < log(n)\ns \u221a nm }) ij . For any matrix M , define\n\u2016M\u2016F (pr,pc) = \u2225\u2225\u2225diag (pr)1/2Mdiag (pc)1/2\u2225\u2225\u2225\nF .\nNow take any M with \u2016M\u2016F (pr,pc) \u2264 1. Let M \u2032 = diag (pr) 1/2 Mdiag (pc) 1/2 , then \u2016M \u2032\u2016F \u2264 1. We have\u2211\nij:p(i,j)< log(n)\ns \u221a nm\np (i, j)Mij = \u2211 ij IijM \u2032 ij = \u3008I,M \u2032\u3009 \u2264 \u2016I\u2016F \u00b7 \u2016M \u2032\u2016F\n\u2264 \u2016I\u2016F = \u221a\u221a\u221a\u221a\u2211 ij p (i, j) I { p (i, j) < log(n) s \u221a nm } \u2264 \u221a nm \u00b7 log(n) s \u221a nm = \u221a\u221a nm log(n) s .\nSince \u2016M\u2016F \u2264 \u2016M\u2016tr for any matrix M , we then have, for any X \u2208 Wr [p], \u2016X\u2016F (pr,pc) \u2264 \u2016X\u2016tr(pr,pc) \u2264 \u221a r, and so\n|Lp(X)\u2212 Lp(Z(X))| = \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 ij 6\u2208I p (i, j) (`(Xij , Yij)\u2212 `(0, Yij)) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 l \u00b7\n\u2211 ij 6\u2208I p (i, j) |Xij | \u2264 \u221a l2r \u221a nm log(n) s .\nAnd, fixing some X\u2217 \u2208 Wr [p] such that Lp(X\u2217) = infX\u2208Wr[p] Lp(X),\nE [ sup\nX\u2208Wr[p] L\u0302S(Z(X))\u2212 L\u0302S(X)\n] + E [ L\u0302S(X \u2217)\u2212 L\u0302S(Z(X\u2217)) ]\n= E [ sup\nX\u2208Wr[p]\n1\ns s\u2211 t=1 I {(it, jt) 6\u2208 I} (`(0, Yitjt)\u2212 `(Xitjt , Yitjt))\n]\n+ E\n[ 1\ns s\u2211 t=1 I {(it, jt) 6\u2208 I} ( `(X\u2217itjt , Yitjt)\u2212 `(0, Yitjt)\n)]\n= E [ sup\nX\u2208Wr[p]\n1\ns s\u2211 t=1 I {(it, jt) 6\u2208 I} ( `(X\u2217itjt , Yitjt)\u2212 `(Xitjt , Yitjt)\n)]\n\u2264 E [ sup\nX\u2208Wr[p]\n1\ns s\u2211 t=1 I {(it, jt) 6\u2208 I} `(X\u2217itjt , Yitjt)\n] \u2264 l \u00b7E [ 1\ns s\u2211 t=1 I {(it, jt) 6\u2208 I} |X\u2217itjt |\n]\n= l \u00b7E [ I {(i1, j1) 6\u2208 I} |X\u2217i1j1 | ] = l \u00b7 \u2211 ij 6\u2208I p (i, j) |X\u2217ij | \u2264 \u221a l2r \u221a nm log(n) s\nThen writing\nLp(X\u0302S)\u2212 Lp(X\u2217) = (Lp(X\u0302S)\u2212 Lp(Z(X\u0302S))) + (Lp(Z(X\u0302S))\u2212 L\u0302S(Z(X\u0302S))) + (L\u0302S(Z(X\u0302S))\u2212 L\u0302S(X\u0302S))\n+(L\u0302S(X\u0302S)\u2212 L\u0302S(X\u2217)) + (L\u0302S(X\u2217)\u2212 L\u0302S(Z(X\u2217))) + (L\u0302S(Z(X\u2217))\u2212Lp(Z(X\u2217))) + (Lp(Z(X\u2217))\u2212Lp(X\u2217)) , we obtain\nE [ Lp(X\u0302S)\u2212 Lp(X\u2217) ] \u2264 O\n(\u221a l2rn log(n)\ns\n) .\nA.2 Proof of Theorem 2\nAssume ` is l-Lipschitz and b-bounded, and r \u2265 1. We will show that (for any p)\nES\u223cp [ R\u0302S(` \u25e6Wr [p]) ] = O ( (l + b) \u00b7 3 \u221a rn log(n)\ns\n) .\nGiven a sample S, define\nT 0S = { t : pr (it) or p c (jt) < 3 \u221a l2r log(n)\nb2sn2\n} , T 1S = {1, . . . , s}\\T 0S .\nWe have\nR\u0302S(` \u25e6Wr [p]) = E\u03c3\u223c{\u00b11}s [\nsup \u2016X\u2016tr(pr,pc)\u2264 \u221a r\n1\ns s\u2211 t=1 \u03c3t \u00b7 `(Xitjt , Yitjt)\n]\n\u2264 E\u03c3  sup \u2016X\u2016tr(pr,pc)\u2264 \u221a r 1 s \u2211 t\u2208T 0S \u03c3t \u00b7 `(Xitjt , Yitjt) + E\u03c3  sup \u2016X\u2016tr(pr,pc)\u2264 \u221a r 1 s \u2211 t\u2208T 1S \u03c3t \u00b7 `(Xitjt , Yitjt) \nBounding the first term,\nE\u03c3  sup \u2016X\u2016tr(pr,pc)\u2264 \u221a r 1 s \u2211 t\u2208T 0S \u03c3t \u00b7 `(Xitjt , Yitjt)  \u2264 E\u03c3 1 s \u2211 t\u2208T 0S |\u03c3t| \u00b7 b  = b s \u00b7 \u2223\u2223T 0S\u2223\u2223 .\nIn expectation over S,\nES\n[ b s \u00b7 \u2223\u2223T 0S\u2223\u2223] = b \u00b7Eij\u223cp [ I { pr (i) or pc (j) < 3 \u221a l2r log(n) b2sn2 }]\n= b \u00b7 \u2211 ij p (i, j) I\n{ pr (i) or pc (j) < 3 \u221a l2r log(n)\nb2sn2\n}\n\u2264 b \u00b7 \u2211 i:pr(i)< 3 \u221a l2r log(n)\nb2sn2\n\u2211 j p (i, j) + b \u00b7 \u2211\nj:pc(j)< 3 \u221a l2r log(n)\nb2sn2\n\u2211 i p (i, j) \n= b \u00b7 \u2211 i:pr(i)< 3 \u221a l2r log(n)\nb2sn2\npr (i) + b \u00b7 \u2211\nj:pc(j)< 3 \u221a l2r log(n)\nb2sn2\npc (j)  \u2264 bn \u00b7 3 \u221a l2r log(n)\nb2sn2 + bm\n3\n\u221a l2r log(n)\nb2sn2 \u2264 2 3\n\u221a l2brn log(n)\ns .\nTo bound the second term, we use the fact that \u2016abs(X)\u2016tr \u2264 \u2016X\u2016tr for any matrix X, where abs(X)\nis the matrix defined via abs(X)ij = |Xij |. We have\nE\u03c3  sup \u2016X\u2016tr(pr,pc)\u2264 \u221a r 1 s \u2211 t\u2208T 1S \u03c3t \u00b7 `(Xitjt , Yitjt)  \u2264 E\u03c3\n sup \u2016X\u2016tr(pr,pc)\u2264 \u221a r 1 s \u2211 t\u2208T 1S \u03c3t \u00b7 (`(Xitjt , Yitjt)\u2212 `(0, Yitjt) + E\u03c3  sup \u2016X\u2016tr(pr,pc)\u2264 \u221a r 1 s \u2211 t\u2208T 1S \u03c3t \u00b7 `(0, Yitjt)  = E\u03c3\n sup \u2016X\u2016tr(pr,pc)\u2264 \u221a r 1 s \u2211 t\u2208T 1S \u03c3t \u00b7 (`(Xitjt , Yitjt)\u2212 `(0, Yitjt)  \u2264 l \u00b7E\u03c3  sup \u2016X\u2016tr(pr,pc)\u2264 \u221a r 1 s \u2211 t\u2208T 1S \u03c3t \u00b7 |Xitjt |  = l \u00b7E\u03c3\n sup \u2016X\u2032\u2016tr\u2264 \u221a r 1 s \u2211 t\u2208T 1S \u03c3t\u221a pr (it) pc (jt) \u00b7 |X \u2032itjt |  \u2264 l \u00b7E\u03c3  sup \u2016X\u2032\u2032\u2016tr\u2264 \u221a r 1 s \u2211 t\u2208T 1S \u03c3t\u221a pr (it) pc (jt) \u00b7X \u2032\u2032itjt \n= l \u221a r \u00b7E\u03c3  \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 1 s s\u2211 t=1 \u03c3t e(it,jt)I { pr (it) , p c (jt) \u2265 3 \u221a l2r log(n) b2sn2 } \u221a pr (it) pc (jt) \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 sp  ,\nDefining Qt = \u03c3t e(it,jt)I\n{ pr(it),p c(jt)\u2265 3 \u221a l2r log(n)\nb2sn2 } \u221a pr(it)pc(jt) , we can follow identical arguments as in the proof of the\nfirst bound of this theorem. We have\n\u2016Qt\u2016sp \u2264 maxij\nI { pr (i) , pc (j) \u2265 3 \u221a l2r log(n) b2sn2 } \u221a pr (i) pc (j) \u2264 3 \u221a b2sn2 l2r log(n) . = R ,\nand\n\u03c32 . = max {\u2225\u2225\u2225\u2211E [QTt Qt]\u2225\u2225\u2225 sp , \u2225\u2225\u2225\u2211E [QtQTt ]\u2225\u2225\u2225 sp }\n\u2264 s \u00b7max maxi \u2211 j p (i, j) I { pr (i) , pc (j) \u2265 3 \u221a l2r log(n) b2sn2 } pr (i) pc (j) ,\nmax j \u2211 i\np (i, j) I { pr (i) , pc (j) \u2265 3 \u221a l2r log(n) b2sn2 } pr (i) pc (j)  \u2264 s \u00b7 3 \u221a b2sn2\nl2r log(n) \u00b7max maxi \u2211 j p (i, j) pr (i) ,max j \u2211 i p (i, j) pr (i)  = 3 \u221a b2s4n2 l2r log(n)\nThen applying [14], we get\nE\u03c3  sup \u2016X\u2016tr(pr,pc)\u2264 \u221a r 1 s \u2211 t\u2208T 1S \u03c3t \u00b7 `(Xitjt , Yitjt)  = l\u221ar s ES,\u03c3 \u2225\u2225\u2225\u2225\u2225 s\u2211 t=1 Qt \u2225\u2225\u2225\u2225\u2225 sp  \u2264 O ( l \u221a r\ns\n( \u03c3 \u221a log(n) +R log(n) ))\n\u2264 O\n( l \u221a r\ns\n( 6 \u221a b2s4n2\nl2r log(n)\n\u221a log(n) + 3\n\u221a b2sn2\nl2r log(n) log(n)\n))\n\u2264 O l2/3b1/3 3\u221arn log(n) s + l 1/3b 2/3 ( 3 \u221a rn log(n) s )2 . If s \u2265 rn log(n), then this proves the bound. If not, then the result is trivial, since Lp(X) \u2264 b for any X.\nA.3 Proof of Theorem 4\nThroughout this section, assume s \u2265 24n log(n). (If this is not the case, then we only need to prove excess error \u2264 O(l \u221a r), which is trivial given the class Wr [p\u030c].) We also assume s \u2264 O (nm log(nm)). (If this is not the case, then with high probability, we observe all entries of the matrix and obtain optimal recovery.) The lemmas which are cited in this proof, are proved below.\nDefine X\u2217 = arg min\nX\u2208Wr[p\u0303] Lp(X), r\n\u2217 = \u2016X\u2217\u20162tr(pr,pc) \u2264 r .\nFor any sample S, define\nc(S) = max { 0, \u2225\u2225\u2225\u2225 1\u221ar\u2217X\u2217 \u2225\u2225\u2225\u2225\ntr(p\u030cr,p\u030cc)\n\u2212 1 } .\nThen, for a fixed S,\n\u2016(1\u2212 c(S))X\u2217\u2016tr(p\u030cr,p\u030cc) = \u221a r\u2217(1\u2212 c(S)) \u2225\u2225\u2225\u2225 1\u221ar\u2217X\u2217 \u2225\u2225\u2225\u2225\ntr(p\u0303r,p\u0303c)\n\u2264 \u221a r \u21d2 (1\u2212 c(S))X\u2217 \u2208 Wr [p\u030c] .\nApplying Lemma 1 and Theorem 3,\nE [ Lp(X\u0302S)\u2212 L\u0302S(X\u0302S) ] \u2264 E [ sup\nX\u2208Wr[p\u030c]\n( Lp(X)\u2212 L\u0302S(X)\n)]\n\u2264 E [ sup\nX\u22082\u00b7Wr[p\u0303]\n( Lp(X)\u2212 L\u0302S(X) )] + 8 \u221a l2rnm\nn2 \u2264 O\n(\u221a l2rn log(n)\ns\n) + 8 \u221a l2rnm\nn2\n\u2264 O\n(\u221a l2rn log(n)\ns\n) .\nAnd, similarly,\nE [ L\u0302S((1\u2212 c(S))X\u2217)\u2212 Lp((1\u2212 c(S))X\u2217) ] \u2264 E [ sup\nX\u2208Wr[p\u030c]\n( L\u0302S(X)\u2212 Lp(X)\n)]\n\u2264 E [ sup\nX\u22082\u00b7Wr[p\u0303]\n( L\u0302S(X)\u2212 Lp(X) )] + 8 \u221a l2rnm\nn2 \u2264 O\n(\u221a l2rn log(n)\ns\n) + 8 \u221a l2rnm\nn2\n\u2264 O\n(\u221a l2rn log(n)\ns\n) .\nBy definition, since (1\u2212 c(S))X\u2217 \u2208 Wr [p\u030c],\nE [ L\u0302S(X\u0302S)\u2212 L\u0302S((1\u2212 c(S))X\u2217) ] \u2264 0 .\nFinally, by Lemma 3,\nE [Lp((1\u2212 c(S))X\u2217)\u2212 Lp(X\u2217)] \u2264 \u221a 2l2rn\ns .\nCombining all of the above, we get\nE [ Lp(X\u0302S)\u2212 Lp(X\u2217) ] \u2264 O\n(\u221a l2rn log(n)\ns\n) .\nA.3.1 Lemmas for Theorem 6\nLemma 1.\nE [ sup\nX\u2208Wr[p\u030c]\n( Lp(X)\u2212 L\u0302S(X) )] \u2264 E [ sup\nX\u22082\u00b7Wr[p\u0303]\n( Lp(X)\u2212 L\u0302S(X) )] + 8 \u221a l2rnm\nn2 .\nE [ sup\nX\u2208Wr[p\u030c]\n( L\u0302S(X)\u2212 Lp(X) )] \u2264 E [ sup\nX\u22082\u00b7Wr[p\u0303]\n( L\u0302S(X)\u2212 Lp(X) )] + 8 \u221a l2rnm\nn2 .\nProof. By Lemma 2, with probability at least 1\u2212 2n\u22122, for all i, j,\np\u030cr (i) \u2265 1 2 p\u0303r (i) , p\u030cc (j) \u2265 1 2 p\u0303c (j) .\nLet A be the event that these inequalities hold. If A occurs, then for any X \u2208 Wr [p\u030c],\n\u2016X\u2016tr(pr,pc) = \u2225\u2225\u2225diag (p\u0303r (i))1/2Xdiag (p\u0303c (j))1/2\u2225\u2225\u2225\ntr\n= \u2225\u2225\u2225\u2225\u2225diag ( p\u0303r (i) p\u030cr (i) )1/2 diag (p\u030cr (i)) 1/2 Xdiag (p\u030cc (j)) 1/2 diag ( p\u0303c (j) p\u030cc (j) )1/2\u2225\u2225\u2225\u2225\u2225 tr\n\u2264 2 \u2225\u2225\u2225diag (p\u030cr (i))1/2Xdiag (p\u030cc (j))1/2\u2225\u2225\u2225\ntr = 2 \u2016X\u2016tr(p\u030cr,p\u030cc) \u2264 2\n\u221a r .\nIn this case, Wr [p\u030c] \u2282 2 \u00b7 Wr [p\u0303], and therefore,\nsup X\u2208Wr[p\u030c]\n[ (Lp(X)\u2212 Lp(0n\u00d7m))\u2212 ( L\u0302S(X)\u2212 L\u0302S(0n\u00d7m) )] \u2264 sup X\u22082\u00b7Wr[p\u0303] [ (Lp(X)\u2212 Lp(0n\u00d7m))\u2212 ( L\u0302S(X)\u2212 L\u0302S(0n\u00d7m) )] .\nNext we consider the case that A does not occur. For any X \u2208 Wr [p\u030c],\n|X|\u221e \u2264 \u2016X\u2016F = 2 \u221a nm \u2225\u2225\u2225\u2225\u2225diag ( 1 2n 1n )1/2 Xdiag ( 1 2m 1m )1/2\u2225\u2225\u2225\u2225\u2225 F\n\u2264 2 \u221a nm \u2225\u2225\u2225\u2225\u2225diag ( 1 2n 1n )1/2 Xdiag ( 1 2m 1m )1/2\u2225\u2225\u2225\u2225\u2225 tr \u2264 2 \u221a nm \u2016X\u2016tr(p\u030cr,p\u030cc) \u2264 2 \u221a rnm .\nTherefore,\nsup X\u2208Wr[p\u030c]\n[ (Lp(X)\u2212 Lp(0n\u00d7m))\u2212 ( L\u0302S(X)\u2212 L\u0302S(0n\u00d7m) )]\n\u2264 sup X\u2208Wr[p\u030c] \u2211 ij p (i, j) (`(Xij , Yij)\u2212 `(0, Yij))\u2212 1 s \u2211 t (`(Xitjt , Yitjt)\u2212 `(0, Yitjt))  \u2264 l \u00b7 sup\nX\u2208Wr[p\u030c] \u2211 ij p (i, j) \u00b7 |Xij |+ 1 s \u2211 t |Xitjt |  \u2264 l \u00b7 sup\nX\u2208Wr[p\u030c] \u2211 ij p (i, j) \u00b7 2 \u221a rnm+ 1 s \u2211 t 2 \u221a rnm  \u2264 4\u221al2rnm And so,\nES\n[ sup\nX\u2208Wr[p\u030c]\n[ (Lp(X)\u2212 Lp(0n\u00d7m))\u2212 ( L\u0302S(X)\u2212 L\u0302S(0n\u00d7m)\n)]]\n= ES\n[ sup\nX\u2208Wr[p\u030c]\n[ (Lp(X)\u2212 Lp(0n\u00d7m))\u2212 ( L\u0302S(X)\u2212 L\u0302S(0n\u00d7m) )] \u00b7 I {A}\n]\n+ ES\n[ sup\nX\u2208Wr[p\u030c]\n[ (Lp(X)\u2212 Lp(0n\u00d7m))\u2212 ( L\u0302S(X)\u2212 L\u0302S(0n\u00d7m) )] \u00b7 I {Ac}\n]\n\u2264 ES [ sup\nX\u22082\u00b7Wr[p\u0303]\n[ (Lp(X)\u2212 Lp(0n\u00d7m))\u2212 ( L\u0302S(X)\u2212 L\u0302S(0n\u00d7m) )] \u00b7 I {A} ] + P (Ac) \u00b7 4 \u221a l2rnm\n\u2264 ES [ sup\nX\u22082\u00b7Wr[p\u0303]\n[ (Lp(X)\u2212 Lp(0n\u00d7m))\u2212 ( L\u0302S(X)\u2212 L\u0302S(0n\u00d7m) )] \u00b7 I {A} ] + 8 \u221a l2rnm\nn2\n\u2264 ES [ sup\nX\u22082\u00b7Wr[p\u0303]\n[ (Lp(X)\u2212 Lp(0n\u00d7m))\u2212 ( L\u0302S(X)\u2212 L\u0302S(0n\u00d7m) )]] + 8 \u221a l2rnm\nn2 .\nwhere the last step is true because, since 0n\u00d7m \u2208 2 \u00b7 Wr [p\u0303], for any S,\nsup X\u22082\u00b7Wr[p\u0303]\n[ (Lp(X)\u2212 Lp(0n\u00d7m))\u2212 ( L\u0302S(X)\u2212 L\u0302S(0n\u00d7m) )] \u2265 0 .\nAnd, ES [ Lp(0n\u00d7m)\u2212 L\u0302S(0n\u00d7m) ] = 0, so therefore,\nES\n[ sup\nX\u2208Wr[p\u030c]\n( Lp(X)\u2212 L\u0302S(X) )] \u2264 E [ sup\nX\u22082\u00b7Wr[p\u0303]\n( Lp(X)\u2212 L\u0302S(X) )] + 8 \u221a l2rnm\nn2 .\nThe second claim can be proved with identical arguments.\nLemma 2. With probability at least 1\u2212 2n\u22122, for all i and all j,\np\u030cr (i) \u2265 1 2 p\u0303r (i) , p\u030cc (j) \u2265 1 2 p\u0303c (j) .\nProof. Take any row i. Suppose that pr (i) \u2264 1n . Then p\u0303 r (i) \u2264 1n , while p\u030c r (i) = 12 ( p\u0302r (i) + 1n ) \u2265 12n . Therefore, in this case, p\u030cr (i) \u2265 12 p\u0303 r (i) with probability 1.\nNext, suppose that pr (i) > 1n . Then, by the Chernoff inequality,\nP ( p\u0302r (i) < 1\n2 pr (i)\n) = P ( Bin(s, pr (i)) < spr (i) ( 1\u2212 1\n2\n)) \u2264 e\u2212 spr(i) 8\n\u2264 e\u2212 s8n \u2264 e\u22123 log(n) = n\u22123 .\nTherefore, with probability at least 1\u2212 n\u22123, p\u0302r (i) \u2265 12p r (i), and so\np\u030cr (i) = 1\n2\n( p\u0302r (i) + 1\nn\n) \u2265 1\n2\n( 1\n2 pr (i) +\n1\nn\n) \u2265 1\n2 p\u0303r (i) .\nTherefore, for any row i, with probability at least 1 \u2212 n\u22123, p\u030cr (i) \u2265 12 p\u0303 r (i). The same reasoning applies to every column j. Therefore, with probability at least 1 \u2212 2n\u22122, the statement holds for all i and all j.\nLemma 3. Fix X\u2217 with \u2016X\u2217\u20162tr(p\u0303r,p\u0303c) = r\u2217 \u2264 r, and define\nc(S) = max { 0, \u2225\u2225\u2225\u2225 1\u221ar\u2217X\u2217 \u2225\u2225\u2225\u2225\ntr(p\u030cr,p\u030cc)\n\u2212 1 } .\nThen\nE [Lp((1\u2212 c(S))X\u2217)\u2212 Lp(X\u2217)] \u2264 \u221a 2l2rn\ns .\nProof.\nLp((1\u2212 c(S))X\u2217)\u2212 Lp(X\u2217) = \u2211 ij p (i, j) ( `((1\u2212 c(S))X\u2217ij , Yij)\u2212 `(X\u2217ij , Yij) ) \u2264 l \u00b7\n\u2211 ij p (i, j) |(1\u2212 c(S))X\u2217ij \u2212X\u2217ij | = l \u00b7 c(S) \u00b7 \u2211 ij p (i, j) |X\u2217ij |\n= l \u00b7 c(S) \u00b7 \u2211 ij p (i, j)\u221a p\u0303r (i) p\u0303c (j) \u00b7 \u221a p\u0303r (i) p\u0303c (j) \u00b7 |X\u2217ij |\nDefining M = ( p(i,j)\u221a p\u0303r(i)p\u0303c(j) ) ij ,\n= l \u00b7 c(S) \u00b7 \u3008M, ( diag (p\u0303r (i)) 1/2 X\u2217diag (p\u0303c (j)) 1/2 ) ij \u3009\n\u2264 l \u00b7 c(S) \u00b7 \u2016M\u2016sp \u00b7 \u2225\u2225\u2225\u2225(diag (p\u0303r (i))1/2X\u2217diag (p\u0303c (j))1/2)\nij \u2225\u2225\u2225\u2225 tr\n\u2264 l \u221a r \u00b7 c(S) \u00b7 \u2016M\u2016sp .\nNow we show that \u2016M\u2016sp \u2264 2. Take any unit vectors u \u2208 Rm, v \u2208 Rn. Then\nuTMv = \u2211 ij p (i, j) \u00b7 \u221a u2i p\u0303r (i) \u00b7 \u221a v2j p\u0303c (j) \u2264 1 2 \u2211 ij p (i, j) ( u2i p\u0303r (i) + v2j p\u0303c (j) )\n= 1\n2 \u2211 i pr (i) \u00b7 u 2 i p\u0303r (i) + 1 2 \u2211 j pc (j) \u00b7 v2j p\u0303c (j) \u2264 1 2 \u2211 i 2u2i + 1 2 \u2211 j 2v2j = 2 .\nSo, by Lemma 4,\nE [Lp((1\u2212 c(S))X\u2217)\u2212 Lp(X\u2217)] \u2264 2l \u221a r \u00b7E [c(S)] \u2264 2l \u221a r \u00b7 \u221a n\n2s .\nLemma 4. For any p, for any fixed X with \u2016X\u2016tr(p\u0303r,p\u0303c) = 1,\nE [ max{0, \u2016X\u2016tr(p\u030cr,p\u030cc) \u2212 1} ] \u2264 \u221a n\n2s .\nProof. By properties of the trace-norm [5], we can write diag (p\u0303r) 1/2 Xdiag (p\u0303c) 1/2 = ABT , where \u2016A\u20162F = \u2016B\u20162F = \u2016X\u2016tr(pr,pc) = 1. Define\nD1 = diag (p\u030c r) diag (p\u0303r) \u22121 , D2 = diag (p\u030c c) diag (p\u0303c) \u22121 .\nThen, by properties of the trace-norm [5],\n\u2016X\u2016tr(p\u030cr,p\u030cc) = \u2225\u2225\u2225diag (p\u030cr)1/2Xdiag (p\u030cc)1/2\u2225\u2225\u2225\ntr = \u2225\u2225\u2225\u2225(D1/21 A)(D1/22 B)T\u2225\u2225\u2225\u2225 tr\n\u2264 1 2 \u2225\u2225\u2225D1/21 A\u2225\u2225\u22252 F + 1 2 \u2225\u2225\u2225D1/22 B\u2225\u2225\u22252 F = 1\n2 \u2211 i p\u030cr (i) p\u0303r (i) \u2016A(i)\u201622 + 1 2 \u2211 j p\u030cc (j) p\u0303c (j) \u2016B(j)\u201622\n= 1\n4 \u2211 i p\u0302r (i) + 1n p\u0303r (i) \u2016A(i)\u201622 + 1 4 \u2211 j p\u0302c (j) + 1m p\u0303c (j) \u2016B(j)\u201622\n= 1\n4 \u2211 i Nri + s n sp\u0303r (i) \u2016A(i)\u201622 + 1 4 \u2211 j N cj + s m sp\u0303c (j) \u2016B(j)\u201622 ,\nwhere Nri is the number of samples in row i, and N c j is the number of samples in column j. Clearly,\nE 1 4 \u2211 i Nri + s n sp\u0303r (i) \u2016A(i)\u201622 + 1 4 \u2211 j N cj + s m sp\u0303c (j) \u2016B(j)\u201622  = 1\n4 \u2211 i spr (i) + sn sp\u0303r (i) \u2016A(i)\u201622 + 1 4 \u2211 j spc (j) + sm sp\u0303c (j) \u2016B(j)\u201622\n= 1\n4 \u2211 i 2sp\u0303r (i) sp\u0303r (i) \u2016A(i)\u201622 + 1 4 \u2211 j 2sp\u0303c (j) sp\u0303c (j) \u2016B(j)\u201622\n= 1\n2 \u2016A\u20162F +\n1 2 \u2016B\u20162F = 1 .\nAnd, we can compute\nVar(Nri ) \u2264 spr (i) , Cov(Nri , Nri\u2032) < 0, Var(N cj ) \u2264 spc (j) , Cov(N cj , N cj\u2032) < 0 .\nTherefore,\nVar \u2211 i Nri sp\u0303r (i) \u2016A(i)\u201622 + \u2211 j N cj sp\u0303c (j) \u2016B(j)\u201622  \u2264 2Var\n(\u2211 i Nri sp\u0303r (i) \u2016A(i)\u201622 ) + 2Var \u2211 j N cj sp\u0303c (j) \u2016B(j)\u201622  = \u2211 i 1 s2p\u0303r (i) 2 Var(N r i )\u2016A(i)\u201642 + 2 \u2211 i<i\u2032 1 s2p\u0303r (i) p\u0303r (i\u2032) Cov(Nri , N r i\u2032)\u2016A(i)\u201622\u2016A(i\u2032)\u201622\n+ \u2211 j\n1\ns2p\u0303c (j) Var(N cj )\u2016B(j)\u201642 + 2 \u2211 j<j\u2032\n1\ns2p\u0303c (j) p\u0303c (j\u2032) Cov(N cj , N c j\u2032)\u2016B(j)\u201622\u2016B(j\u2032)\u201622\n\u2264 \u2211 i\n1\ns2p\u0303r (i) 2 Var(N\nr i )\u2016A(i)\u201642 + \u2211 j\n1\ns2p\u0303c (j) Var(N cj )\u2016B(j)\u201642\n\u2264 \u2211 i spr (i) s2p\u0303r (i) 2 \u2016A(i)\u2016 4 2 + \u2211 j spc (j) s2p\u0303c (j) \u2016B(j)\u201642\nSince p\u0303r (i) \u2265 12p r (i) and p\u0303r (i) \u2265 12n , and similarly for the columns, we continue:\n\u2264 \u2211 i 4n s \u2016A(i)\u201642 + \u2211 j 4m s \u2016B(j)\u201642 \u2264 4n s (\u2211 i \u2016A(i)\u201622 )2 + 4m s \u2211 j \u2016B(j)\u201622  \u2264 4n\ns \u2016A\u20164F +\n4m\ns \u2016B\u20164F \u2264\n4(n+m)\ns .\nSo, we have\nE [ max{0, \u2016X\u2016tr(p\u030cr,p\u030cc) \u2212 1} ] \u2264 E max 0, 14 \u2211\ni\nNri + s n\nsp\u0303r (i) \u2016A(i)\u201622 +\n1\n4 \u2211 j N cj + s m sp\u0303c (j) \u2016B(j)\u201622 \u2212 1  \n\u2264 \u221a\u221a\u221a\u221a\u221aVar 1\n4 \u2211 i Nri + s n sp\u0303r (i) \u2016A(i)\u201622 + 1 4 \u2211 j N cj + s m sp\u0303c (j) \u2016B(j)\u201622\n\n= \u221a\u221a\u221a\u221a\u221aVar 1\n4 \u2211 i Nri sp\u0303r (i) \u2016A(i)\u201622 + 1 4 \u2211 j N cj sp\u0303c (j) \u2016B(j)\u201622  \u2264\u221a (n+m) 4s\nB Proofs for the transductive setting\nB.1 Proof of Theorem 5\nLet S \u2282 [n]\u00d7 [m] be a subset of size 2s. Let p denote the smoothed empirical marginals of S. Now choose any S \u2282 S, a training set of size s. Without loss of generality, write S = {(i1, j1), . . . , (i2s, j2s)} and S = {(i1, j1), . . . , (is, js)}.\nFirst, we bound transductive Rademacher complexity. By Lemma 12 in [5], for any sample S, R\u0302S(Wr [p]) = E\u03c3\u223c{\u00b11}s [\nsup X\u2208Wr[p]\n1\ns s\u2211 t=1 \u03c3tXitjt\n]\n= E\u03c3\u223c{\u00b11}s  sup X\u2208Wr[p] 1 s \u2211 ij Xij  \u2211 t:(it,jt)=(i,j) \u03c3t  \u2264 E\u03c3\u223c{\u00b11}n\u00d7m  sup X\u2208Wr[p] 1 s \u2211 ij Xij\u03c3ij \u00b7#{t : (it, jt) = (i, j), 1 \u2264 t \u2264 s}\n = E\u03c3\u223c{\u00b11}n\u00d7m  sup X\u2208Wr[p] 1 s \u2211 ij Xij\u03c3ij \u00b7 I {(i, j) \u2208 S}\n . Now define matrix \u03a3 via\n\u03a3ij = I {(i, j) \u2208 S} s \u221a pr (i) pc (j) .\nWe have\nES\u223cp [ R\u0302S(Wr [p]) ] \u2264 ES E\u03c3\u223c{\u00b11}n\u00d7m  sup X\u2208Wr[p] 1 s \u2211 ij Xij\u03c3ij \u00b7 I {(i, j) \u2208 S}  = ES E\u03c3\u223c{\u00b11}n\u00d7m  sup X\u2208Wr[p] \u2211 ij (\u221a pr (i)Xij \u221a pc (j) ) \u03c3ij\u03a3ij\n = ES [ E\u03c3\u223c{\u00b11}n\u00d7m [ sup\nX:\u2016X\u2016tr\u2264 \u221a r\nXij\u03c3ij\u03a3ij ]] = \u221a r \u00b7ES [ E\u03c3\u223c{\u00b11}n\u00d7m [ \u2016\u03c3 \u2022 \u03a3\u2016sp ]] ,\nwhere \u03c3 \u2022 \u03a3 is the element-wise product of \u03a3 with the random sign matrix \u03c3 = (\u03c3ij). By [20], E\u03c3\u223c{\u00b11}n\u00d7m [ \u2016\u03c3 \u2022 \u03a3\u2016sp ] \u2264 O ( log1/4(n+m) ) \u00b7max { max i \u2225\u2225\u03a3(i)\u2225\u22252 ,maxj \u2225\u2225\u2225\u03a3(j)\u2225\u2225\u22252 } .\nWe now bound \u2225\u2225\u03a3(i)\u2225\u22252 and \u2225\u2225\u03a3(j)\u2225\u22252. Fix any i. Then\u2225\u2225\u03a3(i)\u2225\u222522 = \u2211 j \u03a32ij = m\u2211 j=1 I {(i, j) \u2208 S} (spr (i)) \u00b7 (spc (j)) \u2264 m\u2211 j=1 I { (i, j) \u2208 S } (spr (i)) \u00b7 ( s \u00b7 12m\n) \u2264\nm\u2211 j=1 #{t : (it, jt) = (i, j), 1 \u2264 t \u2264 2s}( 1 4# {t : it = i, 1 \u2264 t \u2264 2s} ) \u00b7 ( s \u00b7 12m ) \u2264 #{t : it = i, 1 \u2264 t \u2264 2s}( 1 4# {t : it = i, 1 \u2264 t \u2264 2s} ) \u00b7 ( s \u00b7 12m ) \u2264 8m s .\nSimilarly, for all j, \u2225\u2225\u03a3(j)\u2225\u22252\n2 \u2264 8ns . Therefore,\nES\u223cp [ R\u0302S(Wr [p]) ] \u2264 \u221a r \u00b7ES [ E\u03c3\u223c{\u00b11}n\u00d7m [ \u2016\u03c3 \u2022 \u03a3\u2016sp ]] \u2264 \u221a r \u00b7ES [ O ( log1/4(n) ) \u00b7max { max i \u2225\u2225\u03a3(i)\u2225\u22252 ,maxj \u2225\u2225\u2225\u03a3(j)\u2225\u2225\u22252 }] \u2264 O \u221arn log1/2(n) s\n . Applying Theorem 5 of [12] (using integration to obtain a bound in expectation from a bound in\nprobability),\nES [ L\u0302S\\S(X\u0302S)\u2212 inf\nX\u2208Wr[p] L\u0302S\\S(X)\n] \u2264 O \u221a l2rn log1/2(n) + b2 s  .\nB.2 Transductive version of Theorem 1\nLet p now denote the (unsmoothed) empirical marginals of S. If pr (i) \u2265 1Cn and p c (j) \u2265 1Cm for all i, j, defining\nX\u0302S = arg min X\u2208Wr[p] L\u0302S(X) ,\nwe can then show that, for an l-Lipschitz loss ` bounded by b, in expectation over the split of S into training set S and test set T ,\nL\u0302T (X\u0302S) \u2264 inf X\u2208Wr[p] L\u0302T (X) + O\nC1/2l \u00b7 \u221a rn log 1/2(n) + b2\ns  . We prove this by following identical arguments as in the proof of Theorem 5, we define\n\u03a3ij = I {(i, j) \u2208 S} s \u221a pr (i) pc (j) ,\nand obtain \u2225\u2225\u03a3(i)\u2225\u222522 ,\u2225\u2225\u03a3(j)\u2225\u222522 \u2264 2Cns for all i, j, which yields\nES [ L\u0302S\\S(X\u0302S)\u2212 inf\nX\u2208Wr[p] L\u0302S\\S(X)\n] \u2264 O \u221aCl2rn log1/2(n) + b2 s  . In fact, we can obtain the same result with a weaker requirement on p, namely\ns n max { max i \u2016\u03a3(i)\u201622,max j \u2016\u03a3(j)\u201622 } \u2264 max maxi 1m m\u2211 j=1 1 s I { (i, j) \u2208 S } pr (i) pc (j) ,max j 1 n n\u2211 i=1 1 s I { (i, j) \u2208 S } pr (i) pc (j)  \u2264 C . For instance, this quantity is likely to be bounded if S is a sample drawn from a product distribution on the matrix.\nB.3 Transductive version of Theorem 2\nLet p now denote the (unsmoothed) empirical marginals of S. We define\nX\u0302S = arg min X\u2208Wr[p] L\u0302S(X) ,\nwe can then show that, for an l-Lipschitz loss ` bounded by b, without any requirements on p, in expectation over the split of S into training set S and test set T ,\nL\u0302T (X\u0302S) \u2264 inf X\u2208Wr[p] L\u0302T (X) + O\n( (l + b) \u00b7 3 \u221a rn log(n)\ns\n) .\nWe prove this by combining the proof techniques used in the proofs of Theorems 2 and 5. Define\nT 0S = { t : 1 \u2264 t \u2264 2s, pr (it) or pc (jt) < 3 \u221a l2r log(n)\nb2sn2\n} , T 1S = {1, . . . , 2s}\\T 0S .\nWe then have R\u0302S(` \u25e6Wr [p]) = E\u03c3\u223c{\u00b11}s [\nsup X\u2208Wr[p]\n1\ns s\u2211 t=1 \u03c3t`(Xitjt , Yitjt)\n]\n\u2264 E\u03c3\u223c{\u00b11}n\u00d7m  sup X\u2208Wr[p] 1 s \u2211 ij `(Xitjt , Yitjt)\u03c3ij \u00b7 I {(i, j) \u2208 S}  \u2264 E\u03c3  sup X\u2208Wr[p] 1 s \u2211 ij `(Xitjt , Yitjt)\u03c3ij \u00b7 I { (i, j) \u2208 S, and pr (i) , pc (j) \u2265 3 \u221a l2r log(n) b2sn2\n} + E\u03c3  sup X\u2208Wr[p] 1 s \u2211 ij `(Xitjt , Yitjt)\u03c3ij \u00b7 I { (i, j) \u2208 S, and pr (i) or pc (j) < 3 \u221a l2r log(n) b2sn2\n} . = (Term 1) + (Term 2); .\nNow define matrix \u03a3 via\n\u03a3ij =\nI { (i, j) \u2208 S, and pr (i) , pc (j) \u2265 3 \u221a\nl2r log(n) b2sn2 } s \u221a pr (i) pc (j) .\nFollowing the same arguments as in the proof of Theorem 5, we obtain for all i, j,\n\u2016\u03a3(i)\u201622, \u2016\u03a3(j)\u201622 \u2264 4 s \u00b7 3 \u221a b2sn2 l2r log(n)\nTherefore, using the same arguments as in the proof of Theorem 2,\n(Term 1) \u2264 l \u221a rO log1/4(n) \u221a\u221a\u221a\u221a4 s \u00b7 3 \u221a b2sn2 l2r log(n)  = O( 3\u221a l2brn log(n) s ) .\nNext we have\n(Term 2) = E\u03c3  sup X\u2208Wr[p] 1 s \u2211 ij `(Xitjt , Yitjt)\u03c3ij \u00b7 I { (i, j) \u2208 S, and pr (i) or pc (j) < 3 \u221a l2r log(n) b2sn2 } \u2264 sup X\u2208Wr[p] 1 s \u2211 ij `(Xitjt , Yitjt) \u00b7 I { (i, j) \u2208 S, and pr (i) or pc (j) < 3 \u221a l2r log(n) b2sn2 }\n\u2264 1 s \u2211 ij b \u00b7 I\n{ (i, j) \u2208 S, and pr (i) or pc (j) < 3 \u221a l2r log(n)\nb2sn2\n}\n\u2264 1 s \u2211 i:pr(i)< 3 \u221a l2r log(n)\nb2sn2\n \u2211 j:(i,j)\u2208S b + 1 s \u2211 j:pc(j)< 3 \u221a l2r log(n)\nb2sn2\n \u2211 i:(i,j)\u2208S b  \u2264 2n\ns\n( b \u00b7 2s \u00b7 3 \u221a l2r log(n)\nb2sn2\n) \u2264 O ( 3 \u221a l2rbn log(n)\ns\n) .\nCombining the two, we get Rs(` \u25e6 Wr [p]) \u2264 O ( 3 \u221a l2rbn log(n)\ns\n) , and therefore, in expectation over\nthe split of S into S and T ,\nL\u0302T (X\u0302S) \u2264 inf X\u2208Wr[p] L\u0302T (X) + O\n( (l + b) \u00b7 3 \u221a rn log(n)\ns\n) ."}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": "We provide rigorous guarantees on learning with the weighted trace-norm under arbitrary<lb>sampling distributions. We show that the standard weighted trace-norm might fail when the<lb>sampling distribution is not a product distribution (i.e. when row and column indexes are<lb>not selected independently), present a corrected variant for which we establish strong learning<lb>guarantees, and demonstrate that it works better in practice. We provide guarantees when<lb>weighting by either the true or empirical sampling distribution, and suggest that even if the<lb>true distribution is known (or is uniform), weighting by the empirical distribution may be<lb>beneficial.", "creator": "LaTeX with hyperref package"}}}