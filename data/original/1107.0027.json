{"id": "1107.0027", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2011", "title": "Effective Dimensions of Hierarchical Latent Class Models", "abstract": "Hierarchical latent class (HLC) models are tree-structured Bayesian networks where leaf nodes are observed while internal nodes are latent. There are no theoretically well justified model selection criteria for HLC models in particular and Bayesian networks with latent nodes in general. Nonetheless, empirical studies suggest that the BIC score is a reasonable criterion to use in practice for learning HLC models. Empirical studies also suggest that sometimes model selection can be improved if standard model dimension is replaced with effective model dimension in the penalty term of the BIC score. Effective dimensions are difficult to compute. In this paper, we prove a theorem that relates the effective dimension of an HLC model to the effective dimensions of a number of latent class models. The theorem makes it computationally feasible to compute the effective dimensions of large HLC models. The theorem can also be used to compute the effective dimensions of general tree models.", "histories": [["v1", "Thu, 30 Jun 2011 20:34:11 GMT  (177kb)", "http://arxiv.org/abs/1107.0027v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["t kocka", "n l zhang"], "accepted": false, "id": "1107.0027"}, "pdf": {"name": "1107.0027.pdf", "metadata": {"source": "CRF", "title": "Effective Dimensions of Hierarchical Latent Class Models", "authors": ["Nevin L. Zhang", "Tom\u00e1\u0161 Ko\u010dka"], "emails": ["lzhang@cs.ust.hk", "kocka@lisp.vse.cz"], "sections": [{"heading": null, "text": "Effective dimensions are difficult to compute. In this paper, we prove a theorem that relates the effective dimension of an HLC model to the effective dimensions of a number of latent class models. The theorem makes it computationally feasible to compute the effective dimensions of large HLC models. The theorem can also be used to compute the effective dimensions of general tree models."}, {"heading": "1. Introduction", "text": "Hierarchical latent class (HLC) models (Zhang, 2002) are tree-structured Bayesian networks (BNs) where leaf nodes are observed while internal nodes are latent. They generalize latent class models (Lazarsfeld and Henry, 1968) and were first identified as a potentially useful class of Bayesian networks by Pearl (1988). We are concerned with learning HLC models from data. A fundamental question is how to select among competing models.\nThe BIC score (Schwarz, 1978) is a popular metric that researchers use to select among Bayesian network models. It consists of a loglikelihood term that measures the fitness to data and a penalty term that depends linearly upon standard model dimension, i.e. the number of linearly independent standard model parameters. When all variables are observed, the BIC score is an asymptotic approximation of (the logarithm) of the marginal likelihood (Schwarz, 1978). It is also consistent in the sense that, given sufficient data, the BIC score of the generative model \u2014 the model from which data were sampled \u2014 is larger than those of any other models that are not equivalent to the generative model.\nWhen latent variables are present, the BIC score is no longer an asymptotic approximation of the marginal likelihood (Geiger et al., 1996). This can be remedied, to some extent, using the concept of effective model dimension. In fact if we replace standard model dimension with effective model dimension in the BIC score, the resulting scoring function, called the BICe score, is an asymptotic approximation of the marginal likelihood almost everywhere except for some singular points (Rusakov and Geiger, 2002).\nc\u00a92004 AI Access Foundation. All rights reserved.\nNeither BIC nor BICe have been proved to be consistent for latent variable models. As a matter of fact, it has not even been defined what it means for a model selection criterion to be consistent for latent variable models. Empirical studies suggest that the BIC score is well-behaved in practice for the task of learning HLC models. There are three related searchbased algorithms for learning HLC models, namely double hill-climbing (DHC) (Zhang, 2002), single hill-climbing (SHC) (Zhang et al., 2003), and heuristic SHC (HSHC) (Zhang, 2003). In the absence of a theoretically well justified model selection criterion, Zhang (2002) tested DHC with four existing scoring functions, namely the AIC score (Akaike, 1974), the BIC score, the Cheeseman-Stutz (CS) score (Cheeseman and Stutz, 1995), and the holdout logarithmic score (HLS)(Cowell et al., 1999). Both real-world and synthetic data were used. On the real-world data, BIC and CS have enabled DHC to find models that are regarded as the best by domain experts. On the synthetic data, BIC and CS have enabled DHC to find models that either are identical to or resemble closely the true generative models. When coupled with AIC and HLS, on the other hand, DHC performed significantly worse. SHC and HSHC were tested on synthetic data sampled from fairly large HLC models (as much as 28 nodes). Only BIC was used in those tests. In all cases, BIC has enabled SHC and HSHC to find models that either are identical to or resemble closely the true generative models. Those empirical results not only indicate that the algorithms perform well, but also suggest that the BIC is a reasonable scoring function to use for learning HLC models.\nThe experiments also reveal that model selection can sometimes be improved if the BICe score is used instead of the BIC score. We will explain this in detail in Section 3\nIn order to use the BICe score in practice, we need a way to compute effective dimensions. This is not a trivial task. The effective dimension of an HLC model is the rank of the Jacobian matrix of the mapping from the parameters of the model to the parameters of the joint distribution of the observed variables. The number of rows in the Jacobian matrix increases exponentially with the number of observed variables. The construction of the Jacobian matrix and the calculation of its rank are both computationally demanding. Moreover they have to be done algebraically or with very high numerical precision to avoid degenerate cases. The necessary precision grows with the size of the matrix.\nSettimi and Smith (1998, 1999) studied effective dimensions for two classes of models: trees with binary variables and latent class (LC) models with two observed variables. They have obtained a complete characterization of these two classes. Geiger et al. (1996) computed the effective dimensions of a number of models. They conjectured that it is rare for the effective and standard dimensions of an LC model to differ. As a matter of fact, they found only one such model. Kocka and Zhang (2002) found quite a number of LC models whose effective and standard dimensions differ. They also proposed an easily computable formula for estimating effective dimensions of LC models. The estimation formula has been empirically shown to be very accurate.\nIn this paper, we prove a theorem that relates the effective dimension of an HLC model to the effective dimensions of two other HLC models that contain fewer latent variables. Repeated application of the theorem allows one to reduce the task of computing the effective dimension of an HLC model to subtasks of computing effective dimensions of LC models. This makes it computationally feasible to compute the effective dimensions of large HLC models.\nWe start in Section 2 with a formal definition of effective dimensions for Bayesian networks with latent variables. In Section 3, we provide empirical evidence that suggest the use of BICe instead of BIC sometimes improves model selection. Section 4 presents the main theorem and Section 5 is devoted to the proof of the theorem. In Section 6, we prove a theorem about effective dimensions of general tree models and explain how this and our main theorem allows one to compute the effective dimension of arbitrary tree models. Finally, concluding remarks are provided in Section 7."}, {"heading": "2. Effective Dimensions of Bayesian Networks", "text": "In this paper, we use capital letters such as X and Y to denote variables and lower case letters such as x and y to denote states of variables. The domain and cardinality of a variable X will be denoted by \u2126X and |X| respectively. Bold face capital letters such as Y denote sets of variables. \u2126Y denotes the Cartesian product of the domains of all variables in the set Y. Elements of \u2126Y will be denoted by bold lower case letters such as y and will sometimes be referred to as states of Y. We will consider only variables that have a finite number of states.\nConsider a Bayesian network model M that possibly contains latent variables. The standard dimension ds(M) of M is the number of linearly independent parameters in the standard parameterization of M . The parameters denote, for each variable and each parent configuration of the variable, the probability that the variable is in some state (except one) given the parent configuration. Suppose M consist of k variables x1, x2, . . . , xk. Let ri and qi be respectively the number of states of xi and the number of all possible combinations of the states of its parents. If xi has no parent, let qi be 1. Then ds(M) is given by\nds(M) = k\u2211\ni=1\nqi(ri \u2212 1).\nFor notational simplicity, denote the standard dimension of M by n. Let ~\u03b8=(\u03b81, \u03b82, . . . , \u03b8n) be a vector of n linearly independent model parameters of M . Further let Y be the set of observed variables. Suppose Y has m+1 possible states. We enumerate the first m states as y1, y1, . . . , ym.\nFor any i (1\u2264i\u2264m), P (yi) is a function of the parameters ~\u03b8. So we have a mapping from the n dimensional parameter space (a subspace of Rn) to Rm, namely T : (\u03b81, \u03b82, . . . , \u03b8n) \u22a2 (P (y1), P (y2), . . . , P (ym)). The Jacobian matrix of this mapping is the following m\u00d7n matrix:\nJM (~\u03b8) = [Jij ] = [ \u2202P (yi)\n\u2202\u03b8j ]\nFor convenience, we will often write the matrix as JM = [ \u2202P (Y)\n\u2202\u03b8j ], with the understanding\nthat elements of the j-th column are obtained by allowing Y run over all its possible states except one.\nFor each i, P (yi) is a function of ~\u03b8. For most commonly used parameterizations of Bayesian networks, it is actually a polynomial function of ~\u03b8. Hence we make the following assumption:\nAssumption 1 The Bayesian network M is so parameterized that the parameters for the joint distribution of the observed variables are polynomial functions of the parameters for M .\nAn obvious consequence of the assumption is that elements of JM are also polynomial functions of ~\u03b8.\nFor a given value of ~\u03b8, JM is a matrix of real numbers. Due to Assumption 1, the rank of this matrix is some constant d almost everywhere in the parameter space (Geiger et al., 1996. Also see Section 5.1.). To be more specific, the rank is d everywhere except in a set of measure zero where it is smaller than d. The constant is called the regular rank of JM .\nThe regular rank of JM is also called the effective dimension of the Bayesian network model M . Hence we denote it by de(M). To understand the term \u201ceffective dimension\u201d, consider the subspace of Rm spanned by the joint probability P (Y) of observed variables, or equivalently the range of the mapping T . The term reflects the fact that, for almost every value of ~\u03b8, a small enough open ball around T (~\u03b8) resembles Euclidean space of dimension d (Geiger et al., 1996).\nThere are multiple ways to parameterize a given Bayesian network model. However, the choice of parameterization does not affect the space spanned by the joint probability P (Y). Together with the interpretation of the previous paragraph, this implies that the definition of effective dimension does not depend on the particular parameterization that one uses."}, {"heading": "3. Selecting among HLC Models", "text": "A hierarchical latent class (HLC) model is a Bayesian network where (1) the network structure is a rooted tree and (2) the variables at the leaf nodes are observed and all the other variables are not. The observed variables are sometimes referred to as manifest variables and all the other variables as latent variables. Figure 1 shows the structures of two HLC models. A latent class (LC) model is an HLC model where there is only one latent variable.\nThe theme of this paper is the computation of effective dimensions of HLC models. As mentioned in the introduction, this is interesting because effective dimension, when used in the BIC score, gives us a better approximation of the marginal likelihood. In this section, we give an example to illustrate that the use of effective dimension sometimes also leads to better model selection. We will also motivate and introduce the concept of regularity that will be used in subsequent sections."}, {"heading": "3.1 An Example of Model Selection", "text": "Consider the two HLC models shown in Figure 1. In one experiment, we instantiated the parameters of M1 in a random fashion and sampled a set D1 of 10,000 data records on the observed variables. Then we ran SHC and HSHC on the data set D1 under the guidance of the BIC score. Both algorithms produced model M2. In the following, we explain why, based on D1, one would prefer M2 over M1 if BIC is used for model selection and why M1 would be preferred if BICe is used instead. We argue that M1 should be preferred based on D1 and hence BICe is a better scoring metric for this case.\nThe BIC and BICe scores of a model M given a data set D are defined as follows:\nBIC(M |D) = logP (D|M, ~\u03b8\u2217) \u2212 ds(M)\n2 logN,\nBICe(M |D) = logP (D|M, ~\u03b8\u2217) \u2212 de(M)\n2 logN\nwhere ~\u03b8\u2217 is the maximum likelihood estimate of the parameters of M based on D and N is the sample size.\nIn our example, notice that M2 includes M1 in the sense that M2 can represent any probability distributions of the observed variables that M1 can. In fact, if we make the conditional probability distributions of the observed variables in M2 the same as in M1 and set PM2(X2) and PM2(X3|X2) such that\nPM2(X2)PM2(X3|X2) = \u2211\nX1\nPM1(X1)PM1(X2|X1)PM1(X3|X1),\nthen the probability distribution of the observed variables in the two models are identical. Because M2 includes M1, we have logP (D1|M1, ~\u03b8 \u2217 1) \u2264 logP (D1|M2,\n~\u03b8\u22172). Together with the fact that D1 is sampled from M1, this implies that logP (D1|M1, ~\u03b8 \u2217 1) \u2248 logP (D1|M2,\n~\u03b8\u22172) for sufficiently large enough sample size. The standard dimension of M1 is 45, while that of M2 is 44. Hence\nBIC(M1|D1) < BIC(M2|D1).\nOn the other hand, the effective dimensions of M1 and M2 are 43 and 44 respectively. Hence\nBICe(M1|D1) > BICe(M2|D1).\nModel M2 includes M1. The opposite is clearly not true because the effective dimension of M1 is smaller than that of M2. So, M2 is in reality a more complex model than M1. Both model fit data D1 equally well. Hence the simpler one, i.e. M1, should be preferred over the other. This agrees with the choice of the BICe score, while disagrees with the choice of the BIC score. Hence, BICe is more appropriate than BIC in this case."}, {"heading": "3.2 Regularity", "text": "Now consider another model M \u20321 that is the same as M1 except that the cardinality of X1 is increased from 2 to 3. It is easy to show that M2 includes M \u2032 1 and vice versa. So, the two models are equivalent in terms of their capabilities of representing probability distributions of the observed variables. They are hence said to be marginally equivalent. However, M \u20321 has more standard parameters than M2 and hence we would always prefer M2 over M \u2032 1. To formalize this consideration, we introduce a concept of regularity. For a latent variable Z in an HLC model, enumerate its neighbors (parent and children) as X1, X2, . . . , Xk. An HLC model is regular if for any latent variable Z,\n|Z| \u2264\n\u220fk i=1 |Xi|\nmaxki=1 |Xi| , (1)\nand the strict inequality holds when Z has two neighbors and at least one of them is a latent node. Models M1 and M2 are regular, while model M \u2032 1 is not.\nFor any irregular model M there always exists a regular model that is marginally equivalent to M and has fewer standard parameters (Zhang, 2003b). The regular model can be obtained from M as follows: For any latent node that has only two neighbors and its cardinality is no smaller than that of one of the neighbors, then remove the latent node and connect the two neighbors. For any latent node that has more than two neighbors and that violates (1), reduce it\u2019s cardinality to the quantity on the right hand side. Repeat both steps until no more changes can be made.\nIt is also interesting to note that the collection of all regular HLC models for a given set of observed variables is finite (Zhang, 2002). This provides a finite search space for the task of learning regular HLC models.1 In the rest of this paper, we will consider only regular HLC models.\nBefore ending this subsection, we point out a nice property of effective model dimension in relation to model inclusion. If an HLC model includes another model, then its effective dimension is no less than that of the latter. As a consequence, two marginally equivalent models have the same effective dimensions and hence the same BICe score. The same is not true for standard model dimension and the BIC score."}, {"heading": "3.3 The CS and CSe Scores", "text": "We have argued on empirical grounds that the BIC score is a reasonable scoring function to use for learning HLC models and that the BICe score can sometimes improve model selection. But the two scores are not free of problems. One problem is that their derivation as Laplace approximations of the marginal likelihood are not valid at the boundary of the parameter space. The CS score in a way alleviates this problem. It involves the BIC score based on completed data and the BIC score based on original data. In other words, it involves two Laplace approximations of the marginal likelihood. It lets errors in the two approximation cancel each other.\nChickering and Heckerman (1997) empirically found the CS score to be a quite accurate approximation of the marginal likelihood and robust at the boundary of the parameter\n1. The definition of regularity given in this paper is slightly different from the one given in Zhang (2002). Nonetheless, the two conclusions mentioned in this paragraph remain true.\nspace. They realized the need for effective model dimension in the CS score, although they did not actually use it. This would not have made any differences to their experiments because, for the models they used, the standard and effective dimensions agree.\nWe use CSe to refer to the scoring function one obtains by replacing standard model dimension in the CS score with effective model dimensions. Just as BICe is better than BIC as approximations of the marginal likelihood (Geiger et al., 1996), CSe is better than CS. To compute CSe, we also need to calculate effective dimensions."}, {"heading": "4. Effective Dimensions of HLC Models", "text": "As we have seen, effective model dimension is interesting for a number of reasons. Our main result in this paper is a theorem about the effective dimension de(M) of a regular HLC model M that contains more than one latent variable. Let X be the root of M , which is a latent node. Because there are at least two latent nodes, there must exist another latent node Z that is a child of X. In the following, we will use the terms X-branch and Z-branch to respectively refer to the sets of nodes that are separated from Z by X or from X by Z. Let Y be the set of observed variables in the Z-branch and let O be the set of all other observed variables. Note that the X-branch doesn\u2019t contain the node X. The relationship among X, Z, Y, and O is depicted in the left-most picture of Figure 2.\nThe standard parameterization of M includes parameters for P (X) and parameters for P (Z|X). For convenience, we replace those parameters with parameters for P (X,Z). As mentioned at the end of Section 2, such reparameterization does not affect the effective dimension de(M). To reflect the reparameterization, the edge between X and Z is not directed in Figure 2.\nSuppose P (X,Z) has k0 parameters \u03b8 (0) 1 , \u03b8 (0) 2 , . . . , \u03b8 (0) k0 . Suppose the conditional distri-\nbutions of variables in the X-branch consists of k1 parameters \u03b8 (1) 1 , \u03b8 (1) 2 , . . . , \u03b8 (1) k1 and the conditional distributions of variables in the Z-branch consists of k2 parameters \u03b8 (2) 1 , \u03b8 (2) 2 , . . . , \u03b8 (2) k2 . For convenience we will sometimes refer to those three groups of parameters using three vectors ~\u03b8(0), ~\u03b8(1) and ~\u03b8(2) respectively.\nIn the following, we will define two other HLC models M1 and M2 starting from M and establish a relationship between their effective dimensions and the effective dimension of M . In this context, M , M1, and M2 are regarded purely as Mathematical objects. The semantics of their variables are of no concern. In particular, a variable H that is latent\nin M might be designated to be observed in M1 or M2 as part of the definition of those Mathematical objects.\nWe obtain a Bayesian network model B1 from M by deleting the Z-branch. Strictly speaking B1 is not Bayesian network due to the parameterization it inherits from M : instead of probability tables P (X) and P (Z|X), we have table P (X,Z). But P (X) and P (Z|X) can readily be obtained from P (X,Z). With this in mind, we view B1 as a Bayesian network. This network is obviously tree-structured. It\u2019s leaf variables include those in the set O and the variable Z. We define M1 to be the HLC model that share the same structure as B1 and where the variable Z and all the variables in O are observed. The parameters of M1 are ~\u03b8(0) and ~\u03b8(1).\nSimilarly let B2 be the Bayesian network model obtained from M by deleting the Xbranch. It is a tree-structure and its leaf variables include those in Y and the variable X. We define M2 to be the HLC model that share the same structure as B2 and where the variable X and all the variables in Y are observed. The parameters of M2 are ~\u03b8 (0) and ~\u03b8(2).\nTheorem 1 Suppose M is a regular HLC model that contains two or more latent nodes. Then the two HLC models M1 and M2 defined in the text are also regular. Moreover,\nde(M) = de(M1)+de(M2)\u2212[ds(M1)+ds(M2)\u2212ds(M)]. (2)"}, {"heading": "In words, the effective dimension of M equals the sum of the effective dimensions of M1 and M2 minus the number of common parameters that M1 and M2 share.", "text": "To appreciate the significance of this theorem, consider the task of computing the effective dimension of a regular HLC model that contains two or more latent nodes. By\nrepeatedly applying the theorem, we can reduce the task into subtasks of calculating effective dimensions of LC models. As an example, consider the HLC model depicted by the picture on the left in Figure 3. Theorem 1 allows us to, for the purpose of computing its effective dimension, decompose the HLC model into five LC models, which are shown on the right in Figure 3.\nHow might one compute the effective dimension of an LC model? One way is to use the algorithm suggested by Geiger et al. (1996). The algorithm first symbolically computes the Jacobian matrix, which is possible due to Assumption 1. Then it randomly assigns values to the parameters, resulting a numerical matrix. The rank of the numerical matrix is computed by diagonalization. Because the rank of Jacobian matrix equals the effective dimension of the LC model almost everywhere, we get the regular rank with probability one. This algorithm has recently been implemented by Rusakov and Geiger (2003). Kocka and Zhang (2002) suggest an alternative algorithm that computes an upper bound. The algorithm is fast and has been empirically shown to produce extremely tight bounds.\nGoing back to our example, the effective dimension of the LC models for X1, X2, X3, X4 and X5 are 26, 23, 23, 34 and 17 respectively. Thus the effective dimension of the HLC model in Figure 3 is 26+23+34+23+17\u2212(5\u22173\u22121)\u2212(3\u22176\u22121)\u2212(6\u22173\u22121)\u2212(3\u22175\u22121) = 61. In contrast, the standard dimension of the model is 5+6\u22172+6\u22172+6\u22172+3\u22174+5\u22175+5+3\u22174+5\u22172+5 = 110."}, {"heading": "5. Proof of Main Result", "text": "This section is devoted to the proof of Theorem 1. We begin with some properties of Jacobian matrices of Bayesian network models."}, {"heading": "5.1 Properties of Jacobian Matrices", "text": "Consider the Jacobian matrix JM of a Bayesian network model M . It is a matrix parameterized by the parameters ~\u03b8 of M . Let v1, v2, . . . , vm be column vectors of JM .\nLemma 1 A number of column vectors v1, v2, . . . , vm of the Jacobian matrix JM are either linearly dependent everywhere or linearly independent almost everywhere. They are linearly dependent everywhere if and only if there exists at least one column vector vj that can be expressed as a linear combination of other column vectors everywhere.\nProof: Consider diagonalizing the following transposed matrix:\n[v1, v2, . . . , vm] T .\nAccording to Assumption 1, elements of the matrix are polynomials (of ~\u03b8). Hence we would multiply rows with polynomials or fraction of polynomials. Of course, we need also to add one row to another row. At the end of the process, we get a diagonal matrix whose nonzero elements are polynomials or fractions of polynomials. Suppose there are k nonzero rows and suppose they correspond to v1, v2, . . . , vk.\nBecause elements of the diagonalized matrix are polynomials or fractions of polynomials, they are well-defined 2 and nonzero almost everywhere (i.e. for almost all values of ~\u03b8). If k=m, then the m vectors are linearly independent of each other almost everywhere.\n2. A fraction is not well defined if the denominator is zero.\nIf k<m, there exist, for each j (k<j\u2264m), polynomials or fractions of polynomials ci (1\u2264i\u2264k) such that\nvj = k\u2211\ni=1\ncivi. (3)\nThe coefficients ci\u2019s can be determined by tracing the diagonalization process. So vj can be expressed as a linear combination of {vi|i = 1, . . . , k} everywhere 3. \u2737\nAlthough it might sound trivial, this lemma is actually quite interesting. This is because JM is a parameterized matrix. The first part, for example, implies that there do not exist two subspaces of the parameter space that both have nonzero measures such that the m vectors are linearly independent in one subspace while linearly dependent in the other.\nIf m is the total number of column vectors of JM , we get the following lemma:\nLemma 2 In the Jacobian matrix JM , there exists a collection of column vectors that form a basis of its column space almost everywhere. The number of vectors in the collection equals to the regular rank of the matrix. Moreover, the collection can be chosen to include any given set of column vectors that are linearly independent almost everywhere.\nProof: The first part has already been proved. The second part follows from the definition of regular rank. The last part is true because we could start the diagonalization process with the transpose of the vectors in the set on the top of the matrix. \u2737"}, {"heading": "5.2 Proof of Theorem 1", "text": "We now set out to prove Theorem 1. It is straightforward to verify that the HLC models M1 and M2 are regular. So it suffices to prove equation (2). This is what we do in the rest of this section.\nThe set of observed variables in M is O \u222a Y, the set of observed variables in M1 is O \u222a {Z} and the set of observed variables in M2 is Y \u222a {X}. Hence the Jacobian matrices of models M , M1, and M2 can be respectively written as follows:\nJM = [ \u2202P (O,Y)\n\u2202\u03b8 (0) 1\n, . . . , \u2202P (O,Y)\n\u2202\u03b8 (0) k0\n; \u2202P (O,Y)\n\u2202\u03b8 (1) 1\n, . . . , \u2202P (O,Y)\n\u2202\u03b8 (1) k1\n; \u2202P (O,Y)\n\u2202\u03b8 (2) 1\n, . . . , \u2202P (O,Y)\n\u2202\u03b8 (2) k2\n]\nJM1 = [ \u2202P (O, Z)\n\u2202\u03b8 (0) 1\n, . . . , \u2202P (O, Z)\n\u2202\u03b8 (0) k0\n; \u2202P (O, Z)\n\u2202\u03b8 (1) 1\n, . . . , \u2202P (O, Z)\n\u2202\u03b8 (1) k1\n]\nJM2 = [ \u2202P (X,Y)\n\u2202\u03b8 (0) 1\n, . . . , \u2202P (X,Y)\n\u2202\u03b8 (0) k0\n; \u2202P (X,Y)\n\u2202\u03b8 (2) 1\n, . . . , \u2202P (X,Y)\n\u2202\u03b8 (2) k2\n]\n3. There is a subtle point here. Being fractions of polynomials of ~\u03b8, the ci\u2019s might be undefined for some values of ~\u03b8. So from equation (3) alone, we cannot conclude that vj linearly depends on {vi|i = 1, . . . , k} everywhere.\nThe conclusion is nonetheless true for two reasons. First the set of ~\u03b8 values where the ci\u2019s are undefined has measure zero. Second, if vj does not linearly depend on {vi|i = 1, . . . , k} at one value of ~\u03b8, then the same would be true in a sufficiently small and nonetheless measure-positive ball around that value.\nIt is clear that there is a one-to-one correspondence between the first k0+k1 column vectors of JM with the column vectors of JM1 and there is a one-to-one correspondence between the first k0 and the last k2 column vectors of JM with the column vectors of JM2 . We will first show\nClaim 1: The first k0 vectors of JM (JM1 or JM2) are linearly independent almost everywhere.\nTogether with Lemma 2, Claim 1 implies that there is a collection of column vectors in JM1 that includes the first k0 vectors and that is a basis of the column space of JM1 almost everywhere. In particular, this implies that de(M1)\u2265k0. Suppose de(M1)=k0+r. Without loss of generality, suppose the basis vectors are\n\u2202P (O, Z)\n\u2202\u03b8 (0) 1\n, . . . , \u2202P (O, Z)\n\u2202\u03b8 (0) k0\n; \u2202P (O, Z)\n\u2202\u03b8 (1) 1\n, . . . , \u2202P (O, Z)\n\u2202\u03b8 (1) r\n. (4)\nBy symmetry, we can assume that de(M2)=k0+s where s\u22650 and that the following column vectors form a basis for JM2 almost everywhere:\n\u2202P (X,Y)\n\u2202\u03b8 (0) 1\n, . . . , \u2202P (X,Y)\n\u2202\u03b8 (0) k0\n; \u2202P (X,Y)\n\u2202\u03b8 (2) 1\n, . . . , \u2202P (X,Y)\n\u2202\u03b8 (2) s\n. (5)\nNow consider the following list of vectors in JM :\n\u2202P (O,Y)\n\u2202\u03b8 (0) 1\n, . . . , \u2202P (O,Y)\n\u2202\u03b8 (0) k0\n; \u2202P (O,Y)\n\u2202\u03b8 (1) 1\n, . . . , \u2202P (O,Y)\n\u2202\u03b8 (1) r\n; \u2202P (O,Y)\n\u2202\u03b8 (2) 1\n, . . . , \u2202P (O,Y)\n\u2202\u03b8 (2) s\n. (6)\nWe will show\nClaim 2: All column vectors of JM linearly depend on the vectors listed in (6) everywhere.\nClaim 3: The vectors listed in (6) are linearly independent almost everywhere.\nThose two claims imply that the vectors listed in (6) form a basis of the column space of JM almost everywhere. Therefore\nde(M) = k0+r+s = de(M1)+de(M2)\u2212k0.\nIt is clear that k0=ds(M1)+ds(M2)\u2212ds(M). Therefore Theorem 1 is proved. \u2737"}, {"heading": "5.3 Proof of Claim 1", "text": "Lemma 3 Let Z be a latent node in an HLC model M and Y be the set of the observed nodes in the subtree rooted at Z. If M is regular, then we can set conditional distributions of nodes in the subtree in such a way that they encode an injective mapping \u03c1 from \u2126Z to \u2126Y in the sense that P (Y=\u03c1(z)|Z=z) = 1 for all z \u2208 \u2126Z .\nProof: We prove this lemma by induction on the number of latent nodes in the subtree rooted at Z. First consider the case when there is only one latent node, namely Z. In this case, Z is the parent of all nodes in Y. Enumerate all these nodes as Y1, Y2, . . . , Yk. Because M is regular, we have |Z| \u2264 \u220fk i=1 |Yi|. Hence we can define an injective mapping \u03c1 from \u2126Z to \u2126Y = \u220fk\ni=1 \u2126Yi . For each state z of Z, \u03c1(z) can be written as y = (y1, y2, . . . , yk), where yi is a state of Yi. Now if we set\nP (Yi=yi|Z=z) = 1,\nthen P (Y=\u03c1(z)|Z=z) = 1. Now consider the case when there are at least two hidden nodes in the subtree rooted at Z. Let W be one such latent node that has no latent node descendants. Let Y(1) be the set of observed nodes in the subtree rooted at W and Y(2)=Y\\Y(1). By the induction hypothesis, we can parameterize the subtree rooted at W in such a way that it encodes an injective mapping from \u2126W to \u2126Y(1) . Moreover, if all nodes below W are removed from M , M remains a regular HLC model. In that model, we can parameterize the subtree rooted at Z in such a way that it encodes an injective mapping from \u2126Z to \u2126(W,Y(2)) = \u2126W \u00d7 \u2126Y(2) . Together, those two facts prove the lemma. \u2737\nCorollary 1 Let Z be a latent node in an HLC model M . Suppose Z have a latent neighbor X. Let Y be the set of the observed nodes separated from X by Z. If M is regular, then we can set probability distributions of nodes separated from X by Z in such a way that they encode an injective mapping \u03c1 from \u2126Z to \u2126Y in the sense that P (Y=\u03c1(z)|Z=z) = 1 for all z \u2208 \u2126Z .\nProof: The corollary follows readily from Lemma 3 and the property of the root-walking operation (Zhang, 2002). \u2737\nProof of Claim 1: Consider the following matrix\n[ \u2202P (X,Z)\n\u2202\u03b8 (0) 1\n. . . , \u2202P (X,Z)\n\u2202\u03b8 (0) k0\n] (7)\nBecause \u03b8 (0) 1 , \u03b8 (0) 2 , . . . , \u03b8 (0) k0\nare the parameters for the joint distribution P (X,Z), this matrix is the identity matrix if the rows are properly arranged. So its column vectors are linearly independent almost everywhere.\nNow consider the first k0 column vectors of JM : \u2202P (O,Y)/\u2202\u03b8 (0) 1 , . . . , \u2202P (O,Y)/\u2202\u03b8 (0) k0\n. They must be linearly independent almost everywhere. If not, one of the vectors, say \u2202P (O,Y)/\u2202\u03b8 (0) k0\n, would linearly depend on the rest everywhere according to Lemma 1. Observe that for any i (1\u2264i\u2264k0),\n\u2202P (O,Y)\n\u2202\u03b8 (0) i\n= \u2211\nX,Z\nP (O|X)P (Y|Z) \u2202P (X,Z)\n\u2202\u03b8 (0) i\n.\nChoose P (O|X) and P (Y|Z) as in Corollary 1. The vector \u2202P (O,Y)/\u2202\u03b8 (0) i might contain zero elements. If we remove the zero elements, what remains of the vector is identical to \u2202P (X,Z)/\u2202\u03b8 (0) i . So we can conclude that \u2202P (X,Z)/\u2202\u03b8 (0) k0 linearly depends on\n\u2202P (X,Z)/\u2202\u03b8 (0) 1 . . . , \u2202P (X,Z)/\u2202\u03b8 (0) k0\u22121\neverywhere, which contradicts the conclusion of the previous paragraph. Hence the first k0 vectors of JM must be linearly independent almost everywhere.\nIt is evident that, using similar arguments, we can also show that the first k0 vectors of JM1 (JM2) are linearly independent almost everywhere. Claim 1 is therefore proved. \u2737"}, {"heading": "5.4 Proof of Claim 2", "text": "Every column vector of JM1 linearly depends on vectors listed in (4) everywhere. Observe that\n\u2202P (O,Y)\n\u2202\u03b8 (0) i\n= \u2211\nZ\nP (Y|Z) \u2202P (O, Z)\n\u2202\u03b8 (0) i\n, i = 1, . . . , k0\n\u2202P (O,Y)\n\u2202\u03b8 (1) i\n= \u2211\nZ\nP (Y|Z) \u2202P (O, Z)\n\u2202\u03b8 (1) i\n, i = 1, . . . , k1.\nTherefore every column vector of JM that corresponds to vectors in JM1 linearly depends on the first k0+r vectors listed in (6) everywhere.\nBy symmetry, every column vector of JM that corresponds to vectors in JM2 linearly depends on the first k0 and the last s vectors listed in (6) everywhere. The claim is proved. \u2737"}, {"heading": "5.5 Proof of Claim 3", "text": "We prove this claim by contradiction. Assume the vectors listed in (6) were not linearly independent almost everywhere. According to Lemma 1, one of them, say v, must linearly depend on the rest everywhere. Because of Claim 1 and Lemma 2, we can assume that v is among the last r+s vectors. Without loss of generality, we assume that v is \u2202P (O,Y)/\u2202\u03b8 (2) s . Then for any value of ~\u03b8, there exist real numbers ci (1\u2264i\u2264k0), c (1) i (1\u2264i\u2264r), and c (2) i (1\u2264i\u2264s\u22121) such that\n\u2202P (O,Y)\n\u2202\u03b8 (2) s\n= k0\u2211\ni=1\nci \u2202P (O,Y)\n\u2202\u03b8 (0) i\n+ r\u2211\ni=1\nc (1) i\n\u2202P (O,Y)\n\u2202\u03b8 (1) i\n+ s\u22121\u2211\ni=1\nc (2) i\n\u2202P (O,Y)\n\u2202\u03b8 (2) i\n.\nNote that in the last term on the right hand side, i runs from 1 to s\u22121.\nThe parameter vector ~\u03b8 consists of three subvectors ~\u03b8(0), ~\u03b8(1) and ~\u03b8(2). Set the parameters ~\u03b8(1) (for the X-branch) as in Lemma 3. Then there exists an injective mapping \u03c1 from \u2126X to \u2126O such that\nP (O=\u03c1(x)|X=x) = 1 for all x \u2208 \u2126X . (8)\nFor each of the vectors in (6), consider the subvector consisting only of elements for those states of O that are the images of states of X under the mapping \u03c1. Such subvectors will be denoted by \u2202P (OX ,Y)/\u2202\u03b8 (0) i , \u2202P (OX ,Y)/\u2202\u03b8 (1) i , and \u2202P (OX ,Y)/\u2202\u03b8 (2) i . For any values of ~\u03b8(0) and ~\u03b8(2), we still have\n\u2202P (OX ,Y)\n\u2202\u03b8 (2) s\n= k0\u2211\ni=1\nci \u2202P (OX ,Y)\n\u2202\u03b8 (0) i\n+ r\u2211\ni=1\nc (1) i\n\u2202P (OX ,Y)\n\u2202\u03b8 (1) i\n+ s\u22121\u2211\ni=1\nc (2) i\n\u2202P (OX ,Y)\n\u2202\u03b8 (2) i\n. (9)\nConsider the first two terms on the right hand side:\nk0\u2211\ni=1\nci \u2202P (OX ,Y)\n\u2202\u03b8 (0) i\n+ r\u2211\ni=1\nc (1) i\n\u2202P (OX ,Y)\n\u2202\u03b8 (1) i\n= k0\u2211\ni=1\nci \u2211\nZ\nP (Y|Z) \u2202P (OX , Z)\n\u2202\u03b8 (0) i\n+ r\u2211\ni=1\nc (1) i\n\u2211\nZ\nP (Y|Z) \u2202P (OX , Z)\n\u2202\u03b8 (1) i\n= \u2211\nZ\nP (Y|Z){ k0\u2211\ni=1\nci \u2202P (OX , Z)\n\u2202\u03b8 (0) i\n+ r\u2211\ni=1\nc (1) i\n\u2202P (OX , Z)\n\u2202\u03b8 (1) i\n}\nBecause of (8) and the fact that P (O, Z) = \u2211\nX P (X,Z)P (O|X), the column vector\n\u2202P (OX , Z)/\u2202\u03b8 (0) i is identical to the vector \u2202P (X,Z)/\u2202\u03b8 (0) i . As we have argued when proving Claim 1, the vectors {\u2202P (X,Z)/\u2202\u03b8 (0) i |i=1, . . . , k0} constitute a basis for the k0-dimensional Euclidian space. This implies that, each of the vectors \u2202P (OX , Z)/\u2202\u03b8 (1) i can be represented as a linear combination of the vectors {\u2202P (OX , Z)/\u2202\u03b8 (0) i |i = 1, . . . , k0}. Consequently, there exist c\u2032i (1\u2264i\u2264k0) such that\nk0\u2211\ni=1\nci \u2202P (OX , Z)\n\u2202\u03b8 (0) i\n+ r\u2211\ni=1\nc (1) i\n\u2202P (OX , Z)\n\u2202\u03b8 (1) i\n= k0\u2211\ni=1\nc\u2032i \u2202P (OX , Z)\n\u2202\u03b8 (0) i\nHence\nk0\u2211\ni=1\nci \u2202P (OX ,Y)\n\u2202\u03b8 (0) i\n+ r\u2211\ni=1\nc (1) i\n\u2202P (OX ,Y)\n\u2202\u03b8 (1) i\n= k0\u2211\ni=1\nc\u2032i \u2202P (OX ,Y)\n\u2202\u03b8 (0) i\nCombining this equation with equation (9), we get\n\u2202P (OX ,Y)\n\u2202\u03b8 (2) s\n= k0\u2211\ni=1\nc\u2032i \u2202P (OX ,Y)\n\u2202\u03b8 (0) i\n+ s\u22121\u2211\ni=1\nc (2) i\n\u2202P (OX ,Y)\n\u2202\u03b8 (2) i\n.\nBecause of (8) and the fact that the fact that P (O,Y) = \u2211\nX P (X,Y)P (O|X), the column\nvector \u2202P (OX ,Y)/\u2202\u03b8 (1) i is identical to the vector \u2202P (X,Y)/\u2202\u03b8 (1) i and the column vector \u2202P (OX ,Y)/\u2202\u03b8 (2) i is identical to the vector \u2202P (X,Y)/\u2202\u03b8 (2) i . Hence\n\u2202P (X,Y)\n\u2202\u03b8 (2) s\n= k0\u2211\ni=1\nc\u2032i \u2202P (X,Y)\n\u2202\u03b8 (0) i\n+ s\u22121\u2211\ni=1\nc (2) i\n\u2202P (X,Y)\n\u2202\u03b8 (2) i\n.\nThis contradicts the fact that the vectors in the equation form a basis for the column space of JM2 almost everywhere (see (5) in Section 5.2) Therefore, Claim 3 must be true. \u2737"}, {"heading": "6. Effective Dimensions of Trees", "text": "Let us use the term tree model to refer to Markov random fields on undirected trees over a finite number of random variables. If we root a tree model at any of its nodes, we get a tree-structured Bayesian network model. In a tree model, define leaf nodes be those that have only one neighbor. An HLC model is a tree model where all leaf nodes are observed while all others are latent.\nIt turns out that Theorem 1 enables us to compute the effective dimension of any tree model. Consider an arbitrary tree model. If some of its leaf nodes are latent, we can remove such nodes without affecting its effective dimension.\nAfter removing latent leaf nodes, all the leaf nodes are observed. If some non-leaf nodes are also observed, we can decompose the model into submodels at any observed non-leaf node. The following theorem tells us how the model and the submodels are related in terms of effective dimensions.\nTheorem 2 Suppose Y is an observed non-leaf node in a tree model M . If M decomposes at Y into k submodels M1, . . . , Mk, then\nde(M) = k\u2211\ni=1\nde(Mi) \u2212 (k \u2212 1)(|Y | \u2212 1).\nAfter all possible decompositions, the final submodels either do not contain latent nodes or are HLC models. Effective dimensions of submodels with no latent variables are simply their standard dimensions. If an HLC submodel is irregular, we make it regular by applying the transformation mentioned at the end of Section 3.2. The transformation does not affect the effective dimensions of the submodels. Finally, effective dimensions of regular HLC submodels can be computed using Theorem 1.\nProof of Theorem 2: It is possible to prove this theorem starting from the Jacobian matrix. Here we take a less formal but more revealing approach.\nIt suffices to consider case of k being 2. The two submodels M1 and M2 share only one node, namely Y . Let O1 and O2 be respectively the sets of observed nodes in those two submodels excluding Y . Root M at Y . Then we have\nP (Y,O1,O2)P (Y ) = P (O1, Y )P (O2, Y ).\nLet ~\u03b80 be the set of parameters in the distribution P (Y ), ~\u03b81 and ~\u03b82 be respectively the sets of parameters in the conditional probability distributions of nodes in M1 and M2. Consider fixing ~\u03b80 and letting ~\u03b81 and ~\u03b82 vary. In this case, the space spanned by P (Y ) consists of only one vector, namely ~\u03b80 itself. Moreover, there is a one-to-one correspondence between vectors in the space spanned by P (Y,O1,O2) and vectors in the Cartesian product of the spaces spanned by P (O1, Y ) and P (O2, Y ). Now let ~\u03b80 vary. This adds |Y |\u22121 dimensions to each of the four spaces spanned by P (Y,O1,O2), P (Y ), P (O1, Y ), and P (O2, Y ). Consequently, we have\nde(M) = de(M1) + de(M2) \u2212 (|Y | \u2212 1).\nThe theorem is proved. \u2737"}, {"heading": "7. Concluding Remarks", "text": "In this paper we study the effective dimensions of HLC models. The work is motivated by empirical evidence that the BIC behaves quite well when used with several hill-climbing algorithms for learning HLC models and that the BICe score sometimes leads to better model selection than the BIC score. We have proved a theorem that relates the effective dimension of an HLC model to the effective dimensions of two other HLC models that contain fewer latent variables. Repeated application of the theorem allows one to reduce the task of computing the effective dimension of an HLC model to subtasks of computing effective dimensions of LC models. This makes it computationally feasible to compute the effective dimensions of large HLC models. In addition, we have proved a theorem about effective dimensions of general tree models. This and our main theorem allows one to compute the effective dimension of arbitrary tree models."}, {"heading": "Acknowledgements", "text": "This work was initiated when the authors were visiting Department of Computer Science, Aalborg University, Denmark. We thank Poul S. Eriksen, Finn V. Jensen, Jiri Vomlel, Marta Vomlelova, Thomas D. Nielsen, Olav Bangso, Jose Pena, Kristian G. Olesen. We are also grateful to the annonymous reviewers whose comments have helped us greatly in improving this paper. Research on this paper was partially supported by GA CR grant 201/02/1269 and by Hong Kong Research Grant Council under grant HKUST6088/01E."}], "references": [{"title": "A new look at the statistical model identification", "author": ["H. Akaike"], "venue": "IEEE Trans. Autom. Contr.,", "citeRegEx": "Akaike,? \\Q1974\\E", "shortCiteRegEx": "Akaike", "year": 1974}, {"title": "Latent variable models and factor analysis, 2nd edition", "author": ["D.J. Bartholomew", "M. Knott"], "venue": "Kendall\u2019s Library of Statistics", "citeRegEx": "Bartholomew and Knott,? \\Q1999\\E", "shortCiteRegEx": "Bartholomew and Knott", "year": 1999}, {"title": "Bayesian classification (AutoClass): Theory and results", "author": ["P. Cheeseman", "J. Stutz"], "venue": "Advancesin Knowledge Discovery and Data Mining,", "citeRegEx": "Cheeseman and Stutz,? \\Q1995\\E", "shortCiteRegEx": "Cheeseman and Stutz", "year": 1995}, {"title": "Efficient Approximations for the Marginal Likelihood of Bayesian Networks with Hidden variables", "author": ["M. Chickering D", "D. Heckerman"], "venue": "Machine Learning,", "citeRegEx": "D. and Heckerman,? \\Q1997\\E", "shortCiteRegEx": "D. and Heckerman", "year": 1997}, {"title": "Probabilistic networks and expert", "author": ["R.G. Cowell", "A.P. Dawid", "S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": null, "citeRegEx": "Cowell et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Cowell et al\\.", "year": 1999}, {"title": "Dimension correction for hierarchical latent class models", "author": ["T. Kocka", "N.L. Zhang"], "venue": "in Proc. of the 18th Conference on Uncertainty in Artificial Intelligence (UAI-02)", "citeRegEx": "Kocka and Zhang,? \\Q2002\\E", "shortCiteRegEx": "Kocka and Zhang", "year": 2002}, {"title": "Asymptotic model selection for directed networks with hidden variables", "author": ["D. Geiger", "D. Heckerman", "C. Meek"], "venue": "In Proc. of the 12th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Geiger et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Geiger et al\\.", "year": 1996}, {"title": "Exploratory latent structure analysis using both identifiable and unidentifiable", "author": ["L.A. Goodman"], "venue": "models. Biometrika,", "citeRegEx": "Goodman,? \\Q1974\\E", "shortCiteRegEx": "Goodman", "year": 1974}, {"title": "Latent structure analysis", "author": ["P.F. Lazarsfeld", "N.W. Henry"], "venue": null, "citeRegEx": "Lazarsfeld and Henry,? \\Q1968\\E", "shortCiteRegEx": "Lazarsfeld and Henry", "year": 1968}, {"title": "Asymptotic model selection for Naive Bayesian networks. UAI-02", "author": ["D. Rusakov", "D. Geiger"], "venue": null, "citeRegEx": "Rusakov and Geiger,? \\Q2002\\E", "shortCiteRegEx": "Rusakov and Geiger", "year": 2002}, {"title": "Automated analytic asymptotic evaluation of marginal likelihood for latent models. UAI-03", "author": ["D. Rusakov", "D. Geiger"], "venue": null, "citeRegEx": "Rusakov and Geiger,? \\Q2003\\E", "shortCiteRegEx": "Rusakov and Geiger", "year": 2003}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "In Annals of Statistics,", "citeRegEx": "Schwarz,? \\Q1978\\E", "shortCiteRegEx": "Schwarz", "year": 1978}, {"title": "On the geometry of Bayesian graphical models with hidden variables", "author": ["R. Settimi", "J.Q. Smith"], "venue": "In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Settimi and Smith,? \\Q1998\\E", "shortCiteRegEx": "Settimi and Smith", "year": 1998}, {"title": "Geometry, moments and Bayesian networks with hidden variables", "author": ["R. Settimi", "J.Q. Smith"], "venue": "In Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Settimi and Smith,? \\Q1999\\E", "shortCiteRegEx": "Settimi and Smith", "year": 1999}, {"title": "Hierarchical latent class models for cluster analysis", "author": ["L. Zhang N"], "venue": null, "citeRegEx": "N.,? \\Q2002\\E", "shortCiteRegEx": "N.", "year": 2002}, {"title": "Learning hierarchical latent class models", "author": ["N.L. Zhang", "T. Kocka", "G. Karciauskas", "F.V. Jensen"], "venue": "Technical Report HKUST-CS03-01,", "citeRegEx": "Zhang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2003}, {"title": "Structural EM for Hierarchical Latent Class Models", "author": ["N.L. Zhang"], "venue": "Technical Report HKUST-CS03-06,", "citeRegEx": "Zhang,? \\Q2003\\E", "shortCiteRegEx": "Zhang", "year": 2003}, {"title": "Hierarchical latent class models for cluster analysis", "author": ["N.L. Zhang"], "venue": "Journal of Machine Learning Research, to appear", "citeRegEx": "Zhang,? \\Q2003\\E", "shortCiteRegEx": "Zhang", "year": 2003}], "referenceMentions": [{"referenceID": 8, "context": "They generalize latent class models (Lazarsfeld and Henry, 1968) and were first identified as a potentially useful class of Bayesian networks by Pearl (1988).", "startOffset": 36, "endOffset": 64}, {"referenceID": 11, "context": "The BIC score (Schwarz, 1978) is a popular metric that researchers use to select among Bayesian network models.", "startOffset": 14, "endOffset": 29}, {"referenceID": 11, "context": "When all variables are observed, the BIC score is an asymptotic approximation of (the logarithm) of the marginal likelihood (Schwarz, 1978).", "startOffset": 124, "endOffset": 139}, {"referenceID": 6, "context": "When latent variables are present, the BIC score is no longer an asymptotic approximation of the marginal likelihood (Geiger et al., 1996).", "startOffset": 117, "endOffset": 138}, {"referenceID": 9, "context": "In fact if we replace standard model dimension with effective model dimension in the BIC score, the resulting scoring function, called the BICe score, is an asymptotic approximation of the marginal likelihood almost everywhere except for some singular points (Rusakov and Geiger, 2002).", "startOffset": 259, "endOffset": 285}, {"referenceID": 7, "context": "They generalize latent class models (Lazarsfeld and Henry, 1968) and were first identified as a potentially useful class of Bayesian networks by Pearl (1988). We are concerned with learning HLC models from data.", "startOffset": 37, "endOffset": 158}, {"referenceID": 15, "context": "There are three related searchbased algorithms for learning HLC models, namely double hill-climbing (DHC) (Zhang, 2002), single hill-climbing (SHC) (Zhang et al., 2003), and heuristic SHC (HSHC) (Zhang, 2003).", "startOffset": 148, "endOffset": 168}, {"referenceID": 16, "context": ", 2003), and heuristic SHC (HSHC) (Zhang, 2003).", "startOffset": 34, "endOffset": 47}, {"referenceID": 0, "context": "In the absence of a theoretically well justified model selection criterion, Zhang (2002) tested DHC with four existing scoring functions, namely the AIC score (Akaike, 1974), the BIC score, the Cheeseman-Stutz (CS) score (Cheeseman and Stutz, 1995), and the holdout logarithmic score (HLS)(Cowell et al.", "startOffset": 159, "endOffset": 173}, {"referenceID": 2, "context": "In the absence of a theoretically well justified model selection criterion, Zhang (2002) tested DHC with four existing scoring functions, namely the AIC score (Akaike, 1974), the BIC score, the Cheeseman-Stutz (CS) score (Cheeseman and Stutz, 1995), and the holdout logarithmic score (HLS)(Cowell et al.", "startOffset": 221, "endOffset": 248}, {"referenceID": 4, "context": "In the absence of a theoretically well justified model selection criterion, Zhang (2002) tested DHC with four existing scoring functions, namely the AIC score (Akaike, 1974), the BIC score, the Cheeseman-Stutz (CS) score (Cheeseman and Stutz, 1995), and the holdout logarithmic score (HLS)(Cowell et al., 1999).", "startOffset": 289, "endOffset": 310}, {"referenceID": 0, "context": "In the absence of a theoretically well justified model selection criterion, Zhang (2002) tested DHC with four existing scoring functions, namely the AIC score (Akaike, 1974), the BIC score, the Cheeseman-Stutz (CS) score (Cheeseman and Stutz, 1995), and the holdout logarithmic score (HLS)(Cowell et al., 1999). Both real-world and synthetic data were used. On the real-world data, BIC and CS have enabled DHC to find models that are regarded as the best by domain experts. On the synthetic data, BIC and CS have enabled DHC to find models that either are identical to or resemble closely the true generative models. When coupled with AIC and HLS, on the other hand, DHC performed significantly worse. SHC and HSHC were tested on synthetic data sampled from fairly large HLC models (as much as 28 nodes). Only BIC was used in those tests. In all cases, BIC has enabled SHC and HSHC to find models that either are identical to or resemble closely the true generative models. Those empirical results not only indicate that the algorithms perform well, but also suggest that the BIC is a reasonable scoring function to use for learning HLC models. The experiments also reveal that model selection can sometimes be improved if the BICe score is used instead of the BIC score. We will explain this in detail in Section 3 In order to use the BICe score in practice, we need a way to compute effective dimensions. This is not a trivial task. The effective dimension of an HLC model is the rank of the Jacobian matrix of the mapping from the parameters of the model to the parameters of the joint distribution of the observed variables. The number of rows in the Jacobian matrix increases exponentially with the number of observed variables. The construction of the Jacobian matrix and the calculation of its rank are both computationally demanding. Moreover they have to be done algebraically or with very high numerical precision to avoid degenerate cases. The necessary precision grows with the size of the matrix. Settimi and Smith (1998, 1999) studied effective dimensions for two classes of models: trees with binary variables and latent class (LC) models with two observed variables. They have obtained a complete characterization of these two classes. Geiger et al. (1996) computed the effective dimensions of a number of models.", "startOffset": 160, "endOffset": 2273}, {"referenceID": 0, "context": "In the absence of a theoretically well justified model selection criterion, Zhang (2002) tested DHC with four existing scoring functions, namely the AIC score (Akaike, 1974), the BIC score, the Cheeseman-Stutz (CS) score (Cheeseman and Stutz, 1995), and the holdout logarithmic score (HLS)(Cowell et al., 1999). Both real-world and synthetic data were used. On the real-world data, BIC and CS have enabled DHC to find models that are regarded as the best by domain experts. On the synthetic data, BIC and CS have enabled DHC to find models that either are identical to or resemble closely the true generative models. When coupled with AIC and HLS, on the other hand, DHC performed significantly worse. SHC and HSHC were tested on synthetic data sampled from fairly large HLC models (as much as 28 nodes). Only BIC was used in those tests. In all cases, BIC has enabled SHC and HSHC to find models that either are identical to or resemble closely the true generative models. Those empirical results not only indicate that the algorithms perform well, but also suggest that the BIC is a reasonable scoring function to use for learning HLC models. The experiments also reveal that model selection can sometimes be improved if the BICe score is used instead of the BIC score. We will explain this in detail in Section 3 In order to use the BICe score in practice, we need a way to compute effective dimensions. This is not a trivial task. The effective dimension of an HLC model is the rank of the Jacobian matrix of the mapping from the parameters of the model to the parameters of the joint distribution of the observed variables. The number of rows in the Jacobian matrix increases exponentially with the number of observed variables. The construction of the Jacobian matrix and the calculation of its rank are both computationally demanding. Moreover they have to be done algebraically or with very high numerical precision to avoid degenerate cases. The necessary precision grows with the size of the matrix. Settimi and Smith (1998, 1999) studied effective dimensions for two classes of models: trees with binary variables and latent class (LC) models with two observed variables. They have obtained a complete characterization of these two classes. Geiger et al. (1996) computed the effective dimensions of a number of models. They conjectured that it is rare for the effective and standard dimensions of an LC model to differ. As a matter of fact, they found only one such model. Kocka and Zhang (2002) found quite a number of LC models whose effective and standard dimensions differ.", "startOffset": 160, "endOffset": 2507}, {"referenceID": 6, "context": "The term reflects the fact that, for almost every value of ~ \u03b8, a small enough open ball around T (~ \u03b8) resembles Euclidean space of dimension d (Geiger et al., 1996).", "startOffset": 145, "endOffset": 166}, {"referenceID": 15, "context": "The definition of regularity given in this paper is slightly different from the one given in Zhang (2002). Nonetheless, the two conclusions mentioned in this paragraph remain true.", "startOffset": 93, "endOffset": 106}, {"referenceID": 6, "context": "Just as BICe is better than BIC as approximations of the marginal likelihood (Geiger et al., 1996), CSe is better than CS.", "startOffset": 77, "endOffset": 98}, {"referenceID": 5, "context": "How might one compute the effective dimension of an LC model? One way is to use the algorithm suggested by Geiger et al. (1996). The algorithm first symbolically computes the Jacobian matrix, which is possible due to Assumption 1.", "startOffset": 107, "endOffset": 128}, {"referenceID": 5, "context": "How might one compute the effective dimension of an LC model? One way is to use the algorithm suggested by Geiger et al. (1996). The algorithm first symbolically computes the Jacobian matrix, which is possible due to Assumption 1. Then it randomly assigns values to the parameters, resulting a numerical matrix. The rank of the numerical matrix is computed by diagonalization. Because the rank of Jacobian matrix equals the effective dimension of the LC model almost everywhere, we get the regular rank with probability one. This algorithm has recently been implemented by Rusakov and Geiger (2003). Kocka and Zhang (2002) suggest an alternative algorithm that computes an upper bound.", "startOffset": 107, "endOffset": 599}, {"referenceID": 5, "context": "Kocka and Zhang (2002) suggest an alternative algorithm that computes an upper bound.", "startOffset": 0, "endOffset": 23}], "year": 2011, "abstractText": "Hierarchical latent class (HLC) models are tree-structured Bayesian networks where leaf nodes are observed while internal nodes are latent. There are no theoretically well justified model selection criteria for HLC models in particular and Bayesian networks with latent nodes in general. Nonetheless, empirical studies suggest that the BIC score is a reasonable criterion to use in practice for learning HLC models. Empirical studies also suggest that sometimes model selection can be improved if standard model dimension is replaced with effective model dimension in the penalty term of the BIC score. Effective dimensions are difficult to compute. In this paper, we prove a theorem that relates the effective dimension of an HLC model to the effective dimensions of a number of latent class models. The theorem makes it computationally feasible to compute the effective dimensions of large HLC models. The theorem can also be used to compute the effective dimensions of general tree models.", "creator": "dvips(k) 5.90a Copyright 2002 Radical Eye Software"}}}