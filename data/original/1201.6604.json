{"id": "1201.6604", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2012", "title": "Gaussian Processes for Sample Efficient Reinforcement Learning with RMAX-like Exploration", "abstract": "We present an implementation of model-based online reinforcement learning (RL) for continuous domains with deterministic transitions that is specifically designed to achieve low sample complexity. To achieve low sample complexity, since the environment is unknown, an agent must intelligently balance exploration and exploitation, and must be able to rapidly generalize from observations. While in the past a number of related sample efficient RL algorithms have been proposed, to allow theoretical analysis, mainly model-learners with weak generalization capabilities were considered. Here, we separate function approximation in the model learner (which does require samples) from the interpolation in the planner (which does not require samples). For model-learning we apply Gaussian processes regression (GP) which is able to automatically adjust itself to the complexity of the problem (via Bayesian hyperparameter selection) and, in practice, often able to learn a highly accurate model from very little data. In addition, a GP provides a natural way to determine the uncertainty of its predictions, which allows us to implement the \"optimism in the face of uncertainty\" principle used to efficiently control exploration. Our method is evaluated on four common benchmark domains.", "histories": [["v1", "Tue, 31 Jan 2012 16:36:51 GMT  (307kb)", "http://arxiv.org/abs/1201.6604v1", "European Conference on Machine Learning (ECML'2010)"]], "COMMENTS": "European Conference on Machine Learning (ECML'2010)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["tobias jung", "peter stone"], "accepted": false, "id": "1201.6604"}, "pdf": {"name": "1201.6604.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["tjung@cs.utexas.edu", "pstone@cs.utexas.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 1.\n66 04\nv1 [\ncs .A\nI] 3\n1 Ja"}, {"heading": "1 Introduction", "text": "In reinforcement learning (RL), an agent interacts with an environment and attempts to choose its actions such that an externally defined performance measure, the accumulated per-step reward, is maximized over time. One defining characteristic of RL is that the environment is unknown and that the agent has to learn how to act directly from experience. In practical applications, e.g., in robotics, obtaining this experience means having a physical system interact with the physical environment in real time. Therefore, RL methods that are able to learn quickly and minimize the amount of time the robot needs to interact with the environment until good or optimal behavior is learned, are highly desirable.\nIn this paper we are interested in online RL for tasks with continuous state spaces and smooth transition dynamics that are typical for robotic control domains. Our primary goal is to have an algorithm which keeps sample complexity as low as possible."}, {"heading": "1.1 Overview of the contribution", "text": "To maximize sample efficiency, we consider online RL that is model-based in the spirit of RMAX [3], but extended to continuous state spaces similar to [1,10,5]. As in RMAX and related methods, our algorithm, GP-RMAX, consists of two parts: a model-learner and a planner. The model-learner estimates the dynamics of the environment from the sample transitions the agent experiences while interacting with the environment. The planner is used to find the best possible action, given the current model. As the predictions of the model-learner become increasingly more accurate, the actions derived become increasingly closer to optimal. To control the amount of exploration, the \u201coptimism in the face of uncertainty\u201d principle is employed which makes the agent visit unexplored states first. In our algorithm, the model-learner is implemented by Gaussian process (GP) regression; being non-parametric, GPs give us enhanced modeling flexibility. GPs allow Bayesian model selection and automatic relevance determination. In addition, GPs provide a natural way to determine the uncertainty of predictions, which allows us to implement the \u201coptimism in the face of uncertainty\u201d exploration of RMAX in a principled way. The planner uses the estimated transition function (as estimated by the model) to solve the Bellman equation via value iteration on a uniform grid.1\nThe key point of our algorithm is that we separate the steps estimating a function from samples in the model-learner from solving the Bellman equation in the planner. The rationale behind this is that, if the transition function is relatively simple, it can be estimated accurately from only few sample transitions. On the other hand, the optimal value function, due to the inclusion of the max operator, often is a complex function with sharp discontinuities. Solving the Bellman equation, however, does not require actual \u201csamples\u201d; instead, we must only be able to evaluate the Bellman operator in arbitrary points of the state space. This way, when the transition function can be learned from only a few samples, large gains in sample efficiency are possible. Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.\nConceptually, our approach is closely related to Fitted R-MAX, which was proposed in [10] and uses an instance-based approach in the model-learner, and related work in [5,1], which uses grid-based interpolation in the model-learner. The primary contribution of this paper is to use GPs instead. Doing this means we are willing to trade off theoretical analysis with practical performance. For example, unlike the recent ARL [1], for which PAC-style performance bounds could be derived (because of its grid-based implementation of model-learning), a GP is much better able to handle generalization and as a consequence can achieve much lower sample complexity.\n1 While certainly more advanced methods exist, e.g., [9,14], for our purpose here, a uniform grid is sufficient as proof of concept."}, {"heading": "1.2 Assumptions and limitations", "text": "Our approach makes the following assumptions (most of which are also made in related work, even if it is not always explicitly stated):\n\u2013 Low dimensionality of the state space. With a uniform grid, the number of grid points for solving the Bellman equation scales exponentially with the dimensionality. While more advanced methods, such as sparse grids or adaptive grids, may allow us to somewhat reduce this exponential increase, at the end they do not break the curse of dimensionality. Alternatively, one can use nonlinear function approximation; however, despite some encouraging results, it is unclear as to whether this approach would really do any better in general applications. Today, breaking the curse of dimensionality is still an open research problem. \u2013 Discrete actions. While continuous actions may be discretized, in practice, for higher dimensional action spaces this becomes infeasible. \u2013 Smooth transition function. Performing an action from states that are \u201cclose\u201d must lead to successor states that are \u201cclose\u201d. (Otherwise both the generalization in the model learner and the interpolation in the value function approximation would not work). \u2013 Deterministic transitions. This is not a fundamental requirement of our approach, since GPs can also learn noisy functions (either due to observation noise or random disturbances with small magnitude), and the Bellman operator can be evaluated in the resulting predictive distribution. Rather it is one taken for convenience. \u2013 Known reward function. Assuming that the reward function is known and only the transition function needs to be learned is what is different from most comparable work. While it is not a fundamental requirement of our approach (since we could learn the reward function as well), it is an assumption that we think is well justified: for one, reward is the performance criterion and specifies the goal. For the type of control problems we consider here, reward is always externally defined and never something that is \u201cgenerated\u201d from within the environment. Two, reward sometimes is a discontinuous function, e.g., +1 at the goal state and 0 elsewhere. Which makes it not very amenable for function approximation."}, {"heading": "2 Background: Planning when the model is exact", "text": "Consider the reinforcement learning problem for MDPs with continuous state space, finite action space, discounted reward criterion and deterministic dynamics [19]. In this section we assume that dynamics and rewards are available to the learning agent. Let state space X be a hyperrectangle in Rd (this assumption is justified if, for example, the system is a motor control task), A be the finite action space (assuming continuous controls are discretized), xt+1 = f(xt, at) be the transition function (assuming that continuous time problems are discretized in time), and r(x, a) be the reward function. For the following theoretical argument\nwe require that both transition and reward function are Lipschitz continuous in the actions; i.e., there exist constants Lf , Lr such that \u2016f(x, a)\u2212 f(x\n\u2032, a)\u2016 \u2264 Lf \u2016x\u2212 x\u2032\u2016, and |r(x, a) \u2212 r(x\u2032, a)| \u2264 Lr \u2016x\u2212 x\u2032\u2016, \u2200x, x\u2032 \u2208 X , a \u2208 A. In addition, we assume that the reward is bounded, |r(x, a)| \u2264 RMAX, \u2200x, a. Note that in practice, while the first condition, continuity in the transition function, is usually fulfilled for domains derived from physical systems, the second condition, continuity in the rewards, is often violated (e.g. in the mountain car domain, reward is 0 in the goal and \u22121 everywhere else). Despite that we find that in many of these cases the outlined procedure may still work well enough.\nFor any state x, we are interested in determining a sequence of actions a0, a1, a2, . . . such that the accumulated reward is maximized,\nV \u2217(x) := max a0,a1,...\n{ \u221e \u2211\nt=0\n\u03b3tr(xt, at) | x0 = x, xt+1 = f(xt, at) } ,\nwhere 0 < \u03b3 < 1. Using the Q-notation, where Q\u2217(x, a) := r(x, a)+\u03b3V \u2217(f(x, a)), the optimal decision policy \u03c0\u2217 is found by first solving the Bellman equation in the unknown function Q,\nQ(x, a) = r(x, a) + \u03b3max a\u2032\nQ(f(x, a), a\u2032) \u2200x \u2208 X , a \u2208 A (1)\nto yield Q\u2217, and then choosing the action with the highest Q-value,\n\u03c0\u2217(x) = argmax a\u2032 Q\u2217(x, a\u2032).\nThe Bellman operator T related to (1) is defined by\n( TQ )\n(x, a) := r(x, a) + \u03b3max a\u2032\nQ(f(x, a), a\u2032). (2)\nIt is well known that T is a contraction and Q\u2217 the unique bounded solution to the fixed point problem Q(x, a) = ( TQ ) (x, a), \u2200x, a.\nIn order to solve the infinite dimensional problem in (1) numerically, we have to reduce it to a finite dimensional problem. This is done by introducing a discretization \u0393 of X into a finite number of elements, applying the Bellman operator to only the nodes and interpolating in between.\nIn the following we will consider a uniform grid \u0393h with N vertices \u03bei and d-dimensional tensor B-spline interpolation of order 1. The solution of (1) is then obtained in the space of piecewise affine functions.\nFor a fixed action a\u2032, the value Q\u0393h(z, a\u2032) of any state z with respect to grid \u0393h can be written as a convex combination of the vertices \u03bej of the grid cell enclosing z with coefficients wij (see Figure 1a). For example, consider the 2-dimensional case (bilinear interpolation) in Figure 1b. Let z = (x, y) \u2208 R2. To determine Q\u0393h(z, a\u2032), we find the four vertices \u03be00, \u03be01, \u03be10, \u03be11 \u2208 R2 of the enclosing cell with known function values qa \u2032\n00 := Q \u0393h(\u03be00, a \u2032), . . . etc. We then perform two linear interpolations along the x-coordinate (order invariant) in the auxilary points x0, x1 to obtain\nQ\u0393h(x0, a \u2032) = (1\u2212 \u03bb0)q\na\u2032 00 + \u03bb0q a\u2032 01\nQ\u0393h(x1, a \u2032) = (1\u2212 \u03bb0)q\na\u2032 10 + \u03bb0q a\u2032 11\nwhere \u03bb0 := dx/hx (see Figure 1b for a definition of dx, hx, x0, x1). We then perform another linear interpolation in x0, x1 along the y-coordinate to obtain\nQ\u0393h(z, a\u2032) = (1\u2212 \u03bb1)(1\u2212 \u03bb0)q a\u2032 00 + (1\u2212 \u03bb1)\u03bb0q a\u2032 01 + \u03bb1(1\u2212 \u03bb0)q a\u2032 10 + \u03bb1\u03bb0q a\u2032 11 (3)\nwhere \u03bb1 := dy/hy. Weights wij now correspond to the coefficients in (3). An analogous procedure applies to higher dimensions.\nLet Qa \u2032 be the N\u00d71 vector with entries [Qa \u2032 ]i = Q \u0393h(\u03bei, a \u2032). Let za1 , . . . , z a N \u2208 R d denote the successor state we obtain when we apply the transition function f to vertices \u03bei using action a, i.e., z a i := f(\u03bei, a). Let [w a i ]j = w a ij denote the 1\u00d7N vector of coefficients for zai from (3). The Q-value of z a i for any action a \u2032 with respect to grid \u0393h can thus be written as Q \u0393h(zai , a \u2032) = \u2211N j=1[w a i ]j [Q\na\u2032 ]j . Let W a with rows [wai ] be the N \u00d7N matrix of all coefficients. (Note that this matrix is sparse: each row contains only 2d nonzero entries).\nLet Ra be the N \u00d7 1 vector of associated rewards, [Ra]i := r(\u03bei, a). Now we can use (2) to obtain a fixed point equation in the vertices of the grid \u0393h,\nQ\u0393h(\u03bei, a) = ( T \u0393hQ\u0393h ) (\u03bei, a) i = 1, . . . , N, a = 1, . . . , |A|, (4)\nwhere (\nT \u0393hQ\u0393h )\n(\u03bei, a) := r(\u03bei, a) + \u03b3max a\u2032\nQ\u0393h(f(\u03bei, a), a \u2032).\nSlightly abusing the notation, we can write this more compactly in terms of matrices and vectors,\nT \u0393hQ\u0393h := Ra + \u03b3max a\u2032\n{ W aQa \u2032} \u2200a. (5)\nThe Q-function is now represented by |A| N -dimensional vectors Qa \u2032\n, each containing the values for the vertices \u03bei. The discretized Bellman operator T \u0393h is\na contraction in Rd \u00d7 A and therefore has a unique fixed point Q\u2217 \u2208 Rd \u00d7 A. Let function Q\u2217,\u0393h : (Rd \u00d7 A) \u2192 R be the Q-function obtained by linear interpolation of vector Q\u2217 along states. The function Q\u2217,\u0393h can now be used to determine (approximately) optimal control actions: for any state x \u2208 X , we simply determine\n\u03c0\u2217,\u0393h(x) = argmax a\u2032 Q\u2217,\u0393h(x, a\u2032).\nIn order to estimate how well function Q\u2217,\u0393h approximates the true Q\u2217, a posteriori estimates can be defined that are based on local errors, i.e. the maximum of residual in each grid cell. The local error in a grid cell in turn depends on the granularity of the grid, h, and the modulus of continuity Lf , Lg (e.g., see [9,14] for details)."}, {"heading": "3 Our algorithm: GP-RMAX", "text": "In the last section we have seen how, for a continuous state space, optimal behavior of an agent can be obtained in a numerically robust way, given that the transition function xt+1 = f(xt, at) is known. 2\nFor model-based RL we are now interested in solving the same problem for the case that the transition function is not known. Instead, the agent has to interact with the environment, and only use the samples it observes to compute optimal behavior. Our goal in this paper is to develop a learning framework\n2 Remember our working assumption: reward as a performance criterion is externally given and does not need to be estimated by the agent. Also note that discretization (even with more advanced methods like adaptive or sparse grids) is likely to be feasible only in state spaces with low to medium dimensionality. Breaking the curse of dimensionality is an open research problem.\nwhere this number is kept as small as possible. This will be done by using the samples to learn an estimate f\u0303(x, a) of f(x, a) and then use this estimate f\u0303 in place of f in the numerical procedure outlined in the previous section."}, {"heading": "3.1 Overview", "text": "A sketch of our architecture is shown in Figure 2. GP-RMAX consists of the two parts model learning and planning which are interwoven for online learning. The model-learner estimates the dynamics of the environment from the sample transitions the agent experiences while interacting with the environment. The planner is used to find the best possible action, given the current model. As the predictions of the model-learner become increasingly more accurate, the actions derived from the planner become increasingly closer to optimal. Below is a highlevel overview of the algorithm:\n\u2013 Input: \u2022 Reward function r(x, a) \u2022 Discount factor \u03b3 \u2022 Performance parameters:\n\u2217 planning and model-update frequency K \u2217 model accuracy \u03b4M1 , \u03b4 M 2 (stopping criterion for model-learning) \u2217 discretization of planner N\n\u2013 Initialize: \u2022 Model M1, Q-function Q1, observed transitions D1\n\u2013 Loop: t = 1, 2, . . . \u2022 Interact with system:\n\u2217 observe current state xt \u2217 choose action at greedy with respect to Qt\nat = argmax a\u2032\nQt(xt, a \u2032)\n\u2217 execute action at, observe next state xt+1, store transition Dt+1 = Dt \u222a {xt, at, xt+1} \u2022 Model learning: (see Section 3.2) \u2217 only every K steps, and only if Mt is not sufficiently exact (as determined by evaluating the stopping criterion) \u00b7 Mt+1 = update model (Mt,Dt+1) \u00b7 evaluate stopping criterion (Mt+1,Mt, \u03b4M1 , \u03b4 M 2 )\n\u2217 else \u00b7 Mt+1 = Mt\n\u2022 Planning with model: (see Section 3.3) \u2217 only every K steps, and only if Mt is not sufficiently exact (as determined by evaluating the stopping criterion) \u00b7 Qt+1 = augmented value iteration (Q\u2294,Mt+1,@r(x, u), \u03b3,N)\n\u2217 else \u00b7 Qt+1 = Qt\nNext, we will explain in more detail how each of the two functional modules \u201cmodel-learner\u201d and \u201cplanner\u201d is implemented."}, {"heading": "3.2 Model learning with GPs", "text": "In essence, estimating f\u0303 from samples is a regression problem. While in theory any nonlinear regression algorithm could serve this purpose, we believe that GPs are particularly well-suited: (1) being non-parametric means great modeling flexibility; (2) setting the hyperparameters can be done automatically (and in a principled way) via optimization of the marginal likelihood and allows automatic determination of relevant inputs; and (3) GPs provide a natural way to determine the uncertainty of its predictions which will be used to guide exploration. Furthermore, uncertainty in GPs is supervised in that it depends on the target function that is estimated (because of (2)); other methods only consider the density of the data (unsupervised) and will tend to overexplore if the target function is simple.\nAssume we have observed a number of transitions, given as triplets of state, performed action, and resulting successor state, e.g., D = {xt, at, xt+1}t=1,2,... where xt+1 = f(xt, at). Note that f is a d-dimensional function, f(xt, at) = [\nf1(xt, at), . . . , fd(xt, at) ]T\n. Instead of trying to estimate f directly (which corresponds to absolute transitions), we try to estimate the relative change xt+1\u2212xt as in [10]. The effect of each action on each state variable will be treated independently: we train multiple univariate GPs and combine the individual predictions afterwards. Each individual GP ij is trained in the respective subset of data in D, e.g., GP ij is trained on all xt as input, and x (i) t+1 \u2212 x (i) t as output, where at = j. Each individual GP ij has its own set of hyperparameters obtained from optimizing the marginal likelihood.\nThe details of working3 with GPs can be found in [17]; using GPs to learn a model for RL was previously also studied in [6] (for offline RL and without uncertainty-guided exploration). One characteristic of GPs is that their functional form is given in terms of a parameterized covariance function. Here we use the squared exponential,\nk(x, x\u2032; v0, b, \u03b8) = v0 exp { \u22120.5(x\u2212 x\u2032)T\u2126(x\u2212 x\u2032) } + b,\nwhere matrix \u2126 is either one of the following: (1) \u2126 = \u03b8I (uniform), (2) \u2126 = diag(\u03b81, . . . , \u03b8d) (axis aligned ARD), (3) \u2126 = MkM T k (factor analysis). Scalars v0, b and the (\u2126-dependent number of) entries of \u03b8 constitute the hyperparameters of the GP and are adapted from the training data (likelihood\n3 There is also the problem of implementing GPs efficiently when dealing with a possible large number of data points. For the lack of space we can only sketch our particular implementation, see [16] for more detailed information. Our GP implementation is based on the subset of regressors approximation. The elements of the subset are chosen by a stepwise greedy procedure aimed at minimizing the error incurred from using a low rank approximation (incomplete Cholesky decomposition). Optimization of the likelihood is done on random subsets of the data of fixed size. To avoid a degenerate predictive variance, the projected process approximation was used.\noptimization). Note that variant (2) and (3) implement automatic relevance determination: relevant inputs or linear projections of inputs are automatically identified, whereby model complexity is reduced and generalization sped up.\nOnce trained, for any testpoint x, GP ij provides a distribution over target values, N (\u00b5ij(x), \u03c32ij(x)), with mean \u00b5ij(x) and variance \u03c3 2 ij(x) (exact formulas for \u00b5 and \u03c3 can be found in [17]). Each individual mean \u00b5ij predicts the change in the i-th coordinate of the state under the j-th action. Each individual variance \u03c32ij can be interpreted as the associated uncertainty; it will be close to 0 if GPij is certain, and close to k(x, x) if it is uncertain (the value of k(x, x) depends on the hyperparameters of GP ij). Stacking the individual predictions together, our model-learner produces in summary\nf\u0303(x, a) :=\n\n \nx(1)\n... x(d)\n\n  +\n\n  \u00b51a(x) ...\n\u00b5da(x)\n\n  , c(x, a) := max\ni=1,...,d\n(\nnormalizeia(\u03c3 2 ia)\n)\n, (6)\nwhere f\u0303(x, a) is the predicted successor state and c(x, a) the associated uncertainty (taken as maximum over the normalized per-coordinate uncertainties, where normalization ensures that the values lie between 0 and 1)."}, {"heading": "3.3 Planning with a model", "text": "At any time t, the planner receives as input model Mt. For any state x and action a, model Mt can be evaluated to \u201cproduce\u201d the transition f\u0303(x, a) along with normalized scalar uncertainty c(x, a) \u2208 [0, 1], where 0 means maximally certain and 1 maximally uncertain (see Section 3.2)\nLet \u0393h be the discretization of the state space X with nodes \u03bei, i = 1, . . . , N . We now solve the planning stage by plugging f\u0303 into the procedure described in Section 2. First, we compute z\u0303ai = f\u0303(\u03bei, a), c(\u03bei, a) from (6) and the associated interpolation coefficients waij from (3) for each node \u03bei and action a.\nLet Ca denote the N \u00d7 1 vector corresponding to the uncertainties, [Ca]i = c(\u03bei, a); and R\na be theN\u00d71 vector corresponding to the rewards, [Ra]i = r(\u03bei, a). To solve the discretized Bellman equation in Eq. (4), we perform basic Jacobi iteration:\n\u2013 Initialize [Qa0 ]i, i = 1, . . . , N , a = 1, . . . , |A| \u2013 Repeat for k = 0, 1, 2, . . .\n[Qak+1]i = [R a]i + \u03b3max\na\u2032\n\n\n\nN \u2211\nj=1\nwaij [Q a\u2032 k ]j\n\n\n\n\u2200i, a (7)\nuntil |Qak+1\u2212Q a k|\u221e < tol, \u2200a, or a maximum number of iterations is reached.\nTo reduce the number of iterations necessary, we adapt Gru\u0308ne\u2019s increasing coordinate algorithm [9] to the case of Q-functions: instead of Eq. (7), we perform\nupdates of the form\n[Qak+1]i = [1\u2212 \u03b3w a ii] \u22121\n\n[Ra]i + \u03b3max a\u2032\n\n\n\nN \u2211\nj=1,j 6=i\nwaij [Q a\u2032 k ]j\n\n\n\n\n . (7\u2019)\nIn [9] it was proved that Eq. (7\u2019) converges to the same fixed point as Eq. (7), and it was empirically demonstrated that convergence can occur in significantly fewer iterations. The exact reduction is problem-dependent, savings will be greater for small \u03b3 and large cells where self-transitions occur (i.e., \u03bei is among the vertices of the cell enclosing z\u0303ai ).\nTo implement the \u201coptimism in the face of uncertainty\u201d principle, that is, to make the agent explore regions of the state space where the model predictions are uncertain, we employ the heuristic modification of the Bellman operator which was suggested in [15] and shown to perform well. Instead of Eq. (7\u2019), the update rule becomes\n[Qak+1]i = (1\u2212 [C a]i)[1\u2212 \u03b3w a ii] \u22121\n\n[Ra]i + \u03b3max a\u2032\n\n\n\nN \u2211\nj=1,j 6=i\nwaij [Q a\u2032 k ]j\n\n\n\n\n+\n+ [Ca]iVMAX (7\u201d)\nwhere VMAX := RMAX/(1 \u2212 \u03b3). Eq. (7\u201d) can be seen as a generalization of the binary uncertainty in the original RMAX paper to continuous uncertainty; whereas in RMAX a state was either \u201cknown\u201d (sufficiently explored), in which case the unmodified update was used, or \u201cunknown\u201d (not sufficiently explored), in which case the value VMAX was assigned, here the shift from exploration to exploitation is more gradual.\nFinally we can take advantage of the fact that the planning function will be called many times during the process of learning. Since the discretization \u0393h is kept fixed, we can reuse the final Q-values obtained in one call to plan as initial values for the next call to plan. Since updates to the model often affect only states in some local neighborhood (in particular in later stages), the number of necessary iterations in each call to planning will be further reduced.\nA summary of our model-based planning function is shown below.\n\u2013 Input: \u2022 Model Mt, initial [Qa0]i, i = 1, . . . , N , a = 1, . . . , |A|\n\u2013 Static inputs: \u2022 Grid \u0393h with nodes \u03be1, . . . , \u03beN , discount factor \u03b3, reward function r(x, a) evaluated in nodes giving [Ra]i\n\u2013 Initialize: \u2022 Compute z\u0303ai = f\u0303(\u03bei, a) and [C\na]i from Mt (see Eq. (6)) \u2022 Compute weights waij for each z\u0303 a i (see Eq. (3))\n\u2013 Loop: \u2022 Repeat update Eq. (7\u201d) until |Qak+1 \u2212Q a k|\u221e < tol, \u2200a, or the maximum\nnumber of iterations is reached."}, {"heading": "4 Experiments", "text": "We now examine the online learning performance of GP-RMAX in various wellknown RL benchmark domains."}, {"heading": "4.1 Description of domains", "text": "In particular, we choose the following domains (where a large number of comparative results is available in the literature):\nMountain car: In mountain car, the goal is to drive an underpowered car from the bottom of a valley to the top of one hill. The car is not powerful enough to climb the hill directly, instead it has to build up the necessary momentum by reversing throttle and going up the hill on the opposite side first. The problem is 2-dimensional, state variable x1 \u2208 [\u22121.2, 0.5] describes the position of the car, x2 \u2208 [\u22120.07, 0.07] its velocity. Possible actions are a \u2208 {\u22121, 0,+1}. Learning is episodic: every step gives a reward of \u22121 until the top of the hill at x1 \u2265 0.5 is reached. Our experimental setup (dynamics and domain specific constants) is the same as in [19], with the following exceptions: maximal episode length is 500 steps, discount factor \u03b3 = 0.99 and every episode starts with the agent being at the bottom of the valley with zero velocity, xstart = (\u2212\u03c0/6, 0).\nInverted pendulum: The next task is to swing up and stabilize a single-link inverted pendulum. As in mountain car, the motor does not provide enough torque to push the pendulum up in a single rotation. Instead, the pendulum needs to be swung back and forth to gather energy, before being pushed up and balanced. This creates a more difficult, nonlinear control problem. The state space is 2-dimensional, \u03b8 \u2208 [\u2212\u03c0, \u03c0] being the angle, \u03b8\u0307 \u2208 [\u221210, 10] the angular velocity. Control force is discretized to a \u2208 {\u22125,\u22122.5, 0,+2.5,+5} and held constant for 0.2sec. Reward is defined as r(x, a) := \u22120.1x21 \u2212 0.01x 2 2 \u2212 0.01a\n2. The remaining experimental setup (equations of motion and domain specific constants) is the same as in [6]. The task is made episodic by resetting the system every 500 steps to the initial state xstart = (0, 0). Discount factor \u03b3 = 0.99.\nBicycle: Next we consider the problem of balancing a bicycle that rides at a constant speed [8],[12]. The problem is 4-dimensional: state variables are the roll angle \u03c9 \u2208 [\u221212\u03c0/180, 12\u03c0/180], roll rate \u03c9\u0307 \u2208 [\u22122\u03c0, 2\u03c0], angle of the handle bar \u03b1 \u2208 [\u221280\u03c0/180, 80\u03c0/180], and the angular velocity \u03b1\u0307 \u2208 [\u22122\u03c0, 2\u03c0]. The action space is inherently 2-dimensional (displacement of rider from the vertical and turning the handlebar); in RL it is usually discretized into 5 actions. Our experimental setup so far is similar to [8]. To allow a more conclusive comparison of performance, instead of just being able to keep the bicycle from falling, we define a more discriminating reward r(x, a) = \u2212x21, and r(x, a) = \u221210 for |x1| < 12\u03c0/180 (bicycle has fallen). Learning is episodic: every episode starts in one of two (symmetric) states close to the boundary from where recovery is impossible: xstart = (10\u03c0/180, 0, 0, 0) or xstart = (\u221210\u03c0/180, 0, 0, 0), and proceeds for 500 steps or until the bicycle has fallen. Discount factor \u03b3 = 0.98.\nAcrobot: Our final problem is the acrobot swing-up task [19]. The goal is to swing up the tip of the lower link of an underactuated two-link robot over a given height (length of first link). Since only the lower link is actuated, this is a rather challenging problem. The state space is 4-dimensional: \u03b81 \u2208 [\u2212\u03c0, \u03c0], \u03b8\u03071 \u2208 [\u22124\u03c0, 4\u03c0], \u03b82 \u2208 [\u2212\u03c0, \u03c0], \u03b8\u03072 \u2208 [\u22129\u03c0, 9\u03c0]. Possible actions are a \u2208 {\u22121,+1}. Our experimental setup and implementation of state transition dynamics is similar to [19]. The objective of learning is to reach a goal state as quickly as possible, thus r(x, a) = \u22121 for every step. The initial state for every episode is xstart = (0, 0, 0, 0). An episode ends if either a goal state is reached or 500 steps have passed. The discount factor was set to \u03b3 = 1, as in [19]."}, {"heading": "4.2 Results", "text": "We now apply our algorithm GP-RMAX to each of the four problems. The granularity of the discretization \u0393h in the planner is chosen such that for the 2- dimensional problems, the loss in performance due to discretization is negligible. For the 4-dimensional problems, we ran offline trials with the true transition function to find the best compromise of granularity and computational efficiency. As result, we use a 100 \u00d7 100 grid for mountain car and inverted pendulum, a 20 \u00d7 20 \u00d7 20 \u00d7 20 grid for the bicycle balancing task, and a 25 \u00d7 25 \u00d7 25 \u00d7 25 grid for the acrobot. The maximum number of value iterations was set to 500, tolerance was < 10\u22122. In practice, running the full planning step took between 0.1-10 seconds for the small problems, and less than 5 min for the large problems (where often more than 50% of the CPU time was spent on computing the GP predictions in all the nodes of the grid). Using the planning module offline with the true transition function, we computed the best possible performance for each domain in advance. We obtained: mountain car (103 steps), inverted pendulum (-18.41 total cost), bicycle balancing (-3.49 total cost), and acrobot (64 steps).4\nFor the GP-based model-learner, we set the maximum size of the subset to 1000, and ICD tolerance to 10\u22122. The hyperparameters of the covariance were not manually tuned, but found from the data by likelihood optimiziation.\nSince it would be computationally too expensive to update the model and perform the full planning step after every single observation, we set the planning frequency K to 50 steps. To gauge if optimal behavior is reached and further learning becomes unnessecary, we monitor the change in the model predictions and uncertainties between successive updates and stop if both fall below a threshold (test points in a fixed coarse grid).\nWe consider the following variations of the base algorithm: (1)GP-RMAXexp, which actively explores by adjusting the Bellman updates in Eq. (7\u201d) according to the uncertainties produced by the GP prediction; (2) GP-RMAXgrid, which does the same but uses binary uncertainty by overlaying a uniform grid on top of the state-action space and keeping track which cells are visited; and (3) GPRMAXnoexp, which does not actively explore (see Eq. (7\u2019)). For comparison,\n4 Note that 64 steps is not the optimal solution, [2] demonstrated swing-up with 61 steps.\nwe repeat the experiments using the standard online model-free RL algorithm Sarsa(\u03bb) with tile coding [19], where we consider two different setup of the tilings (one finer and one coarser).\nFigure 3 shows the result of online learning with GP-RMAX and Sarsa. In short, the graphs show us two things in particular: (1) GP-RMAX learns very quickly; and (2) GP-RMAX learns a behavior that is very close to optimal. In comparison, Sarsa(\u03bb) has a much higher sample complexity and does not always learn the optimal behavior (exception is the acrobot). While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.\nExamining the plots in more detail, we find that, while GP-RMAXgrid is somewhat less sample efficient (explores more), GP-RMAXexp and GPRMAXnoexp perform nearly the same. Initially, this appears to be in contrast with the whole point of RMAX, which is efficient exploration guided by the uncertainty of the predictions. Here, we believe that this behavior can be explained by the good generalization capabilities of GPs. Figure 4 illustrates model learning and certainty propagation with GPs in the mountain car domain (predicting acceleration as function of state). The state of the model-learner is shown for two snapshots: after 40 transitions and after 120 transitions. The top row shows the value function that results from applying value iteration with the update modified for uncertainty, see Eq. (7\u201d). The bottom row shows the observed samples and the associated certainty of the predictions. As expected, certainty is high in regions where data was observed. However, due to the generalization of GPs and data-dependent hyperparameter selection, certainty is also high in unexplored regions; and in particular it is constant along the y-coordinate. To understand this, we have to look at the state transition function of the mountain car: acceleration of the car indeed only depends on the position, but not on velocity. This shows that certainty estimates of GPs are supervised and take the properties of the target function into account, whereas prior RMAX treatments of uncertainty are unsupervised and only consider the density of samples to decide if a state is \u201cknown\u201d. For comparison, we also show what GP-RMAX with grid-based uncertainty would produce in the same situation."}, {"heading": "5 Summary", "text": "We presented an implementation of model-based online reinforcement learning similar to RMAX for continuous domains by combining GP-based model learning and value iteration on a grid. Doing so, our algorithm separates the problem function approximation in the model-learner from the problem function approximation/interpolation in the planner. If the transition function is easier to learn, i.e., requires only few samples relative to the representation of the optimal value\nfunction, then large savings in sample-complexity can be gained. Related modelfree methods, such as fitted Q-iteration, can not take advantage of this situation. The fundamental limitation of our approach is that it relies on solving the Bellman equation globally over the state space. Even with more advanced discretization methods, such as adaptive grids, or sparse grids, the curse of dimensionality limits the applicability to problems with low or moderate dimensionality. Other, more minor limitations, concern the simplifying assumptions we made: deterministic state transitions and known reward function. However, these are not conceptual limitations but rather simplifying assumptions made for the present paper; they could be easily addressed in future work."}, {"heading": "Acknowledgments", "text": "This work has taken place in the Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory, The University of Texas at Austin. LARG research is supported in part by grants from the National Science Foundation\n(IIS-0917122), ONR (N00014-09-1-0658), DARPA (FA8650-08-C-7812), and the Federal Highway Administration (DTFH61-07-H-00030)."}], "references": [{"title": "Adaptive-resolution reinforcement learning with efficient exploration", "author": ["A. Bernstein", "N. Shimkin"], "venue": "Machine Learning (published online: 5 May 2010). DOI:10.1007/s10994-010-5186-7,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Minimum-time control of the acrobot", "author": ["G. Boone"], "venue": "Proc. of IEEE International Conference on Robotics and Automation, 4:3281\u20133287,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "R-MAX, a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R. Brafman", "M. Tennenholtz"], "venue": "JMLR, 3:213\u2013231,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Online least-squares policy iteration for reinforcement learning control", "author": ["L. Busoniu", "D. Ernst", "B. De Schutter", "R. Babuska"], "venue": "In American Control Conference (ACC-10),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Multidimensional triangulation and interpolation for reinforcement learning", "author": ["S. Davies"], "venue": "In NIPS 9. Morgan,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Gaussian process dynamic programming", "author": ["M.P. Deisenroth", "C.E. Rasmussen", "J. Peters"], "venue": "Neurocomputing, 72(7-9):1508\u20131524,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Bayes meets Bellman: The Gaussian process approach to temporal difference learning", "author": ["Y. Engel", "S. Mannor", "R. Meir"], "venue": "In Proc. of ICML 20, pages 154\u2013161,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Tree-based batch mode reinforcement learning", "author": ["D. Ernst", "P. Geurts", "L. Wehenkel"], "venue": "JMLR, 6:503\u2013556,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "An adaptive grid scheme for the discrete Hamilton-Jacobi-Bellman equation", "author": ["L. Gr\u00fcne"], "venue": "Numerische Mathematik, 75:319\u2013337,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Model-based exploration in continuous state spaces", "author": ["N.K. Jong", "P. Stone"], "venue": "In The 7th Symposium on Abstraction, Reformulation and Approximation,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning robocup-keepaway with kernels", "author": ["T. Jung", "D. Polani"], "venue": "JMLR: Workshop and Conference Proceedings (Gaussian Processes in Practice), 1:33\u201357,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "JMLR, 4:1107\u20131149,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Online exploration in least-squares policy iteration", "author": ["L. Li", "M.L. Littman", "C.R. Mansley"], "venue": "In Proc. of 8th AAMAS,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Variable resolution discretization in optimal control", "author": ["R. Munos", "A. Moore"], "venue": "Machine Learning, 49:291\u2013323,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Multi-resolution exploration in continuous spaces", "author": ["A. Nouri", "M.L. Littman"], "venue": "In NIPS 21,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Approximation methods for gaussian process regression", "author": ["J. Qui\u00f1onero-Candela", "C.E. Rasmussen", "C.K.I. Williams"], "venue": "In Leon Bottou, Olivier Chapelle, Dennis DeCoste, and Jason Weston, editors, Large Scale Learning Machines, pages 203\u2013 223. MIT Press,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": "MIT Press,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural fitted q-iteration", "author": ["M. Riedmiller"], "venue": "In Proc. of 16th ECML,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "MIT Press,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 2, "context": "To maximize sample efficiency, we consider online RL that is model-based in the spirit of RMAX [3], but extended to continuous state spaces similar to [1,10,5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 0, "context": "To maximize sample efficiency, we consider online RL that is model-based in the spirit of RMAX [3], but extended to continuous state spaces similar to [1,10,5].", "startOffset": 151, "endOffset": 159}, {"referenceID": 9, "context": "To maximize sample efficiency, we consider online RL that is model-based in the spirit of RMAX [3], but extended to continuous state spaces similar to [1,10,5].", "startOffset": 151, "endOffset": 159}, {"referenceID": 4, "context": "To maximize sample efficiency, we consider online RL that is model-based in the spirit of RMAX [3], but extended to continuous state spaces similar to [1,10,5].", "startOffset": 151, "endOffset": 159}, {"referenceID": 17, "context": "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.", "startOffset": 57, "endOffset": 66}, {"referenceID": 7, "context": "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.", "startOffset": 57, "endOffset": 66}, {"referenceID": 14, "context": "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.", "startOffset": 57, "endOffset": 66}, {"referenceID": 11, "context": "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.", "startOffset": 108, "endOffset": 120}, {"referenceID": 3, "context": "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.", "startOffset": 108, "endOffset": 120}, {"referenceID": 12, "context": "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.", "startOffset": 108, "endOffset": 120}, {"referenceID": 10, "context": "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.", "startOffset": 108, "endOffset": 120}, {"referenceID": 9, "context": "Conceptually, our approach is closely related to Fitted R-MAX, which was proposed in [10] and uses an instance-based approach in the model-learner, and related work in [5,1], which uses grid-based interpolation in the model-learner.", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "Conceptually, our approach is closely related to Fitted R-MAX, which was proposed in [10] and uses an instance-based approach in the model-learner, and related work in [5,1], which uses grid-based interpolation in the model-learner.", "startOffset": 168, "endOffset": 173}, {"referenceID": 0, "context": "Conceptually, our approach is closely related to Fitted R-MAX, which was proposed in [10] and uses an instance-based approach in the model-learner, and related work in [5,1], which uses grid-based interpolation in the model-learner.", "startOffset": 168, "endOffset": 173}, {"referenceID": 0, "context": "For example, unlike the recent ARL [1], for which PAC-style performance bounds could be derived (because of its grid-based implementation of model-learning), a GP is much better able to handle generalization and as a consequence can achieve much lower sample complexity.", "startOffset": 35, "endOffset": 38}, {"referenceID": 8, "context": ", [9,14], for our purpose here, a uniform grid is sufficient as proof of concept.", "startOffset": 2, "endOffset": 8}, {"referenceID": 13, "context": ", [9,14], for our purpose here, a uniform grid is sufficient as proof of concept.", "startOffset": 2, "endOffset": 8}, {"referenceID": 18, "context": "Consider the reinforcement learning problem for MDPs with continuous state space, finite action space, discounted reward criterion and deterministic dynamics [19].", "startOffset": 158, "endOffset": 162}, {"referenceID": 8, "context": ", see [9,14] for details).", "startOffset": 6, "endOffset": 12}, {"referenceID": 13, "context": ", see [9,14] for details).", "startOffset": 6, "endOffset": 12}, {"referenceID": 9, "context": "Instead of trying to estimate f directly (which corresponds to absolute transitions), we try to estimate the relative change xt+1\u2212xt as in [10].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "The details of working with GPs can be found in [17]; using GPs to learn a model for RL was previously also studied in [6] (for offline RL and without uncertainty-guided exploration).", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "The details of working with GPs can be found in [17]; using GPs to learn a model for RL was previously also studied in [6] (for offline RL and without uncertainty-guided exploration).", "startOffset": 119, "endOffset": 122}, {"referenceID": 15, "context": "For the lack of space we can only sketch our particular implementation, see [16] for more detailed information.", "startOffset": 76, "endOffset": 80}, {"referenceID": 16, "context": "Once trained, for any testpoint x, GP ij provides a distribution over target values, N (\u03bcij(x), \u03c3 ij(x)), with mean \u03bcij(x) and variance \u03c3 2 ij(x) (exact formulas for \u03bc and \u03c3 can be found in [17]).", "startOffset": 190, "endOffset": 194}, {"referenceID": 0, "context": "For any state x and action a, model Mt can be evaluated to \u201cproduce\u201d the transition f\u0303(x, a) along with normalized scalar uncertainty c(x, a) \u2208 [0, 1], where 0 means maximally certain and 1 maximally uncertain (see Section 3.", "startOffset": 144, "endOffset": 150}, {"referenceID": 8, "context": "To reduce the number of iterations necessary, we adapt Gr\u00fcne\u2019s increasing coordinate algorithm [9] to the case of Q-functions: instead of Eq.", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "In [9] it was proved that Eq.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "To implement the \u201coptimism in the face of uncertainty\u201d principle, that is, to make the agent explore regions of the state space where the model predictions are uncertain, we employ the heuristic modification of the Bellman operator which was suggested in [15] and shown to perform well.", "startOffset": 255, "endOffset": 259}, {"referenceID": 18, "context": "Our experimental setup (dynamics and domain specific constants) is the same as in [19], with the following exceptions: maximal episode length is 500 steps, discount factor \u03b3 = 0.", "startOffset": 82, "endOffset": 86}, {"referenceID": 5, "context": "The remaining experimental setup (equations of motion and domain specific constants) is the same as in [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "Bicycle: Next we consider the problem of balancing a bicycle that rides at a constant speed [8],[12].", "startOffset": 92, "endOffset": 95}, {"referenceID": 11, "context": "Bicycle: Next we consider the problem of balancing a bicycle that rides at a constant speed [8],[12].", "startOffset": 96, "endOffset": 100}, {"referenceID": 7, "context": "Our experimental setup so far is similar to [8].", "startOffset": 44, "endOffset": 47}, {"referenceID": 18, "context": "Acrobot: Our final problem is the acrobot swing-up task [19].", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "Our experimental setup and implementation of state transition dynamics is similar to [19].", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "The discount factor was set to \u03b3 = 1, as in [19].", "startOffset": 44, "endOffset": 48}, {"referenceID": 1, "context": "4 Note that 64 steps is not the optimal solution, [2] demonstrated swing-up with 61 steps.", "startOffset": 50, "endOffset": 53}, {"referenceID": 18, "context": "we repeat the experiments using the standard online model-free RL algorithm Sarsa(\u03bb) with tile coding [19], where we consider two different setup of the tilings (one finer and one coarser).", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.", "startOffset": 98, "endOffset": 107}, {"referenceID": 7, "context": "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.", "startOffset": 98, "endOffset": 107}, {"referenceID": 14, "context": "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.", "startOffset": 98, "endOffset": 107}, {"referenceID": 11, "context": "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.", "startOffset": 147, "endOffset": 159}, {"referenceID": 3, "context": "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.", "startOffset": 147, "endOffset": 159}, {"referenceID": 12, "context": "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.", "startOffset": 147, "endOffset": 159}, {"referenceID": 10, "context": "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.", "startOffset": 147, "endOffset": 159}, {"referenceID": 6, "context": "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.", "startOffset": 190, "endOffset": 195}, {"referenceID": 5, "context": "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.", "startOffset": 190, "endOffset": 195}, {"referenceID": 18, "context": "This in turn allows the optimal value function to be learned from very few sample transitions: panel (b) shows that after only 120 transitions (still in the middle of the very first episode) the approximated value function already resembles the true one [19].", "startOffset": 254, "endOffset": 258}], "year": 2012, "abstractText": "We present an implementation of model-based online reinforcement learning (RL) for continuous domains with deterministic transitions that is specifically designed to achieve low sample complexity. To achieve low sample complexity, since the environment is unknown, an agent must intelligently balance exploration and exploitation, and must be able to rapidly generalize from observations. While in the past a number of related sample efficient RL algorithms have been proposed, to allow theoretical analysis, mainly model-learners with weak generalization capabilities were considered. Here, we separate function approximation in the model learner (which does require samples) from the interpolation in the planner (which does not require samples). For model-learning we apply Gaussian processes regression (GP) which is able to automatically adjust itself to the complexity of the problem (via Bayesian hyperparameter selection) and, in practice, often able to learn a highly accurate model from very little data. In addition, a GP provides a natural way to determine the uncertainty of its predictions, which allows us to implement the \u201coptimism in the face of uncertainty\u201d principle used to efficiently control exploration. Our method is evaluated on four common benchmark domains.", "creator": "LaTeX with hyperref package"}}}