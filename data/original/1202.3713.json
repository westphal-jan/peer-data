{"id": "1202.3713", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Bayesian network learning with cutting planes", "abstract": "The problem of learning the structure of Bayesian networks from complete discrete data with a limit on parent set size is considered. Learning is cast explicitly as an optimisation problem where the goal is to find a BN structure which maximises log marginal likelihood (BDe score). Integer programming, specifically the SCIP framework, is used to solve this optimisation problem. Acyclicity constraints are added to the integer program (IP) during solving in the form of cutting planes. Finding good cutting planes is the key to the success of the approach -the search for such cutting planes is effected using a sub-IP. Results show that this is a particularly fast method for exact BN learning.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (184kb)", "http://arxiv.org/abs/1202.3713v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["james cussens"], "accepted": false, "id": "1202.3713"}, "pdf": {"name": "1202.3713.pdf", "metadata": {"source": "CRF", "title": "Bayesian network learning with cutting planes", "authors": ["James Cussens"], "emails": ["jc@cs.york.ac.uk"], "sections": [{"heading": null, "text": "The problem of learning the structure of Bayesian networks from complete discrete data with a limit on parent set size is considered. Learning is cast explicitly as an optimisation problem where the goal is to find a BN structure which maximises log marginal likelihood (BDe score). Integer programming, specifically the SCIP framework, is used to solve this optimisation problem. Acyclicity constraints are added to the integer program (IP) during solving in the form of cutting planes. Finding good cutting planes is the key to the success of the approach\u2014the search for such cutting planes is effected using a sub-IP. Results show that this is a particularly fast method for exact BN learning."}, {"heading": "1 Introduction", "text": "A Bayesian network (BN) encodes conditional independence relations between random variables using an acyclic directed graph (DAG) whose vertices are the random variables. The DAG can be used to \u2018read off\u2019 conditional independence relations thus providing insight into the structure of the joint probability distribution represented by the BN. As a result BNs are a very popular probabilistic model and there is great interest in \u2018learning\u2019 BNs from data (i.e. doing statistical model selection where BNs are the model class).\nOne standard approach to BN learning is \u2018search and score\u2019. A score is chosen to represent how well any candidate BN is supported by the observed data (and any prior knowledge) and then a search is conducted with the goal of finding a BN with maximal score. BN learning is therefore an optimisation problem. Unfortunately, as is well-known [3], this optimisation problem is NP-hard for any reasonable score, even if the\nnumber of parents for any vertex in the DAG is limited to two.\nGiven this hardness result most BN learning work has concentrated on heuristic search where there is no guarantee that an optimal BN has been found. However, there is a growing body of work on exact BN structure learning. One approach is to use dynamic programming [10, 12, 5] which has successfully been used for exact learning up to around 30 vertices.\nIn this paper integer programming (IP) is used for exact BN learning. IP has already been used by Cussens [7] for the special case of BN pedigree reconstruction (see Section 8) and by Jaakkola et al [9] for general BN learning with a limit on parent set size. As explained in Section 3 the work presented here is most closely related to that of Jaakkola et al. However here IP is effected in a more conventional manner than Jaakkola et al, using the SCIP (Solving Constraint Integer Programs) framework [2]. In addition a different approach is taken to searching for good cutting planes.\nThis paper assumes a basic knowledge of what Bayesian networks are, but is intended to be comprehensible by a reader without much knowledge of integer programming. The paper is organised as follows. After explaining the important characteristics of the BDe score in Section 2, a method for encoding the BN structure learning problem as an IP is given in Section 3. The central contribution of the paper is in Section 4 where the cutting plane method is given. After presenting the results of this method in Section 5, ongoing work by the author and related work by others is discussed in Sections 6 and 7. The paper ends with conclusions and pointers to future work (Section 8)."}, {"heading": "2 Scoring Bayesian networks", "text": "As is commonly done, candidate BNs are scored using log marginal likelihood with Dirichlet priors over the BN parameters (BDe score). Let V be the set of\nvertices in a BN, each corresponding to a random variable and let D be the observed data. Throughout this paper |V | will be abbreviated to n. The BDe score for DAG G has the following form:\nlogP (G|D) = \u2211 u\u2208V Scoreu(G) (1)\nSo the total score is a sum of local scores Scoreu(G). Moreover (given fixed data D) Scoreu(G) is entirely determined by the parents that u has in G.\nThis decomposition motivates the following preprocessing step: for each variable u and each candidate parent set W for u, compute and store the corresponding local score. Denote such a local score by c(u,W ). Evidently, such an approach is only feasible if there are not too many candidate parent sets. Here this is achieved in the standard way: by limiting the number of parents a vertex can have.\nIf c(u,W ) < c(u,W \u2032) for some vertex u, and candidate parent sets W and W \u2032 where W \u2032 \u2282W , it follows that W cannot be a parent set for u in an optimal BN\u2014 replacing W by W \u2032 as parents for u in any BN will increase the score and cannot introduce a cycle. As in other work ([6, 8, 7, 9, 5]) this simple observation is used to prune the set of local scores. (Cowell [5] uses this pruning but does not mention it.)"}, {"heading": "3 Encoding the BN learning problem as an integer program", "text": "The BN learning problem is encoded as an integer program using the same variables as in some previous work [6, 7, 9]. For each variable u and candidate parent set W corresponding to a stored local score, a binary variable I(W \u2192 u) is created. I(W \u2192 u) = 1 iff W are the parents of u in an optimal BN. These variables will be referred to as family variables. BN learning can then be cast as the following constrained optimisation problem:\nInstantiate the I(W \u2192 u) to maximise:\u2211 u,W c(u,W )I(W \u2192 u) subject to the I(W \u2192 u) representing a DAG.\nInteger programming is linear programming with integrality constraints on the variables. To use an IP approach it is thus necessary to use linear constraints to ensure that only valid DAGs are feasible. Convexity constraints, stating that each variable has exactly one (perhaps empty) parent set, are easy to express as linear constraints:\n\u2200u : \u2211 W I(W \u2192 u) = 1 (2)\nWith (2) any feasible integer solution represents a digraph, but this digraph may have cycles. The key question is how to effectively rule out cycles. A number of approaches are possible. In [7], Cussens considers two approaches. In the first, auxiliary binary variables I(u < v) are created for each distinct pair of variables. I(u < v) = 1 indicates that u comes before v in a topological ordering associated with the optimal BN. In the second approach auxiliary integervalued variables gen(v) are created where gen(v) is the index of v in the topological ordering.\nTogether with appropriate (linear) constraints linking auxiliary variables to I(W \u2192 u) family variables, these approaches can rule out cycles and produced good results on the relatively easy problem of pedigree learning (see Section 8). However, Jaakkola et al [9] provide a much tighter class of constraints without the need for auxiliary variables. This class of constraints rests upon the observation that any subset C of vertices in a DAG must contain at least one vertex who has no parent in that subset. These are called cluster-based constraints and can be expressed as linear constraints:\n\u2200C \u2286 V : \u2211 u\u2208C \u2211 W :W\u2229C=\u2205 I(W \u2192 u) \u2265 1 (3)\nIn this paper, this constraint is generalised. Any DAG has a topological ordering of the variables such that a vertex\u2019s parents must appear earlier in the ordering. Given any subset C of vertices in the DAG, the earliest vertex must have no parent in C, the next-earliest can have at most one, the next at most two and so on. It follows that the earliest k vertices all have fewer than k parents in the subset. This leads to a linear constraint for each subset C and each k = 1, . . . , |C| as shown in (4):\n\u2200C \u2286 V,\u2200k, 1 \u2264 k \u2264 |C| : (4)\u2211 u\u2208C \u2211 W :|W\u2229C|<k I(W \u2192 u) \u2265 k\nCall these k-cluster-based constraints. Constraint class (3) is a special case of k-cluster-based constraints when k = 1; these will be referred to as 1-cluster-based constraints henceforth.\nA k-cluster-based constraint for cluster C has an associated non-negative surplus variable skC which measures how far the LHS is above its lower bound. It is interesting to consider the values of surplus variables for DAGs, i.e. integer solutions. s1{u,v} = 0 if and only if there is a line between u and v in the undirected skeleton. For any three vertices u, v and w, there is an immorality u \u2192 w \u2190 v if and only if s1{u,w} = 0, s1{v,w} = 0, s 1 {u,v} = 1 and s 1 {u,v,w} = 1. It follows\nthat the Markov equivalence class for a DAG is determined by the values of its surplus variables for clusters of sizes 2 and 3.\nIf a cluster constraint is tight for a DAG then it lies on the relevant hyperplane. It is interesting to consider when k-cluster-based constraints are tight for different values of k since this gives insight into the geometry of the linear polytope. For example, consider three vertices u, v and w. In a DAG if the subgraph for {u, v, w} has three arrows (the maximum) then both 1- and 2- cluster constraints are tight: s1{u,v,w} = s 2 {u,v,w} = 0. For subgraphs of the form u \u2192 v \u2192 w or u \u2190 v \u2192 w the 1-cluster is tight but not the 2-cluster. For subgraphs of the form u \u2192 v \u2190 w (immoralities) the 2-cluster is tight but not the 1-cluster."}, {"heading": "4 Cutting planes", "text": "To rule out cycles it is enough to add all 1-clusterbased constraints, but there are far too many of these to include in an integer program. Instead such constraints will be added as cutting planes during the solving process.\nInitially an integer program with just the convexity constraints (2) is constructed. Following the standard approach to integer programming, the linear relaxation of this IP is solved. In the linear relaxation the integrality constraint on family variables is removed and they can take any value in [0, 1]. The linear relaxation (which is a linear program (LP)) is quickly solved using (a version of) the simplex algorithm, resulting in an LP solution x\u030c which is an instantiation of all family variables.\nThis first LP solution will correspond to choosing the best scoring parents for each vertex and (barring exceptional cases) will contain cycles. It is thus necessary to add a cluster-based constraint to remove the LP solution from the feasible region. This is known as separation. This leads to a new IP whose linear relaxation can then also be solved.\nA linear constraint added to an IP to remove an LP solution is known as a cutting plane since the LP solution is cut away from the feasible region. Note that the feasible region for the linear relaxation will be a convex polytope and LP solutions will be vertices of this polytope. Adding a cutting plane \u2018snips\u2019 off this vertex.\nThe approach taken here is to repeatedly (1) solve the current linear relaxation and (2) add cutting planes until either no further cutting planes can be found or an LP solution corresponding to a DAG is returned. Since the score of an LP solution is an upper bound on that of the optimal BN any such DAG will be the\noptimal BN."}, {"heading": "4.1 Searching for good cutting planes", "text": "The key issue is how to quickly find good cutting planes. The overall goal is to reduce the upper bound given by LP solutions as much as possible by cutting deep into the LP polytope. It is generally beneficial to add several cutting planes removing a given LP solution. There are three important quality criteria for a collection of cutting planes \u201cthe efficacy of the cuts, i.e., the distance of their corresponding hyperplanes to the current LP solution, the orthogonality of the cuts with respect to each other, and the parallelism of the cuts with respect to the objective function.\u201d [1].\nIn this work the focus is on finding cutting planes with high efficacy (and hoping for the best as regards orthogonality and objective function parallelism). Let x\u030cI(W\u2192u) be the value of I(W \u2192 u) in the current LP solution x\u030c, then if the 1-cluster constraint defined using cluster C is a cutting plane removing x\u030c then its efficacy is:\n1\u2212 \u2211 u\u2208C \u2211\nW :W\u2229C=\u2205 x\u030cI(W\u2192u)\u221a\u2211 u\u2208C \u2211 W :W\u2229C=\u2205 1\n(5)\nInstead of attempting to maximise (5) directly its numerator is maximised by searching for a cluster C minimising \u2211\nu\u2208C \u2211 W :W\u2229C=\u2205 x\u030cI(W\u2192u) (6)\nThis minimisation is effected by a sub-IP. In this subIP a binary variable J(W \u2192 u) is created for each family variable I(W \u2192 u) in the main IP. The objective function for the sub-IP is:\u2211\nu,W\nx\u030cI(W\u2192u)J(W \u2192 u) (7)\nA constraint is placed stating that only solutions where the objective value is strictly less than 1 are permitted. This ensures that all solutions correspond to a cutting plane. Constraints are then placed upon the J(W \u2192 u) such that any feasible joint instantiation corresponds to a cluster C and where (7) is equivalent to (6) for this cluster. In other words, J(W \u2192 u) = 1 iff u \u2208 C and W \u2229 C = \u2205.\nA binary variable I(u) is created for each u \u2208 V . I(u) = 1 iff u is in the cluster C. For any J(W \u2192 u) we then have the following constraint:\nJ(W \u2192 u) = 1\u21d4 I(u) = 1 \u2227 \u2227\nu\u2032\u2208W I(u\u2032) = 0 (8)\nNote that from (8) it follows that J(\u2205 \u2192 u) = 1 \u21d4 I(u) = 1 and so in the actual sub-IP I(u) variables are\nreplaced by J(\u2205 \u2192 u) variables. The final constraint in the sub-IP is that at least 2 of the J(\u2205 \u2192 u) are set to 1.\nAn empirical comparison between a pure constraint processing approach to solving this sub-IP and the standard approach using linear relaxations shows that the former is more efficient. The constraints (8) are effected using SCIP\u2019s built-in \u2018and\u2019 constraint. Depthfirst search is then used to search for a minimising feasible solution. Only J(\u2205 \u2192 u) variables are made available for branching. To choose between these, SCIP\u2019s default branching score function is replaced by one which attempts to balance the search tree as much as possible. Once a J(\u2205 \u2192 u) variable has been branched on the J(\u2205 \u2192 u) = 1 subtree is always explored first.\nIn most cases, the sub-IP was run until an optimal cluster was found, the rationale being that this effort is worth expending to find good cutting planes. SCIP does not just return the optimal solution but all suboptimal solutions found during the search. Collecting all solutions is important since each corresponds to a cutting plane and the joint benefit of a collection of cutting planes can be considerably greater than one (apparently) best one.\nTo reduce the number of variables involved in a cutting plane, the sum \u2211 W :W\u2229C=\u2205 I(W \u2192 u) in (3) is\nreplaced with 1 \u2212 \u2211\nW :W\u2229C 6=\u2205 I(W \u2192 u) whenever the latter has fewer summands.\nWhen the LP solution happens to be entirely integral and corresponds to a cyclic digraph the sub-IP is solved very quickly and only one cutting plane is returned. This is because the sub-IP is just finding a cycle in a manner similar to that of standard depth-first search [4]. When the LP solution contains fractional values, solving is slower but multiple cutting planes are found, sometimes more than 100.\nOnly k-cluster-based constraints for k = 1 are searched for by this sub-IP. It is of course possible to set up further sub-IPs to search for k-cluster-based constraints for k > 1. This was done for k = 2 but the time taken to solve this sub-IP was not compensated by the effectiveness of the cutting planes found. Instead a simple approach is taken: for each 1-cluster-based constraint found the corresponding 2-cluster-based constraint is also added."}, {"heading": "4.2 Gomory cuts", "text": "Using the DAG-specific cutting planes described above is the key to the success of this IP approach to BN learning. However, these are not the only useful cutting planes. It can happen that there are no 1-clusterbased constraints separating the current LP solution.\nIf that is the case then SCIP is asked to look for Gomory cuts. Gomory cuts are general-purpose cutting planes which can be quickly computed from the simplex tableau. For further information on Gomory cuts see, for example, Wolsey [13]. Relying on Gomory cuts alone would be inefficient since they are generally not very deep. However, they turn out to be crucial (see Section 5). Even a weak Gomory cut at least separates the current LP solution thus leading to a new linear relaxation\u2014one with a different LP solution, for which the cutting plane algorithm described in Section 4.1 may well find good cutting planes. In short, Gomory cuts can help a pure DAG-specific cutting plane approach from getting stuck. However, it has been found important to only add Gomory cuts when no clusterbased constraints can be found (delayed Gomory cuts). Since quite weak cuts are included in the approach presented here, adding all possible Gomory cuts can lead to poor results. For example, as will be shown in Section 5, the Water100 problem can be solved in 9 seconds using 2196 generated cuts with a delayed Gomory cut approach. With Gomory cuts not delayed solving was abandoned after more than 5.5 hours of failing to find an optimal BN! In this case the large number of cuts generated (over 6745) led to very slow LP solving with over 1.3 million LP iterations done by the time of abandonment.\nGomory cuts have another, psychological, benefit. By inspecting Gomory cuts and determining what they mean in graph-theoretic terms it is possible to discover new classes of cutting planes. This is how it came to be realised that the 1-cluster-based constraint of Jaakkola et al could be generalised to k-cluster-based constraints."}, {"heading": "5 Results", "text": "The approach described in the preceding two sections was implemented in C using the SCIP framework. SCIP is available from http://scip.zib.de/. SCIP has a plug-in architecture and the approach presented here was effected by writing a constraint handler. (See http://scip.zib.de/doc/html/CONS. html.) CPLEX 12 was used as the LP solver. All experiments were performed using a 64-bit Linux kernel on a single-thread of a dual 3GHz CPU with 3.8Gb of RAM. SCIP was configured to allow the inclusion of even very weak cuts. For Gomory cuts the relevant C source in SCIP was edited to allow very low quality Gomory cuts to be generated. (See Section 4.1 for an explanation of cut quality.)\nTen different BNs were used. Various datasets were sampled from them and local scores were computed. The details can be found in Table 1. The \u2018upper\u2019\ncollection of BNs in Table 1 all came from Bayesian Network Repository (http://compbio.cs.huji.ac. il/Repository/). Local scores were computed using an unoptimised Python program which meant that it could take as long as an hour to compute local scores for the 3 slowest cases, with the most slow taking a little over 6 hours. The \u2018lower\u2019 4 datasets were supplied as local scores and are the same as those used in [9].\nThe main results of this paper are contained in the \u2018Time\u2019 columns of Tables 2 and 3 which display the time in seconds (rounded to the nearest second, as measured by the UNIX time command, system call time included) to find an optimal BN for all datasets. In Table 2 both 1- and 2-cluster constraints are added. In Table 3 only 1-cluster constraints are used and results for \u2018easy\u2019 problems have been omitted in the in-\nterests of space\u2014they are similar to those found in Table 2. Neither option is consistently better, however using 1-cluster constraints only solves the biggest problem (carpo10000) in \u2248 7.4 hours whereas with both 1- and 2-cluster constraints it takes almost 12 hours. It seems likely that a more careful combination of 1- and 2- cluster constraint cutting planes will be the best option.\nFor two datasets, carpo100 and carpo10000, it was necessary to impose a time limit of 100 seconds on the sub-IP used to find cutting planes: any solutions found within this limit are added as cutting planes.\nOn a number of datasets SCIP reached a point at which it could find no further cutting planes but had not found an optimal BN and thus had to resort to branch-and-bound. The \u2018Nodes\u2019 column in the tables is the number of nodes in the branch-and-bound tree at the time of solution, so that if Nodes=1 the problem was solved without branching. A branch-and-cut approach was taken: cluster-based cutting planes and Gomory cutting planes are generated not only in the root node of the branch-and-bound tree but at every other node in the tree.\nSCIP has a collection of primal heuristics which periodically search for good solutions. Even if such a solution does not turn out to be optimal it can be useful since it may allow pruning of the branch-and-bound tree. In this paper only fast primal heuristics were allowed to run. Typically, the optimal solution (and a number of suboptimal solutions) were found during the running of the simplex algorithm computing a LP solution: if an integral solution is visited in one of the simplex iterations and it turns out to be a valid DAG with a better score than the current incumbent solution, it becomes the new incumbent. This is an instance of SCIP\u2019s simple rounding primal heuristic; in this case there are zero roundings required to produce a solution. In some cases a more sophisticated rounding approach which uses the LP solution, found the optimal BN. Turning off all heuristics modestly speeds up solving for all problems which can be solved in the root, since in that case it is enough to wait for the cutting planes to lead to an LP solution which is entirely integral.\nIn Tables 2 and 3 \u2018Cuts\u2019 is the total number of cuts generated whereas \u2018Rows\u2019 is the total number of linear constraints in effect at the point at which an optimal solution was found; this number is only really informative for problems solved without branching. \u2018Rows\u2019 is generally smaller than \u2018Cuts\u2019 since cuts can be become redundant with the addition of further cuts. The number of cuts is rounded to the nearest 1000 where the number is large.\nTo test the effectiveness of Gomory cuts, solving was attempted again for each dataset with both 1- and 2- cluster constraints but with Gomory cuts disabled. For all but the largest problems solving was still successful and was generally a little quicker. However for big problems disabling Gomory cuts could be disastrous. alarm10000 took 18462 seconds to solve (as opposed to 12872 with Gomory cuts). hailfinder10000, which took only 169 seconds to solve with Gomory cuts had still failed to find an optimal BN after 85075 seconds without them (and had exhausted RAM by this point). With Gomory cuts, the 358\u221256 = 302 non-redundant cuts generated for hailfinder10000 had reduced the upper bound on the objective function to \u22124.970794\u00d7105 before SCIP had to resort to branching. Since the optimal BN has score \u22124.976318\u00d7105 this upper bound is quite tight allowing for a reasonably fast branch-andcut search. Without Gomory cuts the 502\u2212 56 = 446 non-redundant cuts generated had only reduced this upper bound to \u22124.946169\u00d7 105. Using Gomory cuts had allowed for fewer but better cuts. The very largest problems, carpo100 and carpo10000 were not even attempted without Gomory cuts."}, {"heading": "5.1 How to reproduce these results", "text": "Go to: http://www.cs.york.ac.uk/~jc/research/uai11"}, {"heading": "6 Ongoing work", "text": "The approach described above is the most successful to date but there are a number of elaborations which have been attempted. Although these have yet to render any consistent improvement it is instructive to list them since (1) it may be that some variant will succeed and (2) negative results are of independent interest.\nThe most obvious idea is to add a reasonably small number of k-cluster constraints directly into the IP so that they do not have to be found as cutting planes. There is the possibility that SCIP\u2019s pre-processing algorithms might be able to simplify the problem using these constraints. In the event, pre-adding k-cluster constraints led to slower solving and increased memory requirements.\nSince SCIP has to resort to branching on bigger problems a primal heuristic specific to BN learning is motivated. This is a function, with access to any current LP solution, which generates DAGs in the hope that they score better than the current incumbent solution. High scoring incumbent solutions allow pruning of the branch-and-cut tree thus speeding up solving, sometimes considerably. Two primal heuristics have been attempted. The first very simple approach is to gener-\nate very many random orders of the BN variables and return the highest-scoring BN consistent with each order, ultimately returning the best BN overall. In the second a variable ordering is generated from the current LP solution with a view to being \u2018as consistent as possible\u2019 with it. The best BN for that ordering is then proposed. Both of these approaches are quick, but the BNs found are less useful than those found by rounding (Section 5).\nThe presented approach makes no use of Markov equivalence classes which partition the BN space into equivalent statistical models. BNs in a Markov equivalent class will have the same score, for any reasonable scoring function. Since two BNs are Markov equivalent if they have the same undirected skeleton and set of \u2018immoralities\u2019 (unmarried parents) ( n 2 ) auxiliary binary line variables were created each indicating the presence/absence of an edge in the undirected skeleton. Since such variables can be defined in terms of family variables there is no real increase in the number of variables in the IP. Branching priorities were set so that branching could only occur on line variables. The idea here is that branching divides Markov equivalence classes rather than just BNs. In most cases this approach slowed down solving but for alarm10000 and alarm solving took only 6526 and 84 seconds respectively, which is better than the results reported in Tables 2 and 3."}, {"heading": "7 Related work", "text": "As previously noted, this work is most closely related to that of Jaakkola et al [9], since the key 1-cluster constraints were introduced in that paper. The main difference is that these constraints are applied here in a conventional manner: as cutting planes in an existing integer programming framework (SCIP). Although, as here, Jaakkola et al add 1-cluster constraints \u2018on the fly\u2019 they use a different search approach to find good clusters (actually two search methods are used). When branching they always choose the node with the best LP solution (in the hope that the optimal BN is in that subtree). The variable to branch on is determined by a rule informed by the existence of cluster constraints. Here, in contrast, SCIP\u2019s default approach to branchand-cut is used.\nAn empirical comparison is also possible since the same local scores have been used. Jaakkola et al used a 2.4GHz Dual Core Macbook pro Laptop with 4GB of memory and report solving times of 8 seconds, 7 minutes and 21 minutes for datasets phen, wdbc and alarm respectively. Here these were solved in 9 seconds, 1.3 minutes and 2.55 minutes respectively. Note that SCIP does not resort to branching on 2 of the 4\ndatasets from Jaakkola et al.\nOther much bigger problems have also been solved, albeit sometimes very slowly. Note that in common with Jaakkola et al no consideration is given to how \u2018close\u2019 the found optimal BNs are to the true data-generating BNs, the focus here is exclusively on the \u2018search\u2019 part of a \u2018search-and-score\u2019 learning algorithm. This closeness depends crucially on the score (particularly the choice of Dirichlet priors [11]) as well as, of course, the amount of data."}, {"heading": "8 Conclusions and future work", "text": "The main finding of this paper is that integer programming can be used for exact learning of Bayesian networks with quite a large number of variables, the carpo datasets have 60 BN variables\u2014as long as there is a limit on parent set sizes.\nIn some cases there is a known limit on parent set sizes. For example in the BN representation of a pedigree, variables represent individuals and the arrows in the DAG represent real parenthood relationships. Pedigrees are often called \u2018family trees\u2019 although the DAG need not be a tree if there has been inbreeding. Pedigrees are learnt (or \u2018reconstructed\u2019 as it is known in the statistical genetics literature) from DNA marker data. Cowell [5] has applied a dynamic programming approach to exact pedigree reconstruction whereas Cussens [7] used the IP approach outlined in Section 3\nIn most other applications the limit on parent set size is artificial. An interesting avenue for future work is to see whether a column generation approach using a variable pricer can be used to overcome this limitation. A variable pricer allows the dynamic generation of new variables. Here these would be new family variables. SCIP provides a convenient and effective way of creating variable pricers due to its plugin approach.\nA more fundamental issue is whether it is worth the effort to do \u2018exact\u2019 BN learning to select a single BN model as our best guess for the true BN. One motivation is that an optimal BN can be very much more probable than any sub-optimal one\u2014the relevant landscape is very \u2018spiky\u2019, particularly for large datasets. (Recall that BDe corresponds to posterior probability with a uniform structure prior.) However, from a Bayesian perspective at least, returning only a single model is dubious since it gives little notion of the degree of model uncertainty inherent in any learning problem. However, the current approach can be used as a subroutine in a search-based model averaging approach (but without pruning of local scores). After the optimal BN has been found its entire Markov equiva-\nlence class can be ruled out via appropriate constraints and a new search can be started. An advantage of this approach is that there is a guarantee that any yet-tobe-found BNs can have a score no better than those found already. This could be used to provide a bound of the probability mass found so far."}, {"heading": "Acknowledgements", "text": "Many thanks to Tommi Jaakkola for providing datasets and answering various questions. Thanks to Ricardo Silva for pointing me to Jaakkola et al [9]. The following SCIP developers helped with various aspects of their framework: Tobias Achterberg, Timo Berthold, Ambros Gleixner, Stefan Heinz, Michael Winkler and Kati Wolter. Particular thanks to Kati Wolter for advice on how to generate extra Gomory cuts. Three anonymous reviewers provided valuable advice on how to improve the original submission."}], "references": [{"title": "Constraint Integer Programming", "author": ["Tobias Achterberg"], "venue": "PhD thesis, TU Berlin,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "SCIP: Solving constraint integer programs", "author": ["Tobias Achterberg"], "venue": "Mathematical Programming Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Large-sample learning of Bayesian networks is NP-hard", "author": ["David Maxwell Chickering", "David Heckerman", "Christopher Meek"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Introduction to Algortithms", "author": ["Thomas H. Cormen", "Charles E. Leiserson", "Ronald L. Rivest"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "Efficient maximum likelihood pedigree reconstruction", "author": ["Robert G. Cowell"], "venue": "Theoretical Popluation Biology,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Bayesian network learning by compiling to weighted MAX-SAT", "author": ["James Cussens"], "venue": "In Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence (UAI", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Maximum likelihood pedigree reconstruction using integer programming", "author": ["James Cussens"], "venue": "In Proceedings of the Workshop on Constraint Based Methods for Bioinformatics (WCB-10),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Structure learning of Bayesian networks using constraints", "author": ["C de Campos", "Z Zeng", "Q Ji"], "venue": "In Proc. of the 26th International Conference on Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Learning Bayesian network structure using LP relaxations", "author": ["Tommi Jaakkola", "David Sontag", "Amir Globerson", "Marina Meila"], "venue": "In Proceedings of 13th International Conference on Artificial Intelligence and Statistics (AISTATS 2010),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Exact Bayesian structure discovery in Bayesian networks", "author": ["Mikko Koivisto", "Kismat Sood"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "On sensitivity of the MAP Bayesian network structure to the equivalent sample size parameter", "author": ["T. Silander", "P. Kontkanen", "P. Myllym\u00e4ki"], "venue": "Proceedings of the The 23rd Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "A simple approach for finding the globally optimal Bayesian network structure", "author": ["Tomi Silander", "Petri Myllym\u00e4ki"], "venue": "In UAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}], "referenceMentions": [{"referenceID": 2, "context": "Unfortunately, as is well-known [3], this optimisation problem is NP-hard for any reasonable score, even if the number of parents for any vertex in the DAG is limited to two.", "startOffset": 32, "endOffset": 35}, {"referenceID": 9, "context": "One approach is to use dynamic programming [10, 12, 5] which has successfully been used for exact learning up to around 30 vertices.", "startOffset": 43, "endOffset": 54}, {"referenceID": 11, "context": "One approach is to use dynamic programming [10, 12, 5] which has successfully been used for exact learning up to around 30 vertices.", "startOffset": 43, "endOffset": 54}, {"referenceID": 4, "context": "One approach is to use dynamic programming [10, 12, 5] which has successfully been used for exact learning up to around 30 vertices.", "startOffset": 43, "endOffset": 54}, {"referenceID": 6, "context": "IP has already been used by Cussens [7] for the special case of BN pedigree reconstruction (see Section 8) and by Jaakkola et al [9] for general BN learning with a limit on parent set size.", "startOffset": 36, "endOffset": 39}, {"referenceID": 8, "context": "IP has already been used by Cussens [7] for the special case of BN pedigree reconstruction (see Section 8) and by Jaakkola et al [9] for general BN learning with a limit on parent set size.", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "However here IP is effected in a more conventional manner than Jaakkola et al, using the SCIP (Solving Constraint Integer Programs) framework [2].", "startOffset": 142, "endOffset": 145}, {"referenceID": 5, "context": "As in other work ([6, 8, 7, 9, 5]) this simple observation is used to prune the set of local scores.", "startOffset": 18, "endOffset": 33}, {"referenceID": 7, "context": "As in other work ([6, 8, 7, 9, 5]) this simple observation is used to prune the set of local scores.", "startOffset": 18, "endOffset": 33}, {"referenceID": 6, "context": "As in other work ([6, 8, 7, 9, 5]) this simple observation is used to prune the set of local scores.", "startOffset": 18, "endOffset": 33}, {"referenceID": 8, "context": "As in other work ([6, 8, 7, 9, 5]) this simple observation is used to prune the set of local scores.", "startOffset": 18, "endOffset": 33}, {"referenceID": 4, "context": "As in other work ([6, 8, 7, 9, 5]) this simple observation is used to prune the set of local scores.", "startOffset": 18, "endOffset": 33}, {"referenceID": 4, "context": "(Cowell [5] uses this pruning but does not mention it.", "startOffset": 8, "endOffset": 11}, {"referenceID": 5, "context": "The BN learning problem is encoded as an integer program using the same variables as in some previous work [6, 7, 9].", "startOffset": 107, "endOffset": 116}, {"referenceID": 6, "context": "The BN learning problem is encoded as an integer program using the same variables as in some previous work [6, 7, 9].", "startOffset": 107, "endOffset": 116}, {"referenceID": 8, "context": "The BN learning problem is encoded as an integer program using the same variables as in some previous work [6, 7, 9].", "startOffset": 107, "endOffset": 116}, {"referenceID": 6, "context": "In [7], Cussens considers two approaches.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "However, Jaakkola et al [9] provide a much tighter class of constraints without the need for auxiliary variables.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "In the linear relaxation the integrality constraint on family variables is removed and they can take any value in [0, 1].", "startOffset": 114, "endOffset": 120}, {"referenceID": 0, "context": "\u201d [1].", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": "This is because the sub-IP is just finding a cycle in a manner similar to that of standard depth-first search [4].", "startOffset": 110, "endOffset": 113}, {"referenceID": 8, "context": "The \u2018lower\u2019 4 datasets were supplied as local scores and are the same as those used in [9].", "startOffset": 87, "endOffset": 90}, {"referenceID": 8, "context": "As previously noted, this work is most closely related to that of Jaakkola et al [9], since the key 1-cluster constraints were introduced in that paper.", "startOffset": 81, "endOffset": 84}, {"referenceID": 10, "context": "This closeness depends crucially on the score (particularly the choice of Dirichlet priors [11]) as well as, of course, the amount of data.", "startOffset": 91, "endOffset": 95}, {"referenceID": 4, "context": "Cowell [5] has applied a dynamic programming approach to exact pedigree reconstruction whereas Cussens [7] used the IP approach outlined in Section 3", "startOffset": 7, "endOffset": 10}, {"referenceID": 6, "context": "Cowell [5] has applied a dynamic programming approach to exact pedigree reconstruction whereas Cussens [7] used the IP approach outlined in Section 3", "startOffset": 103, "endOffset": 106}, {"referenceID": 8, "context": "Thanks to Ricardo Silva for pointing me to Jaakkola et al [9].", "startOffset": 58, "endOffset": 61}], "year": 2011, "abstractText": "The problem of learning the structure of Bayesian networks from complete discrete data with a limit on parent set size is considered. Learning is cast explicitly as an optimisation problem where the goal is to find a BN structure which maximises log marginal likelihood (BDe score). Integer programming, specifically the SCIP framework, is used to solve this optimisation problem. Acyclicity constraints are added to the integer program (IP) during solving in the form of cutting planes. Finding good cutting planes is the key to the success of the approach\u2014the search for such cutting planes is effected using a sub-IP. Results show that this is a particularly fast method for exact BN learning.", "creator": "TeX"}}}