{"id": "1202.3767", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Distributed Anytime MAP Inference", "abstract": "We present a distributed anytime algorithm for performing MAP inference in graphical models. The problem is formulated as a linear programming relaxation over the edges of a graph. The resulting program has a constraint structure that allows application of the Dantzig-Wolfe decomposition principle. Subprograms are defined over individual edges and can be computed in a distributed manner. This accommodates solutions to graphs whose state space does not fit in memory. The decomposition master program is guaranteed to compute the optimal solution in a finite number of iterations, while the solution converges monotonically with each iteration. Formulating the MAP inference problem as a linear program allows additional (global) constraints to be defined; something not possible with message passing algorithms. Experimental results show that our algorithm's solution quality outperforms most current algorithms and it scales well to large problems.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (418kb)", "http://arxiv.org/abs/1202.3767v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["joop van de ven", "fabio ramos"], "accepted": false, "id": "1202.3767"}, "pdf": {"name": "1202.3767.pdf", "metadata": {"source": "CRF", "title": "Distributed Anytime MAP Inference", "authors": ["Joop van de Ven", "Fabio Ramos"], "emails": ["j.vandeven@acfr.usyd.edu.au", "f.ramos@acfr.usyd.edu.au"], "sections": [{"heading": null, "text": "We present a distributed anytime algorithm for performing MAP inference in graphical models. The problem is formulated as a linear programming relaxation over the edges of a graph. The resulting program has a constraint structure that allows application of the Dantzig-Wolfe decomposition principle. Subprograms are defined over individual edges and can be computed in a distributed manner. This accommodates solutions to graphs whose state space does not fit in memory. The decomposition master program is guaranteed to compute the optimal solution in a finite number of iterations, while the solution converges monotonically with each iteration. Formulating the MAP inference problem as a linear program allows additional (global) constraints to be defined; something not possible with message passing algorithms. Experimental results show that our algorithm\u2019s solution quality outperforms most current algorithms and it scales well to large problems."}, {"heading": "1 Introduction", "text": "Undirected graphical models are powerful tools for modelling many real world problems. They have been successfully applied to a diverse set of problems such as: image processing [1], protein design [2], and text labelling [3]. One desirable operation on such models is to infer their most probable configuration; the maximum a posteriori (MAP) problem. For tree-structured graphs algorithms exist that are guaranteed to compute the globally optimal MAP solution in polynomial time (see for example: [4, 5]).\nFinding the MAP solution for arbitrary graphs has been proven to be NP-hard [6]. For such graphs ap-\nproximation algorithms are required to generate solutions in a feasible time-span. One particularly popular algorithm is based on Max-Product Belief Propagation due to Pearl [5]. Max-Product is exact for treestructured graphs. For arbitrary graphs the algorithm has been adapted such that it runs for a number of iterations and is known as Loopy Belief Propagation (LBP, see [7]). LBP has (at best) weak guarantees on convergence and optimality, i.e. local optimality or guarantees for specific types of loopy graphs. Despite this, the algorithm has been shown to generate good results for a large number of problems.\nIn this paper we propose a novel algorithm for the MAP inference problem of graphical model G = (V,E). Starting from a quadratic formulation over the nodes, s \u2208 V (analogous to [8]), the problem is transformed into an integer formulation over the edges, (s, t) \u2208 E. It is subsequently relaxed into a Linear Program (LP). The transformation from quadratic to linear increases both the number of variables (O(|V |k) \u2192 O(|E|k2)) as well as constraints (O(|V |) \u2192 O(|E| +  s\u2208V (|N (s)|\u2212 1)k)). Where for ease of notation each node has k states, while N (s) are the neighbours of node s. The resulting LP formulation is equivalent to the standard MAP LP formulation (see for example [9]). However, defining the LP over the edge variables has certain advantages as discussed next.\nFor problems where the proposed LP relaxation fits into memory any LP solver may be used to compute the optimal solution directly. For medium to largescale problems there may not be sufficient memory available to solve the LP. Our algorithm is particularly suited for these cases as we explore the structure of the constraints to allow the application of the Dantzig-Wolfe decomposition principle [10]. The decomposition principle reformulates the LP into a number of subprograms, one for each edge in our case, together with a master program. The subprograms can be solved independently and distributed, while the\nmaster program solves for the optimal solution using a significantly reduced number of variables. The algorithm is solved iteratively, solutions to the subprograms are used to update the master program and vice versa. At each iteration the decomposition principle guarantees that the solution will be closer to the optimal solution, i.e. an anytime algorithm.\nThe advantages of our algorithm are: 1) it is able to solve very complex problems that few other algorithms are able to solve; 2) it scales well for large problems and allows the use of linear programming even for graphs where the state space does not fit in memory; 3) it can be distributed and effectively use the multicore hardware currently available and; 4) it allows the definition of global constraints which are difficult to enforce in message-passing based algorithms. This is important for a number of real-world problems involving graph matching and data association."}, {"heading": "2 Related Work", "text": "Several variants of Max-Product Belief Propagation have been proposed. Generalised Belief Propagation [11] extends the message passing from pairs of connected nodes to higher order cliques resulting in better approximations. Tree-Reweighted Max-Product methods (TRW, [12, 13]) on the other hand decompose the original graph into a convex combination of treestructured graphs. The tree-structured graphs guarantee efficient computations, while the convex combination allows the computation of an upper bound on the optimal solution. However, Yanover et al. [14] showed that TRW fails to solve the problems used in the experiments of section 5.1.\nTRW has strong connections to the Max-Product Linear Programming (MPLP) algorithm proposed by Globerson and Jaakkola [15]. MPLP is defined as a block coordinate descent in the dual of a LP relaxation constrained by the local marginal polytope. MPLP has all the advantages of message passing algorithms but it also has strong convergence guarantees. However, as the problem is solved in the dual, it means optimising an upper bound and not the MAP problem directly. As a result, MPLP is also unable to solve most of the problems used in the experiments (see section 5.1). The method developed by Sontag et al. [2, 16] is considered the state of the art in MAP inference. It extends MPLP by iteratively adding clusters to the MPLP formulation thus approximating the marginal polytope.\nKomodakis et al. [17] solve the MAP problem by decomposition. Starting from the same LP relaxation as MPLP, the problem is transformed into its unconstrained dual Lagrangian using Lagrange multipliers.\nIn the dual the problem is decomposed, where each sub problem is formed by a spanning tree (similar to TRW methods). The solutions of the sub problems are used in a projected sub-gradient method to update the potential values. However, their method (Dual Decomposition, DD) requires all potential values to be updated and communicated to each sub problem. Thus making this approach sub-optimal for problems with large state spaces.\nRavikumar and Lafferty [8] formulate the MAP problem as a Quadratic Program (QP) relaxation. QP relaxations are a more natural fit to the MAP problem as pair-wise potentials are quadratic in nature. For many practical problems the QP relaxation is non-convex, thus requiring further approximations in order to produce an algorithm that is solvable in polynomial time. The drawback to using a QP relaxation is that it requires memory squared in the number of states. So for medium to large scale problems this approach quickly becomes impractical. In addition, the QP relaxation has been shown to generate poorer results compared to a LP relaxation [18].\nMore recently, Kumar and Zilberstein [19] approached the MAP estimation problem with an interesting mean field approximation method. Their method approximates the problem by considering only distributions that factorise as a product of distributions over individual nodes. The resulting non-convex problem is represented by an equivalent mixture of Bayes nets with one network for each edge. ExpectationMaximisation (EM, [20]) is subsequently used to derive a message passing algorithm. The EM message passing algorithm is computationally efficient but sensitive to initial conditions.\nThe performance of message passing algorithms degrades significantly for large scale problems. Due to memory restrictions it may not be possible to keep all pair-wise potentials in memory. In such cases, potentials will have to be recomputed with each message sent. This is not an algorithmic issue but a practical one, nonetheless it adds to the computational cost. In addition large messages need to be constructed, again increasing the cost. Our approach does not suffer this limitation as it is intrinsically distributed. Edge potentials are computed only once for each distributed subprogram. Furthermore, our approach permits specification of constraints on the solution; something not possible with the above methods. Finally, we show that our algorithm is able to solve graphs that TRW methods and MPLP ([12, 13] and [15] respectively) are unable to solve, since our method solves the primal directly rather than optimise a bound."}, {"heading": "3 LP Formulation", "text": "An undirected graphical model G = (V,E) represents a probability distribution pG(x1, . . . , xN ) overN = |V | variables. The vertices s \u2208 V of the graph index the random variables xs of the distribution, while the edges (s, t) \u2208 E of the graph capture relationships between variables xs and xt. Let C be the set of all cliques of the graph. The distribution must factor as a product of clique potentials \u03c6c(Xc), where c \u2208 C and Xc are the clique\u2019s variables. Yedidia et al. [11] showed that, without loss of generality, it is possible to assume that the graph is a pair-wise Markov Random Field, i.e. the set of cliques CM = {(s, t) \u2208 E}. As a result, the log of the distribution of X = {x1, . . . , xN}, for the graph G with potentials \u03a6 = {\u03c6c|c \u2208 CM} is given by:\nlog pG(X;\u03a6) = \ns\u2208V \u03c6s(xs)+\n\n(s,t)\u2208E\n\u03c6st(xs, xt)\u2212 C, (1)\nwhere C is the log of the partition function. In the remainder we only consider variables xs that take on values from a finite discrete set Xs. Each xs is a vector of length |Xs| with elements xis \u2208 {0, 1} and  i x i s = 1. Furthermore, without loss of generality we include the local potentials in the pair-wise potentials. For discrete graphical models the combined pair-wise potential may then be expressed as:\nQst = \u03c6st + (e\u03c6 T s )/|N (s)|+ (\u03c6teT )/|N (t)|, (2)\nwhere N (s) is the set of neighbours of node s. Division by the number of neighbours distributes the local potential evenly over the pair-wise potentials while leaving the MAP value unaltered. The vector e is an appropriately sized vector of 1s. This leads to the following quadratic integer MAP problem,\nX MAP = argmax\nX log pG(X;\u03a6)\n= argmax X\n\n(s,t)\u2208E\nx T s Qstxt. (3)\nFor small scale problems equation 3 may be solved by a QP relaxation. However, for medium or large scale problems the resulting relaxation will quickly become too large to fit into memory. Instead we reformulate the quadratic objective function into a LP by substitution. The substitution transforms the problem from an optimisation over the nodes into one over the edges with a constraint structure that allows the LP to be solved in a distributed manner.\nFor each edge (s, t) \u2208 E define the edge variable\nyst = (x i sx j t |i = 1, . . . , |Xs|, j = 1, . . . , |Xt|, (s, t) \u2208 E) (4)\nwhich, by the discrete nature of xs and xt, has elements ykst \u2208 {0, 1} and  k y k st = 1 (k = 1, . . . , |Xs||Xt|). Equally, the cost cst can be constructed from Qst by ordering the elements of Qst into a vector corresponding to the elements of yst. The LP relaxation to the MAP problem is then formulated as:\nMaximise (s,t)\u2208E c T styst Subject to Astyst \u2212Asuysu = 0 \u2200s \u2208 V, t \u2208 N (s),\n\u2200u \u2208 N (s) \\ t k y k st = 1 \u2200(s, t) \u2208 E 0 \u2264 ykst \u2264 1 \u2200(s, t) \u2208 E, k = 1, . . . , |Xs||Xt|.\n(5)\nThe constraints defined by the matrix coefficients Ast are the consistency constraints. The elements of Ast are 0 or 1 such that Ai,\u2022st yst = x i s, where A i,\u2022 st is the i-th row of Ast. Consistency constraints are discussed in more detail in section 3.1. The second set of constraints express a uniqueness of solution for each edge; only one element of the edge variable yst may be active. While the third set of constraints capture the relaxation from an integer program to a linear program.\nEquation 5 is equivalent to standard MAP LP formulation (see for example [9]). The advantage of using equation 5 over the standard formulation is that it has fewer variables and constraints. The reduction in variables is straightforward since local potentials (variables) are included in the pair-wise variables. The constraints are less obvious but more important. As shall be shown in section 4, the communication overhead is proportional to the number of constraints; fewer constraints are preferable. For problems with large state space and small treewidth, equation 5 has significantly fewer constraints compared to variables (section 5.2 contains an example). As a result the proposed method will have a smaller communications overhead compared to, for example, the Dual Decomposition method [17]."}, {"heading": "3.1 Solution Consistency", "text": "Equation 5 is defined over the edges. As such constraints are required to ensure the solution remains consistent in the node variables.\nDefinition Let ms|t = Astyst be a marginal, for node variable xs, of the edge variable yst.\nProposition 3.1 The solution for the edge variables {yst|\u2200t \u2208 N (s)} is consistent in the node variable xs when the marginals {ms|t|\u2200t \u2208 N (s)} are all equal.\nProof The proof can be obtained by simple substitution of ykst = x i sx j t and  i x i s = 1.\nConsistency constraints are specified over pairs of edges, i.e. as the difference between pairs of marginals ms|t and ms|u. For a given node s one edge is used as the reference edge; edge (s, t) in equation 5. All consistency constraints are specified relative to the reference edge resulting in a minimum of constraints generated. Subsequently solving equation 5 will result in a solution for the edge variables. The mapping from yst to the node variables xs is given by the following proposition.\nProposition 3.2 If the linear program of equation 5 has a feasible solution, then the mapping from yst to xs is given by xs = ms|t for any t \u2208 N (s).\nProof The equality xs = ms|t follows directly from the definitions of Ast, yst and ms|t. Proposition 3.1 permits any t \u2208 N (s) provided the solution is consistent. Solution consistency, and therefore proposition 3.1, is ensured by virtue of a feasible solution; all constraints are met."}, {"heading": "3.2 Additional Requirements", "text": "Certain optimisation problems have additional requirements (or constraints) imposed on them. To ensure neighbouring nodes xs and xt have distinct (or equality) solutions for states i and j respectively, add one of the following constraints to equation 5 for each such state:\nDst,ijyst \u2264 1 \u2200(s, t) \u2208 E, \u2200(i, j) : xis = x j t Est,ijyst = 0 \u2200(s, t) \u2208 E, \u2200(i, j) : xis = x j t ,\n(6)\nwhere Dst,ij = A i,\u2022 st + A j,\u2022 ts ensures a distinct solution, while Est,ij = A i,\u2022 st \u2212A j,\u2022 ts ensures an equal solution between the states i and j of nodes xs and xt. Note that the constraints of equation 6 are defined locally. This approach can easily be extended to global constraints. However, this is omitted for brevity."}, {"heading": "4 Decomposition", "text": "The Dantzig-Wolfe decomposition principle [10] allows a LP with a special block-matrix structure to be broken up into a number of independent subprograms. The subprograms are iteratively adjusted to take into account global state (simplex multipliers) due to a master program. The reader is referred to [10, Chapter 10] for a detailed discussion and proofs. In this\nsection we provide an interpretation of the principle in the context of MAP inference for graphical models.\nThe block-angular system,\nMaximise cT1 y1 + . . . + c T KyK Subject to B1y1 + . . . + BKyK = b F1y1 = f1\n. . . ...\nFKyK = fK yi \u2265 0 i = 1, . . . ,K,\n(7)\nallows decomposition to be applied to the MAP problem for K = |E|. The matrices {Bi|i = 1, . . . ,K} form the coupling constraints, they capture interactions between subprograms. The constraints unique to each subprogram are constructed from {Fi|i = 1, . . . ,K}.\nThe constraints of equation 5 have this block-angular structure. Bst is constructed from the set {Ast} for edge (s, t), appropriately padded with 0s to ensure all B have the same number of rows. If additional requirements are specified (see section 3.2) then the sets {Dst} and {Est} are also included in Bst. The subprogram constraints are the uniqueness of solution constraints; Fst =  yst.\nThe decomposition principle exploits the Resolution Theorem [10]. Briefly, the Resolution Theorem states that every feasible solution of the convex polyhedral set Ax = b, x \u2265 0 can be represented as a convex combination of its extreme points1 (see [10, Theorem 10.5] for more details). Using the Resolution Theorem, equations of the form of equation 7 can be transformed into a master program together with K subprograms. The master program maximises a convex combination of extreme points, while the subprograms generate extreme points at each iteration.\nWe now present the steps of the algorithm, each of the steps are discussed in more detail in the sections to follow:\n1. Initialise the algorithm (section 4.1) to find an initial basic feasible solution.\n2. Solve the master program using columns corresponding to the initial basic feasible solution. This provides global state in the form of the simplex multipliers (\u03c0, \u03b3); see section 4.3 for more details.\n3. Solve all subprograms using the current simplex multipliers (section 4.2).\n1Normalised extreme homogeneous solutions are omitted as our subprograms cannot generate these.\n4. Add columns to the master program according to optimality of subprogram solutions and corresponding column cost. Solve the master program to obtain new simplex multipliers (\u03c0, \u03b3); section 4.3.\n5. If the master program has found the optimal solution go to step 6, if not go to step 3.\n6. Transform the master program\u2019s solution to the solution of equation 5 and perform rounding if required, see section 4.4 for more details."}, {"heading": "4.1 Initialisation", "text": "The aim of initialisation is to find an initial basic feasible solution. One common approach to initialising Dantzig-Wolfe decomposition is using a Simplex Phase 1 approach [10, Section 10.2.4]. This involves finding the maxima of each subprogram using the actual costs. The resulting solutions are used to start the master program. However, in our case the subprograms are trivial. This generally means that the consistency constraints are violated, thus preventing the decomposition from even starting. Instead when no additional requirements are specified, the procedure of algorithm 1 can be used to find an initial basic feasible solution in one step.\nAlgorithm 1 Pseudo-code of algorithm initialisation.\n1: Input: Graph G = (V,E), potentials \u03c6s, \u2200s \u2208 V and \u03c6st, \u2200(s, t) \u2208 E 2: Output: Initial basic feasible solution {y\u0303st|\u2200(s, t) \u2208 E} 3: for s \u2208 V do 4: \u03c6\u0303\u2190 \u03c6s 5: for t \u2208 N (s) do 6: \u03c6\u0303\u2190 \u03c6\u0303+\n xt \u03c6st\n7: end for 8: x\u0303s \u2190 argmaxxs(\u03c6\u0303) 9: end for\n10: for (s, t) \u2208 E do 11: y\u0303st \u2190 (x\u0303isx\u0303 j t |i = 1, . . . , |Xs|, j = 1, . . . , |Xt|) 12: end for\nAs can be seen from algorithm 1, a node\u2019s initial solution x\u0303s is found by maximising over the sum of local and marginalised pair-wise potentials. Once the initial solutions for each node have been found, they are mapped to their equivalent subprogram initial basic feasible solutions y\u0303st. Since the initial solutions y\u0303st are based on node solutions, the consistency constraints are always met. The subprograms\u2019 initial basic feasible solutions are subsequently used to get the decomposition master program started.\nWhen additional requirements (see section 3.2) are specified the above procedure is not guaranteed to find an initial basic feasible solution. In such cases one can adjust line 8 such that it takes account of additional requirements. If this is not possible then an initial solution will have to be obtained via other means. This requires replacing the for-loop on line 3. For example, many solution constraints allow a trivial solution, x\u0303s can be initialised with this trivial solution. In the more general case, algorithms that solve Constraint Satisfaction Problems (see for example [21]) can be used to find an initial solution for x\u0303s."}, {"heading": "4.2 Subprogram", "text": "For inference in a graph, the subprograms maximise a linear program over the edges as follows:\nMaximise cTstyst \u2212BTst\u03c0 Subject to k y k st = 1\n0 \u2264 ykst \u2264 1 k = 1, . . . , |Xs||Xt|.\n(8)\nThe objective function of equation 8 is the actual cost of the edge, cst, adjusted by the current state of the interactions, BTst\u03c0. Here B T st are the concatenated consistency constraints (and optional additional requirements) while \u03c0 are the corresponding simplex multipliers (see section 4.3). This adjusted cost finds the edge\u2019s maximum based on the current global state of the algorithm. It is however not necessary to invoke a LP solver for each subprogram. There are two constraints for each edge, these express the uniqueness of solution (ykst \u2208 {0, 1} and  k y k st = 1). A solution to the subprograms can therefore be found by a simple maximisation over a vector; i.e. the solution is always an extreme point.\nLet y\u0302st,i represents the optimal solution for edge (s, t) at iteration i. If cTsty\u0302st,i \u2212 BTst\u03c0 = \u03b3st then this solution is globally sub-optimal and it may be incorporated into the master program, provided it has not previously been incorporated. In case of a tie (multiple solutions with the same maximum) standard simplex tie breaking rules can be applied. In the experiments we select either the solution with the lowest index k (analogous to Bland\u2019s rule [22]), or the index k for which the actual cost is maximal (analogous to the Largest-Coefficient rule [23])."}, {"heading": "4.3 Master Program", "text": "The purpose of the master program is twofold. First, it generates global state in the form of the simplex multipliers \u03c0 and \u03b3. Second, any feasible solution to the master program can be transformed into a solu-\ntion of the original LP, equation 5. At each iteration of the algorithm columns are added to the master program depending on optimality of subprogram solutions (see section 4.2). The added columns allow the master program to update the simplex multipliers based on subprogram solutions.\nFor the MAP inference problem, the master program is defined as shown in equation 9.\nMaximise (s,t)\u2208E  i\u2208Lst g i st\u03b1 i st Subject to (s,t)\u2208E  i\u2208Lst G \u2022,i st \u03b1 i st = 0\ni\u2208Lst \u03b1 i st = 1 \u2200(s, t) \u2208 E\n\u03b1ist \u2265 0 \u2200(s, t) \u2208 E, \u2200i \u2208 Lst,\n(9)\nwhere Lst is the set of iteration indices, of edge (s, t), for which columns have been added to the master program. The element gist = c T sty\u0302st,i is the cost of the subprogram solution at iteration i. While G\u2022,ist = Bsty\u0302st,i is the column of corresponding consistency constraints. The elements \u03b1ist are the convexity variables from the Resolution Theorem. They have a particularly elegant interpretation for the MAP problem; they represent the likelihood of the subprogram solutions y\u0302st,i. Note that \u03c0 are the simplex multipliers corresponding to the consistency constraints while \u03b3 are the simplex multipliers of the convexity variables.\nUp to |E| columns may be added to the master program, one for each sub-optimal subprogram, at each iteration. However, not all of these prospective columns will aid in finding a solution. Quite to the contrary, often they will add unnecessary complexity to the master program. Instead of adding all prospective columns to the master program, a limited number of columns may be added at each iteration. In such cases columns are selected based on their cost. At iteration i only columns corresponding to maximal costs gist are added (similar to pivoting in the simplex method).\nWith each iteration the master program will grow in size. It is possible that, after a number of iterations, the master program will become too complex for the solver to find a solution efficiently. In such situations the master program can be reduced in size and complexity simply by removing non-basic columns. Removal of non-basic columns does not impact on the solution quality as a LP solves a convex optimisation problem. It will however impact on the number of iterations until convergence. With more columns available the decomposition is able to find the optimal solution in fewer iterations, but it may take longer to solve each individual iteration.\nColumn generation procedures, such as Dantzig-Wolfe\nDecomposition, have the desirable property that both the size as well as the growth of the master program can be controlled. As a result even very large problems can be efficiently dealt with."}, {"heading": "4.4 Optimal Solution", "text": "Dantzig-Wolfe decomposition is guaranteed to converge in a finite number of iterations [10, Theorem 10.4]. Once converged, the solution to equation 5 can be found from the convexity variables \u03b1ist and the corresponding subprograms\u2019 optimal solutions y\u0302st,i:\ny\u030cst = \ni\u2208Lst\n\u03b1 i sty\u0302st,i. (10)\nThe globally optimal solution y\u030cst is the sum of the subprograms\u2019 optimal solutions scaled by their corresponding convexity variables (or likelihoods). The y\u030cst can be mapped back to a corresponding optimal node solution x\u030cs (see section 3.1).\nRounding may have to be applied to x\u030cs to find an integer solution. Instead of applying rounding schemes such as [24, 25], we construct an Integer Program (IP) over the non-zeros solution states of x\u030cs. For each graph the percentage of nodes for which there is no integer solution is generally quite low (less than 5% in the experiments). These fractional nodes generate a set of probable answers. Analysis also showed that often one of these probable answers is the true optimal solution.\nThe IP operates on the same graph but on only those states i (and corresponding potential values) for which x\u030cis = 0. The resulting IP is small and can be solved in a fraction of the time of the decomposition (typically less than 10% of the running time). The benefit of using an IP is that it is guaranteed to always find the best solution out of the set of probable answers.\nDW-LP\n2500 TRW-S\n2500 MPLP\n2500 MPLP-T\n2500"}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Side-Chain Prediction", "text": "The performance of the proposed algorithm (DW-LP) is measured on the Rosetta Side-Chain Prediction data set [14, 2]. This involves finding the three-dimensional configuration of rotamers given the backbone structure of a protein [14]. Following Sontag et al. [2] we apply our algorithm to the 30 graphs that TRW [12] is unable to solve. We compare our algorithm against TRW-S (which improves on TRW, see [13]) and MPLP [15] as both, like our algorithm, consider only the local marginal polytope. In addition we also show the result for MPLP with tightening [2]. This is the current state of the art, we refer to it as MPLP-T.\nBoth TRW-S and MPLP are run for 1000 iterations or until convergence, whichever comes first. MPLPT is operated as described in [2]. Like TRW-S and MPLP our algorithm is run for a maximum of 1000 iterations or until convergence. In addition up to 200 columns corresponding to maximal costs are added to the DW-LP master program at each iteration. While no non-basic columns are removed from the master program (see section 4.3).\nOnce finished the MAP value is computed from the node assignments. The MAP values are compared against their true values, differences smaller than 1e-6 are considered equal. The comparisons are given in table 1 and figure 2.\nAs can be seen from table 1 and figure 2, a like-for-like comparison shows that the proposed algorithm significantly outperforms both TRW-S as well as MPLP. It is able to solve more than half of the graphs to their optimal solution while the MAP error is much smaller in comparison. Note also that for all 30 graphs our algorithm converged to a solution within 1000 iterations (TRW-S and MPLP did not converge for all graphs). MPLP-T is able to solve all the problems as it itera-\ntively adds triplet clusters to the LP formulation and re-solves using MPLP. We believe that our algorithm can use such an iterative approach as well at the expense of computational cost.\nFigure 1 displays the monotonic convergence property of our algorithm for graph 1ug6. At the last iteration, when the master program finds its optimal solution, the size of the master program consist of fewer than 70,000 variables. On the other hand, equation 5 for the same graph consists of approximately 650,000 variables; a significant reduction. The average size of equation 5 is approximately 460,000 variables (for the 30 graphs), the average size of the master program (upon convergence) is approximately 42,000 variables.\nThe implementation for TRW-S and MPLP methods are optimised C++ implementations from the respective authors\u2019 web-sites. Their average running times over all 30 graphs are: 2.70s, 37.16s, and 42.75s for TRW-S, MPLP, and MPLP-T respectively. Our algorithm is fully implemented in Matlab with the exception of the LP solver (CPLEX). The average running time for our algorithm is 46.49s when solving equation 5 directly (as all graphs fit in memory). When using the decomposition (equations 8 and 9) the average running time increases to 200.74s which is mostly due to additional overhead added in Matlab. The decomposition takes longer, but its strength is that it can handle large-scale problems with global constraints (which\nnone of the other methods can) as shown in the next section."}, {"heading": "5.2 Shape Matching", "text": "Shape matching can be seen as a data association problem where each point of a curve needs to be uniquely associated to another point of a different curve. In [26] Ramos et al. introduce a Conditional Random Field (CRF) for matching two sets of laser range finder data using only local shape information. Compared to the Side-Chain Prediction problem of section 5.1, this problem is not as difficult to optimise (the distribution is quite peaked). However, the scan matching problem is characterised by a very large state space if formulated as a LP relaxation, up to 362361 possible combinations. The reader is referred to [26] for further details on the CRF and its feature functions.\nThe quality of scan matching solutions improves if all nodes occupy a unique state (with the exception of a catch-all outlier state). In [26] LBP is used which does not allow for such constraints to be considered in the inference process. We therefore demonstrate the strength of our algorithm for dealing with both large-scale problems as well as external global constraints. Note that we are unable to apply TRW or MPLP methods as their implementations require all pair-wise potentials to be kept in memory. This exceeds the memory available on our test environment (4GB). Our LBP implementation overcomes the memory issue by recomputing the pair-wise potentials with each message sent.\nIn [26] 20 labelled data sets are used for training. Leave one out cross-validation is used on these 20 data sets to compare the quality of solution of LBP and the proposed algorithm. LBP will run for a maximum of 10 iterations or until convergence, whichever occurs first. The proposed algorithm will be run in a distributed environment. Subprograms are solved by a cluster of 5 machines with 4 or 8 CPU cores each. At each iteration all sub-optimal subprograms are added to the master program. Once the master program takes more than 2.5 seconds to solve, all non-basic columns are removed. Finally, the algorithm is run for a maximum of 250 iterations or until convergence. The comparisons are given in table 2 while figure 3 shows partial matching results for a single graph.\nIn figure 3 the blue crosses and red stars represent positions of an object measured by the laser range finder. The range finder will measure the same object but from different poses. The aim is to find unique matches between blue crosses and red stars represented by the black lines.\nThe results of table 2 show that our algorithm pro-\nduces a higher accuracy compared to LBP. The removal of many-to-one matches is particularly important for scan matching. However, LBP matches several points of an object in one set to the same point in the other set (figure 3), something not physically possible with rigid objects. Our DW-LP algorithm permits constraints on the solution, thereby eliminating all many-to-one matches and preserving object rigidity."}, {"heading": "6 Discussion", "text": "Many real-world problems are characterised, not only by the difficulty in solving them, but also by the size of the problem and constraints on its solution. Such problems require a different approach in algorithm design.\nThis paper presented a novel algorithm for distributed MAP inference based on LP decomposition. Unlike other LP (or QP) formulations ours is defined over edge variables instead of node variables. The advantage of such a formulation is that the LP has fewer constraints and allows decomposition into a number of subprograms (one for each edge) together with a small master program. The subprograms can be distributed over a network to allow large-scale problems to be solved efficiently. In addition, the master program monotonically converges to its optimal solution\nresulting in an anytime algorithm for performing MAP inference.\nExperimental results show that the algorithm finds solutions comparable to current state-of-the-art and scales well to large problems. Additionally, the experiments showed that the algorithm can successfully be applied to problems that involve global constraints; a difficult task for message-passing based algorithms."}], "references": [{"title": "Learning conditional random fields for stereo", "author": ["D. Scharstein", "C. Pal"], "venue": "In Proc. of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Tightening lp relaxations for map using message-passing", "author": ["D. Sontag", "T. Meltzer", "A. Globerson", "Y. Weiss", "T. Jaakkola"], "venue": "In 24th Conference in Uncertainty in Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In Proc. of the International Conference on Machine Learning (ICML),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Exact maximum a posteriori estimation for binary images", "author": ["D.M. Greig", "B.T. Porteous", "A.H. Seheult"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1989}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1988}, {"title": "Finding maps for belief networks is np-hard", "author": ["S.E. Shimony"], "venue": "Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["K. Murphy", "Y. Weiss", "M. Jordan"], "venue": "In Proc. of the Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Quadratic programming relaxations for metric labeling and markov random field map estimation", "author": ["P. Ravikumar", "J. Lafferty"], "venue": "Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "A linear programming approach to maxsum problem: A review", "author": ["T. Werner"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Linear Programming 2: Theory and Extensions. Springer Series in Operations Research and Financial Engineering", "author": ["G.B. Dantzig", "M.N. Thapa"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Exploring Artificial Intelligence in the New Millennium, chapter Understanding Belief Propagation and Its Generalizations", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Map estimation via agreement on trees: Message passing and linear programming", "author": ["M. Wainwright", "T. Jaakola", "A. Willsky"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Convergent tree-reweighted message passing for energy minimization", "author": ["V. Kolmogorov"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Linear programming relaxations and belief propagation \u2013 an empirical study", "author": ["C. Yanover", "T. Meltzer", "Y. Weiss"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Fixing max-product: Convergent message passing algorithms for map lprelaxations", "author": ["A. Globerson", "T. Jaakkola"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Clusters and coarse partitions in lp relaxations", "author": ["D. Sontag", "A. Globerson", "T. Jaakkola"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Mrf optimization via dual decomposition: Message-passing revisited", "author": ["N. Komodakis", "N. Paragios", "G Tziritas"], "venue": "In In ICCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "An analysis of convex relaxations for map estimation of discrete mrfs", "author": ["M. Pawan Kumar", "V. Kolmogorov", "P.H.S. Torr"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Map estimation for graphical models by likelihood maximization", "author": ["A. Kumar", "S. Zilberstein"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1977}, {"title": "Constraint Processing", "author": ["R Dechter"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "New finite pivoting rules for the simplex method", "author": ["R.G. Bland"], "venue": "Mathematics of Operations Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1977}, {"title": "Finite Mathematics: An Applied Approach (Eighth Edition)", "author": ["A. Mizrahi", "M. Sullivan"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2000}, {"title": "Randomized rounding: a technique for provably good algorithms and algorithmic proofs", "author": ["P. Raghavan", "Clark D. Tompson"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1987}, {"title": "Convex relaxation methods for graphical models: Lagrangian and maximum entropy approaches", "author": ["J.K. Johnson"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "CRFmatching: Conditional random fields for feature-based scan matching", "author": ["F. Ramos", "D. Fox", "H. Durrant-Whyte"], "venue": "In Proc. of Robotics: Science and Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "They have been successfully applied to a diverse set of problems such as: image processing [1], protein design [2], and text labelling [3].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "They have been successfully applied to a diverse set of problems such as: image processing [1], protein design [2], and text labelling [3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "They have been successfully applied to a diverse set of problems such as: image processing [1], protein design [2], and text labelling [3].", "startOffset": 135, "endOffset": 138}, {"referenceID": 3, "context": "For tree-structured graphs algorithms exist that are guaranteed to compute the globally optimal MAP solution in polynomial time (see for example: [4, 5]).", "startOffset": 146, "endOffset": 152}, {"referenceID": 4, "context": "For tree-structured graphs algorithms exist that are guaranteed to compute the globally optimal MAP solution in polynomial time (see for example: [4, 5]).", "startOffset": 146, "endOffset": 152}, {"referenceID": 5, "context": "Finding the MAP solution for arbitrary graphs has been proven to be NP-hard [6].", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "One particularly popular algorithm is based on Max-Product Belief Propagation due to Pearl [5].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "For arbitrary graphs the algorithm has been adapted such that it runs for a number of iterations and is known as Loopy Belief Propagation (LBP, see [7]).", "startOffset": 148, "endOffset": 151}, {"referenceID": 7, "context": "Starting from a quadratic formulation over the nodes, s \u2208 V (analogous to [8]), the problem is transformed into an integer formulation over the edges, (s, t) \u2208 E.", "startOffset": 74, "endOffset": 77}, {"referenceID": 8, "context": "The resulting LP formulation is equivalent to the standard MAP LP formulation (see for example [9]).", "startOffset": 95, "endOffset": 98}, {"referenceID": 9, "context": "Our algorithm is particularly suited for these cases as we explore the structure of the constraints to allow the application of the Dantzig-Wolfe decomposition principle [10].", "startOffset": 170, "endOffset": 174}, {"referenceID": 10, "context": "Generalised Belief Propagation [11] extends the message passing from pairs of connected nodes to higher order cliques resulting in better approximations.", "startOffset": 31, "endOffset": 35}, {"referenceID": 11, "context": "Tree-Reweighted Max-Product methods (TRW, [12, 13]) on the other hand decompose the original graph into a convex combination of treestructured graphs.", "startOffset": 42, "endOffset": 50}, {"referenceID": 12, "context": "Tree-Reweighted Max-Product methods (TRW, [12, 13]) on the other hand decompose the original graph into a convex combination of treestructured graphs.", "startOffset": 42, "endOffset": 50}, {"referenceID": 13, "context": "[14] showed that TRW fails to solve the problems used in the experiments of section 5.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "TRW has strong connections to the Max-Product Linear Programming (MPLP) algorithm proposed by Globerson and Jaakkola [15].", "startOffset": 117, "endOffset": 121}, {"referenceID": 1, "context": "[2, 16] is considered the state of the art in MAP inference.", "startOffset": 0, "endOffset": 7}, {"referenceID": 15, "context": "[2, 16] is considered the state of the art in MAP inference.", "startOffset": 0, "endOffset": 7}, {"referenceID": 16, "context": "[17] solve the MAP problem by decomposition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Ravikumar and Lafferty [8] formulate the MAP problem as a Quadratic Program (QP) relaxation.", "startOffset": 23, "endOffset": 26}, {"referenceID": 17, "context": "In addition, the QP relaxation has been shown to generate poorer results compared to a LP relaxation [18].", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "More recently, Kumar and Zilberstein [19] approached the MAP estimation problem with an interesting mean field approximation method.", "startOffset": 37, "endOffset": 41}, {"referenceID": 19, "context": "ExpectationMaximisation (EM, [20]) is subsequently used to derive a message passing algorithm.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "Finally, we show that our algorithm is able to solve graphs that TRW methods and MPLP ([12, 13] and [15] respectively) are unable to solve, since our method solves the primal directly rather than optimise a bound.", "startOffset": 87, "endOffset": 95}, {"referenceID": 12, "context": "Finally, we show that our algorithm is able to solve graphs that TRW methods and MPLP ([12, 13] and [15] respectively) are unable to solve, since our method solves the primal directly rather than optimise a bound.", "startOffset": 87, "endOffset": 95}, {"referenceID": 14, "context": "Finally, we show that our algorithm is able to solve graphs that TRW methods and MPLP ([12, 13] and [15] respectively) are unable to solve, since our method solves the primal directly rather than optimise a bound.", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "[11] showed that, without loss of generality, it is possible to assume that the graph is a pair-wise Markov Random Field, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Equation 5 is equivalent to standard MAP LP formulation (see for example [9]).", "startOffset": 73, "endOffset": 76}, {"referenceID": 16, "context": "As a result the proposed method will have a smaller communications overhead compared to, for example, the Dual Decomposition method [17].", "startOffset": 132, "endOffset": 136}, {"referenceID": 9, "context": "The Dantzig-Wolfe decomposition principle [10] allows a LP with a special block-matrix structure to be broken up into a number of independent subprograms.", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "The decomposition principle exploits the Resolution Theorem [10].", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "In the more general case, algorithms that solve Constraint Satisfaction Problems (see for example [21]) can be used to find an initial solution for x\u0303s.", "startOffset": 98, "endOffset": 102}, {"referenceID": 21, "context": "In the experiments we select either the solution with the lowest index k (analogous to Bland\u2019s rule [22]), or the index k for", "startOffset": 100, "endOffset": 104}, {"referenceID": 22, "context": "which the actual cost is maximal (analogous to the Largest-Coefficient rule [23]).", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "Instead of applying rounding schemes such as [24, 25], we construct an Integer Program (IP) over the non-zeros solution states of x\u030cs.", "startOffset": 45, "endOffset": 53}, {"referenceID": 24, "context": "Instead of applying rounding schemes such as [24, 25], we construct an Integer Program (IP) over the non-zeros solution states of x\u030cs.", "startOffset": 45, "endOffset": 53}, {"referenceID": 13, "context": "The performance of the proposed algorithm (DW-LP) is measured on the Rosetta Side-Chain Prediction data set [14, 2].", "startOffset": 108, "endOffset": 115}, {"referenceID": 1, "context": "The performance of the proposed algorithm (DW-LP) is measured on the Rosetta Side-Chain Prediction data set [14, 2].", "startOffset": 108, "endOffset": 115}, {"referenceID": 13, "context": "This involves finding the three-dimensional configuration of rotamers given the backbone structure of a protein [14].", "startOffset": 112, "endOffset": 116}, {"referenceID": 1, "context": "[2] we apply our algorithm to the 30 graphs that TRW [12] is unable to solve.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[2] we apply our algorithm to the 30 graphs that TRW [12] is unable to solve.", "startOffset": 53, "endOffset": 57}, {"referenceID": 12, "context": "We compare our algorithm against TRW-S (which improves on TRW, see [13]) and MPLP [15] as both, like our algorithm, consider only the local marginal polytope.", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "We compare our algorithm against TRW-S (which improves on TRW, see [13]) and MPLP [15] as both, like our algorithm, consider only the local marginal polytope.", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "In addition we also show the result for MPLP with tightening [2].", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "MPLPT is operated as described in [2].", "startOffset": 34, "endOffset": 37}, {"referenceID": 25, "context": "In [26] Ramos et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "The reader is referred to [26] for further details on the CRF and its feature functions.", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "In [26] LBP is used which does not allow for such constraints to be considered in the inference process.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "In [26] 20 labelled data sets are used for training.", "startOffset": 3, "endOffset": 7}], "year": 2011, "abstractText": "We present a distributed anytime algorithm for performing MAP inference in graphical models. The problem is formulated as a linear programming relaxation over the edges of a graph. The resulting program has a constraint structure that allows application of the Dantzig-Wolfe decomposition principle. Subprograms are defined over individual edges and can be computed in a distributed manner. This accommodates solutions to graphs whose state space does not fit in memory. The decomposition master program is guaranteed to compute the optimal solution in a finite number of iterations, while the solution converges monotonically with each iteration. Formulating the MAP inference problem as a linear program allows additional (global) constraints to be defined; something not possible with message passing algorithms. Experimental results show that our algorithm\u2019s solution quality outperforms most current algorithms and it scales well to large problems.", "creator": "TeX"}}}