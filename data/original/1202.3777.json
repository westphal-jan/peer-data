{"id": "1202.3777", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Belief Propagation by Message Passing in Junction Trees: Computing Each Message Faster Using GPU Parallelization", "abstract": "Compiling Bayesian networks (BNs) to junction trees and performing belief propagation over them is among the most prominent approaches to computing posteriors in BNs. However, belief propagation over junction tree is known to be computationally intensive in the general case. Its complexity may increase dramatically with the connectivity and state space cardinality of Bayesian network nodes. In this paper, we address this computational challenge using GPU parallelization. We develop data structures and algorithms that extend existing junction tree techniques, and specifically develop a novel approach to computing each belief propagation message in parallel. We implement our approach on an NVIDIA GPU and test it using BNs from several applications. Experimentally, we study how junction tree parameters affect parallelization opportunities and hence the performance of our algorithm. We achieve speedups ranging from 0.68 to 9.18 for the BNs studied.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (156kb)", "http://arxiv.org/abs/1202.3777v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DC", "authors": ["lu zheng", "ole mengshoel", "jike chong"], "accepted": false, "id": "1202.3777"}, "pdf": {"name": "1202.3777.pdf", "metadata": {"source": "CRF", "title": "Belief Propagation by Message Passing in Junction Trees: Computing Each Message Faster Using GPU Parallelization", "authors": ["Lu Zheng", "Ole Mengshoel", "Jike Chong"], "emails": ["ole.mengsheol@sv.cmu.edu", "jike@berkeley.edu"], "sections": [{"heading": null, "text": "Compiling Bayesian networks (BNs) to junction trees and performing belief propagation over them is among the most prominent approaches to computing posteriors in BNs. However, belief propagation over junction tree is known to be computationally intensive in the general case. Its complexity may increase dramatically with the connectivity and state space cardinality of Bayesian network nodes. In this paper, we address this computational challenge using GPU parallelization. We develop data structures and algorithms that extend existing junction tree techniques, and specifically develop a novel approach to computing each belief propagation message in parallel. We implement our approach on an NVIDIA GPU and test it using BNs from several applications. Experimentally, we study how junction tree parameters affect parallelization opportunities and hence the performance of our algorithm. We achieve speedups ranging from 0.68 to 9.18 for the BNs studied."}, {"heading": "1 Introduction", "text": "Bayesian networks (BNs) are an effective tool in a diverse range of applications that require representation and reasoning with uncertain knowledge and data. Inference over BNs can be either exact or approximate. Perhaps the most popular exact inference algorithm, belief propagation in junction trees, relies on the compilation of a BN into a junction tree. Exact belief updating (or marginalization) is then performed by message passing over the junction tree [6]. Each node of a\n\u2217Email:lu.zheng, ole.mengsheol@sv.cmu.edu \u2020Email:jike@berkeley.edu\njunction tree is a clique computed from the moralized graph based on the original BN.\nHowever, belief propagation over junction trees is known to be computationally hard. Computational difficulty increases dramatically with the density of the BN, the treewidth of the network, and the number of states of each network node [9]. In addition, some practical issues associated with the specific implementation platform also affect the computation performance [1]. In our work, we address the computational problem of belief propagation on both the analytical and implementation levels.\nTwo fundamental issues, which may cause large cliques in junction trees, are: (i) the topology and connectedness of a BN [9] and (ii) the high cardinality of a significant set of discrete BN nodes [13]. Discrete BN nodes can have high cardinalities for several reasons: First, they may represent discrete parameters, for example categorical parameters, that inherently take a large number of values [13]. A second reason for highcardinality, discrete BN nodes is that they are used to represent continuous parameters. The number of states grows exponentially with the number of bits used when representing a quantized continuous parameter. Consequently, if a fine-grained discretization is used in BN nodes, the difficulty of computation may become a major challenge.\nThe above issues may cause very large cliques to be formed in junction trees, and thus hinder the application of BNs in cases where real-time inference is required. In addition, there can be major computational challenges when BN inference is in the inner loop of iterative algorithms like the EM algorithm [5]. Therefore, it is of great interest to develop parallel computing techniques to speed up junction tree inference. Recently, graphic processing units (GPUs) have become increasingly programmable and their parallel processing power can now be used for general purpose computation with Compute Unified Device Architecture (CUDA). However, due to the intricate nature of join\ntree computation and the distinctive GPU programming architecture, it is still a major challenge to adapt junction tree algorithms to the GPU. In this paper, we discuss data structures and algorithms that extend existing junction tree techniques [1,6], and specifically develop a novel approach to parallel message computation using belief propagation in junction trees.\nParallelization of Bayesian network computation has been investigated in previous research [2\u20134, 7, 8, 10, 14, 15]. A data parallel implementation for junction tree inference has been developed for a cachecoherent shared-address-space machine with physically distributed main memory [4]. Parallelism in the basic sum-product computation has been investigated for GPUs [14]. The efficiency in using disk memory for exact inference, using parallelism and other techniques, has been improved [3]; parallel techniques for BN structure learning have also been developed [7]. An algorithm for parallel BN inference using pointer jumping has been introduced [10]. Both parallelization based on graph structure [8] as well as node level primitives for parallel computing based on a table extension idea have been developed [15]; a GPU implementation based on this idea was later developed [2].\nIn this paper, we also focus on node level parallelism, motivated by the existence of very large cliques in junction trees from applications. In such settings, nodelevel operations are often the dominating part of the problem [15]. However, we take a different approach from previous research [2, 15], and in particular our approach is motivated by the cluster-sepset mapping method of Huang and Darwiche [1]. We develop a parallel message computation algorithm for junction tree belief propagation. The speedup of this parallel algorithm, relative to the sequential algorithm, is analyzed theoretically. Experimental results, with speedups ranging from 0.68 to 9.18, show our GPU implementation\u2019s performance as it varies according to the junction tree topology.\nOur paper is organized as follows: In Section 2, we review BNs, junction trees, and parallel computing using GPUs. In Section 3, we describe our parallel approach to message computation for belief propagation in junction trees. Experimental results are discussed in Section 4. In Section 5 we conclude and outline future research."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Belief Propagation in Junction Trees", "text": "A BN is a compact representation of a joint distribution over a set of random variables X . A BN is structured as a directed acyclic graph (DAG) whose ver-\ntices are the random variables and the directed edges represent dependency relationship among the random variables. The evidence in a Bayesian network consists of variables that have been instantiated.\nThe junction tree algorithm propagates beliefs (or posteriors) over a derived graph called a junction tree. A junction tree is generated from a BN by means of moralization and triangulation [6]. Each vertex Ci of the junction tree contains a subset of the random variables that forms a clique in the moralized and triangulated BN, denoted by Xi \u2286 X . Associated with each vertex of the junction tree there is a potential table \u03c6Xi . With the above notations, a junction tree can be defined as J = (T,\u03a6), where T represents a tree and \u03a6 represents all the potential tables associated with this tree. Assuming Ci and Cj are adjacent, a separator Sij is induced on a connecting edge. The variables contained in Sij are defined to be Xi \u2229 Xj .\nBelief propagation is invoked when we get new evidence e for a set of variables E \u2286 X . We need to update the potential tables \u03a6 to reflect this new information. To do this, belief propagation over the junction tree is used, this is a two-phase procedure: evidence collection and evidence distribution. For the evidence collection phase, messages are collected from the leaf vertices all the way up to a designated root vertex. For the evidence distribution phase, messages are distributed from the root vertex to the leaf vertices.\nFigure 1 shows a toy BN and the corresponding junction tree. As shown in the figure, during the evidence collection phase, the two leaf nodes pass messages to the root node {A,B,D}, updating its potential table. During the evidence distribution phase, the root node passes messages back to the two leaf nodes.\nMessage passing can be viewed as the atomic operation for belief propagation, both for evidence collection and distribution. Mathematically, a message passed from vertex i to vertex j can be written as:\n\u03c6\u2217Sij = \u2211\nXi/Sij\n\u03c6Xi , \u03c6 \u2217 Xj = \u03c6Xj \u03c6\u2217Sij \u03c6Sij , (1)\nwhere \u03c6\u2217X represents the updated potential table of vertex X . From these potential tables we can marginalize and compute P (X|e) where X \u2208 X and e is the evidence."}, {"heading": "2.2 CPU/GPU Platform", "text": "GPUs are designed for compute-intensive, highly parallel computations. Compared to CPUs, more transistors are in GPUs devoted to data processing rather than data caching and flow control. GPUs are well-suited to problems that can be expressed as\ndata-parallel computations where data elements are mapped to parallel processing threads. GPUs are typically used to accelerate compute-intensive parts of an application, and thus attached to a host CPU performing control-dominant computations. Today, a CPU and its GPU communicate via a PCI-Express bus.\nCUDA is a general-purpose parallel computing architecture developed by NVIDIA. CUDA consists of three key parallel computing abstractions: a hierarchy of thread groups, shared memories, and barrier synchronization. These abstractions are exposed to the programmer as a programming language, making CUDA a model that scales to an increasing number of processor cores [11]. Specifically, CUDA provides a fine-grained data and thread parallelism nested within coarse-grained data and task parallelism. A kernel is organized as a set of thread blocks when executed. A thread block is a batch of threads that all execute on one of the multiprocessors. As a result, they can cooperate by efficiently sharing data through shared memory, and can synchronize their execution to coordinate memory access. Blocks of the same size can execute the same kernel batched together as a grid of blocks. However, threads in different blocks cannot communicate and synchronize with each other."}, {"heading": "3 Computing Each Message Faster", "text": "We parallelize the atomic operation of belief propagation\u2013message passing, as shown in Figure 1. The advantage of doing so is that atomic level parallelism can be embedded in different belief propagation algorithms unobtrusively, without any change of those algorithms.\nAssociated with each junction tree vertex Ci and the contained set of variables Xi, there is a potential table \u03c6Xi containing non-negative real numbers that are proportional to the joint distribution of Xi. If each variable can take sj states, the size of the potential table is |\u03c6Xi | = \u220f|Xi|\nj=1 sj , where |Xi| is the cardinality of Xi.\nMessage passing from Ci to an adjacent vertex Ck, with separator Sik, involves two steps:\n1. Marginalization. The potential table \u03c6Sik of the separator is updated to \u03c6\u2217Sik by marginalizing the potential table \u03c6Xi :\n\u03c6\u2217Sik = \u2211\nXi/Sik\n\u03c6Xi . (2)\n2. Scattering. The potential table of Ck is updated using both the old and new table of Sik:\n\u03c6\u2217Xk = \u03c6Xk \u03c6\u2217Sik \u03c6Sik . (3)\nWe define 00 = 0 in this case, that is, if the denominator in (3) is zero, then we simply set the corresponding \u03c6\u2217Xk to zeros."}, {"heading": "3.1 Index Mapping for Parallelism", "text": "Although written in a compact form, each of equation (2) and (3) is actually a set of many equations updating all the cells in the potential tables \u03c6Sik and \u03c6Xk . Our key contribution is to efficiently parallelize the computations in (2) and (3) by partitioning these sets of equations into independent subsets of equations.\nThis can be done by taking a closer look at the data flow in the message passing procedure. We concentrate on the j-th element of the separator\u2019s potential table, i.e., \u03c6Sik(j). Here, \u03c6Sik(j) is the potential value associated with a specific instantiation of the variables in Sik. In the marginalization step, to update the value of \u03c6Sik(j), we need to retrieve values from the elements in \u03c6Xi which have the same instantiation for those variables. Note, the values of those elements in \u03c6Xi are only required for the computation related to \u03c6Sik(j). Similarly, in the scattering step, for all the elements in \u03c6Xk that have the same instantiation for the variables in Sik as \u03c6Sik(j), we need to multiply their value by \u03c6\u2217Sik(j)/\u03c6Sik(j). This suggests a natural way to compute (2) and (3) using data parallelism. To handle the computation related to each specific element in \u03c6Sik , we can assign a separate thread, as long as the GPU has threads available.\nFigure 2 illustrates the data flow in a message passing from the left child to the root node in Figure 1. In this\ncase, Xi = {A,B,D}, Xk = {B,C} and Sik = {B}. Let us assume that all random variables in {A,B,C} are binary with states {0, 1}. The light gray boxes in Figure 2 are the values used in computations related to the first element of \u03c6Sik , B = 0. The dark gray boxes are for computations related to the second element of \u03c6Sik , B = 1. The computation related to different elements in \u03c6Sik are independent, providing a natural opportunity for parallelism.\nThe only problem left is that we need to figure out the \u201cmapping relationship\u201d from the elements of \u03c6Xi and \u03c6Xk to the elements of \u03c6Sik . The mapping rule is that the corresponding elements in \u03c6Xi and \u03c6Sik should have the same instantiation for the variables in Sik. To this end, for a certain element in \u03c6Sik , say, \u03c6Sik(r), we first convert the index r into a state string Y r = (xr1, . . . , x r |Sik|\n) and then scan through all the elements of \u03c6Xi or \u03c6Xk . For the j-th element in \u03c6Xi (j = 1, . . . , |\u03c6Xi |), we convert the index j into a state string as Xji = (x j 1, . . . , x j |Xi| ) and check whether the variables that also appear in Sik take the same states as in Y r. If yes, \u03c6Xi(j) should be among the data to load in when we perform the marginalization for \u03c6Sik(r). Similarly, we can determine which elements should be updated in \u03c6Xk during the scattering phase."}, {"heading": "3.2 Index Mapping Table", "text": "Suppose the j-th random variable in Xi can take sj states. To convert the index j into the sequence of variable states takes O(|Xi| \u2211 j sj). Then the whole\nscanning process to match the elements of Xi and S will take O(|\u03c6Xi ||Xi| \u2211 j sj) time. This could be a considerable amount of computation time when the potential table size is large. It is extremely inefficient for every thread to scan through the whole potential table \u03c6Xi and \u03c6Xk , since only a small fraction of them will be used by each thread. To tackle this potential inefficiency, we introduce an index mapping table technique inspired by the cluster-sepset mapping (CSM) technique [1], where a mapping table \u00b5X ,S is created to store the index mappings from \u03c6X to \u03c6S . To adapt CSM to parallel computing, instead of creating one mapping table [1], we create |\u03c6Sik | mapping tables. In each mapping table \u00b5Xi,\u03c6Sik (j) we store the indices of the elements of \u03c6Xi mapping to the j-th separator table element. Mathematically, \u00b5Xi,\u03c6Sik (j) = {r \u2208 [0, |\u03c6Xi | \u2212 1] : \u03c6Xi(r) is mapped to \u03c6Sik(j)}.\nAlgorithm 1 Message Passing(\u03c6Xi , \u03c6Xk , \u03c6Sik)\nInput: \u03c6Xi , \u03c6Xk , \u03c6Sik . for j = 1 to |\u03c6Sik | in parallel do\nsep star=0; for n = 1 to |\u00b5Xi,sj | do sep star[j] = sep star[j]+\u03c6Xi(\u00b5Xi,sj [n]) end for for n = 1 to |\u00b5Xk,sj | do\n\u03c6Xk(\u00b5Xk,sj [n]) = sep star[j] \u03c6Sik [j] \u03c6Xk(\u00b5Xk,sj [n])\nend for end for\nThe thread that handles the j-th element of the separator potential table just needs to look up \u00b5Xi , \u03c6Sik(j) and retrieve the corresponding data from \u03c6Xi , as shown in Figure 3. Further, we avoid unnecessary recomputation of the mappings by precomputing them\nwhen the junction tree is established. Despite the requirement for memory (increased by the size of the clique potential table), our index mapping table often provides a large speedup.\nOur novel algorithm for one message passing is shown in Algorithm 1. A function that runs on the GPU as different threads is called a kernel. Algorithm 1 is wrapped into a kernel function, thus enabling parallelism. While beneficial from a parallelism perspective, there are kernel invocation overhead and memory latency issues associated with this use of a GPU, as we will further discuss below."}, {"heading": "3.3 Belief Propagation Algorithm", "text": "Algorithm 2 Collect Evidence(J, Ci)\nfor each child of Ci do Message Passing(Ci, Collect Evidence(J ,child)) end for return(Ci)\nAlgorithm 3 Distribute Evidence(J, Ci)\nfor each child of Ci do Message Passing(Ci, child) Distribute Evidence(J , child) end for\nAlgorithm 4 Belief Propagation(J, Croot)\nInput: J, Croot Initialization (J) Collect Evidence(J, Croot) Distribute Evidence(J, Croot)\nBelief propagation can be done using both breadthfirst and depth-first traversal over a junction tree. In our work, we consider the Hugin algorithm, which adopts depth-first belief propagation. Given an established junction tree J with root vertex Croot, the pseudo code is shown in Algorithm 4. We first initialize the junction tree by multiplying together the Bayesian network potential tables (CPTs). Then, a two phase belief propagation is adopted [6]: collect evidence and then distribute evidence."}, {"heading": "3.4 Analysis of Speedup", "text": "From the description above, one can see that the amount of parallelism is determined by the number of elements in the separators\u2019 potential table |\u03c6S |. Suppose the junction tree has n vertices; then the total number of message passings for full belief propagation is 2(n \u2212 1). Considering a message passed from Ci to\nCk, the total number of additions is (|\u03c6Xi |\u2212|\u03c6Sik |) and the total number of multiplications is (|\u03c6Xk |+ |\u03c6Sik |). Therefore the theoretical time complexity of one message passing between vertex i and k is\n(|\u03c6Xi | \u2212 |\u03c6Sik |) + (|\u03c6Xk |+ |\u03c6Sik |)\n|\u03c6Sik | =\n|\u03c6Xi |+ |\u03c6Xk |\n|\u03c6Sik | ,\nwhich gives |\u03c6Sik | times speedup over sequential code. Belief propagation is just a sequence of messages passed in a certain order [6].\nLet Ne(C) denote the neighbors of C in the join tree. The time complexity for belief propagation is\n\u2211\ni\n\u2211\nk\u2208Ne(Ci)\n|\u03c6Xi |+ |\u03c6Xk |\n|\u03c6Sik | . (4)\nKernel invocation overhead, incurred each time Algorithm 1 is invoked, turns out to be an important performance factor. If we model the invocation overhead for each kernel call to be a constant \u03c4 , then the time complexity becomes\n\u2211\ni\ndi\u03c4 + \u2211\ni\n\u2211\nk\u2208Ne(Ci)\n|\u03c6Xi |+ |\u03c6Xk |\n|\u03c6Sik | , (5)\nwhere di is the degree of a node Ci. In a tree structure,\u2211 di = 2(n\u2212 1). Thus the GPU time complexity is\n2(n\u2212 1)\u03c4 + \u2211\ni\n\u2211\nk\u2208Ne(Ci)\n|\u03c6Xi |+ |\u03c6Xk |\n|\u03c6Sik | . (6)\nFrom this equation, we can see that junction tree topology impacts GPU performance in at least two ways: the total invocation overhead is proportional to the number of nodes in the junction tree, while the separator table sizes determine the degree of parallelism.\nThe overall speedup of our novel parallel belief propagation approach is determined by the equation\nSpeedup =\n\u2211 i \u2211 k\u2208Ne(Ci) (|\u03c6Xi |+ |\u03c6Xk |)\n2(n\u2212 1)\u03c4 + \u2211\ni \u2211 k\u2208Ne(Ci) (|\u03c6Xi |+|\u03c6Xk |)\n|\u03c6Sik |\n.\nClearly, performance is closely related to the distribution of the size of the separators\u2019 and cliques\u2019 potential tables. A simple bound for the speedup is mini,k |\u03c6Sik | \u2264 Speedup \u2264 maxi,k |\u03c6Sik |. For the kind of junction tree that has mostly large separators, our parallel algorithm is expected to perform very well. The worst case is that all the separators of the junction tree are small. However, even in this case, since |\u03c6S | \u2265 2, we are in theory guaranteed to have at least two times speedup over sequential code. However, taking into account that the CPU/GPU platform incurs invocation overhead and the long memory\nlatency when loading data from slow device memory to fast shared memory, the theoretical speedup is hard to achieve in practice.\nFrom the equations above, we can estimate the overall belief propagation speedup to be around the average potential table size \u00af|\u03c6Sik |. We take an experimental approach to study how the structure of the junction trees affects the performance of our parallel technique on the CPU/GPU setting in Section 4."}, {"heading": "4 Experimental Results", "text": "In our work, we use the NVIDIA GeForce GTX460 as the platform for our implementation. This device consists of seven multiprocessors, and each multiprocessor consists of 48 cores and 48K on-chip shared memory per thread block. The peak thread level parallelism achieves 907GFlop/s. In addition to the fast shared memory, a much larger but slower off-chip global memory (785 MB) that is shared by all multiprocessors is provided. The bandwidth between the global and shared memories is about 90 Gbps. In the computation, we are using single precision."}, {"heading": "4.1 Methods and Data", "text": "Our implementation is tested on a number of Bayesian networks (see http://bndg.cs.aau.dk/\nhtml/bayesian_networks.html). They are from different problem domains, with varying structures and state spaces. These differences lead to very different junction trees, as reflected in Table 1. In our work, we would like to not only to compare the performance of our parallel code to the sequential code, but also study how the structure of junction tree\u2014for example the size of the separators\u2019 potential table\u2014affects performance in the parallel case versus the sequential case. We compile the Bayesian networks into the junction trees offline and then run belief propagation over the junction trees, see Algorithm 4.\nAs mentioned in Section 3, the performance is related to the distribution of the size of the separators\u2019 potential table, i.e., |\u03c6S |. Hence we also present histograms of the potential table sizes for all the junction trees in Figure 4."}, {"heading": "4.2 Optimization on GPU", "text": "Our novel message computation algorithm (Algorithm 1) is wrapped into a kernel to enable GPU parallelism, and a kernel is organized as a set of thread blocks when executed. Varying the thread block size may impact performance, and we would like to optimize thread block and grid size for each of the experimental junction trees. We experimented with varying block sizes and picked the best one for a given BN.\nFigure 5 shows how execution time changes with block size for Bayesian networks Barley and Munin3. For Barley, GTX460 achieves the optimal performance when each block contains 48 threads. While for Munin3, optimal performance is found with 16 threads per block. The performance differences can be explained by the degree of match between the configuration of the GPU architecture and the junction tree structure. On the GPU, each thread block is executed on one multiprocessor. To fully make use of the GPU\u2019s computing resource, at least 7 thread blocks are needed, each assigned to one multiprocessor. However, for Munin3, the separator potential tables are very small. Less than 7 blocks are created for the message passing, leaving some multiprocessor unused. The size of Barley \u2019s separator potential tables are mostly large enough to fully use the computing resource. In Figure 4, we present the histogram counts of the potential table sizes of Barley and Munin3.\nFigure 6 illustrates the scalability of the speedup. We order the junction trees according to the average size of the separator potential tables, and plot the speedup relative to the average separator potential table. In general, the junction tree with larger average separator potential table has better speedup. This coincides with our analysis (see Section 3.4) that larger separator potential tables provide more opportunity for parallelization and hence better performance."}, {"heading": "4.3 Performance Comparison with Sequential Code", "text": "As a baseline, we implemented a sequential program on an Intel CPU. The execution time of the program is comparable to that of GeNie/SMILE [12], a widely used C++ software package for Bayesian network inference. We do not directly use GeNie/SMILE as the baseline here, because we do not know the implementation details of GeNie/SMILE. Detailed information for the CPU and GPU platforms is in Table 2.\nTable 1 gives the execution time comparison for the GTX460 and the Intel CPU. The obtained speedup ranges from 0.68 to 9.18. The performance is an overall effect of many factors such as parallelism, memory latency, kernel invocation overhead, etc. Those factors, in turn, are closely correlated with the underlying structures of the junction trees. Networks Pigs, Munin2, Munin3 and Munin4 mostly consist of small vertices and separators (see Figure 4). There are only limited opportunities for message computation parallelism, resulting in limited speedup. On the\nother hand, for Mildew, Diabetes, Barley and Water, the potential histograms in Figure 4 skew to the right. This explains why they have very good speedup. Two extreme examples are Barley and Pigs. The best performer, Barley, has a total of 36 cliques, and an average separator table size of 39,318. This topology provides abundant opportunity for parallelism and consequently a good speedup over the sequential code. Pigs, on the other hand, has 368 cliques, and furthermore the average separator size is as small as 339. This is a junction tree with a large number of small cliques and separators, with very restricted opportunity for parallelism in message computation."}, {"heading": "4.4 Kernel Overhead", "text": "For our parallel inference algorithm implementation on the GPU, we should also consider the overhead incurred when launching a kernel (kernel overhead). Figure 7 shows kernel overhead as a fraction of total execution time. Kernel overhead percentage is determined by the number of kernel invocations and the amount of computation per kernel invocation, which in turn is determined by the structure of the junction trees.\nKernel overhead may greatly affect the performance. For Munin3, for example, the overhead counts for as\nmuch as 36% in the overall execution time. This gives an upper bound of 2.71 on the speedup over sequential code. However, for Barley, the overhead is only 1.7% of the overall execution time. The variation in overhead percentages is caused by the differences in the structures of the Munin3 and Barley junction trees, see Table 1 and Figure 4."}, {"heading": "4.5 Memory Layout", "text": "Appropriate data layout in the global GPU memory makes a big difference in the memory latency associated with bringing data into shared GPU memory. In our algorithm, the mapping table method is essentially an indirect addressing for the data. In Figure 8, we compare two memory layouts for the mapping tables. On the left hand side of Figure 8 is a naive approach; mapping tables are just placed sequentially in global memory. This may cause bank conflicts when loading the data into the shared memory. Therefore, we introduce an advanced approach: for the mapping tables from a separator to a clique potential table, we put the elements with the same index in mapping tables in adjacent memory cells, as shown on the right hand side of Figure 8. In our experience, this advanced memory layout gave a 20% - 30 % improvement in the overall speedup compared to the naive layout."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we have developed a novel approach to parallel belief propagation over junction trees, based on the cluster-sepset mapping method. Our approach focuses on the parallelization of message computation for message passing in junction trees. In our approach, the parallel opportunity is in theory equal to the size of the separator potential table. Although practical issues such as kernel overhead and memory latency make it hard to achieve this theoretic performance, our experimental results still indicate that performance\nscales well with the separator potential table sizes.\nIn experiments with a CUDA implementation of our parallel message computation algorithm executing on an NVIDIA GeForce GTX460 GPU, we explored how performance varies with different junction tree structures. As expected from our analysis, we found that our approach performs well for junction trees with large separator potential tables. However, performance is compromised if the junction tree consists of many small nodes with corresponding small separator potential tables. Speedup ranged from 0.68 to 9.18.\nOur future work will be focused on improving the parallel computing performance for junction tree message passing over small separators. A possible solution is merging small separators into large ones, and performing belief propagation over such modified junction trees."}, {"heading": "Acknowledgments", "text": "This material is based upon work supported by NSF awards CCF0937044 and ECCS0931978."}], "references": [{"title": "Inference in belief networks: A procedural guide", "author": ["C. Huang", "A. Darwiche"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Parallel exact inference on a CPU-GPGPU heterogenous system", "author": ["H. Jeon", "Y. Xia", "V.K. Prasanna"], "venue": "In Proc. of the 39th International Conference on Parallel Processing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "BEEM: bucket elimination with external memory", "author": ["K. Kask", "R. Dechter", "A. Gelfand"], "venue": "In Proc. of the 26th Annual Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "A parallel Lauritzen-Spiegelhalter algorithm for probabilistic inference", "author": ["A.V. Kozlov", "J.P. Singh"], "venue": "In Proc. of the 1994 ACM/IEEE conference on Supercomputing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "The EM algorithm for graphical association models with missing data", "author": ["S.L. Lauritzen"], "venue": "Computational Statistics and Data Analysis,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Local computations with probabilities on graphical structures and their application to expert systems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1988}, {"title": "Highthroughput Bayesian network learning using heterogeneous multicore computers", "author": ["M.D. Linderman", "R. Bruggner", "V. Athalye", "T.H. Meng", "N.B. Asadi", "G.P. Nolan"], "venue": "In Proc. of the 24th ACM International Conference on Supercomputing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "GraphLab: A new framework for parallel machine learning", "author": ["Y. Low", "J. Gonzalez", "A. Kyrola", "D. Bickson", "C. Guestrin", "J. Hellerstein"], "venue": "In Proc. of the 26th Annual Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Understanding the scalability of Bayesian network inference using clique tree growth curves", "author": ["O.J. Mengshoel"], "venue": "Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Scalable parallel implementation of exact inference in Bayesian networks", "author": ["V.K. Namasivayam", "V.K. Prasanna"], "venue": "In Proc. of the 12th International Conference on Parallel and Distributed System,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Efficient inference in large discrete domains", "author": ["R. Sharma", "D. Poole"], "venue": "In Proc. of the 19th Annual Conference on Uncertainity in Artificial Intelligence", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Efficient computation of sumproducts on GPUs through software-managed cache", "author": ["M. Silberstein", "A. Schuster", "D. Geiger", "A. Patney", "J.D. Owens"], "venue": "In Proc. of the 22nd ACM International Conference on Supercomputing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Node level primitives for parallel exact inference", "author": ["Y. Xia", "V.K. Prasanna"], "venue": "In Proc. of the 19th International Symposium on Computer Architechture and High Performance Computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}], "referenceMentions": [{"referenceID": 5, "context": "Exact belief updating (or marginalization) is then performed by message passing over the junction tree [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 8, "context": "Computational difficulty increases dramatically with the density of the BN, the treewidth of the network, and the number of states of each network node [9].", "startOffset": 152, "endOffset": 155}, {"referenceID": 0, "context": "In addition, some practical issues associated with the specific implementation platform also affect the computation performance [1].", "startOffset": 128, "endOffset": 131}, {"referenceID": 8, "context": "Two fundamental issues, which may cause large cliques in junction trees, are: (i) the topology and connectedness of a BN [9] and (ii) the high cardinality of a significant set of discrete BN nodes [13].", "startOffset": 121, "endOffset": 124}, {"referenceID": 10, "context": "Two fundamental issues, which may cause large cliques in junction trees, are: (i) the topology and connectedness of a BN [9] and (ii) the high cardinality of a significant set of discrete BN nodes [13].", "startOffset": 197, "endOffset": 201}, {"referenceID": 10, "context": "Discrete BN nodes can have high cardinalities for several reasons: First, they may represent discrete parameters, for example categorical parameters, that inherently take a large number of values [13].", "startOffset": 196, "endOffset": 200}, {"referenceID": 4, "context": "In addition, there can be major computational challenges when BN inference is in the inner loop of iterative algorithms like the EM algorithm [5].", "startOffset": 142, "endOffset": 145}, {"referenceID": 0, "context": "In this paper, we discuss data structures and algorithms that extend existing junction tree techniques [1,6], and specifically develop a novel approach to parallel message computation using belief propagation in junction trees.", "startOffset": 103, "endOffset": 108}, {"referenceID": 5, "context": "In this paper, we discuss data structures and algorithms that extend existing junction tree techniques [1,6], and specifically develop a novel approach to parallel message computation using belief propagation in junction trees.", "startOffset": 103, "endOffset": 108}, {"referenceID": 1, "context": "Parallelization of Bayesian network computation has been investigated in previous research [2\u20134, 7, 8, 10, 14, 15].", "startOffset": 91, "endOffset": 114}, {"referenceID": 2, "context": "Parallelization of Bayesian network computation has been investigated in previous research [2\u20134, 7, 8, 10, 14, 15].", "startOffset": 91, "endOffset": 114}, {"referenceID": 3, "context": "Parallelization of Bayesian network computation has been investigated in previous research [2\u20134, 7, 8, 10, 14, 15].", "startOffset": 91, "endOffset": 114}, {"referenceID": 6, "context": "Parallelization of Bayesian network computation has been investigated in previous research [2\u20134, 7, 8, 10, 14, 15].", "startOffset": 91, "endOffset": 114}, {"referenceID": 7, "context": "Parallelization of Bayesian network computation has been investigated in previous research [2\u20134, 7, 8, 10, 14, 15].", "startOffset": 91, "endOffset": 114}, {"referenceID": 9, "context": "Parallelization of Bayesian network computation has been investigated in previous research [2\u20134, 7, 8, 10, 14, 15].", "startOffset": 91, "endOffset": 114}, {"referenceID": 11, "context": "Parallelization of Bayesian network computation has been investigated in previous research [2\u20134, 7, 8, 10, 14, 15].", "startOffset": 91, "endOffset": 114}, {"referenceID": 12, "context": "Parallelization of Bayesian network computation has been investigated in previous research [2\u20134, 7, 8, 10, 14, 15].", "startOffset": 91, "endOffset": 114}, {"referenceID": 3, "context": "A data parallel implementation for junction tree inference has been developed for a cachecoherent shared-address-space machine with physically distributed main memory [4].", "startOffset": 167, "endOffset": 170}, {"referenceID": 11, "context": "Parallelism in the basic sum-product computation has been investigated for GPUs [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "The efficiency in using disk memory for exact inference, using parallelism and other techniques, has been improved [3]; parallel techniques for BN structure learning have also been developed [7].", "startOffset": 115, "endOffset": 118}, {"referenceID": 6, "context": "The efficiency in using disk memory for exact inference, using parallelism and other techniques, has been improved [3]; parallel techniques for BN structure learning have also been developed [7].", "startOffset": 191, "endOffset": 194}, {"referenceID": 9, "context": "An algorithm for parallel BN inference using pointer jumping has been introduced [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 7, "context": "Both parallelization based on graph structure [8] as well as node level primitives for parallel computing based on a table extension idea have been developed [15]; a GPU implementation based on this idea was later developed [2].", "startOffset": 46, "endOffset": 49}, {"referenceID": 12, "context": "Both parallelization based on graph structure [8] as well as node level primitives for parallel computing based on a table extension idea have been developed [15]; a GPU implementation based on this idea was later developed [2].", "startOffset": 158, "endOffset": 162}, {"referenceID": 1, "context": "Both parallelization based on graph structure [8] as well as node level primitives for parallel computing based on a table extension idea have been developed [15]; a GPU implementation based on this idea was later developed [2].", "startOffset": 224, "endOffset": 227}, {"referenceID": 12, "context": "In such settings, nodelevel operations are often the dominating part of the problem [15].", "startOffset": 84, "endOffset": 88}, {"referenceID": 1, "context": "However, we take a different approach from previous research [2, 15], and in particular our approach is motivated by the cluster-sepset mapping method of Huang and Darwiche [1].", "startOffset": 61, "endOffset": 68}, {"referenceID": 12, "context": "However, we take a different approach from previous research [2, 15], and in particular our approach is motivated by the cluster-sepset mapping method of Huang and Darwiche [1].", "startOffset": 61, "endOffset": 68}, {"referenceID": 0, "context": "However, we take a different approach from previous research [2, 15], and in particular our approach is motivated by the cluster-sepset mapping method of Huang and Darwiche [1].", "startOffset": 173, "endOffset": 176}, {"referenceID": 5, "context": "A junction tree is generated from a BN by means of moralization and triangulation [6].", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "To tackle this potential inefficiency, we introduce an index mapping table technique inspired by the cluster-sepset mapping (CSM) technique [1], where a mapping table \u03bcX ,S is created to store the index mappings from \u03c6X to \u03c6S .", "startOffset": 140, "endOffset": 143}, {"referenceID": 0, "context": "To adapt CSM to parallel computing, instead of creating one mapping table [1], we create |\u03c6Sik | mapping tables.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "Then, a two phase belief propagation is adopted [6]: collect evidence and then distribute evidence.", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "Belief propagation is just a sequence of messages passed in a certain order [6].", "startOffset": 76, "endOffset": 79}], "year": 2011, "abstractText": "Compiling Bayesian networks (BNs) to junction trees and performing belief propagation over them is among the most prominent approaches to computing posteriors in BNs. However, belief propagation over junction tree is known to be computationally intensive in the general case. Its complexity may increase dramatically with the connectivity and state space cardinality of Bayesian network nodes. In this paper, we address this computational challenge using GPU parallelization. We develop data structures and algorithms that extend existing junction tree techniques, and specifically develop a novel approach to computing each belief propagation message in parallel. We implement our approach on an NVIDIA GPU and test it using BNs from several applications. Experimentally, we study how junction tree parameters affect parallelization opportunities and hence the performance of our algorithm. We achieve speedups ranging from 0.68 to 9.18 for the BNs studied.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}