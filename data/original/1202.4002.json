{"id": "1202.4002", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2012", "title": "Generalized Principal Component Analysis (GPCA)", "abstract": "This paper presents an algebro-geometric solution to the problem of segmenting an unknown number of subspaces of unknown and varying dimensions from sample data points. We represent the subspaces with a set of homogeneous polynomials whose degree is the number of subspaces and whose derivatives at a data point give normal vectors to the subspace passing through the point. When the number of subspaces is known, we show that these polynomials can be estimated linearly from data; hence, subspace segmentation is reduced to classifying one point per subspace. We select these points optimally from the data set by minimizing certain distance function, thus dealing automatically with moderate noise in the data. A basis for the complement of each subspace is then recovered by applying standard PCA to the collection of derivatives (normal vectors). Extensions of GPCA that deal with data in a high- dimensional space and with an unknown number of subspaces are also presented. Our experiments on low-dimensional data show that GPCA outperforms existing algebraic algorithms based on polynomial factorization and provides a good initialization to iterative techniques such as K-subspaces and Expectation Maximization. We also present applications of GPCA to computer vision problems such as face clustering, temporal video segmentation, and 3D motion segmentation from point correspondences in multiple affine views.", "histories": [["v1", "Fri, 17 Feb 2012 20:07:25 GMT  (864kb)", "http://arxiv.org/abs/1202.4002v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["rene vidal", "yi ma", "shankar sastry"], "accepted": false, "id": "1202.4002"}, "pdf": {"name": "1202.4002.pdf", "metadata": {"source": "CRF", "title": "Generalized Principal Component Analysis (GPCA)", "authors": ["Ren\u00e9 Vidal"], "emails": [], "sections": [{"heading": null, "text": "unknown and varying dimensions from sample data points. We represent the subspaces with a set of homogeneous polynomials whose degree is the number of subspaces and whose derivatives at a data point give normal vectors to the subspace passing through the point. When the number of subspaces is known, we show that these polynomials can be estimated linearly from data; hence, subspace segmentation is reduced to classifying one point per subspace. We select these points optimally from the data set by minimizing certain distance function, thus dealing automatically with moderate noise in the data. A basis for the complement of each subspace is then recovered by applying standard PCA to the collection of derivatives (normal vectors). Extensions of GPCA that deal with data in a highdimensional space and with an unknown number of subspaces are also presented. Our experiments on low-dimensional data show that GPCA outperforms existing algebraic algorithms based on polynomial factorization and provides a good initialization to iterative techniques such as K-subspaces and Expectation Maximization. We also present applications of GPCA to computer vision problems such as face clustering, temporal video segmentation, and 3Dmotion segmentation from point correspondences in multiple affine views.\nIndex Terms\u2014Principal component analysis (PCA), subspace segmentation, Veronese map, dimensionality reduction, temporal\nvideo segmentation, dynamic scenes and motion segmentation."}, {"heading": "1 INTRODUCTION", "text": "PRINCIPAL Component Analysis (PCA) [12] refers to theproblem of fitting a linear subspace S IRD of unknown dimension d < D to N sample points fxjgNj\u00bc1 in S. This problem shows up in a variety of applications inmany fields, e.g., pattern recognition, data compression, regression, image processing, etc., and can be solved in a remarkably simple way from the singular value decomposition (SVD) of the (mean-subtracted) data matrix \u00bdx1; x2; . . . ; xN 2 IRD N .1 With noisy data, this linear algebraic solution has the geometric interpretation of minimizing the sum of the squared distances from the (noisy) data points xj to their projections ~xj in S.\nIn addition to these algebraic and geometric interpretations, PCA can also be understood in a probabilistic manner. In Probabilistic PCA [20] (PPCA), the noise is assumed to be drawn from an unknown distribution and the problem becomes one of identifying the subspace and distribution parameters in a maximum-likelihood sense. When the noise\ndistribution is Gaussian, the algebro-geometric and prob-\nabilistic interpretations coincide [2]. However, when the\nnoise distribution is non-Gaussian, the solution toPPCA is no\nlonger linear, as shown in [2], where PCA is generalized to\narbitrary distributions in the exponential family. Another extension of PCA is nonlinear principal compo-\nnents (NLPCA) or Kernel PCA (KPCA),which is the problem of identifying a nonlinear manifold from sample points. The standard solution toNLPCA [16] is based on first embedding the data into a higher-dimensional feature space F and then applying standard PCA to the embedded data. Since the dimension of F can be large, a more practical solution is obtained from the eigenvalue decomposition of the so-called kernel matrix; hence, the name KPCA. One of the disadvantages of KPCA is that, in practice, it is difficult to determine which kernel function to use because the choice of the kernel naturally depends on the nonlinear structure of the manifold to be identified. In fact, learning kernels is an active topic of research in machine learning. To the best of our knowledge, our work is the first one to prove analytically that the Veronese map (a polynomial embedding) is the natural embedding for data lying in a union of multiple subspaces.\nIn this paper, we consider the following alternative\nextension of PCA to the case of data lying in a union of subspaces, as illustrated in Fig. 1 for two subspaces of IR3.\nProblem (Subspace Segmentation). Given a set of pointsX \u00bc fxj2 IRDgNj\u00bc1 drawn from n 1 different linear subspaces fSi IRDgni\u00bc1 of dimension di \u00bc dim\u00f0Si\u00de, 0 < di < D, without knowing which points belong to which subspace:\n1. find the number of subspaces n and their dimensions fdigni\u00bc1, 2. find a basis for each subspace Si (or for S ? i ), and 3. group the N sample points into the n subspaces.\n. R. Vidal is with the Center for Imaging Science, Department of Biomedical Engineering, The Johns Hopkins University, 308B Clark Hall, 3400 N. Charles Street, Baltimore, MD 21218. E-mail: rvidal@cis.jhu.edu. . Y. Ma is with the Electrical and Computer Engineering Department, University of Illinois at Urbana-Champaign, 145 Coordinated Science Laboratory, 1308 West Main Street, Urbana, IL 61801. E-mail: yima@uiuc.edu. . S. Sastry is with the Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, 514 Cory Hall, Berkeley, CA 94720. E-mail: sastry@eecs.berkeley.edu.\nManuscript received 21 May 2004; revised 11 Mar. 2005; accepted 10 May 2005; published online 13 Oct. 2005. Recommended for acceptance by Y. Amit. For information on obtaining reprints of this article, please send e-mail to: tpami@computer.org, and reference IEEECS Log Number TPAMI-0253-0504.\n1. In the context of stochastic signal processing, PCA is also known as the Karhunen-Loeve transform [18]; in the applied statistics literature, SVD is also known as the Eckart and Young decomposition [4].\n0162-8828/05/$20.00 2005 IEEE Published by the IEEE Computer Society"}, {"heading": "1.1 Previous Work on Subspace Segmentation", "text": "Subspace segmentation is a fundamental problem in many applications in computer vision (e.g., image/motion/video segmentation), image processing (e.g., image representation and compression), and systems theory (e.g., hybrid system identification), which is usually regarded as \u201cchicken-andegg.\u201d If the segmentation of the data was known, one could easily fit a single subspace to each group of points using standard PCA. Conversely, if the subspace bases were known, one could easily find the data points that best fit each subspace. Since, in practice, neither the subspace bases nor the segmentation of the data are known, most existing methods randomly choose a basis for each subspace and then iterate between data segmentation and subspace estimation. This can be done using, e.g., K-subspaces [10], an extension of K-means to the case of subspaces, subspace growing and subspace selection [15], or Expectation Maximization (EM) for mixtures of PCAs [19]. Unfortunately, most iterative methods are, in general, very sensitive to initialization; hence, they may not converge to the global optimum [21].\nThe need for initialization methods has motivated the recent development of algebro-geometric approaches to subspace segmentation that do not require initialization. In [13] (see, also, [3]), it is shown that when the subspaces are orthogonal, of equal dimension d, and intersect only at the origin, which implies thatD nd, one can use the SVD of the data to define a similarity matrix from which the segmentation of the data can be obtained using spectral clustering techniques.Unfortunately, thismethod is sensitive to noise in the data, as shown in [13], [27] where various improvements are proposed, and fails when the subspaces intersect arbitrarily [14], [22], [28]. The latter case has been addressed in an ad hoc fashion by using clustering algorithms such as K-means, spectral clustering, or EM [14], [28] to segment the data and PCA to obtain a basis for each group. The only algebraic approaches that deal with arbitrary intersections are [17], which studies the case of two planes in IR3 and [24] which studies the case of subspaces of codimension one, i.e., hyperplanes, and shows that hyperplane segmentation is equivalent to homogeneous polynomial factorization. Our previous work [23] extended this framework to subspaces of unknown and possibly different dimensions under the additional assumption that the number of subspaces is known. This paper unifies the results of [24] and [23] and extends to the case inwhich both the number anddimensions of the subspaces are unknown."}, {"heading": "1.2 Paper Organization and Contributions", "text": "In this paper, we propose an algebro-geometric approach to subspace segmentation called Generalized Principal Component Analysis (GPCA), which is based on fitting, differentiating, and dividing polynomials. Unlike prior work, we do not restrict the subspaces to be orthogonal, trivially intersecting, or with known and equal dimensions. Instead, we address the most general case of an arbitrary number of subspaces of unknown and possibly different dimensions (e.g., Fig. 1) and with arbitrary intersections among the subspaces.\nIn Section 2,wemotivate andhighlight the key ideas of our approach by solving the simple example shown in Fig. 1.\nIn Section 3, we generalize this example to the case of data lying in a known number of subspaces with unknown and possibly different dimensions. We show that one can represent the union of all subspaces as the zero set of a collection of homogeneous polynomials whose degree is the number of subspaces and whose factors encode normal vectors to the subspaces. The coefficients of these polynomials can be linearly estimated from sample data points on the subspaces and the set of normal vectors to each subspace can be obtained by evaluating the derivatives of these polynomials at any point lying on the subspace. Therefore, subspace segmentation is reduced to the problem of classifying onepoint per subspace.When those points are given (e.g., in semisupervised learning), this means that in order to learn the mixture of subspaces, it is sufficient to have one positive example per class. When all the data points are unlabeled (e.g., in unsupervised learning), we use polynomial division to recursively select points in the data set that minimize their distance to the algebraic set; hence, dealing automatically with moderate noise in the data. A basis for the complement of each subspace is then recoveredby applying standardPCA to thederivatives of thepolynomials (normalvectors) at those points. The final result is a global, noniterative subspace segmentation algorithm based on simple linear and polynomial algebra.\nIn Section 4, we discuss some extensions of our approach. We show how to deal with low-dimensional subspaces of a high-dimensional space via a linear projection onto a lowdimensional subspace that preserves the number and dimensions of the subspaces.Wealso showhow togeneralize the basic GPCA algorithm to the case in which the number of subspaces is unknownvia a recursive partitioning algorithm.\nIn Section 5, we present experiments on low-dimensional data showing thatGPCAgives about half the error of existing algebraic algorithms based on polynomial factorization, and improves the performance of iterative techniques, such as K-subspaces and EM, by about 50 percent with respect to random initialization. We also present applications of GPCA to computer vision problems such as face clustering, temporal video segmentation, and 3D motion segmentation from point correspondences in multiple affine views."}, {"heading": "2 AN INTRODUCTORY EXAMPLE", "text": "Imagine that we are given data in IR3 drawn from a line S1 \u00bc fx : x1 \u00bc x2 \u00bc 0g and a plane S2 \u00bc fx : x3 \u00bc 0g, as shown in Fig. 1. We can describe the two subspaces as\nS1 [ S2 \u00bc fx : \u00f0x1 \u00bc x2 \u00bc 0\u00de _ \u00f0x3 \u00bc 0\u00deg \u00bc fx : \u00f0x1x3 \u00bc 0\u00de ^ \u00f0x2x3 \u00bc 0\u00deg:\nTherefore, even though each individual subspace is de-\nscribedwithpolynomialsofdegreeone (linear equations), the\nmixture of two subspaces is described with two polynomials of degree two, namely,p21\u00f0x\u00de \u00bc x1x3 andp22\u00f0x\u00de \u00bc x2x3.More generally, any two linear subspaces in IR3 can be represented\nas the set of points satisfying some polynomials of the form\nc1x 2 1 \u00fe c2x1x2 \u00fe c3x1x3 \u00fe c4x22 \u00fe c5x2x3 \u00fe c6x23 \u00bc 0:\nAlthough these polynomials are nonlinear in each data point \u00bdx1; x2; x3 T , they are actually linear in the coefficient vector c \u00bc \u00bdc1; . . . ; c6 T . Therefore, given enough data points, one can linearly fit these polynomials to the data.\nGiven the collection of polynomials that vanish on the\ndata points, we would like to compute a basis for each subspace. In our example, let P2\u00f0x\u00de \u00bc \u00bdp21\u00f0x\u00de; p22\u00f0x\u00de and consider the derivatives of P2\u00f0x\u00de at two points in each of the subspaces y1 \u00bc \u00bd0; 0; 1 T 2 S1 and y2 \u00bc \u00bd1; 1; 0 T 2 S2:\nDP2\u00f0x\u00de \u00bc x3 0 0 x3\nx1 x2\n2 64\n3 75)\nDP2\u00f0y1\u00de \u00bc 1 0 0 1\n0 0\n2 64\n3 75; DP2\u00f0y2\u00de \u00bc 0 0 0 0\n1 1\n2 64\n3 75:\nNote that the columns of DP2\u00f0y1\u00de span S?1 and the columns of DP2\u00f0y2\u00de span S?2 (see Fig. 1). Also, the dimension of the line is d1 \u00bc 3 rank\u00f0DP2\u00f0y1\u00de\u00de \u00bc 1 and the dimension of the plane is d2 \u00bc 3 rank\u00f0DP2\u00f0y2\u00de\u00de \u00bc 2. Thus, if we are given one point in each subspace, we can obtain the subspace bases\nand their dimensions from the derivatives of the polynomials at\nthese points. The final question is to find one point per subspace, so that\nwe can obtain the normal vectors from the derivatives ofP2 at\nthose points.With perfect data,wemay choose a first point as\nany of the points in the data set.With noisy data, wemay first define a distance from any point in IR3 to one of the subspaces, e.g., the algebraic distance d2\u00f0x\u00de2 \u00bc p21\u00f0x\u00de2 \u00fe p22\u00f0x\u00de2 \u00bc \u00f0x21 \u00fe x22\u00dex23, and then choose a point in the data set that minimizes this distance. Say, we pick y2 2 S2 as such point. We can then compute the normal vector b2 \u00bc \u00bd0; 0; 1 T to S2 fromDP \u00f0y2\u00de. As it turns out, we can pick a second point in S1 but not in S2 by polynomial division. We can just divide the original polynomials of degree n \u00bc 2 by \u00f0bT2 x\u00de to obtain polynomials of degree n 1 \u00bc 1:\np11\u00f0x\u00de \u00bc p21\u00f0x\u00de bT2 x \u00bc x1 and p12\u00f0x\u00de \u00bc p22\u00f0x\u00de bT2 x \u00bc x2:\nSince these new polynomials vanish on S1 but not on S2, we can find a point y1 in S1 but not in S2, as a point in the data set that minimizes d1\u00f0x\u00de2 \u00bc p11\u00f0x\u00de2 \u00fe p12\u00f0x\u00de2 \u00bc x21 \u00fe x22. As wewill show in the next section, one can also solve the\nmore general problem of segmenting a union of n subspaces fSi IRDgni\u00bc1 of unknown and possibly different dimensions fdigni\u00bc1 by polynomial fitting (Section 3.3), differentiation (Section 3.4), and division (Section 3.5)."}, {"heading": "3 GENERALIZED PRINCIPAL COMPONENT ANALYSIS", "text": "In this section, we derive a constructive algebro-geometric solution to the subspace segmentation problem when the number of subspaces n is known. The case in which the number of subspaces is unknown will be discussed in Section 4.2. Our algebro-geometric solution is summarized in the following theorem:\nTheorem 1 (Generalized Principal Component Analysis). A union of n subspaces of IRD can be represented with a set of homogeneous polynomials of degree n in D variables. These polynomials can be estimated linearly given enough sample points in general position in the subspaces. A basis for the complement of each subspace can be obtained from the derivatives of these polynomials at a point in each of the subspaces. Such points can be recursively selected via polynomial division. Therefore, the subspace segmentation problem ismathematically equivalent to fitting, differentiating and dividing a set of homogeneous polynomials."}, {"heading": "3.1 Notation", "text": "Let x be a vector in IRD. A homogeneous polynomial of degree n in x is a polynomial pn\u00f0x\u00de such that pn\u00f0 x\u00de \u00bc npn\u00f0x\u00de for all in IR. The space of all homogeneous polynomials of degree n in D variables is a vector space of dimension Mn\u00f0D\u00de \u00bc n\u00feD 1D 1 . A particular basis for this space is given by all the monomials of degree n in D variables, that is xI \u00bc xn11 x n2 2 x nD D with 0 nj n for j \u00bc 1; . . . ; D, and n1 \u00fe n2 \u00fe \u00fe nD \u00bc n. Thus, each homogeneous polynomial can be written as a linear combination of the monomials xI with coefficient vector cn 2 IRMn\u00f0D\u00de as\npn\u00f0x\u00de \u00bc cTn n\u00f0x\u00de \u00bc X cn1;n2;...;nDx n1 1 x n2 2 x nD D ; \u00f01\u00de\nwhere n : IR D ! IRMn\u00f0D\u00de is the Veronese map of degree n [7], also known as the polynomial embedding in machine learning, defined as n : \u00bdx1; . . . ; xD T 7!\u00bd. . . ; xI ; . . . T with I chosen in the degree-lexicographic order.\nExample1(TheVeronesemapofdegree2inthreevariables). If x \u00bc \u00bdx1; x2; x3 T 2 IR3, the Veronese map of degree 2 is given by:\n2\u00f0x\u00de \u00bc \u00bdx21; x1x2; x1x3; x22; x2x3; x23 T 2 IR6: \u00f02\u00de\n3.2 Representing a Union of n Subspaces with a Set of Homogeneous Polynomials of Degree n We represent a subspace Si IRD of dimension di, where 0 < di < D, by choosing a basis\nBi\u00bc: \u00bdbi1; . . . ; bi\u00f0D di\u00de 2 IRD \u00f0D di\u00de \u00f03\u00de\nfor its orthogonal complement S?i . One could also choose a basis forSi directly, especially if di D. Section 4.1will show that the problem can be reduced to the caseD \u00bc maxfdig \u00fe 1; hence, the orthogonal representation is more convenient if maxfdig is small.With this representation, each subspace can be expressed as the set of points satisfying D di linear equations (polynomials of degree one), that is,\nSi \u00bc fx 2 IRD : BTi x \u00bc 0g \u00bc n x 2 IRD: D\u0302 di\nj\u00bc1 \u00f0bTijx \u00bc 0\u00de\no : \u00f04\u00de\nFor affine subspaces (which do not necessarily pass through the origin), we use homogeneous coordinates so that they become linear subspaces.\nWe now demonstrate that one can represent the union of n subspaces fSi IRDgni\u00bc1 with a set of polynomials whose degree is n rather than one. To see this, notice that x 2 IRD belongs to [ni\u00bc1Si if and only if it satisfies \u00f0x 2 S1\u00de _ . . . _ \u00f0x 2 Sn\u00de. This is equivalent to\n_n i\u00bc1 \u00f0x 2 Si\u00de , _n i\u00bc1 D\u0302 di j\u00bc1 \u00f0bTijx \u00bc 0\u00de , ^ _n i\u00bc1 \u00f0bTi \u00f0i\u00dex \u00bc 0\u00de; \u00f05\u00de\nwhere the right-hand side is obtained by exchanging ands and ors using De Morgan\u2019s laws and is a particular choice of one normal vector bi \u00f0i\u00de from each basis Bi. Since each one of the Qn i\u00bc1\u00f0D di\u00de equations in (5) is of the form\n_n i\u00bc1 \u00f0bTi \u00f0i\u00dex \u00bc 0\u00de , Yn i\u00bc1 \u00f0bTi \u00f0i\u00dex\u00de \u00bc 0 , \u00f0pn \u00f0x\u00de \u00bc 0\u00de; \u00f06\u00de\ni.e., a homogeneouspolynomial ofdegreen inDvariables,we can write each polynomial as a linear combination of monomials xI with coefficient vector cn 2 IRMn\u00f0D\u00de, as in (1). Therefore, we have the following result.\nTheorem 2 (Representing Subspaces with Polynomials). A union of n subspaces can be represented as the set of points satisfying a set of homogeneous polynomials of the form\npn\u00f0x\u00de \u00bc Yn i\u00bc1 \u00f0bTi x\u00de \u00bc cTn n\u00f0x\u00de \u00bc 0; \u00f07\u00de\nwhere bi 2 IRD is a normal vector to the ith subspace.\nThe importance of Theorem 2 is that it allows us to solve the \u201cchicken-and-egg\u201d problem described in Section 1.1 algebraically, because the polynomials in (7) are satisfied by all data points, regardless of which point belongs to which subspace. We can then use all the data to estimate all the subspaces,withoutprior segmentationandwithouthaving to iterate between data segmentation and model estimation, as we will show in Sections 3.3, 3.4, and 3.5."}, {"heading": "3.3 Fitting Polynomials to Data Lying in Multiple Subspaces", "text": "Thanks to Theorem 2, the problem of identifying a union of n subspaces fSigni\u00bc1 from a set of data pointsX\u00bc\n: fxjgNj\u00bc1 lying in the subspaces is equivalent to solving for the normal bases fBign1\u00bc1 from the set of nonlinear equations in (6). Although these polynomial equations are nonlinear in each data point x, they are actually linear in the coefficient vector cn. Indeed, since each polynomial pn\u00f0x\u00de \u00bc cTn n\u00f0x\u00de must be satisfied by every data point, we have cTn n\u00f0xj\u00de \u00bc 0 for all j \u00bc 1; . . . ; N . We use In to denote the space of coefficient vectors cn of all homogeneous polynomials that vanish on the n subspaces. Obviously, the coefficient vectors of the factorizable polynomials defined in (6) span a (possibly proper) subspace in In:\nspan fpn g In: \u00f08\u00de\nAs every vector cn in In represents a polynomial that vanishes on all the data points (on the subspaces), the vector must satisfy the system of linear equations\ncTnV n\u00f0D\u00de\u00bc : cTn \u00bd n\u00f0x1\u00de . . . n\u00f0xN\u00de \u00bc 0T : \u00f09\u00de\nV n\u00f0D\u00de 2 IRMn\u00f0D\u00de N is called the embedded data matrix. Obviously, we have the relationship\nIn null\u00f0V n\u00f0D\u00de\u00de:\nAlthough we know that the coefficient vectors cn of vanishing polynomials must lie in the left null space of V n\u00f0D\u00de, we do not know if every vector in the null space corresponds to a polynomial that vanishes on the subspaces. Therefore, we would like to study under what conditions on the data points, we can solve for the unique mn\u00bc: dim\u00f0In\u00de independent polynomials that vanish on the subspaces from the null space of V n. Clearly, a necessary condition is to have N Pn i\u00bc1 di points in[ni\u00bc1Si, with at least di points in general positionwithin each subspace Si, i.e., the di points must span Si. However, because we are representing each polynomial pn\u00f0x\u00de linearly via the coefficient vector cn, we need a number of samples such that a basis for In can be uniquely recovered from null\u00f0V n\u00f0D\u00de\u00de. That is, the number of samplesN must be such that\nrank\u00f0V n\u00f0D\u00de\u00de \u00bcMn\u00f0D\u00de mn Mn\u00f0D\u00de 1: \u00f010\u00de\nTherefore, if the number of subspaces n is known, we can recover In from null\u00f0V n\u00f0D\u00de\u00de given N Mn\u00f0D\u00de 1 points in general position. A basis of In can be computed linearly as the set ofmn left singular vectors of V n\u00f0D\u00de associatedwith itsmn zero singular values. Thus, we obtain a basis of polynomials of degree n, say fpn\u2018gmn\u2018\u00bc1, that vanish on the n subspaces. Remark 1 (GPCA and Kernel PCA). KernelPCAidentifiesa\nmanifold from sample data by embedding the data into a higher-dimensional feature space F such that the embedded data points lie in a linear subspace of F . Unfortunately, there isnogeneralmethodology for finding the appropriate embedding for a particular problem becausetheembeddingnaturallydependsonthegeometry of the manifold. The above derivation shows that the commonly used polynomial embedding n is the appropriate embedding to use in KPCAwhen the original data lie in a union of subspaces, because the embedded data points f n\u00f0xj\u00degNj\u00bc1 lie in a subspace of IRMn\u00f0D\u00de of dimension Mn\u00f0D\u00de mn, where mn \u00bc dim\u00f0In\u00de. Notice also that the matrix C \u00bc V n\u00f0D\u00deV n\u00f0D\u00deT 2 IRMn\u00f0D\u00de Mn\u00f0D\u00de is exactly the covariance matrix in the feature space and K \u00bc V n\u00f0D\u00deTV n\u00f0D\u00de 2 IRN N is the kernel matrix associatedwith theN embedded samples.\nRemark 2 (Estimation from Noisy Data). In the presence of moderatenoise,wecanstillestimate thecoefficientsofeach polynomial in a least-squares sense as the singular vectors of V n\u00f0D\u00de associated with its smallest singular values. However, we cannot directly estimate the number of polynomials fromtherankofV n\u00f0D\u00debecauseV n\u00f0D\u00demaybe of full rank.We use model selection to determinemn as\nmn \u00bc argmin m 2m\u00fe1\u00f0V n\u00f0D\u00de\u00dePm j\u00bc1 2 j \u00f0V n\u00f0D\u00de\u00de \u00fe m; \u00f011\u00de\nwith j\u00f0V n\u00f0D\u00de\u00de the jth singular vector of V n\u00f0D\u00de and a parameter. An alternative way of selecting the correct linear model (in feature space) for noisy data can be found in [11].\nRemark 3 (Suboptimality in the Stochastic Case). Notice that, in the case of hyperplanes, the least-squares solution for cn is obtained by minimizing kcTV n\u00f0D\u00dek2 subject to kcnk \u00bc 1. However, when n > 1 the so-found cn does not minimize the sum of least-square errorsP\nj mini\u00bc1;...;n\u00f0bTi xj\u00de 2. Instead, it minimizes a \u201cweighted\nversion\u201d of the least-square errors X j j min i\u00bc1;...;n \u00f0bTi xj\u00de 2\u00bc: X j Yn i\u00bc1 \u00f0bTi xj\u00de 2 \u00bc kcTV n\u00f0D\u00dek2; \u00f012\u00de\nwhere the weight j is conveniently chosen so as to eliminate the minimization over i \u00bc 1; . . . ; n. Such a \u201csoftening\u201d of the objective function permits a global algebraic solution because the softened errordoes notdependon the membership of one point to one of the hyperplanes. This least-squares solution for cn offers a suboptimal approximation for the original stochastic objective when the variance of the noise is small. This solution can be used to initializeother iterativeoptimizationschemes(suchasEM) to further minimize the original stochastic objective."}, {"heading": "3.4 Obtaining a Basis and the Dimension of Each Subspace by Polynomial Differentiation", "text": "In this section, we show that one can obtain the bases fBigni\u00bc1 for the complement of the n subspaces and their dimensions fdigni\u00bc1 by differentiating all the polynomials obtained from the left null space of the embedded data matrix V n\u00f0D\u00de. For the sake of simplicity, let us first consider the case of hyperplanes, i.e., subspaces of equal dimension di \u00bc D 1, for i \u00bc 1; . . . ; n. In this case, there is only one vector bi 2 IRD normal to subspace Si. Therefore, there is only one polynomial representing the n hyperplanes, namely, pn\u00f0x\u00de \u00bc \u00f0bT1 x\u00de \u00f0bTnx\u00de \u00bc cTn n\u00f0x\u00de and its coefficient vector cn can be computed as the unique vector in the left null space of V n\u00f0D\u00de. Consider now the derivative of pn\u00f0x\u00de\nDpn\u00f0x\u00de \u00bc @pn\u00f0x\u00de @x \u00bc @ @x Yn i\u00bc1 \u00f0bTi x\u00de \u00bc Xn i\u00bc1 \u00f0bi\u00de Y \u2018 6\u00bci \u00f0bT\u2018 x\u00de; \u00f013\u00de\nat a point yi 2 Si, i.e., yi is such that bTi yi \u00bc 0. Then, all terms in (13), except the ith, vanish, because Q \u2018 6\u00bci\u00f0bT\u2018 yj\u00de \u00bc 0 for j 6\u00bc i, so that we can immediately obtain the normal vectors as\nbi \u00bc Dpn\u00f0yi\u00de kDpn\u00f0yi\u00dek ; i \u00bc 1; . . . ; n: \u00f014\u00de\nTherefore, in a semisupervised learning scenario in which we are given only one positive example per class, the hyperplane segmentation problem can be solved analytically by evaluating the derivatives of pn\u00f0x\u00de at the points with known labels.\nAs it turns out, the same principle applies to subspaces of arbitrary dimensions. This fact should come at no surprise. The zero set of each vanishing polynomial pn\u2018 is just a surface in IRD; therefore, the derivative of pn\u2018 at a point yi 2 Si,Dpn\u2018\u00f0yi\u00de, gives a vector normal to the surface. Since a union of subspaces is locally flat, i.e., in a neighborhood of yi the surface is merely the subspace Si, then the derivative at yi lies in the orthogonal complement S ? i of Si. By evaluating\nthe derivatives of all the polynomials in In at the same point yi, we obtain a set of normal vectors that span the orthogonal complement of Si, as stated in Theorem 3. Fig. 2 illustrates the theorem for the case of a plane and a line described in Section 2.\nTheorem 3 (Obtaining Subspace Bases and Dimensions\nby Polynomial Differentiation). Let In be (the space of coefficient vectors of) the set of polynomials of degree n that vanish on the n subspaces. If the data set X is such that dim\u00f0null\u00f0V n\u00f0D\u00de\u00de\u00de \u00bc dim\u00f0In\u00de \u00bc mn and one point yi 2 Si but yi =2Sj for j 6\u00bc i is given for each subspace Si, then we have\nS?i \u00bc span n @ @x cTn n\u00f0x\u00de x\u00bcyi ; 8cn 2 null\u00f0V n\u00f0D\u00de\u00de o : \u00f015\u00de\nTherefore, the dimensions of the subspaces are given by\ndi \u00bc D rank DPn\u00f0yi\u00de for i \u00bc 1; . . . ; n; \u00f016\u00de\nwith Pn\u00f0x\u00de \u00bc \u00bdpn1\u00f0x\u00de; . . . ; pnmn\u00f0x\u00de 2 IR1 mn and DPn\u00f0x\u00de \u00bc \u00bdDpn1\u00f0x\u00de; . . . ; Dpnmn\u00f0x\u00de 2 IRD mn .\nAs a consequence of Theorem 3, we already have the sketch of an algorithm for segmenting subspaces of arbitrary dimensions in a semisupervised learning scenario in which we are given one positive example per class fyi 2 Signi\u00bc1:\n1. Compute a basis for the left null space of V n\u00f0D\u00de using, for example, SVD. 2. Evaluate the derivatives of the polynomial cTn n\u00f0x\u00de at yi for each cn in the basis of null\u00f0V n\u00f0D\u00de\u00de to obtain a set of normal vectors in S?i . 3. Compute a basis Bi for S ? i by applying PCA to the\nnormal vectors obtained in Step 2. PCA automatically gives the dimension of each subspace di \u00bc dim\u00f0Si\u00de. 4. Cluster the data by assigning point xj to subspace i if\ni \u00bc arg min \u2018\u00bc1;...;n kBT\u2018 xjk: \u00f017\u00de\nRemark 4 (Estimating the Bases from Noisy Data Points). Withamoderate levelofnoise inthedata,wecanstillobtain abasis for eachsubspaceandcluster thedataasabove.This is because we are applying PCA to the derivatives of the polynomials and both the coefficients of the polynomials and their derivatives depend continuously on the data.\nNotice also that we can obtain the dimension of each\nsubspace by looking at the singular values of thematrix of\nderivatives, similarly to (11).\nRemark 5 (Computing Derivatives of Homogeneous\nPolynomials). Notice that given cn the computation of the derivatives of pn\u00f0x\u00de \u00bc cTn n\u00f0x\u00de does not involve taking derivatives of the (possibly noisy) data. For instance, one may compute the derivatives as @pn\u00f0x\u00de@xk \u00bc c T n n\u00f0x\u00de @xk \u00bc cTnEnk n 1\u00f0x\u00de, where Enk2 IRMn\u00f0D\u00de Mn 1\u00f0D\u00de is a constant matrix that depends on the exponents of the different monomials in the Veronese map n\u00f0x\u00de."}, {"heading": "3.5 Choosing One Point per Subspace by Polynomial Division", "text": "Theorem 3 demonstrates that one can obtain a basis for each S?i directly from the derivatives of the polynomials representing the union of subspaces. However, in order to\nproceed we need to have one point per subspace, i.e., we need to know the vectors fyigni\u00bc1. In this section, we show how to select these n points in the\nunsupervised learning scenario in which we do not know the\nlabel for any of the data points. To this end, notice thatwe can\nalways choose a point yn lying on one of the subspaces, say Sn, by checking that Pn\u00f0yn\u00de \u00bc 0T . Since we are given a set of data pointsX \u00bc fxjgnj\u00bc1 lying on the subspaces, in principle, we could choose yn to be any of the data points. However, in the presence of noise and outliers, a random choice of yn may be far from the true subspaces. In Section 2, we chose a point in the data set X that minimizes kPn\u00f0x\u00dek. However, such a choice has the following problems:\n1. The value kPn\u00f0x\u00dek is merely an algebraic error, i.e., it does not represent the geometric distance from x to its closest subspace. In principle, finding the geometric distance from x to its closest subspace is a difficult problem because we do not know the normal bases fBigni\u00bc1. 2. Points x lying close to the intersection of two or more subspaces could be chosen. However, at a point x in the intersection of two or more subspaces, we often have Dpn\u00f0x\u00de \u00bc 0. Thus, one should avoid choosing such points, as they give very noisy estimates of the normal vectors.\nAs it turns out, one can avoid both of these problems\nthanks to the following lemma:\nLemma 1. Let ~x be the projection of x 2 IRD onto its closest subspace. The Euclidean distance from x to ~x is\nkx ~xk \u00bc n ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi Pn\u00f0x\u00de DPn\u00f0x\u00deTDPn\u00f0x\u00de y Pn\u00f0x\u00deT r\n\u00feO kx ~xk2 ;\nwhere Pn\u00f0x\u00de \u00bc \u00bdpn1\u00f0x\u00de; . . . ; pnmn\u00f0x\u00de 2 IR1 mn , DPn\u00f0x\u00de \u00bc Dpn1\u00f0x\u00de; . . . ; Dpnmn\u00f0x\u00de\u00bd 2 IRD mn , and Ay is the MoorePenrose inverse of A.\nProof. The projection ~x of a point x onto the zero set of the\npolynomials fpn\u2018gmn\u2018\u00bc1 can be obtained as the solution of the following constrained optimization problem\nmin k~x xk2 subject to pn\u2018\u00f0~x\u00de \u00bc 0 \u2018 \u00bc 1; . . . ;mn:\n\u00f018\u00de\nBy using Lagrange multipliers 2 IRmn , we can convert this problem into theunconstrained optimizationproblem\nmin ~x; k~x xk2 \u00fe Pn\u00f0~x\u00de : \u00f019\u00de\nFrom the first order conditions with respect to ~x, we have 2\u00f0~x x\u00de \u00feDPn\u00f0~x\u00de \u00bc 0. After multiplying on the left by \u00f0~x x\u00deT and \u00f0DPn\u00f0~x\u00de\u00deT , respectively, we obtain\nk~x xk2 \u00bc 1 2 xTDPn\u00f0~x\u00de ; and \u00f020\u00de\n\u00bc 2 DPn\u00f0~x\u00deTDPn\u00f0~x\u00de y DPn\u00f0~x\u00deTx; \u00f021\u00de\nwherewe have used the fact that \u00f0DPn\u00f0~x\u00de\u00deT ~x \u00bc nPn\u00f0~x\u00de \u00bc 0 because D n\u00f0~x\u00deT ~x \u00bc n n\u00f0~x\u00de. After replacing (21) on (20), the squared distance from x to its closest subspace is given by\nk~x xk2 \u00bc xTDPn\u00f0~x\u00de DPn\u00f0~x\u00deTDPn\u00f0~x\u00de y DPn\u00f0~x\u00deTx: \u00f022\u00de\nAfter expanding in Taylor series about ~x \u00bc x and noticing that DPn\u00f0x\u00deTx \u00bc nPn\u00f0x\u00deT , we obtain\nk~x xk2 n2Pn\u00f0x\u00de DPn\u00f0x\u00deTDPn\u00f0x\u00de y Pn\u00f0x\u00deT ; \u00f023\u00de\nwhich completes the proof. tu Thanks to Lemma 1, we can immediately choose a point\nyn lying in (close to) one of the subspaces and not in (far from) the other subspaces as\nyn \u00bc arg min x2X :DPn\u00f0x\u00de6\u00bc0 Pn\u00f0x\u00de DPn\u00f0x\u00deTDPn\u00f0x\u00de\ny Pn\u00f0x\u00deT ; \u00f024\u00de\nand then compute the basis Bn 2 IRD \u00f0D dn\u00de for S?n by applying PCA to DPn\u00f0yn\u00de.\nIn order to find a point yn 1 lying in (close to) one of the remaining \u00f0n 1\u00de subspaces but not in (far from) Sn, we find a new set of polynomials fp\u00f0n 1\u00de\u2018\u00f0x\u00deg defining the algebraic set [n 1i\u00bc1 Si. In the case of hyperplanes, there is only one such polynomial, namely,\npn 1\u00f0x\u00de\u00bc: \u00f0b1x\u00de \u00f0bTn 1x\u00de \u00bc pn\u00f0x\u00de bTnx \u00bc cTn 1 n 1\u00f0x\u00de:\nTherefore, we can obtain pn 1\u00f0x\u00de by polynomial division. Notice thatdividingpn\u00f0x\u00deby bTnx is a linear problemof the form cTn 1Rn\u00f0bn\u00de \u00bc cTn , where Rn\u00f0bn\u00de2 IRMn 1\u00f0D\u00de Mn\u00f0D\u00de. This is because solving for the coefficients of pn 1\u00f0x\u00de is equivalent to solving the equations \u00f0bTnx\u00de\u00f0cTn 1 n\u00f0x\u00de\u00de \u00bc cTn n\u00f0x\u00de, where bn and cn are already known. Example 2. If n \u00bc 2 and b2 \u00bc \u00bdb1; b2; b3 T , then the matrix R2\u00f0b2\u00de is given by\nR2\u00f0b2\u00de \u00bc b1 b2 b3 0 0 0 0 b1 0 b2 b3 0 0 0 b1 0 b2 b3\n2 4\n3 5 2 IR3 6:\nIn the case of subspaces of varying dimensions, in principle, we cannot simply divide the entries of the polynomial vector Pn\u00f0x\u00de by bTnx for any column bn of Bn\nbecause the polynomials fpn\u2018\u00f0x\u00deg may not be factorizable.2 Furthermore, they do not necessarily have the common factor bTnx. The following theorem resolves this difficulty by showing how to compute the polynomials associated with the remaining subspaces [n 1i\u00bc1 Si: Theorem 4 (Obtaining Points by Polynomial Division). Let\nIn be (the space of coefficient vectors of) the set of polynomials vanishing on the n subspaces. If the data set X is such that dim\u00f0null\u00f0V n\u00f0D\u00de\u00de\u00de \u00bc dim\u00f0In\u00de, then the set of homogeneous polynomials of degree \u00f0n 1\u00de that vanish on the algebraic set [n 1i\u00bc1 Si is spanned by fcTn 1 n 1\u00f0x\u00deg, where the vectors of coefficients cn 12 IRMn 1\u00f0D\u00de must satisfy\ncTn 1Rn\u00f0bn\u00deV n\u00f0D\u00de \u00bc 0T ; for all bn 2 S?n : \u00f025\u00de\nProof. We first show the necessity. That is, any polynomial of degree n 1, cTn 1 n 1\u00f0x\u00de, that vanishes on[n 1i\u00bc1 Si satisfies the above equation. Since a point x in the original algebraic set [ni\u00bc1Si belongs to either [n 1i\u00bc1 Si or Sn, we have cTn 1 n 1\u00f0x\u00de \u00bc 0 or bTnx \u00bc 0 for all bn 2 S?n . Hence, pn\u00f0x\u00de\u00bc: \u00f0cTn 1 n 1\u00f0x\u00de\u00de\u00f0bTnx\u00de \u00bc 0. If we denote pn\u00f0x\u00de as cTn n\u00f0x\u00de, then the coefficient vector cn must be in null\u00f0V n\u00f0D\u00de\u00de. From cTn n\u00f0x\u00de \u00bc \u00f0cTn 1 n 1\u00f0x\u00de\u00de\u00f0bTnx\u00de, the relationship between cn and cn 1 can be written as cTn 1Rn\u00f0bn\u00de \u00bc cTn . Since cTnV n\u00f0D\u00de \u00bc 0T , cn 1 needs to satisfy the following linear system of equations cTn 1Rn\u00f0bn\u00deV n\u00f0D\u00de \u00bc 0T .\nWe now show the sufficiency. That is, if cn 1 is a solution to (25), then for all bn 2 S?n , cTn \u00bc cTn 1Rn\u00f0bn\u00de is in null\u00f0V n\u00f0D\u00de\u00de. From the construction of Rn\u00f0bn\u00de, we have cTn n\u00f0x\u00de \u00bc \u00f0cTn 1 n 1\u00f0x\u00de\u00de\u00f0bTnx\u00de. Then, for every x 2 [n 1i\u00bc1 Si but not in Sn, we have c T n 1 n 1\u00f0x\u00de \u00bc 0 because there is a bn such that b T nx 6\u00bc 0. Therefore, cTn 1 n 1\u00f0x\u00de is a homogeneous polynomial of degree \u00f0n 1\u00de that vanishes on [n 1i\u00bc1 Si. tu Thanks to Theorem 4, we can obtain a collection of polynomials fp\u00f0n 1\u00de\u2018\u00f0x\u00degmn 1\u2018\u00bc1 representing [n 1i\u00bc1 Si from the intersection of the left null spaces of Rn\u00f0bn\u00deV n\u00f0D\u00de 2 IRMn 1\u00f0D\u00de N for all bn 2 S?n . We can then repeat the same procedure to find a basis for the remaining subspaces. We thus obtain the following Generalized Principal Component Analysis (GPCA) algorithm (Algorithm 1) for segmenting n subspaces of unknown and possibly different dimensions.\nAlgorithm 1 (GPCA: Generalized Principal Component Analysis) set V n \u00bc\u00bd n\u00f0x1\u00de; . . . ; n\u00f0xN\u00de 2 IRMn\u00f0D\u00de N ; for i \u00bc n : 1 do solve cTV i \u00bc 0 to obtain a basis fci\u2018gmi\u2018\u00bc1 of null\u00f0V i\u00de, where the number of polynomials mi is obtained as in (11); set Pi\u00f0x\u00de \u00bc \u00bdpi1\u00f0x\u00de; . . . ; pimi\u00f0x\u00de 2 IR1 mi , where pi\u2018\u00f0x\u00de \u00bc cTi\u2018 i\u00f0x\u00de for \u2018 \u00bc 1; . . . ;mi; do\nyi \u00bc arg min x2X :DPi\u00f0x\u00de6\u00bc0 Pi\u00f0x\u00de DPi\u00f0x\u00deTDPi\u00f0x\u00de\ny Pi\u00f0x\u00deT ;\nBi \u00bc PCA DPi\u00f0yi\u00de ;\nV i 1 \u00bc Ri\u00f0bi1\u00deV i; . . . ; Ri\u00f0bi;D di\u00deV i\n;with bij columns of Bi;\nend do end for for j \u00bc 1 : N do assign point xj to subspace Si if i \u00bc argmin\u2018 kBT\u2018 xjk; end for\nRemark 6 (Avoiding Polynomial Division).Notice that one may avoid computing Pi for i < n by using a heuristic distance function to choose the points fyigni\u00bc1. Since a point in [n\u2018\u00bciS\u2018 must satisfy kBTi xk kBTnxk \u00bc 0, we can choose a point yi 1 on [i 1\u2018\u00bc1S\u2018 as:\nyi 1\u00bc arg min x2X :DPn\u00f0x\u00de6\u00bc0\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi Pn\u00f0x\u00de\u00f0DPn\u00f0x\u00deTDPn\u00f0x\u00de\u00deyPn\u00f0x\u00deT q \u00fe\nkBTi xk kBTnxk \u00fe ;\nwhere a small number > 0 is chosen to avoid cases in which both the numerator and the denominator are zero (e.g., with perfect data).\nRemark 7 (Robustness and Outlier Rejection). In practice, there could be points in X that are far away from any of the subspaces, i.e., outliers. By detecting and rejecting outliers, we can typically ensure a much better estimate of the subspaces. Many methods from robust statistics can be deployed to detect and reject the outliers [5], [11]. For instance, the function\nd2\u00f0x\u00de \u00bc Pn\u00f0x\u00de DPn\u00f0x\u00deTDPn\u00f0x\u00de y Pn\u00f0x\u00deT\napproximates the squared distance of a point x to the subspaces. From the d2-histogram of the sample setX , we may exclude from X all points that have unusually large d2 values and use only the remaining sample points to reestimate the polynomials before computing the normals. For instance, if we assume that the sample points are drawn around each subspace from independent Gaussian distributions with a small variance 2, then d 2\n2 is approximately a 2-distribution with P i\u00f0D di\u00de degrees of freedom. We can apply standard 2-test to reject sampleswhichdeviate significantly from this distribution. Alternatively, one can detect and reject outliers using Random Sample Consensus (RANSAC) [5]. One can chooseMn\u00f0D\u00dedata points at random, estimate a collection of polynomials passing through those points, determine their degree of support among the other points, and then choose the set of polynomials giving a large degree of support. This method is expected to be effective when Mn\u00f0D\u00de is small. An open problem is how to combine GPCA with methods from robust statistics in order to improve the robustness of GPCA to outliers."}, {"heading": "4 EXTENSIONS TO THE BASIC GPCA ALGORITHM", "text": "In this section, we discuss some extensions of GPCA that deal with practical situations such as low-dimensional subspaces of a high-dimensional space and unknown number of subspaces."}, {"heading": "4.1 Projection and Minimum Representation", "text": "When the dimension of the ambient space D is large, the complexity of GPCA becomes prohibitive because Mn\u00f0D\u00de is of the order nD. However, in most practical situations, we are interested in modeling the data as a union of subspaces\n2. Recall that we can only compute a basis for the null space of V n\u00f0D\u00de, and that linear combinations of factorizable polynomials are not necessarily factorizable. For example, x21 \u00fe x1x2 and x22 x1x2 are both factorizable, but their sum x21 \u00fe x22 is not.\nof relatively small dimensions fdi Dg. In such cases, it seems rather redundant to use IRD to represent such a lowdimensional linear structure. One way of reducing the dimensionality is to linearly project the data onto a lowerdimensional (sub)space. An example is shown in Fig. 3, where two lines L1 and L2 in IR3 are projected onto a plane P. In this case, segmenting the two lines in the threedimensional space IR3 is equivalent to segmenting the two projected lines l1 and l2 in the plane P.\nIngeneral,wewilldistinguishbetweentwodifferentkinds of linear \u201cprojections.\u201d The first kind corresponds to the case in which the span of all the subspaces is a proper subspace of the ambient space, i.e., span\u00f0[ni\u00bc1Si\u00de IRD. In this case, one may simply apply the classic PCA algorithm to the original data to eliminate the redundant dimensions. The secondkind corresponds to the case in which the largest dimension of the subspaces, denoted by dmax, is strictly less thanD 1. When dmax is known, one may choose a \u00f0dmax \u00fe 1\u00de-dimensional subspace P such that, by projecting onto this subspace:\nP : x 2 IRD 7! x 0 \u00bc P\u00f0x\u00de 2 P;\nthe dimension of each original subspace Si is preserved, 3 and the number of subspaces is preserved,4 as stated in the following theorem:\nTheorem 5 (Segmentation-Preserving Projections). If a set of vectors fxjg lie in n subspaces of dimensions fdigni\u00bc1 in IRD and if P is a linear projection into a subspace P of dimension D0, then the points f P\u00f0xj\u00deg lie in n0 n linear subspaces of P of dimensions fd0i dig n i\u00bc1. Furthermore, ifD > D\n0 > dmax, then there is an open and dense set of projections that preserve the number and dimensions of the subspaces, i.e.,n0 \u00bc n and d0i \u00bc di for i \u00bc 1; . . . ; n. Thanks to Theorem 5, if we are given a data set X drawn from a union of low-dimensional subspaces of a highdimensional space, we can cluster the data set by first projecting X onto a generic subspace of dimension D0 \u00bc dmax \u00fe 1 and then applying GPCA to the projected subspaces, as illustrated with the following sequence of steps:\nX ! P X 0 !GPCA [ni\u00bc1 P\u00f0Si\u00de ! 1P [ni\u00bc1 Si:\nHowever, even though we have shown that the set of \u00f0dmax \u00fe 1\u00de-dimensional subspaces P IRD that preserve the\nnumber and dimensions of the subspaces is an open and dense set, it remains unclear what a \u201cgood\u201d choice for P is, especiallywhen there is noise in thedata. Inpractice, onemay simply select a few random projections and choose the one that results in the smallest fitting error. Another alternative is to apply classic PCA to project onto a \u00f0dmax \u00fe 1\u00de-dimensional affine subspace. The reader may refer to [1] for alternative ways of choosing a projection."}, {"heading": "4.2 Identifying an Unknown Number of Subspaces of Unknown Dimensions", "text": "The solution to the subspace segmentationproblemproposed in Section 3 assumes prior knowledge of the number of subspacesn. In practice, however, the number of subspaces n maynot be knownbeforehand, hence,we cannot estimate the polynomials representing the subspaces directly.\nFor the sake of simplicity, let us first consider the problem of determining the number of subspaces from a generic data set lying in a union of n different hyperplanes Si \u00bc fx : bTi x \u00bc 0g. From Section 3, we know that in this case there is a unique polynomial of degree n that vanishes in Z \u00bc [ni\u00bc1Si, namely, pn\u00f0x\u00de \u00bc \u00f0bT1 x\u00de \u00f0bTnx\u00de \u00bc cTn n\u00f0x\u00de and that its coefficient vector cn lives in the left null space of the embedded data matrix V n\u00f0D\u00de defined in (9), hence, rank\u00f0V n\u00de \u00bcMn\u00f0D\u00de 1. Clearly, there cannot be a polynomial of degree i < n that vanishes in Z; otherwise, the data would lie in a union of i < n hyperplanes. This implies that V i\u00f0D\u00demust be full rank for all i < n. In addition, notice that there is more than one polynomial of degree i > n that vanishes on Z, namely, any multiple of pn, hence, rank\u00f0V i\u00f0D\u00de\u00de < Mi\u00f0D\u00de 1 if i > n. Therefore, the number of hyperplanes can be determined as the minimum degree such that the embedded data matrix drops rank, i.e.,\nn \u00bc minfi : rank\u00f0V i\u00f0D\u00de\u00de < Mi\u00f0D\u00deg: \u00f026\u00de\nConsider now the case of data lying in subspaces of equal dimension d1 \u00bc d2 \u00bc dn \u00bc d < D 1. For example, consider a set of pointsX \u00bc fxig lying in two lines in IR3, say,\nS1 \u00bc fx : x2 \u00bc x3 \u00bc 0g and S2 \u00bc fx : x1 \u00bc x3 \u00bc 0g: \u00f027\u00de\nIfwe construct thematrix of embeddeddata points V n\u00f0D\u00de for n \u00bc 1, we obtain rank\u00f0V 1\u00f03\u00de\u00de \u00bc 2 < 3 because all the points lie also in the plane x3 \u00bc 0. Therefore, we cannot determine the number of subspaces as in (26) because we would obtain n \u00bc 1, which is not correct. In order to determine the correct number of subspaces, recall from Section 4.1 that a linear projection onto a generic \u00f0d\u00fe 1\u00de-dimensional subspace P preserves the number and dimensions of the subspaces. Therefore, if we project the data onto P, then the projected data lies in a union ofnhyperplanes of IRd\u00fe1. By applying (26) to the projected data, we can obtain the number of subspaces from the embedded (projected) data matrix V i\u00f0d\u00fe 1\u00de as\nn \u00bc minfi : rank\u00f0V i\u00f0d\u00fe 1\u00de\u00de < Mi\u00f0d\u00fe 1\u00deg: \u00f028\u00de\nOf course, in order to apply this projection, we need to know the common dimension d of all the subspaces. Clearly, ifwe project onto a subspace of dimension \u2018\u00fe 1 < d\u00fe 1, then the number and dimension of the subspaces are no longer preserved. In fact, the projected data points lie in one subspace of dimension \u2018\u00fe 1, and V i\u00f0\u2018\u00fe 1\u00de is of full rank for all i (as long asMi\u00f0D\u00de < N). Therefore, we can determine the dimension of the subspaces as the minimum integer \u2018 such that there is a degree i for which V i\u00f0\u2018\u00fe 1\u00de drops rank, that is,\n3. This requires that P be transversal to each S?i , i.e., spanfP; S?i g \u00bc IRD for every i \u00bc 1; . . . ; n. Since n is finite, this transversality condition can be easily satisfied. Furthermore, the set of positions for P which violate the transversality condition is only a zero-measure closed set [9].\n4. This requires that all P\u00f0Si\u00de be transversal to each other in P, which is guaranteed if we require P to be transversal to S?i \\ S?j for i; j \u00bc 1; ::; n. All Ps which violate this condition form again only a zero-measure set.\nd \u00bc minf\u2018 : 9 i 1 such rank\u00f0V i\u00f0\u2018\u00fe 1\u00de\u00de < Mi\u00f0\u2018\u00fe 1\u00deg: \u00f029\u00de\nIn summary, when the subspaces are of equal dimension d, both the number of subspaces n and their common dimension d can be retrieved from (28) and (29) and the subspace segmentation problem can be subsequently solved by first projecting the data onto a \u00f0d\u00fe 1\u00de-dimensional subspace and then applying GPCA (Algorithm 1) to the projected data points.\nRemark 8. In the presence of noise, one may not be able to estimate d and n from (29) and (28), respectively, because the matrix V i\u00f0\u2018\u00fe 1\u00de may be of full rank for all i and \u2018. Similarly to Remark 2, one can use model selection techniques to determine the rank of V i\u00f0\u2018\u00de. However, in practice this requires searching for up to possibly \u00f0D 1\u00de values ford and dN=\u00f0D 1\u00deevalues forn.Onemay refer to [11] for a more detailed discussion on selecting the best multiple-subspace model from noisy data, using modelselection criteria such as MML, MDL, AIC, and BIC.\nUnfortunately, the situation is not so simple for subspaces of different dimensions. For instance, imagine that in addition to the two lines S1 and S2 we are also given data points on a plane S3 \u00bc fx : x1 \u00fe x2 \u00bc 0g, so that the overall configuration is similar to that shown in Fig. 4. In this case, we have rank\u00f0V 1\u00f03\u00de\u00de \u00bc 3 6< 3, rank\u00f0V 2\u00f03\u00de\u00de \u00bc 5 < 6, and rank\u00f0V 3\u00f03\u00de\u00de \u00bc 6 < 10. Therefore, if we try to determine the number of subspaces as the degree of the embedding for which the embedded data matrix drops rank we would obtain n \u00bc 2, which is incorrect again. The reason for this is clear: We can fit the data either with one polynomial of degree n \u00bc 2, which corresponds to the plane S3 and the plane P spanned by the two lines, or with four polynomials of degree n \u00bc 3, which vanish precisely on the two lines S1, S2, and the plane S3.\nTo resolve thedifficulty insimultaneouslydetermining the number and dimension of the subspaces, notice that the algebraic set Z \u00bc [nj\u00bc1Sj can be decomposed into irreducible subsets Sjs\u2014an irreducible algebraic set is also called a variety\u2014and that the decomposition of Z into fSjgnj\u00bc1 is always unique [8]. Therefore, as long as we are able to correctly determine from the given sample points the underlying algebraic set Z or the associated radical ideal I\u00f0Z\u00de,5 in principle, the number of subspaces n and their dimensions fdjgnj\u00bc1 can always be uniquely determined in a purely algebraic fashion. InFig. 4, for instance, the first interpretation (2 linesand1plane)wouldbe the right oneand the secondone (two planes) would be incorrect because the two lines, which span one of the planes, are not an irreducible algebraic set.\nHaving established that the problem of subspace segmentation is equivalent to decomposing the algebraic ideal\nassociated with the subspaces, we are left with deriving a computable scheme to achieve the goal of decomposing algebraic sets into varieties. To this end, notice that the set of all homogeneous polynomials that vanish inZ can be graded by degree as\nI\u00f0Z\u00de \u00bc Im Im\u00fe1 In ; \u00f030\u00de where m n is the degree of the polynomial of minimum degree that fits all the data points. For each degree i m, we can evaluate the derivatives of the polynomials in I i at points in subspace Sj and denote the collection of derivatives as\nDi;j\u00bc: span f[x2Sjfrf jx ; 8f 2 I igg; j \u00bc 1; 2; . . . ; n: \u00f031\u00de\nObviously, we have the following relationship:\nDi;j Di\u00fe1;j S?j ; 8i m: \u00f032\u00de\nTherefore, for each degree i m, we may compute a union of up to n subspaces,\nZi\u00bc: D?i;1 [D?i;2 [ [D?i;n Z; \u00f033\u00de\nwhich contains the original n subspaces. Therefore, we can further partition Zi to obtain the original subspaces. More specifically, in order to segment an unknown number of subspacesofunknownandpossiblydifferentdimensions,we can first search for the minimum degree i and dimension \u2018 such that V i\u00f0\u2018\u00fe 1\u00de drops rank. In our example in Fig. 4, we obtain i \u00bc 2 and \u2018 \u00bc 2. By applying GPCA to the data set projected onto an \u00f0\u2018\u00fe 1\u00de-dimensional space, we partition the data into up to n subspaces Zi which contain the original n subspaces. In our example, we partition the data into two planes P and S3. Once these subspaces have been estimated, we can reapply the same process to each reducible subspace. In our example, theplanePwill be separated into two linesS1 and S2, while the plane S3 will remain unchanged. This recursive process stopswhen every subspace obtained canno longer be separated into lower-dimensional subspaces, or whenaprespecifiedmaximumnumber of subspacesnmax has been reached.\nWe summarize the above derivation with the recursive GPCA algorithm (Algorithm 2).\nAlgorithm 2 Recursive GPCA Algorithm n \u00bc 1; repeat build a data matrix V n\u00f0D\u00de\u00bc: \u00bd n\u00f0x1\u00de; . . . ; n\u00f0xN\u00de 2 IRMn\u00f0D\u00de N via the Veronese map n of degree n; if rank\u00f0V n\u00f0D\u00de\u00de < Mn\u00f0D\u00de then\ncompute the basis fcn\u2018g of the left null space of V n\u00f0D\u00de; obtain polynomials fpn\u2018\u00f0x\u00de\u00bc: cTn\u2018 n\u00f0x\u00deg; Y \u00bc ;; for j \u00bc 1 : n do select a point xj from X nY (similar to Algorithm 1); obtain the subspace S?j spanned by the derivatives spanfDpn\u2018\u00f0xj\u00deg; find the subset of points Xj X that belong to the subspace Sj; Y Y [Xj; Recursive-GPCA(Xj); (with Sj now as the ambient space) end for n nmax;\nelse n n\u00fe 1;\nend if until n nmax.\n5. The ideal of an algebraic set Z is the set of all polynomials that vanish in Z. An ideal I is called radical if f 2 I whenever f s 2 I for some integer s."}, {"heading": "5 EXPERIMENTAL RESULTS AND APPLICATIONS IN", "text": "COMPUTER VISION\nIn this section, we first evaluate the performance of GPCA on synthetically generated data by comparing and combining it with the following approaches:\n1. Polynomial Factorization Algorithm (PFA). This algorithm is only applicable to the case of hyperplanes. It computes the normal vectors fbigni\u00bc1 to the n hyperplanes by factorizing the homogeneous polynomial pn\u00f0x\u00de \u00bc \u00f0bT1 x\u00de\u00f0bT2 x\u00de \u00f0bTnx\u00de into a product of linear factors. See [24] for further details.\n2. K-subspaces. Given an initial estimate for the subspace bases, this algorithmalternates between clustering the data points using the distance residual to the different subspaces and computing a basis for each subspace using standard PCA. See [10] for further details.\n3. Expectation Maximization (EM). This algorithm assumes that the data is corrupted with zero-mean Gaussian noise in the directions orthogonal to the subspace. Given an initial estimate for the subspace bases, EM alternates between clustering the data points (E-step) and computing a basis for each subspace (M-step) by maximizing the log-likelihood of the corresponding probabilistic model. See [19] for further details.\nWe then apply GPCA to various problems in computer vision such as face clustering under varying illumination, temporal video segmentation, two-view segmentation of linear motions, and multiview segmentation of rigid-body motions. However, it is not our intention to convince the reader that the proposed GPCA algorithm offers an optimal solution to each of these problems. In fact, one can easily obtain better segmentation results by using algorithms/ systems specially designed for each of these tasks.Wemerely wish to point out that GPCA provides an effective tool to automatically detect themultiple-subspace structure present in these data sets in a noniterative fashion and that it provides a good initial estimate for any iterative algorithm."}, {"heading": "5.1 Experiments on Synthetic Data", "text": "The experimental setup consists of choosing n \u00bc 2; 3; 4 collections of N \u00bc 200n points in randomly chosen planes in IR3. Zero-mean Gaussian noise with s.t.d. from 0 percent\nto 5 percent along the subspace normals is added to the sample points. We run 1,000 trials for each noise level. For each trial, the error between the true (unit) normal vectors fbigni\u00bc1 and their estimates fb\u0302ig n i\u00bc1 is computed as the mean angle between the normal vectors:\nerror\u00bc: 1 n Xn i\u00bc1 acos bTi b\u0302i \u00f0degrees\u00de: \u00f034\u00de\nFig. 5a plots themean error as a function of noise forn \u00bc 4. Similar results were obtained for n \u00bc 2; 3, though with smaller errors. Notice that the estimates of GPCA with the choice of \u00bc 0:02 (see Remark 6) have an error that is only about 50 percent the error of the PFA. This is because GPCA deals automatically with noisy data by choosing the points fyigni\u00bc1 in an optimal fashion. The choice of was not important (results were similar for 2 \u00bd0:001; 0:1 ). Notice also that both the K-subspaces and EM algorithms have a nonzero error in the noiseless case, showing that they frequently converge to a local minimum when a single randomly chosen initialization is used.When initializedwith GPCA, both the K-subspaces and EM algorithms reduce the error to approximately 35-50 percent with respect to random initialization. The best performance is achieved by using GPCA to initialize the K-subspaces and EM algorithms.\nFig. 5b plots the estimation error of GPCA as a function of the number of subspaces n, for different levels of noise. As expected, the error increases rapidly as a function of n because GPCA needs a minimum of O\u00f0n2\u00de data points to linearly estimate the polynomials (see Section 4.1).\nTable 1 shows the mean computing time and the mean number of iterations for a MATLAB implementation of each one of the algorithms over 1,000 trials. Among the algebraic algorithms, the fastest one is PFAwhich directly factors pn\u00f0x\u00de given cn. The extra cost of GPCA relative to the PFA is to compute thederivativesDpn\u00f0x\u00de forallx 2 X andtodivide the polynomials.Overall, GPCAgives about half the error of PFA in about twice as much time. Notice also that GPCA reduces the number of iterations of K-subspaces and EM to approximately 1/3 and 1/2, respectively. The computing times for K-subspacesandEMarealsoreducedincluding theextra time spent on initialization with GPCA or GPCA + K-subspaces."}, {"heading": "5.2 Face Clustering under Varying Illumination", "text": "Given a collection of unlabeled images fIj 2 IRDgNj\u00bc1 of n different faces taken under varying illumination, wewould like to cluster the imagescorresponding to the faceof the same person. For aLambertian object, it has been shown that the set of all images taken under all lighting conditions forms a cone in the image space,which can bewell approximated by a lowdimensional subspace [10]. Therefore, we can cluster the collection of images by estimating a basis for eachoneof those subspaces, because images of different faces will lie in different subspaces. Since, in practice, the number of pixels D is large comparedwith the dimension of the subspaces, we first apply PCA to project the images onto IRD 0 with D0 D (see Section 4.1). More specifically, we compute the SVD of the data I1 I2 IN\u00bd D N\u00bc U V T and consider a matrix X 2 IRD 0 N consisting of the first D0 columns of V T . We obtain a\nnew set of data points in IRD 0 from each one of the columns of X. We use homogeneous coordinates fxj 2 IRD 0\u00fe1gNj\u00bc1 so that eachprojected subspacegoes through theorigin.We consider a subset of the Yale Face Database B consisting of N \u00bc 64n frontal views of n \u00bc 3 faces (subjects 5, 8, and 10) under 64 varying lighting conditions. For computational efficiency,we downsampled each image to D \u00bc 30 40 pixels. Then, we projected the data onto the firstD0 \u00bc 3principal components, as shown in Fig. 6. We applied GPCA to the data in homogeneous coordinates and fitted three linear subspaces of dimensions 3, 2, and 2. GPCA obtained a perfect segmentation as shown in Fig. 6b."}, {"heading": "5.3 Temporal Segmentation of Video Sequences", "text": "Consider a news video sequence in which the camera is switching among a small number of scenes. For instance, the host could be interviewing a guest and the camera may be switching between the host, the guest, and both of them, as shown in Fig. 7a. Given the frames fIj 2 IRDgNj\u00bc1, we would like to cluster them according to the different scenes. We assume that all the frames corresponding to the same scene live in a low-dimensional subspace of IRD and that different scenes correspond to different subspaces. As in the case of face clustering, we may segment the video sequence into different scenes by applying GPCA to the image data projected onto the first few principal components. Fig. 7b shows the segmentation results for two video sequences. In both cases, a perfect segmentation is obtained."}, {"heading": "5.4 Segmentation of Linearly Moving Objects", "text": "In this section, we apply GPCA to the problem of\nsegmenting the 3D motion of multiple objects undergoing\na purely translational motion. We refer the reader to [25],\n[26], where for the case of arbitrary rotation and translation\nvia the segmentation of a mixture of fundamental matrices.\nWe assume that the scene can be modeled as a mixture of purely translational motion models, fTigni\u00bc1, where Ti 2 IR3 represents the translation of object i relative to the camera\nbetween the two consecutive frames. Given the images x1 and x2 of a point in object i in the first and second frame, respectively, the rays x1, x2 and Ti are coplanar. Therefore x1, x2 and Ti must satisfy the well-known epipolar constraint for linear motions\nxT2 \u00f0Ti x1\u00de \u00bc 0: \u00f035\u00de\nIn the case of an uncalibrated camera, the epipolar constraint reads xT2 \u00f0ei x1\u00de \u00bc 0, where ei 2 IR3 is known as the epipole and is linearly related to the translation vector Ti 2 IR3. Since the epipolar constraint can be conveniently rewritten as\neTi \u00f0x2 x1\u00de \u00bc 0; \u00f036\u00de\nwhere ei 2 IR3 represents the epipole associated with the ith motion, i \u00bc 1; . . . ; n, if we define the epipolar line \u2018 \u00bc \u00f0x2 x1\u00de 2 IR3 as a data point, then we have that eTi \u2018 \u00bc 0. Therefore, the segmentation of a set of images f\u00f0xj1; x j 2\u00degNj\u00bc1 of a collection of N points in 3D undergoing n distinct linear motions e1; . . . ; en 2 IR3, can be interpreted as a subspace segmentation problem with d \u00bc 2 and D \u00bc 3, where the\nepipoles feigni\u00bc1 are the normal to the planes and the epipolar lines f\u2018jgNj\u00bc1 are the data points. One can use (26) and Algorithm 1 to determine the number of motions n and the\nepipoles ei, respectively.\nFig. 8a shows the first frame of a 320 240 video sequence containing a truck and a car undergoing two 3D translational motions. We applied GPCA with D \u00bc 3, and \u00bc 0:02 to the epipolar lines obtained from a total of N \u00bc 92 features, 44 in the truck and 48 in the car. The algorithm obtained a perfect\nsegmentation of the features, as shown in Fig. 8b, and\nestimated the epipoles with an error of 5.9 degrees for the\ntruck and 1.7 degrees for the car.\nWe also tested the performance of GPCA on synthetic\npoint correspondences corrupted with zero-mean Gaussian\nnoise with s.t.d. between 0 and 1 pixels for an image size of\n500 500 pixels. For comparison purposes, we also implemented the PFA and the EM algorithm for segmenting hyperplanes in IR3. Figs. 8c and 8d show the performance of all the algorithms as a function of the level of noise for n \u00bc 2 moving objects. The performance measures are the mean\nerror between the estimated and the true epipoles (in\ndegrees) and the mean percentage of correctly segmented\nfeature points using 1,000 trials for each level of noise. Notice\nthat GPCA gives an error of less than 1.3 degrees and a\nclassification performance of over 96 percent. Thus, GPCA\ngives approximately 1/3 the error of PFA and improves the\nclassification performance by about 2 percent. Notice also\nthat EM with the normal vectors initialized at random (EM)\nyields a nonzero error in the noise free case, because it\nfrequently converges to a local minimum. In fact, our\nalgorithm outperforms EM. However, if we use GPCA to\ninitialize EM (GPCA + EM), the performance of both\nalgorithms improves, showing that our algorithm can be\neffectively used to initialize iterative approaches to motion\nsegmentation. Furthermore, the number of iterations of\nGPCA + EM is approximately 50 percent with respect to\nEM randomly initialized; hence, there is also a gain in\ncomputing time. Figs. 8e and 8f show the performance of\nGPCA as a function of the number of moving objects for\ndifferent levels of noise. As expected, the performance\ndeteriorates as the number of moving objects increases,\nthough the translation error is still below 8 degrees and the\npercentage of correct classification is over 78 percent."}, {"heading": "5.5 Three-Dimensional Motion Segmentation from Multiple Affine Views", "text": "Let fxfp 2 IR2gp\u00bc1;...;Nf\u00bc1;...;F be a collection of F images of N 3D points fXp 2 IR3gNj\u00bc1 taken by a moving affine camera. Under the affine camera model, which gener-\nalizes orthographic, weak perspective, and paraperspec-\ntive projection, the images satisfy the equation\nxfp \u00bc AfXp; \u00f037\u00de\nwhere Af 2 IR2 4 is the affine camera matrix for frame f , which depends on the position and orientation of the camera as well as the internal calibration parameters. Therefore, if we stack all the image measurements into a 2F N matrix W , we obtain\nW \u00bcMST\nx11 x1N .. . .. .\nxF1 xFN\n2 664\n3 775 2F N \u00bc A1 .. . AF 2 664 3 775 2F 4 X1 XN\u00bd 4 N:\n\u00f038\u00de\nIt follows from (38) that rank\u00f0W\u00de 4; hence, the 2D trajectories of the image points across multiple frames, that is, the columns of W , live in a subspace of IR2F of\ndimension 2, 3, or 4 spanned by the columns of the motion matrix M 2 IR2F 4.\nConsider now the case in which the set of points fXpgNp\u00bc1 corresponds to n moving objects undergoing n different\nmotions. In this case, each moving object spans a different d-dimensional subspace of IR2F , where d \u00bc 2, 3, or 4. Solving the motion segmentation problem is hence equivalent to\nfinding a basis for each one of such subspaces without\nknowing which points belong to which subspace. Therefore,\nwe can apply GPCA to the image measurements projected onto a subspace of IR2F of dimensionD \u00bc dmax \u00fe 1 \u00bc 5. That is, if W \u00bc U V T is the SVD of the data matrix, then we can solve themotion segmentationproblembyapplyingGPCAto the first five columns of V T .\nWe tested GPCA on two outdoor sequences taken by a\nmoving camera tracking a car moving in front of a parking\nlot and a building (sequences A and B), and one indoor\nsequence taken by a moving camera tracking a person\nmoving his head (sequence C), as shown in Fig. 9. The data\nfor these sequences are taken from [14] and consist of point\ncorrespondences in multiple views, which are available at\nhttp://www.suri.it.okayama-u.ac.jp/data.html. For all\nsequences, the number of motions is correctly estimated from (11) as n \u00bc 2 for all values of 2 \u00bd2; 20 10 7. Also, GPCA gives a percentage of correct classification of\n100.0 percent for all three sequences, as shown in Table 2.\nThe table also shows results reported in [14] from existing\nmultiframe algorithms for motion segmentation. The com-\nparison is somewhat unfair, because our algorithm is purely\nalgebraic, while the others use iterative refinement to deal\nwith noise. Nevertheless, the only algorithm having a\ncomparable performance to ours is Kanatani\u2019s multistage\noptimization algorithm, which is based on solving a series of\nEM-like iterative optimization problems, at the expense of a\nsignificant increase in computation."}, {"heading": "6 CONCLUSIONS AND OPEN ISSUES", "text": "We have proposed an algebro-geometric approach to sub-\nspace segmentation called Generalized Principal Component\nAnalysis (GPCA). Our approach is based on estimating a\ncollection of polynomials fromdata and then evaluating their\nderivatives at a data point in order to obtain a basis for the\nsubspace passing through that point. Our experiments\nshowed that GPCA gives about half of the error with respect\nto existing algebraic algorithms based on polynomial\nfactorization, and significantly improves the performance of\niterative techniques such as K-subspaces and EM. We also\ndemonstrated the performance of GPCA on vision problems\nsuch as face clustering and video/motion segmentation. At present, GPCA works well when the number and the dimensions of the subspaces are small, but the performance deteriorates as the number of subspaces increases. This is because GPCA starts by estimating a collection of polynomials in a linear fashion, thus neglecting the nonlinear constraints among the coefficients of those polynomials, the so-called Brill\u2019s equations [6]. Another open issue has to do with the estimation of the number of subspaces n and their dimensions fdigni\u00bc1 by harnessing additional algebraic properties of the vanishing ideals of subspace arrangements (e.g., the Hilbert function of the ideals). Throughout the paper, we hinted at a connection between GPCA and Kernel Methods, e.g., the Veronese map gives an embedding that satisfies the modeling assumptions of KPCA (see Remark 1). Further connections between GPCA and KPCA are worthwhile investigating. Finally, the current GPCA algorithm does not assume the existence of outliers in the given sample data, though one can potentially incorporate statistical methods such as influence theory and random sampling consensus to improve its robustness.Wewill investigate these problems in future research."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank Drs. Jacopo Piazzi and Kun Huang for their contributions to this work, and Drs. Frederik Schaffalitzky and Robert Fossum for insightful discussions on the topic. This work was partially supported by Hopkins WSE startup funds, UIUC ECE startup funds, and by grants NSF CAREER IIS-0347456, NSF CAREER IIS0447739, NSF CRS-EHS-0509151, ONR YIP N00014-05-10633, ONR N00014-00-1-0621, ONR N000140510836, and DARPA F33615-98-C-3614."}], "references": [{"title": "A New Approach to Dimensionality Reduction Theory and Algorithms", "author": ["D.S. Broomhead", "M. Kirby"], "venue": "SIAM J. Applied Math., vol. 60, no. 6, pp. 2114-2142, 2000.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "A Generalization of Principal Component Analysis to the Exponential Family", "author": ["M. Collins", "S. Dasgupta", "R. Schapire"], "venue": "Advances on Neural Information Processing Systems, vol. 14, 2001.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "A Multibody Factorization Method for Independently Moving Objects", "author": ["J. Costeira", "T. Kanade"], "venue": "Int\u2019l J. Computer Vision, vol. 29, no. 3, pp. 159-179, 1998.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "The Approximation of One Matrix by Another of Lower Rank", "author": ["C. Eckart", "G. Young"], "venue": "Psychometrika, vol. 1, pp. 211-218, 1936.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1936}, {"title": "RANSAC Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography", "author": ["M.A. Fischler", "R.C. Bolles"], "venue": "Comm. ACM, vol. 26, pp. 381-395, 1981.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1981}, {"title": "Zelevinsky, Discriminants, Resultants, and Multidimensional Determinants", "author": ["I.M. Gelfand", "M.M. Kapranov", "A.V"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Algebraic Geometry: A", "author": ["J. Harris"], "venue": "First Course. Springer-Verlag,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1992}, {"title": "Clustering Apperances of Objects under Varying Illumination Conditions", "author": ["J. Ho", "M.-H. Yang", "J. Lim", "K.-C. Lee", "D. Kriegman"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. 1, pp. 11-18, 2003.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Minimum Effective Dimension for Mixtures of Subspaces: A Robust GPCA Algorithm and Its Applications", "author": ["K. Huang", "Y. Ma", "R. Vidal"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. 2, pp. 631-638, 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Principal Component Analysis", "author": ["I. Jolliffe"], "venue": "New York: Springer- Verlag,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1986}, {"title": "Motion Segmentation by Subspace Separation and Model Selection", "author": ["K. Kanatani"], "venue": "Proc. IEEE Int\u2019l Conf. Computer Vision, vol. 2, pp. 586-591, 2001.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Multi-Stage Optimization for Multi- Body Motion Segmentation", "author": ["K. Kanatani", "Y. Sugaya"], "venue": "Proc. Australia-Japan Advanced Workshop Computer Vision, pp. 335-349, 2003.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Multiple Eigenspaces", "author": ["A. Leonardis", "H. Bischof", "J. Maver"], "venue": "Pattern Recognition, vol. 35, no. 11, pp. 2613-2627, 2002.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Nonlinear Component Analysis as a Kernel Eigenvalue Problem", "author": ["B. Scholkopf", "A. Smola", "K.-R. Muller"], "venue": "Neural Computation, vol. 10, pp. 1299-1319, 1998.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "A Unified Computational Theory for Motion Transparency and Motion Boundaries Based on Eigenenergy Analysis", "author": ["M. Shizawa", "K. Mase"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition, pp. 289-295, 1991.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1991}, {"title": "Probability and Random Processes with Applications to Signal Processing, third ed", "author": ["H. Stark", "J.W. Woods"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Mixtures of Probabilistic Principal Component Analyzers,\u201dNeural", "author": ["M. Tipping", "C. Bishop"], "venue": "Computation, vol. 11,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Probabilistic Principal Component Analysis", "author": ["M. Tipping", "C. Bishop"], "venue": "J. Royal Statistical Soc., vol. 61, no. 3, pp. 611-622, 1999.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "An Integrated Bayesian Approach to Layer Extraction from Image Sequences", "author": ["P. Torr", "R. Szeliski", "P. Anandan"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 23, no. 3, pp. 297-303, Mar. 2001.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "Motion Segmentation with Missing Data by PowerFactorization and Generalized PCA", "author": ["R. Vidal", "R. Hartley"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. II, pp. 310-316, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "A New GPCA Algorithm for Clustering Subspaces by Fitting, Differentiating, and Dividing Polynomials", "author": ["R. Vidal", "Y. Ma", "J. Piazzi"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. I, pp. 510-517, 2004.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Generalized Principal Component Analysis (GPCA)", "author": ["R. Vidal", "Y. Ma", "S. Sastry"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. I, pp. 621-628, 2003.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Two-View Multibody Structure from Motion", "author": ["R. Vidal", "Y. Ma", "S. Soatto", "S. Sastry"], "venue": "Int\u2019l J. Computer Vision, to be published in 2006.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "A Unified Algebraic Approach to 2-D and 3- D Motion Segmentation", "author": ["R. Vidal", "Y. Ma"], "venue": "Proc. European Conf. Computer Vision, pp. 1-15, 2004.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Multibody Grouping via Orthogonal Subspace Decomposition", "author": ["Y. Wu", "Z. Zhang", "T.S. Huang", "J.Y. Lin"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. 2, pp. 252-257, 2001.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 9, "context": "PRINCIPAL Component Analysis (PCA) [12] refers to the problem of fitting a linear subspace S IR of unknown dimension d < D to N sample points fxjgNj1\u204441 in S.", "startOffset": 35, "endOffset": 39}, {"referenceID": 17, "context": "In Probabilistic PCA [20] (PPCA), the noise is assumed to be drawn from an unknown distribution and the problem becomes one of identifying the subspace and distribution parameters in a maximum-likelihood sense.", "startOffset": 21, "endOffset": 25}, {"referenceID": 1, "context": "When the noise distribution is Gaussian, the algebro-geometric and probabilistic interpretations coincide [2].", "startOffset": 106, "endOffset": 109}, {"referenceID": 1, "context": "noise distribution is non-Gaussian, the solution toPPCA is no longer linear, as shown in [2], where PCA is generalized to", "startOffset": 89, "endOffset": 92}, {"referenceID": 13, "context": "The standard solution toNLPCA [16] is based on first embedding the data into a higher-dimensional feature space F and then applying standard PCA to the embedded data.", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "In the context of stochastic signal processing, PCA is also known as the Karhunen-Loeve transform [18]; in the applied statistics literature, SVD is also known as the Eckart and Young decomposition [4].", "startOffset": 98, "endOffset": 102}, {"referenceID": 3, "context": "In the context of stochastic signal processing, PCA is also known as the Karhunen-Loeve transform [18]; in the applied statistics literature, SVD is also known as the Eckart and Young decomposition [4].", "startOffset": 198, "endOffset": 201}, {"referenceID": 7, "context": ", K-subspaces [10], an extension of K-means to the case of subspaces, subspace growing and subspace selection [15], or Expectation Maximization (EM) for mixtures of PCAs [19].", "startOffset": 14, "endOffset": 18}, {"referenceID": 12, "context": ", K-subspaces [10], an extension of K-means to the case of subspaces, subspace growing and subspace selection [15], or Expectation Maximization (EM) for mixtures of PCAs [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": ", K-subspaces [10], an extension of K-means to the case of subspaces, subspace growing and subspace selection [15], or Expectation Maximization (EM) for mixtures of PCAs [19].", "startOffset": 170, "endOffset": 174}, {"referenceID": 18, "context": "Unfortunately, most iterative methods are, in general, very sensitive to initialization; hence, they may not converge to the global optimum [21].", "startOffset": 140, "endOffset": 144}, {"referenceID": 10, "context": "In [13] (see, also, [3]), it is shown that when the subspaces are orthogonal, of equal dimension d, and intersect only at the origin, which implies thatD nd, one can use the SVD of the data to define a similarity matrix from which the segmentation of the data can be obtained using spectral clustering techniques.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "In [13] (see, also, [3]), it is shown that when the subspaces are orthogonal, of equal dimension d, and intersect only at the origin, which implies thatD nd, one can use the SVD of the data to define a similarity matrix from which the segmentation of the data can be obtained using spectral clustering techniques.", "startOffset": 20, "endOffset": 23}, {"referenceID": 10, "context": "Unfortunately, thismethod is sensitive to noise in the data, as shown in [13], [27] where various improvements are proposed, and fails when the subspaces intersect arbitrarily [14], [22], [28].", "startOffset": 73, "endOffset": 77}, {"referenceID": 24, "context": "Unfortunately, thismethod is sensitive to noise in the data, as shown in [13], [27] where various improvements are proposed, and fails when the subspaces intersect arbitrarily [14], [22], [28].", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "Unfortunately, thismethod is sensitive to noise in the data, as shown in [13], [27] where various improvements are proposed, and fails when the subspaces intersect arbitrarily [14], [22], [28].", "startOffset": 176, "endOffset": 180}, {"referenceID": 19, "context": "Unfortunately, thismethod is sensitive to noise in the data, as shown in [13], [27] where various improvements are proposed, and fails when the subspaces intersect arbitrarily [14], [22], [28].", "startOffset": 182, "endOffset": 186}, {"referenceID": 11, "context": "The latter case has been addressed in an ad hoc fashion by using clustering algorithms such as K-means, spectral clustering, or EM [14], [28] to segment the data and PCA to obtain a basis for each group.", "startOffset": 131, "endOffset": 135}, {"referenceID": 14, "context": "The only algebraic approaches that deal with arbitrary intersections are [17], which studies the case of two planes in IR and [24] which studies the case of subspaces of codimension one, i.", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "The only algebraic approaches that deal with arbitrary intersections are [17], which studies the case of two planes in IR and [24] which studies the case of subspaces of codimension one, i.", "startOffset": 126, "endOffset": 130}, {"referenceID": 20, "context": "Our previous work [23] extended this framework to subspaces of unknown and possibly different dimensions under the additional assumption that the number of subspaces is known.", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "This paper unifies the results of [24] and [23] and extends to the case inwhich both the number anddimensions of the subspaces are unknown.", "startOffset": 34, "endOffset": 38}, {"referenceID": 20, "context": "This paper unifies the results of [24] and [23] and extends to the case inwhich both the number anddimensions of the subspaces are unknown.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": ";nDx n1 1 x n2 2 x nD D ; \u00f01\u00de where n : IR D ! IRMn\u00f0D\u00de is the Veronese map of degree n [7], also known as the polynomial embedding in machine learning, defined as n : 1\u20442x1; .", "startOffset": 87, "endOffset": 90}, {"referenceID": 8, "context": "An alternative way of selecting the correct linear model (in feature space) for noisy data can be found in [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 4, "context": "Many methods from robust statistics can be deployed to detect and reject the outliers [5], [11].", "startOffset": 86, "endOffset": 89}, {"referenceID": 8, "context": "Many methods from robust statistics can be deployed to detect and reject the outliers [5], [11].", "startOffset": 91, "endOffset": 95}, {"referenceID": 4, "context": "Alternatively, one can detect and reject outliers using Random Sample Consensus (RANSAC) [5].", "startOffset": 89, "endOffset": 92}, {"referenceID": 0, "context": "The reader may refer to [1] for alternative ways of choosing a projection.", "startOffset": 24, "endOffset": 27}, {"referenceID": 8, "context": "Onemay refer to [11] for a more detailed discussion on selecting the best multiple-subspace model from noisy data, using modelselection criteria such as MML, MDL, AIC, and BIC.", "startOffset": 16, "endOffset": 20}, {"referenceID": 21, "context": "See [24] for further details.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "See [10] for further details.", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "See [19] for further details.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "For aLambertian object, it has been shown that the set of all images taken under all lighting conditions forms a cone in the image space,which can bewell approximated by a lowdimensional subspace [10].", "startOffset": 196, "endOffset": 200}, {"referenceID": 22, "context": "We refer the reader to [25],", "startOffset": 23, "endOffset": 27}, {"referenceID": 23, "context": "[26], where for the case of arbitrary rotation and translation", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "for these sequences are taken from [14] and consist of point", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "The table also shows results reported in [14] from existing", "startOffset": 41, "endOffset": 45}, {"referenceID": 11, "context": "Segmenting the point correspondences of sequences A (left), B (center), and C (right) in [14] for each pair of consecutive frames by segmenting subspaces in IR.", "startOffset": 89, "endOffset": 93}, {"referenceID": 5, "context": "This is because GPCA starts by estimating a collection of polynomials in a linear fashion, thus neglecting the nonlinear constraints among the coefficients of those polynomials, the so-called Brill\u2019s equations [6].", "startOffset": 210, "endOffset": 213}], "year": 2005, "abstractText": "This paper presents an algebro-geometric solution to the problem of segmenting an unknown number of subspaces of unknown and varying dimensions from sample data points. We represent the subspaces with a set of homogeneous polynomials whose degree is the number of subspaces and whose derivatives at a data point give normal vectors to the subspace passing through the point. When the number of subspaces is known, we show that these polynomials can be estimated linearly from data; hence, subspace segmentation is reduced to classifying one point per subspace. We select these points optimally from the data set by minimizing certain distance function, thus dealing automatically with moderate noise in the data. A basis for the complement of each subspace is then recovered by applying standard PCA to the collection of derivatives (normal vectors). Extensions of GPCA that deal with data in a highdimensional space and with an unknown number of subspaces are also presented. Our experiments on low-dimensional data show that GPCA outperforms existing algebraic algorithms based on polynomial factorization and provides a good initialization to iterative techniques such as K-subspaces and Expectation Maximization. We also present applications of GPCA to computer vision problems such as face clustering, temporal video segmentation, and 3Dmotion segmentation from point correspondences in multiple affine views.", "creator": "3B2 Total Publishing System 7.51c/W"}}}