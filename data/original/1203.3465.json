{"id": "1203.3465", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Compiling Possibilistic Networks: Alternative Approaches to Possibilistic Inference", "abstract": "Qualitative possibilistic networks, also known as min-based possibilistic networks, are important tools for handling uncertain information in the possibility theory frame- work. Despite their importance, only the junction tree adaptation has been proposed for exact reasoning with such networks. This paper explores alternative algorithms using compilation techniques. We first propose possibilistic adaptations of standard compilation-based probabilistic methods. Then, we develop a new, purely possibilistic, method based on the transformation of the initial network into a possibilistic base. A comparative study shows that this latter performs better than the possibilistic adap- tations of probabilistic methods. This result is also confirmed by experimental results.", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (559kb)", "http://arxiv.org/abs/1203.3465v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["raouia ayachi", "nahla ben amor", "salem benferhat", "rolf haenni"], "accepted": false, "id": "1203.3465"}, "pdf": {"name": "1203.3465.pdf", "metadata": {"source": "CRF", "title": "Compiling Possibilistic Networks : Alternative Approaches to Possibilistic Inference", "authors": ["Raouia Ayachi", "Nahla Ben Amor"], "emails": ["raouia.ayachi@gmail.com", "nahla.benamor@gmx.fr", "benferhat@cril.univ-artois.fr", "rolf.haenni@bfh.ch"], "sections": [{"heading": null, "text": "Qualitative possibilistic networks, also known as min-based possibilistic networks, are important tools for handling uncertain information in the possibility theory framework. Despite their importance, only the junction tree adaptation has been proposed for exact reasoning with such networks. This paper explores alternative algorithms using compilation techniques. We first propose possibilistic adaptations of standard compilation-based probabilistic methods. Then, we develop a new, purely possibilistic, method based on the transformation of the initial network into a possibilistic base. A comparative study shows that this latter performs better than the possibilistic adaptations of probabilistic methods. This result is also confirmed by experimental results."}, {"heading": "1 INTRODUCTION", "text": "In possibility theory there are two different ways to define the counterpart of Bayesian networks. This is due to the existence of two definitions of possibilistic conditioning: product-based and min-based conditioning (Dubois and Prade, 1988). When we use the product form of conditioning, we get a possibilistic network close to the probabilistic one sharing the same features and having the same theoretical and practical results. However, this is not the case with min-based networks. In this paper, we are interested in the inference problem in multiply connected networks, which is known as a hard problem (Cooper, 1990). More precisely, we propose three compilation methods for min-based possibilistic networks.\nThe compilation of Bayesian networks is always considered as an important area. Recently, researchers\nhave been interested in various kinds of exact and approximate Bayesian networks inference algorithms using compilation techniques (Darwiche, 2003) (Chavira and Darwiche, 2005) (Wachter and Haenni, 2007), etc.\nDespite the importance of possibility theory, there is no compilation that has been proposed for possibilistic networks. This paper analyzes this issue by first adapting well-known compilation-based probabilistic inference approaches, namely the arithmetic circuit method (Darwiche, 2003) and the logical compilation of Bayesian Networks (Wachter and Haenni, 2007). Both of them are based on a network\u2019s encoding into a logical representation and a compilation into a target compilation language, namely \u03a0-DNNF. From there, all possible queries are answered in polynomial time. The third method exploits results obtained on one hand in (Benferhat et al., 2002) that transforms a minbased possibilistic network into a possibilistic knowledge base, and on the other hand results obtained regarding compilation of possibilistic bases (Benferhat et al., 2007) in order to assure inference in polytime. This method that is purely possibilistic is flexible since it permits to exploit efficiently all the existing propositional compilers.\nThe rest of this paper is organized as follows: Section 2 gives a briefly background on possibility theory, possibilistic logic, possibilistic networks and introduces some compilation concepts. Section 3 is dedicated to possibilistic adaptations of compilation-based probabilistic inference methods. Section 4 presents a new inference method in possibilistic networks using compiled possibilistic knowledge bases. Experimental study is presented in Section 5."}, {"heading": "2 BASIC CONCEPTS", "text": ""}, {"heading": "2.1 POSSIBILITY THEORY", "text": "This subsection briefly recalls some elements of possibility theory, for more details we refer to (Dubois and Prade, 1988). Let V = {X1, X2, ..., XN} be a set of\nvariables. We denote by DXi = {x1, .., xn} the domain associated with the variable Xi. By xi we denote any instance of Xi. \u2126 denotes the universe of discourse, which is the Cartesian product of all variable domains in V . Each element \u03c9 \u2208 \u2126 is called a state of \u2126. The notion of possibility distribution denoted by \u03c0 is a mapping from the universe of discourse to the unit interval [0, 1]. To this scale, two interpretations can be attributed, a quantitative one when values have a real sense and a qualitative one when values reflect only an order between the different states of the world. This paper focuses on the qualitative interpretation of possibility theory.\nGiven a possibility distribution \u03c0, we can define a mapping grading the possibility measure of an event \u03c6 \u2286 \u2126 by \u03a0(\u03c6) = max\u03c9\u2208\u03c6\u03c0(\u03c9). \u03a0 has a dual measure which is the necessity measure N(\u03c6) = 1\u2212\u03a0(\u00ac\u03c6).\nConditioning consists in modifying our initial knowledge, encoded by a possibility distribution \u03c0, by the arrival of a new certain piece of information \u03c6 \u2286 \u2126. The qualitative interpretation of the scale [0, 1] leads to the well known definition of min-conditioning (Hisdal, 1978), (Dubois and Prade, 1988):\n\u03a0(\u03c8 | \u03c6) = {\n\u03a0(\u03c8 \u2227 \u03c6) if \u03a0(\u03c8 \u2227 \u03c6) < \u03a0(\u03c6) 1 otherwise (1)"}, {"heading": "2.2 POSSIBILISTIC LOGIC", "text": "Possibilistic logic (Dubois et al., 1994) handles qualitative uncertainty in a logical setting. A possibilistic logic formula is a pair (p, a) where p is a propositional formula and a its uncertainty degree which estimates to what extent it is certain that p is true. The higher is the weight, the more certain is the formula. A possibilistic knowledge base \u03a3 is made up of a finite set of weighted formulas, i.e.,\n\u03a3 = {(pi, ai), i = 1, .., n} (2)\nwhere ai is the lower bound on N(pi).\nEach possibilistic knowledge base induces a unique possibility distribution such that \u2200 \u03c9 \u2208 \u2126 and \u2200 (pi, ai) \u2208 \u03a3:\n\u03c0\u03a3(\u03c9) = {\n1 if \u03c9 |= pi 1\u2212max {ai : \u03c9 2 pi} otherwise\n(3)\nwhere |= is propositional logic entailment."}, {"heading": "2.3 POSSIBILISTIC NETWORKS", "text": "A min-based possibilistic network over a set of variables V , denoted by \u03a0Gmin is composed of: - a graphical component that is a DAG (Directed\nAcyclic Graph) where nodes represent variables and edges encode the links between the variables. The parent set of a node Xi is denoted by Ui = {Ui1, Ui2, ..., Uim}. For any ui of Ui we have ui = {ui1, ui2, ..., uim} where m is the number of parents of Xi. In what follows, we use xi, ui, uij to denote, respectively, possible instances of Xi, Ui and Uij . - a numerical component that quantifies different links. For every root node Xi (Ui = \u2205), uncertainty is represented by the a priori possibility degree \u03a0(xi) of each instance xi \u2208 DXi , such that maxxi\u03a0(xi) = 1. For the rest of the nodes (Ui 6= \u2205) uncertainty is represented by the conditional possibility degree \u03a0(xi|ui) of each instances xi \u2208 DXi and ui \u2208 DUi . These conditional distributions satisfy the following normalization condition: maxxi\u03a0(xi|ui) = 1, for any ui.\nThe set of a priori and conditional possibility degrees in a min-based possibilistic network induce a unique joint possibility distribution defined by the following chain rule:\n\u03c0min(X1, .., XN ) = min i=1..N \u03a0(Xi | Ui) (4)"}, {"heading": "2.4 COMPILATION CONCEPTS", "text": "A target compilation language is a class of formulas which is tractable for a set of transformations and queries. Compilation languages are compared in terms of their spatial efficiency via the succinctness criteria and also in terms of the set of logical queries and transformations they support in polynomial time (see (Darwiche and Marquis, 2002) for more details).\nWithin the most effective target compilation languages, we cite the Decomposable Negation Normal Form (DNNF) (Darwiche, 2001). This language is universal and presents a number of properties (determinism, smoothness, etc.) that makes it of a great interest. It supports a rich set of polynomial-time logical operations. To define DNNF, the starting point is Negation Normal Form (NNF) which is a set of propositional formulas where possible connectives are conjunctions, disjunctions and negations. A set of important properties may be imposed to NNF, such that: - Decomposability : the conjuncts of any conjunction in NNF do not share variables. - Determinism: two disjuncts of any disjunction in NNF are logically contradictory. - Smoothness: the disjunct of any disjunction in NNF mentions the same variables.\nThese properties lead to a number of interesting subsets of NNF. Within these subsets, the language DNNF (Darwiche, 2001) is one of the most effective target compilation languages that supports the decomposability. We can also mention, the d-DNNF sat-\nisfying determinism, sd-DNNF satisfying smoothness and determinism, etc. Each compilation language supports some queries and transformations in polynomial time. In what follows we are in particular interested by conditioning and forgetting transformations (Darwiche and Marquis, 2002)."}, {"heading": "3 POSSIBILISTIC ADAPTATIONS", "text": "OF COMPILATION-BASED PROBABILISTIC INFERENCE METHODS\nThere are several compilation methods which handle the inference problem in probabilistic graphical models. In this section, we first propose an adaptation of the arithmetic circuit method of (Darwiche, 2003). Then we will study one of its variants proposed in (Wachter and Haenni, 2007), namely the logical compilation of Bayesian Networks.\nDNNF has been introduced for propositional language. Recall that in qualitative possibility theory, we basically manipulate two main operators Max and Min. These operators fully make sense when we deal with qualitative plausibility ordering. Therefore, we propose to define concepts of \u03a0-DNNF (resp. \u03a0-d-DNNF, \u03a0-sd-DNNF) as adaptations of the DNNF language (resp. d-DNNF, sd-DNNF) (Darwiche, 2001) in the possibilistic setting (definition 1).\nDefinition 1. A sentence in \u03a0-DNNF is a rooted DAG where each leaf node is labeled with true, false or variable\u2019s instances and each internal node is labeled with max or min operators and can have arbitrarily several children. Roughly speaking, \u03a0-DNNF is the same as the classical DNNF although its operators are max and min instead of \u2228 and \u2227, respectively. Example 1. Figure 1 depicts a sentence in \u03a0-DNNF. Consider the Min-node (root) in this figure. This node has two children, the first contains variables A, B while the second contains variables C, D. This node is decomposable since its two children do not share variables.\nA sentence in \u03a0-d-DNNF is a sentence in \u03a0-DNNF satisfying decomposability and determinism (viewing\n\u2228 and \u2227 as max and min operators, respectively). A sentence in \u03a0-sd-DNNF is a sentence in \u03a0-DNNF satisfying decomposability, determinism and smoothness."}, {"heading": "3.1 INFERENCE USING POSSIBILISTIC CIRCUITS", "text": "In (Darwiche, 2003), authors have focused on inference in compiled Bayesian networks. The main idea is based on representing the network using a polynomial and then retrieving answers to probabilistic queries by evaluating and differentiating the polynomial. This latter itself is exponential in size, so it has been represented efficiently using an arithmetic circuit that can be evaluated and differentiated in time and space linear in the circuit size. In what follows, we propose a direct adaptation of this method in the possibilistic setting. Given a min-based possibilistic network, we first encode it using a possibilistic function fmin defined by two types of variables:\n\u2022 Evidence indicators: for each variable Xi in the network , we have a variable \u03bbxi for each instance xi \u2208 DXi .\n\u2022 Network parameters: for each variable Xi and its parents Ui in the network, we have a variable \u03b8xi|ui for each instance xi \u2208 DXi and ui \u2208 DUi .\nfmin = max x min (xi,ui)\u223cx \u03bbxi\u03b8xi|ui (5)\nwhere x represents instantiations of all network variables and ui \u223c x denotes the compatibility relationship among ui and x. The possibilistic function fmin of a possibilistic network represents the possibility distribution and allows to compute possibility degrees of variables of interest. Namely, for any piece of evidence e which is an instantiation of some variables E in the network, we can instantiate fmin as it returns the possibility of e, \u03a0(e) (Definition 2 and Proposition 1).\nDefinition 2. The value of the possibilistic function fmin at evidence e, denoted by fmin(e), is the result of replacing each evidence indicator \u03bbxi in fmin with 1 if xi is consistent with e, and with 0 otherwise.\nProposition 1. Let \u03a0Gmin be a possibilistic network representing the possibility distribution \u03c0 and having the possibilistic function fmin. For any evidence e, we have fmin(e) = \u03c0(e).\nLet figure 2 be the min-based possibilistic network used throughout the paper.\nThe possibilistic function of the network in figure 2 has 8 terms corresponding to the 8 instantiations of variables F,B,D. Two of these terms are as follows:\nfmin = max(min(\u03bbd1 , \u03bbf1 , \u03bbb1 , \u03b8d1|f1,b1 , \u03b8f1 , \u03b8b1); min (\u03bbd1 , \u03bbf2 , \u03bbb1 , \u03b8d1|f2,b1 , \u03b8f2 , \u03b8b1); \u00b7 \u00b7 \u00b7 )\nIf the evidence e = (d1, b1) then fmin(d1, b1) is obtained by applying the following substitutions to fmin: \u03bbd1 = 1, \u03bbd2 = 0, \u03bbb1 = 1, \u03bbb2 = 0, \u03bbf1 = \u03bbf2 = 1. This leads to \u03a0(e) = 0.7.\nThe possibilistic function fmin is then encoded on a propositional theory (CNF) using \u03bbxi and \u03b8xi|ui . For each network variable Xi, the encoding contains the following clauses:\n\u03bbxi \u2228 \u03bbxj (6)\n\u00ac\u03bbxi \u2228 \u00ac\u03bbxj , i 6= j (7)\nMoreover, for each propositional variable \u03b8xi|ui , the encoding contains the clause:\n\u03bbxi \u2227 \u03bbui1 \u2227 . . . \u2227 \u03bbuim \u2194 \u03b8xi|ui (8)\nThe CNF encoding, denoted by Kfmin recovers the min-joint possibility distribution (proposition 2).\nProposition 2. The CNF encoding Kfmin of a possibilistic network encodes the joint distribution of given network.\nOnce the CNF encoding is accomplished, it is then compiled into a \u03a0-DNNF, from which we extract the possibilistic circuit \u03b6p (definition 3) that implements the encoded fmin.\nDefinition 3. A possibilistic circuit \u03b6p encoded by a \u03a0-DNNF sentence \u03bec is a DAG in which leaf nodes correspond to circuit inputs, internal nodes correspond to max and min operators, and the root corresponds to the circuit output.\nAs in the probabilistic case (Darwiche, 2003), this circuit can be used for linear-time inference. More precisely, computing the possibility degree of an event consists on evaluating \u03b6p by setting each evidence indicator \u03bbx to 1 if the event is consistent with x, to 0 otherwise and applying operators in a bottom-up way. This possibility degree corresponds exactly to the one computed from the min-joint possibility distribution (proposition 3). This method referred to \u03a0-DNNFPF\nis outlined by algorithm 1. Note that the suffix PF is added to signify that this method uses a possibilistic function (fmin) before ensuring the CNF encoding.\nAlgorithm 1: Inference using \u03a0-DNNF (\u03a0-DNNFPF )\nData: \u03a0Gmin , instance of interest x, evidence e Result: \u03a0(x|e) begin\nCompilation into \u03a0-DNNF Encode \u03a0Gmin into fmin using equation 5 EncodeCNF of \u03a0Gmin into \u03be using equations 6, 7, 8 Compile \u03be into \u03bec \u03b6p \u2190 Possibilistic Circuit of \u03bec Inference Applying Operators on \u03b6p \u03a0(x, e) \u2190 Root Value (\u03b6p; (x,e)) \u03a0(e) \u2190 Root Value (\u03b6p; e) if \u03a0(x, e) \u227a \u03a0(e) then \u03a0(x|e) \u2190 \u03a0(x, e) else \u03a0(x|e) \u2190 1 return \u03a0(x|e)\nend\nProposition 3. Let \u03a0Gmin be a possibilistic network. Let \u03c0min be a joint distribution obtained by chain rule. Then for any a \u2208 Da and e \u2208 DE, we have \u03a0(A = a|E = e) = \u03a0min(A = a|E = e) where \u03a0min(A = a|E = e) is obtained from \u03c0min using equation 1 and \u03a0(A = a|E = e) is obtained from algorithm 1.\nThe key point to observe here is that this approach can handle possibilistic circuits of manageable size as in the probabilistic case since some possibility values may have some specific values; for instance, whether they are equal to 0 or 1, and whether some possibilities are equal. In this case, we can say that the network exhibit some local structure. By exploiting it, the produced circuits can be smaller. In fact, the normalization constraint relative to the initial network will mean that we will have several values equal to 1. Thus the idea is to make an advantage from such a local structure which has a particular behavior with the max operator in order to construct more compact possibilistic circuits w.r.t. standard ones as stated by the following proposition: Proposition 4. Let Nbposs and Nbproba be the number of clauses in the possibilistic and probabilistic cases, respectively. Then Nbposs \u2264 Nbproba.\nNote that for particular situations where probability values are 1 or 0, we have Nbposs = Nbproba, otherwise Nbposs \u227a Nbproba. Example 2. To illustrate algorithm 1 we will consider the min-based possibilistic network represented in figure 2. We are looking for \u03a0(f2|d1) with f2 as instance of interest and d1 as evidence. First, we encode the network as a possibilistic function and encode it on CNF. This latter is then compiled into \u03a0-DNNF from which a possibilistic circuit is extracted. The possibility degree \u03a0(f2|d1) is computed using this circuit in polynomial time. For instance, \u03a0(f2, d1) is computed using \u03b6p by just replacing\n\u03bbf2 = \u03bbd1 = \u03bbb1 = \u03bbb2 = 1 and applying possibilistic operators in a bottom-up way as shown in figure 3. Hence, \u03a0(f2|d1) = \u03a0(f2, d1) = 0.4 since \u03a0(f2, d1) = 0.4 \u227a 1."}, {"heading": "3.2 INFERENCE USING POSSIBILISTIC COMPILED REPRESENTATIONS", "text": "DNNF plays an interesting role in compiling propositional knowledge bases. It has been used to compile probabilistic networks. More precisely in (Wachter and Haenni, 2007), authors have been interested in performing a CNF logical encoding of the probability distribution induced by a bayesian network, then a compilation phase from CNF to d-DNNF. In this section, we propose to adapt this encoding in the possibilistic setting by taking into consideration the local structure aspect. This allows to reduce the number of additional variables comparing to the probabilistic encoding. Let \u2206 be propositions linked to network\u2019s variables and let \u03b8 be propositions linked to the possibility distribution entries (equal to 1). We start by looking at the possibility distribution encoding. The logical representation of a network variable Xi is defined by: \u03c8Xi =\u2227 ui ( \u2227 \u03b8xi |ui\u2208\u2126\u03b8Xi |ui ( ui1\u2227\u00b7 \u00b7 \u00b7\u2227uim\u2227\u03b8xi|ui \u2192 xi )) (9)\nBy taking the conjunction of all logical representations of variables, we obtain the network\u2019s representation \u03c8 as follows:\n\u03c8 = \u2227 Xi\u2208\u2206 \u03c8Xi (10)\nThe CNF encoding, denoted by K\u03c8 indeed recovers the min-joint possibility distribution (proposition 5). Proposition 5. Let \u03c0min be the joint possibility distribution obtained using the chain rule with the minimum operator and \u03a0 be the possibility degree computed\nfrom a function f\u03c8 encoding the CNF. Then, we have \u03c0min(xi, ..., xj) = \u03a0(xi, ..., xj), i.e. f\u03c8 recovers the min-joint possibility distribution \u03c0min.\nComparing theoretically the probabilistic and the possibilistic case allows us to deduce the following proposition: Proposition 6. The possibilistic encoding of a possibilistic network given by K\u03c8 (equation 10) is more compact than the probabilistic encoding given in (Wachter and Haenni, 2007). In fact, the number of variables used in K\u03c8 is less than the one used in (Wachter and Haenni, 2007). In particular for parameters, our approach uses one variable per different weight, while in the probabilistic encoding one variable per parameter. For each clause in K\u03c8 there exists a clause of the same size in the probabilistic encoding. The converse is false.\nOnce the qualitative network is encoded by K\u03c8, it is compiled into a compilation language that supports the transformations conditioning and forgetting and the query possibilistic computation. This language is \u03a0-DNNF (proposition 7). Therefore, the CNF encoding is first compiled, and the resulting \u03a0-DNNF is then used to compute efficiently, i.e. in polynomial time a-posteriori possibility degrees (proposition 8). This method referred to \u03a0-DNNF is outlined by algo. 2. Proposition 7. \u03a0-DNNF supports conditioning, forgetting and possibilistic computation.\nAlgorithm 2: Inference using \u03a0-DNNF\nData: \u03a0Gmin , instance of interest x, evidence e Result: \u03a0(x|e) begin\nCompilation into \u03a0-DNNF EncodeCNF of \u03a0Gmin into \u03c8 using equation 10 Compile \u03c8 into \u03c8cp Inference v1 \u2190 Explore \u03a0-DNNF(x \u2227 e, \u03c8cp) v2 \u2190 Explore \u03a0-DNNF(e, \u03c8cp) if v1 \u227a v2 then \u03a0(x|e) \u2190 v1 else \u03a0(x|e) \u2190 1 return \u03a0(x|e)\nend\nProposition 8. Let \u03a0Gmin be a possibilistic network. Let \u03c0min be a joint distribution obtained by chain rule. Then for any a \u2208 Da and e \u2208 DE, we have \u03a0(A = a|E = e) = \u03a0min(A = a|E = e) where \u03a0min(A = a|E = e) is obtained from \u03c0min using equation 1 and \u03a0(A = a|E = e) is obtained from algorithm 2. Example 3. Let us illustrate algorithm 2. In fact, \u03c8 of the network of figure 2 is : \u03c8 = \u03c8F \u2227 \u03c8B \u2227 \u03c8D = {(\u03b81 \u2228 f2) \u2227 (\u03b82 \u2228 b1) \u2227 (f2 \u2228 b2 \u2228 \u03b82 \u2228 d1) \u2227 (f2 \u2228 b1 \u2228 \u03b81 \u2228 d2) \u2227 (f1 \u2228 b2 \u2228 \u03b83 \u2228 d2) \u2227 (f1 \u2228 b1 \u2228 \u03b84 \u2228 d2)} such as \u03b81, \u03b82, \u03b83 and \u03b84 correspond respectively to 0.8, 0.7, 0.4 and 0.2.\nTo compute \u03a0(f2|d1), we should first compute \u03a0(f2, d1) using algorithm 3. The first step is to check if we have at least\nAlgorithm 3: Explore \u03a0-DNNF\nData: a set of instances x, compiled representation \u03c8cp\nResult: \u03a0(x)\nbegin if \u2200 xi \u2208 x, \u03b8xi|Ui is not a leaf node then\n\u03a0(x) \u2190 1 else\ny= {xi | \u2200, \u03b8xi|Ui is a leaf node \u2200 Ui \u2286 x} \u03c8cp|y \u2190 Condition \u03c8cp on y \u03c8cp\u2193|y \u2190 Forget \u2206 from \u03c8cp|y Applying Operators on \u03c8cp\u2193|y \u03a0(x) \u2190 Root Value of \u03c8cp\u2193|y\nreturn \u03a0(x)\nend\none \u03b8 as a leaf node. In this example, we have \u03b8d1|f2,b1 and \u03b8d1|f2,b2 as leaf nodes, hence conditioning should be performed. Then, a computation step is required by applying in a bottom-up way Min and Max operators on the forgotten \u03a0-DNNF. Therefore, \u03a0(f2|d1) = \u03a0(f2, d1) = 0.4."}, {"heading": "4 NEW POSSIBILISTIC INFERENCE ALGORITHM", "text": "In (Benferhat et al., 2002), authors have been interested in the transition of possibilistic networks into possibilistic logic bases. The starting point is that the possibilistic base associated to a possibilistic network is the result of the fusion of elementary bases. Definition 4 presents the transformation of a min-based possibilistic network into a possibilistic knowledge base.\nDefinition 4. A binary variable Xi of a possibilistic network can be expressed by a local possibilistic knowledge base as follows: \u03a3Xi = {(\u00acxi \u2228 \u00acui, \u03b1i) : \u03b1i = 1\u2212 \u03c0(xi|ui) 6= 0}. The possibilistic knowledge base of the whole network is: \u03a3min = \u03a3X1 \u222a \u03a3X2 \u222a \u00b7 \u00b7 \u00b7 \u222a \u03a3Xn .\nIn another angle, researchers in (Benferhat et al., 2007) have focused on the compilation of bases under the possibilistic logic policy in order to be able to process inference from it in a polynomial time. The combination of these methods allows us to propose a new alternative approach to possibilistic inference. This is justified by the fact that the possibilistic logic reasoning machinery can be applied to directed possibilistic networks (Benferhat et al., 2002).\nThe idea is to encode the possibilistic knowledge base \u03a3min into a classical propositional base (CNF). Let A = {a1, ..., an} with a1 ... an the different weights used in \u03a3min. A set of additional propositional variables, denoted by Ai, which correspond exactly to the number of different weights, are incorporated and for each formula \u03c6i, ai will correspond the propositional formula \u03c6i\u2228Ai. Hence, the propositional\nencoding of \u03a3min, denoted by K\u03a3 is defined by:\nK\u03a3 = {\u03c6i \u2228Ai : (\u03c6i, ai) \u2208 \u03a3min} (11)\nThe following proposition shows that the CNF encoding K\u03a3 recovers the min-joint possibility distribution. Proposition 9. Let \u03c0min be the joint possibility distribution obtained using the chain rule with the minimum-based conditioning and let K\u03a3 be the propositional base associated with the possibilistic network given by equation 11. Let \u03c6i be a propositional formula associated with a degree ai. Then \u2200\u03c9 \u2208 \u2126, \u03a0(\u03c9) = 1 iff {\u00acA1, ...,\u00acAn} \u2227 \u03c9 \u2227 K\u03a3 is consistent. \u03a0(\u03c9) = ai iff {\u00acA1, ...,\u00acAi} \u2227 \u03c9 \u2227K\u03a3 is inconsistent and {\u00acA1, ...,\u00acAi\u22121} \u2227 \u03c9 \u2227K\u03a3 is consistent.\nThe CNF encoding K\u03a3 is then compiled into a target compilation language in order to compute a-posteriori possibility degrees in an efficient way. Here, we are interested in a particular query useful for possibilistic networks, namely what is the possibility degree of an event A = a given an evidence E = e? Therefore, we propose to adapt the algorithm given in (Benferhat et al., 2007) in order to respond to this query as shown by algorithm 4. Proposition 10 shows that the possibility degree computed using algorithm 4 and the one computed using the min-based joint possibility distribution are equal. Note that this approach is qualified to be flexible since it takes advantage of existing propositional knowledge bases compilation methods (Benferhat et al., 2007). This method referred to DNNF-PKB is outlined by algorithm 4.\nAlgorithm 4: Inference using DNNF\nData: \u03a0Gmin , instance of interest x, evidence e Result: \u03a0(x|e) begin\nTransformation into K\u03a3 Transform \u03a0Gmin into \u03a3min using definition 4 Transform \u03a3min into K\u03a3 using equation 11 Inference Kc\u03a3 \u2190 Target(K\u03a3) K \u2190 Kc\u03a3 StopCompute \u2190 false i \u2190 1 \u03a0(x|e) \u2190 1 while (K 2 Ai \u2228 \u00ace) and (i \u2264 k) and (StopCompute=false) do\nK \u2190 condition (K, \u00acAi) if K \u00acx then\nStopCompute\u2190 true \u03a0(x|e) \u2190 1-degree(i)\nelse i\u2190 i+ 1\nreturn \u03a0(x|e) end\nProposition 10. Let \u03a0Gmin be a possibilistic network. Let \u03c0min be a joint distribution obtained by\nchain rule. Then for any a \u2208 Da and e \u2208 DE, we have \u03a0(A = a|E = e) = \u03a0min(A = a|E = e) where \u03a0min(A = a|E = e) is obtained from \u03c0min using equation 1 and \u03a0(A = a|E = e) is obtained from Algo. 4.\nExample 4. To illustrate algorithm 4 we will consider the min-based possibilistic network represented in figure 2."}, {"heading": "The CNF encoding is as follows : K\u03a3 =", "text": "(d2 \u2228 f1 \u2228 b2 \u2228A1) , (b1 \u2228A2) , (d1 \u2228 f2 \u2228 b2 \u2228A2) , (f2 \u2228A3) , (d2 \u2228 f2 \u2228 b1 \u2228A3) , (d2 \u2228 f1 \u2228 b1 \u2228A4) such as A1 (0.8), A2 (0.6), A3(0.3) and A4 (0.2) are propositional variables followed by their weights under brackets. Compiling K\u03a3 into DNNF results in: K c \u03a3 = ((b2 \u2227A2) \u2227 [(A3 \u2227 f1) \u2228 (f2 \u2227 [d2 \u2228 (A4 \u2227 d1)])]) \u2228 (b1 \u2227 [[f2 \u2227 (d2 \u2228 (A1 \u2228 d1))] \u2228 [(f1 \u2227A3) \u2227 (d1 \u2228 (A2 \u2227 d2))]]). The computation of \u03a0(f2|d1) using Kc\u03a3 requires two iterations. Therefore, \u03a0(f2|d1) = 1\u2212 degree(2) = 0.4.\nDue to the compilation step, this algorithm runs in polynomial time. Moreover, the number of additional variables is low since it corresponds exactly to the number of priority levels existing in the base."}, {"heading": "5 COMPARATIVE AND EXPERIMENTAL STUDIES", "text": "The paper analyzes three compilation-based methods, namely DNNF-PKB, \u03a0-DNNF and \u03a0-DNNFPF . The first dimension that differentiates the three approaches proposed in this paper is the CNF encoding. It consists of specifying the number of variables and clauses per approach.\nThe CNF of DNNF-PKB is based on encoding \u00acx where x is an instance of interest having a possibility degree different from 1. In \u03a0-DNNF, we write implications relative to instances having 1 as possibility degree. We can notice that the local structure in both methods is exploited in semantically different ways. In DNNF-PKB, the encoding uses the number of different weights as the number of additional variables while the \u03a0-DNNF encoding uses the number of the non-redundant possibility degrees different from 1 in the distributions. Regarding the number of clauses, both methods handle possibility degrees different from 1. This leads us to the following proposition: Proposition 11. The CNF encodings of DNNF-PKB and \u03a0-DNNF have the same number of variables and clauses.\nThe CNF encoding of \u03a0-DNNFPF is different from the ones of DNNF-PKB and \u03a0-DNNF. Proposition 12 shows the difference between \u03a0-DNNFPF and DNNFPKB in terms of number of variables and clauses. Proposition 12. The number of variables and clauses in \u03a0-DNNFPF is more important than those in DNNF-PKB.\nIndeed, in \u03a0-DNNFPF , we associate propositional variables not only to possibility degrees (parameters), but also to each value xi of Xi. While in DNNF-PKB only m new variables are added (one variable per different degree).\nLet us now analyze these three approaches from experimental points of view. Our experimentation is performed on random possibilistic networks. More precisely, we have compared DNNF-PKB and \u03a0DNNFPF on 100 possibilistic networks having from 10 to 50 nodes. As mentioned that the approaches focus mainly on encoding the possibilistic network as a CNF then compile it into the appropriate language, hence, it should be interesting to compare the CNF parameters (the number of variables and clauses) and the DNNF parameters (the number of nodes and edges) for the two methods."}, {"heading": "5.1 CNF PARAMETERS", "text": "First we propose to test the CNF encodings characterized by the number of variables and the number of clauses. Regarding DNNF-PKB, the number of additional variables correspond to the number of weights which are different. While in \u03a0-DNNFPF , variables are both those associated to the possibility degrees of each distribution and those to variable\u2019s instances. The number of clauses for each method is related to the CNF encoding itself. Figure 4 shows the results of this experimentation. Each approach is characterized by a curve for the average number of variables and a curve for the average number of clauses. It is clear that the higher the number of nodes considered in the possibilistic network, the higher the number of variables and clauses. Figure 4 shows that DNNF-PKB has the lower number of variables and clauses comparing to \u03a0-DNNFPF , which confirms the theoretical results detailed above."}, {"heading": "5.2 DNNF PARAMETERS", "text": "Once we obtain the CNF encodings, it is important to compare the number of nodes and edges for each compiled base. Figure 5 represents the average size of the compiled bases for the two methods in terms of nodes and edges numbers. We remark that the number of nodes and edges depends deeply on CNF parameters. More precisely, the number of nodes and edges in DNNF-PKB is considered narrow comparing to \u03a0-DNNFPF . This can be explained by the lower number of variables and clauses on CNFs and the local structure which shrinks the sizes of compiled bases. Comparing DNNF-PKB to \u03a0-DNNFPF , the behavior of DNNF-PKB is important."}, {"heading": "6 CONCLUSION", "text": "This paper proposes algorithms that ensure inference in possibilistic networks using compilation techniques. First, we have proposed possibilistic adaptations of two compilation-based probabilistic methods, namely \u03a0-DNNFPF and \u03a0-DNNF. Then we have developed a new possibilistic inference method DNNF-PKB based on a transformation phase from a possibilistic network into a compiled possibilistic knowledge base. We theoretically show that DNNF-PKB and \u03a0-DNNF share the same number of variables and clauses even they are based on different computations in their inference process since the first is based on necessity degrees and the second on possibility degrees. We have also shown that DNNF-PKB is more compact than \u03a0-DNNFPF which proves the importance of the possibilistic setting versus the probabilistic setting. All these results were confirmed by experimental results. A future work will be to compare these algorithms with the well-known junction tree propagation algorithm. Another future work is to exploit results of this paper in order to infer efficiently interventions in possibilistic causal networks\n(Pearl, 2000) (Benferhat and Smaoui, 2007)."}, {"heading": "Acknowledgements", "text": "We thank the anonymous reviewers for many interesting comments and suggestions. Also, we wish to thank Mark Chavira for our valuable discussions on this subject. The third author would like to thank the project ANR Placid."}, {"heading": "Benferhat, S., Dubois, D., Garcia, L., and Prade, H.", "text": "(2002). On the transformation between possibilistic logic bases and possibilistic causal networks. International Journal of Approximate Reasoning, 29(2):135\u2013 173."}, {"heading": "Benferhat, S. and Smaoui, S. (2007). Possibilistic causal", "text": "networks for handling interventions: A new propagation algorithm. In AAAI, pages 373\u2013378.\nBenferhat, S., Yahi, S., and Drias, H. (2007). On the compilation of stratified belief bases under linear and possibilistic logic policies. In International Journal of Approximate Reasoning, pages 2425\u20132430."}, {"heading": "Chavira, M. and Darwiche, A. (2005). Compiling bayesian", "text": "networks with local structure. In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI), pages 1306\u20131312."}, {"heading": "Cooper, G. F. (1990). The computational complexity of", "text": "probabilistic inference using bayesian belief networks (research note). Artif. Intell., 42(2-3):393\u2013405.\nDarwiche, A. (2001). Decomposable negation normal form. Journal of the ACM, 48(4):608\u2013647."}, {"heading": "Darwiche, A. (2003). A differential approach to inference", "text": "in bayesian networks. Journal of the ACM, 50(3):280\u2013 305.\nDarwiche, A. and Marquis, P. (2002). A knowledge compilation map. Journal of Artificial Intelligence Research, 17:229\u2013264.\nDubois, D., Lang, J., and Prade, H. (1994). Possibilistic logic. In Handbook on Logic in Artificial Intelligence and Logic Programming, volume 3, pages 439\u2013 513. Oxford University press.\nDubois, D. and Prade, H. (1988). Possibility theory:An approach to computerized, Processing of uncertainty. Plenium Press, New York.\nHisdal, E. (1978). Conditional possibilities independence and non interaction. Fuzzy Sets and Systems, 1.\nPearl, J. (2000). Causality: Models, reasonning and inference. Cambridge University Press."}, {"heading": "Wachter, M. and Haenni, R. (2007). Logical compilation", "text": "of bayesian networks with discrete variables. In European Conf. on Symbolic and Quantitative Approaches to Reasoning with Uncertainty, pages 536\u2013547."}], "references": [{"title": "On the transformation between possibilistic logic bases and possibilistic causal networks", "author": ["S. Benferhat", "D. Dubois", "L. Garcia", "H. Prade"], "venue": "International Journal of Approximate Reasoning, 29(2):135\u2013 173.", "citeRegEx": "Benferhat et al\\.,? 2002", "shortCiteRegEx": "Benferhat et al\\.", "year": 2002}, {"title": "Possibilistic causal networks for handling interventions: A new propagation algorithm", "author": ["S. Benferhat", "S. Smaoui"], "venue": "AAAI, pages 373\u2013378.", "citeRegEx": "Benferhat and Smaoui,? 2007", "shortCiteRegEx": "Benferhat and Smaoui", "year": 2007}, {"title": "On the compilation of stratified belief bases under linear and possibilistic logic policies", "author": ["S. Benferhat", "S. Yahi", "H. Drias"], "venue": "International Journal of Approximate Reasoning, pages 2425\u20132430.", "citeRegEx": "Benferhat et al\\.,? 2007", "shortCiteRegEx": "Benferhat et al\\.", "year": 2007}, {"title": "Compiling bayesian networks with local structure", "author": ["M. Chavira", "A. Darwiche"], "venue": "Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI), pages 1306\u20131312.", "citeRegEx": "Chavira and Darwiche,? 2005", "shortCiteRegEx": "Chavira and Darwiche", "year": 2005}, {"title": "The computational complexity of probabilistic inference using bayesian belief networks (research note)", "author": ["G.F. Cooper"], "venue": "Artif. Intell., 42(2-3):393\u2013405.", "citeRegEx": "Cooper,? 1990", "shortCiteRegEx": "Cooper", "year": 1990}, {"title": "Decomposable negation normal form", "author": ["A. Darwiche"], "venue": "Journal of the ACM, 48(4):608\u2013647.", "citeRegEx": "Darwiche,? 2001", "shortCiteRegEx": "Darwiche", "year": 2001}, {"title": "A differential approach to inference in bayesian networks", "author": ["A. Darwiche"], "venue": "Journal of the ACM, 50(3):280\u2013 305.", "citeRegEx": "Darwiche,? 2003", "shortCiteRegEx": "Darwiche", "year": 2003}, {"title": "A knowledge compilation map", "author": ["A. Darwiche", "P. Marquis"], "venue": "Journal of Artificial Intelligence Research, 17:229\u2013264.", "citeRegEx": "Darwiche and Marquis,? 2002", "shortCiteRegEx": "Darwiche and Marquis", "year": 2002}, {"title": "Possibilistic logic", "author": ["D. Dubois", "J. Lang", "H. Prade"], "venue": "Handbook on Logic in Artificial Intelligence and Logic Programming, volume 3, pages 439\u2013 513. Oxford University press.", "citeRegEx": "Dubois et al\\.,? 1994", "shortCiteRegEx": "Dubois et al\\.", "year": 1994}, {"title": "Possibility theory:An approach to computerized, Processing of uncertainty", "author": ["D. Dubois", "H. Prade"], "venue": "Plenium Press, New York.", "citeRegEx": "Dubois and Prade,? 1988", "shortCiteRegEx": "Dubois and Prade", "year": 1988}, {"title": "Conditional possibilities independence and non interaction", "author": ["E. Hisdal"], "venue": "Fuzzy Sets and Systems, 1.", "citeRegEx": "Hisdal,? 1978", "shortCiteRegEx": "Hisdal", "year": 1978}, {"title": "Causality: Models, reasonning and inference", "author": ["J. Pearl"], "venue": "Cambridge University Press.", "citeRegEx": "Pearl,? 2000", "shortCiteRegEx": "Pearl", "year": 2000}, {"title": "Logical compilation of bayesian networks with discrete variables", "author": ["M. Wachter", "R. Haenni"], "venue": "European Conf. on Symbolic and Quantitative Approaches to Reasoning with Uncertainty, pages 536\u2013547.", "citeRegEx": "Wachter and Haenni,? 2007", "shortCiteRegEx": "Wachter and Haenni", "year": 2007}], "referenceMentions": [{"referenceID": 9, "context": "This is due to the existence of two definitions of possibilistic conditioning: product-based and min-based conditioning (Dubois and Prade, 1988).", "startOffset": 120, "endOffset": 144}, {"referenceID": 4, "context": "In this paper, we are interested in the inference problem in multiply connected networks, which is known as a hard problem (Cooper, 1990).", "startOffset": 123, "endOffset": 137}, {"referenceID": 6, "context": "Recently, researchers have been interested in various kinds of exact and approximate Bayesian networks inference algorithms using compilation techniques (Darwiche, 2003) (Chavira and Darwiche, 2005) (Wachter and Haenni, 2007), etc.", "startOffset": 153, "endOffset": 169}, {"referenceID": 3, "context": "Recently, researchers have been interested in various kinds of exact and approximate Bayesian networks inference algorithms using compilation techniques (Darwiche, 2003) (Chavira and Darwiche, 2005) (Wachter and Haenni, 2007), etc.", "startOffset": 170, "endOffset": 198}, {"referenceID": 12, "context": "Recently, researchers have been interested in various kinds of exact and approximate Bayesian networks inference algorithms using compilation techniques (Darwiche, 2003) (Chavira and Darwiche, 2005) (Wachter and Haenni, 2007), etc.", "startOffset": 199, "endOffset": 225}, {"referenceID": 6, "context": "This paper analyzes this issue by first adapting well-known compilation-based probabilistic inference approaches, namely the arithmetic circuit method (Darwiche, 2003) and the logical compilation of Bayesian Networks (Wachter and Haenni, 2007).", "startOffset": 151, "endOffset": 167}, {"referenceID": 12, "context": "This paper analyzes this issue by first adapting well-known compilation-based probabilistic inference approaches, namely the arithmetic circuit method (Darwiche, 2003) and the logical compilation of Bayesian Networks (Wachter and Haenni, 2007).", "startOffset": 217, "endOffset": 243}, {"referenceID": 0, "context": "The third method exploits results obtained on one hand in (Benferhat et al., 2002) that transforms a minbased possibilistic network into a possibilistic knowledge base, and on the other hand results obtained regarding compilation of possibilistic bases (Benferhat et al.", "startOffset": 58, "endOffset": 82}, {"referenceID": 2, "context": ", 2002) that transforms a minbased possibilistic network into a possibilistic knowledge base, and on the other hand results obtained regarding compilation of possibilistic bases (Benferhat et al., 2007) in order to assure inference in polytime.", "startOffset": 178, "endOffset": 202}, {"referenceID": 9, "context": "This subsection briefly recalls some elements of possibility theory, for more details we refer to (Dubois and Prade, 1988).", "startOffset": 98, "endOffset": 122}, {"referenceID": 10, "context": "The qualitative interpretation of the scale [0, 1] leads to the well known definition of min-conditioning (Hisdal, 1978), (Dubois and Prade, 1988):", "startOffset": 106, "endOffset": 120}, {"referenceID": 9, "context": "The qualitative interpretation of the scale [0, 1] leads to the well known definition of min-conditioning (Hisdal, 1978), (Dubois and Prade, 1988):", "startOffset": 122, "endOffset": 146}, {"referenceID": 8, "context": "Possibilistic logic (Dubois et al., 1994) handles qualitative uncertainty in a logical setting.", "startOffset": 20, "endOffset": 41}, {"referenceID": 7, "context": "Compilation languages are compared in terms of their spatial efficiency via the succinctness criteria and also in terms of the set of logical queries and transformations they support in polynomial time (see (Darwiche and Marquis, 2002) for more details).", "startOffset": 207, "endOffset": 235}, {"referenceID": 5, "context": "Within the most effective target compilation languages, we cite the Decomposable Negation Normal Form (DNNF) (Darwiche, 2001).", "startOffset": 109, "endOffset": 125}, {"referenceID": 5, "context": "Within these subsets, the language DNNF (Darwiche, 2001) is one of the most effective target compilation languages that supports the decomposability.", "startOffset": 40, "endOffset": 56}, {"referenceID": 7, "context": "In what follows we are in particular interested by conditioning and forgetting transformations (Darwiche and Marquis, 2002).", "startOffset": 95, "endOffset": 123}, {"referenceID": 6, "context": "In this section, we first propose an adaptation of the arithmetic circuit method of (Darwiche, 2003).", "startOffset": 84, "endOffset": 100}, {"referenceID": 12, "context": "Then we will study one of its variants proposed in (Wachter and Haenni, 2007), namely the logical compilation of Bayesian Networks.", "startOffset": 51, "endOffset": 77}, {"referenceID": 5, "context": "d-DNNF, sd-DNNF) (Darwiche, 2001) in the possibilistic setting (definition 1).", "startOffset": 17, "endOffset": 33}, {"referenceID": 6, "context": "In (Darwiche, 2003), authors have focused on inference in compiled Bayesian networks.", "startOffset": 3, "endOffset": 19}, {"referenceID": 6, "context": "As in the probabilistic case (Darwiche, 2003), this circuit can be used for linear-time inference.", "startOffset": 29, "endOffset": 45}, {"referenceID": 12, "context": "More precisely in (Wachter and Haenni, 2007), authors have been interested in performing a CNF logical encoding of the probability distribution induced by a bayesian network, then a compilation phase from CNF to d-DNNF.", "startOffset": 18, "endOffset": 44}, {"referenceID": 12, "context": "The possibilistic encoding of a possibilistic network given by K\u03c8 (equation 10) is more compact than the probabilistic encoding given in (Wachter and Haenni, 2007).", "startOffset": 137, "endOffset": 163}, {"referenceID": 12, "context": "In fact, the number of variables used in K\u03c8 is less than the one used in (Wachter and Haenni, 2007).", "startOffset": 73, "endOffset": 99}, {"referenceID": 0, "context": "In (Benferhat et al., 2002), authors have been interested in the transition of possibilistic networks into possibilistic logic bases.", "startOffset": 3, "endOffset": 27}, {"referenceID": 2, "context": "In another angle, researchers in (Benferhat et al., 2007) have focused on the compilation of bases under the possibilistic logic policy in order to be able to process inference from it in a polynomial time.", "startOffset": 33, "endOffset": 57}, {"referenceID": 0, "context": "This is justified by the fact that the possibilistic logic reasoning machinery can be applied to directed possibilistic networks (Benferhat et al., 2002).", "startOffset": 129, "endOffset": 153}, {"referenceID": 2, "context": "Here, we are interested in a particular query useful for possibilistic networks, namely what is the possibility degree of an event A = a given an evidence E = e? Therefore, we propose to adapt the algorithm given in (Benferhat et al., 2007) in order to respond to this query as shown by algorithm 4.", "startOffset": 216, "endOffset": 240}, {"referenceID": 2, "context": "Note that this approach is qualified to be flexible since it takes advantage of existing propositional knowledge bases compilation methods (Benferhat et al., 2007).", "startOffset": 139, "endOffset": 163}, {"referenceID": 11, "context": "Another future work is to exploit results of this paper in order to infer efficiently interventions in possibilistic causal networks (Pearl, 2000) (Benferhat and Smaoui, 2007).", "startOffset": 133, "endOffset": 146}, {"referenceID": 1, "context": "Another future work is to exploit results of this paper in order to infer efficiently interventions in possibilistic causal networks (Pearl, 2000) (Benferhat and Smaoui, 2007).", "startOffset": 147, "endOffset": 175}], "year": 2010, "abstractText": "Qualitative possibilistic networks, also known as min-based possibilistic networks, are important tools for handling uncertain information in the possibility theory framework. Despite their importance, only the junction tree adaptation has been proposed for exact reasoning with such networks. This paper explores alternative algorithms using compilation techniques. We first propose possibilistic adaptations of standard compilation-based probabilistic methods. Then, we develop a new, purely possibilistic, method based on the transformation of the initial network into a possibilistic base. A comparative study shows that this latter performs better than the possibilistic adaptations of probabilistic methods. This result is also confirmed by experimental results.", "creator": "TeX"}}}