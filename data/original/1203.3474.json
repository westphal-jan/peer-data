{"id": "1203.3474", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Distribution over Beliefs for Memory Bounded Dec-POMDP Planning", "abstract": "We propose a new point-based method for approximate planning in Dec-POMDP which outperforms the state-of-the-art approaches in terms of solution quality. It uses a heuristic estimation of the prior probability of beliefs to choose a bounded number of policy trees: this choice is formulated as a combinatorial optimisation problem minimising the error induced by pruning.", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (411kb)", "http://arxiv.org/abs/1203.3474v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gabriel corona", "francois charpillet"], "accepted": false, "id": "1203.3474"}, "pdf": {"name": "1203.3474.pdf", "metadata": {"source": "CRF", "title": "Distribution over Beliefs for Memory Bounded Dec-POMDP Planning", "authors": ["Gabriel Corona", "Fran\u00e7ois Charpillet"], "emails": [], "sections": [{"heading": null, "text": "We propose a new point-based method for approximate planning in Dec-POMDP which outperforms the state-of-the-art approaches in terms of solution quality. It uses a heuristic estimation of the prior probability of beliefs to choose a bounded number of policy trees: this choice is formulated as a combinatorial optimisation problem minimising the error induced by pruning."}, {"heading": "1 Introduction", "text": "The problem of the control of a team of cooperative agents in interaction with a stochastic environment has a large field of applications: packet routing in a network, robot coordination, control of sensor networks, zone surveillance etc. The Dec-POMDP (Decentralised Partially Observable Markov Decision Process) formal framework may be used to tackle this problem rigorously. We focus here on the particular case of planning: given the rules of the environment and a goal modelled as a reward function, we are searching for a policy to assign to each agent in order to maximise the rewards. The computation of an optimal joint policy is NEXP-complete in a finite horizon [Bernstein et al., 2000] when the horizon is smaller than the number of states. The exact computation is thus not usable except for very small problems (especially, with a small number of observations) and for small horizons.\nSeveral approaches have been proposed to solve Dec-POMDPs: bottom-up dynamic programming [Hansen et al., 2004], [Szer and Charpillet, 2006], topdown search [Szer et al., 2005] and mathematical programming [Aras et al., 2007]. The number of partial policies to consider in an exact dynamic programming approach is generally exponential in the planning horizon and the number of agents and doubly exponential\nin the number of observations [Hansen et al., 2004]. Some algorithms have been proposed to overcome this problem: MBDP (Memory Bounded Dynamic Programming) [Seuken and Zilberstein, 2007b], IMBDP (Improved MBDP) [Seuken and Zilberstein, 2007a], MBDP-OC (MBDP with Observation Compression) [Carlin and Zilberstein, 2008], PBIP (Point Based Incremental Pruning) [Dibangoye et al., 2009]. They use only a bounded number of partial plans at each planning step. By bounding the memory, they are able to solve problems with significantly longer horizons than previous algorithms: the complexity is linear with the horizon. By avoiding the exhaustive backup, some of them can solve problems with a higher number of observations.\nIn this article, we propose a new point based approach, PSMBDP (Policy Search Memory Bounded Dynamic Programming), which uses a heuristic estimation of the prior probability distribution of reachable beliefs and formulates the choice of a bounded number of partial plans as a combinatorial optimisation problem. Using this heuristics information, our algorithm finds solutions of better quality than state-of-the-art approaches such as MBDP and PBIP."}, {"heading": "2 Models", "text": ""}, {"heading": "2.1 MDP", "text": "The MDP framework (Markov Decision Process) models the decision problem of a single agent which seeks to maximise rewards by interacting with a fully observable environment. The time is discrete and at each time step t, the agent knows exactly the state St of the environment and chooses an action At+1. A MDP is a tupleM = \u3008S,A, T ,R\u3009 where: S et A are respectively the sets of states s of the environment and of actions a of the agent; the transition function T represents the dynamic of the system by defining the transition probabilities, T (s\u2032|s, a) = Pr(St+1 = s\u2032|St = s,At+1 = a), from state s to state s\u2032 when performing action a; the\nreward function R represents the goal of the agent by defining the expectation R(s, a) = E[Rt+1|St = s,At+1 = a] of the reward Rt+1 obtained after doing action a in state s. In finite horizon H, the goal of the agent is to maximise the expected sum of the rewards, E[ \u2211H t=1R t]."}, {"heading": "2.2 POMDP", "text": "The POMDP framework (Partially Observable MDP) generalises the MDP framework for partially observable environments: instead of observing the state St, the agent receives an observations Ot which can be used to infer St. At each time t, the agent chooses an action At given the past observations O1 . . . Ot\u22121 and actions A1 . . . At\u22121. A POMDP is a tuple M = \u3008S,A, T ,R,\u2126,O\u3009 where: \u2126 is the set of observations o that the agent can receive; the observation function O defines the observation probabilities O(o|s, a, s\u2032) = Pr(Ot+1 = o|St = s,At+1 = a, St+1 = s\u2032) of observing o in the transition from s to s\u2032 with action a.\nA probability distribution Bt \u2208 \u2206S represents the belief of the agent, i.e. the Bayesian knowledge it has inferred on the current state St given the past observations and actions:\nBt(s) = Pr(St = s|B0, A1, O1 . . . At, Ot) = Pr(St = s|Bt\u22121, At, Ot)\nWe note \u03c4 the belief update function : Bt = \u03c4(Bt\u22121, At, Ot). The belief is a sufficient statistic for the agent so a POMDP policy is usually a mapping from belief to action.\nFor convenience, we use the notation:\nO(o|b, a) = Pr(Ot+1 = o|Bt = b, At = a) = \u2211\n(s,s\u2032)\u2208S2 T (s\u2032|s, a)O(o|s, a, s\u2032)b(s)"}, {"heading": "2.3 Dec-POMDP", "text": "The Dec-POMDP framework generalises the POMDP framework for a team of agents. A Dec-POMDP is a tupleM = \u3008I,S, (Ai)i\u2208I , T ,R, (\u2126i)i\u2208I ,O\u3009 where I is the finite set of agents i. Each agent has its own sets Ai and \u2126i of local actions ai and observations oi: a joint action a = (a1, . . . an) \u2208 A is a tuple of local actions where A = \u220f i\u2208I Ai is the set of joint actions; a joint observation o = (o1, . . . on) \u2208 \u2126 is a tuple of local observations where \u2126 = \u220f i\u2208I \u2126i is the set of joint observations. At each time t, each agent i chooses a local action Ati given the past local observations O1i . . . O t\u22121 i and actions A1i . . . A t\u22121 i . The initial belief B\n0 is a prior on the initial state S0 shared by the agents. The joint\npolicy \u03c0 is searched as a tuple of local policies \u03c0i, one for each agent i.\nThe underlying POMDP of a Dec-POMDP M is the POMDP obtained by replacing the |I| agents by a single agent receiving the joint observation and choosing the joint action. The underlying MDP is defined as the MDP obtained by observing the state directly."}, {"heading": "2.4 Policy Trees", "text": "In a finite horizon problem, local deterministic policies of a Dec-POMDP can be modelled as policy trees (figure 1) whose nodes are actions and whose branches are observations. We note aqi the root action of policy tree qi and qi(oi) its subtree for local observation oi. At each time step, the internal state of the agent Qti is defined by a local policy tree qi.\nJoint policies trees are tuples of local policy trees (one for each agent), q = (q1, . . . , qn) \u2208 Q = \u220f iQi. We note aq = (aq1 , . . . , aqn) the root joint action of q made of the local root actions aqi and q(o) = (q1(o1), . . . , qn(on)) the joint sub-policy tree made of subtrees qi(oi) of the local policy trees qi for the local observations oi. A joint policy tree q can be evaluated using its value function Vq defined for every distribution b on the states:\nVq(b) = \u2211 s\u2208S b(s)Vq(s)\nVq(s) = R(s, aq) + \u2211\n(s\u2032,o)\u2208S\u00d7\u2126\nT (s\u2032|s, aq)O(o|s, aq, s\u2032)Vq(o)(s\u2032)\nThe value function can be represented by its \u03b1vector, a |S|-vector containing the Vq(s): \u03b1q = (Vq(s1), . . . , Vq(s|S|)) T and Vq(b) = b \u00b7 \u03b1q."}, {"heading": "3 State of the Art", "text": ""}, {"heading": "3.1 Dynamic Programming", "text": "Dynamic Programming (DP) Dec-POMDP planning builds policy trees in a bottom-up way\n[Hansen et al., 2004] (see algorithm 1). The DP operator (line 2) builds the sets Qti of policy trees of horizon H \u2212 t (executed from time t to H) from the sets Qt+1i of policy trees of horizon H\u2212 t\u22121 (executed from time t+ 1 to H). This operator is usually made of two phases: backup and pruning (algorithm 2).\nAlgorithm 1: Dynamic Programming planning Result: \u2200i,Q0i and their \u03b1-vectors foreach i \u2208 I do QHi \u2190 {\u03c8} (empty tree)1 for t=H-1 to 0 do (Qt)i\u2208I \u2190 DP((Qt+1i )i\u2208I)2\nAlgorithm 2: General Two Phases DP operator (Q\u0304ti)i\u2208I \u2190 Backup((Q t+1 i )i\u2208I)1 (Qti)i\u2208I \u2190 Prune((Q\u0304ti)i\u2208I)2\nBackup The exhaustive backup step builds the sets Q\u0304ti of local policy trees of horizon H \u2212 t by building every tree whose root is an action of Ai and whose branches associate a local observation to a tree of Qt+1i from the previous iteration: Q\u0304ti = Ai\u00d7(Q t+1 i )\n\u2126i . However the number of policy trees of agent i is exponential with the number of local observations \u2126i and doubly exponential with the horizon.\nPruning In order to limit the growth of the number of policy trees, a pruning phase (algorithm 3) is used which consists of eliminating trees of Q\u0304ti to produce the sets Qti. In exact pruning, the dominated policy trees are pruned. This is checked by solving a linear program. In general, the number of policy trees is still in the same order of magnitude.\nAlgorithm 3: Exact Prune \u2200i \u2208 I,Qti \u2190 Q\u0304ti1 flag\u2190 true2 while flag do3 flag\u2190 false4 foreach i \u2208 I do5 foreach qi \u2208 Qti do6 if qi dominated then7 Qti \u2190 Qti \\ {qi}8 flag\u2190 true9"}, {"heading": "3.2 MBDP", "text": "Memory Bounded Dynamic Programming (MBDP) [Seuken and Zilberstein, 2007b] has been proposed to solve Dec-POMDPs with a high horizon (algorithm 4).\nIt replaces exact pruning by a memory bounded pruning: for each agent i, at most W policy trees from Q\u0304ti are kept in the set Qti. W points (probability distribution on the states) are generated using some heuristic policies. For each point b, the best joint policy, q = (q1, . . . , qn), is selected (line 4) and the corresponding local trees qi are moved from Q\u0304ti to Qti (lines 5 and 6).\nAlgorithm 4: DP Operator for MBDP Data: Bt, W points Data: \u2200i,Qt+1i and their \u03b1-vectors Result: \u2200i,Qti and their \u03b1-vectors with Qti| \u2264W foreach i \u2208 I do Q\u0304ti \u2190 Ai \u00d7 (Q t+1 i ) \u2126i1 foreach i \u2208 I do Qti \u2190 \u22052 foreach b \u2208 Bt do3 (qi)i\u2208I \u2190 arg maxq\u2208\u220fi Q\u0304ti Vq(b)4 foreach i \u2208 I do Qti \u2190 Qti \u222a {qi}5 foreach i \u2208 I do Q\u0304ti \u2190 Q\u0304ti \\ {qi}6\nPartial backup The complexity of the exhaustive backup is exponential with the number of observations. Improved MBDP [Seuken and Zilberstein, 2007a] and MBDP with Observation Compression [Carlin and Zilberstein, 2008], replace the exhaustive backup step by a partial backup in order to scale with the number of observations."}, {"heading": "3.3 PBIP", "text": "Point Based Incremental Pruning (PBIP) [Dibangoye et al., 2009] uses another approach to solve the scalability problem of exhaustive backup. It avoids the exhaustive generation and enumeration of joint policies by using one phase of branch-and-bound search instead of the two phases of backup and pruning (algorithm 5). For each point b, a depth-first branch-and-bound search of the optimal joint tree is done (line 5).\nBranch A search tree node q\u0303 represents a set of joint policy trees q defined by a set of constraints. It can be seen as a partially defined policy tree. Each child node of the search tree is a subset of its parent defined by an additional constraint on q, either by fixing the joint root action aq \u2208 A or by fixing a joint subtree q(o) \u2208 Qt+1 for a given joint observation o (see figure 2). The policy tree must not already have been selected: this constraint (q /\u2208 Sel) is checked on the leaf nodes.\nDecentralisability A joint policy tree q is decentralisable i.e. it can be divided in local policy trees qi such that:\n\u2200o = (o1, . . . , on) \u2208 \u2126, q(o) = (q1(o1), . . . , qn(on)) (1)\nThis constraint restricts the valid assignments of subtrees q(o).\nBound The heuristic value f(q\u0303) of a search tree node q\u0303 is a upper bound of the value Vq(b) of the best policy tree q \u2208 q\u0303 matching the constraints. The set \u2126 of joint observations is partitioned in two subsets, \u2126 = \u21261\u222a\u21262:\n\u2022 \u21261 is the set of joint observations o whose joint subtree q(o) is already fixed, either directly by a q(o) = q\u2032 constraint or indirectly through the decentralisability constraint;\n\u2022 \u21262 is the set of joint observations o whose joint subtree q(o) is not completely fixed yet; they may be partially constrained by the decentralisability constraint.\nThe problem is relaxed by ignoring the decentralisability constraints for the subtrees q(o) of observation o \u2208 \u21262. The contribution of the best joint subtree is used:\nf(q\u0303) = R(b, aq) + \u2211 o\u2208\u21261 O(o|b, aq) [ \u03b1q(o) \u00b7 \u03c4(b, aq, o) ] + \u2211 o\u2208\u21262 O(o|b, aq) max q\u2032\u2208Qt+1 [\u03b1q\u2032 \u00b7 \u03c4(b, aq, o)]\nThe theoretical complexity is still exponential with maxi |\u2126i| but in practice a large part of the search tree is pruned. In the benchmarks, PBIP handles problems with a higher number of observations than MBDP and finds better solutions than the approaches using partial backup for a lower computation time."}, {"heading": "4 Proposition", "text": ""}, {"heading": "4.1 Intuitive Idea", "text": "For simplicity of the presentation, our approach is first described in the single agent case. A first approach for\nAlgorithm 5: DP Operator for PBIP Data: Bt, W points Data: \u2200i,Qt+1i and their \u03b1-vectors Result: \u2200i,Qti and their \u03b1-vectors, with |Qti| \u2264W Sel\u2190 \u22051 foreach i \u2208 I do Qti \u2190 \u22052 foreach b \u2208 Bt do3\nLet the search space Qt+1 = \u220f iAi \u00d7 (Q t+1 i ) \u2126i4 Search (branch and bound) q = (qi)i\u2208I as5 arg maxq\u2208Qt+1\\Sel Vq(b) Sel\u2190 Sel \u222a {q}6 foreach i \u2208 I do \u2200i \u2208 I,Qti \u2190 Qti \u222a {qi}7\nthe selection of trees consists of maximising the mean of V t(b) (figure 3):\nmaximise \u222b V t(b) db\nThe choice of the set Qt ofW trees among the |A|W |\u2126| (generated by exhaustive backup) becomes a combinatorial optimisation problem of variable Qt:\nmax \u222b V t(b) db with V t(b) = max\nq\u2208Qt Vq(b)\ns.t. Qt \u2286 Q\u0304t = A\u00d7 (Qt+1)\u2126\n|Qt| \u2264W\nb0 1 b(s1)\n1 0b(s2)\nV\nRetained \u03b1 vectors\nIgnored \u03b1 vectors\nMean regret (minimised)\nMean value (maximised)\nFigure 3: Intuitive Selection Criterion\nHeuristic distribution Only a subset of the beliefs may be reachable and this information can be used for planning [Szer and Charpillet, 2006]. More generally some beliefs may more probable than others. We introduce a heuristic probability distribution1 \u00b5t of the belief Bt at time step t in the criterion:\nmaximise \u222b V t(b)\u00b5t(b) db = E\nb\u223c\u00b5t\n[ V t(b) ] 1Bt \u2208 \u2206S is a posterior probability distribution on states s \u2208 S so \u00b5t \u2208 \u2206\u2206S is a meta-distribution on states.\nThe prior probability distribution of Bt cannot be determined before planning because it depends on the policy. In practice, a heuristic policy \u03c0\u0303 is used to generate the heuristic distributions \u00b5t. \u00b5t is the prior probability distribution2 of Bt for the given heuristic policy \u03c0\u0303: \u00b5t(b) = Pr(Bt = b|B0, \u03c0\u0303).\nMonte-Carlo Sampling The \u00b5t distribution is a finite distribution (in finite horizon, for a finite number of observations and for a given initial belief B0 = b0) and the integration is a finite sum:\nE b\u223c\u00b5t\n[ V t(b) ] = \u2211 b Pr(Bt = b|B0, \u03c0)V t(b)\nHowever the size of the set of reachable beliefs is generally exponential with the horizon which hinders exact evaluation. Monte-Carlo sampling on the heuristic probability distribution of Bt can be used to approximate this criterion,\nE b\u223c\u00b5t\n[ V t(b) ] \u2248 1 N N\u2211 k=1 V t(btk) (2)\nwhere the btk are independent samples (the points of the method) of \u00b5t and N is the number of samples. Each sample is generated by simulating the heuristic policy until time t."}, {"heading": "4.2 Criterion", "text": "A Dec-POMDP can be seen as a POMDP with a constraint on the form of its policy: the policy must be decentralisable. This constraint is added to criterion (2). Given the points Bt, we are searching, for each agent i, for a Qti set of at most W policy trees among the |Ai|W |\u2126i| generated by exhaustive backup:\nmaximise \u2211 b\u2208Bt max q\u2208 \u220f i\u2208I Qti Vq(b) (3)\ns.t. \u2200i \u2208 I,Qti \u2286 Q\u0304ti = Ai \u00d7 (Qt+1i ) \u2126i\n\u2200i \u2208 I, |Qti| \u2264W\nThe samples are taken from the prior distribution of the POMDP beliefs using some heuristic policies. In order to better take into account the decentralised nature of the problem, we might want to work with DecPOMDP beliefs: it is however not clear how those beliefs should be handled as each agent has its own beliefs; attempts have been made to use them but they are not presented in this paper as they do not provide significant improvements over the first approach\n2The Bt random variable is a function of the history of actions an observations, (A1,\u21261 . . .At,\u2126t). We are interested in the problem of prediction of Bt from time 0.\non the benchmarks. The heuristics from MBDP can be used as well.\nOne key difference with MBDP based approaches is that in those approaches each joint policy tree is selected based on only one point b. In PSMBDP, the policy trees are selected as a whole based on the distribution \u00b5t. This distribution is approximated by MonteCarlo sampling so the number N of points used can be arbitrarily higher than the W parameter."}, {"heading": "4.3 Algorithm", "text": "PSMBDP builds a policy with a memory bounded DP operator: the DP operator uses an exhaustive backup phase and a pruning phase solving (3). Algorithm 6 is a general PSMBDP algorithm using a two step DP operator. Contrary to the approaches based on MBDP, the number N of samples can be higher than the number W of trees which allows to better cover the belief space. In practice, a heuristic solution of (3) is searched.\nIn the current implementation, criterion (3) is solved using a heuristic greedy approach (algorithm 7). The joint policy tree q maximising this criterion is first selected (line 2): this can be done using a PBIP search for the mean belief, b\u0304 = 1|Bt| \u2211 n\u2208Bt b. Its local trees are placed into the sets Qti (line 3). Then, the algorithm chooses an agent i and adds the best local tree (searched at line 11) in order to optimise:\nmax f(qi) = \u2211 b\u2208Bt max q\u2032\u2208X Vq\u2032(b) (4)\nwhere X = { (q\u2032j)j\u2208I \u2223\u2223q\u2032i \u2208 Qti \u222a {qi},\u2200j 6= i, q\u2032j \u2208 Qtj}\ns.t. qi \u2208 Q\u0304ti = Ai \u00d7 (Qt+1i ) \u2126i\nThis step is repeated until the maximum number of trees is reached for each agent or the criterion cannot be improved further by adding other local trees.\nAlgorithm 6: Two Phases DP Operator for PSMBDP Data: N points Bt Data: \u2200i,Qt+1i and their \u03b1-vectors Result: \u2200i,Qti and their \u03b1-vectors foreach i \u2208 I do Q\u0304ti \u2190 Ai \u00d7 (Q\u0304 t+1 i ) \u2126i1 Qt \u2190 Prune((Q\u0304ti)i\u2208I ,Bt), solving (3)2"}, {"heading": "4.4 Branch-and-Bound Search", "text": "Our original algorithm does not scale with the number of observations because the exhaustive backup builds a number of policy trees exponential with the number of observations. In order to scale better with the number\nAlgorithm 7: Prune with Incremental Greedy Optimisation Data: N points Bt Data: \u2200i, Q\u0304ti and their \u03b1-vectors Result: \u2200i,Qti and their \u03b1-vectors Let the criterion \u2200Q, I(Q) = \u2211 b\u2208Bt maxq\u2208Q Vq(b)1\n(qi)i\u2208I \u2190 arg maxq\u2208\u220fi Q\u0304ti I({q}) (PBIP search)2 foreach i \u2208 I do Qti \u2190 {qi}3 foreach i \u2208 I do Q\u0304it \u2190 Q\u0304it \\ {qi}4 flag\u2190 true5 while \u2203i, |Qti| 6= W \u2227 flag do6 flag\u2190 false7 foreach i \u2208 I do8 if |Qti| 6= W then9 Let \u2200qi,K(qi) = (Qti \u222a {qi})\u00d7 \u220f j 6=iQtj10 qi \u2190 arg maxq\u0304i\u2208Q\u0304ti I(K(q\u0304i))11 if enhancement of I(Qt) then12 Qti \u2190 Qti \u222a {qi}13 Q\u0304ti \u2190 Q\u0304ti \\ {qi}14 flag\u2190 true15\nof observations, the combinatorial optimisation problem (4) (line 11 of algorithm 7) can be solved without explicitly building and exhaustively enumerating the set Q\u0304ti = Ai \u00d7 (Q t+1 i )\n\u2126i of local policy trees: it can be enumerated implicitly by branch-and-bound search similarly to what is done in PBIP. The backup phase is not needed anymore.\nBranch A search tree node q\u0303i represents a set of local policy trees qi defined by a set of constraints. It can be seen as a partially defined local policy tree. Each child node q\u0303i of the search tree is a subset of its parent defined by adding a constraint on qi, either by fixing the local action aqi \u2208 Ai or by fixing a joint subtree qi(oi) \u2208 Qt+1i for a given local observation oi (see figure 4). A difference with PBIP, is that the search is done in the space of local policy trees instead of the space of joint policy trees: the search space is smaller, |Ai|W |\u2126i| instead of |A|(W |I|)maxi\u2208I |\u2126i| for PBIP and every search tree node is valid.\nBound The heuristic value f(q\u0303i) of a search tree node q\u0303i is a upper bound of the value f(qi) for every local policy tree qi \u2208 q\u0303i matching the constraints (see criterion 4):\nf(q\u0303i) = \u2211 b\u2208Bt ( max q\u2208Qt Vq(b), max q\u2212i\u2208Qt\u2212i V +\u3008q\u0303i,q\u2212i\u3009(b) )\nwhere q\u0303 = \u3008q\u0303i, q\u2212i\u3009 = {\u3008qi, q\u2212i\u3009|qi \u2208 q\u0303i} is the set of joint policy trees q = \u3008qi, q\u2212i\u3009 made of the policy trees\nq\u2212i and one local policy tree qi \u2208 q\u0303\u2212i. It can be seen as a partially defined joint policy tree made of q\u2212i and the partially defined local policy tree q\u0303i. V +q\u0303 is a upper bound of the value of the best policy tree q \u2208 q\u0303. For a given search tree node q\u0303i, the set of local observations is partitioned in two subsets, \u2126 = \u21261\u222a\u21262: \u21261i is the set of local observations oi of agent i whose subtree qi(oi) is already fixed; \u21262i is the set of local observations oi of agent i whose subtree qi(oi) is not fixed yet. Let \u21261 = \u21261i \u00d7 \u2126\u2212i and \u21262 = \u21262i \u00d7 \u2126\u2212i be the corresponding sets of joint observations. The problem is relaxed by ignoring the decentralisability constraints for o \u2208 \u21262 and by choosing a different policy tree q = \u3008qi, q\u2212i\u3009 for each belief b and each q\u2212i. The contribution of the best possible local subtree q\u2032i \u2208 Q t+1 i is used:\nV +q\u0303 (b) = R(b, aq)+ \u2211 o\u2208\u21261 O(o|b, aq) [ \u03b1q(o) \u00b7 \u03c4(b, aq, o) ] + \u2211 o\u2208\u21262 O(o|b, aq) max q\u2032i\u2208Q t+1 i \u03b1\u3008q\u2032i,q\u2212i(o\u2212i)\u3009 \u00b7 \u03c4(b, aq, o)\nThe algorithm is the same as 7 but line 11 is solved with branch-and-bound search and Q\u0304ti is not computed explicitly by the DP operator.\nWe have tested both depth-first and best-first search. The latter uses more memory but the memory requirement is still reasonable and the computation is much faster on the tested benchmarks.\nAs with PBIP, the size of the search space is exponential with the number of observations. If a huge part of the search space is pruned in practice, we have no guarantee that, in general, the explored search space is not exponential with the number of observations."}, {"heading": "5 Experiments", "text": "This section contains results of different algorithms for some benchmark problems, the average expected reward (AEV), its standard deviation (\u03c3) and the mean computation time (T ) in seconds.\nAlgorithms PBIP/BeFS is a variation of PBIP with best-first search. PSMBDP and PSMBDP/BeFS are respectively exhaustive PSMBDP and PSMBDP with best-first search. In some cases, the value of the optimal policy of the underlying MDP is shown as well: it is an upper bound of the Dec-POMDP policy values.\nHeuristics PSMBDP considers the prior of the beliefs of the underlying POMDP when using a heuristic policy and sample belief points on those distributions: a half of the points uses the POMDP policy greedy with respect to the optimal MDP value function; the other half uses the random policy. MBDP and PBIP use MDP heuristic based on the prior distribution on states: a half of the points is taken from the MDP heuristic of MBDP; the other half is taken from the random heuristic of MBDP."}, {"heading": "5.1 Dec-Tiger", "text": "Table 1 shows results for the Dec-Tiger problem [Nair et al., 2003] for H = 100, with 50 executions and N = 100 for PSMBDP. The (b) variant use the same heuristic as MBDP. Using the POMDP belief prior as heuristic does not give good results. It is not necessary to give a high value of W to PSMBDP to find a good solution because a smaller (usually 5) number of policy trees per time step are kept by PSMBDP.\nComputation times are lower with PSMBDP for this problem, because only a small number of trees per agent are kept: the average width of the local policy trees is 5 for all heuristics. The other trees are not kept as they do not enhance the given criterion. The number of trees and of \u03b1-vectors generated by exhaustive backup is thus lower."}, {"heading": "5.2 Firefighters", "text": "Results for the firefighting problem [Oliehoek et al., 2008] with 2 agents, 4 houses and 3 fire levels are shown in table 2 for horizon 50\nusing W = 7. For PSMBDP, N = 100. PSMBDP finds solutions of better quality than MBDP."}, {"heading": "5.3 Modified Firefighters", "text": "In order to test problems with a higher number of observations, we use a modified firefighting problem where each agent observes the resulting fire intensity in the house it goes to. Table 3 show results for this modified problem with |I| = 2, 4 houses, fire from 0 to 3, H = 100, averaged on 25 executions with W = 3 and N = 100 for PSMBDP. PSMBDP finds solutions of better quality than PBIP."}, {"heading": "5.4 Cooperative Box Pushing", "text": "The Cooperative Box Pushing problem [Seuken and Zilberstein, 2007a] is a problem with a higher number of observations (5 observations per agent for two agents) and has been proposed to benchmark algorithms which can handle a larger number of observations. The results shown in table 4 are average over 25 executions. PBIP times out for H = 50 (T > 20, 000s). PSMBDP finds slightly better solutions than PBIP for a comparable computation time."}, {"heading": "6 Conclusion", "text": "We have introduced a new optimisation criterion to choose the policy trees in a memory-bounded bottomup dynamic programming approach, i.e. the maximisation of the expected reward given a heuristic policy over beliefs. We have presented PSMBDP, a planning algorithm for Dec-POMDPs based on this principle. A simple implementation using exhaustive enumeration of policies may be used for small problems. In order to handle problems with a higher number of observations, a version with implicit enumeration of joint policy trees by branch-and-bound search may be used.\nPSMBDP is able to find solutions of better quality than MBDP and PBIP. However, the results depend strongly on the problems and heuristics used. Our optimisation scheme use a simple greedy heuristic so we may be able to find solutions of better quality by using better optimisation methods."}], "references": [{"title": "Mixed integer linear programming for exact finite-horizon planning in decentralized POMDPs", "author": ["Aras et al", "R. 2007] Aras", "A. Dutech", "F. Charpillet"], "venue": "In Proceedings of the Thirteenth International Conference on Automated Planning and", "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "The complexity of decentralized control of markov decision processes", "author": ["Bernstein et al", "D.S. 2000] Bernstein", "S. Zilberstein", "N. Immerman"], "venue": "In Mathematics of Operations Research,", "citeRegEx": "al. et al\\.,? \\Q2000\\E", "shortCiteRegEx": "al. et al\\.", "year": 2000}, {"title": "Value-based observation compression for DEC-POMDPs", "author": ["Carlin", "Zilberstein", "A. 2008] Carlin", "S. Zilberstein"], "venue": "Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems,", "citeRegEx": "Carlin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Carlin et al\\.", "year": 2008}, {"title": "Point-based incremental pruning heuristic for solving finitehorizon dec-pomdps", "author": ["Dibangoye et al", "J.S. 2009] Dibangoye", "Mouaddib", "A.-I", "B. Chaib-draa"], "venue": "In AAMAS \u201909: Proceedings of The 8th International Conference on Au-", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Dynamic programming for partially observable stochastic games", "author": ["Hansen et al", "E. 2004] Hansen", "D. Bernstein", "S. Zilberstein"], "venue": "In Proceedings of the Nineteenth National Conference on Artificial Intelligence,", "citeRegEx": "al. et al\\.,? \\Q2004\\E", "shortCiteRegEx": "al. et al\\.", "year": 2004}, {"title": "Taming Decentralized POMDPs: Towards Efficient Policy Computation for Multiagent Settings", "author": ["Nair et al", "R. 2003] Nair", "M. Tambe", "M. Yokoo", "D. Pynadath", "S. Marsella"], "venue": "Proceedings of the Twentieth International Joint Con-", "citeRegEx": "al. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al. et al\\.", "year": 2003}, {"title": "Optimal and approximate Q-value functions for decentralized POMDPs", "author": ["Oliehoek et al", "F.A. 2008] Oliehoek", "M.T.J. Spaan", "N. Vlassis"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Improved Memory-Bounded Dynamic Programming for Decentralized POMDPs", "author": ["Seuken", "Zilberstein", "S. 2007a] Seuken", "S. Zilberstein"], "venue": "In Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligences (UAI-07),", "citeRegEx": "Seuken et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Seuken et al\\.", "year": 2007}, {"title": "Memory-Bounded Dynamic Programming for DEC-POMDPs", "author": ["Seuken", "Zilberstein", "S. 2007b] Seuken", "S. Zilberstein"], "venue": "Proceedings of the Twentieth International Joint Conference on Artificial Intelligences (IJCAI-07)", "citeRegEx": "Seuken et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Seuken et al\\.", "year": 2007}, {"title": "Point-based dynamic programming for DEC-POMDPs", "author": ["Szer", "Charpillet", "D. 2006] Szer", "F. Charpillet"], "venue": "In Proceedings of the TwentyFirst AAAI National Conference on Artificial Intelligence", "citeRegEx": "Szer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Szer et al\\.", "year": 2006}, {"title": "MAA*: A Heuristic Search Algorithm for Solving Decentralized POMDPs", "author": ["Szer et al", "D. 2005] Szer", "F. Charpillet", "S. Zilberstein"], "venue": "In Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "al. et al\\.,? \\Q2005\\E", "shortCiteRegEx": "al. et al\\.", "year": 2005}], "referenceMentions": [], "year": 2010, "abstractText": "We propose a new point-based method for approximate planning in Dec-POMDP which outperforms the state-of-the-art approaches in terms of solution quality. It uses a heuristic estimation of the prior probability of beliefs to choose a bounded number of policy trees: this choice is formulated as a combinatorial optimisation problem minimising the error induced by pruning.", "creator": "TeX"}}}