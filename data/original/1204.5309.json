{"id": "1204.5309", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2012", "title": "Analysis Operator Learning and Its Application to Image Reconstruction", "abstract": "Exploiting a priori known structural information lies at the core of many image reconstruction methods that can be stated as inverse problems. The synthesis model, which assumes that images can be decomposed into a linear combination of very few atoms of some dictionary, is now a well established tool for the design of image reconstruction algorithms. An interesting alternative is the analysis model, where the signal is multiplied by an analysis operator and the outcome is assumed to be the sparse. This approach has only recently gained increasing interest. The quality of reconstruction methods based on an analysis model severely depends on the right choice of the suitable operator.", "histories": [["v1", "Tue, 24 Apr 2012 08:56:42 GMT  (4344kb,D)", "http://arxiv.org/abs/1204.5309v1", "18 pages, 43 figures"], ["v2", "Sun, 16 Sep 2012 19:34:35 GMT  (3127kb,D)", "http://arxiv.org/abs/1204.5309v2", "12 pages, 7 figures"], ["v3", "Tue, 26 Mar 2013 11:51:49 GMT  (3217kb,D)", "http://arxiv.org/abs/1204.5309v3", "12 pages, 7 figures"]], "COMMENTS": "18 pages, 43 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["simon hawe", "martin kleinsteuber", "klaus diepold"], "accepted": false, "id": "1204.5309"}, "pdf": {"name": "1204.5309.pdf", "metadata": {"source": "CRF", "title": "Analysis Operator Learning and Its Application to Image Reconstruction", "authors": ["Simon Hawe", "Martin Kleinsteuber", "Klaus Diepold"], "emails": [], "sections": [{"heading": null, "text": "In this work, we present an algorithm for learning an analysis operator from training images. Our method is based on an `p-norm minimization on the set of full rank matrices with normalized columns. We carefully introduce the employed conjugate gradient method on manifolds, and explain the underlying geometry of the constraints. Moreover, we compare our approach to state-of-the-art methods for image denoising, inpainting, and single image super-resolution. Our numerical results show competitive performance of our general approach in all presented applications compared to the specialized state-of-the-art techniques.\nIndex Terms\nAnalysis Operator Learning, Inverse Problems, Image Reconstruction,Geometric Conjugate Gradient, Oblique Manifold\nI. INTRODUCTION"}, {"heading": "A. Problem Description", "text": "L INEAR inverse problems are ubiquitous in the field of image processing. Prominent examples are imagedenoising [1], inpainting [2], super-resolution [3], or image reconstruction from few indirect measurements as in Compressive Sensing [4]. Basically, in all these problems the goal is to reconstruct an unknown image s \u2208 Rn as accurately as possible from a set of indirect and maybe corrupted measurements y \u2208 Rm with n \u2265 m, see [5] for a detailed introduction to inverse problems. Formally, this measurement process can be written as\ny = As + e, (1) where the vector e \u2208 Rm models sampling errors and noise, and A \u2208 Rm\u00d7n is the measurement matrix modeling the sampling process. In many cases, reconstructing s by simply inverting Equation (1) is highly ill-posed because either the exact measurement process and hence A is unknown as in blind image deconvolution, or the number of observations is much smaller compared to the dimension of the signal, which is the case in Compressive Sensing or image inpainting. To overcome the ill-posedness and to stabilize the solution, prior knowledge or assumptions about the general statistics of images can be exploited."}, {"heading": "B. Synthesis Model and Dictionary Learning", "text": "One assumption that has proven to be successful in image reconstruction, cf. [6], is that natural images admit a sparse representation x \u2208 Rd over some dictionary D \u2208 Rn\u00d7d with d \u2265 n. A vector x is called sparse when most of its coefficients are equal to zero or small in magnitude. When s admits a sparse representation over D, it can be\nThe authors are with the Department of Electrical Engineering, Technische Universit\u00e4t M\u00fcnchen, Arcisstra\u00dfe 21, Munich 80290, Germany (e-mail: {simon.hawe,kleinsteuber,kldi}@tum.de, web: www.gol.ei.tum.de)\nar X\niv :1\n20 4.\n53 09\nv1 [\ncs .L\nG ]\n2 4\nA pr\n2 01\n2\nexpressed as a linear combination of only very few columns of the dictionary {di}di=1, called atoms, which reads as\ns = Dx. (2) For d > n, the dictionary is said to be overcomplete or redundant.\nNow, using the knowledge that (2) allows a sparse solution, an estimation of the original signal in (1) can be obtained from the measurements y by first solving\nx? = arg min x\u2208Rd g(x) subject to \u2016ADx\u2212 y\u201622 \u2264 , (3)\nand afterwards synthesizing the signal from the computed sparse coefficients via s? = Dx?. Therein, g : Rd \u2192 R is a function that promotes or measures sparsity, and \u2208 R+ is an estimated upper bound on the noise power \u2016e\u201622. Common choices for g include `p-norms\n\u2016v\u2016pp := \u2211 i |vi|p, (4)\nwith 0 < p \u2264 1 and differentiable approximations of (4). As the signal is synthesized from the sparse coefficients, the reconstruction model (3) is called the synthesis reconstruction model [7].\nTo find the minimizer of Problem (3), various algorithms based on convex or non-convex optimization, greedy pursuit methods, or Bayesian frameworks exist that may employ different choices of g. For a broad overview of such algorithms, we refer the interested reader to [8]. What all these algorithms have in common, is that their performance regarding the reconstruction quality severely depends on an appropriately chosen dictionary D. Ideally, one is seeking for a dictionary where s can be represented most accurately with a coefficient vector x that is as sparse as possible. Basically, dictionaries can be assigned to two major classes: analytic dictionaries and learnt dictionaries.\nAnalytic dictionaries are built on mathematical models of a general type of signal, e.g. natural images, they should represent. Popular examples include Wavelets [9], Bandlets [10], and Curvlets [11] among several others, or a concatenation of various such bases/dictionaries. They offer the advantages of low computational complexity and of being universally applicable to a wide set of signals. However, this universality comes at the cost of not giving the optimally sparse representation for more specific classes of signals, e.g. face images.\nIt is now well known that signals belonging to a specific class can be represented with fewer coefficients over a dictionary that has been learnt using a representative training set, than over analytic dictionaries. This is desirable for various image reconstruction applications as it readily improves their performance and accuracy [12]\u2013[14]. Basically, the goal is to find a dictionary over which a training set admits a maximally sparse representation. In contrast to analytic dictionaries that can be applied globally to an entire image, learnt dictionaries are small dense matrices that have to be applied locally to small image patches. Hence, the training set consists of small patches extracted from some example images. This restriction to patches mainly arises from limited memory, and limited computational resources.\nRoughly speaking, starting from some initial dictionary the learning algorithms iteratively update the atoms of the dictionary, such that the sparsity of the training set is increased. This procedure is often performed via blockcoordinate relaxation, which alternates between finding the sparsest representation of the training set while fixing the atoms, and optimizing the atoms that most accurately reproduce the training set using the previously determined sparse representation. Three conceptually different approaches for learning a dictionary became well established, which are probabilistic ones like [15] or [16], clustering based ones such as K-SVD [17], and recent approaches which aim at learning dictionaries with specific matrix structures that allow fast computations like [18]. For a comprehensive overview of dictionary learning techniques see [19]."}, {"heading": "C. Analysis Model", "text": "An alternative to the synthesis model (3) for reconstructing a signal, is to solve\ns? = arg min s\u2208Rn g(\u2126s) subject to \u2016As\u2212 y\u201622 \u2264 , (5)\nwhich is known as the analysis model [7]. Therein, \u2126 \u2208 Rk\u00d7n with k \u2265 n is called the analysis operator, and the analyzed vector \u2126s \u2208 Rk is assumed to be sparse, where sparsity is again measured via an appropriate function g. In contrast to the synthesis model, where a signal is fully described by the non-zero elements of x, in the analysis model the zero elements of the analyzed vector \u2126s described the subspace containing the signal. To emphasize this difference, the term cosparsity has been introduced in [20], which simply counts the number of zero elements of \u2126s. As the sparsity in the synthesis model depends on the chosen dictionary, the cosparsity of an analyzed signal depends on the choice of the analysis operator \u2126.\nDifferent analysis operators proposed in the literature include the fused Lasso [21], the translation invariant wavelet transform [22], and probably best known the finite difference operator which is closely related to the total-variation [23]. They all have shown very good performance when used within the analysis model for solving diverse inverse problems in imaging. The question is: can the performance of analysis based signal reconstruction be improved upon when a learnt analysis operator is applied instead of a predefined one, as it is the case for the synthesis model where learnt dictionaries outperform analytic dictionaries? As it has been discussed in [7], the two models differ significantly, and the na\u00efve way of learning a dictionary and simply employing its transposed or its pseudo-inverse as the learnt analysis operator fails. Hence, different algorithms are required for analysis operator learning."}, {"heading": "D. Contributions", "text": "We introduce a new algorithm to learn a patch based analysis operator from a set of training samples. The method relies on several constraints on the operator, which are motivated in Section II-B. One of these constraints requires the analysis operator to have full rank and normalized rows. To enforce this constraint, we propose an efficient geometric conjugate gradient method on the so-called oblique manifold in Section III. Thereby, an initial operator is iteratively updated such that the `p-pseudo-norm of the corresponding analyzed training samples is minimized, while preventing overfitting to a subset of the training samples. Furthermore, in Section IV we explain how to apply the local patch based analysis operator to achieve global reconstruction results. We use the learnt analysis operator in Section V for the tasks of image denoising, inpainting, and single image super-resolution. The examples and numerical results show the broad and effective applicability of our general approach."}, {"heading": "E. Notations", "text": "Matrices are written as capital calligraphic letters like X , column vectors are denoted by boldfaced small letters e.g. x whereas scalars are either capital or small letters like n,N . By vi we denote the ith element of the vector v, vij denotes the ith element in the jth column of a matrix V . The vector v:,i denotes the ith column of V whereas vi,: denotes the transposed of the ith row of V . By Eij , we denote a matrix whose ith entry in the jth column is equal to one, and all others are zero. Ik denotes the identity matrix of dimension (k\u00d7k), 0 denotes the zero-matrix of appropriate dimension, and ddiag(V) is the diagonal matrix whose entries on the diagonal are those of V . By \u2016V\u20162F = \u2211 i,j v 2 ij we denote the squared Frobenius norm of a matrix V , tr(V) is the trace of V , and rk(V) denotes the rank.\nII. ANALYSIS OPERATOR LEARNING"}, {"heading": "A. Prior Art", "text": "The topic of analysis operator learning has only recently started to be investigated, and only few prior work exist. In the sequel, we shortly review analysis operator learning methods that are applicable for image processing tasks.\nGiven a set of M training samples { si \u2208 Rn }M i=1\n, the goal of analysis operator learning is to find a matrix \u2126 \u2208 Rk\u00d7n with k \u2265 n, which results in a maximally cosparse representation \u2126si of each training sample. As mentioned in Subsection I-B, the training samples are distinctive vectorized image patches extracted from a set of example images. Let S = [s1, . . . , sM ] \u2208 Rn\u00d7M be a matrix where the training samples constitute its columns, the goal can be formulated as\n\u2126? = arg min \u2126 g(\u2126S), (6)\nwhere \u2126 is subject to some constraints, and g is some function that measures the sparsity of the matrix \u2126S. In [24], an algorithm is proposed where the rows of the analysis operator are found sequentially by identifying directions that are orthogonal to a subset of the training samples. Starting from a randomly initialized vector \u03c9 \u2208 Rn, a candidate row is found by first computing the inner product of \u03c9 with the entire training set, followed by extracting the reduced training set SR of samples whose inner product with \u03c9 is smaller than a threshold. Thereafter, \u03c9 is updated by taking the eigenvector corresponding to the smallest eigenvalue of SRS>R . This procedure is iterated several times until a convergence criterion is met. If the determined candidate vector is sufficiently distinctive from already found ones, it is added as a new row to \u2126, otherwise it is discarded. This process is repeated until the desired number k of rows have been found.\nAn adaption of the widely known K-SVD dictionary learning algorithm to the problem of analysis operator learning is presented in [25]. As in the original K-SVD algorithm, g(\u2126S) = \u2016\u2126S\u20160 is employed as the sparsifying function and the target cosparsity is required as an input to the algorithm. The arising optimization problem is solved by alternating between a sparse coding stage over each training sample while fixing \u2126 using an ordinary analysis pursuit method, and updating the analysis operator using the optimized training set. Thereby, each row of \u2126 is updated in a similar way as described in the previous paragraph for the method of [24]. Interestingly, the operator learnt on piecewise constant image patches by [24] and [25] closely mimics the finite difference operator.\nIn [26], the authors employ g(\u2126S) = \u2016\u2126S\u20161 as the sparsity promoting function and suggest a constrained optimization technique that utilizes a projected subgradient method for iteratively solving (6). To exclude the trivial solution, the set of possible analysis operators is restricted to the set of Uniform Normalized Tight Frames, i.e. matrices with uniform row norm and orthonormal columns. The authors state that this algorithm has the limitation of requiring noiseless training samples whose analyzed vectors {\u2126si}Mi=1 are exactly cosparse.\nTo overcome this restriction, the same authors propose an extension of this algorithm that simultaneously learns the analysis operator and denoises the training samples, cf. [27]. This is achieved by alternating between updating the analysis operator via the projected subgradient algorithm and denoising the samples using an Augmented Lagrangian method. Therein, the authors state that their results for image denoising using the learnt operator are only slightly worse compared to employing the commonly used finite difference operator."}, {"heading": "B. Motivation of Our Approach", "text": "The learning algorithm presented here finds an analysis operator \u2126 from a set of training samples S by solving a problem related to (6). Therein, we employ the mixed (p, q)-pseudo-norm as a measure of sparsity, which for some matrix V \u2208 Rk\u00d7M is given by\nJ(p,q)(V) := 1q M\u2211 j=1 ( 1 p k\u2211 i=1 |vij |p )q = 1q M\u2211 j=1 ( 1 p\u2016v:,j\u2016pp )q , (7)\nwith 0 \u2264 p \u2264 1 and q > 1. Here, we employ V = \u2126S and the interpretation of the sparsity measure (7) is as follows. The `p-pseudo-norm promotes sparsity of each analyzed training sample \u2126si. The exponent q > 1 ensures that sparsity is distributed over all analyzed training samples, and not concentrated onto only a small part. In this way, we find an analysis operator which is applicable to a broad set of signals, and not overfitted to some specific subset.\nCertainly, without additional prior assumptions on \u2126, the useless solution \u2126 = 0 is the global minimizer of Problem (6). To avoid the trivial solution and for other reasons explained later in this section, we regularize the problem by imposing the following three constraints on \u2126.\n(i) The rows of \u2126 have unit Euclidean norm, i.e. \u2016\u03c9i,:\u20162 = 1 for i = 1, . . . , k. (ii) The analysis operator \u2126 has full rank, i.e. rk(\u2126) = n.\n(iii) The analysis operator \u2126 does not have trivially linear dependent rows, i.e. \u03c9i,: 6= \u00b1\u03c9j,: for i 6= j. The rank condition (ii) on \u2126 is motivated by the fact that different input samples s1, s2 \u2208 Rn with s1 6= s2 should be mapped to different analyzed vectors \u2126s1 6= \u2126s2. With Condition (iii) redundant transform coefficients in an analyzed vector are avoided.\nThese constraints motivate the consideration of the set of full rank matrices with normalized columns, which admits a manifold structure known as the oblique manifold [28]\nOB(n, k) := {X \u2208 Rn\u00d7k| rk(X ) = n, ddiag(X>X ) = Ik}. (8)\nNote, that this definition only yields a non-empty set if k \u2265 n, which is the interesting case in this work. Thus, from now on, we assume k \u2265 n. Remember that we require the rows of \u2126 to have unit Euclidean norm, hence, we restrict the transposed of the learnt analysis operator to be an element of OB(n, k).\nSince OB(n, k) is open and dense in the set of matrices with normalized columns, we need a penalty function that ensures the rank constraint (ii) and prevents iterates to approach the boundary of OB(n, k).\nLemma 1: The inequality 0 < det( 1kXX>) \u2264 ( 1n)n holds true for all X \u2208 OB(n, k), where 1 < n \u2264 k. Proof: Due to the full rank condition on X , the product XX> is positive definite, consequently the strict\ninequality 0 < det( 1kXX>) applies. To see the second inequality of Lemma 1, observe that\n\u2016X\u20162F = tr(XX>) = k, (9) which implies tr( 1kXX>) = 1. Since the trace of a matrix is equal to the sum of its eigenvalues, which are strictly positive in our case, it follows that the strict inequality 0 < \u03bbi < 1 holds true for all eigenvalues \u03bbi of 1kXX>. From the well known relation between the arithmetic and the geometric mean we see\nn \u221a \u03a0\u03bbi \u2264 1n \u2211 \u03bbi. (10)\nNow, since the determinant of a matrix is equal to the product of its eigenvalues, and with \u2211 \u03bbi = tr( 1 kXX>) = 1, we have det( 1kXX>) = \u03a0\u03bbi \u2264 ( 1n)n, (11)\nwhich completes the proof. Recalling that \u2126> \u2208 OB(n, k) and considering Lemma 1, we can enforce the full rank constraint with the penalty function\nh(\u2126) := \u2212 1n log(n) log det( 1k\u2126>\u2126). (12) Regarding Condition (iii), the following result proves useful. Lemma 2: For a matrix X \u2208 OB(n, k) with 1 < n \u2264 k, the inequality |x>:,ix:,j | \u2264 1 applies, where equality holds true if and only if x:,i = \u00b1x:,j . Proof: By the definition of OB(n, k) the columns of X are normalized, consequently Lemma 2 follows directly from the Cauchy-Schwarz inequality. Thus, Condition (iii) can be enforced via the logarithmic barrier function of the scalar products between all distinctive rows of \u2126, i.e.\nr(\u2126) := \u2212 \u2211\n1\u2264i<j\u2264k log(1\u2212 (\u03c9>i,:\u03c9j,:)2). (13)\nFinally, combining all the introduced constraints, our optimization problem for learning the transposed analysis operator reads as\n\u2126> = arg min X\u2208OB(n,k) J(p,q)(X>S) + \u03ba h(X>) + \u00b5 r(X>). (14)\nTherein, the two weighting factors \u03ba, \u00b5 \u2208 R+ control the influence of the two constraints on the final solution. The following lemma clarifies the role of \u03ba. Lemma 3: Let \u2126 be a minimum of h in the set of transposed oblique matrices, i.e.\n\u2126> \u2208 arg min X\u2208OB(n,k) h(X>), (15)\nthen the condition number of \u2126 is equal to one. Proof: It is well known that equality of the arithmetic and the geometric mean in Equation (10) holds true, if and only if all eigenvalues \u03bbi of 1kXX> are equal, i.e. \u03bb1 = . . . = \u03bbn. Hence, if \u2126> \u2208 arg min X\u2208OB(n,k) h(X>), then det( 1k\u2126 >\u2126) = ( 1n)\nn, and consequently all singular values of \u2126 coincide. This implies that the condition number of \u2126, which is defined as the quotient of the largest to the smallest singular value, is equal to one.\nFrom Lemma 3 we can conclude that with larger \u03ba the condition number of \u2126 approaches one. Now, recall the inequality\n\u03c3min\u2016s1 \u2212 s2\u20162 \u2264 \u2016\u2126(s1 \u2212 s2)\u20162 \u2264 \u03c3max\u2016s1 \u2212 s2\u20162, (16) with \u03c3min being the smallest and \u03c3max being the largest singular value of \u2126. From this it follows that an analysis operator found with a large \u03ba, i.e. obeying \u03c3min \u2248 \u03c3max, carries over distinctness of different signals to their analyzed versions.\nThe parameter \u00b5 regulates the redundancy between the rows of the analysis operator and consequently avoids redundant coefficients in the analyzed vector \u2126s.\nLemma 4: The difference between any two entries of the analyzed vector \u2126s is bounded by |\u03c9>i,:s\u2212 \u03c9>j,:s| \u2264 \u221a 2(1\u2212 \u03c9>i,:\u03c9j,:) \u2016s\u20162. (17)\nProof: From the Cauchy-Schwarz inequality we get\n|\u03c9>i,:s\u2212 \u03c9>j,:s| = |(\u03c9i,: \u2212 \u03c9j,:)>s| \u2264 \u2016\u03c9i,: \u2212 \u03c9j,:\u20162\u2016s\u20162. (18) Since by definition \u2016\u03c9i,:\u20162 = \u2016\u03c9j,:\u20162 = 1, it follows that \u2016\u03c9i,: \u2212 \u03c9j,:\u20162 = \u221a\n2(1\u2212 \u03c9>i,:\u03c9j,:). The above lemma implies, that if the ith entry of the analyzed vector is significantly larger than 0 and if \u03c9>i,:\u03c9j,: is close to 1, then the jth entry is also significantly larger than 0. In order to achieve large cosparsity, we aim to avoid this effect.\nIn the next section, we explain how the manifold structure of OB(n, k) can be exploited to efficiently learn the analysis operator."}, {"heading": "III. ANALYSIS OPERATOR LEARNING ALGORITHM", "text": "Knowing that the feasible set of solutions to Problem (14) is restricted to a smooth manifold allows us to formulate a geometric conjugate gradient (CG-) method to learn the analysis operator. Geometric CG-methods have been proven efficient in various applications, due to the combination of moderate computational complexity and good convergence properties, see e.g. [29] for a CG-type method on the oblique manifold.\nTo make this work self contained, we start by shortly reviewing the general concepts of optimization on matrix manifolds. After that we present the concrete formulas and implementation details for our optimization problem on the oblique manifold. For an in-depth introduction on optimization on matrix manifolds, we refer the interested reader to [30]."}, {"heading": "A. Optimization on Matrix Manifolds", "text": "Let M be a smooth Riemannian submanifold of Rn\u00d7k with the standard Frobenius inner product \u3008Q,P\u3009 := tr(Q>P), and let f : Rn\u00d7k \u2192 R be a differentiable cost function. We consider the problem of finding\narg min X\u2208M\nf(X ). (19)\nThe concepts presented in this subsection are visualized in Figure 1 to alleviate the understanding. To every point X \u2208 M one can assign a tangent space TXM. The tangent space at X is a real vector space containing all possible directions that tangentially pass through X . An element \u039e \u2208 TXM is called a tangent vector at X . Each tangent space is associated with an inner product inherited from the surrounding Rn\u00d7k, which allows to measure distances and angles on M.\nThe Riemannian gradient of f at X is an element of the tangent space TXM that points in the direction of steepest ascent of the cost function on the manifold. As we require M to be a submanifold of Rn\u00d7k and since by assumption f is defined on the whole Rn\u00d7k, the Riemannian gradient G(X ) is simply the orthogonal projection of the (standard) gradient \u2207f(X ) onto the tangent space TXM. In formulas, this reads as\nG(X ) := \u03a0TXM(\u2207f(X )). (20)\nTECHNICAL REPORT, TECHNISCHE UNIVERSIT\u00c4T M\u00dcNCHEN 7\nX Y\nTXM\nTYM\n\u2207f(X ) G := \u03a0TXM(\u2207f(X ))\nH \u0393(X ,H, t)\nT (G,X ,H, \u03b1) M\nX Y\nTXM\nTYM\n\u2207f(X ) G := \u03a0TXM(\u2207f(X ))\nH \u0393(X ,H, t)\nT (G,X ,H, \u03b1) M\nX Y\nTXM\nTYM\n\u2207f(X ) G := \u03a0TXM(\u2207f(X ))\nH \u0393(X ,H, t)\nT (G,X ,H, \u03b1) M\nX Y\nTXM\nTYM\n\u2207f(X ) G := \u03a0TXM(\u2207f(X ))\nH \u0393(X ,H, t)\nT (G,X ,H, \u03b1)\nM\nX Y\nTXM\nTYM\n\u2207f(X ) G := \u03a0TXM(\u2207f(X ))\nH \u0393(X ,H, t)\nT (G,X ,H, \u03b1) M\nX Y\nTXM\nTYM\n\u2207f(X ) G := \u03a0TXM(\u2207f(X ))\nH \u0393(X ,H, t)\nT (G,X ,H, \u03b1) M\nX Y\nTXM\nTYM\n\u2207f(X ) G := \u03a0TXM(\u2207f(X ))\nH \u0393(X ,H, t)\nT (G,X ,H, \u03b1) M\nX Y\nTXM\nTYM\n\u2207f(X ) G := \u03a0TXM(\u2207f(X ))\nH \u0393(X ,H, t)\nT (G,X ,H, \u03b1) M\nX Y\nT M\nT M\n\u2207f(X ) G := \u03a0 X (\u2207f(X ))\nH \u0393(X ,H, t)\nT (G,X ,H, \u03b1) M\nX Y\nTXM\nTYM\n\u2207f(X ) G := \u03a0TXM(\u2207f(X ))\nH \u0393(X ,H, t)\nT (G,X ,H, \u03b1) M\nFig. 1. This figure shows two points X and Y on a manifold M together with their tangent spaces TXM and TYM. Furthermore, the Euclidean gradient \u2207f(X ) and its projection onto the tangent space \u03a0TXM(\u2207f(X )) are depicted. The geodesic \u0393(X ,H, t) in the direction of H \u2208 TXM connecting the two points is shown. The dashed line typifies the role of a parallel transport of the gradient in TXM to TYM.\nA geodesic is a smooth curve \u0393(X ,\u039e, t) emanating from X in the direction of \u039e \u2208 TXM, which locally describes the shortest path between two points. Intuitively, it can be interpreted as the generalization of a straight line to a manifold.\nConventional line search methods search for the next iterate along a straight line. This is generalized to the manifold setting as follows.Given a current optimal point X (i) and a search direction H(i) \u2208 TX (i)M at the ith iteration, the step size \u03b1(i) which results in sufficient decrease of f can be determined by finding the minimizer of\n\u03b1(i) = arg min t\u22650 f(\u0393(X (i),H(i), t)). (21)\nOnce \u03b1(i) has been determined, the new iterate is computed by\nX (i+1) = \u0393(X (i),H(i), \u03b1(i)). (22)\nNow, one straightforward approach to minimize f is to alternate Equations (20), (21), and (22) usingH(i) = \u2212G(i), with the short hand notation G(i) := G(X (i)), which corresponds to the steepest descent on a Riemmanian manifold. However, as in standard optimization, steepest descent only has a linear rate of convergence. Therefore, we employ a conjugate gradient method on a manifold, as it offers a superlinear rate of convergence, while still being applicable to large scale optimization problems with low computational complexity.\nThereby, the updated search direction H(i+1) \u2208 TX (i+1)M is a linear combination of the gradient G(i+1) \u2208 TX (i+1)M and the previous search direction H(i) \u2208 TX (i)M. Since addition of vectors from different tangent spaces is not defined, we need to map H(i) from TX (i)M to TX (i+1)M. This is done by the so-called parallel transport T (\u039e,X (i),H(i), \u03b1(i)), which transports a tangent vector \u039e \u2208 TX (i)M along the geodesic \u0393(X (i),H(i), t) to the tangent space TX (i+1)M. Now, using the shorthand notation\nT (i+1)\u039e := T (\u039e,X (i),H(i), \u03b1(i)), (23) the new search direction is computed by\nH(i+1) = \u2212G(i+1) + \u03b2(i)T (i+1)H(i) , (24)\nwhere \u03b2(i) \u2208 R is calculated by some update formula adopted to the manifold setting. Most popular are the updates known as Fletcher-Reeves (FR), Polak-Ribi\u00e8re (PR), Hestenes-Stiefel (HS), and Dai-Yuan (DY) formulas. With Y(i+1) = G(i+1) \u2212 T (i+1)G(i) , they read as\n\u03b2 (i) FR = \u3008G(i+1),G(i+1)\u3009 \u3008G(i),G(i)\u3009 , (25)\n\u03b2 (i) PR = \u3008G(i+1),Y(i+1)\u3009 \u3008G(i),G(i)\u3009 , (26)\n\u03b2 (i) HS = \u3008G(i+1),Y(i+1)\u3009 \u3008T (i+1)H(i) ,Y(i+1)\u3009 , (27)\n\u03b2 (i) DY = \u3008G(i+1),G(i+1)\u3009 \u3008T (i+1)H(i) ,Y(i+1)\u3009 . (28)\nNow, a solution to Problem (19) is computed by alternating between finding the search direction on M and updating the current optimal point until some user specified convergence criterion is met, or a maximum number of iterations has been reached."}, {"heading": "B. Geometric Conjugate Gradient for Analysis Operator Learning", "text": "In this subsection we derive all ingredients to implement the geometric conjugate gradient method as described in the previous subsection for the task of learning the analysis operator. Results regarding the geometry of OB(n, k) are derived e.g. in [30]. To enhance legibility, and since the dimensions n and k are fixed throughout the rest of the paper, the oblique manifold is further on denoted by OB.\nThe tangent space at X \u2208 OB is given by TXOB = {\u039e \u2208 Rn\u00d7k| ddiag(X>\u039e) = 0}. (29)\nThe orthogonal projection of a matrix Q \u2208 Rn\u00d7k onto the tangent space TXOB is \u03a0TXOB(Q) = Q\u2212X ddiag(X>Q). (30)\nRegarding geodesics, note that in general a geodesic is the solution of a second order ordinary differential equation, meaning that for arbitrary manifolds, its computation as well as computing the parallel transport is not feasible. Fortunately, as the oblique manifold is a Riemannian submanifold of a product of k unit spheres Sn\u22121, the formulas for parallel transport and the exponential mapping allow an efficient implementation.\nLet x \u2208 Sn\u22121 be a point on a sphere and h \u2208 TxSn\u22121 be a tangent vector at x, then the geodesic in the direction of h is a great circle\n\u03b3(x,h, t) = { x, if \u2016h\u20162 = 0 x cos(t\u2016h\u20162) + h sin(t\u2016h\u20162)\u2016h\u20162 , otherwise.\n(31)\nThe associated parallel transport of a tangent vector \u03be \u2208 TxSn\u22121 along the great circle \u03b3(x,h, t) reads as\n\u03c4(\u03be,x,h, t) = \u03be \u2212 \u03be >h\n\u2016h\u201622\n( x\u2016h\u20162 sin(t\u2016h\u20162)+\nh(1\u2212 cos(t\u2016h\u20162)) ) . (32)\nAs OB is a submanifold of the product of unit spheres, the geodesic through X \u2208 OB in the direction of H \u2208 TXOB is simply the combination of the great circles emerging by concatenating each column of X with the corresponding column of H, i.e.\n\u0393(X ,H, t) = [ \u03b3(x:,1,h:,1, t), . . . , \u03b3(x:,k,h:,k, t) ] . (33)\nAccordingly, the parallel transport of \u039e \u2208 TXOB along the geodesic \u0393(X ,H, t) is given by T (\u039e,X ,H, t) =[\n\u03c4(\u03be:,1,x:,1,h:,1, t), . . . , \u03c4(\u03be:,k,x:,k,h:,k, t)\n] .\n(34)\nNow, to use the geometric CG-method for learning the analysis operator, we require a differentiable cost function f . However, the cost function presented in Problem (14) is not differentiable as the mixed (p, q)-pseudo-norm (7) is not smooth. We overcome this problem by exchanging function (7) with a smooth approximation, explicitly given by\nJ(p,q),\u03bd(V) := M\u2211 j=1 1 q ( 1 p k\u2211 i=1 (v2ij + \u03bd) p 2 )q , (35)\nwith \u03bd \u2208 R+ being the smoothing parameter. The smaller \u03bd is, the more closely the approximation resembles the original function. Again, taking V = \u2126S and with the shorthand notation zij := (\u2126S)ij , the gradient of the applied sparsity promoting function (35) reads as\n\u2202 \u2202\u2126J(p,q),\u03bd(\u2126S) =  M\u2211 j=1 ( 1 p k\u2211 i=1 (z2ij + \u03bd) p 2 )q\u22121 k\u2211 i=1 { zij(z 2 ij + \u03bd) p 2 \u22121Eij }] S>. (36)\nThe gradient of the rank penalty term (12) is \u2202 \u2202\u2126h(\u2126) = \u2212 2kn log(n)\u2126( 1k\u2126>\u2126)\u22121 (37)\nand the gradient of the logarithmic barrier function (13) is\n\u2202 \u2202\u2126r(\u2126) =  \u2211 1\u2264i<j\u2264k\n2\u03c9>i,:\u03c9j,:\n1\u2212 (\u03c9>i,:\u03c9j,:)2 (Eij + Eji) \u2126. (38) Combining Equations (36), (37), and (38), the gradient of the cost function\nf(X ) := J(p,q),\u03bd(X>S) + \u03ba h(X>) + \u00b5 r(X>) (39) applied for learning the analysis operator reads as\n\u2207f(X ) = \u2202\u2202X J(p,q),\u03bd(X>S) + \u03ba \u2202\u2202X h(X>) + \u00b5 \u2202\u2202X r(X>). (40)\nRegarding the CG-update parameter \u03b2(i), we employ a hybridization of the Hestenes-Stiefel Formula (27) and the Dai Yuan formula (28)\n\u03b2 (i) hyb = max\n( 0,min(\u03b2\n(i) DY , \u03b2 (i) HS) ) , (41)\nwhich has been suggested in [31]. As explained therein, formula (41) combines the good numerical performance of HS with the desirable global convergence properties of DY.\nFinally, to compute the step size \u03b1(i), we use an adaption of the well-known backtracking line search to the geodesic \u0393(X (i),H(i), t). Thereby, an initial step size t(i)0 is iteratively decreased by a constant factor c1 < 1 until the Armijo condition is met, see Algorithm 1 for the entire procedure. In our implementation we empirically chose c1 = 0.9 and c2 = 10\u22122. As an initial guess for the step size at the first CG-iteration i = 0, we choose\nt (0) 0 = \u2016G(0)\u2016\u22121F , (42)\nas proposed in [32]. In the subsequent iterations, the backtracking line search is initialized by the previous step size divided by the line search parameter, i.e. t(i)0 = \u03b1(i\u22121) c1 . Our complete approach for learning the analysis operator is summarized in Algorithm 2.\nAlgorithm 1 Backtracking Line Search on Oblique Manifold\nInput: t(i)0 > 0, 0 < c1 < 1, 0 < c2 < 0.5, X (i),G(i),H(i) Set: t\u2190 t(i)0\nwhile f(\u0393(X (i),H(i), t)) > f(X (i)) + tc2\u3008G(i),H(i)\u3009 do t\u2190 c1t\nend while Output: \u03b1(i) \u2190 t\nAlgorithm 2 Analysis Operator Learning Input: Initial analysis operator \u2126init, training data S, parameters p, q, \u03bd, \u03ba, \u00b5 Set: i\u2190 0, X (0) \u2190 \u2126>init, H(0) \u2190 \u2212G(0)\nrepeat \u03b1(i) \u2190 arg min\nt\u22650 f(\u0393(X (i),H(i), t)), cf. Algorithm 1 in conjunction with Equation (39)\nX (i+1) \u2190 \u0393(X (i),H(i), \u03b1(i)), cf. Equation (33) G(i+1) \u2190 \u03a0TX(i+1)M(\u2207f(X (i+1))), cf. Equations (30) and (40) \u03b2(i) \u2190 max ( 0,min(\u03b2\n(i) DY , \u03b2 (i) HS) ) , cf. Equations (27), (28)\nH(i+1) \u2190 \u2212G(i+1) + \u03b2(i)T (i+1)H(i) , cf. Equation (34) i\u2190 i+ 1\nuntil \u2016X (i) \u2212X (i\u22121)\u2016F < 10\u22124 \u2228 i = maximum # iterations Output: \u2126? \u2190 X (i)>"}, {"heading": "IV. ANALYSIS OPERATOR BASED IMAGE RECONSTRUCTION", "text": "In this section we explain how the analysis operator \u2126? \u2208 Rk\u00d7n is utilized for reconstructing an unknown image s \u2208 RN from some measurements y \u2208 Rm following the analysis approach (5). Here, the vector s \u2208 RN denotes a vectorized image of dimension N = wh, with w being the width and h being the height of the image, respectively, obtained by stacking the columns of the image among each other. In the following, we will loosely speak of s as the image.\nRemember, that the size of \u2126? is very small compared to the size of the image, and it has to be applied locally to small image patches rather than globally to the entire image. For global reconstruction, the simplest way is to partition the image into non-overlapping patches, reconstruct each patch individually, and stick the reconstructed patches together to form the final image. However, the success of this approach highly depends on the partitioning of the image. Furthermore, it leads to spurious artifacts at patch boundaries, and fails for example in inpainting tasks where large holes compared to the patch size have to be filled. Commonly, artifacts at patch boundaries are reduced by taking overlapping patches, reconstruct them again individually, and form the entire image by averaging the final reconstruction results in the overlapping regions. Still, this method misses global support during the reconstruction process, hence, it leads to poor inpainting results and is not applicable for Compressive Sensing tasks. To overcome these drawbacks, we introduce a method related to the patch based synthesis approach presented in [12] and the MRF approach from [33], which provides global support from local information. Instead of optimizing over each patch individually and combining them in a final step, we optimize over the entire image demanding that the analysis coefficients of each patch that is part of some predefined partition are sparse. When all possible patch positions are taken into account, the proposed reconstruction procedure is entirely partitioning-invariant. For legibility, we assume square patches of odd size, i.e. of size ( \u221a n\u00d7\u221an) with \u221an being odd. The non-square case can be easily adapted. Formally, let r \u2286 {1, . . . , h} and c \u2286 {1, . . . , w} denote a set of row and column indices, respectively, with ri+1 \u2212 ri = dv, ci+1 \u2212 ci = dh and 1 \u2264 dv, dh \u2264 \u221a n. Thereby, dv, dh determine the degree of overlap between two adjacent patches in vertical, and horizontal direction, respectively. We consider all image patches whose center pixel position is an element of the cartesian product set r\u00d7 c. Hence, with | \u00b7 | denoting the cardinality of a set, the\ntotal number of patches being considered is equal to |r||c|. Now, let Prc be a binary (n\u00d7N) matrix that extracts the patch centered at position (r, c). With this notation, we formulate the (global) sparsity promoting function as\u2211\nr\u2208r \u2211 c\u2208c k\u2211 i=1 ((\u2126?Prcs)2i + \u03bd) p 2 . (43)\nIt measures the overall approximated `p-pseudo-norm of the considered analyzed image patches. Again as in Equation (35), \u03bd \u2208 R+ is a smoothing parameter. To use the same notation as in the standard analysis model formulation (5), we compactly rewrite Equation (43) as\ng(\u2126F s) := K\u2211 i=1 ( (\u2126F s)2i + \u03bd ) p 2 , (44)\nwith K = k|r||c| and\n\u2126F :=  \u2126?Pr1c1 \u2126?Pr1c2 ...\n\u2126?Pr|r|c|c|  \u2208 RK\u00d7N (45) being the global analysis operator that expands the patch based one to the entire image. We treat image boundary effects by employing constant padding, i.e. replicating the values at the image boundaries b \u221a n\n2 c times, where b\u00b7c denotes rounding to the smaller integer. Certainly, for a real image processing application \u2126F is too huge for being created explicitly and being applied in terms of matrix vector multiplication. Fortunately, applying \u2126F and its transposed can be implemented efficiently using sliding window techniques, and the matrix vector notation is solely used to facilitate legibility.\nIn addition to the sparsity assumption, we exploit the fact that the range of pixel intensities is generally limited by a lower bound bl and an upper bound bu. We enforce this bounding constraint by minimizing the differentiable\nfunction b(s) := N\u2211 i=1 b(si), where b is a penalty term given as\nb(s) =  |s\u2212 bu|2 if s \u2265 bu |s\u2212 bl|2 if s \u2264 bl\n0 otherwise\n. (46)\nFinally, combining the two constraints (44) and (46) with the data fidelity term, the analysis based image recovery problem is to solve\ns? = arg min s\u2208RN 1 2\u2016As\u2212 y\u201622 + b(s) + \u03bbg(\u2126F s). (47)\nTherein, \u03bb \u2208 R+ balances between the sparsity of the solution\u2019s analysis coefficients and the solution\u2019s fidelity to the measurements. The measurement matrix A \u2208 Rm\u00d7N and the measurements y \u2208 Rm depend on the respective application to be handled."}, {"heading": "V. APPLICATIONS AND EXPERIMENTAL RESULTS", "text": "This section demonstrates various results for classical image reconstruction problems by employing an analysis operator \u2126? that has been learnt with our algorithm proposed in Section III. We compare all our results with a total variation based approach [34]. It serves as a general benchmark for all the considered reconstruction applications since it is the most commonly used analysis operator. The present evaluation is limited to grayscale images, but the general concept can be straightforwardly extended to color images. Considering the dimension of \u2126? \u2208 Rk\u00d7n, we found from various tests that a two times overcomplete operator, i.e. k = 2n, results in good reconstruction performance independent from the selected patch size n. For the following experimental results we chose patches of size (7\u00d7 7), i.e. n = 49.\nNote that we learnt one general analysis operator \u2126? \u2208 R98\u00d749 from the five test images shown in Figure 2, and used it unaltered in all presented applications. The employed training set consisted of M = 200 000 7 \u00d7 7- dimensional image patches each normalized to have unit Euclidean norm that have been randomly extracted from the five training images. Certainly, these images are not considered within the subsequent performance evaluations.\nWe initialized the learning process by a random R98\u00d749 matrix with normalized rows. Tests with various other initializations like an overcomplete DCT showed that the initializing does not influence the final operator. For the sparsity promoting function (39), we chose p = 0.4 and q = 2. To closely approximate the `p-pseudo-norm, we chose a small smoothing parameter \u03bd = 10\u22126. The optimal weighting factors \u03ba and \u00b5 depend on the number of training samples and on the parameters used in (39). For the present situation we empirically found that \u03ba = 106 and \u00b5 = 2000 lead to a suitable \u2126?. In Figure 3 we show the learnt atoms of the analysis operator \u2126? used throughout all our experiments.\nWe want to point out that this choice of parameters leads to a suitable general analysis operator, i.e. an operator with sparsifying property for the class of all natural images. It is thinkable that for more specific classes of images, for example medical imagery, other parameters may lead to a more suitable operator.\nFor all applications we reconstructed the images by solving the minimization problem (47) using the conjugate gradient method proposed in [35]. Considering the pixel intensity bounds, we used bl = 0 and bu = 255, which is the common intensity range in 8-bit grayscale image formats. For the sparsity promoting function (44), we chose the exponent p = 0.4 and the smoothing parameter \u03bd = 10\u22126, which are the same values used for learning the operator \u2126?. As explained in the previous section, our reconstruction algorithm works on overlapping image patches. We achieved the best results for the maximum possible overlap dh = dv = 1. The Lagrange multiplier \u03bb and the measurements matrix A depend on the application, and are briefly discussed in each of the following subsections individually. The respective application scenarios show the general applicability of the learnt analysis operator.\nA. Image Denoising\nFirst, we present our results for denoising images that are corrupted by additive white Gaussian noise (AWGN) of different standard deviation \u03c3noise, and compare them with those achieved by current state-of-the-art denoising methods. To allow a fair comparison, the images and the noise levels presented here are an excerpt of those commonly used in the literature. As usual, the peak signal-to-noise ratio (PSNR)\nPSNR = 10 log (\n2552N\u2211N i=1(si\u2212s?i )2\n) (48)\nis used to quantify the reconstruction quality. For the case of image denoising, the measurements matrix A is simply the identity matrix, i.e. A = IN . Regarding the Lagrange multiplier \u03bb, the larger it is chosen, the more noise is expected to be present. As it is common in the denoising literature, we assume the noise level \u03c3noise to be given and adjust \u03bb accordingly. From our experiments, we found that \u03bb = \u03c3noise16 is a good choice. We terminate our algorithm after 6\u221230 iterations depending on the noise level, i.e. the higher the noise level is the more iterations are required.\nTable I presents a comparison of the reconstruction results achieved by our method with (i) the K-SVD approach specifically designed for image denoising [12], (ii) the currently best denoising algorithm BM3D [36], and (iii) total-variation based denoising [34]. It can be seen, that in most of the cases our method performs slightly better than the K-SVD approach, especially for higher noise levels, and it is at most \u2248 0.5dB worse than BM3D. Compared to the TV approach, the reconstruction quality of our method is always significantly better independent of the noise level or the image content. For a visual assessment, Figure 4 shows exemplarily results achieved by the four denoising approaches compared here. Basically BM3D, K-SVD, and our method create natural similar looking images, whereas the TV based denoising method results in images showing the well known cartoon-like structure.\nB. Image Inpainting\nIn image inpainting as originally proposed in [2], the goal is to fill up a set of damaged or disturbing pixels such that the resulting image is visually appealing. This is necessary for the restoration of old damaged photographs, for removing disturbances caused by defect hardware, or for deleting unwanted objects from an image. Typically, the positions of the pixels to be filled up are a priori given. In our formulation, when N \u2212m pixels must be inpainted, this leads to a binary m\u00d7N dimensional measurements matrix A, where each row contains exactly one entry equal to one. Its position corresponds to a pixel with known grayscale value. In other words, A reflects the available\nimage information. Regarding \u03bb, it can be used in a way that our method simultaneously inpaints missing pixels and denoises the remaining ones.\nFor the first experiment, we disturbed some ground-truth images artificially by removing N \u2212m pixels randomly distributed over the entire image as shown in Figure 5(a) and 5(c). In that way, the reconstruction quality can be judged both visually, and quantitatively in PSNR. We assumed the data to be free of noise, and empirically selected \u03bb = 10\u22122. Figure 5(b) and 5(d) present two exemplary reconstruction results for reconstructing the \"Lena\" image from 40% and 10% of all pixels, respectively.\nThe second experiment demonstrates another typical inpainting problem taken from the literature [34], where a text overlaying an image has to be removed, see Figure 6. Additionally, we corrupted the remaining pixels by AWGN with \u03c3noise = 10 to perform simultaneous inpainting and denoising. As for the denoising examples, we set \u03bb = \u03c3noise16 . Note that the regions that have to be filled are severely bigger than the dimension of the patches our analysis operator has been trained on. In Figure 6(b) we show the results achieved by our algorithm using the learnt general analysis operator \u2126?. For comparison, we present the TV based results from [34] in Figure 6(c)."}, {"heading": "C. Single Image Super-Resolution", "text": "In single image super-resolution (SR), the goal is to reconstruct a high resolution image s \u2208 RN from an observed low resolution image y \u2208 Rm. Thereby, y is assumed to be a blurred and downsampled version of s. Mathematically, this process can be formulated as y = DBs + e with D \u2208 Rm\u00d7N being a decimation operator and B \u2208 RN\u00d7N being a blur operator. Hence, the measurement matrix is given by A = DB. In the ideal case, the\nexact blur kernel is known or an estimate is given. Here, we consider the more realistic case of an unknown blur kernel. Therefore, to apply our approach for magnifying an image by a factor of d in both vertical and horizontal dimension, we model the blur via a Gaussian kernel of dimension (2d\u2212 1)\u00d7 (2d\u2212 1) and with standard deviation \u03c3blur = d 3 .\nFor our experiments, we artificially created a low resolution image by downsampling a ground-truth image by a factor of d using bicubic interpolation. Then, we magnified the low resolution image by the same factor, and compared them with the original image. In Figure 7(e) and (j), we show the reconstruction results of our method for magnifying the \"Girl\" and the \"Baboon\" image by d = 3 and compare it with bicubic interpolation Figure 7(b) and (g), TV based upsampling Figure 7(d) and (i), and the SR algorithm presented in [37] Figure 7(c) and (h). It can be seen, that our method can compete with the current state-of-the-art. We want to emphasize that the blur kernel used for downsampling is different from the blur kernel used in our upsampling scheme.\nNote that many single image super-resolution algorithms rely on clean noise free input data, whereas the general analysis approach as formulated in Equation (47) naturally handles noisy data, and is able to perform simultaneous upsampling and denoising. In Figure 8 we present some results achieved for simultaneously denoising and upsampling a low resolution image by a factor of d = 3 which has been corrupted by AWGN with \u03c3noise = 8. As it can be seen, our method produces the best results both visually and quantitatively."}, {"heading": "VI. CONCLUSION", "text": "This paper dealt with the topic of learning the analysis operator from example image patches, and how to apply it for accurately solving several inverse problems in imaging. To learn the operator, we motivated an `pminimization on the set of full-rank matrices with normalized columns. A geometric conjugate gradient method on the oblique manifold is suggested to solve the arising optimization task. Furthermore, we gave a partitioning invariant method for employing the local patch based analysis operator such that globally consistent reconstruction results are achieved. For the famous tasks of image denoising, image inpainting, and single image super-resolution, we provide promising results that are competitive with current state-of-the-art techniques. Similar as for the synthesis signal reconstruction model with dictionaries, we expect that depending on the application at hand, the performance of the analysis approach can be further increased by learning the particular operator with regard to the specific problem, or employing a specialized training set."}], "references": [{"title": "Image denoising using scale mixtures of gaussians in the wavelet domain", "author": ["J. Portilla", "V. Strela", "M. Wainwright", "E. Simoncelli"], "venue": "IEEE Transactions on Image Processing, vol. 12, no. 11, pp. 1338\u20131351, 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Image inpainting", "author": ["M. Bertalm\u00eco", "G. Sapiro", "V. Caselles", "C. Ballester"], "venue": "ACM SIGGRAPH, 2000, pp. 417\u2013424.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Example-based super-resolution", "author": ["W.T. Freeman", "T.R. Jones", "E.C. Pasztor"], "venue": "IEEE Computer Graphics and Applications, vol. 22, no. 2, pp. 56\u201365, 2002.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information", "author": ["E.J. Cand\u00e8s", "J. Romberg", "T. Tao"], "venue": "IEEE Transactions on Information Theory, vol. 52, no. 2, pp. 489\u2013509, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "An introduction to the mathematical theory of inverse problems", "author": ["A. Kirsch"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1991}, {"title": "On the role of sparse and redundant representations in image processing", "author": ["M. Elad", "M.A.T. Figueiredo", "Y.M."], "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 972\u2013982, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Analysis versus synthesis in signal priors", "author": ["M. Elad", "P. Milanfar", "R. Rubinstein"], "venue": "Inverse Problems, vol. 3, no. 3, pp. 947\u2013968, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Computational methods for sparse solution of linear inverse problems", "author": ["J.A. Tropp", "S.J. Wright"], "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 948\u2013958, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "A theory for multiresolution signal decomposition: the wavelet representation", "author": ["S. Mallat"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 11, no. 7, pp. 674\u2013693, 1989.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1989}, {"title": "Sparse geometric image representations with bandelets", "author": ["E. Le Pennec", "S. Mallat"], "venue": "IEEE Transactions on Image Processing, vol. 14, no. 4, pp. 423\u2013438, 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "The curvelet transform for image denoising", "author": ["J.-L. Starck", "E.J. Cand\u00e8s", "D.L. Donoho"], "venue": "IEEE Transactions on Image Processing, vol. 11, no. 6, pp. 670\u2013684, 2002.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["M. Elad", "M. Aharon"], "venue": "IEEE Transactions on Image Processing, vol. 15, no. 12, pp. 3736\u20133745, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "Journal of Machine Learning Research, vol. 11, no. 1, pp. 19\u201360, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonparametric bayesian dictionary learning for analysis of noisy and incomplete images", "author": ["M. Zhou", "H. Chen", "J. Paisley", "L. Ren", "L. Li", "Z. Xing", "D. Dunson", "G. Sapiro", "L. Carin"], "venue": "IEEE Transactions on Image Processing, vol. 21, no. 1, pp. 130\u2013144, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Dictionary learning algorithms for sparse representation", "author": ["K. Kreutz-Delgado", "J.F. Murray", "B.D. Rao", "K. Engan", "T.W. Lee", "T.J. Sejnowski"], "venue": "Neural computation, vol. 15, no. 2, pp. 349\u2013396, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Method of optimal directions for frame design", "author": ["K. Engan", "S. Aase", "J. Hakon Husoy"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing, 1999, pp. 2443\u20132446.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1999}, {"title": "K-svd: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Transactions on Signal Processing, vol. 54, no. 11, pp. 4311\u20134322, 2006.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Double sparsity: Learning sparse dictionaries for sparse signal approximation", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "IEEE Transactions on Signal Processing, vol. 58, no. 3, pp. 1553\u20131564, 2010.  TECHNICAL REPORT, TECHNISCHE UNIVERSIT\u00c4T M\u00dcNCHEN  18", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Dictionary learning", "author": ["I. To\u0161i\u0107", "P. Frossard"], "venue": "IEEE Signal Processing Magazine, vol. 28, no. 2, pp. 27\u201338, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Cosparse analysis modeling - uniqueness and algorithms", "author": ["S. Nam", "M. Davies", "M. Elad", "R. Gribonval"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, 2011, pp. 5804\u20135807.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparsity and smoothness via the fused lasso", "author": ["R. Tibshirani", "M. Saunders", "S. Rosset", "J. Zhu", "K. Knight"], "venue": "Journal of the Royal Statistical Society Series B, pp. 91\u2013108, 2005.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Signal restoration with overcomplete wavelet transforms: Comparison of analysis and synthesis priors", "author": ["I.W. Selesnick", "M.A.T. Figueiredo"], "venue": "In Proceedings of SPIE Wavelets XIII, 2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonlinear total variation based noise removal algorithms", "author": ["L.I. Rudin", "S. Osher", "E. Fatemi"], "venue": "Physica D, vol. 60, no. 1-4, pp. 259\u2013268, 1992.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1992}, {"title": "Sequential minimal eigenvalues - an approach to analysis dictionary learning", "author": ["B. Ophir", "M. Elad", "N. Bertin", "M.D. Plumbley"], "venue": "European Signal Processing Conference, 2011, pp. 1465\u20131469.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "K-SVD dictionary-learning for the analysis sparse model", "author": ["R. Rubinstein", "T. Faktor", "M. Elad"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Analysis operator learning for overcomplete cosparse representations", "author": ["M. Yaghoobi", "S. Nam", "R. Gribonval", "M.E. Davies"], "venue": "European Signal Processing Conference, 2011, pp. 1470\u20131474.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Noise aware analysis operator learning for approximately cosparse signals", "author": ["\u2014\u2014"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing, 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "A continuous-time approach to the oblique procrustes problem", "author": ["N.T. Trendafilov"], "venue": "Behaviormetrika, vol. 26, no. 2, pp. 167\u2013181, 1999.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1999}, {"title": "Blind source separation with compressively sensed linear mixtures", "author": ["M. Kleinsteuber", "H. Shen"], "venue": "IEEE Signal Processing Letters, vol. 19, no. 2, pp. 107\u2013110, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Optimization Algorithms on Matrix Manifolds", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "An efficient hybrid conjugate gradient method for unconstrained optimization", "author": ["Y. Dai", "Y. Yuan"], "venue": "Annals of Operations Research, vol. 103, no. 1-4, pp. 33\u201347, 2001.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "Global convergence properties of conjugate gradient methods for optimization", "author": ["J.C. Gilbert", "J. Nocedal"], "venue": "SIAM Journal on Optimization, vol. 2, no. 1, pp. 21\u201342, 1992.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1992}, {"title": "Fields of experts: a framework for learning image priors", "author": ["S. Roth", "M. Black"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2005, pp. 860\u2013867.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Algorithms and software for total variation image reconstruction via first-order methods", "author": ["J. Dahl", "P.C. Hansen", "S. Jensen", "T.L. Jensen"], "venue": "Numerical Algorithms, vol. 53, no. 1, pp. 67\u201392, 2010.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Cartoon-like image reconstruction via constrained `p-minimization", "author": ["S. Hawe", "M. Kleinsteuber", "K. Diepold"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing, 2012, pp. 717\u2013720.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Image denoising by sparse 3-d transform-domain collaborative filtering", "author": ["K. Dabov", "A. Foi", "V. Katkovnik", "K. Egiazarian"], "venue": "IEEE Transactions on Image Processing, vol. 16, no. 8, pp. 2080\u20132095, 2007.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "Image super-resolution via sparse representation", "author": ["J. Yang", "J. Wright", "T. Huang", "Y. Ma"], "venue": "IEEE Transactions on Image Processing, vol. 19, no. 11, pp. 2861\u20132873, 2010.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Prominent examples are image denoising [1], inpainting [2], super-resolution [3], or image reconstruction from few indirect measurements as in Compressive Sensing [4].", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "Prominent examples are image denoising [1], inpainting [2], super-resolution [3], or image reconstruction from few indirect measurements as in Compressive Sensing [4].", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "Prominent examples are image denoising [1], inpainting [2], super-resolution [3], or image reconstruction from few indirect measurements as in Compressive Sensing [4].", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "Prominent examples are image denoising [1], inpainting [2], super-resolution [3], or image reconstruction from few indirect measurements as in Compressive Sensing [4].", "startOffset": 163, "endOffset": 166}, {"referenceID": 4, "context": "Basically, in all these problems the goal is to reconstruct an unknown image s \u2208 Rn as accurately as possible from a set of indirect and maybe corrupted measurements y \u2208 Rm with n \u2265 m, see [5] for a detailed introduction to inverse problems.", "startOffset": 189, "endOffset": 192}, {"referenceID": 5, "context": "[6], is that natural images admit a sparse representation x \u2208 Rd over some dictionary D \u2208 Rn\u00d7d with d \u2265 n.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "As the signal is synthesized from the sparse coefficients, the reconstruction model (3) is called the synthesis reconstruction model [7].", "startOffset": 133, "endOffset": 136}, {"referenceID": 7, "context": "For a broad overview of such algorithms, we refer the interested reader to [8].", "startOffset": 75, "endOffset": 78}, {"referenceID": 8, "context": "Popular examples include Wavelets [9], Bandlets [10], and Curvlets [11] among several others, or a concatenation of various such bases/dictionaries.", "startOffset": 34, "endOffset": 37}, {"referenceID": 9, "context": "Popular examples include Wavelets [9], Bandlets [10], and Curvlets [11] among several others, or a concatenation of various such bases/dictionaries.", "startOffset": 48, "endOffset": 52}, {"referenceID": 10, "context": "Popular examples include Wavelets [9], Bandlets [10], and Curvlets [11] among several others, or a concatenation of various such bases/dictionaries.", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "This is desirable for various image reconstruction applications as it readily improves their performance and accuracy [12]\u2013[14].", "startOffset": 118, "endOffset": 122}, {"referenceID": 13, "context": "This is desirable for various image reconstruction applications as it readily improves their performance and accuracy [12]\u2013[14].", "startOffset": 123, "endOffset": 127}, {"referenceID": 14, "context": "Three conceptually different approaches for learning a dictionary became well established, which are probabilistic ones like [15] or [16], clustering based ones such as K-SVD [17], and recent approaches which aim at learning dictionaries with specific matrix structures that allow fast computations like [18].", "startOffset": 125, "endOffset": 129}, {"referenceID": 15, "context": "Three conceptually different approaches for learning a dictionary became well established, which are probabilistic ones like [15] or [16], clustering based ones such as K-SVD [17], and recent approaches which aim at learning dictionaries with specific matrix structures that allow fast computations like [18].", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "Three conceptually different approaches for learning a dictionary became well established, which are probabilistic ones like [15] or [16], clustering based ones such as K-SVD [17], and recent approaches which aim at learning dictionaries with specific matrix structures that allow fast computations like [18].", "startOffset": 175, "endOffset": 179}, {"referenceID": 17, "context": "Three conceptually different approaches for learning a dictionary became well established, which are probabilistic ones like [15] or [16], clustering based ones such as K-SVD [17], and recent approaches which aim at learning dictionaries with specific matrix structures that allow fast computations like [18].", "startOffset": 304, "endOffset": 308}, {"referenceID": 18, "context": "For a comprehensive overview of dictionary learning techniques see [19].", "startOffset": 67, "endOffset": 71}, {"referenceID": 6, "context": "which is known as the analysis model [7].", "startOffset": 37, "endOffset": 40}, {"referenceID": 19, "context": "To emphasize this difference, the term cosparsity has been introduced in [20], which simply counts the number of zero elements of \u03a9s.", "startOffset": 73, "endOffset": 77}, {"referenceID": 20, "context": "Different analysis operators proposed in the literature include the fused Lasso [21], the translation invariant wavelet transform [22], and probably best known the finite difference operator which is closely related to the total-variation [23].", "startOffset": 80, "endOffset": 84}, {"referenceID": 21, "context": "Different analysis operators proposed in the literature include the fused Lasso [21], the translation invariant wavelet transform [22], and probably best known the finite difference operator which is closely related to the total-variation [23].", "startOffset": 130, "endOffset": 134}, {"referenceID": 22, "context": "Different analysis operators proposed in the literature include the fused Lasso [21], the translation invariant wavelet transform [22], and probably best known the finite difference operator which is closely related to the total-variation [23].", "startOffset": 239, "endOffset": 243}, {"referenceID": 6, "context": "The question is: can the performance of analysis based signal reconstruction be improved upon when a learnt analysis operator is applied instead of a predefined one, as it is the case for the synthesis model where learnt dictionaries outperform analytic dictionaries? As it has been discussed in [7], the two models differ significantly, and the na\u00efve way of learning a dictionary and simply employing its transposed or its pseudo-inverse as the learnt analysis operator fails.", "startOffset": 296, "endOffset": 299}, {"referenceID": 23, "context": "In [24], an algorithm is proposed where the rows of the analysis operator are found sequentially by identifying directions that are orthogonal to a subset of the training samples.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "An adaption of the widely known K-SVD dictionary learning algorithm to the problem of analysis operator learning is presented in [25].", "startOffset": 129, "endOffset": 133}, {"referenceID": 23, "context": "Thereby, each row of \u03a9 is updated in a similar way as described in the previous paragraph for the method of [24].", "startOffset": 108, "endOffset": 112}, {"referenceID": 23, "context": "Interestingly, the operator learnt on piecewise constant image patches by [24] and [25] closely mimics the finite difference operator.", "startOffset": 74, "endOffset": 78}, {"referenceID": 24, "context": "Interestingly, the operator learnt on piecewise constant image patches by [24] and [25] closely mimics the finite difference operator.", "startOffset": 83, "endOffset": 87}, {"referenceID": 25, "context": "In [26], the authors employ g(\u03a9S) = \u2016\u03a9S\u20161 as the sparsity promoting function and suggest a constrained optimization technique that utilizes a projected subgradient method for iteratively solving (6).", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "These constraints motivate the consideration of the set of full rank matrices with normalized columns, which admits a manifold structure known as the oblique manifold [28] OB(n, k) := {X \u2208 Rn\u00d7k| rk(X ) = n, ddiag(X>X ) = Ik}.", "startOffset": 167, "endOffset": 171}, {"referenceID": 28, "context": "[29] for a CG-type method on the oblique manifold.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "For an in-depth introduction on optimization on matrix manifolds, we refer the interested reader to [30].", "startOffset": 100, "endOffset": 104}, {"referenceID": 29, "context": "in [30].", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "which has been suggested in [31].", "startOffset": 28, "endOffset": 32}, {"referenceID": 31, "context": "as proposed in [32].", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "To overcome these drawbacks, we introduce a method related to the patch based synthesis approach presented in [12] and the MRF approach from [33], which provides global support from local information.", "startOffset": 110, "endOffset": 114}, {"referenceID": 32, "context": "To overcome these drawbacks, we introduce a method related to the patch based synthesis approach presented in [12] and the MRF approach from [33], which provides global support from local information.", "startOffset": 141, "endOffset": 145}, {"referenceID": 33, "context": "We compare all our results with a total variation based approach [34].", "startOffset": 65, "endOffset": 69}, {"referenceID": 34, "context": "For all applications we reconstructed the images by solving the minimization problem (47) using the conjugate gradient method proposed in [35].", "startOffset": 138, "endOffset": 142}, {"referenceID": 35, "context": "(b) BM3D denoising [36], PSNR = 30.", "startOffset": 19, "endOffset": 23}, {"referenceID": 11, "context": "(c) K-SVD denoising [12], PSNR = 29.", "startOffset": 20, "endOffset": 24}, {"referenceID": 33, "context": "(d) TV denoising [34], PSNR = 28.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "Table I presents a comparison of the reconstruction results achieved by our method with (i) the K-SVD approach specifically designed for image denoising [12], (ii) the currently best denoising algorithm BM3D [36], and (iii) total-variation based denoising [34].", "startOffset": 153, "endOffset": 157}, {"referenceID": 35, "context": "Table I presents a comparison of the reconstruction results achieved by our method with (i) the K-SVD approach specifically designed for image denoising [12], (ii) the currently best denoising algorithm BM3D [36], and (iii) total-variation based denoising [34].", "startOffset": 208, "endOffset": 212}, {"referenceID": 33, "context": "Table I presents a comparison of the reconstruction results achieved by our method with (i) the K-SVD approach specifically designed for image denoising [12], (ii) the currently best denoising algorithm BM3D [36], and (iii) total-variation based denoising [34].", "startOffset": 256, "endOffset": 260}, {"referenceID": 1, "context": "Image Inpainting In image inpainting as originally proposed in [2], the goal is to fill up a set of damaged or disturbing pixels such that the resulting image is visually appealing.", "startOffset": 63, "endOffset": 66}, {"referenceID": 35, "context": "EACH CELL PRESENTS FOUR PSNR RESULTS IN DB ACHIEVED BY DENOISING THE RESPECTIVE IMAGE WITH FOUR DIFFERENT ALGORITHMS, WHICH ARE: TOP LEFT BM3D [36](THE CURRENTLY BEST RATED DENOISING ALGORITHM), TOP RIGHT TOTAL-VARIATION DENOISING (BEST WORKING ANALYSIS METHOD), BOTTOM LEFT K-SVD DENOISING [12], BOTTOM RIGHT PROPOSED METHOD.", "startOffset": 143, "endOffset": 147}, {"referenceID": 11, "context": "EACH CELL PRESENTS FOUR PSNR RESULTS IN DB ACHIEVED BY DENOISING THE RESPECTIVE IMAGE WITH FOUR DIFFERENT ALGORITHMS, WHICH ARE: TOP LEFT BM3D [36](THE CURRENTLY BEST RATED DENOISING ALGORITHM), TOP RIGHT TOTAL-VARIATION DENOISING (BEST WORKING ANALYSIS METHOD), BOTTOM LEFT K-SVD DENOISING [12], BOTTOM RIGHT PROPOSED METHOD.", "startOffset": 291, "endOffset": 295}, {"referenceID": 33, "context": "The second experiment demonstrates another typical inpainting problem taken from the literature [34], where a text overlaying an image has to be removed, see Figure 6.", "startOffset": 96, "endOffset": 100}, {"referenceID": 33, "context": "For comparison, we present the TV based results from [34] in Figure 6(c).", "startOffset": 53, "endOffset": 57}, {"referenceID": 36, "context": "In Figure 7(e) and (j), we show the reconstruction results of our method for magnifying the \"Girl\" and the \"Baboon\" image by d = 3 and compare it with bicubic interpolation Figure 7(b) and (g), TV based upsampling Figure 7(d) and (i), and the SR algorithm presented in [37] Figure 7(c) and (h).", "startOffset": 269, "endOffset": 273}, {"referenceID": 36, "context": "(c) Method [37], PSNR = 32.", "startOffset": 11, "endOffset": 15}, {"referenceID": 36, "context": "(h) Method [37], PSNR = 23.", "startOffset": 11, "endOffset": 15}, {"referenceID": 36, "context": "(d) Method [37], PSNR = 22.", "startOffset": 11, "endOffset": 15}], "year": 2017, "abstractText": "Exploiting a priori known structural information lies at the core of many image reconstruction methods that can be stated as inverse problems. The synthesis model, which assumes that images can be decomposed into a linear combination of very few atoms of some dictionary, is now a well established tool for the design of image reconstruction algorithms. An interesting alternative is the analysis model, where the signal is multiplied by an analysis operator and the outcome is assumed to be the sparse. This approach has only recently gained increasing interest. The quality of reconstruction methods based on an analysis model severely depends on the right choice of the suitable operator. In this work, we present an algorithm for learning an analysis operator from training images. Our method is based on an `p-norm minimization on the set of full rank matrices with normalized columns. We carefully introduce the employed conjugate gradient method on manifolds, and explain the underlying geometry of the constraints. Moreover, we compare our approach to state-of-the-art methods for image denoising, inpainting, and single image super-resolution. Our numerical results show competitive performance of our general approach in all presented applications compared to the specialized state-of-the-art techniques.", "creator": "LaTeX with hyperref package"}}}