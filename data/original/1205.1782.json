{"id": "1205.1782", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2012", "title": "Approximate Dynamic Programming By Minimizing Distributionally Robust Bounds", "abstract": "Approximate dynamic programming is a popular method for solving large Markov decision processes. This paper describes a new class of approximate dynamic programming (ADP) methods- distributionally robust ADP-that address the curse of dimensionality by minimizing a pessimistic bound on the policy loss. This approach turns ADP into an optimization problem, for which we derive new mathematical program formulations and analyze its properties. DRADP improves on the theoretical guarantees of existing ADP methods-it guarantees convergence and L1 norm based error bounds. The empirical evaluation of DRADP shows that the theoretical guarantees translate well into good performance on benchmark problems.", "histories": [["v1", "Tue, 8 May 2012 19:22:43 GMT  (64kb,D)", "https://arxiv.org/abs/1205.1782v1", null], ["v2", "Mon, 21 May 2012 16:30:22 GMT  (62kb,D)", "http://arxiv.org/abs/1205.1782v2", "In Proceedings of International Conference on Machine Learning, 2012"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["marek petrik"], "accepted": true, "id": "1205.1782"}, "pdf": {"name": "1205.1782.pdf", "metadata": {"source": "CRF", "title": "Approximate Dynamic Programming By Minimizing Distributionally Robust Bounds", "authors": ["Marek Petrik"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Large Markov decision processes (MDPs) are common in reinforcement learning and operations research and are often solved by approximate dynamic programming (ADP). Many ADP algorithms have been developed and studied, often with impressive empirical performance. However, because many ADP methods must be carefully tuned to work well and offer insufficient theoretical guarantees, it is important to develop new methods that have both good theoretical guarantees and empirical performance.\nApproximate linear programming (ALP)\u2014an ADP method\u2014has been developed with the goal of achieving convergence and good theoretical guarantees (de Farias & van Roy, 2003). Approximate bilinear programming (ABP) improves on the theoretical properties of ALP at the cost of additional computational complexity (Petrik & Zilberstein, 2009, 2011). Both ALP and ABP provide guarantees that rely on conservative error bounds in terms of the L\u221e norm and often under-perform in practice (Petrik & Zilberstein, 2009). It is, therefore, desirable to develop ADP methods that offer both tighter bounds and better empirical performance.\nIn this paper, we propose and analyze distributionally robust approximate dynamic programming (DRADP)\u2014a new approximate dynamic programming method. DRADP improves on approximate linear and bilinear programming both in terms of theoretical properties and empirical performance. This method builds on approximate linear and bilinear programming but achieves better solution quality by explicitly optimizing tighter, less conservative, bounds stated in terms of a weighted L1 norm. In particular, DRADP computes a good solution for a given initial distribution instead of attempting to find a solution that is good for all initial distributions.\nThe objective in ADP is to compute a policy \u03c0 with the maximal return \u03c1(\u03c0). Maximizing the return also minimizes the loss with respect to the optimal policy \u03c0?\u2014known as the policy loss and defined as \u03c1(\u03c0?)\u2212 \u03c1(\u03c0). There are two main challenges in computing a good policy for a large MDP. First, it is\nar X\niv :1\n20 5.\n17 82\nv2 [\nst at\n.M L\n] 2\n1 M\nay 2\n01 2\nnecessary to efficiently evaluate its return; evaluation using simulation is time consuming and often impractical. Second, the return of a parameterized policy may be a function that is hard to optimize. DRADP addressed both these issues by maximizing a simple lower bound \u03c1\u0303(\u03c0) on the return using ideas from robust optimization. This lower bound is easy to optimize and can be computed from a small sample of the domain, eliminating the need for extensive simulation.\nMaximizing a lower bound on the return corresponds to minimizing an upper bound \u03c1(\u03c0?)\u2212 \u03c1\u0303(\u03c0) on the policy loss. The main reason to minimize an upper bound\u2014as opposed to a lower bound\u2014is that the approximation error can be bounded by the difference \u03c1(\u03c0?)\u2212 \u03c1\u0303(\u03c0?) for the optimal policy only, instead of the difference for the set of all policies, as we show formally in Section 4.\nThe lower bound on the return in DRADP is based on an approximation of the state occupancy distributions or frequencies (Puterman, 2005). The state occupancy frequency represents the fraction of time that is spent in the state and is in some sense the dual of a value function. Occupancy frequencies have been used, for example, to solve factored MDPs (Dolgov & Durfee, 2006) and in dual dynamic programming (Wang, 2007; Wang et al., 2008) (The term \u201cdual dynamic programming\u201d also refers to unrelated linear stochastic programming methods). These methods can improve the empirical performance, but proving bounds on the policy loss has proved challenging. We take a different approach to prove tight bounds on the policy loss. While the existing methods approximate the state occupancy frequencies by a subset, we approximate it by a superset.\nWe call the DRADP approach distributionally robust because it uses the robust optimization methodology to represent and simplify the set of occupancy distributions (Delage & Ye, 2010). Robust optimization is a recently revived approach for modeling uncertainty in optimization problems (Ben-Tal et al., 2009). It does not attempt to model the uncertainty precisely, but instead computes solutions that are immunized against its effects. In distributionally robust optimization, the uncertainty is in probability distributions. DRADP introduces the uncertainty in state occupancy frequencies in order to make very large MDPs tractable and uses the robust optimization approach to compute solutions that are immune to this uncertainty.\nThe remainder of the paper is organized as follows. First, in Section 2, we define the basic framework including MDPs and value functions. Then, Section 3 introduces the general DRADP method in terms of generic optimization problems. Section 4 analyzes approximation errors involved in DRADP and shows that standard concentration coefficient assumptions on the MDP (Munos, 2007) can be used to derive tighter bounds. To leverage existing mathematical programming methods, we show that DRADP can be formulated in terms of standard mathematical optimization models in Section 5. Finally, Section 6 presents experimental results on standard benchmark problems.\nWe consider the offline\u2014or batch\u2014setting in this paper, in which all samples are generated in advance of computing the value function. This setting is identical to that of LSPI (Lagoudakis & Parr, 2003) and ALP (de Farias & van Roy, 2003)."}, {"heading": "2 Framework and Notation", "text": "In this section, we define the basic concepts required for solving Markov decision processes: value functions, and occupancy frequencies. We use the following general notation throughout the paper. The symbols 0 and 1 denote vectors of all zeros or ones of appropriate dimensions respectively; the symbol I denotes an identity matrix of an appropriate dimension. The operator [\u00b7]+ denotes an element-wise non-negative part of a vector. We will often use linear algebra and expectation notations interchangeably; for example: Eu [X ] = uTx , where x is a vector of the values of the random variable X . We also\nuse RX to denote the set of all functions from a finite set X to R; note that RX is trivially a vector space.\nA Markov Decision Process is a tuple (S ,A , P, r,\u03b1). Here, S is a finite set of states,A is a finite set of actions, P : S \u00d7A \u00d7S 7\u2192 [0, 1] is the transition function (P(s, a, s\u2032) is the probability of transiting to state s\u2032 from state s given action a), and r : S \u00d7A 7\u2192 R is a reward function. The initial distribution is: \u03b1 : S 7\u2192 [0,1], such that \u2211\ns\u2208S \u03b1(s) = 1. The set of all state-action pairs isW = S \u00d7A . For the sake of simplicity, we assume that all actions can be taken in all states. To avoid technicalities that detract from the main ideas of the paper, we assume finite state and action sets but the results apply with additional compactness assumptions to infinite sets. We will use S and W to denote random variables with values in S and W .\nThe solution of an MDP is a stationary deterministic policy \u03c0 : S \u2192A , which determines the action to take in any state; the set of all deterministic policies is denoted by \u03a0D. A stationary randomized\u2014or stochastic\u2014policy \u03c0 : S \u00d7A \u2192 [0,1] assigns the probability to all actions for every state; the set of all randomized policies is denoted by \u03a0R. Clearly \u03a0D \u2286 \u03a0R holds by mapping the chosen action to the appropriate distribution. A randomized policy can be thought of as a vector on W that assigns the appropriate probabilities to each state\u2013action pair.\nFor any \u03c0 \u2208 \u03a0R, we can define the transition probability matrix and the reward vector as follows: P\u03c0(s, s\u2032) = \u2211 a\u2208A P(s, a, s \u2032) \u00b7 \u03c0(s, a) and r\u03c0(s) = \u2211\na\u2208A r(s, a) \u00b7 \u03c0(s, a). We use Pa and ra to represent values for a policy that always takes action a \u2208A . We also define a matrix A and a vector b as follows:\nAT = I\u2212 \u03b3PTa1 I\u2212 \u03b3P T a2 \u00b7 \u00b7 \u00b7 , bT = rTa1 r T a2 \u00b7 \u00b7 \u00b7\nValues A and b are usually used in approximate linear programming (ALP) (Schweitzer & Seidmann, 1985) and linear program formulations of MDPs (Puterman, 2005). The main objective in solving an MDP is to compute a policy with the maximal return.\nDefinition 2.1. The return \u03c1 : \u03a0R \u2192 R of \u03c0 \u2208 \u03a0R is defined as: \u03c1(\u03c0) = \u2211\u221e n=0\u03b1 T(\u03b3 \u00b7 P\u03c0)n r\u03c0. The optimal policy solves \u03c0? \u2208 arg max\u03c0\u2208\u03a0R \u03c1(\u03c0) and we use \u03c1 ? = \u03c1(\u03c0?).\nDRADP relies on two main solution concepts: state occupancy frequencies and value functions. State occupancy frequencies\u2014or measures\u2014intuitively represent the probability of terminating in each state when the discount factor \u03b3 is interpreted as a probability of remaining in the system (Puterman, 2005). State-action occupancy frequencies are defined for state\u2013action pairs and represent the joint probability of being in the state and taking the action.\nState occupancy frequency for \u03c0 \u2208 \u03a0R is denoted by d\u03c0 \u2208 RS and is defined as:\nd\u03c0 = (1\u2212 \u03b3) \u00b7 \u221e \u2211\nt=0\n(\u03b3 \u00b7 PT\u03c0 ) t\u03b1= (1\u2212 \u03b3) \u00b7 I\u2212 \u03b3 \u00b7 PT\u03c0 \u22121 \u03b1 .\nState-action occupancy frequency is denoted by u\u03c0 \u2208 RW (its set-valued equivalent is U(\u03c0)) and is a product of state\u2013occupancy frequencies and action probabilities:\nu\u03c0(s, a) = d\u03c0(s) \u00b7\u03c0(s, a) , U(\u03c0) = {u\u03c0} .\nNote that U(\u03c0) is a set-valued function with the output set of cardinality 1. State and state-action occupancy frequencies represent valid probability measures over S and W respectively. We use d? = d\u03c0? and u\n? = u\u03c0? to denote the optimal measures. Finally, we use u|\u03c0 \u2208 RS+ for \u03c0 \u2208 \u03a0D to denote a restriction of u to \u03c0 such that u|\u03c0(s) = u(s,\u03c0(s)).\nState-action occupancy frequencies are closely related to the set U of dual feasible solutions of the linear program formulation of an MDP, which is defined as (e.g. Section 6.9 of (Puterman, 2005)):\nU = \u00a6 u \u2208 RW+ : A Tu= (1\u2212 \u03b3) \u00b7\u03b1 \u00a9 . (2.1)\nThe following well-known proposition characterizes the basic properties of the set U .\nProposition 2.2 (e.g. Theorem 6.9 in (Puterman, 2005)). The set of occupancy frequencies satisfies the following properties.\n(i) U = \u22c3\n\u03c0\u2208\u03a0R U(\u03c0) = conv(\n\u22c3\n\u03c0\u2208\u03a0D U(\u03c0)).\n(ii) For each u\u0304 \u2208 U , define \u03c0\u2032(s, a) = u\u0304(s, a)/ \u2211 a\u2032\u2208A u\u0304(s, a \u2032). Then u\u03c0\u2032 = u\u0304.\n(iii) 1Tu= 1 for each u \u2208 U . (iv) ATu= (1\u2212 \u03b3) \u00b7\u03b1 for each u \u2208 U .\nPart (i), in particular, holds because deterministic policies represent the basic feasible solutions of the dual linear program for an MDP.\nA value function v\u03c0 \u2208 RS of \u03c0 \u2208 \u03a0R maps states to the return obtained when starting in them and is defined by:\nv\u03c0 = \u221e \u2211\nt=0\n(\u03b3 \u00b7 P\u03c0)t r\u03c0 = I\u2212 \u03b3 \u00b7 P\u03c0 \u22121 r\u03c0 .\nThe set of all possible value functions is denoted by V . It is well known that a policy \u03c0? with the value function v? is optimal if and only if v? \u2265 v\u03c0 for every \u03c0 \u2208 \u03a0R. The value function update L\u03c0 for a policy \u03c0 and the Bellman operator L are defined as: L\u03c0v = \u03b3P\u03c0v+ r\u03c0 and Lv =max\u03c0\u2208\u03a0R L\u03c0v.\nThe optimal value function v? satisfies Lv? = v?. The following proposition states the well-known connection between state\u2013action occupancy frequencies and value functions.\nProposition 2.3 (e.g. Chapter 6 in (Puterman, 2005)). For each \u03c0 \u2208 \u03a0R: \u03c1(\u03c0) = E\u03b1 v\u03c0(S) = Eu\u03c0 [r(W )]/(1\u2212 \u03b3) .\nThe value function, computed by a dynamic programming algorithm, is typically then used to derive the greedy policy. A greedy policy takes in every state an action that maximizes the expected conditional return.\nDefinition 2.4. A policy \u03c0 \u2208 \u03a0D is greedy with respect to a value function v when L\u03c0v = Lv; in other words:\n\u03c0(s) \u2208 arg max a\u2208A\nr(s, a) + \u03b3 \u00b7 \u2211\ns\u2032\u2208S\nP(s, a, s\u2032) \u00b7 v(s\u2032) ,\nfor each s \u2208 S with ties broken arbitrarily.\nMDP is a very general model. Often, specific properties of the MDP can be used to compute better solutions and to derive tighter bounds. One common assumption\u2014used to derive L2 bounds for API\u2014is a smoothness of transition probabilities (Munos, 2003), also known as the concentration coefficient (Munos, 2007); this property can be used to derive tighter DRADP bounds.\nAssumption 1 (Concentration coefficient). There exists a probability measure \u00b5 \u2208 [0, 1]S and a constant C \u2208 R+ such that for all s, s\u2032 \u2208 S and all \u03c0 \u2208 \u03a0D the transition probability is bounded as: P(s,\u03c0(s), s\u2032)\u2264 C \u00b7\u00b5(s\u2032)."}, {"heading": "3 Distributionally Robust Approximate Dynamic Programming", "text": "In this section, we formalize DRADP and describe it in terms of generic optimization problems. Practical DRADP implementations are sampled versions of the optimization problems described in this section. However, as it is common in ADP literature, we do not explicitly analyze the sampling method used with DRADP in this paper, because the sampling error can simply be added to the error bounds that we derive. The sampling is performed and errors bounded identically to approximate linear programming and approximate bilinear programming\u2014state and action samples are used to select a subset of constraints and variables (de Farias & van Roy, 2003; Petrik et al., 2010; Petrik & Zilberstein, 2011).\nThe main objective of ADP is to compute a policy \u03c0 \u2208 \u03a0R that maximizes the return \u03c1(\u03c0). Because the MDPs of interest are very large, a common approach is to simplify them by restricting the set of policies that are considered to a smaller set \u03a0\u0303 \u2286 \u03a0R. For example, policies may be constrained to take the same action in some states; or to be greedy with respect to an approximate value function. Since it is not possible to compute an optimal policy, the common objective is to minimize the policy loss. Policy loss captures the difference in the discounted return when following policy \u03c0 instead of the optimal policy \u03c0?.\nDefinition 3.1. The expected policy loss of \u03c0 \u2208 \u03a0R is defined as:\n\u03c1? \u2212\u03c1(\u03c0) = bT(u? \u2212 u\u03c0)\n1\u2212 \u03b3 = \u2016v? \u2212 v\u03c0\u20161,\u03b1 ,\nwhere \u2016 \u00b7 \u20161,\u03b1 represents an \u03b1-weighted L1 norm.\nADP relies on approximate value functions V\u0303 \u2286 V that are a subset of all value functions. In DRADP, approximate value functions are used simultaneously to both restrict the space of policies and to approximate their returns. We, in addition, define a set of approximate occupancy frequencies U\u0303(\u03c0) \u2287 U(\u03c0) that are a superset of the true occupancy frequencies. We call any element in the appropriate approximate sets representable.\nWe consider linear function approximation, in which the values for states are represented as a linear combination of nonlinear basis functions (vectors). For each s \u2208 S , we define a vector \u03c6(s) of features with |\u03c6| being the dimension of the vector. The rows of the basis matrix \u03a6 correspond to \u03c6(s), and the approximation space is generated by the columns of \u03a6. Approximate value functions and policy-dependent state occupancy measures for linear approximations are defined for some given feature matrices \u03a6u and \u03a6v as:\nV\u0303 = n v \u2208 V : v = \u03a6v x , x \u2208 R|\u03c6| o , (3.1)\nU\u0303(\u03c0) = n u \u2208 RA+ : \u03a6Tu A Tu= (1\u2212 \u03b3) \u00b7\u03a6Tu \u03b1, u(s, a)\u2264 \u03c0(s, a) o . (3.2)\nClearly, U\u0303(\u03c0) \u2287 U(\u03c0) from the definition of u\u03c0. We will assume the following important assumption without reference for the remainder of the paper.\nAssumption 2. One of the features in each of \u03c6u and \u03c6v is a constant; that is, 1= \u03a6u xu and 1= \u03a6v xv for some xu and xv .\nThe following lemma, which can be derived directly from the definition of U\u0303 and Proposition 2.2, shows the importance of Assumption 2.\nLemma 3.2. Suppose that Assumption 2 holds. Then for each \u03c0 \u2208 \u03a0R: u \u2208 U\u0303(\u03c0)\u21d2 1Tu= 1 .\nApproximate policies \u03a0\u0303 are most often represented indirectly\u2014by assuming policies that are greedy to the approximate value functions. The set G of all such greedy policies is defined by: G = { \u03c0 \u2208 \u03a0D : L\u03c0v = Lv, v \u2208 V\u0303 }. Although DRADP applies to other approximate policy sets we will particularly focus on the set \u03a0\u0303 = G .\nWe are now ready to define the basic DRADP formulation which is analyzed in the remainder of the paper.\nDefinition 3.3. DRADP computes an approximate policy by solving the following optimization problem:\narg max \u03c0\u2208\u03a0\u0303 \u03c1\u0303(\u03c0) = arg min \u03c0\u2208\u03a0\u0303 \u03c1? \u2212 \u03c1\u0303(\u03c0) , (DRADP)\nwhere the function \u03c1\u0303 : \u03a0R\u2192 R is defined by:\n\u03c1\u0303(\u03c0) =max v\u2208V\u0303 \u03b1Tv\u2212 max u\u2208U\u0303(\u03c0) uT(Av\u2212 b) 1\u2212 \u03b3 . (3.3)\nNote that the solution of (DRADP) is a policy; this policy is not necessarily greedy with respect to the optimal v in (3.3) unlike in most other ADP approaches. The expression (3.3) can be understood intuitively as follows. The first term, \u03b1Tv, represents the expected return if v is the value function of \u03c0. The second term maxu\u2208U\u0303(\u03c0)(u\nT(Av\u2212 b))/(1\u2212 \u03b3) is a penalty function, which offsets any gains when v 6= v\u03c0 and is motivated by the primal-dual slack variables in the LP formulation of the MDP. Given this interpretation, DRADP simultaneously restricts the set of value functions and upper-approximates the penalty function.\nThe following theorem states an important property of Definition 3.3, which is used to derive approximation error bounds.\nTheorem 3.4. For each \u03c0 \u2208 \u03a0R, \u03c1\u0303 lower-bounds the true return: \u03c1\u0303(\u03c0)\u2264 \u03c1(\u03c0). In addition, when \u03a6u and \u03a6v are invertible and \u03c0 \u2208 \u03a0D then \u03c1(\u03c0) = \u03c1\u0303(\u03c0).\nProof of Theorem 3.4. The theorem follows by complementary slackness from the fact that U\u0303(\u03c0) \u2287 U(\u03c0). We use that dT\u03c0 (I\u2212 \u03b3P\u03c0) = (1\u2212 \u03b3) \u00b7\u03b1 T for any \u03c0 \u2208 \u03a0R from the definition of d\u03c0 to derive:\n\u03c1(\u03c0) = E\u03b1 v\u03c0 = \u03b1Tv\u03c0 = rT\u03c0 d\u03c0 1\u2212 \u03b3 +max v\u2208V\u0303 \u03b1T \u2212 dT\u03c0 (I\u2212 \u03b3 \u00b7 P\u03c0) 1\u2212 \u03b3 v\n= rT\u03c0 d\u03c0 1\u2212 \u03b3 +max v\u2208V\u0303 \u03b1Tv\u2212 dT\u03c0 (I\u2212 \u03b3 \u00b7 P\u03c0)v 1\u2212 \u03b3\n=max v\u2208V\u0303\n\u03b1Tv\u2212 dT\u03c0 (I\u2212 \u03b3 \u00b7 P\u03c0)v \u2212 d T \u03c0 r\u03c0\n1\u2212 \u03b3\n=max v\u2208V\u0303\n\u03b1Tv\u2212 dT\u03c0 v\u2212 L\u03c0v\n1\u2212 \u03b3\n=max v\u2208V\u0303\n\u03b1Tv\u2212 uT\u03c0 (Av\u2212 b)\n1\u2212 \u03b3\n(3.2) \u2265 max\nv\u2208V\u0303 min u\u2208U\u0303(\u03c0)\n\u03b1Tv\u2212 uT (Av\u2212 b)\n1\u2212 \u03b3\n= \u03c1\u0303(\u03c0) .\nNow, assume that \u03a6u is the identity matrix. Then, using (i) of Proposition 2.2, U\u0303(\u03c0) = {u\u03c0} whenever \u03c0 \u2208 \u03a0D. Note that this does not hold for \u03c0 \u2208 \u03a0R \\\u03a0D.\nWe now show that the lower bound \u03c1\u0303 in (3.3) can be simplified in some cases by ignoring the value functions for any \u03c0 \u2208 \u03a0R; the formulation (3.3) will nevertheless be particularly useful in the theoretical analysis because it relates value functions and occupancy frequencies.\n\u03c1\u0303\u2032(\u03c0) = min u\u2208U\u0303 \u2032(\u03c0)\nuTb\n1\u2212 \u03b3 , (3.4)\nwhere U\u0303 \u2032(\u03c0) is defined equivalently to U\u0303(\u03c0) with the exception that \u03a6u = \u03a6v = \u03a6 for some \u03a6.\nProposition 3.5. When \u03a6v = \u03a6u, then \u03c1\u0303(\u03c0) = \u03c1\u0303\u2032(\u03c0). When \u03a6v 6= \u03a6u, then define U\u0303 \u2032 and \u03c1\u0303\u2032 using a new representation \u03a6\u2032 = [\u03a6v \u03a6u]. Then: \u03c1\u0303(\u03c0) = \u03c1\u0303\u2032(\u03c0).\nProof of Proposition 3.5. Recall that:\nU\u0303(\u03c0) = \u00a6 u \u2208 RA+ : \u03a6 T u A Tu= (1\u2212 \u03b3) \u00b7\u03a6Tu \u03b1, u(s, a)\u2264 \u03c0(s, a) \u00a9 .\nAssume that \u03a6 = \u03a6v = \u03a6u. First, note that u \u2208 U\u0303(\u03c0) implies that \u03a6Tu Au= (1\u2212\u03b3)\u03a6 T u \u03b1 and v \u2208 V\u0303 implies that v = \u03a6v x for some x . Then:\n\u03c1\u0303(\u03c0) =max v\u2208V\u0303 min u\u2208U\u0303(\u03c0)\n\u03b1Tv \u2212 uT(Av\u2212 b)\n1\u2212 \u03b3\n(3.1) = max\nx min\nu\u2208U\u0303(\u03c0)\n\u03b1T\u03a6x \u2212 uT(A\u03a6x \u2212 b)\n1\u2212 \u03b3\n(3.2) = max\nx min\nu\u2208U\u0303(\u03c0)\n\u03b1T\u03a6x \u2212 (1\u2212 \u03b3)\u03b1T\u03a6x\n1\u2212 \u03b3 +\nuTb\n1\u2212 \u03b3\n=max x min u\u2208U\u0303(\u03c0)\nuTb\n1\u2212 \u03b3\n= min u\u2208U\u0303(\u03c0)\nuTb\n1\u2212 \u03b3 .\nWhen \u03a6v 6= \u03a6u, define \u03a6 = [\u03a6v \u03a6u]. Then:\n\u03c1\u0303(\u03c0) (3.1) = max\nx min\nu\u2208U\u0303(\u03c0)\n\u03b1T\u03a6v x \u2212 uT(A\u03a6v x \u2212 b)\n1\u2212 \u03b3\n=max x \u03b1T\u03a6v x \u2212 max u\u2208U\u0303(\u03c0) uT(A\u03a6v x \u2212 b) 1\u2212 \u03b3\n(?) = max\nn\n\u03b1T (1\u2212 \u03b3) \u00b7\u03a6u\u03bb1 +\u03a6v x \u2212\u03c0T\u03bb2 : A \u03a6u\u03bb1 + \u03a6v x 1\u2212 \u03b3 \u2212 b \u2264 \u03bb2, \u03bb2 \u2265 0 o\n=max n\n\u03b1T[\u03a6u \u03a6v] T y \u2212\u03c0T\u03bb2 : A[\u03a6u \u03a6v]T y \u2212 b \u2264 (1\u2212 \u03b3) \u00b7\u03bb2,\u03bb2 \u2265 0\no\n(?) = min\nu\u2208U\u0303c(\u03c0)\nuTb\n1\u2212 \u03b3 ,\nwhere: U\u0303c(\u03c0) = u : [\u03a6u \u03a6v] TATu= (1\u2212 \u03b3) \u00b7 [\u03a6u \u03a6v]T\u03b1, 0\u2264 u\u2264 \u03c0 .\nAbove, (?) equalities are derived from LP duality. This shows that simply combining the features to [\u03a6u \u03a6v] leads to the same approximate return function as using them separately.\nFor the remainder of the paper assume that \u03a6v = \u03a6u since assuming that they are the same does not reduce the solution quality.\nA potential challenge with DRADP is in representing the set of approximate policies \u03a0\u0303, because a policy must generalize to all states even when computed from a small sample. Note, that for a fixed value function v in (3.3) the policy that solves min\u03c0\u2208\u03a0\u0303 \u03c1\u0303(\u03c0) is not necessarily the greedy with respect to v. The following representation theorem, however, shows that when the set of representable policies \u03a0\u0303 is sufficiently rich, then the computed policy will be greedy with respect to a representable value function.\nTheorem 3.6. Suppose that \u03a0\u0303\u2287 G . Then: (i) max\u03c0\u2208\u03a0\u0303 \u03c1\u0303(\u03c0) =max\u03c0\u2208G \u03c1\u0303(\u03c0).\n(ii) \u2203\u03c0\u0304 \u2208 argmax\u03c0\u2208\u03a0\u0303 \u03c1\u0303(\u03c0) such that \u03c0\u0304 \u2208 G .\nProof of Theorem 3.6. Recall, that given Proposition 3.5, we can assume the simple representation of the return \u03c1\u2032(\u03c0). For a fixed \u03c0 \u2208 \u03a0R the optimization in \u03c1\u2032(\u03c0) over u represents the following linear program:\nmin u\nuTb/(1\u2212 \u03b3)\ns.t. \u03a6TATu= (1\u2212 \u03b3) \u00b7\u03a6T\u03b1\n0\u2264 u\u2264 \u03c0 .\nThe dual of this linear program is:\nmax \u03bb1,\u03bb2 \u03b1T\u03a6\u03bb1 \u2212\u03c0T\u03bb2 s.t. A\u03a6\u03bb1 \u2264 b+ (1\u2212 \u03b3) \u00b7\u03bb2\n\u03bb2 \u2265 0 .\n(3.5)\nNow, suppose a fixed \u03bb1 in (3.5) and let v = \u03a6\u03bb1. Then, \u03c0 \u2265 0 implies there exists one optimal \u03bb2 such that:\n\u03bb2(s, a) =\n\nv(s)\u2212 \u03b3 \u00b7 \u2211\ns\u2032\u2208S\nP(s, a, s\u2032)v(s\u2032)\u2212 r(s, a)\n\n\n+\n\u2200s \u2208 S , a \u2208A .\nThis means that (3.5) is equivalent to:\nmax \u03bb1\n\u03b1T\u03a6\u03bb1 \u2212 \u03c0T A\u03a6\u03bb1 \u2212 b +\n1\u2212 \u03b3\nNow, consider policies \u03c01,\u03c02 \u2208 \u03a0R that are identical in all states except in s \u2208 S . That is \u03c01(s) = a1,\u03c02(s) = a2 and assume that a1 is the greedy action. To simplify the notation, we are assuming that\na2 can represent a randomized action. Then:\n\u03b3 \u00b7 \u2211\ns\u2032\u2208S\nP(s, a1, s \u2032) \u00b7 v(s\u2032)\u2212 r(s, a1)\nD 2.4 \u2265 \u03b3 \u00b7 \u2211\ns\u2032\u2208S\nP(s, a2, s \u2032) \u00b7 v(s\u2032)\u2212 r(s, a2) ,\nv(s)\u2212 \u03b3 \u00b7 \u2211\ns\u2032\u2208S\nP(s, a1, s \u2032) \u00b7 v(s\u2032)\u2212 r(s, a1)\u2264 v(s)\u2212 \u03b3 \u00b7\n\u2211\ns\u2032\u2208S\nP(s, a2, s \u2032) \u00b7 v(s\u2032)\u2212 r(s, a2) ,\n\nv(s)\u2212 \u03b3 \u00b7 \u2211\ns\u2032\u2208S\nP(s, a1, s \u2032) \u00b7 v(s\u2032)\u2212 r(s, a1)\n\n\n+\n\u2264\n\nv(s)\u2212 \u03b3 \u00b7 \u2211\ns\u2032\u2208S\nP(s, a2, s \u2032) \u00b7 v(s\u2032)\u2212 r(s, a2)\n\n\n+\n,\n\u03c0T1 A\u03a6\u03bb1 \u2212 b + \u2264 \u03c0 T 2 A\u03a6\u03bb1 \u2212 b + ,\n\u03c1\u0303\u2032(\u03c01)\u2265 \u03c1\u0303\u2032(\u03c02) .\nApplying this argument inductively to every state, one can construct a policy \u03c0 \u2208 G \u2229 \u03a0\u0303 with respect to the approximate value function. Because this argument applies to each state individually, it also applies to DRADP from an incomplete sample.\nNote that the assumption \u03a0\u0303 \u2287 G simply implies that DRADP can select a policy that is greedy with respect to any approximate value function v \u2208 V\u0303 . This is an implicit assumption in many ADP algorithms, including ALP and LSPI. We state the assumption explicitly to indicate results that do not hold in case there are additional restrictions on the set of policies that is considered.\nTheorem 3.6 implies that it is only necessary to consider policies that are greedy with respect to representable value functions which is the most common approach in ADP. However, other approaches for representing policies may have better theoretical or empirical properties and should be also studied."}, {"heading": "4 Approximation Error Bounds", "text": "This section describes the a priori approximation properties of DRADP solutions; these bounds can be evaluated before a solution is computed. We focus on several types of bounds that not only show the performance of the method, but also make it easier to theoretically compare DRADP to existing ADP methods. These bounds show that DRADP has stronger theoretical guarantees than most other ADP methods. The first bound mirrors some simple bounds for approximate policy iteration (API) in terms of the L\u221e norm (Munos, 2007):\nlim sup k\u2192\u221e \u2016v? \u2212 v\u03c0k\u2016\u221e \u2264 2 \u00b7 \u03b3 (1\u2212 \u03b3)2 limsup k\u2192\u221e \u03b5k , (4.1)\nwhere \u03c0k and \u03b5k are the policy and L\u221e approximation error at iteration k.\nTheorem 4.1. Suppose that \u03a0\u0303\u2287 G and that \u03c0\u0304 \u2208 argmax\u03c0\u2208\u03a0\u0303 \u03c1\u0303(\u03c0) in (DRADP). The policy loss \u03c1?\u2212\u03c1(\u03c0\u0304) is then bounded as:\n\u2016v? \u2212 v\u03c0\u0304\u20161,\u03b1 \u2264 2\n1\u2212 \u03b3 min v\u2208V\u0303 \u2016v \u2212 Lv\u2016\u221e . (4.2)\nProof of Theorem 4.1. Assume that \u03c0\u0304 is an optimal solution of (DRADP). First, the policy loss can be\nexpressed in terms of approximate values and occupancy frequencies:\n\u03c1? \u2212\u03c1(\u03c0\u0304) T 3.4 \u2264 \u03c1? \u2212 \u03c1\u0303(\u03c0\u0304) =min\n\u03c0\u2208\u03a0\u0303 ( \u03c1? \u2212 \u03c1\u0303(\u03c0) )\nG\u2286\u03a0\u0303 \u2264 min\n\u03c0\u2208G ( \u03c1? \u2212 \u03c1\u0303(\u03c0) )\n=min v\u2208V\u0303 min \u03c0\u2208G max u\u2208U\u0303(\u03c0) uT(Av\u2212 b) 1\u2212 \u03b3 +\u03b1T(v? \u2212 v)\nP 2.2 =\n1\n1\u2212 \u03b3 min v\u2208V\u0303 min \u03c0\u2208G max u\u2208U\u0303(\u03c0)\nuT(Av\u2212 b) + (u?)TA(v? \u2212 v)\nP 2.3 =\n1\n1\u2212 \u03b3 min v\u2208V\u0303 min \u03c0\u2208G max u\u2208U\u0303(\u03c0)\nuT(Av\u2212 b) + (u?)T(b\u2212 Av)\n(3.2) \u2264 1\n1\u2212 \u03b3 min v\u2208V\u0303 min \u03c0\u2208G max u1,u2\u2208U\u0303(\u03c0) uT1 (Av\u2212 b) + u T 2 (b\u2212 Av)\nui\u22650 \u2264 1\n1\u2212 \u03b3 min v\u2208V\u0303 max u1,u2\u2208U\u0303(\u03c0) uT1 [Av\u2212 b]+ + u T 2 [b\u2212 Av]+\n\u2264 2\n1\u2212 \u03b3 min v\u2208V\u0303 max u\u2208U\u0303(\u03c0)\nuT|Av\u2212 b| .\nThen, the loss can be upper bounded by choosing the greedy policy:\n\u03c1? \u2212\u03c1(\u03c0\u0304)\u2264 2\n1\u2212 \u03b3 min v\u2208V\u0303 min \u03c0\u2208G max u\u2208U\u0303(\u03c0)\nuT|Av\u2212 b|\n\u2264 2\n1\u2212 \u03b3 min v\u2208V\u0303 min \u03c0\u2208G max u\u2208U\u0303(\u03c0) u|T\u03c0 |v\u2212 L\u03c0v|\n\u2264 2\n1\u2212 \u03b3 min v\u2208V\u0303 max u\u2208U\u0303(\u03c0) u|T\u03c0 |v \u2212 Lv| ,\nwhere \u03c0 is a policy greedy with respect to v when not specified otherwise. Finally, the bound is established by relaxing the constraints on u:\n\u03c1? \u2212\u03c1(\u03c0\u0304) L 3.2 \u2264 2\n1\u2212 \u03b3 min v\u2208V\u0303 max u\nn\nu|T\u03c0 |v\u2212 Lv| : 1 Tu= 1,0\u2264 u\u2264 \u03c0\no\n\u2264 2\n1\u2212 \u03b3 min v\u2208V\u0303 max u\nn\nu|T\u03c0 |v \u2212 Lv| : 1 Tu\u2264 1, u\u2265 0\no\n= 2\n1\u2212 \u03b3 min v\u2208V\u0303 max u\nn u|T\u03c0 |v \u2212 Lv| : \u2016u\u20161 \u2264 1 o\n(?) \u2264 2\n1\u2212 \u03b3 min v\u2208V\u0303 \u2016v\u2212 Lv\u2016\u221e .\nThe inequality (?) follows by the Holder\u2019s inequality.\nTheorem 4.1 highlights several advantages of the DRADP bound (4.2) over (4.1): 1) it bounds the expected loss \u2016v? \u2212 v\u03c0\u0304\u20161,\u03b1 instead of the worst-case loss \u2016v? \u2212 v\u03c0\u0304\u2016\u221e, 2) it is smaller by a factor of 1/(1\u2212\u03b3), 3) it holds in finite time instead of a limit, and 4) its right-hand side is with respect to the best approximation of the optimal value function instead of the worst case approximation over all iteration.\nIn comparison with approximate linear programming bounds, (4.2) bounds the true policy loss and not simply the approximation of v? (de Farias & van Roy, 2003). The limitation of (4.2), however, is that it relies on an L\u221e norm which can be quite conservative. We address this issue in two ways. First, we prove a bound of a different type.\nTheorem 4.2. Suppose that \u03a0\u0303 \u2287 G and that \u03c0\u0304 \u2208 argmax\u03c0\u2208\u03a0\u0303 \u03c1\u0303(\u03c0) in (DRADP). Then, the policy loss \u03c1? \u2212\u03c1(\u03c0\u0304) is bounded as:\n\u2016v? \u2212 v\u03c0\u0304\u20161,\u03b1 \u2264 min v\u2208V\u0303 ,v\u2264v? \u2016v \u2212 v?\u20161,\u03b1 . (4.3)\nProof of Theorem 4.2. Assume that \u03c0\u0304 is an optimal solution of (DRADP). Then:\n\u03c1? \u2212\u03c1(\u03c0\u0304) T 3.4 \u2264 \u03c1? \u2212 \u03c1\u0303(\u03c0\u0304)\u2264 min\n\u03c0\u2208\u03a0R ( \u03c1? \u2212 \u03c1\u0303(\u03c0) \u2264 \u03c1? \u2212 \u03c1\u0303(\u03c0?) .\nNow, using the dual representation of \u03c1\u0303(\u00b7) as in Theorem 3.6 (see also Theorem 5.1 below) we get:\n\u03c1? \u2212\u03c1(\u03c0\u0304)\u2264min \u00a6 \u03c1? \u2212\u03b1T\u03a6\u03bb1 \u2212\u03bbT2 \u03c0 ? : (1\u2212 \u03b3) \u00b7\u03bb2 \u2265 A\u03a6\u03bb1 \u2212 b, \u03bb2 \u2265 0 \u00a9 .\nIt can be readily shown by subtracting 1 from \u03a6\u03bb1 that there exists an optimal \u03bb1,\u03bb2 such that \u03bbT2 \u03c0 ? = 0 (see (ii) of Theorem 5.1). Then:\n\u03c1? \u2212\u03c1(\u03c0\u0304)\u2264min \u00a6 \u03c1? \u2212\u03b1T\u03a6\u03bb1 : (1\u2212 \u03b3) \u00b7\u03bb2 \u2265 A\u03a6\u03bb1 \u2212 b, \u03bb2 \u2265 0, \u03bbT2 \u03c0 ? = 0 \u00a9\n=min \u00a6 \u03c1? \u2212\u03b1T\u03a6\u03bb1 : (I\u2212 \u03b3 \u00b7 P\u03c0?)\u03a6\u03bb1 \u2264 r\u03c0? \u00a9 =min \u00a6 \u03c1? \u2212\u03b1Tv : (I\u2212 \u03b3 \u00b7 P\u03c0?)v \u2264 r\u03c0? , v \u2208 V\u0303 \u00a9\nL A.2 = min \u00a6 \u03c1? \u2212\u03b1Tv : v \u2264 I\u2212 \u03b3 \u00b7 P\u03c0? \u22121 r\u03c0? , v \u2208 V\u0303 \u00a9\nP 2.3 = min \u00a6 \u03b1Tv? \u2212\u03b1Tv : v \u2264 v?, v \u2208 V\u0303 \u00a9 .\nThe bound (4.3), unlike bounds in most ADP algorithms, does not contain a factor of 1/(1\u2212 \u03b3) of any power. Although (4.3) does not involve an L\u221e norm, it does require that v \u2264 v? which may be undesirable. Next, we show bounds that rely purely on weighted norms under additional assumptions on the concentration coefficient.\nAs mentioned above, Assumption 1 can be used to improve the solutions of DRADP and to derive tighter bounds. Note that this assumption must be known in advance and cannot be gleaned from the samples. To this end, for some fixed C \u2208 R+ and \u00b5 \u2208 RS in Assumption 1, define:\nU\u0303S(\u03c0) =\n(\nu \u2208 U\u0303(\u03c0) : \u2211\na\u2208A u(s, a)\u2264 \u03c3(s), \u2200s \u2208 S\n)\n,\n\u03c3(s) = \u03b3 \u00b7\u00b5(s) + (1\u2212 \u03b3) \u00b7\u03b1(s),\n\u03c1\u0303S(\u03c0) =max v\u2208V\u0303 min u\u2208U\u0303S(\u03c0)\n\u03b1Tv\u2212 uT(Av\u2212 b)\n1\u2212 \u03b3\n.\nThese assumptions imply the following structure of all admissible state frequencies.\nLemma 4.3. Suppose that Assumption 1 holds with constants C and \u00b5. Then: d \u2264 C \u00b7\u03c3 for each d \u2208 U\u0303S(\u03c0) and \u03c0 \u2208 \u03a0R.\nProof of Lemma 4.3. Suppose any \u03c0 \u2208 \u03a0R and its occupancy frequency u. Then from the definition of u:\nd(s) = \u2211\na\u2208A u(s, a) = \u03b3 \u00b7\n\u2211\ns\u2032\u2208S a\u2032\u2208A\nP(s\u2032, a\u2032, s) \u00b7 u(s\u2032, a\u2032) + (1\u2212 \u03b3) \u00b7\u03b1(s)\nu\u22650 \u2264 \u03b3 \u00b7 \u2211\ns\u2032\u2208S a\u2032\u2208A\nC \u00b7\u00b5(s) \u00b7 u(s\u2032, a\u2032) + (1\u2212 \u03b3) \u00b7\u03b1(s)\n\u2264 \u03b3 \u00b7 C \u00b7\u00b5(s)\n\u2211\ns\u2032\u2208S ,a\u2032\u2208A\nu(s\u2032, a\u2032) + (1\u2212 \u03b3) \u00b7\u03b1(s)\nP 2.2 \u2264 \u03b3 \u00b7 C \u00b7\u00b5(s) + (1\u2212 \u03b3) \u00b7\u03b1(s)\u2264 C \u00b7\u03c3(s) ,\nwhich implies that U\u0303S(\u03c0)\u2287 U(\u03c0) for any \u03c0 \u2208 \u03a0R . Therefore \u03c1\u0303S(\u03c0)\u2264 \u03c1(\u03c0) from Theorem 3.4.\nThe following theorem shows a tighter bound on the DRADP policy loss for MDPs that satisfy the smoothness assumption.\nTheorem 4.4. Suppose that Assumption 1 holds with constants C and \u00b5, \u03a0\u0303\u2287 G , and that \u03c0\u0304 \u2208 arg max\u03c0\u2208\u03a0\u0303 \u03c1\u0303(\u03c0) in (DRADP). Then, the loss of \u03c0\u0304 is bounded as:\n\u03c1? \u2212\u03c1(\u03c0\u0304)\u2264 2 \u00b7 C 1\u2212 \u03b3 min v\u2208V\u0303 \u2016v\u2212 Lv\u20161,\u03c3 . (4.4)\nProof of Theorem 4.4. Using the same derivation as in the proof of Theorem 4.1, we have that:\n\u03c1? \u2212\u03c1(\u03c0\u0304)\u2264 2\n1\u2212 \u03b3 min v\u2208V\u0303 max u\nn u|T\u03c0 |v\u2212 Lv| : u \u2208 U\u0303S(\u03c0) o\nThen, the bound can be derived from the assumption as follows:\n\u03c1? \u2212\u03c1(\u03c0\u0304) L 4.3 \u2264 2\n1\u2212 \u03b3 min v\u2208V\u0303 max u\nn\nu|T\u03c0 |v\u2212 Lv| : 0\u2264 u, \u2211\na\u2208A u(s, a)\u2264 C \u00b7\u03c3(s)\no\n\u2264 2 \u00b7 C 1\u2212 \u03b3 min v\u2208V\u0303 max u n u|T\u03c0 diag(\u03c3)|v\u2212 Lv| : 0\u2264 u, \u2211 a\u2208A u(s, a)\u2264 1 o \u2264 2 \u00b7 C 1\u2212 \u03b3 min v\u2208V\u0303 max d n dT diag(\u03c3)|v\u2212 Lv| : \u2016d\u2016\u221e \u2264 1 o\n(?) \u2264 2 \u00b7 C 1\u2212 \u03b3 min v\u2208V\u0303 \u2016v \u2212 Lv\u20161,\u03c3.\nThe inequality (?) follows by Holder\u2019s inequality.\nThe bound in Theorem 4.4 is similar to comparable Lp bounds for API (Munos, 2003), except it relies on a weighted L1 norm instead of the L2 norm and preserves all the advantages of Theorem 4.1.\nTheorem 4.4 exploits that the set of occupancy frequencies is restricted under the smoothness assumption which leads to a tighter lower bound \u03c1\u0303S on the return.\nFinally, DRADP is closely related to robust ABP (Petrik & Zilberstein, 2009, 2011) but provides several significant advantages. First, DRADP does not require transitive feasible (Petrik & Zilberstein, 2011) value functions, which simplifies the use of constraint generation. Second, ABP minimizes L\u221e bounds \u03c1r : \u03a0R \u2192 R on the policy loss, which can be too conservative. In fact, it is easy to show that DRADP solutions can be better than ABP solutions by an arbitrarily large factor."}, {"heading": "5 Computational Models", "text": "In this section, we describe how to solve the DRADP optimization problem. Since DRADP generalizes ABP (Petrik & Zilberstein, 2009), it is necessarily NP complete to solve in theory, but relatively easy to solve in practice. Note that the NP-completeness is in terms of the number of samples and features and not in the number of states or actions of the MDP. In addition, the NP completeness a is favorable property when compared to API algorithms, such as LSPI, which may never converge (Lagoudakis & Parr, 2003).\nTo solve DRADPs in practice, we derive bilinear and mixed integer linear program formulations for which many powerful solvers have been developed. These formulations lead to anytime solvers\u2014even approximate solutions result in valid policies\u2014and can therefore easily trade off solution quality with time complexity.\nTo derive bilinear formulations of DRADP, we represent the set of policies \u03a0\u0303 using linear equalities as: \u03a0\u0303 = \u00a6 \u03c0 \u2208 [0, 1]W : \u2211\na\u2208A \u03c0(s, a) = 1 \u00a9\n. This set can be defined using matrix notation as B\u03c0 = 1 and \u03c0 \u2265 0, where B : |S | \u00d7 |W | is defined as: B(s\u2032, (s\u2032, a)) = 1 when s = s\u2032 and 0 otherwise. Clearly \u03a0\u0303\u2287 G , which implies that the computed policy is greedy with respect to a representable value function from Theorem 3.6 even as sampled. It would be easy to restrict the set \u03a0\u0303 by assuming the same action must be taken in a subset of states: one would add constraints \u03c0(s, a) = \u03c0(s\u2032, a) for some s, s\u2032 \u2208 S and all a \u2208A .\nWhen the set of approximate policies is represented by linear inequalities, the DRADP optimization problem can be formulated as the following separable bilinear program (Horst & Tuy, 1996).\nmax \u03c0,\u03bb1,\u03bb2 \u03b1T\u03a6\u03bb1 \u2212\u03c0T\u03bb2 s.t. B\u03c0= 1, \u03c0\u2265 0, \u03bb2 \u2265 0,\n(1\u2212 \u03b3) \u00b7\u03bb2 \u2265 A\u03a6\u03bb1 \u2212 b .\n(5.1)\nBilinear programs are a generalization of linear programs and are in general NP hard to solve.\nTheorem 5.1. Suppose that \u03a0\u0303\u2287 G . Then the sets of optimal solutions of (5.1) and (DRADP) are identical and there exists an optimal solution (\u03c0\u0304, \u03bb\u03041, \u03bb\u03042) of (5.1) such that:\n(i) \u03c0\u0304 is deterministic and greedy with respect to \u03a6\u03bb\u03041, (ii) \u03c0\u0304T\u03bb\u03042 = 0.\nProof of Theorem 5.1. The equivalence of the sets of optimal solutions follows directly from the dual representation from the proof of Theorem 3.6. We next prove the two properties separately:\n(i) Follows directly from (ii) of Theorem 3.6. (ii) To prove by contradiction that there is an optimal solution \u03c0 with \u03c0T\u03bb2 = 0, assume an optimal\nsolution \u03c0 such that \u03c0T\u03bb2 > 0. Since \u03c0 \u2265 0, \u03bb2 \u2265 0, there exists an index i (when treating \u03c0 as\na simple vector), such that \u03c0(i) = 1 and \u03bb2(i)> 0 (from the integrality of \u03c0). We then show that there exists a solution with a lower value \u03c0T\u03bb2, smaller number of j such that \u03bb2( j)> 0, without decreasing the objective value. From the optimality of \u03bb2 and the positivity of \u03bb2(i), we have that (the constraint in (5.1) is active):\n(1\u2212 \u03b3) \u00b7\u03bb2(i) = aTi \u03a6\u03bb1 \u2212 bi > 0,\nwhere aTi is the i-th row of the matrix A and bi is the i-th element of the vector b. Now, from Assumption 2, there exists \u03bb\u20321 such that:\n\u03a6\u03bb\u20321 = \u03a6\u03bb1 \u2212\u03bb2(i) \u00b7 1.\nThis \u03bb1 is trivially feasible in (5.1). Let (1\u2212 \u03b3) \u00b7 \u03bb\u20322 = A\u03a6\u03bb\u20321 \u2212 b + . Now using Lemma A.1 it is readily seen that: A1= (1\u2212 \u03b3) \u00b7 1. Then:\nA\u03a6\u03bb\u20321 \u2212 b= A\u03a6\u03bb1 \u2212 b\u2212\u03bb2(i) \u00b7 1, A\u03a6\u03bb\u20321 \u2212 b\u2264 A\u03a6\u03bb1 \u2212 b,\nA\u03a6\u03bb\u20321 \u2212 b + \u2264 A\u03a6\u03bb1 \u2212 b + ,\n\u03bb\u20322 \u2264 \u03bb2.\nIn addition, \u03bb\u20322(i) = 0, and thus: \u03bb \u2032 2 \u2264 \u03bb2 \u2212\u03bb2(i). Then:\n\u03b1T\u03a6\u03bb\u20321 \u2212\u03c0 T\u03bb\u20322 \u2265 \u03b1 T\u03a6\u03bb1 \u2212\u03bb2(i)\u2212\u03c0T\u03bb2 +\u03bb2(i)\u2265 \u03b1T\u03a6\u03bb1 \u2212\u03c0T\u03bb2 .\nTherefore \u03bb\u20321,\u03bb \u2032 2 are also optimal with one less nonzero index i such that \u03c0(i)> 0 and \u03bb \u2032 2(\u03c0)> 0. The claim then follows by choosing i with the maximal \u03bb2(i).\nBecause there are few, if any, industrial solvers for bilinear programs, we reformulate (5.1) as a mixed integer linear program (MILP). Any separable bilinear program can be formulated as a MILP (Horst & Tuy, 1996), but such generic formulations are impractical because they lead to large MILPs with weak linear relaxations. Instead, we derive a more compact and structured MILP formulation that exploits the existence of optimal deterministic policies in DRADP (see (i) of Theorem 5.1) and is based on McCormic inequalities on the bilinear terms (Linderoth, 2005). To formulate the MILP, assume a given upper bound \u03c4 \u2208 R on any optimal solution \u03bb2? of (5.1) such that \u03c4 \u2265 \u03bb2?(s, a) for all s \u2208 S and a \u2208A . Then:\nmax z,\u03c0,\u03bb1,\u03bb2\n\u03b1T\u03a6\u03bb1 \u2212 1Tz\ns.t. z \u2265 \u03bb2 \u2212\u03c4 \u00b7 (1\u2212\u03c0),\n(1\u2212 \u03b3) \u00b7\u03bb2 \u2265 A\u03a6\u03bb1 \u2212 b, B\u03c0= 1, \u03c0 \u2208 {0,1}|S ||A |\nz \u2265 0, \u03bb2 \u2265 0 .\n(5.2)\nTheorem 5.2. Suppose that \u03a0\u0303\u2287 G and (\u03c0\u0304, \u03bb\u03041, \u03bb\u03042, z\u0304) is an optimal solution of (5.2). Then, (\u03c0\u0304, \u03bb\u03041, \u03bb\u03042) is an optimal solution of (5.1) with the same objective value given that \u03c4 > \u2016\u03bb\u03042\u2016\u221e.\nProof of Theorem 5.2. We use McCormic inequalities, as described in (Linderoth, 2005). In particular, assume that (x , y) \u2208 {(x , y) : lx \u2264 x \u2264 ux , l y \u2264 y \u2264 uy}. Then the following holds:\nx \u00b7 y \u2265max{l y \u00b7 x + lx \u00b7 y \u2212 lx \u00b7 l y , uy \u00b7 x + ux \u00b7 y \u2212 ux \u00b7 uy} x \u00b7 y \u2264min{uy \u00b7 x + lx \u00b7 y \u2212 lx \u00b7 uy , l y \u00b7 x + ux \u00b7 y \u2212 ux \u00b7 l y} .\nTo simplify the notation, we use a single index i \u2208 W to denote the individual elements of \u03c0,\u03bb2, z. From the assumptions of the theorem and the constraints on policies, we have that: \u03c0(i) \u2208 [0,1] and \u03bb2 \u2208 [0,\u03c4]. Then for all i the McCormic inequalities for (5.1) are:\n\u03c0(i) \u00b7\u03bb2(i)\u2265max{0,\u03bb2(i)\u2212\u03c4 \u00b7 (1\u2212\u03c0(i))} \u03c0(i) \u00b7\u03bb2(i)\u2264min{\u03c4 \u00b7\u03c0(i),\u03bb2(i)}.\nSince any optimal z in (5.2) satisfies z(i) = max{0,\u03bb2(i)\u2212 \u03c4 \u00b7 (1\u2212\u03c0(i))}, z(i) is a lower bound on \u03c0(i) \u00b7\u03bb(i). As a result, for any \u03c0 \u2208 \u03a0R, the objective of (5.2) is an upper bound on the objective of (5.1). Then, it can be readily shown that whenever \u03c0(i) \u2208 {0, 1} then \u03c0(i) = \u03bb2(i)\u03c0(i): when \u03c0(i) = 1 then z(i) = \u03bb2(i) = \u03c0(i)\u00b7\u03bb2(i) and when \u03c0(i) = 0 then z(i) =max{0,\u03bb2(i)\u2212\u03c4\u00b7(1\u2212\u03c0(i))}= 0= \u03c0(i)\u00b7\u03bb2(i). This shows the equality of the sets of optimal solutions stated in the theorem.\nAs discussed above, any practical implementation of DRADP must be sample-based. The bilinear program (5.1) is constructed from samples very similarly to ALPs (e.g. Sec 6 of (de Farias & van Roy, 2003)) and identically to ABPs (e.g. Sec 6 of (Petrik & Zilberstein, 2011)). Briefly, the formulation involves only the rows of A that correspond to transitions of sampled state-action pairs and b entries are estimated from the corresponding rewards. As a result, there is one \u03bb1 variable for each feature, and \u03bb2 and \u03c0 are nonzero only for the sampled rows of A (zeros do not need to be considered). The size of the optimization problem (5.1) is then independent of the number of states and actions of the MDP; it depends only on the number of samples and features."}, {"heading": "6 Experimental Results", "text": "In this section, we experimentally evaluate the empirical performance of DRADP. We present results on the inverted pendulum problem\u2014a standard benchmark problem\u2014and a synthetic chain problem. We gather state and action samples in advance and solve MILP (5.2) using IBM CPLEX 12.2. We then compare the results to three related methods which work on offline samples: 1) LSPI (Lagoudakis & Parr, 2003), 2) ALP (de Farias & van Roy, 2003), and 3) ABP (Petrik & Zilberstein, 2009). While solving the MILP formulation of DRADP is NP hard (in the number of features and samples), this does not mean that the computation takes longer than for other ADP methods; for example, the computation time of LSPI is unbounded in the worst case (there are no convergence guarantees). In the experiments, we restrict the computation time for all methods to 60s.\nInverted Pendulum The goal in the inverted pendulum benchmark problem is to balance an inverted pole by accelerating a cart in either of two directions (Lagoudakis & Parr, 2003). There are three actions that represent applying the force of u = \u221250N , u = 0N , and u = 50N to the cart with a uniform noise between \u221210N and 10N . The angle of the inverted pendulum is governed by a differential equation. We used the standard features for this benchmark problem for all the methods: 9 radial basis functions arranged in a grid over the 2-dimensional state space with centers \u00b5i and a constant term required\nby Assumption 2. The problem setting, including the initial distribution is identical to the setting in (Lagoudakis & Parr, 2003).\nFig. 1 shows the number of balancing steps (with a 3000-step bound) for each method as a function of the number of training samples averaged over 5 runs. The figure does not show error bars for clarity; the variance was close to 0 for DRADP. The results indicate that DRADP computes a very good solution for even a small number of samples and significantly outperforms LSPI. Note the poor performance of ABP and ALP with the 10 standard features; better results have been obtained with large and different feature spaces (Petrik et al., 2010) but even these do not match DRADP. The solution quality of ABP decreases with more samples, because the bounds become more conservative and the optimization problems become harder to solve.\nChain Problem Because the solution quality of ADP methods depends on many factors, good results on a single benchmark problem do not necessarily generalize to other domains. We, therefore, compare DRADP to other methods on a large number of randomly generated chain problems. This problem consists of 30 states s1 . . . s30 and 2 actions: left and right with 10% chance of moving the opposite way. The features are 10 orthogonal polynomials. The rewards are 0 except: r(s2) = \u221250, r(s3) = 4, r(s4) = \u221250, r(s20) = 10. Fig. 2 shows the results of 1000 instantiations with randomly chosen initial distributions and indicates that DRADP significantly outperforms other methods including API (a simple version of LSPI)."}, {"heading": "7 Conclusion", "text": "This paper proposes and analyzes DRADP\u2014a new ADP method. DRADP is based on a mathematical optimization formulation\u2014like ALP\u2014but offers significantly stronger theoretical guarantees and better empirical performance. The DRADP framework also makes it easy to improve the solution quality by incorporating additional assumptions on state occupation frequencies, such as the small concentration coefficient. Given the encouraging theoretical and empirical properties of DRADP, we hope it will lead to better methods for solving large MDPs and will help to deepen the understanding of ADP.\nAcknowledgements I thank Dan Iancu and Dharmashankar Subramanian for the discussions that inspired this paper. I also thank the anonymous ICML 2012 and EWRL 2012 reviewers for their detailed comments."}, {"heading": "A Basic Properties of Value Functions", "text": "Lemma A.1. For any v \u2208 V the following holds: L(v + k1) = Lv + \u03b3k1. In addition, the sets of greedy policies with respect to v and v+ k1 are identical.\nLemma A.2. The operators P and (I\u2212 \u03b3P)\u22121 are monotonous for any stochastic matrix P:\nx \u2265 y \u21d2 P x \u2265 P y\nx \u2265 y \u21d2 (I\u2212 \u03b3P)\u22121 x \u2265 (I\u2212 \u03b3P)\u22121 y\nfor all x and y.\nLemma A.3. Suppose that v \u2208 V satisfies v \u2265 Lv. Then v \u2265 v?.\nLemma A.4. Each v \u2208 V satisfies: v\u2212 Lv \u2264 v \u2212 L\u03c0v."}], "references": [{"title": "Robust Optimization", "author": ["Ben-Tal", "Aharon", "Ghaoui", "Laurent El", "Nemirovski", "Arkadi"], "venue": null, "citeRegEx": "Ben.Tal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ben.Tal et al\\.", "year": 2009}, {"title": "The linear programming approach to approximate dynamic programming", "author": ["de Farias", "Daniela P", "van Roy", "Ben"], "venue": "Operations Research,", "citeRegEx": "Farias et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Farias et al\\.", "year": 2003}, {"title": "Distributionally robust optimization under moment uncertainty with application to data driven problems", "author": ["Delage", "Eric", "Ye", "Yinyu"], "venue": "Operations Research,", "citeRegEx": "Delage et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Delage et al\\.", "year": 2010}, {"title": "Symmetric approximate linear programming for factored MDPs with application to constrained problems", "author": ["Dolgov", "Dmitri", "Durfee", "Edmund"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "Dolgov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dolgov et al\\.", "year": 2006}, {"title": "Global optimization: Deterministic approaches", "author": ["Horst", "Reiner", "Tuy", "Hoang"], "venue": null, "citeRegEx": "Horst et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Horst et al\\.", "year": 1996}, {"title": "Least-squares policy iteration", "author": ["Lagoudakis", "Michail G", "Parr", "Ronald"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lagoudakis et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis et al\\.", "year": 2003}, {"title": "A simplicial branch-and-bound algorithm for solving quadratically constrained quadratic programs", "author": ["Linderoth", "Jeff"], "venue": "Mathematical Programming, Series B,", "citeRegEx": "Linderoth and Jeff.,? \\Q2005\\E", "shortCiteRegEx": "Linderoth and Jeff.", "year": 2005}, {"title": "Error bounds for approximate policy iteration", "author": ["Munos", "Remi"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Munos and Remi.,? \\Q2003\\E", "shortCiteRegEx": "Munos and Remi.", "year": 2003}, {"title": "Performance bounds in Lp norm for approximate value iteration", "author": ["Munos", "Remi"], "venue": "SIAM Journal of Control and Optimization,", "citeRegEx": "Munos and Remi.,? \\Q2007\\E", "shortCiteRegEx": "Munos and Remi.", "year": 2007}, {"title": "Approximate dynamic programming by minimizing distributionally robust bounds", "author": ["Petrik", "Marek"], "venue": null, "citeRegEx": "Petrik and Marek.,? \\Q2012\\E", "shortCiteRegEx": "Petrik and Marek.", "year": 2012}, {"title": "Robust value function approximation using bilinear programming", "author": ["Petrik", "Marek", "Zilberstein", "Shlomo"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Petrik et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Petrik et al\\.", "year": 2009}, {"title": "Robust approximate bilinear programming for value function approximation", "author": ["Petrik", "Marek", "Zilberstein", "Shlomo"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Petrik et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Petrik et al\\.", "year": 2011}, {"title": "Feature selection using regularization in approximate linear programs for Markov decision processes", "author": ["Petrik", "Marek", "Taylor", "Gavin", "Parr", "Ron", "Zilberstein", "Shlomo"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Petrik et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Petrik et al\\.", "year": 2010}, {"title": "Markov decision processes: Discrete stochastic dynamic programming", "author": ["Puterman", "Martin L"], "venue": null, "citeRegEx": "Puterman and L.,? \\Q2005\\E", "shortCiteRegEx": "Puterman and L.", "year": 2005}, {"title": "Generalized polynomial approximations in Markovian decision processes", "author": ["Schweitzer", "Paul J", "Seidmann", "Abraham"], "venue": "Journal of Mathematical Analysis and Applications,", "citeRegEx": "Schweitzer et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Schweitzer et al\\.", "year": 1985}, {"title": "New Representations and Approximations for Sequential Decision Making", "author": ["Wang", "Tao"], "venue": "PhD thesis, University of Alberta,", "citeRegEx": "Wang and Tao.,? \\Q2007\\E", "shortCiteRegEx": "Wang and Tao.", "year": 2007}, {"title": "Stable dynamic programming", "author": ["Wang", "Tao", "Lizotte", "Daniel", "Bowling", "Michael", "Schuurmans", "Dale"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 16, "context": "Occupancy frequencies have been used, for example, to solve factored MDPs (Dolgov & Durfee, 2006) and in dual dynamic programming (Wang, 2007; Wang et al., 2008) (The term \u201cdual dynamic programming\u201d also refers to unrelated linear stochastic programming methods).", "startOffset": 130, "endOffset": 161}, {"referenceID": 0, "context": "Robust optimization is a recently revived approach for modeling uncertainty in optimization problems (Ben-Tal et al., 2009).", "startOffset": 101, "endOffset": 123}, {"referenceID": 12, "context": "The sampling is performed and errors bounded identically to approximate linear programming and approximate bilinear programming\u2014state and action samples are used to select a subset of constraints and variables (de Farias & van Roy, 2003; Petrik et al., 2010; Petrik & Zilberstein, 2011).", "startOffset": 210, "endOffset": 286}, {"referenceID": 12, "context": "Note the poor performance of ABP and ALP with the 10 standard features; better results have been obtained with large and different feature spaces (Petrik et al., 2010) but even these do not match DRADP.", "startOffset": 146, "endOffset": 167}], "year": 2012, "abstractText": "Approximate dynamic programming is a popular method for solving large Markov decision processes. This paper describes a new class of approximate dynamic programming (ADP) methods\u2014 distributionally robust ADP\u2014that address the curse of dimensionality by minimizing a pessimistic bound on the policy loss. This approach turns ADP into an optimization problem, for which we derive new mathematical program formulations and analyze its properties. DRADP improves on the theoretical guarantees of existing ADP methods\u2014it guarantees convergence and L1 norm-based error bounds. The empirical evaluation of DRADP shows that the theoretical guarantees translate well into good performance on benchmark problems.", "creator": "LaTeX with hyperref package"}}}