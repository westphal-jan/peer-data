{"id": "1206.3286", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "New Techniques for Algorithm Portfolio Design", "abstract": "We present and evaluate new techniques for designing algorithm portfolios. In our view, the problem has both a scheduling aspect and a machine learning aspect. Prior work has largely addressed one of the two aspects in isolation. Building on recent work on the scheduling aspect of the problem, we present a technique that addresses both aspects simultaneously and has attractive theoretical guarantees. Experimentally, we show that this technique can be used to improve the performance of state-of-the-art algorithms for Boolean satisfiability, zero-one integer programming, and A.I. planning.", "histories": [["v1", "Wed, 13 Jun 2012 15:45:20 GMT  (295kb)", "http://arxiv.org/abs/1206.3286v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["matthew streeter", "stephen f smith"], "accepted": false, "id": "1206.3286"}, "pdf": {"name": "1206.3286.pdf", "metadata": {"source": "CRF", "title": "New Techniques for Algorithm Portfolio Design", "authors": ["Matthew Streeter", "Stephen F. Smith"], "emails": ["sfs}@cs.cmu.edu"], "sections": [{"heading": null, "text": "We present and evaluate new techniques for designing algorithm portfolios. In our view, the problem has both a scheduling aspect and a machine learning aspect. Prior work has largely addressed one of the two aspects in isolation. Building on recent work on the scheduling aspect of the problem, we present a technique that addresses both aspects simultaneously and has attractive theoretical guarantees. Experimentally, we show that this technique can be used to improve the performance of state-of-the-art algorithms for Boolean satisfiability, zero-one integer programming, and A.I. planning."}, {"heading": "1 Introduction", "text": "Many computational problems that arise in the world are NP-hard, and thus likely to be intractable from a worst-case point of view. However, the particular instances of these problems that are actually encountered can often be solved effectively using heuristics that do not have good worst-case guarantees. Typically there are a number of heuristics available for solving any particular NP-hard problem, and there is no one heuristic that performs best on all problem instances. Thus, when solving a particular instance of an NP-hard problem, it is not clear a priori how to best make use of the available CPU time.\nSpecifically, suppose you wish to solve an instance x of a computational problem, and there are k heuristics available for solving it. Each heuristic, when run on instance x, will either solve the instance in finite time (e.g., by returning a provably correct \u201cyes\u201d or \u201cno\u201d answer to a decision problem, returning a provably optimal solution to an optimization problem), or will run forever without solving it. When solving x, you will in general have some prior knowledge of how each\nof the k heuristics behaves on other instances of the same computational problem. Naturally, you would like to solve x as quickly as possible.\nIn this situation, a natural approach would be to label each previously-encountered problem instance with a set of features, and then to use some machine learning algorithm to predict which of the k heuristics will return an answer in the shortest amount of time. However, if we then run the predicted fastest heuristic and it does not yield an answer after some sufficiently large amount of time, we might suspect that the machine learning algorithm\u2019s prediction was a mistake, and might try running a different heuristic instead. Alternatively, if the heuristic is randomized, we might try restarting it and running with a fresh random seed.\nWe refer to the general problem of determining how to solve a problem instance in this setting as algorithm portfolio design [5, 6]. As just illustrated, the problem has both a machine learning aspect (predicting which heuristic will solve the instance first) and a scheduling aspect (determining how long to run a heuristic before giving up and trying a different heuristic). Previous work (e.g., [6, 8, 10]) has largely addressed one of the two aspects in isolation (we discuss previous work in detail in \u00a76). In this work, we present an approach that addresses both aspects of the problem simultaneously and has attractive theoretical guarantees.\nWe note up front that our work does not address all possible aspects of the algorithm portfolio design problem. For example, we ignore the possibility of making scheduling decisions dynamically based on the observed behavior of the heuristics (e.g., if a heuristic has a progress bar that indicates how close it is to solving the instance). We also ignore the possibility of sharing information (e.g., upper and lower bounds on the optimal value of the objective function) between heuristics as they are executing."}, {"heading": "1.1 Formal setup", "text": "We are given as input a set H of heuristics (i.e., algorithms with potentially large running time) for solving some computational problem. Heuristic h, when run on problem instance x, runs for T (h, x) time units before solving the problem. If h is randomized, then T (h, x) is a random variable whose outcome depends on the sequence of random bits supplied as input to h.\nWe will be interested in interleaving the execution of heuristics according to schedules of the following form.\nDefinition (schedule). A schedule S = \u3008(h1, \u03c41), (h2, \u03c42), . . .\u3009 is a sequence of pairs (h, \u03c4) \u2208 H \u00d7 R>0, where each pair (h, \u03c4) represents running heuristic h for time t.\nWhen interpreting a schedule, we allow each heuristic h \u2208 H to be executed in one of two models (the choice of model need not be the same for all heuristics). If h is executed in the suspend-and-resume model, then a pair (h, \u03c4) represents continuing a run of heuristic h for an additional \u03c4 time units. The run of h is then temporarily suspended and kept resident in memory, to be potentially resumed later on. In contrast, if h is executed in the restart model, then a pair (h, \u03c4) represents running h from scratch for time \u03c4 , and then deleting the run from memory (if h is randomized, the run is performed with a fresh random seed).\nAbusing notation slightly, we use T (S, x) to denote the time required to solve problem instance x using schedule S. We illustrate the definition of T (S, x) with an example. Consider the schedule\nS = \u3008(h1, 2), (h2, 2), (h1, 4), . . .\u3009\nillustrated in Figure 1. Suppose H = {h1, h2}, both heuristics are deterministic, and T (h1, x) = T (h2, x) = 3. Then T (S, x) = 5 if h1 is executed in the suspend-and-resume model, whereas T (S, x) = 7 if h1 is executed in the restart model. Note that in calculating T (S, x) when S is executed in the suspendand-resume model, we ignore any overhead associated with context-switching.\nThis class of schedules is quite flexible, and includes restart-schedules [11] and task-switching schedules [13] as special cases. A restart schedule is a schedule for a single randomized heuristic, executed in the restart model. A task-switching schedule is a schedule for a set of one or more deterministic heuristics, each executed in the suspend-and-resume model.\nAn algorithm portfolio is a way to decide what schedule to use to solve a particular problem instance.\nDefinition (algorithm portfolio). An algorithm portfolio is a procedure \u03c6 that, given a problem instance x, returns a schedule \u03c6(x) to use to solve x.\nWe measure the performance of a schedule S on a problem instance x in terms of E [T (S, x)], where the expectation is over the random bits used in the runs that S performs. We are interested in optimizing this objective in two settings: offline and online.\nIn the offline setting, we are given as input a set of training instances, along with the value of T (x, h) (or in general, an estimate of its distribution) for each heuristic h and training instance x. Our goal is to construct an algorithm portfolio (within some class) that performs optimally on the set of training instances. We would then use such a portfolio to solve additional, similar problem instances more efficiently.\nIn the online setting, we are fed a sequence X = \u3008x1, x2, . . . , xn\u3009 of problem instances one at a time and must obtain a solution to each instance (via some schedule) before moving on to the next instance. When selecting a schedule Si to use to solve instance xi, we have knowledge of the previous instances x1, x2, . . . , xi\u22121 but we have no knowledge of xi itself or of any subsequent instances. In this setting, our goal is to learn an effective algorithm portfolio on-the-fly, again with the aim of minimizing average CPU time."}, {"heading": "1.2 Summary of results", "text": "In \u00a72, we review recent results on a pure scheduling approach to the algorithm portfolio design problem. For the offline setting, the main result is a greedy algorithm that returns a 4-approximation to the optimal schedule; achieving a 4\u2212 approximation for any > 0 is NP-hard. For the online setting, the main result is an online schedule-selection algorithm whose worst-case performance guarantees converge to those of the offline greedy approximation algorithm, asymptotically as the number of instances grows large. Note that the latter guarantee does not require any statistical assumptions about the sequence of problem instances.\nIn \u00a73, we discuss how the online algorithm discussed in \u00a72 can be combined with algorithms for solving the\nso-called sleeping experts problem in order to take advantage of Boolean features of an instance when selecting a schedule. This approach yields an online algorithm that, simultaneously for each feature f , is guaranteed to perform near-optimally (i.e., average CPU time asymptotically at most 4 times that of any schedule) on the subset of instances for which f is true.\nIn \u00a74, we evaluate these techniques experimentally, and show that they can be used to improve the performance of state-of-the-art heuristics for Boolean satisfiability, A.I. planning, and zero-one integer programming.\nThe results just described apply only to the objective of minimizing average CPU time. In \u00a75, we consider the case in which each heuristic is an anytime algorithm that returns solutions of increasing quality over time. We describe how our results for minimizing average CPU time can be generalized to yield schedules with good anytime behavior, and demonstrate the power of this approach by applying it to state-of-theart heuristics for zero-one integer programming."}, {"heading": "2 Background", "text": "In this section we review recent results on a pure scheduling approach to algorithm portfolio design. These results form the basis of the algorithms and experimental results presented in the rest of the paper."}, {"heading": "2.1 Offline greedy approximation algorithm", "text": "Suppose we collect a set of training instances X , and wish to compute the schedule that performs optimally over the training instances (i.e., the schedule S that minimizes \u2211 x\u2208X E [T (S, x)]). We assume that for each heuristic h \u2208 H and training instance x \u2208 X , the distribution of T (h, x) is known exactly (in practice, we would have to estimate it by performing a finite number of runs).\nBuilding on previous work on the Min-Sum Set Cover problem [3], Streeter et al. [16, 17] developed a greedy approximation algorithm for this offline problem. Let f(S) denote the sum, over all instances x \u2208 X , of the probability that executing schedule S yields a solution to instance x. The schedule G = \u3008g1, g2, . . .\u3009 returned by the greedy approximation algorithm can be defined inductively as follows: G1 = \u3008\u3009, Gj = \u3008g1, g2, . . . , gj\u22121\u3009 for j > 1, and\ngj = arg max a=(h,\u03c4)\u2208H\u00d7R>0\n{ f(Gj + a)\u2212 f(Gj)\n\u03c4\n} (1)\nwhere Gj+a denotes the schedule obtained by appending the pair a to Gj .1 Informally, G is constructed by\n1 Evaluating the arg max in (1) requires considering\ngreedily appending a run a = (h, \u03c4) to the schedule so as to maximize the expected number of instances a solves per unit time.\nThe performance of G is summarized by the following theorem. The theorem shows that, assuming P 6= NP, the greedy schedule has optimal worst-case performance from an approximation standpoint (among schedules that can be computed in polynomial time).\nTheorem 1 (Streeter et al., 2007a; 2007b). G is a 4-approximation to the optimal schedule. That is,\n\u2211 x\u2208X E [T (G, x)] \u2264 4 \u00b7min S {\u2211 x\u2208X E [T (S, x)] } .\nFurthermore, for any > 0, obtaining a 4\u2212 approximation to the optimal schedule is NP-hard (even in the special case where all heuristics are deterministic)."}, {"heading": "2.2 Online greedy algorithm", "text": "In the online setting, a sequence \u3008x1, x2, . . . , xn\u3009 of problem instances arrive one at a time, and one must solve each instance xi via some schedule (call it Si) before moving on to instance xi+1. When selecting Si, one has no knowledge of xi itself. After solving xi, one learns only the outcomes of the runs that were actually performed when executing Si. As in the offline setting, the goal is to minimize the average CPU time required to solve each instance in the sequence.\nRecently, Streeter and Golovin [15] developed an online algorithm for an abstract scheduling problem that includes this online problem as a special case. For the results of [15] to apply, we must make some additional assumptions. First, we assume that T (h, xi) is an integer for all heuristics h and instances xi. Second, we assume that the CPU time the online algorithm uses up on any particular instance xi is artificially capped at some value B (without such a cap, the online algorithm could be forced to spend an arbitrarily large amount of CPU time solving a single instance, and we could prove no meaningful bounds on its performance).\nThe algorithm presented in [15] is called OG, for \u201conline greedy\u201d, and can be viewed as an online version of the greedy approximation algorithm described in \u00a72.1. The following theorem shows that its worstcase performance guarantees approach those of the offline greedy algorithm, asymptotically as the number of problem instances approaches infinity. The theorem can be proved as a corollary of [15, Theorem 11] (for a formal derivation, see [14, Chapter 3])."}, {"heading": "O (r |X |) values of \u03c4 per heuristic, where r is the maximum number of runs used to estimate the distribution of", "text": "T (h, x). For more details, see [14].\nTheorem 2 (Streeter and Golovin, 2007). Algorithm OG [15], run with exploration probability \u03b3 = \u0398 ( n\u2212 1 4 ) , has the following guarantee. Let Ti =\nmin {B, T (Si, xi)}, for some B > 0. Then n\u2211 i=1 E [Ti] \u2264 4 \u00b7min S\u2208S { n\u2211 i=1 E [T (S, x)] } +O ( n 3 4 ) ."}, {"heading": "3 Exploiting Features", "text": "The algorithms referred to in theorems 1 and 2 provide no mechanism for tailoring the choice of schedule to the particular problem instance being solved. In practice, there may be quickly-computable features that distinguish one instance from another and suggest the use of different heuristics. In this section, we describe how existing techniques for solving the socalled sleeping experts problem can be used to exploit such features in an attractive way.\nThe sleeping experts problem is defined as follows. One has access to a set of M experts. On each day, a given expert is either awake, in which case the expert dispenses a piece of advice, or the expert is asleep. At the beginning of day i, one must select an awake expert whose advice to follow. Following the advice of expert j on day i incurs a loss `ij \u2208 [0, 1]. At the end of day i, the value of the loss `ij for each (awake) expert j is made public, and can be used as the basis for making choices on subsequent days. Note that the historical performance of an expert does not imply any guarantees about its future performance. Remarkably, randomized expert-selection algorithms nevertheless exist that achieve the following guarantee: simultaneously for each j, one\u2019s expected loss on the subset Dj of days when j was awake is at most\u2211 i\u2208Dj ` i j + O (\u221a n logM + logM ) . Thus, when using such an algorithm2, one asymptotically performs as well as any fixed expert on the subset of days that expert was awake.\nSuppose that each problem instance xi is labeled with the values of M Boolean features. We will exploit such features by applying the sleeping experts algorithm in a standard way. We create, for each feature j, a copy Aj of the online schedule-selection algorithm OG that is only run on instances where feature j is true. We then use an algorithm for the sleeping experts problem to select among the schedules returned by the various copies, as described in the pseudo-code for OGse. Due to space constraints, the pseudo-code refers to [15, 17]\n2See [2] for a description of such an algorithm. The algorithm maintains, for each expert, a weight that is adjusted based on its performance relative to other experts. On each day, experts are selected with probability proportional to their weights.\nfor the details of certain steps. As in \u00a72.2, we use B to denote an artificial bound on CPU time.\nAlgorithm OGse\nInitialization: let E be a copy of the sleeping experts algorithm of [2]; and for each feature j, let Aj be a copy of OG [15].\nFor i from 1 to n: 1. Let Fi be the set of features that are true\nfor xi. For each feature j \u2208 Fi, use Aj to select a schedule Si,j .\n2. Use E to select a feature (expert) ji \u2208 Fi, and select the schedule Si = Si,ji .\n3. With probability \u03b3 = \u0398 ( n\u2212 1 4 ) , explore\nas follows. Using the procedure of [17], run each heuristic for time O (B logB) in order to obtain a function f\u0302 such that for any schedule S, E [ f\u0302(S) ] =\nE [min {B, T (S, xi)}]. Feed f\u0302 back to each Aj , as described in [15]. Finally, for each j, set `ij = 1 B f\u0302(Si,j). Otherwise (with probability 1\u2212 \u03b3) set `ij = 0 for all j.\n4. For each j \u2208 Fi, feed back `ij to E as the loss for expert j.\nThe performance of OGse is summarized by the following theorem.\nTheorem 3. Let Xj be the subset of instances for which feature j is true. Let T (x) be the CPU time spent by OGse on instance x. Then, simultaneously for each j, we have\nE \u2211 x\u2208Xj T (x)  \u2264 4\u00b7min S \u2211 x\u2208Xj E [T (S, x)] +O (n 34) . Proof. As already discussed, the algorithm E used as a subroutine in OGse guarantees that, for any j,\u2211\nx\u2208Xj `iji \u2264 \u2211 x\u2208Xj `ij +R (2)\nwhere R = O (\u221a n logM + logM ) . Define Li(S) =\nE [min {B, T (S, xi)}]. Thus E [ `ij ]\n= \u03b3BLi(Si,j). Taking the expectation of both sides of (2) yields\n\u2211 x\u2208Xj Li(Si) \u2264 \u2211 x\u2208Xj Li(Si,j) + B \u03b3 R .\nNote that B\u03b3 R = O ( n 3 4 ) (for constant M). At the same time, by Theorem 2 we have \u2211 x\u2208Xj Li(Si,j) \u2264 4 \u00b7min S \u2211 x\u2208Xj E [T (S, x)]\n+O (n 34) . Finally, because \u03b3 = \u0398 ( n\u2212 1 4 ) , we have\nE [\u2211 x\u2208Xj T (x) ] \u2264 \u2211 x\u2208Xj Li(Si) + O ( n 3 4 ) . Putting these equations together proves the theorem.\nNote that Theorem 3 provides a very strong guarantee. For example, if each instance is labeled as either \u201clarge\u201d or \u201csmall\u201d and also as either \u201crandom\u201d or \u201cstructured\u201d, then the performance of OGse on large instances will be nearly as good as that of the optimal schedule for large instances, and simultaneously its performance on structured instances will be nearly as good as that of the optimal schedule for structured instances (even though these subsets of instances overlap, and the optimal schedule for each subset may be quite different)."}, {"heading": "4 Experimental Evaluation", "text": "In this section, we evaluate the algorithms presented in the previous section experimentally using data from recent solver competitions."}, {"heading": "4.1 Solver competitions", "text": "Each year, various computer science conferences hold competitions designed to assess the state of the art solvers in some problem domain. In these competitions, each submitted solver is run on a sequence of problem instances, subject to some per-instance time limit. Solvers are awarded points based on the instances they solve and how fast they solve them, and prizes are awarded to the highest-scoring solvers.\nThe experiments reported here make use of data from the following three solver competitions.\n1. SAT 2007. Boolean satisfiability is the task of determining whether there exists an assignment of truth values to a set of Boolean variables that satisfies each clause (disjunction) in set of clauses. SAT solvers are used as subroutines in state-ofthe-art algorithms for hardware and software verification and A.I. planning. The SAT 2007 competition included industrial, random, and handcrafted benchmarks.\n2. IPC-5. A.I. planning is the problem of finding a sequence of actions (called a plan) that leads from\na starting state to a desired goal state, according to some formal model of how actions affect the state of the world. We used data from the optimal planning track of the Fifth International Planning Competition (IPC-5), in which the model of the world is specified in the STRIPS language and the goal is to find a plan with (provably) minimum length.\n3. PB\u201907. Pseudo-Boolean optimization is the task of minimizing a function of zero-one variables subject to algebraic constraints, also known as zeroone integer programming. On many benchmarks, pseudo-Boolean optimizers (which are usually based on SAT solvers) outperform general integer programming packages such as CPLEX [1]. The PB\u201907 evaluation included both optimization and decision (feasibility) problems from a large number of domains, including formal verification and logic synthesis.\nOur experiments for each solver competition followed a common procedure. First, we determined the value of T (h, x) for each heuristic h and benchmark instance x using data available on the competition web site (we did not actually run any of the heuristics). The heuristics considered in these competitions are deterministic (or randomized, but run with a fixed random seed), so T (h, x) is simply a single numeric value. If a heuristic did not finish within the competition time limit, then T (h, x) is undefined. Second, we discarded any instances that none of the heuristics could solve within the time limit.\nGiven a schedule S and instance x, we will not generally be able to determine the true value of T (S, x), due to the fact that T (h, x) is undefined for some heuristics. We can, however, determine the value of min {B, T (S, x)}, where B is the competition time limit. We use this lower bound in all the comparisons that follow."}, {"heading": "4.2 Number of training instances required in practice", "text": "In this section we investigate how the number of available training instances affects the quality of a schedule computed using those training instances. To do so, we adopted the following procedure. Given a set of n instances, we select m < n training instances at random, then use the greedy algorithm from \u00a72.1 to compute an approximately optimal schedule3 for the training instances. We then use this schedule to solve each of\n3For all solver competitions, the number of heuristics was large enough that computing an optimal schedule via dynamic programming was impractical.\nthe n\u2212m remaining instances, and record the average CPU time it requires. We examined all values of m that were powers of 2 less than n. For each value of m, we repeated the experiment 100 times and averaged the results.\nPB'07, opt. small integers\nFigure 2 depicts the results for optimization problems from the \u201csmall integers\u201d track of the PB\u201907 competition. The figure shows average CPU time (on test instances) as a function of the number of training instances, for both versions of the greedy algorithm (suspend-and-resume and restart). For comparison, the figure also shows the average CPU time required by the fastest individual solver, as well as a schedule that simply ran all solvers in parallel (i.e., if there are k solvers, each one receives a 1k fraction of the CPU time).\nFigure 2 has several noteworthy features. First, only a small number of training instances (in this case 16) are required in order to produce a schedule that outperforms both the fastest individual solver and the na\u0308\u0131ve parallel schedule. Second, with a sufficient number of training instances, the gap between the performance of the greedy schedules and that of the fastest individual solver is significant (in this case, more than a factor of 2). Third, the suspend-and-resume model offers only a relatively small advantage over the restart model. We have observed these same three trends in a number of other cases (e.g., see Figure 3).\nWe note that previous work (e.g., [16]) gave learningtheoretic bounds on the number of training instances required to learn a near-optimal schedule; however, these worst-case upper bounds are quite pessimistic relative to our experimental results."}, {"heading": "4.3 Exploiting features", "text": "We now examine the benefit of using Boolean features to help decide which schedule to use for solving a par-\nticular problem instance. We present results for two instance sets: the random category of the SAT 2007 competition, and the optimal planning track of IPC-5. For the SAT instances, we labeled each instance with Boolean features based on the size of the formula, the ratio of clauses to variables, and the number of literals per clause. For the planning instances, we used features based on the planning domain, the number of goals, the number of objects, and the number of predicates in the initial conditions.\nTo evaluate the effect of features, we used a procedure similar to the one used in the experiments summarized in Figure 2. Given a data set, we sample m training instances at random, and examine how average performance (on test instances) varies as a function of m. For each value of m, we again repeated the experiment 100 times and averaged the results. In addition to evaluating the greedy algorithm from \u00a72.1, we now evaluate two other approaches. The first approach, which we refer to as \u201cGreedy w/features\u201d, uses the algorithm OGse from \u00a73 to select (suspend-and-resume) schedules as follows. First, we run OGse on each of the m training instances, with exploration probability \u03b3 = 1. We then run the algorithm on each of the n\u2212m test instances, with exploration probability \u03b3 = 0 (so the algorithm receives no feedback on test instances). The second approach, which we refer to as \u201cFeatures only\u201d below, is similar except that it uses the sleeping experts algorithm of [2] to select a single heuristic (rather than a schedule), and runs that heuristic until it obtains a solution. Here we focus on performance as a function of the number of training instances, because the number of benchmark instances was typically too small to allow for good performance in the online setting of \u00a72.2.\nFigures 3 (A) and (B) present our results for the SAT and planning instances, respectively. Both graphs exhibit two noteworthy features. First, when the number of training instances is relatively small, a pure scheduling approach outperforms a purely feature-based approach; but as the number of training instances increases, the reverse is true. This behavior makes intuitive sense: when the number of training instances is small, committing to a single heuristic based on the training data is a very risky thing to do, and thus a purely feature-based approach can perform very poorly (e.g., worse than the na\u0308\u0131ve parallel schedule); as the number of training instances increases this becomes less of a risk. Second, in all cases, an approach that uses features to select schedules outperforms either a pure scheduling or purely feature-based approach.\nFigure 4 depicts the (suspend-and-resume) schedule returned by the greedy algorithm when all available\n(A) SAT'07, random\nSAT instances are used as training data. As indicated in the figure, the greedy schedule makes use a variety of different SAT solvers, and spends a significant amount of time running solvers whose overall average CPU time did not put them at the top of the competition."}, {"heading": "5 Combining Anytime Algorithms", "text": "Thus far, we have thought of a heuristic as a program that, given a problem instance, runs for some fixed amount of time before definitively solving it (e.g., by returning a provably optimal solution). Now suppose instead that our heuristics are anytime algorithms that return solutions of increasing quality over time. In this case, we would like to construct a schedule that yields near-optimal solutions quickly, in addition to yielding provably optimal solutions quickly.\nOne simple way to do this is as follows. Define, for each instance, a set of objectives to achieve (e.g., finding a solution with cost at most \u03b1 times optimal, for each \u03b1 \u2208 {2, 1.5, 1.01}). For simplicity, consider the offline setting described in \u00a72.1. For each training instance x,\ncreate a new set of fictitious instances x\u03031, x\u03032, . . . , x\u0303k, one for each of the k objectives. For each heuristic h, define T (h, x\u0303i) to be the time that h requires to achieve the ith objective. Then, the average time a schedule or heuristic takes to \u201csolve\u201d the fictitious instances is simply the average time it takes to achieve each of the k objectives on the original instances. If some objectives are more important than others, we can weight the fictitious instances accordingly (the results described in \u00a72 readily extend to weighted sets of instances).\nTo evaluate this approach, we revisit the experiments performed in \u00a74 using the PB\u201907 competition data, but now we measure the performance of a schedule as the average of (i) the time the schedule takes to find a feasible solution, (ii) the time the schedule takes to find an optimal solution, and (iii) the time the schedule takes to prove optimality (or to prove that the problem is infeasible).\nTable 1 summarizes the results of these experiments. For each track of the PB\u201907 competition and for each of the three objectives, we define a speedup factor equal to the (lower bound on) average CPU time required by the fastest individual heuristic to achieve that objective, divided by the corresponding quantity for the (suspend-and-resume) greedy schedule, where the greedy algorithm is evaluated under leave-one-out cross-validation. Note that in general, the three different speedup factors listed for each track represent a comparison against three different heuristics.\nTable 1 shows that for all three tracks, we were able to\ngenerate a schedule that simultaneously outperformed each of the original heuristics in terms of each of the three objectives we considered. The results of these experiments could potentially be improved by using features4 as in \u00a74.3, and by sharing upper and lower bounds on the optimal objective function value among heuristics as they are discovered."}, {"heading": "6 Related Work", "text": "Previous work on algorithm portfolio design has almost always focused on a single aspect of the problem. In particular, almost all previous theoretical work has focused on the scheduling aspect of the problem, whereas the bulk of the experimental work has focused on the machine learning aspect of the problem. We now discuss previous work on each of these two aspects of the problem in greater detail."}, {"heading": "6.1 Scheduling approaches", "text": "A number of papers have considered the problem of coming up with a schedule for allocating time to runs of one or more algorithms.\nThe earliest work on this problem measured the performance of a schedule in terms of its competitive ratio (i.e., the time required to solve a given problem instance using the schedule, divided by the time required by the optimal schedule for that instance). Results of this work include the universal restart schedule of Luby et al. [11] and the schedule of Kao et al. [9] for allocating time among multiple deterministic algorithms subject to memory constraints.\nSubsequent work focused on developing schedules tailored to a particular class of problems. Gomes et al. [7] demonstrated that (then) state-of-the-art heuristics for Boolean satisfiability and constraint satisfaction could be dramatically improved by randomizing the heuristic\u2019s decision-making heuristics and running the randomized heuristic with an appropriate restart\n4We do not present experiments that use features in conjunction with the PB\u201907 data because we could not readily find a suitable set of features.\nschedule. Huberman et al. [8] and Gomes et al. [6] combined multiple algorithms into a portfolio by running each algorithm in parallel at equal strength and assigning each algorithm a fixed restart threshold.\nTo fully realize the power of this approach, one must solve the problem of computing a schedule that performs well on average over a given set of problem instances collected as training data. Independently, Petrik and Zilberstein [12] and Sayag et al. [13] addressed this problem for two classes of schedules: taskswitching schedules and resource-sharing schedules. For each of these two classes of schedules, the problem of computing an optimal schedule is NP-hard, and accordingly their algorithms have exponential running time (as a function of the number of algorithms being scheduled). Recently, Streeter et al. [16] presented a polynomial-time 4 approximation algorithm for computing task-switching schedules, as reviewed in \u00a72.1."}, {"heading": "6.2 Machine learning approaches", "text": "Another approach to algorithm portfolio design is to use features of instances to attempt to predict which algorithm will run the fastest on a given instance, and then simply run that algorithm exclusively. As an example of this approach, Leyton-Brown et al. [10] use least squares regression to estimate the running time of each algorithm based on quickly-computable instance features, and then run the algorithm with the smallest predicted running time. Xu et al. [18] presented an improved version of this approach that used a two-step prediction scheme in which the answer to a decision problem is predicted using a binary classifier, and run times are then estimated conditioned on the classifier\u2019s prediction."}, {"heading": "6.3 Integrated approaches", "text": "In addition to the work just described, there has been previous work that addresses both the scheduling and machine learning aspects of the algorithm portfolio design problem simultaneously. For example, Gagliolo and Schmidhuber [4] presented an approach for allocating CPU time among heuristics in an online setting, based on statistical models of the behavior of the heuristics. Although their approach has no rigorous performance guarantees and would not perform well in the worst-case online setting considered in this paper, it would be interesting to compare their approach to ours experimentally."}, {"heading": "7 Conclusions", "text": "This paper presented a new technique for addressing the scheduling and machine learning aspects of\nthe algorithm portfolio design problem, and evaluated the technique experimentally. Our main experimental findings can be summarized as follows.\n1. In a number of well-studied problem domains, existing state-of-the-art heuristics can be combined into a new and faster heuristic simply by collecting a few dozen training instances and using them to compute a schedule for interleaving the execution of the existing heuristics.\n2. State-of-the-art anytime algorithms for solving optimization problems can be combined, via a schedule, into an algorithm with better anytime performance.\n3. Instance-specific features can be used to generate a custom schedule for a particular problem instance. Using this approach can result in better performance than using either a pure scheduling approach or a purely feature-based approach.\nAs suggested in \u00a71, our experimental results could potentially be improved in at least two ways. First, we could attempt to predict a heuristic\u2019s remaining running time based on its current state and adapt our schedule accordingly. Second, we could share information among heuristics during the process of solving an instance (e.g., when solving optimization problems, the heuristics could share upper and lower bounds on the optimal objective function value)."}, {"heading": "Acknowledgements", "text": "Many thanks to Avrim Blum for suggesting the use of a sleeping experts algorithm in our problem setting. This research was supported in part by DARPA under Contract # FA8750-05-C-0033, and by the CMU Robotics Institute."}], "references": [{"title": "Generic ILP versus specialized 0-1 ILP: An update", "author": ["Fadi A. Aloul", "Arathi Ramani", "Igor L. Markov", "Karem A. Sakallah"], "venue": "In ICCAD,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "From external to internal regret", "author": ["Avrim Blum", "Yishay Mansour"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Approximating min sum set", "author": ["Uriel Feige", "L\u00e1szl\u00f3 Lov\u00e1sz", "Prasad Tetali"], "venue": "cover. Algorithmica,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Dynamic algorithm portfolios", "author": ["Matteo Gagliolo", "J\u00fcrgen Schmidhuber"], "venue": "In AIMATH,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Algorithm portfolio design: Theory vs. practice", "author": ["Carla P. Gomes", "Bart Selman"], "venue": "In UAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Boosting combinatorial search through randomization", "author": ["Carla P. Gomes", "Bart Selman", "Henry Kautz"], "venue": "In AAAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "An economics approach to hard computational problems", "author": ["Bernardo A. Huberman", "Rajan M. Lukose", "Tad Hogg"], "venue": "Science, 275:51\u201354,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Optimal constructions of hybrid algorithms", "author": ["Ming-Yang Kao", "Yuan Ma", "Michael Sipser", "Yiqun Yin"], "venue": "In SODA,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "Boosting as a metaphor for algorithm design", "author": ["Kevin Leyton-Brown", "Eugene Nudelman", "Galen Andrew", "James McFadden", "Yoav Shoham"], "venue": "In CP,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Optimal speedup of Las Vegas algorithms", "author": ["Michael Luby", "Alistair Sinclair", "David Zuckerman"], "venue": "Information Processing Letters,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "Learning parallel portfolios of algorithms", "author": ["Marek Petrik", "Shlomo Zilberstein"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Combining multiple heuristics", "author": ["Tzur Sayag", "Shai Fine", "Yishay Mansour"], "venue": "In STACS, pages 242\u2013253,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Using Online Algorithms to Solve NP-Hard Problems More Efficiently in Practice", "author": ["Matthew Streeter"], "venue": "PhD thesis,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "An online algorithm for maximizing submodular functions", "author": ["Matthew Streeter", "Daniel Golovin"], "venue": "Technical Report CMU-CS-07-171,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Combining multiple heuristics online", "author": ["Matthew Streeter", "Daniel Golovin", "Stephen F. Smith"], "venue": "In AAAI,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Restart schedules for ensembles of problem instances", "author": ["Matthew Streeter", "Daniel Golovin", "Stephen F. Smith"], "venue": "In AAAI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "SATzilla07: The design and analysis of an algorithm portfolio for SAT", "author": ["Lin Xu", "Frank Hutter", "Holger H. Hoos", "Kevin Leyton-Brown"], "venue": "In CP,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}], "referenceMentions": [{"referenceID": 4, "context": "We refer to the general problem of determining how to solve a problem instance in this setting as algorithm portfolio design [5, 6].", "startOffset": 125, "endOffset": 131}, {"referenceID": 6, "context": ", [6, 8, 10]) has largely addressed one of the two aspects in isolation (we discuss previous work in detail in \u00a76).", "startOffset": 2, "endOffset": 12}, {"referenceID": 8, "context": ", [6, 8, 10]) has largely addressed one of the two aspects in isolation (we discuss previous work in detail in \u00a76).", "startOffset": 2, "endOffset": 12}, {"referenceID": 9, "context": "This class of schedules is quite flexible, and includes restart-schedules [11] and task-switching schedules [13] as special cases.", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "This class of schedules is quite flexible, and includes restart-schedules [11] and task-switching schedules [13] as special cases.", "startOffset": 108, "endOffset": 112}, {"referenceID": 2, "context": "Building on previous work on the Min-Sum Set Cover problem [3], Streeter et al.", "startOffset": 59, "endOffset": 62}, {"referenceID": 14, "context": "[16, 17] developed a greedy approximation algorithm for this offline problem.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[16, 17] developed a greedy approximation algorithm for this offline problem.", "startOffset": 0, "endOffset": 8}, {"referenceID": 13, "context": "Recently, Streeter and Golovin [15] developed an online algorithm for an abstract scheduling problem that includes this online problem as a special case.", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "For the results of [15] to apply, we must make some additional assumptions.", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "The algorithm presented in [15] is called OG, for \u201conline greedy\u201d, and can be viewed as an online version of the greedy approximation algorithm described in \u00a72.", "startOffset": 27, "endOffset": 31}, {"referenceID": 12, "context": "For more details, see [14].", "startOffset": 22, "endOffset": 26}, {"referenceID": 13, "context": "Algorithm OG [15], run with exploration probability \u03b3 =", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "Following the advice of expert j on day i incurs a loss `j \u2208 [0, 1].", "startOffset": 61, "endOffset": 67}, {"referenceID": 13, "context": "Due to space constraints, the pseudo-code refers to [15, 17]", "startOffset": 52, "endOffset": 60}, {"referenceID": 15, "context": "Due to space constraints, the pseudo-code refers to [15, 17]", "startOffset": 52, "endOffset": 60}, {"referenceID": 1, "context": "See [2] for a description of such an algorithm.", "startOffset": 4, "endOffset": 7}, {"referenceID": 1, "context": "Initialization: let E be a copy of the sleeping experts algorithm of [2]; and for each feature j, let Aj be a copy of OG [15].", "startOffset": 69, "endOffset": 72}, {"referenceID": 13, "context": "Initialization: let E be a copy of the sleeping experts algorithm of [2]; and for each feature j, let Aj be a copy of OG [15].", "startOffset": 121, "endOffset": 125}, {"referenceID": 15, "context": "Using the procedure of [17], run each heuristic for time O (B logB) in order to obtain a function f\u0302 such that for any schedule S, E [ f\u0302(S) ] =", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "Feed f\u0302 back to each Aj , as described in [15].", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "On many benchmarks, pseudo-Boolean optimizers (which are usually based on SAT solvers) outperform general integer programming packages such as CPLEX [1].", "startOffset": 149, "endOffset": 152}, {"referenceID": 14, "context": ", [16]) gave learningtheoretic bounds on the number of training instances required to learn a near-optimal schedule; however, these worst-case upper bounds are quite pessimistic relative to our experimental results.", "startOffset": 2, "endOffset": 6}, {"referenceID": 1, "context": "The second approach, which we refer to as \u201cFeatures only\u201d below, is similar except that it uses the sleeping experts algorithm of [2] to select a single heuristic (rather than a schedule), and runs that heuristic until it obtains a solution.", "startOffset": 130, "endOffset": 133}, {"referenceID": 9, "context": "[11] and the schedule of Kao et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9] for allocating time among multiple deterministic algorithms subject to memory constraints.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] demonstrated that (then) state-of-the-art heuristics for Boolean satisfiability and constraint satisfaction could be dramatically improved by randomizing the heuristic\u2019s decision-making heuristics and running the randomized heuristic with an appropriate restart", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] and Gomes et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Independently, Petrik and Zilberstein [12] and Sayag et al.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "[13] addressed this problem for two classes of schedules: taskswitching schedules and resource-sharing schedules.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] presented a polynomial-time 4 approximation algorithm for computing task-switching schedules, as reviewed in \u00a72.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] use least squares regression to estimate the running time of each algorithm based on quickly-computable instance features, and then run the algorithm with the smallest predicted running time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] presented an improved version of this approach that used a two-step prediction scheme in which the answer to a decision problem is predicted using a binary classifier, and run times are then estimated conditioned on the classifier\u2019s prediction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "For example, Gagliolo and Schmidhuber [4] presented an approach for allocating CPU time among heuristics in an online setting, based on statistical models of the behavior of the heuristics.", "startOffset": 38, "endOffset": 41}], "year": 2008, "abstractText": "We present and evaluate new techniques for designing algorithm portfolios. In our view, the problem has both a scheduling aspect and a machine learning aspect. Prior work has largely addressed one of the two aspects in isolation. Building on recent work on the scheduling aspect of the problem, we present a technique that addresses both aspects simultaneously and has attractive theoretical guarantees. Experimentally, we show that this technique can be used to improve the performance of state-of-the-art algorithms for Boolean satisfiability, zero-one integer programming, and A.I. planning.", "creator": "LaTeX with hyperref package"}}}