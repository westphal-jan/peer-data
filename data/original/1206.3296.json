{"id": "1206.3296", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Inference for Multiplicative Models", "abstract": "The paper introduces a generalization for known probabilistic models such as log-linear and graphical models, called here multiplicative models. These models, that express probabilities via product of parameters are shown to capture multiple forms of contextual independence between variables, including decision graphs and noisy-OR functions. An inference algorithm for multiplicative models is provided and its correctness is proved. The complexity analysis of the inference algorithm uses a more refined parameter than the tree-width of the underlying graph, and shows the computational cost does not exceed that of the variable elimination algorithm in graphical models. The paper ends with examples where using the new models and algorithm is computationally beneficial.", "histories": [["v1", "Wed, 13 Jun 2012 15:55:04 GMT  (407kb)", "http://arxiv.org/abs/1206.3296v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ydo wexler", "christopher meek"], "accepted": false, "id": "1206.3296"}, "pdf": {"name": "1206.3296.pdf", "metadata": {"source": "CRF", "title": "Inference for Multiplicative Models", "authors": ["Ydo Wexler", "Christopher Meek"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Probabilistic models that represent associations and/or interactions among random variables have been heavily applied in the past century in various fields of science and engineering. The statistical methods originating with the work of Fisher (1925, 1956) [6, 7] culminated in the log-linear models which describe the association patterns among a set of categorical variables without specifying any variable as a response (dependent) variable [1].\nA specific type of probabilistic models, probabilistic graphical models, can be visually described as an interaction graph, and embody independence assumptions in the domain of interest [15]. Their main attraction is that the independences encoded in the structure of the model allow to indirectly specify the join distribution as a product of functions \u03c8i(Di), each depends only on a limited set of variables Di. Algorithms that compute the posterior distribution conditioned on ev-\nidence, called inference algorithms, exploit this structure, avoiding a direct computation of the join probabilities [5, 19]. The complexity of such algorithms depends on the topology of the model, and is exponential in the tree-width of the underlying graph.\nThe common distinction within graphical models is between undirected graphical models [15], a subset of log-linear models, where there are no restrictions on the functions \u03c8, and Bayesian networks (BNs) [19] in which every function is a conditional distribution \u03c8i(Di) = P (Xi|\u03a0i) where \u03a0i is the set of parent variables of Xi in the model. Another type of probabilistic models that can be represented visually, called factor graphs, extends undirected graphical models and incorporates many of the desired properties of graphical modes [14].\nAside of the independences that are imposed by the model\u2019s structure, often there exist additional independences stemming from the specific values of the functions. These independences are not systematically exploited by the traditional inference algorithms, resulting in an unnecessary computational cost. For such non-structural independences we use the name context-specific independence (CSI), which was suggested in previous studies [2, 20]. We note that the term CSI takes here a more general meaning as it is not restricted to any specific type of independence.\nSeveral studies have suggested changes in the traditional representation of graphical models in order to capture context-specific independences. These include similarity networks suggested by Heckerman (1991) [12], multinets (Geiger & Heckerman 1996) [9], asymmetric influence diagrams (Fung and Shachter 1990) [8], and structured representations of the functions \u03c8 based on decision trees (Boutilier et al. 1996 [2], Poole& Zhang 2003 [20]). Other studies resorted to revised representations for specific functions (e.g. Quickscore algorithm by Heckerman 1989 for noisy-OR functions [11]).\nAlthough the new representations proved useful from an empirical view point, they lack the ability to encompass a wide variety of CSI. In addition, the theoretical complexity of inference using these representation remained a function only of the topology of the graph underlying the model.\nIn this paper we approach the problem of inference from a more general perspective. We introduce a set of models called multiplicative models in which the functions \u03c8 that account for the dependency of variables are in a multiplicative representation, where a value of an instance is a product over a set of parameters. We show that multiplicative models generalize over loglinear models, factor graphs, and graphical models. In addition, we show that multiplicative models can capture multiple forms of CSI, including CSIs captured via decision trees, decision graphs, and via noisy-OR functions. This leads to the question whether an inference algorithm that takes advantage of these independences can be constructed without additional cost. We provide such an algorithm, and show how different types of independences are utilized in this procedure to reduce the needed computations. The inference algorithm provided herein simplifies over the inference algorithm suggested by Poole & Zhang (2003) [20] when applied to Bayesian networks, by avoiding the use of tables and tables splitting operations. The more general nature of the algorithm also enables it to deal with different representations, and thus account for CSI that can not be represented by decision trees and decision graphs.\nWe prove the correctness of the inference procedure and give a new notion of complexity instead of the exponent of the tree-width which is commonly used to describe the complexity of inference in graphical models. The new time complexity is shown to be less than or equal to the standard complexity."}, {"heading": "2 Multiplicative models", "text": "We propose a generalization of graphical models, factor graphs and log-linear models which represents the dependency of variables in the model via the notion of multiplicative models. In these models a value of an instance in the dependency function is a product over a specific set of parameters. The definition relies on the concept of a lattice. A lattice (L,E,\u2229,\u222a) is a partially ordered set (poset) with respect to some relation E, in which for every two elements l1, l2 \u2208 L their least upper bound is denoted as l1 \u2229 l2 and their greatest lower bound is denoted as l1 \u222a l2.\nWe usually use upper case letters to denote random variables and sets of random variables, and lower case letters to denote their values. For a variable V we\ndenote its domain, or the set of possible values it can get, by dom(V ). For a set of variables D = {Vi}ni=1, the notation dom(D) corresponds to the cross product of the domains dom(Vi), i = 1, . . . , n.\nLet D = {Vi}ni=1 be a set of n multivalued variables, and let the function \u03c8(D) : dom(D) \u2192 R specify the values in a full table for the set D, then the following is a definition for a mapping function of D.\nDefinition 1 (Mapping function) A function f is called a mapping function of D with respect to the lattice L, if it is defined as f : dom(Z) \u2192 L for every Z \u2286 D, and maps partial instances Z = z onto L.\nWe use this definition to define a lattice multiplicative model of \u03c8(D).\nDefinition 2 (Lattice multiplicative model) A model \u03c1 = {S\u03c1,\u0393\u03c1} of a function \u03c8(D) is called a lattice multiplicative model with respect to a lattice (L,E,\u2229,\u222a) and a mapping function f , if S\u03c1 \u2286 L, \u0393\u03c1 = {\u03b3s \u2208 R : s \u2208 S\u03c1} and \u03c8(D = d) =\n\u220f sEf(d),s\u2208S\u03c1 \u03b3s.\nThe set S\u03c1 is called the structure of the model, and the set \u0393\u03c1 is called the parameters of the model. In multiplicative models elements s \u2208 S for which \u03b3s = 1 can be removed from S.\nHere we focus on a lattice L which is a set of propositional clauses over the variables and their values, and call this model a propositional multiplicative model, or simply a multiplicative model. In this model, the operators on the lattice are \u2227 and \u2228. The mapping function used for this model is called the propositional mapping function, and is defined as follows.\nDefinition 3 (Propositional mapping function) A mapping function f is called a propositional mapping function of D with respect to the lattice L, if for every set Z \u2286 D the function maps every partial instance Z = z into the conjunction\n\u2227 Vi\u2208Z (Vi = vi),\nwhere vi is the projection of z onto the variable Vi.\nDefinition 4 (Propositional multiplicative model) A lattice multiplicative model \u03c1 = {S\u03c1,\u0393\u03c1} of a function \u03c8(D) is called a propositional multiplicative model with respect to a lattice (L, ,\u2227,\u2228) and a propositional mapping function f , if the elements of L are propositional clauses over the variables in D and for two clauses c and c\u2032 we denote c c\u2032 if c is implied by c\u2032.\nExample 1 Consider a set D which contains two ternary variables A and B. The corresponding lattice\ncontains propositional clauses over A and B, and for the two clauses c = (A = 0) and c\u2032 = (A = 0)\u2227(B = 2) we denote c c\u2032. The corresponding mapping function maps the instance A = 0, B = 2 into the propositional clause (A = 0) \u2227 (B = 2), and the partial instance A = 0 into the clause (A = 0).\nIn this definition, the standard model which uses fulltable representations of the functions \u03c8(D), such as graphical models, and handles each instance separately, is also a multiplicative model with the set S containing all mapping f(d) of instances D = d, and with values \u03b3d = \u03c8(d).\nAnother well-known model that falls into Definition 2 is the log-linear model."}, {"heading": "2.1 Log-linear models", "text": "Log-linear models are usually used to analyze categorical data, and are a direct generalization of undirected graphical models. These models that have been heavily used for statistical analysis for the past four decades describe the association patterns among a set of categorical variables without specifying any variable as a response (dependent) variable, treating all variables symmetrically [1].\nFormally, a log-linear model specifies the natural log of the expected frequency of values d for a set of variables D as a linear combination of the main effect \u03bbVivi of every variable Vi \u2208 D, and if |D| > 1 interaction effects \u03bbSs of every subset of variables S \u2286 D, where the instances s are consistent with d. For example, suppose that we want to investigate relationships between three categorical variables, A, B and C, then the full log-linear model is\nln(Fa,b,c) = \u00b5+\u03bbAa +\u03bb B b +\u03bb C c +\u03bb AB ab +\u03bb AC ac +\u03bb BC bc +\u03bb ABC abc\nwhere \u00b5 is the overall mean of the natural log of the expected frequencies.\nClearly in the log-linear models instances are partially ordered by inclusion of their sets and by consistency of instantiations. To formalize log-linear models as a multiplicative models, for every subset Z \u2286 D and for every instantiation Z = z such that \u03bbZz 6= 0, the set S contains all clauses of the form\n\u2227 V \u2208Z (V = v), where v\nis the projection of z onto the variable V . In addition, we set the parameters of the model to \u03b3> = e\u00b5 and \u03b3f(s) = e\u03bb S s ."}, {"heading": "2.2 Context-specific independence", "text": "With the introduction of graphical models and in particular Bayesian Networks (BNs), and the proof that inference in these models is NP-hard [4], several studies looked for further independences encoded in mod-\nels that can potentially reduce the amount of work needed for inference [12, 9]. The notion of ContextSpecific Independence (CSI) was then introduced by Smith et al. (1993) [23] and Boutilier et al. (1996) [2]. Context-specific independence corresponds to regularities within probabilistic models based on the values assigned in the model.\nFormally, we say that the sets of variables X and Y are contextually independent in the context of C = c given Z if P (X,Y |Z = z, C = c) = (1)\nP (X|Z = z, C = c) \u00b7 P (Y |Z = z, C = c)\nfor every value Z = z. One aspect of this equation is that if X and Y are contextually independent given Z, then\nP (X|Y = y1, Z = z, C = c) = P (X|Y = y2, Z = z, C = c) (2) for any two values y1, y2 of Y , which appear as repetitive values in conditional probability tables, such as those used in BNs. These repetition which are the basis of compact representations like decision trees and graphs were exploited for inference in BN [2, 20].\nAnother kind of CSI which was exploited for enhanced inference in BNs is the independence in noisy-OR functions. A noisy-OR function is a conditional probability function of a binary effect variable E given a set of m binary cause variables C = {C1, . . . , Cm}. The conditional probabilities of the function are P (E = 0|C1, . . . , Cm) = c0\n\u220f i:Ci=1 P (E = 0|Ci), where c0 is a\nconstant, and the values P (E = 0|Ci) are some real numbers.\nFor any particular CSI of the sets of variables X and Y in the context C = c given the set Z, as in Eq. 1, there exists a multiplicative model that captures this independence. Such a model is any multiplicative model where the structure does not contain elements s that involve variables from X and Y , such that there exists an instance Z = z for which s \u2227 (Z = z) = \u22a5 and s \u2227 (C = c) 6= \u22a5.\nWe now define two types of multiplicative models that capture two different types of common CSIs."}, {"heading": "2.2.1 Positive models", "text": "Representing the dependency of variables using loglinear models has some desirable properties, such as being general while ensuring the existence of a maximum likelihood without enforcing dependencies to be strictly positive. However, in the representation discussed in Section 2.1 the log-linear models use more parameters than necessary [3, 13]. Take for example the log-linear model for two binary variables A and B.\nAssuming all possible effects exist, the corresponding log-linear model uses eight parameters rather than the four parameters in a standard representation as a full table: \u03bbA0 , \u03bb A 1 , \u03bb B 0 , \u03bb B 1 , \u03bb AB 00 , \u03bb AB 01 , \u03bb AB 10 , \u03bb AB 11 .\nAnother representation of the log-linear models that accounts for these redundancies uses only parameters which involve non-zero instantiations of variables [10]. In the above example the only parameters used in this representation are: \u03bbA1 , \u03bb B 1 , \u03bb AB 11 . We describe this representation of log-linear models as a multiplicative model, which we call here the positive model.\nDefinition 5 (Positive model) A positive model \u03c1 of a function \u03c8(D) is a multiplicative model wrt to the lattice (L, ,\u2227,\u2228) and a (propositional) mapping function f in which S\u03c1 contains only elements s = f(z) where Z \u2286 D and no variable in Z = z is set to zero.\nLog-linear models, and thus positive models, are known to capture conditional and contextual independences [16].\nExample 2 An example is a function \u03c8 over two binary variables A and B where \u03c8(0, 0) \u00b7 \u03c8(1, 1) = \u03c8(0, 1) \u00b7 \u03c8(1, 0). This implies that A is independent of B and the function can be written as \u03c8(A,B) = \u03c8(A) \u00b7 \u03c8(B). In the corresponding positive model the parameter \u03b3(A=1)\u2227(B=1) = \u03c8(0,0)\u00b7\u03c8(1,1) \u03c8(0,1)\u00b7\u03c8(1,0) = 1. Thus, this independence is captured in the model.\nExample 3 In a more complex function with three binary variables A,B and C, every pair of variables is independent whenever the third variable is set to zero. For this function the corresponding positive model assigns \u03b3(V=1)\u2227(U=1) = 1 for every pair of variables V,U \u2208 {A,B,C} and where V 6= U ."}, {"heading": "2.2.2 Decision trees and graphs as multiplicative models", "text": "Common structures for representing functions with contextual independence are decision trees (DTs) and decision graphs (DGs) [22, 18]. These structures capture contextual independences that are the result of repetitive values, as specified in Eq. 2. Several studies have used decision trees to enhance inference in graphical models [2, 20]. We show how DTs and DGs fall into the category of multiplicative models.\nFor a function \u03c8(D) over a set of variables D, a decision tree T that represents \u03c8(D) is a tree with variables from D at internal nodes and values from \u03c8(D) at the leaves. Every edge from a variable V to a child in T corresponds to a different set of values H \u2286 dom(V ), and can be represented as a set of clauses\n\u2228 v\u2208H (V = v).\nA value at the end of a path p = v1 \u2192 v2 \u2192 \u00b7 \u00b7 \u00b7 \u2192 vm, where vi is some value of Vi, equals to the value\nof \u03c8(d = v1v2 \u00b7 \u00b7 \u00b7 vmvm+1 \u00b7 \u00b7 \u00b7 vn), where Vj = vj for m < j \u2264 n is any possible value of Vj . We note that in a decision tree every instance D = d is mapped to a single path in the tree. An example of a decision tree that encodes a function over the variables A,B,C,D is shown in Figure 1.\nOne can choose to use decision graphs [18] instead of decision trees. These are more compact structures that can encode for more distributions. For a function \u03c8(D) over a set of variables D, a decision graph G that represents \u03c8(D) is a directed graph with sets of variables from D at internal nodes and values from \u03c8(D) at the leaves. Similar to decision trees, every edge from a set of variables W to a child Z corresponds to a different set of values H \u2286 dom(W ), and can be represented as a set of clauses\n\u2228 w\u2208H (W = w). A value at the end\nof a path p equals to the value of \u03c8(d), where d is an instance of D consistence with the sets of values encoded by p. Again, as in decision trees, we note that in a decision graph every instance D = d is mapped to a single path in the graph.\nDefinition 6 (Decision-graph model) A decision graph model \u03c1 of a function \u03c8(D) is a multiplicative model wrt to the lattice (L, ,\u2227,\u2228) and a mapping function f where every two elements s1, s2 \u2208 S\u03c1 satisfy s1 \u2227 s2 = \u22a5, and\n\u2228 s\u2208S s = >, where \u22a5 = false and\n> = true.\nFor a specific decision graph G that represents \u03c8(D), the decision graph model of G is \u03c1(G) in which the structure contains one clause for every path from the root to a leaf in G, which is a conjunction of the clauses on the edges. For every such path s, we set \u03b3s to the value at the end of the path. We note that in this model for every instance D = d there is only one element s \u2208 S\u03c1 such that s f(d)."}, {"heading": "3 Inference for multiplicative models", "text": "Consider a model that encodes for the probability distribution P (x) = \u220f i \u03c8i(di), with sets Di = {Xi1 , . . . Ximi}, and multiplicative models \u03c1i = {Si,\u0393i} over all the functions \u03c8i(Di) wrt a lattice (L, ,\u2227,\u2228). We first show how to perform inference, and compute a probability of a set of query variables Q using a multiplicative model. In particular we perform inference for a multiplicative model via the variable elimination scheme (Zhang & Poole 1996 [24], Dechter 1999 [5]) which was originally suggested for inference in BNs. Then, we prove the correctness of the algorithm and analyze its time complexity.\nWe define an operationM(V, {\u03c1i}), which given a variable V \u2208 X and a set of models {\u03c1i}, i = 1, . . . ,m over X returns a model \u03c1\u2032 over the variables X \\ V . This\noperator is analogous to marginalization in standard inference algorithms. In addition, for a model \u03c1 we define a relevance indicator Is(V ) for each element s \u2208 S\u03c1 and each variable V in D, which is set to 1 if there exists a pair of instances d1, d2 of D that differ only by the value of V and for which s f(d1) but s f(d2). Otherwise, Is(V ) is set to 0.\nThese operations allow us to write an inference procedure which computes the probability of a set of query variables in a multiplicative model as in Algorithm 1.\nThe algorithm operates like the bucket-elimination algorithm [5], where given an order on the variables we iterate over them (Line 2), and marginalize out one variable at a time (Line 9). Only elements that include terms that involve the current variable are considered in the marginalization.\nNote that for graphical models, in which the elements of Si are a mapping of instances of the functions Di, this algorithm is exactly the known variable elimination algorithm, in its implementation as bucketelimination [5], where the sets Si[j] are the tables in the bucket of the variable Xj .\nA general algorithm for computingM(V, {\u03c1i}) is given as Algorithm 2. We use there the notation s\u2228V for an element s \u2208 S and a variable V to denote s \u2228 V=v (V = v). This operation removes all terms that specify a value for V . For example, if s = (V = 0) \u2227 (U = 0) then s \u2228 V = (U = 0).\nThe algorithm has two main parts: upto Line 5 the algorithm generates the set R of possible new elements in the model. From Line 6 it computes the new parameters \u03b3r, where at each iteration a \u201cminimal\u201d element\nAlgorithm 1: VE for multiplicative models Input: A model with n variables Xi (i = 1, . . . , n)\nand m functions \u03c8i(Di \u2286 X), that encodes for the distribution P (X). A set of multiplicative models \u03c1i = {Si,\u0393i} wrt a mapping function f , where \u03c1i model \u03c8i(Di), and a set of k query variables Q = {Xi : i \u2264 k}\nOutput: The distribution P (Q).\nt = m+ 1;1 for j = k + 1 to n do2\nfor i = 1 to t\u2212 1 do3\nSi[j]\u2190 {s : s \u2208 Si , Is(Xj) = 1};4 \u0393i[j]\u2190 {\u03b3s : s \u2208 Si[j], \u03b3s \u2208 \u0393i};5 Si \u2190 Si \\ Si[j];6 \u0393i \u2190 \u0393i \\ \u0393i[j];7\nend for;8 {St,\u0393t} \u2190M(V, {Si[j],\u0393i[j]});9 t = t+ 1;10\nend for;11\nP (Q)\u2190 { P (q) =\n\u220f si f(q) \u03b3si : Q = q, si \u2208 Si\n} ;\n12\nreturn P (Q);13\nFigure 2: Algorithm for variable elimination with a multiplicative model\nof R is chosen, and selects those elements r with parameters \u03b3r 6= 1.\nTo compute the possible new elements, Lines 2 and 3 first create a closure under the operator \u2227 of each structure Si. Then, in Line 5 all conjunctions of terms\nfrom the different closures consist of the set of possible new elements. In analogy to inference in graphical models, this operation is equivalent to the operation of tables\u2019 multiplication, often denoted as \u2297. In these models the set R is the set of instances in the table after marginalization.\nWe note that for some models, like graphical models, lines 2-5 are trivial, and are executed implicitly, since the elements in R are known to be all instances of a full-table over variables in \u22c3 Si."}, {"heading": "3.1 Correctness of the inference procedure", "text": "We prove the correctness of Algorithm 1 by showing that the algorithm maintains the property that after iterating over the set of variables U , the models \u03c1i = {Si,\u0393i} encode to the probability distribution P (X \\ U).\nAt the beginning of the algorithm every model \u03c1i represents the corresponding function \u03c8i(Di). Thus,\nP (X = x) = \u220f i \u03c8i(Di = di) = \u220f i \u220f s f(di),s\u2208Si \u03b3is.\nAssume that after removing the set of variable U we are left with the set X \u2032 = X \\ U , and now wish to eliminate a variable V \u2208 X \u2032. We write the probability of an instance x\u2032v of X\n\u2032 \\ V which is the projection of an instance X \u2032 = x\u2032 onto X \u2032 \\V via the parameters \u03b3:\nP (x\u2032v) = \u2211 V=v P (x\u2032) = \u2211 V=v \u220f i \u220f s f(x\u2032),s\u2208Si \u03b3is\nWe can decompose the product into terms that involve the variable V and those which do not. Denoting \u03b1(x\u2032v) = \u220f i \u220f s f(x\u2032),s\u2208Si,Is(V )=0 \u03b3is, we get\nP (x\u2032v) = \u03b1(x \u2032 v) \u00b7 \u2211 V=v \u220f i \u220f s f(x\u2032),s\u2208Si,Is(V )=1 \u03b3is. (3)\nNow, lets examine what the algorithm encodes for after removing variable V , and show that it equals Eq. 3. While the elements that do not involve variable V are not changed, the elements that do involve V are removed and the elements st \u2208 St are added. Therefore, after applying Algorithm 2 for V the remaining sets encode for P\u0302 (x\u2032v) = \u03b1(x \u2032 v) \u00b7 \u03b2(x\u2032v) where\n\u03b2(x\u2032v) = \u220f\nst f(x\u2032),st\u2208St \u03b3st . To express \u03b2(x\n\u2032 v) in the\nterms of Algorithm 2, recall that St \u2286 R and if an element s \u2208 R and s /\u2208 St then \u03b3s = 1. Thus, we can rewrite \u03b2(x\u2032v) using elements of R as\n\u03b2(x\u2032v) = \u220f\nr f(x\u2032),r\u2208R\n\u03b3r.\nFrom lines 2-5 in Algorithm 2, there is one element r\u2217 f(x\u2032) in R for which \u2200r \u2208 R such that r f(x\u2032) also satisfies r r\u2217. First, to show there is such an element r\u2217 we recall from Line 5 that all elements in R can be written as r =\n\u2227 1\u2264i\u2264t ri, where ri \u2208 Ri, and\nRi is the closure of Si under the operator \u2227. Consider the set of elements r\u2217i f(x\u2032), i = 1, . . . , t, for which all other elements ri \u2208 Ri such that ri f(x\u2032) satisfy ri r\u2217i . Then, every element r = \u2227 1\u2264i\u2264t ri such that r f(x\u2032) also satisfies r r\u2217.\nNow, assume by contradiction that there were two such elements, r\u22171 , r \u2217 2 \u2208 R. Then from the definition of r\u22171 and r\u22172 we get r \u2217 1 r\u22172 and r\u22172 r\u22171 , yielding r\u22171 = r\u22172 . Thus, from line 9 in Algorithm 2 \u03b2(x\u2032v) = \u03b3r\u2217 \u00b7 \u220f\nr r\u2217,r\u2208R \u03b3r = \u2211 V=v \u220f i \u220f s (r\u2217\u2227(V=v)),s\u2208Si \u03b3s\nwhere the last equality is due to the fact that the denominator in the computation of \u03b3r\u2217 is \u220f r r\u2217,r\u2208R \u03b3r. In the terms of Algorithm 1 the set {s : s (r\u2217 \u2227 (V =\nv)), s \u2208 Si} can be rewritten as {s : s f(x\u2032), s \u2208 Si, Is(V ) = 1}. Thus, we can write\n\u03b2(x\u2032v) = \u220f i \u220f s f(x\u2032),s\u2208Si,Is(V )=1 \u03b3s\nand from Eq. 3 we get P\u0302 (x\u2032v) = P (x \u2032 v). Namely, the new models encode for P (X \u2032 \\ V )."}, {"heading": "3.2 Incorporating evidence", "text": "In many practical scenarios we observe the value of some of the variables in the model, and wish to incorporate this evidence. The multiplicative models allow us to do so in a most natural way. Consider a set E of evidence nodes for which we observed the values E = e, and a multiplicative model \u03c1 = {S\u03c1,\u0393\u03c1}. Then, in order to incorporate the evidence into \u03c1, we adjust the elements in S\u03c1 by s = s\n\u2227 V \u2208E (V = v), where v is the\nprojection of e onto the variable V \u2208 E. Then, we remove every element not consistent with the evidence, s = \u22a5."}, {"heading": "3.3 Complexity of inference", "text": "It is well known that the complexity of inference in graphical models is NP-hard and its cost exponential in the tree-width of the underlying graph [4].\nWe analyze the time complexity of the inference procedure for multiplicative models given in Algorithm 1. As a by-product we refine the standard complexity and provide a new complexity bound which is based on the representation used. One can then say that the complexity of the problem is the minimum complexity among all possible representations."}, {"heading": "3.3.1 Diameter of multiplicative models", "text": "The structure of a multiplicative model determines the amount of computations needed to obtain the value \u03c8(d) of a single instantiation of values to variables in a set D. Although at first glance it seems that for a model \u03c1 = {S,\u0393} of a function \u03c8(D) the number of operations needed to obtain values of all instances D = d amounts to a total of\n\u2211 D=d |{s : s d}|, the real\nnumber of operations can be dramatically lower and we denote it by \u03b4(\u03c1). For hierarchical models, in which if an element s is not in the structure of the model then all elements s s\u2032 are also not in the model, Good (1963) provides a method that computes all such values in time |S| log |S| [10]. We denote the ratio between the number of computations and the number of elements in S, which is the size of the model, by diam(\u03c1) = \u03b4(\u03c1)|S| and name it the diameter of \u03c1.\nFrom a computational perspective, it is clearly beneficial to use models with a small diameter, as this\ndirectly leads to fewer operations whenever we want to either obtain a value of \u03c8 or update the values \u03b3s. Examples of models with a diameter of 1 are graphical models and decision graph models, in which for every element s \u2208 S, the only element s\u2032 such that s\u2032 s, is s itself. On the other hand, the diameter of a positive model can be as high as log |S|2 . This maximum is achieved for a positive model of m binary variables, when all 2m parameters do not equal one, and hence all possible elements are in S. In this scenario the diameter is exactly m2 .\nAlthough in the worst scenario the diameter of a positive model can be large, often this is not the case, and the diameter is typically bounded to be very small.\nExample 4 Consider as an example the Potts model [21] in which a function \u03c8(D) over a set D = {Vi}ni=1 decomposes according to \u03c8(D = d) = c0\n\u220f i,j \u03c8(vi, vj), where vi and vj are projections of d onto the variables Vi and Vj respectively, and c0 is a constant. Although in general a positive model over n binary variables has a diameter of n2 , in this example, the structure of the positive model includes only elements that involve at most two variables. Therefore, the diameter of the model is bounded by two.\nSimilarly, in a more complex scenario where the function \u03c8 decomposes to functions of k-tuples of variables, the diameter will be bounded by k.\nConsider a tree decomposition of the graph in which there is an edge between a pair of variables V,U if there exists an element s in one of the models for which Is(V ) \u00b7Is(U) = 1. We denote by S(W ) = {s\u2228(X \\Z) : s \u2208 Si} the set of parts of elements in the models \u03c1i that involve variables from the set of graph vertices Z which is mapped onto the tree node W . Further denoting as S\u2212(W ) the closure of S(W ) under the operator \u2227, we say that complexity of the algorithm for this tree decomposition is the maximum over the nodes W in the tree of |S\u2212(W )| \u00b7diam(S\u2212(W )), as described in Section 3.3.1. Then, the overall complexity of the algorithm is the complexity for the tree decomposition that yields the minimum for this term.\nTo see that this is indeed the time complexity of the algorithm, consider the elements in a set R in Algorithm 2. The number of elements there does not exceed the number of elements in S\u2212(W ) for the corresponding tree decomposition and where W maps onto the variables that appear in R. Most of the computation stems from computing the products in Line 9, and these can be done for the entire set of elements of R in time proportional to |R| \u00b7 diam(R). Therefore, having the ability to choose an elimination order, the complexity of the algorithm is |S\u2212(W )|\u00b7diam(S\u2212(W ))\nmaximized over all nodes W in a tree decomposition and minimized over all possible such decompositions."}, {"heading": "3.4 Benefits of inference for multiplicative models", "text": "Different multiplicative models capture different contextual independences, hence specifying different number of parameters. Take for example the function over four binary variables A,B,C,D with values according to the table in Figure 1. The structure of the corresponding decision-tree model contains six elements while the structure of the corresponding positive model contains eight elements. In this latter model, the CSI captured in the decision tree, yielding the value of \u03c8 to be independent of B given that A,C and D are set to one, does not have any effect. This variation and the structure of the model affect the run time of the inference algorithm.\nAn example where there are substantial computational savings when using the inference algorithm proposed can be found in a model such as the QMR-DT network [17], which is comprised of noisy-OR functions, mentioned in Section 2.2. The QMR-DT network is a two-level or bipartite BN where all variables are binary. The top level of the graph contains nodes for the diseases C, and the bottom level contains nodes for the findings E. The conditional probabilities in the network P (Ei = ei|\u03a0i), where \u03a0i are the parents of finding Ei in the network, are represented by noisyOR functions.\nHeckerman (1989) has developed an algorithm, called Quickscore, which takes advantage of the independence of the cause variables in the context of a negative finding Ei = 0 and uses it to speed up inference in the QMR-DT network [11].\nFor every noisy-OR function P (E|C1, . . . , Cm) a structure of a multiplicative model that captures the independence does not contain elements s such that s \u2227 (E = 0) 6= \u22a5 for which Is(Ci) = 1 and Is(Cj) = 1, for all i, j \u2264 m.\nIn addition, running Algorithm 1 using multiplicative models with structures Si = {(Ei = 1) \u2227 Ci\u2208\u03a0i (Ci = ci) : \u2200Ci = ci}\u2228\n{(Ei = 0)\u2227(Ci = 1) : Ci \u2208 \u03a0i}\u2228((Ei = 0) \u2227\nCi\u2208\u03a0i\n(Ci = 0))\nis identical to the Quickscore algorithm and gains the same savings automatically."}], "references": [{"title": "Discrete multivariate analysis", "author": ["Y. Bishop", "E. Fienberg", "P. Holland"], "venue": "MIT Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1975}, {"title": "Context-specific independence in Bayesian networks", "author": ["C. Boutilier"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "Log-Linear Models and Logistic Regression", "author": ["R. Christensen"], "venue": "Springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "The computational complexity of probabilistic inference using bayesian belief networks", "author": ["G. Cooper"], "venue": "Artificial Intelligence, 42:393\u2013405", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "Bucket elimination: A unifying framework for reasoning", "author": ["R. Dechter"], "venue": "Artificial Intelligence, 113:41\u201385", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "Statistical Methods for Research Workers", "author": ["R. Fisher"], "venue": "Macmillan Pub Co", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1925}, {"title": "Statistical Methods and Scientific Inference", "author": ["R. Fisher"], "venue": "Oliver and Boyd", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1956}, {"title": "Contingent influence diagrams", "author": ["R. Fung", "R. Shachter"], "venue": "Working Paper, Dept. of Engineering- Economic Systems, Stanford University", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1990}, {"title": "Knowledge representation and inference in similarity networks and bayesian multinets", "author": ["D. Geiger", "D. Heckerman"], "venue": "Artificial Intelligence, 82:45\u201374", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "Maximum entropy for hypothesis formulation", "author": ["I. Good"], "venue": "especially for multidimensional contingency tables. The Annals of Math. Stat., 34:911\u2013934", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1963}, {"title": "A tractable inference algorithm for diagnosing multiple diseases", "author": ["D. Heckerman"], "venue": "UAI, 230:362\u2013367", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1989}, {"title": "Probabilistic Similarity Networks", "author": ["D. Heckerman"], "venue": "MIT Press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1991}, {"title": "Log-Linear Models", "author": ["D. Knoke", "P. Burke"], "venue": "Sage Publications Inc", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1980}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F.R. Kschischang", "B. Frey", "H. Loeliger"], "venue": "IEEE Trans. Inform. Theory, 47(2):498\u2013519", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Graphical Models", "author": ["S. Lauritzen"], "venue": "Oxford University Press", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "Conditional independence and log linear models for multi-dimensional contingency tables", "author": ["J. Lindsey"], "venue": "Quality and Quantity, 8(4):377\u2013390", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1974}, {"title": "Probabilistic diagnosis using a reformulation of the INTERNIST-1/QMR knowledge base: Part II. Evaluation of diagnostic performance", "author": ["B. Middleton"], "venue": "SIAM Journal on Computing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1991}, {"title": "Decision graphs - an extension of decision trees", "author": ["J.J. Oliver"], "venue": "Proceedings of the Fourth International Workshop on Artificial Intelligence and Statistics, pages 343\u2013350", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1993}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": "Morgan Kaufmann", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1988}, {"title": "Exploiting contextual independence in probabilistic inference", "author": ["D. Poole", "N. Zhang"], "venue": "JAIR, 18:263\u2013313", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Some generalized order-disorder transformations", "author": ["R. Potts"], "venue": "Proceedings of the Cambridge Philosophical Society, 48:106\u2013109", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1952}, {"title": "A survey of decision tree classifier methodology", "author": ["S. Safavian", "D. Landgrebe"], "venue": "IEEE transactions on systems, man, and cybernetics, 21(3):660\u2013674", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1991}, {"title": "Structuring conditional relationships in influence diagrams", "author": ["J. Smith", "S. Holtzman", "J. Matheson"], "venue": "Oper. Res., 41(2):280\u2013297", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1993}, {"title": "Exploiting causal independence in bayesian network inference", "author": ["N. Zhang", "D. Poole"], "venue": "JAIR, 5:301\u2013328", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1996}], "referenceMentions": [{"referenceID": 5, "context": "The statistical methods originating with the work of Fisher (1925, 1956) [6, 7] culminated in the log-linear models which describe the association patterns among a set of categorical variables without specifying any variable as a response (dependent) variable [1].", "startOffset": 73, "endOffset": 79}, {"referenceID": 6, "context": "The statistical methods originating with the work of Fisher (1925, 1956) [6, 7] culminated in the log-linear models which describe the association patterns among a set of categorical variables without specifying any variable as a response (dependent) variable [1].", "startOffset": 73, "endOffset": 79}, {"referenceID": 0, "context": "The statistical methods originating with the work of Fisher (1925, 1956) [6, 7] culminated in the log-linear models which describe the association patterns among a set of categorical variables without specifying any variable as a response (dependent) variable [1].", "startOffset": 260, "endOffset": 263}, {"referenceID": 14, "context": "A specific type of probabilistic models, probabilistic graphical models, can be visually described as an interaction graph, and embody independence assumptions in the domain of interest [15].", "startOffset": 186, "endOffset": 190}, {"referenceID": 4, "context": "Algorithms that compute the posterior distribution conditioned on evidence, called inference algorithms, exploit this structure, avoiding a direct computation of the join probabilities [5, 19].", "startOffset": 185, "endOffset": 192}, {"referenceID": 18, "context": "Algorithms that compute the posterior distribution conditioned on evidence, called inference algorithms, exploit this structure, avoiding a direct computation of the join probabilities [5, 19].", "startOffset": 185, "endOffset": 192}, {"referenceID": 14, "context": "The common distinction within graphical models is between undirected graphical models [15], a subset of log-linear models, where there are no restrictions on the functions \u03c8, and Bayesian networks (BNs) [19] in which every function is a conditional distribution \u03c8i(Di) = P (Xi|\u03a0i) where \u03a0i is the set of parent variables of Xi in the model.", "startOffset": 86, "endOffset": 90}, {"referenceID": 18, "context": "The common distinction within graphical models is between undirected graphical models [15], a subset of log-linear models, where there are no restrictions on the functions \u03c8, and Bayesian networks (BNs) [19] in which every function is a conditional distribution \u03c8i(Di) = P (Xi|\u03a0i) where \u03a0i is the set of parent variables of Xi in the model.", "startOffset": 203, "endOffset": 207}, {"referenceID": 13, "context": "Another type of probabilistic models that can be represented visually, called factor graphs, extends undirected graphical models and incorporates many of the desired properties of graphical modes [14].", "startOffset": 196, "endOffset": 200}, {"referenceID": 1, "context": "For such non-structural independences we use the name context-specific independence (CSI), which was suggested in previous studies [2, 20].", "startOffset": 131, "endOffset": 138}, {"referenceID": 19, "context": "For such non-structural independences we use the name context-specific independence (CSI), which was suggested in previous studies [2, 20].", "startOffset": 131, "endOffset": 138}, {"referenceID": 11, "context": "These include similarity networks suggested by Heckerman (1991) [12], multinets (Geiger & Heckerman 1996) [9], asymmetric influence diagrams (Fung and Shachter 1990) [8], and structured representations of the functions \u03c8 based on decision trees (Boutilier et al.", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "These include similarity networks suggested by Heckerman (1991) [12], multinets (Geiger & Heckerman 1996) [9], asymmetric influence diagrams (Fung and Shachter 1990) [8], and structured representations of the functions \u03c8 based on decision trees (Boutilier et al.", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "These include similarity networks suggested by Heckerman (1991) [12], multinets (Geiger & Heckerman 1996) [9], asymmetric influence diagrams (Fung and Shachter 1990) [8], and structured representations of the functions \u03c8 based on decision trees (Boutilier et al.", "startOffset": 166, "endOffset": 169}, {"referenceID": 1, "context": "1996 [2], Poole& Zhang 2003 [20]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 19, "context": "1996 [2], Poole& Zhang 2003 [20]).", "startOffset": 28, "endOffset": 32}, {"referenceID": 10, "context": "Quickscore algorithm by Heckerman 1989 for noisy-OR functions [11]).", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "The inference algorithm provided herein simplifies over the inference algorithm suggested by Poole & Zhang (2003) [20] when applied to Bayesian networks, by avoiding the use of tables and tables splitting operations.", "startOffset": 114, "endOffset": 118}, {"referenceID": 0, "context": "These models that have been heavily used for statistical analysis for the past four decades describe the association patterns among a set of categorical variables without specifying any variable as a response (dependent) variable, treating all variables symmetrically [1].", "startOffset": 268, "endOffset": 271}, {"referenceID": 3, "context": "With the introduction of graphical models and in particular Bayesian Networks (BNs), and the proof that inference in these models is NP-hard [4], several studies looked for further independences encoded in models that can potentially reduce the amount of work needed for inference [12, 9].", "startOffset": 141, "endOffset": 144}, {"referenceID": 11, "context": "With the introduction of graphical models and in particular Bayesian Networks (BNs), and the proof that inference in these models is NP-hard [4], several studies looked for further independences encoded in models that can potentially reduce the amount of work needed for inference [12, 9].", "startOffset": 281, "endOffset": 288}, {"referenceID": 8, "context": "With the introduction of graphical models and in particular Bayesian Networks (BNs), and the proof that inference in these models is NP-hard [4], several studies looked for further independences encoded in models that can potentially reduce the amount of work needed for inference [12, 9].", "startOffset": 281, "endOffset": 288}, {"referenceID": 22, "context": "(1993) [23] and Boutilier et al.", "startOffset": 7, "endOffset": 11}, {"referenceID": 1, "context": "(1996) [2].", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "These repetition which are the basis of compact representations like decision trees and graphs were exploited for inference in BN [2, 20].", "startOffset": 130, "endOffset": 137}, {"referenceID": 19, "context": "These repetition which are the basis of compact representations like decision trees and graphs were exploited for inference in BN [2, 20].", "startOffset": 130, "endOffset": 137}, {"referenceID": 2, "context": "1 the log-linear models use more parameters than necessary [3, 13].", "startOffset": 59, "endOffset": 66}, {"referenceID": 12, "context": "1 the log-linear models use more parameters than necessary [3, 13].", "startOffset": 59, "endOffset": 66}, {"referenceID": 9, "context": "Another representation of the log-linear models that accounts for these redundancies uses only parameters which involve non-zero instantiations of variables [10].", "startOffset": 157, "endOffset": 161}, {"referenceID": 15, "context": "Log-linear models, and thus positive models, are known to capture conditional and contextual independences [16].", "startOffset": 107, "endOffset": 111}, {"referenceID": 21, "context": "Common structures for representing functions with contextual independence are decision trees (DTs) and decision graphs (DGs) [22, 18].", "startOffset": 125, "endOffset": 133}, {"referenceID": 17, "context": "Common structures for representing functions with contextual independence are decision trees (DTs) and decision graphs (DGs) [22, 18].", "startOffset": 125, "endOffset": 133}, {"referenceID": 1, "context": "Several studies have used decision trees to enhance inference in graphical models [2, 20].", "startOffset": 82, "endOffset": 89}, {"referenceID": 19, "context": "Several studies have used decision trees to enhance inference in graphical models [2, 20].", "startOffset": 82, "endOffset": 89}, {"referenceID": 17, "context": "One can choose to use decision graphs [18] instead of decision trees.", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": "In particular we perform inference for a multiplicative model via the variable elimination scheme (Zhang & Poole 1996 [24], Dechter 1999 [5]) which was originally suggested for inference in BNs.", "startOffset": 118, "endOffset": 122}, {"referenceID": 4, "context": "In particular we perform inference for a multiplicative model via the variable elimination scheme (Zhang & Poole 1996 [24], Dechter 1999 [5]) which was originally suggested for inference in BNs.", "startOffset": 137, "endOffset": 140}, {"referenceID": 4, "context": "The algorithm operates like the bucket-elimination algorithm [5], where given an order on the variables we iterate over them (Line 2), and marginalize out one variable at a time (Line 9).", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "Note that for graphical models, in which the elements of Si are a mapping of instances of the functions Di, this algorithm is exactly the known variable elimination algorithm, in its implementation as bucketelimination [5], where the sets Si[j] are the tables in the bucket of the variable Xj .", "startOffset": 219, "endOffset": 222}, {"referenceID": 3, "context": "It is well known that the complexity of inference in graphical models is NP-hard and its cost exponential in the tree-width of the underlying graph [4].", "startOffset": 148, "endOffset": 151}, {"referenceID": 9, "context": "For hierarchical models, in which if an element s is not in the structure of the model then all elements s s\u2032 are also not in the model, Good (1963) provides a method that computes all such values in time |S| log |S| [10].", "startOffset": 217, "endOffset": 221}, {"referenceID": 20, "context": "Example 4 Consider as an example the Potts model [21] in which a function \u03c8(D) over a set D = {Vi}i=1 decomposes according to \u03c8(D = d) = c0 \u220f", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "An example where there are substantial computational savings when using the inference algorithm proposed can be found in a model such as the QMR-DT network [17], which is comprised of noisy-OR functions, mentioned in Section 2.", "startOffset": 156, "endOffset": 160}, {"referenceID": 10, "context": "Heckerman (1989) has developed an algorithm, called Quickscore, which takes advantage of the independence of the cause variables in the context of a negative finding Ei = 0 and uses it to speed up inference in the QMR-DT network [11].", "startOffset": 229, "endOffset": 233}], "year": 2008, "abstractText": "The paper introduces a generalization for known probabilistic models such as log-linear and graphical models, called here multiplicative models. These models, that express probabilities via product of parameters are shown to capture multiple forms of contextual independence between variables, including decision graphs and noisy-OR functions. An inference algorithm for multiplicative models is provided and its correctness is proved. The complexity analysis of the inference algorithm uses a more refined parameter than the tree-width of the underlying graph, and shows the computational cost does not exceed that of the variable elimination algorithm in graphical models. The paper ends with examples where using the new models and algorithm is computationally beneficial.", "creator": "TeX"}}}