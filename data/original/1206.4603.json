{"id": "1206.4603", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Latent Collaborative Retrieval", "abstract": "Retrieval tasks typically require a ranking of items given a query. Collaborative filtering tasks, on the other hand, learn to model user's preferences over items. In this paper we study the joint problem of recommending items to a user with respect to a given query, which is a surprisingly common task. This setup differs from the standard collaborative filtering one in that we are given a query x user x item tensor for training instead of the more traditional user x item matrix. Compared to document retrieval we do have a query, but we may or may not have content features (we will consider both cases) and we can also take account of the user's profile. We introduce a factorized model for this new task that optimizes the top-ranked items returned for the given query and user. We report empirical results where it outperforms several baselines.", "histories": [["v1", "Mon, 18 Jun 2012 14:41:20 GMT  (223kb)", "http://arxiv.org/abs/1206.4603v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["jason weston", "chong wang", "ron j weiss", "adam berenzweig"], "accepted": true, "id": "1206.4603"}, "pdf": {"name": "1206.4603.pdf", "metadata": {"source": "META", "title": "Latent Collaborative Retrieval", "authors": ["Jason Weston", "Chong Wang", "Ron Weiss"], "emails": ["jweston@google.com", "chongw@cs.princeton.edu", "ronw@google.com", "madadam@google.com"], "sections": [{"heading": "1. Introduction", "text": "There exist today a growing number of applications that seamlessly blend the traditional tasks of retrieval and recommendation. For example, when users shop for a product online they are often recommended items that are similar to the item they are currently browsing. This is a retrieval problem using the currently browsed item as the query, however the user\u2019s profile (including other items they may have browsed, bought or reviewed) should be taken into account making it a personal recommendation problem as well. Another related task is that of automatic playlist creation in music players. The user can request the creation of a\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nplaylist of songs given a query (based for instance on a seed track, artist or genre) but the songs retrieved for the query should also be songs that the user likes given their known profile.\nWe call this class of problems collaborative retrieval tasks. To our knowledge these tasks have not been studied in depth, although there are several related areas which we will discuss later in the paper. Methods designed for this task need to combine both the retrieval and recommendation aspects of the problem into a single predictor. In a standard collaborative filtering (recommendation) setup, one is given a user \u00d7 item matrix indicating the known relevance of the given item to a given user, but many elements of the matrix are unknown. On the other hand, In a typical retrieval task one is given, for each query, a list of relevant items that should be retrieved. Our task is the blend of the two, which is achieved by first building a tensor comprising of the query \u00d7 user \u00d7 item training data. Typically in a retrieval task, and sometimes in a recommendation task as well, one also has access to content-based features for the items, e.g. in document retrieval one has access to the words in the documents. Hence any algorithms designed for the collaborative retrieval task should potentially be able to take advantage of those features, too.\nIn this paper, we develop a novel learning algorithm for the collaborative retrieval task. We introduce a factorized model that optimizes the top-ranked items returned for the given query and user. We also generalize it to work on either the collaborative retrieval tensor only, or using content-based features as well. The rest of the paper is as follows. Section 2 describes the collaborative retrieval task and our method for solving it. Section 3 discusses prior work and connections to other areas. Finally, Section 4 reports empirical results where we show our method outperforms several reasonable baselines, and Section 5 concludes."}, {"heading": "2. Method", "text": "Latent Collaborative Retrieval We define a scoring function for a given query, user and item:\nfFULL(q, u, d) = Rqud\nwhere R is a |Q| \u00d7 |U| \u00d7 |D| tensor, where Q is the (finite) set of possible queries, U is the set of users and D is the set of items. Any given element of the tensor is the \u201crelevance score\u201d of a given item with respect to a given query and a given user, where a high score corresponds to high relevance.\nWe are typically given m training examples {(qi,ui,di)}i=1,...,m \u2208 {1, . . . , |Q|} \u00d7 {1, . . . , |U|} \u00d7 {1, . . . , |D|} and outputs yi \u2208 R, i = 1, . . . ,m. Here, (qi,ui,di) can be used to index a particular element of R (i.e, a particular query, user and item) and yi is the relevance score, for example based on implicit user clicks, activity or explicit user annotations. One could simply collate the training data to build a suitable tensor R and use that, but the problem is that the tensor would be sparse and hence for many queries, users and items no prediction would be made. For that reason, collaborative filtering has connections with matrix completion, and almost all approaches can be seen as estimating the unknown matrix from data. For instance, many approaches such as SVD or NMF (Lee & Seung, 2001), solve such tasks by optimizing the deviation (e.g. squared error) from the known elements of the matrix. However, for retrieval tasks, and even for many recommendation tasks, humans evaluate the performance of a method from the top k results returned. Hence, precision or recall @ k measures are often appropriate. The method we propose in this paper thus has the following properties:\n(i) We learn a ranking of items given a user and a query, thus blending retrieval and recommendation tasks into one model.\n(ii) We learn model parameters for this task that attempt to optimize the performance at the top of the ranked list.\nTo fulfill property (i) we must model the combination of the users, queries and items during inference. We thus propose a model of the following form:\nf(q, u, d) = \u03a6Q(q)>S>UuT\u03a6D(d)+\u03a6U (u)>V >T\u03a6D(d). (1) Here, S is a n\u00d7|Q| matrix, T is a n\u00d7|D| matrix, V is a n\u00d7|U| matrix and n is the low dimensional embedding where queries, users and items will be represented (this\nis a hyperparameter of the system, typically n |D| and n |U|). Ui is a n \u00d7 n matrix per user (i = 1, . . . , |U|). \u03a6D(d) is the feature map of the item, the simplest choice of which is to map to a binary vector of all zeros and a one in the dth position. \u03a6Q(q) and \u03a6U (u) act similarly for queries and users. In that case, the entire model can hence be more succinctly written as:\nf(q, u, d) = (S>q Uu + V > u )Td. (2)\nHowever the \u03a6(\u00b7) notation will be useful for subsequent modifications of the algorithm (later, we will consider general feature transformations rather than just switching on a single dimension).\nAn intutive explanation of our model is as follows. The first term maps both the query (via \u03a6Q(q)>S>) and the item (via T\u03a6D(d)) into a low dimensional space and then measures their similarity in that space, after linearly transforming the space dependent on the user (via Uu). Hence, the first term alone can model the relevance score (match) between a query q and item i with respect to a user u. The second term can be seen as a kind of \u201cbias\u201d that models the relevance score (match) between user u and item i but is constant w.r.t the query.\nIt is also possible to consider some interesting special cases of the above model by further constraining the user-transformation matrices Ui:\n\u2022 Ui = I: by forcing all user-transformations to be the identity matrix we are left with the model:\nf(q, u, d) = S>q Td + V > u Td. (3)\nIn that case, the query \u00d7 item and user \u00d7 item parts of the model are two separate terms, and three-way interactions are not directly considered.\n\u2022 Ui = Di: by constraining each user k to have a diagonal matrix Di only rescaling of the dimensions of the query \u00d7 item similarity space is possible (general linear transformations are not considered). As we will see in Section 3.2 this relates to tensor factorization methods that have been used for document retrieval.\n\u2022 Ui = (ULRi )>ULRi +Di: instead of considering a full matrix U or a diagonal Di we could consider something in between, that of employing a low rank matrix ULR.\nContent-Based Method In the typical collaborative filtering setting one has access to a user \u00d7 item matrix only, and methods are agnostic to the content of the items, be they text documents, audio or images.\nFor some tasks one has access to the actual content of the items as well, for example for each item i one is given a feature representation \u03a6\u0302D(i) \u2208 RnD . In document retrieval this is the more common setting, e.g. \u03a6\u0302D(i) represents the words in the document i. For recommendation tasks this is called content-based recommendation and is particularly useful for the cold-start problem where an item has very few or no users associated to it (the relevant collaborative filtering column of R is very sparse). In that case, collaborative filtering methods have almost no data to generalize from, but content-based methods can perform well. In our setting, latent collaborative retrieval, we can also take advantage of such content features by slightly modifying our method from above. Our proposed contentbased model consists of the following form:\nf(q, u, d) = S>q UuWD\u03a6\u0302D(d) + V > u WD\u03a6\u0302D(d). (4)\nHere, the model is similar to before except an additional set of parameters WD (a n\u00d7 nD matrix) maps from item features to the n-dimensional latent embedding space. Other aspects of the model remain the same.\nFurther, if we are given a feature representation for queries as well, where for each query i we have \u03a6\u0302Q(i) \u2208 RnQ , we can also incorporate this into our model:\nf(q, u, d) = \u03a6\u0302Q(q)>W>QUuWD\u03a6\u0302D(d) + V > u WD\u03a6\u0302D(d), (5) where WQ is a n\u00d7 nQ matrix. This allows us to consider any possible query rather than being restricted to a finite set Q as in our original definition.\nCollaborative and Content-Based Retrieval Finally, we can consider a joint model that takes into account both collaborative filtering (CF) data and content-based (CB) training data. In this case, our model consists of the following form:\nf(q, u, d) = S>q UuWD\u03a6\u0302D(d) + S > q UuTd +\n\u03a6\u0302Q(q)>W>QUuWD\u03a6\u0302D(d) + \u03a6\u0302Q(q) >W>QUuTd + V >u WD\u03a6\u0302D(d) + V > u Td. (6)\nThe first two terms match the query and user with the CF and CB versions of the item respectively. Terms three and four are similar except they use the content features of the query instead. The final two terms are the \u201cbias\u201d terms comparing the user to the CF and content versions of the item. Note this model can be considered a special case of eq. (1).\nTraining To Optimize Retrieval For The Top k We are interested in learning a ranking function where\nthe top k retrieved items are of particular interest as they will be presented to the user. We wish to optimize all the parameters of our model jointly for that goal.\nA standard loss function that is often used for retrieval is the margin ranking criterion (Herbrich et al., 2000; Joachims, 2002), in particular it was used for learning factorized document retrieval models in Bai et al. (2009). Let us first write the predictions of our model for all items in the database as a vector f\u0304(q, u) where the ith index is f\u0304i(q, u) = f(q, u, i). In our collaborative retrieval setting the loss can then be written as: errAUC = m\u2211\ni=1 \u2211 j 6=di max(0, 1\u2212 f\u0304di(qi, ui) + f\u0304j(qi, ui)).\n(7) For each training example i = 1, . . . ,m, the positive item di in the example triplet is compared to all possible negative items j 6= di, and one assigns to each pair a cost if the negative item is larger or within a \u201cmargin\u201d of 1 from the positive item. These costs are called pairwise violations. Note that all pairwise violations are considered equally if they have the same margin violation, independent of their position in the list. For this reason the margin ranking loss might not optimize the top k very accurately as it cares about the average rank.\nTo instead focus on the top of the ranked list of returned items we employ a recently introduced loss function that has been developed for document retrieval (Usunier et al., 2009; Weston et al., 2010; 2012). To the best of our knowledge this method has not been applied to collaborative filtering type tasks before. The main idea is to weigh the pairwise violations depending on their position in the ranked list. One considers a class of ranking error functions:\nerrWARP = m\u2211\ni=1\nL(rankdi(f\u0304(qi,ui))) (8)\nwhere rankdi(f\u0304(qi, ui)) is the margin-based rank of the labeled item given in the ith training example:\nranki(f\u0304(q, u)) = \u2211 j 6=i \u03b8(1 + f\u0304j(q, u) \u2265 f\u0304i(q, u))\nwhere \u03b8 is the indicator function, and L(\u00b7) transforms this rank into a loss:\nL(r) = r\u2211\ni=1\n\u03b1i, with \u03b11 \u2265 \u03b12 \u2265 \u00b7 \u00b7 \u00b7 \u2265 0. (9)\nDifferent choices of \u03b1 define different weights (importance) of the relative position of the positive examples\nin the ranked list. In particular it was shown that by choosing \u03b1i = 1/i a smooth weighting over positions is given, where most weight is given to the top position, with rapidly decaying weight for lower positions. This is useful when one wants to optimize precision at k for a variety of different values of k at once (Usunier et al., 2009). (Note that choosing \u03b1i = 1 for all i we have the same AUC optimization as equation (7)).\nWe optimize this function by stochastic gradient descent (SGD) following the authors of (Weston et al., 2010), that is samples are drawn at random, and a gradient step is made for each draw. Due to the cost of computing the exact rank in (8) it is approximated by sampling. That is, for a given positive label, one draws negative labels until a violating pair is found, and then approximates the rank with\nrankd(f\u0304(q, u)) \u2248 \u230a |D| \u2212 1 N \u230b where b.c is the floor function, |D| is the number of items in the database and N is the number of trials in the sampling step. Intuitively, if we need to sample more negative items before we find a violator then the rank of the true item is likely to be small (i.e., at the top of the list, as few negatives are above it).\nFinally, our models have many parameters to be learnt. One can regularize them by preferring smaller weights. We constrain the parameters using ||Si|| \u2264 C, i = 1, . . . , |Q|, ||Vi|| \u2264 C, i = 1, . . . , |U|, ||Ti|| \u2264 C, i = 1, . . . , |D| (leaving U unconstrained). During SGD one projects the parameters back on to the constraints at each step, following the same procedure used in several other works, e.g. (Weston et al., 2010; Bai et al., 2009)."}, {"heading": "3. Prior Work and Connections", "text": ""}, {"heading": "3.1. Connections to matrix factorization for collaborative filtering", "text": "Many works for collaborative filtering tasks have proposed using factorized models. In particular, Singular Value Decomposition (SVD) and Non-negative Matrix Factorization (NMF) (Billsus & Pazzani, 1998; Lee & Seung, 2001) are two popular choices. The main two differences between our approach and these general matrix factorization techniques is that (i) each recommendation we make is seeded with a query (i.e. the collaborative retrieval task), and (ii) in collaborative retrieval tasks we are interested in the top k returned items, so our method optimizes for that goal.\nMost collaborative filtering work does not consider a ranking type loss that optimizes the top k, but one notable exception is (Weimer et al., 2007). They do\nnot consider tensor factorizations.\nThere are several ways to factorize a tensor, some classical ways are Tucker decomposition (Tucker, 1966) and PARAFAC (Harshman, 1970). Several collaborative filtering techniques have considered tensor factorisations before, in particular for taking into account user context features like tags (Rendle & SchmidtThieme, 2010), web pages (Menon et al., 2011), age and gender (Karatzoglou et al., 2010), time (Xiong et al., 2010) or user location for mobile phone recommendation (Zheng et al., 2010) but not, to our knowledge, for the collaborative retrieval task.\nFinally, we should also note that some works have combined collaborative filtering data with content-based features before, e.g. (Wang & Blei, 2011)."}, {"heading": "3.2. Connections to matrix factorization and information retrieval", "text": "In information retrieval one is required to rank items (documents) given a query using the content-features of the items, e.g. for document retrieval one uses the words in the document. In that case, Latent Semantic Indexing (Deerwester et al., 1990), and related methods such as LDA (Blei et al., 2003), are unsupervised methods that choose a low dimensional feature representation of the words. The parameterization of those models is a special case of our models. If we consider our model from equation (5) but remove the influence of the user model, i.e. set Uu = I and Vu = 0 we are left with a standard document retrieval model:\nfDR(q, d) = \u03a6\u0302Q(q)>W>QWD\u03a6\u0302D(d). (10)\nMore recently, factorized models that are supervised to the task of document retrieval have been proposed, for example Polynomial Semantic Indexing (PSI) (Bai et al., 2009). PSI considers polynomial terms between document words and query words for higher order similarities. For degree 2 it has the form of (10) but for degree 3 it uses tensor factorizations based on:\nf3(q, d) = \u2211\nk\n(S\u03a6\u0302Q(q))k(U \u03a6\u0302D(d))k(V \u03a6\u0302D(d))k.\nThis is closely related to our model (5) when constraining Ui = Di and replacing the user input by the document input, i.e. we compute f(q, d, d) in order to obtain interactions between document words rather than between document and user.\nMethods like LSI or LDA optimize the reconstruction error (mean squared error or likelihood). PSI optimizes the AUC ranking loss, which is more related to our ranking approach but does not optimize the top k\nresults like ours. Methods for annotating images (Weston et al., 2010) and labeling songs with tags (Weston et al., 2012) have been proposed that do use the WARP loss we employ in this paper. Many methods for document retrieval also optimize the top k but typically not using factorized models like ours, see e.g. (Yue et al., 2007).\nFinally, our models are applicable to the task of \u201cpersonalized search\u201d where some topic model approaches have recently been studied (Harvey et al., 2011; Lin et al., 2005; Sun et al., 2005; Saha et al., 2009). We will compare to generalized SVD and NMF models in our experiments which are related to these works."}, {"heading": "4. Experiments", "text": "Traditional collaborative filtering datasets like the Netflix challenge dataset, and information retrieval datasets, like LETOR for instance, cannot be used in the collaborative retrieval framework, as they either lack the query or the user information necessary. We therefore use the three datasets described below."}, {"heading": "4.1. Lastfm Dataset", "text": "We used the \u201cLast.fm Dataset - 1K users\u201d dataset available from http://www.dtic.upf.edu/\u223cocelma/ MusicRecommendationDataset/lastfm-1K.html. This dataset contains (user, timestamp, artist, song) tuples collected from the Last.fm (www.lastfm.com) API. This dataset represents the listening history (until May 5th, 2009) for 992 users and 176,948 artists. Two consecutively played artists by the same user are considered as a query \u00d7 user \u00d7 item triple. This mirrors the task of playlisting, where a user selects a seed track, and the machine has to automatically build a list of tracks. We consider two artists as \u201cconsecutive\u201d if they are played within an hour of each other (via the timestamp), otherwise we ignore the pair. One in every five days (so that the data is disjoint) were left aside for testing, and the remaining data was used for training and validation. Overall this gave 5,408,975 training triples, 500,000 validation triples and 1,434,568 test triples."}, {"heading": "4.2. Playlist head and tail datasets", "text": "We had access to a larger scale proprietary and anonymized dataset of user playlists where we could both construct a query \u00d7 user \u00d7 item matrix from consecutive tracks, and had access to content-based features as well so we can test our content-based feature methods. The first extracted dataset (\u201chead\u201d dataset) consists of 46,000 users and 943,284 tracks\nfrom 146,369 artists (each artist appears at least 10 times). The data is split into 17M training triples for training, 172,000 for validation and 1.7M for test.\nThe above \u201chead\u201d dataset can be built for artists where we have enough training data. However, a user may want to do retrieval with a query or an item for which we have no tensor training data at all (i.e., the cold-start problem). In that case, content-based feature approaches are the only option. To evaluate this setup we hence built a \u201ctail\u201d testing dataset consisting of 10,000 triples from 5442 artists where we only have a single test example. The idea in that case is to train on the head dataset, and test on the tail (as it is not possible to train on the tail).\nFor each track (including head tracks) we have access to the audio features of the track, which we processed using the well-known Mel Frequency Cepstral Coefficent (MFCC) representation. MFCCs take advantage of source/filter deconvolution from the cepstral transform and perceptually-realistic compression of spectra from the Mel pitch scale and have been used widely in music and speech (Foote, 1997; Rabiner & Juang, 1993). We extracted 13 MFCCs every 10ms over a Hamming window of 25ms, and first and second derivatives were concatenated, for a total of 39 features. We then computed a dictionary of 2000 typical MFCC vectors over the training set (using K-means) and represented each song as a vector of counts, over the set of frames in the given song, of the number of times each dictionary vector was nearest to the frame in the MFCC space. The resulting feature vectors thus have dimension nD = 2000."}, {"heading": "4.3. Baselines", "text": "We compare to Singular Value Decomposition (SVD) and Non-negative Matrix Factorization (NMF) which are both popular methods for collaborative filtering tasks. For SVD we use the Matlab implementation and for NMF we use the implementation at http://www. csie.ntu.edu.tw/\u223ccjlin/nmf/. Standard SVD and NMF operate on matrices, not tensors, so we compare our method on those tasks (where we consider only user \u00d7 item matrices or only query \u00d7 item matrices) as well. For the query \u00d7 user \u00d7 item tensor we considered the following generalization of SVD or NMF:\nf(q, u, i) = \u03a6(q)>U>QIVQI\u03a6(d) + \u03b3\u03a6(u) >U>UIVUI\u03a6(d).\nThat is, we perform two SVDs (or NMFs), one for the user \u00d7 item matrix and one for the query \u00d7 item matrix, and then combine them with a mixing parameter \u03b3 which is chosen on the validation set.\nFor LCR, we compare both the versions from equation\n(3) and equation (2) on the query \u00d7 user \u00d7 item task. The former is directly comparable to the SVD and NMF tensor generalizations we use (they are the same paramaterization) while the latter takes into account three-way interactions between query, user and item in a single joint formulation. For the user \u00d7 item and query \u00d7 item tasks we employ either only the first or the second term respectively of equation (3). The validation set is used to choose the hyperparameters, e.g. the best choice of learning rate, regularization parameter and as a stopping criterion for the gradient descent.\nFor content-based features we also compare to LSI (Deerwester et al., 1990) and using cosine similarity. Both of these methods perform retrieval given the query, and ignore the user term."}, {"heading": "4.4. Evaluation", "text": "For any given query q, user u, item i triple we compute f(q, u, i\u0302) using the given algorithm for each possible item i\u0302 and sort them, largest first. For user \u00d7 item or query \u00d7 item tasks the setup is the same except either q or u is not used in all the competing models. The evaluation score for a given triple is then computed according to where item i appears in the ranked list. We measure recall@k, which is 1 if item i appears in the top k, and 0 otherwise. We report mean recall@k over the entire test set. Note that as we only consider\none positive example per query (the element i of the triple) precision@k = recall@k / k."}, {"heading": "4.5. LastFM dataset Results", "text": "We first report results on the lastfm dataset. Detailed results where we fixed the embedding dimension of all methods to n = 50 are given in Table 1. Results for other choices of n are given in Table 4. On all three tasks (query \u00d7 item, user \u00d7 item and query \u00d7 user \u00d7 item) LCR is superior to SVD and NMF for each topranked set k considered. Furthermore, our full LCR query \u00d7 user \u00d7 item model (c.f. equation 2, Ui unconstrained) gives improved results compared to both (i) any competing methods, including LCR itself, that do not take into account both query and user; and (ii) LCR itself (and other methods) that do not model the query and user in a joint similarity function (i.e. LCR query x user x item (cf. eq. (2)) outperforms LCR query x item + user x item (cf. eq. (3))).\nLoss function evaluation Some of the improvement of LCR over SVD and NMF can be explained by the fact that neither SVD nor NMF optimize a ranking function that optimizes the top-ranked items. To show the importance of the loss function, we report the results of LCR using an alternative loss function optimizing average rank (AUC) as in equation (7) instead of the WARP loss from equation (8). The comparison,\nNMF q x i + u x i 12.7% 16.9% 20.5% 23.6% SVD q x i + u x i 13.3% 17.9% 25.9% 25.7% LCR q x i + u x i 22.3% 27.9% 30.2% 31.3% LCR query x user x item 23.3% 28.9% 32.2% 33.3%\ngiven in Table 2 shows a clear gain on all tasks by optimizing for the top k (using WARP). Optimizing AUC instead yields results in fact similar to SVD. SVD optimizes mean squared error, not AUC, but the similarity is that neither loss function pays special attention to the top k results.\nChanging the embedding dimension We report results varying the embedding dimension n in Table 4. It should be noted that n affects both test performance, evaluation time and storage requirements, so low dimensional embeddings are preferable if they perform well enough. LCR outperforms the baselines for all values of n that we tried, however all methods degrade significantly when n = 10. SVD on the query \u00d7 user \u00d7 item shows the same performance for n = 50 and n = 100 while LCR improves slightly."}, {"heading": "4.6. Playlist dataset results", "text": "Collaborative filtering type data The Playlist dataset is larger scale and has both collaborativefiltering type data and content-based features. We first tested using collaborative filtering type data only on the same three tasks as before (query \u00d7 item, user \u00d7 item and query \u00d7 user \u00d7 item). The results are given in Table 3. They again show a performance improvement for LCR over the SVD and NMF baselines on the query \u00d7 item task, although on the user \u00d7 item task it performs similarly to the baselines. However, on the most interesting task, query \u00d7 user \u00d7 item, we again see a large performance gain.\nUsing content-based features We compared different algorithms using content-based features on the tail dataset where collaborative filtering cannot be used. (We also attempted to combine both collaborative filtering and content-based information on the head dataset, but we observed no gain in performance over collaborative filtering alone, probably because the content-based features are not strong enough, which is not really a surprising result (Slaney, 2011)). The results on the tail dataset are given in Table 5. LCR q\u00d7i (which does not use user information, as in eq. (10)) already outperforms cosine similarity and LSI. Adding user information further improves performance: LCR q\u00d7u+q\u00d7i uses the model form of eq. (4) with Ui = I and LCR query\u00d7user\u00d7item uses eq. (4) with Ui = Di."}, {"heading": "5. Conclusion", "text": "In this paper we introduced a new learning framework called collaborative retrieval which links the standard document retrieval and collaborative filtering tasks. Like collaborative filtering, the task is to rank items given a user, but crucially we can also take into ac-\ncount a query term. Like document retrieval we are given a query and the task is to rank items, but crucially we also take into account the user in the form of a user \u00d7 query \u00d7 item tensor of training data.\nWe proposed a novel learning algorithm for this task that learns a factorized model to rank the items given the query and user, and showed it empirically outperforms some standard methods. Collaborative retrieval is rapidly becoming an important task and we expect this to become a well studied research area."}], "references": [{"title": "Polynomial semantic indexing", "author": ["B. Bai", "J. Weston", "D. Grangier", "R. Collobert", "K. Sadamasa", "Y. Qi", "C. Cortes", "M. Mohri"], "venue": null, "citeRegEx": "Bai et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bai et al\\.", "year": 2009}, {"title": "Learning collaborative information filters", "author": ["D. Billsus", "M.J. Pazzani"], "venue": "In ICML,", "citeRegEx": "Billsus and Pazzani,? \\Q1998\\E", "shortCiteRegEx": "Billsus and Pazzani", "year": 1998}, {"title": "Indexing by latent semantic analysis", "author": ["Deerwester", "Scott", "Dumais", "Susan T", "Furnas", "George W", "Landauer", "Thomas K", "Harshman", "Richard"], "venue": "JASIS,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Content-based retrieval of music and audio", "author": ["J.T. Foote"], "venue": "In SPIE, pp", "citeRegEx": "Foote,? \\Q1997\\E", "shortCiteRegEx": "Foote", "year": 1997}, {"title": "Foundations of the parafac procedure: models and conditions for an", "author": ["R.A. Harshman"], "venue": "explanatory\u201d multimodal factor analysis", "citeRegEx": "Harshman,? \\Q1970\\E", "shortCiteRegEx": "Harshman", "year": 1970}, {"title": "Improving social bookmark search using personalised latent variable language models", "author": ["M. Harvey", "I. Ruthven", "M.J. Carman"], "venue": "In WSDM,", "citeRegEx": "Harvey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Harvey et al\\.", "year": 2011}, {"title": "Large margin rank boundaries for ordinal regression", "author": ["R. Herbrich", "T. Graepel", "K. Obermayer"], "venue": "Advances in large margin classifiers,", "citeRegEx": "Herbrich et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Herbrich et al\\.", "year": 2000}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "In Proceedings of the eighth ACM SIGKDD,", "citeRegEx": "Joachims,? \\Q2002\\E", "shortCiteRegEx": "Joachims", "year": 2002}, {"title": "Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering", "author": ["A. Karatzoglou", "X. Amatriain", "L. Baltrunas", "N. Oliver"], "venue": "RecSys \u201910,", "citeRegEx": "Karatzoglou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Karatzoglou et al\\.", "year": 2010}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Lee and Seung,? \\Q2001\\E", "shortCiteRegEx": "Lee and Seung", "year": 2001}, {"title": "Using probabilistic latent semantic analysis for personalized web search", "author": ["C. Lin", "G.R. Xue", "H.J. Zeng", "Y. Yu"], "venue": "Web Technologies Research and DevelopmentAPWeb", "citeRegEx": "Lin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2005}, {"title": "Response prediction using collaborative filtering with hierarchies and side-information", "author": ["A.K. Menon", "K.P. Chitrapura", "S. Garg", "D. Agarwal", "N. Kota"], "venue": "In SIGKDD,", "citeRegEx": "Menon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Menon et al\\.", "year": 2011}, {"title": "Pairwise interaction tensor factorization for personalized tag recommendation", "author": ["S. Rendle", "L. Schmidt-Thieme"], "venue": "In WSDM,", "citeRegEx": "Rendle and Schmidt.Thieme,? \\Q2010\\E", "shortCiteRegEx": "Rendle and Schmidt.Thieme", "year": 2010}, {"title": "Tensor framework and combined symmetry for hypertext mining", "author": ["S. Saha", "CA Murthy", "S.K. Pal"], "venue": "Fundamenta Informaticae,", "citeRegEx": "Saha et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Saha et al\\.", "year": 2009}, {"title": "Web-scale multimedia analysis: Does content matter", "author": ["M. Slaney"], "venue": "Multimedia, IEEE,", "citeRegEx": "Slaney,? \\Q2011\\E", "shortCiteRegEx": "Slaney", "year": 2011}, {"title": "Web-page summarization using clickthrough data", "author": ["J.T. Sun", "D. Shen", "H.J. Zeng", "Q. Yang", "Y. Lu", "Z. Chen"], "venue": "In SIGIR,", "citeRegEx": "Sun et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2005}, {"title": "Some mathematical notes on three-mode factor analysis", "author": ["L.R. Tucker"], "venue": "Psychometrika, 31(3):279\u2013311,", "citeRegEx": "Tucker,? \\Q1966\\E", "shortCiteRegEx": "Tucker", "year": 1966}, {"title": "Ranking with ordered weighted pairwise classification", "author": ["Usunier", "Nicolas", "Buffoni", "David", "Gallinari", "Patrick"], "venue": "In ICML, pp. 1057\u20131064,", "citeRegEx": "Usunier et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Usunier et al\\.", "year": 2009}, {"title": "Collaborative topic modeling for recommending scientific articles", "author": ["C. Wang", "D.M. Blei"], "venue": "In 17th ACM SIGKDD,", "citeRegEx": "Wang and Blei,? \\Q2011\\E", "shortCiteRegEx": "Wang and Blei", "year": 2011}, {"title": "Cofirank-maximum margin matrix factorization for collaborative ranking", "author": ["M. Weimer", "A. Karatzoglou", "Q. Le", "A Smola"], "venue": null, "citeRegEx": "Weimer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Weimer et al\\.", "year": 2007}, {"title": "Large scale image annotation: Learning to rank with joint word-image embeddings", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "In ECML,", "citeRegEx": "Weston et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2010}, {"title": "Large-scale music annotation and retrieval: Learning to rank in joint semantic spaces", "author": ["J. Weston", "S. Bengio", "P. Hamel"], "venue": "In Journal of New Music Research,", "citeRegEx": "Weston et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2012}, {"title": "Temporal collaborative filtering with bayesian probabilistic tensor factorization", "author": ["L. Xiong", "X. Chen", "T.K. Huang", "J. Schneider", "J.G. Carbonell"], "venue": "In Proceedings of SIAM Data", "citeRegEx": "Xiong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2010}, {"title": "A support vector method for optimizing average precision", "author": ["Yue", "Yisong", "T. Finley", "F. Radlinski", "T. Joachims"], "venue": "In SIGIR,", "citeRegEx": "Yue et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2007}, {"title": "Collaborative filtering meets mobile recommendation: A user-centered approach", "author": ["V.W. Zheng", "B. Cao", "Y. Zheng", "X. Xie", "Q. Yang"], "venue": "In AAAI,", "citeRegEx": "Zheng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "A standard loss function that is often used for retrieval is the margin ranking criterion (Herbrich et al., 2000; Joachims, 2002), in particular it was used for learning factorized document retrieval models in Bai et al.", "startOffset": 90, "endOffset": 129}, {"referenceID": 7, "context": "A standard loss function that is often used for retrieval is the margin ranking criterion (Herbrich et al., 2000; Joachims, 2002), in particular it was used for learning factorized document retrieval models in Bai et al.", "startOffset": 90, "endOffset": 129}, {"referenceID": 0, "context": ", 2000; Joachims, 2002), in particular it was used for learning factorized document retrieval models in Bai et al. (2009). Let us first write the predictions of our model for all items in the database as a vector f\u0304(q, u) where the i index is f\u0304i(q, u) = f(q, u, i).", "startOffset": 104, "endOffset": 122}, {"referenceID": 17, "context": "To instead focus on the top of the ranked list of returned items we employ a recently introduced loss function that has been developed for document retrieval (Usunier et al., 2009; Weston et al., 2010; 2012).", "startOffset": 158, "endOffset": 207}, {"referenceID": 20, "context": "To instead focus on the top of the ranked list of returned items we employ a recently introduced loss function that has been developed for document retrieval (Usunier et al., 2009; Weston et al., 2010; 2012).", "startOffset": 158, "endOffset": 207}, {"referenceID": 17, "context": "This is useful when one wants to optimize precision at k for a variety of different values of k at once (Usunier et al., 2009).", "startOffset": 104, "endOffset": 126}, {"referenceID": 20, "context": "We optimize this function by stochastic gradient descent (SGD) following the authors of (Weston et al., 2010), that is samples are drawn at random, and a gradient step is made for each draw.", "startOffset": 88, "endOffset": 109}, {"referenceID": 20, "context": "(Weston et al., 2010; Bai et al., 2009).", "startOffset": 0, "endOffset": 39}, {"referenceID": 0, "context": "(Weston et al., 2010; Bai et al., 2009).", "startOffset": 0, "endOffset": 39}, {"referenceID": 19, "context": "Most collaborative filtering work does not consider a ranking type loss that optimizes the top k, but one notable exception is (Weimer et al., 2007).", "startOffset": 127, "endOffset": 148}, {"referenceID": 16, "context": "There are several ways to factorize a tensor, some classical ways are Tucker decomposition (Tucker, 1966) and PARAFAC (Harshman, 1970).", "startOffset": 91, "endOffset": 105}, {"referenceID": 4, "context": "There are several ways to factorize a tensor, some classical ways are Tucker decomposition (Tucker, 1966) and PARAFAC (Harshman, 1970).", "startOffset": 118, "endOffset": 134}, {"referenceID": 11, "context": "Several collaborative filtering techniques have considered tensor factorisations before, in particular for taking into account user context features like tags (Rendle & SchmidtThieme, 2010), web pages (Menon et al., 2011), age and gender (Karatzoglou et al.", "startOffset": 201, "endOffset": 221}, {"referenceID": 8, "context": ", 2011), age and gender (Karatzoglou et al., 2010), time (Xiong et al.", "startOffset": 24, "endOffset": 50}, {"referenceID": 22, "context": ", 2010), time (Xiong et al., 2010) or user location for mobile phone recommendation (Zheng et al.", "startOffset": 14, "endOffset": 34}, {"referenceID": 24, "context": ", 2010) or user location for mobile phone recommendation (Zheng et al., 2010) but not, to our knowledge, for the collaborative retrieval task.", "startOffset": 57, "endOffset": 77}, {"referenceID": 2, "context": "In that case, Latent Semantic Indexing (Deerwester et al., 1990), and related methods such as LDA (Blei et al.", "startOffset": 39, "endOffset": 64}, {"referenceID": 0, "context": "More recently, factorized models that are supervised to the task of document retrieval have been proposed, for example Polynomial Semantic Indexing (PSI) (Bai et al., 2009).", "startOffset": 154, "endOffset": 172}, {"referenceID": 20, "context": "Methods for annotating images (Weston et al., 2010) and labeling songs with tags (Weston et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 21, "context": ", 2010) and labeling songs with tags (Weston et al., 2012) have been proposed that do use the WARP loss we employ in this paper.", "startOffset": 37, "endOffset": 58}, {"referenceID": 23, "context": "(Yue et al., 2007).", "startOffset": 0, "endOffset": 18}, {"referenceID": 5, "context": "Finally, our models are applicable to the task of \u201cpersonalized search\u201d where some topic model approaches have recently been studied (Harvey et al., 2011; Lin et al., 2005; Sun et al., 2005; Saha et al., 2009).", "startOffset": 133, "endOffset": 209}, {"referenceID": 10, "context": "Finally, our models are applicable to the task of \u201cpersonalized search\u201d where some topic model approaches have recently been studied (Harvey et al., 2011; Lin et al., 2005; Sun et al., 2005; Saha et al., 2009).", "startOffset": 133, "endOffset": 209}, {"referenceID": 15, "context": "Finally, our models are applicable to the task of \u201cpersonalized search\u201d where some topic model approaches have recently been studied (Harvey et al., 2011; Lin et al., 2005; Sun et al., 2005; Saha et al., 2009).", "startOffset": 133, "endOffset": 209}, {"referenceID": 13, "context": "Finally, our models are applicable to the task of \u201cpersonalized search\u201d where some topic model approaches have recently been studied (Harvey et al., 2011; Lin et al., 2005; Sun et al., 2005; Saha et al., 2009).", "startOffset": 133, "endOffset": 209}, {"referenceID": 3, "context": "MFCCs take advantage of source/filter deconvolution from the cepstral transform and perceptually-realistic compression of spectra from the Mel pitch scale and have been used widely in music and speech (Foote, 1997; Rabiner & Juang, 1993).", "startOffset": 201, "endOffset": 237}, {"referenceID": 2, "context": "For content-based features we also compare to LSI (Deerwester et al., 1990) and using cosine similarity.", "startOffset": 50, "endOffset": 75}, {"referenceID": 14, "context": "(We also attempted to combine both collaborative filtering and content-based information on the head dataset, but we observed no gain in performance over collaborative filtering alone, probably because the content-based features are not strong enough, which is not really a surprising result (Slaney, 2011)).", "startOffset": 292, "endOffset": 306}], "year": 2012, "abstractText": "Retrieval tasks typically require a ranking of items given a query. Collaborative filtering tasks, on the other hand, learn to model user\u2019s preferences over items. In this paper we study the joint problem of recommending items to a user with respect to a given query, which is a surprisingly common task. This setup differs from the standard collaborative filtering one in that we are given a query \u00d7 user \u00d7 item tensor for training instead of the more traditional user \u00d7 item matrix. Compared to document retrieval we do have a query, but we may or may not have content features (we will consider both cases) and we can also take account of the user\u2019s profile. We introduce a factorized model for this new task that optimizes the top-ranked items returned for the given query and user. We report empirical results where it outperforms several baselines.", "creator": "LaTeX with hyperref package"}}}