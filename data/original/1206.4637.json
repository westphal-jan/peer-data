{"id": "1206.4637", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Learning to Identify Regular Expressions that Describe Email Campaigns", "abstract": "This paper addresses the problem of inferring a regular expression from a given set of strings that resembles, as closely as possible, the regular expression that a human expert would have written to identify the language. This is motivated by our goal of automating the task of postmasters of an email service who use regular expressions to describe and blacklist email spam campaigns. Training data contains batches of messages and corresponding regular expressions that an expert postmaster feels confident to blacklist. We model this task as a learning problem with structured output spaces and an appropriate loss function, derive a decoder and the resulting optimization problem, and a report on a case study conducted with an email service.", "histories": [["v1", "Mon, 18 Jun 2012 15:15:28 GMT  (542kb)", "http://arxiv.org/abs/1206.4637v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.CL stat.ML", "authors": ["paul prasse", "christoph sawade", "niels landwehr", "tobias scheffer"], "accepted": true, "id": "1206.4637"}, "pdf": {"name": "1206.4637.pdf", "metadata": {"source": "META", "title": "Learning to Identify Regular Expressions that Describe Email Campaigns", "authors": ["Paul Prasse", "Christoph Sawade", "Niels Landwehr", "Tobias Scheffer"], "emails": ["prasse@cs.uni-potsdam.de", "sawade@cs.uni-potsdam.de", "landwehr@cs.uni-potsdam.de", "scheffer@cs.uni-potsdam.de"], "sections": [{"heading": "1. Introduction", "text": "Popular spam dissemination tools allow users to implement mailing campaigns by specifying simple grammars that serve as message templates. A grammar is disseminated to nodes of a bot net, the nodes create messages by instantiating the grammar at random. Email service providers can easily sample elements of new mailing campaigns by collecting messages in spam traps or by tapping into known bot nets. When messages from multiple campaigns are collected in a joint spam trap, clustering tools can separate the campaigns reliably (Haider & Scheffer, 2009). However, probabilistic cluster descriptions that use a bag-of-words representation incur the risk of false positives, and it\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nis difficult for a human to decide whether they in fact characterize the correct set of messages.\nRegular expressions are a standard tool for specifying simple grammars. Widely available tools match strings against regular expressions efficiently and can be used conveniently from scripting languages. A regular expression can be translated into a finite state machine that accepts the language and has an execution time linear in the length of the input string. A specific, comprehensible regular expression which covers the observed instances and has been written by an expert postmaster can be used to blacklist the bulk of emails of that campaign at virtually no risk of covering any other messages.\nLanguage identification has a rich history in the algorithmic learning theory community (see Section 6). Our problem setting differs from the problem of language identification in the learner\u2019s exact goal, and in the available training data. Batches of strings and corresponding regular expressions are observable in the training data. The learner\u2019s goal is to produce a predictive model that maps batches of strings to regular expressions that resemble as closely as possible the regular expressions which the postmaster would have written and feels confident to blacklist (see Figure 1).\n\u0393ysyn(v0) = [b0-9]{2}c(aa|b)\u2217\n\u0393ysyn(v1) = [b0-9]{2}\n\u0393ysyn(v2) = [b0-9]\n\u0393ysyn(v3) = b \u0393 y syn(v4) = 0-9\n\u0393ysyn(v5) = c \u0393 y syn(v6) = (aa|b)\u2217\n\u0393ysyn(v7) = aa|b\n\u0393ysyn(v8) = aa\n\u0393ysyn(v11) = b\n\u0393y,xpar(v \u2032 0) = [b0-9]{2}c(aa|b)\u2217\n\u0393y,xpar(v \u2032 1) = [b0-9]{2}\n\u0393y,xpar(v \u2032 2) = [b0-9]\n\u0393y,xpar(v \u2032 3) = 0-9\n\u0393y,xpar(v \u2032 5) = [b0-9]\n\u0393y,xpar(v \u2032 6) = b\n\u0393y,xpar(v \u2032 7) = c \u0393 y,x par(v \u2032 8) = (aa|b)\u2217\n\u0393y,xpar(v \u2032 9) = \u01eb\nThe rest of this paper is structured as follows. Section 2 reviews regular expressions before Section 3 states the problem setting. Section 4 introduces the feature representation and derives the decoder and the optimization problem. In Section 5, we discuss our findings from a case study with an email service. Section 6 discusses related work; Section 7 concludes."}, {"heading": "2. Regular Expressions", "text": "Syntactically, a regular expression y \u2208 Y\u03a3 is either a character from an alphabet \u03a3, or it is an expression in which an operator is applied to one or several argument expressions. Basic operators are the concatenation (e.g., \u201cabc\u201d), disjunction (e.g., \u201ca|b\u201d), and the Kleene star (\u201c\u2217\u201d), written in postfix notation, that accepts any number of repetitions of its preceding argument expression. Parentheses define the syntactic structure of the expression. Several shorthands improve the readability of regular expressions and can be defined in terms of the basic operators. For instance, the any character symbol (\u201c.\u201d) abbreviates the disjunction of all characters in \u03a3, square brackets accept the disjunction of all characters (e.g., \u201c[abc]\u201d) or ranges (e.g., \u201c[a-z0-9]\u201d) that are included. The postfix operator \u201c+\u201d accepts an arbitrary, positive number of reiterations of the preceding expression, while \u201c{l, u}\u201d accepts between l and u reiterations, where l \u2264 u. We include a set of popular macros\u2014for instance \u201c\\d\u201d for any digit. A formal definition of the set of regular expressions can be found in the online appendix.\nThe syntactic structure of a regular expression is represented by its syntax tree Tysyn = (V ysyn, E y syn,\u0393 y syn,\u2264ysyn). Definition 3 in the online appendix assigns one such tree to each regular expression. A node v \u2208 V ysyn of this tree is tagged by labeling function \u0393ysyn : V y syn \u2192 Y\u03a3 with a subexpression \u0393ysyn(v) = yj . Edges (v, v \u2032) \u2208 Eysyn indicate that node v\u2032 represents an argument expression of v.\nRelation \u2264ysyn\u2286 V ysyn \u00d7 V ysyn defines an ordering on the nodes and identifies the root node.\nA regular expression y defines a regular language L(y). Given the regular expression, a deterministic finite state machine can decide whether a string x is in L(y) in time linear in |x| (Dube\u0301 & Feeley, 2000). The trace of verification is typically represented as a parse tree Ty,xpar = (V y,x par , E y,x par,\u0393 y,x par,\u2264y,xpar), describing how the string x can be derived from the regular expression y. At least one parse tree exists if and only if the string is an element of the language L(y); in this case, y is said to generate x. Nodes v \u2208 V ysyn of the syntax tree generate the nodes of the parse tree v\u2032 \u2208 V y,xpar ; a node of the syntax tree may spawn none (alternatives which are not used to generate a string), one, or several (\u201cloopy\u201d syntactic elements such as \u201c\u2217\u201d or \u201c+\u201d) nodes in the parse tree. In analogy to the syntax trees, the labeling function \u0393y,xpar : V y,x par \u2192 Y\u03a3 assigns a subexpression to each node, and the relation \u2264y,xpar\u2286 V y,xpar \u00d7V y,xpar defines the ordering of sibling nodes. The set of all parse trees for a regular expression y and a string x is denoted by T y,xpar . A formal definition can be found in the online appendix.\nLeaf nodes of a parse tree Ty,xpar are labeled with elements of \u03a3 \u222a { }, where denotes the empty symbol; reading them from left to right gives the generated string x. Non-terminal nodes correspond to subexpressions yj of y which generate substrings of x. To compare different regular expressions with respect to a given string x, we define the set Ty,xpar |i of labels of nodes which are visited on the path from the root to the the i-th character of x in the parse tree Ty,xpar .\nFigure 2 shows an example of a syntax tree Tysyn and a parse tree Ty,xpar for the regular expression y = [b0-9]{2}c(aa|b)\u2217 and the string x = 1bc. Finally, we introduce the concept of a matching list. When a regular expression y generates a set x of strings, and v \u2208 V ysyn is an arbitrary node of the syn-\ntax tree of y, then the matching list My,x(v) characterizes which substrings of the strings in x are generated by the node v of the syntax tree. A node v of the syntax tree generates a substring x\u2032 of x \u2208 x, if v generates a node v\u2032 in the parse tree Ty,xpar of x, and there is a path from v\u2032 in that parse tree to every character in the substring x\u2032. In the above example, for the set of strings x = {12c, b4ca}, the matching list for node v1 that represents subexpression [b0-9]{2} is My,x(v2) = {12, b4}. Definition 4 in the online appendix introduces matching lists formally."}, {"heading": "3. Problem Setting", "text": "Having established the syntax and semantics of regular expressions, we now turn towards the problem setting. An unknown distribution p(x,y) generates regular expressions y \u2208 Y\u03a3 and batches x of strings x \u2208 x that are elements of the language L(y). In our motivating application, the strings x are emails sampled from a bot net, and the y are regular expressions which an expert postmaster believes to identify the campaign template, and feels confident to blacklist.\nA w-parameterized predictive model fw : x 7\u2192 y\u0302 accepts a batch of strings and conjectures a regular expression y\u0302. We now define the loss \u2206(y, y\u0302,x) that captures the deviation of the conjecture y\u0302 from y for batch x. In our application, postmasters will not use an expression to blacklist the campaign unless they consider it to be comprehensibly and neatly written, and believe it to accurately identify the campaign.\nLoss function \u2206(y, y\u0302,x) compares each of the accepting parse trees in T y,xpar , for each string x \u2208 x, with the most similar tree in T y\u0302,xpar ; if no such parse tree exists, the summand is defined as 1|x| (Equation 1). Similarly to a loss function for hierarchical classification (CesaBianchi et al., 2006), the difference of two parse trees for string x is quantified by a comparison of the paths that lead to the characters of the string; paths are compared by means of the intersection of their nodes (Equation 2). By its definition, this loss function is bounded between zero and one; it attains zero if and only if the expressions y and y\u0302 are equal.\n\u2206(y, y\u0302,x) = 1 |x| \u2211 x\u2208x { \u2206tree(y, y\u0302, x) if x\u2208L(y\u0302) 1 otherwise (1)\nwith \u2206tree(y, y\u0302, x) (2)\n= 1\u2212 1|T y,xpar | \u2211\nt\u2208T y,xpar\nmax t\u2032\u2208T y\u0302,xpar\n1\n|x| |x|\u2211 j=1 |t|j \u2229 t\u2032|j | max{|t|j |, |t\u2032|j |}\nWe will also explore the zero-one loss, \u22060/1(y, y\u0302,x) = Jy 6= y\u0302K, where J.K is the indicator function of its boolean argument. The zero-one loss serves as an alternative, conceptually simpler reference model.\nOur goal is to find the model fw with minimal risk\nR[fw] = \u222b\u222b \u2206(y, fw(x),x)p(x,y)dx dy. (3)\nTraining data D = {(xi,yi)}mi=1 consists of pairs of batches xi and generating regular expressions yi, drawn according to p(x,y).\nSince the true distribution p(x,y) is unknown, the risk R[fw] cannot be calculated. We state the learning problem as the problem of minimizing the regularized empirical counterpart of the risk over the parameters w and the regularizer \u2126(w):\nR\u0302[fw] = 1\nm \u2211 (x,y)\u2208D \u2206(y, fw(x),x) + \u2126(w). (4)"}, {"heading": "4. Identifying Regular Expressions", "text": "We model fw as a linear discriminant function wT\u03a8(x,y) for a joint feature representation of the input x and output y (Tsochantaridis et al., 2005):\nfw(x) = arg max y\u2208Y\u03a3\nwT\u03a8(x,y). (5)"}, {"heading": "4.1. Joint Feature Representation", "text": "The joint feature representation \u03a8(y,x) captures structural properties of an expression y and joint properties of input batch x and regular expression y.\nStructural properties of a regular expression y are captured by features that indicate a specific nesting of regular expression operators\u2014for instance, whether a concatenation occurs within a disjunction. More formally, we first define a binary vector\n\u039b(y) =  Jy = y1 . . .ykK Jy = y1| . . . |ykK Jy = [y1 . . .yk]K Jy = y\u22171K Jy = y1?K Jy = y+1 K Jy = y1{l}K Jy = y1{l, u}K Jy = r1K ...\nJy = rlK Jy \u2208 \u03a3K Jy = K\n\n(6)\nencoding the top-level operator used in the regular expression y. In Equation 6, y1, . . . ,yk \u2208 Y\u03a3 are regular\nexpressions, l, u \u2208 N, and {r1, . . . , rl} is a set of ranges and popular macros; for our application, we use the set {0-9, a-f, a-z,A-F,A-Z, \\S, \\e, \\w, \\d, \u201c.\u201d}. For any two nodes v\u2032 and v\u2032\u2032 in the syntax tree of y that are connected by an edge\u2014indicating that y\u2032\u2032 = \u0393ysyn(v\n\u2032\u2032) is an argument subexpression of y\u2032 = \u0393ysyn(v\n\u2032)\u2014the tensor product \u039b(y\u2032) \u2297 \u039b(y\u2032\u2032) defines a binary vector that encodes the specific nesting of operators at node v\u2032. Feature vector \u03a8(x,y) will aggregate these vectors over all pairs of adjacent nodes in the syntax tree of y.\nJoint properties of an input batch x and a regular expression y are encoded as follows. Recall that for any node v\u2032 in the syntax tree, My,x(v\u2032) denotes the set of substrings in x that are generated by the subexpression y\u2032 = \u0393ysyn(v\n\u2032) that v\u2032 is labeled with. We define a vector \u03a6(My,x(v\u2032)) of attributes of this set. Any property may be accounted for; for our application, we include the average string length, the inclusion of the empty string, the proportion of capital letters, and many other attributes. The full list of attributes used in our experiments is included in the online appendix. A joint encoding of properties of the subexpression y\u2032 and the set of substrings generated by y\u2032 is given by the tensor product \u03a6(My,x(v\u2032))\u2297 \u039b(y\u2032). The joint feature vector \u03a8(x,y) is obtained by aggregating operator-nesting information over all edges in the syntax tree, and joint properties of subexpressions y\u2032 and the set of substrings they generate over all nodes in the syntax tree:\n\u03a8(x,y) (7)\n=\n(\u2211 (v\u2032,v\u2032\u2032)\u2208Eysyn \u039b(\u0393 y syn(v\n\u2032))\u2297 \u039b(\u0393ysyn(v\u2032\u2032))\u2211 v\u2032\u2208V ysyn \u03a6(M y,x(v\u2032))\u2297 \u039b(\u0393ysyn(v\u2032))\n) ."}, {"heading": "4.2. Decoding", "text": "At application time, the highest-scoring regular expression fw(x) = arg maxy\u2208Y\u03a3 w\nT\u03a8(x,y) has to be identified. This maximization is over the infinite space of all regular expressions Y\u03a3. To alleviate the intractability of this problem, we approximate this maximum by the maximum over a constrained, finite search space which can be found efficiently.\nThe constrained search space initially contains an alignment of all strings in x. An alignment is a regular expression that contains only constants\u2014which have to occur in all strings of the batch\u2014and the wildcard symbol \u201c(.\u2217)\u201d. The initial alignment ax of x can be thought of as the most-general bound of this space.\nDefinition 1 (Alignment). The set of alignments Ax of a batch of strings x contains all concatenations in which strings from \u03a3+ and the wildcard symbol \u201c(.\u2217)\u201d alternate, and that generate all elements of x.\nAn alignment is maximal if no other alignment in Ax contains more constant symbols. A maximal alignment of two strings can be determined efficiently using Hirschberg\u2019s algorithm (Hirschberg, 1975) which is an instance of dynamic programming. By contrast, finding the maximal alignment of a set of strings is NP-hard (Wang & Jiang, 1994); known algorithms are exponential in the number |x| of strings in x. Progressive alignment heuristics find an alignment of a set of strings by incrementally aligning pairs of strings.\nGiven an alignment ax = a0(. \u2217)a1 . . . (.\u2217)an of all strings in x, the constrained search space\nY\u0302x,D = {a0y1a1 . . .ynan|yj \u2208 Y\u0302MjD } (8)\ncontains all specializations of ax in which the j-th wildcard symbol is replaced by any element of a set Y\u0302MjD . The sets Y\u0302 Mj D are constructed by Algorithm 1. The algorithm starts with YD which we define to be the set of all subexpressions that occur anywhere in the training data D. From this set, it takes a subset such that each regular expression in Y\u0302x,D generates all strings in x, and adds a number of syntactic variants and subexpressions in which constants have been replaced to match the elements of Mj , where Mj is the matching list of the node which belongs to the j-th wildcard symbol. Each of the lines 7, 9, 10, 11, and 12 of Algorithm 1 adds at most one element to Y\u0302MjD \u2014 hence, the search space of possible substitutions for each of the n wildcard symbols is linear in the number of subexpressions that occur in the training sample.\nWe now turn towards the problem of determining the highest-scoring regular expression fw(x). Maximization over all regular expressions is approximated by maximization over the space defined by Equation 8:\narg max y\u2208Y\u03a3 wT\u03a8(x,y) \u2248 arg max y\u2208Y\u0302x,D wT\u03a8(x,y). (9)\nWe will now argue that this maximization problem can be decomposed into independent maximization problems for each of the yj that replaces the j-th wildcard in the alignment ax due to the simple syntactic structure of the alignment and the definition of \u03a8.\nFeature vector \u03a8(x,y) decomposes linearly into a sum over the nodes and a sum over pairs of adjacent nodes (see Equation 7). The syntax tree of an instantiation y = a0y1a1 . . .ynan of the alignment ax consists of a root node labeled as an alternating concatenation of constant strings aj and subexpressions yj (see Figure 3). This root node is connected to a layer on which constant strings aj = aj,1 . . . aj,|aj | and subtrees T yj syn alternate (blue area in Figure 3). However, the terms in Equation 10 that correspond to the root node y and\nAlgorithm 1 Constructing the decoding space Input: Subexpressions YD and alignment ax = a0(.\n\u2217)a1 . . . (.\u2217)an of the strings in x. 1: let T axsyn be the syntax tree of the alignment and v1, . . . , vn be the nodes labeled \u0393 ax syn(vj) = \u201c(.\n\u2217)\u201d. 2: for j = 1 . . . n do 3: let Mj = M ax,x(vj). 4: Initialize Y\u0302MjD to {y \u2208 YD|Mj \u2286 L(y)} 5: let x1, . . . , xm be the elements of Mj ; add (x1| . . . |xm) to Y\u0302MjD . 6: let u be the length of the longest string and l\nbe the length of the shortest string in Mj .\n7: if [\u03b2y1 . . .yk] \u2208 Y\u0302MjD , where \u03b2 \u2208 \u03a3\u2217 and y1 . . .yk are ranges or special macros (e.g.,\na-z, \\e), then add [\u03b1y1 . . .yk] to Y\u0302MjD , where \u03b1 \u2208 \u03a3\u2217 is the longest string that satisfies Mj \u2286 L([\u03b1y1 . . .yk]), if such an \u03b1 exists.\n8: for all [y] \u2208 Y\u0302MjD do 9: add [y]\u2217 and [y]{l, u} to Y\u0302MjD .\n10: if l = u, then add [y]{l} to Y\u0302MjD . 11: if u \u2264 1, then add [y]? to Y\u0302MjD . 12: if l > 0, then add [y]+ to Y\u0302MjD . 13: end for 14: end for Return: Y\u0302M1D , . . . , Y\u0302MnD .\nthe aj are constant for all values of the yj (red area in Figure 3). Since no edges connect multiple wildcards, the feature representation of these subtrees can be decomposed into n independent summands as in Equation 11.\n\u03a8(x, a0y1a1 . . .ynan) (10)\n=  n\u2211 j=1 \u039b(y)\u2297 \u039b(yj) + n\u2211 j=0 |aj |\u2211 q=1 \u039b(y)\u2297 \u039b(aj,q)\n\u03a6({x})\u2297 \u039b(y) + n\u2211 j=0 |aj |\u2211 q=1 \u03a6({aj,q})\u2297 \u039b(aj,q)\n\n+  n\u2211 j=1 \u2211 (v\u2032,v\u2032\u2032)\u2208Eyjsyn \u039b(\u0393 yj syn(v\u2032))\u2297 \u039b(\u0393yjsyn(v\u2032\u2032))\nn\u2211 j=1 \u2211 v\u2032\u2208V yjsyn \u03a6(Myj ,Mj (v\u2032))\u2297 \u039b(\u0393yjsyn(v\u2032))  = ( 0\n\u03a6({x})\u2297 \u039b(y)\n) + n\u2211 j=0 |ai|\u2211 q=1 ( \u039b(y)\u2297 \u039b(aj,q) \u03a6({aj,q})\u2297 \u039b(aj,q) )\n+ n\u2211 j=1 ( \u03a8(yj ,Mj) + ( \u039b(y)\u2297 \u039b(yj) 0 )) (11)\nSince the top-level operator of an alignment is a concatenation for any y \u2208 Y\u0302x,D, we can write \u039b(y) as\na constant \u039b\u2022, defined as the output feature vector (Equation 6) of a concatenation.\nThus, the maximization over all y = a0y1a1 . . .ynan can be decomposed into n maximization problems over\ny\u2217j = arg max yj\u2208Y\u0302 Mj D\nwT (\n\u03a8(yj ,Mj) +\n( \u039b\u2022 \u2297 \u039b(yj)\n0\n))\nwhich can be solved in O(n\u00d7 |YD|)."}, {"heading": "4.3. Optimization Problem", "text": "We will now address the process of minimizing the regularized empirical risk R\u0302, defined in Equation 4, for the `2 regularizer \u2126(w) = 1 2C ||w||2. Loss function \u2206, defined in Equation 1, is not convex. To obtain a convex optimization problem, we upper-bound the loss by its hinged version, following the margin-rescaling approach (Tsochantaridis et al., 2005):\n\u03bei=max y 6=yi {wT(\u03a8(xi,yi)\u2212\u03a8(xi,y)) + \u2206(yi,y,x)}. (12)\nThe maximum in Equation 12 is over all y \u2208 Y\u03a3\\{yi}. When the risk is rephrased as a constrained optimization problem, the maximum produces one constraint per element of y \u2208 Y\u03a3 \\ {yi}. However, since the decoder searches only the set Y\u0302xi,D, it is sufficient to enforce the constraints on this subset.\nWhen the loss is replaced by its upper bound\u2014the slack variable \u03be\u2014and for \u2126(w) = 12C ||w||2, the minimization of the regularized empirical risk (Equation 4) is reduced to Optimization Problem 1.\nOptimization Problem 1. Over parameters w, find\nw\u2217 = arg min w,\u03be\n1 2 ||w||2 + C m m\u2211 i=1 \u03bei, such that (13)\n\u2200i,\u2200y\u0304 \u2208 Y\u0302xi,D\\{yi} : wT(\u03a8(xi,yi)\u2212\u03a8(xi, y\u0304)) (14) \u2265 \u2206(yi, y\u0304,x)\u2212 \u03bei, \u2200i : \u03bei \u2265 0. (15)\nThis optimization problem is convex, since the objective (Equation 13) is convex and the constraints\n(Equation 14 and 15) are affine in w. Hence, the solution is unique and can be found efficiently by cutting plane methods as Pegasos (Shalev-Shwartz et al., 2011) or SVMstruct (Tsochantaridis et al., 2005).\nAlgorithm 2 Most strongly violated constraint\nInput: batch x, model fw, correct output y. 1: Infer alignment ax = a0(.\n\u2217)a1 . . . (.\u2217)an for x. 2: Let T axsyn be the syntax tree of ax and let v1, . . . , vn\nbe the nodes labeled \u0393axsyn(vj) = \u201c(. \u2217)\u201d.\n3: for all j = 1 . . . n do 4: Let Mj = M ax,x(vj) and calculate the Y\u0302MjD using Algorithm 1. 5:\ny\u0304j = arg max y\u2032j\u2208Y\u0302 Mj D\nwT (\n\u03a8(y\u2032j ,Mj)+\n( \u039b\u2022\u2297\u039b(y\u2032j)\n0\n)) +\n\u2206(y,a0(. \u2217)a1. . .(. \u2217)aj\u22121y \u2032 jaj(. \u2217)aj+1. . .(. \u2217)an,x)\n6: end for 7: Let y\u0304 abbreviate a0y\u03041a1 . . . y\u0304nan 8: if y\u0304 = y then 9: Assign a value of y\u0304\u2032j \u2208 Y\u0302 Mj D to one of the\nvariables y\u0304j such that the smallest decrease of fw(x, y\u0304) + \u2206tree(y, y\u0304) is obtained but the constraint y\u0304 6= y is enforced.\n10: end if Return: y\u0304\nDuring the optimization procedure, the regular expression that incurs the highest slack \u03bei for a given xi,\ny\u0304 = arg max y\u2208Y\u0302xi,D\\{yi}\nwT\u03a8(xi,y) + \u2206(yi,y,x),\nhas to be identified repeatedly. Algorithm 1 constructs the constrained search space Y\u0302xi,D such that x \u2208 L(y) for each x \u2208 xi and y \u2208 Y\u0302xi,D. Hence, the \u201cotherwise\u201d-case in Equation 1 never applies within our search space. Without this case, Equations 1 and 2 decompose linearly over the nodes of the parse tree, and therefore the wildcards. Hence, y\u0304 can be identified by maximizing over the variables y\u0304j independently in Step 5 of Algorithm 2. Algorithm 2 finds the constraint that is violated most strongly within the constrained search space in O(n \u00d7 |YD|). This ensures a polynomial execution time of the optimization algorithm. We refer to this learning procedure as REx-SVM ."}, {"heading": "5. Case Study", "text": "We investigate whether postmasters accept the output of REx-SVM to blacklist mailing campaigns during regular operations of a commercial email service. We also evaluate how accurately REx-SVM and reference methods identify the extensions of mailing campaigns."}, {"heading": "5.1. Evaluation by Postmasters", "text": "REx-SVM is trained on the ESP data set that contains 158 batches with a total of 12,763 emails and corresponding regular expressions, collected from the email service provider. The model is deployed; the user interface presents newly detected batches of spam emails together with the regular expression conjectured by REx-SVM to a postmaster during regular operations of the service. The postmaster is charged with blacklisting the campaigns by suitable regular expressions. Over the study, the postmasters created 188 regular expressions. Of these, they created 169 expressions (89%) by copying a substring of the automatically generated expression. We observe that postmasters prefer to describe only a part of the message which they feel is characteristic for the campaign whereas REx-SVM describes the entirety of the messages. In 12 cases, the postmasters edited the string, and in 7 cases they wrote an expression from scratch.\nTo illustrate different cases, Figure 4 compares excerpts of expressions created by REx- and REx0/1SVM (a variant of REx-SVM that uses the zero-one loss instead of \u2206 defined in Equation 1) to expressions of a postmaster. The first example shows a perfect agreement between REx-SVM and postmaster. In the second example, the expressions are close but distinct. In the third example, the SVMs produce expressions that generate an overly general set of URLs and lead to false positives (\u201c\\e\u201d stands for characters that can occur in a URL). In all three cases, REx-SVM is more similar to the postmaster than REx0/1.\nThe top right diagram of Figure 5 shows the average loss \u2206 of REx- and REx0/1-SVM , measured by cross validation with one batch held out. While postmasters show the tendency to write expressions that only characterize about 10% of the message, the REx-SVM\nvariants describe the entirety of the message. This leads to relatively high values of the loss function."}, {"heading": "5.2. Spam Filtering Performance", "text": "We evaluate the ability of REx-SVM and baselines to identify the exact extension of email campaigns. We use the alignment of the strings in x as a baseline. In addition, ReLIE (Li et al., 2008) searches for a regular expression that matches the emails in the input batch and does not match any of the additional negative examples by applying a set of transformation rules; we use the alignment of the input batch as starting point. ReLIE receives an additional 10,000 emails that are not part of any batch as negative data. An additional content-based filter employed by the provider has been trained on several million spam and non-spam emails.\nIn order to be able to measure false-positive rates (the rate at which emails that are not part of a campaign are erroneously included), we combine the ESP data set with an additional 135,000 non-spam emails, also from the provider. Additionally, we use a public data set that consists of 100 batches of emails extracted from the Bruce Guenther archive1, containing a total of 63,512 emails. To measure false-positive rates, we combine this collection of spam batches with 17,419\n1http://untroubled.org/spam/\nemails from the Enron corpus2 of non-spam emails and 76,466 non-spam emails of the TREC corpus3. The public data set is available to researchers.\nIn an outer loop of leave-one-out cross validation, one batch is held back to evaluate the true-positive rate (the proportion of the campaign that is correctly recognized). In an inner loop of 10-fold cross validation, regularization parameter C is tuned.\nFigure 5 shows the true and false positive rates for all methods and both data sets. The horizontal axis displays the number of emails in the input batch x. Error bars indicate the standard error. The alignment exhibits the highest true-positive rate and a high falsepositive rate because it is the most-general bound of the decoder\u2019s search space. ReLIE needs only very few or zero replacement steps until no negative examples are covered. Consequently, it has similarly high true- and false-positive rates. REx-SVM attains a slightly lower true positive rate, and a substantially lower false-positive rate. The false-positive rates of REx and REx0/1 lie more than an order of magnitude below the rate of the commercial content-based spam filter employed by the email service provider. The zero-one loss leads to comparable false-positive but lower true-positive rates, rendering the loss func-\n2http://www.cs.cmu.edu/~enron/ 3http://trec.nist.gov/data/spam.html\ntion of Equation 1 preferable to the zero-one loss.\nThe execution time to learn a model (bottom right) is consistent with prior findings of between linear and quadratic for the SVM optimization process."}, {"heading": "6. Related Work", "text": "Gold (1967) shows that it is impossible to exactly identify any regular language from finitely many positive examples. Our notion of minimizing an expected difference between conjecture and target language over a distribution of input strings reflects a more statistically-inspired notion of learning. Also, in our problem setting the learner has access to pairs of sets of strings and corresponding regular expressions.\nMost work of identification of regular languages focuses on learning automata (Denis, 2001; Clark & Thollard, 2004). While these problems are identical in theory, transforming generated automata into regular expressions can lead to lengthy terms that do not lend themselves to human comprehension (Fernau, 2009). Some work focuses on restricted classes, such as expressions in which each symbol occurs at most k times (Bex et al., 2008), disjunction-free expressions (Bra\u0304zma, 1993), and disjunctions of left-aligned disjunction-free expressions (Fernau, 2009).\nXie et al. (2008) use regular expressions to detect URLs in spam batches and develop a spam filter with low false positive rate. The ReLIE-algorithm (Li et al., 2008) (used as a reference method in our experiments) learns regular expressions from positive and negative examples given an initial expression by applying a set of transformation rules as long as this improves the separation of positive and negative examples."}, {"heading": "7. Conclusions", "text": "Complementing the language-identification paradigm, we pose the problem of learning to map a set of strings to a target regular expression. Training data consists of batches of strings and corresponding expressions. We phrase this problem as a learning problem with structured output spaces and engineer an appropriately loss function. We derive the resulting optimization problem, and devise a decoder that searches a space of specializations of a maximal alignment.\nFrom our case study we conclude that REx-SVM gives a high true positive rate at a false positive rate that is more than an order of magnitude lower than that of a commercial content-based filter. The system is being used by a commercial email service provider and complements content-based and IP-address based filtering."}, {"heading": "Acknowledgments", "text": "This work was funded by a grant from STRATO AG."}, {"heading": "Bex, G., Gelade, W., Neven, F., and Vansummeren, S.", "text": "Learning deterministic regular expressions for the inference of schemas from XML data. In Proceeding of the International World Wide Web Conference, 2008."}, {"heading": "Bra\u0304zma, A. Efficient identification of regular expressions", "text": "from representative examples. In Proceedings of the Annual Conference on Computational Learning Theory, 1993.\nCesa-Bianchi, N., Gentile, C., and Zaniboni, L. Incremental algorithms for hierarchical classification. Machine Learning, 7:31\u201354, 2006."}, {"heading": "Clark, A. and Thollard, F. Pac-learnability of probabilistic", "text": "deterministic finite state automata. Machine Learning Research, 5:473\u2013497, 2004.\nDenis, F. Learning regular languages from simple positive examples. Machine Learning, 44:27\u201366, 2001.\nDube\u0301, D. and Feeley, M. Efficiently building a parse tree from a regular expression. Acta Informatica, 37(2):121\u2013 144, 2000.\nFernau, H. Algorithms for learning regular expressions from positive data. Information and Computation, 207 (4):521\u2013541, 2009.\nGold, E. M. Language identification in the limit. Information and Control, 10:447\u2013474, 1967.\nHaider, P. and Scheffer, T. Bayesian clustering for email campaign detection. In Proceeding of the International Conference on Machine Learning, 2009.\nHirschberg, D. A linear space algorithm for computing maximal common subsequences. Communications of the ACM, 18(6):341\u2013343, 1975."}, {"heading": "Li, Y., Krishnamurthy, R., Raghavan, S., Vaithyanathan,", "text": "S., and Jagadish, H. V. Regular expression learning for information extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2008."}, {"heading": "Shalev-Shwartz, S., Singer, Y., Srebro, N., and Cotter, A.", "text": "Pegasos: primal estimated sub-gradient solver for svm. Mathematical Programming, 127(1):1\u201328, 2011.\nTsochantaridis, I., Joachims, T., Hofmann, T., and Altun, Y. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6:1453\u20131484, 2005.\nWang, L. and Jiang, T. On the complexity of multiple sequence alignment. Journal of Computational Biology, 1(4):337\u2013348, 1994."}, {"heading": "Xie, Y., Yu, F., Achan, K., Panigrahy, R., Hulten, G., and", "text": "Osipkov, I. Spamming botnets: signatures and characteristics. In Proceedings of the ACM SIGCOMM Conference, 2008."}], "references": [{"title": "Learning deterministic regular expressions for the inference of schemas from XML data", "author": ["G. Bex", "W. Gelade", "F. Neven", "S. Vansummeren"], "venue": "In Proceeding of the International World Wide Web Conference,", "citeRegEx": "Bex et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bex et al\\.", "year": 2008}, {"title": "Efficient identification of regular expressions from representative examples", "author": ["A. Br\u0101zma"], "venue": "In Proceedings of the Annual Conference on Computational Learning Theory,", "citeRegEx": "Br\u0101zma,? \\Q1993\\E", "shortCiteRegEx": "Br\u0101zma", "year": 1993}, {"title": "Incremental algorithms for hierarchical classification", "author": ["N. Cesa-Bianchi", "C. Gentile", "L. Zaniboni"], "venue": "Machine Learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2006}, {"title": "Pac-learnability of probabilistic deterministic finite state automata", "author": ["A. Clark", "F. Thollard"], "venue": "Machine Learning Research,", "citeRegEx": "Clark and Thollard,? \\Q2004\\E", "shortCiteRegEx": "Clark and Thollard", "year": 2004}, {"title": "Learning regular languages from simple positive examples", "author": ["F. Denis"], "venue": "Machine Learning,", "citeRegEx": "Denis,? \\Q2001\\E", "shortCiteRegEx": "Denis", "year": 2001}, {"title": "Efficiently building a parse tree from a regular expression", "author": ["D. Dub\u00e9", "M. Feeley"], "venue": "Acta Informatica,", "citeRegEx": "Dub\u00e9 and Feeley,? \\Q2000\\E", "shortCiteRegEx": "Dub\u00e9 and Feeley", "year": 2000}, {"title": "Algorithms for learning regular expressions from positive data", "author": ["H. Fernau"], "venue": "Information and Computation,", "citeRegEx": "Fernau,? \\Q2009\\E", "shortCiteRegEx": "Fernau", "year": 2009}, {"title": "Language identification in the limit", "author": ["E.M. Gold"], "venue": "Information and Control,", "citeRegEx": "Gold,? \\Q1967\\E", "shortCiteRegEx": "Gold", "year": 1967}, {"title": "Bayesian clustering for email campaign detection", "author": ["P. Haider", "T. Scheffer"], "venue": "In Proceeding of the International Conference on Machine Learning,", "citeRegEx": "Haider and Scheffer,? \\Q2009\\E", "shortCiteRegEx": "Haider and Scheffer", "year": 2009}, {"title": "A linear space algorithm for computing maximal common subsequences", "author": ["D. Hirschberg"], "venue": "Communications of the ACM,", "citeRegEx": "Hirschberg,? \\Q1975\\E", "shortCiteRegEx": "Hirschberg", "year": 1975}, {"title": "Regular expression learning for information extraction", "author": ["Y. Li", "R. Krishnamurthy", "S. Raghavan", "S. Vaithyanathan", "H.V. Jagadish"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Li et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Li et al\\.", "year": 2008}, {"title": "Pegasos: primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical Programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "On the complexity of multiple sequence alignment", "author": ["L. Wang", "T. Jiang"], "venue": "Journal of Computational Biology,", "citeRegEx": "Wang and Jiang,? \\Q1994\\E", "shortCiteRegEx": "Wang and Jiang", "year": 1994}, {"title": "Spamming botnets: signatures and characteristics", "author": ["Y. Xie", "F. Yu", "K. Achan", "R. Panigrahy", "G. Hulten", "I. Osipkov"], "venue": "In Proceedings of the ACM SIGCOMM Conference,", "citeRegEx": "Xie et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 12, "context": "We model fw as a linear discriminant function w\u03a8(x,y) for a joint feature representation of the input x and output y (Tsochantaridis et al., 2005):", "startOffset": 117, "endOffset": 146}, {"referenceID": 9, "context": "A maximal alignment of two strings can be determined efficiently using Hirschberg\u2019s algorithm (Hirschberg, 1975) which is an instance of dynamic programming.", "startOffset": 94, "endOffset": 112}, {"referenceID": 12, "context": "To obtain a convex optimization problem, we upper-bound the loss by its hinged version, following the margin-rescaling approach (Tsochantaridis et al., 2005):", "startOffset": 128, "endOffset": 157}, {"referenceID": 11, "context": "Hence, the solution is unique and can be found efficiently by cutting plane methods as Pegasos (Shalev-Shwartz et al., 2011) or SVM (Tsochantaridis et al.", "startOffset": 95, "endOffset": 124}, {"referenceID": 12, "context": ", 2011) or SVM (Tsochantaridis et al., 2005).", "startOffset": 15, "endOffset": 44}, {"referenceID": 10, "context": "In addition, ReLIE (Li et al., 2008) searches for a regular expression that matches the emails in the input batch and does not match any of the additional negative examples by applying a set of transformation rules; we use the alignment of the input batch as starting point.", "startOffset": 19, "endOffset": 36}, {"referenceID": 4, "context": "Most work of identification of regular languages focuses on learning automata (Denis, 2001; Clark & Thollard, 2004).", "startOffset": 78, "endOffset": 115}, {"referenceID": 6, "context": "While these problems are identical in theory, transforming generated automata into regular expressions can lead to lengthy terms that do not lend themselves to human comprehension (Fernau, 2009).", "startOffset": 180, "endOffset": 194}, {"referenceID": 0, "context": "Some work focuses on restricted classes, such as expressions in which each symbol occurs at most k times (Bex et al., 2008), disjunction-free expressions (Br\u0101zma, 1993), and disjunctions of left-aligned disjunction-free expressions (Fernau, 2009).", "startOffset": 105, "endOffset": 123}, {"referenceID": 1, "context": ", 2008), disjunction-free expressions (Br\u0101zma, 1993), and disjunctions of left-aligned disjunction-free expressions (Fernau, 2009).", "startOffset": 38, "endOffset": 52}, {"referenceID": 6, "context": ", 2008), disjunction-free expressions (Br\u0101zma, 1993), and disjunctions of left-aligned disjunction-free expressions (Fernau, 2009).", "startOffset": 116, "endOffset": 130}, {"referenceID": 10, "context": "The ReLIE-algorithm (Li et al., 2008) (used as a reference method in our experiments) learns regular expressions from positive and negative examples given an initial expression by applying a set of transformation rules as long as this improves the separation of positive and negative examples.", "startOffset": 20, "endOffset": 37}], "year": 2012, "abstractText": "This paper addresses the problem of inferring a regular expression from a given set of strings that resembles, as closely as possible, the regular expression that a human expert would have written to identify the language. This is motivated by our goal of automating the task of postmasters of an email service who use regular expressions to describe and blacklist email spam campaigns. Training data contains batches of messages and corresponding regular expressions that an expert postmaster feels confident to blacklist. We model this task as a learning problem with structured output spaces and an appropriate loss function, derive a decoder and the resulting optimization problem, and a report on a case study conducted with an email service.", "creator": "LaTeX with hyperref package"}}}