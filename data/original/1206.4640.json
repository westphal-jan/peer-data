{"id": "1206.4640", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Stability of matrix factorization for collaborative filtering", "abstract": "We study the stability vis a vis adversarial noise of matrix factorization algorithm for matrix completion. In particular, our results include: (I) we bound the gap between the solution matrix of the factorization method and the ground truth in terms of root mean square error; (II) we treat the matrix factorization as a subspace fitting problem and analyze the difference between the solution subspace and the ground truth; (III) we analyze the prediction error of individual users based on the subspace stability. We apply these results to the problem of collaborative filtering under manipulator attack, which leads to useful insights and guidelines for collaborative filtering system design.", "histories": [["v1", "Mon, 18 Jun 2012 15:18:05 GMT  (423kb)", "http://arxiv.org/abs/1206.4640v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.NA cs.LG stat.ML", "authors": ["yu-xiang wang", "huan xu"], "accepted": true, "id": "1206.4640"}, "pdf": {"name": "1206.4640.pdf", "metadata": {"source": "META", "title": "Stability of Matrix Factorization for Collaborative Filtering", "authors": ["Yu-Xiang Wang", "Huan Xu"], "emails": ["yuxiangwang@nus.edu.sg", "mpexuh@nus.edu.sg"], "sections": [{"heading": "1. Introduction", "text": "Collaborative prediction of user preferences has attracted fast growing attention in the machine learning community, best demonstrated by the million-dollar Netflix Challenge. Among various models proposed, matrix factorization is arguably the most widely applied method, due to its high accuracy, scalability (Su & Khoshgoftaar, 2009) and flexibility to incorporating domain knowledge (Koren et al., 2009). Hence, not surprisingly, matrix factorization is the centerpiece of most state-of-the-art collaborative filtering systems, including the winner of Netflix Prize (Bell & Koren, 2007). Indeed, matrix factorization has been widely applied to tasks other than collaborative filtering, including structure from motion, localization in wireless sensor network, DNA microarray estimation and beyond. Matrix factorization is also considered as a fundamental building block of many popular algorithms\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nin regression, factor analysis, dimension reduction, and clustering (Singh & Gordon, 2008).\nDespite the popularity of factorization methods, not much has been done on the theoretical front. In this paper, we fill the blank by analyzing the stability vis a vis adversarial noise of the matrix factorization methods, in hope of providing useful insights and guidelines for practitioners to design and diagnose their algorithm efficiently.\nOur main contributions are three-fold: In Section 3 we bound the gap between the solution matrix of the factorization method and the ground truth in terms of root mean square error. In Section 4, we treat the matrix factorization as a subspace fitting problem and analyze the difference between the solution subspace and the ground truth. This facilitates an analysis of the prediction error of individual users, which we present in Section 5. To validate these results, we apply them to the problem of collaborative filtering under manipulator attack in Section 6. Interestingly, we find that matrix factorization are robust to the so-called \u201ctargeted attack\u201d, but not so to the so-called \u201cmass attack\u201d unless the number of manipulators are small. These results agree with the simulation observations.\nWe briefly discuss relevant literatures. Azar et al. (2001) analyzed asymptotic performance of matrix factorization methods, yet under stringent assumptions on the fraction of observation and on the singular values. Drineas et al. (2002) relaxed these assumptions but it requires a few fully rated users \u2013 a situation that rarely happens in practice. Srebro (2004) considered the problem of the generalization error of learning a low-rank matrix. Their technique is similar to the proof of our first result, yet applied to a different context. Specifically, they are mainly interested in binary prediction (i.e., \u201clike/dislike\u201d) rather than recovering the real-valued ground-truth matrix (and its column subspace). In addition, they did not investigate the stability of the algorithm under noise and manipulators.\nRecently, some alternative algorithms, notably StableMC (Candes & Plan, 2010) based on nuclear norm optimization, and OptSpace (Keshavan et al., 2010b) based on gradient descent over the Grassmannian, have been shown to be stable vis a vis noise (Candes & Plan, 2010; Keshavan et al., 2010a). However, these two methods are less effective in practice. As documented in Mitra et al. (2010); Wen (2010) and many others, matrix factorization methods typically outperform these two methods. Indeed, our theoretical results reassure these empirical observations, see Section 3 for a detailed comparison of the stability results of different algorithms."}, {"heading": "2. Formulation", "text": ""}, {"heading": "2.1. Matrix Factorization with Missing Data", "text": "Let the user ratings of items (such as movies) form a matrix Y , where each column corresponds to a user and each row corresponds to an item. Thus, the ijth entry is the rating of item-i from user-j. The valid range of the rating is [\u2212k,+k]. Y is assumed to be a rank-r matrix1, so there exists a factorization of this rating matrix Y = UV T , where Y \u2208 Rm\u00d7n, U \u2208 Rm\u00d7r, V \u2208 Rn\u00d7r. Without loss of generality, we assume m \u2264 n throughout the paper. Collaborative filtering is about to recover the rating matrix from a fraction of entries possibly corrupted by noise or error. That is, we observe Y\u0302ij for (ij) \u2208 \u2126 the sampling set (assumed to be uniformly random), and\nY\u0302 = Y +E being a corrupted copy of Y , and we want to recover Y . This naturally leads to the optimization program below:\nmin U,V\n1\n2 \u2225\u2225\u2225P\u2126(UV T \u2212 Y\u0302 )\u2225\u2225\u22252 F\nsubject to \u2223\u2223[UV T ]i,j\u2223\u2223 \u2264 k, (1)\nwhere P\u2126 is the sampling operator defined to be:\n[P\u2126(Y )]i,j = { Yi,j if (i, j) \u2208 \u2126; 0 otherwise.\n(2)\nWe denote the optimal solution Y \u2217 = U\u2217V \u2217T and the error \u2206 = Y \u2217 \u2212 Y."}, {"heading": "2.2. Matrix Factorization as Subspace Fitting", "text": "As pointed out in Chen (2008), an alternative interpretation of collaborative filtering is fitting the optimal r-dimensional subspace N to the sampled data. That\n1In practice, this means the user\u2019s preference of movies are influenced by no more than r latent factors.\nis, one can reformulate (1) into an equivalent form2:\nmin N f(N) = \u2211 i \u2016(I \u2212Pi)yi\u20162 = \u2211 i yTi (I \u2212Pi)yi, (3)\nwhere yi is the observed entries in the i th column of Y , N is an m\u00d7 r matrix representing an orthonormal basis3 of N , Ni is the restriction of N to the observed entries in column i, and Pi = Ni(NTi Ni)\u22121NTi is the projection onto span(Ni).\nAfter solving (3), we can estimate the full matrix in a column by column manner via (4). Here y\u2217i denotes the full ith column of recovered rank-r matrix Y \u2217.\ny\u2217i = N(N T i Ni) \u22121NTi yi = Npinv(Ni)yi. (4)\nDue to error term E, the ground truth subspace N gnd can not be obtained. Instead, denote the optimal subspace of (1) (equivalently (3)) by N \u2217, and we bound the gap between these two subspaces using Canonical angle. The canonical angle matrix \u0398 is an r \u00d7 r diagonal matrix, with the ith diagonal entry \u03b8i = arccos\u03c3i((N gnd)TN\u2217).\nThe error of subspace recovery is measured by \u03c1 = \u2016 sin \u0398\u20162, justified by the following properties adapted from Chapter 2 of Stewart & Sun (1990):\n\u2016Pgnd \u2212 PN \u2217 \u2016F = \u221a 2\u2016 sin \u0398\u2016F ,\n\u2016Pgnd \u2212 PN \u2217 \u20162 =\u2016 sin \u0398\u20162 = sin \u03b81.\n(5)"}, {"heading": "2.3. Algorithms", "text": "We focus on the stability of the global optimal solution of Problem (1). As Problem (1) is not convex, finding the global optimum is non-trivial in general. While this is certainly an important question, it is beyond the scope of this paper. Instead, we briefly review some results on this aspect.\nThe simplest algorithm for (1) is perhaps the alternating least square method (ALS) which alternatingly minimizes the objective function over U and V until convergence. More sophisticatedly, second-order algorithms such as Wiberg, Damped Newton and Levenberg Marquadt are proposed with better convergence rate, as surveyed in Okatani & Deguchi (2007). Specific variations for CF are investigated in Taka\u0301cs et al. (2008) and Koren et al. (2009).\nFrom an empirical perspective, Mitra et al. (2010) reported that the global optimum is often obtained in simulation and Chen (2008) demonstrated satisfactory percentage of hits to global minimum from randomly initialized trials on a real data set.\n2Strictly speaking, this is only equivalent to (1) without the box constraint. See the discussion in Supplementary Material for our justifications.\n3It is easy to see N = ortho(U) for U in (1)"}, {"heading": "3. Stability", "text": "We show in this section that when sufficiently many entries are sampled, the global optimal solution of factorization methods is stable vis a vis noise \u2013 i.e., it recovers a matrix \u201cclose to\u201d the ground-truth. This is measured by the root mean square error (RMSE):\nRMSE = 1\u221a mn \u2016Y \u2217 \u2212 Y \u2016 (6)\nTheorem 1. There exists an absolute constant C, such that with probability at least 1\u2212 2 exp(\u2212n),\nRMSE \u2264 1\u221a |\u2126| \u2016P\u2126(E)\u2016F+ \u2016E\u2016F\u221a mn +Ck\n( nr log(n)\n|\u2126|\n) 1 4\n.\nNotice that when |\u2126| nr log(n) the last term diminishes, and the RMSE is essentially bounded by the \u201caverage\u201d magnitude of entries of E, i.e., the factorization methods are stable.\nComparison with related work\nWe recall similar RMSE bounds for StableMC of Candes & Plan (2010) and OptSpace of Keshavan et al. (2010a):\nStableMC: RMSE\n\u2264\n\u221a 32 min (m,n)\n|\u2126| \u2016P\u2126(E)\u2016F + 1\u221a mn \u2016P\u2126(E)\u2016F .\n(7)\nOptSpace: RMSE \u2264 C\u03ba2n \u221a r\n|\u2126| \u2016P\u2126(E)\u20162. (8)\nAlbeit the fact that these bounds are for different algorithms and under different assumptions (see Table 1 for details), it is still interesting to compare the results with Theorem 1. We observe that Theorem 1 is tighter than (7) by a scale of \u221a min (m,n), and tighter\nthan (8) by a scale of \u221a n/ log(n) in case of adversarial noise. However, the latter result is stronger when the noise is stochastic, due to the spectral norm used.\nCompare with an Oracle\nWe next compare the bound with an oracle, introduced in Candes & Plan (2010), that is assumed to know the ground-truth column space N a priori and recover the matrix by projecting the observation to N in the least square sense column by column via (4). It is shown that RMSE of this oracle satsifies,\nRMSE \u2248 \u221a 1/|\u2126|\u2016P\u2126(E)\u2016F . (9)\nNotice that Theorem 1 matches this oracle bound, and hence it is tight up to a constant factor."}, {"heading": "3.1. Proof of Stability Theorem", "text": "We briefly explain the proof idea first. By definition, the algorithm finds the optimal rank-r matrices, measured in terms of the root mean square (RMS) on the sampled entries. To show this implies a small RMS on the entire matrix, we need to bound their gap\n\u03c4(\u2126) , \u2223\u2223\u2223 1\u221a |\u2126| \u2016P\u2126(Y\u0302 \u2212 Y \u2217)\u2016F \u2212 1\u221a mn \u2016Y\u0302 \u2212 Y \u2217\u2016F \u2223\u2223\u2223. To bound \u03c4(\u2126), we require the following theorem.\nTheorem 2. Let L\u0302(X) = 1\u221a |\u2126| \u2016P\u2126(X \u2212 Y\u0302 )\u2016F and L(X) = 1\u221a mn \u2016X\u2212Y\u0302 \u2016F be the empirical and actual loss function respectively. Furthermore, assume entry-wise constraint maxi,j |Xi,j | \u2264 k. Then for all rank-r matrices X, with probability greater than 1 \u2212 2 exp(\u2212n), there exists a fixed constant C such that\nsup X\u2208Sr\n|L\u0302(X)\u2212 L(X)| \u2264 Ck (nr log(n)\n|\u2126|\n) 1 4\n.\nIndeed, Theorem 2 easily implies Theorem 1.\nProof of Theorem 1. The proof makes use of the fact that Y \u2217 is the global optimal of (1).\nRMSE = 1\u221a mn \u2016Y \u2217 \u2212 Y \u2016F = 1\u221a mn \u2016Y \u2217 \u2212 Y\u0302 + E\u2016F\n\u2264 1\u221a mn |Y \u2217 \u2212 Y\u0302 \u2016F + 1\u221a mn \u2016E\u2016F\n(a) \u2264 1\u221a |\u2126| \u2016P\u2126(Y \u2217 \u2212 Y\u0302 )\u2016F + \u03c4(\u2126) + 1\u221a mn \u2016E\u2016F\n(b) \u2264 1\u221a |\u2126| \u2016P\u2126(Y \u2212 Y\u0302 )\u2016F + \u03c4(\u2126) + 1\u221a mn \u2016E\u2016F = 1\u221a |\u2126| \u2016P\u2126(E)\u2016F + \u03c4(\u2126) + 1\u221a mn \u2016E\u2016F .\nHere, (a) holds from definition of \u03c4(\u2126), and (b) holds because Y \u2217 is optimal solution of (1). Since Y \u2217 \u2208 Sr, applying Theorem 2 completes the proof.\nThe proof of Theorem 2 is deferred to Appendix A due to space constraints. The main idea, briefly speaking, is to bound, for a fixed X \u2208 Sr,\u2223\u2223(L\u0302(X))2 \u2212 (L(X))2\u2223\u2223\n= \u2223\u2223 1 |\u2126| \u2016P\u2126(X \u2212 Y\u0302 )\u20162F \u2212 1 mn \u2016X \u2212 Y\u0302 \u20162F \u2223\u2223, using Hoeffding\u2019s inequality for sampling without replacement; then bound\n\u2223\u2223L\u0302(X)\u2212 L(X)\u2223\u2223 using\u2223\u2223L\u0302(X)\u2212 L(X)\u2223\u2223 \u2264\u221a\u2223\u2223(L\u0302(X))2 \u2212 (L(X))2\u2223\u2223; and finally, bound supX\u2208Sr |L\u0302(X) \u2212 L(X)| using an \u2212net argument."}, {"heading": "4. Subspace Stability", "text": "In this section we investigate the stability of recovered subspace using matrix factorization methods. Recall that matrix factorization methods assume that, in the idealized noiseless case, the preference of each user belongs to a low-rank subspace. Therefore, if this subspace can be readily recovered, then we can predict preferences of a new user without re-run the matrix factorization algorithms. We analyze the latter, prediction error on individual users, in Section 5.\nTo illustrate the difference between the stability of the recovered matrix and that of the recovered subspace, consider a concrete example in movie recommendation, where there are both honest users and malicious manipulators in the system. Suppose we obtain an output subspace N\u2217 by (3) and the missing ratings are filled in by (4). If N\u2217 is very \u201cclose\u201d to ground truth subspace N , then all the predicted ratings for honest users will be good. On the other hand, the prediction error of the preference of the manipulators \u2013 who do not follow the low-rank assumption \u2013 can be large, which leads to a large error of the recovered matrix. Notice that we are only interested in predicting the preference of the honest users. Hence the subspace stability provides a more meaningful metric here."}, {"heading": "4.1. Subspace Stability Theorem", "text": "Let N ,M and N \u2217,M\u2217 be the r-dimensional column space-row space pair of matrix Y and Y \u2217 respectively. We\u2019ll denote the corresponding m \u00d7 r and n \u00d7 r orthonormal basis matrix of the vector spaces using N ,M ,N\u2217,M\u2217. Furthermore, Let \u0398 and \u03a6 denote the canonical angles \u2220(N \u2217,N ) and \u2220(M\u2217,M) respectively.\nTheorem 3. When Y is perturbed by additive error E and observed only on \u2126, then there exists a \u2206 satisfying \u2016\u2206\u2016 \u2264 \u221a\nmn |\u2126| \u2016P\u2126(E)\u2016F + \u2016E\u2016F +\n\u221a mn |\u03c4(\u2126)|,\nsuch that:\n\u2016 sin \u0398\u2016 \u2264 \u221a 2\n\u03b4 \u2016(PN \u22a5 \u2206)\u2016; \u2016 sin \u03a6\u2016 \u2264\n\u221a 2\n\u03b4 \u2016(PM \u22a5 \u2206T )\u2016,\nwhere \u2016\u00b7\u2016 is either the Frobenious norm or the spectral norm, and \u03b4 = \u03c3\u2217r , i.e., the r\nth largest singular value of the recovered matrix Y \u2217.\nFurthermore, we can bound \u03b4 by: \u03c3r \u2212 \u2016\u2206\u20162 \u2264 \u03b4 \u2264 \u03c3r + \u2016\u2206\u20162 \u03c3Y\u0303Nr \u2212 \u2016PN \u22a5 \u2206\u20162 \u2264 \u03b4 \u2264 \u03c3Y\u0303Nr + \u2016PN \u22a5\n\u2206\u20162 \u03c3Y\u0303Mr \u2212 \u2016PM \u22a5 \u2206T \u20162 \u2264 \u03b4 \u2264 \u03c3Y\u0303Mr + \u2016PM \u22a5 \u2206T \u20162\nwhere Y\u0303N = Y + PN\u2206 and Y\u0303M = Y + (PM\u2206T )T .\nNotice that in practice, as Y \u2217 is the output of the algorithm, its rth singular value \u03b4 is readily obtainable. Intuitively, Theorem 3 shows that the subspace sensitivity vis a vis noise depends on the singular value distribution of original matrix Y . A well-conditioned rank-r matrix Y can tolerate larger noise, as its rth singular value is of the similar scale to \u2016Y \u20162, its largest singular value."}, {"heading": "4.2. Proof of Subspace Stability", "text": "Proof of Theorem 3. In the proof, we use \u2016 \u00b7 \u2016 when a result holds for both Frobenious norm and for spectral norm. We prove the two parts separately.\nPart 1: Canonical Angles. Let \u2206 = Y \u2217 \u2212 Y . By Theorem 1, we have \u2016\u2206\u2016 \u2264\u221a mn |\u2126| \u2016P\u2126(E)\u2016F +\u2016E\u2016F + \u221a mn |\u03c4(\u2126)|. The rest of the proof relates \u2206 with the deviation of spaces spanned by the top r singular vectors of Y and Y \u2217 respectively. Our main tools are Weyl\u2019s Theorem and Wedin\u2019s Theorem (Lemma F.1 and F.2 in Appendix F). We express singular value decomposition of Y and Y \u2217 in block matrix form as in (F.1) and (F.2) of Appendix F, and set the dimension of \u03a31 and \u03a3\u03021 to be r \u00d7 r. Recall, rank(Y ) = r, so \u03a31 = diag(\u03c31, ..., \u03c3r), \u03a32 = 0, \u03a3\u03021 = diag(\u03c3 \u2032 1, ..., \u03c3 \u2032\nr). By setting \u03a3\u03022 to 0 we obtained Y \u2032, the nearest rank-r matrix to Y \u2217. Observe that N\u2217 = L\u03021, M \u2217 = (R\u03021) T .\nTo apply Wedin\u2019s Theorem (Lemma F.2), we have the residual Z and S as follows:\nZ = YM\u2217 \u2212N\u2217\u03a3\u03021, S = Y TN\u2217 \u2212M\u2217\u03a3\u03021,\nwhich leads to\n\u2016Z\u2016 = \u2016(Y\u0302 \u2212\u2206)M\u2217 \u2212N\u2217\u03a3\u03021\u2016 = \u2016\u2206M\u2217\u2016, \u2016S\u2016 = \u2016(Y\u0302 \u2212\u2206)TN\u2217 \u2212M\u2217\u03a3\u03021\u2016 = \u2016\u2206TN\u2217\u2016.\nSubstitute this into the Wedin\u2019s inequality, we have\n\u221a \u2016 sin \u03a6\u20162 + \u2016 sin \u0398\u20162 \u2264\n\u221a \u2016\u2206TN \u2032\u20162 + \u2016\u2206M \u2032\u20162\n\u03b4 ,\n(10) where \u03b4 satisfies (F.3) and (F.4). Specifically, \u03b4 = \u03c3\u2217r . Observe that Equation (10) implies\n\u2016 sin \u0398\u2016 \u2264 \u221a 2\n\u03b4 \u2016\u2206\u2016; \u2016 sin \u03a6\u2016 \u2264\n\u221a 2\n\u03b4 \u2016\u2206\u2016.\nTo reach the equations presented in the theorem, we can tighten the above bound by decomposing \u2206 into two orthogonal components.\nY \u2217 = Y + \u2206 = Y + PN\u2206 + PN \u22a5 \u2206 := Y\u0303 N + PN \u22a5\n\u2206. (11)\nIt is easy to see that column space of Y and Y\u0303N are identical. So the canonical angle \u0398 between Y \u2217 and Y are the same as that between Y \u2217 and Y\u0303N . Therefore, we can replace \u2206 by PN\u22a5\u2206 to obtain the equation presented in the theorem. The corresponding result for row subspace follows similarly, by decomposing \u2206T to its projection on M and M\u22a5. Part 2: Bounding \u03b4. We now bound \u03b4, or equivalently \u03c3\u2217r . By Weyl\u2019s theorem (Lemma F.1), we have\n|\u03b4 \u2212 \u03c3r| < \u2016\u2206\u20162.\nMoreover, Applying Weyl\u2019s theorem on Equation (11), we have\n|\u03b4 \u2212 \u03c3Y\u0303Nr | \u2264 \u2016PN\u22a5\u2206\u20162.\nSimilarly, we have\n|\u03b4 \u2212 \u03c3Y\u0303Mr | \u2264 \u2016PM\u22a5\u2206T \u20162.\nThis establishes the theorem."}, {"heading": "5. Prediction Error of individual user", "text": "In this section, we analyze how confident we can predict the ratings of a new user y \u2208 N gnd, based on the subspace recovered via matrix factorization methods. In particular, we bound the prediction \u2016y\u0303\u2217\u2212y\u2016, where y\u0303\u2217 is the estimation from partial rating using (4), and y is the ground truth.\nWithout loss of generality, if the sampling rate is p, we assume observations occur in first pm entries, such\nthat y = ( y1 y2 ) with y1 observed and y2 unknown."}, {"heading": "5.1. Prediction of y With Missing data", "text": "Theorem 4. With all the notations and definitions above, and let N1 denote the restriction of N on the\nobserved entries of y. Then the prediction for y \u2208 N gnd has bounded performance:\n\u2016y\u0303\u2217 \u2212 y\u2016 \u2264 ( 1 + 1\n\u03c3min\n) \u03c1\u2016y\u2016,\nwhere \u03c1 = \u2016 sin \u0398\u2016 (see Theorem 3), \u03c3min is the smallest non-zero singular value of N1 (r\nth when N1 is nondegenerate).\nProof. By (4), and recall that only the first pm entries are observed, we have\ny\u0303\u2217 = N \u00b7 pinv(N1)y1 := ( y1 \u2212 e\u03031 y2 \u2212 e\u03032 ) := y + e\u0303.\nLet y\u2217 be the vector obtained by projecting y onto sub-\nspace N , and denote y\u2217 = ( y\u22171 y\u22172 ) = ( y1 \u2212 e1 y2 \u2212 e2 ) = y \u2212 e, we have:\ny\u0303\u2217 =N \u00b7 pinv(N1)(y\u22171 + e1) =N \u00b7 pinv(N1)y\u22171 +N \u00b7 pinv(N1)e1 =y\u2217 +N \u00b7 pinv(N1)e1.\nThen\n\u2016y\u0303\u2217 \u2212 y\u2016 =\u2016y\u2217 \u2212 y +N \u00b7 pinv(N1)e1\u2016\n\u2264\u2016y\u2217 \u2212 y\u2016+ 1 \u03c3min \u2016e1\u2016 \u2264\u03c1\u2016y\u2016+ 1 \u03c3min \u2016e1\u2016.\nFinally, we bound e1 as follows\n\u2016e1\u2016 \u2264 \u2016e\u2016 = \u2016y \u2212 y\u2217\u2016 \u2264 \u2016(Pgnd \u2212 PN )y\u2016 \u2264 \u03c1\u2016y\u2016,\nwhich completes the proof.\nSuppose y 6\u2208 N gnd and y = Pgndy + (I \u2212 Pgnd)y := ygnd + ygnd \u22a5 , then we have\n\u2016e1\u2016 \u2264 \u2016(Pgnd \u2212 PN )y\u2016+ \u2016ygnd \u22a5 \u2016 \u2264 \u03c1\u2016y\u2016+ \u2016ygnd \u22a5 \u2016,\nwhich leads to \u2016y\u0303\u2217 \u2212 ygnd\u2016 \u2264 ( 1 + 1\n\u03c3min\n) \u03c1\u2016y\u2016+ \u2016y\ngnd\u22a5\u2016 \u03c3min ."}, {"heading": "5.2. Bound on \u03c3min", "text": "To complete the above analysis, we now bound \u03c3min. Notice that in general \u03c3min can be arbitrarily close to zero, if N is \u201cspiky\u201d. Hence we impose the strong incoherence property introduced in Candes & Tao (2010) (see Appendix C for the definition) to avoid such situation. Due to space constraint, we defer the proof of the following to the Appendix C.\nProposition 1. If matrix Y satisfies strong incoherence property with parameter \u00b5, then:\n\u03c3min(N1) \u2265 1\u2212 ( r m + (1\u2212 p)\u00b5 \u221a r ) 1 2 .\nFor Gaussian Random Matrix\nStronger results on \u03c3min is possible for randomly generated matrices. As an example, we consider the case that Y = UV where U , V are two Gaussian random matrices of size m \u00d7 r and r \u00d7 n, and show that \u03c3min(N1) \u2248 \u221a p. Proposition 2. Let G \u2208 Rm\u00d7r have i.i.d. zero-mean Guassian random entries. Let N be its orthonormal basis4. Then there exists an absolute constant C such that with probability of at least 1\u2212 Cn\u221210,\n\u03c3min(N1) \u2265 \u221a k m \u2212 2 \u221a r m \u2212 C \u221a logm m .\nDue to space limit, the proof of Proposition 2 is deferred to the Supplementary material. The main idea is to apply established results about the singular values of Gaussian random matrix G (e.g., Rudelson & Vershynin, 2009; Silverstein, 1985; Davidson & Szarek, 2001), then show that the orthogonal basis N of G is very close to G itself.\nWe remark that the bound on singular values we used has been generalized to random matrices following subgaussian (Rudelson & Vershynin, 2009) and logconcave distributions (Litvak et al., 2005). As such, the the above result can be easily generalized to a much larger class of random matrices."}, {"heading": "6. Robustness against manipulators", "text": "In this section, we apply our results to study the \u201dprofile injection\u201d attacks on collaborative filtering. According to the empirical study of Mobasher et al. (2006), matrix factorization, as a model-based CF algorithm, is more robust to such attacks compared to similarity-based CF algorithms such as kNN. However, as Cheng & Hurley (2010) pointed out, it may not be a conclusive argument that model-based recommendation system is robust. Rather, it may due to the fact that that common attack schemes, effective to similarity based-approach, do not exploit the vulnerability of the model-based approach.\nOur discovery is in tune with both Mobasher et al. (2006) and Cheng & Hurley (2010). Specifically, we show that factorization methods are resilient to a class of common attack models, but are not so in general.\n4 Hence N is also the orthonormal basis of any Y generated with G being its left multiplier."}, {"heading": "6.1. Attack models", "text": "Depending on purpose, attackers may choose to inject \u201ddummy profiles\u201d in many ways. Models of different attack strategies are surveyed in Mobasher et al. (2007). For convenience, we propose to classify the models of attack into two distinctive categories: Targeted Attack and Mass Attack.\nTargeted Attacks include average attack (Lam & Riedl, 2004), segment attack and bandwagon attack (Mobasher et al., 2007). The common characteristic of targeted attacks is that they pretend to be the honest users in all ratings except on a few targets of interest. Thus, each dummy user can be decomposed into:\ne = egnd + s,\nwhere egnd \u2208 N and s is sparse. Mass Attacks include random attack, love-hate attack (Mobasher et al., 2007) and others. The common characteristic of mass attacks is that they insert dummy users such that many entries are manipulated. Hence, if we decompose a dummy user,\ne = egnd + egnd \u22a5 ,\nwhere egnd = PN e and egnd\u22a5 = (I\u2212PN )e \u2208 N\u22a5, then both components can have large magnitude. This is a more general model of attack."}, {"heading": "6.2. Robustness analysis", "text": "By definition, injected user profiles are column-wise: each dummy user corresponds to a corrupted column in the data matrix. For notational convenience, we re-arrange the order of columns into [Y |E ], where Y \u2208 Rm\u00d7n is of all honest users, and E \u2208 Rm\u00d7ne contains all dummy users. As we only care about the prediction of honest users\u2019 ratings, we can, without loss of generality, set ground truth to be [Y |Egnd ] and the additive error to be [ 0 |Egnd\u22a5 ]. Thus, the recovery error Z = [Y \u2217 \u2212 Y |E\u2217 \u2212 Egnd ]. Proposition 3. Assume all conditions of Theorem 1 hold. Under \u201dTargeted Attacks\u201d, there exists an absolute constant C, such that RMSE \u2264 4k \u221a smaxne |\u2126| +Ck ( (n+ ne)r log(n+ ne) |\u2126| ) 1 4 . (12) Here, smax is maximal number of targeted items of each dummy user.\nProof. In the case of \u201cTargeted Attacks\u201d, we have (recall that k = max(i,j) |Yi,j |)\n\u2016Egnd \u22a5 \u2016F < \u2211 i=1,...,ne \u2016si\u2016 \u2264 \u221a nesmax(2k)2.\nSubstituting this into Theorem 1 establishes the proposition.\nRemark 1. Proposition 3 essentially shows that matrix factorization approach is robust to the targeted attack model due to the fact that smax is small. Indeed, if the sampling rate |\u2126|/(m(n + ne)) is fixed, then RMSE converges to zero as m increases. This coincides with empirical results on Netflix data (Bell & Koren, 2007). In contrast, similarity-based algorithms (kNN) are extremely vulnerable to such attacks, due to the high similarity between dummy users and (some) honest users.\nIt is easy to see that the factorization method is less robust to mass attacks, simply because \u2016Egnd\u22a5\u2016F is not sparse, and hence smax can be as large as m. Thus, the right hand side of (12) may not diminish. Nevertheless, as we show below, if the number of \u201dMass Attackers\u201d does not exceed certain threshold, then the error will mainly concentrates on the E block. Hence, the prediction of the honest users is still acceptable.\nProposition 4. Assume sufficiently random subspace N (i.e., Propostion 2 holds), above definition of \u201cMass Attacks\u201d, and condition number \u03ba. If ne <\u221a n\n\u03ba2r ( E|Yi,j |2 k2 ) and |\u2126| = pm(n + ne) satisfying p > 1/m1/4, furthermore individual sample rate of each users is bounded within [p/2, 3p/2],5 then with probability of at least 1 \u2212 cm\u221210, the RMSE for honest users and for manipulators satisfies:\nRMSEY \u2264 C1\u03bak ( r3 log(n)\np3n\n)1/4 , RMSEE \u2264\nC2k\u221a p ,\nfor some universal constant c, C1 and C2.\nThe proof of Proposition 4, deferred in the supplementary material, involves bounding the prediction error of each individual users with Theorem 4 and sum over Y block and E block separately. Subspace difference \u03c1 is bounded with Theorem 1 and Theorem 3 together. Finally, \u03c3min is bounded via Proposition 2."}, {"heading": "6.3. Simulation", "text": "To verify our robustness paradigm, we conducted simulation for both models of attacks. Y is generated by multiplying two 1000\u00d710 gaussian random matrix and ne attackers are appended to the back of Y . Targeted Attacks are produced by randomly choosing from a column of Y and assign 2 \u201cpush\u201d and 2 \u201cnuke\u201d targets to 1 and -1 respectively. Mass Attacks are generated using uniform distribution. Factorization is performed using ALS. The results of the simulation are\n5This assumption is made to simplify the proof. It easily holds under i.i.d sampling.\nsummarized in Figure 1 and 2. Figure 1 compares the RMSE under two attack models. It shows that when the number of attackers increases, RMSE under targeted attack remains small, while RMSE under random attack significantly increases. Figure 2 compares RMSEE and RMSEY under random attack. It shows that when ne is small, RMSEY RMSEE . However, as ne increases, RMSEY grows and eventually is comparable to RMSEE . Both figures agree with our theoretic prediction."}, {"heading": "7. Concluding discussions", "text": "This paper presented a comprehensive study of the stability of matrix factorization methods. The key results include a near-optimal stability bound, a subspace stability bound and a worst-case bound for individual columns. Then the theory is applied to the notorious manipulator problem in collaborative filtering, which leads to an interesting insight of MF\u2019s inherent robustness.\nMatrix factorization is an important tool both for matrix completion task and for PCA with missing data. Yet, its practical success hinges on its stability \u2013 the\nability to tolerate noise and corruption. This paper is a first attempt to understand the stability of matrix factorization, which we hope will help to guide the application of matrix factorization methods.\nWe list some possible directions to extend this research in future. In the theoretical front, the arguably most important open question is that under what conditions matrix factorization can reach a solution near global optimal. In the algorithmic front, we showed here that matrix factorization methods can be vulnerable to general manipulators. Therefore, it is interesting to develop a robust variation of MF that provably handles arbitrary manipulators."}, {"heading": "Acknowledgments", "text": "This research is partially supported by the National University of Singapore under startup grant R-265000-384-133."}], "references": [{"title": "Spectral analysis of data", "author": ["Y. Azar", "A. Fiat", "A.R. Karlin", "F. McSherry", "J. Saia"], "venue": "In STOC, pp", "citeRegEx": "Azar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2001}, {"title": "Lessons from the netflix prize challenge", "author": ["R.M. Bell", "Y. Koren"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Bell and Koren,? \\Q2007\\E", "shortCiteRegEx": "Bell and Koren", "year": 2007}, {"title": "Matrix completion with noise", "author": ["E.J. Candes", "Y. Plan"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Candes and Plan,? \\Q2010\\E", "shortCiteRegEx": "Candes and Plan", "year": 2010}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["E.J. Candes", "T. Tao"], "venue": "IEEE Info. Theory,", "citeRegEx": "Candes and Tao,? \\Q2010\\E", "shortCiteRegEx": "Candes and Tao", "year": 2010}, {"title": "Optimization algorithms on subspaces: Revisiting missing data problem in low-rank matrix", "author": ["P. Chen"], "venue": null, "citeRegEx": "Chen,? \\Q2008\\E", "shortCiteRegEx": "Chen", "year": 2008}, {"title": "Robustness analysis of modelbased collaborative filtering systems", "author": ["Z. Cheng", "N. Hurley"], "venue": "In AICS\u201909, pp", "citeRegEx": "Cheng and Hurley,? \\Q2010\\E", "shortCiteRegEx": "Cheng and Hurley", "year": 2010}, {"title": "Local operator theory, random matrices and banach spaces", "author": ["K.R. Davidson", "S.J. Szarek"], "venue": "Handbook of the geometry of Banach spaces,", "citeRegEx": "Davidson and Szarek,? \\Q2001\\E", "shortCiteRegEx": "Davidson and Szarek", "year": 2001}, {"title": "Competitive recommendation systems", "author": ["P. Drineas", "I. Kerenidis", "P. Raghavan"], "venue": "In STOC, pp", "citeRegEx": "Drineas et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2002}, {"title": "Matrix completion from noisy entries", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "JMLR, 11:2057\u20132078,", "citeRegEx": "Keshavan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2010}, {"title": "Matrix completion from a few entries", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "IEEE Info. Theory,", "citeRegEx": "Keshavan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2010}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "IEEE Tran. Computer,", "citeRegEx": "Koren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Shilling recommender systems for fun and profit", "author": ["S.K. Lam", "J. Riedl"], "venue": "In WWW\u201904,", "citeRegEx": "Lam and Riedl,? \\Q2004\\E", "shortCiteRegEx": "Lam and Riedl", "year": 2004}, {"title": "Smallest singular value of random matrices and geometry of random polytopes", "author": ["A.E. Litvak", "A. Pajor", "M. Rudelson", "N. TomczakJaegermann"], "venue": "Advances in Mathematics,", "citeRegEx": "Litvak et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Litvak et al\\.", "year": 2005}, {"title": "Large-scale matrix factorization with missing data under additional constraints", "author": ["K. Mitra", "S. Sheorey", "R. Chellappa"], "venue": null, "citeRegEx": "Mitra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitra et al\\.", "year": 2010}, {"title": "Model-based collaborative filtering as a defense against profile injection attacks", "author": ["B. Mobasher", "R. Burke", "J.J. Sandvig"], "venue": "In AAAI\u201906,", "citeRegEx": "Mobasher et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mobasher et al\\.", "year": 2006}, {"title": "Toward trustworthy recommender systems: An analysis of attack models and algorithm robustness", "author": ["B. Mobasher", "R. Burke", "R. Bhaumik", "C. Williams"], "venue": "ACM Tran. Inf. Tech.,", "citeRegEx": "Mobasher et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mobasher et al\\.", "year": 2007}, {"title": "On the wiberg algorithm for matrix factorization in the presence of missing components", "author": ["T. Okatani", "K. Deguchi"], "venue": null, "citeRegEx": "Okatani and Deguchi,? \\Q2007\\E", "shortCiteRegEx": "Okatani and Deguchi", "year": 2007}, {"title": "Smallest singular value of a random rectangular matrix", "author": ["M. Rudelson", "R. Vershynin"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Rudelson and Vershynin,? \\Q2009\\E", "shortCiteRegEx": "Rudelson and Vershynin", "year": 2009}, {"title": "The smallest eigenvalue of a large dimensional wishart matrix", "author": ["J.W. Silverstein"], "venue": "The Annals of Probability,", "citeRegEx": "Silverstein,? \\Q1985\\E", "shortCiteRegEx": "Silverstein", "year": 1985}, {"title": "A unified view of matrix factorization models", "author": ["A. Singh", "G. Gordon"], "venue": "Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Singh and Gordon,? \\Q2008\\E", "shortCiteRegEx": "Singh and Gordon", "year": 2008}, {"title": "Learning with matrix factorizations", "author": ["N. Srebro"], "venue": "PhD thesis, M.I.T.,", "citeRegEx": "Srebro,? \\Q2004\\E", "shortCiteRegEx": "Srebro", "year": 2004}, {"title": "Matrix perturbation theory", "author": ["G.W. Stewart", "J. Sun"], "venue": "Academic press New York,", "citeRegEx": "Stewart and Sun,? \\Q1990\\E", "shortCiteRegEx": "Stewart and Sun", "year": 1990}, {"title": "A survey of collaborative filtering techniques", "author": ["X. Su", "T.M. Khoshgoftaar"], "venue": "Adv. in AI,", "citeRegEx": "Su and Khoshgoftaar,? \\Q2009\\E", "shortCiteRegEx": "Su and Khoshgoftaar", "year": 2009}, {"title": "Investigation of various matrix factorization methods for large recommender systems", "author": ["G. Tak\u00e1cs", "I. Pil\u00e1szy", "B. N\u00e9meth", "D. Tikk"], "venue": "In ICDMW\u201908,", "citeRegEx": "Tak\u00e1cs et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tak\u00e1cs et al\\.", "year": 2008}, {"title": "Solving a low-rank factorization model for matrix completion by a nonlinear successive over-relaxation algorithm", "author": ["Z. Wen"], "venue": "Rice University CAAM Technical Report,", "citeRegEx": "Wen,? \\Q2010\\E", "shortCiteRegEx": "Wen", "year": 2010}], "referenceMentions": [{"referenceID": 10, "context": "Among various models proposed, matrix factorization is arguably the most widely applied method, due to its high accuracy, scalability (Su & Khoshgoftaar, 2009) and flexibility to incorporating domain knowledge (Koren et al., 2009).", "startOffset": 210, "endOffset": 230}, {"referenceID": 0, "context": "Azar et al. (2001) analyzed asymptotic performance of matrix factorization methods, yet under stringent assumptions on the fraction of observation and on the singular values.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Azar et al. (2001) analyzed asymptotic performance of matrix factorization methods, yet under stringent assumptions on the fraction of observation and on the singular values. Drineas et al. (2002) relaxed these assumptions but it requires a few fully rated users \u2013 a situation that rarely happens in practice.", "startOffset": 0, "endOffset": 197}, {"referenceID": 0, "context": "Azar et al. (2001) analyzed asymptotic performance of matrix factorization methods, yet under stringent assumptions on the fraction of observation and on the singular values. Drineas et al. (2002) relaxed these assumptions but it requires a few fully rated users \u2013 a situation that rarely happens in practice. Srebro (2004) considered the problem of the generalization error of learning a low-rank matrix.", "startOffset": 0, "endOffset": 324}, {"referenceID": 8, "context": "Recently, some alternative algorithms, notably StableMC (Candes & Plan, 2010) based on nuclear norm optimization, and OptSpace (Keshavan et al., 2010b) based on gradient descent over the Grassmannian, have been shown to be stable vis a vis noise (Candes & Plan, 2010; Keshavan et al., 2010a). However, these two methods are less effective in practice. As documented in Mitra et al. (2010); Wen (2010) and many others, matrix factorization methods typically outperform these two methods.", "startOffset": 128, "endOffset": 389}, {"referenceID": 8, "context": "Recently, some alternative algorithms, notably StableMC (Candes & Plan, 2010) based on nuclear norm optimization, and OptSpace (Keshavan et al., 2010b) based on gradient descent over the Grassmannian, have been shown to be stable vis a vis noise (Candes & Plan, 2010; Keshavan et al., 2010a). However, these two methods are less effective in practice. As documented in Mitra et al. (2010); Wen (2010) and many others, matrix factorization methods typically outperform these two methods.", "startOffset": 128, "endOffset": 401}, {"referenceID": 4, "context": "As pointed out in Chen (2008), an alternative interpretation of collaborative filtering is fitting the optimal r-dimensional subspace N to the sampled data.", "startOffset": 18, "endOffset": 30}, {"referenceID": 20, "context": "Specific variations for CF are investigated in Tak\u00e1cs et al. (2008) and Koren et al.", "startOffset": 47, "endOffset": 68}, {"referenceID": 9, "context": "(2008) and Koren et al. (2009). From an empirical perspective, Mitra et al.", "startOffset": 11, "endOffset": 31}, {"referenceID": 9, "context": "(2008) and Koren et al. (2009). From an empirical perspective, Mitra et al. (2010) reported that the global optimum is often obtained in simulation and Chen (2008) demonstrated satisfactory percentage of hits to global minimum from randomly initialized trials on a real data set.", "startOffset": 11, "endOffset": 83}, {"referenceID": 4, "context": "(2010) reported that the global optimum is often obtained in simulation and Chen (2008) demonstrated satisfactory percentage of hits to global minimum from randomly initialized trials on a real data set.", "startOffset": 76, "endOffset": 88}, {"referenceID": 8, "context": "We recall similar RMSE bounds for StableMC of Candes & Plan (2010) and OptSpace of Keshavan et al. (2010a):", "startOffset": 83, "endOffset": 107}, {"referenceID": 18, "context": "The main idea is to apply established results about the singular values of Gaussian random matrix G (e.g., Rudelson & Vershynin, 2009; Silverstein, 1985; Davidson & Szarek, 2001), then show that the orthogonal basis N of G is very close to G itself.", "startOffset": 100, "endOffset": 178}, {"referenceID": 12, "context": "We remark that the bound on singular values we used has been generalized to random matrices following subgaussian (Rudelson & Vershynin, 2009) and logconcave distributions (Litvak et al., 2005).", "startOffset": 172, "endOffset": 193}, {"referenceID": 13, "context": "According to the empirical study of Mobasher et al. (2006), matrix factorization, as a model-based CF algorithm, is more robust to such attacks compared to similarity-based CF algorithms such as kNN.", "startOffset": 36, "endOffset": 59}, {"referenceID": 4, "context": "However, as Cheng & Hurley (2010) pointed out, it may not be a conclusive argument that model-based recommendation system is robust.", "startOffset": 12, "endOffset": 34}, {"referenceID": 4, "context": "However, as Cheng & Hurley (2010) pointed out, it may not be a conclusive argument that model-based recommendation system is robust. Rather, it may due to the fact that that common attack schemes, effective to similarity based-approach, do not exploit the vulnerability of the model-based approach. Our discovery is in tune with both Mobasher et al. (2006) and Cheng & Hurley (2010).", "startOffset": 12, "endOffset": 357}, {"referenceID": 4, "context": "However, as Cheng & Hurley (2010) pointed out, it may not be a conclusive argument that model-based recommendation system is robust. Rather, it may due to the fact that that common attack schemes, effective to similarity based-approach, do not exploit the vulnerability of the model-based approach. Our discovery is in tune with both Mobasher et al. (2006) and Cheng & Hurley (2010). Specifically, we show that factorization methods are resilient to a class of common attack models, but are not so in general.", "startOffset": 12, "endOffset": 383}, {"referenceID": 14, "context": "Models of different attack strategies are surveyed in Mobasher et al. (2007). For convenience, we propose to classify the models of attack into two distinctive categories: Targeted Attack and Mass Attack.", "startOffset": 54, "endOffset": 77}, {"referenceID": 15, "context": "Targeted Attacks include average attack (Lam & Riedl, 2004), segment attack and bandwagon attack (Mobasher et al., 2007).", "startOffset": 97, "endOffset": 120}, {"referenceID": 15, "context": "Mass Attacks include random attack, love-hate attack (Mobasher et al., 2007) and others.", "startOffset": 53, "endOffset": 76}], "year": 2012, "abstractText": "We study the stability vis a vis adversarial noise of matrix factorization algorithm for matrix completion. In particular, our results include: (I) we bound the gap between the solution matrix of the factorization method and the ground truth in terms of root mean square error; (II) we treat the matrix factorization as a subspace fitting problem and analyze the difference between the solution subspace and the ground truth; (III) we analyze the prediction error of individual users based on the subspace stability. We apply these results to the problem of collaborative filtering under manipulator attack, which leads to useful insights and guidelines for collaborative filtering system design.", "creator": "LaTeX with hyperref package"}}}