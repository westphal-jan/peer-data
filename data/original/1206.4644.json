{"id": "1206.4644", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Groupwise Constrained Reconstruction for Subspace Clustering", "abstract": "Reconstruction based subspace clustering methods compute a self reconstruction matrix over the samples and use it for spectral clustering to obtain the final clustering result. Their success largely relies on the assumption that the underlying subspaces are independent, which, however, does not always hold in the applications with increasing number of subspaces. In this paper, we propose a novel reconstruction based subspace clustering model without making the subspace independence assumption. In our model, certain properties of the reconstruction matrix are explicitly characterized using the latent cluster indicators, and the affinity matrix used for spectral clustering can be directly built from the posterior of the latent cluster indicators instead of the reconstruction matrix. Experimental results on both synthetic and real-world datasets show that the proposed model can outperform the state-of-the-art methods.", "histories": [["v1", "Mon, 18 Jun 2012 15:19:22 GMT  (554kb)", "http://arxiv.org/abs/1206.4644v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ruijiang li", "bin li", "cheng jin", "xiangyang xue"], "accepted": true, "id": "1206.4644"}, "pdf": {"name": "1206.4644.pdf", "metadata": {"source": "META", "title": "Groupwise Constrained Reconstruction for Subspace Clustering", "authors": ["Ruijiang Li", "Bin Li", "Ke Zhang", "Xiangyang Xue"], "emails": ["rjli@fudan.edu.cn", "bin.li-1@uts.edu.au", "k_zhang@fudan.edu.cn", "jc@fudan.edu.cn", "xyxue@fudan.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "Subspace clustering aims to group the given samples into clusters according to the criterion that samples in the same cluster are drawn from the same linear subspace. In the last decade, a number of subspace clustering methods have been proposed with successful applications in the areas including motion segmentation (Kanatani, 2001; Vidal & Hartley, 2004; Elhamifar & Vidal, 2009), image clustering under different illuminations (Ho et al., 2003), etc. Generally speaking, existing approaches to subspace clustering can be classified into the following categories: matrix factorization\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nbased, algebraic based, statistically modelling, and reconstruction based, among which the reconstruction based approach has been proved most effective and has drawn much attention recently (Elhamifar & Vidal, 2009; Liu et al., 2010; Wang et al., 2011). In this paper, we focus on the reconstruction based approach.\nThe objective of the reconstruction based subspace clustering is to approximate the dataset X \u2208 RD\u00d7N (N is the number of samples andD denotes the sample dimensionality) with the reconstruction XW , where W \u2208 RN\u00d7N is the reconstruction matrix which can be further used to build the affinity matrix |W |+\n\u2223\u2223W>\u2223\u2223 for spectral clustering. The intuition behind the reconstruction is to make the value of wij small or even vanish if samples xi and xj are not in the same subspace, such that the subspaces/clusters can be easily identified by the subsequent spectral clustering.\nAll the existing reconstruction based methods come with proofs claiming that the desired W could be obtained under the subspace independence assumption, i.e., the underlying subspaces S1,S2, \u00b7 \u00b7 \u00b7 ,SK are linearly independent, or mathematically,\ndim( K\u2211 k=1 \u2295Sk) = K\u2211 k=1 dim(Sk) (1)\nUnfortunately, this assumption will be violated if there exist bases shared among the subspaces. For example, given three orthogonal bases, b1, b2, b3, and two subspaces, S1 = b1 \u2295 b2 and S2 = b3 \u2295 b2 (b2 is shared in S1 and S2), the l.h.s. of Eq.(1) is 3, which is smaller than the r.h.s. being 4. In real-world scenarios, the subspace independence assumption does not always hold. For example, in human face clustering, as the number of clusters (persons) increases, the r.h.s. of Eq.(1) will exceed the l.h.s., which is upper bounded by the dimensionality of \u201chuman faces\u201d, so the subspace independence assumption will be violated eventually. Figure 1 illustrates this phenomenon based on the Extended Yale Database B (Georghiades et al.,\n2001). Once the subspace independence assumption is violated, there is no guarantee that the existing reconstruction based methods are able to obtain the desired W . In practice, we observe that the subspace independence assumption is critical to the success of the existing reconstruction based methods. Once the subspace independence assumption is violated, the performance of these existing reconstruction based methods become far from decent, even though the dimensionality of the underlying subspaces is low (shown in Section 4.1.1).\nTo tackle the subspace clustering problem, we propose a Groupwise Constrained Reconstruction (GCR) model, with the advantage that GCR no longer relies on the subspace independence assumption. In GCR, the sample cluster indicators are introduced as latent variables, conditioned on which the Slab-and-Spikelike priors are used as groupwise constraints to suppress the magnitude of certain entries in W . Thanks to these constraints, the requirement of the subspace independence assumption is no longer needed to obtain the desired W . Our method significantly differs from the existing methods in that, the reconstruction in GCR incorporates the information that \u201cthe samples can be grouped into clusters\u201d; whereas in the existing methods, this information is ignored and the reconstruction depends solely on the data.\nAnother advantage of GCR is that, the affinity matrix needed for spectral clustering can be built from the cluster indicators rather than W . In our model, the reconstruction matrixW can be analytically marginalized out. We first use Gibbs Sampler to collect samples from the posterior of the cluster indicators, then use the collected samples to build the \u201cprobabilistic affinity matrix\u201d, which is finally input to the spectral clustering algorithm to obtain the final clustering result. Compared with |W | + \u2223\u2223W>\u2223\u2223, which is used as\nthe affinity matrix in the existing methods, the probabilistic affinity matrix built from the cluster indicators is more sophisticated, because it is naturally positive, symmetric and of clear interpretation. The experimental results on synthetic dataset, motion segmentation dataset and human face dataset show that GCR can outperform the state-of-the-art."}, {"heading": "2. Background", "text": "In this section, we give a brief introduction to the previous works on subspace clustering."}, {"heading": "2.1. Non-Reconstruction Based", "text": "Matrix factorization based methods Costeira & Kanade (1998); Kanatani (2001) approximate the data matrix with the product of two matrices, one containing the bases and the other containing the factors. The final clustering result is obtained by exploiting the factor matrix. These methods are not robust to noise and outliers and will fail if the subspaces are dependent.\nThe algebraic based General Principle Component Analysis (GPCA) (Vidal et al., 2005) fits the samples with a polynomial, with the gradient of a point orthogonal to the subspace containing it. This approach makes fewer assumptions on the subspaces, and the success is guaranteed when certain conditions are met. The major problem of the algebraic based approach is that the computational complexity is high (exponential to the number of subspaces and their dimensions), which restricts its application scenarios. In (Rao et al., 2010), Robust Algebraic Segmentation (RAS) is proposed to handle the data with outliers, but the complexity issue still remains.\nStatistical models assume that the samples in each subspace are drawn from a certain distribution such as Gaussian, and take different objectives to find the optimal clustering result. For example, Mixture of Probabilistic PCA (Tipping & Bishop, 1999) uses the Expectation Maximization (EM) algorithm to find the maximum likelihood over all the samples, k-subspaces method (Ho et al., 2003) alternates between assigning the cluster to each sample and updating the subspaces, Random Sample Consensus (RANSAC) (Fischler & Bolles, 1981) keeps looking for the samples in the same subspace until the number of samples in the subspace is sufficient, then continues searching another subspace after removing these samples. Agglomerative Lossy Compression (ALC) (Ma et al., 2007) searches the latent subspaces by minimizing an objective containing certain information criteria with an agglomerative strategy."}, {"heading": "2.2. Reconstruction Based", "text": "Reconstruction based methods usually consist of the following two steps: 1) Find a reconstruction for all the samples, in the form that each sample is approximated by the weighted sum of the other samples in the dataset. The optimization problem in Eq.(2) is solved to get the reconstruction weight matrix W .\nmin W\n`(X \u2212XW ) + \u03c9\u2126(W ) (2)\ns.t. wii = 0\nwhere the term l(\u00b7) : RD\u00d7N 7\u2192 R measures the error made by approximating xi with its reconstruction\u2211 j 6=i wjixj , the term \u2126(\u00b7) : RN\u00d7N 7\u2192 R is used for regularization, and \u03c9 is a tradeoff parameter. 2) Apply spectral clustering algorithm to get the final clustering result from the reconstruction weightsW . Usually, |W |+ |W |> is treated as the affinity matrix input to the spectral clustering methods.\nThe methods of this class distinguish from each other in employing different regularization terms, i.e., \u2126(W ) in Eq.(2). In Sparse Subspace Clustering (SSC) (Elhamifar & Vidal, 2009), the authors propose to use the l1 norm \u2016W \u20161 to enforce the sparseness in W , in the hope that the sparse coding process could shrink wji to zero if xi and xj are not in the same subspace. In Low-Rank Representation (LRR) (Liu et al., 2010), nuclear norm \u2016W \u2016\u2217 is used to encourage W to have a low rank structure1, and l2,1 norm is used as the `(\u00b7) term in Eq.(2) to make the method more robust to outliers. In SSQP (Wang et al., 2011), the authors choose \u2126(W ) = \u2225\u2225W>W\u2225\u2225 1 , meanwhile force W to be non-negative. As a consequence, the optimization problem in Eq.(2) turns out to be a quadratic programming problem, for which the projected gradient descend method can be used to find a solution."}, {"heading": "3. Groupwise Constrained Reconstruction Model", "text": "Consider a clustering task in which we want to group N samples, denoted by X = [x1,x2, \u00b7 \u00b7 \u00b7 ,xN ] \u2208 RD\u00d7N , intoK clusters, where N is the number of samples, D is the sample dimensionality, and xi \u2208 RD denotes the i-th sample. Let z = [z1; z2; \u00b7 \u00b7 \u00b7 ; zN ] be the cluster indicator vector, where zi \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K} indicates that sample xi is drawn from the zi-th cluster. The goal of subspace clustering is to find the cluster indicators z, such that for each k \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K}, the samples in the k-th cluster, i.e., {xi|zi = k}Ni=1, reside in the same linear space. This objective is quite\n1Nuclear norm \u2016W \u2016\u2217is defined to be the sum of singular values of matrix W .\ndifferent from the objective of traditional clustering methods, in which the variance of inter-cluster samples are minimized, such as K-means; or the \u201cdifference\u201d of clusters are maximized, such as Discriminative Clustering (Ye et al., 2007)."}, {"heading": "3.1. Model", "text": "Following the idea of the reconstruction based approach to subspace clustering, the Groupwise Constrained Reconstruction (GCR) model uses p(X|W ) in Eq.(3) to quantify the reconstruction,\np(X|W ,\u03c3) = N\u220f i=1 N (xi| \u2211 j 6=i wjixj , \u03c3 2 i ) (3)\nwhere N (\u00b7|\u00b5,\u03a3) denotes the Gaussian distribution with mean \u00b5 and variance \u03a3, wji is the element at the j-th row, i-th column of matrixW \u2208 RN\u00d7N , \u03c32i > 0 is a random variable measuring the reconstruction error for the i-th sample, and \u03c3 = [\u03c31;\u03c32; \u00b7 \u00b7 \u00b7 ;\u03c3N ] \u2208 RN . We place an inverse Gamma prior on all the \u03c3i\u2019s:\np(\u03c32i ) = IG(\u03c3 2 i | \u03bd 2 , \u03bd\u03bb 2 ) (4)\nwhere IG denotes the inverse Gamma distribution, and \u03bd > 0 and \u03bb > 0 are given hyperparameters.\nWhat makes the GCR model different is that, GCR explicitly requires every sample to be reconstructed mainly by the samples in the same cluster. In other words, the magnitudes of weights for the samples in different clusters should be small. Intuitively, W should be nearly block-wise diagonal if the samples are rearranged in a proper order (see Figure 2(b) for an illustration). To enforce such property of W , we treat the cluster indicators z as latent random variables, and introduce a prior for W conditioned on z and \u03c3 as follows,\np(W |z,\u03c3) = N\u220f i=1 N\u220f j=1 N (wji|0, \u03c32i \u03b1ji) (5)\n\u03b1ji = \u03b1ij = { \u03b1L zj 6= zi \u03b1H zj = zi\nwhere \u03b1H > \u03b1L \u2265 0 are hyperparameters and \u03b1L\u03b1H is small. This prior is quite similar to the Slab and Spike prior used for variable selection (George & Mcculloch, 1997), with \u03b1H corresponding to the slab and \u03b1L corresponding to the spike. As the effects of Eq.(5), to generate W given the latent cluster indicators, if xi and xj are not in the same cluster/subspace, wji and wij are restricted to be small or close to the mean value 0 of the corresponding Gaussian distribution; if xj and\nxi come from the same cluster/subspace, the values of wji and wij could be either small or big. We makeW dependent on \u03c3 as well, so that both \u03c3 and W can be further marginalized out by combining Eqs.(3), (4) and (5), which will be discussed later.\nFurthermore, we introduce a discrete prior\np(z|\u03b8) = N\u220f i=1 Cate(zi|\u03b8)\nfor the cluster indicators z conditioned on \u03b8 = [\u03b81; \u03b82; \u00b7 \u00b7 \u00b7 ; \u03b8K ] \u2208 RK , where Cate(zi|\u03b8) = \u03b8zi denotes the categorical distribution, and \u03b8k \u2208 [0, 1] can be viewed as the prior knowledge about the proportion of samples in the k-th cluster. Since it is difficult to set \u03b8 beforehand, we use a Dirichlet distribution\np(\u03b8) = Dir(\u03b8|\u03b20 K 1K)\nas a prior for \u03b8, where Dir(\u00b7) denotes the Dirichlet distribution, and 1K = [1, 1, \u00b7 \u00b7 \u00b7 , 1] \u2208 RK .\nThe hierarchical representation for GCR model is shown in Figure 2(a), and the full probability can be written as follows,\np(X,W , z,\u03b8,\u03c3)\n= [p(\u03b8)p(z|\u03b8)] [p(W |z,\u03c3)p(X|W ,\u03c3)p(\u03c3)] (6)\nObserving thatW ,\u03c3 and \u03b8 in Eq.(6) can be marginalized out analytically, we can write down p(z|X), de-\nnoted as q(z) for short, as follows,\nq(z) \u221d f0 N\u220f i=1 fi (7)\nf0 = K\u220f k=1 \u0393 ( \u03b20 K + nk(z) ) fi = det(Ci) \u2212 12 ( x>i C \u22121 i xi + \u03bd\u03bb\n)\u2212D+\u03bd2 Ci = Hzi \u2212 \u03b1Hxix>i Hk =\n\u2211 j|zj=k \u03b1Hxjx > j + \u2211 j|zj 6=k \u03b1Lxjx > j + ID\nwhere f0 and fi comes from the first and the second brackets in Eq.(6), respectively; nk(z) is the number of samples in the k-th cluster; \u0393(\u00b7) denotes the Gamma function; and ID \u2208 RD\u00d7D denotes the identity matrix."}, {"heading": "3.2. Obtaining the Final Clustering Result", "text": "We use the Gibbs Sampling algorithm (MacKay, 2003) to approximate the posterior distribution q(z). In each epoch, for i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , N}, the Gibbs sampler iteratively updates zi to a sample drawn from p(zi|z\u223ci,X) = p(z|X)p(z\u223ci|X) \u221d q(z), where z\u223ci = {zj |j 6= i}. A direct implementation will lead to the time complexity of O(N2D3) for each epoch. Fortunately, the complexity can be reduced to O(N2D + KD2) using rank-1 update. At the end of each epoch, we collect the values of all the cluster indicators as a sample of z. Finally, we save the samples of z from the last M epochs, denoted as s1, s2, \u00b7 \u00b7 \u00b7 , sM , and discard the samples left. We can use the following two approaches to obtain the final clustering result.\nMAP approach. Use the last collected sample sM as an initialization, then maximize the posterior q(z) in Eq.(7) by alternating among z1, z2, \u00b7 \u00b7 \u00b7 , zN . The local maximum is directly used as the clustering result.\nBayesian approach. With the collected samples, we first compute an affinity matrix Gm \u2208 RN\u00d7N over the N samples, where\n(Gm)ij =\n{ 1 (sm)i = (sm)j\n0 (sm)i 6= (sm)j (8)\nthen compute the \u201cprobabilistic affinity matrix\u201d G = 1 M \u2211 mGm; and finally put G into a classical clustering method to obtain the final clustering result.\nHere, Gij can be treated as an approximation to the posterior distribution p(zi = zj |X). Compared with existing reconstruction based methods which use |W |+ |W |> as the affinity matrix input into the spec-\ntral clustering algorithm, our probabilistic affinity matrix G is more sophisticated since Gij can be clearly interpreted as the the possibility that sample i and j share the same cluster label. What is more, our affinity matrixG is naturally positive and symmetric, whereas |W |+ |W |> is somehow like an ad-hoc way to \u201cforce\u201d W to be an affinity matrix."}, {"heading": "3.3. When K \u2192 +\u221e", "text": "From Eq.(8) we see that, to obtain the probabilistic affinity matrix G, it is not mandatory to set K to be the exact number of subspaces. In fact, the probabilistic affinity matrix can be obtained with any positive integer K. Particularly, we are interested in the GCR model when K goes to positive infinity, in which case, the number of non-empty clusters remains a finite number (at most N when each sample forms its own cluster). This strategy is in analogy with the Infinite Gaussian Mixture Model with Dirichlet Process (Rasmussen, 1999). For this reason, we refer to the GCR model with K \u2192 +\u221e as GCR-DP. As the limit of GCR, the posterior of z for GCR-DP is\nq\u0302(z) \u221d f\u03020 N\u220f i=1 fi, f\u03020 = \u03b2 K\u0302\u22121 0 K\u0302\u220f k=1 \u0393(nk(z)) (9)\nwhere fi remains the same as in Eq.(7), and K\u0302 is the number of non-empty clusters 2. The Gibbs sampling procedure is similar to that of the original GCR, and the difference is described as follows. Suppose for now there are K \u2032 non-empty clusters, to update zi, besides computing K \u2032 values for the non-empty clusters by plugging zi \u2190 {1, 2, , \u00b7 \u00b7 \u00b7 , K\u0302} into the r.h.s. of Eq.(9), we need to compute an extra value for a new empty cluster by plugging K\u0302 \u2190 K \u2032 + 1 and zi = K \u2032 + 1 into Eq.(9). Then the Categorical sampler picks a cluster indicator for zi according to these K \u2032 + 1 values. If the indicator for the new cluster (K \u2032+ 1) is picked, we create a new empty cluster and put the i-th sample into it. The variables for the empty clusters can be removed to save the computational resource.\nIn the case of K \u2192 \u221e, Eq.(6) shows that there exists a trade-off among the reconstruction quality, prior for the cluster indicators and p(W |z,\u03c3). p(W |z,\u03c3) prefers more clusters, in which case more spikes in Eq.(5) could be introduced into the model, resulting in high p.d.f. of p(W |z,\u03c3). On the contrary, the Dirichlet process prior favors fewer number of clusters. In the premise of good reconstruction quality (p(X|W ,\u03c3) is high), the competition between the Dirichlet process\n2To use Eq.(9), z should be reorganized so that the first K\u0302 clusters are non-empty.\nprior p(z) and p(W |z,\u03c3) provides a way to circumvent the trivial solutions to the model (all the samples in one cluster or each sample in it\u2019s own cluster).\nDue to the allowance to create more clusters, the outliers, which cannot be well reconstructed by the inliers, have the chance to \u201cstand alone\u201d. As a result, the influence of the outliers can be reduced."}, {"heading": "3.4. Hyperparameters", "text": "\u03b20: Throughout our experiment, \u03b20 for the Dirichlet distribution is always set to 1.\n\u03bb and \u03bd: From Eq.(4) we see that \u03bb and \u03bd control the reconstruction quality. According to the property of the inverse Gamma distribution, we have E(\u03c3\u22121i ) = 1 \u03bb and Var(\u03c3\u22121i ) = 2 \u03bb2\u03bd . Thus, it is reasonable to set \u03bb to a smaller number if the dataset are less noisy, and set \u03bd to a smaller number if the variance of the reconstruction quality for different samples is higher (e.g., the dataset has more outliers). In our experiments, these two parameters are tuned for different datasets.\n\u03b1H and \u03b1L: According to Eq.(5), \u03c32i \u03b1H and \u03c32i \u03b1L directly influence the magnitude of wji. Since E(\u03c3\u22121i ) = 1 \u03bb , we can use \u03bb\u03b1H and \u03bb\u03b1L to control the magnitude of wji intuitively. After integrating out \u03c3, we can rewrite the prior for W as p(W |z) =\u220fN i=1 \u220fN j=1 T (wji|\u03bd, 0, \u03bb\u03b1ji), where T (\u00b7|u, v, w) denotes the student t distribution with degree of freedom u, mean v and variance w. Therefore, it is natural to use the mean value of the t distribution to control the magnitude of W . In practice, we find that \u03bb\u03b1H = 0.1 and \u03b1H\u03b1L = 10000 yield good performance."}, {"heading": "4. Experimental Results", "text": "In this section, we compare our methods with the other three reconstruction based subspace clustering methods: LRR (Liu et al., 2010), SSC (Elhamifar & Vidal, 2009) and SSQP (Wang et al., 2011). In our evaluation, the quality of clustering is measured by accuracy, which is computed as the maximum percentage of match between the clustering result and the ground truth. For GCR, the MAP estimation is directly used as the final clustering result; for GCR-DP, we first compute the probabilistic affinity matrix according to Eq.(8), then use NCut (Shi & Malik, 2000) to get the final clustering result. For MCMC, we treat G(0) =\n\u2223\u2223\u2223(X>X + \u03b4I)\u22121\u2223\u2223\u2223 \u2208 RN\u00d7N as the affinity matrix, and the result of spectral clustering is used as the initialization3. This can be understood by switching\n3\u03b4 is a jitter value making the matrix invertible.\nthe rule between sample (N) and dimension (D), in such a way that G0 becomes the precision matrix over N samples, and \u2223\u2223\u2223G(0)ij \u2223\u2223\u2223 measures the dependency between the i-th and the j-th samples conditioned on the other samples. We set the number of epochs for the Gibbs sampler to 500, and use the last 100 samples to construct the probabilistic affinity matrix. We find that under such settings, our methods runs faster than SSC and SSQP empirically."}, {"heading": "4.1. Synthetic Datasets", "text": "We use synthetic datasets to investigate how these reconstruction based methods perform when the subspace independence assumption mentioned in Section 1 is violated. The synthetic data containing K subspaces are generated as follows: 1) Generate a matrix B \u2208 R2\u00d750, each column of which is drawn from a Gaussian distribution N (\u00b7|0, I2). 2) For the k-th cluster containing nk samples, generate y1 \u2208 Rnk , the elements of which are drawn independently from the uniform distribution defined on [\u22121, 1]. After that, generate y2 = tan 16k17Ky1 (avoiding tan \u03c0 2 ). Finally, generate the nk samples in the k-th cluster as [y1,y2]B \u2208 RnK\u00d750. All the experiments here are repeated for 5 times."}, {"heading": "4.1.1. Violation of Subspace Independence Assumption", "text": "For K = 2, 3, \u00b7 \u00b7 \u00b7 , 8, we generate 7 datasets according to the steps listed above. For these synthetic datasets, the l.h.s. of Eq.(1) is 2, and the r.h.s. of Eq.(1) is K. Thus, the degree of the violation of the subspace independence assumption increases as K increases. The results are reported in Figure 3(a).\nAs we can see, LRR and SSC perform well when the subspace independence assumption holds (K = 2) or is slightly violated (K = 3). However, their performance decreases significantly as the violation degree increases, even though their parameters are tuned for different K. In contrast, GCR and GCR-DP are able to retain high performance even though the violation degree keeps increasing.\nIn the case of K = 8, we compare the affinity matrices produced by these reconstruction based methods, as shown in Figure 4. Obviously, the affinity matrix produced by GCR-DP has stronger discrimination power on the clusters than those of the others. The affinity matrix produced by SSQP looks promising. However, a deep investigation shows that in the matrix the sum of many rows are zero, making the clustering performance less satisfactory."}, {"heading": "4.1.2. Increasing Portion of Noisy Samples", "text": "Consider the case when there exist samples deviating from the exact positions in the subspaces. Following the previous listed steps, we generate a dataset containing 2 subspaces, each of which contains 50 samples. We add Gaussian noises N (\u00b7|0, 3) to 0%, 5%, \u00b7 \u00b7 \u00b7 , 40% of the samples, respectively. The results on the 9 datasets are reported in Figure 3(b).\nThe results show that our methods and LRR are able to maintain high accuracy even though high portion of the samples deviate from their ideal position. The success of LRR is due to the l2,1 norm used for the loss term in Eq.(2), while the success of GCR and GCR-DP may be due to the model in which each sample has its own parameter \u03c3i to measure the reconstruction error. SSC performs less better, and its performance remains acceptable when the noise level is low."}, {"heading": "4.2. Hopkins 155 Dataset", "text": "We evaluate our models on the Hopkins 155 motion dataset. This dataset consists of 155 sequences, each of which contains the coordinates of about 39 \u2212 550 points tracked from 2 or 3 motions. The task is to group the points into clusters according to their motions for each sequence. Since the coordinates of the points from a single motion lie in an affine subspace with the dimensionality at most 4 (Elhamifar & Vidal, 2009), we project the coordinates in each sequence into 4r dimensions with PCA, where r is the number of motions in the sequence, then append 1 as the last dimension of each sample. The results are reported in Table 1. This dataset contains a small number of latent subspaces, and the results of the compared methods have no significant difference."}, {"heading": "4.3. MSRC Dataset", "text": "In the MSRC dataset, 591 images are provided with manually labeled image segmentation results (each re-\ngion is given a label, and there are totally 23 labels). Following (Cheng et al., 2011), for each image, we group the superpixels, which are small patches in an over-segmented result, with subspace clustering methods. The groundtruth (cluster label) for a superpixel is given as the label of region it belongs to.\nIn our experiment, 100 superpixels are extracted for each image with the method described in (Mori et al., 2004), and each superpixel is represented with the RGB Color Histogram feature of dimensionality 768. We discard all the superpixels with label \u201cbackground\u201d, and then discard the images containing only one label. Finally, we get 459 images. For each image, the average number of superpixels is 91.3, and the number of clusters ranges from 2 to 6. We use PCA to reduce the dimensionality to 20 in order to keep 95% energy. The results are show in Table 2.\nClearly, our methods outperform the other three on this dataset. GCR also performs better than GCR-DP because it utilizes the information about the number of latent subspaces during the reconstruction step."}, {"heading": "4.4. Human Face Dataset", "text": "We also evaluate our method on the Extended Yale Database B (Georghiades et al., 2001). This database contains 2414 cropped frontal human face images from 38 subjects under different illuminations, and grouping these images can be treated as a subspace clustering problem, because it is shown in (Ho et al., 2003) that the images for a fixed face under different illuminations can be approximately modeled with low dimensional subspace. To evaluate the performance of all these methods, we form 7 tasks, each of which contains the images from randomly picked {3, 4, \u00b7 \u00b7 \u00b7 , 9} subjects, respectively. We resize the images to 42\u00d748, then use PCA to reduce the dimensionality of the raw features to 30. We repeat the experiment for 5 times and show the results in Figure 5.\nThe performance of GCR and GCR-DP are better than the other three methods. In particular, with the number of subspaces increasing, the difference between the l.h.s. and r.h.s. of Eq.(1) increases (see Figure 1). Consequently, the performance of LRR, SSC and SSQP, which rely on the subspace independence assumption to build the affinity matrix, degrades quickly. On the contrary, GCR and GCR-DP utilize the information that \u201cthe samples can be grouped into subspaces\u201d, thus they are less influenced by the violation of subspace independence assumption."}, {"heading": "5. Conclusion and Discussion", "text": "We propose the Groupwise Constrained Reconstruction (GCR) models for subspace clustering in this paper. Compared with other reconstruction based methods, our models no longer rely on the subspace independence assumption, which usually gets violated in the applications in which the number of subspaces keeps increasing. On the synthetic datasets, we show that existing reconstruction based methods suffer from the violation of the subspace independence assumption, while the affinity matrix produced by our model, which is built from the posterior of the latent cluster indicators, is more sophisticated and of stronger discrimination power on discovering the latent clusters. On the three real-world datasets, our methods show promising results.\nBesides the subspace clustering problem, the idea of groupwise constraints can be further applied to other problems involving graph construction. For example, in semi-supervised learning (SSL), the constraints can be modified such that a sample is only allowed to be reconstructed by its neighbors in the Euclidean space. In this way, the cluster assumption and manifold assumption, which are two fundamental SSL assumptions, can be neatly unified within our framework. For dimension reduction methods such as LLE (Roweis & Saul, 2000), it is also interesting to design new models to use the posterior of the reconstruction matrix for embedding, such that the local and global structure of the data could be preserved simultaneously."}, {"heading": "Acknowledgements", "text": "We thank the anonymous reviewers for valuable comments. This work was partially supported by 973 Program (2010CB327906), Shanghai Leading Academic Discipline Project (B114), Doctoral Fund of Ministry of Education of China (20100071120033), and Shanghai Municipal R&D Foundation (08dz1500109). Bin Li thanks UTS Early Career Researcher Grants."}], "references": [{"title": "Multi-task low-rank affinity pursuit for image segmentation", "author": ["B. Cheng", "G. Liu", "J. Wang", "Z. Huang", "S. Yan"], "venue": "In ICCV,", "citeRegEx": "Cheng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2011}, {"title": "A multibody factorization method for independently moving objects", "author": ["J.P. Costeira", "T. Kanade"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Costeira and Kanade,? \\Q1998\\E", "shortCiteRegEx": "Costeira and Kanade", "year": 1998}, {"title": "Sparse subspace clustering", "author": ["E. Elhamifar", "R. Vidal"], "venue": "In CVPR, pp", "citeRegEx": "Elhamifar and Vidal,? \\Q2009\\E", "shortCiteRegEx": "Elhamifar and Vidal", "year": 2009}, {"title": "Approaches for bayesian variable selection", "author": ["E.I. George", "R.E. Mcculloch"], "venue": "Statistica Sinica, pp", "citeRegEx": "George and Mcculloch,? \\Q1997\\E", "shortCiteRegEx": "George and Mcculloch", "year": 1997}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A.S. Georghiades", "P.N. Belhumeur", "D.J. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Georghiades et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Georghiades et al\\.", "year": 2001}, {"title": "Motion segmentation by subspace separation and model selection", "author": ["K. Kanatani"], "venue": "In ICCV, pp", "citeRegEx": "Kanatani,? \\Q2001\\E", "shortCiteRegEx": "Kanatani", "year": 2001}, {"title": "Robust subspace segmentation by low-rank representation", "author": ["G. Liu", "Z. Lin", "Y. Yu"], "venue": "In ICML, pp", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Segmentation of multivariate mixed data via lossy data coding and compression", "author": ["Y. Ma", "H. Derksen", "W. Hong", "J. Wright"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Ma et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2007}, {"title": "Information theory, inference, and learning algorithms", "author": ["D.J.C. MacKay"], "venue": null, "citeRegEx": "MacKay,? \\Q2003\\E", "shortCiteRegEx": "MacKay", "year": 2003}, {"title": "Recovering human body configurations: Combining segmentation and recognition", "author": ["G. Mori", "X. Ren", "A.A. Efros", "J. Malik"], "venue": "In CVPR", "citeRegEx": "Mori et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mori et al\\.", "year": 2004}, {"title": "Robust algebraic segmentation of mixed rigid-body and planar motions from two views", "author": ["S. Rao", "A.Y. Yang", "S. Sastry", "Y. Ma"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Rao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rao et al\\.", "year": 2010}, {"title": "The infinite gaussian mixture model", "author": ["C.E. Rasmussen"], "venue": "In NIPS, pp", "citeRegEx": "Rasmussen,? \\Q1999\\E", "shortCiteRegEx": "Rasmussen", "year": 1999}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": null, "citeRegEx": "Roweis and Saul,? \\Q2000\\E", "shortCiteRegEx": "Roweis and Saul", "year": 2000}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Shi and Malik,? \\Q2000\\E", "shortCiteRegEx": "Shi and Malik", "year": 2000}, {"title": "Mixtures of probabilistic principal component analyzers", "author": ["M.E. Tipping", "C.M. Bishop"], "venue": "Neural computation,", "citeRegEx": "Tipping and Bishop,? \\Q1999\\E", "shortCiteRegEx": "Tipping and Bishop", "year": 1999}, {"title": "Motion segmentation with missing data using powerfactorization and gpca", "author": ["R. Vidal", "R.I. Hartley"], "venue": "In CVPR", "citeRegEx": "Vidal and Hartley,? \\Q2004\\E", "shortCiteRegEx": "Vidal and Hartley", "year": 2004}, {"title": "Generalized principal component analysis (gpca)", "author": ["R. Vidal", "Ma", "Yi", "S. Sastry"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Vidal et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Vidal et al\\.", "year": 2005}, {"title": "Efficient subspace segmentation via quadratic programming", "author": ["S. Wang", "X. Yuan", "T. Yao", "S. Yan", "J. Shen"], "venue": "In AAAI,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Discriminative k-means for clustering", "author": ["J. Ye", "Z. Zhao", "M. Wu"], "venue": "In NIPS,", "citeRegEx": "Ye et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ye et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 5, "context": "In the last decade, a number of subspace clustering methods have been proposed with successful applications in the areas including motion segmentation (Kanatani, 2001; Vidal & Hartley, 2004; Elhamifar & Vidal, 2009), image clustering under different illuminations (Ho et al.", "startOffset": 151, "endOffset": 215}, {"referenceID": 6, "context": "based, algebraic based, statistically modelling, and reconstruction based, among which the reconstruction based approach has been proved most effective and has drawn much attention recently (Elhamifar & Vidal, 2009; Liu et al., 2010; Wang et al., 2011).", "startOffset": 190, "endOffset": 252}, {"referenceID": 17, "context": "based, algebraic based, statistically modelling, and reconstruction based, among which the reconstruction based approach has been proved most effective and has drawn much attention recently (Elhamifar & Vidal, 2009; Liu et al., 2010; Wang et al., 2011).", "startOffset": 190, "endOffset": 252}, {"referenceID": 5, "context": "Matrix factorization based methods Costeira & Kanade (1998); Kanatani (2001) approximate the data matrix with the product of two matrices, one containing the bases and the other containing the factors.", "startOffset": 61, "endOffset": 77}, {"referenceID": 16, "context": "The algebraic based General Principle Component Analysis (GPCA) (Vidal et al., 2005) fits the samples with a polynomial, with the gradient of a point orthogonal to the subspace containing it.", "startOffset": 64, "endOffset": 84}, {"referenceID": 10, "context": "In (Rao et al., 2010), Robust Algebraic Segmentation (RAS) is proposed to handle the data with outliers, but the complexity issue still remains.", "startOffset": 3, "endOffset": 21}, {"referenceID": 7, "context": "Agglomerative Lossy Compression (ALC) (Ma et al., 2007) searches the latent subspaces by minimizing an objective containing certain information criteria with an agglomerative strategy.", "startOffset": 38, "endOffset": 55}, {"referenceID": 6, "context": "In Low-Rank Representation (LRR) (Liu et al., 2010), nuclear norm \u2016W \u2016\u2217 is used to encourage W to have a low rank structure1, and l2,1 norm is used as the `(\u00b7) term in Eq.", "startOffset": 33, "endOffset": 51}, {"referenceID": 17, "context": "In SSQP (Wang et al., 2011), the authors choose \u03a9(W ) = \u2225\u2225W>W\u2225\u2225 1 , meanwhile force W to be non-negative.", "startOffset": 8, "endOffset": 27}, {"referenceID": 18, "context": "different from the objective of traditional clustering methods, in which the variance of inter-cluster samples are minimized, such as K-means; or the \u201cdifference\u201d of clusters are maximized, such as Discriminative Clustering (Ye et al., 2007).", "startOffset": 224, "endOffset": 241}, {"referenceID": 8, "context": "We use the Gibbs Sampling algorithm (MacKay, 2003) to approximate the posterior distribution q(z).", "startOffset": 36, "endOffset": 50}, {"referenceID": 11, "context": "This strategy is in analogy with the Infinite Gaussian Mixture Model with Dirichlet Process (Rasmussen, 1999).", "startOffset": 92, "endOffset": 109}, {"referenceID": 6, "context": "In this section, we compare our methods with the other three reconstruction based subspace clustering methods: LRR (Liu et al., 2010), SSC (Elhamifar & Vidal, 2009) and SSQP (Wang et al.", "startOffset": 115, "endOffset": 133}, {"referenceID": 17, "context": ", 2010), SSC (Elhamifar & Vidal, 2009) and SSQP (Wang et al., 2011).", "startOffset": 48, "endOffset": 67}, {"referenceID": 0, "context": "Following (Cheng et al., 2011), for each image, we group the superpixels, which are small patches in an over-segmented result, with subspace clustering methods.", "startOffset": 10, "endOffset": 30}, {"referenceID": 9, "context": "In our experiment, 100 superpixels are extracted for each image with the method described in (Mori et al., 2004), and each superpixel is represented with the RGB Color Histogram feature of dimensionality 768.", "startOffset": 93, "endOffset": 112}, {"referenceID": 4, "context": "We also evaluate our method on the Extended Yale Database B (Georghiades et al., 2001).", "startOffset": 60, "endOffset": 86}], "year": 2012, "abstractText": "Reconstruction based subspace clustering methods compute a self reconstruction matrix over the samples and use it for spectral clustering to obtain the final clustering result. Their success largely relies on the assumption that the underlying subspaces are independent, which, however, does not always hold in the applications with increasing number of subspaces. In this paper, we propose a novel reconstruction based subspace clustering model without making the subspace independence assumption. In our model, certain properties of the reconstruction matrix are explicitly characterized using the latent cluster indicators, and the affinity matrix used for spectral clustering can be directly built from the posterior of the latent cluster indicators instead of the reconstruction matrix. Experimental results on both synthetic and realworld datasets show that the proposed model can outperform the state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}