{"id": "1206.4655", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Modelling transition dynamics in MDPs with RKHS embeddings", "abstract": "We propose a new, nonparametric approach to learning and representing transition dynamics in Markov decision processes (MDPs), which can be combined easily with dynamic programming methods for policy optimisation and value estimation. This approach makes use of a recently developed representation of conditional distributions as \\emph{embeddings} in a reproducing kernel Hilbert space (RKHS). Such representations bypass the need for estimating transition probabilities or densities, and apply to any domain on which kernels can be defined. This avoids the need to calculate intractable integrals, since expectations are represented as RKHS inner products whose computation has linear complexity in the number of points used to represent the embedding. We provide guarantees for the proposed applications in MDPs: in the context of a value iteration algorithm, we prove convergence to either the optimal policy, or to the closest projection of the optimal policy in our model class (an RKHS), under reasonable assumptions. In experiments, we investigate a learning task in a typical classical control setting (the under-actuated pendulum), and on a navigation problem where only images from a sensor are observed. For policy optimisation we compare with least-squares policy iteration where a Gaussian process is used for value function estimation. For value estimation we also compare to the NPDP method. Our approach achieves better performance in all experiments.", "histories": [["v1", "Mon, 18 Jun 2012 15:25:58 GMT  (265kb)", "http://arxiv.org/abs/1206.4655v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["steffen gr\u00fcnew\u00e4lder", "guy lever", "luca baldassarre", "massimiliano pontil", "arthur gretton"], "accepted": true, "id": "1206.4655"}, "pdf": {"name": "1206.4655.pdf", "metadata": {"source": "META", "title": "Modelling transition dynamics in MDPs with RKHS embeddings", "authors": ["Steffen Gr\u00fcnew\u00e4lder", "Guy Lever", "Luca Baldassarre", "Arthur Gretton"], "emails": ["steffen@cs.ucl.ac.uk", "g.lever@cs.ucl.ac.uk", "l.baldassarre@cs.ucl.ac.uk", "m.pontil@cs.ucl.ac.uk", "arthur.gretton@gmail.com"], "sections": [{"heading": null, "text": "Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s)."}, {"heading": "1. Introduction", "text": ""}, {"heading": "1.1. Preliminaries", "text": "Throughout we denote expectations by E[\u00b7], and the probability over events by P(\u00b7). We denote by B(X ) and Cb(X ) the Banach spaces of bounded functions and bounded continuous functions on X , each equipped with the sup-norm || \u00b7 ||\u221e.\nWe consider in particular the problem in which we control a trajectory {xt} \u221e t=0 over X by sequentially choosing actions at \u2208 A at each time step t \u2265 0, once xt is revealed, after which we receive a reward rt+1 = r(xt, at). We denote a set of deterministic policies \u03a0 = AX . The objective is to find a policy \u03c0 which maximises the expected sum of rewards obtained by following \u03c0: E [ \u2211\u221e t=0 \u03b3 trt+1(Xt, At)|X0 = x,At = \u03c0(Xt)].\nFor a policy \u03c0 \u2208 \u03a0 we denote the associated value function,\nV \u03c0(x) := E\n[ \u221e\u2211\nt=0\n\u03b3trt+1(Xt, At)|X0 = x,At = \u03c0(Xt) ] ,\nand recall that V \u03c0(x) = r(x, \u03c0(x)) + \u03b3EX\u223cP (\u00b7|x,\u03c0(x))[V\n\u03c0(X)]. We define the optimal value function V \u2217(x) := max\u03c0\u2208\u03a0 V\n\u03c0(x) for all x \u2208 X , and an optimal policy to be any \u03c0\u2217 such that \u03c0\u2217 \u2208 argmax\u03c0\u2208\u03a0 V\n\u03c0(x) for all x \u2208 X . For a given action-value function Q : X \u00d7 A \u2192 R we define the greedy policy w.r.t. Q by \u03c0Q(x) := argmaxa\u2208A Q(x, a) (choosing arbitrarily in the case of a tie) and the optimal action-value function,\nQ\u2217(x, a) := r(x, a) + \u03b3EX\u223cP (\u00b7|x,a)[V \u2217(X)], (1)\nso that \u03c0\u2217 = \u03c0Q\u2217 (see e.g. (Szepesvari, 2009) for this background). We require the following well-known result, which is proved in the Appendix for reference (Gru\u0308newa\u0308lder et al., 2012):\n1Equal contribution.\nLemma 1.1. (Singh & Yee, 1994)[Corollary 2] For any action-value function Q : X \u00d7 A \u2192 R, the greedy policy \u03c0Q satisfies ||V \u03c0Q \u2212 V \u2217||\u221e \u2264 2 1\u2212\u03b3 ||Q \u2217 \u2212Q||\u221e.\nWe are interested in the case where P is unknown but a sample S := {(xi, ai, x \u2032 i)} m i=1 is provided, drawn i.i.d. from a distribution P\u0303 such that P\u0303 (X \u2032i = x \u2032 i|Xi = xi, Ai = ai) = P (x \u2032 i|xi, ai) for all i (the marginal probabilities need not match). Note the abuse of notation here \u2013 subscripts index samples and not time steps."}, {"heading": "1.2. Overview of the approach", "text": "A number of recent studies have focused on efficient evaluation of conditional expectations on functions that are \u201cwell behaved\u201d in the sense that they belong to a reproducing kernel Hilbert space (RKHS). These approaches have been particularly successful in performing inference in graphical models, where the model parameters are learned nonparametrically from data (Song et al., 2010b; 2009; 2011). The key insight in these works is that conditional probabilities can be represented as functions in an RKHS, called conditional distribution embeddings. The conditional expectation of any function in the RKHS then becomes a linear operation, where we take the inner product with the appropriate distribution embedding.\nMany methods for solving problems in MDPs require the computation of expectations of functions (value functions for example) with respect to transition dynamics, and so (approximations of) the operators\nf 7\u2192 EX\u223cP (\u00b7|x,a)[f(X)] (2)\nare required. A direct but computationally costly approach would be to first learn a conditional density estimate (difficult in high dimensions), followed by (possibly intractable) integrals to compute the expectation. By contrast, our approach is a two stage process for learning in MDPs: we first use the theory of RKHS embeddings to estimate the operators (2) directly (over a specific class of functions in an RKHS), then use these estimated operators in standard approaches for solving MDPs \u2013 here we consider dynamic programming methods for value estimation and policy optimisation. The application to dynamic programming is described in more detail in Sec. 3."}, {"heading": "1.3. Advantages of the approach", "text": "A direct kernel-based approach has a number of advantages. First, like density estimates, conditional embeddings can be learned from a training sample: we do not need to address the problem of modeling system dynamics, such as the differential equa-\ntions governing a robot arm. Unlike density estimates, however, distribution embedding estimates do not scale poorly with the dimension d of the underlying space: the risk of a kernel density estimate increases as O(m\u22124/(4+d)) when the optimal bandwidth is used (Wasserman, 2006)[Sec. 6.5]. By contrast, the rate of convergence for conditional mean embeddings is independent of the dimension of the underlying space (Song et al., 2010b)[Thm. 1].\nSecond, the solution to many control problems involves computation of high dimensional integrals to obtain expectations, which is prohibitively costly. By contrast, RKHS embeddings explicitly provide a representation of the expectation operator as an RKHS inner product, which reduces calculating expectations to a computation of linear complexity in the number of training points used to represent the embedding, and avoids any intermediate problems such as density estimation and sample selection for numerical integration. Thus, the approach provides a framework for alleviating the curse of dimensionality in MDPs (particularly if, for example, sparsification of the embedding is considered, which we address briefly in the Appendix (Gru\u0308newa\u0308lder et al., 2012)). The conditional distribution embeddings themselves may be computed exactly at cost cubic in the training sample size, and approximated to good accuracy at linear cost.\nA third advantage is that we can provide convergence results in the infinite sample case. Thm. 3.2 demonstrates how a performance guarantee for value iteration using embeddings decomposes into guarantees for value iteration and gurantees for the embeddings, upper bounding the difference ||V \u03c0\u0302\u03ba \u2212 V \u2217||\u221e between the optimal value V \u2217 and the value V \u03c0\u0302\u03ba of the policy \u03c0\u0302\u03ba found by performing value iteration using the embeddings after \u03ba iterations. This bound contains a term involving how well we can approximate V \u2217 in our model class (a chosen RKHS) \u2013 which usually corresponds to smoothness assumptions on V \u2217 \u2013 and can decrease by increasing the richness of the RKHS. A second term captures how quickly we can learn the embeddings for the operator (2) over functions in the chosen RKHS. This bound can be specialised to give convergence guarantees for specific settings by plugging in guarantees for the two components: in Corollary 3.3, we specialise to the common setting of finite state space and positive definite kernel and obtain that ||V \u03c0\u0302\u03ba \u2212 V \u2217||\u221e \u2192 0.\nAs a final advantage, the method applies wherever kernels may be defined, including on high dimensional or continuous state spaces, manifolds (kernels on the surface of a sphere (Wendland, 2005) are of particu-\nlar interest in robotics), and partially observable tasks where only sensor measurements are available."}, {"heading": "1.4. Relation to existing methods", "text": "Kernel methods have become increasingly popular in RL. Methods include kernel LSTD (Xu et al., 2005) and GPTD (Engel et al., 2005). Both can be used to estimate (q-)values and they differ in this mainly through the regulariser (Taylor & Parr, 2009). Based on the q-value estimates it is possible to optimise the policy. Other related approaches include Rasmussen & Kuss (2003), Deisenroth et al. (2009), Ormoneit & Sen (1999) and Kroemer & Peters (2011). Here, transition models (densities) are learned with the help of Gaussian processes or kernel density estimates. Using them for value estimation or policy optimisation usually leads to difficult integration to be solved numerically via, e.g., an intermediate sampling method. In contrast we use kernels to directly learn the expectation operators and avoid numerical integration. Finally, in (Parr et al., 2008) a way is proposed to approximate expectations in a low dimensional state representation. In contrast to our approach the paper assumes that the true expectation is known."}, {"heading": "2. RKHS embeddings of transition probability kernels", "text": "Given a set Z and a positive semi-definite (p.s.d.) kernel K : Z \u00d7Z \u2192 R (see e.g. Steinwart & Christmann, 2008, for details) we denote byHK \u2286 R\nZ its unique reproducing kernel Hilbert space (RHKS), and by \u3008\u00b7, \u00b7\u3009K the inner product in HK . Due to the reproducing property of K in HK we have h(x) = \u3008K(x, \u00b7), h\u3009K for all h \u2208 HK . We recall the notion of a universal kernel: given a Banach space of functions F \u2286 RZ a kernel is F-universal if HK is dense in F . We denote \u03c1K := supz\u2208Z \u221a K(z, z) and refer to kernels K such that \u03c1K < \u221e as bounded kernels.\nFollowing Sriperumbudur et al. (2010), given any probability distribution P and p.s.d. kernel K on a set Z a distribution embedding of P in HK is an element \u00b5 \u2208 HK such that \u3008\u00b5, h\u3009K = EZ\u223cP [h(Z)] for all h \u2208 HK . In our application, given p.s.d. kernels L : X\u00d7X \u2192 R and K : (X\u00d7A)\u00d7(X\u00d7A) \u2192 R, we are interested in the embedding of the expectation operator (2) corresponding to the state transition probability kernel P , over the domain HL; that is, an element \u00b5(x,a) \u2208 HL such that \u3008\u00b5(x,a), f\u3009L = E[f(Xt+1)|Xt = x,At = a], for all f \u2208 HL and for all t \u2265 0 \u2013 recall that the Markov property implies such a \u00b5(x,a) is independent of time. Recalling Sec. 1.1, given the sample S, we will consider a sample-based estimate of\nthe expectation operator (2). This will be achieved by identifying an element \u00b5(x,a) \u2208 HL such that, for all f \u2208 HL, \u3008\u00b5(x,a), f\u3009L approximates EX\u223cP (\u00b7|x,a)[f(X)]. Following (Song et al., 2009; 2010b) an estimate is\n\u00b5(x,a) :=\nm\u2211\ni=1\n\u03b1i(x, a)L(x \u2032 i, \u00b7) \u2208 HL, (3)\nwhere \u03b1i(x, a) = \u2211m j=1 WijK((xj , aj), (x, a)), and where W := (K + \u03bbmI)\u22121, K = (K((xi, ai), (xj , aj))) m ij=1, and \u03bb is a regularization parameter. We assume w.l.o.g. x\u2032i 6= x \u2032 j for all x\u2032i, x \u2032 j in the expansion (3).\n1 In some situations, the estimate (3) is consistent in the RHKS norm sense and uniformly over X \u00d7A: the following result, proved in the appendix, follows directly from (Song et al., 2010b)[Thm. 1].\nLemma 2.1. Suppose K is a bounded kernel and the conditions of (Song et al., 2010b)[Thm. 1] are satisfied.2 Then sup(x,a)\u2208X\u00d7A{||\u00b5(x,a) \u2212 \u00b5(x,a)||L} \u2208 OP\u0303 (\u03bb 1 2 + \u03bb\u2212 3 2m\u2212 1 2 ), and thus by choosing \u03bb \u2192 0, \u03bb3m \u2192 \u221e we have that, for any \u01eb > 0,\nPS\u223cP\u0303m\n( sup\n(x,a)\u2208X\u00d7A\n||\u00b5(x,a) \u2212 \u00b5(x,a)||L > \u01eb\n) \u2192 0.\nBy the reproducing property of L, we have\n\u3008\u00b5(x,a), f\u3009L =\nm\u2211\ni=1\n\u03b1i(x, a)f(x \u2032 i)\nIn this work, for theoretical analysis, we consider a normalised version of (3):\n\u00b5\u0302(x,a) :=\nm\u2211\ni=1\n\u03b1\u0302i(x, a)L(x \u2032 i, \u00b7) \u2208 HL, (4)\nwhere \u03b1\u0302i(x, a) = \u03b1i(x,a)\u2211\nm j=1 |\u03b1j(x,a)|\n. This is a technical con-\nsideration which will later ensure that we can define a\n1We can otherwise form a new expansion in which the x\u2032i are unique by summing any \u03b1i(x, a) as necessary.\n2These conditions require that the mapping (x, a) 7\u2192 EX\u223cP (\u00b7|(x,a))[f(X)] be an element of HK for all f \u2208 HL, and that the operator CY XC \u22123/2 XX be HilbertSchmidt, where CY X and CXX are covariance operators: see (Song et al., 2009) or Appendix D.1 for details (Gru\u0308newa\u0308lder et al., 2012). The first condition is a smoothness assumption on the distribution, and for the convergence guarantee of Corollary 3.3 we specialise to the simple setting of finite state space, in which case this condition is trivially satisfied.The second condition is guaranteed in our case when, for example, the marginal density of the initial state X from P\u0303 is bounded away from zero and the RKHSs HK , HL are of finite dimensionality.\nAlgorithm 1 Estimate Conditional Expectation input Sample of transitions S := {(x1, a1, x \u2032 1), . . . , (xm, am, x \u2032 m)}, kernel K on X \u00d7A and a kernel L on states X output A conditional expectation estimate \u00b5(x,a) Build kernel matrix K for samples {(x1, a1), . . . , (xm, am)} Calculate coefficient vector \u03b1i := \u2211 j\u2264m WijK((xj , aj), (x, a)), where W := (K + \u03bbmI) \u22121\nCalculate the estimate \u00b5(x,a) := \u2211m i=1 \u03b1i(x, a)L(x \u2032 i, \u00b7)\nAlgorithm 2 Estimate Value\ninput Sample S, policy \u03c0, conditional expectation estimate \u00b5(x,a), discount \u03b3, max. number of iterations N , error threshold \u03b8, reward function r output Value estimate V\u0302 n = 1, error = 1; define a value vector for states x\u20321, ..., x \u2032 m:\nV := 0 while n \u2264 N and error > \u03b8 do\nfor all i \u2264 m do V \u2032(x\u2032i) \u2190 r(x \u2032 i, \u03c0(x \u2032 i)) + \u03b3\u3008\u00b5(x\u2032\ni ,\u03c0(x\u2032 i )), V \u3009L\nend for n \u2190 n+ 1, error \u2190 ||V \u2032 \u2212 V ||\u221e, V \u2190 V \u2032\nend while return V\u0302 (x) = r(x, \u03c0(x)) + \u03b3\u3008\u00b5(x,\u03c0(x)), V \u3009L\nAlgorithm 3 Approximate Value Iteration\ninput Sample S, discount \u03b3, maximum number of iterations N , reward function r, error threshold \u03b8 output \u00b5(x,a), approximate optimal value V\u0302\nn = 1, error = 1; define a value vector for states x\u20321, ..., x \u2032 m: V := 0 Run Alg. 1 to get \u00b5(x,a) while n \u2264 N and error > \u03b8 do\nfor all i \u2264 m do V \u2032(x\u2032i) \u2190 maxa\u2208A r(x \u2032 i, a) + \u03b3\u3008\u00b5(x\u2032\ni ,a), V \u3009L\nend for n \u2190 n+ 1, error \u2190 ||V \u2032 \u2212 V ||\u221e, V \u2190 V \u2032\nend while return V\u0302 (x) = maxa\u2208A r(x, a) + \u03b3\u3008\u00b5(x,a), V \u3009L\ncertain contraction mapping. We now demonstrate the consistency of the estimators defined by (4) for finite state spaces, by showing that in the limit of large data the normalization of \u03b1\u0302 has no effect. The following lemma is proved in the Appendix (Gru\u0308newa\u0308lder et al., 2012).\nLemma 2.2. Under the conditions of Lemma 2.1, and if |X | < \u221e and L is strictly positive definite, by choosing \u03bb \u2192 0, \u03bb3m \u2192 \u221e we have that, for any \u01eb > 0,\nPS\u223cP\u0303m\n( sup\n(x,a)\u2208X\u00d7A\n||\u00b5(x,a) \u2212 \u00b5\u0302(x,a)||L > \u01eb\n) \u2192 0."}, {"heading": "3. Application to MDPs", "text": "The learnt embeddings are applied to MDPs by recalling (4) and defining an operator\nE\u0302(x,a)[f ] := m\u2211\ni=1\n\u03b1\u0302i(x, a)f(x \u2032 i). (5)\nWhen f \u2208 HL we have that E\u0302(x,a)[f ] = \u3008\u00b5\u0302(x,a), f\u3009L \u2248 EX\u223cP (\u00b7|(x,a))[f(x)]. When f /\u2208 HL the quality of the approximation will further depend upon how well f can be approximated by a low norm function in HL. This operator can be used in place of the true unknown expectation operator (2) in any MDP method which makes use of such expectations, such as dynamic programming. As an example below, we analyse value iteration, but similar considerations yield similar analyses for other methods. We summarize a joint value estimation algorithm and policy optimisation approach in the Algorithm boxes above.\nIf we knew P , and could efficiently compute expectations, we could define the Bellman operator B as\n(BV )(x) := max a\u2208A {r(x, a) + \u03b3EX\u223cP (\u00b7|x,a)[V (X)]}, (6)\nwhere we suppose that the image of B is always a measurable function.3 Recall that picking an arbitrary V0 and iterating Vk+1 = BVk converges in sup-norm, Vk \u2192 V\n\u2217 (see e.g. Szepesvari, 2009). Since we do not know P , we use the embeddings \u00b5\u0302(x,a) and, recalling\n(5), define the operator B\u0302 : B(X ) \u2192 B(X ) as\n(B\u0302V )(x) := max a\u2208A {r(x, a) + \u03b3E\u0302(x,a)[V ]}. (7)\nIt is necessary to define B\u0302 on functions which are not in HL, and this possibility introduces a term in the analysis which captures how well V \u2217 can be approximated in HL (See Thm. 3.2). By Lemma 2.2, in the limit of large data, the operator defined by (7) converges to an expectation operator on functions in HL, and thus B\u0302 can be seen to approximate B defined by (6) on HL. The following result is proved in the Appendix (Gru\u0308newa\u0308lder et al., 2012):\nProposition 3.1. B\u0302 is a sup-norm contraction on the space B(X ) with Lipschitz constant \u03b3.\nSince B\u0302 defines a sup-norm contraction mapping on a complete metric space, by Banach\u2019s fixed point theorem (e.g. Granas & Dugundji, 2003) there exists a\n3We suppose for simplicity that any necessary conditions to ensure this are met, since strictly speaking B is defined only on measurable functions, see for example (Bertsekas & Shreve, 1978) for a discussion of the issues. In particular, these conditions are met when |X | < \u221e.\nunique fixed point V\u0302 \u2217 of B\u0302, such that choosing V\u03020 arbitrarily and iterating V\u0302k+1 = B\u0302V\u0302k converges, V\u0302k \u2192 V\u0302\n\u2217, in sup-norm,\n||V\u0302k \u2212 V\u0302 \u2217||\u221e \u2264\n\u03b3k\n1\u2212 \u03b3 ||V\u03021 \u2212 V\u03020||\u221e. (8)\nSuppose we perform \u03ba iterations, obtaining the estimate V\u0302\u03ba \u2248 V\u0302\n\u2217. Once V\u0302\u03ba is obtained we form a policy \u03c0\u0302\u03ba on-the-fly 4 by acting greedily w.r.t. Q\u0302\u03ba(x, a), where\nQ\u0302\u03ba(x, a) := r(x, a) + \u03b3E\u0302(x,a)[V\u0302\u03ba], (9)\nso that the learned policy is\n\u03c0\u0302\u03ba(x) := argmax a\u2208A Q\u0302\u03ba(x, a). (10)\nfor each x in a trajectory.\nConsistency: We now discuss the consistency of \u03c0\u0302\u03ba as an estimate of an optimal policy \u03c0\u2217. The following theorem decomposes the convergence of V \u03c0\u0302\u03ba to the optimal value function V \u2217, in terms of the convergence of value iteration, the convergence of the embeddings and how well we can approximate V \u2217 in sup-norm by a (low || \u00b7 ||L-norm) function in HL. This is a generic bound into which we can plug any suitable guarntees for an embedding method. In Corollary 3.3, we specialise the result to the finite state space case, where we can approximate V \u2217 arbitrarily well.\nTheorem 3.2.\n||V \u03c0\u0302\u03ba \u2212 V \u2217||\u221e \u2264 2\u03b3\n(1\u2212 \u03b3)2\n( \u03b3\u03ba||V\u03021 \u2212 V\u03020||\u221e\n+ 2||V \u2217\u2212V\u0303 \u2217||\u221e+ sup (x,a) ||\u00b5(x,a) \u2212 \u00b5\u0302(x,a)||L||V\u0303 \u2217||L\n) , (11)\nwhere V\u0303 \u2217 is any element of HL. Thus, whenever sup(x,a) ||\u00b5(x,a) \u2212 \u00b5\u0302(x,a)||L \u2192 0 in P\u0303 -probability, we have that, for any chosen V\u0303 \u2217 \u2208 HL,\n||V \u03c0\u0302\u03ba \u2212 V \u2217||\u221e \u2264 4\u03b3\n(1\u2212 \u03b3)2 ||V \u2217 \u2212 V\u0303 \u2217||\u221e + \u01eb\u03ba + \u01ebm,\n(12)\nwhere \u01eb\u03ba \u2192 0 and \u01ebm \u2192 0 with convergence in P\u0303 - probability.\nProof. (Sketch, see Appendix for full proof (Gru\u0308newa\u0308lder et al., 2012).) The proof hinges upon obtaining the following chain of convergences,\nE\u0302(x,a)[V\u0302\u03ba] \u2192(a) E\u0302(x,a)[V\u0302 \u2217] \u2248(b) E\u0302(x,a)[V \u2217]\n\u2248 E\u0302(x,a)[V\u0303 \u2217] \u2192(c) EX\u223cP (\u00b7|(x,a))[V\u0303 \u2217(X)] \u2248 EX\u223cP (\u00b7|(x,a))[V \u2217(X)].\n4Meaning that we only need to calculate \u03c0\u0302\u03ba(x) at points x in a trajectory as and when required.\nThe convergence (a) is a standard result for contraction mappings, (b) requires a new lemma relating the fixed points of similar contraction mappings, and (c) is possible using Lemma 2.2 because V\u0303 \u2217 \u2208 HL. Once this is obtained we recall that \u03c0\u0302\u03ba is greedy w.r.t. Q\u0302\u03ba defined by (9), and apply Lemma 1.1, since the optimal policy is greedy w.r.t. Q\u2217.\nWe now interpret Thm. 3.2. The upper bound is,\n||V \u03c0\u0302\u03ba \u2212 V \u2217||\u221e \u2264 2\u03b3\n(1\u2212 \u03b3)2\n( (i)\ufe37 \ufe38\ufe38 \ufe37\n\u03b3\u03ba||V\u03021 \u2212 V\u03020||\u221e\n+ 2||V \u2217 \u2212 V\u0303 \u2217||\u221e\ufe38 \ufe37\ufe37 \ufe38 (ii) + sup (x,a) ||\u00b5(x,a) \u2212 \u00b5\u0302(x,a)||L||V\u0303 \u2217||L\n\ufe38 \ufe37\ufe37 \ufe38 (iii)\n) .\nHere (i) is the standard difference between the value estimate of the initial policy and the value estimate of the policy that we get after applying one dynamic programming update. This term decreases to 0 with growing \u03ba because \u03b3 < 1. (ii) is the distance from the optimal value V \u2217 to any approximation V\u0303 \u2217 in the RKHS, and is therefore small when V\u0303 \u2217 is close to V \u2217 and so can be smaller when HL is chosen to be a richer class. Finally, (iii) measures the quality of the learned embedding: ||\u00b5(x,a) \u2212 \u00b5\u0302(x,a)||L is the distance between the empirical estimate \u00b5\u0302 of the conditional distribution embedding of x\u2032 given (x, a), and the population conditional embedding \u00b5, measured in the RKHS with kernel L. This difference is weighted by ||V\u0303 \u2217||L, the RKHS norm of the approximation V\u0303 \u2217. Intuitively, a lower RKHS norm implies a smoother function: when the norm is smaller, V\u0303 \u2217 is smoother, and the convergence faster. Thus (iii) requires us to obtain a better conditional mean embedding (via more training samples) when the value function is non-smooth. In other words, our approach favors smooth value functions, although given sufficient evidence, non-smooth functions can also be learned. One specialization is to the case when V \u2217 \u2208 Cb(X ) and L is a Cb(X )-universal kernel (Steinwart & Christmann, 2008, Section 4.6). In this case we can choose V\u0303 \u2217 such that ||V \u2217 \u2212 V\u0303 \u2217||\u221e is arbitrarily small in (11).\nWe now specialise Thm. 3.2 to the case where |X | < \u221e and where L is strictly positive definite kernel on X (we then know from Lemma 2.2 that sup(x,a) ||\u00b5(x,a)\u2212 \u00b5\u0302(x,a)||L \u2192 0 and that all real-valued functions are in the associated RKHS). Thus consistency is attained in otherwise very general conditions \u2013 the following is proved in the appendix:\nCorollary 3.3. Let |X | < \u221e and L be strictly positive definite. Under the conditions of Lemma 2.2 we\nModelling transition dynamics in MDPs with RKHS embeddings\n10 00 S am pl es True Value\n10 20 30 40 50\n10\n20\n30\n40\n50\nEstimated Value\n10 20 30 40 50\n10\n20\n30\n40\n50\nPolicy\n10 20 30 40 50\n10\n20\n30\n40\n50\n50 00\nS am\npl es\n10 20 30 40 50\n10\n20\n30\n40\n50\n10 20 30 40 50\n10\n20\n30\n40\n50 10 20 30 40 50\n10\n20\n30\n40\n50\n0 1 2 3 41 2 3 4 5\n2 4 6 8 2 4 6 8\nFigure 1. The left column shows the (true) value of the learned policy (color coded). The middle column shows the estimated value and the right column shows the policy. Actions are shown in the plot via a color code: yellow: go down; brown: left; dark blue: up; light blue: right. The 5000 sample policy is better (see for example the scale on the value color bars) and estimated value is close to true value. The patchy coloring is not a problem as, for example, in the bottom right it does not matter if the agent first goes up or to the left. The method has essentially learnt the task.\nhave that ||V \u03c0\u0302\u03ba \u2212 V \u2217||\u221e \u2192 0 with convergence in P\u0303 - probability.\nComplexity analysis: Once the embeddings are learnt, the complexity of learning the approximate value function Q\u0302\u03ba is O(m\n2|A|\u03ba): due to the expansion of \u00b5\u0302(x,a) in the m points in S, computing each expectation is O(m) and we only ever need to know the evalu-\nation of each iterate V\u0302k at the m points in S. Applying the learnt policy (10) to a trajectory (x0, x1, . . . , xT ) of length T , is similarly O(m|A|T ). In Sec. B of the Supplementary material, we propose a sparser representation of the embedding, using an incomplete Cholesky approximation (Shawe-Taylor & Cristianini, 2004)[Sec. 5.2]. This reduces the cost of learning the embeddings from cubic to linear in m, and allows us to compute subsequent expectations in O(\u2113), where generally \u2113 \u226a m."}, {"heading": "4. Experiments", "text": "We performed three experiments, using the embeddings in value estimation and policy optimization. The first experiment was an MDP with a fully observed discrete state space, to demonstrate convergence of the value function with increasing training sample size. The second and third experiments evaluate our approach on a classical control task and a task with high dimensional states. In policy optimisation we compare to LSPI (Lagoudakis & Parr, 2003) where we use the q-value estimator from (Engel et al., 2005), and for value estimation we compare to NPDP5 (Kroemer & Peters, 2011). We achieve better performance in all our experiments.\nWe briefly address the choice of the regularization term \u03bb. It can be shown that the conditional embeddings\n5We thank the authors for providing code.\nsolve,\n\u00b5\u0302 := argmin \u00b5\u2208H\n[ m\u2211\ni=1\n\u2016L(x\u2032i, \u00b7)\u2212 \u00b5(xi, ai)\u2016 2 L + \u03bb \u2016\u00b5\u2016 2 H\n] .\nwhere H \u2286 (HL) (X\u00d7A), recovering the vector-valued regression setting of Micchelli & Pontil (2005) (see Sec. D for details) which provides cross validation scheme for the parameter \u03bb."}, {"heading": "4.1. Experiment 1", "text": "The first experiment is a navigation experiment in a 50 x 50 room. The reward is a Gaussian centered in the middle of the room. The agent has four actions: go north, east, south or west. Each action has a success rate of 80 % and results in random movement with 20 % chance. The state space is fully observed. We learn the conditional distribution embedding from either 1000 or 5000 uniformly sampled transitions, uniformity ensuring we avoid exploration artifacts. We used a Gaussian kernel and cross-validated to determine the regulariser. Results are shown in Figure 1."}, {"heading": "4.2. Experiment 2", "text": "We consider the under-actuated pendulum swing up task (Deisenroth et al., 2009). We generate a discretetime approximation of the continuous-time pendulum dynamics as done in (Deisenroth et al., 2009). Starting from an arbitrary state the goal is to swing the pendulum up and balance it in the inverted position. The applied torque is u \u2208 [\u22125, 5]Nm and is not sufficient for a direct swing up. The state space is defined by the angle \u03b8 \u2208 [\u2212\u03c0, \u03c0] and the angular velocity, \u03c9 \u2208 [\u22127, 7]. The reward is given by the function r(\u03b8, \u03c9) = exp(\u2212\u03b82 \u2212 0.2\u03c92). For policy learning we compared to the GP-based LSPI approach and for value learning to NPDP. The results of the comparison are shown in Fig. 2.\nDetails for the policy learning setting: We sampled uniformly from the state and action space and used a Gaussian kernel on both, selecting as kernel width the average K-neighbour distance, where K is one quarter of the sample size. We considered a discretization of the action space into 25 actions and we measured the difference between the value function evaluated on a grid of 25 \u00d7 25 points to the optimal value obtained by dynamic programming using the deterministic system dynamics. We compared over different sample sizes and averaged the performance over 10 repetitions.\nDetails for the value estimation setting: We used the optimal policy to generate samples. The goal was to predict the value of the optimal policy. The performance of NPDP depends strongly on the bandwidth parameter of the used kernel (a Gaussian). For parameter selection, we optimised performance on a validation set over a grid all free parameters (bandwidth for NPDP, bandwidth and \u03bb for the embedding), and report the error on an independent test set. The relatively poor scaling of NPDP with increased sample size is due to the numerical integration step in (Kroemer & Peters, 2011, Algorithm 1)."}, {"heading": "4.3. Experiment 3", "text": "Our final experiment is a high dimensional task where sensor measurements are available, and no state description is present. The environment consists of two rooms connected via a short corridor (Bo\u0308hmer, 2012). The sensor measurements are images from a 3D renderer, and we aggregate four orientations (north, east, south and west) for a panorama, since the camera im-\nages are ambiguous, especially close to the walls. The task of the agent is to reach a goal located in one of the rooms, using only the images to orient itself. Training points were chosen uniformly over the input space. We used a Gaussian kernel and cross-validated the regularization parameter. Results for 4000 training points are shown in Figure 3. We compared to the GP based LSPI approach using the same kernel and settings for both approaches; results are shown in Figure 2. Our method improves with increasing sample\nnumbers. The GP based LSPI approach has obvious difficulties with this task and does not improve. We did not apply NPDP to exp. 3, as it would be computationally intractable in given the high dimensionality."}, {"heading": "5. Conclusions and Outlook", "text": "We have proposed a novel application of RKHS embeddings to learning expectation operators associated to transition dynamics in MDPs, with particular focus on their use in dynamic programming methods. The approach avoids the need for density estimates, sampling methods for evaluation of integrals, or explicit models of the system; is computationally efficient, having cost linear in the number of samples used in training (or even sublinear, with appropriate approximations); and has performance guarantees. Future work will focus on generalizing to more complex state and action spaces, and extending the convergence results to continuous state spaces. Another important generalization concerns the sampling distribution, which here is assumed to be iid, but one can expect similar results to hold in the non-iid case."}, {"heading": "Acknowledgements", "text": "The authors want to thank for the support of the EPSRC #EP/H017402/1 (CARDyAL) and the European Union #FP7-ICT-270327 (Complacs)."}], "references": [{"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bach and Jordan,? \\Q2002\\E", "shortCiteRegEx": "Bach and Jordan", "year": 2002}, {"title": "Mathematical issues in dynamic programming", "author": ["D. Bertsekas", "S. Shreve"], "venue": "Academic press,", "citeRegEx": "Bertsekas and Shreve,? \\Q1978\\E", "shortCiteRegEx": "Bertsekas and Shreve", "year": 1978}, {"title": "Robot Navigation using Reinforcement Learning and Slow Feature Analysis", "author": ["W. B\u00f6hmer"], "venue": "ArXiv,", "citeRegEx": "B\u00f6hmer,? \\Q2012\\E", "shortCiteRegEx": "B\u00f6hmer", "year": 2012}, {"title": "Reinforcement learning with gaussian processes", "author": ["Y. Engel", "S. Mannor", "R. Meir"], "venue": "In ICML,", "citeRegEx": "Engel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Engel et al\\.", "year": 2005}, {"title": "Modelling transition dynamics in mdps with rkhs embeddings", "author": ["S. Gr\u00fcnew\u00e4lder", "G. Lever", "L. Baldassarre", "M. Pontil", "A. Gretton"], "venue": "In arXiv,", "citeRegEx": "Gr\u00fcnew\u00e4lder et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gr\u00fcnew\u00e4lder et al\\.", "year": 2012}, {"title": "A non-parametric approach to dynamic programming", "author": ["O. Kroemer", "J. Peters"], "venue": "In NIPS,", "citeRegEx": "Kroemer and Peters,? \\Q2011\\E", "shortCiteRegEx": "Kroemer and Peters", "year": 2011}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "On learning vector-valued functions", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Neural Computation,", "citeRegEx": "Micchelli and Pontil,? \\Q2005\\E", "shortCiteRegEx": "Micchelli and Pontil", "year": 2005}, {"title": "Kernel-based reinforcement learning", "author": ["D. Ormoneit", "S. Sen"], "venue": "In Machine Learning,", "citeRegEx": "Ormoneit and Sen,? \\Q1999\\E", "shortCiteRegEx": "Ormoneit and Sen", "year": 1999}, {"title": "An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning", "author": ["R. Parr", "L. Li", "G. Taylor", "C. Painter-Wakefield", "M.L. Littman"], "venue": "In ICML,", "citeRegEx": "Parr et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Parr et al\\.", "year": 2008}, {"title": "Gaussian processes in reinforcement learning", "author": ["C.E. Rasmussen", "M. Kuss"], "venue": "In NIPS. MIT Press,", "citeRegEx": "Rasmussen and Kuss,? \\Q2003\\E", "shortCiteRegEx": "Rasmussen and Kuss", "year": 2003}, {"title": "Methods of modern mathematical physics. Vol. 1: Functional Analysis", "author": ["M. Reed", "B. Simon"], "venue": null, "citeRegEx": "Reed and Simon,? \\Q1980\\E", "shortCiteRegEx": "Reed and Simon", "year": 1980}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Cristianini,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Cristianini", "year": 2004}, {"title": "An upper bound on the loss from approximate optimal-value functions", "author": ["S.P. Singh", "R.C. Yee"], "venue": "Machine Learning,", "citeRegEx": "Singh and Yee,? \\Q1994\\E", "shortCiteRegEx": "Singh and Yee", "year": 1994}, {"title": "Kernels and regularization on graphs", "author": ["A.J. Smola", "R.I. Kondor"], "venue": "In COLT,", "citeRegEx": "Smola and Kondor,? \\Q2003\\E", "shortCiteRegEx": "Smola and Kondor", "year": 2003}, {"title": "Hilbert space embeddings of conditional distributions with applications to dynamical systems", "author": ["L. Song", "J. Huang", "A.J. Smola", "K. Fukumizu"], "venue": "In ICML,", "citeRegEx": "Song et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Song et al\\.", "year": 2009}, {"title": "Hilbert space embeddings of hidden Markov models", "author": ["L. Song", "B. Boots", "S. Siddiqi", "G. Gordon", "A. Smola"], "venue": "In ICML,", "citeRegEx": "Song et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Song et al\\.", "year": 2010}, {"title": "Nonparametric tree graphical models", "author": ["L. Song", "A. Gretton", "C. Guestrin"], "venue": null, "citeRegEx": "Song et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Song et al\\.", "year": 2010}, {"title": "Kernel belief propagation", "author": ["L. Song", "A. Gretton", "D. Bickson", "Y. Low", "C. Guestrin"], "venue": "In AISTATS,", "citeRegEx": "Song et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Song et al\\.", "year": 2011}, {"title": "Hilbert space embeddings and metrics on probability measures", "author": ["B. Sriperumbudur", "A. Gretton", "K. Fukumizu", "G. Lanckriet", "B. Sch\u00f6lkopf"], "venue": null, "citeRegEx": "Sriperumbudur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2010}, {"title": "Algorithms for reinforcement learning", "author": ["C. Szepesvari"], "venue": null, "citeRegEx": "Szepesvari,? \\Q2009\\E", "shortCiteRegEx": "Szepesvari", "year": 2009}, {"title": "Kernelized value function approximation for reinforcement learning", "author": ["G. Taylor", "R. Parr"], "venue": "In ICML,", "citeRegEx": "Taylor and Parr,? \\Q2009\\E", "shortCiteRegEx": "Taylor and Parr", "year": 2009}, {"title": "Scattered Data Approximation", "author": ["H. Wendland"], "venue": null, "citeRegEx": "Wendland,? \\Q2005\\E", "shortCiteRegEx": "Wendland", "year": 2005}, {"title": "Kernel least-squares temporal difference learning", "author": ["X. Xu", "T. Xie", "D. Hu", "X. Lu"], "venue": "International Journal of Information Technology,", "citeRegEx": "Xu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 20, "context": "(Szepesvari, 2009) for this background).", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "We require the following well-known result, which is proved in the Appendix for reference (Gr\u00fcnew\u00e4lder et al., 2012):", "startOffset": 90, "endOffset": 116}, {"referenceID": 4, "context": "Thus, the approach provides a framework for alleviating the curse of dimensionality in MDPs (particularly if, for example, sparsification of the embedding is considered, which we address briefly in the Appendix (Gr\u00fcnew\u00e4lder et al., 2012)).", "startOffset": 211, "endOffset": 237}, {"referenceID": 22, "context": "As a final advantage, the method applies wherever kernels may be defined, including on high dimensional or continuous state spaces, manifolds (kernels on the surface of a sphere (Wendland, 2005) are of particu-", "startOffset": 178, "endOffset": 194}, {"referenceID": 23, "context": "Methods include kernel LSTD (Xu et al., 2005) and GPTD (Engel et al.", "startOffset": 28, "endOffset": 45}, {"referenceID": 3, "context": ", 2005) and GPTD (Engel et al., 2005).", "startOffset": 17, "endOffset": 37}, {"referenceID": 9, "context": "Finally, in (Parr et al., 2008) a way is proposed to approximate expectations in a low dimensional state representation.", "startOffset": 12, "endOffset": 31}, {"referenceID": 3, "context": ", 2005) and GPTD (Engel et al., 2005). Both can be used to estimate (q-)values and they differ in this mainly through the regulariser (Taylor & Parr, 2009). Based on the q-value estimates it is possible to optimise the policy. Other related approaches include Rasmussen & Kuss (2003), Deisenroth et al.", "startOffset": 18, "endOffset": 284}, {"referenceID": 3, "context": ", 2005) and GPTD (Engel et al., 2005). Both can be used to estimate (q-)values and they differ in this mainly through the regulariser (Taylor & Parr, 2009). Based on the q-value estimates it is possible to optimise the policy. Other related approaches include Rasmussen & Kuss (2003), Deisenroth et al. (2009), Ormoneit & Sen (1999) and Kroemer & Peters (2011).", "startOffset": 18, "endOffset": 310}, {"referenceID": 3, "context": ", 2005) and GPTD (Engel et al., 2005). Both can be used to estimate (q-)values and they differ in this mainly through the regulariser (Taylor & Parr, 2009). Based on the q-value estimates it is possible to optimise the policy. Other related approaches include Rasmussen & Kuss (2003), Deisenroth et al. (2009), Ormoneit & Sen (1999) and Kroemer & Peters (2011).", "startOffset": 18, "endOffset": 333}, {"referenceID": 3, "context": ", 2005) and GPTD (Engel et al., 2005). Both can be used to estimate (q-)values and they differ in this mainly through the regulariser (Taylor & Parr, 2009). Based on the q-value estimates it is possible to optimise the policy. Other related approaches include Rasmussen & Kuss (2003), Deisenroth et al. (2009), Ormoneit & Sen (1999) and Kroemer & Peters (2011). Here, transition models (densities) are learned with the help of Gaussian processes or kernel density estimates.", "startOffset": 18, "endOffset": 361}, {"referenceID": 15, "context": "Following (Song et al., 2009; 2010b) an estimate is", "startOffset": 10, "endOffset": 36}, {"referenceID": 15, "context": "Following Sriperumbudur et al. (2010), given any probability distribution P and p.", "startOffset": 10, "endOffset": 38}, {"referenceID": 15, "context": "These conditions require that the mapping (x, a) 7\u2192 EX\u223cP (\u00b7|(x,a))[f(X)] be an element of HK for all f \u2208 HL, and that the operator CY XC \u22123/2 XX be HilbertSchmidt, where CY X and CXX are covariance operators: see (Song et al., 2009) or Appendix D.", "startOffset": 213, "endOffset": 232}, {"referenceID": 4, "context": "1 for details (Gr\u00fcnew\u00e4lder et al., 2012).", "startOffset": 14, "endOffset": 40}, {"referenceID": 4, "context": "The following lemma is proved in the Appendix (Gr\u00fcnew\u00e4lder et al., 2012).", "startOffset": 46, "endOffset": 72}, {"referenceID": 4, "context": "The following result is proved in the Appendix (Gr\u00fcnew\u00e4lder et al., 2012): Proposition 3.", "startOffset": 47, "endOffset": 73}, {"referenceID": 4, "context": "(Sketch, see Appendix for full proof (Gr\u00fcnew\u00e4lder et al., 2012).", "startOffset": 37, "endOffset": 63}, {"referenceID": 3, "context": "In policy optimisation we compare to LSPI (Lagoudakis & Parr, 2003) where we use the q-value estimator from (Engel et al., 2005), and for value estimation we compare to NPDP (Kroemer & Peters, 2011).", "startOffset": 108, "endOffset": 128}, {"referenceID": 2, "context": "The environment consists of two rooms connected via a short corridor (B\u00f6hmer, 2012).", "startOffset": 69, "endOffset": 83}], "year": 2012, "abstractText": "We propose a new, nonparametric approach to learning and representing transition dynamics in Markov decision processes (MDPs), which can be combined easily with dynamic programming methods for policy optimisation and value estimation. This approach makes use of a recently developed representation of conditional distributions as embeddings in a reproducing kernel Hilbert space (RKHS). Such representations bypass the need for estimating transition probabilities or densities, and apply to any domain on which kernels can be defined. This avoids the need to calculate intractable integrals, since expectations are represented as RKHS inner products whose computation has linear complexity in the number of points used to represent the embedding. We provide guarantees for the proposed applications in MDPs: in the context of a value iteration algorithm, we prove convergence to either the optimal policy, or to the closest projection of the optimal policy in our model class (an RKHS), under reasonable assumptions. In experiments, we investigate a learning task in a typical classical control setting (the under-actuated pendulum), and on a navigation problem where only images from a sensor are observed. For policy optimisation we compare with leastsquares policy iteration where a Gaussian process is used for value function estimation. For value estimation we also compare to the NPDP method. Our approach achieves better performance in all experiments. Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).", "creator": "PDFSplit! (http://www.splitpdf.com)"}}}