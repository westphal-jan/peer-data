{"id": "1206.6383", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Feature Selection via Probabilistic Outputs", "abstract": "This paper investigates two feature-scoring criteria that make use of estimated class probabilities: one method proposed by \\citet{shen} and a complementary approach proposed below. We develop a theoretical framework to analyze each criterion and show that both estimate the spread (across all values of a given feature) of the probability that an example belongs to the positive class. Based on our analysis, we predict when each scoring technique will be advantageous over the other and give empirical results validating our predictions.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (967kb)", "http://arxiv.org/abs/1206.6383v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["andrea pohoreckyj danyluk", "nicholas arnosti"], "accepted": true, "id": "1206.6383"}, "pdf": {"name": "1206.6383.pdf", "metadata": {"source": "META", "title": "Feature Selection via Probabilistic Outputs", "authors": ["Nicholas A. Arnosti", "Andrea Pohoreckyj Danyluk"], "emails": ["narnosti@stanford.edu", "andrea@cs.williams.edu"], "sections": [{"heading": null, "text": "1. Introduction\nData sets used to perform classification often contain redundant and/or irrelevant information. Eliminating unhelpful features can reduce the computational complexity of many learning algorithms, increase the interpretability of the models they produce, and decrease the risk of over-fitting. For these reasons, a great deal of work has been dedicated to the task of feature selection (Guyon & Elisseeff, 2003).\nThis paper explores probabilistic feature selection techniques. We present a feature-scoring criterion based on the work of Shen et al. (2008) and develop a novel theoretical framework to analyze their score and ours. We show that their score approximates an upperbound for the improvement in accuracy that each feature offers to the Bayes-optimal classifier. We demonstrate that both their scoring method and ours estimate the spread (across all values of a given feature) of the probability that an example x belongs to the\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\npositive class. Finally, we begin to characterize when each scoring technique proves advantageous over the other.\nIn the next section we introduce notation, and follow with a discussion of feature selection methods. In Section 4 we present the scoring criteria and our theoretical analysis. Section 5 outlines our predictions for the relative performance of the scores and gives preliminary empirical results. We close by discussing directions for future work.\n2. Notation\nHere we introduce notation that will be used in the remainder of this paper. We say that a training set consists of examples of the form (xi, yi). Each xi is a vector of feature values taken from the space X , and yi is a class label taken from the set Y. The goal is to find a function g : X \u2192 Y such that for unseen examples (x, y) \u2208 X \u00d7 Y, g(x) = y. We use n to refer to the number of training examples and d to represent the number of real-valued features for each example.\nWe use p(x) to represent the density of the distribution from which feature values are drawn. Given a vector x \u2208 Rd, xj refers to the value of the jth element of this vector and x\u2212j \u2208 Rd\u22121 is the vector x with the jth feature removed. Thus we can equivalently express the density p(x) as p(xj |x\u2212j)p(x\u2212j). The notation P(y = 1|x) represents the true probability that the example x belongs to class 1, and P\u0302(y = 1|x) is an estimate of this probability. For binary classification, we take Y = {\u22121,+1}. Though all of our notation assumes real-valued features, the analysis presented applies equally well to variables that take on discrete values.\n3. Feature Selection\nFeature selection approaches generally fall into three broad categories: filter, wrapper, and embedded methods. Filters are effectively pre-processing steps that score features according to some criterion and select those with the highest scores. They are fast and simple, but tend to perform less well than other approaches, partly because they measure the impact of a feature without taking into account the way the classifier will use that feature. Embedded methods involve modification to the training algorithm itself so that features can be selected as part of the training process. They are usually classifier-specific, and thus less general than other selection techniques. For more on these approaches, see Guyon & Elisseeff (2003).\nIn this paper we focus on wrapper methods, which score a set of features according to the loss (on a test set) of a classifier trained using only these features (Guyon & Elisseeff, 2003). This approach considers variables in the context of others, can apply to virtually any classifier, and explicitly measures the use of the feature to the chosen classification algorithm.\nA commonly-implemented wrapper approach, known as recursive feature elimination, greedily constructs nested subsets of features. Starting with the full set of features, a series of classifiers are trained. The variables that cause performance to suffer least when not used are eliminated, and the process repeats.\nThe most common criticism of wrapper methods is that naive implementations tend to be quite slow. Even using the greedy method described above, for a data set with d features, O(d2) classifiers must be trained. This can be prohibitively expensive, so a variety of methods have been developed to approximate the result of this process (Guyon et al., 2002; Maldonado & Weber, 2009)."}, {"heading": "4. Feature Selection Using Probabilistic Outputs", "text": "In classification tasks, the most commonly used loss function is simply the number of classification errors on the test set, or some close variant such as F -measure. This approach can have difficulty identifying significant features in high-dimensional spaces, where the influence of each feature tends to be small and removing any single feature is unlikely to notably affect classification performance.\nOne way to address this concern is to use algorithms that output estimated class probabilities. Since a feature may influence probability estimates without\nchanging the predicted class label, scoring features according to their effect on probability estimates is more sensitive than considering only the misclassification rate.\nIn this section we examine a feature-scoring method that incorporates class probabilities proposed by Shen et al. (2008), and introduce our modified featurescoring criterion. Both scores are intended to be used as part of a recursive feature elimination scheme. For ease of exposition, this section assumes a binary classification problem.\n4.1. Two Feature Scoring Criteria\nAs discussed above, the sensitivity of class probabilities provides a natural measure of each feature\u2019s importance. Shen et al. (2008) propose the following feature ranking criterion based on this idea.\nSS(j) =\n\u222b\nX\n|P(y = 1|x)\u2212 P(y = 1|x\u2212j)|p(x)dx. (1)\nBecause it is impossible to measure the above quantities, the joint density p(x) and the probabilities P(y = 1|x) and P(y = 1|x\u2212j) must be estimated. Shen et al. propose four techniques to approximate (1). They report the best results when using the following estimate:\nS\u0302S(j) = 1\nn\nn\u2211\ni=1\n|P\u0302(y = 1|xi)\u2212 P\u0302(y = 1|x\u2212ji )|. (2)\nThe motivation for the scoring system implied by S\u0302S is clear: features that cause significant changes in the estimated class probabilities are ranked higher than those that do not. At the same time, it seems that an ideal importance measure should take into account not only the magnitude of the change in probability estimates, but also its sign.\nTo illustrate this point, suppose that we have the following classification task with two binary attributes:\nx1 x2 p(x) P(y = 1|x) P(y = 1|x1) P(y = 1|x2) 0 0 10/22 0.495 0.45 0.66 0 1 1/22 0.000 0.45 0.00 1 0 10/22 0.825 0.75 0.66 1 1 1/22 0.000 0.75 0.00\nNote that for an example with features x = (0, 0), P(y = 1|x) = 0.495, whereas if information about x1 is not available, we see that P(y = 1|x\u22121) = 0.66. Though including the first feature in our model changes our estimated probabilities by 0.165 for all examples with feature vector (0, 0), this change is only\nbeneficial 50.5% of the time, since in the other 49.5% of cases, the example belongs to the positive class. Equation (2) does not take this fact into account.\nIn light of this observation, we propose the following scoring criterion:\nS\u0302A(j) = 1\nn\nn\u2211\ni=1\nyi(P\u0302(y = 1|xi)\u2212 P\u0302(y = 1|x\u2212ji )) (3)\nThis score rewards features when their inclusion moves estimated probabilities towards the correct class and punishes for examples such that including the feature worsens our prediction.\nWe now have two proposed feature scoring functions given by S\u0302S and S\u0302A. Our next goal is to develop a theoretical framework to assist in analyzing and understanding these measures.\n4.2. Analysis of the score SS\nIn this section, we analyze the quantity SS given in (1). This analysis is motivated by the thought that before dedicating too much effort to approximating SS , we would like to ensure that it is a reasonable surrogate for the importance of a feature. The contributions of this section are two-fold. First, we demonstrate that SS provides an upper-bound for the improvement in accuracy exhibited by the Bayes-optimal classifier due to the inclusion of the jth feature. We also show that SS measures the expected mean absolute deviation of P(y = 1|x) as the jth feature varies. Ideally, SS should correspond in some way to the utility of the jth feature. Of course, the utility of a feature depends on the procedure by which the data are used (i.e., the classification algorithm of choice). Though Shen et al.\u2019s work focuses on support vector machines (SVMs), we proceed with a general analysis in this section. Rather than measuring the utility of a feature to any particular classifier, we consider the utility provided by that feature to the best possible classifier.\nIf the true function P(y = 1|x) were known, prediction error could be minimized by always predicting the most likely class. This decision rule defines the Bayes-optimal classifier, and for binary classification its expected accuracy is given by\n\u222b\nX\nmax{P(y = 1|x), 1\u2212 P(y = 1|x)}p(x)dx. (4)\nIt is natural to measure each feature\u2019s importance by the improvement in accuracy that it offers the Bayesoptimal classifier:\nSB(j) =\n\u222b\nX\nmax{P(y = 1|x), 1\u2212 P(y = 1|x)}p(x)dx\n\u2212 \u222b\nX\nmax{P(y = 1|x\u2212j), 1\u2212 P(y = 1|x\u2212j)}p(x)dx, (5)\nwhich can be re-expressed using the identity max{p, 1\u2212 p} = 1/2 + |p\u2212 1/2| as\nSB(j) = (6)\u222b\nX\n( |P(y = 1|x)\u2212 1/2| \u2212 |P(y = 1|x\u2212j)\u2212 1/2| ) p(x)dx\nThe reverse triangle inequality implies that\nSB(j) \u2264 (7)\u222b\nX\n|(P(y = 1|x)\u2212 1/2)\u2212 (P(y = 1|x\u2212j)\u2212 1/2)|p(x)dx\n=\n\u222b\nX\n|P(y = 1|x)\u2212 P(y = 1|x\u2212j)|p(x)dx = SS(j).\nThus, SS(j) provides an upper-bound for SB(j) that holds regardless of the number of features, the shape of their joint density, or the form of P(y = 1|x). The following computations serve to provide additional intuition for the quantity measured by SS(j). We can rewrite SS(j) as:\u222b X\u2212j \u222b Xj |P(y = 1|x)\u2212P(y = 1|x\u2212j)|p(xj |x\u2212j)dxj p(x\u2212j)dx\u2212j\n(8)\nThe quantity inside of parentheses is the expected value of |P(y = 1|x) \u2212 P(y = 1|x\u2212j)| given x\u2212j . Because P(y = 1|x\u2212j) = E[P(y = 1|x)|x\u2212j ], this equals the mean absolute deviation (MAD) of P(y = 1|x) given x\u2212j (where MAD[Z] = E|Z \u2212 E[Z]|). Thus,\nSS(j) = EMAD[P(y = 1|x)|x\u2212j ]. (9)\nA visualization of this fact is presented in Figure 1. Intuitively, the most important features are those that cause significant fluctuation in P(y = 1|x), so using a measure of the spread of P(y = 1|x) as a feature ranking criterion seems reasonable. For entirely irrelevant features, P(y = 1|x) does not vary as xj does, so MAD[P(y = 1|x)|x\u2212j ] = 0.\n4.3. Analysis of Our Alternative Score\nHaving provided two reasons that SS seems like a reasonable feature scoring criterion, we now analyze our modified score S\u0302A given in (3).\nFeature Selection via Probabilistic Outputs\nWe first show that using the score S\u0302A(j) as the criterion for feature removal is equivalent to performing recursive feature elimination using absolute loss, i.e. L(P\u0302(y = 1|xi), yi) = |ti \u2212 P\u0302(y = 1|xi)|, where ti is 1 if yi = 1 and 0 otherwise. To see this, note that a recursive feature elimination algorithm using this loss function removes the feature j that minimizes\n1\nn\nn\u2211\ni=1\n|ti \u2212 P\u0302(y = 1|x\u2212ji )| = 1\nn\nn\u2211\ni=1\nyi(ti \u2212 P\u0302(y = 1|x\u2212ji ))\n= 1\nn\nn\u2211\ni=1\nyi(ti \u2212 P\u0302(y = 1|xi)) + S\u0302A(j).\n(10)\nSince the first of these two terms has no j dependence, it follows that the two algorithms yield identical feature rankings. By contrast, using the score S\u0302S(j) does not correspond to any natural loss function.\nWe now establish that S\u0302A(j) estimates a quantity proportional to the expected variance of P(y = 1|x) given x\u2212j . Note that an example with feature values x contributes P\u0302(y = 1|x)\u2212 P\u0302(y = 1|x\u2212j) to the sum in (3) if it belongs to the positive class, and the negative of this quantity otherwise. Since E[yi|xi] = 2P(y = 1|x) \u2212 1, given an example x, its expected contribution to (3) is\n(2P(y = 1|x)\u2212 1)(P\u0302(y = 1|x)\u2212 P\u0302(y = 1|x\u2212j)). (11)\nWeighting the space X with its density function, it follows that the quantity approximated by S\u0302A is:\nSA(j) = (12)\u222b\nX\n(2P(y = 1|x)\u22121)(P(y = 1|x)\u2212 P(y = 1|x\u2212j))p(x)dx\nSince E[P(y = 1|x)] = P(y = 1) = E[P(y = 1|x\u2212j)], the piece of the integrand corresponding to the -1 vanishes, leaving\nSA(j)=2\n\u222b\nX P(y = 1|x)2\u2212P(y = 1|x)P(y = 1|x\u2212j)p(x)dx.\n(13)\nAs in (8), we move the integral with respect to the jth feature inside and observe that\nE[P(y = 1|x)P(y = 1|x\u2212j)|x\u2212j ]=P(y = 1|x\u2212j)2, (14) =E[P(y = 1|x)|x\u2212j ]2.\nUsing the fact that Var[Z] = E[Z2]\u2212 E[Z]2, we get\nSA(j) = 2E[Var(P(y = 1|x))|x\u2212j ]. (15)\nThis holds regardless of the dimension d, the density p(x), or form of P(y = 1|x). Because the variance\nPlot@8x * H1 - x \u00ea 10L^5 * 14 \u00ea 10, 1 \u00ea 3<, 8x, 0, 10<, PlotRange \u00c6 8-.1, 1.1<, Filling \u00c6 81 \u00c6 82<<, FillingStyle \u00c6 Directive@Opacity@0.2D, GrayD, PlotStyle \u00c6 8Thick, Black<D\n2 4 6 8 10\n0.2\n0.4\n0.6\n0.8\n1.0\nFeature Selection via Probabilistic Outputs\nfor a particular synthe ic problem as the class balance\nvaries.\nThe following computations serve to provide addi-\ntional intuition for the quantity measured by SS(j). We can rewrite SS(j) as:\nZ\nX j\n0 @ Z\nXj\n|P(y = 1|x) P(y = 1|x j)|p(xj |x j)dxj 1 A\np(x j)dx j\nThe quantity inside of parentheses is the expected value of |P(y = 1|x) P(y = 1|x j)| given x j , otherwise known as the mean abs lu e deviation (MAD) of P (y = 1|x) given x j (where MAD[Z] = E|Z E[Z]|). Thus,\nSS(j) = E MAD[P(y = 1|x)|x j ]. (6)\nFor entirely irrelevant features, P(y = 1|x) does not vary in Xj , so MAD[P(y = 1|x)] = 0. Intuitively, the most important features are those that cause significant fluctua io in P(y = 1|x), so using a measure of the spread of P(y = 1|x) as a feature ranking criterion is very reasonable.\n4.3. Analysis of Our Proposed Alternative\nIn this section, we analyze our modified score (3). We begin by establishing that this score fits into the recursive feature elimination framework.\nIn fact, using the score S\u0302A(j) as the criterion for feature removal is equivalent to performing recursive feature elimination using the loss function L(P\u0302(y = 1|xi), yi) = |ti P\u0302(y = 1|xi)|, where ti is an indicator that yi = 1. To see this, note that a recursive feature elimination algorithm using absolute loss removes the feature j that minimizes\n1\nn\nnX\ni=1\n|ti P\u0302(y = 1|x ji )|\n= 1\nn\nnX\ni=1\nyi(ti P\u0302(y = 1|x ji ))\n= 1\nn\nnX\ni=1\nyi(ti P\u0302(y = 1|xi)) + S\u0302A(j).\nSince the first of these two terms has no j dependence, it follows that the two algorithms yield identical feature rankings.\nWe now move on to establishing that (3) is in fact estimating a quantity that is proportional to the expected variance of P(y = 1|x) given x j . Note that\nan example with feature values x contributes P\u0302(y =\n1|x) P\u0302(y = 1|x j) to the sum in (3) if it belongs\nto the positive class, and the negative of this quantity\notherwise. Thus, the expected contribution of this example to (3) is\n(2P(y = 1|x) 1)(P\u0302(y = 1|x) P\u0302(y = 1|x j)).\nWeighting the space X with its density function, it follows that the quantity approximated by S\u0302A is:\nSA(j) =\nZ\nX\n(2P(y = 1|x) 1)(P(y = 1|x) (7)\nP(y = 1|x j))p(x)dx\nSince E[P(y = 1|x)] = P(y = 1) = E[P(y = 1|x j)], the piece of the integrand corresponding to the -1 vanishes, leaving\nSA(j) = 2\nZ\nX P(y = 1|x)2\nP(y = 1|x)P(y = 1|x j)p(x)dx.\nAs before, we can move the integral with respect to the jth feature inside and observe that P(y = 1|x j) = E[P(y = 1|x)|x j ]. This yields that the above quantity is twice the expected conditional variance of P(y = 1|x) given x j . This observation holds regardless of the dimension d, the density p(x), or the way in which the function P(y = 1|x) varies in X . Because the variance measures the spread of a distribution, this indicates that SA matches our intuitive understanding that features to which P(y = 1|x) is very sensitive are more important.\n4.4. Further Discussion\nAbove we established that both SS(j) and SA(j) measure the expected variation in the probability P(y = 1|x) as the jth feature fluctuates. A natural conclusion to draw from this is that SS and SA are likely to assign each feature similar scores. In fact, we can show that for any feature j, SS(j) and SA(j) cannot be \u201ctoo far\u201d apart. More precisely, the following chain of inequalities holds:\n0  SA(j), SB(j)  SS(j)  p SA(j)/2  1/2. (8)\nTo help with visualization, these three scores are plotted in Figure 1 for a variation of the synthetic problem from Weston & Watkins (1999). In this problem, most features are noise following a normal distribution, while the informative features are all of the form:\nXc,p \u21e0 \u21e2\nyN(c, 1) : with probability p N(0, 1) : otherwise\n(9)\nFeature Selection via Probabilistic Outputs It seems that an ideal importance measure should take into account not only the magnitude of the change in probability, but also its sign. To illustrate this point, suppose that we have a model P\u0302 such that for some xi, P\u0302(y = 1|xi) = 0.8 and P\u0302(y = 1|x ji ) = 0.6. The fact that the output changes by 0.2 indicates that P\u0302 is sensitive to feature j, but it does not necessarily mean that this feature is improving classification performance. If xi is in the negative class, this suggests that P\u0302(y = 1|xi) should be as low as possible, so the presence of feature j worsens predictions in this case. In light of this observation, we propose the following scoring criterion:\nS\u0302A(j) = 1\nn\nnX\ni=1\nyi(P\u0302(y = 1|xi) P\u0302(y = 1|x ji )) (3)\nwhere the value of any yi is 1 i he posi ive case and -1 otherwise. This formulation incorporates the sign of the change by rewarding features when their inclusion moves estimated probabilities towards the correct class and punishing for cases when including the feature worsens our prediction.\nWe now have two proposed feature scoring functions given by S\u0302S and S\u0302A. Our next goal is to develop a theoretical framework to assist in analyzing and understanding these measures.\n4.2. Analysis of Shen\u2019s Score for Binary Classification\nIn this section, we analyze the quantity SS estimated by S\u0302S . This analysis is motivated by the thought that before dedicating too much e\u21b5ort to approximating SS , we would like to ensure that it is a reasonable surrogate for the importance of a feature. The contributions of this section are two-fold. First, we demonstrate that SS provides an upper-bound fo the improvement in accuracy exhibited by the Bayes-optimal classifier due to the inclusion of the jth feature. We also show t at SS measures the expected mean absolute deviation of P(y = 1|x) as the jth feature varies. Ideally, SS would correspond in some way to the utility of the jth fe ture. Of course, the utility of a fea ure depends on the procedure by which the data are used (i.e., the classification algorithm of choice). Though Shen et al.\u2019s work focuses on support vector machines (SVMs), we proceed with a ge eral analysis in this section. Rather than measuring the utility of a feature to any particular classifier, we consider the utility provided by that feature to the best possible classifier. If the true class probability function P(y = 1|x)\nwere known, prediction error could be minimized by always predicting the most likely class. This decision rule defines the Bayes-optimal classifier, and can be summarized for two-class problems as follows: g(x) = \u21e2 1 P(y = 1|x) 0.5 1 P(y = 1|x) < 0.5 It is straightforward to see that the expected accuracy of this classifier is Z X max{P(y = 1|x), 1 P(y = 1|x)}p(x)dx.\nIt is natural to measure each feature\u2019s importance by the improvement in accuracy that it o\u21b5ers the Bayesoptimal classifier:\nSB(j) =\nZ\nX\nmax{P(y = 1|x), 1 P(y = 1|x)}p(x)dx\n(4)\nZ\nX\nmax{P(y = 1|x j), 1 P(y = 1|x j)}p(x)dx\nwhich can be re-expressed using the identity max{p, 1 p} = 1/2 + |p 1/2| as\nSB(j) = (5)Z\nX\n|P(y = 1 x) 1/2| |P(y = 1|x j) 1/2| p(x)dx\nWe can show that when scoring a single variable with balanced classes, SS(j) is precisely equal to the improvement in optimal accuracy provided by the jth\nfeature.\nWe now consider the correspondence between SS and SB . The reverse triangle inequality implies that\nSB(j) Z\nX\n|(P(y = 1|x) 1/2) (P(y = 1|x j) 1/2)|p(x)dx\n=\nZ\nX\n|P(y = 1|x) P(y = 1|x j)|p(x)dx\n= SS(j).\nThus, SS(j) provides an upper-bound for SB(j) that holds regardless of the number of features, the shape of their joint density, or the functional form of P(y = 1|x). For a visualization of how far these two can be from each other, see Figure 1, which plots SS and SB\nFeature Selection via Probabilistic Outputs It seems that an ideal importance measure should take into account not only the magnitude of the change in probability, but also its sign. To illustrate this point, suppose that we have a model P\u0302 such that for some xi, P\u0302(y = 1|xi) = 0.8 and P\u0302(y = 1|x ji ) = 0.6. The fact that the output changes by 0.2 indicates that P\u0302 is sensitive to feature j, but it does not necessarily mean that this feature is improving classification performance. If xi is in the negative class, this suggests that P\u0302(y = 1|xi) should be as low as possible, so the presence of feature j worsens predictions in this case. In light of this observation, we propose the following scoring criterion:\nS\u0302A(j) = 1\nn\nnX\ni=1\nyi(P\u0302(y = 1|xi) P\u0302(y = 1|x ji )) (3)\nwhere the value of any yi is 1 in the positive case and -1 otherwise. This formulation incorporates the sign of the change by rewarding features when their inclusion moves estimated probabilities towards the correct class and punishing for cases when including the feature worsens our prediction.\nWe now have two proposed feature scoring unctions given by S\u0302S and S\u0302A. Our next goal is to develop a theoretical framework to assist in analyzing and understanding these measures.\n4.2. Analysis of Shen\u2019s Scor for Binary Classification\nIn this section, we analyze the quantity SS estimated by S\u0302S . This analysis is motivated by the thought that before dedicating too much e\u21b5ort to approximating SS , we would like to ensure that it is a reasonable surrogate for the importance of a feature. The contributions of this section are two-fold. First, we demonstrate that SS provides an upper-bound for the improvem nt in accuracy exhibited by the Bayes-optimal classifier due to the inclusion of the jth feature. We also show that SS measures the expected mean absolute deviation of P(y = 1|x) as the jth feature varies. Ideally, SS would correspond in some way to the utility of the jth feature. Of course, the utility of a feature depends on the procedure by which the data are used (i.e., the classification algorithm of choice). Though Shen et al.\u2019s work focuses on support vector machines (SVMs), we proceed with a general analysis in this section. Rather than measuring the utility of a feature to any particular classifier, we consider the utility provided by that feature to the best possible classifier. If the true class probability function P(y = 1|x)\nwere known, prediction error could be minimized by always predicting the most likely clas . This decision rule defines the Bayes-optimal classifier, nd can be summarized for two-class problems as follows: g(x) = \u21e2 1 P(y = 1|x) 0.5 1 P(y = 1|x) < 0.5 It is straightforward to see that the expected accuracy of this classifier is\nZ\nX\nmax{P(y = 1|x), 1 P(y = 1|x)}p(x)dx.\nIt is n tural to easur each feature\u2019s importance by the improvement in accuracy that it o\u21b5ers the Bayes-\noptimal classifier:\nSB(j) =\nZ\nX\nmax{P(y = 1|x), 1 P(y = 1|x)}p(x)dx\n(4)\nZ\nX\nmax{P(y = 1|x j), 1 P(y = 1|x j)}p(x)dx\nwhich can be re-expressed using the identity max{p, 1 p} = 1/2 + |p 1/2| as\nSB(j) = (5)Z\nX\n|P(y = | ) 1/2| |P(y = 1|x j) 1/2| p(x)dx\nWe can show that when scoring a single variable with balanced classes, SS(j) is precisely equal to the improvement in optimal accuracy provided by the jth\nfeature.\nWe now consider the correspondence between SS and SB . Th reverse triangle inequality implies that\nSB(j) Z\nX\n|(P(y = 1|x) 1/2) (P(y = 1|x j) 1/2)|p(x)dx\n=\nZ\nX\n|P(y = 1|x) P(y = 1|x j)|p(x)dx\n= SS(j).\nThus, SS(j) provides an upper-bound for SB(j) that holds regardless of the number of features, th shape of their joint density, or the functional form of P(y = 1|x). For a visualization of how far these two can be from each other, see Figure 1, which plots SS and SB\nPlot@8x * H1 - x \u00ea 10L^5 * 14 \u00ea 10, 1 \u00ea 3<, 8x, 0, 10<, PlotRange \u00c6 8-.1, 1.1<, Filling \u00c6 81 \u00c6 82<<, FillingStyle \u00c6 Directive@Opacity@0.2D, GrayD, PlotStyle \u00c6 8Thick, Black<D\n2 4 6 8 10\n0.2\n0.4\n0.6\n0.8\n1.0\nFeature Selection via Probabilistic Outputs\nfor a particular synthetic problem as the class balance varies. The following computations serve to provide additional intuition for the quantity measured by SS(j). We can rewrite SS(j) as:\nZ\nX j\n0 @ Z\nXj\n|P(y = 1|x) P(y = 1|x j)|p(xj |x j)dxj 1 A\np(x j)dx j\nThe quantity inside of parentheses is the expected\nvalue of |P(y = 1|x) P(y = 1|x j)| given x j , other-\nwise known as the mean absolute deviation (MAD) of\nP (y = 1|x) given x j (where MAD[Z] = E|Z E[Z]|).\nThus,\nSS(j) = E MAD[P(y = 1|x)|x j ]. (6)\nFor entirely irrelevant features, P(y = 1|x) does not vary in Xj , so MAD[P(y = 1|x)] = 0. Intuitively, the most important features are those that cause significant fluctuation in P(y = 1|x), so using a measure of the spread of P(y = 1|x) as a feature ranking criterion is very reasonable.\n4.3. Analysis of Our Proposed Alt rnative In this section, we analyze our modified score (3). We begin by establishing that this score fits into the recursive feature elimination framework. In fact, using the score S\u0302A(j) as the criterion for feature removal is equivalent to performing recursive feature elimination using the loss function L(P\u0302(y = 1|xi), yi) = |ti P\u0302(y = 1|xi)|, where ti is an indicator that yi = 1. To see this, note that a recursive feature elimination algorithm using absolute loss removes the feature j that minimizes\n1 n\nnX\ni=1\n|ti P\u0302(y = 1|x ji )|\n= 1\nn\nnX\ni=1\nyi(ti P\u0302(y = 1|x ji ))\n= 1\nn\nnX\ni=1\nyi(ti P\u0302(y = 1|xi)) + S\u0302A(j).\nSince the first of these two terms has no j dependence, t follows that the two algorithms yield identical feature rankings. We now move on to establishing that (3) is in fact estimating a quantity that is proportional to the expected variance of P(y = 1|x) given x j . Note that\nan example with feature values x contributes P\u0302(y = 1|x) P\u0302(y = 1|x j) to the sum in (3) if it belongs to the positive class, and the negative of this quantity otherwise. Thus, the expected contribution of this example to (3) is\n(2P(y = 1|x) 1)(P\u0302(y = 1|x) P\u0302(y = 1|x j)).\nWeighting the space X with its density function, it follows that the quantity approximated by S\u0302A is:\nSA(j) =\nZ\nX\n(2P(y = 1|x) 1)(P(y = 1|x) (7)\nP(y = 1|x j))p(x)dx\nSince E[P(y = 1|x)] = P(y = 1) = E[P(y = 1|x j)], the piece of the integrand corresponding to the -1 vanishes, leaving\nSA(j) = 2\nZ\nX P(y = 1|x)2\nP(y = 1|x)P(y = 1|x j)p(x)dx.\nAs before, we can move the integral with respect to the jth feature inside and observe that P(y = 1|x j) = E[P(y = 1|x)|x j ]. This yields that the above quantity is twice the expected conditional variance of P(y = 1|x) given x j . This observation holds regardless of the dimension d, the density p(x), or the way in which the function P(y = 1|x) varies in X . Because the variance measures the spread of a distribution, this indicates that SA matches our intuitive understanding that features to which P(y = 1|x) is very sensitive are more important.\n4.4. Further Discussion Above we established that both SS(j) and SA(j) measure the expected variation in the probability P(y = 1|x) as the jth feature fluctuates. A natural conclusion to draw from this is that SS and SA are likely to assign each feature similar scores. In fact, we can show that for any feature j, SS(j) and SA(j) cannot be \u201ctoo far\u201d apart. More precisely, the following chain of inequalities holds:\n0  SA(j), SB(j)  SS(j)  p SA(j)/2  1/2. (8) To help with visualization, these three scores are plotted in Figure 1 for a variation of the synthetic problem from Weston & Watkins (1999). In this problem, most features are noise following a normal distribution, while the informative features are all of the form:\nXc,p \u21e0 \u21e2\nyN(c, 1) : with probability p N(0, 1) : otherwise\n(9)\nFeature Selection via Probabilistic Outputs It seems that an ideal importance measure should take into acc unt not only the magnitude of the change in probability, but also its sign. To illustrate this point, suppose that we have a model P\u0302 such that for some xi, P\u0302(y = 1|xi) = 0.8 and P\u0302(y = 1|x ji ) = 0.6. The fact that the output changes by 0.2 indicates that P\u0302 is sensitive to feature j, but it does not necessarily mean that this feature is improving classification performance. If xi is in the negative class, this suggests that P\u0302(y = 1|xi) should be as low as possible, so the presence of feature j worsens predictions in this case. In light of this observation, we propose the following scoring criterion:\nS\u0302A(j) = 1\nn\nnX\ni=1\nyi(P\u0302(y = 1|xi) P\u0302(y = 1|x ji )) (3)\nwhere the value of any yi is 1 in the positive case and -1 otherwise. This formulation incorporates the sign of the change by rewarding features when their inclusion m ves estimated probabilities towards the correct class and punishing for cases when including the feature worsens our prediction.\nWe now have two proposed feature scoring functions given by S\u0302S and S\u0302A. Our next goal is to develop a theoretical framework to assist in analyzing and un-\nderstanding these measures.\n4.2. Analysis of Shen\u2019s Scor for Binary\nClassification\nIn this section, we analyze the quantity SS estimated by S\u0302S . This analysis is motivated by the thought that before dedicating too much e\u21b5ort to approximating SS , we would like to ensure that it is a reasonable surrogate for the importance of a feature. The contributions of this section are two-fold. First, we demonstrate that SS provides an upper-bound for the improvement in accuracy exhibited by the Bayes-optimal classifier due to the inclusion of the jth feature. We also show at SS measures the expected e n absolute deviation of P(y 1|x) as the jth feature varies. I eally, SS would correspond in some way to the utility of the jth fe ture. Of course, the utility of a fea ure depends on the procedure by which the data are used (i.e., the classification algorithm of choice). Though Shen et al.\u2019s work focuses on support vector machines (SVMs), we proceed with a ge eral analysis in this section. Rather than measuring the utility of a feature to any particular classifier, we consider the utility provided by that feature to the best possible classifier. If the true class probability function P(y = 1|x)\nwere known, prediction error could be minimized by always predicting the most likely class. This decision rule defines the Bayes-optimal classifier, and can be summarized for two-class problems as follows: g(x) = \u21e2 1 P(y = 1|x) 0.5 1 P(y = 1|x) < 0.5\nIt is straightforw rd to se that the expected accuracy of this classifier is\nZ\nX\nmax{P(y = 1|x), 1 P(y = 1|x)}p(x)dx.\nIt is natural to measure each feature\u2019s importance by the improvement in accuracy that it o\u21b5ers the Bayesoptimal classifier:\nSB(j) =\nZ\nX\nmax{P(y = 1|x), 1 P(y = 1|x)}p(x)dx\n(4)\nZ\nX\nmax{P(y = 1|x j), 1 P(y = 1|x j)}p(x)dx\nwhich can be re-expressed using the identity max{ , 1 p} = 1/2 + |p 1/2| as\nSB(j) = (5)Z\nX\n|P(y = 1 x) 1/2| |P(y = 1|x j) 1/2| p(x)dx\nWe can show that when scoring a single variable with balanced classes, SS(j) is precisely equal to the improvement in optimal accuracy provided by the jth feature.\nWe now consider the correspondence between SS and SB . The reverse triangle inequality implies that\nSB(j) Z\nX\n|(P(y = 1|x) 1/2) (P(y = 1|x j) 1/2)|p(x)dx\n=\nZ\nX\n|P(y = 1|x) P(y = 1|x j)|p(x)dx\n= SS(j).\nThus, SS(j) provides an upper-bound for SB(j) that holds regardl ss of the number of features, the shape of their joint density, or the functional form of P(y = 1|x). For a visualization of how far these two can be from each other, see Figure 1, which plots SS and SB\nFeature Selection via Probabilistic Outputs\nIt seems that an ideal importance measure should take into account not only the magnitude of the change in probability, but also its sign. To illustrate this poin , suppose that we have a model P\u0302 such that for some xi, P\u0302(y = 1|xi) = 0.8 and P\u0302(y = 1|x ji ) = 0.6. The fact that the output changes by 0.2 indicates that P\u0302 is sensitive to feature j, but it does not necessarily mean that this feature is improving classification performance. If xi is in the negative class, this suggests that P\u0302(y = 1|xi) should be as low as possible, so the presence of feature j worsens predictions in this case. In light of this observation, we propose the following scoring criterion:\nS\u0302A(j) = 1\nn\nnX\ni=1\nyi(P\u0302(y = 1|xi) P\u0302(y = 1|x ji )) (3)\nwhere the value of any yi is 1 in the positive case and -1 otherwise. This formulation incorporates the sign of the change by rewarding features when their inclusion oves estimated probabilities towards the correct class and punishing for cases when including the feature worsens our prediction.\nWe now have two proposed feature scoring functions given by S\u0302S and S\u0302A. Our next goal is to develop a theoretical framework to assist in analyzing and understanding these measures.\n4.2. Analysis of Shen\u2019s Scor for Binary Classification\nIn this section, we analyze the quantity SS estimated by S\u0302S . This analysis is motivated by the thought that before dedicating too much e\u21b5ort to approximating SS , we would like to ensure that it is a reasonable surrogate for the importance of a feature. The contributions of this section are two-fold. First, we demonstrate that SS provides an upper-b und for the improvem nt in accuracy exhibited by the Bayes-optimal classifier due to the inclusion of the jth feature. We also show that SS measures the expected mean absolute deviation of P(y = 1|x) as the jth feature varies. Ideally, SS would correspond in some way to the utility of the jth feature. Of course, the utility of a feature depends on the procedure by which the data are used (i.e., the classification algorithm of choice). Though Shen et al.\u2019s work focuses on support vector machines (SVMs), we proceed with a general analysis in this section. Rather than measuring the utility of a feature to any particular classifier, we consider the tility provided by that feature to the best possible classifier. If the true class probability function P(y = 1|x)\nwere known, prediction error could be minimized by always predicting the most likely clas . This decision rule defines the Bayes-optimal classifier, nd can be summarized for two-class problems as follows:\ng(x) = \u21e2 1 P(y = 1|x) 0.5 1 P(y = 1|x) < 0.5\nIt is straightforward to see that the expected accuracy of this classifier is\nZ\nX\nmax{P(y = 1|x), 1 P(y = 1|x)}p(x)dx.\nIt is natural to measure each feature\u2019s importance by the improvement in accuracy that it o\u21b5ers the Bayesoptimal classifier:\nSB(j) =\nZ\nX\nmax{P(y = 1|x), 1 P(y = 1|x)}p(x)dx\n(4)\nZ\nX\nmax{P(y = 1|x j), 1 P(y = 1|x j)}p(x)dx\nwhich can be re-expressed using the identity max{p, 1 p} = 1/2 + |p 1/2| as\nSB(j) = (5)Z\nX\n|P(y = 1|x) 1/2| |P(y = 1|x j) 1/2| p(x)dx\nWe c n show that when scoring a single variable wi balanced classes, SS(j) is precisely equal to the improvement in optimal accuracy provided by the jth feature.\nWe now consider the correspondence between SS and SB . Th reverse triangle inequality implies that\nSB(j) Z\nX\n|(P(y = 1|x) 1/2) (P(y = 1|x j) 1/2)|p(x)dx\nZ\nX\n|P(y = 1|x) P(y = 1|x j)|p(x)dx\n= SS(j).\nThus, SS(j) provides an upper-bound for SB(j) that holds regardless of the number of features, th shape of their joint density, or the functional form of P(y = 1|x). For a visualization of how far these two can be from each other, see Figure 1, which plots SS and SB\nFigure 1. A hypothetical dependence of P(y = 1|x) on xj , for a fixed choice of \u2212j . If p(xj |x\u2212j) is uniform on [0, 10], the gray region represents MAD[P(y = 1|x)|x\u2212j ]. SS(j) is the expected area of the gray region across all possibilities for x\u2212j .\nme sures the spr ad of a distribution, this indicates that SA (like SS) matches our intuitive understanding that fe tures to which P(y = 1|x) is very sensitive are more portant.\nWe have stablished that both SS(j) and SA(j) measure th expected variation in P(y = 1|x) along the jth dimension. Intuitively, this suggests that SS(j) and SA(j) should be \u201csimilar.\u201d In fact, we can show that the following chain of inequalities holds:\n0 \u2264 SA(j), SB(j) \u2264 SS(j) \u2264 \u221a SA(j)/2 \u2264 1/2. (16)\nTo help visualize the above inequalities, SS , SA, and SB are lotted in Fig re 2 for a variation of the synthetic problem from Weston & Watkins (1999) 1.\nIt is worth noting that although SS and SA are closely related, this does not mean that they rank features identically. In the task presented in Section 4.1, SS(x 1) = 0.15 > SS(x 2) = 0.109 and SA(x\n1) = 0.0495 < SA(x\n2) = 0.0765, so even though true probabilities are known, SS and SA select different features.\n5. Experiment l Evaluati n\nIn practice, SS and SA cannot be directly computed and must be estimated by S\u0302S and S\u0302A, respectively. In this section we make predictions regarding S\u0302S and S\u0302A and provide results from several tests comparing them.\nComputing either S\u0302S and S\u0302A is essentially a two-step process. First, the training data is used to fit the func-\n1 In Weston\u2019s problem, most features are noise following a normal distribution, while the informative features are all of the form:\nXc,p \u223c {\nyN(c, 1) : with probability p N(0, 1) : otherwise\nThus, even informative features take noisy values with probability 1 \u2212 p. Note that features of this form are increasingly informative as c and p increase. This method for generating features has since been adopted elsewhere, including by Shen et al. (2008).\n0.0 0.2 0.4 0.6 0.8 1.0\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n,\n0.0 0.2 0.4 0.6 0.8 1.0\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n,\n0.0 0.2 0.4 0.6 0.8 1.0\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n,\n0.0 0.2 0.4 0.6 0.8 1.0\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n,\n0.0 0.2 0.4 0.6 0.8 1.0\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n,\n0.0 0.2 0.4 0.6 0.8 1.0\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n,\n0.0 0.2 0.4 0.6 0.8 1.0\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n,\n0.0 0.2 0.4 0.6 0.8 1.0\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30 0.35Figure 2. Scores assigned by SS (red, large dashes), SA (green, small dashes), and SB (blue, solid) to Xc,0.7 in isolation for c \u2208 (0.2, 0.6, 1.0, 1.4). The x-axis represents the percentage of examples belonging to the positive class. When classes are balanced, SS and SB are equal, and SS is always at least as large as the other two.\ntion P\u0302(y = 1|x). Then, this estimate is used to compute the sums in (2) and (3), which approximate the density p(x). It came to our attention that because S\u0302S does not use the labels of each example in this second step, the sum in (2) can be taken over any mix of labeled and unlabeled examples. In many domains, large sets of unlabeled examples are available while labeled training data is relatively sparse, so using unlabeled data in this way could substantially improve the performance of S\u0302S .\nBoth S\u0302S and S\u0302A approximate the term P(y = 1|x) \u2212 P(y = 1|x\u2212j) by assuming a functional form for P(y = 1|x) and fitting parameter values to the training data. Even with a large training set, these estimates may be significantly biased if P(y = 1|x) does not have the assumed form. We argue that including yi in S\u0302A may alleviate this problem. To illustrate our point, suppose that x0 is such that P(y = 1|x0) = 1/2. Then even if the estimates P\u0302(y = 1|x0) and P\u0302(y = 1|x\u2212j0 ) are terribly wrong, as the number of training examples with feature vector x0 grows, we see a law of large numbers effect: the 50% of positive examples with feature vector x0 should cancel the corresponding negative examples in the sum from (3). By contrast, bad estimates for P\u0302(y = 1|x0) and P\u0302(y = 1|x\u2212j0 ) could cause a large contribution when computing S\u0302S(j) via (2), regardless of the training set size.\nOur work from Section 4 and the above discussion lead us to the following predictions:\n\u2022 Because variance is the square of standard deviation, which in turn is closely related to the mean absolute deviation, the score from S\u0302A should vary approximately as the square of S\u0302S . \u2022 When the training set is sampled disproportion-\nately from the domain, applying S\u0302S to large sets of unlabeled examples will provide a more consistent estimate of each feature\u2019s utility than either S\u0302A or S\u0302S applied only to the labeled training data. \u2022 In cases where the assumed model does not fit\nthe true dependence between X and Y, S\u0302A should identify relevant variables more reliably than S\u0302S .\nWe test the first two predictions in Sections 5.1 and 5.2, while leaving the third for future work. Additionally, we present preliminary results from applying S\u0302S and S\u0302A to a real-world data set in Section 5.3.\n5.1. Relationship Between S\u0302S and S\u0302A: a Simple Case\nTo confirm the prediction that S\u0302A should vary approximately as the square of S\u0302S , we ran a trial on the synthetic variables Xc,p as described earlier, with c ranging from 0 to 3 in steps of 0.25 and p in [0, 1] with steps of 0.1. For each (c, p) pair, we trained a linear SVM on a two-variable training set, where the first variable was Xc,p and the second was Gaussian noise.\nSupport vector machines typically do not provide probability estimates, but Vapnik (1998), Hastie & Tibshirani (1998) and Platt (1999) have all proposed methods for using SVMs to generate probabilities. For tests in this section, we obtained and modified source code from Shen et al., which derives probability estimates using the technique proposed in (Platt, 1999) and an SVM implementation from LIBSVM. Results from these trials are shown in Figure 3, and confirm our prediction beautifully.\nS_S (u)\n100\n200\n300\n400\n500\nC o\nr r e c t\nS e le\nc ti\no n\ndata1 data2 data3\n0\n.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n.4\n0 0.1 0.2 0.3 0.4 0.5\n\u221a S\u0302A(Xc,p)/2,\n5.2. Incorporating Unlabeled Examples\nWe designed the next test to explore the potential benefit of using unlabeled examples when computing S\u0302S . In this section, we use S\u0302SU to represent the score (2) augmented by additional unlabeled examples and S\u0302S to stand for the same score applied only to the labeled training instances.\nFor the purposes of this experiment, we constructed a synthetic data set with five nominal features. Each feature is independent of all others and takes on three possible values, say a, b, and c. The frequency with which each variable takes each value is given below:\nx1 x2 x3 x4 x5 a 1/4 1/4 1/3 1/3 1/2 b 1/2 1/2 1/3 1/6 1/4 c 1/4 1/4 1/3 1/2 1/4\nAll but the first feature are noise (i.e. they do not affect the probability that the example belongs to the positive class). We chose P(y = 1|x1 = a) = 1/4, P(y = 1|x1 = b) = 1/2, P(y = 1|x1 = c) = 3/4. Tests described in this section and 5.3 were conducted with a Naive Bayes classifier designed for nominal features. We made this choice because Naive Bayes classifiers explicitly provide estimated class probabilities. Additionally, they afford a natural (and efficient) way to compute P\u0302(y = 1|x\u2212j): missing values are handled by simply not including probabilities from that feature (Kononenko, 1991). Note that on this data set the features are independent, so with adequate training data a Naive Bayes classifier should replicate the true class probabilities.\nExperiments were conducted in three trials. Each trial contained tests on nine different sizes of training sets. For each training size and each trial, 500 runs were performed. A single run for a specified trial and training size consisted of sampling an appropriate number of training examples, training a Naive Bayes classifier on these examples, and using this classifier (along with a collection of unlabeled examples when appropriate) to compute S\u0302A(j), S\u0302S(j), and S\u0302SU (j) for j = 1 . . . 5. For each algorithm, we recorded the number of times (out of 500 runs) that it successfully ranked the first feature as the most informative.\nThe difference between the three trials was the manner in which training examples were selected. During the first trial, examples were drawn from the original distribution. During the second, the x1 training values were equally likely to be a, b, and c (effectively undersampling from b and oversampling from a and c, thereby making x1 seem more informative than it\nactually is). In the third trial, x1 took the values a and c each with probability 1/8, and took the value b with probability 3/4. This has the effect of understating the importance of x1. In all trials, the dependency P(y = 1|x) remained unchanged and the unlabeled examples provided to S\u0302S were drawn from the original distribution. This design allowed us to explore the question of how each scoring system fares when there is bias in the process of selecting training examples. The results, shown for different training set sizes, are displayed in Figure 4.\nWhen sampling proportionately from the data set, the three scores performed comparably. On the second trial (which we refer to as the \u201coversampling\u201d trial due to the fact that it overstates the importance of X1), S\u0302S and S\u0302A identified x\n1 as the top feature more often than in the first trial (as expected), while S\u0302SU performed nearly identically to the first trial. This suggests that our estimate for P(y = 1|x) is sufficiently accurate that incorporating unlabeled examples yields scores as if the training examples had been sampled proportionately. On the third (\u201cundersampling\u201d) trial, both S\u0302S and S\u0302A identified x\n1 as the most important feature in fewer cases than either of the other trials. The degradation is most notable for S\u0302S . Meanwhile, S\u0302SU performed at approximately the same level as on the other trials. The amount by which S\u0302SU outperforms the other methods on the third trial appears to be independent of the number of training examples. These results suggest that when the training data is not representative of the entire domain and unlabeled examples are available, using them in scoring can be very beneficial. When no unlabeled examples are available, S\u0302A provides a more reliable measure of each feature\u2019s importance than S\u0302S .\n5.3. Breast Cancer Results\nHere we test the performance of S\u0302S and S\u0302A on a realworld data set: Breast Cancer, available from the UCI repository (http://archive.ics.uci.edu/ml/ datasets/). The task is to predict, based on 9 discrete features (each taking values in the set {1, 2, . . . , 10}), whether a tumor is malignant or benign.\nThe distribution of feature values is far from uniform: averaged across all features, 46.2% of values are 1, while only 1.1% take the value 9, and the values 6,7,8 each occur with frequency below 4%. Because we wished to see how S\u0302S and S\u0302A performed when presented with limited training data (in particular, as few as ten labeled examples), this meant that for any given training set, it was likely that most of the possible feature values would not be present in the training data.\nTo alleviate this problem, we converted each feature into a binary attribute according to the mapping f given by: f(x) = ( I(x1 \u2264 5), I(x2 = 1), I(x3 = 1), I(x4 = 1),\nI(x5 \u2264 2), I(x6 = 1), I(x7 \u2264 3), I(x8 = 1), I(x9 = 1) ) .\nOur basic classifier achieved a leave-one-out error rate of 23/699 on the original data set. On the transformed data with binary features, this rate was 24/699, indicating that for a Naive Bayes classifier, effectively no predictive power is lost by our transformation.\nIn order to easily evaluate S\u0302S and S\u0302A, we augmented the data set by adding three purely noisy binary features. We conducted experiments on sets of size 10, 20, . . . , 100. For each size, 20 runs were conducted. A run consisted of sampling a set of positive and negative examples with replacement from the full data set.\nGiven a training set of n examples, n classifiers were trained, each using all but one example to compute estimates for P\u0302(y = 1|x) and P\u0302(y = 1|x\u2212j) on the held-out data point. These estimates were used to compute S\u0302S and S\u0302A, thereby obtaining a ranking of the features.\nFor each training size, we computed the aggregate rank for each feature by ranking them on the basis of their average rank across all 20 runs. As shown in Table 1, for all training sizes and both scores, the three uninformative features were among the five features with the lowest aggregate rank. Additionally, across all runs and training sizes, neither score ever ranked one of our dummy features as its top choice.\nWhen provided with at least 30 training examples, S\u0302S\nranked all three noisy features among the bottom four on all 20 runs. S\u0302A did not perform quite as well: even with 100 training examples, on two of 20 runs one of the uninformative features was ranked as highly as fifth.\nIt is difficult to infer much from the ranking of the original features, because all of them are at least weakly predictive of the class label. S\u0302S and S\u0302A generally agreed that feature 9 was the least useful of the original features. As one might hope, when we tested the performance of classifiers trained on each possible pairs of features, those using x9 performed least well.\n6. Conclusions\nIn this paper, we have considered the feature scoring algorithm presented by Shen et al. (2008) and proposed our own related score for use in feature selection tasks. We focused on these techniques because they consider the importance of each variable in the context of others and can score variables even in highdimensional contexts where each feature\u2019s impact on the final prediction is small. Our primary contribution is a careful analysis of Shen et al. (2008)\u2019s score, SS , and our alternative criterion SA.\nWe demonstrated that the quantity SS(j) is the expected conditional mean absolute deviation of P(y = 1|x) given x\u2212j , and that SA(j) is the expected conditional variance of P(y = 1|x) given x\u2212j . These proofs suggest that each score is a reasonable criterion for feature selection, as SS and SA select as important the features whose values have the greatest influence on P(y = 1|x).\nAs alternative justification for SS we proved that SS(j) provides an upper-bound for the improvement in accuracy of the Bayes-optimal classifier due to the information provided by the jth feature. Additionally, we hypothesized that the approximation S\u0302S could benefit from unlabeled examples. For a simple synthetic task, we confirmed that this data improved the robustness of S\u0302S to variations in the way that the labeled examples were sampled.\nWe motivated our score, S\u0302A(j), by observing that it measures both the magnitude and the sign of changes in estimated class probabilities due to the jth feature. We proved that using S\u0302A to eliminate features from the data set is equivalent to minimizing total loss on the training set when using an L1 loss function. For the problem described in Section 5.2, we concluded that when there was no unlabeled data available to supplement the training set, S\u0302A was less sensitive than S\u0302S to sampling variations in the training data.\n7. Future Work\nWe view this paper as a beginning, rather than conclusive, investigation of feature selection using probabilistic outputs. As such, there are many interesting directions for future work.\nMuch of the analysis here pertains to the quantities SS and SA, but in practice we are forced to use approximations. An open question is the extent to which the approximations used in this paper are \u201cgood.\u201d One way to quantify this would be to give sufficient conditions for these estimates to converge to SS and SA as the number of training examples grows.\nWe argued in Section 5 that the fact that S\u0302A incorporates the sign of changes in predictions should help to mitigate the presence of bias due to modeling assumptions. One major goal for the future is to validate this prediction, either empirically or theoretically.\nThe eventual goal of this work is to develop the theory of feature selection using probabilistic outputs to the point where, given a data set, we can choose a feature scoring algorithm that is likely to perform well in the specified domain. In order to accomplish this, we hope\nto perform tests on real-world data to determine the extent to which the theory developed in this paper extends to different learning algorithms and domains.\nReferences\nGuyon, Isabelle and Elisseeff, Andre. An introduction to variable and feature selection. J. Mach. Learn. Res., 3:1157\u20131182, March 2003. ISSN 1532-4435.\nGuyon, Isabelle, Weston, Jason, Barnhill, Stephen, and Vapnik, Vladimir. Gene selection for cancer classification using support vector machines. Machine Learning, 46:389\u2013422, 2002. ISSN 0885-6125. 10.1023/A:1012487302797.\nHastie, Trevor and Tibshirani, Robert. Classification by pairwise coupling. The Annals of Statistics, 26 (2):pp. 451\u2013471, 1998. ISSN 00905364.\nKononenko, Igor. Semi-naive bayesian classifier. In Kodratoff, Yves (ed.), Machine Learning - EWSL91, volume 482 of Lecture Notes in Computer Science, pp. 206\u2013219. Springer Berlin / Heidelberg, 1991.\nMaldonado, Sebastia\u0301n and Weber, Richard. A wrapper method for feature selection using support vector machines. Inf. Sci., 179:2208\u20132217, June 2009. ISSN 0020-0255. doi: 10.1016/j.ins.2009.02.014.\nPlatt, J. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers, 10 (3), 1999.\nShen, Kai-Quan, Ong, Chong-Jin, Li, Xiao-Ping, and Wilder-Smith, Einar. Feature selection via sensitivity analysis of svm probabilistic outputs. Machine Learning, 70:1\u201320, 2008. ISSN 0885-6125. 10.1007/s10994-007-5025-7.\nVapnik, Vladimir N. Statistical Learning Theory. Wiley-Interscience, 9 1998. ISBN 9780471030034. URL http://amazon.com/o/ASIN/0471030031/.\nWeston, J. and Watkins, C. Support vector machines for multi-class pattern recognition. European Symposium on Artificial Neural Networks, 1999."}], "references": [{"title": "An introduction to variable and feature selection", "author": ["Guyon", "Isabelle", "Elisseeff", "Andre"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Guyon et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Guyon et al\\.", "year": 2003}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["Guyon", "Isabelle", "Weston", "Jason", "Barnhill", "Stephen", "Vapnik", "Vladimir"], "venue": "Machine Learning,", "citeRegEx": "Guyon et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Guyon et al\\.", "year": 2002}, {"title": "Semi-naive bayesian classifier. In Kodratoff, Yves (ed.), Machine Learning - EWSL91, volume 482 of Lecture Notes in Computer Science, pp. 206\u2013219", "author": ["Kononenko", "Igor"], "venue": null, "citeRegEx": "Kononenko and Igor.,? \\Q1991\\E", "shortCiteRegEx": "Kononenko and Igor.", "year": 1991}, {"title": "A wrapper method for feature selection using support vector machines", "author": ["Maldonado", "Sebasti\u00e1n", "Weber", "Richard"], "venue": "Inf. Sci.,", "citeRegEx": "Maldonado et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Maldonado et al\\.", "year": 2009}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["J. Platt"], "venue": "Advances in Large Margin Classifiers,", "citeRegEx": "Platt,? \\Q1999\\E", "shortCiteRegEx": "Platt", "year": 1999}, {"title": "Feature selection via sensitivity analysis of svm probabilistic outputs", "author": ["Shen", "Kai-Quan", "Ong", "Chong-Jin", "Li", "Xiao-Ping", "Wilder-Smith", "Einar"], "venue": "Machine Learning,", "citeRegEx": "Shen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2008}, {"title": "Support vector machines for multi-class pattern recognition", "author": ["J. Weston", "C. Watkins"], "venue": "European Symposium on Artificial Neural Networks,", "citeRegEx": "Weston and Watkins,? \\Q1999\\E", "shortCiteRegEx": "Weston and Watkins", "year": 1999}], "referenceMentions": [{"referenceID": 5, "context": "This paper investigates two feature-scoring criteria that make use of estimated class probabilities: one method proposed by Shen et al. (2008) and a complementary approach proposed below.", "startOffset": 124, "endOffset": 143}, {"referenceID": 5, "context": "We present a feature-scoring criterion based on the work of Shen et al. (2008) and develop a novel theoretical framework to analyze their score and ours.", "startOffset": 60, "endOffset": 79}, {"referenceID": 1, "context": "This can be prohibitively expensive, so a variety of methods have been developed to approximate the result of this process (Guyon et al., 2002; Maldonado & Weber, 2009).", "startOffset": 123, "endOffset": 168}, {"referenceID": 5, "context": "In this section we examine a feature-scoring method that incorporates class probabilities proposed by Shen et al. (2008), and introduce our modified featurescoring criterion.", "startOffset": 102, "endOffset": 121}, {"referenceID": 5, "context": "Shen et al. (2008) propose the following feature ranking criterion based on this idea.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "This method for generating features has since been adopted elsewhere, including by Shen et al. (2008).", "startOffset": 83, "endOffset": 102}, {"referenceID": 4, "context": ", which derives probability estimates using the technique proposed in (Platt, 1999) and an SVM implementation from LIBSVM.", "startOffset": 70, "endOffset": 83}, {"referenceID": 4, "context": "Support vector machines typically do not provide probability estimates, but Vapnik (1998), Hastie & Tibshirani (1998) and Platt (1999) have all proposed methods for using SVMs to generate probabilities.", "startOffset": 122, "endOffset": 135}, {"referenceID": 5, "context": "In this paper, we have considered the feature scoring algorithm presented by Shen et al. (2008) and proposed our own related score for use in feature selection tasks.", "startOffset": 77, "endOffset": 96}, {"referenceID": 5, "context": "In this paper, we have considered the feature scoring algorithm presented by Shen et al. (2008) and proposed our own related score for use in feature selection tasks. We focused on these techniques because they consider the importance of each variable in the context of others and can score variables even in highdimensional contexts where each feature\u2019s impact on the final prediction is small. Our primary contribution is a careful analysis of Shen et al. (2008)\u2019s score, SS , and our alternative criterion SA.", "startOffset": 77, "endOffset": 465}], "year": 2012, "abstractText": "This paper investigates two feature-scoring criteria that make use of estimated class probabilities: one method proposed by Shen et al. (2008) and a complementary approach proposed below. We develop a theoretical framework to analyze each criterion and show that both estimate the spread (across all values of a given feature) of the probability that an example belongs to the positive class. Based on our analysis, we predict when each scoring technique will be advantageous over the other and give empirical results validating our predictions.", "creator": "LaTeX with hyperref package"}}}