{"id": "1206.6397", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Communications Inspired Linear Discriminant Analysis", "abstract": "We study the problem of supervised linear dimensionality reduction, taking an information-theoretic viewpoint. The linear projection matrix is designed by maximizing the mutual information between the projected signal and the class label (based on a Shannon entropy measure). By harnessing a recent theoretical result on the gradient of mutual information, the above optimization problem can be solved directly using gradient descent, without requiring simplification of the objective function. Theoretical analysis and empirical comparison are made between the proposed method and two closely related methods (Linear Discriminant Analysis and Information Discriminant Analysis), and comparisons are also made with a method in which Renyi entropy is used to define the mutual information (in this case the gradient may be computed simply, under a special parameter setting). Relative to these alternative approaches, the proposed method achieves promising results on real datasets.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (326kb)", "http://arxiv.org/abs/1206.6397v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["minhua chen", "william r carson", "miguel r d rodrigues", "lawrence carin", "a robert calderbank"], "accepted": true, "id": "1206.6397"}, "pdf": {"name": "1206.6397.pdf", "metadata": {"source": "META", "title": "Communications Inspired Linear Discriminant Analysis", "authors": ["Minhua Chen", "William Carson", "Miguel Rodrigues", "Robert Calderbank"], "emails": ["minhua.chen@duke.edu", "william.carson@paconsulting.com", "m.rodrigues@ee.ucl.ac.uk", "robert.calderbank@duke.edu", "lcarin@duke.edu"], "sections": [{"heading": "1. Introduction", "text": "The analysis of high-dimensional data is of interest in many applications. To reduce the cost of data processing, and to increase the interpretability of the data, one typically employs dimensionality reduction as a pre-processing step. It also plays the role of regularization for the data. Although nonlinear dimensionality reduction methods (Tenenbaum et al., 2000; Song et al., 2007) have become popular recently, linear di-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nmensionality reduction methods still play an important role, mainly due to their simplicity. Linear dimensionality reduction based on random projections has gained significant attention recently, as a result of success in compressive sensing (Candes & Wakin, 2008) and other applications (Liu & Fieguth, 2012). However, random projections may not be the best choice if we know the statistical properties of the underlying signal (Duarte-Carvajalino & Sapiro, 2009). Hence, an important question to be answered is how to design the projection matrix so that the measurement is the most informative.\nIn this paper we focus on projection design for classification, or supervised dimensionality reduction. Linear Discriminant Analysis (LDA) (Fisher, 1936) is one of the most important supervised dimensionality reduction methods. The design criterion of LDA maximizes the between-class scattering while minimizing the within-class scattering of the projected data, with these two criteria addressed simultaneously. It has been proven that under mild conditions this criterion is Bayes optimal (Hamsici & Martinez, 2008). However, this method has two disadvantages. First, the dimensionality of the projected space in LDA can only be less than the number of data classes, which greatly restricts its applicability. Second, LDA only uses first and second order statistics of the data, ignoring higherorder information. To overcome these two disadvantages, other criteria have been proposed in the literature (Tao et al., 2009), out of which an important category is the information-theoretic criterion.\nIn the information-theoretic approach, the projection matrix is designed by maximizing the mutual information (MI) between the projected signal and the class label (Torkkola, 2003; 2001; Nenadic, 2007; Kaski & Peltonen, 2003; Hild et al., 2006). Intuitively, the larger\nthe mutual information is, the better it is for the projected signal to recover the label information. Theoretically, the Bayes classification error is bounded by the MI (Nenadic, 2007) (based on a Shannon entropy measure). However, the MI is not easy to calculate, posing a significant obstacle to its optimization. Almost all existing information-theoretic-based algorithms seek an approximation to the Shannon MI, hence compromising the objective function. For example, in recent studies (Torkkola, 2003; 2001; Hild et al., 2006), the quadratic mutual information (with quadratic Re\u0301nyi entropy) is used instead of the Shannon-based MI; this is because with the use of quadratic Re\u0301nyi entropy, the gradient of MI can be calculated analytically under the assumption of a Gaussian mixture model (GMM) signal model. In the work of (Kaski & Peltonen, 2003), the Shannon MI is approximated by its empirical estimation on the training data. Using Information Discriminant Analysis (IDA) (Nenadic, 2007), the entropy of the GMM in the MI calculation, where the higher-order information comes into play, is approximated with the entropy of a global Gaussian distribution, which again loses the higher-order information. The LDA method, although not proposed under the information-theoretic criterion, can also be viewed as an approximation to the MI objective function.\nThe main contribution of this paper is to show that the use of Shannon MI optimization, for linear feature design in classification, can be solved directly, without compromising or simplifying the objective function. The key tool is a theoretical result that recently appeared in the communications literature, which gives an explicit expression for the gradient of Shannon MI with respect to the projection matrix in linear vector Gaussian channels (Palomar & Verdu, 2006). This theorem has found applications in the area of precoder design for communication systems (Xiao et al., 2011; Carson et al., 2012), but is not widely appreciated in the machine learning and signal processing communities, except for a few papers on optical imaging system design (Ashok et al., 2008; Baheti & Neifeld, 2009). Our paper is the first to apply this theorem to the supervised dimensionality reduction problem. As a result, we obtain a new explicit expression for the gradient of the Shannon MI objective function for any input signal distribution, which is not achieved in any of the methods mentioned above. Consequently, numerical optimization methods (e.g., gradient descent) can be applied. Since we make no assumptions on the input signal distribution in each class, our analytical result is very general and can be applied to a broad spectrum of applications. Additionally, we perform a theoretical analysis of this design metric, providing\nnew insights. To connect to the quadratic mutual information approach (Torkkola, 2003; 2001), we adopt the mixture-of-GMMs signal model."}, {"heading": "2. Main Result", "text": "Suppose the label and data are generated i.i.d. via the following process: c \u223c Mult(c; 1,w); x|c \u223c p(x|c) where w \u2208 RM\u00d71 is the prior distribution on the M classes, x \u2208 Rp\u00d71 is the original signal, and p(x|c) is the data distribution for class c. Hence the joint density is p(x, c) = wcp(x|c), and the global signal density can be written as\np(x) = M\u2211 m=1 wmp(x|m). (1)\nHere we make no assumption on the form of p(x|m); hence the above signal model is very general. We do assume that w and p(x|m) are known or can be estimated from training data.\nIn supervised dimensionality reduction, we seek a projection matrix \u03a6 \u2208 Rd\u00d7p such that the projected signal\ny = \u03a6x+ (2)\nis the most informative in identifying the underlying class label c. We assume the measurement noise is Gaussian, i.e., p(y|x) = N (y; \u03a6x,R\u22121) where R is the known noise precision matrix. We adopt the information-theoretic criterion (Nenadic, 2007) as\nmax \u03a6\nI(C;Y ) s.t. \u03a6\u03a6> = Id (3)\nwhere C and Y represent c and y as random variables, I(C;Y ) denotes the MI, and the orthonormality constraint is common in the literature (Nenadic, 2007). Intuitively, the larger the MI is, the better it is for the projected signal y to predict the latent class label c. Theoretically, there is also a strong justification for the above criterion. The Bayes classification error, defined as Pe = \u222b p(y)(1 \u2212 maxc p(c|y))dy, can be bounded by I(C;Y ) as follows (Hellman & Raviv, 1970; Fano, 1961; Nenadic, 2007)\nH(C|Y )\u2212H(Pe) logM \u2264 Pe \u2264 1 2 H(C|Y ) (4)\nwhere H(C|Y ) = H(C)\u2212I(C;Y ) and 0 \u2264 H(Pe) \u2264 1. Hence, the smaller H(C|Y ) the tighter the bound will be for Pe, and minimizing H(C|Y ) corresponds to maximizing I(C;Y ). Note that (4) is based on a Shannon definition of entropy (e.g., not a Re\u0301nyi entropy measure (Torkkola, 2003), with a comparison to results based on Re\u0301nyi entropy discussed below). Unless\nstated otherwise, all measures of entropy and differential entropy discussed below are based on a Shannon definition (Cover & Thomas, 2006).\nIn order to solve the optimization problem in (3), we first introduce a theoretical result that appeared in the communications literature:\nTheorem 1. (Palomar & Verdu, 2006) Given the measurement model in (2), the gradient of mutual information I(X;Y ) with respect to the projection matrix \u03a6 can be expressed as\n\u2207\u03a6I(X;Y ) = R\u03a6\u03a3 (5)\nwhere \u03a3 = \u222b p(y) \u222b p(x|y)(x\u2212 xy)(x\u2212 xy)>dxdy is\nthe MMSE matrix, and xy = \u222b xp(x|y)dx is the posterior mean.\nThis theorem provides a connection between information theory and estimation theory, by linking the gradient of mutual information to the MMSE matrix. It has found applications in precoder design for communications systems (Xiao et al., 2011; Carson et al., 2012). However, the power of this theorem has not been widely applied in the machine learning and signal processing communities. The only studies we found are (Ashok et al., 2008; Baheti & Neifeld, 2009) which use the above theorem to design optical imaging systems. This paper is the first work to apply and extend the above theorem to the supervised lineardimensionality-reduction problem. Our main result is summarized in the following new theorem:\nTheorem 2. Given the measurement model in (2) and the multi-class signal model in (1), the gradient of mutual information I(C;Y ) with respect to the projection matrix \u03a6 can be expressed as\n\u2207\u03a6I(C;Y ) = R\u03a6\u03a3\u0303 (6)\nwith the equivalent MMSE matrix \u03a3\u0303 expressed as\n\u03a3\u0303 = \u03a3\u2212 M\u2211\nm=1\nwm\u03a3m (7)\n= M\u2211 m=1 wm \u222b p(y|m)(xy(m)\u2212 xy)(xy(m)\u2212 xy)>dy\nwhere \u03a3 is the global MMSE matrix with input distribution p(x) and posterior mean xy, and \u03a3m is the local MMSE matrix with input distribution p(x|m) and posterior mean xy(m).\nProof. Since I(C;Y ) = h(Y )\u2212 h(Y |C) = I(X;Y )\u2212 I(X;Y |C) and p(x) = \u2211M m=1 wmp(x|m), according\nto Theorem 1, \u2207\u03a6I(C;Y ) is equal to\n\u2207\u03a6I(X;Y )\u2212\u2207\u03a6I(X;Y |C)=R\u03a6(\u03a3\u2212 M\u2211\nm=1\nwm\u03a3m)\nwhere \u03a3 and \u03a3m are the global and local MMSE matrix with input distribution p(x) and p(x|m) respectively. From Bayes rule,\np(x|y) = p(x)p(y|x) p(y) = \u2211M m=1 wmp(x|m)p(y|x)\u2211M\nm=1 wmp(y|m)\n= \u2211M m=1 wmp(y|m)p(x|y,m)\u2211M\nm=1 wmp(y|m) = M\u2211 m=1 w\u0303mp(x|y,m)\nw\u0303m = p(m|y) = wmp(y|m)\u2211M m\u2032=1 wm\u2032p(y|m\u2032) = wmp(y|m) p(y) ;\np(x|y,m) = p(x|m)p(y|x) p(y|m) ; xy(m) =\n\u222b xp(x|y,m)dx\nxy =\n\u222b xp(x|y)dx = M\u2211 m=1 w\u0303mxy(m).\n(8)\nConsequently, we have\n\u03a3= \u222b p(y) \u222b M\u2211 m=1 w\u0303mp(x|y,m)(x\u2212xy)(x\u2212xy)>dxdy\n= \u222b M\u2211 m=1 wmp(y|m)( \u222b p(x|y,m)(x\u2212xy(m))(x\u2212xy(m))>dx\n+ (xy(m)\u2212xy)(xy(m)\u2212xy)>)dy\n= M\u2211 m=1 wm(\u03a3m+ \u222b p(y|m)(xy(m)\u2212xy)(xy(m)\u2212xy)>dy)\nsince \u03a3m = \u222b p(y|m) \u222b p(x|y,m)(x \u2212 xy(m))(x \u2212 xy(m)) >dxdy and p(y)w\u0303m = wmp(y|m). Consequently, equation (7) is proved.\nThe significance of Theorem 2 is that we obtain an explicit expression for the gradient of the MI objective function in (3) under any input signal distribution (1). Consequently, numerical optimization methods (e.g., gradient descent) can be applied to solve (3). The equivalent MMSE matrix in (7) can be computed via Monte Carlo simulation and Bayesian inference (we discuss in Section 4 how we do this in practice). Our analytical result in Theorem 2 is very general, in the sense that we make no assumption on the signal distribution in each class. The algorithm can be summarized in the following steps:\n1. Obtain the input signal distribution in (1) from training data. Initialize \u03a6.\n2. Compute the equivalent MMSE matrix in (7) via Monte Carlo simulation.\n3. Compute the gradient in (6) and update the projection matrix as \u03a6 \u2190 orth(\u03a6 + \u03b7\u2207\u03a6I(C;Y )), where \u03b7 is the step size and orth(A) means projecting A to an orthonormal matrix.\n4. If converge, stop. Otherwise, go to step 2."}, {"heading": "3. Theoretical Analysis", "text": "The orthonormal constraint on the projection matrix complicates the theoretical analysis of the optimal design. By relaxing this constraint and instead considering a power constraint we can leverage more results from communications and recent work in image reconstruction (Carson et al., 2012). The relaxed problem is\nmax \u03a6\nI(C;Y ) s.t. 1 d tr ( \u03a6\u03a6> ) = 1 (9)\nwhere the trace constraint ensures that the rows of the projection matrix have on average unit-norm.\nThe following theorem characterizes the optimal projection matrix for the relaxed problem in terms of the singular value decompositions (SVD) of the noise covariance R\u22121 = U>R D \u22121 R UR and the equivalent MMSE matrix \u03a3\u0303 = U\u03a3\u0303 D\u03a3\u0303 U > \u03a3\u0303 . Theorem 3. Given the measurement model in (2) and the multi-class signal model in (1), the projection matrix \u03a6 which optimizes the relaxed problem in (9) can be expressed via its SVD as \u03a6? = U?\u03a6 D ? \u03a6 V ?> \u03a6 where D?\u03a6 is a square diagonal matrix of optimal singular values and the orthonormal matrices of optimal singular vectors are U?\u03a6 = UR and V ? \u03a6 = U\u03a3\u0303? \u03a0\n? for some optimal permutation matrix \u03a0?.\nProof. From the KKT optimality conditions we know \u2207\u03a6 { \u2212I(C;Y )\u2212 \u03b7 \u00b7 [ 1\u2212 1 d tr ( \u03a6\u03a6> ) ]} \u2223\u2223\u2223\u2223 \u03a6=\u03a6?\n= \u2212R \u03a6?\u03a3\u0303? + 2\u03b7 d \u00b7\u03a6? = 0\nwhere the Lagrange multiplier \u03b7 \u2265 0, \u03a3\u0303? is the equivalent MMSE matrix associated with the optimal projection matrix \u03a6? and we have used the gradient result in Theorem 2. The optimal projection matrix must therefore also satisfy\n2 \u03b7\nd \u00b7\u03a6?\u03a6?> = R\n( \u03a6?\u03a3\u0303?\u03a6?> ) . (10)\nThe left-hand side of this equation is symmetric and is diagonalized by U?>\u03a6 , which means that matrices R and \u03a6?\u03a3?\u03a6?> commute and are simultaneously diagonalized by U?>\u03a6 . We can therefore write without loss of generality the optimal unitary matrices as\nU?\u03a6 = UR \u03a0 ? UDU ; V ? \u03a6 = U\u03a3\u0303?\u03a0 ? VDV\nwhere DU and DV are diagonal matrices with unit modulus diagonal elements, and \u03a0?U and \u03a0 ? V are permutation matrices. Noting that the action of the two permutation matrices can be captured by a single permutation matrix \u03a0? and both mutual information and the MMSE matrix are independent of the unit modulus matrices, the result follows.\nThe characterization in Theorem 3 of the projection matrix for the relaxed problem provides possible solutions to (3). For example, by setting the diagonal matrix D?\u03a6 to be the identity matrix we satisfy the orthonormal constraint on the projection matrix. This could be useful in the implementation of the gradient descent algorithm. For example,\n\u2022 Theorem 3 takes the form of a fixed-point equation and could be used as stopping criteria in the proposed algorithm that indicates convergence.\n\u2022 It is not known whether the mutual information is concave in \u03a6, Theorem 3 suggests an alternative (or extension) to gradient descent that could help avoid local optima.\n\u2022 The projection matrix now consists of two rotation matrices, one of which always diagonalizes the noise which simplifies the calculation of the gradient.\nA solution to the relaxed problem will necessarily be better than or equal to a solution to (3), since the constraint in (3) is a subset of that in (9) . Therefore the mutual information can be further improved by optimization over the singular values of the projection matrix. In the signal reconstruction scenarios in communications and image reconstruction, the mutual information is known to be concave in the squared singular values of the projection matrix when U?\u03a6 = UR (Carson et al., 2012). This property can be used to give guarantees on convergence. However, the mutual information is not concave in this scenario. Nevertheless, by inserting the result for the optimal orthonormal matrices back into (10), the optimal squared singular values of the projection matrix satisfy\n2 \u03b7\nd D2?\u03a6 = D 2? \u03a6 D ? R\n( \u03a0?>D? \u03a3\u0303 \u03a0? ) .\nThe equivalent MMSE matrix is a function of the projection matrix and therefore either [ D2?\u03a6 ] ii\nis chosen to satisfy\n2 \u03b7\nd = [D?R]ii\n[ \u03a0?>D? \u03a3\u0303 \u03a0? ] ii\nor if no solution exists we choose [ D2?\u03a6 ] ii\n= 0. Note that from (7) in Theorem 2 we know that the equivalent MMSE matrix is positive semi-definite."}, {"heading": "4. Mixture of GMMs Signal Model", "text": "In this section we focus on a specific signal input distribution, the mixture-of-GMMs signal model, in which signal from each class m is modeled as a Gaussian Mixture Model (GMM), i.e.,\np(x|m) = Om\u2211 o=1 \u03c0moN (x;\u00b5mo,\u2126mo) (11)\nwhere Om is the number of Gaussian components for class m. As a result, the density in (1) reduces to\np(x) = M\u2211 m=1 wm Om\u2211 o=1 \u03c0moN (x;\u00b5mo,\u2126mo)\nwhich is the mixture-of-GMMs signal model.\nUnder this specific signal model, the general Bayesian inference in (8) reduces to the inference of x under GMM priors. According to (Chen et al., 2010), the detailed Bayesian inference can be derived as\np(x|y)= M\u2211\nm=1\nw\u0303mp(x|y,m)\np(x|y,m)= Om\u2211 o=1 \u03c0\u0303moN (x; \u00b5\u0303mo, \u2126\u0303mo) \u2126\u0303mo=(\u03a6 >R\u03a6+\u2126\u22121mo) \u22121; \u00b5\u0303mo=\u2126\u0303mo(\u03a6 >Ry+\u2126\u22121mo\u00b5mo)\np(y|m)= Om\u2211 o\u2032=1 \u03c0mo\u2032N (y; \u03a6\u00b5mo\u2032 ,\u03a6\u2126mo\u2032\u03a6>+R\u22121) \u03c0\u0303mo =\u03c0moN (y; \u03a6\u00b5mo,\u03a6\u2126mo\u03a6>+R\u22121)/p(y|m) w\u0303m = wmp(y|m)\u2211M\nm\u2032=1 wm\u2032p(y|m\u2032) .\nThe marginal density p(y) = \u2211M\nm\u2032=1 wm\u2032p(y|m\u2032) expressed in the denominator of w\u0303m is also a mixture of GMM. The Matrix Inversion Lemma can be used to expedite the computations. Using the above equations, the equivalent MMSE matrix in (7) can be readily computed via Monte Carlo draws from p(y), with xy and xy(m) provided by the above inference. Moreover, the inference naturally induces a Bayes classifier maxc p(c|y) where p(c|y) = w\u0303c. We will use this mixture-of-GMMs signal model and the induced Bayesian classifier in the experiments."}, {"heading": "5. Related Methods", "text": "Information-theoretic supervised dimensionality reduction was studied in (Torkkola, 2003; 2001). Instead of using Shannon entropy to define the mutual information, they used quadratic Re\u0301nyi entropy to define a quadratic mutual information as\nIT (C;Y ) = \u2211\nc\n\u222b (p(y, c)\u2212 p(y)p(c))2dy\nwhere p(c) = wc, and p(y) is a mixture-of-GMMs defined in the same way as that in Section 4. The main advantage of using quadratic Re\u0301nyi entropy is that the quadratic mutual information and its derivative can be expressed analytically for the GMM signal model without Monte Carlo simulations, due to the following property of Gaussian:\u222b p(y|m)p(y|c)dy=\n\u222b Om\u2211 o=1 \u03c0moN (y;\u03a6\u00b5mo,\u03a6\u2126mo\u03a6>+R\u22121)\n\u00d7 Oc\u2211 r=1 \u03c0crN (y; \u03a6\u00b5cr,\u03a6\u2126cr\u03a6>+R\u22121)dy\n= Om\u2211 o=1 Oc\u2211 r=1 \u03c0mo\u03c0crN (0;\u03a6(\u00b5mo\u2212\u00b5cr),\u03a6(\u2126mo+\u2126cr)\u03a6>+2R\u22121).\nIn this paper, we will use a similar but different definition of quadratic mutual information\nI2(C;Y ) = h2(Y )\u2212 M\u2211\nm=1\nwmh2(Y |m)\nwhere h2(Y ) = \u2212 log \u222b p(y)2dy is the quadratic Re\u0301nyi entropy. This definition is more relevant to our Shannon entropy based approach, since by replacing h2(Y ) with h(Y ), I2(C;Y ) reduces to I(C;Y ). The optimization of I2(C;Y ) is also straightforward, since the gradient can be expressed analytically due to the above property of Gaussians. We will compare this Re\u0301nyi entropy based approach to our Shannon entropy based approach in the experiments. We emphasize that both IT (C;Y ) and I2(C;Y ) are approximations to I(C;Y ) for the sake of optimization, hence they cannot satisfy the bound in (4).\nThe Information Discriminant Analysis (IDA) (Nenadic, 2007) and Linear Discriminant Analysis (LDA) (Fisher, 1936) are derived under GMM signal model, which is a simplification and a special case of the mixture-of-GMMs signal model discussed in Section 4. It is interesting to compare our method with these two quantitatively. In the GMM signal model, the signal distribution in each class is modeled as a single Gaussian, i.e., Om = 1 in (11) for all m. Hence p(x|m) = N (x;\u00b5m,\u03a3m) and p(x) =\u2211M\nm=1 wmN (x;\u00b5m,\u03a3m). Thus the Bayesian inference\nin Section 4 can be further simplified. Under this simplified model assumption, the MI objective function in (3) can be expressed as\nI(C;Y ) = h(Y )\u2212 M\u2211\nm=1\nwmh(Y |m)\n= h(Y )\u2212 1 2 M\u2211 m=1 wm log((2\u03c0e) d det(\u03a6\u2126m\u03a6 > +R\u22121)).\nAs illustrated above, p(y) is also a GMM whose entropy cannot be expressed analytically. To overcome this problem, IDA approximates h(Y ) with a single Gaussian entropy with the same covariance matrix as the GMM p(y), hence the objective function can be expressed as IIDA(C;Y ) = 1\n2 log((2\u03c0e)d det(\u03a6\u2126\u03a6> +R\u22121))\n\u2212 1 2 M\u2211 m=1 wm log((2\u03c0e) d det(\u03a6\u2126m\u03a6 > +R\u22121))\nwhere \u2126 = \u2211M\nm=1 wm(\u2126m+(\u00b5m\u2212\u00b5)(\u00b5m\u2212\u00b5)>) is the prior covariance matrix for x and \u00b5 = \u2211M m=1 wm\u00b5m is the prior mean. Then the optimization of IIDA(C;Y ) can be solved via gradient descent (Nenadic, 2007).\nThe LDA method (Fisher, 1936) simultaneously maximizes the between-class scattering and minimizes the within-class scattering of the projected data. It has been proven that under mild conditions this criterion is Bayes optimal (Hamsici & Martinez, 2008). The LDA criterion can be expressed as ILDA(C;Y ) = 1\n2 log((2\u03c0e)d det(\u03a6\u2126\u03a6> +R\u22121))\n\u2212 1 2 log((2\u03c0e)d det(\u03a6( M\u2211 m=1 wm\u2126m)\u03a6 > +R\u22121)).\nAn analytical solution can be found for maximizing ILDA(C;Y ), however the solution only permits the number of projections d to be less than the class number M .\nIt is easy to prove that IIDA(C;Y ) \u2265 I(C;Y ) (maximum entropy principle) (Nenadic, 2007) and IIDA(C;Y ) \u2265 ILDA(C;Y ) (concavity of log det(\u00b7)). Clearly, only I(C;Y ) is the exact informationtheoretic principle satisfying the Bayes error bound in (4), while the other two are approximations to the MI objective function. Another advantage of our method is that the higher-order information of the signal distribution is preserved in the objective function via h(Y ), while the other two methods only use first and second order statistics of the data. Even though I(C;Y ) cannot be expressed analytically, the optimization can still be done using the tool developed in Section 2."}, {"heading": "6. Experiments", "text": "We test our method on three real datasets: Satellite, Letter and USPS. The first two are used in IDA (Nenadic, 2007) and can be downloaded from the UCI Machine Learning Repository. The third one is a standard digit recognition dataset with higher feature dimensions, which can also be downloaded from the Internet. A detailed description of the three datasets is as follows:\n1. The 36-dimensional feature vectors in the Satellite data consist of pixel values of a 3\u00d73 neighborhood in 4 spectral channels. The label for the central pixel belongs to one of the following six classes: real soil, cotton crop, grey soil, damp grey soil, soil with vegetation stubble and very damp grey soil. The training set contains 4435 samples, and the testing set contains 2000 samples.\n2. The Letter data contains 16-dimensional feature vectors (statistical moments and edge counts) extracted from character images for the 26 capital letters (A to Z) with different fonts and random distortions. The training set has 16000 such stimuli and the testing set 4000.\n3. The USPS data contains grey scale images of dimension 16 \u00d7 16 = 256 for handwritten digits (0 \u223c 9). There are 7291 training samples and 2007 testing samples.\nThe mixture-of-GMMs signal model is used, and the GMM density for each class is learned on the training data via the EM algorithm. Dirichlet Process (Blei & Jordan, 2006) GMM learning with variational Bayes inference was also tried to infer the mixture-of-GMMs model, yielding similar results. Two settings for the number of Gaussian components (Om) are considered: Om = 1 for all m, which reduces to the GMM signal model, and Om = 10 for all m. The noise covariance matrix R\u22121 in (2) is set to be very small (10\u22126Id). Four dimensionality reduction methods are considered: LDA, IDA, the quadratic Re\u0301nyi entropy based method with objective function I2(C;Y ), and the proposed Shannon entropy based method. For the proposed method, 2000 Monte Carlo particles are simulated to compute the equivalent MMSE matrix, and the step size for the gradient descent is set to be 0.01. The Bayes classifier maxc p(c|y) is employed using the learned signal model. The results are summarized in the following tables.\nWe observe that for all cases, the proposed method either gives the best performance, or is very near to the best. The state-of-art performance on the USPS\ndata was obtained in (Tao et al., 2009). By adopting a nearest neighborhood rule-based classifier, they obtained classification accuracies of 0.7259, 0.8672, 0.8991 and 0.9182 using 3, 5, 7 and 9 designed projections respectively. Comparing to results in the table, we see that the proposed method is very competitive. Our strong result on this USPS dataset gives confidence to our method in general. The reason for our good performance is that we are directly maximizing I(C;Y ), which bounds the Bayes classification error Pe through (4). The larger I(C;Y ) is, the smaller the upper bound of Pe will be. All other objective functions (ILDA(C;Y ), IIDA(C;Y ) and I2(C;Y )) are approximations to I(C;Y ), hence their performances are generally weaker.\nWe also observe that the performance using the mixture-of-GMMs signal model (Om = 10) is generally better than that of the GMM signal model (Om = 1), which is most obvious for the Letter dataset. This is\nbecause the mixture of GMM can model the data more precisely, which effectively improves the projection design and the Bayes classification.\nThe LDA and IDA method assume a GMM signal model (Om = 1), hence the mixture of GMM signal model (Om = 10) will not affect the projection design for LDA and IDA. However, as explained earlier, a finer signal model can help improve the classification performance. This is why we often observe a higher classification accuracy in LDA(10) (or IDA(10)) than that in LDA(1) (or IDA(1)), even though the designed projection matrices are exactly the same in the two cases; the brackets (\u00b7) indicate the number of GMM mixture components.\nIDA performs poorly on the USPS data. This is because the global Gaussian approximation made in the IIDA(C;Y ) objective function may not be appropriate for the USPS data with so much heterogeneity. The performance of the quadratic Re\u0301nyi entropy based method is competitive, especially on the USPS dataset when d \u2264 4. However, it is generally not as good as the proposed method, for reasons explained earlier. For all three datasets we also considered random projections designed based on draws from N (0, 1) with orthonormalization, and those were significantly worse than those of LDA, IDA, Renyi and the proposed method.\nIn summary, the performance of the proposed method is very promising. Its computational load is heavier than LDA and IDA, but the performance gain warrants the effort. Moreover, the projection design is done offline, so the testing speed will not be affected."}, {"heading": "7. Conclusion", "text": "By harnessing a recent theoretical result on the gradient of MI with respect to the projection matrix (Palomar & Verdu, 2006), we have derived a new counterpart theorem for supervised dimensionality reduction. As a result, the Shannon MI objective function can be optimized directly without any approximation. We compared the proposed method to LDA, IDA and a quadratic Re\u0301nyi entropy based method, both theoretically and empirically. Results on real datasets show the advantage of the proposed method. This study can be viewed as an example of how a research product from one area (communications theory) can benefit research in a seemingly different area (machine learning)."}, {"heading": "Acknowledgement", "text": "The research reported here was supported by ARO, NGA, ONR and DARPA (KeCom program)."}], "references": [{"title": "Compressive imaging system design using task-specific information", "author": ["A. Ashok", "P. Baheti", "M. Neifeld"], "venue": "Applied Optics,", "citeRegEx": "Ashok et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ashok et al\\.", "year": 2008}, {"title": "Recognition using informationoptimal adaptive feature-specific imaging", "author": ["P. Baheti", "M. Neifeld"], "venue": "Journal of the Optical Society of America A,", "citeRegEx": "Baheti and Neifeld,? \\Q2009\\E", "shortCiteRegEx": "Baheti and Neifeld", "year": 2009}, {"title": "Variational inference for dirichlet process mixtures", "author": ["D. Blei", "M. Jordan"], "venue": "Bayesian Analysis,", "citeRegEx": "Blei and Jordan,? \\Q2006\\E", "shortCiteRegEx": "Blei and Jordan", "year": 2006}, {"title": "An introduction to compressive sampling", "author": ["E. Candes", "M. Wakin"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Candes and Wakin,? \\Q2008\\E", "shortCiteRegEx": "Candes and Wakin", "year": 2008}, {"title": "How to focus the discriminative power of a dictionary", "author": ["W. Carson", "M. Rodrigues", "M. Chen", "L. Carin", "R. Calderbank"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Kyoto, Japan,", "citeRegEx": "Carson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Carson et al\\.", "year": 2012}, {"title": "Compressive sensing on manifolds using a nonparametric mixture of factor analyzers: Algorithm and performance bounds", "author": ["M. Chen", "J. Silva", "J. Paisley", "C. Wang", "D. Dunson", "L. Carin"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "Chen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "Learning to sense sparse signals: Simultaneous sensing matrix and sparsifying dictionary optimization", "author": ["J. Duarte-Carvajalino", "G. Sapiro"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "Duarte.Carvajalino and Sapiro,? \\Q2009\\E", "shortCiteRegEx": "Duarte.Carvajalino and Sapiro", "year": 2009}, {"title": "Transmission of Information: A Statistical theory of Communications", "author": ["R. Fano"], "venue": null, "citeRegEx": "Fano,? \\Q1961\\E", "shortCiteRegEx": "Fano", "year": 1961}, {"title": "The use of multiple measurements in taxonomic problems", "author": ["R. Fisher"], "venue": "Annals of Human Genetics,", "citeRegEx": "Fisher,? \\Q1936\\E", "shortCiteRegEx": "Fisher", "year": 1936}, {"title": "Bayes optimality in linear discriminant analysis", "author": ["O. Hamsici", "A. Martinez"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Hamsici and Martinez,? \\Q2008\\E", "shortCiteRegEx": "Hamsici and Martinez", "year": 2008}, {"title": "Probability of error, equivocation, and the chernoff bound", "author": ["M. Hellman", "J. Raviv"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Hellman and Raviv,? \\Q1970\\E", "shortCiteRegEx": "Hellman and Raviv", "year": 1970}, {"title": "Feature extraction using information-theoretic learning", "author": ["K. Hild", "D. Erdogmus", "K. Torkkola", "J. Principe"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Hild et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hild et al\\.", "year": 2006}, {"title": "Informative discriminant analysis", "author": ["S. Kaski", "J. Peltonen"], "venue": "In Proceedings of the Twentieth International Conference on Machine Learning (ICML), Washington DC,", "citeRegEx": "Kaski and Peltonen,? \\Q2003\\E", "shortCiteRegEx": "Kaski and Peltonen", "year": 2003}, {"title": "Texture classification from random features", "author": ["L. Liu", "P. Fieguth"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Liu and Fieguth,? \\Q2012\\E", "shortCiteRegEx": "Liu and Fieguth", "year": 2012}, {"title": "Information discriminant analysis: Feature extraction with an information-theoretic objective", "author": ["Z. Nenadic"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Nenadic,? \\Q2007\\E", "shortCiteRegEx": "Nenadic", "year": 2007}, {"title": "Gradient of mutual information in linear vector gaussian channels", "author": ["D. Palomar", "S. Verdu"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Palomar and Verdu,? \\Q2006\\E", "shortCiteRegEx": "Palomar and Verdu", "year": 2006}, {"title": "Colored maximum variance unfolding", "author": ["L. Song", "A. Smola", "K. Borgwardt", "A. Gretton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Song et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Song et al\\.", "year": 2007}, {"title": "Geometric mean for subspace selection", "author": ["D. Tao", "X. Li", "X. Wu", "S. Maybank"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Tao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tao et al\\.", "year": 2009}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J. Tenenbaum", "V. Silva", "J. Langford"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Learning discriminative feature transforms to low dimensions in low dimensions", "author": ["K. Torkkola"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Torkkola,? \\Q2001\\E", "shortCiteRegEx": "Torkkola", "year": 2001}, {"title": "Feature extraction by non-parametric mutual information maximization", "author": ["K. Torkkola"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Torkkola,? \\Q2003\\E", "shortCiteRegEx": "Torkkola", "year": 2003}, {"title": "Globally optimal linear precoders for finite alphabet signals over complex vector gaussian channels", "author": ["C. Xiao", "Y. Zheng", "Z. Ding"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "Xiao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 18, "context": "Although nonlinear dimensionality reduction methods (Tenenbaum et al., 2000; Song et al., 2007) have become popular recently, linear di-", "startOffset": 52, "endOffset": 95}, {"referenceID": 16, "context": "Although nonlinear dimensionality reduction methods (Tenenbaum et al., 2000; Song et al., 2007) have become popular recently, linear di-", "startOffset": 52, "endOffset": 95}, {"referenceID": 8, "context": "Linear Discriminant Analysis (LDA) (Fisher, 1936) is one of the most important supervised dimensionality reduction methods.", "startOffset": 35, "endOffset": 49}, {"referenceID": 17, "context": "To overcome these two disadvantages, other criteria have been proposed in the literature (Tao et al., 2009), out of which an important category is the information-theoretic criterion.", "startOffset": 89, "endOffset": 107}, {"referenceID": 20, "context": "In the information-theoretic approach, the projection matrix is designed by maximizing the mutual information (MI) between the projected signal and the class label (Torkkola, 2003; 2001; Nenadic, 2007; Kaski & Peltonen, 2003; Hild et al., 2006).", "startOffset": 164, "endOffset": 244}, {"referenceID": 14, "context": "In the information-theoretic approach, the projection matrix is designed by maximizing the mutual information (MI) between the projected signal and the class label (Torkkola, 2003; 2001; Nenadic, 2007; Kaski & Peltonen, 2003; Hild et al., 2006).", "startOffset": 164, "endOffset": 244}, {"referenceID": 11, "context": "In the information-theoretic approach, the projection matrix is designed by maximizing the mutual information (MI) between the projected signal and the class label (Torkkola, 2003; 2001; Nenadic, 2007; Kaski & Peltonen, 2003; Hild et al., 2006).", "startOffset": 164, "endOffset": 244}, {"referenceID": 14, "context": "Theoretically, the Bayes classification error is bounded by the MI (Nenadic, 2007) (based on a Shannon entropy measure).", "startOffset": 67, "endOffset": 82}, {"referenceID": 20, "context": "For example, in recent studies (Torkkola, 2003; 2001; Hild et al., 2006), the quadratic mutual information (with quadratic R\u00e9nyi entropy) is used instead of the Shannon-based MI; this is because with the use of quadratic R\u00e9nyi entropy, the gradient of MI can be calculated analytically under the assumption of a Gaussian mixture model (GMM) signal model.", "startOffset": 31, "endOffset": 72}, {"referenceID": 11, "context": "For example, in recent studies (Torkkola, 2003; 2001; Hild et al., 2006), the quadratic mutual information (with quadratic R\u00e9nyi entropy) is used instead of the Shannon-based MI; this is because with the use of quadratic R\u00e9nyi entropy, the gradient of MI can be calculated analytically under the assumption of a Gaussian mixture model (GMM) signal model.", "startOffset": 31, "endOffset": 72}, {"referenceID": 14, "context": "Using Information Discriminant Analysis (IDA) (Nenadic, 2007), the entropy of the GMM in the MI calculation, where the higher-order information comes into play, is approximated with the entropy of a global Gaussian distribution, which again loses the higher-order information.", "startOffset": 46, "endOffset": 61}, {"referenceID": 21, "context": "This theorem has found applications in the area of precoder design for communication systems (Xiao et al., 2011; Carson et al., 2012), but is not widely appreciated in the machine learning and signal processing communities, except for a few papers on optical imaging system design (Ashok et al.", "startOffset": 93, "endOffset": 133}, {"referenceID": 4, "context": "This theorem has found applications in the area of precoder design for communication systems (Xiao et al., 2011; Carson et al., 2012), but is not widely appreciated in the machine learning and signal processing communities, except for a few papers on optical imaging system design (Ashok et al.", "startOffset": 93, "endOffset": 133}, {"referenceID": 0, "context": ", 2012), but is not widely appreciated in the machine learning and signal processing communities, except for a few papers on optical imaging system design (Ashok et al., 2008; Baheti & Neifeld, 2009).", "startOffset": 155, "endOffset": 199}, {"referenceID": 20, "context": "To connect to the quadratic mutual information approach (Torkkola, 2003; 2001), we adopt the mixture-of-GMMs signal model.", "startOffset": 56, "endOffset": 78}, {"referenceID": 14, "context": "We adopt the information-theoretic criterion (Nenadic, 2007) as", "startOffset": 45, "endOffset": 60}, {"referenceID": 14, "context": "where C and Y represent c and y as random variables, I(C;Y ) denotes the MI, and the orthonormality constraint is common in the literature (Nenadic, 2007).", "startOffset": 139, "endOffset": 154}, {"referenceID": 7, "context": "The Bayes classification error, defined as Pe = \u222b p(y)(1 \u2212 maxc p(c|y))dy, can be bounded by I(C;Y ) as follows (Hellman & Raviv, 1970; Fano, 1961; Nenadic, 2007)", "startOffset": 112, "endOffset": 162}, {"referenceID": 14, "context": "The Bayes classification error, defined as Pe = \u222b p(y)(1 \u2212 maxc p(c|y))dy, can be bounded by I(C;Y ) as follows (Hellman & Raviv, 1970; Fano, 1961; Nenadic, 2007)", "startOffset": 112, "endOffset": 162}, {"referenceID": 20, "context": ", not a R\u00e9nyi entropy measure (Torkkola, 2003), with a comparison to results based on R\u00e9nyi entropy discussed below).", "startOffset": 30, "endOffset": 46}, {"referenceID": 21, "context": "It has found applications in precoder design for communications systems (Xiao et al., 2011; Carson et al., 2012).", "startOffset": 72, "endOffset": 112}, {"referenceID": 4, "context": "It has found applications in precoder design for communications systems (Xiao et al., 2011; Carson et al., 2012).", "startOffset": 72, "endOffset": 112}, {"referenceID": 0, "context": "The only studies we found are (Ashok et al., 2008; Baheti & Neifeld, 2009) which use the above theorem to design optical imaging systems.", "startOffset": 30, "endOffset": 74}, {"referenceID": 4, "context": "By relaxing this constraint and instead considering a power constraint we can leverage more results from communications and recent work in image reconstruction (Carson et al., 2012).", "startOffset": 160, "endOffset": 181}, {"referenceID": 4, "context": "In the signal reconstruction scenarios in communications and image reconstruction, the mutual information is known to be concave in the squared singular values of the projection matrix when U \u03a6 = UR (Carson et al., 2012).", "startOffset": 199, "endOffset": 220}, {"referenceID": 5, "context": "According to (Chen et al., 2010), the detailed Bayesian inference can be derived as", "startOffset": 13, "endOffset": 32}, {"referenceID": 20, "context": "Information-theoretic supervised dimensionality reduction was studied in (Torkkola, 2003; 2001).", "startOffset": 73, "endOffset": 95}, {"referenceID": 14, "context": "The Information Discriminant Analysis (IDA) (Nenadic, 2007) and Linear Discriminant Analysis (LDA) (Fisher, 1936) are derived under GMM signal model, which is a simplification and a special case of the mixture-of-GMMs signal model discussed in Section 4.", "startOffset": 44, "endOffset": 59}, {"referenceID": 8, "context": "The Information Discriminant Analysis (IDA) (Nenadic, 2007) and Linear Discriminant Analysis (LDA) (Fisher, 1936) are derived under GMM signal model, which is a simplification and a special case of the mixture-of-GMMs signal model discussed in Section 4.", "startOffset": 99, "endOffset": 113}, {"referenceID": 14, "context": "Then the optimization of IIDA(C;Y ) can be solved via gradient descent (Nenadic, 2007).", "startOffset": 71, "endOffset": 86}, {"referenceID": 8, "context": "The LDA method (Fisher, 1936) simultaneously maximizes the between-class scattering and minimizes the within-class scattering of the projected data.", "startOffset": 15, "endOffset": 29}, {"referenceID": 14, "context": "It is easy to prove that IIDA(C;Y ) \u2265 I(C;Y ) (maximum entropy principle) (Nenadic, 2007) and IIDA(C;Y ) \u2265 ILDA(C;Y ) (concavity of log det(\u00b7)).", "startOffset": 74, "endOffset": 89}, {"referenceID": 14, "context": "The first two are used in IDA (Nenadic, 2007) and can be downloaded from the UCI Machine Learning Repository.", "startOffset": 30, "endOffset": 45}, {"referenceID": 17, "context": "data was obtained in (Tao et al., 2009).", "startOffset": 21, "endOffset": 39}], "year": 2012, "abstractText": "We study the problem of supervised linear dimensionality reduction, taking an information-theoretic viewpoint. The linear projection matrix is designed by maximizing the mutual information between the projected signal and the class label. By harnessing a recent theoretical result on the gradient of mutual information, the above optimization problem can be solved directly using gradient descent, without requiring simplification of the objective function. Theoretical analysis and empirical comparison are made between the proposed method and two closely related methods, and comparisons are also made with a method in which R\u00e9nyi entropy is used to define the mutual information (in this case the gradient may be computed simply, under a special parameter setting). Relative to these alternative approaches, the proposed method achieves promising results on real datasets.", "creator": "LaTeX with hyperref package"}}}