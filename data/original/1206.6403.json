{"id": "1206.6403", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Two Step CCA: A new spectral method for estimating vector models of words", "abstract": "Unlabeled data is often used to learn representations which can be used to supplement baseline features in a supervised learner. For example, for text applications where the words lie in a very high dimensional space (the size of the vocabulary), one can learn a low rank \"dictionary\" by an eigen-decomposition of the word co-occurrence matrix (e.g. using PCA or CCA). In this paper, we present a new spectral method based on CCA to learn an eigenword dictionary. Our improved procedure computes two set of CCAs, the first one between the left and right contexts of the given word and the second one between the projections resulting from this CCA and the word itself. We prove theoretically that this two-step procedure has lower sample complexity than the simple single step procedure and also illustrate the empirical efficacy of our approach and the richness of representations learned by our Two Step CCA (TSCCA) procedure on the tasks of POS tagging and sentiment classification.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (579kb)", "http://arxiv.org/abs/1206.6403v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["paramveer dhillon", "jordan rodu", "dean foster", "lyle ungar"], "accepted": false, "id": "1206.6403"}, "pdf": {"name": "1206.6403.pdf", "metadata": {"source": "META", "title": "Two Step CCA: A new spectral method for estimating vector models of words", "authors": ["Paramveer S. Dhillon"], "emails": ["dhillon@cis.upenn.edu", "jrodu@wharton.upenn.edu", "foster@wharton.upenn.edu", "ungar@cis.upenn.edu"], "sections": [{"heading": "1. Introduction and Related Work", "text": "Over the past decade there has been increased interest in using unlabeled data to supplement the labeled\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ndata in semi-supervised learning settings. Such methods help overcome the inherent data sparsity and provide improved generalization accuracies in high dimensional domains such as NLP. Approaches like (Ando & Zhang, 2005; Suzuki & Isozaki, 2008) have been empirically very successful and have achieved excellent accuracies on a variety of text data. However, it is often difficult to adapt these approaches to use in conjunction with existing supervised text learning systems, as these approaches enforce a particular choice of model.\nAn increasingly popular alternative is to learn representational embeddings for words from a large collection of unlabeled data (typically using a generative model), and to use these embeddings to augment the feature set of a supervised learner. Embedding methods learn \u201cdictionaries\u201d in low dimensional spaces or over a small vocabulary size, unlike the traditional approach of working in the original high dimensional vocabulary space with only one dimension \u201con\u201d at a given time. Note that the dictionary provides a low (\u223c 30 \u2212 50) dimensional real-valued vector for each word type; I.e., all mentions of \u201cbank\u201d have the same vector associated with them.\nThe embedding methods broadly fall into two main categories:\n1. Clustering Based Embeddings: Clustering methods, often hierarchical, are used to group distributionally similar words based on their contexts e.g. Brown Clustering (Brown et al., 1992; Pereira et al., 1993).\n2. Dense Embeddings: These methods learn dense,\nlow dimensional, real-valued embeddings. Each dimension of these representations captures latent information about a combination of syntactic and semantic word properties. They can either be induced using neural networks like CW embeddings (Collobert & Weston, 2008) and Hierarchical log-linear (HLBL) embeddings (Mnih & Hinton, 2007) or by an eigen-decomposition of the word co-occurrence matrix, e.g. Latent Semantic Analysis/Latent Semantic Indexing (LSA/LSI) (Dumais et al., 1988) and Low Rank Multi-View Learning (LR-MVL) (Dhillon et al., 2011).\nOur main focus in this paper is on eigen-decomposition based methods, as they have been shown to be fast and scalable for learning from large amounts of unlabeled data (Turney & Pantel, 2010; Dhillon et al., 2011), have a strong theoretical grounding, and are guaranteed to converge to globally optimal solutions (Hsu et al., 2009). Particularly, we are interested in Canonical Correlation Analysis (CCA) based methods as:\n\u2022 Firstly, unlike PCA or LSA based methods they are scale invariant.\n\u2022 Secondly, unlike LSA they can capture multi-view information. In text applications the left and right contexts of the words provide a natural split into two views which is totally ignored by LSA as it throws the entire context into a bag of words while constructing the term-document matrix.\nOur main contributions in this paper are two fold. Firstly, we provide an improved method for learning an eigenword dictionary from unlabeled data using CCA \u2013 Two Step CCA (TSCCA). TSCCA computes two set of CCAs, the first one between the left and right contexts of the given word and the second one between the projections resulting from this CCA and the word itself. We prove theoretically that this two-step procedure has lower sample complexity than the simple single step procedure and also illustrate the empirical efficacy of our approach on the tasks of POS tagging and sentiment classification. Secondly, we show empirically that the dictionaries learned using CCA capture richer and more discriminative information than PCA or LSA on the same context, due to the fact that PCA and LSA are scale dependent and LSA further ignores the word order and hence the multi-view nature of context.\nThe remainder of the paper is organized as follows. In the next section we give our problem formulation and give a brief overview of CCA, which forms the core of\nour method. Section 3 describes our proposed two step CCA (TSCCA) algorithm in detail and gives theory supporting its performance. Section 4 demonstrates the effectiveness of TSCCA on tasks of POS tagging and sentiment classification. We conclude with a brief summary in Section 5."}, {"heading": "2. Problem Formulation", "text": "Our goal is to estimate a vector for each word type that captures the distributional properties of that word in the form of a low dimensional representation of the correlation between that word and the words in its immediate context.\nMore formally, assume a document (in practice a concatenation of a large number of documents) consisting of n tokens {w1,w2, ...,wn}, each drawn from a vocabulary of v words. Define the left and right contexts of each token wi as the h words to the left or right of that token. The context sits in a very high dimensional space, since for a vocabulary of size v, each of the 2h words in the combined context requires an indicator function of dimension v. The tokens themselves sit in a v dimensional space of words which we want to project down to a k dimensional state space. We call the mapping from word types to their latent vectors the eigenword dictionary.\nFor a set of documents containing n tokens, define Ln\u00d7vh and Rn\u00d7vh as the matrices specifying the left and right contexts of the tokens, and Wn\u00d7v as the matrix of the tokens themselves. In W, we represent the presence of the jth word type in the ith position in a document by setting matrix element wij = 1. L and R are similar, but have columns for each word in each position in the context. (For example, in the sentence \u201cI ate green apples yesterday.\u201d, for a context of size h = 2, the left context of \u201cgreen\u201d would be \u201cI ate\u201d and the right context would be \u201capples yesterday\u201d and the third row of W would have a \u201c1\u201d in the column corresponding to the word \u201cgreen\u201d.)\nDefine the complete context matrix C as the concatenation [L R]. Thus, for a trigram representation with vocabulary size v words, history size h = 1, C has 2v columns \u2013 one for each possible word to the left of the target word and one for each possible word to the right of the target word.\nAcw = C >W then contains the counts of how often each word w occurs in each context c, the matrix Acc = C\n>C gives the covariance of the contexts, and Aww = W\n>W, the word covariance matrix, is a diagonal matrix with the counts of each word on the\ndiagonal.1\nWe want to find a vector representation of each of the v word types such that words that are distributionally similar (ones that have similar contexts) have similar state vectors. We will do this using Canonical Correlation Analysis (CCA) (Hotelling, 1935; Hardoon & Shawe-Taylor, 2008), by taking the CCA between the combined left and right contexts C = [L R] and their associated tokens, W.\nCCA (Hotelling, 1935) is the analog to Principal Component Analysis (PCA) for pairs of matrices. PCA computes the directions of maximum covariance between elements in a single matrix, whereas CCA computes the directions of maximal correlation between a pair of matrices. Unlike PCA (and its variant LSA), CCA does not depend on how the observations are scaled. This invariance of CCA to linear data transformations allows proofs that keeping the dominant singular vectors (those with largest singular values) will faithfully capture any state information (Kakade & Foster, 2007). Secondly, CCA extends more naturally than LSA to sequences of words.2 Remember that LSA uses \u201cbags of words\u201d, which are good for capturing topic information, but fail for problems like part of speech (POS) tagging which need sequence information. Finally, as we show in the next section, the CCA formulation can be naturally extended to a two step procedure that, while equivalent in the limit of infinite data, gives higher accuracies for finite corpora.\nMore specifically, given a set of n paired observation vectors {(l1, r1), ..., (ln, rn)}\u2013in our case the two matrices are the left (L) and right (R) context matrices of a word\u2013we would like to simultaneously find the directions \u03a6l and \u03a6r that maximize the correlation of the projections of L onto \u03a6l with the projections of R onto \u03a6r. This is expressed as\nmax \u03a6l,\u03a6r E[\u3008L,\u03a6l\u3009\u3008R,\u03a6r\u3009]\u221a E[\u3008L,\u03a6l\u30092]E[\u3008R,\u03a6r\u30092]\n(1)\nwhere E denotes the empirical expectation. We use the notation Clr (Cll) to denote the cross (auto) covariance matrices between L and R (i.e. L>R and L>L respectively.).\nThe left and right canonical correlates are the solutions\n1We will pretend that the means are all in fact zero and refer to these Acc etc. as covariance matrices, when in fact they are second moment matrices.\n2It is important to note that it is possible to come up with PCA variants which take sequence information into account, for instance by finding principal components of the Acw matrix.\n\u3008\u03a6l,\u03a6r\u3009 of the following equations:\nCll \u22121ClrCrr \u22121Crl\u03a6l = \u03bb\u03a6l\nCrr \u22121CrlCll \u22121Clr\u03a6r = \u03bb\u03a6r (2)\nWe keep the k left and right singular vectors (\u03a6l and \u03a6r) corresponding to the k largest singular values.\nUsing the above, we can define a \u201cOne step CCA\u201d (OSCCA), procedure to estimate the eigenword dictionary as follows:\nCCA(C,W)\u2192 (\u03a6C,\u03a6W) (3)\nwhere the v \u00d7 k matrix \u03a6W contains the \u201ceigenword dictionary\u201d that characterizes each of the v words in the vocabulary using a k dimensional vector. More generally, the \u201cstate\u201d vectors S for the n tokens can be estimated either from the context as C\u03a6C or (trivially) from the tokens themselves as W\u03a6W. Its important to note that both these estimation procedures give a redundant estimate of the same hidden \u201cstate.\u201d\nThe right canonical correlates found by OSCCA give an optimal approximation to the state of each word, where \u201coptimal\u201d means that it gives the linear model of a given size, k that is best able to estimate labels that depend linearly on state, subject to only using the word and not its context. The left canonical correlates similarly give optimal state estimates given the context. See (Kakade & Foster, 2007) for more technical details, including the fact that these results are asymptotic in the limit of infinite data.\nOSCCA, as defined in Equations 2 and 3 thus gives an efficient way to calculate the attribute dictionary \u03a6W for a set of v words given the context and associated word matrices from a corpus. As mentioned, OSCCA is optimal only in the limit of infinite data. In practice, data is, of course, always limited. In languages, lack of data comes about in two ways. Some languages are resource poor; one just does not have that many tokens of them (especially languages that lack a significant written literature). Even for most modern languages, many of the individual words in them are quite rare. Due to the Zipfian distribution of words, many words do not show up very often. A typical year\u2019s worth of Wall Street Journal text only has \u201clasagna\u201d or \u201cbackpack\u201d a handful of times and \u201cziti\u201d at most once or twice. To overcome these issues we propose a two-step CCA (TSCCA) procedure which has better sample complexity for rare words."}, {"heading": "3. Two Step CCA (TSCCA) Algorithm", "text": "We now introduce our two step procedure (TSCCA) of computing an eigenword dictionary and show theoret-\nAlgorithm 1 Two step CCA\n1: Input: L,W,R 2: CCA(L,R)\u2192 (\u03a6L,\u03a6R) 3: S = [L\u03a6L R\u03a6R] 4: CCA(S,W)\u2192 (\u03a6S,\u03a6W) 5: Output: \u03a6W, the eigenword dictionary\nically that it gives better estimates than the OSCCA method described above.\nIn the two-step method, instead of taking the CCA between the combined context [L R] and the words W, we first take the CCA between the left and right contexts and use the result of that CCA to estimate the state S of all the tokens in the corpus from their contexts. Note that we get partially redundant state estimates from the left context and from the right context; these are concatenated to make combined state estimate. This will contain some redundant information, but will not lose any of the differences in information from the left and right sides. We then take the CCA between S and the words W to get our final eigenword dictionary. This is summarized in Algorithm 1. The first step, the CCA between L and R, must produce at least as many canonical components as the second step, which produces the final output.\nThe two step method requires fewer tokens of data to get the same accuracy in estimating the eigenword dictionary because its final step estimates fewer parameters O(vk) than the OSCCA does O(v2).\nBefore stating the theorem, we first explain this intuitively. Predicting each word as a function of all other word combinations that can occur in the context is far sparser than predicting low dimensional state from context, and then predicting word from state. Thus, for relatively infrequent words, OSCCA should have significantly lower accuracy than the two step version. Phrased differently, mapping from context to state and then from state to word (TSCCA) gives a more parsimonious model than mapping directly from context to word (OSCCA).\nThe relative ability of OSCCA to estimate hidden state compared to that of TSCCA can be summarized as follows:\nTheorem 1 Given a matrix of words, W and their associated left and right contexts, L and R with vocabulary size v, context size h, and corpus of n tokens. The ratio of the dimension of the hidden state that needs to be estimated by TSCCA in order to recover with high probability the information in the true state to the corresponding dimension needed for OS-\nCCA is h+khv .\nPlease see the supplementary material for a proof of the above theorem.\nSince the corpora we care about (i.e. text and language corpora) usually have vh h+ k, the TSCCA procedure will in expectation correctly estimate hidden state with a much smaller number of components k than the one step procedure. Or, equivalently, for an estimated hidden state of given size k, TSCCA will correctly estimate more of the hidden state components.\nAs mentioned earlier, words have a Zipfian distribution so most words are rare. For such rare words, if one does a CCA between them and their contexts, one will have very few observations, and hence will get a low quality estimate of their eigenword vector. If, on the other hand, one first estimates a state vector for the rare words, and then does a CCA between this state vector and the context, the rare words can be thought of as borrowing strength from more common distributionally similar words. For example, \u201cumbrage\u201d (56,020) vs. \u201cannoyance\u201d (777,061) or \u201cunmeritorious\u201d (9,947) vs. \u201cundeserving\u201d (85,325). The numbers in parentheses are the number of occurrences of these words in the Google n-gram collection used in our experiments."}, {"heading": "3.1. Practical Considerations", "text": "As mentioned in Section 2, CCA (either one-step or two-step) is essentially done by taking the singular value decomposition of a matrix. For small matrices, this can be done using standard functions in e.g. MATLAB, but for very large matrices (e.g. for vocabularies of tens or hundreds of thousands of words), it is important to take advantage of recent advances in SVD algorithms. For the experiments presented in this paper we use the method of (Halko et al., 2011), which uses random projections to compute SVD of large matrices."}, {"heading": "4. Experimental Results", "text": "This section describes the performance (accuracy and richness of representation) of our eigenword dictionary learned via CCA. We evaluate the quality of the eigenword dictionary by using it in a supervised learning setting to predict a wide variety of labels that can be attached to words. For simplicity, all results shown here map from word type to label3; i.e. each word\n3Its conceivable to learn eigenword dictionaries which map each token to a label, e.g. as done by (Dhillon et al., 2011) but that is not the focus of this paper.\ntype is assumed to have a single POS tag or type of sentiment.\nWe first compare the One-Step vs. Two Step CCA (TSCCA) procedures on a set of Part of Speech (POS) tagging problems for different languages, looking at how the predictive accuracy scales with corpus size for predictions on a fixed vocabulary. These results use small corpora. We then turn to the question of what types of semantic category information is captured by our eigenword dictionaries and how favorably its predictiveness compares with other state-of-the-art embeddings e.g. CW (Collobert & Weston, 2008) and HLBL (Mnih & Hinton, 2007). Here, we use the RCV1 Reuters newswire data as the unlabeled data."}, {"heading": "4.1. POS Tagging: One step CCA (OSCCA) vs. Two step CCA (TSCCA)", "text": "We compare performance of OSCCA and TSCCA on the task of POS tagging in four different languages.\nTable 1 provides statistics of all the corpora used, namely: the Wall Street Journal portion of the Penn treebank (Marcus et al., 1993) (we consider both the 17 tags of (PTB 17) (Smith & Eisner, 2005) and the 45 tags version of it (PTB 45)), the Bosque subset of the Portuguese Floresta Sinta(c)tica Treebank (Afonso et al., 2002), the Bulgarian BulTreeBank (Simov et al., 2002) (with only the 12 coarse tags), and the Danish Dependency Treebank (DDT) (Kromann, 2003).\nNote that some corpora like English have \u223c 1 million tokens whereas Danish only has \u223c 100k tokens. So, to address this data imbalance we kept only the first \u223c 100k tokens of the larger corpora so as to perform a uniform evaluation across all corpora.\nTheorem 1 implies that the difference between OSCCA and TSCCA should be more pronounced at smaller sample sizes, where the errors are higher and that they should have similar predictive power asymptotically when we learn them using large amounts of data. So, we evaluate the performance of the methods on varying data sizes ranging from 5k to the entire 100k tokens. We take history size of h = 1 for CCA i.e. a word to the left and a word to the right; for PCA this reduces to a bag of trigrams. The PCA baseline used is similar to the one that has recently been proposed by (Lamar\net al., 2010) except that here we are interested in supervised accuracy and not the unsupervised accuracy as in that paper. It is important to note that for POS tagging usually a trigram context (in our case h = 1) is sufficient to get state-of-the-art performance as can be substantiated by trigram POS taggers e.g. (Merialdo, 1994), so we need not consider longer contexts.\nAs mentioned earlier, for the unlabeled learning part i.e. learning using CCA/PCA we are interested in seeing the eigenword dictionary estimates for the word types (for a fixed vocabulary) get better with more data. So, when varying the unlabeled data from 5k to 100k we made sure that they had the exact same vocabulary and that the performance improvement is not coming from word types not present in the 5k tokens but present in the total 100k.\nTo evaluate the predictive accuracy of the descriptors learned using different amounts of unlabeled data, we learn a multi-class SVM (Chang & Lin, 2001) with a linear kernel to predict the POS tag of each type. The SVM was trained using 80% of the word types chosen randomly and then tested on the remaining 20% types and this procedure was repeated 10 times. The hyperparameters of the linear SVM i.e. the cost function C was chosen by cross validation on the training set. Its important to note that our train and test sets do not contain any of the same word types.4 The value of k, the size of low dimensional projection was fixed at 50; The results were robust to the size of k within the range of 30 to 100.\nThe accuracy of using OSCCA, TSCCA and PCA features in a supervised learner are shown in Figure 1 for the task of POS tagging. As can be seen from the results, CCA-based methods are significantly better (5% significance level in a paired t-test) than the PCAbased supervised learner. Among the CCAs, TSCCA is significantly better than OSCCA for small amounts of data, and (as predicted by theory) the two become comparable in accuracy as the amount of unlabeled data used to learn the CCAs becomes large."}, {"heading": "4.2. Sentiment Classification Task: Richness of state learned by CCA", "text": "In the above results, we compared languages using part of speech tags, but the states estimated by CCA are far richer. We illustrate this by using them to build predictive models in English for a number of differ-\n4We are doing non-disambiguating POS tagging i.e. each word type has a single POS tag, so if the same word type occurred in both the training and testing data, a learning algorithm that just memorized the training set would perform reasonably well.\nent word categories and also comparing them against other state-of-the-art embeddings e.g. CW (Collobert & Weston, 2008) and HLBL (Mnih & Hinton, 2007).\nIt is often useful to group words into semantic classes such as colors or numbers, professionals or disciplines, happy or sad words, words of encouragement or discouragement, etc.\nMany people have collected sets of words that indicate positive or negative sentiment. More generally, substantial effort has gone into creating handcurated words that can be used to capture a variety of opinions about different products, papers, or people. For example (Teufel, 2010) contains dozens of carefully constructed lists of words that she uses to categorize what authors say about other scientific papers. Her categories include \u201cproblem nouns\u201d (caveat, challenge, complication, contradiction,. . . ), \u201ccomparison nouns\u201d (accuracy, baseline, comparison, evaluation,. . . ), \u201cwork nouns\u201d (account, analysis, approach,ldots) as well as more standard sets of positive, negative, and comparative adjectives.\nIn the example below, we use words from a set of five dimensions that have been identified in positive psychology under the acronym PERMA (Seligman, 2011):\n\u2022 Positive emotion (aglow, awesome, bliss, . . . ), \u2022 Engagement (absorbed, attentive, busy, . . . ), \u2022 Relationships (admiring, agreeable, . . . ), \u2022 Meaning (aspire, belong, . . . ) \u2022 Achievement (accomplish, achieve, attain, . . . ).\nFor each of these five categories, we have both positive words \u2013 ones that connote, for example, achievement, and negative words, for example, un-achievement (amateurish, blundering, bungling, . . . ). We would hope (and we show below that this is in fact true), that we can use our eigenword dictionary not only to distinguish between different PERMA categories, but also to address the harder task of distinguishing between positive and negative terms in the same category. (The latter task is harder because words that are opposites, such as \u201clarge\u201d and \u201csmall,\u201d often are distributionally similar.)\nThe description of the PERMA datasets is given in Table 2. All of the following predictions use a single eigenword dictionary, which we estimated using TSCCA on the RCV1 corpus which contains Reuters newswire from Aug 96 to Aug 97 (about 63 million tokens in 3.3 million sentences). We scaled our TSCCA features to have a unit `2 norm for each word type. As far as the other\nembeddings are concerned i.e. CW and HLBL, we downloaded them from http://metaoptimize. com/projects/wordreprs with k=50 dimensions and scaled as described in the paper (Turian et al., 2010).5\nFigure 2 shows results for the five PERMA categories. The plots show accuracy as a function of the size of the training set used in the supervised portion of the learning. As earlier, we used SVM with linear kernel for the supervised binary classification, with the cost parameter chosen by cross-validation on training set and the value of k was again fixed at 50.\nAs can be seen from the plots, the CCA variant TSCCA performs significantly (5% significance level in a paired t-test) better than PCA, CW and HLBL in 3 out of 5 cases and is comparable on the remaining 2 cases."}, {"heading": "5. Conclusion", "text": "In this paper we proposed a new and improved spectral method, a two-step alternative (TSCCA) to the standard CCA (OSCCA) which can be used in domains such as Text/NLP which contain word sequences and where one has three views (the left context, the right context, and the words of interest themselves). We showed theoretically that the eigenword dictionaries learned by TSCCA provide more accurate state estimates (lower sample complexity) for small corpora than standard OSCCA. This was evidenced by superior empirical performance of TSCCA as compared to OSCCA and to simple PCA on the task of POS tagging, especially when less unlabeled data was used to learn the PCA or CCA representations.\nWe also showed empirically that the vector representations learned by CCA are much richer and contain more discriminative information than the representations learned by PCA as well as other state-of-the-art embeddings. Since CCA is scale-invariant and is able to take the multi-view nature of word sequence (i.e. the words to the left and words to the right) into account, it is able to learn more fine-grained spectral representations than PCA or LSA which ignore the word ordering."}], "references": [{"title": "Floresta sinta(c)tica: a treebank for portuguese", "author": ["S. Afonso", "E. Bick", "R. Haber", "D. Santos"], "venue": "Proc. LREC, pp. 1698\u20131703,", "citeRegEx": "Afonso et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Afonso et al\\.", "year": 2002}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R. Ando", "T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ando and Zhang,? \\Q2005\\E", "shortCiteRegEx": "Ando and Zhang", "year": 2005}, {"title": "Class-based n-gram models of natural language", "author": ["P. Brown", "deSouza", "R. Mercer", "Pietra", "V. Della", "J. Lai"], "venue": "Comput. Linguist.,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chang", "Chih-Chung", "Lin", "Chih-Jen"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2001}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "ICML \u201908,", "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Multi-view learning of word embeddings via cca", "author": ["Dhillon", "Paramveer S", "Foster", "Dean", "Ungar", "Lyle"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Dhillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2011}, {"title": "Using latent semantic analysis to improve access to textual information", "author": ["S. Dumais", "G. Furnas", "T. Landauer", "S. Deerwester", "R. Harshman"], "venue": "In SIGCHI Conference on human factors in computing systems,", "citeRegEx": "Dumais et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Dumais et al\\.", "year": 1988}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Halko", "Nathan", "Martinsson", "Per-Gunnar", "Tropp", "Joel A"], "venue": "SIAM Rev,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Sparse cca for bilingual word generation", "author": ["Hardoon", "David", "Shawe-Taylor", "John"], "venue": "In EURO Mini Conference, Continuous Optimization and KnowledgeBased Technologies,", "citeRegEx": "Hardoon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2008}, {"title": "Canonical correlation analysis (cca)", "author": ["H. Hotelling"], "venue": "Journal of Educational Psychology,", "citeRegEx": "Hotelling,? \\Q1935\\E", "shortCiteRegEx": "Hotelling", "year": 1935}, {"title": "A spectral algorithm for learning hidden markov models", "author": ["D. Hsu", "S. Kakade", "T. Zhang"], "venue": "In COLT,", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Multi-view regression via canonical correlation analysis", "author": ["Kakade", "S M", "Foster", "Dean P"], "venue": "COLT, volume 4539 of Lecture Notes in Computer Science,", "citeRegEx": "Kakade et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2007}, {"title": "The danish dependency treebank and the underlying linguistic theory. in second workshop on treebanks and linguistic theories (tlt)", "author": ["Kromann", "Matthias T"], "venue": "In In Proc. LREC,", "citeRegEx": "Kromann and T.,? \\Q2003\\E", "shortCiteRegEx": "Kromann and T.", "year": 2003}, {"title": "Svd and clustering for unsupervised pos tagging", "author": ["Lamar", "Michael", "Maron", "Yariv", "Johnson", "Mark", "Bienenstock", "Elie"], "venue": "ACL Short \u201910,", "citeRegEx": "Lamar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lamar et al\\.", "year": 2010}, {"title": "Building a large annotated corpus of english: the penn treebank", "author": ["Marcus", "Mitchell P", "Marcinkiewicz", "Mary Ann", "Santorini", "Beatrice"], "venue": "Comput. Linguist.,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Tagging english text with a probabilistic model", "author": ["Merialdo", "Bernard"], "venue": "Comput. Linguist.,", "citeRegEx": "Merialdo and Bernard.,? \\Q1994\\E", "shortCiteRegEx": "Merialdo and Bernard.", "year": 1994}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "ICML \u201907,", "citeRegEx": "Mnih and Hinton,? \\Q2007\\E", "shortCiteRegEx": "Mnih and Hinton", "year": 2007}, {"title": "Distributional clustering of English words", "author": ["F. Pereira", "N. Tishby", "L. Lee"], "venue": "In 31st Annual Meeting of the ACL,", "citeRegEx": "Pereira et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pereira et al\\.", "year": 1993}, {"title": "Flourish: A Visionary New Understanding of Happiness and Well-being", "author": ["Seligman", "Martin"], "venue": null, "citeRegEx": "Seligman and Martin.,? \\Q2011\\E", "shortCiteRegEx": "Seligman and Martin.", "year": 2011}, {"title": "Building a linguistically interpreted corpus of bulgarian: the bultreebank", "author": ["K. Simov", "P. Osenova", "M. Slavcheva", "S. Kolkovska", "E. Balabanova", "D. Doikoff", "K. Ivanova", "A. Simov", "E. Simov", "M. Kouylekov"], "venue": "Proc. LREC,", "citeRegEx": "Simov et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Simov et al\\.", "year": 2002}, {"title": "Contrastive estimation: training log-linear models on unlabeled data", "author": ["Smith", "Noah A", "Eisner", "Jason"], "venue": "ACL \u201905,", "citeRegEx": "Smith et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2005}, {"title": "Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data", "author": ["J. Suzuki", "H. Isozaki"], "venue": null, "citeRegEx": "Suzuki and Isozaki,? \\Q2008\\E", "shortCiteRegEx": "Suzuki and Isozaki", "year": 2008}, {"title": "The structure of scientific articles", "author": ["Teufel", "Simone"], "venue": "CSLI Publications,", "citeRegEx": "Teufel and Simone.,? \\Q2010\\E", "shortCiteRegEx": "Teufel and Simone.", "year": 2010}, {"title": "Word representations: a simple and general method for semisupervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "ACL \u201910,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "From frequency to meaning: vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "Brown Clustering (Brown et al., 1992; Pereira et al., 1993).", "startOffset": 17, "endOffset": 59}, {"referenceID": 17, "context": "Brown Clustering (Brown et al., 1992; Pereira et al., 1993).", "startOffset": 17, "endOffset": 59}, {"referenceID": 6, "context": "Latent Semantic Analysis/Latent Semantic Indexing (LSA/LSI) (Dumais et al., 1988) and Low Rank Multi-View Learning (LR-MVL) (Dhillon et al.", "startOffset": 60, "endOffset": 81}, {"referenceID": 5, "context": ", 1988) and Low Rank Multi-View Learning (LR-MVL) (Dhillon et al., 2011).", "startOffset": 50, "endOffset": 72}, {"referenceID": 5, "context": "Our main focus in this paper is on eigen-decomposition based methods, as they have been shown to be fast and scalable for learning from large amounts of unlabeled data (Turney & Pantel, 2010; Dhillon et al., 2011), have a strong theoretical grounding, and are guaranteed to converge to globally optimal solutions (Hsu et al.", "startOffset": 168, "endOffset": 213}, {"referenceID": 10, "context": ", 2011), have a strong theoretical grounding, and are guaranteed to converge to globally optimal solutions (Hsu et al., 2009).", "startOffset": 107, "endOffset": 125}, {"referenceID": 9, "context": "We will do this using Canonical Correlation Analysis (CCA) (Hotelling, 1935; Hardoon & Shawe-Taylor, 2008), by taking the CCA between the combined left and right contexts C = [L R] and their associated tokens, W.", "startOffset": 59, "endOffset": 106}, {"referenceID": 9, "context": "CCA (Hotelling, 1935) is the analog to Principal Component Analysis (PCA) for pairs of matrices.", "startOffset": 4, "endOffset": 21}, {"referenceID": 7, "context": "For the experiments presented in this paper we use the method of (Halko et al., 2011), which uses random projections to compute SVD of large matrices.", "startOffset": 65, "endOffset": 85}, {"referenceID": 5, "context": "as done by (Dhillon et al., 2011) but that is not the focus of this paper.", "startOffset": 11, "endOffset": 33}, {"referenceID": 14, "context": "Table 1 provides statistics of all the corpora used, namely: the Wall Street Journal portion of the Penn treebank (Marcus et al., 1993) (we consider both the 17 tags of (PTB 17) (Smith & Eisner, 2005) and the 45 tags version of it (PTB 45)), the Bosque subset of the Portuguese Floresta Sinta(c)tica Treebank (Afonso et al.", "startOffset": 114, "endOffset": 135}, {"referenceID": 0, "context": ", 1993) (we consider both the 17 tags of (PTB 17) (Smith & Eisner, 2005) and the 45 tags version of it (PTB 45)), the Bosque subset of the Portuguese Floresta Sinta(c)tica Treebank (Afonso et al., 2002), the Bulgarian BulTreeBank (Simov et al.", "startOffset": 181, "endOffset": 202}, {"referenceID": 19, "context": ", 2002), the Bulgarian BulTreeBank (Simov et al., 2002) (with only the 12 coarse tags), and the Danish Dependency Treebank (DDT) (Kromann, 2003).", "startOffset": 35, "endOffset": 55}, {"referenceID": 13, "context": "The PCA baseline used is similar to the one that has recently been proposed by (Lamar et al., 2010) except that here we are interested in supervised accuracy and not the unsupervised accuracy as in that paper.", "startOffset": 79, "endOffset": 99}, {"referenceID": 23, "context": "com/projects/wordreprs with k=50 dimensions and scaled as described in the paper (Turian et al., 2010).", "startOffset": 81, "endOffset": 102}], "year": 2012, "abstractText": "Unlabeled data is often used to learn representations which can be used to supplement baseline features in a supervised learner. For example, for text applications where the words lie in a very high dimensional space (the size of the vocabulary), one can learn a low rank \u201cdictionary\u201d by an eigendecomposition of the word co-occurrence matrix (e.g. using PCA or CCA). In this paper, we present a new spectral method based on CCA to learn an eigenword dictionary. Our improved procedure computes two set of CCAs, the first one between the left and right contexts of the given word and the second one between the projections resulting from this CCA and the word itself. We prove theoretically that this two-step procedure has lower sample complexity than the simple single step procedure and also illustrate the empirical efficacy of our approach and the richness of representations learned by our Two Step CCA (TSCCA) procedure on the tasks of POS tagging and sentiment classification.", "creator": "LaTeX with hyperref package"}}}