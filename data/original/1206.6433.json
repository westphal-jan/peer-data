{"id": "1206.6433", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Copula Mixture Model for Dependency-seeking Clustering", "abstract": "We introduce a copula mixture model to perform dependency-seeking clustering when co-occurring samples from different data sources are available. The model takes advantage of the great flexibility offered by the copulas framework to extend mixtures of Canonical Correlation Analysis to multivariate data with arbitrary continuous marginal densities. We formulate our model as a non-parametric Bayesian mixture, while providing efficient MCMC inference. Experiments on synthetic and real data demonstrate that the increased flexibility of the copula mixture significantly improves the clustering and the interpretability of the results.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (3026kb)", "http://arxiv.org/abs/1206.6433v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "stat.ME cs.LG stat.ML", "authors": ["m\u00e9lanie rey", "volker roth 0001"], "accepted": true, "id": "1206.6433"}, "pdf": {"name": "1206.6433.pdf", "metadata": {"source": "META", "title": "Copula Mixture Model for Dependency-seeking Clustering", "authors": ["M\u00e9lanie Rey", "Volker Roth"], "emails": ["melanie.rey@unibas.ch", "volker.roth@unibas.ch"], "sections": [{"heading": "1. Introduction", "text": "When different types of measurements concerning a same underlying phenomenon are available, often appearing in the form of co-occurring samples, combining them is more informative than analysing them separately. First, if we assume that these different measurements, also referred to as the different views, are generated by several data sources with independent noise, analysing them jointly can increase the signal to noise ratio. Second, only a combined analysis can take into consideration the dependencies existing between the different types of measurements. As pointed out in Klami & Kaski (2007), possible dependencies between the views often contain some of the most relevant information about the data. Dependency modelling captures what is common between the views, i.e. the shared underlying signal, and in many applications where several experiments are designed to measure the same object this shared aspect is the focus of interest.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nThe task of detecting dependencies has traditionally been solved by Canonical Correlation Analysis (CCA). This method can however detect only global linear dependency. When the data express not only one global dependency but different local dependencies, a mixture formulation is more adequate. Fern et al. (2005) introduces a mixture of local CCA model which groups pairs of points expressing together a particular linear dependency between the two views. This model is adapted to cases where the data express several different local correlations, but it still focuses exclusively on linear dependencies since it assumes that within each cluster the two views are linearly correlated.\nDependency-seeking clustering goes one step further in the generalisation process by assuming that the views become independent when conditioned on the cluster structure. The aim is to perform clustering in the joint space of the multiple views, while focussing explicitly on inter-view dependencies 1. In the case of two views, the objective is then to group the co-occurring pairs of datapoints according to their inter-view dependency pattern such that when the cluster assignments are known these views become independent. As a consequence, the group structure now has a semantic interpretation in terms of dependency with the partition capturing the dependencies.\nThe starting point of existing dependency-seeking methods is the probabilistic interpretation of CCA given in Bach & Jordan (2005). In Klami & Kaski (2007) a Dirichlet prior Gaussian mixture for dependency-seeking clustering is introduced. However, as pointed out in Klami et al. (2010), when the data are not normally distributed, this method can suffer from a severe model mismatch problem. On application to non-normally distributed data these models have to increase the number of clusters to achieve a reasonable fit. Additional clusters are used to com-\n1The term inter-view dependencies refers to the dependence structure between the different views, whereas intraview dependencies refers to the dependence structure between the different dimensions of one view.\npensate for the inadequate Gaussian assumption. The components of these mixtures will not only be used to reflect differences in dependence structures but will also be used to approximate a non-Gaussian distribution. As a result some points expressing a similar inter-view dependence can be assigned to different groups and the interpretation of the clusters in terms of dependencies is lost. Moreover, the model needs to find a compromise between the cluster homogeneity and the approximation of a non-Gaussian mixture, so that non-homogenous clusters might emerge. Figure 1 illustrates how several Gaussian components can be used to approximate a beta density. An exponential family dependency-seeking method is proposed in Klami et al. (2010) to overcome this problem. This model can however be too restrictive when the views are multidimensional. Although the 1-dimensional exponential family covers many interesting distributions, only a few of them have convenient multivariate forms. In particular their dependence structure between dimensions is often very restrictive. Another restriction of that model is that all the dimensions in all the views must have the same univariate distribution whereas in practice different data sources are likely to produce differently distributed data.\nTo overcome these limitations we take advantage of the copulas framework to build a dependency-seeking clustering method suitable for data with any type of continuous densities. We use Gaussian copulas to construct Dirichlet prior mixtures of multivariate distributions with arbitrary continuous margins, the only restriction being that a density must exist. The model combines the adaptability of Bayesian non-parametric mixtures with the flexibility of copula-based distributions. Our approach focusses on Gaussian copulas for two main reasons. Firstly, their parametrisation using\na correlation matrix covers many different dependence patterns ranging from independence to comonotonicity (perfect dependence). Secondly, the model can be reformulated using multivariate Gaussian latent variables which enables efficient MCMC inference."}, {"heading": "2. Dependency-seeking clustering", "text": "Consider a p-dimensional random vector (rv) X and a q-dimensional rv Y which constitute two different sources of information about an object of interest. For example, several corporal measurements of a patient and the levels of different drugs administrated can serve as two sources of information about a medical treatment. We assume that X and Y have cooccurring samples (x1, . . . , xn) and (y1, . . . , yn) with xi \u2208 Rp and yi \u2208 Rq, i = 1, . . . , n. The probabilistic interpretation of CCA given by Bach & Jordan (2005) uses the following latent variable formulation:\nZ \u223c Nd (0, Id) , (X,Y ) |Z \u223c Np+q (WZ + \u00b5,\u03a8) ,\nwhere \u00b5 = (\u00b5x, \u00b5y) \u2208 Rp+q, W = ( Wx Wy ) \u2208 R(p+q)\u00d7d, 1 \u2264 d \u2264 min (p, q) and the covariance matrix \u03a8 has a block diagonal form:\n\u03a8 = ( \u03a8x 0 0 \u03a8y ) . (1)\nThey showed that the maximum likelihood estimate of W is connected to the canonical directions and correlations:\nW\u0302x = \u03a3\u0303xUxMx, W\u0302y = \u03a3\u0303yUyMy,\nwhere \u03a3\u0303x, \u03a3\u0303y are the sample covariance matrices, and Ux and Uy are the first d canonical directions. Mx and My are matrices such that MxM T y = Pd where Pd is the diagonal matrix containing the first d canonical correlations. Based on the above formulation, the following dependency-seeking clustering model is derived in Klami & Kaski (2008) :\nZ \u223c Mult (\u03b8) , (2) (X,Y ) |Z \u223c Np+q (\u00b5z,\u03a8z) , (3)\nwhere \u03a8z has a block structure as in (1):\n\u03a8z =\n( \u03a8zx 0\n0 \u03a8zy\n) . (4)\nand \u00b5z is a mean vector depending on Z. The latent variable Z now represents the clustering assignment. A key property of this model is the block diagonal\nstructure of the covariance matrix \u03a8z. This special form implies that given the cluster assignment the two views are independent, thereby enforcing the cluster structure to capture all the dependencies. This model however explicitly makes a conditional Gaussian assumption and can perform badly when data within a cluster are non-normally distributed as mentioned in section 1. To relax this normality assumption, we present a dependency-seeking clustering model constructed using Gaussian copulas which can be applied to almost any type of continuous data."}, {"heading": "3. Copulas and Gaussian copulas", "text": "A multivariate distribution is constituted of univariate random variables related to each other by a dependence mechanism. Copulas provide a framework to separate the dependence structure from the marginal distributions. Formally, a d-dimensional copula is a multivariate distribution function C : [0, 1]d \u2192 [0, 1] with standard uniform margins. The following theorem Sklar (1959) states the relationship between copulas and multivariate distributions.\nTheorem 1. (Sklar) Let F be a joint distribution function with margins F 1, . . . , F d. Then there exists a copula C : [0, 1]d \u2192 [0, 1] such that\nF ( x1, . . . , xd ) = C ( F 1 ( x1 ) , . . . , F d ( xd )) . (5)\nMoreover, if the margins are continuous, then this copula is unique. Conversely, if C is a copula and F 1, . . . , F d are univariate distribution functions, then F defined as in (5) is a multivariate distribution function with margins F 1, . . . , F d.\nGaussian copulas constitute an important class of copulas. If F is a Gaussian distribution Nd (\u00b5,\u03a3) then the corresponding C fulfilling equation (5) is a Gaussian copula. Since Gaussian copulas are invariant to strictly increasing transformations, the copula of Nd (\u00b5,\u03a3) is the same as the copula of Nd (0, P ) as mentioned in McNeil et al. (2005), where P is the correlation matrix corresponding to the covariance matrix \u03a3. Thus a Gaussian copula is uniquely determined by a correlation matrix P and we denote a Gaussian copula by CP . Using theorem 1 with CP , we can construct multivariate distributions with arbitrary margins and a Gaussian dependence structure. These distributions, called meta-Gaussian distributions, provide a natural way to extend models based on a multivariate normality assumption.\nWhen using a Gaussian copula we do not attempt to directly model the correlation of the original variables, but instead we first apply the transformation\n\u03a6\u22121 ( F j( . ) ) to every margin to obtain normally dis-\ntributed variables \u03a6\u22121 ( F j(Xj) ) \u223c N1 (0, 1) , where \u03a6 is the standard Gaussian cumulative distribution function, and then use P to describe their correlation. We finally note that zero values in P encode independence between the corresponding marginal variables. Therefore, if P has a block diagonal structure as in (1), the conditional independence of X|Z and Y |Z, which was a key property of equation (3), will be preserved in a meta-Gaussian model.\nMultivariate distributions constructed using Theorem 1 do not necessarily possess a density function. When a density exist it can be written as:\nf(x1, . . . , xd) = c ( F 1(x1), . . . , F d(xd) ) d\u220f j=1 f j(xj),\n(6)\nwhere\nc(u1, . . . , ud) = \u2202C(u1, . . . , ud)\n\u2202u1 . . . \u2202ud , (7)\nis the copula density of C. For cases where c has a simple closed form we can obtain an analytical expression for f using (6). This is true for the multivariate normal case and equation (6) becomes:\nf(x) = |P |\u2212 12 exp { \u22121\n2 x\u0303T (P\u22121 \u2212 I)x\u0303 } d\u220f j=1 f j(xj),\n(8) where x\u0303j = \u03a6\u22121 ( F j(xj) ) , x = (x1, . . . , xd), x\u0303 = (x\u03031, . . . , x\u0303d). We denote this density by M(\u03b8, P ), where \u03b8 is the vector containing all parameters of the marginal distributions."}, {"heading": "4. Multi-view clustering with meta-Gaussian distributions", "text": ""}, {"heading": "4.1. Model specification", "text": "Consider the two rv X = ( X1, . . . , Xp ) and Y =(\nY 1, . . . , Y q ) . We assume their joint distribution is a\nDirichlet prior mixture (DPM) given by:\nf(X,Y )(x, y) = \u222b \u222b f(X,Y )|\u03b8,P (x, y)d\u00b5\u03b8,Pd\u00b5G(\u03bb,G0),\nwhere \u00b5G is the distribution of a Dirichlet process (Ferguson, 1973) with base distribution G0 and concentration parameter \u03bb. The novelty here is the choice of f(X,Y )|\u03b8,P . We model the marginal distributions and the dependence structure separately to allow for more freedom:\n1. The margins can be arbitrary continuous distributions (providing the corresponding density exists):\nXj |\u03b8 = Xj |\u03b8jx \u223c F j X|\u03b8, j = 1, . . . , p,\nY j |\u03b8 = Y j |\u03b8jy \u223c F j Y |\u03b8, j = 1, . . . , q, where \u03b8 = ( \u03b81x, . . . , \u03b8 p x, \u03b8 1 y, . . . , \u03b8 q y ) . Note here that F jX|\u03b8 can be different types of distributions for the multiple dimensions j.\n2. The dependence structure is then specified by a Gaussian copula CP with correlation matrix P having a block diagonal structure as in (1).\n3. Finally the constructed multivariate distribution will have the form:\nF(X,Y )|\u03b8,P (x, y) = CP ( F 1X|\u03b8 ( x1 ) , . . . , F qY |\u03b8 (x q) ) .\n(9)"}, {"heading": "4.2. Bayesian inference", "text": "Separating the modelling task between specification of the margins and specification of the dependence structure simplifies the choice of the prior distributions. If we assume a priori independence for \u03b8 and P we can specify prior distributions for the margins and separately choose a prior for the parameters of the copula CP . We specify independent prior distributions for the\nblocks Px and Py, where P = ( Px 0 0 Py ) . For Px and Py we choose the marginally uniform prior given in Barnard et al. (2000). This prior is a multivariate distribution on the space of correlation matrices with uniform margins, i.e. Pij is a uniform variable for i 6= j, and is connected to the inverse-Wishart distribution: if a covariance matrix \u03a8 \u2208 Rd\u00d7d is standard inverseWishart distributed with parameter Id and d + 1 degrees of freedom, then the corresponding correlation matrix R follows the marginally uniform prior distribution.\nInference can be done using MCMC sampling methods for Dirichlet process mixture models. We use a\nsampling scheme for models with non-conjugate prior given in Neal (2011). The method, detailed in Algorithm 1, is composed of three steps: a modified Metropolis-Hastings step, partial Gibbs sampling updates and an update of the parameters \u03b8, P. In the third step we need to update the parameters of every cluster according to their posterior distribution. Since we cannot sample directly from this conditional posterior we developed a sampling scheme similar to the algorithm proposed in Hoff (2007). The main idea is to overparametrize the model by introducing a normally distributed latent vector (X\u0303, Y\u0303 ). The variables in the complete model are then given by:\n(X\u0303, Y\u0303 ) \u223c Np+q(0,\u03a3), (X,Y ) \u223cM(\u03b8, P ),\nC \u223c CRP(\u03bb),\nwhere \u03a3 is a covariance matrix with corresponding correlation matrix P and C denotes the cluster assignments following a Chinese restaurant process distribution. Figure 2 gives a representation of the complete model. In the MCMC scheme we can easily sample \u03a3 conditioned on (X,Y ), (X\u0303, Y\u0303 ) and \u03b8, since we can use the conjugacy property of prior and conditional likelihood. A sample of the correlation matrix can be otained as P(X\u0303), the correlation matrix of the random vector X\u0303. The posterior updates of the parameters are detailed in Algorithm 2. The notations \u03b8?j ,P(X\u0303)?j are used to emphasize that the corresponding vector or matrix is considered as a function of \u03b8j , X\u0303j and parameters for the other dimensions are treated as constants."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Simulated data", "text": "We simulate two different 2-dimensional multi-view data sets with Gaussian intra-view dependence structure. The marginal distributions are Gaussian in the first view, and beta or exponential in the second. Each data set is composed of two clusters which can be identified only by considering the inter-view dependencies. We first simulated data points with a single cluster structure in each view but a strong positive dependence between the first dimensions of the views, i.e. between X1 and Y 1. In a second step we separated the data in two groups of unequal size and randomly permuted their order within groups to suppress any inter-view dependency within these groups. Figure 3 (bottom left panel) shows the resulting cluster structure in the joint space of the two views recovered by the copula mixture model. Parameters used for the simulations can be found in Table 1.\nAlgorithm 1 Markov Chain Sampling\nC1, . . . , Cn are the latent variables of the cluster assignments. \u03b8Ci and PCi are the parameters for cluster Ci. n\u2212i,c is the number of datapoints in cluster c excluding observation i. C\u2212i = {C1, . . . , Ci\u22121, Ci+1, . . . , Cn}. repeat\nfor i = 1, . . . , n do if there exists k such that Ck = Ci then\nCreate a new cluster C\u2217i with parameters \u03b8 \u2217 and P \u2217 drawn from G0; Change Ci to C \u2217 i with probability\nmin ( 1, \u03bbn\u22121 f(X,Y )|\u03b8\u2217,P\u2217 (x,y)\nf (X,Y )|\u03b8Ci ,PCi (x,y)\n) ;\nelse Draw C\u2217i from C\u2212i with P(C \u2217 i = c) =\nn\u2212i,c/(n\u2212 1). Change Ci to C\u2217i with proba-\nbility min ( 1, n\u22121\u03bb f(X,Y )|\u03b8\u2217,P\u2217 (x,y)\nf (X,Y )|\u03b8Ci ,PCi (x,y)\n) ;\nend if end for for i = 1, . . . , n do\nif there exists k such that Ck = Ci then Choose a new value for Ci with P(C \u2217 i = c) \u221d\nn\u2212i,c (n\u22121)f(X,Y )|\u03b8c,P c(x, y);\nend if end for for c \u2208 {C1, . . . , Cn} do\nUpdate the parameters \u03b8c and P c as described in Algorithm 2.\nend for until stopping criterion\nAlgorithm 2 Posterior updates of (\u03b8, P ) | (X,Y ) For clarity we omit the cluster index c.\n1.Sample \u03b8|\u03a3, (X\u0303, Y\u0303 ), (X,Y ) for j = 1, . . . , p do\nDraw \u03b8j using Metropolis-Hastings; \u03b8j \u223c f(\u03b8j |\u03b8\u2212j , X\u0303,X) \u221dM(\u03b8?j ,P(X\u0303))\u03c0(\u03b8j)\nend for Apply the same procedure for Y ; 2. Sample (X\u0303, Y\u0303 )|\u03b8,\u03a3, (X,Y ) for j = 1, . . . , p do\nDraw X\u0303j using Metropolis-Hastings; X\u0303j \u223c f(X\u0303j |X\u0303\u2212j , \u03b8,\u03a3, X) \u221dM(\u03b8,P(X\u0303)?j)N (0,\u03a3)\nend for Apply the same procedure for Y\u0303 ; 3. Sample \u03a3|(X\u0303, Y\u0303 ), \u03b8, (X,Y ): Draw \u03a3x \u223c N (0,\u03a3x)IW(p+ 1, Ip)\n\u223c IW(Ip + \u2211n i=1 X\u0303iX\u0303 T i , p+ 1 + n);\nApply the same procedure to obtain \u03a3y.\nWe compared the copula mixture (CM) with three other methods: a Dirichlet prior Gaussian mixture for dependency-seeking clustering (GM) as derived in Klami & Kaski (2007), a non-Bayesian mixture of canonical correlation models (CCM) (Vrac, 2010) (Fern et al., 2005) and a variational Bayesian mixture of robust CCA models (RCCA) (Viinikanoja et al., 2010). CCM and RCCA both assume that the number of clusters is known or can be determined as explained in (Viinikanoja et al., 2010). In our comparison experiments we gave as input for both methods the correct number of clusters, giving them the advantage of this extra knowledge. Results presented in Figure 4 show that CM applied with the correct marginal distributions\u2019 form produces a better classification. GM does not perform well on those data sets because the number of clusters is overestimated; the model compensates for the inadequate Gaussian assumption by multiplying the number of components and additional clusters are created to approximate non-Gaussian distributions. Since the number of clusters in a Dirichlet prior Gaussian mixture can be reduced by imposing a too-strong prior on the variances, we modified the prior information to enforce artificially high variances in the second view until the mixture is forced to create no more than two clusters. We report both results obtained with less (GM1) and more (GM2) informative priors. As can be seen in Figure 3, when strong prior information is used to artificially reduce the number of clusters, the GM cannot recover the true cluster structure. CCM and RCCA used with the correct number of clusters as input perform comparatively, or better than the GM but clearly worse than CM for those data sets having non-linear inter-view dependencies."}, {"heading": "5.2. Real data", "text": "We perform a combined analysis of two data sets providing information about the regulation of gene expression in yeast under heat shock; each data set being treated as one view. The first data set (published in Gasch et al. (2000)) provides genes expression values measured at 4 time points. The second\nCM, view 1\nX1\nX 2\nGM1, view 1\nX1\nX 2\nGM2, view 1\nX1\nX 2\nCM, view 2\nY1\nY 2\nGM1, view 2\nY1\nY 2\nGM2, view 2\nY1\nY 2\nCM, joint space\nGM1, joint space\nGM2, joint space\ndata set (given in Harbison et al. (2004)) contains binding affinity scores for interactions between these genes and 6 different binding factors. Similar data have already been analysed in Klami & Kaski (2007). 5360 genes present in both views are clustered using a Gaussian dependency-seeking clustering model (GM) and using the copula mixture (CM). CM uses Gaussian marginals in the first view and beta marginals in the second view. Here the choice of the beta distribution is motivated by the fact that observations in the second view are restricted to the [0, 1] interval. For the univariate Gaussian margins we choose normal and inverse-gamma priors for mean and variance respectively, whereas for the beta margins both shape parameters have gamma priors. GM uses the standard conjugate prior 2.\nFor different values of the concentration parameter \u03bb \u2208 {0.01, 0.1, 1, 5, 10}, CM consistently estimates 8 clusters whereas GM estimated between 13 and 15 clusters. In this section we report the results obtained with \u03bb = 1. As we observed with the simulated data more clusters need to be created by the Gaussian mixture to compensate for the model mismatch. This phenomenon is illustrated in Figure 5. The interpretation of the clustering then becomes very arduous since these additional clusters cannot be distinguished from those capturing the dependencies. Another interpretation problem clearly arises in the Gaussian model when we look at the estimated intra-view correlations. Two negative effects accumulate here; first correlation can be an inadequate dependence measure for non-normally distributed data, and second the additional split in many components can change the cluster-specific intra-view dependence as illustrated in Figure 6.\nTo understand what information one could gain by dependency-clustering, we perform three additional clustering of the same data: first we cluster the datapoints on each view separately, then we cluster them in the complete product space of the joint views, i.e. without imposing the constraint of a block structure on the correlation matrix. Priors and hyperparameters are kept constant over experiments. CM finds four clusters in the first view as well as in the second view. Clustering in the product space with full correlation matrix again leads to four groups. Figure 7 illustrates how the three main clusters found in the complete product space are further separated by dependency-seeking clustering, showing dependencies between the two views.\n2The use of conjugate prior does not, in general, increase the number of clusters as shown in Rasmussen & Go\u0308ru\u0308r (2010).\nAs mentioned in section 1, GM cannot exclusively focus on compact clusters because it needs to find a compromise between the cluster homogeneity and the approximation of a non-Gaussian mixture. As a result, non-homogenous clusters might emerge which are needed to fit the margins despite model mismatch. To test if this phenomenon is present here, we perform a gene ontology enrichment analysis (GOEA) using GOrilla (Eden et al., 2009). GOEA is used to test if some of the biological processes associated with the genes are over-represented in the clusters, thereby providing a quality measure for the clustering. The analysis shows that 3 out of 14 clusters (these 3 clusters representing together 17,3% of the data points) found by GM do not express any significant enrichment. By contrast, all 8 clusters produced by CM express a highly significant enrichment and every cluster can be associated with a specific biological processes, e.g. the two largest clusters can be interpreted as groups of genes involved in organelle organization and meiosis respectively. The clear difference in the enrichment analysis results between GM and CM demonstrates that the quality of the clustering is indeed impaired when a model with inadequate margins is used."}, {"heading": "6. Conclusion", "text": "A fundamental aspect in dependency-seeking clustering is that the partition possesses a semantic interpretation in terms of dependency: the dependencies are\ncaptured by the cluster structure. This interpretation is however only valid when the model is rich enough to properly fit each view, which can be particularly difficult to achieve for non-Gaussian data with existing models. This task becomes even more arduous when the dimensions of the views increase since the model then needs to adequately fit every margin while allowing for a sufficiently rich intra-view dependence structure. The copula mixture model offers enough flexibility to cover both aspects: the margins can be specified separately for each dimension and the Gaussian copula allows for a wide range of intra-view dependencies. Using a Gaussian copula also facilitates the inference and we provide an efficient MCMC scheme. Experiments on simulated data show that the copula mixture model significantly improves the clustering results. In a largescale real-world clustering problem of genes expression data and genes binding affinities, the dependencyseeking copula mixture model produces a clustering solution that significantly differs from those obtained on the single views or on the product space, and from that obtained by the standard Gaussian model which clearly suffered from model-mismatch problems. Detailed analysis of the functional annotation of the genes in the clusters discovered by dependency-seeking CM shows that the induced cluster structure allows a plausible biological interpretation in that the groups are clearly enriched by genes involved in distinct biological processes."}], "references": [{"title": "A probabilistic interpretation of canonical correlation analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Technical report 688,", "citeRegEx": "Bach and Jordan,? \\Q2005\\E", "shortCiteRegEx": "Bach and Jordan", "year": 2005}, {"title": "Modeling covariance matrices in terms of standard deviations and correlations, with application to shrinkage", "author": ["J. Barnard", "R. McCulloch", "X. Meng"], "venue": "Statistica Sinica,", "citeRegEx": "Barnard et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Barnard et al\\.", "year": 2000}, {"title": "GOrilla: a tool for discovery and visualization of enriched go terms in ranked gene", "author": ["E. Eden", "R. Navon", "I. Steinfeld", "D. Lipson", "Z. Yakhini"], "venue": "lists. BMC Bioinformatics,", "citeRegEx": "Eden et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Eden et al\\.", "year": 2009}, {"title": "A Bayesian analysis of some nonparametric problems", "author": ["T. Ferguson"], "venue": "Annals of Statistics,", "citeRegEx": "Ferguson,? \\Q1973\\E", "shortCiteRegEx": "Ferguson", "year": 1973}, {"title": "Correlation clustering for learning mixture of canonical correlation models", "author": ["X.Z. Fern", "C.E. Brodley", "M.A. Friedl"], "venue": "Accepted for SIAM International Conference on Data Mining,", "citeRegEx": "Fern et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Fern et al\\.", "year": 2005}, {"title": "Genomic expression programs in the response of yeast cells to environmental changes", "author": ["A.P. Gasch", "P.T. Spellman", "C.M. Kao"], "venue": "Molecular Biology of the Cell,", "citeRegEx": "Gasch et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gasch et al\\.", "year": 2000}, {"title": "Transcriptional regulatory code of a eukaryotic", "author": ["C.T. Harbison", "D.B. Gordon", "T.I. Lee"], "venue": "genome. Nature,", "citeRegEx": "Harbison et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Harbison et al\\.", "year": 2004}, {"title": "Extending the rank likelihood for semiparametric copula estimation", "author": ["Hoff", "Peter D"], "venue": "Annals of Applied Statistics,", "citeRegEx": "Hoff and D.,? \\Q2007\\E", "shortCiteRegEx": "Hoff and D.", "year": 2007}, {"title": "Yeast Stress Responses", "author": ["S. Hohmann", "W.H. Mager"], "venue": "Topics in Current Genetics,", "citeRegEx": "Hohmann and Mager,? \\Q2003\\E", "shortCiteRegEx": "Hohmann and Mager", "year": 2003}, {"title": "Local dependent components", "author": ["A. Klami", "S. Kaski"], "venue": "Proceedings of the 24th International Conference on Machine Learning,", "citeRegEx": "Klami and Kaski,? \\Q2007\\E", "shortCiteRegEx": "Klami and Kaski", "year": 2007}, {"title": "Probabilistic approach to detecting dependencies between data", "author": ["A. Klami", "S. Kaski"], "venue": "sets. Neurocomputing,", "citeRegEx": "Klami and Kaski,? \\Q2008\\E", "shortCiteRegEx": "Klami and Kaski", "year": 2008}, {"title": "Bayesian exponential family projections for coupled data sources", "author": ["A. Klami", "S. Virtanen", "S. Kaski"], "venue": "Uncertainty in Artificial Intelligence,", "citeRegEx": "Klami et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Klami et al\\.", "year": 2010}, {"title": "Quantitative Risk Management", "author": ["A.J. McNeil", "R. Frey", "P. Embrechts"], "venue": null, "citeRegEx": "McNeil et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McNeil et al\\.", "year": 2005}, {"title": "Markov chain sampling methods for Dirichlet process mixture models", "author": ["R.M. Neal"], "venue": "Technical report 9815,", "citeRegEx": "Neal,? \\Q2011\\E", "shortCiteRegEx": "Neal", "year": 2011}, {"title": "Dirichlet process Gaussian mixture models: Choice of the base distribution", "author": ["C.E. Rasmussen", "D. G\u00f6r\u00fcr"], "venue": "Journal of Computer Science and Technology,", "citeRegEx": "Rasmussen and G\u00f6r\u00fcr,? \\Q2010\\E", "shortCiteRegEx": "Rasmussen and G\u00f6r\u00fcr", "year": 2010}, {"title": "Fonctions de r\u00e9partition \u00e0 n dimensions et leurs marges", "author": ["A. Sklar"], "venue": "Publications de l\u2019Institut de Statistique de l\u2019Universite\u0301 de Paris,", "citeRegEx": "Sklar,? \\Q1959\\E", "shortCiteRegEx": "Sklar", "year": 1959}, {"title": "Variational Bayesian mixture of robust CCA models. Principles of Data Mining and Knowledge Discovery", "author": ["J. Viinikanoja", "A. Klami", "S. Kaski"], "venue": null, "citeRegEx": "Viinikanoja et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Viinikanoja et al\\.", "year": 2010}, {"title": "CCMtools: Clustering through \u201dCorrelation Clustering Model", "author": ["Vrac", "Mathieu"], "venue": "(CCM) and cluster analysis tools.,", "citeRegEx": "Vrac and Mathieu.,? \\Q2010\\E", "shortCiteRegEx": "Vrac and Mathieu.", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "Fern et al. (2005) introduces a mixture of local CCA model which groups pairs of points expressing together a particular linear dependency between the two views.", "startOffset": 0, "endOffset": 19}, {"referenceID": 11, "context": "However, as pointed out in Klami et al. (2010), when the data are not normally distributed, this method can suffer from a severe model mismatch problem.", "startOffset": 27, "endOffset": 47}, {"referenceID": 11, "context": "An exponential family dependency-seeking method is proposed in Klami et al. (2010) to overcome this problem.", "startOffset": 63, "endOffset": 83}, {"referenceID": 15, "context": "The following theorem Sklar (1959) states the relationship between copulas and multivariate distributions.", "startOffset": 22, "endOffset": 35}, {"referenceID": 12, "context": "Since Gaussian copulas are invariant to strictly increasing transformations, the copula of Nd (\u03bc,\u03a3) is the same as the copula of Nd (0, P ) as mentioned in McNeil et al. (2005), where P is the correlation matrix corresponding to the covariance matrix \u03a3.", "startOffset": 156, "endOffset": 177}, {"referenceID": 3, "context": "where \u03bcG is the distribution of a Dirichlet process (Ferguson, 1973) with base distribution G0 and concentration parameter \u03bb.", "startOffset": 52, "endOffset": 68}, {"referenceID": 1, "context": "and Py we choose the marginally uniform prior given in Barnard et al. (2000). This prior is a multivariate distribution on the space of correlation matrices with uniform margins, i.", "startOffset": 55, "endOffset": 77}, {"referenceID": 13, "context": "We use a sampling scheme for models with non-conjugate prior given in Neal (2011). The method, detailed in Algorithm 1, is composed of three steps: a modified Metropolis-Hastings step, partial Gibbs sampling updates and an update of the parameters \u03b8, P.", "startOffset": 70, "endOffset": 82}, {"referenceID": 13, "context": "We use a sampling scheme for models with non-conjugate prior given in Neal (2011). The method, detailed in Algorithm 1, is composed of three steps: a modified Metropolis-Hastings step, partial Gibbs sampling updates and an update of the parameters \u03b8, P. In the third step we need to update the parameters of every cluster according to their posterior distribution. Since we cannot sample directly from this conditional posterior we developed a sampling scheme similar to the algorithm proposed in Hoff (2007). The main idea is to overparametrize the model by introducing a normally distributed latent vector (X\u0303, \u1ef8 ).", "startOffset": 70, "endOffset": 509}, {"referenceID": 4, "context": "We compared the copula mixture (CM) with three other methods: a Dirichlet prior Gaussian mixture for dependency-seeking clustering (GM) as derived in Klami & Kaski (2007), a non-Bayesian mixture of canonical correlation models (CCM) (Vrac, 2010) (Fern et al., 2005) and a variational Bayesian mixture of robust CCA models (RCCA) (Viinikanoja et al.", "startOffset": 246, "endOffset": 265}, {"referenceID": 16, "context": ", 2005) and a variational Bayesian mixture of robust CCA models (RCCA) (Viinikanoja et al., 2010).", "startOffset": 71, "endOffset": 97}, {"referenceID": 16, "context": "CCM and RCCA both assume that the number of clusters is known or can be determined as explained in (Viinikanoja et al., 2010).", "startOffset": 99, "endOffset": 125}, {"referenceID": 5, "context": "The first data set (published in Gasch et al. (2000)) provides genes expression values measured at 4 time points.", "startOffset": 33, "endOffset": 53}, {"referenceID": 6, "context": "data set (given in Harbison et al. (2004)) contains binding affinity scores for interactions between these genes and 6 different binding factors.", "startOffset": 19, "endOffset": 42}, {"referenceID": 6, "context": "data set (given in Harbison et al. (2004)) contains binding affinity scores for interactions between these genes and 6 different binding factors. Similar data have already been analysed in Klami & Kaski (2007). 5360 genes present in both views are clustered using a Gaussian dependency-seeking clustering model (GM) and using the copula mixture (CM).", "startOffset": 19, "endOffset": 210}, {"referenceID": 2, "context": "To test if this phenomenon is present here, we perform a gene ontology enrichment analysis (GOEA) using GOrilla (Eden et al., 2009).", "startOffset": 112, "endOffset": 131}], "year": 2012, "abstractText": "We introduce a copula mixture model to perform dependency-seeking clustering when cooccurring samples from different data sources are available. The model takes advantage of the great flexibility offered by the copulas framework to extend mixtures of Canonical Correlation Analysis to multivariate data with arbitrary continuous marginal densities. We formulate our model as a non-parametric Bayesian mixture, while providing efficient MCMC inference. Experiments on synthetic and real data demonstrate that the increased flexibility of the copula mixture significantly improves the clustering and the interpretability of the results.", "creator": "LaTeX with hyperref package"}}}