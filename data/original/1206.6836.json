{"id": "1206.6836", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Methods for computing state similarity in Markov Decision Processes", "abstract": "A popular approach to solving large probabilistic systems relies on aggregating states based on a measure of similarity. Many approaches in the literature are heuristic. A number of recent methods rely instead on metrics based on the notion of bisimulation, or behavioral equivalence between states (Givan et al, 2001, 2003; Ferns et al, 2004). An integral component of such metrics is the Kantorovich metric between probability distributions. However, while this metric enables many satisfying theoretical properties, it is costly to compute in practice. In this paper, we use techniques from network optimization and statistical sampling to overcome this problem. We obtain in this manner a variety of distance functions for MDP state aggregation, which differ in the tradeoff between time and space complexity, as well as the quality of the aggregation. We provide an empirical evaluation of these trade-offs.", "histories": [["v1", "Wed, 27 Jun 2012 16:18:48 GMT  (182kb)", "http://arxiv.org/abs/1206.6836v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["norman ferns", "pablo samuel castro", "doina precup", "prakash panangaden"], "accepted": false, "id": "1206.6836"}, "pdf": {"name": "1206.6836.pdf", "metadata": {"source": "CRF", "title": "Methods for Computing State Similarity in Markov Decision Processes", "authors": ["Norm Ferns", "Pablo Samuel Castro", "Doina Precup", "Prakash Panangaden"], "emails": ["prakash}@cs.mcgill.ca"], "sections": [{"heading": null, "text": "A popular approach to solving large probabilistic systems relies on aggregating states based on a measure of similarity. Many approaches in the literature are heuristic. A number of recent methods rely instead on metrics based on the notion of bisimulation, or behavioral equivalence between states (Givan et al., 2003; Ferns et al., 2004). An integral component of such metrics is the Kantorovich metric between probability distributions. However, while this metric enables many satisfying theoretical properties, it is costly to compute in practice. In this paper, we use techniques from network optimization and statistical sampling to overcome this problem. We obtain in this manner a variety of distance functions for MDP state aggregation that differ in the tradeoff between time and space complexity, as well as the quality of the aggregation. We provide an empirical evaluation of these tradeoffs."}, {"heading": "1 Introduction", "text": "Markov decision processes (MDPs) are the model of choice for decision making under uncertainty (Boutilier et al., 1999), and provide a standard formalism for describing multi-stage decision making in probabilistic environments. The objective of the decision making is to maximize a cumulative measure of long-term performance, called the return. Dynamic programming algorithms, e.g., value iteration or policy iteration (Puterman, 1994), allow computing the optimal expected return for any state, as well as the way of behaving (policy) that generates this return. However, in many practical situations, the state space of an MDP is too large for the standard algorithms to be applied. One popular technique for overcoming this problem is state aggregation: states are grouped together into blocks, or partitions, and a new MDP is defined over these. The hope is that this can be done in such a manner as to construct an \u201cessentially equiv-\nalent\u201d MDP with drastically reduced state space, thereby allowing the use of classical solution methods, while at the same time providing a guarantee that solutions of the reduced MDP can be extended to the original model.\nIt has been well argued that the notion of \u201cessentially equivalent\u201d in probabilistic systems is perhaps best captured formally by bisimulation (Milner, 1980; Park, 1981; Larsen & Skou, 1991). In the context of MDPs, bisimulation can roughly be described as the largest equivalence relation on the state space of an MDP that relates two states precisely when for every action, they achieve the same immediate reward and have the same probability of transitioning to classes of equivalent states. This means that bisimilar states lead to essentially the same long-term behavior. The bisimulation equivalence classes can even be computed iteratively in polynomial time (Givan et al., 2003). However, it has also been well established that using exact equivalences in probabilistic systems is problematic. A notion of equivalence is two-valued: two states are either equivalent or not equivalent. A small perturbation of the transition probabilities can make two equivalent states no longer equivalent. In short, any kind of equivalence is too unstable, too sensitive to perturbations of the numerical values of the transition probabilities.\nA natural remedy is to use (pseudo)metrics. Metrics are natural quantitative analogues of the notion of equivalence relation. For example, the triangle inequality is a natural quantitative analogue of transitivity. The metrics on which we focus specify the degree to which objects of interest behave similarly. In Ferns et al. (2004; 2005), based on similar work in the context of labeled Markov processes (Desharnais et al., 1999; van Breugel and Worrell 2001a; 2001b), we sought to extend bisimulation for MDPs quantitatively in terms of such metrics. Our metrics, based on the Kantorovich probability metric, indeed not only provide the appropriate generalization of bisimulation, but satisfy many nice additional theoretical properties as well. Unfortunately, this integral component, the Kantorovich metric, is also one that makes our metrics expensive to compute in practice.\nIn this paper, we explore ways of obtaining practical distance metrics through efficient computation and approximation of the Kantorovich metric. We use techniques from optimization and sampling to obtain variations on our bisimulation metrics that are easier to compute in practice while still maintaining theoretical guarantees.\nThe paper is organized as follows. In Section 2 we provide the notation and theoretical background necessary for understanding the problem at hand. In Section 3 we introduce various candidate state similarity metrics and discuss the merits and drawbacks of each. We provide experiments in Section 4, to compare and contrast these. Finally, Section 5 contains conclusions and directions for future work."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Markov Decision Processes", "text": "A finite Markov decision process is a quadruple (S,A,{Pass\u2032},{r a s }) where S is a finite set of states, A is a finite set of actions, Pass\u2032 is a (Markovian) probability of transitioning from state s to s\u2032 under action a, and ras is the numerical reward for choosing action a in state s.\nThe discounted, infinite horizon planning task in an MDP is to determine a policy \u03c0 : S \u2192 A that maximizes the value of every state, V \u03c0(s) = E[\u2211\u221et=0 \u03b3trt+1|s0 = s,\u03c0], where s0 is the state at time 0, rt+1 is the reward achieved at time t +1, \u03b3 is a discount factor in (0,1), and the expectation is taken by following the state dynamics induced by \u03c0. The function V \u03c0 is called the value function of policy \u03c0. The optimal value function V \u2217, associated with an optimal policy, is the unique solution of the set of equations:\nV \u2217(s) = max a\u2208A (ras + \u03b3 \u2211 s\u2032\u2208S Pass\u2032V \u2217(s\u2032))\nand can be used to determine an optimal policy, by choosing actions greedily. In fact, the optimal value function can be computed as the limit of a sequence of iterates (Puterman, 1994, theorem 6.2.12) . Define V0 = 0 and Vn+1(s) = maxa\u2208A(ras + \u03b3\u2211s\u2032\u2208S Pass\u2032Vn(s\n\u2032)). Then {Vn} converges uniformly to V \u2217.\nThese results can be realized via a dynamic programming (DP) algorithm; however, it is often the case that the state space of the given MDP is too large for DP to be feasible. When this happens, a standard strategy is to approximate the given MDP by aggregating its state space. The hope is that one can obtain a smaller \u201cequivalent\u201d MDP with an easily computable value function that could provide information about the value function of the original MDP. Givan et al. (2003) investigated several notions of MDP state equivalence and determined that the most appropriate is bisimulation.\nDefinition 2.1. Bisimulation is the largest equivalence re-\nlation \u223c on S satisfying the following property:\ns \u223c s\u2032 \u21d0\u21d2 \u2200a \u2208 A, (ras = r a s\u2032 and\n\u2200C \u2208 S/ \u223c, Pas (C) = P a s\u2032(C))\nUnfortunately, as an exact equivalence, bisimulation suffers from issues of instability; that is, slight numerical differences in the MDP parameters, {ras } and {P a ss\u2032}, can lead to vastly different bisimulation partitions. To get around this, one generalizes the notion of bisimulation equivalence through bisimulation metrics."}, {"heading": "2.2 Bisimulation Metrics", "text": "In Ferns et al. (2004; 2005) we provide the following metric generalization of bisimulation 1:\nTheorem 2.2. Let c\u2208 (0,1) and V be the set of real-valued functions on S\u00d7 S that are bounded by R(1\u2212c) , where R = maxs,s\u2032,a |ras \u2212 r a s\u2032 |. Define F c : V \u2192 V by\nFc(h)(s,s\u2032) = max a\u2208A (|ras \u2212 r a s\u2032 |+ cTK(h)(P a s ,P a s\u2032))\nThen :\n1. Fc has a unique fixed point dcf ix,\n2. dcf ix(s,s \u2032) = 0 \u21d0\u21d2 s \u223c s\u2032, and\n3. for any h0 \u2208 V ,\n\u2016dcf ix \u2212 (F c)n(h0)\u2016 \u2264\ncn\n1\u2212 c \u2016Fc(h0)\u2212h0\u2016.\nHere TK(h)(P,Q) is the Kantorovich probability metric2 applied to distributions P and Q. It is defined as min\u03bb E\u03bb[h] where \u03bb is a joint probability function on S\u00d7S with projections P and Q, i.e.\nmin \u03bbk j\n|S|\n\u2211 k, j=1 \u03bbk jh(sk,s j)\nsubject to: \u2200k. \u2211 j \u03bbk j = P(sk)\n\u2200 j. \u2211 k \u03bbk j = Q(s j)\n\u2200k, j. \u03bbk j \u2265 0\nThis formulation shows that this metric is an instance of the minimum cost flow (MCF) network optimization linear program (LP). Since there exist strongly polynomial algorithms to compute the MCF problem (Orlin, 1988), the\n1Results appear here in slightly modified form. 2Frustratingly also known as Monge-Kantorovich, Kantorovich-Rubinstein, Hutchinson, Mallows, Wasserstein, Vasserstein, Earth Mover\u2019s Distance, and more!\nKantorovich metric can be computed in polynomial time. For our purposes, this amounts to a worst case running time of O(|S|3 log |S|) for each Kantorovich LP (contrast this with the general LP for directly computing the optimal value function: this has |S| variables and |A||S| constraints (Puterman, 1994)).\nThe key property of the Kantorovich metric is that it matches distributions, i.e. assigns them distance zero only when they agree on the equivalence classes induced by the underlying cost function. Therefore, it is not surprising that it can be used to capture the notion of bisimulation, which requires that probabilistic transitions agree on bisimulation equivalence classes.\nThe Kantorovich metric also admits a characterization in terms of the coupling of random variables. We may write TK(h)(P,Q) = min(X ,Y ) E[h(X ,Y )] where the expectation is taken with respect to the joint distribution of (X ,Y ) and the minimum is taken with respect to all pairs of random variables (X ,Y ) such that the marginal distribution of X is P and the marginal distribution of Y is Q."}, {"heading": "3 State Similarity Metrics", "text": ""}, {"heading": "3.1 Fixed Point", "text": "The metric defined by theorem 2.2 can be re-written as:\ndcf ix(s,s \u2032) = max a\u2208A (|ras \u2212 r a s\u2032 |+ cTK(d c f ix)(P a s ,P a s\u2032)).\nEach such metric is continuous in the MDP parameters {ras } and {P a ss\u2032} and admits tight bounds on the optimal value function, since the optimal value function with discount factor \u03b3 is Lipschitz-continuous with respect to each metric satisfying c \u2265 \u03b3. Thus, when using this metric to aggregate states, it is easy to address issues of instability and being able to recover optimal solutions. Moreover, the theorem provides a way of calculating dcf ix: starting with the metric that is zero everywhere, iteratively apply F c until a prescribed degree of accuracy is achieved. Unfortunately, directly computing the Kantorovich metric at each iteration is too costly in practice, severely limiting the use of the fixed point metrics."}, {"heading": "3.2 Fixed Point with Cost Reoptimization", "text": "One way of overcoming the costly computation of the Kantorovich LP for every iteration is to use cost reoptimization. The idea is that if h0 is close to h1 in (uniform norm) distance then optimal solutions to TK(h0)(P,Q) and TK(h1)(P,Q) should be close too; so instead of starting a network optimization algorithm for TK(h1)(P,Q) from scratch we save the optimal solution to TK(h0)(P,Q) and use it as the starting solution3. In a sense, we are comput-\n3In LP jargon this concept is known as sensitivity analysis. Convergence of the iterates (Fc)n(h0) to dcf ix in uniform norm\ning one MCF LP with cost function dcf ix. This idea has been rather extensively and successfully explored in (Frangioni & Manca., 2006). Note that any savings in time comes at the cost of space requirements, as we are now required to save solutions for each Kantorovich LP between iterations."}, {"heading": "3.3 Fixed Point with Sampling", "text": "A more promising approach is a quick and efficient approximation arising from statistical sampling. Suppose P and Q are approximated using the empirical distributions Pi and Qi. That is, we sample i points X1,X2, . . . ,Xi independently according to P and define Pi by Pi(x) = 1i \u2211 i k=1 \u03b4Xk(x). Similarly, write Qi(x) = 1i \u2211 i k=1 \u03b4Yk(x). Then\nTK(h)(Pi,Qi) = min\u03c3 1 i\ni\n\u2211 k=1 h(Xk,Y\u03c3(k)) (1)\nwhere the minimum is taken over all permutations \u03c3 on i elements (see p. 12 of Villani (2002); this is a consequence of Birkhoff\u2019s theorem). Now the Strong Law of Large Numbers (SLLN) tells us that both {Pi(x)} and {Qi(x)} converge almost surely to P(x) and Q(x)4. Let us write T iK(h)(P,Q) for TK(h)(Pi,Qi) when the empirical distributions are fixed. Then as a consequence of the SLLN, {T iK(h)(P,Q)} converges to TK(h)(P,Q) almost surely; moreover replacing TK by T iK in F c yields a metric,\ndci (s,s \u2032) = max a\u2208A (|ras \u2212 r a s\u2032 |+ cT i K(d c i )(P a s ,P a s\u2032)),\nwhich converges to dcf ix as i gets large (see appendix A for an outline of the proof).\nThe importance of this result stems from the fact that the expression in equation (1) is an instance of the assignment problem from network optimization. This is a special form of the MCF problem in which the underlying network is bipartite and all flow assignments are either 0 or 15. Its specialized structure allows for simpler, faster solution methods. For example, the Hungarian algorithm (for a description see Munkres (1957)) runs in worst case time O(i3), where i is the number of samples.\nFor the continuous space R with the usual Euclidean metric this approximation of the Kantorovich distance is commonly known as the empirical Mallows distance. It is used to test equivalence of empirical distributions in statistics."}, {"heading": "3.4 Total Variation", "text": "The standard metric for measuring the distance between probability functions is the total variation metric, defined by TV (P,Q) = 12 \u2211s\u2208S |P(s)\u2212Q(s)|, which is half the L 1-\nguarantees that it applies here. 4Note that both Pi and Qi are random variables. 5In graph theoretic terminology, this is the problem of optimal matching in a weighted graph.\nnorm of P\u2212Q. It is a strong measure of convergence, in the sense that distributions will have distance zero only when they agree exactly on transitions to every state. In contrast, the Kantorovich metric demands agreement only on classes of states. Nevertheless, the total variation is a simple concept and one that is easy to compute.\nIn Ferns et al. (2004), we suggested that in place of iteratively applying Fc to an initial metric h0 until convergence to dcf ix, we start with an appropriately chosen h0 and apply Fc only once. If we take h0 to be equal to the discrete metric assigning distance R(1\u2212c)\n6 to all pairs of unequal states then the resultant metric is\ndcTV (s,s \u2032) = max a\u2208A (|ras \u2212 r a s\u2032 |+ cR 1\u2212 c TV (Pas ,P a s\u2032)).\nThis is in some sense the simplest metric one can compute, yet the one also providing the least guarantees (at the other extreme lies dcf ix). On the other hand, if we take h0 to be equal to the discrete metric assigning distance R(1\u2212c) to all pairs of states that are not bisimilar, then the resulting metric is\ndc\u223c(s,s \u2032) = max a\u2208A (|ras \u2212 r a s\u2032 |+ cR 1\u2212 c TV\u223c(P a s ,P a s\u2032)),\nwhose probability metric component TV\u223c(P,Q) := 1 2 \u2211C\u2208S/\u223c |P(C) \u2212 Q(C)| is the total variation distance defined with respect to the minimized bisimilar MDP. It is relatively simply to compute, requiring only the computation of the exact bisimulation partition. However, as this latter component is unstable, so is the resultant metric. More precisely, this metric is not continuous in the MDP parameters.\nNote that by monotonicity of the functional TK , we immediately have dcf ix \u2264 d c \u223c \u2264 d c TV ."}, {"heading": "4 Experiments", "text": "Experiments were run on four different MDPs: a 3\u00d73 grid world with two actions (move forward and rotate) and a single reward in the center of the room; a 5\u00d75 and a 7\u00d77 grid world each with the same dynamics; and a flattened out version of the coffee robot MDP (Boutilier et al., 1995) where the robot has to get coffee for the user and avoid getting wet. Note that we chose on purpose small environments, which would allow a thorough study of all the properties of the metrics. For all gridworlds, the state includes both the position as well as the orientation of the agent. So the three gridworlds have 36, 100 and 196 states respectively. The actions are deterministic. The coffee domain has 64 states and 4 actions, some with stochastic effects. Five methods were used to compute distances for these MDPs: dcf ix, d c f ix with cost reoptimization, dci via sampling, d c TV , and d c \u223c.\n6This scaling factor is added to ensure that h0 belongs to V .\nThe first two methods find the fixed point metric by computing the distance between two distributions through the Kantorovich metric. This latter computation was done using the MCFZIB Minimum Cost Flow solver (Frangioni & Manca., 2006) for each pair of states and each action. The first of these methods will be referred to as Kantorovich. Cost reoptimization was used in the second of these exact metrics in order to speed up the computation, at the expense of larger space requirements. This second method will thus be referred to as Kantorovich (reoptimization). The third method uses statistical sampling to approximate the transition distributions of each state. For all MDPs, 10 transition samples were taken for each state and action, and this vector of samples was used to estimate the empirical distribution throughout the whole run. The Hungarian algorithm was used to solve the assignment problem, as described in Section 3.3. The distance metric was obtained by averaging the distances obtained over 30 independent runs of this procedure. This method will be referred to as Sample.\nThe fourth method used the total variation metric; this provides a loose upper bound on the other metrics, but is much faster to compute. It will be referred to as TV. The fifth method uses the total variation metric with bisimilar equivalence classes, which provides a tighter upper bound. It will be referred to as Bisim. These metrics were computed using three different values for the discount factor: \u03b3 = {0.1,0.5,0.9}.\nTable 1 summarizes the running times in seconds for each method with the different discount factors. A \u2018-\u2019 means that the algorithm failed to compute the metric.\nThe amount of space used by each method was also compared. This was measured using the massif tool of valgrind (a tool library in Linux). Table 2 presents the maximum number of bytes used by each algorithm when computing the distances for the MDPs.\nThe distance metric computed is then used to reduce the state space of the original MDP by means of two aggrega-\ntion methods. In the first approach, the number of desired partitions is specified beforehand and a greedy, incremental aggregation procedure is used until the desired reduction has been reached. More precisely, each state starts as its own partition. Then, the algorithm greedily picks the two closest partitions and merges them. The distance between partitions is the minimum distance between pairs of states belonging to each partition. The value function is then computed for each state in the original MDP and each partition in the reduced MDP using value iteration. In Figures 1, 2, 3, and 4 we present the quality of the results obtained, measured as the L\u221e norm of the value function error. More precisely, for each state, we compute the difference between its value in the original MDP, and its value as estimated in the aggregate MDP, and take the maximum absolute difference over all states, maxs\u2208S |V \u2217(s)\u2212V \u2217([s])|, where [s] is the cluster containing s.\nThe second aggregation method greedily adds a state to a partition if its minimum distance to any state in the partition is less than \u03b5. Both the process of creating partitions (by picking pairs of states to group) and adding states to partitions is greedy. The algorithm will stop when no more merging can be performed. Note that higher values of \u03b5 will lead to fewer partitions. The results are presented in Figures 5, 6, 7, and 8.\nBased on the results presented here, the sampling method outperforms all the other methods when considering space, time and quality. The quality of the value functions obtained when using this metric for state aggregation is almost identical to the quality obtained for the \u201cperfect\u201d metric, dcf ix. However, the memory requirements and time requirements are significantly smaller. The exact computation of the Kantorovich metric failed to finish for the two larger environments. Using cost reoptimization reduces the computation time by a factor of 2, on these small environments, but requires even more memory, so this method only completed for the smallest environment. The total variation metric, TV, is very cheap to compute but has very poor results in terms of the approximation error. The Bisim metric is slightly better than TV, but significantly worse than the sampling solution. However, the computation time and space are very similar to the ones of the sampling method.7\n7Notice how the graphs plotting \u03b5 versus L\u221e norm and size of aggregated MDP demonstrate that since the approximations are upper bounds, their L\u221e norm appears \u2018better\u2019 than the exact methods, but at the expense of compression."}, {"heading": "5 Conclusions and future work", "text": "In this paper, we discussed four state similarity metrics based on the notion of bisimulation. We compared and contrasted these both in theory and in practice. Based on these results, the metric obtained by means of sampling distributions, appears to be the clear winner: it significantly outperforms the other approaches when considering the tradeoff between the computational requirements of time and space, on one hand, and the quality of the results obtained when using this method for state aggregation, on the other hand. The next step is to test this metric on large-scale environments. Different versions of this metric, based on ideas from incremental reinforcement learning algorithms, rather than batch processing, will also be explored. Using such techniques, the computation could be made significantly faster. Versions of these metrics for factored state spaces are also of great interest.\nThe sampling approach is also promising for computing metrics in continuous state spaces. In prior works, we established the existence of of bisimulation metrics for continuous MDPs (Ferns et al., 2005). We are hopeful that the idea of approximating measures through empirical distributions will enable us to estimate the Kantorovich metric in a manner similar to the discrete case."}, {"heading": "Acknowledgments", "text": "This work has been supported in part by funding from NSERC and CFI."}, {"heading": "Appendix A: Proofs", "text": "Lemma 5.1. For any P and Q, {T iK(h)(P,Q)} converges to TK(h)(P,Q) almost surely for any h \u2208 V .\nProof. Let \u03b5 > 0. By the SLLN for each x \u2208 S, {Pi(x)} converges to P(x) almost surely. In fact, since there are finitely many x we have {Pi(x)} converges to P(x) almost surely for all x, and similarly for Q. So for each x we may choose ix so large that\n|Pi(x)\u2212P(x)|, |Qi(x)\u2212Q(x)| < (1\u2212 c)\u03b5 |S|R for all i \u2265 ix.\nDefine i0 = maxx\u2208S ix. Then for all i \u2265 i0\n|TK(h)(P,Q)\u2212T i K(h)(P,Q)|\n= |TK(h)(P,Q)\u2212TK(h)(Pi,Qi)|\n\u2264 |TK(h)(P,Pi)+TK(h)(Q,Qi)|\n\u2264 R\n1\u2212 c (TV (P,Pi)+TV (Q,Qi))\n= R\n2(1\u2212 c) (\u2211\nx\u2208S |P(x)\u2212Pi(x)|+ \u2211 x\u2208S |Q(x)\u2212Qi(x)|)\n\u2264 R\n2(1\u2212 c) (\u2211\nx\u2208S\n(1\u2212 c)\u03b5 |S|R + \u2211 x\u2208S (1\u2212 c)\u03b5 |S|R )\n\u2264 \u03b5\nHere we have used the triangle inequality, monotonicity of the Kantorovich functional TK , as well as the fact that TK applied to the discrete metric is TV .\nTheorem 5.2. Given the setup of theorem 2.2, define Fci : V \u2192 V by\nFci (h)(s,s \u2032) = max a\u2208A (|ras \u2212 r a s\u2032 |+ cT i K(h)(P a s ,P a s\u2032))\nThen :\n1. Fci has a unique fixed point d c i ,\n2. for any h0 \u2208 V ,\n\u2016dci \u2212 (F c i ) n(h0)\u2016 \u2264 cn\n1\u2212 c \u2016Fci (h0)\u2212h0\u2016,\n3. dci converges to d c f ix in uniform norm (almost surely).\nProof. Note that the first two follow from theorem 2.2; by simply replacing the MDP probability parameters with the empirical distributions we obtain dci as the fixed point metric defined over the new MDP.\nLet \u03b5 > 0. By lemma 5.1 we may choose i so large that for any x,y,a, and h:\n|T iK(h)(P a x ,P a y )\u2212TK(h)(P a x ,P a y )| < (1\u2212 c)\u03b5 c .\nThen\n|dci (x,y)\u2212d c f ix(x,y)|\n\u2264 cmax a\n|T iK(d c i )(P a x ,P a y )\u2212TK(d c f ix)(P a x ,P a y )|\n\u2264 cmax a\n|T iK((d c i \u2212d c f ix)+d c f ix)(P a x ,P a y )\u2212\nTK(d c f ix)(P a x ,P a y )|\n\u2264 c(\u2016dci \u2212d c f ix\u2016+maxa |T iK(d c f ix)(P a x ,P a y )\u2212\nTK(d c f ix)(P a x ,P a y )|)\n\u2264 c(\u2016dci \u2212d c f ix\u2016+ (1\u2212 c)\u03b5 c )\nThus, we obtain \u2016dci \u2212d c f ix\u2016 < \u03b5, as required.\nAppendix B: Experiment Graphs"}], "references": [{"title": "Decisiontheoretic planning: Structural assumptions and computational leverage", "author": ["C. Boutilier", "T. Dean", "S. Hanks"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Boutilier et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1999}, {"title": "Exploiting structure in policy", "author": ["C. Boutilier", "R. Dearden", "M. Goldszmidt"], "venue": "construction. IJCAI,", "citeRegEx": "Boutilier et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1995}, {"title": "Metrics for labeled Markov systems", "author": ["J. Desharnais", "V. Gupta", "R. Jagadeesan", "P. Panangaden"], "venue": "International Conference on Concurrency Theory (pp", "citeRegEx": "Desharnais et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Desharnais et al\\.", "year": 1999}, {"title": "Metrics for finite Markov decision processes Proceedings of the 20th conference on Uncertainty in artificial intelligence", "author": ["N. Ferns", "P. Panangaden", "D. Precup"], "venue": null, "citeRegEx": "Ferns et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ferns et al\\.", "year": 2004}, {"title": "Metrics for Markov decision processes with infinite state spaces Proceedings of the 21st conference on Uncertainty in artificial intelligence", "author": ["N. Ferns", "P. Panangaden", "D. Precup"], "venue": null, "citeRegEx": "Ferns et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ferns et al\\.", "year": 2005}, {"title": "A Computational Study of Cost Reoptimization for Min Cost Flow Problems", "author": ["A. Frangioni", "A. Manca"], "venue": "INFORMS Journal On Computing,", "citeRegEx": "Frangioni and Manca,? \\Q2006\\E", "shortCiteRegEx": "Frangioni and Manca", "year": 2006}, {"title": "Equivalence notions and model minimization in markov decision processes", "author": ["R. Givan", "T. Dean", "M. Greig"], "venue": "Artificial Intelligence,", "citeRegEx": "Givan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Givan et al\\.", "year": 2003}, {"title": "Bisimulation through probabilistic testing", "author": ["K. Larsen", "A. Skou"], "venue": "Information and Computation,", "citeRegEx": "Larsen and Skou,? \\Q1991\\E", "shortCiteRegEx": "Larsen and Skou", "year": 1991}, {"title": "A calculus of communicating systems", "author": ["R. Milner"], "venue": "Lecture Notes in Computer Science", "citeRegEx": "Milner,? \\Q1980\\E", "shortCiteRegEx": "Milner", "year": 1980}, {"title": "Algorithms for the assignment and transportation problems", "author": ["J. Munkres"], "venue": "J. SIAM (pp", "citeRegEx": "Munkres,? \\Q1957\\E", "shortCiteRegEx": "Munkres", "year": 1957}, {"title": "A faster strongly polynomial minimum cost flow algorithm", "author": ["J. Orlin"], "venue": "Proceedings of the Twentieth annual ACM symposium on Theory of Computing (pp", "citeRegEx": "Orlin,? \\Q1988\\E", "shortCiteRegEx": "Orlin", "year": 1988}, {"title": "Concurrency and automata on infinite sequences", "author": ["D. Park"], "venue": "Proceedings of the 5th GI-Conference on Theoretical Computer Science (pp. 167\u2013183)", "citeRegEx": "Park,? \\Q1981\\E", "shortCiteRegEx": "Park", "year": 1981}, {"title": "Markov decision processes: Discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "Puterman,? \\Q1994\\E", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Towards quantitative verification of probabilistic transition systems", "author": ["F. van Breugel", "J. Worrell"], "venue": "Proceedings of the 28th International Colloquium on Automata, Languages, and Programming (ICALP),", "citeRegEx": "Breugel and Worrell,? \\Q2001\\E", "shortCiteRegEx": "Breugel and Worrell", "year": 2001}, {"title": "An algorithm for quantitative verification of probabilistic transition systems", "author": ["F. van Breugel", "J. Worrell"], "venue": "Proceedings of the 12th International Conference on Concurrency Theory (pp. 336\u2013350)", "citeRegEx": "Breugel and Worrell,? \\Q2001\\E", "shortCiteRegEx": "Breugel and Worrell", "year": 2001}, {"title": "Topics in Mass Transportation. [http: //www.math.toronto.edu/hmaroofi", "author": ["C. Villani"], "venue": null, "citeRegEx": "Villani,? \\Q2002\\E", "shortCiteRegEx": "Villani", "year": 2002}], "referenceMentions": [{"referenceID": 6, "context": "A number of recent methods rely instead on metrics based on the notion of bisimulation, or behavioral equivalence between states (Givan et al., 2003; Ferns et al., 2004).", "startOffset": 129, "endOffset": 169}, {"referenceID": 3, "context": "A number of recent methods rely instead on metrics based on the notion of bisimulation, or behavioral equivalence between states (Givan et al., 2003; Ferns et al., 2004).", "startOffset": 129, "endOffset": 169}, {"referenceID": 0, "context": "Markov decision processes (MDPs) are the model of choice for decision making under uncertainty (Boutilier et al., 1999), and provide a standard formalism for describing multi-stage decision making in probabilistic environments.", "startOffset": 95, "endOffset": 119}, {"referenceID": 12, "context": ", value iteration or policy iteration (Puterman, 1994), allow computing the optimal expected return for any state, as well as the way of behaving (policy) that generates this return.", "startOffset": 38, "endOffset": 54}, {"referenceID": 8, "context": "It has been well argued that the notion of \u201cessentially equivalent\u201d in probabilistic systems is perhaps best captured formally by bisimulation (Milner, 1980; Park, 1981; Larsen & Skou, 1991).", "startOffset": 143, "endOffset": 190}, {"referenceID": 11, "context": "It has been well argued that the notion of \u201cessentially equivalent\u201d in probabilistic systems is perhaps best captured formally by bisimulation (Milner, 1980; Park, 1981; Larsen & Skou, 1991).", "startOffset": 143, "endOffset": 190}, {"referenceID": 6, "context": "The bisimulation equivalence classes can even be computed iteratively in polynomial time (Givan et al., 2003).", "startOffset": 89, "endOffset": 109}, {"referenceID": 2, "context": "(2004; 2005), based on similar work in the context of labeled Markov processes (Desharnais et al., 1999; van Breugel and Worrell 2001a; 2001b), we sought to extend bisimulation for MDPs quantitatively in terms of such metrics.", "startOffset": 79, "endOffset": 142}, {"referenceID": 6, "context": "Givan et al. (2003) investigated several notions of MDP state equivalence and determined that the most appropriate is bisimulation.", "startOffset": 0, "endOffset": 20}, {"referenceID": 10, "context": "Since there exist strongly polynomial algorithms to compute the MCF problem (Orlin, 1988), the 1Results appear here in slightly modified form.", "startOffset": 76, "endOffset": 89}, {"referenceID": 12, "context": "For our purposes, this amounts to a worst case running time of O(|S|3 log |S|) for each Kantorovich LP (contrast this with the general LP for directly computing the optimal value function: this has |S| variables and |A||S| constraints (Puterman, 1994)).", "startOffset": 235, "endOffset": 251}, {"referenceID": 15, "context": "12 of Villani (2002); this is a consequence of Birkhoff\u2019s theorem).", "startOffset": 6, "endOffset": 21}, {"referenceID": 9, "context": "For example, the Hungarian algorithm (for a description see Munkres (1957)) runs in worst case time O(i3), where i is the number of samples.", "startOffset": 60, "endOffset": 75}, {"referenceID": 3, "context": "In Ferns et al. (2004), we suggested that in place of iteratively applying Fc to an initial metric h0 until convergence to df ix, we start with an appropriately chosen h0 and apply Fc only once.", "startOffset": 3, "endOffset": 23}, {"referenceID": 1, "context": "Experiments were run on four different MDPs: a 3\u00d73 grid world with two actions (move forward and rotate) and a single reward in the center of the room; a 5\u00d75 and a 7\u00d77 grid world each with the same dynamics; and a flattened out version of the coffee robot MDP (Boutilier et al., 1995) where the robot has to get coffee for the user and avoid getting wet.", "startOffset": 260, "endOffset": 284}, {"referenceID": 4, "context": "In prior works, we established the existence of of bisimulation metrics for continuous MDPs (Ferns et al., 2005).", "startOffset": 92, "endOffset": 112}], "year": 0, "abstractText": "A popular approach to solving large probabilistic systems relies on aggregating states based on a measure of similarity. Many approaches in the literature are heuristic. A number of recent methods rely instead on metrics based on the notion of bisimulation, or behavioral equivalence between states (Givan et al., 2003; Ferns et al., 2004). An integral component of such metrics is the Kantorovich metric between probability distributions. However, while this metric enables many satisfying theoretical properties, it is costly to compute in practice. In this paper, we use techniques from network optimization and statistical sampling to overcome this problem. We obtain in this manner a variety of distance functions for MDP state aggregation that differ in the tradeoff between time and space complexity, as well as the quality of the aggregation. We provide an empirical evaluation of these tradeoffs.", "creator": null}}}