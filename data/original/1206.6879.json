{"id": "1206.6879", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Practical Linear Value-approximation Techniques for First-order MDPs", "abstract": "Recent work on approximate linear programming (ALP) techniques for first-order Markov Decision Processes (FOMDPs) represents the value function linearly w.r.t. a set of first-order basis functions and uses linear programming techniques to determine suitable weights. This approach offers the advantage that it does not require simplification of the first-order value function, and allows one to solve FOMDPs independent of a specific domain instantiation. In this paper, we address several questions to enhance the applicability of this work: (1) Can we extend the first-order ALP framework to approximate policy iteration to address performance deficiencies of previous approaches? (2) Can we automatically generate basis functions and evaluate their impact on value function quality? (3) How can we decompose intractable problems with universally quantified rewards into tractable subproblems? We propose answers to these questions along with a number of novel optimizations and provide a comparative empirical evaluation on logistics problems from the ICAPS 2004 Probabilistic Planning Competition.", "histories": [["v1", "Wed, 27 Jun 2012 16:31:33 GMT  (157kb)", "http://arxiv.org/abs/1206.6879v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["scott sanner", "craig boutilier"], "accepted": false, "id": "1206.6879"}, "pdf": {"name": "1206.6879.pdf", "metadata": {"source": "CRF", "title": "Practical Linear Value-approximation Techniques for First-order MDPs", "authors": ["Scott Sanner", "Craig Boutilier"], "emails": ["ssanner@cs.toronto.edu", "cebly@cs.toronto.edu"], "sections": [{"heading": null, "text": "Recent work on approximate linear programming (ALP) techniques for first-order Markov Decision Processes (FOMDPs) represents the value function linearly w.r.t. a set of first-order basis functions and uses linear programming techniques to determine suitable weights. This approach offers the advantage that it does not require simplification of the first-order value function, and allows one to solve FOMDPs independent of a specific domain instantiation. In this paper, we address several questions to enhance the applicability of this work: (1) Can we extend the first-order ALP framework to approximate policy iteration and if so, how do these two algorithms compare? (2) Can we automatically generate basis functions and evaluate their impact on value function quality? (3) How can we decompose intractable problems with universally quantified rewards into tractable subproblems? We propose answers to these questions along with a number of novel optimizations and provide a comparative empirical evaluation on problems from the ICAPS 2004 Probabilistic Planning Competition."}, {"heading": "1 Introduction", "text": "Markov decision processes (MDPs) have become the de facto standard model for decision-theoretic planning problems. While classic dynamic programming algorithms for MDPs require explicit state and action enumeration, recent techniques for exploiting propositional structure in factored MDPs [4] avoid explicit state and action enumeration. While such techniques for factored MDPs have proven effective, they cannot generally exploit first-order structure. Yet many realistic planning domains are best represented in first-order terms, exploiting the existence of domain objects, relations over these objects, and the ability to express objectives and action effects using quantification.\nAs a result, a new class of algorithms has been introduced to explicitly handle MDPs with relational (RMDP)\nand first-order (FOMDP) structure.1 Symbolic dynamic programming (SDP) [5], first-order value iteration (FOVIA) [12, 13], and the relational Bellman algorithm (ReBel) [14] are model-based algorithms for solving FOMDPs and RMDPs, using appropriate generalizations of value iteration. Approximate policy iteration [7] induces rule-based policies from sampled experience in small-domain instantiations of RMDPs and generalizes these policies to larger domains. In a similar vein, inductive policy selection using first-order regression [9] uses regression to provide the hypothesis space over which a policy is induced. Approximate linear programming (for RMDPs) [10] is an approximation technique using linear program optimization to find a best-fit value function over a number of sampled RMDP domain instantiations.\nA recent technique for first-order approximate linear programming (FOALP) [21] in FOMDPs approximates a value function by a linear combination of first-order basis functions. While FOALP incorporates elements of symbolic dynamic programming (SDP) [5], it uses a more compact approximation framework and avoids the need for logical simplification. This stands in contrast with exact value iteration frameworks [5, 12, 13, 14] that prove intractable in many cases due to blowup of the value function representation and the need to perform complex simplifications. And in contrast to approaches that require sampling of ground domains [7, 9, 10], FOALP solves a FOMDP at the first-order level, thus obviating the need for domain instantiation. However, FOALP is just one of many possible approaches to linear value approximation and this begs the question of whether we can generalize it to other approaches such as first-order approximate policy iteration (FOAPI). If so, it would be informative to obtain a comparative empirical evaluation of these algorithms.\nHowever, determining the most effective algorithm for linear value-approximation is only the first step towards the development of practical approximation techniques for\n1We use the term relational MDP to refer models that allow implicit existential quantification, and first-order MDP for those with explicit existential and universal quantification.\nFOMDPs. We address the important issue of automatic basis function generation by extending regression-based techniques originally proposed by Gretton and Thiebaux [9]. In addition, we address issues that arise with universal rewards\u2014while symbolic dynamic programming is welldefined for FOMDP domains with universal rewards, classical first-order logic is insufficient for defining a compact set of basis functions that can adequately approximate the value function in such domains. We propose a technique for decomposing problems with universal rewards while facilitating \u201ccoordination\u201d among decomposed subproblems. Finally, we present a number of novel optimizations that enhance the performance of FOALP and FOAPI. We provide a comparative evaluation of these algorithms on problems from the ICAPS 2004 International Probabilistic Planning Competition (IPPC)."}, {"heading": "2 Markov Decision Processes", "text": "We first review linear value-approximation of MDPs."}, {"heading": "2.1 MDP Representation", "text": "An MDP consists of: a finite state space S; a finite set of actions A; a transition function T , with T (s, a, \u00b7) denoting a distribution over S for all s \u2208 S, a \u2208 A; and a reward function R : S \u00d7 A \u2192 R. Our goal is to find a policy \u03c0 that maximizes the value function, defined using the infinite horizon, discounted reward criterion: V \u03c0(s) = E\u03c0[ \u2211 \u221e t=0 \u03b3 t \u00b7 rt|s], where rt is a reward obtained at time t and 0 \u2264 \u03b3 < 1 is the discount factor.\nFor any function V over S and policy \u03c0, define the policy backup operator B\u03c0 as: (B\u03c0V )(s) = \u03b3 \u2211\nt T (s, \u03c0(s), t)V (t). The action backup operator Ba for action a is: (BaV )(s) = \u03b3 \u2211\nt T (s, a, t)V (t). The function V \u03c0(s) satisfies the fixed point relationship V \u03c0(s) = R(s, \u03c0(s)) + (B\u03c0V \u03c0)(s). Furthermore, the Q-function Q\u03c0 , denoting the expected future discounted reward achieved by taking action a in state s and following policy \u03c0 thereafter, satisfies Q\u03c0(s, a) = R(s, a) + (BaV \u03c0)(s). We define the greedy policy w.r.t. V as follows: \u03c0gre(V )(s) = arg maxaR(s, a) + (B aV )(s). If \u03c0\u2217 is the optimal policy and V \u2217 its value function, we have the relationship V \u2217(s) = maxaR(s, a) + (BaV \u2217)(s). Letting Q\u2217(s, a) = R(s, a) + (BaV \u2217)(s), we also have \u03c0\u2217(s) = \u03c0gre(V \u2217)(s) = arg maxaQ \u2217(s, a)."}, {"heading": "2.2 Linear Value Approximation for MDPs", "text": "In a linear value function representation, we represent V as a linear combination of k basis functions bj : V (s) =\n\u2211k j=1 wjbj(s). Our goal is to find weights that\napproximate the optimal value function as closely as possible. We note that because both backup operators B\u03c0 and Ba are linear operators, the backup of a linear combination of basis functions is just the linear combination of the\nbackups of the individual basis functions.\nApproximate Linear Programming: One way of finding a good linear approximation is to cast the optimization problem as a linear program that directly solves for the weights of an L1-minimizing approximation of the optimal value function [6]:\nVariables: w1, . . . , wk\nMinimize: \u2211\ns\u2208S\nk \u2211\nj=1\nwjbj(s) (1)\nSubject to: 0 \u2265 R(s, a) + k \u2211\nj=1\nwj [(B a bj)(s) \u2212 bj(s)] ; \u2200a, s\nWhile the size of the objective and the number of constraints in this LP is proportional to the number of states (and therefore exponential), recent solution techniques use compact, factored basis functions and exploit the reward and transition structure of factored MDPs [11, 22], making it possible to avoid generating an exponential number of constraints (and rendering the entire LP compact).\nApproximate Policy Iteration: Likewise, we can generalize policy iteration to the approximate case by calculating successive iterations of weights w(i)j that represent the best approximation of the fixed point value function for policy \u03c0(i) at iteration i. We do this by performing the following two steps at each iteration: (1) deriving the greedy policy: \u03c0(i+1) \u2190 \u03c0gre( \u2211k j=1 w (i) j bj(s)) and (2) using the following LP to determine the weights for the Bellman-errorminimizing approximate value function:\nVariables: w(i+1)1 , . . . , w (i+1) k Minimize: \u03c6(i+1) (2)\nSubject to: \u03c6(i+1) \u2265\n\u2223 \u2223 \u2223 \u2223 R(s, \u03c0(s)) + k \u2211\nj=1\n[w (i+1) j (B \u03c0(i+1) bj)(s)]\n\u2212 k \u2211\nj=1\n[w (i+1) j bj(s)]\n\u2223 \u2223 \u2223 \u2223 ; \u2200a, s\nIf policy iteration converges (i.e., if ~w(i+1) = ~w(i+1)), then Guestrin et al. [11] provide the following bound on the loss of V (i+1) w.r.t. the optimal value function V \u2217 (since the Bellman error is bounded by the objective \u03c6(i+1) of the optimal LP solution at iteration i+ 1):\n\u2016V \u2217 \u2212 V (i+1)(s)\u2016\u221e \u2264 2\u03b3\u03c6(i+1)\n1 \u2212 \u03b3 (3)"}, {"heading": "3 First-Order MDPs", "text": ""}, {"heading": "3.1 The Situation Calculus", "text": "The situation calculus [19] is a first-order language for axiomatizing dynamic worlds. Its basic ingredients consist of actions, situations, and fluents. Actions are first-order terms involving action function symbols. For example, the action of driving a truck t from city c1 to city c2 might\nbe denoted by the action term drive(t, c1, c2). A situation is a first-order term denoting the occurrence of a sequence of actions. These are represented using a binary function symbol do: do(\u03b1, s) denotes the situation resulting from doing action \u03b1 in situation s. In a logistics domain, the situation term do(drive(t, c2, c3), do(drive(t, c1, c2), S0)) denotes the situation resulting from executing sequence [drive(t, c1, c2),drive(t, c2, c3)] in S0. Relations whose truth values vary between states are called fluents, and are denoted by predicate symbols whose last argument is a situation term. For example, TAt(t, paris , s) is a relational fluent meaning that truck t is in paris in situation s.2\nA domain theory is axiomatized in the situation calculus with four classes of axioms [19]. The most important of these are successor state axioms (SSAs). There is one SSA for each fluent F (~x, s), with syntactic form F (~x, do(a, s)) \u2261 \u03a6F (~x, a, s) where \u03a6F (~x, a, s) is a formula with free variables among a, s, ~x. These characterize the truth values of the fluent F in the next situation do(a, s) in terms of the current situation s, and embody a solution to the frame problem for deterministic actions [19].\nThe regression of a formula \u03c8 through an action a is a formula \u03c8\u2032 that holds prior to a being performed iff \u03c8 holds after a. We refer the reader to [5, 21] for a formal definition and discussion of the Regr(\u00b7) operator. Here we simply note that it is defined compositionally and that regression of a formula reduces to the regression of all fluents in a way that is naturally supported by the format of the SSAs."}, {"heading": "3.2 Case Representation and Operators", "text": "Prior to generalizing the situation calculus to permit a firstorder representation of MDPs, we introduce a case notation to allow first-order specifications of the rewards, probabilities, and values required for FOMDPs (see [5, 21] for formal details):\nt = \u03c61 : t1 : : : \u03c6n : tn\n\u2261 \u2228\ni\u2264n{\u03c6i \u2227 t = ti}\nHere the \u03c6i are state formulae (whose situation term does not use do) and the ti are terms. Often the ti will be constants and the \u03c6i will partition state space. For example, using Dst(t, c) to indicate the destination of truck t is city c, we may represent our reward function rCase(s, a) as:\nrCase(s, a) = a = noop \u2227 \u2200t, c TAt(t, c, s) \u2283 Dst(t, c) : 10 a 6= noop \u2227 \u2200t, c TAt(t, c, s) \u2283 Dst(t, c) : 9 \u2203t, c TAt(t, c, s) \u2227 \u00acDst(t, c) : 0\nHere, we receive a reward of 10 (9) if all trucks are at their destination and a noop is (not) performed. In all other cases we receive 0 reward. We use vCase(s) to represent value functions in exactly the same manner.\n2In contrast to states, situations reflect the entire history of action occurrences. However, the specification of dynamics is Markovian and allows recovery of state properties from situation terms.\nIntuitively, to perform an operation on two case statements, we simply take the cross-product of their partitions and perform the corresponding operation on the resulting paired partitions. Letting each \u03c6i and \u03c8j denote generic first-order formulae, we can perform the \u201ccross-sum\u201d \u2295 of two case statements in the following manner:\n\u03c61 : 10 \u03c62 : 20\n\u2295 \u03c81 : 1 \u03c82 : 2\n=\n\u03c61 \u2227 \u03c81 : 11 \u03c61 \u2227 \u03c82 : 12 \u03c62 \u2227 \u03c81 : 21 \u03c62 \u2227 \u03c82 : 22\nLikewise, we can perform and \u2297 by, respectively, subtracting or multiplying partition values (as opposed to adding them) to obtain the result. Some partitions resulting from the application of the \u2295, , and \u2297 operators may be inconsistent; we simply discard such partitions since they can obviously never correspond to any world state.\nWe define four additional operators on cases [5, 21]: Regr(\u00b7), \u2203~x, max, and \u222a. Regression Regr(C) and existential quantification \u2203~xC can both be applied directly to the individual partition formulae \u03c6i of case C. The maximization operation maxC sorts the partitions of case C from largest to smallest, rendering them disjoint in a manner that ensures each portion of state space is assigned the highest value. The union operation C1 \u222aC2 denotes a simple union of the case partitions from cases C1 and C2."}, {"heading": "3.3 Stochastic Actions and the Situation Calculus", "text": "To generalize the classical situation calculus to stochastic actions required by FOMDPs, we decompose stochastic \u201cagent\u201d actions into a collection of deterministic actions, each corresponding to a possible outcome of the stochastic action. We then specify a distribution according to which \u201cnature\u201d may choose a deterministic action from this set whenever that stochastic action is executed. As a consequence we need only formulate SSAs using the deterministic nature\u2019s choices [1, 5], thus obviating the need for a special treatment of stochastic actions in SSAs.\nLetting A(~x) be a stochastic action with nature\u2019s choices (i.e., deterministic actions) n1(~x), \u00b7 \u00b7 \u00b7 , nk(~x), we represent the distribution over ni(~x) given A(~x) using the notation pCase(nj(~x), A(~x), s). Continuing our logistics example, if the effect of driving a truck depends on whether it is snowing in the city of origin, then we decompose the stochastic drive action into two deterministic actions driveS and driveF , denoting success and failure, respectively, and specify a distribution over nature\u2019s choice:\npCase( driveS(t, c1, c2), drive(t, c1, c2), s)\n= snow(c1, s) : 0.6 \u00acsnow(c1, s) : 0.9\npCase( driveF (t, c1, c2), drive(t, c1, c2), s)\n= snow(c1, s) : 0.4 \u00acsnow(c1, s) : 0.1\nNext, we define the SSAs in terms of these deterministic\nchoices.3 Assuming that nature\u2019s choice of deterministic actions for stochastic action drive(t, c1, c2) decomposes as above, we can express an SSA for TAt :\nTAt(t, c, do(a, s)) \u2261\n\u2203c1 TAt(t, c1, s) \u2227 a = driveS(t, c1, c)\u2228\nTAt(t, c, s) \u2227 \u00ac(\u2203c2 c 6= c2 \u2227 a = driveS(t, c, c2))\nIntuitively, the only actions that can change the fluent TAt are successful drive actions. If a successful drive action does not occur then the fluent remains unchanged."}, {"heading": "3.4 Symbolic Dynamic Programming", "text": "Backing up a value function vCase(s) through an action A(~x) yields a case statement containing the logical description of states that would give rise to vCase(s) after doing action A(~x), as well as the values thus obtained (i.e., a Q(s, a) function in classical MDPs). There are in fact three types of backups that we can perform. The first, BA(~x), regresses a value function through an action and produces a case statement with free variables for the action parameters. The second, BA, existentially quantifies over the free variables ~x in BA(~x). The third, BAmax applies the max operator to BA which results in a case description of the regressed value function indicating the best value that could be achieved by executing any instantiation of A(~x) in the pre-action state. To define the backup operators, we first define a slightly modified version of the first-order decision theoretic regression (FODTR) operator [5]:\nFODTR(vCase(s), A(~x)) =\n\u03b3 [\u2295j{pCase(nj(~x), s) \u2297Regr(vCase(do(nj(~x), s)))}]\nWe next next define the three backup operators:\nB A(~x)(vCase(s)) = rCase(s, a) \u2295 FODTR(vCase(s), A(~x))\n(4)\nB A(vCase(s)) = \u2203~x BA(~x)(vCase(s)) (5)\nB A max(vCase(s)) = max(B A(vCase(s))) (6)\nPrevious work [21] provides examples ofBA(~x) andBAmax."}, {"heading": "4 Linear Value Approximation for FOMDPs", "text": ""}, {"heading": "4.1 Value Function Representation", "text": "Following [21], we represent a value function as a weighted sum of k first-order basis functions, denoted bCase i(s), each containing a small number of formulae that provide a first-order abstraction of state space:\nvCase(s) = \u2295ki=1 wi \u00b7 bCasei(s) (7)\nUsing this format, we can often achieve a reasonable approximation of the exact value function by exploiting the\n3SSAs can often be compiled from \u201ceffect\u201d axioms that specify action effects [19] and effect axioms can be compiled from PPDDL probabilistic planning domain specifications [25].\nadditive structure inherent in many real-world problems (e.g., additive reward functions or problems with independent subgoals). Unlike exact solution methods where value functions can grow exponentially in size during the solution process and must be logically simplified [5], here we maintain the value function in a compact form that requires no simplification, just discovery of good weights.\nWe can easily apply the backup operator BA to this representation and obtain some simplification as a result of the structure in Eq. 7. We simply substitute the value function expression in Eq. 7 into the definition BA(~x) (Eq. 4). Exploiting the properties of the Regr and \u2295 operators, we find that the backup BA(~x) of a linear combination of basis functions is simply the linear combination of the FODTR of each basis function:\nB A(~x)(\u2295i wibCasei(s)) = (8)\nrCase(s, a) \u2295 (\u2295i wiFODTR(bCasei(s), A(~x)))\nGiven the definition of BA(~x) for a linear combination of basis functions, corresponding definitions ofBA andBAmax follow directly from Eqs. 5 and 6. It is important to note that during the application of these operators, we never explicitly ground states or actions, in effect achieving both state and action space abstraction."}, {"heading": "4.2 First-order Approximate Linear Programming", "text": "Now we have all of the building blocks required to define first-order approximate linear programming (FOALP) and first-order approximate policy iteration (FOAPI). For now we simply focus on the algorithm definitions; we will address efficient implementation in a subsequent section.\nFOALP was introduced by Sanner and Boutilier [21]. Here we present a linear program (LP) with first-order constraints that generalizes Eq. 1 from MDPs to FOMDPs:\nVariables: wi ; \u2200i \u2264 k\nMinimize: k \u2211\ni=1\nwi \u2211\n\u3008\u03c6j ,tj\u3009\u2208bCasei\ntj\n|bCasei|\nSubject to: 0 \u2265 BAmax(\u2295 k i=1 wi \u00b7 bCasei(s))\n(\u2295ki=1 wi \u00b7 bCasei(s)) ; \u2200 A, s (9)\nThe objective of this LP requires some explanation. If we were to directly generalize the objective for MDPs to that of FOMDPs, the objective would be ill-defined (it would sum over infinitely many situations). To remedy this, we suppose that each basis function partition is chosen because it represents a potentially useful partitioning of state space, and thus sum over each case partition.\nThis LP also contains a first-order specification of constraints, which somewhat complicates the solution. Before tackling this, we introduce a general first-order LP format\nthat we can reuse for FOAPI:\nVariables: v1, . . . , vk ; Minimize: f(v1, . . . , vk) Subject to: 0 \u2265 case1,1(s) \u2295 . . .\u2295 case1,n(s) ; \u2200 s (10)\n:\n0 \u2265 casem,1(s) \u2295 . . .\u2295 casem,n(s) ; \u2200 s\nThe variables and objective are as defined in a typical LP, the main difference being the form of the constraints. While there are an infinite number of constraints (i.e., one for every situation s), we can work around this since case statements are finite. Since the value ti for each case partition \u3008\u03c6i(s), ti\u3009 is piecewise constant over all situations satisfying \u03c6i(s), we can explicitly sum over the casei(s) statements in each constraint to yield a single case statement. For this \u201cflattened\u201d case statement, we can easily verify that the constraint holds in the finite number of piecewise constant partitions of the state space. However, generating the constraints for each \u201ccross-sum\u201d can yield an exponential number of constraints. Fortunately, we can generalize constraint generation techniques [22] to avoid generating all constraints. We refer to [21] for further details. Taken together, these techniques yield a practical FOALP solution to FOMDPs."}, {"heading": "4.3 First-order Approximate Policy Iteration", "text": "We now turn to the first contribution of this paper, a novel generalization of approximate policy iteration from the classical MDP case (Eq. 1) to FOMDPs.\nPolicy iteration requires a suitable first-order policy representation. Given a value function vCase(s) it is easy to derive a greedy policy from it. Assuming we have m parameterized actions {A1(~x), . . . , Am(~x)}, we can represent the policy \u03c0Case(s) as:\n\u03c0Case(s) = max( \u22c3\ni=1...m\nB Ai(vCase(s))) (11)\nHere, BAi(vCase(s)) represents the values that can be achieved by any instantiation of the actionAi(~x). The max case operator enforces that each portion of pre-action state space is assigned the maximal Q-function partition. For bookkeeping purposes, we require that each partition \u3008\u03c6, t\u3009 in BAi(vCase(s)) maintain a mapping to the action Ai that generated it, which we denote as \u3008\u03c6, t\u3009 \u2192 Ai. Then, given a particular world state s, we can evaluate \u03c0Case(s) to determine which policy partition \u3008\u03c6, t\u3009 \u2192 Ai is satisfied in s and thus, which action Ai should be applied. If we retrieve the bindings of the existentially quantified action variables in \u03c6 (recall that BAi existentially quantifies these), we can easily determine the parameterization of action Ai that should apply according to the policy.\nFor our algorithms, it is useful to define a set of case statements for each action Ai that is satisfied only in the world states where Ai should be applied according to\n\u03c0Case(s). Consequently, we define an action restricted policy \u03c0CaseAi(s) as follows:\n\u03c0CaseAi(s) = {\u3008\u03c6, t\u3009|\u3008\u03c6, t\u3009 \u2208 \u03c0Case(s) and \u3008\u03c6, t\u3009 \u2192 Ai}\nFollowing the approach to approximate policy iteration for factored MDPs provided by Guestrin et al. [11], we can generalize approximate policy iteration to the first-order case by calculating successive iterations of weights w(i)j that represent the best approximation of the fixed point value function for policy \u03c0Case(i)(s) at iteration i. We do this by performing the following two steps at every iteration i: (1) Obtaining the policy \u03c0Case(s) from the current value function and weights (\n\u2211k j=1 w (i) j bCasej(s)) using Eq. 11,\nand (2) solving the following LP in the format of Eq. 10 that determines the weights of the Bellman-error-minimizing approximate value function for policy \u03c0Case(s):\nVariables: w(i+1)1 , . . . , w (i+1) k Minimize: \u03c6(i+1) (12) Subject to: \u03c6(i+1) \u2265 \u2223 \u2223 \u2223 \u03c0CaseA(s) \u2295\u2295 k j=1[w (i+1) j bCasej(s)]\n\u2295kj=1w (i+1) j (B A maxbCasej)(s)\n\u2223 \u2223 \u2223 ; \u2200A, s\nWe\u2019ve reached convergence if \u03c0(i+1) = \u03c0(i). If the policy iteration converges, then we note that the loss bounds for API (Eq. 3) generalize directly to the first-order case."}, {"heading": "5 Greedy Basis Function Generation", "text": "The use of linear approximations requires a good set of basis functions that span a space that includes a good approximation to the value function. While some work has addressed the issue of basis function generation [18, 16], none has been applied to RMDPs or FOMDPs. We consider a basis function generation method that draws on the work of Gretton and Thiebaux [9], who use inductive logic programming (ILP) techniques to construct a value function from sampled experience. Specifically, they use regressions of the reward as candidate building blocks for ILP-based construction of the value function. This technique has allowed them to generate fully or k-stage-to-go optimal policies for a range of Blocks World problems.\nWe leverage a similar approach for generating candidate basis functions for use in the FOALP or FOAPI solution techniques. Fig. 1 provides an overview of our basis function generation algorithm. The motivation for this approach is as follows: if some portion of state space \u03c6 has value v > \u03c4 in an existing approximate value function for some nontrivial threshold \u03c4 , then this suggests that states that can reach this region (i.e., found by Regr(\u03c6) through some action) should also have reasonable value. However, since we have already assigned value to \u03c6, we want the new basis function to focus on the area of state space not covered by \u03c6 so we negate it and conjoin it withRegr(\u03c6). This \u201corthogonality\u201d of newly generated basis functions also allows for computation optimizations (see Sec. 7)."}, {"heading": "6 Handling Universal Rewards", "text": "In first-order domains, we are often faced with universal reward expressions that assign some positive value to the world states satisfying a formula of the general form \u2200y \u03c6(y, s), and 0 otherwise. For instance, in our logistics example a reward may be given for having all trucks at their assigned destination: \u2200t, cDst(t, c) \u2192 TAt(t, c, s). One difficulty with such rewards is that our basis function approach provides a piecewise constant approximation to the value function (i.e., each basis function aggregates state space into regions of equal value, with the linear combination simply providing constant values over somewhat smaller regions). However, the value function for problems with universal rewards typically depends (often in a linear or exponential way) on the number of domain objects of interest. For instance, in our example, value at a state depends on the number of trucks not at their proper destination (since that impacts the time it will take to obtain the reward). Unfortunately, this cannot be represented concisely using the piecewise constant decomposition offered by first-order basis functions. As noted by Gretton and Thiebaux [9], effectively handling universally quantified rewards is one of the most pressing issues in the practical solution of FOMDPs.\nTo address this problem we adopt a decompositional approach, motivated in part by techniques for additive rewards in MDPs [3, 23, 17, 18]. Intuitively, given a goaloriented reward that assigns positive reward if \u2200yG(y, s) is satisfied, and zero otherwise, we can decompose it into a set of ground goals {G(~y1), . . . , G( ~yn)} for all possible ~yj in a ground domain of interest. If we reach a state where all ground goals are true, then we have satisfied \u2200yG(y, s).\nOf course, our methods solve FOMDPs without knowledge\nof the specific domain, so the set of ground goals that will be faced at run-time is unknown. So in the offline solution of the MDP we assume a a generic ground goal G(~y\u2217) for a \u201cgeneric\u201d object vector ~y\u2217. It is easy to construct an instance of the reward function rCase(s) for this single goal, and solve for this simplified generic goal using FOALP or FOAPI. This produces a value function and policy that assumes that ~y\u2217 is the only object vector of interest (i.e., satisfying relevant type constraints and preconditions) in the domain. From this, we can also derive the optimal Q-function for the simplified \u201cgeneric\u201d domain (and action template Ai(~x)): QG(~y\u2217)(Ai(~x), s) = BAi(vCase(s)).4 Intuitively, given a ground state s, the optimal action for this generic goal can be determined by finding the ground Ai(~x\u2217) for this s with max Q-value.\nWith the solution (i.e., optimal Q-function) of a generic goal FOMDP in hand, we address the online problem of action selection for a specific domain instantiation. Assume a set of ground goals {G(~y1), . . . , G( ~yn)} corresponding to a specific domain given at run-time. If we assume that (typed) domain objects are treated uniformly in the uninstantiated FOMDP, as is the case in many logistics and planning problems, then we obtain the Qfunction for any goal G(~yj) by replacing all ground terms ~y\u2217 with the respective terms ~yj in QG(~y\u2217)(Ai(~x), s) to ob-\n4Since the BA operator can often retain much of the additive structure in the linear approximation of vCase(s) [21], representation and computation with this Q-function is very efficient.\ntain QG( ~yj)(Ai(~x), s).\nAction selection requires finding an action that maximizes value w.r.t. the original universal reward. Following [3, 17], we do this by treating the sum of the Q-values of any action in the subgoal MDPs as a measure of its Q-value in the joint (original) MDP. Specifically, we assume that each goal contributes uniformly and additively to the reward, so the Q-function for an entire set of ground goals {G(~y1), . . . , G( ~yn)} determined by our domain instantiation is just\n\u2211n j=1 1 n QG( ~yj)(Ai(~x), s). Action selection\n(at run-time) in any ground state is realized by choosing that action with maximum joint Q-value. Naturally, we do not want to explicitly create the joint Q-function, but instead use an efficient scoring technique that evaluates potentially useful actions by iterating through the individual Q-functions as described in Fig. 2. While this additive and uniform decomposition may not be appropriate for all domains with goal-oriented universal rewards, we have found it to be highly effective for the two domains examined in this paper. And while this approach can only currently handle rewards with universal quantifiers, this reflects the form of many planning problems. Nonetheless, there are potential extensions of this technique for more complex universal rewards, the general question being how to assign credit among the constituents of such a reward."}, {"heading": "7 Optimizations", "text": "Following are a few novel additional optimizations that provided substantial performance enhancements of our FOALP and FOAPI implementations. First, and most importantly, the style of generating orthogonal basis functions in Fig. 1 has some very nice computational properties that we can exploit. In short, when searching for the maximum partition among n disjoint basis functions, one need only consider taking 1 of n true partitions (each basis function has \u03c6 and \u00ac\u03c6) setting the other n \u2212 1 basis functions to its false partition. Clearly any other setting would result in an inconsistent state due to the disjointness of the n basis functions. Consequently, the search for a consistent state reduces from an exponential complexity of 2n combinations down to a polynomial complexity of n combinations (trying each true partition of a basis function in turn).\nSecond, one can replace the BAmax operators in the constraints for FOALP and FOAPI with the much simpler BA operator that does not introduce the blowup that occurs from enforcing disjointness in the BAmax operator. Since we know that we only use the constraints when searching for a max (i.e., during constraint generation [21]), the max over BA will implicitly enforce the max constraint of BAmax. While we omit a proof, it is straightforward to show that the maximal value and therefore the maximally violated constraint that we need during constraint generation is the same whether we use BAmax or B A.\nThird, while first-order simplification techniques are not required to perform FOALP or FOAPI, some simplification can save a substantial amount of theorem proving time. We used a simple BDD-based simplification technique as follows: Given a first-order formula, we rearrange and push quantifiers down into subformulae as far as possible. This exposes a propositional super-structure (very common in FOMDP problems) that can be mapped into a BDD. This BDD structure is useful because it reduces propositional redundancy in the formula representation by removing duplicate or inconsistent closed first-order formulae that are repeated frequently due to the naive conjunction of the case operators (mainly \u2295, , and \u2297)."}, {"heading": "8 Empirical Results", "text": "We applied FOALP and FOAPI to the Box World logistics and Blocks World probabilistic planning problems from the ICAPS 2004 IPPC [15]. In the Box World logistics problem, the domain objects consists of trucks, planes, boxes, and cities. The number of boxes and cities varies in each problem instance, but there were always 5 trucks and 5 planes. Trucks and planes are restricted to particular routes between cities in a problem-specific manner. The goal in Box World is to deliver all boxes to their destination cities, despite the fact that trucks and planes may stochastically fail to reach their specified destination. Blocks World is just a stochastic version of the standard domain where blocks are moved between the table and other stacks of blocks to form a goal configuration. In this version, a block may be dropped while picking it up or placing it on a stack.\nWe used the Vampire [20] theorem prover and the CPLEX 9.0 LP solver in our FOALP and FOAPI implementations and applied the basis function generation algorithm given in Fig. 1 to a FOMDP version of these domains. It is important to note that we generate our solution to the Box World and Blocks World domains offline. Since each of these domains has a universally quanitified reward, our offline solution is for a generic instantiation of this reward. Then at evaluation time when we are given an actual problem instance (i.e., a set of domain objects and initial state configuration), we decompose the value function for each ground instantiation of the reward and execute a policy using the approach outlined in Sec. 6. We do not enhance or otherwise modify our offline solution once given actual domain information (this is an avenue for future research).\nWe set an iteration limit of 7 in our offline basis function generation algorithm and recorded the running times per iteration of FOALP and FOAPI; these are shown in Fig. 3. There appears to be exponential growth in the running time as the number of basis functions increases; this reflects the results of previous work [21]. However, we note that if we were not using the \u201corthogonal\u201d basis function generation technique described in Sec. 5 and associated optimizations\nin Sec. 7, we would not get past iteration 2 of basis function generation due to the prohibitive amount of time required by FOALP and FOAPI (> 10 hours). Consequently, we can conclude that our basis function generation algorithm and optimizations have substantially increased the number of basis functions for which FOALP and FOAPI remain viable solution options. In terms of a comparison of the running times of FOALP and FOAPI, it is apparent that each performs better in different settings. In Box World, FOAPI takes fewer iterations of constraint generation than FOALP and thus is slightly faster. In Blocks World, the policies tend to grow more quickly in size because the Vampire theorem prover has difficulty refuting inconsistent partitions on account of the use of equality in this FOMDP domain. This impacts not only the solution time of FOAPI, but also its performance as we will see next.\nWe applied the policies generated by the FOALP and FOAPI versions of our basis function function generation algorithm to three Box World and five Blocks World problem instances from the ICAPS 2004 IPC. We compared our planning system to the three other top-performing planners on these domains: G2 is a temporal logic planner with human-coded control knowledge [8]; P is an RTDP-based planner [2]; J1 is a human-coded planner, J2 is an inductive policy iteration planner, and J3 is a deterministic replanner [24]. Results for all of these planners are given in Table 1.\nWe make four overall observations: (1) FOALP and FOAPI produce the same basis function weights and therefore the same policies for the Box World domain. (2) We only used 7 iterations of basis function generation and this effectively limits the lookahead horizon of our basis functions to 7 steps. It appears that a lookahead of 8 is required to properly plan in the final Box World problem instance and thus both FOALP and FOAPI failed on this instance.5 (3) Due\n5We could not increase the number of iterations to 8 to test this\nto aforementioned problems with the inability of FOAPI to detect inconsistency of policy partitions in the Blocks World domain, its performance is severely degraded on these problem instances in comparison to FOALP. FOALP does not use a policy representation and thus does not encounter these problems. (4) It is important to note that in comparing FOALP and FOAPI to the other planners, G2 and J1 used hand-coded control knowledge and J3 was a very efficient search-based deterministic planner that had a significant advantage because the probabilities in these domains were inconsequential. The only fully autonomous stochastic planners were P and J2, and FOALP performs comparably to both of these planners and outperforms them by a considerable margin on a number of problem instances."}, {"heading": "9 Concluding Remarks", "text": "We have introduced a novel algorithm for performing firstorder approximate policy iteration, as well as new basis function generation techniques that allow FOALP and FOAPI to efficiently exploit their structure, leading to a substantial increase in the number of basis functions that these algorithms can handle. Additionally, we have addressed the intractability of solving problems with universal rewards by automatically decomposing the task into independent subgoals that can be solved and then recombined to determine a policy that facilitates \u201ccoordination\u201d among the subgoals. Taken together, these techniques have enabled us to evaluate FOALP and FOAPI solutions to logistics problems from the ICAPS 2004 Probabilistic Planning Competition. Empirically we have shown that FOALP performs better than other autonomous stochastic planners on these problems and outperforms FOAPI when the pol-\nhypothesis due to memory constraints. We are currently working on additional optimizations to remedy this problem.\nicy representation requires first-order logic constructs that pose difficulties for a state-of-the-art theorem prover. Our approach is competitive on these domains even with planners that exploit (hand-coded) domain-specific knowledge.\nOne pressing issue for future work is to extend our reward decomposition techniques to a wider range of universally quantified formulae. In addition, we note that many domains including the Box World logistics domain covered in this paper have an underlying topological structure that is not exploited in current solution algorithms. The ability to directly exploit topological structure in the problem representation and basis function formulation could potentially help with the limited-horizon lookahead issues that we experienced on Box World. The ability to exploit additional reward and domain structure will help push fully lifted and automated first-order solution techniques further into probabilistic planning domains that previously could not be handled."}], "references": [{"title": "Reasoning about noisy sensors in the situation calculus", "author": ["Fahiem Bacchus", "Joseph Y. Halpern", "Hector J. Levesque"], "venue": "In IJCAI-95,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "mGPT: A probabilistic planner based on heuristic search", "author": ["Blai Bonet", "Hector Geffner"], "venue": "In Online Proceedings for The Probablistic Planning Track of IPC-04: http://www.cs.rutgers.edu/ \u0303mlittman/topics/ipc04pt/proceedings/,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Prioritized goal decomposition of Markov decision processes: Toward a synthesis of classical and decision theoretic planning", "author": ["Craig Boutilier", "Ronen I. Brafman", "Christopher Geib"], "venue": "In IJCAI-97,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Decisiontheoretic planning: Structural assumptions and computational leverage", "author": ["Craig Boutilier", "Thomas Dean", "Steve Hanks"], "venue": "JAIR, 11:1\u201394,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Symbolic dynamic programming for first-order MDPs", "author": ["Craig Boutilier", "Ray Reiter", "Bob Price"], "venue": "In IJCAI-01,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "The linear programming approach to approximate dynamic programming", "author": ["DP de Farias", "Ben Van Roy"], "venue": "Operations Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Approximate policy iteration with a policy language bias", "author": ["Alan Fern", "SungWook Yoon", "Robert Givan"], "venue": "In NIPS-", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Thiebaux. NMRDPP: Decision-theoretic planning with control knowledge", "author": ["Charles Gretton", "David Price", "Sylvie"], "venue": "In Online Proceedings for The Probablistic Planning Track of IPC-04: http://www.cs.rutgers.edu/ \u0303mlittman/topics/ipc04pt/proceedings/,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Exploiting first-order regression in inductive policy selection", "author": ["Charles Gretton", "Sylvie Thiebaux"], "venue": "In UAI-04,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Generalizing plans to new environments in relational MDPs", "author": ["Carlos Guestrin", "Daphne Koller", "Chris Gearhart", "Neal Kanodia"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Efficient solution methods for factored MDPs", "author": ["Carlos Guestrin", "Daphne Koller", "Ronald Parr", "Shobha Venktaraman"], "venue": "JAIR, 19:399\u2013468,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "A logic-based approach to dynamic programming", "author": ["Steffen H\u00f6lldobler", "Olga Skvortsova"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "A heuristic search algorithm for solving first-order MDPs", "author": ["Eldar Karabaev", "Olga Skvortsova"], "venue": "In UAI-2005,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Bellman goes relational", "author": ["Kristian Kersting", "Martijn van Otterlo", "Luc de Raedt"], "venue": "In ICML-04. ACM Press,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Introduction to the probabilistic planning track", "author": ["Michael L. Littman", "Hakan L.S. Younes"], "venue": "In Online Proceedings for The Probablistic Planning Track of IPC-04: http://www.cs.rutgers.edu/ \u0303mlittman/topics/ipc04pt/proceedings/,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Samuel meets Amarel: Automating value function approximation using global state space analysis", "author": ["Sridhar Mahadevan"], "venue": "In AAAI-05,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Solving very large weakly coupled Markov decision processes", "author": ["Nicolas Meuleau", "Milos Hauskrecht", "Kee-Eung Kim", "Leonid Peshkin", "Leslie Pack Kaelbling", "Thomas Dean", "Craig Boutilier"], "venue": "In AAAI-98,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Piecewise linear value function approximation for factored MDPs", "author": ["Pascal Poupart", "Craig Boutilier", "Relu Patrascu", "Dale Schuurmans"], "venue": "In AAAI-02,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Knowledge in Action: Logical Foundations for Specifying and Implementing Dynamical Systems", "author": ["Ray Reiter"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "The design and implementation of vampire", "author": ["Alexandre Riazanov", "Andrei Voronkov"], "venue": "AI Communications,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Approximate linear programming for first-order MDPs", "author": ["Scott Sanner", "Craig Boutilier"], "venue": "In UAI-2005,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Direct value approximation for factored MDPs", "author": ["Dale Schuurmans", "Relu Patrascu"], "venue": "In NIPS-2001,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2001}, {"title": "How to dynamically merge Markov decision processes", "author": ["Satinder P. Singh", "David Cohn"], "venue": "In NIPS-98,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Learning reactive policies for probabilistic planning domains", "author": ["Alan Fern SungWook Yoon", "Robert Givan"], "venue": "In Online Proceedings for The Probablistic Planning Track of IPC-04: http://www.cs.rutgers.edu/ \u0303mlittman/topics/ipc04pt/proceedings/,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "PPDDL: The probabilistic planning domain definition language: http://www.cs.cmu.edu/ \u0303lorens/papers/ppddl.pdf", "author": ["Hakan Younes", "Michael Littman"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}], "referenceMentions": [{"referenceID": 3, "context": "While classic dynamic programming algorithms for MDPs require explicit state and action enumeration, recent techniques for exploiting propositional structure in factored MDPs [4] avoid explicit state and action enumeration.", "startOffset": 175, "endOffset": 178}, {"referenceID": 4, "context": "1 Symbolic dynamic programming (SDP) [5], first-order value iteration (FOVIA) [12, 13], and the relational Bellman algorithm (ReBel) [14] are model-based algorithms for solving FOMDPs and RMDPs, using appropriate generalizations of value iteration.", "startOffset": 37, "endOffset": 40}, {"referenceID": 11, "context": "1 Symbolic dynamic programming (SDP) [5], first-order value iteration (FOVIA) [12, 13], and the relational Bellman algorithm (ReBel) [14] are model-based algorithms for solving FOMDPs and RMDPs, using appropriate generalizations of value iteration.", "startOffset": 78, "endOffset": 86}, {"referenceID": 12, "context": "1 Symbolic dynamic programming (SDP) [5], first-order value iteration (FOVIA) [12, 13], and the relational Bellman algorithm (ReBel) [14] are model-based algorithms for solving FOMDPs and RMDPs, using appropriate generalizations of value iteration.", "startOffset": 78, "endOffset": 86}, {"referenceID": 13, "context": "1 Symbolic dynamic programming (SDP) [5], first-order value iteration (FOVIA) [12, 13], and the relational Bellman algorithm (ReBel) [14] are model-based algorithms for solving FOMDPs and RMDPs, using appropriate generalizations of value iteration.", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "Approximate policy iteration [7] induces rule-based policies from sampled experience in small-domain instantiations of RMDPs and generalizes these policies to larger domains.", "startOffset": 29, "endOffset": 32}, {"referenceID": 8, "context": "In a similar vein, inductive policy selection using first-order regression [9] uses regression to provide the hypothesis space over which a policy is induced.", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "Approximate linear programming (for RMDPs) [10] is an approximation technique using linear program optimization to find a best-fit value function over a number of sampled RMDP domain instantiations.", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "A recent technique for first-order approximate linear programming (FOALP) [21] in FOMDPs approximates a value function by a linear combination of first-order basis functions.", "startOffset": 74, "endOffset": 78}, {"referenceID": 4, "context": "While FOALP incorporates elements of symbolic dynamic programming (SDP) [5], it uses a more compact approximation framework and avoids the need for logical simplification.", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "This stands in contrast with exact value iteration frameworks [5, 12, 13, 14] that prove intractable in many cases due to blowup of the value function representation and the need to perform complex simplifications.", "startOffset": 62, "endOffset": 77}, {"referenceID": 11, "context": "This stands in contrast with exact value iteration frameworks [5, 12, 13, 14] that prove intractable in many cases due to blowup of the value function representation and the need to perform complex simplifications.", "startOffset": 62, "endOffset": 77}, {"referenceID": 12, "context": "This stands in contrast with exact value iteration frameworks [5, 12, 13, 14] that prove intractable in many cases due to blowup of the value function representation and the need to perform complex simplifications.", "startOffset": 62, "endOffset": 77}, {"referenceID": 13, "context": "This stands in contrast with exact value iteration frameworks [5, 12, 13, 14] that prove intractable in many cases due to blowup of the value function representation and the need to perform complex simplifications.", "startOffset": 62, "endOffset": 77}, {"referenceID": 6, "context": "And in contrast to approaches that require sampling of ground domains [7, 9, 10], FOALP solves a FOMDP at the first-order level, thus obviating the need for domain instantiation.", "startOffset": 70, "endOffset": 80}, {"referenceID": 8, "context": "And in contrast to approaches that require sampling of ground domains [7, 9, 10], FOALP solves a FOMDP at the first-order level, thus obviating the need for domain instantiation.", "startOffset": 70, "endOffset": 80}, {"referenceID": 9, "context": "And in contrast to approaches that require sampling of ground domains [7, 9, 10], FOALP solves a FOMDP at the first-order level, thus obviating the need for domain instantiation.", "startOffset": 70, "endOffset": 80}, {"referenceID": 8, "context": "We address the important issue of automatic basis function generation by extending regression-based techniques originally proposed by Gretton and Thiebaux [9].", "startOffset": 155, "endOffset": 158}, {"referenceID": 5, "context": "Approximate Linear Programming: One way of finding a good linear approximation is to cast the optimization problem as a linear program that directly solves for the weights of an L1-minimizing approximation of the optimal value function [6]:", "startOffset": 236, "endOffset": 239}, {"referenceID": 10, "context": "While the size of the objective and the number of constraints in this LP is proportional to the number of states (and therefore exponential), recent solution techniques use compact, factored basis functions and exploit the reward and transition structure of factored MDPs [11, 22], making it possible to avoid generating an exponential number of constraints (and rendering the entire LP compact).", "startOffset": 272, "endOffset": 280}, {"referenceID": 21, "context": "While the size of the objective and the number of constraints in this LP is proportional to the number of states (and therefore exponential), recent solution techniques use compact, factored basis functions and exploit the reward and transition structure of factored MDPs [11, 22], making it possible to avoid generating an exponential number of constraints (and rendering the entire LP compact).", "startOffset": 272, "endOffset": 280}, {"referenceID": 10, "context": "[11] provide the following bound on the loss of V (i+1) w.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The situation calculus [19] is a first-order language for axiomatizing dynamic worlds.", "startOffset": 23, "endOffset": 27}, {"referenceID": 18, "context": "A domain theory is axiomatized in the situation calculus with four classes of axioms [19].", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "These characterize the truth values of the fluent F in the next situation do(a, s) in terms of the current situation s, and embody a solution to the frame problem for deterministic actions [19].", "startOffset": 189, "endOffset": 193}, {"referenceID": 4, "context": "We refer the reader to [5, 21] for a formal definition and discussion of the Regr(\u00b7) operator.", "startOffset": 23, "endOffset": 30}, {"referenceID": 20, "context": "We refer the reader to [5, 21] for a formal definition and discussion of the Regr(\u00b7) operator.", "startOffset": 23, "endOffset": 30}, {"referenceID": 4, "context": "Prior to generalizing the situation calculus to permit a firstorder representation of MDPs, we introduce a case notation to allow first-order specifications of the rewards, probabilities, and values required for FOMDPs (see [5, 21] for formal details):", "startOffset": 224, "endOffset": 231}, {"referenceID": 20, "context": "Prior to generalizing the situation calculus to permit a firstorder representation of MDPs, we introduce a case notation to allow first-order specifications of the rewards, probabilities, and values required for FOMDPs (see [5, 21] for formal details):", "startOffset": 224, "endOffset": 231}, {"referenceID": 4, "context": "We define four additional operators on cases [5, 21]: Regr(\u00b7), \u2203~x, max, and \u222a.", "startOffset": 45, "endOffset": 52}, {"referenceID": 20, "context": "We define four additional operators on cases [5, 21]: Regr(\u00b7), \u2203~x, max, and \u222a.", "startOffset": 45, "endOffset": 52}, {"referenceID": 0, "context": "As a consequence we need only formulate SSAs using the deterministic nature\u2019s choices [1, 5], thus obviating the need for a special treatment of stochastic actions in SSAs.", "startOffset": 86, "endOffset": 92}, {"referenceID": 4, "context": "As a consequence we need only formulate SSAs using the deterministic nature\u2019s choices [1, 5], thus obviating the need for a special treatment of stochastic actions in SSAs.", "startOffset": 86, "endOffset": 92}, {"referenceID": 4, "context": "To define the backup operators, we first define a slightly modified version of the first-order decision theoretic regression (FODTR) operator [5]:", "startOffset": 142, "endOffset": 145}, {"referenceID": 20, "context": "Previous work [21] provides examples ofB andB max.", "startOffset": 14, "endOffset": 18}, {"referenceID": 20, "context": "Following [21], we represent a value function as a weighted sum of k first-order basis functions, denoted bCase i(s), each containing a small number of formulae that provide a first-order abstraction of state space:", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "SSAs can often be compiled from \u201ceffect\u201d axioms that specify action effects [19] and effect axioms can be compiled from PPDDL probabilistic planning domain specifications [25].", "startOffset": 76, "endOffset": 80}, {"referenceID": 24, "context": "SSAs can often be compiled from \u201ceffect\u201d axioms that specify action effects [19] and effect axioms can be compiled from PPDDL probabilistic planning domain specifications [25].", "startOffset": 171, "endOffset": 175}, {"referenceID": 4, "context": "Unlike exact solution methods where value functions can grow exponentially in size during the solution process and must be logically simplified [5], here we maintain the value function in a compact form that requires no simplification, just discovery of good weights.", "startOffset": 144, "endOffset": 147}, {"referenceID": 20, "context": "FOALP was introduced by Sanner and Boutilier [21].", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "Fortunately, we can generalize constraint generation techniques [22] to avoid generating all constraints.", "startOffset": 64, "endOffset": 68}, {"referenceID": 20, "context": "We refer to [21] for further details.", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "[11], we can generalize approximate policy iteration to the first-order case by calculating successive iterations of weights w j that represent the best approximation of the fixed point value function for policy \u03c0Case(s) at iteration i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "While some work has addressed the issue of basis function generation [18, 16], none has been applied to RMDPs or FOMDPs.", "startOffset": 69, "endOffset": 77}, {"referenceID": 15, "context": "While some work has addressed the issue of basis function generation [18, 16], none has been applied to RMDPs or FOMDPs.", "startOffset": 69, "endOffset": 77}, {"referenceID": 8, "context": "We consider a basis function generation method that draws on the work of Gretton and Thiebaux [9], who use inductive logic programming (ILP) techniques to construct a value function from sampled experience.", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "As noted by Gretton and Thiebaux [9], effectively handling universally quantified rewards is one of the most pressing issues in the practical solution of FOMDPs.", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "To address this problem we adopt a decompositional approach, motivated in part by techniques for additive rewards in MDPs [3, 23, 17, 18].", "startOffset": 122, "endOffset": 137}, {"referenceID": 22, "context": "To address this problem we adopt a decompositional approach, motivated in part by techniques for additive rewards in MDPs [3, 23, 17, 18].", "startOffset": 122, "endOffset": 137}, {"referenceID": 16, "context": "To address this problem we adopt a decompositional approach, motivated in part by techniques for additive rewards in MDPs [3, 23, 17, 18].", "startOffset": 122, "endOffset": 137}, {"referenceID": 17, "context": "To address this problem we adopt a decompositional approach, motivated in part by techniques for additive rewards in MDPs [3, 23, 17, 18].", "startOffset": 122, "endOffset": 137}, {"referenceID": 20, "context": "Since the B operator can often retain much of the additive structure in the linear approximation of vCase(s) [21], representation and computation with this Q-function is very efficient.", "startOffset": 109, "endOffset": 113}, {"referenceID": 2, "context": "Following [3, 17], we do this by treating the sum of the Q-values of any action in the subgoal MDPs as a measure of its Q-value in the joint (original) MDP.", "startOffset": 10, "endOffset": 17}, {"referenceID": 16, "context": "Following [3, 17], we do this by treating the sum of the Q-values of any action in the subgoal MDPs as a measure of its Q-value in the joint (original) MDP.", "startOffset": 10, "endOffset": 17}, {"referenceID": 20, "context": ", during constraint generation [21]), the max over B will implicitly enforce the max constraint of B max.", "startOffset": 31, "endOffset": 35}, {"referenceID": 14, "context": "We applied FOALP and FOAPI to the Box World logistics and Blocks World probabilistic planning problems from the ICAPS 2004 IPPC [15].", "startOffset": 128, "endOffset": 132}, {"referenceID": 19, "context": "We used the Vampire [20] theorem prover and the CPLEX 9.", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": "There appears to be exponential growth in the running time as the number of basis functions increases; this reflects the results of previous work [21].", "startOffset": 146, "endOffset": 150}, {"referenceID": 7, "context": "We compared our planning system to the three other top-performing planners on these domains: G2 is a temporal logic planner with human-coded control knowledge [8]; P is an RTDP-based planner [2]; J1 is a human-coded planner, J2 is an inductive policy iteration planner, and J3 is a deterministic replanner [24].", "startOffset": 159, "endOffset": 162}, {"referenceID": 1, "context": "We compared our planning system to the three other top-performing planners on these domains: G2 is a temporal logic planner with human-coded control knowledge [8]; P is an RTDP-based planner [2]; J1 is a human-coded planner, J2 is an inductive policy iteration planner, and J3 is a deterministic replanner [24].", "startOffset": 191, "endOffset": 194}, {"referenceID": 23, "context": "We compared our planning system to the three other top-performing planners on these domains: G2 is a temporal logic planner with human-coded control knowledge [8]; P is an RTDP-based planner [2]; J1 is a human-coded planner, J2 is an inductive policy iteration planner, and J3 is a deterministic replanner [24].", "startOffset": 306, "endOffset": 310}, {"referenceID": 14, "context": ") on the Box World and Blocks World probabilistic planning problems from the ICAPS 2004 IPPC [15] (\u2013 indicates no data).", "startOffset": 93, "endOffset": 97}], "year": 0, "abstractText": "Recent work on approximate linear programming (ALP) techniques for first-order Markov Decision Processes (FOMDPs) represents the value function linearly w.r.t. a set of first-order basis functions and uses linear programming techniques to determine suitable weights. This approach offers the advantage that it does not require simplification of the first-order value function, and allows one to solve FOMDPs independent of a specific domain instantiation. In this paper, we address several questions to enhance the applicability of this work: (1) Can we extend the first-order ALP framework to approximate policy iteration and if so, how do these two algorithms compare? (2) Can we automatically generate basis functions and evaluate their impact on value function quality? (3) How can we decompose intractable problems with universally quantified rewards into tractable subproblems? We propose answers to these questions along with a number of novel optimizations and provide a comparative empirical evaluation on problems from the ICAPS 2004 Probabilistic Planning Competition.", "creator": null}}}