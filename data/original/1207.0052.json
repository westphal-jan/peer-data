{"id": "1207.0052", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2012", "title": "The Complexity of Learning Principles and Parameters Grammars", "abstract": "We investigate models for learning the class of context-free and context-sensitive languages (CFLs and CSLs). We begin with a brief discussion of some early hardness results which show that unrestricted language learning is impossible, and unrestricted CFL learning is computationally infeasible; we then briefly survey the literature on algorithms for learning restricted subclasses of the CFLs. Finally, we introduce a new family of subclasses, the principled parametric context-free grammars (and a corresponding family of principled parametric context-sensitive grammars), which roughly model the \"Principles and Parameters\" framework in psycholinguistics. We present three hardness results: first, that the PPCFGs are not efficiently learnable given equivalence and membership oracles, second, that the PPCFGs are not efficiently learnable from positive presentations unless P = NP, and third, that the PPCSGs are not efficiently learnable from positive presentations unless integer factorization is in P.", "histories": [["v1", "Sat, 30 Jun 2012 06:36:04 GMT  (9kb)", "https://arxiv.org/abs/1207.0052v1", null], ["v2", "Tue, 3 Jul 2012 03:49:57 GMT  (9kb)", "http://arxiv.org/abs/1207.0052v2", null], ["v3", "Fri, 6 Jul 2012 15:20:17 GMT  (9kb)", "http://arxiv.org/abs/1207.0052v3", null]], "reviews": [], "SUBJECTS": "cs.FL cs.CL", "authors": ["jacob", "reas"], "accepted": false, "id": "1207.0052"}, "pdf": {"name": "1207.0052.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n20 7.\n00 52\nv3 [\ncs .F\nL ]\n6 J"}, {"heading": "1 Introduction", "text": "A great deal modern psycholinguistics has concerned itself with resolving the problem of the so-called \u201cpoverty of the stimulus\u201d\u2014the claim that natural languages are unlearnable given only the data available to infants, and consequently that some part of syntax must be \u201cnative\u201d (i.e. prespecified) rather than learned. Gold\u2019s theorem (described below), which states that there exists a superfinite class of languages which is not learnable in the limit from positive presentations, is often offered as proof of this fact (though the extent to which the theorem is psycholinguistically informative remains a contentious issue). [gor90]\nBut how is innate linguistic knowledge represented? One mechanism usually offered is the Chomskian \u201cPrinciples and Parameters\u201d framework [cho93], which suggests that there is a set of universal principles of grammar which inhere in the structure of the brain. In this framework, the process of language learning simply consists of determining appropriate settings for a finite number of parameters which determine how those principles are applied.\nWhile this problem is generally supposed to be easier than unrestricted language learning, we are not aware of any previous work specifically aimed at studying the Principles and Parameters model in a computational setting. In this report, we introduce a family of subclasses of the context-free languages which we believe roughly captures the intuition behind the Principles and Parameters model, and explore the difficulty of learning that model in various learning environments.\nWe begin by presenting an extremely brief survey of the existing literature on the hardness of language learning; we then introduce three hardness results, one unconditional, one complexity-theoretic and one cryptographic, which suggest that the existence of a generalized algorithm for learning in the principles and parameters framework is highly unlikely. While we obviously cannot produce any psychologically definitive results in this setting, we at least hope to challenge the notion that the Principles and Parameters framework is somehow a computationally satisfying explanation of the language learning process."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Definitions", "text": ""}, {"heading": "2.1.1 Learnability in the limit", "text": "Gold defines the language learning problem as follows: [gol67]\nDefinition 1. Given a class of languages L and an algorithm A, we say A identifies L in the limit from positive presentations if \u2200L, \u2200i1, i2, \u00b7 \u00b7 \u00b7 \u2208 L, there is a time t such that for all u > t, hu = ht = A(i1, i2, \u00b7 \u00b7 \u00b7 , it)."}, {"heading": "2.1.2 Exact identification using queries", "text": "Modeling the language learning process as being entirely dependent on positive examples seems rather extreme; it\u2019s useful to consider enviornments in which the learner has access to a richer representation of the language. Angluin [ang90] describes a model of language learnability from oracle queries, as follows:\nDefinition 2. An equivalence oracle for a language L takes as input the representation of a language r(L) and outputs \u201ctrue\u201d if L = L\u2217, or some w \u2208 L\u2206L\u2217 (the symmetric difference of the languages) otherwise. There is an obvious equivalence, first pointed out by Littlestone [lit88], between the equivalence query model and the online mistake bound model.\nDefinition 3. A membership oracle for a language L with start symbol S takes a string w, and outputs true if S \u21d2\u2217 w and false otherwise.\nDefinition 4. A nonterminal membership oracle for a language L takes a string w (not necessarily in L) and a nonterminal A, and outputs whether A \u21d2\u2217 w (i.e. whether the set of possible derivations with A as a start symbol includes w).\nDefinition 5. A class of languages L is learnable from an equivalence oracle (or analogously from an equivalence oracle and a membership oracle, sometimes referred to as a \u201cminimal adequate teacher\u201d) if there exists a learning algorithm with runtime polynomial in the size of the representation of the class and length of the longest counterexample."}, {"heading": "2.2 Hardness of language learning", "text": "Theorem (Gold). There exists a class of languages not learnable in the limit from positive presentations.\nProof sketch. Construct an infinite sequence of languages L1 \u2282 L2 \u2282 \u00b7 \u00b7 \u00b7 , all finite, and let L\u221e = \u22c3\ni Li. Suppose there existed some algorithm A that could identify each Li from positive presentations. Then there is a positive presentation of L\u221e that causes A to make an infinite number of mistakes. First present a set of examples, all in L1, that force A to identify L1. Then present a set of examples forcing it to identify L2, then L3, and so on. An infinite number of mistakes can be forced in this way, so L\u221e is not learnable in the limit.\nWhile space does not permit us to discuss the proof here, we also note the following important result for CFL learning:\nTheorem (Angluin [ang80]). There exists a class of context-free languages with \u201cnatural\u201d representations which are not learnable from equivalence queries in time polynomial in the size of the representation."}, {"heading": "2.3 Learnable subclasses of the CFLs", "text": "While this last result rules out the possibility of a general algorithm for learning CFLs, subsets of the CFLs have been shown to be learnable when given slightly more powerful oracles. These include simple deterministic languages [ish90], one-counter languages [ber87] and so-called very simple languages [yok91]. Particularly heartening is Angluin\u2019s result that k-bounded CFGs can be learned in polynomial time if nonterminal membership queries are permitted [ang87]."}, {"heading": "3 Principled Parametric Grammars", "text": "We now introduce a formal model of the \u201cprinciples and parameters\u201d framework described in the introduction."}, {"heading": "3.1 Motivation", "text": "Before moving on to the details of the construction, it\u2019s useful to consider a few example \u201cprinciples\u201d and \u201cparameters\u201d suggested by proponents of the model.\n\u2022 The pro-drop parameter: does this language allow pronoun dropping? If PNP is a non-terminal symbol designating a pronoun, this parameter determines whether or not a rule of the form PNP \u2192 \u03b5 exists in the language.\n\u2022 The ergative/nominative parameter: ergative languages distinguish between transitive and intransitive senses of verb by marking the subject, while nominative languages (like English) mark the object. Let NP and VP be non-terminal symbols for noun and verb phrases respectively, and let NPtrans and VPtrans be distinguished versions of those symbols for ergative/nominative marking. Now, any language with Verb-Subject-Object order, there will be a rule S \u2192 NP VP. In an ergative language, there is additionally a rule of the form S \u2192 NPtrans VP, and in a nominative language a rule of the form S \u2192 NP VPtrans.\nIn each of these cases, a pattern holds: for every possible possible parameter setting, there is some finite set of context-free productions in the native grammar, from which only one must be selected as the element of the learned grammar. This leads very naturally to the following development of principled parametric context-free grammars as a model of the principles and parameters model."}, {"heading": "3.2 Construction", "text": "Definition 6. An n-principled, k-parametric context-free grammar ((n, k)-PPCFG) \u0393 is a 4-tuple (V,\u03a3,\u03a0, S), where:\n1. V is a finite alphabet of nonterminal symbols\n2. \u03a3 is a finite alphabet of terminal symbols\n3. \u03a0 is a set of n production groups of the form\n(Ai,1 \u2192 \u03b1i,1), \u00b7 \u00b7 \u00b7 , (Ai,j \u2192 \u03b1i,j), \u00b7 \u00b7 \u00b7 , (Ai,k \u2192 \u03b1i,k)\nwhere each \u03b1 \u2208 (V \u222a\u03a3)\u2217, i.e. is a finite sequence of terminals and nonterminals. Let \u03a0i,j denote the production (Ai,j \u2192 \u03b1i,j).\n4. S \u2208 V is the start symbol. Definition 7. A parameter setting p = (p1, p2, \u00b7 \u00b7 \u00b7 , pn) is a sequence of length n, with each pi \u2208 1..k. Then define \u0393p to be the ordinary context-free grammar (V , \u03a3, R, S) with R = {\u03a0i,pi : i \u2208 1..n}.\nAs usual, let L(G) denote the context free language represented by the CFG G. Then let \u039b(\u0393) = {L(G) : \u2203p : G = \u0393p}. Definition 8. An algorithm A learns the PPCFGs from an equivalence oracle if \u2200 PPCFGs \u0393 and languages l \u2208 \u0393, after a finite number of oracle queries, A outputs some p such that L(\u0393p) = l, or determines that no such p exists.\nDefinition 9. A efficiently learns the PPCFGs from an equivalence oracle if the number of oracle queries it makes is bounded by some polynomial function poly(n, k).\nDefinition 10. Finally, a principled parametric context-sensitive grammar is defined exactly as above, with corresponding learning definitions, but with context-sensitive productions in each production group."}, {"heading": "3.3 Equivalence", "text": "Some useful facts about the PPCFGs:\nObservation. A \u201cheterogeneous PPCFG\u201d with a variable number of right hand sides can be transformed into a \u201chomogeneous PPCFG\u201d of the kind described above by \u201cpadding\u201d out the shorter principles with duplicate rules (i.e. to insert an unambiguous production A \u2192 \u03b1 into an (n, 2)-PPCFG, add to \u03a0 the production group (A \u2192 \u03b1), (A \u2192 \u03b1)). Observation. A (n, k)-PPCFG can be converted into an (n(k\u2212 1), 2)-PPCFG as follows: replace each principle\nA \u2192 (\u03b11, \u03b12, . . . , \u03b1k)\nwith a set of principles\nA1 \u2192 (\u03b11, A2) A2 \u2192 (\u03b12, A3)\n...\nAk\u22121 \u2192 (\u03b1k\u22121, \u03b1k)\nThus without loss of generality we may treat every PPCFG as a (n, 2)- PPCFG. The conversion above results in only a polynomial increase in the number of principles, so any algorithm which is polynomial in n, and which assumes k = 2, can be used to solve k > 2 with only a polynomial increase in running time. This also means that we may specify an individual language in a PPCFG by a bit string of length n.\nFinally, note that a k-PPCFG with n rules contains at most kn languages."}, {"heading": "4 Generic hardness results for PPCFGs", "text": "We will construct a minimal adequate teacher T consisting of two oracles EQ (an equivalence oracle) and M (a membership oracle), such that any algorithm A requires an exponential number of queries to identify the correct parameter setting p from a PPCFG \u0393.\nTheorem 1. Without condition, there exists no algorithm A capable of learning the PPCFGs from equivalence queries and membership queries in polynomial time.\nProof. Fix some number N . Construct the PPCFG \u0393 with\nV = Xi : i \u2208 1..N S = START \u03a3 = {0, 1}\nand \u03a0 as defined as follows:\n(START \u2192 X1X2 \u00b7 \u00b7 \u00b7XN ) (Xk \u2192 0, Xk \u2192 1) \u2200k \u2208 1..N\nEvery parameter setting p in this grammar allows it to derive precisely 1 string: every production is deterministic. Consequently, the N possible settings of the grammar derive 2N unique strings. Given some algorithm A for learning PPCFGs, the procedure specified below describes an adversarial distinguisher for this PPCFG which forces the learner to make a total of 2N \u2212 1 queries.\nAfter each query, the number of grammars still possible given the evidence provided so far decreases by precisely 1 (because each grammar is capable of producing only string), so after 2N \u2212 1 queries of either kind, the oracle must output true.\nThus, only after 2N \u2212 1 queries (superpolynomial in |\u0393| and the length of the longest production) can the learner halt, so the grammar is not efficiently learnable from membership and equivalence queries.\ni \u2190 0 while i < 2N \u2212 1 do\non query EQ(\u0393\u2032) if \u0393\u2032 has not been previously queried then\ni \u2190 i+ 1 end if return FALSE, L(\u0393\u2032) \u22b2 L(\u0393\u2032) contains only one string\nend query on query M(w)\nif w has not been previously queried then i \u2190 i+ 1 end if return FALSE\nend query end while on query\nreturn TRUE \u22b2 Only one language is consistent with the evidence end query"}, {"heading": "5 Complexity-theoretic hardness results for", "text": "PPCFGs\nWe will construct a reduction from 3SAT to PPCFG learning. Let X = {xi} be a set of variables and C = {ci} be a set of clauses. Let us write xj \u2208 ci if the jth ith clause is satisfied by the jth variable, and x\u0304j \u2208 ci if the ith clause is satisfied by the negation of the jth variable.\nThen construct the PPCFG \u0393 with V = X \u222a {START},\u03a3 = C, S = START, and \u03a0 with the following production groups:\n(START \u2192 x1x2 \u00b7 \u00b7 \u00b7xn) (xi \u2192 xi,T ), (x1 \u2192 xi,F ) \u2200xi \u2208 X (xi \u2192 \u03b5) \u2200xi \u2208 X (xj,T \u2192 ci) \u2200ci \u2208 C, \u2200xj \u2208 ci (xj,F \u2192 ci) \u2200ci \u2208 C, \u2200x\u0304j \u2208 ci\nNote that only for production groups of the form (xi \u2192 xi,T ), (x1 \u2192 xi,F ) does the parameter setting change the resulting language. These groups may be thought of as assigning truth values to the variables.\nProposition 1. If there exists some l \u2208 \u0393 such that \u2200ci \u2208 C : ci \u2208 l, then the 3SAT instance is satisfiable.\nProof. Set xi true if the rule xi \u2192 xi,T is chosen, and false otherwise. For any\nci in the language, there is a derivation from START \u21d2\u2217 ci of the following form:\nSTART \u21d2 x1x2 \u00b7 \u00b7 \u00b7xn \u21d2 xj \u21d2 xj,a \u21d2 ci\nThen xj satisfies ci.\nProposition 2. If the 3SAT instance is satisfiable, there exists some l \u2208 \u0393 such that \u2200ci \u2208 C : ci \u2208 l.\nProof. Choose the rule (xi \u2192 xi,T ) if xi is set true in the satisfying assignment, and (xi \u2192 xi,F ) if xi is set false. These settings determine l. Then, consider any string ci. There is some variable xj with truth value a which satisfies the corresponding clause; then by assignment l contains a production of the form xj \u2192 xj,a, and by definition contains a production of the form xj,a \u2192 ci, so derivation identical to the one in the previous proposition must exist.\nTheorem 2. If P 6= NP, no efficient algorithm exists for learning PPCFGs from positive presentations.\nProof. Assume that there exists some algorithm A which efficiently learns the PPCFGs from positive presentations. We will use A to construct a SAT solver S by simulating the oracle. Construct \u0393 from the SAT instance as described above. Then S\u2019s interaction with A takes the following form:\nBy assumption, after observing polynomially many positive presentations, and performing polynomially many computations, A outputs a parameter setting p which produces every ci \u2208 C, or a signal indicating no such assignment exists. From Propositions 1 and 2, such a p exists if and only if the SAT instance is satisfiable. Thus S determines in a polynomial number of steps whether the SAT instance is satisfiable, and the existence of A implies P = NP."}, {"heading": "6 Cryptographic hardness results for PPCSGs", "text": "We will construct another reduction, this time from integer factorization to PPCSG learning. Let N be a product of two (n\u2212 1)-digit primes.\nLet A be a set of non-terminal symbols A0..A\u2308lg \u221a N\u2309, and B,C,Z be similar sets of nonterminals of cardinality \u2308lgN\u2309 + 1. Then construct the PPCSG \u0393 with\nV = A \u222aB \u222a C \u222a Z \u222a {S} \u03a3 = {ck} \u2200k \u2208 0..\u2308lgN\u2309 S = S\nand \u03a0 with the following production groups:\n(S \u2192 A\u2308lg\u221aN\u2309S) (S \u2192 \u03b5) (A0 \u2192 B0), (A0 \u2192 \u03b5) (Aj \u2192 Aj\u22121), (Aj \u2192 BjAj\u22121) \u2200j \u2208 1..\u2308lg \u221a N\u2309 (Bj \u2192 Bj\u22121Bj\u22121) \u2200j \u2208 1..\u2308lg \u221a N\u2309 (B0 \u2192 C0) (CkCk \u2192 CkZk) (CkZk \u2192 Ck+1Zk) \u2200k \u2208 0..\u2308lgN\u2309 (Ck+1Zk \u2192 Ck+1) (Ck \u2192 ck)\nIntuitively, the parameter settings in this grammar (Aj \u2192 Aj\u22121), (Aj \u2192 BjAj\u22121) fix some number m between 1 and \u221a N . Each A\u2308lg \u221a N\u2309 \u21d2\u2217 Cm, so S \u21d2\u2217 Cmk for all k, i.e. the unary representation of all multiples of m. This unary string may then be collapsed into a \u2308lgN\u2309-ary representation as a string of terminal cis.\nLet l be the language consisting of the single string w, where w is the concatenation of every ai such that the ith digit of the binary representation of N is 1.\nGiven a parameter setting s for \u0393, for each production group (Aj \u2192 Aj\u22121), (Aj \u2192 BjAj\u22121) in s, let pi = 0 if the first setting is chosen and 1 if the second setting is chosen. Let Ps be the number whose binary representation is given by the pis. Alternatively, given a binary number P let sP be the parameter setting induced by P \u2019s bits.\nFinally, some notation: given a sequence of strings S, let f s\u2208S si denote the\nconcatenation of all sis. Proposition 3. Given numbers P and Q, P \u2264 Q, if PQ = N then w \u2208 L(\u0393sP ). Proof. In \u0393sT ,\nS \u21d2\u2217 SQ\n\u21d2\u2217 (A\u2308lg\u221aN\u2309)Q\n\u21d2\u2217\n\n  \nn\n0\u2264i\u2264\u2308lg \u221a N\u2309\nTi=1\nBi\n\n  \nQ\n\u21d2\u2217 BPQ0 = BN0 \u21d2\u2217 CN0 \u21d2\u2217 w\nProposition 4. If w \u2208 L(\u0393s), then there exists Q such that PsQ = N .\nProof. Certainly if w \u2208 L(\u0393s), CN0 \u21d2\u2217 w. But S \u21d2\u2217 CPsk0 for all k (using the derivation in Proposition 3); then there exists some Q such that PQ = N .\nTheorem 3. If integer factorization is hard, no efficient algorithm exists for learning random PPCSGs with non-negligible probability from an equivalence oracle.\nProof. Assume that there exists some algorithm A which, given \u0393 and the positive presentation of the single string w as specified above, outputs a parameter setting P for \u0393 such that w \u2208 L(\u0393P ) with non-negligible probability a polynomial number of computations. Then we will construct a factorizer F that decomposes N into P and Q.\nFrom the preceding conjectures, if an acceptable P is found then PQ = N , for some Q, so if A can find a parameter setting in polynomial time then this algorithm finds a factorization in polynomial time.\nThis final proof is neither particularly interesting or satisfying: even the task of finding a derivation in a CSG is known to be PSPACE-complete (though it\u2019s easy to see that a polynomial-time parsing algorithm for this particular family of grammars exists). Note that the only context-sensitive production groups employed in this production are used to guarantee a compact encoding of w; we suspect that there is an alternative way of constructing this \u201cgrammar arithmetic\u201d that requires only weaker rules, perhaps mildly context-sensitive or even context-free. We thus close with the following:\nOpen Problem. If integer factorization is hard, does there exist a polynomialtime algorithm for learning random PPCFGs with non-negligible probability from positive presentations?"}, {"heading": "7 Conclusion", "text": "We have introduced a new model, the princpled parametric context-free (also context-sensitive) grammars as a model of the \u201cPrinciples and Parameters\u201d model in psycholinguistics, and presented three hardness-of-learning results for the class of PPCFGs and PPCSGs. While these results certainly do not demonstrate definitively that learning under the Principles and Parameters framework is completely impossible (all that is required for human language learning to be possible is that one PPCFG be efficiently learnable), we have shown that there is likely no generic algorithm for learning a class of PPCFGs given either oracle and membership queries or a positive presentation. In general, these results prove that even radically restricting the class of candidate grammars does not guarantee a successful outcome when attempting to learn CFGs and CSGs."}], "references": [{"title": "Inductive inference of formal languages from positive data.", "author": ["Angluin", "Dana"], "venue": "Info. Contr. 1980,", "citeRegEx": "Angluin and Dana.,? \\Q1980\\E", "shortCiteRegEx": "Angluin and Dana.", "year": 1980}, {"title": "Learning k-bounded context-free grammars.", "author": ["Angluin", "Dana"], "venue": "Yale Technical Report RR-557,", "citeRegEx": "Angluin and Dana.,? \\Q1987\\E", "shortCiteRegEx": "Angluin and Dana.", "year": 1987}, {"title": "Negative Results for Equivalence Queries.", "author": ["Angluin", "Dana"], "venue": "InMachine Learning 1990,", "citeRegEx": "Angluin and Dana.,? \\Q1990\\E", "shortCiteRegEx": "Angluin and Dana.", "year": 1990}, {"title": "Learning one-counter languages in polynomial time.", "author": ["Berman", "Piotr", "Roos", "Robert"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Berman et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Berman et al\\.", "year": 1987}, {"title": "Language Identification in the Limit.", "author": ["Gold", "E. Mark"], "venue": "In Information and Control 1967,", "citeRegEx": "Gold and Mark.,? \\Q1967\\E", "shortCiteRegEx": "Gold and Mark.", "year": 1967}, {"title": "Learnability and Feedback.", "author": ["Gordon", "Peter"], "venue": "In Developmental Psychology 1990,", "citeRegEx": "Gordon and Peter.,? \\Q1990\\E", "shortCiteRegEx": "Gordon and Peter.", "year": 1990}, {"title": "Polynomial Time Learnability of Simple Deterministic Languages.", "author": ["Ishizawa", "Hiroki"], "venue": "In Machine Learning 1990,", "citeRegEx": "Ishizawa and Hiroki.,? \\Q1990\\E", "shortCiteRegEx": "Ishizawa and Hiroki.", "year": 1990}, {"title": "Learning Quickly When Irrelevant Attributes Abound.", "author": ["Littlestone", "Nick"], "venue": "In Machine Learning 1988,", "citeRegEx": "Littlestone and Nick.,? \\Q1988\\E", "shortCiteRegEx": "Littlestone and Nick.", "year": 1988}, {"title": "Polynomial Time Learning of Very Simple Grammars from Positive Data.", "author": ["Yokomori", "Takashi"], "venue": "In proceedings of COLT,", "citeRegEx": "Yokomori and Takashi.,? \\Q1991\\E", "shortCiteRegEx": "Yokomori and Takashi.", "year": 1991}], "referenceMentions": [], "year": 2012, "abstractText": "We investigate models for learning the class of context-free and context-sensitive languages (CFLs and CSLs). We begin with a brief discussion of some early hardness results which show that unrestricted language learning is impossible, and unrestricted CFL learning is computationally infeasible; we then briefly survey the literature on algorithms for learning restricted subclasses of the CFLs. Finally, we introduce a new family of subclasses, the principled parametric context-free grammars (and a corresponding family of principled parametric context-sensitive grammars), which roughly model the \u201cPrinciples and Parameters\u201d framework in psycholinguistics. We present three hardness results: first, that the PPCFGs are not efficiently learnable given equivalence and membership oracles, second, that the PPCFGs are not efficiently learnable from positive presentations unless P = NP, and third, that the PPCSGs are not efficiently learnable from positive presentations unless integer factorization is in P.", "creator": "LaTeX with hyperref package"}}}