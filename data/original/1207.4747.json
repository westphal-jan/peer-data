{"id": "1207.4747", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jul-2012", "title": "Block-Coordinate Frank-Wolfe Optimization for Structural SVMs", "abstract": "We consider the use of Frank-Wolfe optimization algorithms on the dual formulation of structural SVMs. These yield simple algorithms which only need access to an approximate maximization oracle for the structured prediction problem and thus have wide applicability. This perspective provides insights on previous popular algorithms as we show that batch subgradient as well as the cutting plane algorithms are equivalent to versions of Frank-Wolfe algorithms, enabling us to improve on their convergence analysis by harvesting the Frank-Wolfe literature. Moreover, we propose a new stochastic coordinate descent version of Frank-Wolfe which yields a provably convergent optimization algorithm for structural SVMs with total run-time independent of the number of training examples, like Pegasos, but with duality gap certificate guarantees and step-size robustness thanks to the use of line-search. Our experiments on sequence prediction indicate that this simple algorithm outperforms all other optimization algorithms which only have access to the maximization oracle.", "histories": [["v1", "Thu, 19 Jul 2012 18:02:41 GMT  (2347kb,D)", "https://arxiv.org/abs/1207.4747v1", "13 pages main text + 13 pages appendix (short version). Under review"], ["v2", "Mon, 29 Oct 2012 18:03:32 GMT  (3198kb,D)", "http://arxiv.org/abs/1207.4747v2", "10 pages main text + 17 pages appendix. Under review"], ["v3", "Tue, 30 Oct 2012 19:25:10 GMT  (1599kb,D)", "http://arxiv.org/abs/1207.4747v3", "10 pages main text + 17 pages appendix. Under review. Changes from v1 to v3: 1) Re-organized text for clarity + changed title; 2) Added new experiments (more settings, online EG, matching dataset); 3) Added multiplicative approximation result in theorems; 4) Corrected typo for rate of online EG in Table 1. v2 was missing acknowledgment section"], ["v4", "Mon, 14 Jan 2013 13:26:51 GMT  (1895kb,D)", "http://arxiv.org/abs/1207.4747v4", "Appears in Proceedings of the 30th International Conference on Machine Learning (ICML 2013). 9 pages main text + 22 pages appendix. Changes from v3 to v4: 1) Re-organized appendix; improved &amp; clarified duality gap proofs; re-drew all plots; 2) Changed convention for Cf definition; 3) Added weighted averaging experiments + convergence results; 4) Clarified main text and relationship with appendix"]], "COMMENTS": "13 pages main text + 13 pages appendix (short version). Under review", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["simon lacoste-julien", "martin jaggi", "mark w schmidt", "patrick pletscher"], "accepted": true, "id": "1207.4747"}, "pdf": {"name": "1207.4747.pdf", "metadata": {"source": "META", "title": "Block-Coordinate Frank-Wolfe Optimization for Structural SVMs", "authors": ["Simon Lacoste-Julien", "Martin Jaggi", "Mark Schmidt"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Binary SVMs are amongst the most popular classification methods, and this has motivated substantial interest in optimization solvers that are tailored to their specific problem structure. However, despite their wider applicability, there has been much less work on solving the optimization problem associated with structural SVMs, which are the generalization of SVMs to structured outputs like graphs and other combinatorial objects (Taskar et al., 2003; Tsochantaridis et al., 2005). This seems to be due to the difficulty of dealing with the exponential number of constraints in the primal problem, or the exponential number of variables in the dual problem. Indeed, because they achieve an O\u0303(1/\u03b5) convergence rate while only requiring a single\nProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).\ncall to the so-called maximization oracle on each iteration, basic stochastic subgradient methods are still widely used for training structural SVMs (Ratliff et al., 2007; Shalev-Shwartz et al., 2010a). However, these methods are often frustrating to use for practitioners, because their performance is very sensitive to the sequence of step sizes, and because it is difficult to decide when to terminate the iterations.\nTo solve the dual structural SVM problem, in this paper we consider the Frank-Wolfe (1956) algorithm, which has seen a recent surge of interest in machine learning and signal processing (Mangasarian, 1995; Clarkson, 2010; Jaggi, 2011; 2013; Bach et al., 2012), including in the context of binary SVMs (Ga\u0308rtner & Jaggi, 2009; Ouyang & Gray, 2010). A key advantage of this algorithm is that the iterates are sparse, and we show that this allows us to efficiently apply it to the dual structural SVM objective even though there are an exponential number of variables. A second key advantage of this algorithm is that the iterations only require optimizing linear functions over the constrained domain, and we show that this is equivalent to the maximization oracle used by subgradient and cutting-plane methods (Joachims et al., 2009; Teo et al., 2010). Thus, the Frank-Wolfe algorithm has the same wide applicability as subgradient methods, and can be applied to problems such as low-treewidth graphical models (Taskar et al., 2003), graph matchings (Caetano et al., 2009), and associative Markov networks (Taskar, 2004). In contrast, other approaches must use more expensive (and potentially intractable) oracles such as computing marginals over labels (Collins et al., 2008; Zhang et al., 2011) or doing a Bregman projection onto the space of structures (Taskar et al., 2006). Interestingly, for structural SVMs we also show that existing batch subgradient and cutting-plane methods are special cases of Frank-Wolfe algorithms, and this leads to stronger and simpler O(1/\u03b5) convergence rate guarantees for these existing algorithms. ar X iv :1 20 7. 47 47 v4 [ cs .L\nG ]\n1 4\nJa n\n20 13\nAs in other batch structural SVM solvers like cuttingplane methods (Joachims et al., 2009; Teo et al., 2010) and the excessive gap technique (Zhang et al., 2011) (see Table 1 at the end for an overview), each FrankWolfe iteration unfortunately requires calling the appropriate oracle once for all training examples, unlike the single oracle call needed by stochastic subgradient methods. This can be prohibitive for data sets with a large number of training examples. To reduce this cost, we propose a novel randomized block-coordinate version of the Frank-Wolfe algorithm for problems with block-separable constraints. We show that this algorithm still achieves the O(1/\u03b5) convergence rate of the full Frank-Wolfe algorithm, and in the context of structural SVMs, it only requires a single call to the maximization oracle. Although the stochastic subgradient and the novel block-coordinate Frank-Wolfe algorithms have a similar iteration cost and theoretical convergence rate for solving the structural SVM problem, the new algorithm has several important advantages for practitioners:\n\u2022 The optimal step-size can be efficiently computed in closed-form, hence no step-size needs to be selected. \u2022 The algorithm yields a duality gap guarantee, and (at the cost of computing the primal objective) we can compute the duality gap as a proper stopping criterion. \u2022 The convergence rate holds even when using approximate maximization oracles.\nFurther, our experimental results show that the optimal step-size leads to a significant advantage during the first few passes through the data, and a systematic (but smaller) advantage in later passes."}, {"heading": "2. Structural Support Vector Machines", "text": "We first briefly review the standard convex optimization setup for structural SVMs (Taskar et al., 2003; Tsochantaridis et al., 2005). In structured prediction, the goal is to predict a structured object y \u2208 Y(x) (such as a sequence of tags) for a given input x \u2208 X . In the standard approach, a structured feature map \u03c6 : X \u00d7 Y \u2192 Rd encodes the relevant information for input/output pairs, and a linear classifier with parameter w is defined by hw(x) = argmaxy\u2208Y(x)\u3008w,\u03c6(x,y)\u3009. Given a labeled training set D = {(xi,yi)}ni=1, w is estimated by solving\nmin w, \u03be\n\u03bb 2 \u2016w\u20162 + 1 n\nn\u2211\ni=1\n\u03bei (1)\ns.t. \u3008w,\u03c8i(y)\u3009 \u2265 L(yi,y)\u2212 \u03bei \u2200i, \u2200y \u2208 =:Yi\ufe37 \ufe38\ufe38 \ufe37 Y(xi),\nwhere \u03c8i(y) := \u03c6(xi,yi) \u2212 \u03c6(xi,y), and Li(y) := L(yi,y) denotes the task-dependent structured error of predicting output y instead of the observed output yi (typically a Hamming distance between the two labels). The slack variable \u03bei measures the surrogate loss for the i-th datapoint and \u03bb is the regularization parameter. The convex problem (1) is what Joachims et al. (2009, Optimization Problem 2) call the n-slack structural SVM with margin-rescaling. A variant with slack-rescaling was proposed by Tsochantaridis et al. (2005), which is equivalent to our setting if we replace all vectors \u03c8i(y) by Li(y)\u03c8i(y).\nLoss-Augmented Decoding. Unfortunately, the above problem can have an exponential number of constraints due to the combinatorial nature of Y. We can replace the \u2211 i |Yi| linear constraints with n piecewiselinear ones by defining the structured hinge-loss:\n\u2018max oracle\u2019 H\u0303i(w) := max y\u2208Yi Li(y)\u2212 \u3008w,\u03c8i(y)\u3009\ufe38 \ufe37\ufe37 \ufe38 =:Hi(y;w) . (2)\nThe constraints in (1) can thus be replaced with the non-linear ones \u03bei \u2265 H\u0303i(w). The computation of the structured hinge-loss for each i amounts to finding the most \u2018violating\u2019 output y for a given input xi, a task which can be carried out efficiently in many structured prediction settings (see the introduction). This problem is called the loss-augmented decoding subproblem. In this paper, we only assume access to an efficient solver for this subproblem, and we call such a solver a maximization oracle. The equivalent non-smooth unconstrained formulation of (1) is:\nmin w\n\u03bb 2 \u2016w\u20162 + 1 n\nn\u2211\ni=1\nH\u0303i(w). (3)\nHaving a maximization oracle allows us to apply subgradient methods to this problem (Ratliff et al., 2007), as a subgradient of H\u0303i(w) with respect to w is \u2212\u03c8i(y\u2217i ), where y\u2217i is any maximizer of the lossaugmented decoding subproblem (2).\nThe Dual. The Lagrange dual of the above n-slackformulation (1) has m := \u2211 i |Yi| variables or potential \u2018support vectors\u2019. Writing \u03b1i(y) for the dual variable associated with the training example i and potential output y \u2208 Yi, the dual problem is given by\nmin \u03b1\u2208Rm \u03b1\u22650\nf(\u03b1) := \u03bb\n2\n\u2225\u2225A\u03b1 \u2225\u22252 \u2212 bT\u03b1 (4)\ns.t. \u2211 y\u2208Yi \u03b1i(y) = 1 \u2200i \u2208 [n] ,\nwhere the matrix A \u2208 Rd\u00d7m consists of the m columns A := { 1 \u03bbn\u03c8i(y) \u2208 Rd \u2223\u2223 i \u2208 [n],y \u2208 Yi } , and the vector\nb \u2208 Rm is given by b := (\n1 nLi(y) ) i\u2208[n],y\u2208Yi . Given a\ndual variable vector \u03b1, we can use the Karush-KuhnTucker optimality conditions to obtain the corresponding primal variables w = A\u03b1 = \u2211 i,y\u2208Yi \u03b1i(y) \u03c8i(y) \u03bbn , see Appendix E. The gradient of f then takes the simple form \u2207f(\u03b1) = \u03bbATA\u03b1 \u2212 b = \u03bbATw \u2212 b; its (i,y)-th component is \u2212 1nHi(y;w), cf. (2). Finally, note that the domain M \u2282 Rm of (4) is the product of n probability simplices, M := \u2206|Y1| \u00d7 . . .\u00d7\u2206|Yn|."}, {"heading": "3. The Frank-Wolfe Algorithm", "text": "We consider the convex optimization problem min\u03b1\u2208M f(\u03b1), where the convex feasible set M is compact and the convex objective f is continuously differentiable. The Frank-Wolfe algorithm (1956) (shown in Algorithm 1) is an iterative optimization algorithm for such problems that only requires optimizing linear functions over M, and thus has wider applicability than projected gradient algorithms, which require optimizing a quadratic function overM. At every iteration, a feasible search corner s is first found by minimizing over M the linearization of f at the current iterate \u03b1 (see picture in inset).\n\u21b5\nf(\u21b5)\nM\nf\ns\ng(\u21b5) f(\u21b5\n) + \u2326 s 0 \u21b5,\nrf(\u21b5 ) \u21b5 The next iterate is then obtained as a convex combination of s and the previous iterate, with step-size \u03b3. These simple updates yield two interesting properties. First, every iterate \u03b1(k) can be written as a convex combination of the starting point \u03b1(0) and the search corners s found previously. The parameter \u03b1(k) thus has a sparse representation, which makes the algorithm suitable even for cases where the dimensionality of \u03b1 is exponential. Second, since f is convex, the minimum of the linearization of f over M immediately gives a lower bound on the value of the yet unknown optimal solution f(\u03b1\u2217). Every step of the algorithm thus computes for free the following \u2018linearization duality gap\u2019 defined for any feasible point \u03b1 \u2208 M (which is in fact a special case of the Fenchel duality gap as\nAlgorithm 1 Frank-Wolfe on a Compact Domain\nLet \u03b1(0) \u2208M for k = 0 . . .K do\nCompute s := argmin s\u2032\u2208M\n\u2329 s\u2032,\u2207f(\u03b1(k)) \u232a\nLet \u03b3 := 2k+2 , or optimize \u03b3 by line-search Update \u03b1(k+1) := (1\u2212 \u03b3)\u03b1(k) + \u03b3s\nexplained in Appendix D):\ng(\u03b1) := max s\u2032\u2208M\n\u3008\u03b1\u2212 s\u2032,\u2207f(\u03b1)\u3009 = \u3008\u03b1\u2212 s,\u2207f(\u03b1)\u3009. (5)\nAs g(\u03b1) \u2265 f(\u03b1) \u2212 f(\u03b1\u2217) by the above argument, s thus readily gives at each iteration the current duality gap as a certificate for the current approximation quality (Jaggi, 2011; 2013), allowing us to monitor the convergence, and more importantly to choose the theoretically sound stopping criterion g(\u03b1(k)) \u2264 \u03b5. In terms of convergence, it is known that after O(1/\u03b5) iterations, Algorithm 1 obtains an \u03b5-approximate solution (Frank & Wolfe, 1956; Dunn & Harshbarger, 1978) as well as a guaranteed \u03b5-small duality gap (Clarkson, 2010; Jaggi, 2013), along with a certificate to (5). For the convergence results to hold, the internal linear subproblem does not need to be solved exactly, but only to some error. We review and generalize the convergence proof in Appendix C. The constant hidden in the O(1/\u03b5) notation is the curvature constant Cf , an affine invariant quantity measuring the maximum deviation of f from its linear approximation over M (it yields a weaker form of Lipschitz assumption on the gradient, see e.g. Appendix A for a formal definition)."}, {"heading": "4. Frank-Wolfe for Structural SVMs", "text": "Note that classical algorithms like the projected gradient method cannot be tractably applied to the dual of the structural SVM problem (4), due to the large number of dual variables. In this section, we explain how the Frank-Wolfe method (Algorithm 1) can be efficiently applied to this dual problem, and discuss its relationship to other algorithms. The main insight here is to notice that the linear subproblem employed by Frank-Wolfe is actually directly equivalent to the loss-augmented decoding subproblem (2) for each datapoint, which can be solved efficiently (see Appendix B.1 for details). Recall that the optimization domain for the dual variables \u03b1 is the product\nAlgorithm 2 Batch Primal-Dual Frank-Wolfe Algorithm for the Structural SVM\nLet w(0) := 0, `(0) := 0 for k = 0 . . .K do\nfor i = 1 . . . n do Solve y\u2217i := argmax\ny\u2208Yi Hi(y;w\n(k)) cf. (2)\nLet ws :=\nn\u2211\ni=1\n1 \u03bbn\u03c8i(y \u2217 i ) and `s := 1 n n\u2211 i=1 Li(y \u2217 i )\nLet \u03b3 := \u03bb(w (k)\u2212ws)Tw(k)\u2212`(k)+`s\n\u03bb\u2016w(k)\u2212ws\u20162 and clip to [0, 1]\nUpdate w(k+1) := (1\u2212 \u03b3)w(k) + \u03b3ws and `(k+1) := (1\u2212 \u03b3)`(k) + \u03b3 `s\nof n simplices, M = \u2206|Y1| \u00d7 . . . \u00d7 \u2206|Yn|. Since each simplex consists of a potentially exponential number |Yi| of dual variables, we cannot maintain a dense vector \u03b1 during the algorithm. However, as mentioned in Section 3, each iterate \u03b1(k) of the Frank-Wolfe algorithm is a sparse convex combination of the previously visited corners s and the starting point \u03b1(0), and so we only need to maintain the list of previously seen solutions to the loss-augmented decoding subproblems to keep track of the non-zero coordinates of \u03b1, avoiding the problem of its exponential size. Alternately, if we do not use kernels, we can avoid the quadratic explosion of the number of operations needed in the dual by not explicitly maintaining \u03b1(k), but instead maintaining the corresponding primal variable w(k)."}, {"heading": "A Primal-Dual Frank-Wolfe Algorithm for the", "text": "Structural SVM Dual. Applying Algorithm 1 with line search to the dual of the structural SVM (4), but only maintaining the corresponding primal primal iterates w(k) := A\u03b1(k), we obtain Algorithm 2. Note that the Frank-Wolfe search corner s = (ey \u2217 1 , . . . , ey \u2217 n), which is obtained by solving the loss-augmented subproblems, yields the update ws = As. We use the natural starting point \u03b1(0) := (ey1 , . . . , eyn) which yields w(0) = 0 as \u03c8i(yi) = 0 \u2200i.\nThe Duality Gap. The duality gap (5) for our structural SVM dual formulation (4) is given by\ng(\u03b1) := max s\u2032\u2208M \u3008\u03b1\u2212 s\u2032,\u2207f(\u03b1)\u3009 = (\u03b1\u2212 s)T (\u03bbATA\u03b1\u2212 b) = \u03bb(w \u2212As)Tw \u2212 bT\u03b1+ bTs ,\nwhere s is an exact minimizer of the linearized problem given at the point \u03b1. This (Fenchel) duality gap turns out to be the same as the Lagrangian duality gap here (see Appendix B.2), and gives a direct handle on the suboptimality of w(k) for the primal problem (3). Using ws := As and `s := b\nTs, we observe that the gap is efficient to compute given the primal variables w := A\u03b1 and ` := bT\u03b1, which are maintained during the run of Algorithm 2. Therefore, we can use the duality gap g(\u03b1(k)) \u2264 \u03b5 as a proper stopping criterion.\nImplementing the Line-Search. Because the objective of the structural SVM dual (4) is simply a quadratic function in \u03b1, the optimal stepsize for any given candidate search point s \u2208 M can be obtained analytically. Namely, \u03b3LS := argmin\u03b3\u2208[0,1] f ( \u03b1+ \u03b3 ( s\u2212\u03b1 )) is obtained by setting the derivative of this univariate quadratic function in \u03b3 to zero, which here (before restricting to [0, 1]) gives \u03b3opt := \u3008\u03b1\u2212s,\u2207f(\u03b1)\u3009 \u03bb\u2016A(\u03b1\u2212s)\u20162 = g(\u03b1) \u03bb\u2016w\u2212ws\u20162 (used in Algorithms 2 and 4).\nConvergence Proof and Running Time. In the following, we write R for the maximal length of a difference feature vector, i.e. R :=maxi\u2208[n],y\u2208Yi\u2016\u03c8i(y)\u20162, and we write the maximum error as Lmax := maxi,y Li(y). By bounding the curvature constant Cf for the dual SVM objective (4), we can now directly apply the known convergence results for the standard Frank-Wolfe algorithm to obtain the following primaldual rate (proof in Appendix B.3):\nTheorem 1. Algorithm 2 obtains an \u03b5-approximate solution to the structural SVM dual problem (4) and duality gap g(\u03b1(k)) \u2264 \u03b5 after at most O ( R2\n\u03bb\u03b5\n) itera-\ntions, where each iteration costs n oracle calls.\nSince we have proved that the duality gap is smaller than \u03b5, this implies that the original SVM primal objective (3) is actually solved to accuracy \u03b5 as well."}, {"heading": "Relationship with the Batch Subgradient", "text": "Method in the Primal. Surprisingly, the batch Frank-Wolfe method (Algorithm 2) is equivalent to the batch subgradient method in the primal, though Frank-Wolfe allows a more clever choice of step-size, since line-search can be used in the dual. To see the equivalence, notice that a subgradient of (3) is given by dsub = \u03bbw \u2212 1n \u2211 i\u03c8i(y \u2217 i ) = \u03bb(w \u2212 ws), where y\u2217i and ws are as defined in Algorithm 2. Hence, for a step-size of \u03b2, the subgradient method update becomes w(k+1) := w(k) \u2212 \u03b2dsub = w(k) \u2212 \u03b2\u03bb(w(k) \u2212 ws) = (1 \u2212 \u03b2\u03bb)w(k) + \u03b2\u03bbws. Comparing this with Algorithm 2, we see that each Frank-Wolfe step on the dual problem (4) with step-size \u03b3 is equivalent to a batch subgradient step in the primal with a step-size of \u03b2 = \u03b3/\u03bb, and thus our convergence results also apply to it. This seems to generalize the equivalence between Frank-Wolfe and the subgradient method for a quadratic objective with identity Hessian as observed by Bach et al. (2012, Section 4.1)."}, {"heading": "Relationship with Cutting Plane Algorithms.", "text": "In each iteration, the cutting plane algorithm of Joachims et al. (2009) and the Frank-Wolfe method (Algorithm 2) solve the loss-augmented decoding problem for each datapoint, selecting the same new \u2018active\u2019 coordinates to add to the dual problem. The only difference is that instead of just moving towards the corner s, as in classical Frank-Wolfe, the cutting plane algorithm re-optimizes over all the previously added \u2018active\u2019 dual variables (this task is a quadratic program). This shows that the method is exactly equivalent to the \u2018fully corrective\u2019 variant of Frank-Wolfe, which in each iteration re-optimizes over all previously visited corners (Clarkson, 2010; Shalev-Shwartz et al., 2010b). Note that the convergence results for the \u2018fully correc-\nAlgorithm 3 Block-Coordinate Frank-Wolfe Algorithm on Product Domain\nLet \u03b1(0) \u2208M =M(1) \u00d7 . . .\u00d7M(n) for k = 0 . . .K do\nPick i at random in {1, . . . , n} Find s(i) := argmin\ns\u2032 (i) \u2208M(i)\n\u2329 s\u2032(i),\u2207(i)f(\u03b1(k)) \u232a\nLet \u03b3 := 2nk+2n , or optimize \u03b3 by line-search Update \u03b1 (k+1) (i) := \u03b1 (k) (i) + \u03b3 ( s(i) \u2212\u03b1(k)(i) )\ntive\u2019 variant directly follow from the ones for FrankWolfe (by inclusion), thus our convergence results apply to the cutting plane algorithm of Joachims et al. (2009), significantly simplifying its analysis."}, {"heading": "5. Faster Block-Coordinate Frank-Wolfe", "text": "A major disadvantage of the standard Frank-Wolfe algorithm when applied to the structural SVM problem is that each iteration requires a full pass through the data, resulting in n calls to the maximization oracle. In this section, we present the main new contribution of the paper: a block-coordinate generalization of the Frank-Wolfe algorithm that maintains all appealing properties of Frank-Wolfe, but yields much cheaper iterations, requiring only one call to the maximization oracle in the context of structural SVMs. The new method is given in Algorithm 3, and applies to any constrained convex optimization problem of the form\nmin \u03b1\u2208M(1)\u00d7...\u00d7M(n) f(\u03b1) , (6)\nwhere the domain has the structure of a Cartesian product M = M(1) \u00d7 . . . \u00d7M(n) \u2286 Rm over n \u2265 1 blocks. The main idea of the method is to perform cheaper update steps that only affect a single variable block M(i), and not all of them simultaneously. This is motivated by coordinate descent methods, which have a very successful history when applied to large scale optimization. Here we assume that each factor M(i) \u2286 Rmi is convex and compact, with m = \u2211n i=1mi. We will write \u03b1(i) \u2208 Rmi for the i-th block of coordinates of a vector \u03b1 \u2208 Rm. In each step, Algorithm 3 picks one of the n blocks uniformly at random, and leaves all other blocks unchanged. If there is only one block (n = 1), then Algorithm 3 becomes the standard Frank-Wolfe Algorithm 1. The algorithm can be interpreted as a simplification of Nesterov\u2019s \u2018hugescale\u2019 uniform coordinate descent method (Nesterov, 2012, Section 4). Here, instead of computing a projection operator on a block (which is intractable for structural SVMs), we only need to solve one linear subproblem in each iteration, which for structural SVMs is equivalent to a call to the maximization oracle.\nAlgorithm 4 Block-Coordinate Primal-Dual FrankWolfe Algorithm for the Structural SVM\nLet w(0) := wi (0) := w\u0304(0) := 0, `(0) := `i (0) := 0 for k = 0 . . .K do\nPick i at random in {1, . . . , n} Solve y\u2217i := argmax\ny\u2208Yi Hi(y;w\n(k)) cf. (2)\nLet ws := 1 \u03bbn\u03c8i(y \u2217 i ) and `s := 1 n Li(y \u2217 i ) Let \u03b3 := \u03bb(w (k) i \u2212ws)\nTw(k)\u2212`(k)i +`s \u03bb\u2016w(k)i \u2212ws\u20162 and clip to [0, 1]\nUpdate wi (k+1) := (1\u2212 \u03b3)wi(k) + \u03b3ws\nand `i (k+1) := (1\u2212 \u03b3)`i(k) + \u03b3 `s\nUpdate w(k+1) := w(k) +wi (k+1) \u2212wi(k)\nand `(k+1) := `(k) + `i (k+1) \u2212 `i(k)\n(Optionally: Update w\u0304(k+1) := k k+2 w\u0304(k) + 2 k+2 w(k+1))\nConvergence Results. The following main theorem shows that after O(1/\u03b5) many iterations, Algorithm 3 obtains an \u03b5-approximate solution to (6), and guaranteed \u03b5-small duality gap (proof in Appendix C). Here the constant C\u2297f := \u2211n i=1 C (i) f is the sum of the (partial) curvature constants of f with respect to the individual domain block M(i). We discuss this Lipschitz assumption on the gradient in more details in Appendix A, where we compute the constant precisely for the structural SVM and obtain C\u2297f = Cf/n, where Cf is the classical Frank-Wolfe curvature. Theorem 2. For each k \u2265 0, the iterate \u03b1(k) of Algorithm 3 (either using the predefined step-sizes, or using line-search) satisfies E [ f(\u03b1(k)) ] \u2212 f(\u03b1\u2217) \u2264\n2n k+2n ( C\u2297f + h0 ) , where \u03b1\u2217 \u2208M is a solution to problem (6), h0 := f(\u03b1 (0)) \u2212 f(\u03b1\u2217) is the initial error at the starting point of the algorithm, and the expectation is over the random choice of the block i in the steps of the algorithm.\nFurthermore, if Algorithm 3 is run for K \u2265 0 iterations, then it has an iterate \u03b1(k\u0302), 0 \u2264 k\u0302 \u2264 K, with duality gap bounded by E [ g(\u03b1(k\u0302)) ] \u2264 6nK+1 ( C\u2297f + h0) .\nApplication to the Structural SVM. Algorithm 4 applies the block-coordinate Frank-Wolfe algorithm with line-search to the structural SVM dual problem (4), maintaining only the primal variables w. We see that Algorithm 4 is equivalent to Algorithm 3, by observing that the corresponding primal updates become ws = As[i] and `s = b Ts[i]. Here s[i] is the zero-padding of s(i) := e y\u2217i \u2208 M(i) so that s[i] \u2208 M. Note that Algorithm 4 has a primal parameter vector wi (= A\u03b1[i]) for each datapoint i, but that this does not significantly increase the storage cost of the algorithm since each wi has a sparsity pattern that is the union of the corresponding \u03c8i(y \u2217 i ) vectors. If the feature vectors are not sparse, it might be more efficient\nto work directly in the dual instead (see the kernelized version below). The line-search is analogous to the batch Frank-Wolfe case discussed above, and formalized in Appendix B.4.\nBy applying Theorem 2 to the SVM case where C\u2297f = Cf/n = 4R 2/\u03bbn (in the worst case), we get that the number of iterations needed for our new block-wise Algorithm 4 to obtain a specific accuracy \u03b5 is the same as for the batch version in Algorithm 2 (under the assumption that the initial error h0 is smaller than 4R2/\u03bbn), even though each iteration takes n times fewer oracle calls. If h0 > 4R\n2/\u03bbn, we can use the fact that Algorithm 4 is using line-search to get a weaker dependence on h0 in the rate (Theorem C.4). We summarize the overall rate as follows (proof in Appendix B.3): Theorem 3. If Lmax \u2264 4R 2 \u03bbn (so h0 \u2264 4R 2\n\u03bbn ), then Algorithm 4 obtains an \u03b5-approximate solution to the structural SVM dual problem (4) and expected duality gap E[g(\u03b1(k))] \u2264 \u03b5 after at most O ( R2\n\u03bb\u03b5\n) iterations, where\neach iteration costs a single oracle call.\nIf Lmax > 4R2\n\u03bbn , then it requires at most an additional\n(constant in \u03b5) number of O ( n log ( \u03bbnLmax R2 )) steps to get the same error and duality gap guarantees.\nIn terms of \u03b5, the O(1/\u03b5) convergence rate above is similar to existing stochastic subgradient and cuttingplane methods. However, unlike stochastic subgradient methods, the block-coordinate Frank-Wolfe method allows us to compute the optimal step-size at each iteration (while for an additional pass through the data we can evaluate the duality gap (5) to allow us to decide when to terminate the algorithm in practice). Further, unlike cutting-plane methods which require n oracle calls per iteration, this rate is achieved \u2018online\u2019, using only a single oracle call per iteration.\nApproximate Subproblems and Decoding. Interestingly, we can show that all the convergence results presented in this paper also hold if only approximate minimizers of the linear subproblems are used instead of exact minimizers. If we are using an approximate oracle giving candidate directions s(i) in Algorithm 3 (or s in Algorithm 1) with a multiplicative accuracy \u03bd \u2208 (0, 1] (with respect to the the duality gap (5) on the current block), then the above convergence bounds from Theorem 2 still apply. The only change is that the convergence is slowed by a factor of 1/\u03bd2. We prove this generalization in the Theorems of Appendix C. For structural SVMs, this significantly improves the applicability to large-scale problems, where exact decoding is often too costly but approximate loss-augmented decoding may be possible.\nKernelized Algorithms. Both Algorithms 2 and 4 can directly be used with kernels by maintaining the sparse dual variables \u03b1(k) instead of the primal variables w(k). In this case, the classifier is only given implicitly as a sparse combination of the corresponding kernel functions, i.e. w = A\u03b1. Using our Algorithm 4, we obtain the currently best known bound on the number of support vectors, i.e. a guaranteed \u03b5-approximation with only O(R 2\n\u03bb\u03b5 ) support vectors. In comparison, the standard cutting plane method (Joachims et al., 2009) adds n support vectors \u03c8i(y \u2217 i ) at each iteration. More details on the kernelized variant of Algorithm 4 are discussed in Appendix B.5."}, {"heading": "6. Experiments", "text": "We compare our novel Frank-Wolfe approach to existing algorithms for training structural SVMs on the OCR dataset (n = 6251, d = 4028) from Taskar et al. (2003) and the CoNLL dataset (n = 8936, d = 1643026) from Sang & Buchholz (2000). Both datasets are sequence labeling tasks, where the loss-augmented decoding problem can be solved exactly by the Viterbi algorithm. Our third application is a word alignment problem between sentences in different languages in the setting of Taskar et al. (2006) (n = 5000, d = 82). Here, the structured labels are bipartite matchings, for which computing marginals over labels as required by the methods of Collins et al. (2008); Zhang et al. (2011) is intractable, but loss-augmented decoding can be done efficiently by solving a min-cost flow problem.\nWe compare Algorithms 2 and 4, the batch FrankWolfe method (FW )1 and our novel block-coordinate Frank-Wolfe method (BCFW ), to the cutting plane algorithm implemented in SVMstruct (Joachims et al., 2009) with its default options, the online exponentiated gradient (online-EG) method of Collins et al. (2008), and the stochastic subgradient method (SSG) with step-size chosen as in the \u2018Pegasos\u2019 version of Shalev-Shwartz et al. (2010a). We also include the weighted average w\u0304(k) := 2k(k+1) \u2211k t=1 tw (t) of the iterates from SSG (called SSG-wavg) which was recently shown to converge at the faster rate of O(1/k) instead of O ((log k)/k) (Lacoste-Julien et al., 2012; Shamir & Zhang, 2013). Analogously, we average the iterates from BCFW the same way to obtain the BCFW-wavg method (implemented efficiently with the optional line in Algorithm 4), which also has a provable O(1/k) convergence rate (Theorem C.3). The performance of the different algorithms according to several criteria is visualized in Figure 1. The results are discussed\n1This is equivalent to the batch subgradient method with an adaptive step-size, as mentioned in Section 4.\nin the caption, while additional experiments can be found in Appendix F. In most of the experiments, the BCFW-wavg method dominates all competitors. The superiority is especially striking for the first few iterations, and when using a small regularization strength \u03bb, which is often needed in practice. In term of test error, a peculiar observation is that the weighted average of the iterates seems to help both methods significantly: SSG-wavg sometimes slightly outperforms BCFW-wavg despite having the worst objective value amongst all methods. This phenomenon is worth further investigation."}, {"heading": "7. Related Work", "text": "There has been substantial work on dual coordinate descent for SVMs, including the original sequential minimal optimization (SMO) algorithm. The SMO algorithm was generalized to structural SVMs (Taskar, 2004, Chapter 6), but its convergence rate scales badly with the size of the output space: it was estimated as O (n|Y|/\u03bb\u03b5) in Zhang et al. (2011). Further, this method requires an expectation oracle to work with\nits factored dual parameterization. As in our algorithm, Rousu et al. (2006) propose updating one training example at a time, but using multiple Frank-Wolfe updates to optimize along the subspace. However, they do not obtain any rate guarantees and their algorithm is less general because it again requires an expectation oracle. In the degenerate binary SVM case, our block-coordinate Frank-Wolfe algorithm is actually equivalent to the method of Hsieh et al. (2008), where because each datapoint has a unique dual variable, exact coordinate optimization can be accomplished by the line-search step of our algorithm. Hsieh et al. (2008) show a local linear convergence rate in the dual, and our results complement theirs by providing a global primal convergence guarantee for their algorithm of O (1/\u03b5). After our paper had appeared on arXiv, Shalev-Shwartz & Zhang (2012) have proposed a generalization of dual coordinate descent applicable to several regularized losses, including the structural SVM objective. Despite being motivated from a different perspective, a version of their algorithm (Option II of Figure 1) gives the exact same step-size and update direction as BCFW with line-search, and their Corol-\nlary 3 gives a similar convergence rate as our Theorem 3. Balamurugan et al. (2011) propose to approximately solve a quadratic problem on each example using SMO, but they do not provide any rate guarantees. The online-EG method implements a variant of dual coordinate descent, but it requires an expectation oracle and Collins et al. (2008) estimate its primal convergence at only O ( 1/\u03b52 ) .\nBesides coordinate descent methods, a variety of other algorithms have been proposed for structural SVMs. We summarize a few of the most popular in Table 1, with their convergence rates quoted in number of oracle calls to reach an accuracy of \u03b5. However, we note that almost no guarantees are given for the optimization of structural SVMs with approximate oracles. A regret analysis in the context of online optimization was considered by Ratliff et al. (2007), but they do not analyze the effect of this on solving the optimization problem. The cutting plane algorithm of Tsochantaridis et al. (2005) was considered with approximate maximization by Finley & Joachims (2008), though the dependence of the running time on the the approximation error was left unclear. In contrast, we provide guarantees for batch subgradient, cutting plane, and block-coordinate Frank-Wolfe, for achieving an \u03b5approximate solution as long as the error of the oracle is appropriately bounded."}, {"heading": "8. Discussion", "text": "This work proposes a novel randomized blockcoordinate generalization of the classic Frank-Wolfe algorithm for optimization with block-separable constraints. Despite its potentially much lower iteration cost, the new algorithm achieves a similar convergence\nrate in the duality gap as the full Frank-Wolfe method. For the dual structural SVM optimization problem, it leads to a simple online algorithm that yields a solution to an issue that is notoriously difficult to address for stochastic algorithms: no step-size sequence needs to be tuned since the optimal step-size can be efficiently computed in closed-form. Further, at the cost of an additional pass through the data (which could be done alongside a full Frank-Wolfe iteration), it allows us to compute a duality gap guarantee that can be used to decide when to terminate the algorithm. Our experiments indicate that empirically it converges faster than other stochastic algorithms for the structural SVM problem, especially in the realistic setting where only a few passes through the data are possible.\nAlthough our structural SVM experiments use an exact maximization oracle, the duality gap guarantees, the optimal step-size, and a computable bound on the duality gap are all still available when only an appropriate approximate maximization oracle is used. Finally, although the structural SVM problem is what motivated this work, we expect that the blockcoordinate Frank-Wolfe algorithm may be useful for other problems in machine learning where a complex objective with block-separable constraints arises.\nAcknowledgements. We thank Francis Bach, Bernd Ga\u0308rtner and Ronny Luss for helpful discussions, and Robert Carnecky for the 3D illustration. MJ is supported by the ERC Project SIPA, and by the Swiss National Science Foundation. SLJ and MS are partly supported by the ERC (SIERRA-ERC-239993). SLJ is supported by a Research in Paris fellowship. MS is supported by a NSERC postdoctoral fellowship."}, {"heading": "Bach, F., Lacoste-Julien, S., and Obozinski, G. On the", "text": "equivalence between herding and conditional gradient algorithms. In ICML, 2012."}, {"heading": "Balamurugan, P., Shevade, S., Sundararajan, S., and", "text": "Keerthi, S. A sequential dual method for structural SVMs. In SDM, 2011."}, {"heading": "Caetano, T.S., McAuley, J.J., Cheng, Li, Le, Q.V., and", "text": "Smola, A.J. Learning graph matching. IEEE PAMI, 31 (6):1048\u20131058, 2009.\nClarkson, K. Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm. ACM Transactions on Algorithms, 6(4):1\u201330, 2010."}, {"heading": "Collins, M., Globerson, A., Koo, T., Carreras, X., and", "text": "Bartlett, P. L. Exponentiated gradient algorithms for conditional random fields and max-margin Markov networks. JMLR, 9:1775\u20131822, 2008.\nDunn, J.C. and Harshbarger, S. Conditional gradient algorithms with open loop step size rules. Journal of Mathematical Analysis and Applications, 62(2):432\u2013444, 1978."}, {"heading": "Finley, T. and Joachims, T. Training structural SVMs", "text": "when exact inference is intractable. In ICML, 2008.\nFrank, M. and Wolfe, P. An algorithm for quadratic programming. Naval Research Logistics Quarterly, 3:95\u2013 110, 1956.\nGa\u0308rtner, B. and Jaggi, M. Coresets for polytope distance. ACM Symposium on Computational Geometry, 2009.\nHsieh, C., Chang, K., Lin, C., Keerthi, S., and Sundararajan, S. A dual coordinate descent method for large-scale linear SVM. In ICML, pp. 408\u2013415, 2008.\nJaggi, M. Sparse convex optimization methods for machine learning. PhD thesis, ETH Zu\u0308rich, 2011."}, {"heading": "Jaggi, M. Revisiting Frank-Wolfe: Projection-free sparse", "text": "convex optimization. In ICML, 2013."}, {"heading": "Joachims, T., Finley, T., and Yu, C. Cutting-plane training", "text": "of structural SVMs. Machine Learn., 77(1):27\u201359, 2009."}, {"heading": "Lacoste-Julien, S., Schmidt, M., and Bach, F. A simpler", "text": "approach to obtaining an O(1/t) convergence rate for the projected stochastic subgradient method. Technical Report 1212.2002v2 [cs.LG], arXiv, December 2012.\nMangasarian, O.L. Machine learning via polyhedral concave minimization. Technical Report 95-20, University of Wisconsin, 1995.\nNesterov, Yurii. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341\u2013362, 2012.\nOuyang, H. and Gray, A. Fast stochastic Frank-Wolfe algorithms for nonlinear SVMs. SDM, 2010.\nRakhlin, A., Shamir, O., and Sridharan, K. Making gradient descent optimal for strongly convex stochastic optimization. In ICML, 2012."}, {"heading": "Ratliff, N., Bagnell, J. A., and Zinkevich, M. (Online)", "text": "subgradient methods for structured prediction. In AISTATS, 2007."}, {"heading": "Rousu, J., Saunders, C., Szedmak, S., and Shawe-Taylor,", "text": "J. Kernel-based learning of hierarchical multilabel classification models. JMLR, 2006."}, {"heading": "Sang, E.F.T.K. and Buchholz, S. Introduction to the", "text": "CoNLL-2000 shared task: Chunking, 2000."}, {"heading": "Shalev-Shwartz, S. and Zhang, T. Proximal stochastic", "text": "dual coordinate ascent. Technical Report 1211.2717v1 [stat.ML], arXiv, November 2012."}, {"heading": "Shalev-Shwartz, S., Singer, Y., Srebro, N., and Cotter, A.", "text": "Pegasos: primal estimated sub-gradient solver for SVM. Mathematical Programming, 127(1), 2010a.\nShalev-Shwartz, S., Srebro, N., and Zhang, T. Trading accuracy for sparsity in optimization problems with sparsity constraints. SIAM Journal on Optimization, 20: 2807\u20132832, 2010b.\nShamir, O. and Zhang, T. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In ICML, 2013.\nTaskar, B. Learning structured prediction models: A large margin approach. PhD thesis, Stanford, 2004."}, {"heading": "Taskar, B., Guestrin, C., and Koller, D. Max-margin", "text": "Markov networks. In NIPS, 2003.\nTaskar, B., Lacoste-Julien, S., and Jordan, M. I. Structured prediction, dual extragradient and Bregman projections. JMLR, 7:1627\u20131653, 2006."}, {"heading": "Teo, C.H., Vishwanathan, S.V.N., Smola, A.J., and Le,", "text": "Q.V. Bundle methods for regularized risk minimization. JMLR, 11:311\u2013365, 2010.\nTsochantaridis, I., Joachims, T., Hofmann, T., and Altun, Y. Large margin methods for structured and interdependent output variables. JMLR, 6:1453\u20131484, 2005.\nZhang, X., Saha, A., and Vishwanathan, S. V. N. Accelerated training of max-margin Markov networks with kernels. In ALT, pp. 292\u2013307. Springer, 2011.\nSupplementary Material Block-Coordinate Frank-Wolfe Optimization for Structural SVMs\nOutline. In Appendix A, we discuss the curvature constants and compute them for the structural SVM problem. In Appendix B, we give additional details on applying the Frank-Wolfe algorithms to the structural SVM and provide proofs for Theorems 1 and 3. In the main Appendix C, we give a self-contained presentation and analysis of the new block-coordinate Frank-Wolfe method (Algorithm 3), and prove the main convergence Theorem 2. In Appendix D, the \u2018linearization\u2019-duality gap is interpreted in terms of Fenchel duality. For completeness, we include a short derivation of the dual problem to the structural SVM in Appendix E. Finally, we present in Appendix F additional experimental results as well as more detailed information about the implementation.\nA. The Curvature Constants Cf and C \u2297 f\nThe Curvature Constant Cf . The curvature constant Cf is given by the maximum relative deviation of the objective function f from its linear approximations, over the domainM (Clarkson, 2010; Jaggi, 2013). Formally,\nCf := sup x,s\u2208M, \u03b3\u2208[0,1],\ny=x+\u03b3(s\u2212x)\n2\n\u03b32 (f(y)\u2212 f(x)\u2212 \u3008y \u2212 x,\u2207f(x)\u3009) . (7)\nThe assumption of bounded Cf corresponds to a slightly weaker, affine invariant form of a smoothness assumption on f . It is known that Cf is upper bounded by the Lipschitz constant of the gradient \u2207f times the squared diameter of M, for any arbitrary choice of a norm (Jaggi, 2013, Lemma 8); but it can also be much smaller (in particular, when the dimension of the affine hull of M is smaller than the ambient space), so it is a more fundamental quantity in the analysis of the Frank-Wolfe algorithm than the Lipschitz constant of the gradient. As pointed out by Jaggi (2013, Section 2.4), Cf is invariant under affine transformations, as is the Frank-Wolfe algorithm.\nThe Product Curvature Constant C\u2297f . The curvature concept can be generalized to our setting of product domains M :=M(1) \u00d7 . . .\u00d7M(n) as follows: over each individual coordinate block, the curvature is given by\nC (i) f := sup\nx\u2208M, s(i)\u2208M(i), \u03b3\u2208[0,1], y=x+\u03b3(s[i]\u2212x[i])\n2 \u03b32 ( f(y)\u2212 f(x)\u2212 \u3008y(i) \u2212 x(i),\u2207(i)f(x)\u3009 ) , (8)\nwhere the notation x[i] refers to the zero-padding of x(i) so that x[i] \u2208M. By considering the Taylor expansion of f , it is not hard to see that also the \u2018partial\u2019 curvature C\n(i) f is upper bounded by the Lipschitz constant of the\npartial gradient \u2207(i)f times the squared diameter of just one domain block M(i). See also the proof of Lemma A.2 below.\nWe define the global product curvature constant as the sum of these curvatures for each block, i.e.\nC\u2297f :=\nn\u2211\ni=1\nC (i) f (9)\nObserve that for the classical Frank-Wolfe case when n = 1, we recover the original curvature constant."}, {"heading": "Computing the Curvature Constant Cf in the SVM Case.", "text": "Lemma A.1. For the dual structural SVM objective function (4) over the domain M := \u2206|Y1| \u00d7 . . . \u00d7 \u2206|Yn|, the curvature constant Cf , as defined in (7), is upper bounded by\nCf \u2264 4R2\n\u03bb ,\nwhere R is the maximal length of a difference feature vector, i.e. R := max i\u2208[n],y\u2208Yi\n\u2016\u03c8i(y)\u20162 .\nProof of Lemma A.1. If the objective function is twice differentiable, we can plug-in the second degree Taylor expansion of f into the above definition (7) of the curvature, see e.g. (Jaggi, 2011, Inequality (2.12)) or (Clarkson, 2010, Section 4.1). In our case, the gradient at \u03b1 is given by \u03bbATA\u03b1 \u2212 b, so that the Hessian is \u03bbATA, being a constant matrix independent of \u03b1. This gives the following upper bound2 on Cf , which we can separate into two identical matrix-vector products with our matrix A:\nCf \u2264 sup x,y\u2208M,\nz\u2208[x,y]\u2286M\n(y \u2212 x)T\u22072f(z)(y \u2212 x)\n= \u03bb \u00b7 sup x,y\u2208M (A(y \u2212 x))TA(y \u2212 x)\n= \u03bb \u00b7 sup v,w\u2208AM \u2016v \u2212w\u201622 \u2264 \u03bb \u00b7 sup v\u2208AM \u20162v\u201622\nBy definition of our compact domain M, we have that each vector v \u2208 AM is precisely the sum of n vectors, each of these being a convex combination of the feature vectors for the possible labelings for datapoint i.\nTherefore, the norm \u2016v\u20162 is upper bounded by n times the longest column of the matrix A, or more formally \u2016v\u20162 \u2264 n 1\u03bbnR with R being the longest3 feature vector, i.e.\nR := max i\u2208[n],y\u2208Yi\n\u2016\u03c8i(y)\u20162 .\nAltogether, we have obtained that the curvature Cf is upper bounded by 4R2\n\u03bb .\nWe also note that in the worst case, this bound is tight. For example, we can make Cf = 4R2\n\u03bb by having for each datapoint i, two labelings which give opposite difference feature vectors \u03c8i of the same maximal norm R."}, {"heading": "Computing the Product Curvature Constant C\u2297f in the SVM Case.", "text": "Lemma A.2. For the dual structural SVM objective function (4) over the domain M := \u2206|Y1| \u00d7 . . . \u00d7 \u2206|Yn|, the total curvature constant C\u2297f on the product domain M, as defined in (9), is upper bounded by\nC\u2297f \u2264 4R2\n\u03bbn\nwhere R is the maximal length of a difference feature vector, i.e. R := max i\u2208[n],y\u2208Yi\n\u2016\u03c8i(y)\u20162 .\nProof. We follow the same lines as in the above proof of Lemma A.1, but now applying the same bound to the block-wise definition (8) of the curvature on the i-th block. Here, the change from x to y is now restricted to only affect the coordinates in the i-th block M(i). To simplify the notation, let M[i] be M(i) augmented with the zero domain for all the other blocks \u2013 i.e. the analog of x(i) \u2208 M(i) is x[i] \u2208 M[i]. x(i) is the i-th block of x whereas x[i] \u2208 M is x(i) padded with zeros for all the other blocks. We thus require that y \u2212 x \u2208 M[i] for a\n2Because our function is a quadratic function, this is actually an equality. 3This choice of the radius R then gives 1\n\u03bbn R = maxi\u2208[n],y\u2208Yi \u2225\u2225 1 \u03bbn \u03c8i(y) \u2225\u2225 2 = maxi\u2208[n],y\u2208Yi \u2225\u2225A(i,y)\u2225\u2225.\nvalid change from x to y. Again by the degree-two Taylor expansion, we obtain\nC (i) f \u2264 sup\nx,y\u2208M, (y\u2212x)\u2208M[i] z\u2208[x,y]\u2286M\n(y \u2212 x)T\u22072f(z)(y \u2212 x)\n= \u03bb \u00b7 sup x,y\u2208M\n(y\u2212x)\u2208M[i]\n(A(y \u2212 x))TA(y \u2212 x)\n= \u03bb \u00b7 sup v,w\u2208AM(i) \u2016v \u2212w\u201622 \u2264 \u03bb \u00b7 sup v\u2208AM(i) \u20162v\u201622\nIn other words, by definition of our compact domain M(i) = \u2206|Yi|, we have that each vector v \u2208 AM(i) is a convex combination of the feature vectors corresponding to the possible labelings for datapoint i. Therefore, the norm \u2016v\u20162 is again upper bounded by the longest column of the matrix A, which means \u2016v\u20162 \u2264 1\u03bbnR with R := maxi\u2208[n],y\u2208Yi \u2016\u03c8i(y)\u20162. Summing up over the n blocks M(i), we obtain that the product curvature C\u2297f is upper bounded by 4R 2\n\u03bbn .\nFor the same argument as at the end of the proof for Lemma A.1, this bound is actually tight in the worst case."}, {"heading": "B. More Details on the Algorithms for Structural SVMs", "text": ""}, {"heading": "B.1. Equivalence of an Exact Frank-Wolfe Step and Loss-Augmented Decoding", "text": "To see that the proposed Algorithm 2 indeed exactly corresponds to the standard Frank-Wolfe Algorithm 1 applied to the SVM dual problem (4), we verify that the search direction s giving the update ws = As is in fact an exact Frank-Wolfe step, which can be seen as follows:\nLemma B.1. The sparse vector s \u2208 Rn constructed in the inner for-loop of Algorithm 2 is an exact solution to s = argmins\u2032\u2208M \u2329 s\u2032,\u2207f(\u03b1(k)) \u232a for optimization problem (4).\nProof. Over the product domainM = \u2206|Y1|\u00d7 . . .\u00d7\u2206|Yn|, the minimization mins\u2032\u2208M\u3008s\u2032,\u2207f(\u03b1)\u3009 decomposes as\u2211 i minsi\u2208\u2206|Yi|\u3008si,\u2207if(\u03b1)\u3009. The minimization of a linear function over the simplex reduces to a search over its corners \u2013 in this case, it amounts for each i to find the minimal component of \u2212Hi(y;w) over y \u2208 Yi, i.e. solving the loss-augmented decoding problem as used in Algorithm 2 to construct the domain vertex s. To see this, note that for our choice of primal variables w = A\u03b1, the gradient of the dual objective, \u2207f(\u03b1) = \u03bbATA\u03b1\u2212 b, writes as \u03bbATw\u2212 b. This vector is precisely the loss-augmented decoding function \u2212 1nHi(y;w), for i \u2208 [n], y \u2208 Yi, as defined in (2)."}, {"heading": "B.2. Relation between the Lagrange Duality Gap and the \u2018Linearization\u2019 Gap for the Structural SVM", "text": "We show here that the simple \u2018linearization\u2019 gap (5), evaluated on the structural SVM dual problem (4) is actually equivalent to the standard Lagrangian duality gap for the structural SVM primal objective (1) (these two duality gaps are not the same in general4). This is important for the duality gap convergence rate results of our Frank-Wolfe algorithms to be transferable as primal convergence rates on the original structural SVM objective (3), which is the one with statistical meaning (for example with generalization error bounds as given in Taskar et al. (2003)).\nProof. So consider the difference of our objective at w := A\u03b1 in the primal problem (3), and the dual objective\n4For example, the two gaps are different when evaluated on the dual of the conditional random field objective (see, for example, Collins et al. (2008) for the formulation), which does not have a Lipschitz continuous gradient.\nat \u03b1 in problem (4) (in the maximization version). This difference is\ngLagrange(w,\u03b1) = \u03bb\n2 wTw +\n1\nn\nn\u2211\ni=1\nH\u0303i(w)\u2212 ( bT\u03b1\u2212 \u03bb\n2 wTw\n)\n= \u03bbwTw \u2212 bT\u03b1+ 1 n\nn\u2211\ni=1\nmax y\u2208Yi Hi(y;w) .\nNow recall that by the definition of A and b, we have that 1nHi(y;w) = (b\u2212 \u03bbATw)(i,y) = (\u2212\u2207f(\u03b1))(i,y). By summing up over all points and re-using a similar argument as in Lemma B.1 above, we get that\n1\nn\nn\u2211\ni=1\nmax y\u2208Yi Hi(y;w) =\nn\u2211\ni=1\nmax y\u2208Yi (\u2212\u2207f(\u03b1))(i,y) = max s\u2032\u2208M \u3008s\u2032,\u2212\u2207f(\u03b1)\u3009 ,\ngLagrange(w,\u03b1) = (\u03bbw TA\u2212 bT )\u03b1+ 1\nn\nn\u2211\ni=1\nmax y\u2208Yi Hi(y;w)\n= \u3008\u2207f(\u03b1),\u03b1\u3009+ max s\u2032\u2208M \u3008\u2212s\u2032,\u2207f(\u03b1)\u3009 = \u3008\u03b1\u2212 s,\u2207f(\u03b1)\u3009 = g(\u03b1) ,\nas defined in (5)."}, {"heading": "B.3. Convergence Analysis", "text": ""}, {"heading": "B.3.1. Convergence of the Batch Frank-Wolfe Algorithm 2 on the Structural SVM Dual", "text": "Theorem\u2019 1. Algorithm 2 obtains an \u03b5-approximate solution to the structural SVM dual problem (4) and duality gap g(\u03b1(k)) \u2264 \u03b5 after at most O ( R2\n\u03bb\u03b5\n) iterations, where each iteration costs n oracle calls.\nProof. We apply the known convergence results for the standard Frank-Wolfe Algorithm 1, as given e.g. in (Frank & Wolfe, 1956; Dunn & Harshbarger, 1978; Jaggi, 2013), or as given in the paragraph just after the proof of Theorem C.1: For each k \u2265 1, the iterate \u03b1(k) of Algorithm 1 (either using the predefined step-sizes, or using line-search) satisfies E[f(\u03b1(k))]\u2212 f(\u03b1\u2217) \u2264 2Cfk+2 , where \u03b1\u2217 \u2208M is an optimal solution to problem (4).\nFurthermore, if Algorithm 1 is run for K \u2265 1 iterations, then it has an iterate \u03b1(k\u0302), 1 \u2264 k\u0302 \u2264 K, with duality gap bounded by E[g(\u03b1(k\u0302))] \u2264 6CfK+1 . This was shown e.g. in (Jaggi, 2013) with slightly different constants, or also in our analysis presented below (see the paragraph after the generalized analysis provided in Theorem C.3, when the number of blocks n is set to one).\nNow for the SVM problem and the equivalent Algorithm 2, the claim follows from the curvature bound Cf \u2264 4R 2 \u03bb for the dual structural SVM objective function (4) over the domain M := \u2206|Y1| \u00d7 . . . \u00d7\u2206|Yn|, as given in the above Lemma A.1."}, {"heading": "B.3.2. Convergence of the Block-Coordinate Frank-Wolfe Algorithm 4 on the Structural SVM Dual", "text": "Theorem\u2019 3. If Lmax \u2264 4R 2 \u03bbn (so h0 \u2264 4R 2 \u03bbn ), then Algorithm 4 obtains an \u03b5-approximate solution to the structural\nSVM dual problem (4) and expected duality gap E[g(\u03b1(k))] \u2264 \u03b5 after at most O ( R2\n\u03bb\u03b5\n) iterations, where each\niteration costs a single oracle call.\nIf Lmax > 4R2 \u03bbn , then it requires at most an additional (constant in \u03b5) number of O ( n log ( \u03bbnLmax R2 )) steps to get the same error and duality gap guarantees, whereas the predefined step-size variant will require an additional O ( nLmax \u03b5 ) steps.\nProof. Writing h0 = f(\u03b1 (0))\u2212 f(\u03b1\u2217) for the error at the starting point used by the algorithm, the convergence Theorem 2 states that if k \u2265 0 and k \u2265 2n\u03b5 (C\u2297f + h0), then the expected error is E[f(\u03b1(k))] \u2212 f(\u03b1\u2217) \u2264 \u03b5 and\nanalogously for the expected duality gap. The result then follows by plugging in the curvature bound C\u2297f \u2264 4R 2 \u03bbn for the dual structural SVM objective function (4) over the domain M := \u2206|Y1| \u00d7 . . . \u00d7 \u2206|Yn|, as detailed in Lemma A.2 (notice that it is n times smaller than the curvature Cf needed for the batch algorithm) and then bounding h0. To bound h0, we observe that by the choice of the starting point \u03b1 (0) using only the observed labels, the initial error is bounded as h0 \u2264 g(\u03b1(0)) = bTs = 1n \u2211n i=1 maxy\u2208Yi Li(y) \u2264 Lmax. Thus, if Lmax \u2264 4R 2 \u03bbn , then we have C\u2297f + h0 \u2264 8R 2 \u03bbn , which proves the first part of the theorem.\nIn the case Lmax > 4R2 \u03bbn , then the predefined step-size variant will require an additional 2nh0 \u03b5 \u2264 2nLmax\u03b5 steps as we couldn\u2019t use the fact that h0 \u2264 C\u2297f . For the line-search variant, on the other hand, we can use the improved convergence Theorem C.4, which shows that the algorithm require at most k0 \u2264 n log(h0/C\u2297f ) steps to reach the condition h0 \u2264 C\u2297f ; once this condition is satisfied, we can simply re-use Theorem 2 with k redefined as k\u2212k0 to get the final convergence rates. We also point out that the statement of Theorem C.4 stays valid by replacing C\u2297f with any C\u2297f \u2032 \u2265 C\u2297f in it. So plugging in C\u2297f \u2032 = R 2\n\u03bbn and the bound h0 \u2264 Lmax in the k0 quantity gives back the number of additional steps mentioned in the second part of the theorem statement an \u03b5-approximate solution. A similar argument can be made for the expected duality gap by using the improved convergence Theorem C.5, which simply adds the requirement K \u2265 5k0.\nWe note that the condition Lmax \u2264 4R 2\n\u03bbn is not necessarily too restrictive in the case of the structural SVM setup. In particular, the typical range of \u03bb which is needed for a problem is around O(1/n) \u2013 and so the condition becomes Lmax \u2264 4R2 which is typically satisfied when the loss function is normalized.\nB.4. Implementation\nWe comment on three practical implementation aspects of Algorithm 4 on large structural SVM problems:\nMemory. For each datapoint i, our Algorithm 4 stores an additional vector wi \u2208 Rd holding the contribution of its corresponding dual variables \u03b1(i) to the primal vector w = A\u03b1, i.e. wi = A\u03b1[i], where \u03b1[i] is \u03b1(i) padded with zeros so that \u03b1[i] \u2208 Rm and \u03b1 = \u2211 i\u03b1[i]. This means the algorithm needs more memory than the direct (or batch) Frank-Wolfe structural SVM Algorithm 2, but the additional memory can sometimes be bounded by a constant times the size of the input data itself. In particular, in the case that the feature vectors \u03c8i(y) are sparse, we can sometimes get the same improvement in memory requirements for wi, since for fixed i, all vectors \u03c8i(y) usually have the same sparsity pattern. On the other hand, if the feature vectors are not sparse, it might be more efficient to only work with the dual variables instead of the primal variables (see the kernelized version in Appendix B.5 for more details).\nDuality Gap as a Stopping Criterion. Analogous as in the \u2018classical Frank-Wolfe\u2019 structural SVM Algorithm 2 explained in Section 4, we would again like to use the duality gap g(\u03b1(k)) \u2264 \u03b5 as the stopping criterion for the faster Algorithm 4. Unfortunately, since now in every step we only update a single one of the many blocks, such a single direction s(i) will only determine the partial gap g (i)(\u03b1(k)) in the i-th block, but not the full information needed to know the total gap g(\u03b1(k)). Instead, to compute the total gap, a single complete (batch) pass through all datapoints as in Algorithm 2 is necessary, to obtain a full linear minimizer s \u2208M. For efficiency reason, we could therefore compute the duality gap every say Nn iterations for some constant N > 1. Then stopping as soon as g(\u03b1(k)) = g(w(k), `(k),ws, `s) \u2264 \u03b5 will not affect our convergence results.\nLine-Search. To compute the line-search step-size for Frank-Wolfe on the structural SVM, we recall that the analytic formula was given by \u03b3opt := \u3008\u03b1\u2212s,\u2207f(\u03b1)\u3009 \u03bb\u2016A(\u03b1\u2212s)\u20162 , and finally taking \u03b3LS := max {0,min {1, \u03b3opt}}. This is valid for any s \u2208 M. For the block-coordinate Frank-Wolfe Algorithm 4, s is equal to \u03b1 for all blocks, except for the i-th block \u2013 this means that \u03b1 \u2212 s = \u03b1[i] \u2212 s[i], i.e. is zero everywhere except on the i-th block. By recalling that wi = A\u03b1[i] is the individual contribution to w from \u03b1(i) which is stored during the algorithm, we see that the denominator thus becomes \u03bb \u2016A(\u03b1\u2212 s)\u20162 = \u03bb \u2016wi \u2212 ws\u20162. The numerator is \u3008\u03b1\u2212 s,\u2207f(\u03b1)\u3009 = (\u03b1\u2212 s)T (\u03bbATA\u03b1\u2212 b) = \u03bb(wi \u2212 ws)Tw \u2212 `i + `s, where as before `i = bT\u03b1[i] is maintained during Algorithm 4 and so the line-search step-size can be computed efficiently. We mention in passing that when s(i) is the exact minimizer of the linear subproblem on M(i), then the numerator is actually a duality\ngap component g(i)(\u03b1) as defined in (16) \u2013 the total duality gap then is g(\u03b1) = \u2211 i g\n(i)(\u03b1) which can only be computed if we do a batch pass over all the datapoints, as explained in the previous paragraph."}, {"heading": "B.5. More details on the Kernelized Algorithm", "text": "Both Algorithms 2 and 4 can be used with kernels by explicitly maintaining the sparse dual variables \u03b1(k) instead of the primal variables w(k). In this case, the classifier is only given implicitly as a sparse combination of the corresponding kernel functions, i.e. w = A\u03b1, where \u03c8i(y) = k(xi,yi; \u00b7, \u00b7) \u2212 k(xi,y; \u00b7, \u00b7) for a structured kernel k : (X \u00d7 Y)\u00d7 (X \u00d7 Y)\u2192 R. Note that the number of non-zero dual variables is upper-bounded by the number of iterations, and so the time to take dot products grows quadratically in the number of iterations.\nAlgorithm B.1 Kernelized Dual Block-Coordinate Frank-Wolfe for Structural SVM\nLet \u03b1(0) := (ey1 , . . . , eyn) \u2208M = \u2206|Y1| \u00d7 . . .\u00d7\u2206|Yn| and \u03b1\u0304(0) = \u03b1(0) for k = 0 . . .K do\nPick i uniformly at random in {1, . . . , n} Solve y\u2217i := argmax\ny\u2208Yi Hi(y;A\u03b1\n(k)) (solve the loss-augmented decoding problem (2))\ns(i) := e y\u2217i \u2208M(i) (having only a single non-zero entry) Let \u03b3 := 2nk+2n , or optimize \u03b3 by line-search Update \u03b1 (k+1) (i) := (1\u2212 \u03b3)\u03b1 (k) (i) + \u03b3s(i) (Optionally: Update \u03b1\u0304(k+1) := k k+2 \u03b1\u0304(k) + 2 k+2 \u03b1(k+1)) (maintain a weighted average of the iterates)\nTo compute the line-search step-size, we simply re-use the same formula as in Algorithm 4, but reconstructing (implicitly) on the fly the missing quantities such as `i = b T\u03b1[i], wi = A\u03b1[i] and w (k) = A\u03b1(k), and reinterpreting dot products such as wTi w (k) as the suitable sum of kernel evaluations (which has O(k2/n) terms, where k is the number of iterations since the beginning)."}, {"heading": "C. Analysis of the Block-Coordinate Frank-Wolfe Algorithm 3", "text": "This section gives a self-contained presentation and analysis of the new block-coordinate Frank-Wolfe optimization Algorithm 3. The main goal is to prove the convergence Theorem 2, which here is split into two parts, the primal convergence rate in Theorem C.1, and the primal-dual convergence rate in Theorem C.3. Finally, we will present a faster convergence result for the line-search variant in Theorem C.4 and Theorem C.5, which we have used in the convergence for the structural SVM case as presented above in Theorem 3.\nCoordinate Descent Methods. Despite their simplicity and very early appearance in the literature, surprisingly few results were known on the convergence (and convergence rates in particular) of coordinate descent type methods. Recently, the interest in these methods has grown again due to their good scalability to very large scale problems as e.g. in machine learning, and also sparked new theoretical results such as (Nesterov, 2012).\nConstrained Convex Optimization over Product Domains. We consider the general constrained convex optimization problem\nmin x\u2208M f(x) (10)\nover a Cartesian product domain M =M(1) \u00d7 . . .\u00d7M(n) \u2286 Rm, where each factor M(i) \u2286 Rmi is convex and compact, and \u2211n i=1mi = m. We will write x(i) \u2208 Rmi for the i-th block of coordinates of a vector x \u2208 Rm, and x[i] for the padding of x(i) with zeros so that x[i] \u2208 Rm.\nNesterov\u2019s \u2018Huge Scale\u2019 Coordinate Descent. If the objective function f is strongly smooth (i.e. has Lipschitz continuous partial gradients \u2207(i)f(x) \u2208 Rmi), then the following algorithm converges5 at a rate of 1k ,\n5 By additionally assuming strong convexity of f w.r.t. the `1-norm (global onM, not only on the individual factors), one can even get linear convergence rates, see again (Nesterov, 2012) and the follow-up paper (Richta\u0301rik & Taka\u0301c\u030c, 2011).\nor more precisely nk+n , as shown in (Nesterov, 2012, Section 4):\nAlgorithm C.1 Uniform Coordinate Descent Method, (Nesterov, 2012, Section 4)\nLet x(0) \u2208M for k = 0 . . .\u221e do\nPick i uniformly at random in {1, . . . , n} Compute s(i) := argmin\ns(i)\u2208M(i)\n\u2329 s(i),\u2207(i)f(x(k)) \u232a + Li2 \u2225\u2225s(i) \u2212 x(i) \u2225\u22252\nUpdate x (k+1) (i) := x (k) (i) + ( s(i) \u2212 x(k)(i) ) (only affecting the i-th coordinate block)\nUsing Simpler Update Steps: Frank-Wolfe / Conditional Gradient Methods. In some large-scale applications, the above computation of the update direction s(i) can be problematic, e.g. if the Lipschitz constants Li are unknown, or \u2014more importantly\u2014 if the domainsM(i) are such that the quadratic term makes the subproblem for s(i) hard to solve.\nThe structural SVM is a nice example where this makes a big difference. Here, each domain block M(i) is a simplex of exponentially many variables, but nevertheless the linear subproblem over one such factor (also known as loss-augmented decoding) is often relatively easy to solve.\nWe would therefore like to replace the above computation of s(i) by a simpler one, as proposed in the following algorithm variant:\nAlgorithm C.2 Cheaper Coordinate Descent: Block-Coordinate Frank-Wolfe Algorithm\nLet x(0) \u2208M and x\u0304(0)w = x(0) for k = 0 . . .\u221e do\nPick i uniformly at random in {1, . . . , n} Compute s(i) := argmin\ns(i)\u2208M(i)\n\u2329 s(i),\u2207(i)f(x(k)) \u232a\n(or alternatively, find s(i) that solves this linear problem approximately,\neither up to an additive error (11) or up to a multiplicative error (12))\nLet \u03b3 := 2nk+2n , or perform line-search for the step-size: \u03b3 := argmin \u03b3\u2208[0,1]\nf ( x(k) + \u03b3 ( s[i] \u2212 x(k)[i] )) Update x\n(k+1) (i) := x (k) (i) + \u03b3 ( s(i) \u2212 x(k)(i) ) (only affecting the i-th coordinate block)\n(Optionally: Update x\u0304 (k+1) w := k k+2 x\u0304 (k) w + 2 k+2 x(k+1)) (maintain a weighted average of the iterates)\nThis natural coordinate descent type optimization method picks a single one of the n blocks uniformly at random, and in each step leaves all other blocks unchanged.\nIf there is only one factor (n = 1), then Algorithm C.2 becomes the standard Frank-Wolfe (or conditional gradient) algorithm, which is known to converge at a rate of O(1/k) (Frank & Wolfe, 1956; Dunn & Harshbarger, 1978; Clarkson, 2010; Jaggi, 2013).\nUsing Approximate Linear Minimizers. If approximate linear minimizers are used internally in Algorithm C.2, then the necessary approximation quality for the candidate directions s(i) is determined as follows (in either additive or multiplicative quality):\nIn the additive case, we choose a fixed additive error parameter \u03b4 \u2265 0 such that the candidate direction s(i) satisfies \u2329\ns(i),\u2207(i)f(x) \u232a \u2264 min\ns\u2032 (i) \u2208M(i)\n\u2329 s\u2032(i),\u2207(i)f(x) \u232a + 12\u03b4 \u03b3\u0303k C (i) f , (11)\nwhere \u03b3\u0303k := 2n k+2n comes from the default step-size and is used for the convergence results to come. Note that if line-search is used to determine a different step-size, the candidate direction is still defined with respect to the default \u03b3\u0303k.\nIn the multiplicative case, we choose a fixed multiplicative error parameter 0 < \u03bd \u2264 1 such that the candidate directions s(i) attain the current \u2018duality gap\u2019 on the i-th factor up to a multiplicative approximation error of \u03bd, i.e. \u2329\nx\u2212 s(i),\u2207(i)f(x) \u232a \u2265 \u03bd \u00b7 max\ns\u2032 (i) \u2208M(i)\n\u2329 x\u2212 s\u2032(i),\u2207(i)f(x) \u232a . (12)\nIf a multiplicative approximate internal oracle is used together with the predefined step-size instead of doing line-search, then the step-size in Algorithm C.2 needs to be increased to \u03b3k := 2n \u03bdk+2n instead of the original 2n k+2n .\nBoth types of errors can be combined together with the following property for the candidate direction s(i):\n\u2329 x\u2212 s(i),\u2207(i)f(x) \u232a \u2265 \u03bd \u00b7 max\ns\u2032 (i) \u2208M(i)\n\u2329 x\u2212 s\u2032(i),\u2207(i)f(x) \u232a \u2212 12\u03b4 \u03b3\u0303k C (i) f , (13)\nwhere \u03b3\u0303k := 2n\n\u03bdk+2n .\nAveraging the Iterates. In the above Algorithm C.2 we have also added an optional last line which maintains the following weighted average x\u0304 (k) w which is defined for k \u2265 1 as\nx\u0304(k)w := 2\nk(k + 1)\nk\u2211\nt=1\ntx(t) , (14)\nand by convention we also define x\u0304 (0) w := x(0). As our convergence analysis will show, the weighted average of the iterates can yield more robust duality gap convergence guarantees when the duality gap function g is convex in x (see Theorem C.3) \u2013 this is for example the case for quadratic functions such as in the structural SVM objective (4). We will also consider in our proofs a scheme which averages the last (1\u2212\u00b5)-fraction of the iterates for some fixed 0 < \u00b5 < 1:\nx\u0304(k)\u00b5 := 1 k \u2212 d\u00b5ke+ 1 k\u2211\nt=d\u00b5ke\nx(t) . (15)\nThis is what Rakhlin et al. (2012) calls (1 \u2212 \u00b5)-suffix averaging and it appeared in the context of getting a stochastic subgradient method with O(1/k) convergence rate for strongly convex functions instead of the standard O((log k)/k) rate that one can prove for the individual iterates x(k). The problem with (1\u2212 \u00b5)-suffix averaging is that to implement it for a fixed \u00b5 (say \u00b5 = 0.5) without storing a fraction of all the iterates, one needs to know when they will stop the algorithm. An alternative mentioned in Rakhlin et al. (2012) is to maintain a uniform average over rounds of exponentially increasing size (the so-called \u2018doubling trick\u2019). This can give very good performance towards the end of the rounds as we will see in our additional experiments in Appendix F, but the performance varies widely towards the beginning of the rounds. This motivates the simpler and more robust weighted averaging scheme (14), which in the case of the stochastic subgradient method, was also recently proven to have O(1/k) convergence rate by Lacoste-Julien et al. (2012)6 and independently by Shamir & Zhang (2013), who called such schemes \u2018polynomial-decay averaging\u2019.\nRelated Work. In contrast to the randomized choice of coordinate which we use here, the analysis of cyclic coordinate descent algorithms (going through the blocks sequentially) seems to be notoriously difficult, such that until today, no analysis proving a global convergence rate has been obtained as far as we know. Luo & Tseng (1992) has proven a local linear convergence rate for the strongly convex case.\nFor product domains, such a cyclic analogue of our Algorithm C.2 has already been proposed in Patriksson (1998), using a generalization of Frank-Wolfe iterations under the name \u2018cost approximation\u2019. The analysis of Patriksson (1998) shows asymptotic convergence, but since the method goes through the blocks sequentially, no convergence rates could be proven so far.\n6In this paper, they considered a (k + 1)-weight instead of our k-weight, but similar rates can be proven for shifted versions. We motivate skipping the first iterate x(0) in our weighted averaging scheme as sometimes bounds can be proven on the quality of x(1) irrespective of x(0) for Frank-Wolfe (see the paragraph after the proof of Theorem C.1 for example, looking at the n = 1 case)."}, {"heading": "C.1. Setup for Convergence Analysis", "text": "We review below the important concepts needed for analyzing the convergence of the block-coordinate FrankWolfe Algorithm C.2.\nDecomposition of the Duality Gap. The product structure of our domain has a crucial effect on the duality gap, namely that it decomposes into a sum over the n components of the domain. The \u2018linearization\u2019 duality gap as defined in (5) (see also Jaggi (2013)) for any constrained convex problem of the above form (10), for a fixed feasible point x \u2208M, is given by\ng(x) := max s\u2208M\n\u3008x\u2212 s,\u2207f(x)\u3009\n=\nn\u2211\ni=1\nmax s(i)\u2208M(i)\n\u2329 x(i) \u2212 s(i),\u2207(i)f(x) \u232a\n=:\nn\u2211\ni=1\ng(i)(x) .\n(16)\nCurvature. Also, the curvature can now be defined on the individual factors,\nC (i) f := sup\nx\u2208M, s(i)\u2208M(i), \u03b3\u2208[0,1], y=x+\u03b3(s[i]\u2212x[i])\n2 \u03b32 ( f(y)\u2212 f(x)\u2212 \u3008y(i) \u2212 x(i),\u2207(i)f(x)\u3009 ) . (17)\nWe recall that the notation x[i] and x(i) is defined just below (10). We define the global product curvature as the sum of these curvatures for each block, i.e.\nC\u2297f :=\nn\u2211\ni=1\nC (i) f . (18)"}, {"heading": "C.2. Primal Convergence on Product Domains", "text": "The following main theorem shows that after O (\n1 \u03b5\n) many iterations, Algorithm C.2 obtains an \u03b5-approximate\nsolution.\nTheorem C.1 (Primal Convergence). For each k \u2265 0, the iterate x(k) of the exact variant of Algorithm C.2 satisfies\nE[f(x(k))]\u2212 f(x\u2217) \u2264 2n k + 2n\n( C\u2297f + f(x (0))\u2212 f(x\u2217) ) ,\nFor the approximate variant of Algorithm C.2 with additive approximation quality (11) for \u03b4 \u2265 0, it holds that\nE[f(x(k))]\u2212 f(x\u2217) \u2264 2n k + 2n\n( C\u2297f (1 + \u03b4) + f(x (0))\u2212 f(x\u2217) ) .\nFor the approximate variant of Algorithm C.2, with multiplicative approximation quality (12) for 0 < \u03bd \u2264 1, it holds that\nE[f(x(k))]\u2212 f(x\u2217) \u2264 2n \u03bdk + 2n (1 \u03bd C\u2297f + f(x (0))\u2212 f(x\u2217) ) .\nAll convergence bounds hold both if the predefined step-sizes, or line-search is used in the algorithm. Here x\u2217 \u2208M is an optimal solution to problem (10), and the expectation is with respect to the random choice of blocks during the algorithm. (In other words all three algorithm variants deliver a solution of (expected) primal error at most \u03b5 after O( 1\u03b5 ) many iterations.)\nThe proof of the above theorem on the convergence rate of the primal error crucially depends on the following Lemma C.2 on the improvement in each iteration.\nLemma C.2. Let \u03b3 \u2208 [0, 1] be an arbitrary fixed step-size. Moving only within the i-th block of the domain, we consider two variants of steps towards a direction s(i) \u2208 M(i): Let x(k+1)\u03b3 := x(\u03b3) be the point obtained by moving towards s(i) using step-size \u03b3, and let x (k+1) LS := x(\u03b3LS) be the corresponding point obtained by line-search, i.e. \u03b3LS := argmin \u03b3\u0304\u2208[0,1] f (x(\u03b3\u0304)). Here for convenience we have used the notation x(\u03b3\u0304) := x(k) + \u03b3\u0304 ( s[i] \u2212 x(k)[i] ) for \u03b3\u0304 \u2208 [0, 1]. If for each i the candidate direction s(i) satisfies the additive approximation quality (11) for \u03b4 \u2265 0 and some fixed \u03b3\u0303k, then in expectation over the random choice of the block i and conditioned on x (k), it holds that\nE [ f(x\n(k+1) LS ) |x(k)\n] \u2264 E [ f(x(k+1)\u03b3 ) |x(k) ] \u2264 f(x(k))\u2212 \u03b3\nn g(x(k)) +\n1\n2n (\u03b32 + \u03b4\u03b3\u0303k\u03b3)C \u2297 f .\nOn the other hand, if s(i) attains the duality gap g (i)(x) on the i-th block up to a multiplicative approximation quality (12) for 0 < \u03bd \u2264 1, then\nE [ f(x\n(k+1) LS ) |x(k)\n] \u2264 E [ f(x(k+1)\u03b3 ) |x(k) ] \u2264 f(x(k))\u2212 \u03b3\nn \u03bd g(x(k)) +\n\u03b32 2n C\u2297f .\nAll expectations are taken over the random choice of the block i and conditioned on x(k).\nProof. We write x := x(k), y := x (k+1) \u03b3 = x + \u03b3(s[i] \u2212 x[i]), with x[i] and s[i] being zero everywhere except in their i-th block. We also write dx := \u2207(i)f(x) to simplify the notation. From the definition (17) of the curvature constant C\n(i) f of our convex function f over the factor M(i), we have\nf(y) = f(x+ \u03b3(s[i] \u2212 x[i])) \u2264 f(x) + \u03b3\u3008s(i) \u2212 x(i), dx\u3009+ \u03b3 2 2 C (i) f .\nNow we use that by (11), the choice of s(i) with \u2329 s(i),\u2207(i)f(x) \u232a \u2264 min s\u2032 (i) \u2208M(i) \u2329 s\u2032(i),\u2207(i)f(x) \u232a + 12\u03b4\u03b3\u0303kC (i) f is a good descent direction for the linear approximation to f at x, on the i-th factor M(i), giving\n\u3008s(i) \u2212 x(i), dx\u3009 \u2264 \u2212g(i)(x) + \u03b4\u03b3\u0303k2 C (i) f , (19)\nby the definition (16) of the duality gap. Altogether, we have obtained\nf(y) \u2264 f(x) + \u03b3(\u2212g(i)(x) + \u03b4\u03b3\u0303k2 C (i) f ) +\n\u03b32\n2 C (i) f\n= f(x)\u2212 \u03b3g(i)(x) + 12 (\u03b32 + \u03b4\u03b3\u0303k\u03b3)C (i) f .\nUsing that the line-search by definition must lead to an objective value at least as good as the one at the fixed \u03b3, we therefore have shown the inequality\nf(x (k+1) LS ) \u2264 f(x (k+1) \u03b3 ) \u2264 f(x(k))\u2212 \u03b3g(i)(x(k)) + 12 (\u03b32 + \u03b4\u03b3\u0303k\u03b3)C (i) f .\nFinally the claimed bound on the expected improvement directly follows by taking the expectation: With respect to the (uniformly) random choice of the block i, the expected value of the gap g(i)(x(k)) corresponding to the picked i is exactly 1ng(x (k)). Also, the expected curvature of the i-th factor is 1nC \u2297 f . The proof for the case of multiplicative approximation follows completely analogously, using \u3008s(i) \u2212 x(i), dx\u3009 \u2264 \u2212\u03bd g(i)(x), which then gives a step improvement of f(y) \u2264 f(x)\u2212 \u03b3\u03bdg(i)(x) + \u03b322 C (i) f .\nHaving Lemma C.2 at hand, we will now prove our above primal convergence Theorem C.1 using similar ideas as for general domains, such as in Jaggi (2013).\nProof of Theorem C.1. We first prove the theorem for the approximate variant of Algorithm C.2 with multiplicative approximation quality (12) of 0 < \u03bd \u2264 1 \u2013 the exact variant of the algorithm is simply the special case\n\u03bd = 1. From the above Lemma C.2, we know that for every inner step of Algorithm C.2 and conditioned on x(k), we have that E[f(x (k+1) \u03b3 ) |x(k)] \u2264 f(x(k))\u2212 \u03b3\u03bdn g(x(k)) + \u03b32 2nC \u2297 f , where the expectation is over the random choice of the block i (this bound holds independently whether line-search is used or not). Writing h(x) := f(x)\u2212f(x\u2217) for the (unknown) primal error at any point x, this reads as\nE[h(x (k+1) \u03b3 ) |x(k)] \u2264 h(x(k))\u2212 \u03b3\u03bdn g(x(k)) +\n\u03b32 2nC \u2297 f\n\u2264 h(x(k))\u2212 \u03b3\u03bdn h(x(k)) + \u03b32 2nC \u2297 f = (1\u2212 \u03b3\u03bdn )h(x(k)) + \u03b32 2nC \u2297 f ,\n(20)\nwhere in the second line, we have used weak duality h(x) \u2264 g(x) (which follows directly from the definition of the duality gap, together with convexity of f). The inequality (20) is conditioned on x(k), which is a random quantity given the previous random choices of blocks to update. We get a deterministic inequality by taking the expectation of both sides with respect to the random choice of previous blocks, yielding:\nE[h(x (k+1) \u03b3 )] \u2264 (1\u2212 \u03b3\u03bdn ) E[h(x(k))] +\n\u03b32 2nC \u2297 f . (21)\nWe observe that the resulting inequality (21) with \u03bd = 1 is of the same form as the one appearing in the standard Frank-Wolfe primal convergence proof such as in Jaggi (2013), though with a crucial difference of the 1/n factor (and that we are now working with the expected values E[h(x(k))] instead of the original h(x(k))). We will thus follow a similar induction argument over k, but we will see that the 1/n factor will yield a slightly different induction base case (which for n = 1 can be analyzed separately to obtain a better bound). To simplify the notation, let hk := E[h(x (k))].\nBy induction, we are now going to prove that\nhk \u2264 2nC\n\u03bdk + 2n for k \u2265 0 .\nfor the choice of constant C := 1\u03bdC \u2297 f + h0. The base-case k = 0 follows immediately from the definition of C, given that C \u2265 h0. Now we consider the induction step for k \u2265 0. Here the bound (21) for the particular choice of step-size \u03b3k := 2n \u03bdk+2n \u2208 [0, 1] given by Algorithm C.2 gives us (the same bound also holds for the line-search variant, given that the corresponding objective value f(x (k+1) Line-Search) \u2264 f(x(k+1)\u03b3 ) only improves):\nhk+1 \u2264 (1\u2212 \u03b3k\u03bdn )hk + (\u03b3k)2C\u03bd2n = (1\u2212 2\u03bd\u03bdk+2n )hk + ( 2n\u03bdk+2n )2C\u03bd2n \u2264 (1\u2212 2\u03bd\u03bdk+2n ) 2nC\u03bdk+2n + ( 1\u03bdk+2n )22nC\u03bd ,\nwhere in the first line we have used that C\u2297f \u2264 C\u03bd, and in the last inequality we have plugged in the induction hypothesis for hk. Simply rearranging the terms gives\nhk+1 \u2264 2nC\u03bdk+2n ( 1\u2212 2\u03bd\u03bdk+2n + \u03bd\u03bdk+2n )\n= 2nC\u03bdk+2n \u03bdk+2n\u2212\u03bd \u03bdk+2n \u2264 2nC\u03bdk+2n \u03bdk+2n\u03bdk+2n+\u03bd = 2nC\u03bd(k+1)+2n ,\nwhich is our claimed bound for k \u2265 0. The analogous claim for Algorithm C.2 using the approximate linear primitive with additive approximation quality (11) with \u03b3\u0303k = 2n \u03bdk+2n follows from exactly the same argument, by replacing every occurrence of C \u2297 f in the proof here by C\u2297f (1 + \u03b4) instead (compare to Lemma C.2 also \u2013 note that \u03b3 = \u03b3\u0303k here). Note moreover that one can combine easily both a multiplicative approximation with an additive one as in (13), and modify the convergence statement accordingly.\nDomains Without Product Structure: n = 1. Our above convergence result also holds for the case of the standard Frank-Wolfe algorithm, when no product structure on the domain is assumed, i.e. for the case n = 1. In this case, the constant in the convergence can even be improved for the variant of the algorithm without a multiplicative approximation (\u03bd = 1), since the additive term given by h0, i.e. the error at the starting point, can be removed. This is because already after the first step, we obtain a bound for h1 which is independent of h0. More precisely, plugging \u03b30 := 1 and \u03bd = 1 in the bound (21) when n = 1 gives h1 \u2264 0 + C\u2297f (1 + \u03b4) \u2264 C. Using k = 1 as the base case for the same induction proof as above, we obtain that for n = 1:\nhk \u2264 2\nk + 2 C\u2297f (1 + \u03b4) for all k \u2265 1 ,\nwhich matches the convergence rate given in Jaggi (2013). Note that in the traditional Frank-Wolfe setting, i.e. n = 1, our defined curvature constant becomes C\u2297f = Cf .\nDependence on h0. We note that the only use of including h0 in the constant C = \u03bd \u22121C\u2297f +h0 was to satisfy the base case in the induction proof, at k = 0. If from the structure of the problem we can get a guarantee that h0 \u2264 \u03bd\u22121C\u2297f , then the smaller constant C \u2032 = \u03bd\u22121C\u2297f will satisfy the base case and the whole proof will go through with it, without needing the extra h0 factor. See also Theorem C.4 for a better convergence result with a weaker dependence on h0 in the case where the line-search is used."}, {"heading": "C.3. Obtaining Small Duality Gap", "text": "The following theorem shows that after O (\n1 \u03b5\n) many iterations, Algorithm C.2 will have visited a solution with\n\u03b5-small duality gap in expectation. Because the block-coordinate Frank-Wolfe algorithm is only looking at one block at a time, it doesn\u2019t know what is its current true duality gap without doing a full (batch) pass over all blocks. Without monitoring this quantity, the algorithm could miss which iterate had a low duality gap. This is why, if one is interested in having a good duality gap (such as in the structural SVM application), then the averaging schemes considered in (14) and (15) become interesting: the following theorem also says that the bound hold for each of the averaged iterates, if the duality gap function g is convex, which is the case for example when f is a quadratic function.7 Theorem C.3 (Primal-Dual Convergence). For each K \u2265 0, the variants of Algorithm C.2 (either using the predefined step-sizes, or using line-search) will yield at least one iterate x(k\u0302) with k\u0302 \u2264 K with expected duality gap bounded by\nE [ g(x(k\u0302)) ] \u2264 \u03b2 2n\n\u03bd(K + 1) C ,\nwhere \u03b2 = 3 and C = \u03bd\u22121C\u2297f (1 + \u03b4) + f(x (0)) \u2212 f(x\u2217). \u03b4 \u2265 0 and 0 < \u03bd \u2264 1 are the approximation quality parameters as defined in (13) \u2013 use \u03b4 = 0 and \u03bd = 1 for the exact variant. Moreover, if the duality gap g is a convex function of x, then the above bound also holds both for E [ g(x\u0304 (K) w ) ] and E [ g(x\u0304\n(K) 0.5 ) ] for each K \u2265 0, where x\u0304(K)w is the weighted average of the iterates as defined in (14) and x\u0304(K)0.5\nis the 0.5-suffix average of the iterates as defined in (15) with \u00b5 = 0.5.\nProof. To simplify notation, we will again denote the expected primal error and expected duality gap for any iteration k \u2265 0 in the algorithm by hk := E[h(x(k))] := E[f(x(k))\u2212 f(x\u2217)] and gk := E[g(x(k))] respectively. The proof starts again by using the crucial improvement Lemma C.2 with \u03b3 = \u03b3k := 2n \u03bdk+2n to cover both variants of Algorithm C.2 at the same time. As in the beginning of the proof of Theorem C.1, we take the expectation with respect to x(k) in Lemma C.2 and subtract f(x\u2217) to get that for each k \u2265 0 (for the general approximate variant of the algorithm):\nhk+1 \u2264 hk \u2212 1n\u03b3k\u03bd gk + 12n (\u03b3k2 + \u03b4\u03b3\u0303k\u03b3k)C\u2297f = hk \u2212 1n\u03b3k\u03bd gk + 12n\u03b3k2C\u2297f (1 + \u03b4) ,\n7To see that g is convex when f is quadratic, we refer to the equivalence between the gap g(x) and the Fenchel duality p(x)\u2212d(\u2207f(x))) as shown in Appendix D. The dual function d(\u00b7) is concave, so if \u2207f(x)) is an affine function of x (which is the case for a quadratic function), then d will be a concave function of x, implying that g(x) = p(x) \u2212 d(\u2207f(x))) is convex in x, since the primal function p is convex.\nsince \u03b3\u0303k \u2264 \u03b3k. By isolating gk and using the fact that C \u2265 \u03bd\u22121C\u2297f (1 + \u03b4), we get the crucial inequality for the expected duality gap:\ngk \u2264 n\n\u03bd\u03b3k (hk \u2212 hk+1) + \u03b3k\nC 2 . (22)\nThe general proof idea to get an handle on gk is to take a convex combination over multiple k\u2019s of the inequality (22), to obtain a new upper bound. Because a convex combination of numbers is upper bounded by its\nmaximum, we know that the new bound has to upper bound at least one of the gk\u2019s (this gives the existence k\u0302 part of the theorem). Moreover, if g is convex, we can also obtain an upper bound for the expected duality gap of the same convex combination of the iterates. So let {wk}Kk=0 be a set of non-negative weights, and let \u03c1k := wk/SK , where SK := \u2211K k=0 wk. Taking the convex combination of inequality (22) with coefficient \u03c1k, we get\nK\u2211\nk=0\n\u03c1kgk \u2264 n\n\u03bd\nK\u2211\nk=0\n\u03c1k ( hk \u03b3k \u2212 hk+1 \u03b3k ) + K\u2211\nk=0\n\u03c1k\u03b3k C\n2\n= n\n\u03bd ( h0 \u03c10 \u03b30 \u2212 hK+1 \u03c1K \u03b3K ) + n \u03bd K\u22121\u2211\nk=0\nhk+1 ( \u03c1k+1 \u03b3k+1 \u2212 \u03c1k \u03b3k ) + K\u2211\nk=0\n\u03c1k\u03b3k C\n2\n\u2264 n \u03bd h0 \u03c10 \u03b30 + n \u03bd\nK\u22121\u2211\nk=0\nhk+1 ( \u03c1k+1 \u03b3k+1 \u2212 \u03c1k \u03b3k ) + K\u2211\nk=0\n\u03c1k\u03b3k C\n2 , (23)\nusing hK+1 \u2265 0. Inequality (23) can be seen as a master inequality to derive various bounds on gk. In particular, if we define x\u0304 := \u2211K k=0 \u03c1kx (k) and we suppose that g is convex (which is the case for example when f is a\nquadratic function), then we have E[g(x\u0304)] \u2264\u2211Kk=0 \u03c1kgk by convexity and linearity of the expectation.\nWeighted-averaging case. We first consider the weights wk = k which appear in the definition of the weighted average of the iterates x\u0304 (K) w in (14) and suppose K \u2265 1. In this case, we have \u03c1k = k/SK where SK = K(K+1)/2. With the predefined step-size \u03b3k = 2n/(\u03bdk + 2n), we then have\n\u03c1k+1 \u03b3k+1 \u2212 \u03c1k \u03b3k = 1 2nSK ((k + 1)(\u03bd(k + 1) + 2n)\u2212 k(\u03bdk + 2n))\n= \u03bd(2k + 1) + 2n\n2nSK .\nPlugging this in the master inequality (23) as well as using the convergence rate hk \u2264 2nC\u03bdk+2n from Theorem C.1, we obtain\nK\u2211\nk=0\n\u03c1kgk \u2264 n\n\u03bdSK\n[ 0 + K\u22121\u2211\nk=0\n2nC\n\u03bd(k + 1) + 2n\n\u03bd(2k + 1) + 2n\n2n\n] + K\u2211\nk=0\n2nk\n\u03bdk + 2n\nC\n2SK\n\u2264 nC \u03bdSK\n[ 2 K\u22121\u2211\nk=0\n1 +\nK\u2211\nk=1\n1\n]\n= 2nC\n\u03bd(K + 1) \u00b7 3.\nHence we have proven the bound with \u03b2 = 3 for K \u2265 1. For K = 0, the master inequality (23) becomes\ng0 \u2264 n\n\u03bd h0 +\n1 2 C \u2264 nC \u03bd\n( 1 + 1\n2n\n)\nsince h0 \u2264 C and \u03bd \u2264 1. Given that n \u2265 1, we see that the bound also holds for K = 0.\nSuffix-averaging case. For the proof of convergence of the 0.5-suffix averaging of the iterates x\u0304 (K) 0.5 , we refer the reader to the proof of Theorem C.5 which can be re-used for this case (see the last paragraph of the proof to explain how).\nDomains Without Product Structure: n = 1. As we mentioned after the proof of the primal convergence Theorem C.1, we note that if n = 1, then we can replace C in the statement of Theorem C.3 by C\u2297f (1 + \u03b4) for K \u2265 1 when \u03bd = 1, as then we can ensure that h1 \u2264 C which is all what was needed for the primal convergence induction. Again, C\u2297f = Cf when n = 1."}, {"heading": "C.4. An Improved Convergence Analysis for the Line-Search Case", "text": "C.4.1. Improved Primal Convergence for Line-Search\nIf line-search is used, we can improve the convergence results of Theorem C.1 by showing a weaker dependence on the starting condition h0 thanks to faster progress in the starting phase of the first few iterations: Theorem C.4 (Improved Primal Convergence for Line-Search). For each k \u2265 k0, the iterate x(k) of the linesearch variant of Algorithm C.2 (where the linear subproblem is solved with a multiplicative approximation quality (12) of 0 < \u03bd \u2264 1) satisfies\nE [ f(x(k)) ] \u2212 f(x\u2217) \u2264 1\n\u03bd 2nC\u2297f \u03bd(k \u2212 k0) + 2n\n(24)\nwhere k0 := max { 0, \u2308 log ( 2\u03bdh(x(0))\nC\u2297f\n)/ (\u2212 log \u03ben) \u2309} is the number of steps required to guarantee that\nE [ f(x(k)) ] \u2212 f(x\u2217) \u2264 \u03bd\u22121C\u2297f , with x\u2217 \u2208 M being an optimal solution to problem (10), and h(x(0)) := f(x(0)) \u2212 f(x\u2217) is the primal error at the starting point, and \u03ben := 1 \u2212 \u03bdn < 1 is the geometric decrease rate of the primal error in the first phase while k < k0 \u2014 i.e. E [ f(x(k)) ] \u2212 f(x\u2217) \u2264 (\u03ben)k h(x(0)) +C\u2297f /2\u03bd for k < k0."}, {"heading": "If the linear subproblem is solved with an additive approximation quality (11) of \u03b4 \u2265 0 instead, then replace all", "text": "appearances of C\u2297f above with C \u2297 f (1 + \u03b4).\nProof. For the line-search case, the expected improvement guaranteed by Lemma C.2 for the multiplicative approximation variant of Algorithm C.2, in expectation as in (21), is valid for any choice of \u03b3 \u2208 [0, 1]:\nE [ h(x\n(k+1) LS ) ] \u2264 (1\u2212 \u03bd\u03b3n ) E [ h(x(k)) ] + \u03b3 2 2nC \u2297 f . (25)\nBecause the bound (25) holds for any \u03b3, we are free to choose the one which minimizes it subject to \u03b3 \u2208 [0, 1], that is \u03b3\u2217 := min { 1, \u03bdhk\nC\u2297f\n} , where we have again used the identification hk := E [ h(x (k) LS) ] . Now we distinguish\ntwo cases:\nIf \u03b3\u2217 = 1, then \u03bdhk \u2265 C\u2297f . By unrolling the inequality (25) recursively to the beginning and using \u03b3 = 1 at each step, we get:\nhk+1 \u2264 ( 1\u2212 \u03bdn ) hk + 1 2nC \u2297 f\n\u2264 ( 1\u2212 \u03bdn )k+1 h0 + 1 2nC \u2297 f \u2211k t=0 ( 1\u2212 \u03bdn )t \u2264 ( 1\u2212 \u03bdn )k+1 h0 + 1 2nC \u2297 f \u2211\u221e t=0 ( 1\u2212 \u03bdn )t = ( 1\u2212 \u03bdn )k+1 h0 + 1 2nC \u2297 f ( 1 1\u2212(1\u2212\u03bd/n) ) = ( 1\u2212 \u03bdn )k+1 h0 + 1 2\u03bdC \u2297 f .\nWe thus have a geometric decrease with rate \u03ben := 1 \u2212 \u03bdn in this phase. We then get hk \u2264 \u03bd\u22121C\u2297f as soon as (\u03ben)\nkh0 \u2264 C\u2297f /2\u03bd, i.e. when k \u2265 log1/\u03ben(2\u03bdh0/C\u2297f ) = log(2\u03bdh0/C\u2297f )/ \u2212 log(1 \u2212 \u03bdn ). We thus have obtained a logarithmic bound on the number of steps that fall into the first regime case here, i.e. where hk is still \u2018large\u2019. Here it is crucial to note that the primal error hk is always decreasing in each step, due to the line-search, so once we leave this regime of hk \u2265 \u03bd\u22121C\u2297f , then we will never enter it again in subsequent steps.\nOn the other hand, as soon as we reach a step k (e.g. when k = k0) such that \u03b3 \u2217 < 1 or equivalently hk < \u03bd \u22121C\u2297f , then we are always in the second phase where \u03b3\u2217 = \u03bdhk C\u2297f . Plugging this value of \u03b3\u2217 in (25) yields the recurrence bound:\nhk+1 \u2264 hk \u2212 1\n\u03b6 h2k \u2200k \u2265 k0 (26)\nwhere \u03b6 := 2nC\u2297f \u03bd2 , with the initial condition hk0 \u2264 C\u2297f \u03bd = \u03bd\u03b6 2n . This is a standard recurrence inequality which appeared for example in Joachims et al. (2009, Theorem 5, see their Equation (23)) or in the appendix of Teo et al. (2007). We can solve the recurrence (26) by following the argument of Teo et al. (2007), where it was pointed out that since hk is monotonically decreasing, we can upper bound hk by the solution to the corresponding differential equations h\u2032(t) = \u2212h2(t)/\u03b6, with initial condition h(k0) = hk0 . Integrating both sides, we get the solution h(t) = \u03b6t\u2212k0+\u03b6/hk0 . Plugging in the value for hk0 and since hk \u2264 h(k), we thus get the bound:\nhk \u2264 1\n\u03bd 2nC\u2297f \u03bd(k \u2212 k0) + 2n \u2200k \u2265 k0, (27)\nwhich completes the proof for the multiplicative approximation variant.\nFor the additive approximation variant, the inequality (25) with \u03b3 = 1 in Lemma C.2 becomes:\nhk+1 \u2264 ( 1\u2212 \u03bdn ) hk + 1 2n (1 + \u03b4\u03b3\u0303k)C \u2297 f\n\u2264 ( 1\u2212 \u03bdn ) hk + 1 2n (1 + \u03b4)C \u2297 f ,\nsince \u03b3\u0303k \u2264 1. By unrolling this inequality as before, we get the geometric rate of decrease in the initial phase by using \u03b3 = 1 until k = k0 where we can ensure that hk0 \u2264 C\u2297f (1 + \u03b4)/\u03bd. We then finish the proof by reusing the induction proof from Theorem C.1, but with Equation (24) as the induction hypothesis, replacing C\u2297f with C\u2297f (1 + \u03b4). The base case at k = k0 is satisfied by the definition of k0. For the induction step, we use \u03b3k = 2n\n\u03bd(k\u2212k0)+2n (note that because we use line-search, we are free to use any \u03b3 we want in the inequality from\nLemma C.2), and use the crucial fact that \u03b3\u0303k = 2n \u03bdk+2n \u2264 \u03b3k to get a similar argument as in Theorem C.1.\nNumber of Iterations. We now make some observations in the case of \u03b4 = 0 (for simplicity). Note that since for n > 0.5 and \u2212 log ( 1\u2212 \u03bdn ) > \u03bdn for the natural logarithm, we get that k0 \u2264 \u2308 n \u03bd log ( 2\u03bdh(x(0))\nC\u2297f\n)\u2309 and so\nunless the structure of our problem can guarantee that h(x(0)) \u2264 C\u2297f /\u03bd, we get a linear number of steps in n required to reach the second phase, but the dependence is logarithmic in h(x(0)) \u2013 instead of linear in h(x(0)) as given by our previous convergence Theorem C.1 for the fixed step-size variant (in the fixed step-size variant,\nwe would need k0 =\n\u2308 2nh(x (0))\nC\u2297f\n\u2309 steps to guarantee hk0 \u2264 C\u2297f /\u03bd). Therefore, for the line-search variant of our\nAlgorithm C.2, we have obtained guaranteed \u03b5-small error after \u2308 n\n\u03bd log\n( 2\u03bdh(x(0))\nC\u2297f\n)\u2309 + \u2308 2nC\u2297f \u03bd2 \u03b5 \u2309\niterations.\nEffect of Line-Search. It is also interesting to point out that even though we were using the optimal stepsize in the second phase of the above proof (which yielded the recurrence (26)), the second phase bound is not better than what we could have obtained by using a fixed step-size schedule of 2n\u03bd(k\u2212k0)+2n and following the same induction proof line as in the previous Theorem C.1 (using the base case hk0 \u2264 C\u2297f /\u03bd and so we could let C := \u03bd\u22121C\u2297f ). This thus means that the advantage of the line-search over the fixed step-size schedule only appears in knowing when to switch from a step-size of 1 (in the first phase, when hk \u2265 \u03bd\u22121C\u2297f ) to a step-size of 2n\u03bd(k\u2212k0)+2n (in the second phase), which unless we know the value of f(x \u2217), we cannot know in general. In the standard Frank-Wolfe case where n = 1 and \u03bd = 1, there is no difference in the rates for line-search or fixed step-size schedule as in this case we know h1 \u2264 C\u2297f as explained at the end of the proof of Theorem C.1. This also suggests that if k0 > n, it might be more worthwhile in theory to first do one batch Frank-Wolfe step to ensure that h1 \u2264 C\u2297f , and then proceed with the block-coordinate Frank-Wolfe algorithm afterwards.\nC.4.2. Improved Primal-Dual Convergence for Line-Search\nUsing the improved primal convergence theorem for line-search, we can also get a better rate for the expected duality gap (getting rid of the dependence of h0 in the constant C):\nTheorem C.5 (Improved Primal-Dual Convergence for Line-Search). Let k0 be defined as in Theorem C.4. For each K \u2265 5k0, the line-search variant of Algorithm C.2 will yield at least one iterate x(k\u0302) with k\u0302 \u2264 K with expected duality gap bounded by\nE [ g(x(k\u0302)) ] \u2264 \u03b2 2n\n\u03bd(K + 2) C ,\nwhere \u03b2 = 3 and C = \u03bd\u22121C\u2297f (1 + \u03b4). \u03b4 \u2265 0 and 0 < \u03bd \u2264 1 are the approximation parameters as defined in (13) \u2013 use \u03b4 = 0 and \u03bd = 1 for the exact variant. Moreover, if the duality gap g is a convex function of x, then the above bound also holds for E [ g(x\u0304\n(K) 0.5 )\n] for each\nK \u2265 5k0, where x\u0304(K)0.5 is the 0.5-suffix average of the iterates as defined in (15) with \u00b5 = 0.5.\nProof. We follow a similar argument as in the proof of Theorem C.3, but making use of the better primal convergence Theorem C.4 as well as using the 0.5-suffix average for the master inequality (23). Let K \u2265 5k0 be given. Let \u03b3k :=\n2n \u03bd(k\u2212k0)+2n for k \u2265 k0. Note then that \u03b3\u0303k = 2n \u03bdk+2n \u2264 \u03b3k and so the gap inequality (22)\nappearing in the proof of Theorem C.3 is valid for this \u03b3k (because we are considering the line-search variant of Algorithm C.2, we are free to choose any \u03b3 \u2208 [0, 1] in Lemma C.2). This means that the master inequality (23) is also valid here with C = \u03bd\u22121C\u2297f (1 + \u03b4).\nWe consider the weights which appear in the definition of the 0.5-suffix average of iterates x\u0304 (K) 0.5 given in (15), i.e. the average of the iterates x(k) from k = Ks := d0.5Ke to k = K. We thus have \u03c1k = 1/SK for Ks \u2264 k \u2264 K and \u03c1k = 0 otherwise, where SK = K \u2212 d0.5Ke+ 1. Notice that Ks \u2265 k0 by assumption. With these choices of \u03c1k and \u03b3k, the master inequality (23) becomes\nK\u2211\nk=0\n\u03c1kgk \u2264 n\n\u03bdSK [ hKs \u03b3Ks + K\u22121\u2211\nk=Ks\nhk+1\n( 1\n\u03b3k+1 \u2212 1 \u03b3k\n)] + K\u2211\nk=Ks\n\u03b3k C\n2SK\n\u2264 n \u03bdSK\n[ C + K\u22121\u2211\nk=Ks\n2nC \u03bd(k + 1\u2212 k0) + 2n ( \u03bd 2n )\n] + K\u2211\nk=Ks\n2n \u03bd(k \u2212 k0) + 2n C 2SK\n= nC\n\u03bdSK\n[ 1 + K\u22121\u2211\nk=Ks\n1\nk + 1\u2212 k0 + 2n/\u03bd +\nK\u2211\nk=Ks\n1\nk \u2212 k0 + 2n/\u03bd\n]\n\u2264 nC \u03bdSK\n[ 1 + 2 K\u2211\nk=Ks\n1\nk \u2212 k0 + 2n/\u03bd\n]\n\u2264 2nC \u03bd(K + 2)\n[ 1 + 2 K\u2211\nk=Ks\n1\nk \u2212 k0 + 2n/\u03bd\n] , (28)\nwhere in the second line we used the faster convergence rate hk \u2264 2nC\u03bd(k\u2212k0)+2n from Theorem C.4, given that Ks \u2265 k0. In the last line, we used SK \u2264 0.5K + 1. The rest of the proof simply amounts to get an upper bound of \u03b2 = 3 on the term between brackets in (28), thus concluding that \u2211K k=0 \u03c1kgk \u2264 \u03b2 2nC\u03bd(K+2) . Then following a similar argument as in Theorem C.3, this will imply that there exists some gk\u0302 similarly upper bounded (the\nexistence part of the theorem); and that if g is convex, we have that E [ g(x\u0304\n(K) 0.5 )\n] is also similarly upper bounded.\nWe can upper bound the summand term in (28) by using the fact that for any non-negative decreasing integrable function f , we have \u2211K k=Ks f(k) \u2264 \u222bK Ks\u22121 f(t)dt. Let an := k0 \u2212 2n/\u03bd. Using f(k) := 1 k\u2212an , we have that\nK\u2211\nk=Ks\n1 k \u2212 an \u2264 \u222b K\nKs\u22121\n1\nt\u2212 an dt =\n[ log(t\u2212 an) ]t=K t=Ks\u22121\n= log K \u2212 an Ks \u2212 1\u2212 an \u2264 log K \u2212 an 0.5K \u2212 1\u2212 an =: b(K),\nwhere we used Ks \u2265 0.5K. We want to show that b(K) \u2264 1 for K \u2265 5k0 to conclude that \u03b2 = 3 works as a bound in (28) and thus completing the proof. By looking at the sign of the derivative of b(K), we can see that it is an increasing function of K if an \u2264 \u22122 i.e. if 2n/\u03bd \u2265 k0 + 2 (which is always the case if k0 = 0 as n \u2265 1), and a strictly decreasing function of K otherwise. In the case where b(K) is increasing, we have b(K) \u2264 limK 7\u2192\u221e b(K) = log(2) < 1. In the case where b(K) is decreasing, we upper bound it by letting K take its minimal value from the theorem, namely K \u2265 5k0. From the definition of an, we then get that b(5k0) = log\n4k0+2n/\u03bd 1.5k0\u22121+2n/\u03bd , which is an increasing function of k0 as long as 2n/\u03bd \u2265 2 (which is indeed always the\ncase). So letting k0 \u2192\u221e, we get that b(5k0) \u2264 log(4/1.5) \u2248 0.98 < 1, thus completing the proof. We finally note that statement for E [ g(x\u0304\n(K) 0.5 )\n] in Theorem C.3 can be proven using the same argument as\nabove, but with k0 = 0 and C = \u03bd \u22121C\u2297f (1 + \u03b4) + h0 and using the original primal convergence bound on hk in Theorem C.1 instead. This will work for both predefined step-size or the line search variants \u2014 the only place where we used the line-search in the above proof was to use the different primal convergence result as well as shifted-by-k0 step-sizes \u03b3k (which reduce to the standard step-sizes when k0 = 0).\nWe note that we cannot fully get rid of the dependence on h0 for the convergence rate of the expected duality gap of the weighted averaged scheme because we average over k < k0, a regime where the primal error depends on h0. With a more refined analysis for the weighted average with line-search scheme though, we note that one can replace the h0 n K dependence in the bound with a h0( n K )\n2 one, i.e. a quadratic speed-up to forget the initial conditions when line-search is used. We also note that a bound of O(1/K) can be derived similarly for E [ g(x\u0304 (K) \u00b5 ) ] for 0 < \u00b5 < 1 \u2014 namely using the C as in Theorem C.3 and \u03b2 = \u03b2\u00b5 := (1\u2212 \u00b5)\u22121(0.5\u2212 log\u00b5) (notice that \u03b2\u00b5 =\u221e if \u00b5 = 0 or \u00b5 = 1). This result is similar as the one for the stochastic subgradient method and where the O(1/K) rate was derived by Rakhlin et al. (2012) for the (1\u2212 \u00b5)-suffix averaging scheme \u2014 this provided a motivation for the scheme as the authors proved that the full averaging scheme has \u2126((logK)/K) rate in the worst case. If we use \u00b5 = 0 (i.e. we average from the beginning), then the sum in (28) becomes O(logK), yielding O((logK)/K) for the expected gap."}, {"heading": "D. Equivalence of the \u2018Linearization\u2019-Duality Gap to a Special Case of Fenchel Duality", "text": "For our used constrained optimization framework, the notion of the simple duality gap was crucial. Consider a general constrained optimization problem\nmin x\u2208M f(x) , (29)\nwhere the domain (or feasible set) M \u2286 X is an arbitrary compact subset of a Euclidean space X . We assume that the objective function f is convex, but not necessarily differentiable.\nIn this case, the general \u2018linearization\u2019 duality gap (5) as proposed by (Jaggi, 2013) is given by\ng(x; dx) = I \u2217 M(\u2212dx) + \u3008x, dx\u3009 . (30)\nHere dx is an arbitrary subgradient to f at the candidate position x, and I \u2217 M(y) := sups\u2208M \u3008s,y\u3009 is the support function of the set M. Convexity of f implies that the linearization f(x) + \u2329 s\u2212x, dx \u232a always lies below the graph of the function f , as illustrated by the figure in Section 3. This immediately gives the crucial property of the duality gap (30), as being a certificate for the current approximation quality, i.e. upper-bounding the (unknown) error g(x) \u2265 f(x)\u2212f(x\u2217), where x\u2217 is some optimal solution.\nNote that for differentiable functions f , the gradient is the unique subgradient at x, therefore the duality gap equals g(x) := g(x;\u2207f(x)) as we defined in (5).\nFenchel Duality. Here we will additionally explain how the duality gap (30) can also be interpreted as a special case of standard Fenchel convex duality.\nWe consider the equivalent formulation of our constrained problem (29), given by\nmin x\u2208X f(x) + IM(x) .\nHere the set indicator function IM of a subset M\u2286 X is defined as IM(x) := 0 for x \u2208 M and IM(x) := +\u221e for x /\u2208M. The Fenchel conjugate function f\u2217 of a function f is given by f\u2217(y) := supx\u2208X \u3008x,y\u3009 \u2212 f(x). For example, observe that the Fenchel conjugate of a set indicator function IM(.) is given by its support function I\u2217M(.). From the above definition of the conjugate, the Fenchel-Young inequality f(x)+f\u2217(y) \u2265 \u3008x,y\u3009 \u2200x,y \u2208 X follows directly.\nNow we consider the Fenchel dual problem of minimizing p(x) := f(x) + IM(x), which is defined as to maximize d(y) := \u2212f\u2217(y)\u2212 I\u2217M(\u2212y). By the Fenchel-Young inequality, and assuming that x \u2208M, we have that \u2200y \u2208 X ,\np(x)\u2212 d(y) = f(x)\u2212 (\u2212f\u2217(y)\u2212 I\u2217M(\u2212y)) \u2265 \u3008x,y\u3009+ I\u2217M(\u2212y) = g(x;y) .\nFurthermore, this inequality becomes an equality if and only if y is chosen as a subgradient to f at x, that is if y := \u2212dx. The last fact follows from the known equivalent characterization of the subdifferential in terms of the Fenchel conjugate: \u2202f(x) := {y \u2208 X | f(x) + f\u2217(y) = \u3008x,y\u3009}. For a more detailed explanation of Fenchel duality, we refer the reader to the standard literature, e.g. (Borwein & Lewis, 2006, Theorem 3.3.5).\nTo summarize, we have obtained that the simpler \u2018linearization\u2019 duality gap g(x; dx) as given in (30) is indeed the difference of the current objective to the Fenchel dual problem, when being restricted to the particular choice of the dual variable y being a subgradient at the current position x."}, {"heading": "E. Derivation of the n-Slack Structural SVM Dual", "text": "Proof of the dual of the n-Slack-Formulation. See also Collins et al. (2008). For a self-contained explanation of Lagrange duality we refer the reader to Boyd & Vandenberghe (2004, Section 5). The Lagrangian of (1) is\nL(w, \u03be,\u03b1) = \u03bb 2 \u3008w,w\u3009+ 1 n\nn\u2211\ni=1\n\u03bei + \u2211\ni\u2208[n],y\u2208Yi\n1 n \u03b1i(y) (\u2212\u03bei + \u3008w,\u2212\u03c8i(y)\u3009+ Li(y)) ,\nwhere \u03b1 = (\u03b11, . . . ,\u03b1n) \u2208 R|Y1| \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 R|Yn| = Rm are the corresponding (non-negative) Lagrange multipliers. Here we have re-scaled the multipliers (dual variables) by a constant of 1n , corresponding to multiplying the corresponding original primal constraint by 1n on both sides, which does not change the optimization problem.\nSince the objective as well as the constraints are continuously differentiable with respect to (w, \u03be), the Lagrangian L will attain its finite minimum over \u03b1 when \u2207(w,\u03be)L(w, \u03be,\u03b1) = 0. Making this saddle-point condition explicit results in a simplified Lagrange dual problem, which is also known as the Wolfe dual. In our case, this condition from differentiating w.r.t. w is\n\u03bbw = \u2211\ni\u2208[n],y\u2208Yi\n1 n \u03b1i(y)\u03c8i(y) . (31)\nAnd differentiating with respect to \u03bei and setting the derivatives to zero gives 8\n\u2211\ny\u2208Yi\n\u03b1i(y) = 1 \u2200i \u2208 [n] .\n8Note that because the Lagrangian is linear in \u03bei, if this condition is not satisfied, the minimization of the Lagrangian in \u03bei yield \u2212\u221e and so these points can be excluded.\nPlugging this condition and the expression (31) for w back into the Lagrangian, we obtain the Lagrange dual problem\nmax \u03b1 \u2212 \u03bb 2 \u2225\u2225\u2225\u2225\u2225\u2225 \u2211\ni\u2208[n],y\u2208Yi\n\u03b1i(y) \u03c8i(y)\n\u03bbn \u2225\u2225\u2225\u2225\u2225\u2225 2 + \u2211\ni\u2208[n],y\u2208Yi\n\u03b1i(y) Li(y)\nn\ns.t. \u2211\ny\u2208Y \u03b1i(y) = 1 \u2200i \u2208 [n], and \u03b1i(y) \u2265 0 \u2200i \u2208 [n], \u2200y \u2208 Yi ,\nwhich is exactly the negative of the quadratic program claimed in (4)."}, {"heading": "F. Additional Experiments", "text": "Complementing the results presented in Figure 1 in Section 6 of the main paper, here we provide additional experimental results as well as give more information about the experimental setup used.\nFor the Frank-Wolfe methods, Figure 2 presents results on OCR comparing setting the step-size by line-search against the simpler predefined step-size scheme of \u03b3k = 2n/(k + 2n). There, BCFW with predefined stepsizes does similarly as SSG, indicating that most of the improvement of BCFW with line-search over SSG is coming from the optimal step-size choice (and not from the Frank-Wolfe formulation on the dual). We also see that BCFW with predefined step-sizes can even do worse than batch Frank-Wolfe with line-search in the early iterations for small values of \u03bb.\nFigure 3 and Figure 4 show additional results of the stochastic solvers for several values of \u03bb on the OCR and CoNLL datasets. Here we also include the (uniformly) averaged stochastic subgradient method (SSG-avg), which starts averaging at the beginning; as well as the 0.5-suffix averaging versions of both SSG and BCFW (SSG-tavg and BCFW-tavg respectively), implemented using the \u2018doubling trick\u2019 as described just after Equation (15) in Appendix C. The \u2018doubling trick\u2019 uniformly averages all iterates since the last iteration which was a power of 2, and was described by Rakhlin et al. (2012), with experiments for SSG in Lacoste-Julien et al. (2012). In our experiments, BCFW-tavg sometimes slightly outperforms the weighted average scheme BCFW-wavg, but its performance fluctuates more widely, which is why we recommend the BCFW-wavg, as mentioned in the main text. In our experiments, the objective value of SSG-avg is always worse than the other stochastic methods (apart online-EG), which is why it was excluded from the main text. Online-EG performed substantially worse than the other stochastic solvers for the OCR dataset, and is therefore not included in the comparison for the other datasets.9\nFinally, Figure 5 presents additional results for the matching application from Taskar et al. (2006).\n9The worse performance of the online exponentiated gradient method could be explained by the fact that it uses a log-parameterization of the dual variables and so its iterates are forced to be in the interior of the probability simplex, whereas we know that the optimal solution for the structural SVM objective lies at the boundary of the domain and thus these parameters need to go to infinity.\nk+2n\nin the block-coordinate case respectively). See also the original optimization Algorithms 1 and 3.\nMore Information about Implementation. We note that since the value of the true optimum is unknown, the primal suboptimality for each experiment was measured as the difference to the highest dual objective seen for the corresponding regularization parameter (amongst all methods). Moreover, the lower envelope of the obtained primal objective values was drawn in Figure 1 for the batch methods (cutting plane and Frank-Wolfe), given that these methods can efficiently keep track of the best parameter seen so far.\nThe online-EG method used the same adaptive step-size scheme as described in Collins et al. (2008) and with the parameters from their code egstra-0.2 available online.10 Each datapoint has their own step-size, initialized at 0.5. Backtracking line-search is used, where the step-size is halved until the objective is decreased (or a maximum number of halvings has been reached: 2 for the first pass through the data; 5 otherwise). After each line-search, the step-size is multiplied by 1.05. We note that each evaluation of the objective requires a new call to the (expectation) oracle, and we count these extra calls in the computation of the effective number of passes appearing on the x-axis of the plots. Unlike all the other methods which initialize w(0) = 0, online-EG initially sets the dual variables \u03b1 (0) (i) to a uniform distribution, which yields a problem-dependent initialization w (0).\nFor SSG, we used the same step-size as in the \u2018Pegasos\u2019 version of Shalev-Shwartz et al. (2010a): \u03b3k := 1\n\u03bb(k+1) .\nFor the cutting plane method, we use the version 1.1 of the svm-struct-matlab MATLAB wrapper code from Vedaldi (2011) with its default options.\nThe test error for the OCR and CoNLL tasks is the normalized Hamming distance on the sequences.\nFor the matching prediction task, we use the same setting from Taskar et al. (2006), with 5, 000 training examples and 347 Gold test examples. During training, an asymmetric Hamming loss is used where the precision error cost is 1 while the recall error cost is 3. For testing, error is the \u2018alignment error rate\u2019, as defined in Taskar et al. (2006)."}, {"heading": "Supplementary References", "text": "Borwein, J. and Lewis, A. Convex analysis and nonlinear optimization: theory and examples. 2006.\nBoyd, S. and Vandenberghe, L. Convex optimization. 2004.\nLuo, Z Q and Tseng, P. On the convergence of the coordinate descent method for convex differentiable minimization. Journal of Optimization Theory and Applications, 72(1):7\u201335, 1992.\nPatriksson, M. Decomposition methods for differentiable optimization problems over cartesian product sets. Computational Optimization and Applications, 9(1):5\u201342, 1998.\nRichta\u0301rik, P. and Taka\u0301c\u030c, M. Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function. Technical Report 1107.2848v1 [math.OC], arXiv, 2011.\nTeo, C.H., Smola, A.J., Vishwanathan, SVN, and Le, Q.V. A scalable modular convex solver for regularized risk minimization. ACM SIGKDD, pp. 727\u2013736, 2007.\nVedaldi, A. A MATLAB wrapper of SVMstruct. http://www.vlfeat.org/~vedaldi/code/ svm-struct-matlab.html, 2011.\n10http://groups.csail.mit.edu/nlp/egstra/"}], "references": [{"title": "Convex analysis and nonlinear optimization: theory and examples", "author": ["A. Lewis"], "venue": null, "citeRegEx": "Borwein and Lewis,? \\Q2006\\E", "shortCiteRegEx": "Borwein and Lewis", "year": 2006}, {"title": "On the convergence of the coordinate descent method for convex differentiable minimization", "author": ["Z Q Luo", "P. Tseng"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Luo and Tseng,? \\Q1992\\E", "shortCiteRegEx": "Luo and Tseng", "year": 1992}, {"title": "Decomposition methods for differentiable optimization problems over cartesian product sets", "author": ["M. Patriksson"], "venue": "Computational Optimization and Applications,", "citeRegEx": "Patriksson,? \\Q1998\\E", "shortCiteRegEx": "Patriksson", "year": 1998}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": "Technical Report 1107.2848v1 [math.OC],", "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d,? \\Q2011\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d", "year": 2011}, {"title": "A scalable modular convex solver for regularized risk minimization", "author": ["C.H. Teo", "A.J. Smola", "SVN Vishwanathan", "Q.V. Le"], "venue": "ACM SIGKDD, pp. 727\u2013736,", "citeRegEx": "Teo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teo et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 2, "context": "2 has already been proposed in Patriksson (1998), using a generalization of Frank-Wolfe iterations under the name \u2018cost approximation\u2019.", "startOffset": 31, "endOffset": 49}, {"referenceID": 2, "context": "2 has already been proposed in Patriksson (1998), using a generalization of Frank-Wolfe iterations under the name \u2018cost approximation\u2019. The analysis of Patriksson (1998) shows asymptotic convergence, but since the method goes through the blocks sequentially, no convergence rates could be proven so far.", "startOffset": 31, "endOffset": 170}, {"referenceID": 4, "context": "(2009, Theorem 5, see their Equation (23)) or in the appendix of Teo et al. (2007). We can solve the recurrence (26) by following the argument of Teo et al.", "startOffset": 65, "endOffset": 83}, {"referenceID": 4, "context": "(2009, Theorem 5, see their Equation (23)) or in the appendix of Teo et al. (2007). We can solve the recurrence (26) by following the argument of Teo et al. (2007), where it was pointed out that since hk is monotonically decreasing, we can upper bound hk by the solution to the corresponding differential equations h\u2032(t) = \u2212h2(t)/\u03b6, with initial condition h(k0) = hk0 .", "startOffset": 65, "endOffset": 164}], "year": 2013, "abstractText": "We propose a randomized block-coordinate variant of the classic Frank-Wolfe algorithm for convex optimization with block-separable constraints. Despite its lower iteration cost, we show that it achieves a similar convergence rate in duality gap as the full FrankWolfe algorithm. We also show that, when applied to the dual structural support vector machine (SVM) objective, this yields an online algorithm that has the same low iteration complexity as primal stochastic subgradient methods. However, unlike stochastic subgradient methods, the block-coordinate FrankWolfe algorithm allows us to compute the optimal step-size and yields a computable duality gap guarantee. Our experiments indicate that this simple algorithm outperforms competing structural SVM solvers.", "creator": "LaTeX with hyperref package"}}}