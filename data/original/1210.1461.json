{"id": "1210.1461", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Oct-2012", "title": "A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and Tighter Bound", "abstract": "The CUR matrix decomposition is an important extension of Nystr\\\"{o}m approximation to a general matrix. It approximates any data matrix in terms of a small number of its columns and rows. In this paper we propose a novel randomized CUR algorithm with an expected relative-error bound. The proposed algorithm has the advantages over the existing relative-error CUR algorithms that it possesses tighter theoretical bound and lower time complexity, and that it can avoid maintaining the whole data matrix in main memory. Finally, experiments on several real-world datasets demonstrate significant improvement over the existing relative-error algorithms.", "histories": [["v1", "Thu, 4 Oct 2012 14:23:34 GMT  (105kb)", "http://arxiv.org/abs/1210.1461v1", "accepted by NIPS 2012"]], "COMMENTS": "accepted by NIPS 2012", "reviews": [], "SUBJECTS": "cs.LG cs.DM stat.ML", "authors": ["shusen wang", "zhihua zhang"], "accepted": true, "id": "1210.1461"}, "pdf": {"name": "1210.1461.pdf", "metadata": {"source": "CRF", "title": "A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and Tighter Bound\u2217", "authors": ["Shusen Wang", "Zhihua Zhang", "Jian Li"], "emails": ["wss@zju.edu.cn", "zhzhang@zju.edu.cn", "lijian@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n21 0.\n14 61\nv1 [\ncs .L\nKeywords: Large-scale matrix computations, low-rank matrix approximation, CUR matrix decomposition, randomized algorithms"}, {"heading": "1. Introduction", "text": "Large-scale matrices emerging from stocks, genomes, web documents, web images and videos everyday bring new challenges in modern data analysis. Most efforts have been focused on manipulating, understanding and interpreting large-scale data matrices. In many cases, matrix factorization methods are employed to construct compressed and informative representations to facilitate computation and interpretation. A principled approach is the truncated singular value decomposition (SVD) which finds the best low-rank approximation of a data matrix. Applications of SVD such as eigenface (Sirovich and Kirby, 1987, Turk and Pentland, 1991) and latent semantic analysis (Deerwester et al., 1990) have been illustrated to be very successful.\nHowever, the basis vectors resulting from SVD have little concrete meaning, which makes it very difficult for us to understand and interpret the data in question. An example\n\u2217. An extended abstract of this paper has been accepted by NIPS 2012.\nin (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age\u2212 (1/ \u221a 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people\u2019s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: \u201cit would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.\u201d Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix.\nThe CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that A\u0303 = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1.\nThe CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound. Later on, Drineas et al. (2008) devised randomized CUR algorithms with relative error by sampling sufficiently many columns and rows. Particularly, the algorithm has (1 + \u01eb) relative-error ratio with high probability (w.h.p.). Recently, Mackey et al. (2011) established a divide-and-conquer method which solves the CUR problem in parallel.\nUnfortunately, all the existing CUR algorithms require a large number of columns and rows to be chosen. For example, for an m\u00d7 n matrix A and a target rank k \u2264 min{m,n}, the state-of-the-art CUR algorithm \u2014 the subspace sampling algorithm in Drineas et al. (2008) \u2014 requires exactly O(k4\u01eb\u22126) rows or O(k\u01eb\u22124 log2 k) rows in expectation to achieve (1 + \u01eb) relative-error ratio w.h.p. Moreover, the computational cost of this algorithm is at least the cost of the truncated SVD of A, that is, O(min{mn2, nm2}).1 The algorithms are therefore impractical for large-scale matrices.\nIn this paper we develop a CUR algorithm which beats the state-of-the-art algorithm in both theory and experiments. In particular, we show in Theorem 9 a novel randomized CUR algorithm with lower time complexity and tighter theoretical bound in comparison with the state-of-the-art CUR algorithm in Drineas et al. (2008).\nThe rest of this paper is organized as follows. Section 2 lists some notations that will be used in this paper and Section 3 reviews two classes of CUR algorithms. Section 4 mainly introduces a column selection algorithm to which our work is closely related. Section 5 describes and analyzes our novel CUR algorithm. Section 6 empirically compares our proposed algorithm with the state-of-the-art algorithm. All proofs are deferred to Appendix B.\n1. Although some partial SVD algorithms, such as Krylov subspace methods, require only O(mnk) time, they are all numerical unstable. See Halko et al. (2011) for more discussions."}, {"heading": "2. Notations", "text": "For a matrix A = [aij ] \u2208 Rm\u00d7n, let a(i) be its i-th row and aj be its j-th column. Let \u2016A\u20161 = \u2211 i,j |aij | be the \u21131-norm, \u2016A\u2016F = ( \u2211 i,j a 2 ij)\n1/2 be the Frobenius norm, and \u2016A\u20162 = max\u2016x\u20162=1 \u2016Ax\u20162 be the spectral norm. Moreover, let Im denote the m\u00d7m identity matrix, and 0 denotes the zero matrix whose size dependents on the context. Let \u03c1 = rank(A) and k \u2264 \u03c1, the SVD of A can be written as\nA =\n\u03c1 \u2211\ni=0\n\u03c3A,iuA,iv T A,i = UA\u03a3AV T A = UA,k\u03a3A,kV T A,k +UA,k\u22a5\u03a3A,k\u22a5V T A,k\u22a5,\nwhere UA,k, \u03a3A,k, and VA,k correspond to the top k singular values. We denote Ak = UA,k\u03a3A,kV T A,k. Furthermore, let A \u2020 = UA,\u03c1\u03a3 \u22121 A,\u03c1V T A,\u03c1 be the Moore-Penrose inverse of A (Ben-Israel and Greville, 2003).\nGiven matrices A \u2208 Rm\u00d7n, X \u2208 Rm\u00d7p, and Y \u2208 Rq\u00d7n, XX\u2020A = UXUTXA \u2208 Rm\u00d7n is the projection of A onto the column space of X, and AY\u2020Y = AVYV T Y\n\u2208 Rm\u00d7n is the projection of A onto the row space of Y. Finally, given an integer k \u2264 p, we define the matrix \u03a0X,k(A) \u2208 Rm\u00d7n as the best approximation to A within the column space of X that has rank at most k. We have \u03a0X,k(A) = XZ\u0302 where Z\u0302 = argminrank(Z)\u2264k \u2016A\u2212XZ\u2016F . We also have that \u2016A\u2212XX\u2020A\u2016F \u2264 \u2016A\u2212\u03a0X,k(A)\u2016F ."}, {"heading": "3. Previous Work in CUR Matrix Decomposition", "text": "This section discusses two recent developments of the CUR algorithms. Section 3.1 introduces an additive-error CUR algorithm in Drineas et al. (2006), and Section 3.2 describes two relative-error CUR algorithms in Drineas et al. (2008)."}, {"heading": "3.1 The Linear-Time CUR Algorithm", "text": "The linear-time CUR algorithm is proposed by Drineas et al. (2006). It is a highly efficient algorithm. Given a matrix A and a constant k < rank(A), by sampling c = 64k\u01eb\u22124 columns and r = 4k\u01eb\u22122 rows of A and computing an intersection matrix U, the resulting CUR decomposition satisfies the following additive-error bound\nE\u2016A\u2212CUR\u2016F \u2264 \u2016A\u2212Ak\u2016F + \u01eb\u2016A\u2016F .\nFurthermore, the decomposition also satisfies rank(CUR) \u2264 k. Here we give its main results (Theorem 4 of Drineas et al., 2006) in the following proposition.\nProposition 1 (The Linear-Time CUR Algorithm) Given a matrix A \u2208 Rm\u00d7n, we let pi = \u2016a(i)\u201622/\u2016A\u20162F and qj = \u2016aj\u201622/\u2016A\u20162F . The linear-time CUR algorithm randomly samples c columns of A with probabilities {qj}nj=1 and r rows of A with probabilities {pi}mi=1. Then\nE\u2016A\u2212CUR\u2016F \u2264 \u2016A\u2212Ak\u2016F + ( (4k/c)1/4 + (k/r)1/2 ) \u2016A\u2016F .\nThe algorithm costs O(mc2 +nr+ c2r+ c3) time, which is linear in (m+ n) by assuming c and r are constants."}, {"heading": "3.2 The Subspace Sampling CUR Algorithm", "text": "Drineas et al. (2008) proposed a two-stage randomized CUR algorithm which has a relativeerror bound w.h.p. In the first stage the algorithm samples c columns of A to construct C, and in the second stage it samples r rows from A and C simultaneously to construct R and U\u2020. In the first stage the sampling probabilities are proportional to the squared \u21132-norm of the rows of VA,k, in the second stage the sampling probabilities are proportional to the squared \u21132-norm of the rows of UC,k. That is why it is called the \u201csubspace sampling algorithm\u201d. Here we show the main results of the subspace sampling algorithms in the following proposition.\nProposition 2 (The Subspace Sampling CUR Algorithm) Given a matrix A \u2208 Rm\u00d7n and an integer k \u226a min{m,n}, the subspace sampling algorithm uses exactly sampling to select exactly c = O(k2\u01eb\u22122 log(1/\u03b4)) columns of A to construct C, and then exactly r = O(c2\u01eb\u22122 log(1/\u03b4)) rows of A to construct R, or uses expected sampling to select c = O(k\u01eb\u22122 log k log(1/\u03b4)) columns and r = O(c\u01eb\u22122 log c log(1/\u03b4)) rows in expectation. Then with probability at least (1\u2212 \u03b4),\n\u2016A\u2212CUR\u2016F \u2264 (1 + \u01eb)\u2016A\u2212Ak\u2016F .\nHere, the matrix U is a weighted Moore-Penrose inverse of the intersection between C and R. The running time of both algorithms is dominated by the truncated SVD of A.\nAlthough the algorithm is \u01eb-optimal with high probability, it requires too many rows get chosen: at least r = O(k\u01eb\u22124 log2 k) rows in expectation. In this paper we seek to devise an algorithm with mild requirement on column and row numbers."}, {"heading": "4. Theoretical Backgrounds", "text": "Section 4.1 considers the connections between the column selection problem and the CUR matrix decomposition problem. Section 4.2 introduces a near-optimal relative-error column selection algorithm. Our proposed CUR algorithm is motivated by and partly based on the near-optimal column selection algorithm."}, {"heading": "4.1 Connections between Column Selection and CUR Matrix Decomposition", "text": "Column selection is a well-established problem which has been widely studied in the literature: (Frieze et al., 2004, Deshpande et al., 2006, Drineas et al., 2008, Deshpande and Rademacher, 2010, Boutsidis et al., 2011b, Guruswami and Sinop, 2012).\nGiven a matrix A \u2208 Rm\u00d7n, column selection aims to choose c columns of A to construct C \u2208 Rm\u00d7c so that \u2016A \u2212 CC\u2020A\u2016F achieves the minimum. Since there are (nc ) possible choices of constructing C, so selecting the best subset is a hard problem. In recent years, many polynomial-time approximate algorithms have been proposed, among which we are particularly interested in those algorithms with relative-error bounds; that is, with c \u2265 k columns selected from A, there is a constant \u03b7 such that\n\u2016A\u2212CC\u2020A\u2016F \u2264 \u03b7\u2016A\u2212Ak\u2016F .\nWe call \u03b7 the relative-error ratio. For some randomized algorithms, the inequality holds either w.h.p. or in expectation w.r.t. C.\nThe CUR matrix decomposition problem has a close connection with the column selection problem. As aforementioned, the first stage of existing CUR algorithms is simply a column selection procedure. However, the second stage is more complicated. If the second stage is na\u0308\u0131vely solved by a column selection algorithm on AT , then the error ratio will trivially be 2\u03b7.\nFor a relative-error CUR algorithm, the first stage seeks to bound a construction error\nratio of \u2016A\u2212CC \u2020 A\u2016F \u2016A\u2212Ak\u2016F , while the section stage seeks to bound \u2016A\u2212CC \u2020 AR \u2020 R\u2016F \u2016A\u2212CC\u2020A\u2016F given C. Actually, the first stage is a special case of the second stage where C = Ak. Given a matrix A, if an algorithm solving the second stage results in a bound \u2016A\u2212CC \u2020AR\u2020R\u2016F\n\u2016A\u2212CC\u2020A\u2016F \u2264 \u03b7, then this\nalgorithm also solves the column selection problem for AT with an \u03b7 relative-error ratio. Thus the second stage of CUR is a generalization of the column selection problem."}, {"heading": "4.2 The Near-Optimal Column Selection Algorithm", "text": "Recently, Boutsidis et al. (2011a) proposed a randomized algorithm which selects only c = 2k\u01eb\u22121(1 + o(1)) columns to achieve the expected relative-error ratio (1 + \u01eb). Boutsidis et al. (2011a) also proved the lower bound of the column selection problem; that is, at least c = k\u01eb\u22121 columns are selected to achieve the (1 + \u01eb) ratio. Thus this algorithm is near optimal. Though an optimal algorithm recently proposed by Guruswami and Sinop (2012) achieves the the lower bound, the optimal algorithm is quite inefficient compared with the near-optimal algorithm.\nThe near-optimal algorithm has three steps: the approximate SVD via random projection (Halko et al., 2011), the dual set sparsification algorithm (Boutsidis et al., 2011a), and the adaptive sampling algorithm (Deshpande et al., 2006). Here we present the main results of this algorithm in Lemma 3. To better understand the algorithm, we also give the details of the three steps, respectively.\nLemma 3 (Near-Optimal Column Selection Algorithm) Given a matrix A \u2208 Rm\u00d7n of rank \u03c1, a target rank k (2 \u2264 k < \u03c1), and 0 < \u01eb < 1, there exists a randomized algorithm to select at most\nc = 2k\n\u01eb\n( 1 + o(1) )\ncolumns of A to form a matrix C \u2208 Rm\u00d7c such that\nE 2\u2016A\u2212CC\u2020A\u2016F \u2264 E\u2016A\u2212CC\u2020A\u20162F \u2264 (1 + \u01eb)\u2016A\u2212Ak\u20162F ,\nwhere the expectations are taken w.r.t. C. Furthermore, the matrix C can be obtained in O((mnk + nk3)\u01eb\u22122/3).\nThe dual set sparsification algorithm requires the top k right singular vectors of A as inputs. Since SVD is time consuming, Boutsidis et al. (2011a) employed an approximation SVD algorithm (Halko et al., 2011) to speedup computation. We give the theoretical analysis of the approximation SVD via random projection in Lemma 4. The resulting matrix Z approximates VA,k.\nLemma 4 (Randomized SVD via Random Projection) Given a matrix A \u2208 Rm\u00d7n of rank \u03c1, a target rank k (k < \u03c1), and 0 < \u01eb0 < 1, the algorithm computes a factorization A = BZT +E with B = AZ, ZTZ = Ik, and EZ = 0 such that\nE\u2016E\u20162F \u2264 (1 + \u01eb0)\u2016A\u2212Ak\u20162F .\nThe algorithm runs in O(mnk\u01eb\u221210 ) time.\nThe second step of the near-optimal column selection algorithm is the dual set sparsification proposed by Boutsidis et al. (2011a). When ones take A and the top k (approximate) right singular vectors of A as inputs, the dual set sparsification algorithm can deterministically selects c1 columns of A to construct C1. We present their results in Lemma 5 and attach the concrete algorithm in Appendix A.\nLemma 5 (Column Selection via Dual Set Sparsification Algorithm) Given a matrix A \u2208 Rm\u00d7n of rank \u03c1 and a target rank k (< \u03c1), the dual set spectral-Frobenius sparsification algorithm deterministically selects c1 (> k) columns of A to form a matrix C1 \u2208 Rm\u00d7c1 such that\n\u2225 \u2225 \u2225 A\u2212\u03a0C1,k(A) \u2225 \u2225 \u2225\nF \u2264\n\u221a\n1 + 1\n(1\u2212 \u221a k/c1)2\n\u2225 \u2225 \u2225 A\u2212Ak \u2225 \u2225 \u2225\nF .\nMoreover, the matrix C1 can be computed in TVA,k + O(mn + nc1k2), where TVA,k is the time needed to compute the top k right singular vectors of A.\nAfter sampling c1 columns of A, the near-optimal column selection algorithm uses the adaptive sampling of Deshpande et al. (2006) to select c2 columns of A to further reduce the construction error. We present Theorem 2.1 in Deshpande et al. (2006) in the following lemma.\nLemma 6 (The Adaptive Sampling Algorithm) Given a matrix A \u2208 Rm\u00d7n, we let C1 \u2208 Rm\u00d7c1 consists of c1 columns of A, and define the residual B = A\u2212C1C\u20201A. Additionally, for i = 1, \u00b7 \u00b7 \u00b7 , n, we define\npi = \u2016bi\u201622/\u2016B\u20162F .\nWe further sample c2 columns i.i.d. from A, in each trial of which the i-th column is chosen with probability pi. Let C2 \u2208 Rm\u00d7c2 contain the c2 sampled rows and let C = [C1,C2] \u2208 R m\u00d7(c1+c2). Then, for any integer k > 0, the following inequality holds:\nE\u2016A\u2212CC\u2020A\u20162F \u2264 \u2016A\u2212Ak\u20162F + k\nr2 \u2016A\u2212C1C\u20201A\u20162F ,\nwhere the expectation is taken w.r.t. C2.\nAlgorithm 1 The Fast CUR Algorithm.\n1: Input: a real matrixA \u2208 Rm\u00d7n, target rank k, \u01eb \u2208 (0, 1], target column number c = 2k\u01eb ( 1+o(1) ) ,\ntarget row number r = 2c\u01eb ( 1 + o(1) )\n; 2: // Stage 1: select c columns of A to construct C \u2208 Rm\u00d7c 3: Compute approximate truncated SVD via random projection such that Ak \u2248 U\u0303k\u03a3\u0303kV\u0303k; 4: Construct U1 \u2190 columns of (A\u2212 U\u0303k\u03a3\u0303kV\u0303k); V1 \u2190 columns of V\u0303Tk ; 5: Compute s1 \u2190 Dual Set Spectral-Frobenius Sparsification Algorithm (U1, V1, c\u2212 2k/\u01eb); 6: Construct C1 \u2190 ADiag(s1), and then delete the all-zero columns; 7: Residual matrix D \u2190 A\u2212C1C\u20201A; 8: Compute sampling probabilities: pi = \u2016di\u201622/\u2016D\u20162F , i = 1, \u00b7 \u00b7 \u00b7 , n; 9: Sampling c2 = 2k/\u01eb columns from A with probability {p1, \u00b7 \u00b7 \u00b7 , pn} to construct C2; 10: // Stage 2: select r rows of A to construct R \u2208 Rr\u00d7n 11: Construct U2 \u2190 columns of (A\u2212 U\u0303k\u03a3\u0303kV\u0303k)T ; V2 \u2190 columns of U\u0303Tk ; 12: Compute s2 \u2190 Dual Set Spectral-Frobenius Sparsification Algorithm (U2, V2, r \u2212 2c/\u01eb); 13: Construct R1 \u2190 Diag(s2)A, and then delete the all-zero rows; 14: Residual matrix B \u2190 A\u2212AR\u20201R1; 15: Compute sampling probabilities: qj = \u2016b(j)\u201622/\u2016B\u20162F , j = 1, \u00b7 \u00b7 \u00b7 ,m; 16: Sampling r2 = 2c/\u01eb rows from A with probability {q1, \u00b7 \u00b7 \u00b7 , qm} to construct R2; 17: return C = [C1,C2], R = [R T 1 ,R T 2 ] T , and U = C\u2020AR\u2020."}, {"heading": "5. Main Results", "text": "In this section we develop a novel CUR algorithm that we call the fast CUR algorithm due to its lower time complexity in comparison with SVD. We describe the procedure in Algorithm 1 and give theoretical analysis in Theorem 9.\nThe main results of our work are formally shown in three theorems in this section. The proofs are deferred to Appendix B. Theorem 9 relies on Lemma 3 and Theorem 8, and Theorem 8 relies on Theorem 7. Theorem 7 is a generalization of Lemma 6, and Theorem 8 is a generalization of Lemma 3."}, {"heading": "5.1 Adaptive Sampling", "text": "The relative-error adaptive sampling algorithm is established in Theorem 2.1 of Deshpande et al. (2006). The algorithm is based on the following idea: after selecting a proportion of columns from A to form C1 by an arbitrary algorithm, the algorithms randomly samples additional c2 columns according to the residual A \u2212 C1C\u20201A. Boutsidis et al. (2011a) used the adaptive sampling algorithm to decrease the residual of the dual set sparsification algorithm and obtained an (1 + \u01eb) relative-error ratio. Here we prove a new bound for the same adaptive sampling algorithm. Interestingly, this new bound is a generalization of the original one in Theorem 2.1 of Deshpande et al. (2006). In other words, Theorem 2.1 of Deshpande et al. (2006) is a direct corollary of our following theorem when C = Ak.\nTheorem 7 (The Adaptive Sampling Algorithm) Given a matrix A \u2208 Rm\u00d7n and a matrix C \u2208 Rm\u00d7c such that rank(C) = rank(CC\u2020A) = \u03c1 (\u03c1 \u2264 c \u2264 n), we let R1 \u2208 Rr1\u00d7n consist of r1 rows of A, and define the residual B = A \u2212 AR\u20201R1. Additionally, for i = 1, \u00b7 \u00b7 \u00b7 ,m, we define\npi = \u2016b(i)\u201622/\u2016B\u20162F .\nWe further sample r2 rows i.i.d. from A, in each trial of which the i-th row is chosen with probability pi. Let R2 \u2208 Rr2\u00d7n contain the r2 sampled rows and let R = [RT1 ,RT2 ]T \u2208 R (r1+r2)\u00d7n. Then the following inequality holds:\nE\u2016A\u2212CC\u2020AR\u2020R\u20162F \u2264 \u2016A\u2212CC\u2020A\u20162F + \u03c1\nr2 \u2016A\u2212AR\u20201R1\u20162F ,\nwhere the expectation is taken w.r.t. R2."}, {"heading": "5.2 The Fast CUR Algorithm", "text": "Based on the randomized SVD algorithm of Lemma 4, the dual set sparsification algorithm of Lemma 5, and the adaptive sampling algorithm of Theorem 7, we develop a randomized algorithm to solve the second stage of the CUR problem. We present the results of the algorithm in the following theorem.\nTheorem 8 (The Fast Row Selection Algorithm) Given a matrix A \u2208 Rm\u00d7n and a matrix C \u2208 Rm\u00d7c such that rank(C) = rank(CC\u2020A) = \u03c1 (\u03c1 \u2264 c \u2264 n), and a target rank k (\u2264 \u03c1), the proposed randomized algorithm selects r = 2\u03c1\u01eb (1 + o(1)) rows of A to construct R \u2208 Rr\u00d7n, such that\nE\u2016A\u2212CC\u2020AR\u2020R\u20162F \u2264 \u2016A\u2212CC\u2020A\u20162F + \u01eb\u2016A\u2212Ak\u20162F ,\nwhere the expectation is taken w.r.t. R. Furthermore, the matrix R can be computed in O((mnk +mk3)\u01eb\u22122/3) time.\nNote that Lemma 3, i.e., Theorem 5 of Boutsidis et al. (2011a), is a special case of Theorem 8 when C = Ak. Based on Lemma 3 and Theorem 8, we have the main theorem for the fast CUR algorithm as follows.\nTheorem 9 (The Fast CUR Algorithm) Given a matrix A \u2208 Rm\u00d7n and a positive integer k \u226a min{m,n}, the fast CUR algorithm described in Algorithm 1 randomly selects c = 2k\u01eb (1+o(1)) columns of A to construct C \u2208 Rm\u00d7c with the near-optimal column selection algorithm of Lemma 3, and then selects r = 2c\u01eb (1 + o(1)) rows of A to construct R \u2208 Rr\u00d7n with the fast row selection algorithm of Theorem 8. Then we have\nE\u2016A\u2212CUR\u2016F = E\u2016A\u2212C(C\u2020AR\u2020)R\u2016F \u2264 (1 + \u01eb)\u2016A\u2212Ak\u2016F .\nMoreover, the algorithm runs in time O ( mnk\u01eb\u22122/3 + (m+ n)k3\u01eb\u22122/3 +mk2\u01eb\u22122 + nk2\u01eb\u22124 ) .\nSince k, c, r \u226a min{m,n} by the assumption, so the time complexity of the fast CUR algorithm is lower than that of the SVD of A. This is the main reason why we call it the fast CUR algorithm.\nAnother advantage of this algorithm is that it can avoid loading the whole m\u00d7 n data matrix A into main memory. None of three steps \u2014 the randomized SVD, the dual set sparsification algorithm, and the adaptive sampling \u2014 requires loading the whole of A into memory. The most memory-expensive operation throughout the fast CUR Algorithm is computing the Moore-Penrose inverses of C and R, which requires maintaining an m \u00d7 c matrix or an r\u00d7n matrix in memory. In contrast, the subspace sampling algorithm requires loading the whole matrix into memory to compute its truncated SVD."}, {"heading": "6. Empirical Analysis", "text": "In this section we conduct empirical comparisons among the relative-error CUR algorithms on several datasets. We report the relative-error ratio and the running time of each algorithm on each data set. The relative-error ratio is defined by\nRelative-error ratio = \u2016A\u2212CUR\u2016F \u2016A\u2212Ak\u2016F ,\nwhere k is a specified target rank."}, {"heading": "6.1 Datasets", "text": "We implement experiments on five datasets, including natural images, biology data, and bags of words. Table 1 briefly summarizes some information of the datasets. The Redrock and Edinburgh (Agarwala, 2007) are two large size natural images. Arcene and Dexter are both from the UCI datasets (Frank and Asuncion, 2010). Arcene is a biology dataset with 900 instances and 10000 attributes. Dexter is a bag of words dataset with a 20000- vocabulary and 2600 documents. PicasaWeb image dataset (Wang et al., 2012) contains 6.8 million PicasaWeb images. We use the HMAX features (Serre et al., 2007) and the SIFT features (Lowe, 1999) of the first 50000 images; the features provided by Wang et al. (2012) are all of 3000 dimensions. Each dataset is actually represented as a data matrix, upon which we apply the CUR algorithms.\nWhen the data matrices become very large, e.g., say 8K \u00d7 3K, the truncated SVD and the standard SVD are both infeasible in our experiment environment, and so is the subspace sampling algorithm. Therefore we do not conduct experiments on larger data matrices. In contrast, our fast CUR algorithm actually works well even for 30K \u00d7 3K matrices."}, {"heading": "6.2 Setup", "text": "We implement the subspace sampling algorithm and our fast CUR algorithm in MATLAB 7.10.0. We do not compare with the linear-time CUR algorithm for the following reason. There is an implicit projection operation in the linear-time CUR algorithm, so the result satisfies rank(CUR) \u2264 k. However, this inequality does not hold for the subspace sampling algorithm and the fast CUR algorithm. Thus, comparing the construction error among the three CUR algorithm is very unfair for the linear-time CUR algorithm. Actually, the construction error of the linear-time CUR algorithm is much worse than the other two algorithms.\nWe conduct experiments on a workstation with 12 Intel Xeon 3.47GHz CPUs, 12GB memory, and Ubuntu 10.04 system. According to the analysis in Drineas et al. (2008) and this paper, k, c, and r should be integers much less than m and n. For each data set and each algorithm, we set k = 10, 20, or 50, and c = \u03b1k, r = \u03b1c, where \u03b1 ranges in each set of experiments. We repeat each set of experiments for 20 times and report the average and the standard deviation of the error ratios. The results are depicted in Figures 1, 2, 3, 4, 5, and 6."}, {"heading": "6.3 Result Analysis", "text": "The results show that the fast CUR algorithm has much lower relative-error ratio than the subspace sampling algorithm. The experimental results well match our theoretical analyses in Section 5. As for the running time, the fast CUR algorithm is more efficient when c and r are small. When c and r become large, the fast CUR algorithm becomes less efficient. This is because the time complexity of the fast CUR algorithm is linear in \u01eb\u22124 and large c and r imply small \u01eb. However, the purpose of CUR is to select a small number of columns and rows from the data matrix, that is, c \u226a n and r \u226a m. Thus we are not interested in the cases where c and r are large compared with m and n, e.g., say k = 20 and \u03b1 = 10."}, {"heading": "7. Discussions", "text": "In this paper we have proposed a novel randomized algorithm for the CUR matrix decomposition problem. This algorithm is faster, more scalable, and more accurate than the state-of-the-art algorithm, i.e., the subspace sampling algorithm. Our algorithm requires only c = 2k\u01eb\u22121(1+o(1)) columns and r = 2c\u01eb\u22121(1+o(1)) rows to achieve (1+\u01eb) relative-error ratio. To achieve the same relative-error bound, the subspace sampling algorithm requires c = O(k\u01eb\u22122 log k) columns and r = O(c\u01eb\u22122 log c) rows selected from the original matrix. Our algorithm also beats the subspace sampling algorithms in time-complexity. Our algorithm costs O(mnk\u01eb\u22122/3 + (m + n)k3\u01eb\u22122/3 +mk2\u01eb\u22122 + nk2\u01eb\u22124) time, which is lower than O(min{mn2,m2n}) of the subspace sampling algorithms when k is small. Moreover, our algorithm enjoys another advantage of avoiding loading the whole data matrix into main memory, which also makes our algorithm more scalable. Finally, the empirical comparisons have also demonstrated the effectiveness and efficiency of our algorithm.\nHowever, there are several open questions involving the lower bound of the CUR matrix decomposition problem. First, what is the lower bound for the CUR problem? Second, is there any algorithm achieving such a lower bound? Boutsidis et al. (2011b) proved a lower bound for the column selection problem: \u2016A\u2212CC\u2020A\u20162F \u2016A\u2212Ak\u2016 2\nF\n\u2265 1+ kc . We thus wonder if there is a\nsimilar lower bound on the ratio \u2016A\u2212CC\u2020AR\u2020R\u20162F\n\u2016A\u2212CC\u2020A\u20162 F\n, e.g., say (1 + rank(C)r ). We shall address\nthese questions in future work.\nAcknowledgments\nAlgorithm 2 Deterministic Dual Set Spectral-Frobenius Sparsification Algorithm.\n1: Input: U = {xi}ni=1 \u2282 Rl, (l < n); V = {vi}ni=1 \u2282 Rk, with \u2211n i=1 viv T i = Ik (k < n); k < r < n; 2: Initialize: s0 = 0, A0 = 0; 3: Compute \u2016xi\u201622 for i = 1, \u00b7 \u00b7 \u00b7 , n, and then compute \u03b4U = \u2211 n i=1 \u2016xi\u2016 2 2\n1\u2212 \u221a k/r ;\n4: for \u03c4 = 0 to r \u2212 1 do 5: Compute the eigenvalue decomposition of A\u03c4 ; 6: Find an index j in {1, \u00b7 \u00b7 \u00b7 , n} and compute a weight t > 0 such that\n\u03b4\u22121U \u2016xj\u201622 \u2264 t\u22121 \u2264 vTj\n( A\u03c4 \u2212 (L\u03c4 + 1)Ik )\u22122 vj\n\u03c6(L\u03c4 + 1,A\u03c4 )\u2212 \u03c6(L\u03c4 ,A\u03c4 ) \u2212 vTj\n( A\u03c4 \u2212 (L\u03c4 + 1)Ik )\u22121 vj ;\nwhere\n\u03c6(L,A) =\nk \u2211\ni=1\n( \u03bbi(A)\u2212 L )\u22121 , L\u03c4 = \u03c4 \u2212 \u221a rk;\n7: Update the j-th component of s\u03c4 and A\u03c4 : s\u03c4+1[j] = s\u03c4 [j] + t, A\u03c4+1 = A\u03c4 + tvjv T j ; 8: end for 9: return s = 1\u2212 \u221a k/r\nr sr.\nThis work has been supported in part by the Natural Science Foundations of China (No. 61070239) and the Google visiting faculty program."}, {"heading": "Appendix A. The Dual Set Sparsification Algorithm", "text": "For the sake of completeness, we attach the dual set sparsification algorithm here and describe some implementation details. The dual set sparsification algorithms are deterministic algorithms established in Boutsidis et al. (2011a). The fast CUR algorithm calls the dual set spectral-Frobenius sparsification algorithm (Lemma 13 in Boutsidis et al., 2011a) in both stages. We show this algorithm in Algorithm 2 and its bounds in Lemma 10.\nLemma 10 (Dual Set Spectral-Frobenius Sparsification) Let U = {x1, \u00b7 \u00b7 \u00b7 ,xn} \u2282 R l (l < n) contain the columns of an arbitrary matrix X \u2208 Rl\u00d7n. Let V = {v1, \u00b7 \u00b7 \u00b7 ,vn} \u2282 Rk (k < n) be a decompositions of the identity, i.e., \u2211n\ni=1 viv T i = Ik. Given an integer r with\nk < r < n, Algorithm 2 deterministically computes a set of weights si \u2265 0 (i = 1, \u00b7 \u00b7 \u00b7 , n) at most r of which are non-zero, such that\n\u03bbk\n( n \u2211\ni=1\nsiviv T i\n) \u2265 ( 1\u2212 \u221a k\nr\n)2 and tr (\nn \u2211\ni=1\nsixix T i\n)\n\u2264 \u2016X\u20162F .\nThe weights si can be computed deterministically in O(rnk2 + nl) time.\nHere we would like to mention the implementation of Algorithm 2, which is not described by Boutsidis et al. (2011a) in details. In each iteration the algorithm performs once eigenvalue decomposition: A\u03c4 = W\u039bW\nT . Here A\u03c4 is guaranteed to be positive semi-definite in each iteration. Since\n( A\u03c4 \u2212 \u03b1Ik )q = WDiag ( (\u03bb1 \u2212 \u03b1)q, \u00b7 \u00b7 \u00b7 , (\u03bbk \u2212 \u03b1)q ) WT ,\nwe can efficiently compute (A\u03c4 \u2212 (L\u03c4 +1)Ik)q based on the eigenvalue decomposition of A\u03c4 . With the eigenvalues at hand, \u03c6(L,A\u03c4 ) can also be computed directly.\nThe algorithm runs in r iterations. In each iteration, the eigenvalue decomposition of A\u03c4 requires O(k3), and the n comparisons in Line 6 each requires O(k2). Moreover, computing \u2016xi\u201622 for each xi requires O(nl). Overall, the running time of Algorithm 2 is at most O(rk3) +O(rnk2) +O(nl) = O(rnk2 + nl)."}, {"heading": "Appendix B. Proofs", "text": "B.1 The Proof of Theorem 7\nTheorem 7 can be equivalently expressed in Theorem 11. In order to stick to the column space convention of Boutsidis et al. (2011a), we prove Theorem 11 instead of Theorem 7.\nTheorem 11 (Adaptive Sampling Algorithm) Given a matrix A \u2208 Rm\u00d7n and a matrix R \u2208 Rr\u00d7n such that rank(R) = rank(AR\u2020R) = \u03c1 (\u03c1 \u2264 r \u2264 m), let C1 \u2208 Rm\u00d7c1 consist of c1 columns of A, and define the residual B = A\u2212C1C\u20201A. For i = 1, \u00b7 \u00b7 \u00b7 , n, let\npi = \u2016bi\u201622/\u2016B\u20162F ,\nwhere bi is the i-th column of the matrix B. Sample further c2 columns from A in c2 i.i.d. trials, where in each trial the i-th column is chosen with probability pi. Let C2 \u2208 Rm\u00d7c2 contain the c2 sampled columns and C = [C1,C2] \u2208 Rm\u00d7(c1+c2) contain the columns of both C1 and C2, all of which are columns of A. Then the following inequality holds:\nE\u2016A\u2212CC\u2020AR\u2020R\u20162F \u2264 \u2016A\u2212AR\u2020R\u20162F + \u03c1\nc2 \u2016A\u2212C1C\u20201A\u20162F .\nwhere the expectation is taken w.r.t. C2.\nProof With a little abuse of symbols, we use bold uppercase letters to denote matrix random variables and bold lowercase to denote vector random variables, without distinguishing between matrix/vector random variables and constant matrices/vectors.\nWe denote the j-th column of VAR\u2020R,\u03c1 \u2208 Rn\u00d7\u03c1 as vj, and the (i, j)-th entry of VAR\u2020R,\u03c1 as vij . Define vector random variables xj,(l) \u2208 Rm such that for j = 1, \u00b7 \u00b7 \u00b7 , n and l = 1, \u00b7 \u00b7 \u00b7 , c2,\nxj,(l) = vij pi bi = vij pi ( ai \u2212C1C\u20201ai ) with probability pi, for i = 1, \u00b7 \u00b7 \u00b7 , n,\nNote that xj,(l) is a linear function of a column of A sampled from the above defined distribution. We have that\nE[xj,(l)] =\nn \u2211\ni=1\npi vij pi bi = Bvj ,\nE\u2016xj,(l)\u201622 = n \u2211\ni=1\npi v2ij p2i \u2016bi\u201622 = n \u2211\ni=1\nv2ij \u2016bi\u201622/\u2016B\u20162F \u2016bi\u201622 = \u2016B\u20162F .\nThen we let xj = 1 c2 \u2211c2 l=1 xj,(l), we have\nE[xj ] = E[xj,(l)] = Bvj ,\nE\u2016xj \u2212Bvj\u201622 = E \u2225 \u2225 \u2225 xj \u2212 E[xj ] \u2225 \u2225 \u2225 2\n2 =\n1\nc2 E\n\u2225 \u2225 \u2225 xj,(l) \u2212 E[xj,(l)] \u2225 \u2225 \u2225 2\n2 =\n1 c2 E\u2016xj,(l) \u2212Bvj\u201622.\nAccording to the construction of x1, \u00b7 \u00b7 \u00b7 ,x\u03c1, we define the c2 columns of A to be C2 \u2208 R m\u00d7c2 . Note that all the random variables x1 \u00b7 \u00b7 \u00b7 ,x\u03c1 lie in the subspace span(C1) + span(C2). We define random variables\nwj = C1C \u2020 1AR \u2020Rvj + xj = C1C \u2020 1Avj + xj, for j = 1, \u00b7 \u00b7 \u00b7 , \u03c1,\nwhere the second equality follows from Lemma 12 that AR\u2020Rvj = Avj if vj is one of the top \u03c1 right singular vectors of AR\u2020R. Then we have that any set of random variables {w1, \u00b7 \u00b7 \u00b7 ,w\u03c1} lies in span(C) = span(C1) + span(C2). Let W = [w1, \u00b7 \u00b7 \u00b7 ,w\u03c1] be a matrix random variable, we have that span(W) \u2282 span(C). The expectation of wj is\nE[wj] = C1C \u2020 1Avj + E[xj] = C1C \u2020 1Avj +Bvj = Avj,\ntherefore we have that wj \u2212Avj = xj \u2212Bvj .\nThe expectation of \u2016wj \u2212Avj\u201622 is\nE\u2016wj \u2212Avj\u201622 = E\u2016xj \u2212Bvj\u201622 = 1\nc2 E\u2016xj,(l) \u2212Bvj\u201622\n= 1\nc2 E\u2016xj,(l)\u201622 \u2212\n2 c2 (Bvj) T E[xj,(l)] + 1 c2 \u2016Bvj\u201622\n= 1\nc2 E\u2016xj,(l)\u201622 \u2212\n1 c2 \u2016Bvj\u201622 =\n1\nc2 \u2016B\u20162F \u2212\n1 c2 \u2016Bvj\u201622\n\u2264 1 c2 \u2016B\u20162F (1)\nTo complete the proof, we let the matrix variable\nF = (\n\u03c1 \u2211\nq=1\n\u03c3\u22121q wqu T q )AR \u2020R,\nwhere \u03c3q is the q-th largest singular value of AR \u2020R and uq is the corresponding left singular vector of AR\u2020R. The column space of F is contained in span(W) \u2282 span(C), and thus\n\u2016AR\u2020R\u2212CC\u2020AR\u2020R\u20162F \u2264 \u2016AR\u2020R\u2212WW\u2020AR\u2020R\u20162F \u2264 \u2016AR\u2020R\u2212 F\u20162F .\nWe use F to bound the error \u2016AR\u2020R\u2212CC\u2020AR\u2020R\u20162F :\nE\u2016A\u2212CC\u2020AR\u2020R\u20162F = E\u2016A\u2212AR\u2020R+AR\u2020R\u2212CC\u2020AR\u2020R\u20162F = E [ \u2016A\u2212AR\u2020R\u20162F + \u2016AR\u2020R\u2212CC\u2020AR\u2020R\u20162F ]\n(2)\n\u2264 \u2016A\u2212AR\u2020R\u20162F + E\u2016AR\u2020R\u2212 F\u20162F ,\nwhere (2) follows from that A(I \u2212R\u2020R) is orthogonal to (I \u2212CC\u2020)AR\u2020R. Since AR\u2020R and F both lies on the space spanned by the right singular vectors of AR\u2020R, i.e. {vj}\u03c1j=1, we decompose AR\u2020R\u2212 F along {vj}\u03c1j=1:\nE\u2016A\u2212CC\u2020AR\u2020R\u20162F \u2264 \u2016A\u2212AR\u2020R\u20162F + E\u2016AR\u2020R\u2212 F\u20162F ,\n= \u2016A\u2212AR\u2020R\u20162F + \u03c1 \u2211\nj=1\nE\n\u2225 \u2225 \u2225 (AR\u2020R\u2212 F)vj \u2225 \u2225 \u2225 2\n2\n= \u2016A\u2212AR\u2020R\u20162F + \u03c1 \u2211\nj=1\nE\n\u2225 \u2225 \u2225 AR\u2020Rvj \u2212 (\n\u03c1 \u2211\nq=1\n\u03c3\u22121q wqu T q )\u03c3juj\n\u2225 \u2225 \u2225 2\n2\n= \u2016A\u2212AR\u2020R\u20162F + \u03c1 \u2211\nj=1\nE\n\u2225 \u2225 \u2225 AR\u2020Rvj \u2212wj \u2225 \u2225 \u2225 2\n2\n= \u2016A\u2212AR\u2020R\u20162F + \u03c1 \u2211\nj=1\nE\u2016Avj \u2212wj\u201622 (3)\n\u2264 \u2016A\u2212AR\u2020R\u20162F + \u03c1\nc2 \u2016B\u20162F , (4)\nwhere (3) follows from Lemma 12 and (4) follows from (1).\nLemma 12 We are given a matrix A \u2208 Rm\u00d7n and a matrix R \u2208 Rr\u00d7n such that rank(AR\u2020R) = rank(R) = \u03c1 (\u03c1 \u2264 r \u2264 m). Letting vj \u2208 Rn be the j-th top right singular vector of AR\u2020R, we have that\nAR\u2020Rvj = Avj, for j = 1, \u00b7 \u00b7 \u00b7 , \u03c1.\nProof First let VR,\u03c1 \u2208 Rn\u00d7\u03c1 contain the top \u03c1 right singular vectors of R, then the projection of A onto the row space of R is AR\u2020R = AVR,\u03c1V T R,\u03c1. Let the thin SVD of AVR,\u03c1 \u2208 Rm\u00d7\u03c1 be U\u0303\u03a3\u0303V\u0303T , where V\u0303 \u2208 R\u03c1\u00d7\u03c1. Then the compact SVD of AR\u2020R is\nAR\u2020R = AVR,\u03c1V T R,\u03c1 = U\u0303\u03a3\u0303V\u0303 TVTR,\u03c1.\nAccording to the definition, vj is the j-th column of (VR,\u03c1V\u0303) \u2208 Rn\u00d7\u03c1, and thus vj lies on the column space of VR,\u03c1, and vj is orthogonal to VR,\u03c1\u22a5. Finally, since A \u2212 AR\u2020R = AVR,\u03c1\u22a5V T R,\u03c1\u22a5, we have that vj is orthogonal to A\u2212AR\u2020R, that is, (A\u2212AR\u2020R)vj = 0, which directly proves the lemma.\nB.2 The Proof of Theorem 8\nBoutsidis et al. (2011a) proposed a randomized algorithm which achieves the expected relative-error bound in Lemma 13. This algorithm is described in Line 3 to 6 of Algorithm 1. Lemma 13 is a direct corollary of Lemma 4 and Lemma 5. If we apply the same algorithm to AT to select c rows of A to form R1, that is, Line 11 to 13 of Algorithm 1, then a very similar bound is guaranteed.\nLemma 13 (Boutsidis et al. (2011a), Theorem 4) Given a matrix A \u2208 Rm\u00d7n of rank \u03c1, a target rank 2 \u2264 k < \u03c1, and 0 < \u01eb0 < 1, there is a randomized algorithm to select c1 > k columns of A and form a matrix C1 \u2208 Rm\u00d7c1 such that\nE\u2016A\u2212C1C\u20201A\u20162F \u2264 (1 + \u01eb0) ( 1 + 1\n(1\u2212 \u221a k/c1)2\n)\n\u2016A\u2212Ak\u20162F ,\nwhere the expectation is taken w.r.t. C1. The matrix C1 can be computed in O(mnk\u01eb\u221210 + nc1k 2) time.\nWith Theorem 7 and Lemma 13, we now prove Theorem 8 as follows. Proof This randomized algorithm has three steps: approximate SVD via randomized projection (Halko et al., 2011), deterministic column selection via dual set sparsification algorithm (Boutsidis et al., 2011a) shown in Lemma 5, and the adaptive sampling algorithm of Theorem 7 proved in this paper. This algorithm is a generalization of the near-optimal column selection algorithm of Lemma 3.\nGiven A \u2208 Rm\u00d7n and a target rank k < r1, step 1 (Line 3 of Algorithm 1) compute an approximate truncated SVD of A in O(mnk/\u01eb0) time such that Ak \u2248 A\u0303k = U\u0303k\u03a3\u0303kV\u0303Tk . Lemma 4 shows that\nE\u2016A\u2212 U\u0303k\u03a3\u0303kV\u0303Tk \u20162F \u2264 (1 + \u01eb0)\u2016A\u2212Ak\u20162F .\nStep 2 (Line 11 to 13 of Algorithm 1) selects r1 rows of A to construct R1 by the dual set sparsification algorithm taking U and V as input, where U contains all the m columns of (AT \u2212 A\u0303Tk ) \u2208 Rn\u00d7m, V contains all the m columns of U\u0303TA,k \u2208 Rk\u00d7m. Lemma 13 shows that\nE\u2016A\u2212AR\u20201R1\u20162F \u2264 (1 + \u01eb0) ( 1 + 1\n(1\u2212 \u221a k/r1)2\n)\n\u2016A\u2212Ak\u20162F ,\nwhere the expectation is taken w.r.t. R1. Step 2 costs O(mr1k2 +mn) time. Step 3 (Line 14 to 16 of Algorithm 1) samples additional r2 rows of A to construct R2 \u2208 R r2\u00d7n by the adaptive sampling algorithm of Theorem 7. Let R = [RT1 ,R T 2 ]\nT \u2208 R(r1+r2)\u00d7n. We apply Theorem 7 and have that\nER\u2016A\u2212CC\u2020AR\u2020R\u20162F = ER1 [ ER2 [ \u2016A\u2212CC\u2020AR\u2020R\u20162F \u2223 \u2223 \u2223 R1 ]]\n\u2264 ER1 [ \u2016A\u2212CC\u2020A\u20162F + \u03c1\nr2 \u2016A\u2212AR\u20201R1\u20162F\n]\n\u2264 \u2016A\u2212CC\u2020A\u20162F + \u03c1\nr2 (1 + \u01eb0)\n( 1 + 1\n(1\u2212 \u221a k/r1)2\n)\n\u2016A\u2212Ak\u20162F .\nBy setting r1 = O(k\u01eb\u22122/3), r2 \u2248 2\u03c1\u01eb , and \u01eb0 = \u01eb2/3, we conclude that\nE\u2016A\u2212CC\u2020AR\u2020R\u20162F \u2264 \u2016A\u2212CC\u2020A\u20162F + \u01eb\u2016A\u2212Ak\u20162F .\nThe total computation time of the three steps is O(mnk/\u01eb0 +mr1k2 +mn) = O((mnk + mk3)\u01eb\u22122/3)\nB.3 The Proof of Theorem 9\nProof Since C is constructed by columns of A, the column space of C is contained in the column space of A, so rank(CC\u2020A) = rank(C) = \u03c1 \u2264 c, and thus the assumptions of Theorem 8 are satisfied. Lemma 3 and Theorem 8 together prove Theorem 9:\nE 2\u2016A\u2212CUR\u2016F \u2264 E\u2016A\u2212CUR\u20162F = EC,R\u2016A\u2212CC\u2020AR\u2020R\u20162F\n= EC\n[\nER\n[ \u2016A\u2212CC\u2020AR\u2020R\u20162F \u2223 \u2223 \u2223 C ]]\n\u2264 EC [ \u2016A\u2212CC\u2020A\u20162F + \u01eb\u2016A\u2212Ak\u20162F ]\n\u2264 (1 + 2\u01eb)\u2016A\u2212Ak\u20162k.\nFinally we have E\u2016A\u2212CUR\u2016F \u2264 (1 + \u01eb)\u2016A\u2212Ak\u2016k because 1 + 2\u01eb \u2264 (1 + \u01eb)2. The time cost of the fast CUR algorithm is the sum of Stage 1, Stage 2, and the MoorePenrose inverse of C and R, i.e. O((mnk+nk3)\u01eb\u22122/3)+O((mnk+mk3)\u01eb\u22122/3)+O(mc2)+ O(nr2) = O(mnk\u01eb\u22122/3 + (m+ n)k3\u01eb\u22122/3 +mk2\u01eb\u22122 + nk2\u01eb\u22124)."}], "references": [{"title": "Efficient gradient-domain compositing using quadtrees", "author": ["Aseem Agarwala"], "venue": "SIGGRAPH", "citeRegEx": "Agarwala.,? \\Q2007\\E", "shortCiteRegEx": "Agarwala.", "year": 2007}, {"title": "Generalized Inverses: Theory and Applications", "author": ["Adi Ben-Israel", "Thomas N.E. Greville"], "venue": "Second Edition. Springer,", "citeRegEx": "Ben.Israel and Greville.,? \\Q2003\\E", "shortCiteRegEx": "Ben.Israel and Greville.", "year": 2003}, {"title": "Near-optimal column-based matrix reconstruction", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "CoRR, abs/1103.0995,", "citeRegEx": "Boutsidis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2011}, {"title": "Near optimal column-based matrix reconstruction", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "In Proceedings of the 2011 IEEE 52nd Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Boutsidis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2011}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman"], "venue": "Journal of The American Society for Information Science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Efficient volume sampling for row/column subset selection", "author": ["Amit Deshpande", "Luis Rademacher"], "venue": "In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Deshpande and Rademacher.,? \\Q2010\\E", "shortCiteRegEx": "Deshpande and Rademacher.", "year": 2010}, {"title": "Matrix approximation and projective clustering via volume sampling", "author": ["Amit Deshpande", "Luis Rademacher", "Santosh Vempala", "Grant Wang"], "venue": "Theory of Computing,", "citeRegEx": "Deshpande et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Deshpande et al\\.", "year": 2006}, {"title": "Pass-efficient algorithms for approximating large matrices", "author": ["Petros Drineas"], "venue": "Proceeding of the 14th Annual ACM-SIAM Symposium on Dicrete Algorithms,", "citeRegEx": "Drineas.,? \\Q2003\\E", "shortCiteRegEx": "Drineas.", "year": 2003}, {"title": "On the Nystr\u00f6m method for approximating a gram matrix for improved kernel-based learning", "author": ["Petros Drineas", "Michael W. Mahoney"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Drineas and Mahoney.,? \\Q2005\\E", "shortCiteRegEx": "Drineas and Mahoney.", "year": 2005}, {"title": "Fast monte carlo algorithms for matrices iii: Computing a compressed approximate matrix decomposition", "author": ["Petros Drineas", "Ravi Kannan", "Michael W. Mahoney"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Relative-error CUR matrix decompositions", "author": ["Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Drineas et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2008}, {"title": "Fast monte-carlo algorithms for finding low-rank approximations", "author": ["Alan Frieze", "Ravi Kannan", "Santosh Vempala"], "venue": "Journal of the ACM,", "citeRegEx": "Frieze et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Frieze et al\\.", "year": 2004}, {"title": "A theory of pseudoskeleton approximations", "author": ["S.A. Goreinov", "E.E. Tyrtyshnikov", "N.L. Zamarashkin"], "venue": "Linear Algebra and Its Applications,", "citeRegEx": "Goreinov et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Goreinov et al\\.", "year": 1997}, {"title": "Pseudo-skeleton approximations by matrices of maximal volume", "author": ["S.A. Goreinov", "N.L. Zamarashkin", "E.E. Tyrtyshnikov"], "venue": "Mathematical Notes,", "citeRegEx": "Goreinov et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Goreinov et al\\.", "year": 1997}, {"title": "Optimal column-based low-rank matrix reconstruction", "author": ["Venkatesan Guruswami", "Ali Kemal Sinop"], "venue": "In Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Guruswami and Sinop.,? \\Q2012\\E", "shortCiteRegEx": "Guruswami and Sinop.", "year": 2012}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A. Tropp"], "venue": "SIAM Review,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Computer Science Theory for the Information Age", "author": ["John Hopcroft", "Ravi Kannan"], "venue": null, "citeRegEx": "Hopcroft and Kannan.,? \\Q2012\\E", "shortCiteRegEx": "Hopcroft and Kannan.", "year": 2012}, {"title": "Vector algebra in the analysis of genome-wide expression data", "author": ["Finny G. Kuruvilla", "Peter J. Park", "Stuart L. Schreiber"], "venue": "Genome Biology,", "citeRegEx": "Kuruvilla et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kuruvilla et al\\.", "year": 2002}, {"title": "Object recognition from local scale-invariant features", "author": ["David G. Lowe"], "venue": "In Proceedings of the International Conference on Computer Vision, ICCV", "citeRegEx": "Lowe.,? \\Q1999\\E", "shortCiteRegEx": "Lowe.", "year": 1999}, {"title": "Divide-and-conquer matrix factorization", "author": ["Lester Mackey", "Ameet Talwalkar", "Michael I. Jordan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mackey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mackey et al\\.", "year": 2011}, {"title": "CUR matrix decompositions for improved data analysis", "author": ["Michael W. Mahoney", "Petros Drineas"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Mahoney and Drineas.,? \\Q2009\\E", "shortCiteRegEx": "Mahoney and Drineas.", "year": 2009}, {"title": "Robust object recognition with cortex-like mechanisms", "author": ["Thomas Serre", "Lior Wolf", "Stanley Bileschi", "Maximilian Riesenhuber", "Tomaso Poggio"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Serre et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Serre et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 4, "context": "Applications of SVD such as eigenface (Sirovich and Kirby, 1987, Turk and Pentland, 1991) and latent semantic analysis (Deerwester et al., 1990) have been illustrated to be very successful.", "startOffset": 119, "endOffset": 144}, {"referenceID": 20, "context": "The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009).", "startOffset": 129, "endOffset": 156}, {"referenceID": 7, "context": "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age\u2212 (1/ \u221a 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people\u2019s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: \u201cit would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.", "startOffset": 4, "endOffset": 288}, {"referenceID": 7, "context": "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age\u2212 (1/ \u221a 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people\u2019s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: \u201cit would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.\u201d Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that \u00c3 = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1. The CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound.", "startOffset": 4, "endOffset": 1620}, {"referenceID": 7, "context": "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age\u2212 (1/ \u221a 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people\u2019s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: \u201cit would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.\u201d Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that \u00c3 = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1. The CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound. Later on, Drineas et al. (2008) devised randomized CUR algorithms with relative error by sampling sufficiently many columns and rows.", "startOffset": 4, "endOffset": 1704}, {"referenceID": 7, "context": "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age\u2212 (1/ \u221a 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people\u2019s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: \u201cit would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.\u201d Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that \u00c3 = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1. The CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound. Later on, Drineas et al. (2008) devised randomized CUR algorithms with relative error by sampling sufficiently many columns and rows. Particularly, the algorithm has (1 + \u01eb) relative-error ratio with high probability (w.h.p.). Recently, Mackey et al. (2011) established a divide-and-conquer method which solves the CUR problem in parallel.", "startOffset": 4, "endOffset": 1930}, {"referenceID": 7, "context": "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age\u2212 (1/ \u221a 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people\u2019s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: \u201cit would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.\u201d Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that \u00c3 = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1. The CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound. Later on, Drineas et al. (2008) devised randomized CUR algorithms with relative error by sampling sufficiently many columns and rows. Particularly, the algorithm has (1 + \u01eb) relative-error ratio with high probability (w.h.p.). Recently, Mackey et al. (2011) established a divide-and-conquer method which solves the CUR problem in parallel. Unfortunately, all the existing CUR algorithms require a large number of columns and rows to be chosen. For example, for an m\u00d7 n matrix A and a target rank k \u2264 min{m,n}, the state-of-the-art CUR algorithm \u2014 the subspace sampling algorithm in Drineas et al. (2008) \u2014 requires exactly O(k4\u01eb\u22126) rows or O(k\u01eb\u22124 log k) rows in expectation to achieve (1 + \u01eb) relative-error ratio w.", "startOffset": 4, "endOffset": 2276}, {"referenceID": 7, "context": "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age\u2212 (1/ \u221a 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people\u2019s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: \u201cit would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.\u201d Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that \u00c3 = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1. The CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound. Later on, Drineas et al. (2008) devised randomized CUR algorithms with relative error by sampling sufficiently many columns and rows. Particularly, the algorithm has (1 + \u01eb) relative-error ratio with high probability (w.h.p.). Recently, Mackey et al. (2011) established a divide-and-conquer method which solves the CUR problem in parallel. Unfortunately, all the existing CUR algorithms require a large number of columns and rows to be chosen. For example, for an m\u00d7 n matrix A and a target rank k \u2264 min{m,n}, the state-of-the-art CUR algorithm \u2014 the subspace sampling algorithm in Drineas et al. (2008) \u2014 requires exactly O(k4\u01eb\u22126) rows or O(k\u01eb\u22124 log k) rows in expectation to achieve (1 + \u01eb) relative-error ratio w.h.p. Moreover, the computational cost of this algorithm is at least the cost of the truncated SVD of A, that is, O(min{mn2, nm2}).1 The algorithms are therefore impractical for large-scale matrices. In this paper we develop a CUR algorithm which beats the state-of-the-art algorithm in both theory and experiments. In particular, we show in Theorem 9 a novel randomized CUR algorithm with lower time complexity and tighter theoretical bound in comparison with the state-of-the-art CUR algorithm in Drineas et al. (2008). The rest of this paper is organized as follows.", "startOffset": 4, "endOffset": 2908}, {"referenceID": 7, "context": "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age\u2212 (1/ \u221a 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people\u2019s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: \u201cit would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.\u201d Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that \u00c3 = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1. The CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound. Later on, Drineas et al. (2008) devised randomized CUR algorithms with relative error by sampling sufficiently many columns and rows. Particularly, the algorithm has (1 + \u01eb) relative-error ratio with high probability (w.h.p.). Recently, Mackey et al. (2011) established a divide-and-conquer method which solves the CUR problem in parallel. Unfortunately, all the existing CUR algorithms require a large number of columns and rows to be chosen. For example, for an m\u00d7 n matrix A and a target rank k \u2264 min{m,n}, the state-of-the-art CUR algorithm \u2014 the subspace sampling algorithm in Drineas et al. (2008) \u2014 requires exactly O(k4\u01eb\u22126) rows or O(k\u01eb\u22124 log k) rows in expectation to achieve (1 + \u01eb) relative-error ratio w.h.p. Moreover, the computational cost of this algorithm is at least the cost of the truncated SVD of A, that is, O(min{mn2, nm2}).1 The algorithms are therefore impractical for large-scale matrices. In this paper we develop a CUR algorithm which beats the state-of-the-art algorithm in both theory and experiments. In particular, we show in Theorem 9 a novel randomized CUR algorithm with lower time complexity and tighter theoretical bound in comparison with the state-of-the-art CUR algorithm in Drineas et al. (2008). The rest of this paper is organized as follows. Section 2 lists some notations that will be used in this paper and Section 3 reviews two classes of CUR algorithms. Section 4 mainly introduces a column selection algorithm to which our work is closely related. Section 5 describes and analyzes our novel CUR algorithm. Section 6 empirically compares our proposed algorithm with the state-of-the-art algorithm. All proofs are deferred to Appendix B. 1. Although some partial SVD algorithms, such as Krylov subspace methods, require only O(mnk) time, they are all numerical unstable. See Halko et al. (2011) for more discussions.", "startOffset": 4, "endOffset": 3513}, {"referenceID": 1, "context": "Furthermore, let A \u2020 = UA,\u03c1\u03a3 \u22121 A,\u03c1V T A,\u03c1 be the Moore-Penrose inverse of A (Ben-Israel and Greville, 2003).", "startOffset": 77, "endOffset": 108}, {"referenceID": 1, "context": "Furthermore, let A \u2020 = UA,\u03c1\u03a3 \u22121 A,\u03c1V T A,\u03c1 be the Moore-Penrose inverse of A (Ben-Israel and Greville, 2003). Given matrices A \u2208 Rm\u00d7n, X \u2208 Rm\u00d7p, and Y \u2208 Rq\u00d7n, XX\u2020A = UXUXA \u2208 Rm\u00d7n is the projection of A onto the column space of X, and AY\u2020Y = AVYV T Y \u2208 Rm\u00d7n is the projection of A onto the row space of Y. Finally, given an integer k \u2264 p, we define the matrix \u03a0X,k(A) \u2208 Rm\u00d7n as the best approximation to A within the column space of X that has rank at most k. We have \u03a0X,k(A) = X\u1e90 where \u1e90 = argminrank(Z)\u2264k \u2016A\u2212XZ\u2016F . We also have that \u2016A\u2212XXA\u2016F \u2264 \u2016A\u2212\u03a0X,k(A)\u2016F . 3. Previous Work in CUR Matrix Decomposition This section discusses two recent developments of the CUR algorithms. Section 3.1 introduces an additive-error CUR algorithm in Drineas et al. (2006), and Section 3.", "startOffset": 78, "endOffset": 755}, {"referenceID": 1, "context": "Furthermore, let A \u2020 = UA,\u03c1\u03a3 \u22121 A,\u03c1V T A,\u03c1 be the Moore-Penrose inverse of A (Ben-Israel and Greville, 2003). Given matrices A \u2208 Rm\u00d7n, X \u2208 Rm\u00d7p, and Y \u2208 Rq\u00d7n, XX\u2020A = UXUXA \u2208 Rm\u00d7n is the projection of A onto the column space of X, and AY\u2020Y = AVYV T Y \u2208 Rm\u00d7n is the projection of A onto the row space of Y. Finally, given an integer k \u2264 p, we define the matrix \u03a0X,k(A) \u2208 Rm\u00d7n as the best approximation to A within the column space of X that has rank at most k. We have \u03a0X,k(A) = X\u1e90 where \u1e90 = argminrank(Z)\u2264k \u2016A\u2212XZ\u2016F . We also have that \u2016A\u2212XXA\u2016F \u2264 \u2016A\u2212\u03a0X,k(A)\u2016F . 3. Previous Work in CUR Matrix Decomposition This section discusses two recent developments of the CUR algorithms. Section 3.1 introduces an additive-error CUR algorithm in Drineas et al. (2006), and Section 3.2 describes two relative-error CUR algorithms in Drineas et al. (2008). 3.", "startOffset": 78, "endOffset": 841}, {"referenceID": 1, "context": "Furthermore, let A \u2020 = UA,\u03c1\u03a3 \u22121 A,\u03c1V T A,\u03c1 be the Moore-Penrose inverse of A (Ben-Israel and Greville, 2003). Given matrices A \u2208 Rm\u00d7n, X \u2208 Rm\u00d7p, and Y \u2208 Rq\u00d7n, XX\u2020A = UXUXA \u2208 Rm\u00d7n is the projection of A onto the column space of X, and AY\u2020Y = AVYV T Y \u2208 Rm\u00d7n is the projection of A onto the row space of Y. Finally, given an integer k \u2264 p, we define the matrix \u03a0X,k(A) \u2208 Rm\u00d7n as the best approximation to A within the column space of X that has rank at most k. We have \u03a0X,k(A) = X\u1e90 where \u1e90 = argminrank(Z)\u2264k \u2016A\u2212XZ\u2016F . We also have that \u2016A\u2212XXA\u2016F \u2264 \u2016A\u2212\u03a0X,k(A)\u2016F . 3. Previous Work in CUR Matrix Decomposition This section discusses two recent developments of the CUR algorithms. Section 3.1 introduces an additive-error CUR algorithm in Drineas et al. (2006), and Section 3.2 describes two relative-error CUR algorithms in Drineas et al. (2008). 3.1 The Linear-Time CUR Algorithm The linear-time CUR algorithm is proposed by Drineas et al. (2006). It is a highly efficient algorithm.", "startOffset": 78, "endOffset": 943}, {"referenceID": 3, "context": "2 The Subspace Sampling CUR Algorithm Drineas et al. (2008) proposed a two-stage randomized CUR algorithm which has a relativeerror bound w.", "startOffset": 38, "endOffset": 60}, {"referenceID": 15, "context": "The near-optimal algorithm has three steps: the approximate SVD via random projection (Halko et al., 2011), the dual set sparsification algorithm (Boutsidis et al.", "startOffset": 86, "endOffset": 106}, {"referenceID": 6, "context": ", 2011a), and the adaptive sampling algorithm (Deshpande et al., 2006).", "startOffset": 46, "endOffset": 70}, {"referenceID": 2, "context": "2 The Near-Optimal Column Selection Algorithm Recently, Boutsidis et al. (2011a) proposed a randomized algorithm which selects only c = 2k\u01eb\u22121(1 + o(1)) columns to achieve the expected relative-error ratio (1 + \u01eb).", "startOffset": 56, "endOffset": 81}, {"referenceID": 2, "context": "2 The Near-Optimal Column Selection Algorithm Recently, Boutsidis et al. (2011a) proposed a randomized algorithm which selects only c = 2k\u01eb\u22121(1 + o(1)) columns to achieve the expected relative-error ratio (1 + \u01eb). Boutsidis et al. (2011a) also proved the lower bound of the column selection problem; that is, at least c = k\u01eb\u22121 columns are selected to achieve the (1 + \u01eb) ratio.", "startOffset": 56, "endOffset": 239}, {"referenceID": 2, "context": "2 The Near-Optimal Column Selection Algorithm Recently, Boutsidis et al. (2011a) proposed a randomized algorithm which selects only c = 2k\u01eb\u22121(1 + o(1)) columns to achieve the expected relative-error ratio (1 + \u01eb). Boutsidis et al. (2011a) also proved the lower bound of the column selection problem; that is, at least c = k\u01eb\u22121 columns are selected to achieve the (1 + \u01eb) ratio. Thus this algorithm is near optimal. Though an optimal algorithm recently proposed by Guruswami and Sinop (2012) achieves the the lower bound, the optimal algorithm is quite inefficient compared with the near-optimal algorithm.", "startOffset": 56, "endOffset": 491}, {"referenceID": 15, "context": "(2011a) employed an approximation SVD algorithm (Halko et al., 2011) to speedup computation.", "startOffset": 48, "endOffset": 68}, {"referenceID": 2, "context": "Since SVD is time consuming, Boutsidis et al. (2011a) employed an approximation SVD algorithm (Halko et al.", "startOffset": 29, "endOffset": 54}, {"referenceID": 2, "context": "The second step of the near-optimal column selection algorithm is the dual set sparsification proposed by Boutsidis et al. (2011a). When ones take A and the top k (approximate) right singular vectors of A as inputs, the dual set sparsification algorithm can deterministically selects c1 columns of A to construct C1.", "startOffset": 106, "endOffset": 131}, {"referenceID": 6, "context": "After sampling c1 columns of A, the near-optimal column selection algorithm uses the adaptive sampling of Deshpande et al. (2006) to select c2 columns of A to further reduce the construction error.", "startOffset": 106, "endOffset": 130}, {"referenceID": 6, "context": "After sampling c1 columns of A, the near-optimal column selection algorithm uses the adaptive sampling of Deshpande et al. (2006) to select c2 columns of A to further reduce the construction error. We present Theorem 2.1 in Deshpande et al. (2006) in the following lemma.", "startOffset": 106, "endOffset": 248}, {"referenceID": 4, "context": "1 of Deshpande et al. (2006). The algorithm is based on the following idea: after selecting a proportion of columns from A to form C1 by an arbitrary algorithm, the algorithms randomly samples additional c2 columns according to the residual A \u2212 C1C\u20201A.", "startOffset": 5, "endOffset": 29}, {"referenceID": 2, "context": "Boutsidis et al. (2011a) used the adaptive sampling algorithm to decrease the residual of the dual set sparsification algorithm and obtained an (1 + \u01eb) relative-error ratio.", "startOffset": 0, "endOffset": 25}, {"referenceID": 2, "context": "Boutsidis et al. (2011a) used the adaptive sampling algorithm to decrease the residual of the dual set sparsification algorithm and obtained an (1 + \u01eb) relative-error ratio. Here we prove a new bound for the same adaptive sampling algorithm. Interestingly, this new bound is a generalization of the original one in Theorem 2.1 of Deshpande et al. (2006). In other words, Theorem 2.", "startOffset": 0, "endOffset": 354}, {"referenceID": 2, "context": "Boutsidis et al. (2011a) used the adaptive sampling algorithm to decrease the residual of the dual set sparsification algorithm and obtained an (1 + \u01eb) relative-error ratio. Here we prove a new bound for the same adaptive sampling algorithm. Interestingly, this new bound is a generalization of the original one in Theorem 2.1 of Deshpande et al. (2006). In other words, Theorem 2.1 of Deshpande et al. (2006) is a direct corollary of our following theorem when C = Ak.", "startOffset": 0, "endOffset": 410}, {"referenceID": 2, "context": ", Theorem 5 of Boutsidis et al. (2011a), is a special case of Theorem 8 when C = Ak.", "startOffset": 15, "endOffset": 40}, {"referenceID": 0, "context": "The Redrock and Edinburgh (Agarwala, 2007) are two large size natural images.", "startOffset": 26, "endOffset": 42}, {"referenceID": 21, "context": "We use the HMAX features (Serre et al., 2007) and the SIFT features (Lowe, 1999) of the first 50000 images; the features provided by Wang et al.", "startOffset": 25, "endOffset": 45}, {"referenceID": 18, "context": ", 2007) and the SIFT features (Lowe, 1999) of the first 50000 images; the features provided by Wang et al.", "startOffset": 30, "endOffset": 42}, {"referenceID": 0, "context": "The Redrock and Edinburgh (Agarwala, 2007) are two large size natural images. Arcene and Dexter are both from the UCI datasets (Frank and Asuncion, 2010). Arcene is a biology dataset with 900 instances and 10000 attributes. Dexter is a bag of words dataset with a 20000vocabulary and 2600 documents. PicasaWeb image dataset (Wang et al., 2012) contains 6.8 million PicasaWeb images. We use the HMAX features (Serre et al., 2007) and the SIFT features (Lowe, 1999) of the first 50000 images; the features provided by Wang et al. (2012) are all of 3000 dimensions.", "startOffset": 27, "endOffset": 535}, {"referenceID": 7, "context": "According to the analysis in Drineas et al. (2008) and this paper, k, c, and r should be integers much less than m and n.", "startOffset": 29, "endOffset": 51}, {"referenceID": 2, "context": "First, what is the lower bound for the CUR problem? Second, is there any algorithm achieving such a lower bound? Boutsidis et al. (2011b) proved a lower bound for the column selection problem: \u2016A\u2212CC\u2020A\u20162F \u2016A\u2212Ak\u2016 2 F \u2265 1+ kc .", "startOffset": 113, "endOffset": 138}, {"referenceID": 2, "context": "The dual set sparsification algorithms are deterministic algorithms established in Boutsidis et al. (2011a). The fast CUR algorithm calls the dual set spectral-Frobenius sparsification algorithm (Lemma 13 in Boutsidis et al.", "startOffset": 83, "endOffset": 108}, {"referenceID": 2, "context": "Here we would like to mention the implementation of Algorithm 2, which is not described by Boutsidis et al. (2011a) in details.", "startOffset": 91, "endOffset": 116}, {"referenceID": 2, "context": "In order to stick to the column space convention of Boutsidis et al. (2011a), we prove Theorem 11 instead of Theorem 7.", "startOffset": 52, "endOffset": 77}, {"referenceID": 2, "context": "2 The Proof of Theorem 8 Boutsidis et al. (2011a) proposed a randomized algorithm which achieves the expected relative-error bound in Lemma 13.", "startOffset": 25, "endOffset": 50}, {"referenceID": 2, "context": "Lemma 13 (Boutsidis et al. (2011a), Theorem 4) Given a matrix A \u2208 Rm\u00d7n of rank \u03c1, a target rank 2 \u2264 k < \u03c1, and 0 < \u01eb0 < 1, there is a randomized algorithm to select c1 > k columns of A and form a matrix C1 \u2208 Rm\u00d7c1 such that E\u2016A\u2212C1C\u20201A\u2016F \u2264 (1 + \u01eb0) (", "startOffset": 10, "endOffset": 35}, {"referenceID": 15, "context": "Proof This randomized algorithm has three steps: approximate SVD via randomized projection (Halko et al., 2011), deterministic column selection via dual set sparsification algorithm (Boutsidis et al.", "startOffset": 91, "endOffset": 111}], "year": 2012, "abstractText": "The CUR matrix decomposition is an important extension of Nystr\u00f6m approximation to a general matrix. It approximates any data matrix in terms of a small number of its columns and rows. In this paper we propose a novel randomized CUR algorithm with an expected relative-error bound. The proposed algorithm has the advantages over the existing relative-error CUR algorithm that it possesses tighter theoretical bound and lower time complexity, and that it can avoid maintaining the whole data matrix in main memory. Finally, experiments on several real-world datasets demonstrate significant improvement over the existing relative-error algorithms.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}