{"id": "1210.5196", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2012", "title": "Matrix reconstruction with the local max norm", "abstract": "We introduce a new family of matrix norms, the \"local max\" norms, generalizing existing methods such as the max norm, the trace norm (nuclear norm), and the weighted or smoothed weighted trace norms, which have been extensively used in the literature as regularizers for matrix reconstruction problems. We show that this new family can be used to interpolate between the (weighted or unweighted) trace norm and the more conservative max norm. We test this interpolation on simulated data and on the large-scale Netflix and MovieLens ratings data, and find improved accuracy relative to the existing matrix norms. We also provide theoretical results showing learning guarantees for some of the new norms.", "histories": [["v1", "Thu, 18 Oct 2012 17:30:43 GMT  (27kb,D)", "http://arxiv.org/abs/1210.5196v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["rina foygel", "nathan srebro", "ruslan salakhutdinov"], "accepted": true, "id": "1210.5196"}, "pdf": {"name": "1210.5196.pdf", "metadata": {"source": "CRF", "title": "Matrix reconstruction with the local max norm", "authors": ["Rina Foygel", "Nathan Srebro"], "emails": ["rinafb@stanford.edu", "nati@ttic.edu", "rsalakhu@utstat.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "In the matrix reconstruction problem, we are given a matrix Y \u2208 Rn\u00d7m whose entries are only partly observed, and would like to reconstruct the unobserved entries as accurately as possible. Matrix reconstruction arises in many modern applications, including the areas of collaborative filtering (e.g. the Netflix prize), image and video data, and others. This problem has often been approached using regularization with matrix norms that promote low-rank or approximately-low-rank solutions, including the trace norm (also known as the nuclear norm) and the max norm, as well as several adaptations of the trace norm described below.\nIn this paper, we introduce a unifying family of norms that generalizes these existing matrix norms, and that can be used to interpolate between the trace and max norms. We show that this family includes new norms, lying strictly between the trace and max norms, that give empirical and theoretical improvements over the existing norms. We give results allowing for large-scale optimization with norms from the new family. Some proofs are deferred to the Supplementary Materials.\nNotation Without loss of generality we take n \u2265 m. We let R+ denote the nonnegative real numbers. For any n \u2208 N, let [n] = {1, . . . , n}, and define the simplex on [n] as \u2206[n] ={ r \u2208 Rn+ : \u2211 i ri = 1 } . We analyze situations where the locations of observed entries are sampled\ni.i.d. according to some distribution p on [n]\u00d7 [m]. We write pi\u2022 = \u2211 j pij to denote the marginal probability of row i, and prow = (p1\u2022, . . . ,pn\u2022) \u2208 \u2206[n] to denote the marginal row distribution. We define p\u2022j and pcol similarly for the columns."}, {"heading": "1.1 Trace norm and max norm", "text": "A common regularizer used in matrix reconstruction, and other matrix problems, is the trace norm \u2016X\u2016tr, equal to the sum of the singular values ofX . This norm can also be defined via a factorization\nar X\niv :1\n21 0.\n51 96\nv1 [\nst at\n.M L\n] 1\n8 O\nof X [1]:\n1\u221a nm \u2016X\u2016tr = 1 2 min AB>=X  1 n \u2211 i \u2225\u2225A(i)\u2225\u22252 + 1 m \u2211 j \u2225\u2225B(j)\u2225\u22252  , (1)\nwhere M(i) denotes the ith row of a matrix M , and where the minimum is taken over factorizations ofX of arbitrary dimension\u2014that is, the number of columns inA andB is unbounded. Note that we choose to scale the trace norm by 1/ \u221a nm in order to emphasize that we are averaging the squared row norms of A and B.\nRegularization with the trace norm gives good theoretical and empirical results, as long as the locations of observed entries are sampled uniformly (i.e. when p is the uniform distribution on [n]\u00d7[m]), and, under this assumption, can also be used to guarantee approximate recovery of an underlying low-rank matrix [1, 2, 3, 4].\nThe factorized definition of the trace norm (1) allows for an intuitive comparison with the max norm, defined as [1]:\n\u2016X\u2016max = 1\n2 min AB>=X ( sup i \u2225\u2225A(i)\u2225\u222522 + sup j \u2225\u2225B(j)\u2225\u222522) . (2) We see that the max norm measures the largest row norms in the factorization, while the rescaled trace norm instead considers the average row norms. The max norm is therefore an upper bound on the rescaled trace norm, and can be viewed as a more conservative regularizer. For the more general setting where p may not be uniform, Foygel and Srebro [4] show that the max norm is still an effective regularizer (in particular, bounds on error for the max norm are not affected by p). On the other hand, Salakhutdinov and Srebro [5] show that the trace norm is not robust to non-uniform sampling\u2014regularizing with the trace norm may yield large error due to over-fitting on the rows and columns with high marginals. They obtain improved empirical results by placing more penalization on these over-represented rows and columns, described next."}, {"heading": "1.2 The weighted trace norm", "text": "To reduce overfitting on the rows and columns with high marginal probabilities under the distribution p, Salakhutdinov and Srebro propose regularizing with the p-weighted trace norm,\n\u2016X\u2016tr(p) := \u2225\u2225\u2225diag(prow)1/2 \u00b7X \u00b7 diag(pcol)1/2\u2225\u2225\u2225\ntr .\nIf the row and the column of entries to be observed are sampled independently (i.e. p = prow \u00b7 pcol is a product distribution), then the p-weighted trace norm can be used to obtain good learning guarantees even when prow and pcol are non-uniform [3, 6]. However, for non-uniform non-product sampling distributions, even the p-weighted trace norm can yield poor generalization performance. To correct for this, Foygel et al. [6] suggest adding in some \u201csmoothing\u201d to avoid under-penalizing the rows and columns with low marginal probabilities, and obtain improved empirical and theoretical results for matrix reconstruction using the smoothed weighted trace norm:\n\u2016X\u2016tr(p\u0303) := \u2225\u2225\u2225diag(p\u0303row)1/2 \u00b7X \u00b7 diag(p\u0303col)1/2\u2225\u2225\u2225\ntr ,\nwhere p\u0303row and p\u0303col denote smoothed row and column marginals, given by\np\u0303row = (1\u2212 \u03b6) \u00b7 prow + \u03b6 \u00b7 1/n and p\u0303col = (1\u2212 \u03b6) \u00b7 pcol + \u03b6 \u00b7 1/m , (3) for some choice of smoothing parameter \u03b6 which may be selected with cross-validation1. The smoothed empirically-weighted trace norm is also studied in [6], where pi\u2022 is replaced with p\u0302i\u2022 = # observations in row i total # observations , the empirical marginal probability of row i (and same for p\u0302\u2022j). Using empirical rather than \u201ctrue\u201d weights yielded lower error in experiments in [6], even when the true sampling distribution was uniform.\nMore generally, for any weight vectors r \u2208 \u2206[n] and c \u2208 \u2206[m] and a matrix X \u2208 Rn\u00d7m, the (r, c)-weighted trace norm is given by\n\u2016X\u2016tr(r,c) = \u2225\u2225\u2225diag(r)1/2 \u00b7X \u00b7 diag(c)1/2\u2225\u2225\u2225\ntr .\n1Our \u03b6 parameter here is equivalent to 1\u2212 \u03b1 in [6].\nOf course, we can easily obtain the existing methods of the uniform trace norm, (empirically) weighted trace norm, and smoothed (empirically) weighted trace norm as special cases of this formulation. Furthermore, the max norm is equal to a supremum over all possible weightings [7]:\n\u2016X\u2016max = sup r\u2208\u2206[n],c\u2208\u2206[m] \u2016X\u2016tr(r,c) ."}, {"heading": "2 The local max norm", "text": "We consider a generalization of these norms, which lies \u201cin between\u201d the trace norm and max norm. For anyR \u2286 \u2206[n] and C \u2286 \u2206[m], we define the (R, C)-norm of X:\n\u2016X\u2016(R,C) = sup r\u2208R,c\u2208C \u2016X\u2016tr(r,c) .\nThis gives a norm on matrices, except in the trivial case where, for some i or some j, ri = 0 for all r \u2208 R or cj = 0 for all c \u2208 C. We now show some existing and novel norms that can be obtained using local max norms."}, {"heading": "2.1 Trace norm and max norm", "text": "We can obtain the max norm by taking the largest possibleR and C, i.e. \u2016X\u2016max = \u2016X\u2016(\u2206[n],\u2206[m]), and similarly we can obtain the (r, c)-weighted trace norm by taking the singleton sets R = {r} and C = {c}. As discussed above, this includes the standard trace norm (when r and c are uniform), as well as the weighted, empirically weighted, and smoothed weighted trace norm."}, {"heading": "2.2 Arbitrary smoothing", "text": "When using the smoothed weighted max norm, we need to choose the amount of smoothing to apply to the marginals, that is, we need to choose \u03b6 in our definition of the smoothed row and column weights, as given in (3). Alternately, we could regularize simultaneously over all possible amounts of smoothing by considering the local max norm with\nR = {(1\u2212 \u03b6) \u00b7 prow + \u03b6 \u00b7 1/n : any \u03b6 \u2208 [0, 1]} , and same for C. That is, R and C are line segments in the simplex\u2014they are larger than any single point as for the uniform or weighted trace norm (or smoothed weighted trace norm for a fixed amount of smoothing), but smaller than the entire simplex as for the max norm."}, {"heading": "2.3 Connection to (\u03b2, \u03c4)-decomposability", "text": "Hazan et al. [8] introduce a class of matrices defined by a property of (\u03b2, \u03c4)-decomposability: a matrix X satisfies this property if there exists a factorization X = AB> (where A and B may have an arbitrary number of columns) such that\nmax { max i \u2225\u2225A(i)\u2225\u222522 ,maxj \u2225\u2225B(j)\u2225\u222522 } \u2264 2\u03b2, \u2211 i \u2225\u2225A(i)\u2225\u222522 +\u2211 j \u2225\u2225B(j)\u2225\u222522 \u2264 \u03c4 , where A(i) and B(j) are the ith row of A and the jth row of B, respectively2.\nComparing with (1) and (2), we see that the \u03b2 and \u03c4 parameters essentially correspond to the max norm and trace norm, with the max norm being the minimal 2\u03b2\u2217 such that the matrix is (\u03b2\u2217, \u03c4)decomposable for some \u03c4 , and the trace norm being the minimal \u03c4\u2217/2 such that the matrix is (\u03b2, \u03c4\u2217)-decomposable for some \u03b2. However, Hazan et al. go beyond these two extremes, and rely on balancing both \u03b2 and \u03c4 : they establish learning guarantees (in an adversarial online model, and thus also under an arbitrary sampling distribution p) which scale with \u221a \u03b2 \u00b7 \u03c4 . It may therefore be useful to consider a penalty function of the form:\nPenalty(\u03b2,\u03c4)(X) = min X=AB>  \u221a max i \u2225\u2225A(i)\u2225\u222522 + maxj \u2225\u2225B(j)\u2225\u222522 \u00b7 \u221a\u2211\ni \u2225\u2225A(i)\u2225\u222522 +\u2211 j \u2225\u2225B(j)\u2225\u222522  . (4)\n2Hazan et al. state the property differently, but equivalently, in terms of a semidefinite matrix decomposition.\n(Note that max { maxi \u2225\u2225A(i)\u2225\u222522 ,maxj \u2225\u2225B(j)\u2225\u222522} is replaced with maxi \u2225\u2225A(i)\u2225\u222522 + maxj \u2225\u2225B(j)\u2225\u222522, for later convenience. This affects the value of the penalty function by at most a factor of \u221a 2.)\nThis penalty function does not appear to be convex inX . However, the proposition below (proved in the Supplementary Materials) shows that we can use a (convex) local max norm penalty to compute a solution to any objective function with a penalty function of the form (4):\nProposition 1. Let X\u0302 be the minimizer of a penalized loss function with this modified penalty,\nX\u0302 := arg min X\n{ Loss(X) + \u03bb \u00b7 Penalty(\u03b2,\u03c4)(X) } ,\nwhere \u03bb \u2265 0 is some penalty parameter and Loss(\u00b7) is any convex function. Then, for some penalty parameter \u00b5 \u2265 0 and some t \u2208 [0, 1],\nX\u0302 = arg min X\n{ Loss(X) + \u00b5 \u00b7 \u2016X\u2016(R,C) } , where\nR = { r \u2208 \u2206[n] : ri \u2265\nt 1 + (n\u2212 1)t \u2200i } and C = { c \u2208 \u2206[m] : cj \u2265\nt 1 + (m\u2212 1)t \u2200j } .\nWe note that \u00b5 and t cannot be determined based on \u03bb alone\u2014they will depend on the properties of the unknown solution X\u0302 .\nHere the sets R and C impose a lower bound on each of the weights, and this lower bound can be used to interpolate between the max and trace norms: when t = 1, each ri is lower bounded by 1/n (and similarly for cj), i.e. R and C are singletons containing only the uniform weights and we obtain the trace norm. On the other hand, when t = 0, the weights are lower-bounded by zero, and so any weight vector is allowed, i.e. R and C are each the entire simplex and we obtain the max norm. Intermediate values of t interpolate between the trace norm and max norm and correspond to different balances between \u03b2 and \u03c4 ."}, {"heading": "2.4 Interpolating between trace norm and max norm", "text": "We next turn to an interpolation which relies on an upper bound, rather than a lower bound, on the weights. Consider\nR = { r \u2208 \u2206[n] : ri \u2264 \u2200i } and C\u03b4 = { c \u2208 \u2206[n] : cj \u2264 \u03b4 \u2200j } , (5)\nfor some \u2208 [1/n, 1] and \u03b4 \u2208 [1/m, 1]. The (R , C\u03b4)-norm is then equal to the (rescaled) trace norm when we choose = 1/n and \u03b4 = 1/m, and is equal to the max norm when we choose = \u03b4 = 1. Allowing and \u03b4 to take intermediate values gives a smooth interpolation between these two familiar norms, and may be useful in situations where we want more flexibility in the type of regularization.\nWe can generalize this to an interpolation between the max norm and a smoothed weighted trace norm, which we will use in our experimental results. We consider two generalizations\u2014for each one, we state a definition ofR, with C defined analogously. The first is multiplicative:\nR\u00d7\u03b6,\u03b3 := { r \u2208 \u2206[n] : ri \u2264 \u03b3 \u00b7 ((1\u2212 \u03b6) \u00b7 pi\u2022 + \u03b6 \u00b7 1/n) \u2200i } , (6)\nwhere \u03b3 = 1 corresponds to choosing the singleton set R\u00d7\u03b6,\u03b3 = {(1\u2212 \u03b6) \u00b7 prow + \u03b6 \u00b7 1/n} (i.e. the smoothed weighted trace norm), while \u03b3 = \u221e corresponds to the max norm (for any choice of \u03b6) since we would getR\u00d7\u03b6,\u03b3 = \u2206[n].\nThe second option for an interpolation is instead defined with an exponent: R\u03b6,\u03c4 := { r \u2208 \u2206[n] : ri \u2264 ((1\u2212 \u03b6) \u00b7 pi\u2022 + \u03b6 \u00b7 1/n) 1\u2212\u03c4 \u2200i } . (7)\nHere \u03c4 = 0 will yield the singleton set corresponding to the smoothed weighted trace norm, while \u03c4 = 1 will yieldR\u03b6,\u03c4 = \u2206[n], i.e. the max norm, for any choice of \u03b6. We find the second (exponent) option to be more natural, because each of the row marginal bounds will reach 1 simultaneously when \u03c4 = 1, and hence we use this version in our experiments. On the other hand, the multiplicative version is easier to work with theoretically, and we use this in our learning guarantee in Section 4.2. If all of the row and column marginals satisfy some loose upper bound, then the two options will not be highly different."}, {"heading": "3 Optimization with the local max norm", "text": "One appeal of both the trace norm and the max norm is that they are both SDP representable [9, 10], and thus easily optimizable, at least in small scale problems. Indeed, in the Supplementary Materials we show that the local max norm is also SDP representable, as long as the setsR and C can be written in terms of linear or semi-definite constraints\u2014this includes all the examples we mention, where in all of them the setsR and C are specified in terms of simple linear constraints. However, for large scale problems, it is not practical to directly use SDP optimization approaches. Instead, and especially for very large scale problems, an effective optimization approach for both the trace norm and the max norm is to use the factorized versions of the norms, given in (1) and (2), and to optimize the factorization directly (typically, only factorizations of some truncated dimensionality are used) [11, 12, 7]. As we show in Theorem 1 below, a similar factorization-optimization approach is also possible for any local max norm with convexR and C. We further give a simplified representation which is applicable when R and C are specified through element-wise upper bounds R \u2208 Rn+ and C \u2208 Rm+ , respectively: R = {r \u2208 \u2206[n] : ri \u2264 Ri \u2200i} and C = {c \u2208 \u2206[m] : cj \u2264 Cj \u2200j} , (8) with 0 \u2264 Ri \u2264 1, \u2211 iRi \u2265 1, 0 \u2264 Cj \u2264 1, \u2211 j Cj \u2265 1 to avoid triviality. This includes the interpolation norms of Section 2.4. Theorem 1. IfR and C are convex, then the (R, C)-norm can be calculated with the factorization\n\u2016X\u2016(R,C) = 1\n2 inf AB>=X ( sup r\u2208R \u2211 i ri \u2225\u2225A(i)\u2225\u222522 + sup c\u2208C \u2211 j cj \u2225\u2225B(j)\u2225\u222522 ) . (9)\nIn the special case whenR and C are defined by (8), writing (x)+ := max{0, x}, this simplifies to\n\u2016X\u2016(R,C) = 1\n2 inf AB>=X;a,b\u2208R\n{ a+ \u2211 i Ri (\u2225\u2225A(i)\u2225\u222522 \u2212 a)+ + b+\u2211 j Cj (\u2225\u2225B(j)\u2225\u222522 \u2212 b)+ } . Proof sketch for Theorem 1. For convenience we will write r1/2 to mean diag(r)\n1/2, and same for c. Using the trace norm factorization identity (1), we have\n2 \u2016X\u2016(R,C) = 2 sup r\u2208R,c\u2208C \u2225\u2225\u2225r1/2 \u00b7X \u00b7 c1/2\u2225\u2225\u2225 tr = sup r\u2208R,c\u2208C inf CD>=r1/2\u00b7X\u00b7c1/2 ( \u2016C\u20162F + \u2016D\u2016 2 F ) = sup\nr\u2208R,c\u2208C inf AB>=X (\u2225\u2225\u2225r1/2 \u00b7A\u2225\u2225\u22252 F + \u2225\u2225\u2225c1/2 \u00b7B\u2225\u2225\u22252 F ) \u2264 inf AB>=X ( sup r\u2208R \u2225\u2225\u2225r1/2A\u2225\u2225\u22252 F + sup c\u2208C \u2225\u2225\u2225c1/2B\u2225\u2225\u22252 F ) ,\nwhere for the next-to-last step we set C = r1/2A and D = c1/2B, and the last step follows because sup inf \u2264 inf sup always (weak duality). The reverse inequality holds as well (strong duality), and is proved in the Supplementary Materials, where we also prove the special-case result."}, {"heading": "4 An approximate convex hull and a learning guarantee", "text": "In this section, we look for theoretical bounds on error for the problem of estimating unobserved entries in a matrix Y that is approximately low-rank. Our results apply for either uniform or nonuniform sampling of entries from the matrix. We begin with a result comparing the (R, C)-norm unit ball to a convex hull of rank-1 matrices, which will be useful for proving our learning guarantee."}, {"heading": "4.1 Convex hull", "text": "To gain a better theoretical understanding of the (R, C) norm, we first need to define corresponding vector norms on Rn and Rm. For any u \u2208 Rn, let\n\u2016u\u2016R := \u221a\nsup r\u2208R \u2211 i riu2i = sup r\u2208R \u2225\u2225\u2225diag(r)1/2 \u00b7 u\u2225\u2225\u2225 2 .\nWe can think of this norm as a way to interpolate between the `2 and `\u221e vector norms. For example, if we choose R = R as defined in (5), then \u2016u\u2016R is equal to the root-mean-square of the \u22121 largest entries of u whenever \u22121 is an integer. Defining \u2016v\u2016C analogously for v \u2208 Rm, we can now relate these vector norms to the (R, C)-norm on matrices.\nTheorem 2. For any convex R \u2286 \u2206[n] and C \u2286 \u2206[m], the (R, C)-norm unit ball is bounded above and below by a convex hull as:\nConv { uv>:\u2016u\u2016R = \u2016v\u2016C = 1 } \u2286 { X :\u2016X\u2016(R,C) \u2264 1 } \u2286 KG\u00b7Conv { uv>:\u2016u\u2016R = \u2016v\u2016C = 1 } ,\nwhere KG \u2264 1.79 is Grothendieck\u2019s constant, and implicitly u \u2208 Rn, v \u2208 Rm.\nThis result is a nontrivial extension of Srebro and Shraibman [1]\u2019s analysis for the max norm and the trace norm. They show that the statement holds for the max norm, i.e. when R = \u2206[n] and C = \u2206[m], and that the trace norm unit ball is exactly equal to the corresponding convex hull (see Corollary 2 and Section 3.2 in their paper, respectively).\nProof sketch for Theorem 2. To prove the first inclusion, given anyX = uv> with \u2016u\u2016R = \u2016v\u2016C = 1, we apply the factorization result Theorem 1 to see that \u2016X\u2016(R,C) \u2264 1. Since the (R, C)-norm unit ball is convex, this is sufficient. For the second inclusion, we state a weighted version of Grothendieck\u2019s Inequality (proof in the Supplementary Materials):\nsup { \u3008Y, UV >\u3009 : U \u2208 Rn\u00d7k, V \u2208 Rm\u00d7k, \u2225\u2225U(i)\u2225\u22252 \u2264 ai \u2200i, \u2225\u2225V(j)\u2225\u22252 \u2264 bj \u2200j} = KG \u00b7 sup { \u3008Y, uv>\u3009 : u \u2208 Rn, v \u2208 Rm, |ui| \u2264 ai \u2200i, |vj | \u2264 bj \u2200j } .\nWe then apply this weighted inequality to the dual norm to the (R, C)-norm to prove the desired inclusion, as in Srebro and Shraibman [1]\u2019s work for the max norm case (see Corollary 2 in their paper). Details are given in the Supplementary Materials."}, {"heading": "4.2 Learning guarantee", "text": "We now give our main matrix reconstruction result, which provides error bounds for a family of norms interpolating between the max norm and the smoothed weighted trace norm.\nTheorem 3. Let p be any distribution on [n] \u00d7 [m]. Suppose that, for some \u03b3 \u2265 1, R \u2287 R\u00d71/2,\u03b3 and C \u2287 C \u00d7 1/2,\u03b3 , where these two sets are defined in (6). Let S = {(it, jt) : t = 1, . . . , s} be a random sample of locations in the matrix drawn i.i.d. from p, where s \u2265 n. Then, in expectation over the sample S,\u2211\nij\npij \u2223\u2223\u2223Yij \u2212 X\u0302ij\u2223\u2223\u2223 \u2264 inf \u2016X\u2016(R,C)\u2264 \u221a k \u2211 ij\npij |Yij \u2212Xij |\ufe38 \ufe37\ufe37 \ufe38 Approximation error +O\n(\u221a kn\ns\n) \u00b7 (\n1 + log(n) \u221a \u03b3 ) \ufe38 \ufe37\ufe37 \ufe38\nExcess error\n,\nwhere X\u0302 = arg min\u2016X\u2016(R,C)\u2264 \u221a k \u2211s t=1 |Yitjt \u2212Xitjt |. Additionally, if we assume that s \u2265\nn log(n), then in the excess risk bound, we can reduce the term log(n) to \u221a log(n).\nProof sketch for Theorem 3. The main idea is to use the convex hull formulation from Theorem 2 to show that, for any X with \u2016X\u2016(R,C) \u2264 \u221a k, there exists a decomposition X = X \u2032 + X \u2032\u2032 with \u2016X \u2032\u2016max \u2264 O( \u221a k) and \u2016X \u2032\u2032\u2016tr(p\u0303) \u2264 O( \u221a k/\u03b3), where p\u0303 represents the smoothed marginals with smoothing parameter \u03b6 = 1/2 as in (3). We then apply known bounds on the Rademacher complexity of the max norm unit ball [1] and the smoothed weighted trace norm unit ball [6], to bound the Rademacher complexity of { X : \u2016X\u2016(R,C) \u2264 \u221a k }\n. This then yields a learning guarantee by Theorem 8 of Bartlett and Mendelson [13]. Details are given in the Supplementary Materials.\nAs special cases of this theorem, we can re-derive the existing results for the max norm and smoothed weighted trace norm. Specifically, choosing \u03b3 = \u221e gives us an excess error term of order \u221a kn/s for the max norm, previously shown by [1], while setting \u03b3 = 1 yields an excess error term of order\u221a kn log(n)/s for the smoothed weighted trace norm as long as s \u2265 n log(n), as shown in [6].\nWhat advantage does this new result offer over the existing results for the max norm and for the smoothed weighted trace norm? To simplify the comparison, suppose we choose \u03b3 = log2(n), and defineR = R\u00d71/2,\u03b3 and C = C \u00d7 1/2,\u03b3 . Then, comparing to the max norm result (when \u03b3 =\u221e), we see\nthat the excess error term is the same in both cases (up to a constant), but the approximation error term may in general be much lower for the local max norm than for the max norm. Comparing next to the weighted trace norm (when \u03b3 = 1), we see that the excess error term is lower by a factor of log(n) for the local max norm. This may come at a cost of increasing the approximation error, but in general this increase will be very small. In particular, the local max norm result allows us to give a meaningful guarantee for a sample size s = \u0398 (kn), rather than requiring s \u2265 \u0398 (kn log(n)) as for any trace norm result, but with a hypothesis class significantly richer than the max norm constrained class (though not as rich as the trace norm constrained class)."}, {"heading": "5 Experiments", "text": "We test the local max norm on simulated and real matrix reconstruction tasks, and compare its performance to the max norm, the uniform and empirically-weighted trace norms, and the smoothed empirically-weighted trace norm."}, {"heading": "5.1 Simulations", "text": "We simulate n \u00d7 n noisy matrices for n = 30, 60, 120, 240, where the underlying signal has rank k = 2 or k = 4, and we observe s = 3kn entries (chosen uniformly without replacement). We performed 50 trials for each of the 8 combinations of (n, k).\nData For each trial, we randomly draw a matrix U \u2208 Rn\u00d7k by drawing each row uniformly at random from the unit sphere in Rn. We generate V \u2208 Rm\u00d7k similarly. We set Y = UV > + \u03c3 \u00b7 Z, where the noise matrix Z has i.i.d. standard normal entries and \u03c3 = 0.3 is a moderate noise level. We also divide the n2 entries of the matrix into sets S0 tS1 tS2 which consist of s = 3kn training entries, s validation entries, and n2 \u2212 2s test entries, respectively, chosen uniformly at random. Methods We use the two-parameter family of norms defined in (7), but replacing the true marginals pi\u2022 and p\u2022j with the empirical marginals p\u0302i\u2022 and p\u0302\u2022j . We consider \u03b6, \u03c4 \u2208 {0, 0.1, . . . , 0.9, 1}. For each (\u03b6, \u03c4) combination and each penalty parameter value \u03bb \u2208 {21, 22, . . . , 210}, we compute the fitted matrix\nX\u0302 = arg min {\u2211\n(i,j)\u2208S0 (Yij \u2212Xij) 2 + \u03bb \u00b7 \u2016X\u2016(R\u03b6,\u03c4 ,C\u03b6,\u03c4 )\n} . (10)\n(In fact, we use a rank-8 approximation to this optimization problem, as described in Section 3.) For each of the considered matrix norm methods, we use the validation set S1 to select the best combination of \u03b6, \u03c4 , and \u03bb, with restrictions on \u03b6 and/or \u03c4 as specified by the definition of the method (see Table 1). We then report the error of the resulting fitted matrix on the test set S2.\nResults The results for these simulations are displayed in Figure 1. We see that the local max norm results in lower error than any of the tested existing norms, across all the settings used."}, {"heading": "5.2 Movie ratings data", "text": "We next compare several different matrix norms on two collaborative filtering movie ratings datasets, the Netflix [14] and MovieLens [15] datasets. The sizes of the data sets, and the split of the ratings into training, validation and test sets3, are:\nDataset # users # movies Training set Validation set Test set Netflix 480,189 17,770 100,380,507 100,000 1,408,395 MovieLens 71,567 10,681 8,900,054 100,000 1,000,000\n3 For Netflix, the test set we use is their \u201cqualification set\u201d, designed for a more uniform distribution of ratings across users relative to the training set. For MovieLens, we choose our test set at random from the available data.\nWe test the local max norm given in (7) with \u03b6 \u2208 {0, 0.05, 0.1, 0.15, 0.2} and \u03c4 \u2208 {0, 0.05, 0.1}. We also test \u03c4 = 1 (the max norm\u2014here \u03b6 is arbitrary) and \u03b6 = 1, \u03c4 = 0 (the uniform trace norm). We follow the test protocol of [6], with a rank-30 approximation to the optimization problem (10).\nTable 2 shows root mean squared error (RMSE) for the experiments. For both the MovieLens and Netflix data, the local max norm with \u03c4 = 0.05 and \u03b6 = 0.05 gives strictly better accuracy than any previously-known norm studied in this setting. (In practice, we can use a validation set to reliably select good values for the \u03c4 and \u03b6 parameters4.) For the MovieLens data, the local max norm achieves RMSE of 0.7822, compared to 0.7831 achieved by the smoothed empirically-weighted trace norm with \u03b6 = 0.10, which gives the best result among the previously-known norms. For the Netflix dataset the local max norm achieves RMSE of 0.9090, improving upon the previous best result of 0.9096 achieved by the smoothed empirically-weighted trace norm [6]."}, {"heading": "6 Summary", "text": "In this paper, we introduce a unifying family of matrix norms, called the \u201clocal max\u201d norms, that generalizes existing methods for matrix reconstruction, such as the max norm and trace norm. We examine some interesting sub-families of local max norms, and consider several different options for interpolating between the trace (or smoothed weighted trace) and max norms. We find norms lying strictly between the trace norm and the max norm that give improved accuracy in matrix reconstruction for both simulated data and real movie ratings data. We show that regularizing with any local max norm is fairly simple to optimize, and give a theoretical result suggesting improved matrix reconstruction using new norms in this family.\n4 To check this, we subsample half of the test data at random, and use it as a validation set to choose (\u03b6, \u03c4) for each method (as specified in Table 1). We then evaluate error on the remaining half of the test data. For MovieLens, the local max norm gives an RMSE of 0.7820 with selected parameter values \u03b6 = \u03c4 = 0.05, as compared to an RMSE of 0.7829 with selected smoothing parameter \u03b6 = 0.10 for the smoothed weighted trace norm. For Netflix, the local max norm gives an RMSE of 0.9093 with \u03b6 = \u03c4 = 0.05, while the smoothed weighted trace norm gives an RMSE of 0.9098 with \u03b6 = 0.05. The other tested methods give higher error on both datasets."}, {"heading": "A Proof of Theorem 1", "text": "Special case: element-wise upper bounds First, we assume that the general result is true, i.e.\n2 \u2016X\u2016(R,C) = inf AB>=X sup r\u2208R \u2211 i ri \u2225\u2225A(i)\u2225\u222522 + sup c\u2208C \u2211 j cj \u2225\u2225B(j)\u2225\u222522  , (11) and prove the result in the special case, where"}, {"heading": "R = {r \u2208 \u2206[n] : ri \u2264 Ri \u2200i} and C = {c \u2208 \u2206[m] : cj \u2264 Cj \u2200j} .", "text": "Using strong duality for linear programs, we have\nsup r\u2208R \u2211 i ri \u2225\u2225A(i)\u2225\u222522 = sup r\u2208Rn+ {\u2211 i ri \u2225\u2225A(i)\u2225\u222522 : ri \u2264 Ri, \u2211 i ri = 1 } = inf a\u2208R,a1\u2208Rn+ { a+R>a1 : a+ a1i \u2265 \u2225\u2225A(i)\u2225\u222522 \u2200i} .\nIn this last line, if we fix a and want to minimize over a1 \u2208 Rn+, it is clear that the infimum is obtained by setting a1i = ( \u2225\u2225A(i)\u2225\u222522 \u2212 a)+ for each i. This proves that sup r\u2208R \u2211 i ri \u2225\u2225A(i)\u2225\u222522 = infa\u2208R { a+ \u2211 i Ri (\u2225\u2225A(i)\u2225\u222522 \u2212 a)+ } .\nApplying the same reasoning to the columns and plugging everything in to (11), we get\n2 \u2016X\u2016(R,C) = inf AB>=X, a,b\u2208R\n{ a+ \u2211 i Ri (\u2225\u2225A(i)\u2225\u222522 \u2212 a)+ + b+\u2211 j Cj (\u2225\u2225B(j)\u2225\u222522 \u2212 b)+ } .\nGeneral factorization result In the proof sketch given in the main paper, we showed that\n2 \u2016X\u2016(R,C) \u2264 inf AB>=X ( sup r\u2208R \u2225\u2225\u2225r1/2A\u2225\u2225\u22252 F + sup c\u2208C \u2225\u2225\u2225c1/2B\u2225\u2225\u22252 F ) .\nWe now want to prove the reverse inequality. Since \u2016X\u2016(R,C) = \u2016X\u2016(R,C) by definition (where S denotes the closure of a set S), we can assume without loss of generality that R and C are both closed (and compact) sets.\nFirst, we restrict our attention to a special case (the \u201cpositive case\u201d), where we assume that for all r \u2208 R and all c \u2208 C, ri > 0 and cj > 0 for all i and j. (We will treat the general case below.) Therefore, since \u2016X\u2016tr(r,c) is continuous as a function of (r, c) for any fixed X and since R and C are closed, we must have some r? \u2208 R and c? \u2208 C such that \u2016X\u2016(R,C) = \u2016X\u2016tr(r?,c?), with r?i > 0 for all i and c?j > 0 for all j.\nNext, let UDV > = r?1/2 \u00b7X \u00b7 c?1/2 be a singular value decomposition, and let A? = r?\u22121/2UD1/2 and B? = c?\u22121/2V D1/2. Then A?B?> = X , and\u2225\u2225\u2225r?1/2A?\u2225\u2225\u22252 F = \u2225\u2225\u2225UD1/2\u2225\u2225\u22252 F = trace(UDU>) = trace(D) = \u2016X\u2016tr(r?,c?) = \u2016X\u2016(R,C) .\nBelow, we will show that\nr? = arg max r\u2208R \u2225\u2225\u2225r1/2A?\u2225\u2225\u22252 F . (12)\nThis will imply that \u2016X\u2016(R,C) = supr\u2208R \u2225\u2225r1/2A?\u2225\u22252 F , and following the same reasoning for B?, we will have proved\n2 \u2016X\u2016(R,C) = (\nsup r\u2208R \u2225\u2225\u2225r1/2A?\u2225\u2225\u22252 F + sup c\u2208C \u2225\u2225\u2225c1/2B?\u2225\u2225\u22252 F ) \u2265 inf AB>=X ( sup r\u2208R \u2225\u2225\u2225r1/2A\u2225\u2225\u22252 F + sup c\u2208C \u2225\u2225\u2225c1/2B\u2225\u2225\u22252 F ) ,\nwhich is sufficient. It remains only to prove (12). Take any r \u2208 R with r 6= r? and let w = r\u2212 r?. We have \u2225\u2225\u2225r1/2A\u2225\u2225\u22252 F \u2212 \u2225\u2225\u2225r?1/2A\u2225\u2225\u22252 F = \u2211 i wi \u2225\u2225A(i)\u2225\u222522 = \u2211 i wi r?i \u00b7 (UDU>)ii ,\nand it will be sufficient to prove that this quantity is\u2264 0. To do this, we first define, for any t \u2208 [0, 1],\nf(t) := \u2211 i \u221a 1 + t \u00b7 wi r?i \u00b7 (UDU>)ii = trace (( r? + tw r? )1/2 UDU> ) .\nUsing the fact that trace(\u00b7) \u2264 \u2016\u00b7\u2016tr for all matrices, we have\nf(t) \u2264 \u2225\u2225\u2225\u2225\u2225 ( r? + tw r? )1/2 UDU> \u2225\u2225\u2225\u2225\u2225 tr = \u2225\u2225\u2225(r? + tw)1/2Xc?1/2 \u00b7 V U>\u2225\u2225\u2225 tr\n= \u2225\u2225\u2225(r? + tw)1/2Xc?1/2\u2225\u2225\u2225\ntr = \u2016X\u2016tr(r?+tw,c?) \u2264 \u2016X\u2016(R,C) = \u2211 i (UDU>)ii = f(0) ,\nwhere the last inequality comes from the fact that r? + tw \u2208 R by convexity ofR. Therefore,\n0 \u2265 d dt f(t) \u2223\u2223\u2223\u2223 t=0 = d dt (\u2211 i \u221a 1 + t \u00b7 wi r?i \u00b7 (UDU>)ii ) \u2223\u2223\u2223\u2223 t=0 = 1 2 \u00b7 \u2211 i wi r?i \u00b7 (UDU>)ii ,\nas desired. (Here we take the right-sided derivative, i.e. taking a limit as t approaches zero from the right, since f(t) is only defined for t \u2208 [0, 1].) This concludes the proof for the positive case. Next, we prove that the general factorization (11) hold in the general case, where we might have R 6\u2282 Rn++ and/or C 6\u2282 Rm++. If for any i \u2208 [n] we have ri = 0 for all r \u2208 R, we can discard this row of X , and same for any j \u2208 [m]. Therefore, without loss of generality, for all i \u2208 [n] there is some r(i) \u2208 R with r(i)i > 0. Taking a convex combination, r+ = 1n \u2211 i r\n(i) \u2208 R, we have r+ \u2208 R \u2229 Rn++. Similarly, we can construct c+ \u2208 C \u2229 Rm++.\nFix any > 0, and let \u03b4 = min{mini r+i ,minj c + j } \u00b7 2(1+ ) > 0, and define closed subsets R0 = { r \u2208 R : min\ni ri \u2265 \u03b4\n} \u2286 R and C0 = { c \u2208 C : min\ni ci \u2265 \u03b4\n} \u2286 C .\nSince we know that the factorization result holds for the \u201cpositive case\u201d, we have\ninf AB>=X ( sup r\u2208R0 \u2225\u2225\u2225r1/2A\u2225\u2225\u22252 F + sup c\u2208C0 \u2225\u2225\u2225c1/2B\u2225\u2225\u22252 F ) = 2 \u2016X\u2016(R0,C0)\n= 2 sup r\u2208R0,c\u2208C0 \u2225\u2225\u2225r1/2Xc1/2\u2225\u2225\u2225 tr \u2264 2 sup r\u2208R,c\u2208C \u2225\u2225\u2225r1/2Xc1/2\u2225\u2225\u2225 tr = 2 \u2016X\u2016(R,C) .\nNow choose any factorization A\u0303B\u0303> = X such that( sup r\u2208R0 \u2225\u2225\u2225r1/2A\u0303\u2225\u2225\u22252 F + sup c\u2208C0 \u2225\u2225\u2225c1/2B\u0303\u2225\u2225\u22252 F ) \u2264 2 sup r\u2208R,c\u2208C \u2225\u2225\u2225r1/2Xc1/2\u2225\u2225\u2225 tr (1 + /2) . (13)\nNext, we need to show that supr\u2208R \u2225\u2225\u2225r1/2A\u0303\u2225\u2225\u22252\nF is not much larger than supr\u2208R0 \u2225\u2225\u2225r1/2A\u0303\u2225\u2225\u22252 F (and same\nfor B\u0303). Choose any r\u2032 \u2208 R, and let r\u2032\u2032 = (\n1\u2212 \u03b4 mini r + i\n) r\u2032 + ( \u03b4\nmini r + i\n) r+ \u2208 R. Then\nmin i\nr\u2032\u2032i \u2265 (\n\u03b4\nmini r + i\n) min i r+i = \u03b4 ,\nand so r\u2032\u2032 \u2208 R0. We also have r\u2032i \u2264 (\n1\u2212 \u03b4 mini r + i )\u22121 r\u2032\u2032i for all i. Therefore,\u2225\u2225\u2225r\u20321/2A\u0303\u2225\u2225\u2225 F \u2264 ( 1\u2212 \u03b4 mini r + i )\u22121/2 \u2225\u2225\u2225r\u2032\u20321/2A\u0303\u2225\u2225\u2225 F \u2264 ( 1\u2212 \u03b4 mini r + i )\u22121/2 sup r\u2208R0 \u2225\u2225\u2225r1/2A\u0303\u2225\u2225\u2225 F .\nSince this is true for any r\u2032 \u2208 R, applying the definition of \u03b4, we have\nsup r\u2208R \u2225\u2225\u2225r1/2A\u0303\u2225\u2225\u2225 F \u2264 ( 1\u2212 \u03b4 mini r + i )\u22121/2 sup r\u2208R0 \u2225\u2225\u2225r1/2A\u0303\u2225\u2225\u2225 F \u2264 ( 1 + /2 1 + )\u22121/2 sup r\u2208R0 \u2225\u2225\u2225r1/2A\u0303\u2225\u2225\u2225 F .\nApplying the same reasoning for B\u0303 and then plugging in the bound (13), we have\ninf AB>=X ( sup r\u2208R \u2225\u2225\u2225r1/2A\u2225\u2225\u22252 F + sup c\u2208C \u2225\u2225\u2225c1/2B\u2225\u2225\u22252 F ) \u2264 ( sup r\u2208R \u2225\u2225\u2225r1/2A\u0303\u2225\u2225\u2225 F + sup c\u2208C \u2225\u2225\u2225c1/2B\u0303\u2225\u2225\u22252 F ) \u2264 ( 1 + /2\n1 +\n)\u22121 \u00b7 (\nsup r\u2208R0 \u2225\u2225\u2225r1/2A\u0303\u2225\u2225\u22252 F + sup c\u2208C0 \u2225\u2225\u2225c1/2B\u0303\u2225\u2225\u22252 F ) \u2264 ( 1 + /2\n1 +\n)\u22121 (1 + /2) \u00b7 2 \u2016X\u2016(R,C) = (1 + ) \u00b7 2 \u2016X\u2016(R,C) .\nSince this analysis holds for arbitrary > 0, this proves the desired result, that\ninf AB>=X ( sup r\u2208R \u2225\u2225\u2225r1/2A\u2225\u2225\u22252 F + sup c\u2208C \u2225\u2225\u2225c1/2B\u2225\u2225\u22252 F ) \u2264 2 \u2016X\u2016(R,C) ."}, {"heading": "B Proof of Theorem 2", "text": "We follow similar techniques as used by Srebro and Shraibman [1] in their proof of the analogous result for the max norm. We need to show that\nConv { uv> : u \u2208 Rn, v \u2208 Rm, \u2016u\u2016R = \u2016v\u2016C = 1 } \u2286 { X : \u2016X\u2016(R,C) \u2264 1 } \u2286\nKG \u00b7 Conv { uv> : u \u2208 Rn, v \u2208 Rm, \u2016u\u2016R = \u2016v\u2016C = 1 } .\nFor the left-hand inclusion, since \u2016\u00b7\u2016(R,C) is a norm and therefore the constraint \u2016X\u2016(R,C) \u2264 1 is convex, it is sufficient to show that \u2225\u2225uv>\u2225\u2225 (R,C) \u2264 1 for any u \u2208 R\nn, v \u2208 Rm with \u2016u\u2016R = \u2016v\u2016C = 1. This is a trivial consequence of the factorization result in Theorem 1.\nNow we prove the right-hand inclusion. Grothendieck\u2019s Inequality states that, for any Y \u2208 Rn\u00d7m and for any dimension k,\nsup { \u3008Y,UV >\u3009 : U \u2208 Rn\u00d7k, V \u2208 Rm\u00d7k, \u2225\u2225U(i)\u2225\u22252 \u2264 1 \u2200i, \u2225\u2225V(j)\u2225\u22252 \u2264 1 \u2200j} \u2264 KG \u00b7 sup { \u3008Y, uv>\u3009 : u \u2208 Rn, v \u2208 Rm, |ui| \u2264 1 \u2200i, |vj | \u2264 1 \u2200j } ,\nwhere KG \u2208 (1.67, 1.79) is Grothendieck\u2019s constant. We now extend this to a slightly more general form. Take any a \u2208 Rn+ and b \u2208 Rm+ . Then, setting U\u0303 = diag(a)+U and V\u0303 = diag(b)+V (where M+ is the pseudoinverse of M ), and same for u\u0303 and v\u0303, we see that\nsup { \u3008Y, UV >\u3009 : U \u2208 Rn\u00d7k, V \u2208 Rm\u00d7k, \u2225\u2225U(i)\u2225\u22252 \u2264 ai \u2200i, \u2225\u2225V(j)\u2225\u22252 \u2264 bj \u2200j} = sup { \u3008diag(a) \u00b7 Y \u00b7 diag(b), U\u0303 V\u0303 >\u3009 : U\u0303 \u2208 Rn\u00d7k, V\u0303 \u2208 Rm\u00d7k, \u2225\u2225\u2225U\u0303(i)\u2225\u2225\u2225 2 \u2264 1 \u2200i, \u2225\u2225\u2225V\u0303(j)\u2225\u2225\u2225 2 \u2264 1 \u2200j\n} \u2264 KG \u00b7 sup { \u3008diag(a) \u00b7 Y \u00b7 diag(b), u\u0303v\u0303>\u3009 : u\u0303 \u2208 Rn, v\u0303 \u2208 Rm, |u\u0303i| \u2264 1 \u2200i, |v\u0303j | \u2264 1 \u2200j\n} = KG \u00b7 sup { \u3008Y, uv>\u3009 : u \u2208 Rn, v \u2208 Rm, |ui| \u2264 ai \u2200i, |vj | \u2264 bj \u2200j } . (14)\nNow take any Y \u2208 Rn\u00d7m. Let \u2016\u00b7\u2016\u2217(R,C) be the dual norm to the (R, C)-norm. To bound this dual norm of Y , we apply the factorization result of Theorem 1:\n\u2016Y \u2016\u2217(R,C) = sup \u2016X\u2016(R,C)\u22641 \u3008Y,X\u3009\n= sup U,V \u3008Y, UV >\u3009 : 12 sup r\u2208R \u2211 i ri \u2225\u2225U(i)\u2225\u222522 + sup c\u2208C \u2211 j cj \u2225\u2225V(j)\u2225\u222522  \u2264 1 \n(\u2217) = sup\nU,V \u3008Y, UV >\u3009 : supr\u2208R\u2211i ri \u2225\u2225U(i)\u2225\u222522 = sup c\u2208C \u2211 j cj \u2225\u2225V(j)\u2225\u222522 \u2264 1  = sup a\u2208Rn+:\u2016a\u2016R\u22641 b\u2208Rm+ :\u2016b\u2016C\u22641 sup U,V { \u3008Y, UV >\u3009 :\n\u2225\u2225U(i)\u2225\u22252 \u2264 ai \u2200i, \u2225\u2225V(j)\u2225\u22252 \u2264 bj \u2200j} \u2264 KG \u00b7 sup\na\u2208Rn+:\u2016a\u2016R\u22641 b\u2208Rm+ :\u2016b\u2016C\u22641\nsup U,V\n{ \u3008Y, uv>\u3009 : |ui| \u2264 ai \u2200i, |vj | \u2264 bj \u2200j } = KG \u00b7 sup\nu,v\n{ \u3008Y, uv>\u3009 : \u2016u\u2016R \u2264 1, \u2016v\u2016C \u2264 1 } = KG \u00b7 sup\nX\n{ \u3008Y,X\u3009 : X \u2208 Conv { uv> : u \u2208 Rn, v \u2208 Rm, \u2016u\u2016R = \u2016v\u2016C = 1 }} = sup\nX\n{ \u3008Y,X\u3009 : X \u2208 KG \u00b7 Conv { uv> : u \u2208 Rn, v \u2208 Rm, \u2016u\u2016R = \u2016v\u2016C = 1 }} .\nAs in [1], this is sufficient to prove the result. Above, the step marked (*) is true because, given any U and V with\n1\n2 sup r\u2208R \u2211 i ri \u2225\u2225U(i)\u2225\u222522 + sup c\u2208C \u2211 j cj \u2225\u2225V(j)\u2225\u222522  \u2264 1 ,\nwe can replace U and V with U \u2032 := U \u00b7 \u03c9 and V \u2032 := V \u00b7 \u03c9\u22121, where \u03c9 := 4 \u221a supc\u2208C \u2211 j cj\u2016V(j)\u201622\nsupr\u2208R \u2211 i ri\u2016U(i)\u201622 .\nThis will give U \u2032V \u2032> = UV >, and\nsup r\u2208R \u2211 i ri \u2225\u2225\u2225U \u2032(i)\u2225\u2225\u22252 2 = sup c\u2208C \u2211 j cj \u2225\u2225\u2225V \u2032(j)\u2225\u2225\u22252 2 = \u221a sup r\u2208R \u2211 i ri \u2225\u2225U(i)\u2225\u222522 \u00b7 sup c\u2208C \u2211 j cj \u2225\u2225V(j)\u2225\u222522\n\u2264 1 2 sup r\u2208R \u2211 i ri \u2225\u2225U(i)\u2225\u222522 + sup c\u2208C \u2211 j cj \u2225\u2225V(j)\u2225\u222522  \u2264 1 ."}, {"heading": "C Proof of Theorem 3", "text": "Following the strategy of Srebro & Shraibman (2005), we will use the Rademacher complexity to bound this excess risk. By Theorem 8 of Bartlett & Mendelson (2002)5, we know that\nES \u2211 ij pij \u2223\u2223\u2223Yij \u2212 X\u0302ij\u2223\u2223\u2223\u2212 inf \u2016X\u2016(R,C)\u2264 \u221a k \u2211 ij pij |Yij \u2212Xij |  = O ( ES [ R\u0302S ({ X \u2208 Rn\u00d7m : \u2016X\u2016(R,C) \u2264 \u221a k })]) , (15)\nwhere the expected Rademacher complexity is defined as\nES [ R\u0302S ({ X \u2208 Rn\u00d7m : \u2016X\u2016(R,C) \u2264 \u221a k })] := 1\ns ES,\u03bd  sup \u2016X\u2016(R,C)\u2264 \u221a k \u2211 t \u03bdt \u00b7Xitjt  , where \u03bd \u2208 {\u00b11}s is a random vector of independent unbiased signs, generated independently from S.\nNow we bound the Rademacher complexity. By scaling, it is sufficient to consider the case k = 1. The main idea for this proof is to first show that, for any X with \u2016X\u2016(R,C) \u2264 1, we can decompose X into a sum X \u2032 + X \u2032\u2032 where \u2016X \u2032\u2016max \u2264 KG and \u2016X \u2032\u2032\u2016tr(p\u0303) \u2264 2KG\u03b3\u2212\n1/2, where p\u0303 represents the smoothed row and column marginals with smoothing parameter \u03b6 = 1/2, and where KG \u2264 1.79 is Grothendieck\u2019s constant. We will then use known Rademacher complexity bounds for the classes of matrices that have bounded max norm and bounded smoothed weighted trace norm.\nTo construct the decomposition of X , we start with a vector decomposition lemma, proved below. Lemma 1. SupposeR \u2287 R\u00d71/2,\u03b3 . Then for any u \u2208 R\nn with \u2016u\u2016R = 1, we can decompose u into a sum u = u\u2032 + u\u2032\u2032 such that \u2016u\u2032\u2016\u221e \u2264 1 and \u2016u\u2032\u2032\u2016p\u0303row := \u2211 i p\u0303i\u2022u \u2032\u2032 i 2 \u2264 \u03b3\u22121/2.\nNext, by Theorem 2, we can write\nX = KG \u00b7 \u221e\u2211 l=1 tl \u00b7 ulv>l ,\nwhere tl \u2265 0, \u2211\u221e l=1 tl = 1, and \u2016ul\u2016R = \u2016vl\u2016C = 1 for all l. Applying Lemma 1 to ul and to vl for each l, we can write ul = u\u2032l + u \u2032\u2032 l and vl = v \u2032 l + v \u2032\u2032 l , where\n\u2016u\u2032l\u2016\u221e \u2264 1, \u2016u \u2032\u2032 l \u2016p\u0303row \u2264 \u03b3 \u22121/2, \u2016v\u2032l\u2016\u221e \u2264 1, \u2016v \u2032\u2032 l \u2016p\u0303col \u2264 \u03b3 \u22121/2 .\nThen\nX = KG \u00b7 ( \u221e\u2211 l=1 tl \u00b7 u\u2032lv\u2032l> + \u221e\u2211 l=1 tl \u00b7 u\u2032lv\u2032\u2032l > + \u221e\u2211 l=1 tl \u00b7 u\u2032\u2032l vl> ) =: KG (X1 +X2 +X3) .\n5 The statement of their theorem gives a result that holds with high probability, but in the proof of this result they derive a bound in expectation, which we use here.\nFurthermore, \u2016u\u2032l\u2016p\u0303row \u2264 \u2016u \u2032 l\u2016\u221e \u2264 1, and \u2016vl\u2016p\u0303row \u2264 \u2016vl\u2016C \u2264 1. Applying Srebro and Shraibman [1]\u2019s convex hull bounds for the trace norm and max norm (stated in Section 4 of the main paper), we see that \u2016X1\u2016max \u2264 1, and that that \u2016Xi\u2016tr(p\u0303) \u2264 \u03b3\u2212\n1/2 for i = 2, 3. Defining X \u2032 = X1 and X \u2032\u2032 = X2 +X3, we have the desired decomposition.\nApplying this result to every X in the class { X \u2208 Rn\u00d7m : \u2016X\u2016(R,C) \u2264 1 } , we see that\nES [ R\u0302S ({ X \u2208 Rn\u00d7m : \u2016X\u2016(R,C) \u2264 1 })] \u2264 ES [ R\u0302S ({X \u2032 : \u2016X \u2032\u2016max \u2264 KG}) ] + ES [ R\u0302S ({ X \u2032\u2032 : \u2016X \u2032\u2032\u2016tr(p\u0303) \u2264 KG \u00b7 2\u03b3 \u22121/2 })]\n\u2264 KG \u00b7 O (\u221a n\ns\n) +KG \u00b7 2\u03b3\u2212 1/2 \u00b7 O (\u221a n log(n)\ns + n log(n) s\n) ,\nwhere the last step uses bounds on the Rademacher complexity of the max norm and weighted trace norm unit balls, shown in Theorem 5 of [1] and Theorem 3 of [6], respectively. Finally, we want to deal with the last term, n log(n)s , that is outside the square root. Since s \u2265 n by assumption, we\nhave n log(n)s \u2264 \u221a n log2(n) s , and if s \u2265 n log(n), then we can improve this to n log(n) s \u2264 \u221a n log(n)\ns . Returning to (15) and plugging in our bound on the Rademacher complexity, this proves the desired bound on the excess risk.\nC.1 Proof of Lemma 1\nFor u \u2208 Rn with \u2016u\u2016R = 1, we need to find a decomposition u = u\u2032 + u\u2032\u2032 such that \u2016u\u2032\u2016\u221e \u2264 1 and \u2016u\u2032\u2032\u2016p\u0303row = \u221a\u2211 i p\u0303i\u2022u \u2032\u2032 i\n2 \u2264 \u03b3\u22121/2. Without loss of generality, assume |u1| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |un|. Find N \u2208 {1, . . . , n} and t \u2208 (0, 1] so that \u2211N\u22121 i=1 p\u0303i\u2022 + t \u00b7 p\u0303N\u2022 = \u03b3\u22121, and let\nr = \u03b3 \u00b7 (p\u03031\u2022, . . . , p\u0303(N\u22121)\u2022, t \u00b7 p\u0303N\u2022, 0, . . . , 0) \u2208 \u2206[n] .\nClearly, ri \u2264 \u03b3 \u00b7 p\u0303i\u2022 for all i, and so r \u2208 R\u00d71/2,\u03b3 \u2286 R. Now let u\u2032\u2032 = (u1, . . . , uN\u22121, \u221a t \u00b7 uN , 0, . . . , 0), and set u\u2032 = u\u2212 u\u2032\u2032. We then calculate\n\u2016u\u2032\u2032\u20162p\u0303row = N\u22121\u2211 i=1 p\u0303i\u2022u 2 i + t \u00b7 p\u0303N\u2022u2N = \u03b3\u22121 n\u2211 i=1 riu 2 i \u2264 \u03b3\u22121 \u2016u\u2016 2 R \u2264 \u03b3 \u22121 .\nFinally, we want to show that \u2016u\u2032\u2016\u221e \u2264 1. Since u\u2032i = 0 for i < N , we only need to bound |u\u2032i| for each i \u2265 N . We have\n1 = \u2016u\u20162R \u2265 n\u2211\ni\u2032=1\nri\u2032u 2 i\u2032 \u2265 N\u2211 i\u2032=1 ri\u2032u 2 i\u2032 (\u2217) \u2265 u2i \u00b7 N\u2211 i\u2032=1 ri\u2032 (#) = u2i \u2265 u\u2032i2 ,\nwhere the step marked (*) uses the fact that |ui\u2032 | \u2265 |ui| for all i\u2032 \u2264 N , and the step marked (#) comes from the fact that r is supported on {1, . . . , N}. This is sufficient."}, {"heading": "D Proof of Proposition 1", "text": "Let L0 = Loss(X\u0302). Then, by definition, X\u0302 = arg min { Penalty(\u03b2,\u03c4)(X) : Loss(X) \u2264 L0 } .\nThen to prove the lemma, it is sufficient to show that for some t \u2208 [0, 1], X\u0302 = arg min { \u2016X\u2016(R(t),C(t)) : Loss(X) \u2264 L0 } ,\nwhere we set R(t) = { r \u2208 \u2206[n] : ri \u2265\nt 1 + (n\u2212 1)t \u2200i } , C(t) = { c \u2208 \u2206[m] : cj \u2265\nt 1 + (m\u2212 1)t \u2200j } .\nTrivially, we can rephrase these definitions as R(t) = { t\n1 + (n\u2212 1) \u00b7 t \u00b7 (1, . . . , 1) + 1\u2212 t 1 + (n\u2212 1) \u00b7 t \u00b7 r : r \u2208 \u2206[n]\n} and\nC(t) = {\nt 1 + (m\u2212 1) \u00b7 t \u00b7 (1, . . . , 1) + 1\u2212 t 1 + (m\u2212 1) \u00b7 t \u00b7 c : c \u2208 \u2206[m]\n} . (16)\nNote that for any vectors u \u2208 Rn+ and v \u2208 Rm+ ,\nsup r\u2208\u2206[n] \u2211 i riui = max i ui and sup c\u2208\u2206[m] \u2211 j cjvj = max j vj . (17)\nApplying the SDP formulation of the local max norm (proved in Lemma 2 below), we have\n\u2016X\u2016(R(t),C(t)) = 1\n2 inf  supr\u2208R(t) \u2211 i riUii + sup c\u2208C(t) \u2211 j cjVjj : ( U X X> V ) 0  By (16) and (17)\n= 1\n2 inf\n{ t 1 + (n\u2212 1) \u00b7 t \u00b7 \u2211 i Uii + 1\u2212 t 1 + (n\u2212 1) \u00b7 t max i Uii\n+ t 1 + (m\u2212 1) \u00b7 t \u00b7 \u2211 j Vjj + 1\u2212 t 1 + (m\u2212 1) \u00b7 t max j Vjj : ( U X X> V ) 0 }\n= \u03c9t 2 inf { t \u2211 i Aii + (1\u2212 t) max i Aii + t \u2211 j Bjj + (1\u2212 t) max j Bjj : ( A X X> B ) 0 }\n= \u03c9t 2 inf\n{ (1\u2212 t) \u00b7M(A,B) + t \u00b7 T(A,B) : X \u2208 XA,B } , (18)\nwhere for the next-to-last step, we define\nA = U \u00b7 \u221a 1 + (m\u2212 1) \u00b7 t 1 + (n\u2212 1) \u00b7 t , B = V \u00b7 \u221a 1 + (n\u2212 1) \u00b7 t 1 + (m\u2212 1) \u00b7 t , \u03c9t = 1\u221a (1 + (n\u2212 1) \u00b7 t)(1 + (m\u2212 1) \u00b7 t) ,\nand for the last step, we define\nT(A,B) = trace(A) + trace(B), M(A,B) = max i Aii + max j Bjj ,\nand\nXA,B = { X : ( A X X> B ) 0 } .\nNext, we compare this to the (\u03b2, \u03c4) penalty formulated in our main paper. Recall\nPenalty(\u03b2,\u03c4)(X) = inf X=AB>  \u221a max i \u2225\u2225A(i)\u2225\u222522 + maxj \u2225\u2225B(j)\u2225\u222522 \u00b7 \u221a\u2211\ni \u2225\u2225A(i)\u2225\u222522 +\u2211 j \u2225\u2225B(j)\u2225\u222522  .\nApplying Lemma 3 below, we can obtain an equivalent SDP formulation of the penalty\nPenalty(\u03b2,\u03c4)(X) = inf A,B\n{\u221a M(A,B) \u00b7 \u221a T(A,B) : X \u2208 XA,B } . (19)\nSince M(A,B) \u2264 T(A,B) \u2264 max{n,m}M(A,B), and since for any x, y > 0 we know \u221axy \u2264 1 2 ( \u03b1 \u00b7 x+ \u03b1\u22121 \u00b7 y ) for any \u03b1 > 0 with equality attained when \u03b1 = \u221a y/x, we see that\nPenalty(\u03b2,\u03c4)(X\u0302) = 1\n2 inf A,B\n{ inf\n\u03b1\u2208[1, \u221a max{n,m}]\n{ \u03b1 \u00b7M(A,B) + \u03b1\u22121 \u00b7 T(A,B) } : X\u0302 \u2208 XA,B\n}\n= inf \u03b1\u2208[1, \u221a max{n,m}]\n[ 1\n2 inf A,B\n{ \u03b1 \u00b7M(A,B) + \u03b1\u22121 \u00b7 T(A,B) : X\u0302 \u2208 XA,B }] .\nSince the quantity inside the square brackets is nonnegative and is continuous in \u03b1, and we are minimizing over \u03b1 in a compact set, the infimum is attained at some \u03b1\u0302, so we can write\nPenalty(\u03b2,\u03c4)(X\u0302) = 1\n2 inf A,B\n{ \u03b1\u0302 \u00b7M(A,B) + \u03b1\u0302\u22121 \u00b7 T(A,B) : X\u0302 \u2208 XA,B } .\nRecall that X\u0302 minimizes Penalty(\u03b2,\u03c4)(X) subject to the constraint Loss(X) \u2264 L0. Setting t := \u03b1\u0302\u22121 \u03b1\u0302+\u03b1\u0302\u22121 , we get\nX\u0302 \u2208 arg min X { inf A,B { \u03b1\u0302 \u00b7M(A,B) + \u03b1\u0302\u22121 \u00b7 T(A,B) : X \u2208 XA,B } : Loss(X) \u2264 L0 } = arg min\nX { inf A,B { \u03b1\u0302 \u03b1\u0302+ \u03b1\u0302\u22121 \u00b7M(A,B) + \u03b1\u0302 \u22121 \u03b1\u0302+ \u03b1\u0302\u22121 \u00b7 T(A,B) : X \u2208 XA,B } : Loss(X) \u2264 L0 } = arg min\nX { inf A,B {(1\u2212 t) \u00b7M(A,B) + t \u00b7 T(A,B) : X \u2208 XA,B} : Loss(X) \u2264 L0 } = arg min\nX\n{ \u2016X\u2016(R(t),C(t)) : Loss(X) \u2264 L0 } ,\nas desired."}, {"heading": "E Computing the local max norm with an SDP", "text": "Lemma 2. Suppose R and C are convex, and are defined by SDP-representable constraints. Then the (R, C)-norm can be calculated with the semidefinite program\n\u2016X\u2016(R,C) = 1\n2 inf supr\u2208R\u2211i riAii + supc\u2208C \u2211 j cjBjj : ( A X X> B ) 0  . In the special case whereR and C are defined as in (8) in the main paper, then the norm is given by\n\u2016X\u2016(R,C) = 1\n2 inf\n{ a+R>a1 + b+ C >b1 : a1i \u2265 0 and a+ a1i \u2265 Aii \u2200i,\nb1j \u2265 0 and b+ b1j \u2265 Bjj \u2200j, (\nA X X> B\n) 0 } .\nProof. For the general case, based on Theorem 1 in the main paper, we only need to show that\ninf supr\u2208R\u2211i riAii + supc\u2208C \u2211 j cjBjj : ( A X X> B ) 0  = inf sup r\u2208R \u2211 i ri \u2225\u2225A(i)\u2225\u222522 + sup c\u2208C \u2211 j cj \u2225\u2225B(j)\u2225\u222522 : AB> = X\n . This is proved in Lemma 3 below.\nFor the special case where R and C are defined by element-wise bounds, we return to the proof of Theorem 1 given in Section A, where we see that"}, {"heading": "2 \u2016X\u2016(R,C) = inf", "text": "AB>=X,a,b\u2208R a1\u2208Rn+,b1\u2208R m +\n{ a+R>a1+b+C >b1 : a+a1i \u2265 \u2225\u2225A(i)\u2225\u222522 \u2200i, b+b1j \u2265 \u2225\u2225B(j)\u2225\u222522 \u2200j} .\nNoting that \u2225\u2225A(i)\u2225\u222522 = (AA>)ii and \u2225\u2225B(j)\u2225\u222522 = (BB>)jj , we again use Lemma 3 to see that this is equivalent to the SDP\ninf { a+R>a1 + b+ C >b1 : a1i \u2265 0 and a+ a1i \u2265 Aii \u2200i,\nb1j \u2265 0 and b+ b1j \u2265 Bjj \u2200j, (\nA X X> B\n) 0 } .\nLemma 3. Let f : Rn \u00d7Rm \u2192 R be any function that is nondecreasing in each coordinate and let X \u2208 Rn\u00d7m be any matrix. Then\ninf { f (\u2225\u2225A(1)\u2225\u222522 , . . . ,\u2225\u2225A(n)\u2225\u222522 ,\u2225\u2225B(1)\u2225\u222522 , . . . ,\u2225\u2225B(m)\u2225\u222522) : AB> = X}\n= inf { f (\u03a611, . . . ,\u03a6nn,\u03a811, . . . ,\u03a8mm) : ( \u03a6 X X> \u03a8 ) 0 } ,\nwhere the factorization AB> = X is assumed to be of arbitrary dimension, that is, A \u2208 Rn\u00d7k and B \u2208 Rm\u00d7k for arbitrary k \u2208 N.\nProof. We follow similar arguments as in Lemma 14 in [16], where this equality is shown for the special case of calculating a trace norm.\nFor convenience, we write g(A,B) = f (\u2225\u2225A(1)\u2225\u222522 , . . . ,\u2225\u2225A(n)\u2225\u222522 ,\u2225\u2225B(1)\u2225\u222522 , . . . ,\u2225\u2225B(m)\u2225\u222522)\nand h(\u03a6,\u03a8) = f (\u03a611, . . . ,\u03a6nn,\u03a811, . . . ,\u03a8mm) .\nThen we would like to show that\ninf { g(A,B) : AB> = X } = inf { h(\u03a6,\u03a8) : ( \u03a6 X X> \u03a8 ) 0 } .\nFirst, take any factorization AB> = X . Let \u03a6 = AA> and \u03a8 = BB>. Then (\n\u03a6 X X> \u03a8\n) 0,\nand we have g(A,B) = h(\u03a6,\u03a8) by definition. Therefore,\ninf { g(A,B) : AB> = X } \u2265 inf { h(\u03a6,\u03a8) : ( \u03a6 X X> \u03a8 ) 0 } .\nNext, take any \u03a6 and \u03a8 such that (\n\u03a6 X X> \u03a8\n) 0. Take a Cholesky decomposition\n( \u03a6 X X> \u03a8 ) = ( A 0 B C ) \u00b7 ( A 0 B C )> = ( AA> AB> BA> BB> + CC> ) .\nFrom this, we see that AB> = X , that \u03a6ii = \u2225\u2225A(i)\u2225\u222522 for all i, and that \u03a8jj \u2265 \u2225\u2225B(j)\u2225\u222522 for all j. Since f is nondecreasing in each coordinate, we have h(\u03a6,\u03a8) \u2265 g(A,B). Therefore, we see that\ninf { g(A,B) : AB> = X } \u2264 inf { h(\u03a6,\u03a8) : ( \u03a6 X X> \u03a8 ) 0 } ."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "We introduce a new family of matrix norms, the \u201clocal max\u201d norms, generalizing<lb>existing methods such as the max norm, the trace norm (nuclear norm), and the<lb>weighted or smoothed weighted trace norms, which have been extensively used in<lb>the literature as regularizers for matrix reconstruction problems. We show that this<lb>new family can be used to interpolate between the (weighted or unweighted) trace<lb>norm and the more conservative max norm. We test this interpolation on simulated<lb>data and on the large-scale Netflix and MovieLens ratings data, and find improved<lb>accuracy relative to the existing matrix norms. We also provide theoretical results<lb>showing learning guarantees for some of the new norms.", "creator": "LaTeX with hyperref package"}}}