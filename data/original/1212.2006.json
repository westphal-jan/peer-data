{"id": "1212.2006", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Dec-2012", "title": "A Novel Feature-based Bayesian Model for Query Focused Multi-document Summarization", "abstract": "Both supervised learning methods and LDA based topic model have been successfully applied in the field of query focused multi-document summarization. In this paper, we propose a novel supervised approach that can incorporate rich sentence features into Bayesian topic models in a principled way, thus taking advantages of both topic model and feature based supervised learning methods. Experiments on TAC2008 and TAC2009 demonstrate the effectiveness of our approach.", "histories": [["v1", "Mon, 10 Dec 2012 09:41:12 GMT  (316kb)", "http://arxiv.org/abs/1212.2006v1", "Submitted on 1 Oct 2012; Accepted on 8 Dec 2012"], ["v2", "Fri, 27 Dec 2013 17:28:14 GMT  (0kb,I)", "http://arxiv.org/abs/1212.2006v2", "This paper has been withdrawn by the author due to a crucial sign error in equation"]], "COMMENTS": "Submitted on 1 Oct 2012; Accepted on 8 Dec 2012", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["jiwei li", "sujian li"], "accepted": true, "id": "1212.2006"}, "pdf": {"name": "1212.2006.pdf", "metadata": {"source": "CRF", "title": "A Novel Feature-based Bayesian Model for Query Focused Multi- document Summarization", "authors": ["Jiwei Li", "Sujian Li"], "emails": ["jl3226@cornell.edu", "lisujian@pku.edu.cn"], "sections": [{"heading": null, "text": "successfully applied in the field of query focused multi-document summarization. In this paper, we propose a novel supervised approach that can incorporate rich sentence features into Bayesian topic models in a principled way, thus taking advantages of both topic model and feature based supervised learning methods. Experiments on TAC2008 and TAC2009 demonstrate the effectiveness of our approach."}, {"heading": "1 Introduction", "text": "Query-focused multi-document summarization (Nenkova et al., 2006; Wan et al., 2007; Ouyang et al., 2010) can facilitate users to grasp the main idea of a set of documents and has been proven to be an effective way consume massive information. In query-focused summarization, a specific topic description, such as a query or a short narrative is proposed before the documents, which express the most important topic information and a summary would be generated according to the given topic description.\nSupervised models have been widely used in summarization(Li, et al., 2009, Shen et al., 2007, Ouyang et al., 2010). Supervised models usually regard summarization as a classification or regression problem and use various sentence features to build a classifier based on labeled negative or positive samples. However, existing supervised approaches seldom exploit the intrinsic structure among sentences and usually give rise to serious problems such as unbalance and low recall in summaries.\nRecently, LDA-based (Blei et al., 2003) Bayesian topic models have successfully been\napplied to multi-document summarization in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries that other techniques do not have (Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al., 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012). Exiting Bayesian approaches label sentences or words with topics and sentences which are closely related with query or can highly generalize documents can be selected into summaries. LDA topic model suffers from the intrinsic disadvantages that it only uses word frequency for topic modeling and are do not explore more useful text features such as position, word order etc (Zhu and Xing, 2010). For example, the first sentence in a paragraph or a passage may be important for summary since it is more likely to give a global generalization about a paragraph or a passage. But LDA model is hard to consider such information, making useful information lost. It naturally comes to our minds that we can improve summarization performance by making full use of both more useful text features and the latent semantic structures generated by LDA topic models. One related work is by Celikyilmaz and Hakkani-Tur (2010). They built a hierarchical topic model called Hybhsum based on LDA for topic discovery and assumed this model can produce appropriate scores for evaluating sentences. Then the scores are used for tuning the weights of various features which may be helpful to generate summaries. Their work made a good step combining topic model with feature based supervised learning. What their approach confuses us is that whether a topic model only based on word frequency is good enough to generate an appropriate score for tuning feature weights. With the opposite research line, we consider that one topic model only based on word frequency cannot well explore the information latent in the documents and expect more useful features to improve the topic model. Based on this, supervised\ntopic models such as sLDA(Blei and MacAuliffe 2007) give us some inspiration. In sLDA, each document is associated with a labeled feature and sLDA can integrate this feature into LDA model in a principled way.\nWith the reference to the work of supervised LDA models, in this paper, we propose a novel sentence feature based Bayesian model for multidocument summarization. Our approach can naturally combine feature based supervised methods and topic model. The most important and challenging problem in our model is the tuning of feature weights. To solve this problem, we transform the problem of finding optimum feature weights into an optimization algorithm and learn the weights in a supervised way. A set of experiments are conducted based on the benchmark data of TAC2008 and TAC2009, and the experimental results show the effectiveness of our model. The rest of the paper is organized as follows. Section 2 describes some background and related works. Section 3 describes our details of S-sLDA model. Section 4 demonstrates details of our approaches, including learning, inference and summary generation. Section 5 provides experiments results and the conclusion is made in Section 6."}, {"heading": "2 Related Work", "text": "A variety of approaches have been proposed for topic-focused multi-document summarizations such as unsupervised (semi-supervised) approaches, supervised approaches, and Bayesian approaches. Most supervised approaches regard summarization task as a sentence level two class classification problem and distinguish desired summary sentences from undesired ones in regard with sentence features. Supervised machine learning methods such as Support Vector Machine(SVM) (Li, et al., 2009), Maximum Entropy (Osborne, 2002) , Conditional Random Field (Shen et al., 2007) and regression models (Ouyang et al., 2010) have been adopted to leverage the rich sentence features for summarization. Recently, Bayesian topic model has shown its power in summarization for its clear probabilistic interpretation. Daume and Marcu (2006) proposed Bayesum model for sentence extraction based on\nquery expansion concept in information retrieval. Haghighi and Vanderwende (2009) proposed topicsum and hiersum which use a LDA-like topic model and assign each sentence a distribution over background topic, doc-specific topic and content topics. Celikyilmaz and Hakkani-Tur (2010) made a good step in combining topic model with supervised feature based regression for sentence scoring in summarization. In their model, the score of training sentences are firstly got through a novel hierarchical topic model. Then a featured based support vector regression (SVR) is used for sentence score prediction. The problem of Celikyilmaz and Hakkani-Tur\u2019s model is that topic model and feature based regression are two separate processes and the score of training sentences may be biased because their topic model only consider word frequency and fail to consider other important features. Supervised feature based topic models have been proposed in recent years to incorporate different kinds of features into LDA model. Blei (2007) proposed sLDA for document response pairs and Daniel et al. (2009) proposed Labeled LDA by defining a one to one correspondence between latent topic and user tags. Zhu and Xing(2010) proposed conditional topic random field(CTRF) which addresses feature and independent limitation in LDA."}, {"heading": "3 Model description", "text": "In this section, we begin with two basic topic models, LDA and sLDA and then classify how we propose our sentence based S-sLDA model."}, {"heading": "3.1 LDA and sLDA", "text": "The hierarchical Bayesian LDA (Blei et al., 2003) models the probability of a corpus on hidden topics as in Fig. 1(a). Let K be the number of topics , M be the number of documents in the corpus and V be number of terms in the vocabulary.\nThe topic distribution of each document m  is drawn from a prior Dirichlet distribution Dir(\u03b1),\nand each document word mn w is sampled from a topic-word distribution z specified by a drawn\nfrom the topic-document distribution m  .  is a\nK*M dimensional matrix and each k is a\ndistribution over the V terms. For a document m, the generating procedure of LDA is illustrated in\nFig.2. m is a mixture proportion over topics of\ndocument m and mnz is a K dimensional variable that presents the topic assignment distribution of\nword mnw . Supervised LDA (sLDA) (Blei and McAuliffe 2007) is a document feature based model and introduces a response variable to incorporate information for topic discovering, as shown in Fig. 1(b). In the generative procedure of sLDA, the document pairwise label is draw from"}, {"heading": "3.2 S-sLDA", "text": "The generative approach of S-sLDA is shown in Fig.3. We can see that the generation process of a sentence involves not only the generation of words in current sentence, but also sentence features. Let\n, ,m s nw be the observed variable representing the\nnth word in the sth sentence of document m. ,m sz is\nthe hidden variable indicating the topic current sentence belongs to. In S-sLDA, we make an assumption that words in the same sentence are all generated from the same topic which was proposed\nby Gruber(2007). Let msnz denotes the topic word\nmsnw is assigned. According to our assumption,\nmsn msz z for any [1, ]msn N . msY denotes the\nfeature vector of current sentence and represents information such as the position of current sentence in the document, the semantic similarity between current sentence and the query and so on. Feature space will be described in detail in Section 4.2. In the generative model, we have to assume that these features are independent. The mixture\nweights over features in S-sLDA are defined with a generalized linear model(GLM).\nexp( )( | , ) exp( )\nms\nT\nms ms ms ms T\nms msz\nz Y p Y z\nz Y\n \n  \n(1)\nHere we assume that each sentence has T features\nand msY is T*1 dimensional vector.  is the K*T weight matrix of each feature upon topics, which largely controls the feature generation procedure. Unlike s-LDA where  is a latent variable\nestimated from the maximum likelihood estimation algorithm, in S-sLDA the value of  is trained\nthrough a supervised algorithm which will be illustrated in details in Section 3. The generating procedure of S-sLDA is illustrated in figure 4."}, {"heading": "3.3 Posterior Inference and Estimation", "text": "Given a document and labels for each sentence, the posterior distribution of the latent variables is:\n:\n1:1 1 1\n1:1 1\n1: 1: 1:\n( | ) [ ( | ) ( | , ) ( | , )]\n( | ) [ ( | ) ( | , ) ( | , )]\n( , | , , , , )\nm ms\nm ms\nN\nM s N n N m ms m ms ms msn ms Km s n\ns N n N\nm ms m ms ms msn ms Kz s n\nN N K\np p z p Y z p w z\nd p p z p Y z p w z\np z w Y\n    \n     \n     \n  \n   \n\n  \n   (2) Equation(2) can not be efficiently computable. By applying the Jensen\u2019s inequality, we obtain a lower bound of the log likelihood of document.\n1: 1: 1:( , | , , , , )N N Kp z w Y L     , where\n[log ( | )] [log ( | , )]\n[log ( | )] [log ( | , )] ( )\nms ms msms ms\nmsn msm msn\nL E p z E p Y z\nE p E p w z H q\n \n  \n \n  \n \n  (3)\nWhere ( ) [log ]H q E q  and it is the entropy of\nvariational distribution q is defined as\n( , | , ) ( | ) ( | )m msn msmk snq z q q z       (4)\nHere  is a K-dimensional Dirichlet parameter\nvector and multinomial parameters 11 ( ,..., ) MS   are the free variational parameters. The first, third and forth terms of equation (3)are identical to the corresponding terms for unsupervised LDA(Blei et al., 2003). The second term is the expectation of log probability of features given the latent topic assignments.\n[log ( | , )] ( ) log exp( ) ms\nT T\nms ms ms ms ms msz E p Y z E z Y z Y     (5) where ( )TmsE z is 1*K dimensional vector , 1[ ] K ms k k  .\nThe Bayes estimation for S-sLDA model can be got via a variational EM algorithm. In EM procedure, the lower bound is firstly minimized with respect to  and  , and then minimized with\n and  by fixing  and  .\nE-step The updating of Dirichlet parameter  is identical\nto that of unsupervised LDA, and does not involve\nfeature vectorY .\nnew\nm ss m       (6)\n1:\n1 1\n1 1\nexp{ [log | ] [log( | )] }\n=exp[ ( )- ( )+ ]\nmsN T new sk m msn K kt st\nn t\nK T\nmk mk kt st kvv s k t\nE E w Y\nY\n    \n   \n \n  \n  \n \n \n  \n(7)\nwhere ( ) is the first derivative of log function.\nsm denotes the document that sentence s comes\nfrom and stY is the t th feature of sentence s .\nM-step\nThe M-step for updating  is the same as the\nprocedure in unsupervised LDA, where the probability of a word generated from a topic is proportional to the number of times this word assigned to the topic.\n1 1 1\n1( ) m msN NM new k kw msn ms\nm s n w w       (8)"}, {"heading": "4 Our Approach", "text": ""}, {"heading": "4.1 Learning", "text": "In this subsection, we describe how we learn the feature weight  from a supervised model. The\nlearning process of  is a supervised algorithm\ncombined with variational inference of S-sLDA.\nGiven a topic description Q and a training\ncollection of sentences S from related documents, human assessors assign a score\n(v=-2,-1,0,1,2)v to each sentence in S. The score\nis integer between -2 (the least desired summary sentences) and +2 (the most desired summary sentences), and score 0 denotes neutral attitude.\nSuppose 1 2{ , ,..., } (v=-2,-1,0,1,2)v v v vkO o o o is sentence set containing sentences with score v.\n and  have the same meaning as they are in S-\nsLDA model in Section 3. Let Qk denote the\nprobability that query is generated by topic k. Since query topic does not belong to any document\nwe use the following strategy to calculate Qk\n1 1\n1 exp[ ( )- ( )] (9)\nM K\nQk kv mk mk m kv Q M          \nThe first part of Eqn.(9) kv\nv Q    denotes the\nprobability that all terms in query are generated from topic k and the second part of Eqn.(9)\n1 1\n1 exp[ ( )- ( )]\nM K\nmk mk m kM        can be explained as\nthe average probability that all documents in the corpus are talking about topic k. Eqn(9) is based on the assumption that query topic concern about things relevant to the main topic discussed by the document corpus. This is a reasonable assumption and most previous LDA summarization models are based on similar assumptions.\nNext, we define vQ k  for a sentence collection\nvO which can be interpreted as the probability that\nall sentences in collection vO are generated from\ntopic k. vQ k  can be calculated as follows:\n| |\n1\n1 , [1, ], [ 2,2]\n| |\nv\nv\nO\nO k sk\nsv\nk K v O        (10)\nwhere | |vO denotes the number of sentences in\nsentence set vO .Inspired by the idea that desired summary sentences would be more semantically related with the query, we transform problem of finding optimum to the following optimization\nproblem\n2\n2 1\nmin L( )= ( || ); 1 T\nv kt\nv t v KL O Q    \n  (11)\n( || )vKL O Q is the Kullback-Leibler divergence\nbetween the query topic and sentence set vO as is shown in Eqn.(12). | |\n| | 1\n11 1\n1\n| |1 ( || ) log log\n| |\nv\nv v\nv\nO\nOK K sk O k sv\nv O k sk sk k vQk Qk\nO KL O Q\nO\n  \n  \n \n  \n  \n(12)\nIn ( )L  , can see that 2O , which contain desirable sentences, are given the largest penalty for its KL distance from query. This is because there should be smaller KL distance between desired training set and query. The case is just opposite for undesired set.\nOur idea is to incorporate the minimization process of Eqn.(11) into variational inference process of S-sLDA model. Here we perform gradient based optimization method to minimize Eqn(11). Firstlr, we derive the gradient of\n( )L  with respect to  . 2\n2\n( || )L( ) v\nvxy xy\nKL O Q v \n \n \n   (13)\n1\n1 1\n( || ) 1 (1 log )\n| | | |\n1 1 log (14)\n| | | |\nv\nv\nv\nv\nK sks Qv sk s Q kxy v v xy\nK K sks Q Qksk\nQks Q k kv xy v Qk xy\nKL O Q\nQ Q\nQ Q\n \n \n     \n\n \n\n  \n   \n \n  \n \n  \n   \nFor simplification, we regard  and  as constant\nduring updating process of  , so 0 Qk\nxy\n\n\n   1. We\ncan further get first derivative for each labeled sentence.\n1 1\nexp[ ( )- ( )+ ] if k=x\n0 if k x\ns s\nK T\nsy m i m k kt sy kvsk v s k t xy\nY Y       \n   \n    \n  \n(15)\nUp to now, all components needed to evaluate Eqn.(9) have been obtained. We use BFGS (Broyden 1965) for minimizing process. Because the formalized problem may not be convex, gradient may not find the global minimum. We solve this problem by choose different start points. Learning process of our approach is illustrated in Fig.5.\n1 This is reasonable because the influence of  and  have\nbeen embodied in  during each iteration."}, {"heading": "4.2 Feature Space", "text": "Lots of features have been proven to be useful for summarization(Louis et al., 2010). Here we discuss several types of features which adopt in S-sLDA model. The feature values are either binary or normalized to the interval [0,1] divided by the normalized value. Features used in S-sLDA are as follows: Sentence position and Paragraph position. Cosine Similarity with query: Cosine similarity is based on the ti-idf value of terms. Local Inner-document Degree Order: Innerdocument Degree is the number of edges between\nsentence is and sentences from the same document. Local Inner document Degree Order is a binary feature which indicates whether the Inner-\ndocument Degree of sentence is is the largest among its neighbors. Document Specific Word: 1 if a sentence contains document specific word, 0 otherwise. Average Unigram Probability (Nenkova and Vanderwende, 2005; Celikyilmaz and Hakkani-Tur\n2010): As for sentence s, ( ) 1\n( ) | | Dw s p wp s s  ,\nwhere |s| is the number of words in sentence s and\n( ) D p w is the observed unigram probability in document collection. In addition, we also use sentence length and sentence biagram frequency as features."}, {"heading": "4.3 Sentence Selection Strategy", "text": "Next we explain our sentence selection strategy. According to our tuition that the desired summary should have a small KL divergence with query topic, we propose a function to score a set of sentences Sum which is a subset given the sentence\nset S .\nk k\n1 '\n( ) ( || ')\nlog K sum\nsum\nk Q k\nScore Sum KL sum Q\n \n\n \n  (16)\nksum can be calculated via Eqn.(10) and 'Q k can be\ncalculated through Eqn.(9). Score of Sum is\nnegative, but it does not affect our sentence selection process. Let *Sum denote the optimum update summary. We can get *Sum that maximizes the scoring function.\n*\n&& ( ) arg max ( ) Sum S words Sum L Sum Score Sum    (17)\nA greedy algorithm is applied by adding sentence one by one to find *Sum . We use G to denote the sentence set which contains the selected summary sentences. The algorithm first initializes G to  and X to SU. During each iteration, we select from X one sentence (i.e. sm) which makes\n( )mScore s G have the highest score. To avoid\ntopic redundancy in the summary, we also revise the MMR strategy (Goldstein et al., 1999; Ouyang et al., 2007) in the process of sentence selection.\nFor each ms , we compute the semantic similarity\nbetween ms and each sentence ts in set Y as regarding to topic distribution in Eqn.(18).\n2 2\n1 1\ncos_ ( , ) m t\nmk tk\ns k s kk m t\nK K\ns s\nk k\nsem s s  \n   \n\n\n\n  (18)\nWe need to assure that the value of semantic similarity between two sentences is less than semTh .\nThe whole procedure for summarization using SsLDA model is illustrated in Figure 6.\n(1) Learning:\nGiven labeled set vO , learn the feature weight vector  using algorithm in Fig.(5).\n(2) Given new data set and  , use algorithm\nin section 3.3 for inference. (The only difference between this step and step (1)"}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Experiment Set-up", "text": "The query-focused multi-document summarization task defined in DUC 2 (Document Understanding Conference) and TAC3 (Text Analysis Conference) evaluations requires generating a concise and well organized summary for a collection of related news documents according to a given query which describes the user\u2019s information need. The query usually consists of a title and one or more narrative/question sentences. The system-generated summaries for DUC and TAC are respectively limited to 250 words and 100 words. Our experiment data is composed of DUC 2007, TAC4 2008 and TAC 2009 data which have 45, 48 and 44 collections respectively. In our experiments, DUC 2007 data is used as training data and TAC (2008- 2009) data is used as the test data.\nStop-words in both documents and queries are removed using a stop-word list of 598 words, and the remaining words are stemmed by Porter Stemmer 5 . As for the automatic evaluation of summarization, ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures, including ROUGE-1, ROUGE-2, and ROUGESU4 6 and their corresponding 95% confidential intervals, are used to evaluate the performance of the summaries. ROUGE-N is an n-gram recall computed as follows:\n( )\n( )\nN\nN\nmatch NS ref gram S\nNS ref gram S\nCnt gram ROUGE N\nCnt gram\n \n \n   \n  (19)\nwhere N is the length of the N-gram, and ref stands for the reference summaries. Cntmatch(gramN) is the maximum number of N-grams co-occurring in a 2 http://duc.nist.gov/ 3 http://www.nist.gov/tac/ 4 Here, we only use the docset-A data in TAC, since TAC data is composed of docset-A and docset-B data, and the docset-B data is mainly for the update summarization task. 5http://tartarus.org/~martin/PorterStemmer/ 6Jackknife scoring for ROUGE is used in order to compare with the human summaries.\ncandidate summary and the reference summaries, and Cnt(gramN) is the number of N-grams in the reference summaries. In order to obtain a more comprehensive measure of summary quality, we also conduct manual evaluation on TAC data with the reference to (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2011; Delort and Alfonseca, 2011).\n5.2 Parameter Tuning.\nFirst, we need to tune the parameter semTh in MMR. The value of semTh is trained on DUC 2007 data, ranging from 0 to 1 with the interval of 0.1. Fig. 7 presents the ROUGE-2 and ROUGE-SU4 evaluation results of our summarization approach, with regard to different values of semTh . We can see that the ROUGE scores reach their peak around 0.7semTh  and decline afterwards. This means that\nan appropriate threshold value is also important to the summarization results, and in the following experiments semTh is set to 0.7."}, {"heading": "5.3 Comparison with other Bayesian models.", "text": "In this subsection, we compare our model with other Bayesian baselines such as Hybhsum(Celikyilmaz and Hakkani-Tur 2010), HierSum(Haghighi and Vanderwende 2009),\nKLsum (Lin et al., 2006). In Hybhsum, we set 0 to 1 as Celikyilmaz and Hakkani-Tur did. All summaries are truncated to the same length of 100 words.\nFrom Table 1 and Table 2, we can see that among all Bayesian baselines, Hybhsum achieves the best result. This further illustrates the advantages of combining topic model with supervised method. We can also see that Rouge results from S-sLDA model are just a bit better than results got from Hybhsum. Further analysis between the two models will be conducted in the latter part of the paper. Second top baseline is HierSum and our approach outperforms HierSum over ROUGE-1, ROUGE-2, and ROUGE-SU4 by"}, {"heading": "5.4 Comparison with other baselines.", "text": "To further exhibit the precedence of our system over other existing supervised and unsupervised approaches, we choose several widely used summarization algorithms for comparison. Here, the baseline systems are composed of unsupervised methods including Personalized Pagerank (PageRank) (Haveliwala, 2002), Manifold Ranking (Manifold) (Wan et al., 2007), and a supervised method - Support Vector Machine(SVM) (Vapnik 1995). These three algorithms are implemented by ourselves and their parameters with the best ROUGE-2 scores are set. At the same time, we also present the top three participating systems with regard to ROUGE-2 on TAC2008 and TAC2009 for comparison, denoted as (denoted as SysRank 1st, 2nd and 3rd)(Gillick et al., 2008; Zhang et al., 2008; Gillick et al., 2009; Varma et al., 2009). The ROUGE scores of the top TAC system are directly provided by the TAC evaluation. All evaluated summaries are limited to the length of 100 words. From table 3 and table 4, we can see that our approach outperforms all the baselines in terms of ROUGE metrics consistently. When compared with the standard supervised method SVM, in TAC2008 and TAC2009, the relative improvement over the ROUGE-1, ROUGE-2 and ROUGE-SU4 scores is 5.5%, 14.2%, 7.7% respectively on TAC2008 and 7.2%, 28.3%, 15.1% on TAC2009."}, {"heading": "5.5 Manual Evaluations", "text": "Overall quality, Focus and Responsiveness, SsLDA model significantly outputs Hybhsum (based on t-test on 90% confidential level). Fig. 8 shows the example summaries generated respectively by S-sLDA and Hybhsum for document collection D0824E-A of TAC2008. The query for D0824E-A is \u201cDescribe the causes and therapies being studied to help decrease breast cancer.\u201d As we can see from Fig.8(a), there are four summary sentences generated from S-sLDA: Sentence (1) and (4) talk about current therapies; (2) talks about reasons of breast cancer and (3) mentions both. However, summary generated by Hybhsum mostly talks about therapies and seldom mentions the reason of breast cancer. It is not hard to explain why S-sLDA can achieve better results than Hybhsum. S-sLDA model incorporates features into topic model and combine the supervised model with topic model in a principled way. As for Hybhsum, it is still a supervised model in nature. Its improvement over traditional supervised model is that it applies a more sophisticated topic model to predict score for training sentences. But a topic model only based on word frequency is not good enough to generate an appropriate score for tuning feature weights. (1) Olive oil can help fight breast cancer, a discovery that researchers said in a study out Sunday could guide the development of related treatment. (2) Deficiencies in the ability of cells to repair damaged DNA are associated with an increased risk of breast cancer, researchers at Columbia University said Monday. (3) The study found those with an inherited mutation in the BRCA1 gene, a known risk factor for early onset breast cancer, were about four times less likely to develop the disease if they were taking oral contraceptives.(3)Scientists in the Netherlands say they have developed a powerful diagnostic tool, based (a) (1) Olive oil can help fight breast cancer, a discovery that researchers said in a study out Sunday could guide the development of related treatment. (2) The latest findings from a large international study also found that the drug, called an aromatase inhibitor, appears to sharply reduce the chances that many breast cancer patients will develop a new cancer in the other breast or have the cancer spread elsewhere (3) An Australian research suggests that taking oral contraceptives reduces the risk of breast cancer carrying breast cancer genes. (4) A new type of drug shows growing promise as a more potent (b) Figure8: Example summary text generated by systems (a)S-sLDA and (b) Hybhsum. (D0824E-A, TAC2008)"}, {"heading": "6 Conclusion", "text": "In this paper, we propose a novel supervised approach based on revised supervised topic model for query-focused multi document summarization. Our approach naturally combines Bayesian topic model with supervised method and enjoy the advantages of both models. Experiments on benchmark demonstrate good performance of our model."}], "references": [{"title": "Supervised topic models", "author": ["David Blei", "Jon McAuliffe"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Blei and McAuliffe.,? \\Q2007\\E", "shortCiteRegEx": "Blei and McAuliffe.", "year": 2007}, {"title": "Latent dirichlet allocation", "author": ["David Blei", "Andrew Ng", "Micheal Jordan"], "venue": "In The Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "A class of methods for solving nonlinear simultaneous equations", "author": ["Charles Broyden"], "venue": "In Math. Comp.,", "citeRegEx": "Broyden.,? \\Q1965\\E", "shortCiteRegEx": "Broyden.", "year": 1965}, {"title": "The use of MMR, diversity-based reranking for reordering documents and producing summaries", "author": ["Jaime Carbonell", "Jade Goldstein"], "venue": "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development", "citeRegEx": "Carbonell and Goldstein.,? \\Q1998\\E", "shortCiteRegEx": "Carbonell and Goldstein.", "year": 1998}, {"title": "A Hybrid hierarchical model for multi-document summarization", "author": ["Asli Celikyilmaz", "Dilek Hakkani-Tur"], "venue": "In Proceeding of the", "citeRegEx": "Celikyilmaz and Hakkani.Tur.,? \\Q2010\\E", "shortCiteRegEx": "Celikyilmaz and Hakkani.Tur.", "year": 2010}, {"title": "DualSum: a topic-model based approach for update summarization", "author": ["Jean-Yves Delort", "Enrique Alfonseca"], "venue": "In Proceeding of EACL", "citeRegEx": "Delort and Alfonseca.,? \\Q2012\\E", "shortCiteRegEx": "Delort and Alfonseca.", "year": 2012}, {"title": "Summarizing Text Documents: Sentence Selection and Evaluation Metrics", "author": ["Jade Goldstein", "Mark Kantrowitz", "Vibhu Mittal", "Jaime Carbonell"], "venue": "In Proceedings of the 22nd annual international ACM SIGIR conference on Research", "citeRegEx": "Goldstein et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Goldstein et al\\.", "year": 1999}, {"title": "Bayesian Query-Focused Summarization", "author": ["Hal Daume", "Daniel Marcu H"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association", "citeRegEx": "Daume and H.,? \\Q2006\\E", "shortCiteRegEx": "Daume and H.", "year": 2006}, {"title": "Lexrank: graph-based lexical centrality as salience in text summarization", "author": ["Gune Erkan", "Dragomir Radev"], "venue": "In J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Erkan and Radev.,? \\Q2004\\E", "shortCiteRegEx": "Erkan and Radev.", "year": 2004}, {"title": "The ICSI/UTD Summarization System at TAC", "author": ["Dan Gillick", "Benoit Favre", "Dilek Hakkani-Tur", "Berndt Bohnet", "Yang Liu", "Shasha Xie"], "venue": null, "citeRegEx": "Gillick et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2009}, {"title": "Exploring content models for multi-document summarization", "author": ["Aria Haghighi", "Lucy Vanderwende"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association", "citeRegEx": "Haghighi and Vanderwende.,? \\Q2009\\E", "shortCiteRegEx": "Haghighi and Vanderwende.", "year": 2009}, {"title": "The summarization systems at tac 2010", "author": ["Feng Jin", "Minlie Huang", "Xiaoyan Zhu"], "venue": "In Proceedings of the third Text Analysis Conference,", "citeRegEx": "Jin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2010}, {"title": "Enhancing diversity, coverage and balance for summarization through structure learning", "author": ["Liangda Li", "Ke Zhou", "Gui-Rong Xue", "Hongyuan Zha", "Yong Yu"], "venue": "In Proceedings of the 18th international conference on World wide web,", "citeRegEx": "Li et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "An information-theoretic approach to automatic evaluation of summaries", "author": ["Chin-Yew Lin", "Guihong Gao", "Jianfeng Gao", "JianYun Nie"], "venue": "In Proceedings of the main conference on Human Language Technology Conference of the North American", "citeRegEx": "Lin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2006}, {"title": "Discourse indicators for content selection in summarization", "author": ["Annie Louis", "Aravind Joshi", "Ani Nenkova"], "venue": "In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Louis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Louis et al\\.", "year": 2010}, {"title": "Multi-document summarization using minimum distortion", "author": ["Tengfei Ma", "Xiaojun Wan"], "venue": "Proceedings of International Conference of Data Mining", "citeRegEx": "Ma and Wan.,? \\Q2010\\E", "shortCiteRegEx": "Ma and Wan.", "year": 2010}, {"title": "Extractive multi-document summaries should explicitly not contain document-specific content", "author": ["Rebecca Mason", "Eugene Charniak"], "venue": "In proceeding of ACL HLT,", "citeRegEx": "Mason and Charniak.,? \\Q2011\\E", "shortCiteRegEx": "Mason and Charniak.", "year": 2011}, {"title": "The impact of frequency on summarization", "author": ["Ani Nenkova", "Lucy Vanderwende"], "venue": "In Tech. Report MSRTR-2005-101,", "citeRegEx": "Nenkova and Vanderwende.,? \\Q2005\\E", "shortCiteRegEx": "Nenkova and Vanderwende.", "year": 2005}, {"title": "A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization", "author": ["Ani Nenkova", "Lucy Vanderwende", "Kathleen McKeown"], "venue": "In Proceedings of the 29th annual International ACM SIGIR Conference", "citeRegEx": "Nenkova et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nenkova et al\\.", "year": 2006}, {"title": "Using maximum entropy for sentence extraction", "author": ["Miles Osborne"], "venue": "In Proceedings of the ACL-02 Workshop on Automatic Summarization,", "citeRegEx": "Osborne.,? \\Q2002\\E", "shortCiteRegEx": "Osborne.", "year": 2002}, {"title": "Using random walks for question-focused", "author": ["Jahna Otterbacher", "Gunes Erkan", "Dragomir Radev"], "venue": null, "citeRegEx": "Otterbacher et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Otterbacher et al\\.", "year": 2005}, {"title": "Applying regression models to query-focused multidocument summarization", "author": ["You Ouyang", "Wenjie Li", "Sujian Li", "Qin Lua"], "venue": "In Information Processing \\& Management,", "citeRegEx": "Ouyang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ouyang et al\\.", "year": 2011}, {"title": "Developing learning strategies for topic-based summarization", "author": ["You Ouyang", "Sujian. Li", "Wenjie. Li"], "venue": "In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,", "citeRegEx": "Ouyang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ouyang et al\\.", "year": 2007}, {"title": "Labeled LDA: A supervised topic model for credit attribution in multilabeled corpora", "author": ["Daniel Ramage", "David Hall", "Ramesh Nallapati", "Christopher Manning"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural", "citeRegEx": "Ramage et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ramage et al\\.", "year": 2009}, {"title": "Document summarization using conditional random fields", "author": ["Dou She", "Jian-Tao Sun", "Hua Li", "Qiang Yang", "Zheng Chen"], "venue": "In Proceedings of International Joint Conference on Artificial Intelligence,", "citeRegEx": "She et al\\.,? \\Q2007\\E", "shortCiteRegEx": "She et al\\.", "year": 2007}, {"title": "Multi-document Summarization using cluster-based link analysis", "author": ["Xiaojun Wan", "Jianwu Yang"], "venue": "In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Wan and Yang.,? \\Q2008\\E", "shortCiteRegEx": "Wan and Yang.", "year": 2008}, {"title": "Manifold-ranking based topic-focused multidocument summarization", "author": ["Xiaojun Wan", "Jianwu Yang", "Jianguo Xiao"], "venue": "In Proceedings of International Joint Conference on Artificial Intelligence,", "citeRegEx": "Wan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2007}, {"title": "Exploiting Query-Sensitive Similarity for GraphBased Query-Oriented Summarization", "author": ["Furu Wei", "Wenjie Li", "Qin Lu", "Yanxiang He"], "venue": "In Proceedings of the 31st annual International ACM SIGIR Conference on Research and Development", "citeRegEx": "Wei et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2008}, {"title": "ICTCAS's ICTGrasper at TAC 2008: Summarizing Dynamic Information with Signature Terms Based Content Filtering, TAC", "author": ["Jin Zhang", "Xueqi Cheng", "Hongbo Xu", "Xiaolei Wang", "Yiling Zeng"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Ranking on Data Manifolds", "author": ["Dengzhong Zhou", "Jason Weston", "Arthur Gretton", "Olivier Bousquet", "Bernhard Sch\u00f6lkopf"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2003}, {"title": "Conditional Topic Random Fields", "author": ["Jun Zhu", "Eric Xing"], "venue": "In Proceeding of the 27th International Conference on Machine Learning,", "citeRegEx": "Zhu and Xing.,? \\Q2010\\E", "shortCiteRegEx": "Zhu and Xing.", "year": 2010}, {"title": "Semi-supervised Learning using Gaussian Fields and Harmonic Functions", "author": ["Xiaojin Zhu", "Zoubin Ghahramani", "John Lafferty"], "venue": "In Proceedings of International Conference of Machine Learning,", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 18, "context": "Query-focused multi-document summarization (Nenkova et al., 2006; Wan et al., 2007; Ouyang et al., 2010) can facilitate users to grasp the main idea of a set of documents and has been proven to be an effective way consume massive information.", "startOffset": 43, "endOffset": 104}, {"referenceID": 26, "context": "Query-focused multi-document summarization (Nenkova et al., 2006; Wan et al., 2007; Ouyang et al., 2010) can facilitate users to grasp the main idea of a set of documents and has been proven to be an effective way consume massive information.", "startOffset": 43, "endOffset": 104}, {"referenceID": 1, "context": "Recently, LDA-based (Blei et al., 2003) Bayesian topic models have successfully been applied to multi-document summarization in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries that other techniques do not have (Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 10, "context": ", 2003) Bayesian topic models have successfully been applied to multi-document summarization in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries that other techniques do not have (Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al., 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012).", "startOffset": 228, "endOffset": 355}, {"referenceID": 11, "context": ", 2003) Bayesian topic models have successfully been applied to multi-document summarization in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries that other techniques do not have (Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al., 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012).", "startOffset": 228, "endOffset": 355}, {"referenceID": 16, "context": ", 2003) Bayesian topic models have successfully been applied to multi-document summarization in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries that other techniques do not have (Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al., 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012).", "startOffset": 228, "endOffset": 355}, {"referenceID": 5, "context": ", 2003) Bayesian topic models have successfully been applied to multi-document summarization in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries that other techniques do not have (Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al., 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012).", "startOffset": 228, "endOffset": 355}, {"referenceID": 30, "context": "LDA topic model suffers from the intrinsic disadvantages that it only uses word frequency for topic modeling and are do not explore more useful text features such as position, word order etc (Zhu and Xing, 2010).", "startOffset": 191, "endOffset": 211}, {"referenceID": 1, "context": "Recently, LDA-based (Blei et al., 2003) Bayesian topic models have successfully been applied to multi-document summarization in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries that other techniques do not have (Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al., 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012). Exiting Bayesian approaches label sentences or words with topics and sentences which are closely related with query or can highly generalize documents can be selected into summaries. LDA topic model suffers from the intrinsic disadvantages that it only uses word frequency for topic modeling and are do not explore more useful text features such as position, word order etc (Zhu and Xing, 2010). For example, the first sentence in a paragraph or a passage may be important for summary since it is more likely to give a global generalization about a paragraph or a passage. But LDA model is hard to consider such information, making useful information lost. It naturally comes to our minds that we can improve summarization performance by making full use of both more useful text features and the latent semantic structures generated by LDA topic models. One related work is by Celikyilmaz and Hakkani-Tur (2010). They built a hierarchical topic model called Hybhsum based on LDA for topic discovery and assumed this model can produce appropriate scores for evaluating sentences.", "startOffset": 21, "endOffset": 1301}, {"referenceID": 19, "context": ", 2009), Maximum Entropy (Osborne, 2002) , Conditional Random Field (Shen et al.", "startOffset": 25, "endOffset": 40}, {"referenceID": 17, "context": ", 2009), Maximum Entropy (Osborne, 2002) , Conditional Random Field (Shen et al., 2007) and regression models (Ouyang et al., 2010) have been adopted to leverage the rich sentence features for summarization. Recently, Bayesian topic model has shown its power in summarization for its clear probabilistic interpretation. Daume and Marcu (2006) proposed Bayesum model for sentence extraction based on query expansion concept in information retrieval.", "startOffset": 26, "endOffset": 343}, {"referenceID": 9, "context": "Haghighi and Vanderwende (2009) proposed topicsum and hiersum which use a LDA-like topic model and assign each sentence a distribution over background topic, doc-specific topic and content topics.", "startOffset": 0, "endOffset": 32}, {"referenceID": 4, "context": "Celikyilmaz and Hakkani-Tur (2010) made a good step in combining topic model with supervised feature based regression for sentence scoring in summarization.", "startOffset": 0, "endOffset": 35}, {"referenceID": 4, "context": "Celikyilmaz and Hakkani-Tur (2010) made a good step in combining topic model with supervised feature based regression for sentence scoring in summarization. In their model, the score of training sentences are firstly got through a novel hierarchical topic model. Then a featured based support vector regression (SVR) is used for sentence score prediction. The problem of Celikyilmaz and Hakkani-Tur\u2019s model is that topic model and feature based regression are two separate processes and the score of training sentences may be biased because their topic model only consider word frequency and fail to consider other important features. Supervised feature based topic models have been proposed in recent years to incorporate different kinds of features into LDA model. Blei (2007) proposed sLDA for document response pairs and Daniel et al.", "startOffset": 0, "endOffset": 779}, {"referenceID": 4, "context": "Celikyilmaz and Hakkani-Tur (2010) made a good step in combining topic model with supervised feature based regression for sentence scoring in summarization. In their model, the score of training sentences are firstly got through a novel hierarchical topic model. Then a featured based support vector regression (SVR) is used for sentence score prediction. The problem of Celikyilmaz and Hakkani-Tur\u2019s model is that topic model and feature based regression are two separate processes and the score of training sentences may be biased because their topic model only consider word frequency and fail to consider other important features. Supervised feature based topic models have been proposed in recent years to incorporate different kinds of features into LDA model. Blei (2007) proposed sLDA for document response pairs and Daniel et al. (2009) proposed Labeled LDA by defining a one to one correspondence between latent topic and user tags.", "startOffset": 0, "endOffset": 846}, {"referenceID": 4, "context": "Celikyilmaz and Hakkani-Tur (2010) made a good step in combining topic model with supervised feature based regression for sentence scoring in summarization. In their model, the score of training sentences are firstly got through a novel hierarchical topic model. Then a featured based support vector regression (SVR) is used for sentence score prediction. The problem of Celikyilmaz and Hakkani-Tur\u2019s model is that topic model and feature based regression are two separate processes and the score of training sentences may be biased because their topic model only consider word frequency and fail to consider other important features. Supervised feature based topic models have been proposed in recent years to incorporate different kinds of features into LDA model. Blei (2007) proposed sLDA for document response pairs and Daniel et al. (2009) proposed Labeled LDA by defining a one to one correspondence between latent topic and user tags. Zhu and Xing(2010) proposed conditional topic random field(CTRF) which addresses feature and independent limitation in LDA.", "startOffset": 0, "endOffset": 962}, {"referenceID": 1, "context": "The hierarchical Bayesian LDA (Blei et al., 2003) models the probability of a corpus on hidden topics as in Fig.", "startOffset": 30, "endOffset": 49}, {"referenceID": 1, "context": "The first, third and forth terms of equation (3)are identical to the corresponding terms for unsupervised LDA(Blei et al., 2003).", "startOffset": 109, "endOffset": 128}, {"referenceID": 14, "context": "Lots of features have been proven to be useful for summarization(Louis et al., 2010).", "startOffset": 64, "endOffset": 84}, {"referenceID": 6, "context": "To avoid topic redundancy in the summary, we also revise the MMR strategy (Goldstein et al., 1999; Ouyang et al., 2007) in the process of sentence selection.", "startOffset": 74, "endOffset": 119}, {"referenceID": 22, "context": "To avoid topic redundancy in the summary, we also revise the MMR strategy (Goldstein et al., 1999; Ouyang et al., 2007) in the process of sentence selection.", "startOffset": 74, "endOffset": 119}, {"referenceID": 10, "context": "In order to obtain a more comprehensive measure of summary quality, we also conduct manual evaluation on TAC data with the reference to (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2011; Delort and Alfonseca, 2011).", "startOffset": 136, "endOffset": 231}, {"referenceID": 13, "context": "In this subsection, we compare our model with other Bayesian baselines such as Hybhsum(Celikyilmaz and Hakkani-Tur 2010), HierSum(Haghighi and Vanderwende 2009), KLsum (Lin et al., 2006).", "startOffset": 168, "endOffset": 186}, {"referenceID": 26, "context": "Here, the baseline systems are composed of unsupervised methods including Personalized Pagerank (PageRank) (Haveliwala, 2002), Manifold Ranking (Manifold) (Wan et al., 2007), and a supervised method - Support Vector Machine(SVM) (Vapnik 1995).", "startOffset": 155, "endOffset": 173}, {"referenceID": 28, "context": "At the same time, we also present the top three participating systems with regard to ROUGE-2 on TAC2008 and TAC2009 for comparison, denoted as (denoted as SysRank 1, 2 and 3)(Gillick et al., 2008; Zhang et al., 2008; Gillick et al., 2009; Varma et al., 2009).", "startOffset": 174, "endOffset": 258}, {"referenceID": 9, "context": "At the same time, we also present the top three participating systems with regard to ROUGE-2 on TAC2008 and TAC2009 for comparison, denoted as (denoted as SysRank 1, 2 and 3)(Gillick et al., 2008; Zhang et al., 2008; Gillick et al., 2009; Varma et al., 2009).", "startOffset": 174, "endOffset": 258}], "year": 2012, "abstractText": "Both supervised learning methods and LDA based topic model have been successfully applied in the field of query focused multi-document summarization. In this paper, we propose a novel supervised approach that can incorporate rich sentence features into Bayesian topic models in a principled way, thus taking advantages of both topic model and feature based supervised learning methods. Experiments on TAC2008 and TAC2009 demonstrate the effectiveness of our approach.", "creator": "Microsoft Office Word 2007"}}}