{"id": "1212.5637", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2012", "title": "Random Spanning Trees and the Prediction of Weighted Graphs", "abstract": "We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space. Experiments on real-world datasets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice.", "histories": [["v1", "Fri, 21 Dec 2012 23:51:21 GMT  (146kb,D)", "http://arxiv.org/abs/1212.5637v1", "Appeared in ICML 2010"]], "COMMENTS": "Appeared in ICML 2010", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["nicol\u00f2 cesa-bianchi", "claudio gentile", "fabio vitale", "giovanni zappella"], "accepted": true, "id": "1212.5637"}, "pdf": {"name": "1212.5637.pdf", "metadata": {"source": "CRF", "title": "Random Spanning Trees and the Prediction of Weighted Graphs", "authors": ["Nicol\u00f2 Cesa-Bianchi", "Claudio Gentile"], "emails": ["nicolo.cesa-bianchi@unimi.it", "claudio.gentile@uninsubria.it", "fabio.vitale@unimi.it", "giovanni.zappella@unimi.it"], "sections": [{"heading": "1 Introduction", "text": "A widespread approach to the solution of classification problems is representing datasets through a weighted graph where nodes are the data items and edge weights quantify the similarity between\nar X\niv :1\n21 2.\npairs of data items. This technique for coding input data has been applied to several domains, including Web spam detection [19], classification of genomic data [27], face recognition [10], and text categorization [13]. In many applications, edge weights are computed through a complex datamodelling process and typically convey information that is relevant to the task of classifying the nodes.\nIn the sequential version of this problem, nodes are presented in an arbitrary (possibly adversarial) order, and the learner must predict the binary label of each node before observing its true value. Since real-world applications typically involve large datasets (i.e., large graphs), online learning methods play an important role because of their good scaling properties. An interesting special case of the online problem is the so-called transductive setting, where the entire graph structure (including edge weights) is known in advance. The transductive setting is interesting in that the learner has the chance of reconfiguring the graph before learning starts, so as to make the problem look easier. This data preprocessing can be viewed as a kind of regularization in the context of graph prediction.\nWhen the graph is unweighted (i.e., when all edges have the same common weight), it was found in previous works [17, 16, 14, 15, 8] that a key parameter to control the number of online prediction mistakes is the size of the cut induced by the unknown adversarial labeling of the nodes, i.e., the number of edges in the graph whose endpoints are assigned disagreeing labels. However, while the number of mistakes is obviously bounded by the number of nodes, the cutsize scales with the number of edges. This naturally led to the idea of solving the prediction problem on a spanning tree of the graph [7, 18, 19], whose number of edges is exactly equal to the number of nodes minus one. Now, since the cutsize of the spanning tree is smaller than that of the original graph, the number of mistakes in predicting the nodes is more tightly controlled. In light of the previous discussion, we can also view the spanning tree as a \u201cmaximally regularized\u201d version of the original graph.\nSince a graph has up to exponentially many spanning trees, which one should be used to maximize the predictive performance? This question can be answered by recalling the adversarial nature of the online setting, where the presentation of nodes and the assignment of labels to them are both arbitrary. This suggests to pick a tree at random among all spanning trees of the graph so as to prevent the adversary from concentrating the cutsize on the chosen tree [7]. Kirchoff\u2019s equivalence between the effective resistance of an edge and its probability of being included in a random spanning tree allows to express the expected cutsize of a random spanning tree in a simple form. Namely, as the sum of resistances over all edges in the cut of G induced by the adversarial label assignment.\nAlthough the results of [7] yield a mistake bound for arbitrary unweighted graphs in terms of the cutsize of a random spanning tree, no general lower bounds are known for online unweighted graph prediction. The scenario gets even more uncertain in the case of weighted graphs, where the only previous papers we are aware of [16, 14, 15] essentially contain only upper bounds. In this paper we fill this gap, and show that the expected cutsize of a random spanning tree of the graph delivers a convenient parametrization1 that captures the hardness of the graph learning problem in the general weighted case. Given any weighted graph, we prove that any online prediction algorithm must err on a number of nodes which is at least as big as the expected cutsize of the\n1 Different parametrizations of the node prediction problem exist that lead to bounds which are incomparable to ours \u2014see Section 2.\ngraph\u2019s random spanning tree (which is defined in terms of the graph weights). Moreover, we exhibit a simple randomized algorithm achieving in expectation the optimal mistake bound to within logarithmic factors. This bound applies to any sufficiently connected weighted graph whose weighted cutsize is not an overwhelming fraction of the total weight.\nFollowing the ideas of [7], our algorithm first extracts a random spanning tree of the original graph. Then, it predicts all nodes of this tree using a generalization of the method proposed by [18]. Our tree prediction procedure is extremely efficient: it only requires constant amortized time per prediction and space linear in the number of nodes. Again, we would like to stress that computational efficiency is a central issue in practical applications where the involved datasets can be very large. In such contexts, learning algorithms whose computation time scales quadratically, or slower, in the number of data points should be considered impractical.\nAs in [18], our algorithm first linearizes the tree, and then operates on the resulting line graph via a nearest neighbor rule. We show that, besides running time, this linearization step brings further benefits to the overall prediction process. In particular, similar to [16, Theorem 4.2], the algorithm turns out to be resilient to perturbations of the labeling, a clearly desirable feature from a practical standpoint.\nIn order to provide convincing empirical evidence, we also present an experimental evaluation of our method compared to other algorithms recently proposed in the literature on graph prediction. In particular, we test our algorithm against the Perceptron algorithm with Laplacian kernel by [16, 19], and against a version of the label propagation algorithm by [31]. These two baselines can viewed as representatives of global (Perceptron) and local (label propagation) learning methods on graphs. The experiments have been carried out on five medium-sized real-world datasets. The two tree-based algorithms (ours and the Perceptron algorithm) have been tested using spanning trees generated in various ways, including committees of spanning trees aggregated by majority votes. In a nutshell, our experimental comparison shows that predictors based on our online algorithm compare well to all baselines while being very efficient in most cases.\nThe paper is organized as follows. Next, we recall preliminaries and introduce our basic notation. Section 2 surveys related work in the literature. In Section 3 we prove the general lower bound relating the mistakes of any prediction algorithm to the expected cutsize of a random spanning tree of the weighted graph. In the subsequent section, we present our prediction algorithm WTA (Weighted Tree Algorithm), along with a detailed mistake bound analysis restricted to weighted trees. This analysis is extended to weighted graphs in Section 5, where we provide an upper bound matching the lower bound up to log factors on any sufficiently connected graph. In Section 6, we quantify the robustness of our algorithm to label perturbation. In Section 7, we provide the constant amortized time implementation of WTA. Based on this implementation, in Section 8 we present the experimental results. Section 9 is devoted to conclusive remarks."}, {"heading": "1.1 Preliminaries and Basic Notation", "text": "Let G = (V,E,W ) be an undirected, connected, and weighted graph with n nodes and positive edge weights wi,j > 0 for (i, j) \u2208 E. A labeling of G is any assignment y = (y1, . . . , yn) \u2208 {\u22121,+1}n of binary labels to its nodes. We use (G,y) to denote the resulting labeled weighted graph.\nThe online learning protocol for predicting (G,y) can be defined as the following game be-\ntween a (possibly randomized) learner and an adversary. The game is parameterized by the graph G = (V,E,W ). Preliminarly, and hidden to the learner, the adversary chooses a labeling y of G. Then the nodes of G are presented to the learner one by one, according to a permutation of V , which is adaptively selected by the adversary. More precisely, at each time step t = 1, . . . , n the adversary chooses the next node it in the permutation of V , and presents it to the learner for the prediction of the associated label yit . Then yit is revealed, disclosing whether a mistake occurred. The learner\u2019s goal is to minimize the total number of prediction mistakes. Note that while the adversarial choice of the permutation can depend on the algorithm\u2019s randomization, the choice of the labeling is oblivious to it. In other words, the learner uses randomization to fend off the adversarial choice of labels, whereas it is fully deterministic against the adversarial choice of the permutation. The requirement that the adversary is fully oblivious when choosing labels is then dictated by the fact that the randomized learners considered in this paper make all their random choices at the beginning of the prediction process (i.e., before seeing the labels).\nNow, it is reasonable to expect that prediction performance degrades with the increase of \u201crandomness\u201d in the labeling. For this reason, our analysis of graph prediction algorithms bounds from above the number of prediction mistakes in terms of appropriate notions of graph label regularity. A standard notion of label regularity is the cutsize of a labeled graph, defined as follows. A \u03c6-edge of a labeled graph (G,y) is any edge (i, j) such that yi 6= yj . Similarly, an edge (i, j) is \u03c6-free if yi = yj . Let E\u03c6 \u2286 E be the set of \u03c6-edges in (G,y). The quantity \u03a6G(y) =\n\u2223\u2223E\u03c6\u2223\u2223 is the cutsize of (G,y), i.e., the number of \u03c6-edges in E\u03c6 (independent of the edge weights). The weighted cutsize of (G,y) is defined by\n\u03a6WG (y) = \u2211\n(i,j)\u2208E\u03c6 wi,j .\nFor a fixed (G,y), we denote by rWi,j the effective resistance between nodes i and j of G. In the interpretation of the graph as an electric network, where the weightswi,j are the edge conductances, the effective resistance rWi,j is the voltage between i and j when a unit current flow is maintained through them. For (i, j) \u2208 E, let also pi,j = wi,jrWi,j be the probability that (i, j) belongs to a random spanning tree T \u2014see, e.g., the monograph of [22]. Then we have\nE\u03a6T (y) = \u2211\n(i,j)\u2208E\u03c6 pi,j = \u2211 (i,j)\u2208E\u03c6 wi,jr W i,j , (1)\nwhere the expectation E is over the random choice of spanning tree T . Observe the natural weightscale independence properties of (1). A uniform rescaling of the edge weights wi,j cannot have an influence on the probabilities pi,j , thereby making each product wi,jrWi,j scale independent. In addition, since \u2211 (i,j)\u2208E pi,j is equal to n \u2212 1, irrespective of the edge weighting, we have 0 \u2264 E\u03a6T (y) \u2264 n \u2212 1. Hence the ratio 1n\u22121E\u03a6T (y) \u2208 [0, 1] provides a density-independent measure of the cutsize in G, and even allows to compare labelings on different graphs.\nNow contrast E\u03a6T (y) to the more standard weighted cutsize measure \u03a6WG (y). First, \u03a6WG (y) is clearly weight-scale dependent. Second, it can be much larger than n on dense graphs, even in the unweighted wi,j = 1 case. Third, it strongly depends on the density of G, which is generally related to \u2211 (i,j)\u2208E wi,j . In fact, E\u03a6T (y) can be much smaller than \u03a6WG (y) when there are strongly connected regions in G contributing prominently to the weighted cutsize. To see this, consider the\nfollowing scenario: If (i, j) \u2208 E\u03c6 and wi,j is large, then (i, j) gives a big contribution to \u03a6WG (y).2 However, this does not necessarily happen with E\u03a6T (y). In fact, if i and j are strongly connected (i.e., if there are many disjoint paths connecting them), then rWi,j is very small and so are the terms wi,jr W i,j in (1). Therefore, the effect of the large weight wi,j may often be compensated by the small probability of including (i, j) in the random spanning tree. See Figure 1 for an example. A different way of taking into account graph connectivity is provided by the covering ball approach taken by [14, 15] \u2013see the next section."}, {"heading": "G C2C1", "text": ""}, {"heading": "2 Related Work", "text": "With the above notation and preliminaries in hand, we now briefly survey the results in the existing literature which are most closely related to this paper. Further comments are made at the end of Section 5.\nStandard online linear learners, such as the Perceptron algorithm, are applied to the general (weighted) graph prediction problem by embedding the n vertices of the graph in Rn through a map i 7\u2192 K\u22121/2ei, where ei \u2208 Rn is the i-th vector in the canonical basis of Rn, and K is a positive definite n \u00d7 n matrix. The graph Perceptron algorithm [17, 16] uses K = LG + 11>, where LG is the (weighted) Laplacian of G and 1 = (1, . . . , 1). The resulting mistake bound is of the form \u03a6WG (y)D W G , where D W G = maxi,j r W i,j is the resistance diameter of G. As expected, this\n2 It is easy to see that in such cases \u03a6WG (y) can be much larger than n.\nbound is weight-scale independent, but the interplay between the two factors in it may lead to a vacuous result. At a given scale for the weights wi,j , if G is dense, then we may have DWG = O(1) while \u03a6WG (y) is of the order of n\n2. If G is sparse, then \u03a6WG (y) = O(n) but then DWG may become as large as n.\nThe idea of using a spanning tree to reduce the cutsize ofG has been investigated by [19], where the graph Perceptron algorithm is applied to a spanning tree T ofG. The resulting mistake bound is of the form \u03a6WT (y)D W T , i.e., the graph Perceptron bound applied to tree T . Since \u03a6 W T (y) \u2264 \u03a6WG (y) this bound has a smaller cutsize than the previous one. On the other hand, DWT can be much larger thanDWG because removing edges may increase the resistance. Hence the two bounds are generally incomparable.\n[19] suggest to apply the graph Perceptron algorithm to the spanning tree T with smallest geodesic diameter. The geodesic diameter of a weighted graph G is defined by\n\u2206WG = max i,j min \u03a0i,j \u2211 (r,s)\u2208\u03a0i,j 1 wi,j\nwhere the minimum is over all paths \u03a0i,j between i and j. The reason behind this choice of T is that, for the spanning tree T with smallest geodesic diameter, it holds that DWT \u2264 2\u2206WG . However, one the one hand DWG \u2264 \u2206WG , so there is no guarantee that DWT = O ( DWG ) , and on the other hand the adversary may still concentrate all \u03c6-edges on the chosen tree T , so there is no guarantee that \u03a6WT (y) remains small either.\n[18] introduce a different technique showing its application to the case of unweighted graphs. After reducing the graph to a spanning tree T , the tree is linearized via a depth-first visit. This gives a line graph S (the so-called spine of G) such that \u03a6S(y) \u2264 2 \u03a6T (y). By running a Nearest Neighbor (NN) predictor on S, [18] prove a mistake bound of the form \u03a6S(y) log ( n / \u03a6S(y) )\n+ \u03a6S(y). As observed by [11], similar techniques have been developed to solve low-congestion routing problems.\nAnother natural parametrization for the labels of a weighted graph that takes the graph structure into account is clusterability, i.e., the extent to which the graph nodes can be covered by a few balls of small resistance diameter. With this inductive bias in mind, [14] developed the Pounce algorithm, which can be seen as a combination of graph Perceptron and NN prediction. The number of mistakes has a bound of the form\nmin \u03c1>0\n( N (G, \u03c1) + \u03a6WG (y)\u03c1 ) (2)\nwhere N (G, \u03c1) is the smallest number of balls of resistance diameter \u03c1 it takes to cover the nodes of G. Note that the graph Perceptron bound is recovered when \u03c1 = DWG . Moreover, observe that, unlike graph Perceptron\u2019s, bound (2) is never vacuous, as it holds uniformly for all covers of G (even the one made up of singletons, corresponding to \u03c1\u2192 0). A further trick for the unweighted case proposed by [18] is to take advantage of both previous approaches (graph Perceptron and NN on line graphs) by building a binary tree on G. This \u201csupport tree\u201d helps in keeping the diameter of G as small as possible, e.g., logarithmic in the number of nodes n. The resulting prediction algorithm is again a combination of a Perceptron-like algorithm and NN, and the corresponding number of mistakes is the minimum over two earlier bounds: a NN-based bound of the form \u03a6G(y)(log n) 2 and an unweighted version of bound (2).\nGenerally speaking, clusterability and resistance-weighted cutsize E\u03a6T (y) exploit the graph structure in different ways. Consider, for instance, a barbell graph made up of two m-cliques joined by k unweighted \u03c6-edges with no endpoints in common (hence k \u2264 m).3 If m is much larger than k, then bound (2) scales linearly with k (the two balls in the cover correspond to the two m-cliques). On the other hand, E\u03a6T (y) tends to be constant: Because m is much larger than k, the probability of including any \u03c6-edge in T tends to 1/k, as m increases and k stays constant. On the other hand, if k gets close to m the resistance diameter of the graph decreases, and (2) becomes a constant. In fact, one can show that when k = m even E\u03a6T (y) is a constant, independent of m. In particular, the probability that a \u03c6-edge is included in the random spanning tree T is upper bounded by 3m\u22121\nm(m+1) , i.e., E\u03a6T (y)\u2192 3 when m grows large.4\nWhen the graph at hand has a large diameter, e.g., an m-line graph connected to an m-clique (this is sometimes called a \u201clollipop\u201d graph) the gap between the covering-based bound (2) and E\u03a6T (y) is magnified. Yet, it is fair to say that the bounds we are about to prove for our algorithm have an extra factor, beyond E\u03a6T (y), which is logarithmic in m. A similar logarithmic factor is achieved by the combined algorithm proposed in [18].\nAn even more refined way of exploiting cluster structure and connectivity in graphs is contained in the paper of [15], where the authors provide a comprehensive study of the application of dualnorm techniques to the prediction of weighted graphs, again with the goal of obtaining logarithmic performance guarantees on large diameter graphs. In order to trade-off the contribution of cutsize \u03a6WG and resistance diameter D W G , the authors develop a notion of p-norm resistance. The obtained bounds are dual norm versions of the covering ball bound (2). Roughly speaking, one can select the dual norm parameter of the algorithm to obtain a logarithmic contribution from the resistance diameter at the cost of squaring the contribution due to the cutsize. This quadratic term can be further reduced if the graph is well connected. For instance, in the unweighted barbell graph mentioned above, selecting the norm appropriately leads to a bound which is constant even when k m.\nFurther comments on the comparison between the results presented by [15] and the ones in our paper are postponed to the end of Section 5.\nDeparting from the online learning scenario, it is worth mentioning the significantly large literature on the general problem of learning the nodes of a graph in the train/test transductive setting: Many algorithms have been proposed, including the label-consistent mincut approach of [4, 5] and a number of other \u201cenergy minimization\u201d methods \u2014e.g., the ones by [31, 2] of which label propagation is an instance. See the work of [3] for a relatively recent survey on this subject.\nOur graph prediction algorithm is based on a random spanning tree of the original graph. The problem of drawing a random spanning tree of an arbitrary graph has a long history \u2014see, e.g., the recent monograph by [22]. In the unweighted case, a random spanning tree can be sampled with a random walk in expected time O(n lnn) for \u201cmost\u201d graphs, as shown by [6]. Using the beautiful algorithm of [30], the expected time reduces to O(n) \u2014see also the work of [1]. However, all known techniques take expected time \u0398(n3) on certain pathological graphs. In the weighted case, the above methods can take longer due to the hardness of reaching, via a random walk, portions of the graph which are connected only via light-weighted edges. To sidestep this issue, in our\n3 This is one of the examples considered in [15]. 4 This can be shown by computing the effective resistance of \u03c6-edge (i, j) as the minimum, over all unit-strength\nflow functions with i as source and j as sink, of the squared flow values summed over all edges, see, e.g., [22].\nexperiments we tested a viable fast approximation where weights are disregarded when building the spanning tree, and only used at prediction time. Finally, the space complexity for generating a random spanning tree is always linear in the graph size.\nTo conclude this section, it is worth mentioning that, although we exploit random spanning trees to reduce the cutsize, similar approaches can also be used to approximate the cutsize of a weighted graph by sparsification \u2014see, e.g., the work of [26]. However, because the resulting graphs are not as sparse as spanning trees, we do not currently see how to use those results."}, {"heading": "3 A General Lower Bound", "text": "This section contains our general lower bound. We show that any prediction algorithm must err at least 1\n2 E\u03a6T (y) times on any weighted graph.\nTheorem 1. LetG = (V,E,W ) be a weighted undirected graph with n nodes and weightswi,j > 0 for (i, j) \u2208 E. Then for all K \u2264 n there exists a randomized labeling y of G such that for all (deterministic or randomized) algorithms A, the expected number of prediction mistakes made by A is at least K/2, while E\u03a6T (y) < K.\nProof. The adversary uses the weighting P induced by W and defined by pi,j = wi,jrWi,j . By (1), pi,j is the probability that edge (i, j) belongs to a random spanning tree T of G. Let Pi = \u2211 j pi,j be the sum over the induced weights of all edges incident to node i. We call Pi the weight of node i. Let S \u2286 V be the set of K nodes i in G having the smallest weight Pi. The adversary assigns a random label to each node i \u2208 S. This guarantees that, no matter what, the algorithm A will make on average K/2 mistakes on the nodes in S. The labels of the remaining nodes in V \\ S are set either all +1 or all \u22121, depending on which one of the two choices yields the smaller \u03a6PG(y). See\nFigure 2 for an illustrative example. We now show that the weighted cutsize \u03a6PG(y) of this labeling y is less than K, independent of the labels of the nodes in S.\nSince the nodes in V \\ S have all the same label, the \u03c6-edges induced by this labeling can only connect either two nodes in S or one node in S and one node in V \\S. Hence \u03a6PG(y) can be written as\n\u03a6PG(y) = \u03a6 P,int G (y) + \u03a6 P,ext G (y)\nwhere \u03a6P,intG (y) is the cutsize contribution within S, and \u03a6 P,ext G (y) is the one from edges between\nS and V \\S. We can now bound these two terms by combining the definition of S with the equality\u2211 (i,j)\u2208E pi,j = n\u2212 1 as in the sequel. Let\nP intS = \u2211\n(i,j)\u2208E : i,j\u2208S\npi,j and P extS = \u2211\n(i,j)\u2208E : i\u2208S, j\u2208V \\S\npi,j .\nFrom the very definition of P intS and \u03a6 P,int G (y) we have \u03a6 P,int G (y) \u2264 P intS . Moreover, from the way the labels of nodes in V \\ S are selected, it follows that \u03a6P,extG (y) \u2264 P extS /2. Finally,\u2211 i\u2208S Pi = 2P int S + P ext S\nholds, since each edge connecting nodes in S is counted twice in the sum \u2211\ni\u2208S Pi. Putting everything together we obtain\n2P intS + P ext S = \u2211 i\u2208S Pi \u2264 K n \u2211 i\u2208V Pi = 2K n \u2211 (i,j)\u2208E pi,j = 2K(n\u2212 1) n\nthe inequality following from the definition of S. Hence\nE\u03a6T (y) = \u03a6PG(y) = \u03a6 P,int G (y) + \u03a6 P,ext G (y) \u2264 P int S + P extS 2 \u2264 K(n\u2212 1) n < K\nconcluding the proof."}, {"heading": "4 The Weighted Tree Algorithm", "text": "We now describe the Weighted Tree Algorithm (WTA) for predicting the labels of a weighted tree. In Section 5 we show how to apply WTA to the more general weighted graph prediction problem. WTA first transforms the tree into a line graph (i.e., a list), then runs a fast nearest neighbor method to predict the labels of each node in the line. Though this technique is similar to that one used by [18], the fact that the tree is weighted makes the analysis significantly more difficult, and the practical scope of our algorithm significantly wider. Our experimental comparison in Section 8 confirms that exploiting the weight information is often beneficial in real-world graph prediction problem.\nGiven a labeled weighted tree (T,y), the algorithm initially creates a weighted line graph L\u2032 containing some duplicates of the nodes in T . Then, each duplicate node (together with its incident edges) is replaced by a single edge with a suitably chosen weight. This results in the final weighted line graph L which is then used for prediction. In order to create L from T , WTA performs the following tree linearization steps:\n1. An arbitrary node r of T is chosen, and a line L\u2032 containing only r is created.\n2. Starting from r, a depth-first visit of T is performed. Each time an edge (i, j) is traversed (even in a backtracking step) from i to j, the edge is appended to L\u2032 with its weight wi,j , and j becomes the current terminal node of L\u2032. Note that backtracking steps can create in L\u2032 at most one duplicate of each edge in T , while nodes in T may be duplicated several times in L\u2032.\n3. L\u2032 is traversed once, starting from terminal r. During this traversal, duplicate nodes are eliminated as soon as they are encountered. This works as follows. Let j be a duplicate node, and (j\u2032, j) and (j, j\u2032\u2032) be the two incident edges. The two edges are replaced by a new edge (j\u2032, j\u2032\u2032) having weight wj\u2032,j\u2032\u2032 = min { wj\u2032,j, wj,j\u2032\u2032 } .5 Let L be the resulting line.\nThe analysis of Section 4.1 shows that this choice of wj\u2032,j\u2032\u2032 guarantees that the weighted cutsize of L is smaller than twice the weighted cutsize of T .\nOnce L is created from T , the algorithm predicts the label of each node it using a nearestneighbor rule operating on L with a resistance distance metric. That is, the prediction on it is the label of is\u2217 , being s\u2217 = argmins<t d(is, it) the previously revealed node closest to it, and d(i, j) = \u2211k s=1 1/wvs,vs+1 is the sum of the resistors (i.e., reciprocals of edge weights) along the unique path i = v1 \u2192 v2 \u2192 \u00b7 \u00b7 \u00b7 \u2192 vk+1 = j connecting node i to node j. Figure 3 gives an example of WTA at work."}, {"heading": "4.1 Analysis of WTA", "text": "The following lemma gives a mistake bound on WTA run on any weighted line graph. Given any labeled graph (G,y), we denote by RWG the sum of resistors of \u03c6-free edges in G,\nRWG = \u2211\n(i,j)\u2208E\\E\u03c6\n1\nwi,j .\nAlso, given any \u03c6-free edge subset E \u2032 \u2282 E \\ E\u03c6, we define RWG (\u00acE \u2032) as the sum of the resistors of all \u03c6-free edges in E \\ (E\u03c6 \u222a E \u2032),\nRWG (\u00acE \u2032) = \u2211\n(i,j)\u2208E\\(E\u03c6\u222aE\u2032)\n1\nwi,j .\nNote that RWG (\u00acE \u2032) \u2264 RWG , since we drop some edges from the sum in the defining formula. Finally, we use f O= g as shorthand for f = O(g). The following lemma is the starting point of our theoretical investigation \u2014please see Appendix A for proofs.\nLemma 2. If WTA is run on a labeled weighted line graph (L,y), then the total number mL of mistakes satisfies\nmL O = \u03a6L(y) ( 1 + log ( 1 +\nRWL (\u00acE \u2032) \u03a6WL (y) \u03a6L(y)\n)) + |E \u2032|\n5 By iterating this elimination procedure, it might happen that more than two adjacent nodes get eliminated. In this case, the two surviving terminal nodes are connected in L by the lightest edge among the eliminated ones in L\u2032.\nfor all subsets E \u2032 of E \\ E\u03c6.\nNote that the bound of Lemma 2 implies that, for any K = |E \u2032| \u2265 0, one can drop from the bound the contribution of any set of K resistors in RWL at the cost of adding K extra mistakes. We now provide an upper bound on the number of mistakes made by WTA on any weighted tree T = (V,E,W ) in terms of the number of \u03c6-edges, the weighted cutsize, and RWT .\nTheorem 3. If WTA is run on a labeled weighted tree (T,y), then the total numbermT of mistakes satisfies\nmT O = \u03a6T (y) ( 1 + log ( 1 +\nRWT (\u00acE \u2032) \u03a6WT (y) \u03a6T (y)\n)) + |E \u2032|\nfor all subsets E \u2032 of E \\ E\u03c6.\nThe logarithmic factor in the above bound shows that the algorithm takes advantage of labelings such that the weights of \u03c6-edges are small (thus making \u03a6WT (y) small) and the weights of \u03c6-free edges are high (thus makingRWT small). This matches the intuition behind WTA\u2019s nearest-neighbor rule according to which nodes that are close to each other are expected to have the same label. In particular, observe that the way the above quantities are combined makes the bound independent of rescaling of the edge weights. Again, this has to be expected, since WTA\u2019s prediction is scale insensitive. On the other hand, it may appear less natural that the mistake bound also depends linearly on the cutsize \u03a6T (y), independent of the edge weights. The specialization to trees of our lower bound (Theorem 1 in Section 3) implies that this linear dependence of mistakes on the unweighted cutsize is necessary whenever the adversarial labeling is chosen from a set of labelings with bounded \u03a6T (y)."}, {"heading": "5 Predicting a Weighted Graph", "text": "In order to solve the more general problem of predicting the labels of a weighted graph G, one can first generate a spanning tree T of G and then run WTA directly on T . In this case, it is possible to rephrase Theorem 3 in terms of the properties of G. Note that for each spanning tree T of G, \u03a6WT (y) \u2264 \u03a6WG (y) and \u03a6T (y) \u2264 \u03a6G(y). Specific choices of the spanning tree T control in different ways the quantities in the mistake bound of Theorem 3. For example, a minimum spanning tree tends to reduce the value of R\u0303WT , betting on the fact that \u03c6-edges are light. The next theorem relies on random spanning trees.\nTheorem 4. If WTA is run on a random spanning tree T of a labeled weighted graph (G,y), then the total number mG of mistakes satisfies\nEmG O = E [ \u03a6T (y) ]( 1 + log ( 1 + w\u03c6maxE [ RWT ])) , (3)\nwhere w\u03c6max = max (i,j)\u2208E\u03c6 wi,j .\nNote that the mistake bound in (3) is scale-invariant, since E [ \u03a6T (y) ] = \u2211 (i,j)\u2208E\u03c6 wi,jr W i,j cannot be affected by a uniform rescaling of the edge weights (as we said in Subsection 1.1), and so is the product w\u03c6maxE [ RWT ] = w\u03c6max \u2211 (i,j)\u2208E\\E\u03c6 r W i,j .\nWe now compare the mistake bound (3) to the lower bound stated in Theorem 1. In particular, we prove that WTA is optimal (up to log n factors) on every weighted connected graph in which the \u03c6-edge weights are not \u201csuperpolynomially overloaded\u201d w.r.t. the \u03c6-free edge weights. In order to rule out pathological cases, when the weighted graph is nearly disconnected, we impose the following mild assumption on the graphs being considered.\nWe say that a graph is polynomially connected if the ratio of any pair of effective resistances (even those between nonadjacent nodes) in the graph is polynomial in the total number of nodes n. This definition essentially states that a weighted graph can be considered connected if no pair of nodes can be found which is substantially less connected than any other pair of nodes. Again, as one would naturally expect, this definition is independent of uniform weight rescaling. The following corollary shows that if WTA is not optimal on a polynomially connected graph, then the labeling must be so irregular that the total weight of \u03c6-edges is an overwhelming fraction of the overall weight.\nCorollary 5. Pick any polynomially connected weighted graph G with n nodes. If the ratio of the total weight of \u03c6-edges to the total weight of \u03c6-free edges is bounded by a polynomial in n, then the total number of mistakes mG made by WTA when run on a random spanning tree T of G satisfies EmG O = E [ \u03a6T (y) ] log n.\nNote that when the hypothesis of this corollary is not satisfied the bound of WTA is not necessarly vacuous. For example, E [ RWT ] w\u03c6max = n\npolylog(n) implies an upper bound which is optimal up to polylog(n) factors. In particular, having a constant number of \u03c6-free edges with exponentially large resistance contradicts the assumption of polynomial connectivity, but it need not lead to a vacuous bound in Theorem 4. In fact, one can use Lemma 2 to drop from the mistake bound of Theorem 4 the contribution of any set of O(1) resistances in E [ RWT ] = \u2211 (i,j)\u2208E\\E\u03c6 r W i,j at the cost of adding just O(1) extra mistakes. This could be seen as a robustness property of WTA\u2019s bound against graphs that do not fully satisfy the connectedness assumption.\nWe further elaborate on the robustness properties of WTA in Section 6. In the meanwhile, note how Corollary 5 compares to the expected mistake bound of algorithms like graph Perceptron (see Section 2) on the same random spanning tree. This bound depends on the expectation of the product \u03a6WT (y)D W T , where D W T is the diameter of T in the resistance distance metric. Recall from the discussion in Section 2 that these two factors are negatively correlated because \u03a6WT (y) depends linearly on the edge weights, while DWT depends linearly on the reciprocal of these weights. Moreover, for any given scale of the edge weights, DWT can be linear in the number n of nodes.\nAnother interesting comparison is to the covering ball bounds of [14, 15]. Consider the case when G is an unweighted tree with diameter D. Whereas the dual norm approach of [15] gives a mistake bound of the form \u03a6G(y)2 logD, our approach, as well as the one by [18], yields \u03a6G(y) log n. Namely, the dependence on \u03a6G(y) becomes linear rather than quadratic, but the diameter D gets replaced by n, the number of nodes in G. Replacing n by D seems to be a benefit brought by the covering ball approach.6 More generally, one can say that the covering ball approach seems to allow to replace the extra log n term contained in Corollary 5 by more refined structural parameters of the graph (like its diameter D), but it does so at the cost of squaring the dependence on the cutsize. A typical (and unsurprising) example where the dual-norm covering\n6 As a matter of fact, a bound of the form \u03a6G(y) logD on unweighted trees is also achieved by the direct analysis of [7].\nball bounds are better then the one in Corollary 5 is when the labeled graph is well-clustered. One such example we already mentioned in Section 2: On the unweighted barbell graph made up of m-cliques connected by k m \u03c6-edges, the algorithm of [15] has a constant bound on the number of mistakes (i.e., independent of both m and k), the Pounce algorithm has a linear bound in k, while Corollary 5 delivers a logarithmic bound in m + k. Yet, it is fair to point out that the bounds of [14, 15] refer to computationally heavier algorithms than WTA: Pounce has a deterministic initialization step that computes the inverse Laplacian matrix of the graph (this is cubic in n, or quadratic in the case of trees), the minimum (\u03a8, p)-seminorm interpolation algorithm of [15] has no initialization, but each step requires the solution of a constrained convex optimization problem (whose time complexity was not quantified by the authors). Further comments on the time complexity of our algorithm are given in Section 7."}, {"heading": "6 The Robustness of WTA to Label Perturbation", "text": "In this section we show that WTA is tolerant to noise, i.e., the number of mistakes made by WTA on most labeled graphs (G,y) does not significantly change if a small number of labels are perturbed before running the algorithm. This is especially the case if the input graph G is polynomially connected (see Section 5 for a definition).\nAs in previous sections, we start off from the case when the input graph is a tree, and then we extend the result to general graphs using random spanning trees.\nSuppose that the labels y in the tree (T,y) used as input to the algorithm have actually been obtained from another labeling y\u2032 of T through the perturbation (flipping) of some of its labels. As explained at the beginning of Section 4, WTA operates on a line graph L obtained through the linearization process of the input tree T . The following theorem shows that, whereas the cutsize differences |\u03a6WT (y)\u2212\u03a6WT (y\u2032)| and |\u03a6T (y)\u2212\u03a6T (y\u2032)| on tree T can in principle be very large, the cutsize differences |\u03a6WL (y)\u2212\u03a6WL (y\u2032)| and |\u03a6L(y)\u2212\u03a6L(y\u2032)| on the line graph L built by WTA are always small.\nIn order to quantify the above differences, we need a couple of ancillary definitions. Given a labeled tree (T,y), define \u03b6T (K) to be the sum of the weights of the K heaviest edges in T ,\n\u03b6T (K) = max E\u2032\u2286E : |E\u2032|=K \u2211 (i,j)\u2208E\u2032 wi,j .\nIf T is unweighted we clearly have \u03b6T (K) = K. Moreover, given any two labelings y and y\u2032 of T \u2019s nodes, we let \u03b4(y,y\u2032) be the number of nodes for which the two labelings differ, i.e., \u03b4(y,y\u2032) = \u2223\u2223{i = 1, . . . , n : yi 6= y\u2032i}\u2223\u2223 . Theorem 6. On any given labeled tree (T,y) the tree linearization step of WTA generates a line graph L such that:\n1. \u03a6WL (y) \u2264 min y\u2032\u2208{\u22121,+1}n\n2 (\n\u03a6WT (y \u2032) + \u03b6T\n( \u03b4(y,y\u2032) )) ;\n2. \u03a6L(y) \u2264 min y\u2032\u2208{\u22121,+1}n 2 (\u03a6T (y \u2032) + \u03b4(y,y\u2032)) .\nIn order to highlight the consequences of WTA\u2019s linearization step contained in Theorem 6, consider as a simple example an unweighted star graph (T,y) where all labels are +1 except for the central node c whose label is\u22121. We have \u03a6T (y) = n\u22121, but flipping the sign of yc we would obtain the star graph (T,y\u2032) with \u03a6T (y\u2032) = 0. Using Theorem 6 (item 2) we get \u03a6L(y) \u2264 2. Hence, on this star graph WTA\u2019s linearization step generates a line graph with a constant number of \u03c6-edges even if the input tree T has no \u03c6-free edges. Because flipping the labels of a few nodes (in this case the label of c) we obtain a tree with a much more regular labeling, the labels of those nodes can naturally be seen as corrupted by noise.\nThe following theorem quantifies to what extent the mistake bound of WTA on trees can take advantage of the tolerance to label perturbation contained in Theorem 6. Introducing shorthands for the right-hand side expressions in Theorem 6,\n\u03a6\u0303WT (y) = min y\u2032\u2208{\u22121,+1}n\n2 (\n\u03a6WT (y \u2032) + \u03b6T\n( \u03b4(y,y\u2032) )) and\n\u03a6\u0303T (y) = min y\u2032\u2208{\u22121,+1}n\n2 (\u03a6T (y \u2032) + \u03b4(y,y\u2032)) ,\nwe have the following robust version of Theorem 3.\nTheorem 7. If WTA is run on a weighted and labeled tree (T,y), then the total number mT of mistakes satisfies\nmT O = \u03a6\u0303T (y) ( 1 + log ( 1 +\nRWT (\u00acE \u2032) \u03a6\u0303WT (y) \u03a6\u0303T (y)\n)) + \u03a6T (y) + |E \u2032|\nfor all subsets E \u2032 of E \\ E\u03c6.\nAs a simple consequence, we have the following corollary.\nCorollary 8. If WTA is run on a weighted and polynomially connected labeled tree (T,y), then the total number mT of mistakes satisfies\nmT O = \u03a6\u0303T (y) log n .\nTheorem 7 combines the result of Theorem 3 with the robustness to label perturbation of WTA\u2019s tree linearization procedure. Comparing the two theorems, we see that the main advantage of the tree linearization lies in the mistake bound dependence on the logarithmic factors occurring in the formulas: Theorem 7 shows that, when \u03a6\u0303T (y) \u03a6T (y), then the performance of WTA can be just linear in \u03a6T (y). Theorem 3 shows instead that the dependence on \u03a6T (y) is in general superlinear even in cases when flipping few labels of y makes the cutsize \u03a6T (y) decrease in a substantial way. In many cases, the tolerance to noise allows us to achieve even better results: Corollary 8 states that, if T is polynomially connected and there exists a labeling y\u2032 with small \u03b4(y,y\u2032) such that \u03a6T (y\u2032) is much smaller than \u03a6T (y), then the performance of WTA is about the same as if the algorithm were run on (T,y\u2032). In fact, from Lemma 2 we know that when T is polynomially connected the mistake bound of WTA mainly depends on the number of \u03c6-edges in (L,y), which can often be much smaller than those in (T,y). As a simple example, let T be an unweighted star\ngraph with a labeling y and z be the difference between the number of +1 and the number of \u22121 in y. Then the mistake bound of WTA is linear in z log n irrespective of \u03a6T (y) and, specifically, irrespective of the label assigned to the central node of the star, which can greatly affect the actual value of \u03a6T (y).\nWe are now ready to extend the above results to the case when WTA operates on a general weighted graph (G,y) via a uniformly generated random spanning tree T . As before, we need some shorthand notation. Define \u03a6\u2217G(y) as\n\u03a6\u2217G(y) = min y\u2032\u2208{\u22121,+1}n\n( E [ \u03a6T (y \u2032) ] + \u03b4(y,y\u2032) ) ,\nwhere the expectation is over the random draw of a spanning tree T of G. The following are the robust versions of Theorem 4 and Corollary 5.\nTheorem 9. If WTA is run on a random spanning tree T of a labeled weighted graph (G,y), then the total number mG of mistakes satisfies\nEmG O = \u03a6\u2217G(y) ( 1 + log ( 1 + w\u03c6maxE [ RWT ])) + E [ \u03a6T (y) ] ,\nwhere w\u03c6max = max (i,j)\u2208E\u03c6 wi,j .\nCorollary 10. If WTA is run on a random spanning tree T of a labeled weighted graph (G,y) and the ratio of the weights of each pair of edges of G is polynomial in n, then the total number mG of mistakes satisfies\nEmG O = \u03a6G(y) log n .\nThe relationship between Theorem 9 and Theorem 4 is similar to the one between Theorem 7 and Theorem 3. When there exists a labeling y\u2032 such that \u03b4(y,y\u2032) is small and E [ \u03a6T (y \u2032) ]\nE [ \u03a6T (y) ] , then Theorem 9 allows a linear dependence on E [ \u03a6T (y) ] . Finally, Corollary 10 quantifies the advantages of WTA\u2019s noise tolerance under a similar (but stricter) assumption as the one contained in Corollary 5."}, {"heading": "7 Implementation", "text": "As explained in Section 4, WTA runs in two phases: (i) a random spanning tree is drawn; (ii) the tree is linearized and labels are sequentially predicted. As discussed in Subsection 1.1, Wilson\u2019s algorithm can draw a random spanning tree of \u201cmost\u201d unweighted graphs in expected time O(n). The analysis of running times on weighted graphs is significantly more complex, and outside the scope of this paper. A naive implementation of WTA\u2019s second phase runs in time O(n log n) and requires linear memory space when operating on a tree with n nodes. We now describe how to implement the second phase to run in time O(n), i.e., in constant amortized time per prediction step.\nOnce the given tree T is linearized into an n-node line L, we initially traverse L from left to right. Call j0 the left-most terminal node of L. During this traversal, the resistance distance d(j0, i) is incrementally computed for each node i in L. This makes it possible to calculate d(i, j)\nin constant time for any pair of nodes, since d(i, j) = |d(j0, i) \u2212 d(j0, j)| for all i, j \u2208 L. On top of L, a complete binary tree T \u2032 with 2dlog2 ne leaves is constructed.7 The k-th leftmost leaf (in the usual tree representation) of T \u2032 is the k-th node in L (numbering the nodes of L from left to right). The algorithm maintains this data-structure in such a way that at time t: (i) the subsequence of leaves whose labels are revealed at time t are connected through a (bidirectional) list B, and (ii) all the ancestors in T \u2032 of the leaves of B are marked. See Figure 4.\nWhen WTA is required to predict the label yit , the algorithm looks for the two closest revealed leaves i\u2032 and i\u2032\u2032 oppositely located in L with respect to it. The above data structure supports this operation as follows. WTA starts from it and goes upwards in T \u2032 until the first marked ancestor anc(it) of it is reached. During this upward traversal, the algorithm marks each internal node of T \u2032 on the path connecting it to anc(it). Then, WTA starts from anc(it) and goes downwards in order to find the leaf i\u2032 \u2208 B closest to it. Note how the algorithm uses node marks for finding its way down: For instance, in Figure 4 the algorithm goes left since anc(it) was reached from below through the right child node, and then keeps right all the way down to i\u2032. Node i\u2032\u2032 (if present) is then identified via the links in B. The two distances d(it, i\u2032) and d(it, i\u2032\u2032) are compared, and the closest node to it within B is then determined. Finally, WTA updates the links of B by inserting it between i\u2032 and i\u2032\u2032.\nIn order to quantify the amortized time per trial, the key observation is that each internal node k of T \u2032 gets visited only twice during upward traversals over the n trials: The first visit takes place when k gets marked for the first time, the second visit of k occurs when a subsequent upward visit also marks the other (unmarked) child of k. Once both of k\u2019s children are marked, we are guaranteed that no further upward visits to k will be performed. Since the preprocessing operations takeO(n), this shows that the total running time over the n trials is linear in n, as anticipated. Note,\n7 For simplicity, this description assumes n is a power of 2. If this is not the case, we could add dummy nodes to L before building T \u2032.\nhowever, that the worst-case time per trial isO(log n). For instance, on the very first trial T \u2032 has to be traversed all the way up and down.\nThis is the way we implemented WTA on the experiments described in the next section."}, {"heading": "8 Experiments", "text": "We now present the results of an experimental comparison on a number of real-world weighted graphs from different domains: text categorization, optical character recognition, spam detection and bioinformatics. Although our theoretical analysis is for the sequential prediction model, all experiments are carried out using a more standard train-test scenario. This makes it easy to compare WTA against popular non-sequential baselines, such as Label Propagation.\nWe compare our algorithm to the following other methods, intended as representatives of two different ways of coping with the graph prediction problem: global vs. local prediction.\nPerceptron with Laplacian kernel. Introduced by [16] and here abbreviated as GPA (graph Perceptron algorithm). This algorithm sequentially predicts the nodes of a weighted graph G = (V,E) after mapping V via the linear kernel based on L+G +11\n>, where LG is the laplacian matrix of G. Following [19], we run GPA on a spanning tree T of the original graph. This is because a careful computation of the Laplacian pseudoinverse of a n-node tree takes time \u0398(n+m2 +mD) where m is the number of training examples plus the number of test examples (labels to predict), and D is the tree diameter \u2014see the work of [19] for a proof of this fact. However, in most of our experiments m = n, implying a running time of \u0398(n2) for GPA.\nNote that GPA is a global approach, in that the graph topology affects, via the inverse Laplacian, the prediction on all nodes.\nWeighted Majority Vote. Introduced here and abbreviated as WMV. Since the common underlying assumption to graph prediction algorithms is that adjacent nodes are labeled similarly, a very intuitive and fast algorithm for predicting the label of a node i is via a weighted majority vote on the available labels of the adjacent nodes. More precisely, WMV predicts using the sign of\u2211\nj : (i,j)\u2208E\nyjwi,j\nwhere yj = 0 if node j is not available in the training set. The overall time and space requirements are both of order \u0398(|E|), since we need to read (at least once) the weights of all edges. WMV is also a local approach, in the sense that prediction at each node is only affected by the labels of adjacent nodes.\nLabel Propagation. Introduced by [31] and here abbreviated as LABPROP. This is a batch transductive learning method based on solving a (possibly sparse) linear system of equations which requires \u0398(mn) time on an n-node graph with m edges. This bad scalability prevented us from carrying out comparative experiments on larger graphs of 106 or more nodes. Note that WMV can be viewed as a fast approximation of LABPROP.\nIn our experiments, we combined WTA and GPA with spanning trees generated in different ways (note that WMV and LABPROP do not use spanning trees).\nRandom Spanning Tree (RST). Each spanning tree is taken with probability proportional to the product of its edge weights \u2014see, e.g., [22, Chapter 4]. In addition, we also tested WTA combined with RST generated by ignoring the edge weights (which were then restored before running WTA). This second approach gives a prediction algorithm whose total expected running time, including the generation of the spanning tree, is \u0398(n) on most graphs. We abbreviate this spanning tree as NWRST (non-weighted RST).\nDepth-first spanning tree (DFST). This spanning tree is created via the following randomized depth-first visit: A root is selected at random, then each newly visited node is chosen with probability proportional to the weights of the edges connecting the current vertex with the adjacent nodes that have not been visited yet. This spanning tree is faster to generate than RST, and can be viewed as an approximate version of RST.\nMinimum Spanning Tree (MST). The spanning tree minimizing the sum of the resistors of all edges. This is the tree whose Laplacian best approximates the Laplacian of G according to the trace norm criterion \u2014see, e.g., the paper of [19].\nShortest Path Spanning Tree (SPST). [19] use the shortest path tree because it has a small diameter (at most twice the diameter of G). This allows them to better control the theoretical performance of GPA. We generated several shortest path spanning trees by choosing the root node at random, and then took the one with minimum diameter.\nIn order to check whether the information carried by the edge weight has predictive value for a nearest neighbor rule like WTA, we also performed a test by ignoring the edge weights during both the generation of the spanning tree and the running of WTA\u2019s nearest neighbor rule. This is essentially the algorithm analyzed by [18], and we denote it by NWWTA (non-weighted WTA). We combined NWWTA with weighted and unweighted spanning trees. So, for instance, NWWTA+RST runs a 1-NN rule (NWWTA) that does not take edge weights into account (i.e., pretending that all weights are unitary) on a random spanning tree generated according to the actual edge weights. NWWTA+NWRST runs NWWTA on a random spanning tree that also disregars edge weights.\nFinally, in order to make the classifications based on RST\u2019s more robust with respect to the variance associated with the random generation of the spanning tree, we also tested committees of RST\u2019s. For example, K*WTA+RST denotes the classifier obtained by drawing K RST\u2019s, running WTA on each one of them, and then aggegating the predictions of the K resulting classifiers via a majority vote. For our experiments we chose K = 7, 11, 17.\nWe ran our experiments on five real-world datasets:\nRCV1. The first 10,000 documents8 (in chronological order) of Reuters Corpus Volume 1, with TF-IDF preprocessing and Euclidean normalization.\n8 Available at trec.nist.gov/data/reuters/reuters.html.\nUSPS. The USPS dataset9 with features normalized into [0, 2].\nKROGAN. This is a high-throughput protein-protein interaction network for budding yeast. It has been used by [21] and [23].\nCOMBINED. A second dataset from the work of [23]. It is a combination of three datasets: [12]\u2019s, [20]\u2019s, and [28]\u2019s.\nWEBSPAM. A large dataset (110,900 nodes and 1,836,136 edges) of inter-host links created for the10 Web Spam Challenge 2008 [24]. This is a weighted graph with binary labels and a pre-defined train/test split: 3,897 training nodes and 1,993 test nodes (the remaining ones being unlabeled).\nWe created graphs from RCV1 and USPS with as many nodes as the total number of examples (xi, yi) in the datasets. That is, 10,000 nodes for RCV1 and 7291+2007 = 9298 for USPS. Following previous experimental settings [31, 2], the graphs were constructed using k-NN based on the standard Euclidean distance \u2016xi \u2212 xj\u2016 between node i and node j. The weight wi,j was set to wi,j = exp ( \u2212\u2016xi \u2212 xj\u20162 / \u03c32i,j ) , if j is one of the k nearest neighbors of i, and 0 otherwise. To set \u03c32i,j , we first computed the average square distance between i and its k nearest neighbors (call it \u03c32i ), then we computed \u03c3 2 j in the same way, and finally set \u03c3 2 i,j = ( \u03c32i + \u03c3 2 j )/ 2. We generated two graphs for each dataset by running k-NN with k = 10 (RCV1-10 and USPS-10) and k = 100 (RCV1-100 and USPS-100). The labels were set using the four most frequent categories in RCV1 and all 10 categories in USPS.\nIn KROGAN and COMBINED we only considered the biggest connected components of both datasets, obtaining 2,169 nodes and 6,102 edges for KROGAN, and 2,871 nodes and 6,407 edges for COMBINED. In these graphs, each node belongs to one or more classes, each class representing a gene function. We selected the set of functional labels at depth one in the FunCat classification scheme of the MIPS database [25], resulting in seventeen classes per dataset.\nIn order to associate binary classification tasks with the six non-binary datasets/graphs (RCV110, RCV1-100, USPS-10, USPS-100, KROGAN, COMBINED) we binarized the corresponding multiclass problems via a standard one-vs-rest scheme. We thus obtained: four binary classification tasks for RCV1-10 and RCV1-100, ten binary tasks for USPS-10 and USPS-100, seventeen binary tasks for both KROGAN and COMBINED. For a given a binary task and dataset, we tried different proportions of training set and test set sizes. In particular, we used training sets of size 5%, 10%, 25% and 50%. For any given size, the training sets were randomly selected.\nWe report error rates and F-measures on the test set, after macro-averaging over the binary tasks. The results are contained in Tables 1\u20137 (Appendix 9) and in Figures 5\u20136. Specifically, Tables 1\u20136 contain results for all combinations of algorithms and train/test split for the first six datasets (i.e., all but WEBSPAM).\nThe WEBSPAM dataset is very large, and requires us a lot of computational resources in order to run experiments on this graph. Moreover, GPA has always shown inferior accuracy performance\n9 Available at www-i6.informatik.rwth-aachen.de/\u02dckeysers/usps.html. 10 The dataset is available at barcelona.research.yahoo.net/webspam/datasets/. We do not compare our results to those obtained in the challenge since we are only exploiting the graph (weighted) topology here, disregarding content features.\nthan the corresponding version of WTA (i.e., the one using the same kind of spanning tree) on all other datasets. Hence we decided not to go on any further with the refined implementation of GPA on trees we mentioned above. In Table 7 we only report test error results on the four algorithms WTA, WMV, LABPROP, and WTA with a committee of seven (nonweighted) random spanning trees.\nIn our experimental setup we tried to control the sources of variance in the first six datasets as follows:\n1. We first generated ten random permutations of the node indices for each one of the six graphs/datasets;\n2. on each permutation we generated the training/test splits;\n3. we computed MST and SPST for each graph and made (for WTA, GPA, WMV, and LABPROP) one run per permutation on each of the 4+4+10+10+17+17 = 62 binary problems, averaging results over permutations and splits;\n4. for each graph, we generated ten random instances for each one of RST, NWRST, DFST, and then operated as in step 2, with a further averaging over the randomness in the tree generation.\nFigure 5 extracts from Tables 1\u20136 the error levels of the best spanning tree performers, and compared them to WMV and LABPROP. For comparison purposes, we also displayed the error levels achieved by WTA operating on a committee of seventeen random spanning trees (see below). Figure 6 (left) contains the error level on WEBSPAM reported in Table 7. Finally, Figure 6 (right) is meant to emphasize the error rate differences between RST and NWRST run with WTA.\nSeveral interesting observations and conclusions can be drawn from our experiments.\n1. WTA outperforms GPA on all datasets and with all spanning tree combinations. In particular, though we only reported aggregated results, the same relative performance pattern among the two algorithms repeats systematically over all binary classification problems. In addition, WTA runs significantly faster than GPA, requires less memory storage (linear in n, rather than quadratic), and is also fairly easy to implement.\n2. By comparing NWWTA to WTA, we see that the edge weight information in the nearest neighbor rule increases accuracy, though only by a small amount.\n3. WMV is a fast and accurate approximation to LABPROP when either the graph is dense (RCV1-100, and USPS-100) or the training set is comparatively large (25%\u201350%), although neither of the two situations often occurs in real-world applications.\n4. The best performing spanning tree for both WTA and GPA is MST. This might be explained by the fact that MST tends to select light \u03c6-edges of the original graph.\n5. NWRST and DFST are fast approximations to RST. Though the use of NWRST and DFST does not provide theoretical performance guarantees as for RST, in our experiments they do actually perform comparably. Hence, in practice, NWRST and DFST might be viewed as fast and practical ways to generate spanning trees for WTA.\n6. The prediction performance of WTA+MST is sometimes slightly inferior to LABPROP\u2019s. However, it should be stressed that LABPROP takes time \u0398(mn), wherem is the number of edges, whereas a single sweep of WTA+MST over the graph just takes time O(m log n).11 Committees of spanning trees are a simple way to make WTA approach, and sometimes surpass, the performance of LABPROP. One can see that on sparse graphs using committees gives a good performances improvement. In particular, committees of WTA can reach the same performances of LABPROP while adding just a constant factor to their (linear) time complexity."}, {"heading": "9 Conclusions and Open Questions", "text": "We introduced and analyzed WTA, a randomized online prediction algorithm for weighted graph prediction. The algorithm uses random spanning trees and has nearly optimal performance guarantees in terms of expected prediction accuracy. The expected running time of WTA is optimal when the random spanning tree is drawn ignoring edge weigths. Thanks to its linearization phase, the algorithm is also provably robust to label noise.\nOur experimental evaluation shows that WTA outperforms other previously proposed online predictors. Moreover, when combined with an aggregation of random spanning trees, WTA also tends to beat standard batch predictors, such as label propagation. These features make WTA (and its combinations) suitable to large scale applications.\nThere are two main directions in which this work can improved. First, previous analyses [7] reveal that WTA\u2019s analysis is loose, at least when the input graph is an unweighted tree with small diameter. This is the main source of the \u2126(ln |V |) slack between WTA upper bound and the general lower bound of Theorem 1. So we ask whether, at least in certain cases, this slack could be reduced. Second, in our analysis we express our upper and lower bounds in terms of the cutsize. One may object that a more natural quantity for our setting is the weighted cutsize, as this better reflects the assumption that \u03c6-edges tend to be light, a natural notion of bias for weighted graphs. In more\n11 The MST of a graph G = (V,E) can be computed in time O(|E| log |V |). Slightly faster implementations do actually exist which rely on Fibonacci heaps.\ngenerality, we ask what are other criteria that make a notion of bias better than another one. For example, we may prefer a bias which is robust to small perturbations of the problem instance. In this sense \u03a6\u2217G, the cutsize robust to label perturbation introduced in Section 6, is a better bias than E\u03a6T . We thus ask whether there is a notion of bias, more natural and robust than E\u03a6T , which captures as tightly as possible the optimal number of online mistakes on general weighted graphs. A partial answer to this question is provided by the recent work of [29]. It would also be nice to tie this machinery with recent results in the active node classification setting on trees contained in [9].\nAcknowledgments This work was supported in part by Google Inc. through a Google Research Award, and by the PASCAL2 Network of Excellence under EC grant 216886. This publication only reflects the authors views."}, {"heading": "Appendix A", "text": "This appendix contains the proofs of Lemma 2, Theorem 3, Theorem 4, Corollary 5, Theorem 6, Theorem 7, Corollary 8, Theorem 9, and Corollary 10. Notation and references are as in the main text. We start by proving Lemma 2.\nLemma 2. Let a cluster be any maximal sub-line of L whose edges are all \u03c6-free. Then L contains exactly \u03a6L(y) + 1 clusters, which we number consecutively, starting from one of the two terminal nodes. Consider the k-th cluster ck. Let v0 be the first node of ck whose label is predicted by WTA. After yv0 is revealed, the cluster splits into two edge-disjoint sub-lines c \u2032 k and c \u2032\u2032 k, both having v0 as terminal node.12 Let v\u2032k and v \u2032\u2032 k be the closest nodes to v0 such that (i) yv\u2032k = yv\u2032\u2032k 6= yv0 and (ii) v \u2032 k is adjacent to a terminal node of c\u2032k, and v \u2032\u2032 k is adjacent to a terminal node of c \u2032\u2032 k. The nearest neighbor prediction rule of WTA guarantees that the first mistake made on c\u2032k (respectively, c \u2032\u2032 k) must occur on a node v1 such that d(v0, v1) \u2265 d(v1, v\u2032k) (respectively, d(v0, v1) \u2265 d(v1, v\u2032\u2032k)). By iterating this argument for the subsequent mistakes we see that the total number of mistakes made on cluster ck is bounded by\n1 + \u230a log2 R\u2032k + (w \u2032 k) \u22121\n(w\u2032k) \u22121\n\u230b + \u230a log2 R\u2032\u2032k + (w \u2032\u2032 k) \u22121\n(w\u2032\u2032k) \u22121 \u230b where R\u2032k is the resistance diameter of sub-line c \u2032 k, and w \u2032 k is the weight of the \u03c6-edge between v \u2032 k and the terminal node of c\u2032k closest to it (R \u2032\u2032 k and w \u2032\u2032 k are defined similarly). Hence, summing the above displayed expression over clusters k = 1, . . . ,\u03a6L(y) + 1 we obtain\nmL O = \u03a6L(y) + \u2211 k ( log ( 1 +R\u2032kw \u2032 k ) + log ( 1 +R\u2032kw \u2032\u2032 k )) O = \u03a6L(y) ( 1 + log ( 1 + 1\n\u03a6L(y) \u2211 k R\u2032kw \u2032 k\n) + log ( 1 +\n1\n\u03a6L(y) \u2211 k R\u2032\u2032kw \u2032\u2032 k )) O = \u03a6L(y) ( 1 + log ( 1 + RWL \u03a6 W L (y)\n\u03a6L(y) )) 12 With no loss of generality, we assume that neither of the two sub-lines is empty, so that v0 is not a terminal node\nof ck.\nwhere in the second step we used Jensen\u2019s inequality and in the last one the fact that \u2211\nk(R \u2032 k +\nR\u2032\u2032k) = R W L and maxk w \u2032 k O = \u03a6WL (y), maxk w \u2032\u2032 k O = \u03a6WL (y). This proves the lemma in the case E \u2032 \u2261 \u2205. In order to conclude the proof, observe that if we take any semi-cluster c\u2032k (obtained, as before, by splitting cluster ck, being v0 \u2208 ck the first node whose label is predicted by WTA), and pretend to split it into two sub-clusters connected by a \u03c6-free edge, we could repeat the previous dichotomic argument almost verbatim on the two sub-clusters at the cost of adding an extra mistake. We now make this intuitive argument more precise. Let (i, j) be a \u03c6-free edge belonging to semi-cluster c\u2032k, and suppose without loss of generality that i is closer to v0 than to j. If we remove edge (i, j) then c\u2032k splits into two subclusters: c \u2032 k(v0) and c \u2032 k(j), containing node v0 and j, respectively (see Figure 7). Let mc\u2032k , mc\u2032k(v0) and mc\u2032k(j) be the number of mistakes made on c \u2032 k, c \u2032 k(v0) and c \u2032 k(j), respectively. We clearly have mc\u2032k = mc\u2032k(v0) +mc\u2032k(j).\nLet now \u03b3\u2032k be the semi-cluster obtained from c \u2032 k by contracting edge (i, j) so as to make i coincide with j (we sometimes write i \u2261 j). Cluster \u03b3\u2032k can be split into two parts which overlap only at node i \u2261 j: \u03b3\u2032k(v0), with terminal nodes v0 and i (coinciding with node j), and \u03b3\u2032k(j). In a similar fashion, let m\u03b3\u2032k , m\u03b3\u2032k(v0), and m\u03b3\u2032k(j) be the number of mistakes made on \u03b3 \u2032 k, \u03b3 \u2032 k(v0) and \u03b3\u2032k(j), respectively. We have m\u03b3\u2032k = m\u03b3\u2032k(v0) + m\u03b3\u2032k(j) \u2212 1, where the \u22121 takes into account that \u03b3\u2032k(v0) and \u03b3 \u2032 k(j) overlap at node i \u2261 j.\nObserving now that, for each node v belonging to c\u2032k(v0) (and \u03b3 \u2032 k(v0)), the distance d(v, v \u2032 k)\nis smaller on \u03b3k than on c\u2032k, we can apply the abovementioned dichotomic argument to bound the mistakes made on c\u2032k, obtaining m\u03b3\u2032k(v0) \u2264 mc\u2032k(v0). Since mc\u2032k(j) = m\u03b3\u2032k(j), we can finally write mc\u2032k = mc\u2032k(v0) +mc\u2032k(j) \u2264 m\u03b3\u2032k(v0) +m\u03b3\u2032k(j) = m\u03b3\u2032k + 1. Iterating this argument for all edges in E \u2032 concludes the proof.\nIn view of proving Theorem 3, we now prove the following two lemmas.\nLemma 11. Given any tree T , let E(T ) be the edge set of T , and let E(L\u2032) and E(L) be the edge sets of line graphs L\u2032 and L obtained via WTA\u2019s tree linearization of T . Then the following holds.\n1. There exists a partition PL\u2032 of E(L\u2032) in pairs and a bijective mapping \u00b5L\u2032 : PL\u2032 \u2192 E(T ) such that the weight of both edges in each pair S \u2032 \u2208 PL\u2032 is equal to the weight of the edge \u00b5L\u2032(S \u2032).\n2. There exists a partition PL of E(L) in sets S such that |S| \u2264 2, and there exists an injective mapping \u00b5L : PL \u2192 E(T ) such that the weight of the edges in each pair S \u2208 PL is equal to the weight of the edge \u00b5L(S).\nProof. We start by defining the bijective mapping \u00b5L\u2032 : PL\u2032 \u2192 E(T ). Since each edge (i, j) of T is traversed exactly twice in the depth-first visit that generates L\u2032,13 once in a forward step and once in a backward step, we partition E(L\u2032) in pairs S \u2032 such that \u00b5L\u2032(S \u2032) = (i, j) if and only if S \u2032 contains the pair of distinct edges created in L\u2032 by the two traversals of (i, j). By construction, the edges in each pair S \u2032 have weight equal to \u00b5L\u2032(S \u2032). Moreover, this mapping is clearly bijective, since any edge of L\u2032 is created by a single traversal of an edge in T . The second mapping \u00b5L : P(L)\u2192 E(T ) is created as follows. PL is created from PL\u2032 by removing from each S \u2032 \u2208 PL\u2032 the edges that are eliminated when L\u2032 is transformed into L. Note that we have\n\u2223\u2223PL\u2223\u2223 \u2264 \u2223\u2223PL\u2032\u2223\u2223 and for any S \u2208 PL there is a unique S \u2032 \u2208 PL\u2032 such that S \u2286 S \u2032. Now, for each S \u2208 PL let \u00b5L(S) = \u00b5L\u2032(S \u2032), where S \u2032 is such that S \u2286 S \u2032. Since \u00b5L\u2032 is bijective, \u00b5L is injective. Moreover, since the edges in S \u2032 have the same weight as the edge \u00b5L\u2032(S \u2032), the same property holds for \u00b5L.\nLemma 12. Let (T,y) be a labeled tree, let (L,y) be the linearization of T , and let L\u2032 be the line graph with duplicates (as described above). Then the following holds.14\n1. \u03a6WL (y) \u2264 \u03a6WL\u2032 (y) \u2264 2\u03a6WT (y);\n2. \u03a6L(y) \u2264 \u03a6L\u2032(y) \u2264 2\u03a6T (y).\nProof. From Lemma 11 (part 1) we know that L\u2032 contains a duplicated edge for each edge of T . This immediately implies \u03a6L\u2032(y) \u2264 2\u03a6T (y) and \u03a6WL\u2032 (y) \u2264 2\u03a6WT (y).\nTo prove the remaining inequalities, note that from the description of WTA in Section 4 (step 3), we see that when L\u2032 is transformed into L the pair of edges (j\u2032, j) and (j, j\u2032\u2032) of L\u2032, which are incident to a duplicate node j, gets replaced in L (together with j) by a single edge (j\u2032, j\u2032\u2032). Now each such edge (j\u2032, j\u2032\u2032) cannot be a \u03c6-edge in L unless either (j, j\u2032) or (j, j\u2032\u2032) is a \u03c6-edge in L\u2032, and this establishes \u03a6L(y) \u2264 \u03a6L\u2032(y). Finally, if (j\u2032, j\u2032\u2032) is a \u03c6-edge in L, then its weight is not larger than the weight of the associated \u03c6-edge in L\u2032 (step 3 of WTA), and this establishes \u03a6WL (y) \u2264 \u03a6WL\u2032 (y).\n13 For the sake of simplicity, we are assuming here that the depth-first visit of T terminates by backtracking over all nodes on the path between the last node visited in a forward step and the root.\n14 Item 2 in this lemma is essentially contained in the paper by [18].\nRecall that, given a labeled graph G = (V,E) and any \u03c6-free edge subset E \u2032 \u2282 E \\ E\u03c6, the quantity RWG (\u00acE \u2032) is the sum of the resistors of all \u03c6-free edges in E \\ (E\u03c6 \u222a E \u2032).\nLemma 13. If WTA is run on a weighted line graph (L,y) obtained through the linearization of a given labeled tree (T,y) with edge set E, then the total number mT of mistakes satisfies\nmT O = \u03a6L(y) ( 1 + log2 ( 1 +\nRWT (\u00acE \u2032) \u03a6WL (y) \u03a6L(y)\n)) + \u03a6T (y) + |E \u2032| ,\nwhere E \u2032 is an arbitrary subset of E \\ E\u03c6.\nProof. Lemma 11 (Part 2), exhibits an injective mapping \u00b5L : P \u2192 E, where P is a partition of the edge set E(L) of L, such that every S \u2208 P satisfies |S| \u2264 2. Hence, we have |E \u2032(L)| \u2264 2|E \u2032|, whereE \u2032(L) is the union of the pre-images of edges inE \u2032 according to \u00b5L \u2014note that some edge in E \u2032 might not have a pre-image in E(L). By the same argument, we also establish |E0(L)| \u2264 2\u03a6T , where E0(L) is the set of \u03c6-free edges of L that belong to elements S of the partition PL such that \u00b5L(S) \u2208 E\u03c6.\nSince the edges of L that are neither in E0(L) nor in E \u2032(L) are partitioned by PL in edge sets having cardinality at most two, which in turn can be injectively mapped via \u00b5L to E \\ (E\u03c6 \u222a E \u2032), we have RWL ( \u00ac ( E \u2032(L)\u222aE0(L) )) \u2264 2RWT (\u00acE \u2032) . Finally, we use |E \u2032(L)| \u2264 2|E \u2032| and |E0(L)| \u2264 2\u03a6T (y) (which we just established) and apply Lemma 2 withE \u2032 \u2261 E \u2032(L)\u222aE0(L). This concludes the proof.\nof Theorem 3. We use Lemma 12 to establish \u03a6L(y) \u2264 2\u03a6T (y) and \u03a6WL (y) \u2264 2\u03a6WT (y). We then conclude with an application of Lemma 13.\nLemma 14. If WTA is run on a weighted line graph (L,y) obtained through the linearization of random spanning tree T of a labeled weighted graph (G,y), then the total number mG of mistakes satisfies\nEmG O = E [ \u03a6L(y) ] ( 1 + log ( 1 + w\u03c6maxE [ RWT ]) + E [ \u03a6T (y) ]) ,\nwhere w\u03c6max = max(i,j)\u2208E\u03c6 wi,j .\nProof. Using Lemma 13 with E \u2032 \u2261 \u2205 we can write\nEmG O = E [ \u03a6L(y) ( 1 + log ( 1 + RWT \u03a6 W L (y)\n\u03a6L(y)\n)) + \u03a6T ] O = E [ \u03a6L(y) ( 1 + log ( 1 +RWT w \u03c6 max )) + \u03a6T\n] O = E [ \u03a6L(y) ]( 1 + log ( 1 + E [ RWT ] w\u03c6max )) + E [ \u03a6T (y) ] ,\nwhere the second equality follows from the fact that \u03a6WL (y) \u2264 \u03a6L(y)w\u03c6max, which in turn follows from Lemma 11, and the third one follows from Jensen\u2019s inequality applied to the concave function (x, y) 7\u2192 x ( 1 + log ( 1 + y w\u03c6max )) for x, y \u2265 0.\nTheorem 4. We apply Lemma 14 and then Lemma 12 to get \u03a6L(y) \u2264 2\u03a6T (y).\nCorollary 5. Let f > poly(n) denote a function growing faster than any polynomial in n. Choose a polynomially connected graph G and a labeling y. For the sake of contradiction, assume that WTA makes more thanO(E [ \u03a6T (y) ] log n) mistakes on (G,y). Then Theorem 4 impliesw\u03c6maxE [ RWT ] >\npoly(n). Since E [ RWT ] = \u2211 (i,j)\u2208E\\E\u03c6 r W i,j , we have that w \u03c6 max max(i,j)\u2208E\\E\u03c6 r W i,j > poly(n). Together with the assumption of polynomial connectivity for G, this implies w\u03c6maxr W i,j > poly(n) for all \u03c6-free edges (i, j). By definition of effective resistance, wi,jrWi,j \u2264 1 for all (i, j) \u2208 E. This gives w\u03c6max/wi,j > poly(n) for all \u03c6-free edges (i, j), which in turn implies\u2211\n(i,j)\u2208E\u03c6 wi,j\u2211 (i,j)\u2208E\\E\u03c6 wi,j > poly(n) .\nAs this contradicts our hypothesis, the proof is concluded.\nTheorem 6. We only prove the first part of the theorem. The proof of the second part corresponds to the special case when all weights are equal to 1.\nLet \u2206(y,y\u2032) \u2286 V be the set of nodes i such that yi 6= y\u2032i. We therefore have \u03b4(y,y\u2032) = |\u2206(y,y\u2032)|. Since in a line graph each node is adjacent to at most two other nodes, the label flip of any node j \u2208 \u2206(y,y\u2032) can cause an increase of the weighted cutsize of L by at most wi\u2032,j + wj,i\u2032\u2032 , where i\u2032 and i\u2032\u2032 are the two nodes adjacent to j in L.15 Hence, flipping the labels of all nodes in \u2206(y,y\u2032), we have that the total cutsize increase is bounded by the sum of the weights of the 2\u03b4(y,y\u2032) heaviest edges in L, which implies\n\u03a6WL (y) \u2264 \u03a6WL (y\u2032) + \u03b6L ( 2\u03b4(y,y\u2032) ) .\nBy Lemma 12, \u03a6WL (u) \u2264 2\u03a6WT (u). Moreover, Lemma 11 gives an injective mapping \u00b5L : PL \u2192 E (E is the edge set of T ) such that the elements of P have cardinality at most two, and the weight of each edge \u00b5L(S) is the same as the weights of the edges in S. Hence, the total weight of the 2\u03b4(y,y\u2032) heaviest edges in L is at most twice the total weight of the \u03b4(y,y\u2032) heaviest edges in T . Therefore \u03b6L ( 2\u03b4(y,y\u2032) ) \u2264 2\u03b6T ( \u03b4(y,y\u2032) ) . Hence, we have obtained\n\u03a6WL (y) \u2264 2\u03a6WT (y\u2032) + 2\u03b6T ( \u03b4(y,y\u2032) ) ,\nconcluding the proof.\nTheorem 7. We use Theorem 6 to bound \u03a6L(y) and \u03a6WL (y) in the mistake bound of Lemma 13.\nCorollary 8. Recall that the resistance between two nodes i and j of any tree is simply the sum of the inverse weights over all edges on the path connecting the two nodes. Since T is polynomially connected, we know that the ratio of any pair of edge weights is polynomial in n. This implies that RWL \u03a6 W L (y) is polynomial in n, too. We apply Theorem 6 to bound \u03a6L(y) in the mistake bound of Lemma 2 with E \u2032 = \u2205. This concludes the proof. 15 In the special case when j is terminal node we can set wj,i\u2032\u2032 = 0.\nLemma 15. If WTA is run on a line graph L obtained by linearizing a random spanning tree T of a labeled and weighted graph (G,y), then we have\nE [ \u03a6L(y) ] O = \u03a6\u2217G(y) .\nProof. Recall that Theorem 6 holds for any spanning tree T of G. Thus it suffices to apply part 2 of Theorem 6 and use E [ minX ] \u2264 minE[X] .\nTheorem 9. We apply Lemma 15 to bound E [ \u03a6L(y) ] in Lemma 14.\nCorollary 10. Since the ratio of the weights of any pair of edges in G is polynomial in n, the spanning tree T must be polynomially connected. Thus we can use Corollary 8, and bound E [ \u03a6L(y) ] via Lemma 15."}, {"heading": "Appendix B", "text": "This appendix summarizes all our experimental results. For each combination of dataset, algorithm, and train/test split, we provide macro-averaged error rates and F-measures on the test set. The algorithms are WTA, NWWTA, and GPA (all combined with various spanning trees), WMV, LABPROP, and WTA run with committees of random spanning trees. WEBSPAM was too large a dataset to perform as thorough an investigation. Hence we only report test error results on the four algorithms WTA, WMV, LABPROP, and WTA with a committee of 7 (nonweighted) random spanning trees."}], "references": [{"title": "Many random walks are faster than one", "author": ["N. Alon", "C. Avin", "M. Kouck\u00fd", "G. Kozma", "Z. Lotker", "M.R. Tuttle"], "venue": "Proc. of the 20th Annual ACM Symposium on Parallel Algorithms and Architectures, pages 119\u2013128. Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Regularization and semi-supervised learning on large graphs", "author": ["M. Belkin", "I. Matveeva", "P. Niyogi"], "venue": "Proc. of the 17th Annual Conference on Learning Theory, pages 624\u2013638. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Label propagation and quadratic criterion", "author": ["Y. Bengio", "O. Delalleau", "N. Le Roux"], "venue": "Semi-Supervised Learning, pages 193\u2013216. MIT Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning from labeled and unlabeled data using graph mincuts", "author": ["A. Blum", "S. Chawla"], "venue": "Proc. of the 18th International Conference on Machine Learning, pages 19\u201326. Morgan Kaufmann", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Semi-supervised learning using randomized mincuts", "author": ["A. Blum", "J. Lafferty", "M. Rwebangira", "R. Reddy"], "venue": "Proc. of the 21st International Conference on Machine Learning, pages 97\u2013104", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Generating random spanning trees", "author": ["A. Broder"], "venue": "Proc. of the 30th Annual Symposium on Foundations of Computer Science, pages 442\u2013447. IEEE Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1989}, {"title": "Fast and optimal prediction of a labeled tree", "author": ["N. Cesa-Bianchi", "C. Gentile", "F. Vitale"], "venue": "Proceedings of the 22nd Annual Conference on Learning Theory. Omnipress", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Random spanning trees and the prediction of weighted graphs", "author": ["N. Cesa-Bianchi", "C. Gentile", "F. Vitale", "G. Zappella"], "venue": "Proceedings of the 27th International Conference on Machine Learning (27th ICML)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Active learning on trees and graphs", "author": ["N. Cesa-Bianchi", "C. Gentile", "F. Vitale", "G. Zappella"], "venue": "Proceedings of the 23rd Conference on Learning Theory (23rd COLT)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Graph Laplacian kernels for object classification from a single example", "author": ["H. Chang", "D.Y. Yeung"], "venue": "Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 2011\u20132016. IEEE Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Low congestion online routing and an improved mistake bound for online prediction of graph labeling", "author": ["J. Fakcharoenphol", "B. Kijsirikul"], "venue": "CoRR, abs/0809.2075", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Functional organization of the yeast proteome by systematic analysis of protein complexes", "author": ["A.-C. Gavin"], "venue": "Nature, 415(6868):141\u2013147,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Seeing stars when there aren\u2019t many stars: Graph-based semisupervised learning for sentiment categorization", "author": ["A. Goldberg", "X. Zhu"], "venue": "HLT-NAACL 2006 Workshop on Textgraphs: Graph-based algorithms for Natural Language Processing", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Exploiting cluster-structure to predict the labeling of a graph", "author": ["M. Herbster"], "venue": "Proc. of the 19th International Conference on Algorithmic Learning Theory, pages 54\u201369. Springer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Predicting the labelling of a graph via minimum p-seminorm interpolation", "author": ["M. Herbster", "G. Lever"], "venue": "Proc. of the 22nd Annual Conference on Learning Theory. Omnipress", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Prediction on a graph with the Perceptron", "author": ["M. Herbster", "M. Pontil"], "venue": "Advances in Neural Information Processing Systems 21, pages 577\u2013584. MIT Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Online learning over graphs", "author": ["M. Herbster", "M. Pontil", "L. Wainer"], "venue": "Proc. of the 22nd International Conference on Machine Learning, pages 305\u2013312. ACM Press", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Online prediction on large diameter graphs", "author": ["M. Herbster", "G. Lever", "M. Pontil"], "venue": "Advances in Neural Information Processing Systems 22, pages 649\u2013656. MIT Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast prediction on a tree", "author": ["M. Herbster", "M. Pontil", "S. Rojas-Galeano"], "venue": "Advances in Neural Information Processing Systems 22, pages 657\u2013664. MIT Press", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "A comprehensive two-hybrid analysis to explore the yeast protein interactome", "author": ["T. Ito", "T. Chiba", "R. Ozawa", "M. Yoshida", "M. Hattori", "Y. Sakaki"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, 98(8):4569\u20134574", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Global landscape of protein complexes in the yeast", "author": ["N.J. Krogan"], "venue": "Saccharomyces cerevisiae. Nature,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Probability on trees and networks", "author": ["R. Lyons", "Y. Peres"], "venue": "Manuscript", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Association analysis-based transformations for protein interaction networks: a function prediction case study", "author": ["G. Pandey", "M. Steinbach", "R. Gupta", "T. Garg", "V. Kumar"], "venue": "Proc. of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 540\u2013549. ACM Press", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "The FunCat", "author": ["A. Ruepp"], "venue": "a functional annotation scheme for systematic classification of proteins from whole genomes. Nucleic Acids Research, 32(18):5539\u20135545", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Graph sparsification by effective resistances", "author": ["D.A. Spielman", "N. Srivastava"], "venue": "Proc. of the 40th annual ACM symposium on Theory of computing, pages 563\u2013568. ACM Press", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Protein functional class prediction with a combined graph", "author": ["H. Shin K. Tsuda", "B. Sch\u00f6lkopf"], "venue": "Expert Systems with Applications,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "A comprehensive analysis of protein-protein interactions in Saccharomyces cerevisiae", "author": ["P. Uetz"], "venue": "Nature, 6770(403):623\u2013627,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2000}, {"title": "See the tree through the lines: the Shazoo algorithm", "author": ["F. Vitale", "N. Cesa-Bianchi", "C. Gentile", "G. Zappella"], "venue": "Proc. of the 25th Annual Conference on Neural Information Processing Systems, pages 1584-1592. Curran Associates", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Generating random spanning trees more quickly than the cover time", "author": ["D.B. Wilson"], "venue": "Proc. of the 28th ACM Symposium on the Theory of Computing, pages 296\u2013303. ACM Press", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1996}, {"title": "Semi-supervised learning using Gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "ICML Workshop on the Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 18, "context": "This technique for coding input data has been applied to several domains, including Web spam detection [19], classification of genomic data [27], face recognition [10], and text categorization [13].", "startOffset": 103, "endOffset": 107}, {"referenceID": 25, "context": "This technique for coding input data has been applied to several domains, including Web spam detection [19], classification of genomic data [27], face recognition [10], and text categorization [13].", "startOffset": 140, "endOffset": 144}, {"referenceID": 9, "context": "This technique for coding input data has been applied to several domains, including Web spam detection [19], classification of genomic data [27], face recognition [10], and text categorization [13].", "startOffset": 163, "endOffset": 167}, {"referenceID": 12, "context": "This technique for coding input data has been applied to several domains, including Web spam detection [19], classification of genomic data [27], face recognition [10], and text categorization [13].", "startOffset": 193, "endOffset": 197}, {"referenceID": 16, "context": ", when all edges have the same common weight), it was found in previous works [17, 16, 14, 15, 8] that a key parameter to control the number of online prediction mistakes is the size of the cut induced by the unknown adversarial labeling of the nodes, i.", "startOffset": 78, "endOffset": 97}, {"referenceID": 15, "context": ", when all edges have the same common weight), it was found in previous works [17, 16, 14, 15, 8] that a key parameter to control the number of online prediction mistakes is the size of the cut induced by the unknown adversarial labeling of the nodes, i.", "startOffset": 78, "endOffset": 97}, {"referenceID": 13, "context": ", when all edges have the same common weight), it was found in previous works [17, 16, 14, 15, 8] that a key parameter to control the number of online prediction mistakes is the size of the cut induced by the unknown adversarial labeling of the nodes, i.", "startOffset": 78, "endOffset": 97}, {"referenceID": 14, "context": ", when all edges have the same common weight), it was found in previous works [17, 16, 14, 15, 8] that a key parameter to control the number of online prediction mistakes is the size of the cut induced by the unknown adversarial labeling of the nodes, i.", "startOffset": 78, "endOffset": 97}, {"referenceID": 7, "context": ", when all edges have the same common weight), it was found in previous works [17, 16, 14, 15, 8] that a key parameter to control the number of online prediction mistakes is the size of the cut induced by the unknown adversarial labeling of the nodes, i.", "startOffset": 78, "endOffset": 97}, {"referenceID": 6, "context": "This naturally led to the idea of solving the prediction problem on a spanning tree of the graph [7, 18, 19], whose number of edges is exactly equal to the number of nodes minus one.", "startOffset": 97, "endOffset": 108}, {"referenceID": 17, "context": "This naturally led to the idea of solving the prediction problem on a spanning tree of the graph [7, 18, 19], whose number of edges is exactly equal to the number of nodes minus one.", "startOffset": 97, "endOffset": 108}, {"referenceID": 18, "context": "This naturally led to the idea of solving the prediction problem on a spanning tree of the graph [7, 18, 19], whose number of edges is exactly equal to the number of nodes minus one.", "startOffset": 97, "endOffset": 108}, {"referenceID": 6, "context": "This suggests to pick a tree at random among all spanning trees of the graph so as to prevent the adversary from concentrating the cutsize on the chosen tree [7].", "startOffset": 158, "endOffset": 161}, {"referenceID": 6, "context": "Although the results of [7] yield a mistake bound for arbitrary unweighted graphs in terms of the cutsize of a random spanning tree, no general lower bounds are known for online unweighted graph prediction.", "startOffset": 24, "endOffset": 27}, {"referenceID": 15, "context": "The scenario gets even more uncertain in the case of weighted graphs, where the only previous papers we are aware of [16, 14, 15] essentially contain only upper bounds.", "startOffset": 117, "endOffset": 129}, {"referenceID": 13, "context": "The scenario gets even more uncertain in the case of weighted graphs, where the only previous papers we are aware of [16, 14, 15] essentially contain only upper bounds.", "startOffset": 117, "endOffset": 129}, {"referenceID": 14, "context": "The scenario gets even more uncertain in the case of weighted graphs, where the only previous papers we are aware of [16, 14, 15] essentially contain only upper bounds.", "startOffset": 117, "endOffset": 129}, {"referenceID": 6, "context": "Following the ideas of [7], our algorithm first extracts a random spanning tree of the original graph.", "startOffset": 23, "endOffset": 26}, {"referenceID": 17, "context": "Then, it predicts all nodes of this tree using a generalization of the method proposed by [18].", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "As in [18], our algorithm first linearizes the tree, and then operates on the resulting line graph via a nearest neighbor rule.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "In particular, we test our algorithm against the Perceptron algorithm with Laplacian kernel by [16, 19], and against a version of the label propagation algorithm by [31].", "startOffset": 95, "endOffset": 103}, {"referenceID": 18, "context": "In particular, we test our algorithm against the Perceptron algorithm with Laplacian kernel by [16, 19], and against a version of the label propagation algorithm by [31].", "startOffset": 95, "endOffset": 103}, {"referenceID": 29, "context": "In particular, we test our algorithm against the Perceptron algorithm with Laplacian kernel by [16, 19], and against a version of the label propagation algorithm by [31].", "startOffset": 165, "endOffset": 169}, {"referenceID": 21, "context": ", the monograph of [22].", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "Hence the ratio 1 n\u22121E\u03a6T (y) \u2208 [0, 1] provides a density-independent measure of the cutsize in G, and even allows to compare labelings on different graphs.", "startOffset": 31, "endOffset": 37}, {"referenceID": 13, "context": "A different way of taking into account graph connectivity is provided by the covering ball approach taken by [14, 15] \u2013see the next section.", "startOffset": 109, "endOffset": 117}, {"referenceID": 14, "context": "A different way of taking into account graph connectivity is provided by the covering ball approach taken by [14, 15] \u2013see the next section.", "startOffset": 109, "endOffset": 117}, {"referenceID": 16, "context": "The graph Perceptron algorithm [17, 16] uses K = LG + 11>, where LG is the (weighted) Laplacian of G and 1 = (1, .", "startOffset": 31, "endOffset": 39}, {"referenceID": 15, "context": "The graph Perceptron algorithm [17, 16] uses K = LG + 11>, where LG is the (weighted) Laplacian of G and 1 = (1, .", "startOffset": 31, "endOffset": 39}, {"referenceID": 18, "context": "The idea of using a spanning tree to reduce the cutsize ofG has been investigated by [19], where the graph Perceptron algorithm is applied to a spanning tree T ofG.", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "[19] suggest to apply the graph Perceptron algorithm to the spanning tree T with smallest geodesic diameter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] introduce a different technique showing its application to the case of unweighted graphs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "By running a Nearest Neighbor (NN) predictor on S, [18] prove a mistake bound of the form \u03a6S(y) log ( n / \u03a6S(y) ) + \u03a6S(y).", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "As observed by [11], similar techniques have been developed to solve low-congestion routing problems.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "With this inductive bias in mind, [14] developed the Pounce algorithm, which can be seen as a combination of graph Perceptron and NN prediction.", "startOffset": 34, "endOffset": 38}, {"referenceID": 17, "context": "A further trick for the unweighted case proposed by [18] is to take advantage of both previous approaches (graph Perceptron and NN on line graphs) by building a binary tree on G.", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "A similar logarithmic factor is achieved by the combined algorithm proposed in [18].", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "An even more refined way of exploiting cluster structure and connectivity in graphs is contained in the paper of [15], where the authors provide a comprehensive study of the application of dualnorm techniques to the prediction of weighted graphs, again with the goal of obtaining logarithmic performance guarantees on large diameter graphs.", "startOffset": 113, "endOffset": 117}, {"referenceID": 14, "context": "Further comments on the comparison between the results presented by [15] and the ones in our paper are postponed to the end of Section 5.", "startOffset": 68, "endOffset": 72}, {"referenceID": 3, "context": "Departing from the online learning scenario, it is worth mentioning the significantly large literature on the general problem of learning the nodes of a graph in the train/test transductive setting: Many algorithms have been proposed, including the label-consistent mincut approach of [4, 5] and a number of other \u201cenergy minimization\u201d methods \u2014e.", "startOffset": 285, "endOffset": 291}, {"referenceID": 4, "context": "Departing from the online learning scenario, it is worth mentioning the significantly large literature on the general problem of learning the nodes of a graph in the train/test transductive setting: Many algorithms have been proposed, including the label-consistent mincut approach of [4, 5] and a number of other \u201cenergy minimization\u201d methods \u2014e.", "startOffset": 285, "endOffset": 291}, {"referenceID": 29, "context": ", the ones by [31, 2] of which label propagation is an instance.", "startOffset": 14, "endOffset": 21}, {"referenceID": 1, "context": ", the ones by [31, 2] of which label propagation is an instance.", "startOffset": 14, "endOffset": 21}, {"referenceID": 2, "context": "See the work of [3] for a relatively recent survey on this subject.", "startOffset": 16, "endOffset": 19}, {"referenceID": 21, "context": ", the recent monograph by [22].", "startOffset": 26, "endOffset": 30}, {"referenceID": 5, "context": "In the unweighted case, a random spanning tree can be sampled with a random walk in expected time O(n lnn) for \u201cmost\u201d graphs, as shown by [6].", "startOffset": 138, "endOffset": 141}, {"referenceID": 28, "context": "Using the beautiful algorithm of [30], the expected time reduces to O(n) \u2014see also the work of [1].", "startOffset": 33, "endOffset": 37}, {"referenceID": 0, "context": "Using the beautiful algorithm of [30], the expected time reduces to O(n) \u2014see also the work of [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 14, "context": "3 This is one of the examples considered in [15].", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": ", [22].", "startOffset": 2, "endOffset": 6}, {"referenceID": 24, "context": ", the work of [26].", "startOffset": 14, "endOffset": 18}, {"referenceID": 17, "context": "Though this technique is similar to that one used by [18], the fact that the tree is weighted makes the analysis significantly more difficult, and the practical scope of our algorithm significantly wider.", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "Another interesting comparison is to the covering ball bounds of [14, 15].", "startOffset": 65, "endOffset": 73}, {"referenceID": 14, "context": "Another interesting comparison is to the covering ball bounds of [14, 15].", "startOffset": 65, "endOffset": 73}, {"referenceID": 14, "context": "Whereas the dual norm approach of [15] gives a mistake bound of the form \u03a6G(y) logD, our approach, as well as the one by [18], yields \u03a6G(y) log n.", "startOffset": 34, "endOffset": 38}, {"referenceID": 17, "context": "Whereas the dual norm approach of [15] gives a mistake bound of the form \u03a6G(y) logD, our approach, as well as the one by [18], yields \u03a6G(y) log n.", "startOffset": 121, "endOffset": 125}, {"referenceID": 6, "context": "A typical (and unsurprising) example where the dual-norm covering 6 As a matter of fact, a bound of the form \u03a6G(y) logD on unweighted trees is also achieved by the direct analysis of [7].", "startOffset": 183, "endOffset": 186}, {"referenceID": 14, "context": "One such example we already mentioned in Section 2: On the unweighted barbell graph made up of m-cliques connected by k m \u03c6-edges, the algorithm of [15] has a constant bound on the number of mistakes (i.", "startOffset": 148, "endOffset": 152}, {"referenceID": 13, "context": "Yet, it is fair to point out that the bounds of [14, 15] refer to computationally heavier algorithms than WTA: Pounce has a deterministic initialization step that computes the inverse Laplacian matrix of the graph (this is cubic in n, or quadratic in the case of trees), the minimum (\u03a8, p)-seminorm interpolation algorithm of [15] has no initialization, but each step requires the solution of a constrained convex optimization problem (whose time complexity was not quantified by the authors).", "startOffset": 48, "endOffset": 56}, {"referenceID": 14, "context": "Yet, it is fair to point out that the bounds of [14, 15] refer to computationally heavier algorithms than WTA: Pounce has a deterministic initialization step that computes the inverse Laplacian matrix of the graph (this is cubic in n, or quadratic in the case of trees), the minimum (\u03a8, p)-seminorm interpolation algorithm of [15] has no initialization, but each step requires the solution of a constrained convex optimization problem (whose time complexity was not quantified by the authors).", "startOffset": 48, "endOffset": 56}, {"referenceID": 14, "context": "Yet, it is fair to point out that the bounds of [14, 15] refer to computationally heavier algorithms than WTA: Pounce has a deterministic initialization step that computes the inverse Laplacian matrix of the graph (this is cubic in n, or quadratic in the case of trees), the minimum (\u03a8, p)-seminorm interpolation algorithm of [15] has no initialization, but each step requires the solution of a constrained convex optimization problem (whose time complexity was not quantified by the authors).", "startOffset": 326, "endOffset": 330}, {"referenceID": 15, "context": "Introduced by [16] and here abbreviated as GPA (graph Perceptron algorithm).", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": "Following [19], we run GPA on a spanning tree T of the original graph.", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "This is because a careful computation of the Laplacian pseudoinverse of a n-node tree takes time \u0398(n+m +mD) where m is the number of training examples plus the number of test examples (labels to predict), and D is the tree diameter \u2014see the work of [19] for a proof of this fact.", "startOffset": 249, "endOffset": 253}, {"referenceID": 29, "context": "Introduced by [31] and here abbreviated as LABPROP.", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": ", the paper of [19].", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "[19] use the shortest path tree because it has a small diameter (at most twice the diameter of G).", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "This is essentially the algorithm analyzed by [18], and we denote it by NWWTA (non-weighted WTA).", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "The USPS dataset9 with features normalized into [0, 2].", "startOffset": 48, "endOffset": 54}, {"referenceID": 20, "context": "It has been used by [21] and [23].", "startOffset": 20, "endOffset": 24}, {"referenceID": 22, "context": "It has been used by [21] and [23].", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "A second dataset from the work of [23].", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "It is a combination of three datasets: [12]\u2019s, [20]\u2019s, and [28]\u2019s.", "startOffset": 39, "endOffset": 43}, {"referenceID": 19, "context": "It is a combination of three datasets: [12]\u2019s, [20]\u2019s, and [28]\u2019s.", "startOffset": 47, "endOffset": 51}, {"referenceID": 26, "context": "It is a combination of three datasets: [12]\u2019s, [20]\u2019s, and [28]\u2019s.", "startOffset": 59, "endOffset": 63}, {"referenceID": 29, "context": "Following previous experimental settings [31, 2], the graphs were constructed using k-NN based on the standard Euclidean distance \u2016xi \u2212 xj\u2016 between node i and node j.", "startOffset": 41, "endOffset": 48}, {"referenceID": 1, "context": "Following previous experimental settings [31, 2], the graphs were constructed using k-NN based on the standard Euclidean distance \u2016xi \u2212 xj\u2016 between node i and node j.", "startOffset": 41, "endOffset": 48}, {"referenceID": 23, "context": "We selected the set of functional labels at depth one in the FunCat classification scheme of the MIPS database [25], resulting in seventeen classes per dataset.", "startOffset": 111, "endOffset": 115}, {"referenceID": 6, "context": "First, previous analyses [7] reveal that WTA\u2019s analysis is loose, at least when the input graph is an unweighted tree with small diameter.", "startOffset": 25, "endOffset": 28}, {"referenceID": 27, "context": "A partial answer to this question is provided by the recent work of [29].", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "It would also be nice to tie this machinery with recent results in the active node classification setting on trees contained in [9].", "startOffset": 128, "endOffset": 131}, {"referenceID": 17, "context": "14 Item 2 in this lemma is essentially contained in the paper by [18].", "startOffset": 65, "endOffset": 69}], "year": 2012, "abstractText": "We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space. Experiments on real-world datasets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice.", "creator": "LaTeX with hyperref package"}}}