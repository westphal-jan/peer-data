{"id": "1301.1936", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jan-2013", "title": "Risk-Aversion in Multi-armed Bandits", "abstract": "Stochastic multi-armed bandits solve the Exploration-Exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk-aversion where the objective is to compete against the arm with the best risk-return trade-off. This setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we introduce two new algorithms, investigate their theoretical guarantees, and report preliminary empirical results.", "histories": [["v1", "Wed, 9 Jan 2013 18:02:54 GMT  (429kb,D)", "http://arxiv.org/abs/1301.1936v1", "(2012)"]], "COMMENTS": "(2012)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["amir sani", "alessandro lazaric", "r\u00e9mi munos"], "accepted": true, "id": "1301.1936"}, "pdf": {"name": "1301.1936.pdf", "metadata": {"source": "CRF", "title": "Risk\u2013Aversion in Multi\u2013armed Bandits", "authors": ["Amir Sani", "Alessandro Lazaric", "R\u00e9mi Munos"], "emails": ["amir.sani@inria.fr", "alessandro.lazaric@inria.fr", "remi.munos@inria.fr"], "sections": [{"heading": null, "text": "Stochastic multi\u2013armed bandits solve the Exploration\u2013Exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk\u2013aversion where the objective is to compete against the arm with the best risk\u2013return trade\u2013off. This setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we introduce two new algorithms, investigate their theoretical guarantees, and report preliminary empirical results."}, {"heading": "1 Introduction", "text": "The multi\u2013armed bandit [13] elegantly formalizes the problem of on\u2013line learning with partial feedback, which encompasses a large number of real\u2013world applications, such as clinical trials, online advertisements, adaptive routing, and cognitive radio. In the stochastic multi\u2013armed bandit model, a learner chooses among several arms (e.g., different treatments), each characterized by an independent reward distribution (e.g., the treatment effectiveness). At each point in time, the learner selects one arm and receives a noisy reward observation from that arm (e.g., the effect of the treatment on one patient). Given a finite number of n rounds (e.g., patients involved in the clinical trial), the learner faces a dilemma between repeatedly exploring all arms and collecting reward information versus exploiting current reward estimates by selecting the arm with the highest estimated reward. Roughly speaking, the learning objective is to solve this exploration\u2013exploitation dilemma and accumulate as much reward as possible over n rounds. In particular, multi\u2013arm bandit literature typically focuses on the problem of finding a learning algorithm capable of maximizing the expected cumulative reward (i.e., the reward collected over n rounds averaged over all possible observation realizations), thus implying that the best arm returns the highest expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not always the most desirable objective. For instance, in clinical trials, the treatment which works best on average might also have considerable variability; resulting in adverse side effects for some patients. In this case, a treatment which is less effective on average but consistently effective on different patients may be preferable w.r.t. an effective but risky treatment. More generally, some application objectives require an effective trade\u2013off between risk and reward.\nThere is no agreed upon definition for risk. A variety of behaviours result in an uncertainty which might be deemed unfavourable for a specific application and referred to as a risk. For example, a solution with guarantees over multiple runs of an algorithm may not satisfy the desire for a solution with low variability over a single implementation of an algorithm. Two foundational risk modeling paradigms are Expected Utility theory [12] and the historically popular and accessible Mean-Variance paradigm [10]. A large part of decision\u2013making theory focuses on defining and managing risk (see e.g., [9] for an introduction to risk from an expected utility theory perspective).\nar X\niv :1\n30 1.\n19 36\nv1 [\ncs .L\nG ]\n9 J\nan 2\n01 3\nRisk has mostly been studied in on\u2013line learning within the so\u2013called expert advice setting (i.e., adversarial full\u2013information on\u2013line learning). In particular, [8] showed that in general, although it is possible to achieve a small regret w.r.t. to the expert with the best average performance, it is not possible to compete against the expert which best trades off between average return and risk. On the other hand, it is possible to define no\u2013regret algorithms for simplified measures of risk\u2013return. [15] studied the case of pure risk minimization (notably variance minimization) in an on-line setting where at each step the learner is given a covariance matrix and must choose a weight vector that minimizes the variance. The regret is then computed over horizon n and compared to the fixed weights minimizing the variance in hindsight. In the multi\u2013arm bandit domain, the most interesting results are by [5] and [14]. [5] introduced an analysis of the expected regret and its distribution, revealing that an anytime version of UCB [6] and UCB-V might have large regret with some nonnegligible probability.1 This analysis is further extended by [14] who derived negative results which show no anytime algorithm can achieve a regret with both a small expected regret and exponential tails. Although these results represent an important step towards the analysis of risk within bandit algorithms, they are limited to the case where an algorithm\u2019s cumulative reward is compared to the reward obtained by pulling the arm with the highest expectation.\nIn this paper, we focus on the problem of competing against the arm with the best risk\u2013return trade\u2013 off. In particular, we refer to the first and most popular measure of risk\u2013return, the mean\u2013variance model introduce by [10]. In Section 2 we introduce notation and define the mean\u2013variance bandit problem. In Section 3 we introduce a confidence\u2013bound algorithm and study its theoretical properties. In Section 5 we report a set of numerical simulations aiming at validating the theoretical results. Finally, in Section 7 we conclude with a discussion on possible extensions. The proofs and additional experiments are reported in the appendix."}, {"heading": "2 Mean\u2013Variance Multi\u2013arm Bandit", "text": "In this section we introduce the main notation used throughout the paper and define the mean\u2013 variance multi\u2013arm bandit problem.\nWe consider the standard multi\u2013arm bandit setting withK arms, each characterized by a distribution \u03bdi bounded in the interval [0, 1]. Each distribution has a mean \u00b5i and a variance \u03c32i . The bandit problem is defined over a finite horizon of n rounds. We denote by Xi,s \u223c \u03bdi the s-th random sample drawn from the distribution of arm i. All arms and samples are independent. In the multi\u2013 arm bandit protocol, at each round t, an algorithm selects arm It and observes sample XIt,Ti,t , where Ti,t is the number of samples observed from arm i up to time t (i.e., Ti,t = \u2211t s=1 I{It = i}).\nWhile in the standard literature on multi\u2013armed bandits the objective is to select the arm leading to the highest reward in expectation (the arm with the largest expected value \u00b5i), here we focus on the problem of finding the arm which effectively trades off between its expected reward (i.e., the return) and its variability (i.e., the risk). Although a large number of models for risk\u2013return trade\u2013off have been proposed, here we focus on the most historically popular and simple model: the mean\u2013variance model proposed by [10],2 where the return of an arm is measured by the expected reward and its risk by its variance.\nDefinition 1. The mean\u2013variance of an arm i with mean \u00b5i, variance \u03c32i and coefficient of absolute risk tolerance \u03c1 is defined as3 MVi = \u03c32i \u2212 \u03c1\u00b5i.\nThus it easily follows that the best arm minimizes the mean\u2013variance, that is i\u2217 = arg mini=1,...,K MVi. We notice that we can obtain two extreme settings depending on the value of risk tolerance \u03c1. As \u03c1\u2192\u221e, the mean\u2013variance of arm i tends to the opposite of its expected value \u00b5i and the problem reduces to the standard expected reward maximization traditionally considered in multi\u2013arm bandit problems. With \u03c1 = 0, the mean\u2013variance reduces to minimizing the variance \u03c32i and the objective becomes variance minimization.\n1Although the analysis is mostly directed to the pseudo\u2013regret, as commented in Remark 2 at page 23 of [5], it can be extended to the true regret.\n2We discuss the limitations of this model and possible extensions to other models of risk in Section 7. 3The coefficient of risk tolerance is the inverse of the more popular coefficient of risk aversion A = 1/\u03c1.\nGiven {Xi,s}ts=1 i.i.d. samples from the distribution \u03bdi, we define the empirical mean\u2013variance of an arm i with t samples as M\u0302Vi,t = \u03c3\u03022i,t \u2212 \u03c1\u00b5\u0302i,t, where\n\u00b5\u0302i,t = 1\nt\nt\u2211\ns=1\nXi,s, \u03c3\u0302 2 i,t =\n1\nt\nt\u2211\ns=1\n( Xi,s \u2212 \u00b5\u0302i,t )2 . (1)\nWe now consider a learning algorithm A and its corresponding performance over n rounds. Similar to a single arm i we define its empirical mean\u2013variance as\nM\u0302Vn(A) = \u03c3\u03022n(A)\u2212 \u03c1\u00b5\u0302n(A), (2) where\n\u00b5\u0302n(A) = 1\nn\nn\u2211\nt=1\nZt, \u03c3\u0302 2 n(A) =\n1\nn\nn\u2211\nt=1\n( Zt \u2212 \u00b5\u0302n(A) )2 , (3)\nwith Zt = XIt,Ti,t , that is the reward collected by the algorithm at time t. This leads to a natural definition of the (random) regret at each single run of the algorithm as the difference in the mean\u2013 variance performance of the algorithm compared to the best arm. Definition 2. The regret for a learning algorithm A over n rounds is defined as\nRn(A) = M\u0302Vn(A)\u2212 M\u0302Vi\u2217,n. (4)\nGiven this definition, the objective is to design an algorithm whose regret decreases as the number of rounds increases (in high probability or in expectation).\nWe notice that the previous definition actually depends on unobserved samples. In fact, M\u0302Vi\u2217,n is computed on n samples i\u2217 which are not actually observed when running A. This matches the definition of true regret in standard bandits (see e.g., [5]). Thus, in order to clarify the main components characterizing the regret, we introduce additional notation. Let\nYi,t =    Xi\u2217,t if i = i\u2217 Xi\u2217,t\u2032 with t\u2032 = Ti\u2217,n + \u2211\nj<i,j 6=i\u2217 Tj,n + t otherwise\nbe a renaming of the samples from the optimal arm, such that while the algorithm was pulling arm i for the t-th time, Yi,t is the unobserved sample from i\u2217. Then we define the corresponding mean and variance as\n\u00b5\u0303i,Ti,n = 1\nTi,n\nTi,n\u2211\nt=1\nYi,t, \u03c3\u0303 2 i,Ti,n =\n1\nTi,n\nTi,n\u2211\nt=1\n( Yi,t \u2212 \u00b5\u0303i,Ti,n )2 . (5)\nGiven these additional definitions, we can rewrite the regret as (see Appendix A.1)\nRn(A) = 1\nn\n\u2211 i 6=i\u2217 Ti,n [ (\u03c3\u03022i,Ti,n \u2212 \u03c1\u00b5\u0302i,Ti,n)\u2212 (\u03c3\u03032i,Ti,n \u2212 \u03c1\u00b5\u0303i,Ti,n) ]\n+ 1\nn\nK\u2211\ni=1\nTi,n ( \u00b5\u0302i,Ti,n \u2212 \u00b5\u0302n(A) )2 \u2212 1 n K\u2211\ni=1\nTi,n ( \u00b5\u0303i,Ti,n \u2212 \u00b5\u0302i\u2217,n )2 . (6)\nSince the last term is always negative and small 4, our analysis focuses on the first two terms which reveal two interesting characteristics ofA. First, an algorithmA suffers a regret whenever it chooses a suboptimal arm i 6= i\u2217 and the regret corresponds to the difference in the empirical mean\u2013variance of i w.r.t. the optimal arm i\u2217. Such a definition has a strong similarity to the standard definition of regret, where i\u2217 is the arm with highest expected value and the regret depends on the number of times suboptimal arms are pulled and their respective gaps w.r.t. the optimal arm i\u2217. In contrast to the standard formulation of regret, A also suffers an additional regret from the variance \u03c3\u03022n(A), which depends on the variability of pulls Ti,n over different arms. Recalling the definition of the mean\n4More precisely, it can be shown that this term decreases with rateO(K log(1/\u03b4)/n) with probability 1\u2212\u03b4.\n\u00b5\u0302n(A) as the weighted mean of the empirical means \u00b5\u0302i,Ti,n with weights Ti,n/n (see eq. 3), we notice that this second term is a weighted variance of the means and illustrates the exploration risk of the algorithm. In fact, if an algorithm simply selects and pulls a single arm from the beginning, it would not suffer any exploration risk (secondary regret) since \u00b5\u0302n(A) would coincide with \u00b5\u0302i,Ti,n for the chosen arm and all other components would have zero weight. On the other hand, an algorithm accumulates exploration risk through this second term as the mean \u00b5\u0302n(A) deviates from any specific arm; where the maximum exploration risk peaks at the mean \u00b5\u0302n(A) furthest from all arm means. The previous definition of regret can be further elaborated to obtain the upper bound (see App. A.1)\nRn(A) \u2264 1\nn\n\u2211 i 6=i\u2217 Ti,n\u2206\u0302i + 1 n2 K\u2211 i=1 \u2211 j 6=i Ti,nTj,n\u0393\u0302 2 i,j , (7)\nwhere \u2206\u0302i = (\u03c3\u03022i,Ti,n \u2212 \u03c3\u03032i,Ti,n) \u2212 \u03c1(\u00b5\u0302i,Ti,n \u2212 \u00b5\u0303i,Ti,n) and \u0393\u03022i,j = (\u00b5\u0302i,Ti,n \u2212 \u00b5\u0302j,Tj,n)2. Unlike the definition in eq. 6, this upper bound explicitly illustrates the relationship between the regret and the number of pulls Ti,n; suggesting that a bound on the pulls is sufficient to bound the regret.\nFinally, we can also introduce a definition of the pseudo-regret.\nDefinition 3. The pseudo regret for a learning algorithm A over n rounds is defined as\nR\u0303n(A) = 1\nn\n\u2211 i 6=i\u2217 Ti,n\u2206i + 2 n2 K\u2211 i=1 \u2211 j 6=i Ti,nTj,n\u0393 2 i,j , (8)\nwhere \u2206i = MVi \u2212MVi\u2217 and \u0393i,j = \u00b5i \u2212 \u00b5j .\nIn the following, we denote the two components of the pseudo\u2013regret as\nR\u0303\u2206n (A) = 1\nn\n\u2211 i6=i\u2217 Ti,n\u2206i, and R\u0303\u0393n(A) = 2 n2 K\u2211 i=1 \u2211 j 6=i Ti,nTj,n\u0393 2 i,j . (9)\nWhere R\u0303\u2206n (A) constitutes the standard regret derived from the traditional formulation of the multiarm bandit problem and R\u0303\u0393n(A) denotes the exploration risk. This regret can be shown to be close to the true regret up to small terms with high probability.\nLemma 1. Given definitions 2 and 3,\nRn(A) \u2264 R\u0303n(A) + (5 + \u03c1) \u221a 2K log(6nK/\u03b4)\nn + 4 \u221a 2 K log(6nK/\u03b4) n ,\nwith probability at least 1\u2212 \u03b4.\nThe previous lemma shows that any (high\u2013probability) bound on the pseudo\u2013regret immediately translates into a bound on the true regret. Thus, we report most of the theoretical analysis according to R\u0303n(A). Nonetheless, it is interesting to notice the major difference between the true and pseudo\u2013 regret when compared to the standard bandit problem. In fact, it is possible to show in the risk\u2013 averse case that the pseudo\u2013regret is not an unbiased estimator of the true regret, i.e., E[Rn] 6= E[R\u0303n]. Thus, in order to bound the expectation of Rn we build on the high\u2013probability result from Lemma 1."}, {"heading": "3 The Mean\u2013Variance Lower Confidence Bound Algorithm", "text": "In this section we introduce a novel risk\u2013averse bandit algorithm whose objective is to identify the arm which best trades off risk and return. The algorithm is a natural extension of UCB1 [6] and we report a theoretical performance analysis on how well it balances the exploration needed to identify the best arm versus the risk of pulling arms with different means."}, {"heading": "3.1 The Algorithm", "text": "We propose an index\u2013based bandit algorithm which estimates the mean\u2013variance of each arm and selects the optimal arm according to the optimistic confidence\u2013bounds on the current estimates. A sketch of the algorithm is reported in Figure 1. For each arm, the algorithm keeps track of the empirical mean\u2013variance M\u0302Vi,s computed according to s samples. We can build high\u2013probability confidence bounds on empirical mean\u2013variance through an application of the Chernoff\u2013Hoeffding inequality (see e.g., [1] for the bound on the variance) on terms \u00b5\u0302 and \u03c3\u03022.\nLemma 2. Let {Xi,s} be i.i.d. random variables bounded in [0, 1] from the distribution \u03bdi with mean \u00b5i and variance \u03c32i , and the empirical mean \u00b5\u0302i,s and variance \u03c3\u0302 2 i,s computed as in Equation 1, then\nP [ \u2203i = 1, . . . ,K, s = 1, . . . , n, |M\u0302Vi,s \u2212MVi| \u2265 (5 + \u03c1) \u221a log 1/\u03b4\n2s\n] \u2264 6nK\u03b4,\nThe algorithm in Figure 1 implements the principle of optimism in the face of uncertainty used in many multi\u2013arm bandit algorithms. On the basis of the previous confidence bounds, we define a lower\u2013confidence bound on the mean\u2013variance of arm i when it has been pulled s times as\nBi,s = M\u0302Vi,s \u2212 (5 + \u03c1) \u221a log 1/\u03b4\n2s , (10)\nwhere \u03b4 is an input parameter of the algorithm. Given the index of each arm at each round t, the algorithm simply selects the arm with the smallest mean\u2013variance index, i.e., It = arg miniBi,Ti,t\u22121 . We refer to this algorithm as the mean\u2013variance lower\u2013confidence bound (MV-LCB ) algorithm.\nRemark 1. We notice that the algorithm reduces to UCB1 whenever \u03c1\u2192\u221e. This is coherent with the fact that for \u03c1\u2192\u221e the mean\u2013variance problem reduces to the maximization of the cumulative reward, for which UCB1 is already known to be nearly-optimal. On the other hand, for \u03c1 = 0, which leads to the problem of cumulative reward variance minimization, the algorithm plays according to a lower\u2013confidence\u2013bound on the variances.\nRemark 2. The MV-LCB algorithm is parameterized by a parameter \u03b4 which defines the confidence level of the bounds employed in the definition of the index (10). In Theorem 1 we show how to optimize the parameter when the horizon n is known in advance. On the other hand, if n is not known, it is possible to design an anytime version of MV-LCB by defining a non-decreasing exploration sequence (\u03b5t)t instead of the term log 1/\u03b4."}, {"heading": "3.2 Theoretical Analysis", "text": "In this section we report the analysis of the regret Rn(A) of MV-LCB (Fig. 1). As highlighted in eq. 7, it is enough to analyze the number of pulls for each of the arms to recover a bound on the regret. The proofs (reported in the appendix) are mostly based on similar arguments to the proof of UCB.\nWe derive the following regret bound in high probability and expectation.\nTheorem 1. Let the optimal arm i\u2217 be unique and b = 2(5 + \u03c1), the MV-LCB algorithm achieves a pseudo\u2013regret bounded as\nR\u0303n(A) \u2264 b2 log 1/\u03b4\nn\n(\u2211\ni 6=i\u2217\n1\n\u2206i + 4\n\u2211\ni 6=i\u2217\n\u03932i\u2217,i \u22062i + 2b2 log 1/\u03b4 n\n\u2211\ni 6=i\u2217\n\u2211\nj 6=i j 6=i\u2217\n\u03932i,j \u22062i\u2206 2 j\n) + 5K\nn ,\nwith probability at least 1\u2212 6nK\u03b4. Similarly, if MV-LCB is run with \u03b4 = 1/n2 then\nE[R\u0303n(A)] \u2264 2b2 log n\nn\n(\u2211\ni 6=i\u2217\n1\n\u2206i + 4\n\u2211\ni 6=i\u2217\n\u03932i\u2217,i \u22062i + 4b2 log n n\n\u2211\ni 6=i\u2217\n\u2211\nj 6=i j 6=i\u2217\n\u03932i,j \u22062i\u2206 2 j\n) + (17 + 6\u03c1) K\nn .\nRemark 1 (the bound). Let \u2206min = mini 6=i\u2217 \u2206i and \u0393max = maxi |\u0393i|, then a rough simplification of the previous bound leads to\nE[R\u0303n(A)] \u2264 O ( K\n\u2206min\nlog n\nn +K2 \u03932max \u22064min log2 n n\n) .\nFirst we notice that the regret decreases as O(log2 n/n), implying that MV-LCB is a consistent algorithm. As already highlighted in Definition 2, the regret is mainly composed by two terms. The first term is due to the difference in the mean\u2013variance of the best arm and the arms pulled by the algorithm, while the second term denotes the additional variance introduced by the exploration risk of pulling arms with different means. In particular, it is interesting to note that this additional term depends on the squared difference in the means of the arms \u03932i,j . Thus, if all the arms have the same mean, this term would be zero.\nRemark 2 (worst\u2013case analysis). We can further study the result of Theorem 1 by considering the worst\u2013case performance of MV-LCB, that is the performance when the distributions of the arms are chosen so as to maximize the regret. In order to illustrate our argument we consider the simple case of K = 2 arms, \u03c1 = 0 (variance minimization), \u00b51 6= \u00b52, and \u03c321 = \u03c322 = 0 (deterministic arms). 5 In this case we have a variance gap \u2206 = 0 and \u03932 > 0. According to the definition of MV-LCB, the index Bi,s would simply reduce to Bi,s = \u221a log(1/\u03b4)/s, thus forcing the algorithm to pull both arms uniformly (i.e., T1,n = T2,n = n/2 up to rounding effects). Since the arms have the same variance, there is no direct regret in pulling either one or the other. Nonetheless, the algorithm has an additional variance due to the difference in the samples drawn from distributions with different means. In this case, the algorithm suffers a constant (true) regret\nRn(MV-LCB) = 0 + T1,nT2,n n2 \u03932 = 1 4 \u03932,\nindependent from the number of rounds n. This argument can be generalized to multiple arms and \u03c1 6= 0, since it is always possible to design an environment (i.e., a set of distributions) such that \u2206min = 0 and \u0393max 6= 0. 6 This result is not surprising. In fact, two arms with the same mean\u2013 variance are likely to produce similar observations, thus leading MV-LCB to pull the two arms repeatedly over time, since the algorithm is designed to try to discriminate between similar arms. Although this behavior does not suffer from any regret in pulling the \u201csuboptimal\u201d arm (the two arms are equivalent), it does introduce an additional variance, due to the difference in the means of the arms (\u0393 6= 0), which finally leads to a regret the algorithm is not \u201caware\u201d of. This argument suggests that, for any n, it is always possible to design an environment for which MV-LCB has a constant regret. This is particularly interesting since it reveals a huge gap between the mean\u2013variance problem and the standard expected regret minimization problem and will be further investigated in the numerical simulations presented in Section 5. In fact, in the latter case, UCB is known to have a worst\u2013case regret per round of \u2126(1/ \u221a n) [3], while in the worst case, MV-LCB suffers a constant regret. In the next section we introduce a simple algorithm able to deal with this problem and achieve a vanishing worst\u2013case regret.\n5Note that in this case (i.e., \u2206 = 0), Theorem 1 does not hold, since the optimal arm is not unique. 6Notice that this is always possible for a large majority of distributions for which the mean and variance are\nindependent or mildly correlated."}, {"heading": "4 The Exploration\u2013Exploitation Algorithm", "text": "The ExpExp algorithm divides the time horizon n into two distinct phases of length \u03c4 and n \u2212 \u03c4 respectively. During the first phase all the arms are explored uniformly, thus collecting \u03c4/K samples each 7. Once the exploration phase is over, the mean\u2013variance of each arm is computed and the arm with the smallest estimated mean\u2013variance MVi,\u03c4/K is repeatedly pulled until the end.\nThe MV-LCB is specifically designed to minimize the probability of pulling the wrong arms, so whenever there are two equivalent arms (i.e., arms with the same mean\u2013variance), the algorithm tends to pull them the same number of times, at the cost of potentially introducing an additional variance which might result in a constant regret. On the other hand, ExpExp stops exploring the arms after \u03c4 rounds and then elicits one arm as the best and keeps pulling it for the remaining n\u2212 \u03c4 rounds. Intuitively, the parameter \u03c4 should be tuned so as to meet different requirements. The first part of the regret (i.e., the regret coming from pulling the suboptimal arms) suggests that the exploration phase \u03c4 should be long enough for the algorithm to select the empirically best arm i\u0302\u2217 at \u03c4 equivalent to the actual optimal arm i\u2217 with high probability; and at the same time, as short as possible to reduce the number of times the suboptimal arms are explored. On the other hand, the second part of the regret (i.e., the variance of pulling arms with different means) is minimized by taking \u03c4 as small as possible (e.g., \u03c4 = 0 would guarantee a zero regret). The following theorem illustrates the optimal trade-off between these contrasting needs.\nTheorem 2. Let ExpExp be run with \u03c4 = K(n/14)2/3, then for any choice of distributions {\u03bdi} the expected regret is E[R\u0303n(A)] \u2264 2 Kn1/3 .\nRemark 1 (the bound). We first notice that this bound suggests that ExpExp performs worse than MV-LCB on easy problems. In fact, Theorem 1 demonstrates that MV-LCB has a regret decreasing as O(K log(n)/n) whenever the gaps \u2206 are not small compared to n, while in the remarks of Theorem 1 we highlighted the fact that for any value of n, it is always possible to design an environment which leads MV-LCB to suffer a constant regret. On the other hand, the previous bound for ExpExp is distribution independent and indicates the regret is still a decreasing function of n even in the worst case. This opens the question whether it is possible to design an algorithm which works as well as MV-LCB on easy problems and as robustly as ExpExp on difficult problems.\nRemark 2 (exploration phase). The previous result can be improved by changing the exploration strategy used in the first \u03c4 rounds. Instead of a pure uniform exploration of all the arms, we could adopt a best\u2013arm identification algorithms such as Successive Reject or UCB-E, which maximize the probability of returning the best arm given a fixed budget of rounds \u03c4 (see e.g., [4])."}, {"heading": "5 Numerical Simulations", "text": "In this section we report numerical simulations aimed at validating the main theoretical findings reported in the previous sections. In the following graphs we study the true regret Rn(A) averaged over 500 runs. We first consider the variance minimization problem (\u03c1 = 0) with K = 2 Gaussian\n7In the definition and in the following analysis we ignore rounding effects.\narms set to \u00b51 = 1.0, \u00b52 = 0.5, \u03c321 = 0.05, and \u03c3 2 2 = 0.25 and run MV-LCB 8. In Figure 2 we report the true regret Rn (as in the original definition in eq. 4) and its two components R\u2206\u0302n and R\u0393\u0302n (these two values are defined as in eq. 9 with \u2206\u0302 and \u0393\u0302 replacing \u2206 and \u0393). As expected (see e.g., Theorem 1), the regret is characterized by the regret realized from pulling suboptimal arms and arms with different means (Exploration Risk) and tends to zero as n increases. Indeed, if we considered two distributions with equal means (\u00b51 = \u00b52), the average regret coincides with R\u2206\u0302n . Furthermore, as shown in Theorem 1 the two regret terms decrease with the same rate O(log n/n).\nA detailed analysis of the impact of \u2206 and \u0393 on the performance of MV-LCB is reported in Appendix D. Here we only compare the worst\u2013case performance of MV-LCB to ExpExp (see Figure 2). In order to have a fair comparison, for any value of n and for each of the two algorithms, we select the pair \u2206w,\u0393w which corresponds to the largest regret (we search in a grid of values with \u00b51 = 1.5, \u00b52 \u2208 [0.4; 1.5], \u03c321 \u2208 [0.0; 0.25], and \u03c322 = 0.25, so that \u2206 \u2208 [0.0; 0.25] and \u0393 \u2208 [0.0; 1.1]). As discussed in Section 4, while the worst\u2013case regret of ExpExp keeps decreasing over n, it is always possible to find a problem for which regret of MV-LCB stabilizes to a constant. For numerical results with multiple values of \u03c1 and 15 arms, please see Appendix D."}, {"heading": "6 Discussion", "text": "In this paper we evaluate the risk of an algorithm in terms of the variability of the sequences of samples that it actually generates. Although this notion might resemble other analyses of UCBbased algorithms (see e.g., the high-probability analysis in [5]), it captures different features of the learning algorithm. Whenever a bandit algorithm is run over n rounds, its behavior, combined with the arms\u2019 distributions, generates a probability distribution over sequences of n rewards. While the quality of this sequence is usually defined by its cumulative sum (or average), here we say that a sequence of rewards is good if it displays a good trade-off between its (empirical) mean and variance. It is important to notice that this notion of risk-return tradeoff does not coincide with the variance of the algorithm over multiple runs.\nLet us consider a simple case with two arms that deterministically generate 0s and 1s respectively, and two different algorithms. Algorithm A1 pulls the arms in a fixed sequence at each run (e.g., arm 1, arm 2, arm 1, arm 2, and so on), so that each arm is always pulled n/2 times. Algorithm A2 chooses one arm uniformly at random at the beginning of the run and repeatedly pulls this arm for n rounds. Algorithm A1 generates sequences such as 010101... which have high variability within each run, incurs a high regret (e.g., if \u03c1 = 0), but has no variance over multiple runs because it always generates the same sequence. On the other hand,A2 has no variability in each run, since it generates sequences with only 0s or only 1s, suffers no regret in the case of variance minimization, but has high variance over multiple runs since the two completely different sequences are generated with equal probability. This simple example demonstrates that an algorithm with a very small standard regret w.r.t. the cumulative reward (e.g., A1), might result in a very high variability in a single run of the algorithm, while an algorithm with small mean-variance regret (e.g., A2) could have a high variance over multiple runs."}, {"heading": "7 Conclusions", "text": "The majority of multi\u2013armed bandit literature focuses on the problem of minimizing the regret w.r.t. the arm with the highest return in expectation. We study the notion of risk associated to the variance over multiple runs and risk of variability associated to a single run of an algorithm. The later case highlights an interesting effect on the regret due to the need to estimate variability within a single sequence of finite random samples before making a risk-averse decision. Further, controling the variance risk over multiple runs does not necessarily control the risk of variability over a single run. In this paper, we introduced a novel multi\u2013armed bandit setting where the objective is to perform as well as the arm with the best risk\u2013return trade\u2013off. In particular, we relied on the mean\u2013variance model introduced in [10] to measure the performance of the arms and define the regret of a learning algorithm. We proposed two novel algorithms to solve the mean\u2013variance bandit problem and we reported their corresponding theoretical analysis. While MV-LCB shows a small regret of order O(log n/n) on \u201ceasy\u201d problems (i.e., where the mean\u2013variance gaps \u2206 are big w.r.t. n), we showed that it has a constant worst\u2013case regret. On the other hand, we proved that ExpExp has a vanishing\n8Notice that although in the paper we assumed the distributions to be bounded in [0, 1] all the results can be extended to sub-Gaussian distributions.\nworst\u2013case regret at the cost of worse performance on \u201ceasy\u201d problems. To the best of our knowledge this is the first work introducing risk\u2013aversion in the multi\u2013armed bandit setting and it opens a series of interesting questions.\nLower bound. In this paper we introduced two algorithms, MV-LCB and ExpExp. As discussed in the remarks of Theorem 1 and Theorem 2, MV-LCB has a regret of order O( \u221a K/n) on easy prob-\nlems and O(1) on difficult problems, while ExpExp achieves the same regret O(K/n1/3) over all problems. The primary open question is whetherO(K/n1/3) is actually the best possible achievable rate (in the worst\u2013case) for this problem or a better rate is possible. This question is of particular interest since the standard reward expectation maximization problem has a known lower\u2013bound of \u2126( \u221a\n1/n), and a minimax rate of \u2126(1/n1/3) for the mean\u2013variance problem would imply that the risk\u2013averse bandit problem is intrinsically more difficult than standard bandit problems.\nDifferent measures of return\u2013risk. Considering alternative notions of risk is a straightforward extension to the previous setting. In fact, over the years the mean\u2013variance model has often been criticized. From a point of view of the expected utility theory, the mean\u2013variance model is only justified under a Gaussianity assumption on the arm distributions. It also violates the monotonocity condition due to the different orders of the mean and variance and is not a coherent measure of risk [2]. Furthermore, the variance is a symmetric measure of risk, while it is often the case that only one\u2013sided deviations from the mean are undesirable (e.g., in finance only losses w.r.t. to the expected return are considered as a risk, while any positive deviation is not considered as a real risk). A popular replacement for the mean\u2013variance is to use the \u03b1 value\u2013at\u2013risk (i.e., the quantile) to measure the risk of a random variable. The main challenge in this case is the estimation of the value\u2013at\u2013risk for each arm. In fact, while the cumulative distribution of a random variable can be reliably estimated (see e.g., [11]), estimating the quantile might be more difficult.\nIn [2] axiomatic rules are listed to define coherent measures of risk. Though \u03b1 value\u2013at\u2013risk violates these rules, Conditional Value at Risk (otherwise known as average value at risk, tail value at risk, expected shortfall and lower tail risk) passes these rules as a coherent measure of risk. One can easily imagine a lower confidence bound algorithm based on [7] in the same composition as MVLCB which replaces the variance by the conditional value at risk.\nThe notion of optimality in the risk sensitive setting also depends on the selection of a single-period or multi-period risk evaluation. While the single-period risk of an arm is simply the risk of its distribution, in a multi-period evaluation we consider the risk of the sum of rewards obtained by repeatedly pulling the same arm over n rounds. Unlike the variance, for which the variance of a sum of n independent realizations of the same random variable is simply n times its variance, for other measures of risk (e.g., \u03b1 value\u2013at\u2013risk) this is not necessarily the case. As a result, an arm with the smallest single-period risk might not be the optimal choice over an horizon of n rounds. Therefore, the performance of a learning algorithm should be compared to the smallest risk that can be achieved by any sequence of arms over n rounds, thus requiring a new definition of regret.\nLinear bandits. In linear bandits, each arm is characterized by a marginal distribution with expected value \u00b5i and a covariance matrix C. At each step the learner chooses a combination of arms and observes the corresponding combined reward. In this case, the best combination is obtained by solving the mean\u2013variance quadratic program minx(x>Cx\u2212 \u03c1x>\u00b5) where x is usually a point in the K-dimensional simplex (e.g., in finance x is in the simplex when no short\u2013selling is allowed). Similar to the multi\u2013arm case, the objective is to define an algorithm able to achieve a mean\u2013variance as small as the best point in the simplex over n rounds.\nSimple regret. Finally, an interesting related problem is the simple regret setting where the learner is allowed to explore over n rounds and it only suffers a regret defined on the solution returned at the end. It is known that it is possible to design algorithm able to effectively estimate the mean of the arms and finally return the best arm with high probability. In the risk-return setting, the objective would be to return the arm with the best risk-return tradeoff.\nAcknowledgments This work was supported by Ministry of Higher Education and Research, NordPas de Calais Regional Council and FEDER through the \u201ccontrat de projets \u00e9tat region 2007\u20132013\", French National Research Agency (ANR) under project LAMPADA n\u25e6 ANR-09-EMER-007, European Community\u2019s Seventh Framework Programme (FP7/2007-2013) under grant agreement n\u25e6 270327, and PASCAL2 European Network of Excellence."}, {"heading": "A The Regret", "text": "A.1 The True Regret\nWe recall the definition of the (empirical) regret as\nRn(A) = M\u0302Vn(A)\u2212 M\u0302Vi\u2217,n.\nGiven the definitions reported in the main paper, we first elaborate on the two mean terms in the regret as\n\u00b5\u0302i\u2217,n = 1\nn\nK\u2211\ni=1\nTi,n\u2211\nt=1\nYi,t = 1\nn\nK\u2211\ni=1\nTi,n\u00b5\u0303i,Ti,n ,\nand\n\u00b5\u0302n(A) = 1\nn\nK\u2211\ni=1\nTi,n\u2211\nt=1\nXi,t = 1\nn\nK\u2211\ni=1\nTi,n\u00b5\u0302i,Ti,n .\nSimilarly, the two variance terms can be written as\n\u03c3\u03022n(A) = 1\nn\nK\u2211\ni=1\nTi,n\u2211\nt=1\n( Xi,t \u2212 \u00b5\u0302n(A) )2\n= 1\nn\nK\u2211\ni=1\nTi,n\u2211\nt=1\n( Xi,t \u2212 \u00b5\u0302i,Ti,n )2 + 1\nn\nK\u2211\ni=1\nTi,n\u2211\nt=1\n( \u00b5\u0302i,Ti,n \u2212 \u00b5\u0302n(A) )2 + 2\nn\nK\u2211\ni=1\nTi,n\u2211\nt=1\n( Xi,t \u2212 \u00b5\u0302i,Ti,n )( \u00b5\u0302i,Ti,n \u2212 \u00b5\u0302n(A) )\n= 1\nn\nK\u2211\ni=1\nTi,n\u03c3\u0302 2 i,Ti,n +\n1\nn\nK\u2211\ni=1\nTi,n ( \u00b5\u0302i,Ti,n \u2212 \u00b5\u0302n(A) )2 + 0,\nand\n\u03c32i\u2217,n = 1\nn\nK\u2211\ni=1\nTi,n\u2211\nt=1\n( Yi,t \u2212 \u00b5\u0302i\u2217,n )2\n= 1\nn\nK\u2211\ni=1\nTi,n\u2211\nt=1\n( Yi,t \u2212 \u00b5\u0303i,Ti,n )2 + 1\nn\nK\u2211\ni=1\nTi,n\u2211\nt=1\n( \u00b5\u0303i,Ti,n \u2212 \u00b5\u0302i\u2217,n )2 + 2\nn\nK\u2211\ni=1\nTi,n\u2211\nt=1\n( Yi,t \u2212 \u00b5\u0303i,Ti,n )( \u00b5\u0303i,Ti,n \u2212 \u00b5\u0302i\u2217,n )\n= 1\nn\nK\u2211\ni=1\nTi,t\u03c3\u0303 2 i,Ti,n +\n1\nn\nK\u2211\ni=1\nTi,n ( \u00b5\u0303i,Ti,n \u2212 \u00b5\u0302i\u2217,n )2 + 0.\nPutting together these terms, we obtain the regret (see eq. 4)\nRn(A) = 1\nn\n\u2211 i6=i\u2217 Ti,n [ (\u03c3\u03022i,Ti,n \u2212 \u03c3\u03032i,Ti,n)\u2212 \u03c1(\u00b5\u0302i,Ti,n \u2212 \u00b5\u0303i,Ti,n) ]\n+ 1\nn\nK\u2211\ni=1\nTi,n ( \u00b5\u0302i,Ti,n \u2212 \u00b5\u0302n(A) )2 \u2212 1 n K\u2211\ni=1\nTi,n ( \u00b5\u0303i,Ti,n \u2212 \u00b5\u0302i\u2217,n )2\nIf we further elaborate the second term, we obtain\n1\nn\nK\u2211\ni=1\nTi,n ( \u00b5\u0302i,Ti,n \u2212 \u00b5\u0302n(A) )2 = 1\nn\nK\u2211\ni=1\nTi,n ( \u00b5\u0302i,Ti,n \u2212 1\nn\nK\u2211\nj=1\nTj,n\u00b5\u0302j,Tj,n\n)2\n= 1\nn\nK\u2211\ni=1\nTi,n\n( K\u2211\nj=1\nTj,n n\n(\u00b5\u0302i,Ti,n \u2212 \u00b5\u0302j,Tj,n) )2\n\u2264 1 n\nK\u2211\ni=1\nTi,n\nK\u2211\nj=1\nTj,n n (\u00b5\u0302i,Ti,n \u2212 \u00b5\u0302j,Tj,n)2\n= 1\nn2\nK\u2211\ni=1\n\u2211 j 6=i Ti,nTj,n(\u00b5\u0302i,Ti,n \u2212 \u00b5\u0302j,Tj,n)2.\nUsing the definitions \u2206\u0302i = (\u03c3\u03022i,Ti,n \u2212 \u03c3\u03032i,Ti,n)\u2212 \u03c1(\u00b5\u0302i,Ti,n \u2212 \u00b5\u0303i,Ti,n) and \u0393\u03022i,j = (\u00b5\u0302i,Ti,n \u2212 \u00b5\u0302j,Tj,n)2 we finally obtain an upper\u2013bound on the regret of the form\nRn(A) \u2264 1\nn\n\u2211 i 6=i\u2217 Ti,n\u2206\u0302i + 1 n2 K\u2211 i=1 \u2211 j 6=i Ti,nTj,n\u0393\u0302 2 i,j .\nIn the following we refer to the two terms asR\u2206\u0302n andR\u0393\u0302n.\nA.2 The Pseudo\u2013Regret\nSimilar to what is done in the standard bandit problem, we can introduce a different notion of regret. Starting from the last equation in the previous section, we define the pseudo\u2013regret\nR\u0303n(A) = 1\nn\n\u2211 i 6=i\u2217 Ti,n\u2206i + 2 n2 K\u2211 i=1 \u2211 j 6=i Ti,nTj,n\u0393 2 i,j ,\nwhere the empirical values \u2206\u0302i and \u0393\u0302i,j are substituted by their corresponding exact values 9. In the following we show that the true and pseudo regrets different for values that tend to zero with high probability.\nProof. (Lemma 1)\nWe define a high\u2013probability event in which the empirical values and the true values only differ for small quantities E = { \u2200i = 1, . . . ,K, \u2200s = 1, . . . , n, \u2223\u2223\u00b5\u0302i,s \u2212 \u00b5i \u2223\u2223 \u2264 \u221a log 1/\u03b4\n2s and\n\u2223\u2223\u03c3\u03022i,s \u2212 \u03c32i \u2223\u2223 \u2264 5\n\u221a log 1/\u03b4\n2s\n} .\nUsing Chernoff\u2013Hoeffding inequality and a union bound over arms and rounds, we have that P[EC ] \u2264 6nK\u03b4. Under this event we rewrite the empirical \u2206\u0302i as\n\u2206\u0302i = \u2206i \u2212 (\u03c32i \u2212 \u03c32i\u2217) + \u03c1(\u00b5i \u2212 \u00b5i\u2217) + (\u03c3\u03022i,Ti,n \u2212 \u03c3\u03032i,Ti,n)\u2212 \u03c1(\u00b5\u0302i,Ti,n \u2212 \u00b5\u0303i,Ti,n)\n\u2264 \u2206i + 2(5 + \u03c1) \u221a log 1/\u03b4\n2Ti,n .\nSimilarly, \u0393\u0302i,j is upper\u2013bounded as\n|\u0393\u0302i,j | = |\u0393i,j \u2212 \u00b5i + \u00b5j + \u00b5\u0302i,Ti,n \u2212 \u00b5\u0302j,Tj,n |\n\u2264 |\u0393i,j |+ \u221a log 1/\u03b4\n2Ti,n +\n\u221a log 1/\u03b4\n2Tj,n .\n9Notice that the factor 2 in front of the second term is due to a rough upper bounding used in the proof of Lemma 1.\nThus the regret can be written as\nRn(A) \u2264 1\nn \u2211 i 6=i\u2217 Ti,n ( \u2206i + 2(5 + \u03c1)\n\u221a log 1/\u03b4\n2Ti,n\n) + 1\nn2 K\u2211 i=1 \u2211 j 6=i Ti,nTj,n ( |\u0393i,j |+\n\u221a log 1/\u03b4\n2Ti,n +\n\u221a log 1/\u03b4\n2Tj,n )2 \u2264 1 n \u2211 i 6=i\u2217 Ti,n\u2206i + 5 + \u03c1 n \u2211 i 6=i\u2217 \u221a 2Ti,n log 1/\u03b4 + 2 n2 K\u2211 i=1 \u2211 j 6=i Ti,nTj,n\u0393 2 i,j\n+ 2 \u221a 2\nn2 K\u2211 i=1 \u2211 j 6=i Tj,n log 1/\u03b4 + 2 \u221a 2 n2 K\u2211 i=1 \u2211 j 6=i Ti,n log 1/\u03b4\n\u2264 1 n \u2211 i 6=i\u2217 Ti,n\u2206i + 2 n2 K\u2211 i=1 \u2211 j 6=i Ti,nTj,n\u0393 2 i,j + (5 + \u03c1)\n\u221a 2K log 1/\u03b4\nn + 4 \u221a 2 K log 1/\u03b4 n .\nwhere in the next to last passage we used Jensen\u2019s inequality for concave functions and rough upper bounds on other terms (K \u2212 1 < K, \u2211i 6=i\u2217 Ti,n \u2264 n). By recalling the definition of R\u0303n(A) we finally obtain\nRn(A) \u2264 R\u0303n(A) + (5 + \u03c1) \u221a 2K log 1/\u03b4\nn + 4 \u221a 2 K log 1/\u03b4 n ,\nwith probability 1\u22126nK\u03b4. Thus we can conclude that any upper bound on the pseudo\u2013regret R\u0303n(A) is a valid upper bound for the true regretRn(A), up to a decreasing term of order O( \u221a K/n).\nB MV-LCB Theoretical Analysis\nIn order to simplify the notation in the following we use b = 2(5 + \u03c1).\nProof. (Theorem 1)\nWe begin by defining a high\u2013probability event E as\nE = { \u2200i = 1, . . . ,K, \u2200s = 1, . . . , n, \u2223\u2223\u00b5\u0302i,s \u2212 \u00b5i \u2223\u2223 \u2264 \u221a log 1/\u03b4\n2s and\n\u2223\u2223\u03c3\u03022i,s \u2212 \u03c32i \u2223\u2223 \u2264 5\n\u221a log 1/\u03b4\n2s\n} .\nUsing Chernoff\u2013Hoeffding inequality and a union bound over arms and rounds, we have that P[EC ] \u2264 6nK\u03b4. We now introduce the definition of the algorithm. Consider any time t when arm i 6= i\u2217 is pulled (i.e., It = i). By definition of the algorithm in Figure 1, i is selected if its corresponding index Bi,Ti,t\u22121 is bigger than for any other arm, notably the best arm i\n\u2217. By recalling the definition of the index and the empirical mean\u2013variance at time t, we have\n\u03c3\u03022i,Ti,t\u22121 \u2212 \u03c1\u00b5\u0302i,Ti,t\u22121 \u2212 (5 + \u03c1) \u221a log 1/\u03b4\n2Ti,t\u22121 = Bi,Ti,t\u22121 \u2264\n\u2264 Bi\u2217,Ti\u2217,t\u22121 = \u03c3\u03022i\u2217,Ti\u2217,t\u22121 \u2212 \u03c1\u00b5\u0302i\u2217,Ti\u2217,t\u22121 \u2212 (5 + \u03c1) \u221a log 1/\u03b4\n2Ti\u2217,t\u22121 .\nOver all the possible realizations, we now focus on the realizations in E . In this case, we can rewrite the previous condition as\n\u03c32i \u2212 \u03c1\u00b5i \u2212 2(5 + \u03c1) \u221a log 1/\u03b4\n2Ti,t\u22121 \u2264 Bi,Ti,t\u22121 \u2264 Bi\u2217,Ti\u2217,t\u22121 \u2264 \u03c32i\u2217 \u2212 \u03c1\u00b5i\u2217 .\nLet time t be the last time when arm i is pulled until the final round n, then Ti,t\u22121 = Ti,n \u2212 1 and\nTi,n \u2264 2(5 + \u03c1)2\n\u22062i log\n1 \u03b4 + 1,\nwhich suggests that the suboptimal arms are pulled only few times with high probability. Plugging the bound in the regret in eq. 8 leads to the final statement\nR\u0303n(A) \u2264 1\nn\n\u2211\ni 6=i\u2217\nb2 log 1/\u03b4\n\u2206i +\n1\nn\n\u2211\ni 6=i\u2217\n4b2 log 1/\u03b4\n\u22062i \u03932i\u2217,i +\n1\nn2\n\u2211\ni 6=i\u2217\n\u2211\nj 6=i j 6=i\u2217\n2b4(log 1/\u03b4)2\n\u22062i\u2206 2 j\n\u03932i,j + 5K\nn ,\nwith probability 1\u2212 6nK\u03b4. We now move from the previous high\u2013probability bound to a bound in expectation. The pseudo\u2013 regret is (roughly) bounded as R\u0303n(A) \u2264 2 + \u03c1 (by bounding \u2206i \u2264 1 + \u03c1 and \u0393 \u2264 1), thus\nE[R\u0303n(A)] = E [ R\u0303n(A)I{E} ] + E [ R\u0303n(A)I{EC} ] \u2264 E [ R\u0303n(A)I{E} ] + (2 + \u03c1)P [ EC ] .\nBy using the previous high\u2013probability bound and recalling that P[EC ] \u2264 6nK\u03b4, we have\nE[R\u0303n(A)] \u2264 1\nn\n\u2211\ni 6=i\u2217\nb2 log 1/\u03b4\n\u2206i +\n1\nn\n\u2211\ni 6=i\u2217\n4b2 log 1/\u03b4\n\u22062i \u03932i\u2217,i +\n1\nn2\n\u2211\ni 6=i\u2217\n\u2211\nj 6=i j 6=i\u2217\n2b4(log 1/\u03b4)2\n\u22062i\u2206 2 j\n\u03932i,j\n+ 5K\nn + (2 + \u03c1)6nK\u03b4.\nThe final statement of the lemma follows by tuning the parameter \u03b4 = 1/n2 so as to have a regret bound decreasing with n.\nWhile a high\u2013probability bound forRn can be immediately obtained from Lemma 1, the expectation ofRn is reported in the next corollary.\nProof. Since the mean\u2013variance\u2212\u03c1 \u2264 M\u0302V \u2264 1/4, the regret is bounded by\u22121/4\u2212\u03c1 \u2264 Rn(A) \u2264 1/4 + \u03c1. Thus we have\nE[Rn(A)] =\u2264 uP[Rn(A) \u2264 u] + (1 4 + \u03c1 ) P[Rn(A) > u].\nBy taking u equal to the previous high\u2013probability bound and recalling that P[EC ] \u2264 6nK\u03b4, we have\nE[Rn(A)] \u2264 1\nn\n\u2211\ni 6=i\u2217\nb2 log 1/\u03b4\n\u2206i +\n1\nn\n\u2211\ni 6=i\u2217\n4b2 log 1/\u03b4\n\u22062i \u03932i\u2217,i +\n1\nn2\n\u2211\ni 6=i\u2217\n\u2211\nj 6=i j 6=i\u2217\n2b4(log 1/\u03b4)2\n\u22062i\u2206 2 j\n\u03932i,j\n+ 5K\nn + b\n\u221a K log 1/\u03b4\n2n + 4 \u221a 2 K log 1/\u03b4 n + (1 4 + \u03c1 ) 6nK\u03b4.\nThe final statement of the lemma follows by tuning the parameter \u03b4 = 1/n2 so as to have a regret bound decreasing with n."}, {"heading": "C Exp\u2013Exp Theoretical Analysis", "text": "During the exploitation phase the algorithm pulls arm i\u0302\u2217 with the smallest empirical variance estimated during the exploration phase of length \u03c4 . As a result, the number of pulls of each arm is\nTi,n = \u03c4 K + (n\u2212 \u03c4)I{i = i\u0302\u2217} (11)\nWe analyze the two terms of the regret separately.\nR\u0303\u2206n = 1\nn\n\u2211\ni 6=i\u2217\n( \u03c4 K + (n\u2212 \u03c4)I{i = i\u0302\u2217} ) \u2206i = \u03c4 nK \u2211\ni 6=i\u2217 \u2206i +\nn\u2212 \u03c4 n\n\u2211 i 6=i\u2217 \u2206iI{i = i\u0302\u2217}\ufe38 \ufe37\ufe37 \ufe38\n(a)\n.\nWe notice that the only random variable in this formulation is the best arm i\u0302\u2217 at the end of the exploration phase. We thus compute the expected value of R\u0303\u2206n .\nE[(a)] = P[i = i\u0302\u2217]\u2206i = P[\u2200j 6= i, \u03c3\u03022i,\u03c4/K \u2264 \u03c3\u03022j,\u03c4/K ]\u2206i \u2264 P[\u03c3\u03022i,\u03c4/K \u2264 \u03c3\u03022i\u2217,\u03c4/K ]\u2206i = P [ (\u03c3\u03022i,\u03c4/K \u2212 \u03c32i ) + (\u03c32i\u2217 \u2212 \u03c3\u03022i\u2217,\u03c4/K) \u2264 \u2206i ] \u2206i\n\u2264 2\u2206i exp ( \u2212 \u03c4 K \u22062i )\nThe second term in the regret can be bounded as follows.\nR\u0303\u0393n = 1\nn2\nK\u2211\ni=1\n\u2211\nj 6=i\n( \u03c4 K + (n\u2212 \u03c4)I{i = i\u0302\u2217} )( \u03c4 K + (n\u2212 \u03c4)I{j = i\u0302\u2217} ) \u03932i,j\n= 1\nn2\nK\u2211\ni=1\n\u2211\nj 6=i\n( \u03c42 K2 + (n\u2212 \u03c4)2I{i = i\u0302\u2217}I{j = i\u0302\u2217}+ \u03c4 K (n\u2212 \u03c4)I{j = i\u0302\u2217}+ \u03c4 K (n\u2212 \u03c4)I{i = i\u0302\u2217} ) \u03932i,j\n= \u03c42\nn2K2\nK\u2211\ni=1\n\u2211 j 6=i \u03932i,j + 2 (n\u2212 \u03c4)\u03c4 Kn2 K\u2211 i=1 \u2211 j 6=i \u03932i,jI{i = i\u0302\u2217}\n\u2264 \u03c4 n2 + 2 (n\u2212 \u03c4)\u03c4 n2 \u2264 2 \u03c4 n\nGrouping all the terms, ExpExp has an expected regret bounded as\nE[R\u0303n(A)] \u2264 2 \u03c4\nn + 2\n\u2211 i 6=i\u2217 \u2206i exp ( \u2212 \u03c4 K \u22062i )\nWe can now move to the worst\u2013case analysis of the regret. Let f(\u2206i) = \u2206i exp ( \u2212 \u03c4K\u22062i ) , the \u201cadversarial\u201d choice of the gap is determined by maximizing the regret, which corresponds to\nf \u2032(\u2206i) = exp ( \u2212 \u03c4 K \u22062i ) + \u2206i\n( \u2212 2 \u03c4\nK \u2206i exp ( \u2212 \u03c4 K \u22062i\n))\n= (\n1\u2212 2 \u03c4 K \u22062i\n) exp ( \u2212 \u03c4 K \u22062i ) = 0,\nand leads to a worst\u2013case choice for the gap of\n\u2206i =\n\u221a K\n2\u03c4 .\nThe worst\u2013case regret is then\nE[R\u0303n(A)] \u2264 2 \u03c4\nn + (K \u2212 1)\n\u221a 2K\n1\u221a \u03c4 exp(\u22120.5) \u2264 2 \u03c4 n +K3/2 1\u221a \u03c4\nWe can now choose the parameter \u03c4 minimizing the worst\u2013case regret. Taking the derivative of the regret w.r.t. \u03c4 we obtain\ndE[R\u0303n(A)] d\u03c4 = 2 n \u2212 1 2 (K \u03c4 )3/2 = 0,\nthus leading to the optimal parameter \u03c4 = (n/4)2/3K. The final regret is thus bounded as\nE[R\u0303n(A)] \u2264 3 K\nn1/3 ."}, {"heading": "D Additional Simulations", "text": "D.1 Comparison between MV-LCB and ExpExp with K = 2\nWe consider the variance minimization problem (\u03c1 = 0) with K = 2 Gaussian arms with different means and variances. In particular, we consider a grid of values with \u00b51 = 1.5, \u00b52 \u2208 [0.4; 1.5], \u03c321 \u2208 [0.0; 0.25], and \u03c322 = 0.25, so that \u2206 \u2208 [0.0; 0.25] and \u0393 \u2208 [0.0; 1.1] and number of rounds n \u2208 [50; 2.5\u00d7 105]. Figures 3 and 4 report the mean regret for different values of n. The colors are renormalized in each plot so that dark blue corresponds to the smallest regret and red to the largest regret. The results confirm the theoretical findings of Theorem 1 and 2. In fact, for simple problems (large gaps \u2206) MV-LCB converges to a zero\u2013regret faster than ExpExp, while for \u2206 close to zero (i.e., equivalent arms), MV-LCB has a constant regret which does not decrease with n and the regret of ExpExp slowly decreases to zero.\nD.2 Risk tolerance sensitivity\nIn Section 5 we report numerical results demonstrating the composition of the regret and performance of algorithms with only 2 arms in the case of variance minimization. Here we report results for a wide range of risk tolerance \u03c1 \u2208 [0.0; 10.0] and K = 15 arms. We set the mean and variance for each of the 15 arms so that a subset of arms is always dominated (i.e., for any \u03c1, MV\u03c1i > MV \u03c1 i\u2217\u03c1\n) demonstrating the effect of different \u03c1 values on the position of the optimal arm i\u2217\u03c1.\nIn Figure 2 we arranged the true values of each arm along the red fronteir and the \u03c1-directed performance of the algorithms in a standard deviation\u2013mean plot. The green and blue lines show the standard deviation and mean for the performance of each algorithm for a specific \u03c1 setting and fi-\nnite time n, where each point represents the resulting mean\u2013standard deviation of the sequence of pulls on the arms by the algorithm with that specific value of \u03c1. The gap between the \u03c1 specific performance of the algorithm and the corresponding optimal arm along the red frontier represents the regret for the specific \u03c1 value. Accordingly, the gap between the algorithm performance curves represents the gap in performance with regard to MV-LCB versus ExpExp. Where a lot of arms have big gaps (e.g., all the dominated arms have a large gap for any value of \u03c1), MV-LCB tends to perform better than ExpExp. The series of plots represent increasing values of n and demonstrate the relative algorithm performance versus the optimal red frontier. The set of plots represent the two settings reported in Figure 5. We chose the values of the arms so as to have configurations with different complexities. In particular, configuration 1 corresponds to \u201ceasy\u201d problems for MV-LCB since the arms all have quite large gaps (for different values of \u03c1) and this should allow it to perform well. On the other hand, the second configuration has much smaller gaps and, thus, higher complexity. According to the bounds for MV-LCB we know that a good proxy for its learning complexity is represented by the term \u2211 i 1/\u2206 2 i,\u03c1. In Figure 5 we report such complexity for different values of \u03c1 and, as it can be noticed, configuration 2 has always a higher complexity than configuration 1.\nAs we notice, in both configurations the performance of MV-LCB and ExpExp approach one of the optimal arms i\u2217\u03c1 for each specific \u03c1 as n increases. Nonetheless, in configuration 1 the large number of suboptimal arms (e.g., arms with large gaps) allows MV-LCB to outperform ExpExp and converge faster to the optimal arm (and thus zero regret). On the other hand, in configuration 2 there are more arms with similar performance and for some values of \u03c1 ExpExp eventually achieves better performance than MV-LCB."}], "references": [{"title": "Active learning in heteroscedastic noise", "author": ["Andr\u00e1s Antos", "Varun Grover", "Csaba Szepesv\u00e1ri"], "venue": "Theoretical Computer Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Coherent measures of risk", "author": ["P Artzner", "F Delbaen", "JM Eber", "D Heath"], "venue": "Mathematical finance,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Regret bounds and minimax policies under partial monitoring", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Best arm identification in multiarmed bandits", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck", "R\u00e9mi Munos"], "venue": "In Proceedings of the Twenty-third Conference on Learning Theory (COLT\u201910),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Exploration-exploitation trade-off using variance estimates in multi-armed bandits", "author": ["Jean-Yves Audibert", "R\u00e9mi Munos", "Csaba Szepesv\u00e1ri"], "venue": "Theoretical Computer Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1902}, {"title": "Finite-time analysis of the multi-armed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Large deviations bounds for estimating conditional value-at-risk", "author": ["David B. Brown"], "venue": "Operations Research Letters,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Risk-sensitive online learning", "author": ["Eyal Even-Dar", "Michael Kearns", "Jennifer Wortman"], "venue": "In Proceedings of the 17th international conference on Algorithmic Learning Theory (ALT\u201906),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "The Economics of Risk and Time", "author": ["Christian Gollier"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Portfolio selection", "author": ["Harry Markowitz"], "venue": "The Journal of Finance,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1952}, {"title": "The tight constant in the dvoretzky-kiefer-wolfowitz inequality", "author": ["Pascal Massart"], "venue": "The Annals of Probability,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1990}, {"title": "Theory of games and economic behavior", "author": ["J Neumann", "O Morgenstern"], "venue": "Princeton University,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1947}, {"title": "Some aspects of the sequential design of experiments", "author": ["Herbert Robbins"], "venue": "Bulletin of the AMS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1952}, {"title": "Deviations of stochastic bandit regret", "author": ["Antoine Salomon", "Jean-Yves Audibert"], "venue": "In Proceedings of the 22nd international conference on Algorithmic learning theory (ALT\u201911),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}], "referenceMentions": [{"referenceID": 12, "context": "1 Introduction The multi\u2013armed bandit [13] elegantly formalizes the problem of on\u2013line learning with partial feedback, which encompasses a large number of real\u2013world applications, such as clinical trials, online advertisements, adaptive routing, and cognitive radio.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "Two foundational risk modeling paradigms are Expected Utility theory [12] and the historically popular and accessible Mean-Variance paradigm [10].", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "Two foundational risk modeling paradigms are Expected Utility theory [12] and the historically popular and accessible Mean-Variance paradigm [10].", "startOffset": 141, "endOffset": 145}, {"referenceID": 8, "context": ", [9] for an introduction to risk from an expected utility theory perspective).", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": "In particular, [8] showed that in general, although it is possible to achieve a small regret w.", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "In the multi\u2013arm bandit domain, the most interesting results are by [5] and [14].", "startOffset": 68, "endOffset": 71}, {"referenceID": 13, "context": "In the multi\u2013arm bandit domain, the most interesting results are by [5] and [14].", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "[5] introduced an analysis of the expected regret and its distribution, revealing that an anytime version of UCB [6] and UCB-V might have large regret with some nonnegligible probability.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[5] introduced an analysis of the expected regret and its distribution, revealing that an anytime version of UCB [6] and UCB-V might have large regret with some nonnegligible probability.", "startOffset": 113, "endOffset": 116}, {"referenceID": 13, "context": "1 This analysis is further extended by [14] who derived negative results which show no anytime algorithm can achieve a regret with both a small expected regret and exponential tails.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "In particular, we refer to the first and most popular measure of risk\u2013return, the mean\u2013variance model introduce by [10].", "startOffset": 115, "endOffset": 119}, {"referenceID": 0, "context": "We consider the standard multi\u2013arm bandit setting withK arms, each characterized by a distribution \u03bdi bounded in the interval [0, 1].", "startOffset": 126, "endOffset": 132}, {"referenceID": 9, "context": "Although a large number of models for risk\u2013return trade\u2013off have been proposed, here we focus on the most historically popular and simple model: the mean\u2013variance model proposed by [10],2 where the return of an arm is measured by the expected reward and its risk by its variance.", "startOffset": 181, "endOffset": 185}, {"referenceID": 4, "context": "Although the analysis is mostly directed to the pseudo\u2013regret, as commented in Remark 2 at page 23 of [5], it can be extended to the true regret.", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": ", [5]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": "The algorithm is a natural extension of UCB1 [6] and we report a theoretical performance analysis on how well it balances the exploration needed to identify the best arm versus the risk of pulling arms with different means.", "startOffset": 45, "endOffset": 48}, {"referenceID": 0, "context": ", [1] for the bound on the variance) on terms \u03bc\u0302 and \u03c3\u0302.", "startOffset": 2, "endOffset": 5}, {"referenceID": 0, "context": "random variables bounded in [0, 1] from the distribution \u03bdi with mean \u03bci and variance \u03c3 i , and the empirical mean \u03bc\u0302i,s and variance \u03c3\u0302 2 i,s computed as in Equation 1, then P [ \u2203i = 1, .", "startOffset": 28, "endOffset": 34}, {"referenceID": 2, "context": "In fact, in the latter case, UCB is known to have a worst\u2013case regret per round of \u03a9(1/ \u221a n) [3], while in the worst case, MV-LCB suffers a constant regret.", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": ", [4]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": ", the high-probability analysis in [5]), it captures different features of the learning algorithm.", "startOffset": 35, "endOffset": 38}, {"referenceID": 9, "context": "In particular, we relied on the mean\u2013variance model introduced in [10] to measure the performance of the arms and define the regret of a learning algorithm.", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "On the other hand, we proved that ExpExp has a vanishing Notice that although in the paper we assumed the distributions to be bounded in [0, 1] all the results can be extended to sub-Gaussian distributions.", "startOffset": 137, "endOffset": 143}, {"referenceID": 1, "context": "It also violates the monotonocity condition due to the different orders of the mean and variance and is not a coherent measure of risk [2].", "startOffset": 135, "endOffset": 138}, {"referenceID": 10, "context": ", [11]), estimating the quantile might be more difficult.", "startOffset": 2, "endOffset": 6}, {"referenceID": 1, "context": "In [2] axiomatic rules are listed to define coherent measures of risk.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "One can easily imagine a lower confidence bound algorithm based on [7] in the same composition as MVLCB which replaces the variance by the conditional value at risk.", "startOffset": 67, "endOffset": 70}, {"referenceID": 0, "context": "References [1] Andr\u00e1s Antos, Varun Grover, and Csaba Szepesv\u00e1ri.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] P Artzner, F Delbaen, JM Eber, and D Heath.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Jean-Yves Audibert and S\u00e9bastien Bubeck.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Jean-Yves Audibert, S\u00e9bastien Bubeck, and R\u00e9mi Munos.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Jean-Yves Audibert, R\u00e9mi Munos, and Csaba Szepesv\u00e1ri.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Peter Auer, Nicol\u00f2 Cesa-Bianchi, and Paul Fischer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] David B.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Eyal Even-Dar, Michael Kearns, and Jennifer Wortman.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Christian Gollier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Harry Markowitz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Pascal Massart.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] J Neumann and O Morgenstern.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Herbert Robbins.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Antoine Salomon and Jean-Yves Audibert.", "startOffset": 0, "endOffset": 4}], "year": 2013, "abstractText": "Stochastic multi\u2013armed bandits solve the Exploration\u2013Exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk\u2013aversion where the objective is to compete against the arm with the best risk\u2013return trade\u2013off. This setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we introduce two new algorithms, investigate their theoretical guarantees, and report preliminary empirical results.", "creator": "LaTeX with hyperref package"}}}