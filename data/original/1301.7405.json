{"id": "1301.7405", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2013", "title": "Flexible Decomposition Algorithms for Weakly Coupled Markov Decision Problems", "abstract": "This paper presents two new approaches to decomposing and solving large Markov decision problems (MDPs), a partial decoupling method and a complete decoupling method. In these approaches, a large, stochastic decision problem is divided into smaller pieces. The first approach builds a cache of policies for each part of the problem independently, and then combines the pieces in a separate, light-weight step. A second approach also divides the problem into smaller pieces, but information is communicated between the different problem pieces, allowing intelligent decisions to be made about which piece requires the most attention. Both approaches can be used to find optimal policies or approximately optimal policies with provable bounds. These algorithms also provide a framework for the efficient transfer of knowledge across problems that share similar structure.", "histories": [["v1", "Wed, 30 Jan 2013 15:06:15 GMT  (349kb)", "http://arxiv.org/abs/1301.7405v1", "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)"]], "COMMENTS": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ron parr"], "accepted": false, "id": "1301.7405"}, "pdf": {"name": "1301.7405.pdf", "metadata": {"source": "CRF", "title": "Flexible Decomposition Algorithms for Weakly Coupled Markov Decision Problems", "authors": ["Romild Parr"], "emails": ["parr@cs.stanford.edu"], "sections": [{"heading": null, "text": "This paper presents two new approaches to de composing and solving large Markov decision problems (MDPs), a partial decoupling method and a complete decoupling method. In these ap proaches, a large, stochastic decision problem is divided into smaller pieces. The first approach builds a cache of policies for each part of the problem independently, and then combines the pieces in a separate, light-weight step. A second approach also divides the problem into smaller pieces, but information is communicated be tween the different problem pieces, allowing in telligent decisions to be made about which piece requires the most attention. Both approaches can be used to find optimal policies or approximately optimal policies with provable bounds. These al gorithms also provide a framework for the effi cient transfer of knowledge across problems that share similar structure.\n1 Introduction\nThe Markov Decision Problem (MDP) framework pro- \u00b7 vides a formal framework for modeling a large variety of stochastic, sequential decision problems. It is a well understood framework with well-known on-line and off line algorithms for determining optimal behavior (see e.g. Puterman (1994)). The limitations of this framework are also well-known: compliance with the Markov property generally requires a very fine granularity description of the environment. Thus, most interesting problems have large state spaces.\nOne of the main research thrusts for MDPs has been the development of methods for large state spaces. A major complicating factor in this line of research is the appar ent non-decomposability of MDPs - the utility or value of any state can, in general, be affected indirectly by the cost structure and the dynamics of any other state. This thwarts efforts to decompose MDPs into completely independent\nsubproblems and complicates efforts to reduce computa tion time through parallelization.\nWhile some progress has been made on understanding some very special cases where MDPs may be decomposed into independent subproblems (Singh, 1992; Lin, 1997), much of the effort has focused on methods that decom pose MDPs into \"communicating\" subproblems (Bertsekas & Tsitsiklis, 1989; Dean & Lin, 1995). In these iterative methods, information about subproblem solutions is com municated to neighboring subproblems. The solution for each subproblem may need to be updated many times until a globally optimal solution is obtained.\nThis paper considers a special, but fairly general class of problem decompositions where each subproblem is \"weakly\" coupled with the neighboring subproblems. This means that the number of states connecting the two sub problems is small, a relationship that appears naturally in many problems. For example, the problem of moving from one's office to one's house has this structure: one's office is a small region that is connected by a much smaller re gion, the door, to an external corridor. Many other of fices may be connected to this corridor, each with a similar structure. The corridor could be fairly large and connected to other corridors by relatively small intersection regions. Most buildings have a small number of doorways that con nect them to the streets outside. Each street has a rela tively small number of points where it connects to other streets. One such street connects to the house one calls home, which is itself an aggregation of weakly connected pieces. An MDP is weakly coupled if it can be divided into two or more subproblems that are weakly coupled with each other. Figure 1 shows a simple navigation MDP di vided into four rooms, each of which can be considered a subproblem. 1\nThis paper extends the communicating MDP solution methods with the goal of avoiding iterating between differ ent subproblem solutions by constructing a cache of poli cies independently for each subproblem that is guaranteed\n1 Similar examples and pictures are used by Precup and Sutton (1998) and Hauskrecht, Meuleau, Boutilier, Kaelbling, and Dean (1998).\na priori to provide performance within a constant of the optimal, regardless of the structure of the other subprob lems. This permits a complete decoupling of the MDP into independent subproblems that can be solved in par allel and then recombined in a single step. The decoupling is achieved through two new algorithms that build a cache of policies iteratively by discovering the point at which the current cache performs the worst, then adding a new policy to the cache to cover the worst case.\nThe efficient manipulation of policy caches also provides a formal basis for the transfer of knowledge across problems with similar substructures. The simplest case of this occurs when the reward structure for a problem changes. Suppose, for example that the reward in the navigation problem is moved from room 2 to room 4. Policy caches devised for rooms 1 and 3 can be can be transferred to the new prob lem. Similarly, if one's destination is now a cafe instead of home, the policies designed for one's office and the con taining building should transfer to the new problem.\nSince the number of possible policies for a subproblem is exponential in the number of states in the subproblem, there may exist problems and accuracy requirements for which the size of the policy cache will be exponential. In these cases there still will be some benefit to constructing a small policy cache, even if it does not provide the desired accu racy guarantees. This paper presents an algorithm that aug ments standard communicating MDP algorithms with the use of a policy cache. The policy cache can be used to de termine lower and upper bounds on the values that states in the subproblem can assume, and this provides a means of deciding when it is worth using a cached solution and when it is worth producing a new subproblem solution. This is particularly useful in determining if subproblem solutions from a related problem can be applied to a new one.\n2 Markov Decision Problems\nTo review the basic MDP framework, an MDP is a 4-tuple, (S, A, T, R) where Sis a set of states, A is a set of ac-\nFlexible Decomposition for MOPs 423\ntions, T is a transition model mapping S x A x S into probabilities in (0, 1], and n is a reward function mapping S x A into real-valued rewards. Algorithms for solving MDPs can return a policy, 1r, that maps from S to A, or a real-valued value function V. In this paper, the focus is on infinite-horizon MDPs with a discount factor (3. The aim in these problems is to find an optimal policy 7r*, that maxi mizes the expected discounted total reward of the agent, or to find an approximately optimal policy that comes within some bound of optimal.\nValue iteration, policy iteration or linear programming can be used to determine the optimal policy for an MDP. These algorithms all use some form of the Bellman equation (Bellman, 1957):\nV*(s) = m;xR(s, a)+ f3 LT(s, a, s')V*(s'). s'\nWhen the Bellman equation is satisfied, the maximizing ac tion for each state is the optimal action.\nFor a particular policy, the Bellman equation becomes a system of linear equations:\nv7r, (s) = R(s, 7rl(s), s') + f3 LT(s, 7rl(s), s')V7r, (s'). s'\nThese can be solved to determine, v7r,' the value of fol lowing 1r1 from any state. The Bellman error for a particu lar policy at a particular state is the difference between the value function for that policy and the right-hand side of the Bellman equation:\nBE(V1r(s)) = m;xR(s, a)+/3 LT(s, a, s')V1r(s')-V1r(s) s'\nFor any policy, the maximum Bellman error over all states, BE(V1r) = max. BE(V1r(s)), is a well-known bound on the distance from the optimal value function (Williams & Baird, 1993):\nThis is used below to produce error bounds on policy caches.\n3 A Class of Decomposition Algorithms\nThe first step for a decomposition method for MDPs is the division of the state space into disjoint subsets, G1 . . . Gm. In the simple navigation problem of Figure 1, each room could be a subset. For each subset, G;, there exists a set of states not in G; that are reachable in one step from G;. Call this the out-space of G;. In Figure 1, the out-space of the top-left room contains one state in the room to the right, and one in the room below. The in-space of region G is defined as the set of states inside of G reachable in one step from a region outside of G.\n424 Parr\nThe next step is the introduction of a set of policy caches, II 1 . . . IIm, defined over each of the regions. It is well known that the optimal assignment of policies to regions can be determined by solving a \"high-level\" reduced deci sion problem defined over only the states in the out-spaces of the regions. This reduced decision problem removes all but the out-space states from the problem. Actions in the reduced problem correspond to assignments of policies to regions in the original decision problem. This transforma tion is the basic insight of Forestier and Varaiya ( 1978) and it follows as a special case of the hierarchical results in Parr and Russell (1998). The approach is also investigated in (Hauskrecht et a!., 1998). This type of problem also can be viewed as Semi-Markov decision problem (SMDP), where each low-level policy becomes a primitive SMDP action as in Parr (1998).\nIn Figure 1, the high level problem would contain just the eight specially marked states. An action in the high level problem would correspond to a decision to adopt some policy from the cache upon entering a room, and staying with this policy until the next out-space state is reached. The solution to the high-level problem may produce a non stationary policy at the low-level, which means that the ac tions taken in any room may depend upon the manner in which the room is entered. A non-stationary policy of this type can be converted easily to a stationary policy that is at least as good (See Parr (1998)).\nThe relationship between the size of the out-spaces and the complexity of the high-level problem should make the im portance of weak coupling clear. If the size of the out spaces approaches the size of the original MDP, then the high-level decision problem will be as difficult as the orig inal MDP.\nAn algorithm that completely decomposed an MDP would produce a II; for each G;, combine these to produce an op timal or approximately optimal overall solution, and never need to revise any of the II;. Unless the II; are chosen very carefully, or the caches are very large, combinations of policies in the initial policy caches may not suffice. There are several approaches to revising the policy caches. One extreme end of this spectrum is the approach in Sutton, Pre cup, and Singh (1998), where policies and low-level ac tions are mixed together in the same SMDP. This sacrifices the reduction in computational complexity obtained from solving a reduced decision problem in favor of a guaran tee of obtaining optimality. Another approach considered by Dean and Lin (1995) updates each II; directly. Dean and Lin considered a special case in which the old policies were discarded at each iteration, and a new policy was com puted for each region based upon the high-level decision problem's current estimates forthe value of the out-space states. Note that the high-level decision problem was actu ally a trivial value determination problem and not really a decision problem since III; I = 1 for all i.\nThe approach advocated by Dean and Lin is guaranteed_\nto converge to the optimal policy. However, it is just one special case of a general class of methods that must con verge. Any reasonable scheme that improves the policies in the regions and propagates those improvements through the high-level decision problem is guaranteed to produce an optimal policy as long as no regions \"starve\", i.e., never have their policies improved. This result follows directly from the observation that the high-level problem of assign ing policies to rooms is really just an SMDP where the set of permitted actions for the SMDP are just the set of possi ble policies defined over regions.\nThe algorithms in this paper all aim to minimize the num ber of policies that are computed for MDP subproblems. The extent to which this can be minimized is a measure of how effectively an MDP has been decomposed. If each subproblem requires only a small cache of candidate so lutions, this means that the subproblem solutions are rel atively independent. These are precisely the situations in which a large computational benefit is reaped from decom position, since the MDP can be divided and conquered by solving a reasonable number of small subproblems. The size of the policy caches also gives some measure of the parallelizability of the problem. If a region can be solved with a small cache of policies, this suggests that the entire cache could be constructed a priori as a completely inde pendent subprocess.\nThe following section describes several algorithms for con structing policy caches with minimal knowledge of how the subproblem is connected to overall MDP. These algorithms aim to minimize the size of the cache, while ensuring that solutions using the cache will be within a bound of opti mal. The succeeding section describes a scheme for work ing with policy caches for which optimality bounds have not been established a priori. This method efficiently es tablishes bounds on the benefit of adding a new policy to a cache, based upon the current contents of the cache.\n4 Complete decoupling\nThis section presents algorithms that find a policy cache, II, for a particular region, G, such that II is guaranteed to provide policies that are within a constant of optimal when a high level problem using II for G is solved. The only assumptions that are made about the regions to which G connects is that the states assume values on [V min ... V max]. Define yg, as a vector of values that the states in the out space of G can take on (the subscript will be dropped when there can be no confusion about the region in question). The fan-out of a region is defined as the dimension of this vector.\nIn addition to storing a cache of policies it is useful to store a cache of functions, J1r,(s, Y0) for each 1r; E II. Each f 1r, ( s, Y0) is linear function that provides the value of any state s E G as a linear function of Y0. For any policy, 7r;, it is quite straightforward to determine these functions by\nsolving a system of linear equations, as in Parr (1998).\nThe goal in constructing a policy cache for a region is to produce a cache such that for every possible value of the corresponding out-space states, there is a policy in the re gion's cache for which the performance in the region will be within a bound of optimal. A policy, 11\", for region, G, is optimal with respect to V0 if 11\" is the solution to the MDP defined just over the states in G, with the assumption that states in the out-space of G are absorbing states with val ues locked at the value of the corresponding entry in V0. In room 1 of the four-room example, the optimal policy for V0 would be determined by solving an MDP with just the states in room 1 and the two connecting states in room 2 and room 3. The value of the connecting state in room 2 would be treated as a constant with value V0 [0) and the value of the connecting state in room 3 would be a constant with value V0 [1]. A policy, 11\", is said to be c-optimal with respect to V0 if \\is E G, BE(V\") \ufffd f when the values of the states in the out-space of G are fixed by Y0.\nFor any state and any value of V0, there must be one pol icy in the cache that appears at least as good as all of the others. A policy, 11\" dominates at t for a particular V0, if f\"(t, V0) 2: f\"j(t, V0) \\fj. This means that the low-level policy, 11\", appears to be the best high level action at state t for a particular yo. A cache of policies is c-optimal at t if, for any V0, the dominating policy is c-optimal. A cache of policies is c-optimal if it is c-optimal at all t in the in-space of G.\nTheorem 1 If an MDP is divided into m regions, G1 .. . Gm and an c-optimal cache of policies, Il1 ... 11m, is determined for each region, these policies can be com bined to produce a globally (l\ufffd,6) -optimal policy by solving an MDP with at most L_i IYg, I states and L_i I IIi I ac tions.\nProof: As in Section 3 a high-level decision problem can be defined over the states U = U; G;. The solution to this decision problem assigns values to the states in U and assigns dominating policies to each state in the in-space of each region in U based upon these values. Consider an arbitrary region G and the policy, 11\", assigned to it when it is entered at state t. By the definition of c-optimality, BE(V\"(s)) \ufffd c for all s in G. Since this will be true for all states in all regions, the maximum Bellman error for any policy that will be used in any region will be less than f, which means that the maximum Bellman error for the entire problem will be less than f, which is sufficient to ensure (l\ufffd,6) -optimality. \u2022\nThis theorem provides a means of combining a set of ap proximately optimal solutions to produce a global solution that is also approximately optimal. Of course, if f = 0, then the solution will be optimal. The following subsections will describe three algorithms for constructing c-optimal pol icy caches.\nFlexible Decomposition for MDPs 425\n4.1 The c-grid Approach\nThe most straightforward approach to devising a policy cache, as described in Hauskrecht et al. (1998), is to create an c resolution grid over the space of possible values for V0, then produce an optimal policy with respect to each grid point. This can be proved to be sufficient by observing that the value of any policy can change by at most f when moving within one cell of the grid. This bounds the loss from using a the policy defined for the nearest grid point instead of the optimal policy. This result is established for mally using an alternative argument in (Hasukrecht, 1998).\nThe main problem with the c-grid approach is that it can require a huge number of policies, ( Vma.x\ufffd Vm;n )d. This will be unmanageable unless the range of values is very small, the fan out of the region is very small, or f is very large.\n4.2 Value Space Search\nThis section presents an algorithm that aims to avoid con structing an exponential number of policies by searching through V0 space to find a point at which the current pol icy cache is not adequate. If such a point is found, a new policy is added to the cache, and the process is repeated until no points can be found for which the current cache is inadequate. The following formal results are the basis of the value space search algorithm:\nLemma 1 If the dominating policy in a policy cache is used for any V0, then the value function for any state, s, is a piecewise-linear, convex function ofV0.\nProof: This follows from the observation that using the best policy means taking the maximum over a set of linear policy functions. \u2022\nThis lemma is demonstrated with a simple example using the model in Figure 2, where there is a room with just one exit, but a + 1 absorbing reward state in the center of the room. The value function for this room can be displayed in two dimensions since V0 is just a scalar. Figure 3 shows linear functions for the value of the top-left state for the optimal policy for V0 = 0, V0 = 1.09, and V0 = 2.\n426 Parr\nTheorem 2 For region G, with n states, a actions, and pol icy cache, II = 7rt . . . 1r m\u2022 the point in V0 space for which the Bellman error of the dominating policy is largest, can be determined in time that is polynomial in n, a, m, and d.\nProof: This is achieved by means of a linear program. For all t in the in-space of G, for all s in G, for all a, and for all 1r E II, the following linear program is solved:\nMaximize:\nR(s, a)+ LT(s, a, s')f,(s', V0)- f,(s, V0) \u2022'\nSubject to:\nf,(t, V0) < f,;(t, V0) '<h; E II V0[i] < Vmax, 1 :S i :S d\nNote that the free variables in the system are the compo nents of V0. The objective function maximizes the Bell man error at state s under the assumption that action a is taken. The first set of constraints identifies the region in V0 space for which 1r dominates at t. If this region exists, it is guaranteed to be a single, continuous facet of a convex surface by Lemma 1. The last set of constraints bounds V0 to be within the range of possible values.\nThe largest value returned by the linear program over all s, a, and 1r provides the point at which the current cache of policies will have the largest Bellman error. The time bound is satisfied because linear programming is polyno mial in the size of its inputs. \u2022\nThis provides a basis for determining if a policy cache is f-optimal. If the largest Bellman error returned by the lin ear program in the above theorem is less than f, this means that no matter what values the states in the out-space of G assume, the policy assigned to G will produce state values with a Bellman error of less than f. Thus, if the largest Bell man error is less than f, the cache is f-optimal. The com putational consequences of Theorem 2 are non-trivial and\ndeserve emphasis: When combined with Theorem 1, this appears to be the first efficient method known for determin ing if a set of policies for a region of an MDP is sufficient to produce a global solution that is within a bound of the optimal global solution. Note that the conditions checked by Theorem 2 are sufficient, but have not been shown to be necessary; more efficient methods may be possible.\nAssuming, that the minimum possible state value is Vmin\u2022 this theorem provides a means of constructing an f-optimal cache of policies. Suppose that Theorem 2 is implemented as a function, find-worst, that takes a policy cache and returns two values, the point at which the Bell man error is worst, and the magnitude of this this error.\nII= {optimal policy for V0 :::: (Vmin, ... , Vm;n)} quit= false Repeat until quit\n(worst-error, worst-point) = find-worst(II) if worst-error > f\nII = II u {optimal policy for worst-point} else\nquit= true\nThis algorithm keeps adding policies to the policy cache until the policy cache is proven to be f-optimal. Each pol icy that is added covers at least one case where the current cache is inadequate. Note that, worst-point will always be at a corner of the convex facet defined by some dominat ing policy. In practice, it is preferable to add a policy that is optimal for a point slightly towards the interior of the facet instead of at a corner. Note also that this algorithm implicitly assumes that the each new policy that is added to the cache will improve the value of every state in the re gion when V0 = worst-point. This will be true if actions are sufficiently noisy such that reducing the Bellman error in any state will produce at least a minute improvement in the value of other states. It is possible to construct mod els where this assumption does not hold and in such cases additional constraints must be added to the linear program to break ties between policies. (See Parr (1998) for a more detailed discussion of these points.)\nThis approach has some similarities to and was inspired by algorithms for partially observable MDPs (POMDPs) (see Lovejoy (1991) for a survey), and in particular, the Witness algorithm (Cassandra, Kaelbling, & Littman, 1994). The treatment of policies as linear functions, the maximum over which forms a convex surface, is common in the POMDP literature. The value space search algorithm uses a simi lar approach to that of the Witness to search a continuous space to find the place where the error in the current set of policies is largest. The Witness algorithm is a synchronous value iteration algorithm that searches through belief space for a partially observable prob,em. The value space search algorithm searches through the space of state values to find the point at which the error in a set of infinite horizon MDP value functions is the largest.\nThe value space search algorithm was used to find an <-optimal policy cache for room 1 of Figure 1. This sub problem contains 25 states and has a fan-out of 2. Possible actions are right, left, up and down, but these actions are unreliable, resulting in movement in one of the three other axis-parallel directions 20% of the time. The discount fac tor used was 0.95. There are 425 \ufffd 1014 possible policies for this subproblem. Of course, many of these are unrea sonable policies that, for example, move the agent in cir cles. However, a variety of policies can still be induced by different values of the out-space states, even in such a sim ple problem. One would think that the agent would simply aim for the exit with the highest state value. However, the noise in the action model ensures that there is always some chance that the agent will wind up unintentionally exiting the wrong way. Thus, as the relative difference between the two out-space states increases, the optimal policy will take a more circuitous route towards the desired exit, hug ging the walls to avoid accidentally getting too close to the undesired means of egress.\nIf the values of the states are assumed to be on [0 . . . 20], then the <-grid approach for this problem would require 4 million policies for < = 0.01. The value space search al gorithm produced a policy cache with the same optimality guarantees with just 22 policies. For E = 0.001, the < grid approach would require 400 million policies, while the value space search algorithm produced the same 22 poli cies.\nIn this particular case, the value space search algorithm has captured the intuition that this type of subproblem should not be that hard. A few seconds of computation has pro duced a small cache of that will ensure a nearly optimal solution for this region no matter what happens in any con necting region. This small subproblem is now decoupled and completely solved - at least for < 2: 0.001 and for problems where the neighboring states can assume values on [0 ... 20]. Any MDP satisfying these conditions and with an optimality requirement of no more than 1\ufffd\ufffd0\ufffd5 = 0.02 will never need another policy defined on this region.\nAn important unanswered question is whether this algo rithm is guaranteed to find a polynomial size <-optimal cache of policies if such a cache exists. The idea of cre ating a new policy near the point where the current policy cache performs the worst is plausible, but there is not yet a proof that this constructs a cache of policies that is in any way minimal. A drawback of this algorithm is that it constructs a large number of linear programs. This can be onerous if the number of states in the region is large. Of course, this price is paid only once, and the cache can be reused indefinitely in any MDP that contains the same subproblem. Moreover, many implementation tricks can be used to reduce the size and number of linear programs constructed. For example, the maximum Bellman error for any (entry-point,state,action,policy) quadruple is non increasing as the policy-cache grows, so the solutions to previous linear programs can be cached across calls to find-\nFlexible Decomposition for MDPs 427\nworst. A new linear program is needed only if the cached error is greater the maximum error detected so far in in the current call to find-worst.\n4.3 The Convex Hull Bounding Approach\nThis section sketches a third algorithm with a compu tational geometry flavor. This algorithm also provides slightly different optimality guarantees than the previous algorithms. It guarantees that a cache of policies will be< optimal at the high-level. Recall that high-level actions cor respond to policies at the low level. If a cache is <-optimal at the high level, this means that there is no low-level pol icy that could improve the value of a high-level state by more than c In the four-room example, this would mean that no policy could improve the value of one of the con necting states by more than E. However, there could be a policy that could improve the value of some other state, for example, the state in the top left corner of the model, by more than<, but the expected effect of this change on any of the high-level states must be less than this.\nHigh-level <-optimality implies that any policy starting from a high-level state (e.g. one of the states connecting the rooms) will have expected value within 1\ufffd,a of opti mal. This could be a problem, however, if the agent typ ically starts in some state that is not a high-level state. In such cases, the starting position of the agent can be treated as if it were a connecting state by adding it to the in-space of the enclosing region and constructing a policy cache as if it were a connecting state. If desired, every state could be treated as if it were an in-space state, ensuring full low level optimality as well.\nThe algorithm presented in this section has run time that is exponential in d, the fan-out of the region, but unlike the <-grid approach, it does not depend explicitly on 1/ < and unlike the value space search algorithm, it can avoid considering every state inside of a region if high-level < optimality is sufficient. The algorithm relies upon the fol lowing formal results:\nLemma 2 For any point V0, set of points, V1 ... V d+l, with set of policies, 1r1 ... 1r d+l> such that 11'; is optimal with respect to V; and such that the V; form a convex hull around V0, the optimal policy with respect to V0 at any state s is bounded from below by max; J'lr,(s, V0) and from above by the hyperplane containing each of the (V;, f1r, ( s, V;)).\nProof: Bounding from below is obvious and follows from Lemma 1: the optimal policy at any point must do at least as well as the dominating policy in the cache. The bound from above is somewhat more subtle: Let H1 be the hyper plane containing the (V;,J1r,(s, V;)). Suppose that there exists some 1r and corresponding f1r such that for somes, f1r (s, V0) is above H1. Let H2 be the hyperplane corre sponding to the linear value function of this policy at s. There must exist some corner of the convex hull used to create H1 (some (V;,J'lr,(s, V;))) where H2 is above H1,\n428 Parr\ni.e., frr(s, Vi) > frr.(s, V;). However, frr, is known to be optimal with respect to V;, so this is a contradiction. \u2022\nA simple example of this lemma is shown in Figure 4. Two functions for policies from Figure 3 are shown. One is op tima at V0 == 0 and the other is optimal at V0 = 2. For any 0 :::; V0 :::; 2, the linear function for the optimal policy cannot cross 0.8 at V0 = 0 or V0 = 1.84. Thus, the value of optimal policy is bounded by the line shown.\nTheorem 3 For region G and cache of policies, 1r1 ... 11\"m, that are optimal at V1 ... Vm, the optimal policy value for any s with respect to any V0 is bounded from below by the convex surface formed by the maxi mum over the corresponding frr, ... frrm and bounded from above by the convex hull containing the points: (V1 . frr1 (s, VI)),\u00b7\u00b7\u00b7 (Vm, frrm (s, Vm)).\nProof: The bound from below is a direct consequence of Lemma 1. The bound from above follows from Lemma 2 and noting that the lowest bounding hyper plane for any V0 must form a facet in the convex hull of (V1 .frr1 (s, VI)),\u00b7\u00b7\u00b7 (Vm, frrm (s, Vm)).\nThis theorem also suggests an algorithm for finding points in value space where the current policy cache is not f-optimal: For each facet in the upper bounding hull, find the point in the lower hull that maximizes the distance be tween the two surfaces. If no point can be found where the distance is greater than f, then the policy cache is f optimal. If the facets in the upper hull are enumerated as a set of lin ear functions, 91 ... 91, then the maximum distance can be checked for each s, frr,, and 9i as follows:\nMaximize\nSubject to\n9j(S, V0) frrk(s, V0) < 9k(s,V0)'Vk < frri(s, V0) 'Vk\nV0[k] < Vmax 'Vk\nThe last constraint bounds V0 to lie in the permitted range. The first two sets of constraints identify the area in V0 space in which frr, is a facet on the lower bounding hull and 9i is a facet on the upper bounding hull. If such an area exists, the objective function finds the point at which the distance from the upper hull to the lower hull is greatest.\nThe above linear program can be used to generate a cache of policies in a fashion similar to the value space search al gorithm. By searching all pairs of upper bounding facets and lower bounding facets, the point at which the gap be tween these surfaces is greatest can be used to determine a new policy. One complicating factor is that the upper bounding hull may not cover the entire space of values for V0 . For points outside the hull, the value of the optimal policy can be bounded from above by ymax. In some cases this can be tightened by observing that no policy will do better than the sum of value of the optimal pol icy at cvmin, ... , ymin)' and f3 max; V0 [ i]' since the op timal policy for the lowest values of the out-space states will maximize the reward received within the region and no policy can do better than receiving this reward and then moving to the highest valued state in one step.\nThe more serious complication for this algorithm is the general result from computational geometry that the convex hull of m points in d dimensional space can have 0( ml \ufffdJ) facets, making this algorithm exponential in d. Still, the convex hull bounding algorithm is superior to the f-grid approach since the f-grid approach has run time that de pends directly on \ufffd and the range of values possible in the out-space, while the bounding approach depends on the number of policies in the cache.\n5 Partial Decoupling\nThe previous section presented 3 algorithms for completely decoupling MDPs. These algorithms are quite computa tionally intensive and there are no guarantees that the size of the policy cache required for a desired solution qual ity will be manageably small. In such cases, one may be forced to use a policy cache that is not known a priori to be f-optimal for the range of values the out-space of a re gion will take on when reconnected to the rest of the MDP. For example, a rough policy cache could be constructed for each of the rooms in a building. When a high-level prob lem that combines these rooms is solved, some decisions will need to be made on-the-fly about whether the policies in the rough cache are adequate for the larger problem.\nMore specifically, suppose an MDP has been divided into disjoint regions and a policy cache has been constructed for each region. A high-level decision problem can be de fined over the out-spaces of these regions. For a particu lar region, G, an algorithm solving this high-level decision problem has the option of using one of the policies in the\npolicy cache for G, or generating a new policy that is opti mal for the algorithm's current estimate of vg. A straight forward way to answer this question would be to use the cache f\" functions to assign values to every state in the problem and then compute the Bellman error for each state. However, this approach would require so much computa tion that it would essentially defeat the purpose of solving a high-level problem. Instead, high-level optimality can be checked quite efficiently by using the tools of the convex hull bounding algorithm.\nStarting with some policy cache, 1r1 \u2022 \u2022 \u2022 7T m, the elements of which are optimal at the corresponding V1 ... Vm, for any particular vg, the value of any state under the opti mal policy with respect to vg is bounded from below by max\", f\".(s, V0), and the value is bounded from above the convex hull formed by /\"1 (s, Vt) ... f\"m (s, Vm) (The orem 3). The situation here is slightly different from the bounding algorithm in that Vf_? is fixed and known. Instead of a high-dimensional convex hull problem, the bounds for a particular V 0 can be determined by solving a linear pro gram. In the following f* is an unknown linear equation, i.e., the coefficients and constant are free variables:\nMaximize:\nSubject to:\nf*(s, V;) < J\",(s, V;), 1:::; i:::; m f*(s,Vo) < Vmax\nTo reassure oneself that this is indeed a linear program, re call that in this context, V0, the V;, and coefficients and constants for the /\", are all known constants. The only variables are the components of f*. The first set of con straints requires that f* be no better than the optimal pol icy for s at points in value space where the optimal policy is known. This is, essentially, a restatement of Lemma 2. The second set of constraints requires that f* never exceeds the maximum value any state can assume in this problem. Thus, the objective function forces the linear program to find the highest hyperplane that does not violate Lemma 2 or the bound on state values. If V0 lies in the convex hull of V1 ... Vm, then f* will be the facet of the upper bounding convex hull from Theorem 3. Note that if V0 does not lie in the convex hull, Vmax will be returned. This bound can be tightened by requiring that the constant of f* be no longer than the value of the optimal policy at V0 = (Vmin ... Vmin ) and that the coefficients of f* sum to be no more than 1.\nIf the distance between the dominating policy and the upper bound returned by the above linear program is less than f for every state in the in-space of G, then the policy cache for G is sufficient to produce a high-level f-optimal policy for the current value of V0. This means that a high-level decision problem can, for now, avoid updating the policy for region G and focus attention on other regions. This\nFlexible Decomposition for MDPs 429\ndecision will need to be reevaluated as values of the states in the out-space of G change. One way to view this result is that it enables a form of high-level prioritized sweeping (Moore & Atkeson, 1993; Andre, Friedman, & Parr, 1998).\nThis result also has significant consequences for the trans fer of knowledge across problems. Suppose, for exam ple, that a particular model substructure appears in many different problems. Consider a larger version of the four room problem with many interconnected rooms. Different tasks in this domain would correspond to different posi tions of the reward in different rooms. Every time a policy is produced for a room it can be added to the room's policy cache. The above linear program can be used to determine quickly if for some new problem, the cache in a particu lar room is adequate. Thus, a form of cross-task learning is achieved where the time required to plan for new objec tives declines as experience is gained with the environment. Moreover, intelligent allocation of computational resources will be possible since parts of the value space that have already been mastered will no longer drain computational resources.\n6 Conclusion\nThis paper presented two approaches to decoupling MDPs, a complete decoupling approach and a partial decoupling approach. With complete decoupling, the problem is di vided into independent subproblems, and the solutions to these subproblems are combined in a single step. Two new algorithms for determining f-optimal policy caches for a subproblem are presented. The significance of the first al gorithm is that it runs in polynomial time, regardless of the fan-out of the region. The second algorithm uses a compu tational geometry approach that can be exponential in the fan-out of the subproblem, but can be more efficient than the first algorithm if the fan-out is small.\nSince complete decoupling may not always be possible, a method for partial decoupling is presented. This method as sumes that an imperfect policy cache is used by a high-level asynchronous MDP algorithm. It uses the policy cache to bound the optimal values of states in a region with respect to the values of the states in the out-space of the region. By providing upper and lower bounds, this permits intelli gent decisions about when to update the policy cache for a region based upon the algorithm's current estimate of the values of the states in the out-space of the region.\nTogether these results provide a framework for large-scale parallelization of MDPs and a formal framework for the transfer of knowledge across problems that share common structures. These results can be applied hierarchically, al though the optimality requirements for the subproblems will become stricter with each division if the same level of optimality is to be maintained at the top level.\nThis work does not address the questions of state abstrac tion or value function approximation. Fortunately, these\n430 Parr\ntechniques will compliment the results presented here. The decoupled MDP algorithms will benefit from any approach that compresses the state space, especially if the compres sion reduces the fan-out of the regions in some decomposi tion of the space.\nA limitation of this work is that it applies mainly to a restricted class of MDPs, those that are weakly coupled. Moreover, the efficiency of the methods described here will depend heavily upon the manner in which the MDP is decomposed into subproblems, and, in particular, the fan-out of the regions in the decomposition. The reader should keep in mind, however, that this appears to be one of the first attempts to aggressively decouple MDPs and that while the algorithms involved are, admittedly, complex, the potential benefits in parallelization and knowledge transfer across problems resulting from this line of research are sub stantial.\n7 Acknowledgment\nThis work was support in part by DARPA contract DACA 76-93-C-0025 under subcontract to Information Ex traction and Transport, Inc., and through the generosity of the Powell Foundation and the Sloan Foundation and by DARPA Prime contract IET-1004-96-009. Some of this was done at the University of California at Berkeley, where it was supported in part, by ONR grant N00014-97l -0942 and ARO MURI grant DAAH04-96-l-0341. The author benefited from helpful discussions about this and re lated work with David Andre, Craig Boutilier, Mike Bowl ing, Tom Dean, Nir Friedman, Milos Hauskrecht, Daphne Koller, Uri Lerner, Stuart Russell, Mehran Sahami and Rich Sutton. The reviewers also provided some extremely helpful comments.\nReferences\nAndre, D., Friedman, N., & Parr, R. (1998). Generalized prioritized sweeping. In Advances in Neural Information Processing Systems 10: Proceedings of the 1997 Confer ence, to appear. MIT Press.\nBellman, R. E. (1957). Dynamic Programming. Princeton University Press, Princeton, New Jersey.\nBertsekas, D. C., & Tsitsiklis, J. N. (1989). Parallel and Distributed Computation: Numerical Methods. Prentice Hall, Englewood Cliffs, New Jersey.\nCassandra, A. R., Kaelbling, L. P., & Littman, M. L. (1994). Acting optimally in partially observable stochastic domains. In Proceedings of the Twelfth National Confer ence on Artificial Intelligence (AAAI-94), pp. 1023-1028 Seattle, Washington. AAAI Press.\nDean, T., & Lin, S.-H. (1995). Decomposition techniques for planning in stochastic domains. In Proceedings of the\nFourteenth International Joint Conference on Artificial In telligence (IJCAI-95), pp. 1121-1127 Montreal, Canada. Morgan Kaufmann.\nForestier, J.-P., & Varaiya, P. (1978). Multilayer control of large Markov chains. IEEE Transactions on Automatic Control, AC-23, 298-304.\nHasukrecht, M. (1998). Planning with temporally abstract actions. Tech. rep. cs-98-01, Computer Science Depart ment, Brown University, Providence, Rhode Island.\nHauskrecht, M., Meuleau, N., Boutilier, C., Kaelbling, L. P., & Dean, T. (1998). Hierarchical solution of Markov decision processes using macro-actions. In Proceedings of the Fourteenth Conference on Uncertainty in Artificial In telligence (UAI-98). To appear.\nLin, S.-H. (1997). Exploiting Structure for Planning and Control. Ph.D. thesis, Computer Science Department, Brown University, Providence, Rhode Island.\nLovejoy, W. S. (1991). A survey of algorithmic methods for partially observed Markov decision processes. Annals of Operations Research, 28(1-4), 47-66.\nMoore, A. W., & Atkeson, C. G. (1993). Prioritized sweeping-reinforcement learning with less data and less time. Machine Learning, 13, 103-130.\nParr, R. (1998). Hierarchical Control and Learning for Markov Decision Processes. Ph.D. thesis, University of California, Computer Science Division, Berkeley, Califor nia.\nParr, R., & Russell, S. (1998). Reinforcement learning with hierarchies of machines. In Advances in Neural Informa tion Processing Systems 10: Proceedings of the 1997 Con ference, to appear. MIT Press.\nPrecup, D., & Sutton, R. S. (1998). Multi-time models for temporally abstract planning. In Advances in Neural In formation Processing Systems 10: Proceedings of the 1997 Conference, to appear. MIT Press.\nPuterman, M. L. (1994). Markov Decision Processes. Wi ley, New York.\nSingh, S. P. (1992). Transfer of learning by composing solutions of elemental sequential tasks. Machine Learning, 8(3), 323-340.\nSutton, R. S., Precup, D., & Singh, S. P. (1998). Between MDPs and semi-MDPs: Learning, planning, and represent ing knowledge at multiple temporal scales. in prep.\nWilliams, R. J., & Baird, L. C. I. (1993). Tight perfor mance bounds on greedy policies based on imperfect value functions. Tech. rep. NU-CCS-93-14, College of Computer Science, Northeastern University, Boston, Massachusetts."}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": "This paper presents two new approaches to de\u00ad<lb>composing and solving large Markov decision<lb>problems (MDPs), a partial decoupling method<lb>and a complete decoupling method. In these ap\u00ad<lb>proaches, a large, stochastic decision problem is<lb>divided into smaller pieces. The first approach<lb>builds a cache of policies for each part of the<lb>problem independently, and then combines the<lb>pieces in a separate, light-weight step. A second<lb>approach also divides the problem into smaller<lb>pieces, but information is communicated be\u00ad<lb>tween the different problem pieces, allowing in\u00ad<lb>telligent decisions to be made about which piece<lb>requires the most attention. Both approaches can<lb>be used to find optimal policies or approximately<lb>optimal policies with provable bounds. These al\u00ad<lb>gorithms also provide a framework for the effi\u00ad<lb>cient transfer of knowledge across problems that<lb>share similar structure.", "creator": "pdftk 1.41 - www.pdftk.com"}}}