{"id": "1302.0723", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2013", "title": "Multi-Robot Informative Path Planning for Active Sensing of Environmental Phenomena: A Tale of Two Algorithms", "abstract": "A key problem of robotic environmental sensing and monitoring is that of active sensing: How can a team of robots plan the most informative observation paths to minimize the uncertainty in modeling and predicting an environmental phenomenon? This paper presents two principled approaches to efficient information-theoretic path planning based on entropy and mutual information criteria for in situ active sensing of an important broad class of widely-occurring environmental phenomena called anisotropic fields. Our proposed algorithms are novel in addressing a trade-off between active sensing performance and time efficiency. An important practical consequence is that our algorithms can exploit the spatial correlation structure of Gaussian process-based anisotropic fields to improve time efficiency while preserving near-optimal active sensing performance. We analyze the time complexity of our algorithms and prove analytically that they scale better than state-of-the-art algorithms with increasing planning horizon length. We provide theoretical guarantees on the active sensing performance of our algorithms for a class of exploration tasks called transect sampling, which, in particular, can be improved with longer planning time and/or lower spatial correlation along the transect. Empirical evaluation on real-world anisotropic field data shows that our algorithms can perform better or at least as well as the state-of-the-art algorithms while often incurring a few orders of magnitude less computational time, even when the field conditions are less favorable.", "histories": [["v1", "Mon, 4 Feb 2013 15:34:12 GMT  (598kb,D)", "https://arxiv.org/abs/1302.0723v1", "12th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2013), Extended version with proofs, 15 pages"], ["v2", "Tue, 5 Feb 2013 05:50:14 GMT  (598kb,D)", "http://arxiv.org/abs/1302.0723v2", "12th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2013), Extended version with proofs, 15 pages"]], "COMMENTS": "12th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2013), Extended version with proofs, 15 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.MA cs.RO", "authors": ["nannan cao", "kian hsiang low", "john m dolan"], "accepted": false, "id": "1302.0723"}, "pdf": {"name": "1302.0723.pdf", "metadata": {"source": "CRF", "title": "Multi-Robot Informative Path Planning for Active Sensing of Environmental Phenomena: A Tale of Two Algorithms", "authors": ["Nannan Cao", "Kian Hsiang Low", "John M. Dolan"], "emails": ["lowkh}@comp.nus.edu.sg", "jmd@cs.cmu.edu"], "sections": [{"heading": "Categories and Subject Descriptors", "text": "G.3 [Probability and Statistics]: Stochastic processes; I.2.9 [Robotics]: Autonomous vehicles"}, {"heading": "General Terms", "text": "Algorithms, Performance, Experimentation, Theory"}, {"heading": "Keywords", "text": "Multi-robot exploration and mapping, adaptive sampling, active learning, Gaussian process, non-myopic path planning"}, {"heading": "1. INTRODUCTION", "text": "Research in environmental sensing and monitoring has recently gained significant attention and practical interest, es-\nAppears in: Proceedings of the 12th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2013), Ito, Jonker, Gini, and Shehory (eds.), May, 6\u201310, 2013, Saint Paul, Minnesota, USA. Copyright c\u00a9 2012, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.\npecially in supporting environmental sustainability efforts worldwide. A key direction of this research aims at sensing, modeling, and predicting the various types of environmental phenomena spatially distributed over our natural and built-up habitats so as to improve our knowledge and understanding of their economic, environmental, and health impacts and implications. This is non-trivial to achieve due to a trade-off between the quantity of sensing resources (e.g., number of deployed sensors, energy consumption, mission time) and the uncertainty in predictive modeling. In the case of deploying a limited number of mobile robotic sensing assets, such a trade-off motivates the need to plan the most informative resource-constrained observation paths to minimize the uncertainty in modeling and predicting a spatially varying environmental phenomenon, which constitutes the active sensing problem to be addressed in this paper.\nA wide multitude of natural and urban environmental phenomena is characterized by spatially correlated field measurements, which raises the following fundamental issue faced by the active sensing problem:\nHow can the spatial correlation structure of an environmental phenomenon be exploited to improve the active sensing performance and computational efficiency of robotic path planning?\nThe works of [11, 12, 13] have tackled this issue specifically in the context of an environmental hotspot field by studying how its spatial correlation structure affects the performance advantage of adaptivity in path planning: If the field is large with a few small hotspots exhibiting extreme measurements and much higher spatial variability than the rest of the field, then adaptivity can provide better active sensing performance. On the other hand, non-adaptive sampling techniques [2, 8, 14] suffice for smoothly-varying fields.\nIn this paper, we will investigate the above issue for another important broad class of environmental phenomena called anisotropic fields that exhibit a (often much) higher spatial correlation along one direction than along its perpendicular direction. Such fields occur widely in natural and built-up environments and some of them include (a) ocean and freshwater phenomena like plankton density [6], fish abundance [23], temperature and salinity [22]; (b) soil and atmospheric phenomena like peat thickness [25], surface soil moisture [26], rainfall [18]; (c) mineral deposits like radioactive ore [19]; (d) pollutant and contaminant concentration like air [1], heavy metals [16]; and (e) ecological abundance like vegetation density [9].\nThe geostatistics community has examined a related issue of how the spatial correlation structure of an anisotropic field\nar X\niv :1\n30 2.\n07 23\nv2 [\ncs .L\nG ]\n5 F\neb 2\n01 3\ncan be exploited to improve the predictive performance of a sampling design for a static sensor network. To resolve this, the following heuristic design [25] is commonly used for sampling the anisotropic fields described above: Arrange and place the static sensors in a rectangular grid such that one axis of the grid is aligned along the direction of lowest spatial correlation (i.e., highest spatial variability) and the grid spacing along this axis as compared to that along its perpendicular axis is proportional to the ratio of their respective spatial correlations. In the case of path planning for k robots, one may consider the sampling locations of the rectangular grid as cities to be visited in a k-traveling salesman problem so as to minimize the total distance traveled or mission time [15]. However, since the resulting observation paths are constrained by the heuristic sampling design, they are suboptimal in solving the active sensing problem (i.e., minimizing the predictive uncertainty). This drawback is exacerbated when the robots are capable of sampling at a higher resolution along their paths (e.g., due to high sensor sampling rate) than that of the grid, hence gathering suboptimal observations while traversing between grid locations.\nThis paper presents two principled approaches to efficient information-theoretic path planning based on entropy and mutual information (respectively, Sections 3 and 4) criteria for in situ active sensing of environmental phenomena. In contrast to the existing methods described above, our proposed path planning algorithms are novel in addressing a trade-off between active sensing performance and computational efficiency. An important practical consequence is that our algorithms can exploit the spatial correlation structure of anisotropic fields to improve time efficiency while preserving near-optimal active sensing performance. The specific contributions of our work in this paper include: \u2022 Analyzing the time complexity of our proposed algorithms\nand proving analytically that they scale better than stateof-the-art information-theoretic path planning algorithms [8, 13] with increasing length of planning horizon (Sections 3 and 4); \u2022 Providing theoretical guarantees on the active sensing per-\nformance of our proposed algorithms (Sections 3 and 4) for a class of exploration tasks called the transect sampling task (Section 2.1), which, in particular, can be improved with longer planning time and/or lower spatial correlation along the transect; \u2022 Empirically evaluating the time efficiency and active sens-\ning performance of our proposed algorithms on real-world temperature and plankton density field data (Section 5)."}, {"heading": "2. BACKGROUND", "text": ""}, {"heading": "2.1 Transect Sampling Task", "text": "In a transect sampling task [14, 24], a team of k robots is tasked to explore and sample an environmental phenomenon spatially distributed over a transect (Fig. 1) that is discretized into a r \u00d7 n grid of sampling locations where the number n of columns is assumed to be much larger than the number r of sampling locations in each column, r is expected to be small in a transect, and k \u2264 r. The columns are indexed in an increasing order from left to right. The k robots are constrained to simultaneously explore forward one column at a time from the leftmost column \u20181\u2019 to the rightmost column \u2018n\u2019 such that each robot samples one location per column for a total of n locations. Hence, each robot, given its current location, can move to any of the r\nlocations in the adjacent column on its right. In practice, the transect sampling task is especially appropriate for and widely performed by mobile robots with limited maneuverability (e.g., unmanned aerial vehicles, autonomous surface and underwater vehicles (AUVs) [21]) because it involves less complex path maneuvers that can be achieved more reliably using less sophisticated on-board control algorithms. In terms of practical applicability, transect sampling is a particularly useful exploration task to be performed during the transit from the robot\u2019s current location to a distant planned waypoint [10, 24] to collect the most informative observations. For active sensing of ocean and freshwater phenomena, the transect can span a spatial feature of interest such as a harmful algal bloom or pollutant plume to be explored and sampled by a fleet of AUVs being deployed off a ship vessel."}, {"heading": "2.2 Gaussian Process-Based Anisotropic Field", "text": "An environmental phenomenon is defined to vary as a realization of a rich class of Bayesian non-parametric models called the Gaussian process (GP) [20] that can formally characterize its spatial correlation structure and be refined with increasing number of observations. More importantly, GP can provide formal measures of predictive uncertainty (e.g., based on an entropy or mutual information criterion) for directing the robots to explore the highly uncertain areas of the phenomenon.\nLet D be a set of sampling locations representing the domain of the environmental phenomenon such that each location x \u2208 D is associated with a realized (random) measurement zx (Zx) if x is sampled/observed (unobserved). Let {Zx}x\u2208D denote a GP, that is, every finite subset of {Zx}x\u2208D has a multivariate Gaussian distribution [20]. The GP is fully specified by its prior mean \u00b5x , E[Zx] and covariance \u03c3xx\u2032 , cov[Zx, Zx\u2032 ] for all x, x\n\u2032 \u2208 D. In the experiments (Section 5), we assume that the GP is second-order stationary, i.e., it has a constant prior mean and a stationary prior covariance structure (i.e., \u03c3xx\u2032 is a function of x\u2212 x\u2032 for all x, x\u2032 \u2208 D), both of which are assumed to be known. In particular, its covariance structure is defined by the widely-used squared exponential covariance function\n\u03c3xx\u2032 , \u03c3 2 s exp { \u22121\n2 (x\u2212 x\u2032)TM\u22122(x\u2212 x\u2032)\n} + \u03c32n\u03b4xx\u2032 (1)\nwhere \u03c32s and \u03c3 2 n are, respectively, the signal and noise variances controlling the intensity and noise of the measurements, M is a diagonal matrix with length-scale components `1 and `2 controlling the degree of spatial correlation or \u201csimilarity\u201d between measurements along (i.e., horizontal direction) and perpendicular to (i.e., vertical direction) the transect, respectively, and \u03b4xx\u2032 is a Kronecker delta of value 1 if x = x\u2032, and 0 otherwise. For anisotropic fields, `1 6= `2.\nAn advantage of using GP to model the environmental phenomenon is its probabilistic regression capability: Given\na vector s of sampled locations and a column vector zs of corresponding measurements, the joint distribution of the measurements at any vector u of \u03ba unobserved locations remains Gaussian with the following posterior mean vector and covariance matrix\n\u00b5u|s = \u00b5u + \u03a3us\u03a3 \u22121 ss (zs \u2212 \u00b5s) (2)\n\u03a3uu|s = \u03a3uu \u2212 \u03a3us\u03a3\u22121ss \u03a3su (3) where \u00b5u (\u00b5s) is a column vector with mean components \u00b5x for every location x of u (s), \u03a3us (\u03a3ss) is a covariance matrix with covariance components \u03c3xx\u2032 for every pair of locations x of u (s) and x\u2032 of s, and \u03a3su is the transpose of \u03a3us. The posterior mean vector \u00b5u|s (2) is used to predict the measurements at vector u of \u03ba unobserved locations. The uncertainty of these predictions can be quantified using the posterior covariance matrix \u03a3uu|s (3), which is independent of the measurements zs, in two ways: (a) the trace of \u03a3uu|s yields the sum of posterior variances \u03a3xx|s over every location x of u; (b) the determinant of \u03a3uu|s is used in calculating the Gaussian posterior joint entropy\nH(Zu|Zs) , 1\n2 log(2\u03c0e)\u03ba \u2223\u2223\u03a3uu|s\u2223\u2223 . (4) Unlike the first measure of predictive uncertainty which assumes conditional independence between measurements at vector u of unobserved locations, the entropy-based measure (4) accounts for their correlation, thereby not overestimating their uncertainty. Hence, we will focus on using the entropy-based measure of uncertainty in this paper."}, {"heading": "3. ENTROPY-BASED PATH PLANNING", "text": "Notations. Each planning stage i is associated with column i of the transect for i = 1, . . . , n. In each stage i, the team of k robots samples from column i a total of k observations (each of which comprises a pair of a location and its measurement) that are denoted by a pair of vectors xi of k locations and Zxi of the corresponding random measurements. Let Xi denote the set of all possible robots\u2019 sampling locations xi in stage i. It can be observed that \u03c7 , |X1| = . . . = |Xn| = rCk. We assume that the robots can deterministically (i.e., no stochasticity in motion) move from their current locations xi\u22121 in column i \u2212 1 to the next locations xi in column i. Let xi:j and Zxi:j denote vectors concatenating robots\u2019 sampling locations xi, . . . , xj and concatenating corresponding random measurements Zxi , . . . , Zxj over stages i to j, respectively, and Xi:j denote the set of all possible xi:j . Maximum Entropy Path Planning (MEPP). The work of [13] has proposed planning non-myopic observation paths x\u22171:n with maximum entropy (i.e., highest uncertainty):\nx\u22171:n = arg max x1:n\u2208X1:n H(Zx1:n) (5)\nthat, as proven in an equivalence result, minimize the posterior entropy/uncertainty remaining in the unobserved locations of the transect. Computing the maximum entropy paths x\u22171:n incurs O ( \u03c7n(kn)3 ) , which is exponential in the length n of planning horizon. To mitigate this computational difficulty, an anytime heuristic search algorithm [7] is used to compute (5) approximately. However, its performance cannot be guaranteed. Furthermore, as reported in [14], when \u03c7 or n is large, its computed paths perform poorly even after incurring a huge amount of search time and space.\nApproximate MEPP(m). To establish a trade-off between active sensing performance and computational effi-\nciency, the key idea is to exploit a property of the covariance function (1) that the spatial correlation of measurements between any two locations decreases exponentially with increasing distance between them. Intuitively, such a property makes the measurements Zxi to be observed next in column i near-independent of the past distant measurements Zx1:i\u2212m\u22121 observed from columns 1 to i\u2212m\u22121 (i.e., far from column i) for a sufficiently large m by conditioning on the closer measurements Zxi\u2212m:i\u22121 observed in columns i\u2212m to i\u22121 (i.e., closer to column i). Consequently, H(Zxi |Zx1:i\u22121) can still be closely approximated by H(Zxi |Zxi\u2212m:i\u22121) after assuming a m-th order Markov property, thus yielding the following approximation of the joint entropy H(Zx1:n) in (5):\nH(Zx1:n) = H(Zx1:m) + \u2211n i=m+1 H(Zxi |Zx1:i\u22121)\n\u2248 H(Zx1:m) + \u2211n i=m+1 H(Zxi |Zxi\u2212m:i\u22121) . (6)\nThe first equality is due to the chain rule for entropy [3]. Using (6), MEPP (5) can be approximated by the following stage-wise dynamic programming equations, which we call MEPP(m):\nVi(xi\u2212m:i\u22121) = max xi\u2208Xi H(Zxi |Zxi\u2212m:i\u22121) + Vi+1(xi\u2212m+1:i) Vn(xn\u2212m:n\u22121) = max\nxn\u2208Xn H(Zxn |Zxn\u2212m:n\u22121)\n(7) for stage i = m+1, . . . , n\u22121, each of which induces a corresponding optimal vector xEi of k locations given the optimal vector xEi\u2212m:i\u22121 obtained from previous stages i\u2212m to i\u221211. Let the optimal observation paths of MEPP(m) be denoted by xE1:n that concatenates\nxE1:m = arg max x1:m\u2208X1:m H(Zx1:m) + Vm+1(x1:m) (8)\nfor the first m stages and xEm+1, . . . , x E n derived using (7) for the subsequent stages m+ 1 to n. Our proposed MEPP(m) algorithm generalizes that of [14] which is essentially MEPP(1).\nTheorem 1 (Time Complexity). Deriving xE1:n of MEPP(m) requires O ( \u03c7m+1[n+ (km)3] ) time.\nThe proof of Theorem 1 is given in Appendix A.1. Unlike MEPP which scales exponentially in the planning horizon length n, our MEPP(m) algorithm scales linearly in n.\nLet \u03c91 and \u03c92 be the horizontal and vertical separation widths between adjacent grid locations, respectively, `\u20321 , `1/\u03c91 and ` \u2032 2 , `2/\u03c92 denote the normalized horizontal and vertical length-scale components, respectively, and \u03b7 , \u03c32n/\u03c3 2 s . The following result bounds the loss in active sensing performance of the MEPP(m) algorithm (i.e., (7) and (8)) relative to that of MEPP (5):\nTheorem 2 (Performance Guarantee). The paths xE1:n are -optimal in achieving the maximum entropy criterion, i.e., H(Zx\u22171:n)\u2212H(ZxE1:n) \u2264 where\n, [k(n\u2212m)]2 log { 1 + exp { \u2212(m+ 1)2/(2`\u203221 ) }2 \u03b7(1 + \u03b7) } .\nThe proof of Theorem 2 is given in Appendix A.3. Theorem 2 reveals that the active sensing performance of MEPP(m)\n1In fact, solving MEPP(m) (7) yields a policy that, in each stage i, induces an optimal vector for every possible vector xi\u2212m:i\u22121 (including possible diverged paths from x E i\u2212m:i\u22121 due to external forces) obtained from previous m stages.\ncan be improved by decreasing , which is achieved using higher noise-to-signal ratio \u03b7 (i.e., noisy, less intense fields), smaller number k of robots, shorter planning horizon length n, larger m, and/or lower spatial correlation `\u20321 along the transect. Two important implications result: (a) Increasing m trades off computational efficiency (Theorem 1) for better active sensing performance, and (b) if the spatial correlation of the anisotropic field along the transect is sufficiently low to maintain a relatively tight bound such that only a small m is needed, then MEPP(m) can exploit this spatial correlation structure to gain time efficiency while preserving near-optimal active sensing performance. In practice, it is often possible to obtain prior knowledge on a direction of low spatial correlation (refer to ocean and freshwater phenomena in Section 1 for examples) and align it with the horizontal axis of the transect."}, {"heading": "4. MUTUAL INFORMATION-BASED PATH PLANNING", "text": "Notations. Recall that the team of k robots selects k locations xi to be sampled from column i of the transect for i = 1, . . . , n. Let ui denote a vector of remaining r \u2212 k unobserved locations in column i and Zui denote a vector of the corresponding random measurements. Let ui:j and Zui:j denote vectors concatenating remaining unobserved locations ui, . . . , uj and concatenating corresponding random measurements Zui , . . . , Zuj over stages i to j, respectively. Maximum Mutual Information Path Planning (M2IPP). An alternative to MEPP is to plan non-myopic observation paths x?1:n that share the maximum mutual information with the remaining unobserved locations u?1:n of the transect:\nx?1:n = arg max x1:n\u2208X1:n I(Zx1:n ;Zu1:n)\nI(Zx1:n ;Zu1:n) , H(Zu1:n)\u2212H(Zu1:n |Zx1:n) . (9)\nFrom (9), I(Zx1:n ;Zu1:n) measures the reduction in entropy/ uncertainty of the measurements Zu1:n at the remaining unobserved locations u1:n of the transect by observing the measurements Zx1:n to be sampled along the paths x1:n. So, the path planning of M2IPP (9) is equivalent to the selection of remaining unobserved locations with the largest entropy reduction (i.e., determining u?1:n). This may be mistakenly perceived as the selection of remaining unobserved locations with the lowest uncertainty (i.e., minimizing posterior entropy term H(Zu1:n |Zx1:n) in (9)), which is exactly what the path planning of MEPP (5) can achieve, as mentioned in Section 3. Note, however, that the maximum mutual information paths (9) planned by M2IPP can in fact induce a very large prior entropy H(Zu1:n) but not necessarily the smallest posterior entropy H(Zu1:n |Zx1:n). Consequently, MEPP and M2IPP exhibit different path planning behaviors and resulting active sensing performances, as shown empirically in Section 5.\nSimilar to MEPP, M2IPP incurs exponential time in the length of planning horizon. To relieve this computational burden, we will describe an approximation algorithm for planning maximum mutual information paths next.\nApproximate M2IPP(m). We will exploit the same property of the covariance function (1) as that used by MEPP(m) (Section 3) to establish a trade-off between active sensing performance and computational efficiency for our M2IPP(m) algorithm. However, this is not as straightforward to achieve as that to derive MEPP(m) where a m-th order Markov\nproperty can simply be imposed on each posterior entropy term in (6). To illustrate this, using the chain rule for mutual information [3],\nI(Zx1:n ;Zu1:n) = I(Zx1:m ;Zu1:n) + n\u2212m\u22121\u2211 i=m+1 I(Zxi ;Zu1:n |Zx1:i\u22121) + I(Zxn\u2212m:n ;Zu1:n |Zx1:n\u2212m\u22121) , after which a m-th order Markov property is assumed to yield the following approximation:\nI(Zx1:n ;Zu1:n) \u2248 I(Zx1:m ;Zu1:n) + n\u2212m\u22121\u2211 i=m+1 I(Zxi ;Zu1:n |Zxi\u2212m:i\u22121)\n+ I(Zxn\u2212m:n ;Zu1:n |Zxn\u22122m:n\u2212m\u22121) . (10)\nFrom (10), note that each conditional mutual information term I(Zxi ;Zu1:n |Zxi\u2212m:i\u22121) cannot be evaluated individually because the remaining unobserved locations u1:n of the transect (specifically, u1:i\u2212m\u22121 and ui+1:n in the respective columns 1 to i\u2212m\u22121 and i+ 1 to n) cannot be determined simply by knowing the robots\u2019 past and current sampling locations xi\u2212m:i\u22121 and xi in columns i\u2212m to i.\nTo resolve this, we exploit the same property of the covariance function (1) as that used by MEPP(m) (Section 3) again: It makes the measurements Zxi to be observed next in column i near-independent of the distant unobserved measurements Zu1:i\u2212m\u22121 and Zui+m+1:n in the respective columns 1 to i \u2212 m \u2212 1 and i + m + 1 to n (i.e., far from column i) for a sufficiently large m by conditioning on the closer measurements Zxi\u2212m:i\u22121 and Zui\u2212m:i+m in columns i \u2212 m to i + m (i.e., closer to column i). As a result, each term I(Zxi ;Zu1:n |Zxi\u2212m:i\u22121) in (10) can be closely approximated by I(Zxi ;Zui\u2212m:i+m |Zxi\u2212m:i\u22121) for i = m+1, . . . , n\u2212m\u22121:\nI(Zxi ;Zu1:n |Zxi\u2212m:i\u22121) = H(Zxi |Zxi\u2212m:i\u22121)\u2212H(Zxi |Zxi\u2212m:i\u22121 , Zu1:n) \u2248 H(Zxi |Zxi\u2212m:i\u22121)\u2212H(Zxi |Zxi\u2212m:i\u22121 , Zui\u2212m:i+m) = I(Zxi ;Zui\u2212m:i+m |Zxi\u2212m:i\u22121)\nwhere the approximation follows from the above-mentioned conditional independence assumption and the equalities are due to the definition of conditional mutual information [3]. Similarly, I(Zx1:m ;Zu1:n) and I(Zxn\u2212m:n ;Zu1:n |Zxn\u22122m:n\u2212m\u22121) in (10) are, respectively, approximated by I(Zx1:m ;Zu1:2m) and I(Zxn\u2212m:n ;Zun\u22122m:n |Zxn\u22122m:n\u2212m\u22121). Then,\nI(Zx1:n ;Zu1:n) \u2248 I(Zx1:m ;Zu1:2m)\n+ n\u2212m\u22121\u2211 i=m+1 I(Zxi ;Zui\u2212m:i+m |Zxi\u2212m:i\u22121) + I(Zxn\u2212m:n ;Zun\u22122m:n |Zxn\u22122m:n\u2212m\u22121)\n= I(Zx1:m ;Zu1:2m) + n\u22121\u2211 i=2m+1 I(Zxi\u2212m ;Zui\u22122m:i |Zxi\u22122m:i\u2212m\u22121)\n+ I(Zxn\u2212m:n ;Zun\u22122m:n |Zxn\u22122m:n\u2212m\u22121) . (11) Using (11), M2IPP (9) can be approximated by the following stage-wise dynamic programming equations, which we call M2IPP(m):\nUi(xi\u22122m:i\u22121) = max xi\u2208Xi I(Zxi\u2212m ;Zui\u22122m:i |Zxi\u22122m:i\u2212m\u22121) + Ui+1(xi\u22122m+1:i)\nUn(xn\u22122m:n\u22121) = max xn\u2208Xn I(Zxn\u2212m:n ;Zun\u22122m:n |Zxn\u22122m:n\u2212m\u22121) (12) for stage i = 2m+1, . . . , n\u22121, each of which induces a corresponding optimal vector xMi of k locations given the optimal vector xMi\u22122m:i\u22121 obtained from previous stages i \u2212 2m to\ni\u221212. Note that the term I(Zxi\u2212m ;Zui\u22122m:i |Zxi\u22122m:i\u2212m\u22121) in each stage i can be evaluated now because the remaining unobserved locations ui\u22122m:i in columns i\u2212 2m to i can be determined since the robots\u2019 past and current sampling locations xi\u22122m:i\u22121 and xi in the same columns are given (i.e., as input to Ui and under the max operator, respectively). Let the optimal observation paths of M2IPP(m) be denoted by xM1:n that concatenates\nxM1:2m = arg max x1:2m\u2208X1:2m I(Zx1:m , Zu1:2m) + U2m+1(x1:2m) (13) for the first 2m stages and xM2m+1, . . . , x M n derived using (12) for the subsequent stages 2m+ 1 to n.\nTheorem 3 (Time Complexity). Deriving xM1:n of M2IPP(m) requires O ( \u03c72m+1[n+ 2(r(2m+ 1))3] ) time.\nThe proof of Theorem 3 is given in Appendix B.1. Unlike M2IPP that scales exponentially in the planning horizon length n, our M2IPP(m) algorithm scales linearly in n.\nThe following result bounds the loss in active sensing performance of the M2IPP(m) algorithm (i.e., (12) and (13)) relative to that of M2IPP (9):\nTheorem 4 (Performance Guarantee). The paths xM1:n are \u03b5-optimal in achieving the maximum mutual information criterion, i.e., I(Zx?1:n ;Zu?1:n) \u2212 I(ZxM1:n ;ZuM1:n) \u2264 \u03b5 where\n\u03b5 , k(n\u22122m) [ rn+ 1\n2 k(n\u2212 2m)\n] log 1+ exp { \u2212 (m+1) 2 2`\u203221 }2 \u03b7(1 + \u03b7) . The proof of Theorem 4 is given in Appendix B.3. As shown in Theorem 4, decreasing \u03b5 improves the active sensing performance of M2IPP(m); this can be achieved in a similar manner to that for decreasing the loss bound of MEPP(m) (see paragraph after Theorem 2) since the two loss bounds \u03b5 and are similar. In addition, smaller number r of sampling locations in each column decreases \u03b5. M2IPP(m) shares the same implications as that of MEPP(m): (a) Increasing m trades off time efficiency (Theorem 3) for improved active sensing performance, and (b) M2IPP(m) can exploit a low spatial correlation `\u20321 of the anisotropic field along the transect to improve time efficiency (i.e., only requiring a small m) while preserving near-optimal active sensing performance (i.e., still maintaining a relatively tight bound \u03b5)."}, {"heading": "5. EXPERIMENTS AND DISCUSSION", "text": "This section evaluates the active sensing performance and computational efficiency of the MEPP(m) (i.e., (7) and (8)) and M2IPP(m) (i.e., (12) and (13)) algorithms empirically on two real-world datasets: (a) May 2009 temperature field data of Panther Hollow Lake in Pittsburgh, PA spatially distributed over a 25 m by 150 m transect that is discretized into a 5\u00d7 30 grid [17], and (b) June 2009 plankton density field data of Chesapeake Bay spatially distributed over a 314 m by 1765 m transect that is discretized into a 8\u00d745 grid [5]. These environmental phenomena are modeled by GPs with hyperparameters (i.e., horizontal and vertical lengthscales, signal and noise variances) (Section 2.2) learned using maximum likelihood estimation (MLE) [20]: (a) `1 = 2Similar to MEPP(m), solving M2IPP(m) (12) yields a policy that, in each stage i, induces an optimal vector for every possible vector xi\u22122m:i\u22121 (including possible diverged paths from xEi\u22122m:i\u22121) obtained from previous 2m stages.\n40.45 m, `2 = 16.00 m, \u03c3 2 s = 0.1542, and \u03c3 2 n = 0.0036 for the temperature field, and (b) `1 = 27.53 m, `2 = 134.64 m, \u03c32s = 2.152, and \u03c3 2 n = 0.041 for the plankton density field. It can be observed that the temperature and plankton density fields have low noise-to-signal ratios \u03b7 of 0.023 and 0.019, respectively. Also, though both fields are observed to be highly anisotropic, the spatial correlation of the temperature field is much higher along the transect than perpendicular to it. According to Theorems 2 and 4, such field conditions lead to loose performance loss bounds for both algorithms, which does not necessarily imply their poor performance. So, the empirical evaluation here complements our theoretical results by assessing their performance-efficiency trade-off (i.e., by varying m) under these less favorable field conditions. To further investigate our algorithms\u2019 trade-off behaviors under different horizontal and vertical spatial correlations, the corresponding length-scales `1 and `2 of the original temperature field (Fig. 2d) are reduced and fixed to produce three other modified fields (Figs. 2a, 2b, 2c) with the signal and noise variances \u03c32s and \u03c3 2 n learned using MLE.\nComparison with Active Sensing Algorithms. The performance of our proposed algorithms is compared to that of state-of-the-art information-theoretic path planning algorithms for active sensing: The work of [13] has proposed the following greedy maximum entropy path planning (gMEPP) algorithm:\nV gi (x1:i\u22121) = max xi\u2208Xi H(Zxi |Zx1:i\u22121) (14)\nfor stage i = 1, . . . , n, each of which induces a corresponding optimal vector xEi of k locations given the optimal vector xE1:i\u22121 obtained from previous stages 1 to i \u2212 1. A greedy maximum mutual information path planning (gM2IPP) algorithm is devised by [8] as follows:\nUgi (x1:i\u22121) = max xi\u2208Xi I(Zx1:i ;Zx1:i) (15)\nfor stage i = 1, . . . , n, each of which induces a corresponding optimal vector xMi of k locations given the optimal vector xM1:i\u22121 obtained from previous stages 1 to i \u2212 1, and x1:i denotes a vector of all sampling locations in the domain D excluding those of x1:i. As mentioned earlier in Section 3, the work of [14] has developed MEPP(1), which is a special case of our MEPP(m) algorithm.\nIn contrast to our MEPP(m) and M2IPP(m) algorithms that scale linearly in the length n of planning horizon (Theorems 1 and 3), deriving xE1:n of gMEPP and x M 1:n of gM\n2IPP incurs quartic time in n. Hence, if the required value of m is sufficiently small, then MEPP(m) and M2IPP(m) can be more efficient than the greedy algorithms, as shown below.\nPerformance Metrics. The tested algorithms are evaluated using three different metrics: The (a) entropy metric EN(x1:n) , H(Zu1:n |Zx1:n) and (b) mutual information metric MI(x1:n) , I(Zx1:n ;Zu1:n) measure, respectively, the\nTable 1: Comparison of EN(x1:n), MI(x1:n), and ER(x1:n) (\u00d710\u22125) performance for different temperature fields shown in Fig. 2 with varying number of robots. For our proposed M2IPP(m) and MEPP(m) algorithms, every performance result is preceded by the value of m (in round brackets) used.\nEN(x1:n) MI(x1:n) ER(x1:n)\n1 robot Field Field Field\nAlgorithm a b c d a b c d a b c d\ngM2IPP: xM1:n[8] -64.4 -123.9 -173.3 -182.2 27.9 48.4 46.0 39.5 1.764 0.581 0.088 0.042 gMEPP: xE1:n[13] -64.8 -128.4 -173.3 -182.4 26.5 44.7 46.0 39.5 2.792 0.572 0.077 0.037 M2IPP(m): xM1:n (1) -64.5 (1) -123.9 (1) -167.2 (1) -182.0 (1) 27.9 (1) 48.4 (1) 39.6 (1) 39.4 (1) 1.764 (1) 0.581 (1) 0.488 (1) 0.049\n(2) -173.2 (2) 45.8 (2) 0.110 (2) 0.042 (3) 0.034\nMEPP(m): xE1:n (1) -64.8 (1) -128.4 (1) -161.2 (1) -180.4 (1) 23.9 (1) 44.7 (1) 33.2 (1) 36.9 (1) 5.115 (1) 0.572 (1) 3.765 (1) 0.757 (2) -64.9 (2) -167.2 (2) -182.4 (2) 26.3 (2) 39.6 (2) 39.5 (2) 2.315 (2) 0.501 (2) 0.026\n(3) -171.6 (3) 44.2 (3) 2.080 (3) 0.241 (4) -173.4 (4) 46.1 (4) 0.068\n2 robots Field Field Field\nAlgorithm a b c d a b c d a b c d\ngM2IPP: xM1:n[8] -57.8 -100.5 -132.9 -138.0 41.7 62.0 45.8 36.9 1.153 0.265 0.019 0.016 gMEPP: xE1:n[13] -59.8 -112.2 -132.9 -138.8 41.2 55.8 45.9 36.2 0.521 0.439 0.033 0.018 M2IPP(m): xM1:n (1) -57.8 (1) -100.5 (1) -132.9 (1) -138.2 (1) 41.2 (1) 62.0 (1) 45.9 (1) 36.9 (1) 0.605 (1) 0.265 (1) 0.020 (1) 0.018\n(2) 41.8 (2) 0.014\nMEPP(m): xE1:n (1) -59.8 (1) -113.0 (1) -129.3 (1) -138.4 (1) 41.6 (1) 56.4 (1) 41.8 (1) 36.9 (1) 0.662 (1) 0.378 (1) 0.286 (1) 0.012 (2) -60.0 (2) -132.9 (2) 45.9 (2) 0.018\n3 robots Field Field Field\nAlgorithm a b c d a b c d a b c d\ngM2IPP: xM1:n[8] -46.5 -80.5 -89.5 -92.8 40.8 61.3 41.4 31.6 0.272 0.012 0.018 0.008 gMEPP: xE1:n[13] -46.3 -80.6 -89.5 -93.2 40.5 60.6 41.3 28.6 0.257 0.024 0.017 0.009 M2IPP(m): xM1:n (1) -46.5 (1) -72.0 (1) -89.4 (1) -92.1 (1) 40.8 (1) 60.0 (1) 38.8 (1) 32.0 (1) 0.272 (1) 0.123 (1) 0.016 (1) 0.008\n(2) -89.5 (2) 41.3 (2) 0.229 (2) 0.014\nMEPP(m): xE1:n (1) -45.9 (1) -81.3 (1) -89.4 (1) -93.5 (1) 40.2 (1) 61.6 (1) 38.7 (1) 28.2 (1) 0.231 (1) 0.014 (1) 0.013 (1) 0.007 (2) -46.5 (2) 40.8 (4) 41.1 (3) 28.6\n(4) 29.0\nposterior entropy/uncertainty and the reduction in entropy/ uncertainty at the remaining unobserved locations u1:n of the transect given the observation paths x1:n. The difference between the entropy and mutual information metrics has been explained in the paragraph after (9) in Section 4.\nThe (c) ER(x1:n) , ||zu1:n \u2212 \u00b5u1:n|x1:n || 2 2/{\u00b52n(r \u2212 k)} metric measures the mean-squared relative prediction error resulting from using the posterior mean \u00b5u|x1:n (2) to predict the measurements at the remaining n(r\u2212k) unobserved locations u1:n of the transect given the measurements sampled along the observation paths x1:n where \u00b5 = 1\n>zu1:n/ {n(r \u2212 k)}. It has an advantage over the two informationtheoretic metrics of using ground truth measurements to evaluate if the phenomenon is being predicted accurately. However, unlike the EN(x1:n) and MI(x1:n) metrics that account for the spatial correlation between measurements at the unobserved locations u1:n, the ER(x1:n) metric assumes conditional independence between them. In contrast to the ER(x1:n) metric, the EN(x1:n) and MI(x1:n) metrics consequently do not overestimate their uncertainty."}, {"heading": "5.1 Temperature Field Data", "text": "Table 1 shows the results of EN(x1:n), MI(x1:n), and ER(x1:n) performance of tested algorithms for temperature fields with different horizontal and vertical length-scales (Fig. 2) and with varying number of robots. For our proposed M2IPP(m) and MEPP(m) algorithms, the results are reported in an increasing order of m until the performance has stabilized. It can be observed from Table 1 that MEPP(m) with m > 1 or M2IPP(m) often outperforms MEPP(1) [14] in the three metrics, as discussed and explained later. Note that every increment of m increases the length of history of sampling locations considered in each stage by two for M2IPP(m) instead of by one for MEPP(m); this can be seen from the inputs to Ui (12) and Vi (7), respectively. The observations of the results are detailed in the rest of this subsection.\n5.1.1 Entropy Metric EN(x1:n) As expected, the entropy-based MEPP(m) and gMEPP\nalgorithms generally perform better than or at least as well\n1 2 3 4 5 6 7\n10 \u22121\n10 0\n10 1\n10 2\n10 3\n10 4\nm\nT im\ne (s\n)\n1 2 3 4 5 10\n\u22121\n10 0\n10 1\n10 2\n10 3\n10 4\nm\nT im\ne( s)\ngMEPP gM2IPP MEPP(m) M2IPP(m)\n1 2 3 4 5\n10 \u22121\n10 0\n10 1\n10 2\n10 3\n10 4\nm\nT im\ne (s\n)\n(a) 1 robot. (b) 2 robots. (c) 3 robots.\nFigure 3: Graphs of incurred time by different active sensing algorithms vs. m for temperature fields with varying number of robots.\nas the mutual information-based M2IPP(m) and gM2IPP algorithms in this metric.\nFor fields a, b, and d (i.e., of small `1 or large `2) with any number of robots, MEPP(m) can produce EN(xE1:n) values lower than or comparable to that achieved by gMEPP and gM2IPP using small values of m (i.e., m = 1 or 2), hence incurring 1 to 4 orders of magnitude less computational time, as shown in Fig. 3. This can be explained by one of the following reasons: (a) A low spatial correlation along the transect cannot be exploited by gMEPP and gM2IPP, which consider the entire history of past measurements for improving active sensing performance; (b) a high correlation perpendicular to the transect can be exploited by MEPP(m) for better active sensing performance; and (c) unlike the greedy gMEPP and gM2IPP algorithms, MEPP(m) is capable of non-myopic planning to improve active sensing performance.\nFor field c (i.e., of large `1 and small `2) with 1 robot, MEPP(m) cannot exploit the low spatial correlation perpendicular to the transect for improving active sensing performance. Therefore, it needs to raise the value of m up to 4 in order to better exploit the high spatial correlation along the transect. Consequently, MEPP(m) can achieve EN(xE1:n) performance comparable to that achieved by gMEPP and gM2IPP while incurring similar computational time as gMEPP and about 2 orders of magnitude less time than gM2IPP. Increasing the number of robots allows MEPP(m) to achieve EN(xE1:n) performance comparable to that of gMEPP and gM2IPP using smaller values of m (i.e., m = 1 or 2), hence incurring 1 to 4 orders of magnitude less time.\n5.1.2 Mutual Information Metric MI(x1:n) The mutual information-based M2IPP(m) and gM2IPP\nalgorithms often perform better than or at least as well as the entropy-based MEPP(m) and gMEPP in this metric.\nFor fields a, b, and d (i.e., of small `1 or large `2) with any number of robots, M2IPP(m) can generally yield MI(xM1:n) values higher than or comparable to that achieved by gM2IPP and gMEPP using a small m value of 1, hence incurring less computational time (in particular, about 2 orders of magnitude less time than gM2IPP), as shown in Fig. 3. This can be explained by the same reasons as that discussed previously in Section 5.1.1.\nFor field c (i.e., of large `1 and small `2) with 1 or 3 robots, M2IPP(m) cannot exploit the low spatial correlation perpendicular to the transect for improving active sensing performance. So, it has to increase the value of m to 2 in order to better exploit the high correlation along the transect. As a result, M2IPP(m) can achieve MI(xM1:n) performance comparable to that achieved by gM2IPP and gMEPP while incurring less time with 1 robot and slightly more time with 3 robots than gM2IPP. With 2 robots, m = 1 suffices for M2IPP(m) to achieve MI(xM1:n) performance comparable to that achieved by gM2IPP and gMEPP while incurring less time (Fig. 3). A computationally cheaper alternative for active sensing of field c is to consider using MEPP(m) with larger m: When the values of m are raised to 4, 2, and 4 for the respective 1-, 2-, and 3-robot cases, it can produce MI(xE1:n) performance comparable to that achieved by gM2IPP and gMEPP while incurring similar or less time.\n5.1.3 Prediction Error Metric ER(x1:n) For field c (i.e., of large `1 and small `2) with any num-\nber of robots, MEPP(m) and M2IPP(m) cannot exploit the low spatial correlation perpendicular to the transect for improving active sensing performance. Hence, their values of m need to be raised in order to exploit the high correlation along the transect. Compared to M2IPP(m), it is computationally cheaper (Fig. 3) and offers greater performance improvement (Table 1) to increase the value of m of MEPP(m), which can then produce ER(xE1:n) values lower than that achieved by gMEPP and gM2IPP while incurring similar computational time to gMEPP and about 2 orders of magnitude less time than gM2IPP with 1 robot and 1 to 4 orders of magnitude less time than both with 2 or 3 robots. For field d (i.e., of large `1 and large `2) with any number of robots, MEPP(m) can now exploit the high spatial correlation perpendicular to the transect for better active sensing performance. As a result, MEPP(m) can yield better ER(xE1:n) performance than gMEPP and gM\n2IPP using smaller values of m (i.e., m = 1 or 2), hence incurring 1 to 4 orders of magnitude less time.\nFor fields a and b (i.e., of small `1) with 1 or 2 robots, M2IPP(m) can produce ER(xM1:n) values lower than or comparable to that achieved by gM2IPP and gMEPP using a small m value of 1, hence incurring less time (in particular, about 2 orders of magnitude less time than gM2IPP),\nas shown in Fig. 3. Increasing to 3 robots allows MEPP(m) to achieve ER(xE1:n) performance better than or comparable to that of gMEPP and gM2IPP using a small m value of 1, hence incurring 3 to 4 orders of magnitude less time (Fig. 3). These can be explained by the same reasons as that discussed previously in Section 5.1.1."}, {"heading": "5.2 Plankton Density Field Data", "text": "Table 2 shows the results of EN(x1:n), MI(x1:n), and ER(x1:n) performance of tested algorithms for the plankton density field (Fig. 4) with varying number of robots. For our proposed M2IPP(m) and MEPP(m) algorithms, the results are only reported for m = 1, at which their performance has already stabilized. As mentioned earlier in the first paragraph of Section 5, the plankton density field exhibits low and high spatial correlations, respectively, along and perpendicular to the transect, which resemble that of temperature field b.\nThe observations are as follows: With any number of robots, MEPP(1) can produce EN(xE1:n) values lower than that achieved by gMEPP and gM2IPP while incurring 2 to 5 orders of magnitude less time, as shown in Fig. 5. On the other hand, M2IPP(1) can yield MI(xM1:n) and ER(x M 1:n) performance better than or comparable to that achieved by gM2IPP and gMEPP while incurring less time (in particular, about 2 orders of magnitude less time than gM2IPP) (Fig. 5). These can be explained by the same reasons as that discussed previously in Section 5.1.1."}, {"heading": "5.3 Summary of Test Results", "text": "The observations of the above results are summarized below: For anisotropic fields with low spatial correlation along the transect (e.g., temperature fields a and b and plankton density field), MEPP(m) can perform better or at least as well as gMEPP and gM2IPP in the prediction error (i.e., with 3 robots) and entropy metrics using small m values of 1 or 2, hence incurring 1 to 4 orders of magnitude less time. M2IPP(m) can generally perform likewise in the prediction error (i.e., with 1 or 2 robots) and mutual information metrics using a small m value of 1, hence incurring less time as well (in particular, 2 orders of magnitude less time than gM2IPP). These observations are previously explained in Section 5.1.1. Note that they corroborate the second implications of Theorems 2 and 4 on the performance guarantees of MEPP(m) and M2IPP(m).\nFor anisotropic fields with high spatial correlation along the transect (e.g., temperature fields c and d), a larger m value is needed in order for MEPP(m) and M2IPP(m) to exploit it if the correlation perpendicular to the transect is low (i.e., field c). Compared to M2IPP(m), it is computationally cheaper to increase the value of m of MEPP(m) such that it performs better or at least as well as gMEPP and gM2IPP in all three metrics while incurring similar time to gMEPP and about 2 orders of magnitude less time than gM2IPP with 1 robot and often 1 to 4 orders of magnitude less time than both with 2 or 3 robots. If the correlation perpendicular to\nthe transect is high (i.e., field d) instead, it can be exploited by MEPP(m) and M2IPP(m) to improve active sensing performance and consequently allow m to be reduced to small values of 1 or 2: MEPP(m) can perform better or, if not, at least as well as gMEPP and gM2IPP in the prediction error and entropy metrics while incurring 1 to 4 orders of magnitude less time. M2IPP(m) can perform likewise in the mutual information metric while incurring less time (in particular, 2 orders of magnitude less time than gM2IPP)."}, {"heading": "6. CONCLUSION", "text": "This paper describes two principled information-theoretic path planning algorithms based on entropy and mutual information criteria (respectively, MEPP(m) and M2IPP(m)) for active sensing of GP-based anisotropic fields. Two important practical implications result from the theoretical guarantees on the active sensing performance of our algorithms (Theorems 2 and 4): Increasing m trades off computational efficiency (Theorems 1 and 3) for better active sensing performance, and our algorithms can exploit a low spatial correlation along the transect to improve time efficiency (i.e., only needing a small m) while preserving nearoptimal active sensing performance. This motivates the use of prior knowledge, if available, on a direction of low spatial correlation in order to align it with the horizontal axis of the transect. Empirical evaluation of real-world anisotropic temperature and plankton density field data reveals that our algorithms can perform better or at least as well as gMEPP and gM2IPP while often incurring a few orders of magnitude less time. In particular, it can be observed that anisotropic fields with low spatial correlation along the transect or high correlation perpendicular to the transect allow our algorithms to perform well using small values of m, thus yielding significant computational gain over gMEPP and gM2IPP. To perform well in a field with high correlation along the transect and low correlation perpendicular to the transect (i.e., less favorable conditions), our algorithms have to increase the value of m or the number of robots but can still achieve comparable or better time efficiency than gMEPP and gM2IPP."}, {"heading": "7. REFERENCES", "text": "[1] J. B. Boisvert and C. V. Deutsch. Modeling locally\nvarying anisotropy of CO2 emissions in the United States. Stoch. Environ. Res. Risk Assess., 25:1077\u20131084, 2011.\n[2] J. Chen, K. H. Low, C. K.-Y. Tan, A. Oran, P. Jaillet, J. M. Dolan, and G. S. Sukhatme. Decentralized data fusion and active sensing with mobile sensors for modeling and predicting spatiotemporal traffic phenomena. In Proc. UAI, pages 163\u2013173, 2012.\n[3] T. Cover and J. Thomas. Elements of Information Theory. John Wiley & Sons, NY, 1991.\n[4] A. Das and D. Kempe. Algorithms for subset selection in linear regression. In Proc. STOC, pages 45\u201354, 2008.\n[5] J. M. Dolan, G. Podnar, S. Stancliff, K. H. Low, A. Elfes, J. Higinbotham, J. C. Hosler, T. A. Moisan, and J. Moisan. Cooperative aquatic sensing using the telesupervised adaptive ocean sensor fleet. In Proc. SPIE Conference on Remote Sensing of the Ocean, Sea Ice, and Large Water Regions, volume 7473, 2009.\n[6] D. Kitsiou, G. Tsirtsis, and M. Karydis. Developing an optimal sampling design: A case study in a coastal marine ecosystem. Environmental Monitoring and Assessment, 71(1):1\u201312, 2001.\n[7] R. Korf. Real-time heuristic search. Artif. Intell., 42(2-3):189\u2013211, 1990.\n[8] A. Krause, A. Singh, and C. Guestrin. Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies. JMLR, 9:235\u2013284, 2008.\n[9] P. Legendre and M.-J. Fortin. Spatial pattern and ecological analysis. Vegetatio, 80:107\u2013138, 1989.\n[10] N. E. Leonard, D. Paley, F. Lekien, R. Sepulchre, D. M. Fratantoni, and R. Davis. Collective motion, sensor networks and ocean sampling. Proc. IEEE, 95(1):48\u201374, 2007.\n[11] K. H. Low, J. Chen, J. M. Dolan, S. Chien, and D. R. Thompson. Decentralized active robotic exploration and mapping for probabilistic field classification in environmental sensing. In Proc. AAMAS, pages 105\u2013112, 2012.\n[12] K. H. Low, J. M. Dolan, and P. Khosla. Adaptive multi-robot wide-area exploration and mapping. In Proc. AAMAS, pages 23\u201330, 2008.\n[13] K. H. Low, J. M. Dolan, and P. Khosla. Information-theoretic approach to efficient adaptive path planning for mobile robotic environmental sensing. In Proc. ICAPS, pages 233\u2013240, 2009.\n[14] K. H. Low, J. M. Dolan, and P. Khosla. Active Markov information-theoretic path planning for robotic environmental sensing. In Proc. AAMAS, pages 753\u2013760, 2011.\n[15] K. H. Low, G. J. Gordon, J. M. Dolan, and P. Khosla. Adaptive sampling for multi-robot wide-area exploration. In Proc. IEEE ICRA, 2007.\n[16] D. McGrath, C. Zhang, and O. T. Carton. Geostatistical analyses and hazard assessment on soil lead in Silvermines area, Ireland. Environmental Pollution, 127:239\u2013248, 2004.\n[17] G. Podnar, J. M. Dolan, K. H. Low, and A. Elfes. Telesupervised remote surface water quality sensing. In Proc. IEEE Aerospace Conference, 2010.\n[18] C. Prudhomme and D. W. Reed. Mapping extreme rainfall in a mountainous region using geostatistical techniques: A case study in Scotland. Int. J. Climatol., 19:1337\u20131356, 1999.\n[19] N. Rabesiranana, M. Rasolonirina, A. F. Solonjara, and R. Andriambololona. Investigating the spatial anisotropy of soil radioactivity in the region of Vinaninkarena, Antsirabe - Madagascar. In Proc. 4th High-Energy Physics International Conference, 2009.\n[20] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.\n[21] D. L. Rudnick, R. E. Davis, C. C. Eriksen, D. Fratantoni, and M. J. Perry. Underwater gliders for ocean research. Mar. Technol. Soc. J., 38:73\u201384, 2004.\n[22] S. Sokolov and S. R. Rintoul. Some remarks on interpolation of nonstationary oceanographic fields. J. Atmos. Oceanic Technol., 16:1434\u20131449, 1999.\n[23] J. C. Taylor, J. S. Thompson, P. S. Rand, and M. Fuentes. Sampling and statistical considerations for hydroacoustic surveys used in estimating abundance of forage fishes in reservoirs. North American Journal of Fisheries Management, 25:73\u201385, 2005.\n[24] D. R. Thompson and D. Wettergreen. Intelligent maps for autonomous kilometer-scale science survey. In Proc. i-SAIRAS, 2008.\n[25] R. Webster and M. Oliver. Geostatistics for Environmental Scientists. John Wiley & Sons, Inc., NY, 2nd edition, 2007.\n[26] J. G. Zhang, H. S. Chen, Y. R. Su, X. L. Kong, W. Zhang, Y. Shi, H. B. Liang, and G. M. Shen. Spatial variability and patterns of surface soil moisture in a field plot of karst area in southwest China. Plant Soil. Environ., 57(9):409\u2013417, 2011."}, {"heading": "APPENDIX", "text": "Notations. Let \u03c32x , \u03c3xx and \u03c3 2 x|s , \u03a3xx|s in (3) for any location x. Let \u03be , exp{\u2212(m+ 1)2/(2`\u203221 )}."}, {"heading": "A. ENTROPY-BASED PATH PLANNING", "text": ""}, {"heading": "A.1 Proof of Theorem 1", "text": "Given each vector xi\u2212m:i\u22121, the time needed to evaluate the posterior entropy H(Zxi |Zxi\u2212m:i\u22121) over all possible xi \u2208 Xi is \u03c7\u00d7O((km)3) = O(\u03c7(km)3). The time needed to perform this over all \u03c7m possible vectors xi\u2212m:i\u22121 in each stage i is \u03c7m \u00d7 O(\u03c7(km)3) = O(\u03c7m+1(km)3). Since the covariance function is stationary (i.e., it only depends on the distance between locations), the entropies calculated in a stage are the same as those in every other stage. The time needed to propagate the optimal values from stages n\u22121 to m+ 1 is O(\u03c7m+1(n\u2212m\u2212 1)). To obtain the optimal vector xE1:m, the joint entropy H(Zx1:m) has to be evaluated over all possible vectors x1:m. Hence, the time needed to solve for the optimal vector xE1:m is O(\u03c7m(km)3). As a result, the time complexity of the MEPP(m) algorithm is O(\u03c7m+1[(n\u2212 m\u2212 1) + (km)3] + \u03c7m(km)3) = O(\u03c7m+1[n+ (km)3])."}, {"heading": "A.2 Proof of Some Lemmas", "text": "Before giving the proof of Theorem 2, the following lemmas are needed.\nLemma 5. For any observation paths x1:n,\nH(ZxE1:m ) + n\u2211 i=m+1 H(ZxEi |ZxEi\u2212m:i\u22121)\n\u2265 H(Zx1:m) + n\u2211\ni=m+1\nH(Zxi |Zxi\u2212m:i\u22121) .\nProof. Using (7),\nVm+1(x1:m)\n= max xm+1\u2208Xm+1 H(Zxm+1 |Zx1:m) + Vm+2(x2:m+1)\n= max xm+1\u2208Xm+1 H(Zxm+1 |Zx1:m) +\nmax xm+2\u2208Xm+2 H(Zxm+2 |Zx2:m+1) + Vm+3(x3:m+2)\n= max xm+1\u2208Xm+1,xm+2\u2208Xm+2 H(Zxm+1 |Zx1:m) +\nH(Zxm+2 |Zx2:m+1) + Vm+3(x3:m+2) . . .\n= max xm+1\u2208Xm+1,...,xn\u2208Xn n\u2211 i=m+1 H(Zxi |Zxi\u2212m:i\u22121) . (16)\nGiven x1:m, the vectors xm+1, . . . , xn that maximize the term \u2211n i=m+1 H(Zxi |Zxi\u2212m:i\u22121) in (16) can be obtained.\nUsing (8), the observation paths x1:n that maximizeH(Zx1:m) + \u2211n i=m+1 H(Zxi |Zxi\u2212m:i\u22121) can be obtained. Therefore, Lemma 5 holds.\nLemma 6. In a GP, given an unobserved location y and a vector A of sampled locations, \u03c32y|A \u2265 \u03c32n.\nProof. We know that \u03c32y|A \u2265 0. So, if \u03c32n > 0,\n\u03c32y|A = \u03c3 2 s + \u03c3 2 n \u2212 \u03a3yA\u03a3\u22121AA\u03a3Ay \u2265 0 (17)\nwhere the covariance components in the diagonal of \u03a3AA are \u03c32s + \u03c3 2 n. On the other hand, if \u03c3 2 n = 0,\n\u03c32y|A = \u03c3 2 s \u2212 \u03a3yA\u03a3\u22121BB\u03a3Ay \u2265 0 (18)\nwhere \u03a3BB , \u03a3AA \u2212 \u03c32nI. Let A , \u03a3AA,B , \u03a3BB ,E , \u03c32nI,Y , \u03a3Ay,Y\n> , \u03a3yA, and W , \u03a3\u22121AA\u03a3Ay = A \u22121Y. Then, Y = AW.\nW>EW + W>E>B\u22121EW \u2265 0 (19)\n\u21d2W>B>B\u22121EW + W>E>B\u22121EW \u2265 0 (20)\n\u21d2W>(B + E)>B\u22121EW \u2265 0\n\u21d2W>A>B\u22121EW + W>A>W \u2265W>A>W\n\u21d2W>A>(B\u22121E + I)W \u2265W>A>W\n\u21d2W>A>B\u22121(E + B)W \u2265W>A>A\u22121AW\n\u21d2 (AW)>B\u22121AW \u2265 (AW)>A\u22121AW\n\u21d2 Y>B\u22121Y \u2265 Y>A\u22121Y \u21d2 \u03a3yA\u03a3\u22121BB\u03a3Ay \u2265 \u03a3yA\u03a3 \u22121 AA\u03a3Ay . (21)\nTo derive (19), since W is a vector and E = \u03c32nI, W >EW \u2265 0. Since B is a covariance matrix that is invertible and positive semi-definite, B\u22121 is positive semi-definite. Hence, W>E>B\u22121EW \u2265 0 and (19) therefore holds. Since B is symmetric, B> = B. Hence, (20) can be obtained from (19). The rest of the derivation from (20) to (21) is straightforward. From (17),\n\u03c32y|A = \u03c3 2 s + \u03c3 2 n \u2212 \u03a3yA\u03a3\u22121AA\u03a3Ay\n\u2265 \u03c32s + \u03c32n \u2212 \u03a3yA\u03a3\u22121BB\u03a3Ay (22) \u2265 \u03c32n . (23)\nNote that (22) and (23) follow from (21) and (18), respectively. Therefore, Lemma 6 holds.\nLemma 7. H(Zxi\u2212m\u22121 |Zxi\u2212m:i\u22121)\u2212H(Zxi\u2212m\u22121 |Zxi\u2212m:i\u22121 , Zxi) \u2264 k2 log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} .\nProof. We will first prove for the single-robot case. This result will be used later for the multi-robot case. Let xA , xi\u2212m:i\u22121 and xp , xi\u2212m\u22121.\n\u03c32xi\u2212m\u22121|xi\u2212m:i\u22121 \u2212 \u03c3 2 xi\u2212m\u22121|(xi\u2212m:i\u22121,xi)\n= \u03c32xp|xA \u2212 \u03c3 2 xp|(xA,xi) \u2264 \u03c32xp \u2212 \u03c3 2 xp|xi\n= \u03c32xp \u2212 ( \u03c32xp \u2212\n\u03c3xpxi\u03c3xixp \u03c32xi\n)\n\u2264 \u03c34s exp\n{ \u2212 (m+1) 2\n2`\u203221 }2 \u03c32s + \u03c32n\n= \u03c32s\u03be 2\n1 + \u03b7 . (24)\nThe first inequality follows from the property that variance reduction is submodular [4] in many practical cases (e.g., further conditioning on ZxA does not make Zxp and Zxi more correlated). To intuitively understand this notion of submodularity, observing a new location xi will reduce the variance at location xp more if few or no observations are made, and less if many observations are already taken (e.g.,\nat locations xA). The second equality is due to (3). The second inequality follows from the fact that the distance between any two locations from stage i and stage i\u2212m\u2212 1 is at least \u03c91(m+1). So, \u03c3xpxi \u2264 \u03c32s exp{\u2212(m+1)2/(2`\u203221 )}.\nH(Zxi\u2212m\u22121 |Zxi\u2212m:i\u22121)\u2212H(Zxi\u2212m\u22121 |Zxi\u2212m:i\u22121 , Zxi) = H(Zxp |ZxA)\u2212H(Zxp |ZxA , Zxi)\n= 1\n2 log \u03c32xp|xA \u03c32xp|(xA,xi)\n(25)\n\u2264 1 2\nlog \u03c32xp|(xA,xi) +\n\u03c32s\u03be 2 1+\u03b7\n\u03c32xp|(xA,xi) (26)\n= 1\n2 log\n{ 1 +\n\u03c32s\u03be 2\n\u03c32xp|(xA,xi)(1 + \u03b7)\n}\n\u2264 1 2 log\n{ 1 + \u03c32s\u03be 2\n\u03c32n(1 + \u03b7)\n} (27)\n\u2264 log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} . (28)\nUsing (4), (25) can be obtained. Inequality (26) results from (24). Inequality (27) can be obtained using Lemma 6.\nWe will now prove for the k-robot case where k > 1. Then, vectors xi and xp comprise k locations each. Let x j i (x j p) denote the j-th location component in vector xi (xp). Let x j:t i (xj:tp ) denote a vector comprising the j-th to t-th location components in vector xi (xp). Using the chain rule for entropy [3],\nH(Zxp |ZxA)\u2212H(Zxp |ZxA , Zxi) = H(Zx1p |ZxA)\u2212H(Zx1p |ZxA , Zxi) + (29)\nk\u2211 j=2 H(Z x j p |Z x 1:j\u22121 p , ZxA)\u2212H(Zxjp |Zx1:j\u22121p , ZxA , Zxi) .\n(30)\nFor (29),\nH(Zx1p |ZxA)\u2212H(Zx1p |ZxA , Zx1:ki )\n= H(Zx1p |ZxA)\u2212 [H(Zx1p |ZxA , Zx1i ) +\nH(Zx2:ki |Zx1p , ZxA , Zx1i )\u2212H(Zx2:ki |ZxA , Zx1i )]\n= H(Zx1p |ZxA)\u2212H(Zx1p |ZxA , Zx1i ) +\nH(Zx2:ki |ZxA , Zx1i )\u2212H(Zx2:ki |Zx1p , ZxA , Zx1i )\n= H(Zx1p |ZxA)\u2212H(Zx1p |ZxA , Zx1i ) + k\u2211 t=2 H(Zxti |ZxA , Zx1:t\u22121i )\u2212H(Zxti |Zx1p , ZxA , Zx1:t\u22121i )\n\u2264 k log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} . (31)\nThe inequality follows from a derivation similar to (28). Let xAj denote a vector concatenating x 1:j\u22121 p and xA. For\n(30),\nk\u2211 j=2 H(Z x j p |ZxAj )\u2212H(Zxjp |ZxAj , Zx1:ki )\n= k\u2211 j=2 H(Z x j p |ZxAj )\u2212 [H(Zxjp |ZxAj , Zx1i ) +\nH(Zx2:ki |Z x j p , ZxAj , Zx1i )\u2212H(Zx2:ki |ZxAj , Zx1i )]\n= k\u2211 j=2 H(Z x j p |ZxAj )\u2212H(Zxjp |ZxAj , Zx1i ) +\nH(Zx2:ki |ZxAj , Zx1i )\u2212H(Zx2:ki |Zxjp , ZxAj , Zx1i )\n= k\u2211 j=2 H(Z x j p |ZxAj )\u2212H(Zxjp |ZxAj , Zx1i ) +\nk\u2211 t=2 H(Zxti |ZxAj , Zx1:t\u22121i )\u2212H(Zxti |Zxjp , ZxAj , Zx1:t\u22121i )\n\u2264 k(k \u2212 1) log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} . (32)\nThe inequality follows from a derivation similar to (28). Combining (31) and (32), Lemma 7 results.\nCorollary 8. For t = 1, . . . , i\u2212m\u2212 1,\nH(Zxt |Zxt+1:i\u22121)\u2212H(Zxt |Zxt+1:i\u22121 , Zxi) \u2264 k 2 log\n{ 1 +\n\u03be2\n\u03b7(1 + \u03b7)\n} .\nProof. The proof is similar to that of Lemma 7.\nLemma 9.\nH(Zxi |Zxi\u2212m:i\u22121)\u2212H(Zxi |Zx1:i\u22121) \u2264 (i\u2212m\u22121)k 2 log\n{ 1 +\n\u03be2\n\u03b7(1 + \u03b7)\n} .\nProof. Using the chain rule for entropy [3],\nH(Zx1:i\u2212m\u22121 , Zxi |Zxi\u2212m:i\u22121) = H(Zxi |Zxi\u2212m:i\u22121) +H(Zx1:i\u2212m\u22121 |Zxi\u2212m:i\u22121 , Zxi) .\n(33)\nH(Zx1:i\u2212m\u22121 , Zxi |Zxi\u2212m:i\u22121) = H(Zx1:i\u2212m\u22121 |Zxi\u2212m:i\u22121) +H(Zxi |Zx1:i\u2212m\u22121 , Zxi\u2212m:i\u22121) = H(Zx1:i\u2212m\u22121 |Zxi\u2212m:i\u22121) +H(Zxi |Zx1:i\u22121) . (34)\nFrom (33) and (34),\nH(Zxi |Zxi\u2212m:i\u22121)\u2212H(Zxi |Zx1:i\u22121) = H(Zx1:i\u2212m\u22121 |Zxi\u2212m:i\u22121)\u2212H(Zx1:i\u2212m\u22121 |Zxi\u2212m:i\u22121 , Zxi) .\n(35)\nApplying the chain rule for entropy [3] to (35),\nH(Zx1:i\u2212m\u22121 |Zxi\u2212m:i\u22121)\u2212H(Zx1:i\u2212m\u22121 |Zxi\u2212m:i\u22121 , Zxi)\n= i\u2212m\u22121\u2211 t=1 H(Zxt |Zxt+1:i\u22121)\u2212H(Zxt |Zxt+1:i\u22121 , Zxi) (36)\n\u2264 (i\u2212m\u2212 1)k2 log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} . (37)\nThe inequality (37) follows from Corollary 8. So, Lemma 9 holds."}, {"heading": "A.3 Proof of Theorem 2", "text": "Let\n\u03b8 , H(ZxE1:m) + n\u2211\ni=m+1\nH(ZxEi |ZxEi\u2212m:i\u22121) \u2212\n{H(Zx\u22171:m) + n\u2211\ni=m+1\nH(Zx\u2217i |Zx\u2217i\u2212m:i\u22121)} . (38)\nFrom Lemma 5, \u03b8 \u2265 0. By the chain rule for entropy [3],\nH(Zx\u22171:n)\u2212H(ZxE1:n)\n= H(Zx\u22171:m) + n\u2211 i=m+1 H(Zx\u2217i |Zx\u22171:i\u22121) \u2212\n{H(ZxE1:m) + n\u2211\ni=m+1\nH(ZxEi |ZxE1:i\u22121)}. (39)\nLet \u2206\u2217i , H(Zx\u2217i |Zx\u2217i\u2212m:i\u22121) \u2212 H(Zx\u2217i |Zx\u22171:i\u22121) and \u2206 E i , H(ZxEi |ZxEi\u2212m:i\u22121) \u2212 H(ZxEi |ZxE1:i\u22121) for i = m + 1, . . . , n. Then, (39) can be re-written as\nH(Zx\u22171:n)\u2212H(ZxE1:n)\n= H(Zx\u22171:m) + n\u2211 i=m+1 [H(Zx\u2217i |Zx\u2217i\u2212m:i\u22121)\u2212\u2206 \u2217 i ] \u2212\n{H(ZxE1:m) + n\u2211\ni=m+1\n[H(ZxEi |ZxEi\u2212m:i\u22121)\u2212\u2206 E i ]}\n= H(Zx\u22171:m) + n\u2211 i=m+1 H(Zx\u2217i |Zx\u2217i\u2212m:i\u22121)\u2212 n\u2211 i=m+1 \u2206\u2217i \u2212\n[H(ZxE1:m ) + n\u2211 i=m+1 H(ZxEi |ZxEi\u2212m:i\u22121)\u2212 n\u2211 i=m+1 \u2206Ei ]\n= n\u2211 i=m+1 [\u2206Ei \u2212\u2206\u2217i ]\u2212 \u03b8 (40)\n\u2264 n\u2211\ni=m+1\n[\u2206Ei \u2212\u2206\u2217i ] (41)\n\u2264 n\u2211\ni=m+1\n\u2206Ei . (42)\nSince \u03b8 \u2265 0, (41) results. Since \u2206\u2217i \u2265 0 for i = m+ 1, . . . , n, (42) follows. By Lemma 9,\n\u2206Ei \u2264 (i\u2212m\u2212 1)k2 log { 1 + \u03be2\n\u03b7(1 + \u03b7) } for i = m+ 1, . . . , n. Then, Theorem 2 follows."}, {"heading": "B. MUTUAL INFORMATION-BASED PATH PLANNING", "text": ""}, {"heading": "B.1 Proof of Theorem 3", "text": "Given each vector xi\u22122m:i\u22121, the time needed to evaluate I(Zxi\u2212m ;Zui\u22122m:i |Zxi\u22122m:i\u2212m\u22121) over all possible xi \u2208 Xi is \u03c7\u00d7O([r(2m+1)]3) = O(\u03c7[r(2m+1)]3). The time needed to perform this over all \u03c72m possible vectors xi\u22122m:i\u22121 in each stage i is \u03c72m \u00d7O(\u03c7[r(2m+ 1)]3) = O(\u03c72m+1[r(2m+ 1)]3). Similar to the MEPP(m) algorithm, the conditional mutual information terms calculated in a stage are the same\nas those in every other stage. The time needed to propagate the optimal values from stages n \u2212 1 to 2m + 1 is O(\u03c72m+1(n\u2212 2m\u2212 1)). Similarly, the time needed to evaluate I(Zxn\u2212m:n ;Zun\u22122m:n |Zxn\u22122m:n\u2212m\u22121) over all possible xn \u2208 Xn and all \u03c72m possible vectors xn\u22122m:n\u22121 in stage n is O(\u03c72m+1[r(2m + 1)]3). To obtain the optimal vector xM1:2m, I(Zx1:m , Zu1:2m) has to be evaluated over all possible x1:2m. Hence, the time needed to solve for the optimal vector xM1:2m is O(\u03c72m[r(2m)]3). As a result, the time complexity of the M2IPP(m) algorithm is O(\u03c72m+1(n \u2212 2m \u2212 1 + [r(2m + 1)]3) + \u03c72m+1[r(2m + 1)]3 + \u03c72m[r(2m)]3) = O(\u03c72m+1(n+ 2[r(2m+ 1)]3))."}, {"heading": "B.2 Proof of Some Lemmas", "text": "Before giving the proof of Theorem 4, the following lemmas are needed.\nLemma 10. For any observation paths x1:n,\nI(ZxM1:m ;ZuM1:2m ) + n\u22121\u2211 i=2m+1 I(ZxMi\u2212m ;ZuMi\u22122m:i |ZxMi\u22122m:i\u2212m\u22121)\n+ I(ZxMn\u2212m:n ;ZuMn\u22122m:n |ZxMn\u22122m:n\u2212m\u22121) \u2265\nI(Zx1:m ;Zu1:2m) + n\u22121\u2211 i=2m+1 I(Zxi\u2212m ;Zui\u22122m:i |Zxi\u22122m:i\u2212m\u22121)\n+ I(Zxn\u2212m:n ;Zun\u22122m:n |Zxn\u22122m:n\u2212m\u22121) . Proof. Using (12),\nU2m+1(x1:2m)\n= max x2m+1\u2208X2m+1 I(Zxm+1 ;Zu1:2m+1 |Zx1:m) + U2m+2(x2:2m+1)\n= max x2m+1\u2208X2m+1 I(Zxm+1 ;Zu1:2m+1 |Zx1:m) +\nmax x2m+2\u2208X2m+2 I(Zxm+2 ;Zu2:2m+2 |Zx2:m+1) + U2m+3(x3:2m+2)\n= max x2m+1\u2208X2m+1,x2m+2\u2208X2m+2 I(Zxm+1 ;Zu1:2m+1 |Zx1:m) +\nI(Zxm+2 ;Zu2:2m+2 |Zx2:m+1) + U2m+3(x3:2m+2) . . .\n= max x2m+1\u2208X2m+1,...,xn\u2208Xn n\u22121\u2211 i=2m+1 I(Zxi\u2212m ;Zui\u22122m:i |Zxi\u22122m:i\u2212m\u22121) +\nI(Zxn\u2212m:n ;Zun\u22122m:n |Zxn\u22122m:n\u2212m\u22121) . (43)\nGiven x1:2m, the vectors x2m+1, . . . , xn that maximize the term \u2211n\u22121 i=2m+1 I(Zxi\u2212m ;Zui\u22122m:i |Zxi\u22122m:i\u2212m\u22121) + I(Zxn\u2212m:n ;Zun\u22122m:n |Zxn\u22122m:n\u2212m\u22121) in (43) can be obtained. Using (13), the paths x1:n that maximize I(Zx1:m ;Zu1:2m)+\u2211n\u22121 i=2m+1 I(Zxi\u2212m ;Zui\u22122m:i |Zxi\u22122m:i\u2212m\u22121) + I(Zxn\u2212m:n ;Zun\u22122m:n |Zxn\u22122m:n\u2212m\u22121) can be obtained. Therefore, Lemma 10 holds.\nLemma 11. For t = 1, . . . , i\u2212 2m\u2212 1,\nH(Zxt |Zxt+1:i\u2212m\u22121 , Zui\u22122m:i)\u2212H(Zxt |Zxt+1:i\u2212m\u22121 , Zui\u22122m:i , Zxi\u2212m) \u2264 k2 log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} .\nProof. The proof is similar to that of Corollary 8.\nCorollary 12. For t = 1, . . . , i\u2212 2m\u2212 1,\nH(Zut |Zx1:i\u2212m\u22121 , Zut+1:i)\u2212H(Zut |Zx1:i\u2212m\u22121 , Zut+1:i , Zxi\u2212m) \u2264 k(r \u2212 k) log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} .\nProof. Note that the size of vector ut is r\u2212k. The proof is similar to that of Corollary 8.\nCorollary 13. For t = i+ 1, . . . , n,\nH(Zut |Zx1:i\u2212m\u22121 , Zu1:t\u22121)\u2212H(Zut |Zx1:i\u2212m\u22121 , Zu1:t\u22121 , Zxi\u2212m) \u2264 k(r \u2212 k) log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} .\nProof. The proof is similar to that of Corollary 12.\nLemma 14.\nH(Zxi\u2212m |Zxi\u22122m:i\u2212m\u22121 , Zui\u22122m:i)\u2212H(Zxi\u2212m |Zx1:i\u2212m\u22121 , Zu1:n) \u2264 (n\u2212 2m\u2212 1)rk log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} .\nProof. Let x\u2206 (u\u2206) denote a vector of all the locations of x1:i\u2212m\u22121 (u1:n) excluding those of xi\u22122m:i\u2212m\u22121 (ui\u22122m:i). That is, x\u2206 , x1:i\u22122m\u22121 and u\u2206 , (u1:i\u22122m\u22121, ui+1:n).\nH(Zxi\u2212m |Zxi\u22122m:i\u2212m\u22121 , Zui\u22122m:i)\u2212H(Zxi\u2212m |Zx1:i\u2212m\u22121 , Zu1:n) = H(Zxi\u2212m |Zxi\u22122m:i\u2212m\u22121 , Zui\u22122m:i) \u2212\n[H(Zxi\u2212m |Zxi\u22122m:i\u2212m\u22121 , Zui\u22122m:i) + H(Zx\u2206 , Zu\u2206 |Zxi\u22122m:i\u2212m\u22121 , Zui\u22122m:i , Zxi\u2212m) \u2212 H(Zx\u2206 , Zu\u2206 |Zxi\u22122m:i\u2212m\u22121 , Zui\u22122m:i)] (44) = H(Zx\u2206 , Zu\u2206 |Zxi\u22122m:i\u2212m\u22121 , Zui\u22122m:i) \u2212 H(Zx\u2206 , Zu\u2206 |Zxi\u22122m:i\u2212m\u22121 , Zui\u22122m:i , Zxi\u2212m) (45) = [H(Zx\u2206 |Zxi\u22122m:i\u2212m\u22121 , Zui\u22122m:i) \u2212 H(Zx\u2206 |Zxi\u22122m:i\u2212m\u22121 , Zui\u22122m:i , Zxi\u2212m)] + [H(Zu\u2206 |Zx1:i\u2212m\u22121 , Zui\u22122m:i) \u2212 H(Zu\u2206 |Zx1:i\u2212m\u22121 , Zui\u22122m:i , Zxi\u2212m)] (46)\n= i\u22122m\u22121\u2211 t=1 [H(Zxt |Zxt+1:i\u2212m\u22121 , Zui\u22122m:i) \u2212\nH(Zxt |Zxt+1:i\u2212m\u22121 , Zui\u22122m:i , Zxi\u2212m)] + i\u22122m\u22121\u2211 t=1 [H(Zut |Zx1:i\u2212m\u22121 , Zut+1:i) \u2212\nH(Zut |Zx1:i\u2212m\u22121 , Zut+1:i , Zxi\u2212m)] + n\u2211\nt=i+1\n[H(Zut |Zx1:i\u2212m\u22121 , Zu1:t\u22121) \u2212\nH(Zut |Zx1:i\u2212m\u22121 , Zu1:t\u22121 , Zxi\u2212m)] (47) \u2264 [(i\u2212 2m\u2212 1)k2 + (n\u2212 2m\u2212 1)(r \u2212 k)k] log { 1 + \u03be2\n\u03b7(1 + \u03b7) } (48)\n\u2264 (n\u2212 2m\u2212 1)[k2 + (r \u2212 k)k] log { 1 + \u03be2\n\u03b7(1 + \u03b7) } = (n\u2212 2m\u2212 1)rk log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} . (49)\nUsing the chain rule for entropy [3], (44), (46), and (47) can be obtained. Using Lemma 11 and Corollaries 12 and 13, (48) can be obtained.\nLemma 15.\nI(Zxi\u2212m ;Zu1:n |Zx1:i\u2212m\u22121)\u2212 I(Zxi\u2212m ;Zui\u22122m:i |Zxi\u22122m:i\u2212m\u22121) = Ai\u2212m \u2212Bi\u2212m\nwhere\nAi\u2212m = H(Zxi\u2212m |Zxi\u22122m:i\u2212m\u22121 , Zui\u22122m:i) \u2212 H(Zxi\u2212m |Zx1:i\u2212m\u22121 , Zu1:n) , Bi\u2212m = H(Zxi\u2212m |Zxi\u22122m:i\u2212m\u22121)\u2212H(Zxi\u2212m |Zx1:i\u2212m\u22121)\nand Ai\u2212m \u2264 (n\u2212 2m\u2212 1)rk log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} ,\nBi\u2212m \u2264 (i\u2212 2m\u2212 1)k2 log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} .\nProof. By the definition of conditional mutual information,\nI(Zxi\u2212m ;Zu1:n |Zx1:i\u2212m\u22121) = H(Zxi\u2212m |Zx1:i\u2212m\u22121)\u2212H(Zxi\u2212m |Zx1:i\u2212m\u22121 , Zu1:n) ,\n(50)\nI(Zxi\u2212m ;Zui\u22122m:i |Zxi\u22122m:i\u2212m\u22121) = H(Zxi\u2212m |Zxi\u22122m:i\u2212m\u22121)\u2212H(Zxi\u2212m |Zxi\u22122m:i\u2212m\u22121 , Zui\u22122m:i) .\n(51)\nUsing (50) and (51),\nI(Zxi\u2212m ;Zu1:n |Zx1:i\u2212m\u22121)\u2212 I(Zxi\u2212m ;Zui\u22122m:i |Zxi\u22122m:i\u2212m\u22121) = Ai\u2212m \u2212Bi\u2212m .\nBy Lemma 14, Ai\u2212m \u2264 (n\u2212 2m\u2212 1)rk log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} .\nUsing Lemma 9, Bi\u2212m \u2264 (i\u2212 2m\u2212 1)k2 log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} .\nTherefore, Lemma 15 holds."}, {"heading": "B.3 Proof of Theorem 4", "text": "Let \u03b8 , I(ZxM1:m ;ZuM1:2m) + n\u22121\u2211\ni=2m+1\nI(ZxMi\u2212m ;ZuMi\u22122m:i |ZxMi\u22122m:i\u2212m\u22121) +\nI(ZxMn\u2212m:n ;ZuMn\u22122m:n |ZxMn\u22122m:n\u2212m\u22121) \u2212\n[I(Zx?1:m ;Zu?1:2m) + n\u22121\u2211 i=2m+1 I(Zx?i\u2212m ;Zu?i\u22122m:i |Zx?i\u22122m:i\u2212m\u22121) + I(Zx?n\u2212m:n ;Zu?n\u22122m:n |Zx?n\u22122m:n\u2212m\u22121)] . (52)\nFrom Lemma 10, \u03b8 \u2265 0. By the chain rule for mutual information [3],\nI(Zx?1:n , Zu?1:n)\u2212 I(ZxM1:n , ZuM1:n)\n= I(Zx?1:m ;Zu?1:n) + n\u22121\u2211 i=2m+1 I(Zx?i\u2212m ;Zu?1:n |Zx?1:i\u2212m\u22121) +\nI(Zx?n\u2212m:n ;Zu?1:n |Zx?1:n\u2212m\u22121) \u2212\n[I(ZxM1:m ;ZuM1:n ) + n\u22121\u2211 i=2m+1 I(ZxMi\u2212m ;ZuM1:n |ZxM1:i\u2212m\u22121) +\nI(ZxMn\u2212m:n ;ZuM1:n |ZxM1:n\u2212m\u22121)] . (53)\nBy the definition of mutual information,\nI(Zx1:m ;Zu1:n) = H(Zx1:m)\u2212H(Zx1:m |Zu1:n) , (54) I(Zx1:m ;Zu1:2m) = H(Zx1:m)\u2212H(Zx1:m |Zu1:2m) . (55)\nUsing the chain rule for entropy [3],\nH(Zx1:m |Zu1:2m)\u2212H(Zx1:m |Zu1:n) = H(Zx1 |Zu1:2m)\u2212H(Zx1 |Zu1:n) + . . .+ H(Zxm |Zx1:m\u22121 , Zu1:2m)\u2212H(Zxm |Zx1:m\u22121 , Zu1:n)\n= m\u2211 t=1 H(Zxt |Zx1:t\u22121 , Zu1:2m)\u2212H(Zxt |Zx1:t\u22121 , Zu1:n)\n\u2264 m(n\u2212 2m)(r \u2212 k)k log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} . (56)\nInequality (56) can be obtained using a proof similar to Lemma 14. Applying (56) to (54) and (55),\nI(Zx1:m ;Zu1:n)\u2212 I(Zx1:m ;Zu1:2m) = A1:m (57)\nwhere A1:m \u2264 m(n\u2212 2m)(r \u2212 k)k log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} . (58)\nBy the definition of mutual information,\nI(Zxn\u2212m:n ;Zu1:n |Zx1:n\u2212m\u22121) = H(Zxn\u2212m:n |Zx1:n\u2212m\u22121)\u2212H(Zxn\u2212m:n |Zx1:n\u2212m\u22121 , Zu1:n) ,\n(59)\nI(Zxn\u2212m:n ;Zun\u22122m:n |Zxn\u22122m:n\u2212m\u22121) = H(Zxn\u2212m:n |Zxn\u22122m:n\u2212m\u22121) \u2212 H(Zxn\u2212m:n |Zxn\u22122m:n\u2212m\u22121 , Zun\u22122m:n) . (60)\nUsing the chain rule of entropy,\nH(Zxn\u2212m:n |Zxn\u22122m:n\u2212m\u22121)\u2212H(Zxn\u2212m:n |Zx1:n\u2212m\u22121)\n= n\u2211 t=n\u2212m H(Zxt |Zxn\u22122m:t\u22121)\u2212H(Zxt |Zx1:t\u22121) (61)\n\u2264 (m+ 1)(n\u2212 2m\u2212 1)k2 log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} . (62)\nUsing Lemma 9, inequality (62) can be obtained. By the chain rule of entropy,\nH(Zxn\u2212m:n |Zxn\u22122m:n\u2212m\u22121 , Zun\u22122m:n) \u2212 H(Zxn\u2212m:n |Zx1:n\u2212m\u22121 , Zu1:n)\n= n\u2211 t=n\u2212m H(Zxt |Zxn\u22122m:t\u22121 , Zun\u22122m:n)\u2212H(Zxt |Zx1:t\u22121 , Zu1:n)\n\u2264 (m+ 1)(n\u2212 2m\u2212 1)rk log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} . (63)\nUsing a proof similar to Lemma 14, inequality (63) can be obtained. Applying the results (62) and (63) to (59) and (60),\nI(Zxn\u2212m:n ;Zu1:n |Zx1:n\u2212m\u22121) \u2212 I(Zxn\u2212m:n ;Zun\u22122m:n |Zxn\u22122m:n\u2212m\u22121) = An\u2212m:n \u2212Bn\u2212m:n (64)\nwhere An\u2212m:n \u2264 (m+ 1)(n\u2212 2m\u2212 1)rk log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} ,\n(65) Bn\u2212m:n \u2264 (m+ 1)(n\u2212 2m\u2212 1)k2 log { 1 + \u03be2\n\u03b7(1 + \u03b7)\n} .\n(66)\nUsing the above results, (53) can be rewritten as\nI(Zx?1:n ;Zu?1:n)\u2212 I(ZxM1:n ;ZuM1:n)\n= A?1:m + I(Zx?1:m ;Zu?1:2m) +\nn\u22121\u2211 i=2m+1 [A?i\u2212m \u2212B?i\u2212m + I(Zx?i\u2212m ;Zu?i\u22122m:i |Zx?i\u22122m:i\u2212m\u22121)]+ A?n\u2212m:n \u2212B?n\u2212m:n + I(Zx?n\u2212m:n ;Zu?n\u22122m:n |Zx?n\u22122m:n\u2212m\u22121)\u2212 {AM1:m + I(ZxM1:m ;ZuM1:2m) + n\u22121\u2211\ni=2m+1\n[AMi\u2212m \u2212BMi\u2212m + I(ZxMi\u2212m ;ZuMi\u22122m:i |ZxMi\u22122m:i\u2212m\u22121)]+\nAMn\u2212m:n \u2212BMn\u2212m:n + I(ZxMn\u2212m:n ;ZuMn\u22122m:n |ZxMn\u22122m:n\u2212m\u22121)} (67)\n= A?1:m + n\u22121\u2211 i=2m+1 (A?i\u2212m \u2212B?i\u2212m) + (A?n\u2212m:n \u2212B?n\u2212m:n) \u2212\n[AM1:m + n\u22121\u2211 i=2m+1 (AMi\u2212m \u2212BMi\u2212m) + (AMn\u2212m:n \u2212BMn\u2212m:n)]\u2212 \u03b8\n(68)\n\u2264 A?1:m + n\u22121\u2211\ni=2m+1\nA?i\u2212m +A ? n\u2212m:n + n\u22121\u2211 i=2m+1 BMi\u2212m +B M n\u2212m:n \u2212\n[AM1:m + n\u22121\u2211 i=2m+1 AMi\u2212m +A M n\u2212m:n + n\u22121\u2211 i=2m+1 B?i\u2212m +B ? n\u2212m:n]\n(69)\n\u2264 A?1:m + n\u22121\u2211\ni=2m+1\nA?i\u2212m +A ? n\u2212m:n + n\u22121\u2211 i=2m+1 BMi\u2212m +B M n\u2212m:n\n(70)\n\u2264 [m(n\u2212 2m)(r \u2212 k)k + (n\u2212 2m\u2212 1 +m+ 1)(n\u2212 2m\u2212 1)rk + 1\n2 (n\u2212 2m\u2212 1)(n\u2212 2m\u2212 2)k2 + (m+ 1)(n\u2212 2m\u2212 1)k2]\nlog { 1 +\n\u03be2\n\u03b7(1 + \u03b7)\n} (71)\n\u2264 [m(n\u2212 2m)(r \u2212 k)k + (n\u2212m)(n\u2212 2m)rk + 1\n2 (n\u2212 2m)(n\u2212 2m\u2212 2)k2 + (m+ 1)(n\u2212 2m)k2] log\n{ 1 +\n\u03be2\n\u03b7(1 + \u03b7) } = [m(n\u2212 2m)(r \u2212 k)k + (n\u2212m)(n\u2212 2m)rk +\n1 2 (n\u2212 2m)(n\u2212 2m)k2 +m(n\u2212 2m)k2)] log\n{ 1 +\n\u03be2\n\u03b7(1 + \u03b7) } = [mr + (n\u2212m)r + 1\n2 (n\u2212 2m)k](n\u2212 2m)k log\n{ 1 +\n\u03be2\n\u03b7(1 + \u03b7) } = [nr + 1\n2 (n\u2212 2m)k](n\u2212 2m)k log\n{ 1 +\n\u03be2\n\u03b7(1 + \u03b7)\n} .\nUsing (57), (64), and Lemma 15, (67) can be obtained. Applying \u03b8 to (67), (68) can be obtained. Since \u03b8 \u2265 0, (69) can\nbe obtained. Using (58), (65), (66), and Lemma 15, (70) and (71) can be obtained. Therefore, Theorem 4 holds."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "A key problem of robotic environmental sensing and moni-<lb>toring is that of active sensing: How can a team of robots<lb>plan the most informative observation paths to minimize<lb>the uncertainty in modeling and predicting an environmen-<lb>tal phenomenon? This paper presents two principled ap-<lb>proaches to efficient information-theoretic path planning based<lb>on entropy and mutual information criteria for in situ ac-<lb>tive sensing of an important broad class of widely-occurring<lb>environmental phenomena called anisotropic fields. Our pro-<lb>posed algorithms are novel in addressing a trade-off between<lb>active sensing performance and time efficiency. An impor-<lb>tant practical consequence is that our algorithms can exploit<lb>the spatial correlation structure of Gaussian process-based<lb>anisotropic fields to improve time efficiency while preserv-<lb>ing near-optimal active sensing performance. We analyze<lb>the time complexity of our algorithms and prove analyti-<lb>cally that they scale better than state-of-the-art algorithms<lb>with increasing planning horizon length. We provide the-<lb>oretical guarantees on the active sensing performance of<lb>our algorithms for a class of exploration tasks called tran-<lb>sect sampling, which, in particular, can be improved with<lb>longer planning time and/or lower spatial correlation along<lb>the transect. Empirical evaluation on real-world anisotropic<lb>field data shows that our algorithms can perform better or<lb>at least as well as the state-of-the-art algorithms while of-<lb>ten incurring a few orders of magnitude less computational<lb>time, even when the field conditions are less favorable.", "creator": "LaTeX with hyperref package"}}}