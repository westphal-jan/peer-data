{"id": "1302.1543", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2013", "title": "Probability Update: Conditioning vs. Cross-Entropy", "abstract": "Conditioning is the generally agreed-upon method for updating probability distributions when one learns that an event is certainly true. But it has been argued that we need other rules, in particular the rule of cross-entropy minimization, to handle updates that involve uncertain information. In this paper we re-examine such a case: van Fraassen's Judy Benjamin problem, which in essence asks how one might update given the value of a conditional probability. We argue that -- contrary to the suggestions in the literature -- it is possible to use simple conditionalization in this case, and thereby obtain answers that agree fully with intuition. This contrasts with proposals such as cross-entropy, which are easier to apply but can give unsatisfactory answers. Based on the lessons from this example, we speculate on some general philosophical issues concerning probability update.", "histories": [["v1", "Wed, 6 Feb 2013 15:55:49 GMT  (971kb)", "http://arxiv.org/abs/1302.1543v1", "Appears in Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI1997)"]], "COMMENTS": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI1997)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["adam j grove", "joseph y halpern"], "accepted": false, "id": "1302.1543"}, "pdf": {"name": "1302.1543.pdf", "metadata": {"source": "CRF", "title": "Probability Update: Conditioning vs. Cross-Entropy", "authors": ["Adam J. Grove", "Joseph Y. Halpern"], "emails": ["grove@research.nj.nec.com", "halpern@cs.cornell.edu"], "sections": [{"heading": null, "text": "1 INTRODUCTION\nHow should one update one's beliefs, represented as a probability distribution Pr over some space S, when new evidence is received? The standard Bayesian an swer is applicable whenever the new evidence asserts that some event T \ufffd S is true (and furthermore, this is all that the evidence tells us). In this case we simply condition on T, leading to the distribution Pr{\u00b7IT).\nFor successful \"real-world\" applications of probability theory so far, conditioning has been a mostly sufficient answer to the problem of update. But many people have argued that conditioning is not a philosophically adequate answer (in particular, [Jeffrey 1983]). Once we try to build a truly intelligent agent interacting in complex ways with a rich world, conditioning may end up being practically inadequate. as well.\nThe problem is that some of the information that we receive is not of the form \"Tis (definitely) true\" for\nany T. What would one do with a constraint such as \"Pr(T) = 2/3\" or \"the expected value of some ran dom variable on S is 2/3\". We cannot condition on this information, since it is not an event in S. Yet it is clearly useful information. So how should we in corporate it? There is in fact a rich literature on the subject (e.g., see [Bacchus, Grove, Halpern, and Koller 1994; Diaconis and Zabell 1982; Jeffrey 1983; Jaynes 1983; Paris and Vencovska 1992; Uffink 1995]). Most proposals attempt to find the probability distribution that satisfies the new information and is in some sense the \"closest\" to the original distribution Pr. Certainly the best known and most studied of these proposals is to use the rule of minimizing cross-entropy [Kullback and Leibler 1951] as a way of updating with general probabilistic information. This rule can also be shown to generalize Jeffrey's rule [Jeffrey 1983], which in turn generalizes conditioning.\nBut is cross-entropy ( CE) really such a good rule? The traditional justifications of CE are that it satisfies vari ous sets of criteria (such as those of [Shore and Johnson 1980]) which, while plausible, are certainly not com pelling [Uffink 1995]. Van Fraassen, in a paper entitled \"A problem for relative information [CE] minimizers in probability kinematics\" [1981] instead approached the question in a different way: he looked at how CE behaves on a simple specific example. He calls his ex ample the Judy Benjamin (JB) problem; in essence it is just the question of how one should update by a conditional probability assertion, i.e., \"Pr(AIB) = c\" for some events A, B and c E [0, 1]. As we now explain, van Fraassen uncovers what seems (to us) to be an unintuitive feature of cross-entropy, although in later papers on the same issue he endorses CE and a family of other similar rules. Furthermore, none of his rules agree with most people's strong in tuition about the solution to his problem. The pur pose of this paper is to give a new analysis, which is based on simple conditionalization, and is (we argue) in good agreement with people's expectations. Our hope is that the example and our analysis will be an instructive study of the subtleties involved in proba bility update, and in particular the dangers involved in indiscriminately applying supposedly \"simple\" and\nProbability Update: Conditioning vs. Cross-Entropy 209\n\"general\" rules like CE.\nVan Fraassen explains the JB problem as follows [1987]:\n[The story] derives from the movie Private Benjamin, in which Goldie Hawn, playing the title character, joins the Army. She and her platoon, participating in war games on the side of the \"Blue Army\", are dropped in the wilderness, to scout the opposition (\"Red Army\"). They are soon lost. Leaving the movie script now, suppose the area is di vided into two halves, Blue and Red territory, which each territory is divided into Head quarters Company area and Second Com pany area. They were dropped more or less at the center, and therefore feel it is equally likely that they are now located in one area as in another. This gives us the following muddy Venn diagram, drawn as a map of the area:\n1/4 Red 2nd 1/4 RedHQ\n12 B !Ue\nThey have some difficulty contacting their own HQ by radio, but finally succeed and de scribe what they can see around them. After a while, the office at HQ radios: \"I can't be sure where you are. If you are in Red ter ritory, the odds are 3:1 that you are in HQ Company area ... \" At this point the radio gives out. We must now consider how Judy Benjamin should adjust her opinions, if she accepts this radio message as the sole and correct con straint to impose. The question on which we should focus is: what does it do to the proba bility that they are in friendly Blue territory? Does it increase, or decrease, or stay at its present level of l /2?\nThe intuitive response is that the message should not change the a priori probability of 1/2 of being in Blue territory. More precisely, according to this response, Judy's posterior probability distribution should place probability 1/4 on being at each of the two quad rants in the Blue territory, probability 3/8 on being in the Headquarters Company area of Red territory, and probability 1/8 on being in the Second Company area of Red territory. Van Fraassen [1987] notes that his many informal surveys of seminars and conference audiences find that people overwhelmingly give this answer.\nHowever, this intuitive answer is inconsistent with cross-entropy. In fact, it can be shown that cross entropy has the rather peculiar property that if HQ had said \"If you are in Red territory, the odds are o: : 1 that you are in HQ company area ... \", then for all o: f: 1, the posterior probability of being in Blue territory (according to the distribution that minimizes cross-entropy and satisfies this constraint) would be greater than 1/2; it would stay at 1/2 only if o: = 1. This seems (to us, at least) highly counterintuitive. Why should Judy come to believe she is more likely to be in Blue territory, almost no matter what the mes sage says about o:? For example, what if Judy knew in advance that she would receive such a message for some a f: 1, and simply did not know the value of o:. Should she then increase the probability of being in Blue territory even before hearing the message? As van Fraassen [1981] says, as part of an extended dis cussion of this phenomenon:\nIt is hard not to speculate that the dangerous implications of being in the enemy's head quarters area are causing Judy Benjamin to indulge in wishful thinking ... 1\nHowever, as van Fraassen points out (crediting Peter Williams for the observation), there also seems to be a problem with the intuitive response. Presumably there is nothing special about hearing the odds of being in Red territory are 3:1. The posterior probability of be ing in Blue territory should be 1/2 no matter what the odds are, if we really believe that this information is irrelevant to the probability of being in Blue terri tory. But if o: = 0 then, to quote van Fraassen [1987], \"he would have told her, in effect, 'You are not in Red Second Company area'\". Assuming that this is indeed equivalent, it seems that Judy could have used simple conditionalization, with the result that her posterior probability of being in Blue territory would be 2/3, not 1/2.\nIn [van Fraassen 1987; van Fraassen, Hughes, and Harman 1986], van Fraassen and his colleagues for mulate various principles that they argue an update rule should satisfy. Their first principle is motivated by the observation above and simply says that, when conditioning seems applicable, the answer should be that obtained by conditioning. To state this more pre cisely, let q = o:/(l+o:) be the probability (rather than the odds) of being in red HQ company area given that Judy is in Red territory. In the case of the JB problem, the first principle becomes:\nIf q = 1 the prior is transformed by simple conditionalization on the event \"Red HQ area or Blue territory\"; if q = 0 by simple condi tionalization on \"Red 2nd company area or Blue territory\".\n1We remark that this behavior of CE has also been dis cussed and criticized in a more general setting [Seidenfeld 1987].\n210 Grove and Halpern\nThis first principle already eliminates the intuitive rule, i.e., the rule that the posterior probability should stay at 1/2 no matter what q is. (Note also that we cannot make the rule consistent with this principle by trea.ting q = 0 and q = 1 as special cases, unless we are prepared to accept a rule that is discontinuous in q.) For van Fraassen, this is apparently a decisive refuta tion of the intuitive rule, which he thus says is flawed [van Fraassen 1987]. However, in this paper we give a new2 and simple anal ysis of the JB problem. We believe that our solution is well-motivated, and it agrees completely with the in tuitive answer. It thus also does not exhibit the coun terintuitive behavior of CE. Our basic idea is simply to use conditioning, but to do so in a larger space where it makes sense (i.e., where the information we receive is an event). Of course, people have always realized that this option is avail able. It is perhaps not popular because it appears to pose certain serious philosophical and practical prob lems as a general approach. In particular, which larger space do we use? There may be many equally natural possibilities, leading to different answers, so the rule will be indeterminate. Also possible is that all ex tended spaces we can think of seem equally unnatural and contrived; again, we will be stuck. In addition, there is the practical concern that a rich enough space might be vastly larger and more complicated to work in than the original.\nAgainst this, a rule like cross-entropy seems extremely attractive. It provides a single general recipe which can be mechanically applied to a huge space of up dates. Even families of rules, such as van Fraassen proposes, are not so bad: after one has chosen a rule (usually by selecting a single real-valued parameter [Uffink 1995]), the rest is again mechanical, general, and determinate. Furthermore, all these rules work in the original spaceS, without requiring expansion, and so may be more practical in a computational sense. Since all we do in this paper is analyze one particular problem, we must be careful in making general state ments on the basis of our results. Nevertheless, they do seem to support the claim that sometimes, the only \"right\" way to update, especially in an incompletely specified situation, is to think very carefully about the real nature and origin of the information we receive, and then (try to) do whatever is necessary to find a suitable larger space in which we can condition. If this doesn't lead to conclusive results, perhaps this is because we do not understand the information well enough to update with it. However much we might wish for one, a genemlly satisfactory mechanical rule such as cross-entropy, which saves us all this question ing and work, probably does not exist.\n2 Although we are aware of no published analysis simi lar to our own, we have learned that Seidenfeld has earlier presented a closely related analysis in several lectures [Sei denfeld 1997].\nThis does not deny the usefulness of rules like CE. There will be some (perhaps large) family of situations in which CE is indeed appropriate, and we would like to better understand what this family is and how to recognize it. But if CE (or any other rule) is blindly applied whenever the information is of the appropriate syntactic form, we should not be surprised if the results are often unexpected and unhelpful.\n2 CONDITIONING\nIn this section, we present our alternative analysis of the JB problem. In the following, let B1, B2, R\ufffd, R2 denote the events that Judy is in, respectively, Blue HQ, Blue Second Company, Red HQ, and Red Second Company areas. Let B = B1 V B2 and R = R1 V R2. The message HQ sent, \"If you are in Red territory, the odds are 3:1 that you are in HQ Company area\", is equivalent to asserting that the conditional proba bility of R1 given R is true is 3/4. In general, let M(q) be the similar message asserting that this probability is q E [0, lj for some q not necessarily = 3/4 (i.e., the announced odds are q/(1 - q) : 1 instead of 3 : 1). We use Prjrior to denote Judy's prior beliefs (i.e., be fore the message is received) and Prj to denote her posterior distribution after receiving M(q).\nThe key step is to re-examine the problem from the be ginning, and ask ourselves how Judy should treat HQ's message. Note that Van Ftaassen explicitly assumes, in his statement of the problem, that HQ's statement should be treated as a constraint on Judy's beliefs. Thus, he interprets it as an imperative: \"Make your beliefs be such that this is true!\" This interpretation of probabilistic information as constraints is a common one (especially in the context of CE), but is difficult to justify [Uffi.nk 1996]. Van Fraassen is, of course, quite aware of the philosophical issues raised by his interpretation; see [van Fraassen 1980].\nBut is the interpretation that HQ's statement should be regarded as a constraint on Judy's beliefs the only possible one? Note that, as the story is presented, it certainly sounds as though HQ was trying to give Judy some true and useful information. But, at the time M is sent, the statement that Prjriar(RtJR) = 3/4 is clearly not true of Judy's beliefs. Thus, if we wish to interpret M as referring directly to Judy's beliefs, we will be unable to regard it as a factual assertion in any straightforward sense.\nBut suppose we instead view M(q) as being a cor rect statement regarding HQ 's beliefs; i.e., as asserting PrnQ(RtJR) = q where PrnQ denotes HQ's distribu tion over where Judy might be. This certainly seems to be a reasonable interpretation in light of the story. How should Judy react to it? Unfortunately, the story does not give us enough information to be able to pro vide a definite answer to this question. Judy's correct reaction to the message depends on aspects of the situ ation that were not included in the problem statement.\nProbability Update: Conditioning vs. Cross-Entropy 211\nThe following is a partial list of things that could be relevant: What does Judy know about HQ's beliefs and knowledge? How did HQ expect Judy to react to the message, and what did Judy know about these ex pectations? What messages could HQ have sent? For instance , might HQ have sent M(q) for some other value of q if it were appropriate, or would it have said something else entirely if q # 3/4? Does Judy believe that HQ even has the option of sending messages that are not of the form M(q); if so, what messages?3 And so on. We now give one particular analysis, which follows by filling in such missing details in what we feel is a plau sible way. As we said, we assume that M(3/4) is a fac tual statement about HQ's beliefs. We further assume that, no matter what HQ actually knows, its message would always have been simply M(q) for the appropri ate value of q. Thus, Judy can read nothing more into M(q) other than to regard it as a true statement about HQ's beliefs. As noted in the previous paragraph , even to get this far relies on several strong assumptions .\nOnce Judy hears M(q), it seems natural to want to condition on it. The problem is that, as yet, we have not introduced a space in which M(q) is an event. Of course, there are many possible such spaces. To construct an appropriate one, we must consider how Judy models HQ's beliefs, and what she believes about these beliefs. Again, here we make perhaps the simplest possible choice. We suppose that Judy models HQ's beliefs as a distribution over the four quadrants R1, R2, B1, B2. Let Pr\ufffd\ufffdc be the distribution on { R11 R2, B11 B2} such that Pr\ufffd\ufffdc ( R1) = a, P a ,b,c (R ) b p a,b,c(B ) p a,b,c(B ) r HQ 2 = , r HQ 1 = c, r HQ 2 = 1 - a - b - c. Thus the set of all possible distri butions HQ might have, given our assumptions, is PHQ = {Pr\ufffd\ufffdcla,b,c2:0, a+b+c\ufffdl}. In the following, we view PrnQ(R1 IR), PrHQ(RI) , PrHQ(B), and so on, as random variables on the space P HQ\u00b7 Thus, for example, \"Pr nQ(R1IR) < q\" denotes the event {Pr\ufffd\ufffdc I Pr';;\ufffdc (R1IR) < q}. Again, we stress that we are not forced to use P HQ\u00b7 Judy might actually have a richer model of HQ's be liefs (e.g., she might think that HQ makes finer geo graphical distinctions than simply the four quadrants) or a coarser model (e.g., Judy might take as the space of possibilities the possible values of PrnQ(RliR), and not reason about the rest of HQ's distribution). How ever, given the description of the story, PHQ seems to be the most natural space for Judy to model her beliefs\n3To see the possible relevance of this, note that if there are other possible messages, then the very fact that HQ's first message was not one of these others could be impor tant information in and of itself: Judy might reason that M(3/4) must have been the most important fact HQ pos sessed. On the other hand, since the radio died before the message was completed, such inferences depend heavily on the protocol Judy expects HQ to follow.\nabout HQ's beliefs. Since Judy does not know what HQ actually believes, her beliefs will be a distribution over distributions, i.e., a distribution over PnQ\u00b7 Which distribution? Again, we have many choices, but a natural one is to suppose that before Judy hears the message, she considers a uniform distribution over (a, b, c) tuples . Formally, we consider the distribution function defined by Prj7\ufffdq{Pr\ufffd8c I a::; A,b::; B,c::; C}) =ABC, so that the density function is just 1. We also use the notation Prj/HQ to denote Judy's beliefs about HQ's beliefs after receiving M(q).\nIt might be thought that, having decided to take PHQ as the set of possible beliefs that HQ could have and given the (implicit) assumption that Judy is initially completely ignorant of HQ's beliefs, the prior density on P HQ is determined completely. Unfortunately, this is not the case. There is no unique \"uniform distribu tion\" on PnQ\u00b7 Uniformity depends on how we choose to parameterize the space. We have chosen to param eterize the elements of this space by a triple (a, b, c) denoting the probabilities of R1, R2, and Bt, respec tively. However, we could have chosen to characterize an element of the space by a triple (a', b', c') denot ing the square of the probabilities of R1, R2, and B1, respectively. Or perhaps more reasonably, we could have chosen to characterize an element by a triple (a11, b1 , c\") denoting the probability of R, the probabil ity of R1 given R, and the probability of B1 given B. A uniform distribution with respect to either of these parameterizations would be far from uniform with re spect to the parameterization we have chosen, and vice versa.4 This is, of course, just an instance of the well known impossibility of defining a unique notion of 'l.mi form in a continuous space [Howson and Urbach 1989]. Since M(q) is an event in the new space 'PHQ, Judy should be able to condition on it. One might object that, since M(q) is an event of measure 0, condition ing is not well defined. This is t rue, but there are two (closely related) ways of dealing with this prob lem. The more elementary and intuitive approach is based on the observation that, in practice, HQ w ill not (in general) be able to announce its value for PrnQ(RI IR) exactly, since this could require HQ to announce an infinite-precision real number. It seems more reasonable to regard the announced value of q as being rounded or approximated in some fashion. In particular, we might suppose that M(q) really means PtHQ(RIIR) E [q - E, q + e) for some small value E > 0. This event has non-zero probability according to Prj7\ufffd'Q, and so conditioning is unproblematic.\n4We note, however, that the uniform densities with respect to all 4 possible parameterizations that involv ing choosing 3 out of the 4 probabilities from PrnQ(Rt), PrnQ(R2), PrnQ(Bt), PrnQ(Bz) do lead to the same dis tribution over PnQ. and so o ur decision to use the first three of these probabilities does not affect our analysis.\n212 Grove and Halpern\nThe second approach directly invokes the standard def inition of conditioning on (the value of) a random variable. We briefly review the details here. Sup pose we have two random variables X and Y. If Pr(X = a) > 0, then Pr(Y = bJX = a) is just defined as Pr(Y = b (l X = a)/ Pr(X = a) as we would expect. If Pr(X = a) = 0, then we take the straightforward analogue of this definition using den sity functions. If fxy(x, y) is the joint density function for X and Y, and f x ( x) is the density for X alone, then the conditional density of Y given X is given by Jy,x(yJx) = fxy(x,y)ffx(x). Using the density function we can then compute the probability by inte grating as usual. Further details can be found in any standard text on probability (for instance [Papoulis 1984]). To use this approach, we need to identify a random variable X and value b such that M(q) corresponds to the event X = b. The choice of random variable is crucial; we can easily have two random variables X and X' such that X == b and X' = b are the same event, yet conditioning on X =band X'= b leads to different results, since X and X' have different density functions. In our case there is an obvious choice of random variable, given our description of the situation: PrnQ(RtiR). With this choice of random variable, it is easy to see that the two approaches give us the same answer; the use of the density function corresponds to considering a small interval around PrnQ(RtiR} = q, and then considering the limit as the interval width tends to 0.\nBefore computing the result of conditioning on M(q) (under either approach), it turns out to be use ful to do some more general computations. Since Pr';;\ufffdc(RtiR) = af(a +b), we have\nPrjj\ufffd'Q(PrnQ(RtiR) < q) 1q r t-a. 11-a.-b d a=O h=a(l-q) c=O 1 cdbda\nq 11 rl a11-a-b1dcdhda a.=O Jb=O c=O = 6 r 1\n1 -a 11-a-b 1dcdhda Ja=O b=\ufffd c=O q\n= 6 1:0 1:\ufffd;1_\u20221 (1- a- b) dbda q lq (q- a)2 == 6 22 da a=O q\n= q Two other results, which are derived in a similar fash ion, also tmn out to be useful:\nPrj/;Q(PrnQ(B) < p) = (3- 2p)p2, and (1) Prjj\ufffdQ(Prnq(R1IR) < qAPrnQ(B) < p) == q(3-2p)p2 . The point here is not just the values themselves, but, more importantly, that the final distribution function is the product of the first two. That is, the events\nPrHq(B) < p and Prnq(R1 JR) < q are independent! This is of course extremely intuitive: It seems reason\" able that HQ's beliefs about the probability of Judy being in Blue Territory should be independent of HQ's beliefs of her being in Red HQ area, given that she is in Red territory.\nUsing this, it is trivial to prove the following proposi\" tion, which holds whether we choose to use any par\" ticular t: > 0, or if we use the standard definition of conditioning on the value of a random variable (which, as we have said, essentially corresponds to considering the limit as t-> 0). In this proposition, pr8(p) denotes the density function for the random variable Prm\ufffd(B); i.e., prB(P) = dPrJ/HQ(PraQ{B) < p)jdp. Similarly, prB (P I M(q)) = dPrJ/HQ(PrnQ(B) < PI M(q))/dp. (Note that from (1), it immediately follows that pr8(p) = 6p- 6p2, although we do not need this fact for the next result.)\nProposition 2.1 : In (P HQ, Prjj\ufffdrQ), the events PrnQ(B) = p and M(q) are independent. Formally, prB(P) = pr8(pJM(q)). With this result, we are very close to showing that Judy's beliefs regarding the probability of her be ing in Blue territory don't change as a result of the message. Notice that Prj\ufffdQ is a distribution on PnQ, that is, on Judy's beliefs about HQ's beliefs regarding where Judy is . The posterior distribution Prj/HQ(\u00b7) = Prj/;Q(\u00b7I M(q)) is still a distribution on Pnq. As a result of conditioning, Judy revises her beliefs about HQ's beliefs. But to determine Judy's beliefs about where she is we need a distribution on { Rt, R2, Bt, B2 }. The question is how Judy's beliefs about HQ's beliefs about where she is are related to her beliefs about where she is. Notice that there is no necessary relation. After all, for all we know, Judy might think that HQ has no reliable information, and thus decide to ignore HQ's statement when forming her opinion regarding where she is. But, in keeping with the spirit of the story, we assume that Judy trusts HQ, and thus forms her beliefs by taking expectations over her beliefs about HQ's beliefs. For example, if Judy was certain that HQ was certain that she was in Blue territory, then she would ascribe probability 1 to being in Blue territory. More generally, Judy weights HQ's beliefs according to her beliefs about how likely it is that HQ holds those beliefs. We formalize this trust assumption as follows:\n(Trust) At any point in time, Judy's beliefs about any event in the space {R1, R2, B1. B2} are formed by taking expectations of HQ's probability of the same event, according to Judy's distribu tion over HQ's possible beliefs. Formally, we have:\nPr\ufffd(E) = f Pr\ufffd; HQ(Pr';J8c) Pr';J8c(E) dabc lPr\ufffd\u00b7\ufffdcEP HQ\nProbability Update: Conditioning vs. Cross-Entropy 213\n( = 1:0 prk(e) e de)\nfor t E {prior} U [0, 1]. (In the second line, which follows from standard probability theory, pr}.;(e) is the density function of the random variable Pr\ufffd8c(E); i.e., pr1; (e) = dPr\ufffd/HQ(Pr\ufffd8c(E) < e)jde.)\nNote that when we apply this rule before Judy receives the message, so that t = prior, we have Pr\ufffd;'-ior(B1) = Pr\ufffdrior(B2) = Prrior(Rl) = Prrior(R2) = 1/4, which is consistent with our earlier assumption that Judy started with a uniform prior on { Bt, B2, Rt, R2}. The desired result now follows quite readily using the trust principle after Judy has received M(q). The re sult is that, no matter what the value of q is, her beliefs regarding being in Blue territory remain unchanged, exactly in accord with most people's strong intuitions.\nTheorem 2.2: Pr'j(B) = 1/2 for all q E [0, 1]. Proof: Prj(B) { Prj/HQ(Pr';[8c) Pr';j\ufffdc(B) dabc }p,\ufffd\u00b7Q\u2022EPHQ\n= i:/rB(PIM(q))pdp\n1:0 pr8(p) pdp (by Theorem 2.2)\n1:0 (6p - 6p2)p dp (from (1))\n= 1/2. I Note that this theorem applies even if q = 1. Van Fraassen would interpret the message M(l) as mean ing that Judy is definitely not in R2. We interpret this it as PrnQ(R1IR) E [1 - t:, 1] for some (arbitrar ily) small and unspecified \u20ac > 0. Although the two interpretations seem close (after all, they differ by at most t: in the probability that they assign to R1 and R2), they are not equivalent. As Theorem 2.2 shows, this is a significant difference. It is the claimed equiv alence of the two interpretations that was behind van Fraassen's first principle, and hence his rejection of the intuitive answer that Prj (B) = 1/2. This equivalence may be correct under van Fraassen's constraint based interpretation of M, but it is not inevitable under our alternative reading, in which M is indeed a factual announcement (but about HQ's beliefs, not Judy's).\n3 DISCUSSION\nIt is worth reviewing the assumptions that were nec essary for us to prove Theorem 2.2. We assumed:\n1. HQ's belief as to where Judy is can be characterized by a distribution on the space {Rt, R2,B1, B2}.\n2. Judy's beliefs regarding HQ's beliefs were characterized by the uniform distribution on HQ's beliefs, when parameterized by the tuple (PrsQ(RI), PrsQ(R2), PrnQ(Bt )).\n3. The only messages that HQ could send were those of the form M(q), and the one that HQ did send was the one that correctly reflected HQ's beliefs.\n4. M(q) is interpreted as meaning that PrnQ(q) E [q - \ufffd, q + \u20ac], so that we can condition on this event. (However, our result holds no matter what t: is. Thus we can regard t: as an arbitrarily small and unknown nonzero quantity.)\n5. Judy's distribution on {Bt, B2, Rt, R2} was de rived by taking the expectation of her beliefs re garding HQ's beliefs.\nAlthough these assumptions seem to us quite reason able, they are clearly not the only assumptions that could have been made. It is certainly worth trying to understand to what extent our results depend on these assumptions, and in particular whether they can be ex tended to provide more general statements of how to update by probabilistic information.\nWhere does this leave CE and all the other methods considered by van Fraassen and his colleagues? As we said in the introduction, such rules may be useful in certain cases, but we believe it is an important research question to understand and explain precisely when. We do not, in particular, find CE to give particularly plausible results in the JB problem. But how could this have been predicted in advance?\nThe JB problem shows that we need more than just an axiomatic justification for the use of CE (or any other method of update). An alternative to the use of a rule is to do what we have done for the JB problem in this paper: that is, to try to \"complete the picture\" in as much detail as possible, so that ultimately all we need to do is condition. In practice, this may be unneces sarily complex and shortcuts (such as CE) might exist. However, it would be useful to understand better the assumptions that are necessary for their use to cor respond to conditioning. In any case we believe that some of the issues we addressed cannot be avoided: it will never be sensible to blindly apply a rule, like CE, to all information that merely \"seems\" probabilistic or can be reformulated as such. Rather, one must always think carefully about precisely what the information means.\nAcknowledgments\nWe thank Teddy Seidenfeld and Bas van Fraa.ssen for useful comments. The second author's work was sup ported in part by the NSF, under grant IRl-96-25901, and the Air Force Office of Scientific Research ( AFSC), under grant F94620-96-1-0323.\n214 Grove and Halpern\nReferences Bacchus, F., A. J. Grove, J. Y. Halpern,\nand D. Koller (1994). Generating new be liefs from old. In Proc. Tenth Annual Confer ence on Uncertainty Artificial Intelligence, pp. 37-45. Available by anonymous ftp from lo gos.uwaterloo.cajpub/bacchus or via WWW at http:/ /logos.uwaterloo.ca.\nDiaconis, P. and S. L. Zabell (1982). Updating sub jective probability. Journal of the American Sta tistical Society 77(380), 822-830.\nHowson, C. and P. Urbach (1989). Scientific Rea soning: The Bayesian Approach. La Salle, Illi nois: Open Court.\nJaynes, E . T. (1983). Papers on Probability, Statis tics, and Statistical Physics. Dordrecht, Nether lands: Riedel. Edited by R. Rosenkrantz.\nJeffrey, R. C. (1983). The Logic of Decision. Chicago: University of Chicago Press. First Edi tion Published 1965.\nKullback, S. and R. A. Leibler (1951). On infor mation and sufficiency. Annals of Mathematical Statistics 22, 76-86.\nPapoulis, A. (1984). Probability, Random Variables, and Stochastic Processes. Chicago: McGraw Hill.\nParis, J. B. and A. Vencovska (1992). A method for updating justifying minimum cross entropy. International Journal of Approximate Reason ing 7, 1-18. Seidenfeld, T. (1987). Entropy and uncertainty. In I. B. MacNeill and G. J. Umphrey (Eds.), Foun dations of Statistical Inference, pp. 259-287. Rei del. An earlier version appeared in Philosophy of Science, vol. 53, pp. 467-491. Seidenfeld, T. (1997). Personal communication. Shore, J. E. and R. W. Johnson (1980). Axiomatic\nderivation of the principle of maximum entropy and the principle of minimimum cross-entropy. IEEE Transactions on Information Theory IT26(1), 26-37.\nUffink, J. (1995). Can the maximum entropy prin ciple be explained as a consistency requirement? Stud. Hist. Phil. Mod. Phys. 26(3), 223-261.\nUffink, J. (1996). The constraint rule of the max imum entropy principle. Stud. Hist. Phil. Mod. Phys. 27(1), 47-79.\nvan Fraassen, B. C. (1980). Rational belief and prob ability kinematics. Philosophy of Science 41, 165-187.\nvan Fraassen, B. C. (1981). A problem for relative information minimizers. British Journal for the Philosophy of Science 32, 375-379.\nvan Fraassen, B. C. (1987). Symmetries of personal probability kinematics. In N. Rescher (Ed.), Sci entific Enquiry in Philsophical Perspective, pp. 183-223. Lanham, Maryland: University Press of America.\nvan Fraassen, B. C., R. I. G. Hughes, and G. Har man (1986). A problem for relative information minimizers in probability kinematics, continued. British Journal for the Philosophy of Science 37, 453-475."}], "references": [{"title": "Generating new be\u00ad liefs from old", "author": ["J.Y. Halpern", "D. Koller"], "venue": "Proc. Tenth Annual Confer\u00ad ence on Uncertainty Artificial Intelligence, pp. 37-45. Available by anonymous ftp from lo\u00ad", "citeRegEx": "Halpern and Koller,? 1994", "shortCiteRegEx": "Halpern and Koller", "year": 1994}, {"title": "Updating sub\u00ad jective probability", "author": ["P. Diaconis", "S.L. Zabell"], "venue": "Journal of the American Sta\u00ad tistical Society 77(380), 822-830.", "citeRegEx": "Diaconis and Zabell,? 1982", "shortCiteRegEx": "Diaconis and Zabell", "year": 1982}, {"title": "Scientific Rea\u00ad soning: The Bayesian Approach", "author": ["C. Howson", "P. Urbach"], "venue": "La Salle, Illi\u00ad nois: Open Court.", "citeRegEx": "Howson and Urbach,? 1989", "shortCiteRegEx": "Howson and Urbach", "year": 1989}, {"title": "Papers on Probability, Statis\u00ad tics, and Statistical Physics", "author": ["Jaynes", "E . T."], "venue": "Dordrecht, Nether\u00ad lands: Riedel. Edited by R. Rosenkrantz.", "citeRegEx": "Jaynes and T.,? 1983", "shortCiteRegEx": "Jaynes and T.", "year": 1983}, {"title": "The Logic of Decision", "author": ["R.C. Jeffrey"], "venue": "Chicago: University of Chicago Press. First Edi\u00ad tion Published 1965.", "citeRegEx": "Jeffrey,? 1983", "shortCiteRegEx": "Jeffrey", "year": 1983}, {"title": "On infor\u00ad mation and sufficiency", "author": ["S. Kullback", "R.A. Leibler"], "venue": "Annals of Mathematical Statistics 22, 76-86.", "citeRegEx": "Kullback and Leibler,? 1951", "shortCiteRegEx": "Kullback and Leibler", "year": 1951}, {"title": "Probability, Random Variables, and Stochastic Processes", "author": ["A. Papoulis"], "venue": "Chicago: McGraw\u00ad Hill.", "citeRegEx": "Papoulis,? 1984", "shortCiteRegEx": "Papoulis", "year": 1984}, {"title": "A method for updating justifying minimum cross entropy", "author": ["J.B. Paris", "A. Vencovska"], "venue": "International Journal of Approximate Reason\u00ad ing 7, 1-18.", "citeRegEx": "Paris and Vencovska,? 1992", "shortCiteRegEx": "Paris and Vencovska", "year": 1992}, {"title": "Entropy and uncertainty", "author": ["T. Seidenfeld"], "venue": "I. B. MacNeill and G. J. Umphrey (Eds.), Foun\u00ad dations of Statistical Inference, pp. 259-287. Rei\u00ad del. An earlier version appeared in Philosophy of Science, vol. 53, pp. 467-491.", "citeRegEx": "Seidenfeld,? 1987", "shortCiteRegEx": "Seidenfeld", "year": 1987}, {"title": "Axiomatic derivation of the principle of maximum entropy and the principle of minimimum cross-entropy", "author": ["J.E. Shore", "R.W. Johnson"], "venue": "IEEE Transactions on Information Theory IT26(1), 26-37.", "citeRegEx": "Shore and Johnson,? 1980", "shortCiteRegEx": "Shore and Johnson", "year": 1980}, {"title": "Can the maximum entropy prin\u00ad ciple be explained as a consistency requirement? Stud", "author": ["J. Uffink"], "venue": "Hist. Phil. Mod. Phys. 26(3), 223-261.", "citeRegEx": "Uffink,? 1995", "shortCiteRegEx": "Uffink", "year": 1995}, {"title": "The constraint rule of the max\u00ad imum entropy principle", "author": ["J. Uffink"], "venue": "Stud. Hist. Phil. Mod. Phys. 27(1), 47-79.", "citeRegEx": "Uffink,? 1996", "shortCiteRegEx": "Uffink", "year": 1996}, {"title": "Rational belief and prob\u00ad ability kinematics", "author": ["B.C. van Fraassen"], "venue": "Philosophy of Science", "citeRegEx": "Fraassen,? \\Q1980\\E", "shortCiteRegEx": "Fraassen", "year": 1980}, {"title": "A problem for relative information minimizers", "author": ["B.C. van Fraassen"], "venue": "British Journal for the Philosophy of Science", "citeRegEx": "Fraassen,? \\Q1981\\E", "shortCiteRegEx": "Fraassen", "year": 1981}, {"title": "Symmetries of personal probability kinematics", "author": ["B.C. van Fraassen"], "venue": "In N. Rescher (Ed.), Sci\u00ad entific Enquiry in Philsophical Perspective,", "citeRegEx": "Fraassen,? \\Q1987\\E", "shortCiteRegEx": "Fraassen", "year": 1987}], "referenceMentions": [{"referenceID": 12, "context": "In this paper we re-examine such a case: van Fraassen's Judy Benjamin problem [1987], which in essence asks how one might update given the value of a conditional probability.", "startOffset": 45, "endOffset": 85}, {"referenceID": 4, "context": "But many people have argued that conditioning is not a philosophically adequate answer (in particular, [Jeffrey 1983]).", "startOffset": 103, "endOffset": 117}, {"referenceID": 1, "context": ", see [Bacchus, Grove, Halpern, and Koller 1994; Diaconis and Zabell 1982; Jeffrey 1983; Jaynes 1983; Paris and Vencovska 1992; Uffink 1995]).", "startOffset": 6, "endOffset": 140}, {"referenceID": 4, "context": ", see [Bacchus, Grove, Halpern, and Koller 1994; Diaconis and Zabell 1982; Jeffrey 1983; Jaynes 1983; Paris and Vencovska 1992; Uffink 1995]).", "startOffset": 6, "endOffset": 140}, {"referenceID": 7, "context": ", see [Bacchus, Grove, Halpern, and Koller 1994; Diaconis and Zabell 1982; Jeffrey 1983; Jaynes 1983; Paris and Vencovska 1992; Uffink 1995]).", "startOffset": 6, "endOffset": 140}, {"referenceID": 10, "context": ", see [Bacchus, Grove, Halpern, and Koller 1994; Diaconis and Zabell 1982; Jeffrey 1983; Jaynes 1983; Paris and Vencovska 1992; Uffink 1995]).", "startOffset": 6, "endOffset": 140}, {"referenceID": 5, "context": "Certainly the best known and most studied of these proposals is to use the rule of minimizing cross-entropy [Kullback and Leibler 1951] as a way of updating with general probabilistic information.", "startOffset": 108, "endOffset": 135}, {"referenceID": 4, "context": "This rule can also be shown to generalize Jeffrey's rule [Jeffrey 1983], which in turn generalizes conditioning.", "startOffset": 57, "endOffset": 71}, {"referenceID": 9, "context": "But is cross-entropy ( CE) really such a good rule? The traditional justifications of CE are that it satisfies vari\u00ad ous sets of criteria (such as those of [Shore and Johnson 1980]) which, while plausible, are certainly not com\u00ad pelling [Uffink 1995].", "startOffset": 156, "endOffset": 180}, {"referenceID": 10, "context": "But is cross-entropy ( CE) really such a good rule? The traditional justifications of CE are that it satisfies vari\u00ad ous sets of criteria (such as those of [Shore and Johnson 1980]) which, while plausible, are certainly not com\u00ad pelling [Uffink 1995].", "startOffset": 237, "endOffset": 250}, {"referenceID": 9, "context": "But is cross-entropy ( CE) really such a good rule? The traditional justifications of CE are that it satisfies vari\u00ad ous sets of criteria (such as those of [Shore and Johnson 1980]) which, while plausible, are certainly not com\u00ad pelling [Uffink 1995]. Van Fraassen, in a paper entitled \"A problem for relative information [CE] minimizers in probability kinematics\" [1981] instead approached the question in a different way: he looked at how CE behaves on a simple specific example.", "startOffset": 157, "endOffset": 372}, {"referenceID": 12, "context": "Van Fraassen explains the JB problem as follows [1987]:", "startOffset": 4, "endOffset": 55}, {"referenceID": 12, "context": "Van Fraassen [1987] notes that", "startOffset": 4, "endOffset": 20}, {"referenceID": 12, "context": "in Blue territory even before hearing the message? As van Fraassen [1981] says, as part of an extended dis\u00ad cussion of this phenomenon:", "startOffset": 58, "endOffset": 74}, {"referenceID": 12, "context": "But if o: = 0 then, to quote van Fraassen [1987], \"he would have told her, in effect, 'You are not in Red", "startOffset": 33, "endOffset": 49}, {"referenceID": 8, "context": "cussed and criticized in a more general setting [Seidenfeld 1987].", "startOffset": 48, "endOffset": 65}, {"referenceID": 10, "context": "Even families of rules, such as van Fraassen proposes, are not so bad: after one has chosen a rule (usually by selecting a single real-valued parameter [Uffink 1995]), the rest is again mechanical, general, and determinate.", "startOffset": 152, "endOffset": 165}, {"referenceID": 2, "context": "mi\u00ad form in a continuous space [Howson and Urbach 1989].", "startOffset": 31, "endOffset": 55}, {"referenceID": 6, "context": "Further details can be found in any standard text on probability (for instance [Papoulis 1984]).", "startOffset": 79, "endOffset": 94}], "year": 2011, "abstractText": "Conditioning is the generally agreed-upon method for updating probability distribu\u00ad tions when one learns that an event is cer\u00ad tainly true. But it has been argued that we need other rules, in particular the rule of cross-entropy minimization, to handle up\u00ad dates that involve uncertain information. In this paper we re-examine such a case: van Fraassen's Judy Benjamin problem [1987], which in essence asks how one might update given the value of a conditional probability. We argue that-contrary to the suggestions in the literature-it is possible to use simple conditionalization in this case, and thereby obtain answers that agree fully with intu\u00ad ition. This contrasts with proposals such as cross-entropy, which are easier to apply but can give unsatisfactory answers. Based on the lessons from this example, we speculate on some general philosophical issues concern\u00ad ing probability update.", "creator": "pdftk 1.41 - www.pdftk.com"}}}