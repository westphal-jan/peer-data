{"id": "1303.4169", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2013", "title": "Markov Chain Monte Carlo for Arrangement of Hyperplanes in Locality-Sensitive Hashing", "abstract": "Since Hamming distances can be calculated by bitwise computations, they can be calculated with less computational load than L2 distances. Similarity searches can therefore be performed faster in Hamming distance space. The elements of Hamming distance space are bit strings. On the other hand, the arrangement of hyperplanes induce the transformation from the feature vectors into feature bit strings. This transformation method is a type of locality-sensitive hashing that has been attracting attention as a way of performing approximate similarity searches at high speed. Supervised learning of hyperplane arrangements allows us to obtain a method that transforms them into feature bit strings reflecting the information of labels applied to higher-dimensional feature vectors. In this p aper, we propose a supervised learning method for hyperplane arrangements in feature space that uses a Markov chain Monte Carlo (MCMC) method. We consider the probability density functions used during learning, and evaluate their performance. We also consider the sampling method for learning data pairs needed in learning, and we evaluate its performance. We confirm that the accuracy of this learning method when using a suitable probability density function and sampling method is greater than the accuracy of existing learning methods.", "histories": [["v1", "Mon, 18 Mar 2013 07:14:15 GMT  (114kb)", "http://arxiv.org/abs/1303.4169v1", "13 pages, 10 figures"]], "COMMENTS": "13 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yui noma", "makiko konoshima"], "accepted": false, "id": "1303.4169"}, "pdf": {"name": "1303.4169.pdf", "metadata": {"source": "CRF", "title": "Markov Chain Monte Carlo for Arrangement of Hyperplanes in Locality-Sensitive Hashing", "authors": ["Yui Noma", "Makiko Konoshima"], "emails": ["makiko@jp.fujitsu.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 3.\n41 69\nv1 [\ncs .L\nG ]\n1 8\nM ar\n2 01\n3\nSince Hamming distances can be calculated by bitwise computations, they can be calculated with less computational load than L2 distances. Similarity searches can therefore be performed faster in Hamming distance space. The elements of Hamming distance space are bit strings. On the other hand, the arrangement of hyperplanes induce the transformation from the feature vectors into feature bit strings. This transformation method is a type of locality-sensitive hashing that has been attracting attention as a way of performing approximate similarity searches at high speed. Supervised learning of hyperplane arrangements allows us to obtain a method that transforms them into feature bit strings reflecting the information of labels applied to higher-dimensional feature vectors. In this paper, we propose a supervised learning method for hyperplane arrangements in feature space that uses a Markov chain Monte Carlo (MCMC) method. We consider the probability density functions used during learning, and evaluate their performance. We also consider the sampling method for learning data pairs needed in learning, and we evaluate its performance. We confirm that the accuracy of this learning method when using a suitable probability density function and sampling method is greater than the accuracy of existing learning methods. Keyword: Higher dimensional feature vector, Locality-sensitive hashing, Arrangement of hyperplanes, Similarity search, Markov chain Monte Carlo, Low-temperature limit"}, {"heading": "1 Introduction", "text": "Unstructured data such as audio and images includes complex content. This makes it difficult to search for unstructured data directly. A common approach has therefore been to perform searches based on feature vectors extracted from unstructured data. To reflect the complexity of unstructured data, these feature vectors generally consist of higher-dimensional data with hundreds or even thousands of dimensions. There are a wide range of applications for high-speed similarity searching using higher-dimensional feature\n\u2217Software Systems Laboratories, FUJITSU LABORATORIES LTD. 1-1, Kamikodanaka 4-chome, Nakahara-ku Kawasaki, 211-8588 Japan. \u2020E-mail: makiko@jp.fujitsu.com\nquantities extracted from unstructured data. Examples include authentication of people by fingerprint recognition, speech recognition in call centers, management of products and components based on CAD data, and detecting abnormal situations from surveillance video. For these applications, there are two things that are very important. One is a high-speed similarity search method. The other is a data structure that permits high-speed similarity searching, and a method for extracting feature quantities that reflect the properties of unstructured data.\nThe high-speed similarity search method is described first. To perform a similarity search, the feature space should be a metric space. In most cases, the feature space is treated as an L2 metric space. Many studies have devised an index structure aimed at performing similarity searches at high speed. For example, the literatures [1, 2] are two of them. However, in higher-dimensional space, due to the so-called \u201cthe curse of dimensionality\u201d, all distances between data items are of similar size. Consequently, searches in higher-dimensional data using these methods end up having processing times that are similar to those of searches performed without using a special index [3].\nHamming distances can be calculated by bitwise operations, which means that similarity searches are fast in Hamming metric space without using a specific index structure. In a method called locality-sensitive hashing [4], the feature vectors are transformed into bit strings. For this transformation, methods that involve the use of hyperplanes in feature space have been intensively studied [5\u20138]. In these methods, multiple hyperplanes are considered as a means of partitioning the feature space. A bit string is assigned to each partitioned region, determined from the orientations of hyperplanes. Feature vectors extracted from the data are allocated in the same way as bit strings assigned to the regions that include the feature vectors. Similar feature vectors are included in neighboring regions, so the bit strings allocated to these feature vectors are similar and are separated by small Hamming distances. In the following, we will use the term \u201chashing\u201d to refer to the process of transforming higher-dimensional feature vectors into feature bit strings.\nNext, we consider a data structure that can be searched at high speed, and a method for extracting feature quantities reflecting the properties of unstructured data. From the above discussion, we decided to use bit strings as feature quantities, since these are data\nstructures that can be searched at high speed. When data has been labeled, supervised learning can be used to extract feature bit strings that reflect the labeled information. In the following, feature quantities that reflect the labeled information are described as highprecision quantities. Also, a learning method that can extract high-precision feature quantities is described as a high-performance learning method.\nStudies aimed at increasing the precision of feature quantities associated with the hyperplane hashing method include the following references [9\u201311]. In learning, the normal vectors of the hyperplanes are determined by making the Hamming distances smaller between data pairs with a common label and larger between data pairs that do not have any common label. As the number of bits increases, the degree of freedom also increases so that greater precision becomes possible.\nBased on this reasoning, we can draw the following conclusions regarding high-precision similarity searching of large quantities of unstructured data. Highspeed similarity searches are achieved by using bit strings in Hamming metric space as feature quantities. High precision is achieved by using a large number of bits and performing supervised learning with labeled data as the training data. However, to reflect the complexity of unstructured data, it should be noted that a single item of unstructured data will not necessarily have just one label.\nIn this paper, apart from the use of feature bit strings, no consideration is given to the processing time of the similarity search. Our main focus is on using supervised learning to improve the precision of feature bit strings.\nThe method proposed in this paper performs supervised learning using MCMC. The transformation of feature vectors into feature bit strings is a discontinuous mapping. This makes it impossible to perform na\u0308\u0131ve learning based on gradients. Another approach involves introducing a loss function so that the transformation method can be approximated by a continuous function. However, the only loss functions found so far are strongly dependent on the properties of the data set. In our proposed method, each normal vector is regarded as a particle on a unit sphere in feature space, and a random walk is performed on this unit sphere. In the random walk, a discontinuous function can be treated as an evaluation function. We also considered sampling methods for training data pairs and evaluation functions for use in learning.\nThis paper is structured as follows. First, in section 2 we describe the existing learning methods. Then in section 3, we describe our proposed method. We considered evaluation functions needed during learning, and sampling methods for training data pairs. In section 4, we perform experiments using various data sets. At the same time, we also evaluate the evaluation functions and the sampling methods. In section 5, we show that the proposed learning method performs\nbetter than existing methods. Finally, in section 6 we summarize our work and discuss the future prospects of this approach."}, {"heading": "2 Background and related work", "text": "In this section, we describe the use of hyperplanes for locality-sensitive hashing, which is the basis of the proposed technique. We then describe some related existing techniques."}, {"heading": "2.1 Conventional locality-sensitive hashing with hyperplanes", "text": "The hashing method using hyperplanes is described below. A space V in which there are higher-dimensional feature quantities is regarded as an N -dimensional vector space. The configurations of multiple hyperplanes in V are referred to as hyperplane arrangements. Consider B hyperplanes passing through the origin of V . A hyperplane passing through the origin is identified by its normal vector. An N -dimensional feature vector ~x is transformed into a bit string by registering a 1 if its dot product with each normal vector is positive, and a zero otherwise. Therefore, the length of the bit string is equal to the number of hyperplanes B.\nA hyperplane that does not pass through the origin can easily be constructed from a hyperplane that does. In reference [11], an experiment is performed where the hashing of hyperplanes that do not pass through the origin is learned by learning the hashing of hyperplanes that do pass through the origin. When developing a new learning method for hyperplanes, it is easier to work with hyperplanes that pass through the origin. In the following discussion, therefore, all hyperplanes are assumed to pass through the origin.\nWhen labels have been applied to the data, it is sometimes the case that the angles or L2 distances do not exhibit a suitable degree of dissimilarity. In such cases, the hyperplanes can be determined by supervised learning. In supervised learning, the hyperplanes are determined so that data pairs with a common label are separated by small Hamming distances, and data pairs that do not have any common label are separated by large Hamming distances.\nA single hyperplane can be specified by specifying its normal vector. Since the length of the normal vector specifying a hyperplane is immaterial, these lengths are chosen so that the configuration space of normal vectors corresponds to an N \u2212 1-dimensional hypersphere SN\u22121. When distinguishing between B hyperplanes, the configuration space of the hyperplanes is (SN\u22121)B . In one hashing method, the B hyperplanes are set randomly [5]. In the following, this is referred to as the LSH method.\nOther references such as [6\u201310] describe hashing methods that use hyperplanes. In particular, MLH [9] and S-LSH [10] are described in subsections 2.2 and 2.3."}, {"heading": "2.2 Minimal loss hashing", "text": "One existing learning method is Minimal Loss Hashing (MLH) [9]. In MLH, the aim is to minimize an empirical loss function on (SN\u22121)B. However, the empirical loss function is discontinuous, so it is not possible to use learning methods based on gradients. Therefore, the empirical loss is replaced by a differentiable upper bound function g, and gradients are used to minimize g instead. The point that gives the minimum value determines the coordinates of the B learned hyperplanes. Function g has several parameters that need to be adjusted. Some of these parameters are dependent on the data pairs used for training. All the data pairs for learning are chosen at random."}, {"heading": "2.3 Locality-sensitive hashing with margin based feature selection", "text": "In this subsection, we describe the concept of an existing learning method called locality-sensitive hashing with margin based feature selection (S-LSH) [10]. In S-LSH, the normal vectors of hyperplanes are not used directly for learning. B\u0303 hyperplanes (B\u0303 > B) are randomly provided. A degree of importance is allocated to each hyperplane, and these degrees of importance are calculated by learning. The degrees of importance are arranged in descending order, and the topmost B normal vectors are selected. The distance calculations during learning are performed using weighted Hamming distances. Two types of data pairs are used during learning. The learning data pairs are selected as follows. A feature vector a is randomly selected from the learning data. The first type of data pair consists of the pair (a, b), where b is the feature vector with the smallest weighted Hamming distance in the data set that has a common label as a. The second type of data pair consists of the pair (a, c), where c is the feature vector with the smallest weighted Hamming distance in the data set that does not have any common label as a.\nS-LSH has been shown to have good learning performance in many data sets [10]. It is particularly effective for learning in cases where there are many labels, and data with the same label has little cardinality."}, {"heading": "3 The proposed method", "text": ""}, {"heading": "3.1 Motivation", "text": "To perform a high-speed similarity search that accurately represents the latent similarities of unstructured data, we consider performing learning with a greater number of bits B. In learning, the normal vectors of the hyperplanes are determined by making the Hamming distances smaller between data pairs with a common label and larger between data pairs that do not have any common label. In the following, we will refer to a data pair with a common label as a \u201cpositive\npair\u201d, and to a data pair that do not have any common label as a \u201cnegative pair\u201d.\nThe configuration space of a set of B hyperplanes is (SN\u22121)B . When there is an evaluation function U \u2032 on (SN\u22121)B that has the following performance, learning a set of hyperplanes can be regarded as an optimization problem that globally maximizes U \u2032. The argument of U \u2032 is the arrangement of multiple hyperplanes. Each hyperplane divides the feature space V into two regions. The value of U \u2032 increases as the number of positive pairs whose feature vectors are in a same region and negative pairs whose feature vectors are in different regions increase. In most cases, a function U \u2032 having this property is thought to have multiple local maxima. When B is large, the dimension of (SN\u22121)B increases and it becomes harder to solve the optimization problem. Since we are concerned here with hashing using hyperplanes, the values of function U \u2032 can also be discrete. Therefore, it is unnatural to require continuity of the U \u2032 configuration space (SN\u22121)B . Since U \u2032 is not necessarily differentiable, it cannot be globally maximized by methods that use the gradient of U \u2032.\nInstead of solving an optimization problem in (SN\u22121)B , we can consider a method where optimization problems in SN\u22121 are solved B times, and these solutions are bundled together. That is, instead of learning a set of B hyperplanes, the individual hyperplanes are separately learned and the results are bundled together. However, if we obtain B solutions to the optimization problem in SN\u22121, then the performance is severely impaired for the following reason. Consider an evaluation function U on SN\u22121. Assume that the points SN\u22121 where the value of U is larger correspond to a good hyperplane. That is, we assume the following property. When the feature space V is partitioned into two regions by a single hyperplane, the value of U increases as the number of positive pairs whose feature vectors are in a same region and negative pairs whose feature vectors are in the different regions increase. A specific example of an evaluation function is shown in subsection 3.3. We will assume that the evaluation function U has a global maximum value on p\u2217 \u2208 SN\u22121. If all the hyperplanes exist in p\u2217, then they are all degenerate. In this case, the feature space V is only divided into two regions, and there are only two types of representative bit string. Clearly it would not be possible to capture the features of unstructured data with these bit strings. For this reason, when we consider bundling together the learning results of individual hyperplanes, it can be said that individual hyperplanes are not necessarily learned by finding the point where the evaluation function U on SN\u22121 is maximized. Therefore, in the following we consider finding B points where the evaluation function U has a local maximum value in SN\u22121 when performing learning with individual hyperplanes. Multiple hyperplanes are learned by bundling these together. Here, we must ensure that the multiple hyperplanes are not oriented in the same direction.\nIn the remainder of this section, we describe the proposed method, which is a hyperplane normal vector learning method (referred to below as M-LSH) based on the Markov Chain Monte Carlo method. At the same time, we consider and discuss a number of evaluation functions (which have a strong influence on the performance of M-LSH learning), and data pair subsampling methods."}, {"heading": "3.2 Learning hyperplanes with the Markov chain Monte Carlo method", "text": "In this section, we describe the proposed method. Our proposed method is a supervised learning method for hyperplanes using MCMC. The aim of this learning method is to probabilistically determine the point where the evaluation function U reaches its local maximum value. The advantage of this method is that it does not require a differentiable evaluation function. Its disadvantage is that because it uses a Monte Carlo method, the point where the evaluation function is locally maximized cannot be determined with perfect accuracy. However, from the properties of MCMC, the learned results are highly likely to be close to the point where the evaluation function is locally maximized. The probability that a particle is in such a place is high so that the local maximum value is high, and the peak is sharp.\nIn the following, we will assume that the evaluation function U is a function whose values are positive and are bounded on SN\u22121. Also, the details of the evaluation function are assumed to depend on the training data pairs. Specific examples of U are given in subsection 3.3. Consider a particle on SN\u22121 whose position equates to the normal vector of a hyperplane. If U\u0303 := \u2212U is regarded as the potential energy, then to obtain the local minimum value of U\u0303 , we need to consider the motion of dissipative particles. However, since U is generally not differentiable, we cannot use optimization methods based on gradients, that is, continuous particle motions. Therefore we can consider obtaining a minimum solution by a random walk method. We regard U as the probability density function of SN\u22121 (except for a normalization constant), and use MCMC to evaluate the temporal evolution of particles. This method is our proposed M-LSH method. We use the Metropolis-Hastings algorithm for MCMC [12]. For the proposed density function, we use the normal distribution. In M-LSH, particles perform random walks a fixed number of times. This is the temporal evolution of the particles. We refer to this temporal evolution as a single batch process. Since the details of the evaluation function are determined by deciding on the training data pairs, the handling of the training data pairs may lead to incidental local maximum values of the evaluation function. To prevent the particles from becoming trapped at this sort of point, batch processing is performed a number of times, and\nthe learning data pairs are replaced for each batch process.\nBy performing learning with multiple hyperplanes, we obtain multiple points where the evaluation function is locally maximized. As described in subsection 3.1, it is necessary to prevent the learning of points where multiple hyperplanes produce the same local maximum value. In M-LSH, this issue is resolved by using the following method. MCMC exhibits a property whereby particles tend to accumulate at places where the probability density function is locally maximized. The sharper the peak in the evaluation function close to the local maximum value, the more intense this trend becomes. In most cases, since U is multimodal, making its peaks sharper and randomly setting the initial positions of the particles will cause the particles to collect at peaks close to their initial positions. Therefore, we can prevent the particles from all moving towards the same point.\nMany variants of M-LSH can be considered. These variants can be obtained by combining the evaluation function U with sampling methods for training data pairs that determine the details of the evaluation function. Table 1 lists these combinations. Each of these items is described below in subsections 3.3 and 3.4."}, {"heading": "3.3 Evaluation function", "text": "When learning is performed by M-LSH, the type of evaluation function must be determined. A number of possible evaluation function types are considered below. First we will introduce some nomenclature. PP denotes the set of all given positive pairs, and NP denotes the set of all given negative pairs. The angles subtended by the two feature vectors of a pair p relative to the normal vector of a hyperplane are \u03b81(p) and \u03b82(p), respectively. The following subsets are defined.\nPP+ := {p \u2208 PP | cos(\u03b81(p)) \u2217 cos(\u03b82(p)) > 0} ,(1)\nNP\u2212 := {p \u2208 NP | cos(\u03b81(p)) \u2217 cos(\u03b82(p)) < 0} .(2)\nIn the following, the cardinality of a set A is denoted by #A.\nIn the following formulas, we assume an evaluation function U = exp(x/T ) that uses an enumerated value x. Here, T = 1.\nCOUNT\nx = #PP+ +#NP\u2212. (3)\nRATIO\nx = #PP+ #PP + #NP\u2212 #NP . (4)\nCOSINE\nx = \u2211\np\u2208PP\n| cos(\u03b81(p)) + cos(\u03b82(p))|\n+ \u2211\np\u2208NP\n| cos(\u03b81(p))\u2212 cos(\u03b82(p))|. (5)\nCOSINE RATIO\nx = 1\n#PP\n\u2211\np\u2208PP\n| cos(\u03b81(p)) + cos(\u03b82(p))|\n+ 1\n#NP\n\u2211\np\u2208NP\n| cos(\u03b81(p))\u2212 cos(\u03b82(p))|.(6)\nIf \u2212x and T are regarded as the particle\u2019s energy and temperature respectively, then U can be regarded as a Boltzmann weight. From this perspective, the low-temperature limit is where T is zero, and the hightemperature limit is where T is \u221e."}, {"heading": "3.4 Sampling method for training data", "text": "The evaluation function defined in subsection 3.3 must include both PP and NP . PP and NP can be determined by considering all combinations of the training data. However, the way in which the data pairs are obtained means that the number of pairs is only half the square of the number of training data items. When the cardinality of PP and NP is large, it can take a long time to calculate the evaluation function. Therefore in this subsection we consider a number of different selection methods for PP and NP , and we discuss their advantages and disadvantages. Here, we will use the term \u201cdistance\u201d to refer to L2 distance unless otherwise noted. We will also use the following nomenclature. L is the set of all the training data. La represents a data set having a common label as an element a \u2208 L, and Lca represents L \\ La. The distance between two elements a, b \u2208 L is denoted by dist(a, b). We will start by discussing the selection method for NP . We will consider the following sampling method.\nRandommiss\nAfter a \u2208 L has been randomly selected, b \u2208 Lca is randomly selected to form a negative pair (a, b).\nNearmiss\nAfter a \u2208 L has been randomly selected, a negative pair (a, b) is formed such that b := argminc\u2208Lc\na (dist(a, c)).\nBoundarymiss\nAfter a \u2208 L has been randomly selected, a negative pair (a\u2032, b) is formed such that b := argminc\u2208Lc\na (dist(a, c)) and\na\u2032 := argminc\u2208La\u2229Lcb(dist(b, c)).\nSince Boundarymiss as defined above is a new sampling method, we will describe it in more detail here. Consider two elements a, b \u2208 L that do not have any common label and La\u2229Lb 6= \u2205. Since the labels applied to unstructured data can be of more than one type, this sort of situation can occur frequently. Since the distributions of La and Lb are overlapping, it is not possible to obtain a hyperplane that separates them completely. If we are allowed to bisect La \u2229 Lb with a hyperplane, then it may also be possible to separate the difference sets La\\Lb and Lb \\La. To learn a hyperplane that bisects La \u2229Lb, we can form a negative pair by selecting one data item from each of La\\Lb and Lb\\La. Boundarymiss is one of the ways in which negative pairs of this sort can be made. Furthermore, Boundarymiss is expected to lie close to the boundary between La \\ Lb and Lb \\ La. Please refer to Fig. 2. In Randommiss sampling, there is a high possibility of selecting a pair comprising an element close to the center of gravity of La and an element close to the center of gravity of Lca. This makes it easier to learn a hyperplane that separates the center of gravity of La from the center of gravity of L c a. When L c a is distributed over a broader region than La, it is expected that the resulting hyperplane will be deviated from the boundary of La and L c a. In particular, when the number of labels applied to the training data is large and the sets of each label have similar cardinality, the distribution of Lca tends to become broader than that of La, so the tendency for the learned hyperplanes to be separated from the boundary is thought to become more pronounced as the number of labels increases. Figure 2 shows some typical data pairs obtained by Randommiss sampling, and the hyperplanes learned from these pairs. The dotted circles in these figures show the approximate regions over which these sets are distributed. In Nearmiss sampling, an element selected from Lca lies close to the boundary of La and L c a, so it is possible to avoid the above drawback of the Randommiss sampling method. However, when La is distributed over a wide region, there is a greater likelihood of a \u2208 L being deviated from the boundary between La and L c a. Figure 2 shows some typical data pairs obtained by Nearmiss sampling, and the hyperplanes learned from these pairs. When Boundarymiss sampling is performed, it can compensate for the abovementioned drawbacks of the Nearmiss method, but is liable to choose data pairs\nthat are separated by smaller distances and is therefore more susceptible to noise in the data. We will now describe the selection method for PP . For the reasons discussed below, it is better to consider the handling of positive pairs in terms of applying corrections to the discriminant planes used for the discrimination of negative pairs. For example, if we learn with only positive pairs, because the hyperplanes should not separate feature vectors in each positive pair, the hyperplanes may stay away from all the feature vectors. In this case, all the bit strings of the training data will be identical, making it impossible to separate the feature vectors. We therefore consider positive pairs to have the role of preventing La from becoming separated by the hyperplane. We will consider the following sampling methods for positive pairs.\nRandomhit\nAfter a \u2208 L has been randomly selected, b \u2208 La is randomly selected to form a positive pair (a, b).\nNearhit\nAfter a \u2208 L has been randomly selected, a positive pair (a, b) is formed such that b := argminc\u2208La(dist(a, c)).\nFarhit\nAfter a \u2208 L has been randomly selected, a positive pair (a, b) is formed such that b := argmaxc\u2208La(dist(a, c)).\nWe consider a data set whose element have a single label. In this case, it is considered that Farhit sampling frail against outliers. It is thought that Randomhit sampling is robust against outliers. Nearhit sampling is expected to have poor performance because it is not possible to prevent data other than the selected data pair in La from being arranged in different directions of the hyperplane. It is thought that this performance degradation is particularly severe when there are many data items with the same label. We consider a data set whose element have an arbitrary number of labels. Here, we consider the case where there are three positive pairs (a1, b1), (a2, b2), (a3, b3) \u2208 PP , such that a1 /\u2208 La2 \u2227 b1 \u2208 Lb2 \u2227 a3, b3 \u2208 Lb2 . In particular, when a3 and b3 are close to b1 and b2 respectively, we shall refer to these data pairs as overlapping data pairs. This is summarized in Fig. 3. When learning is performed in this case, it becomes difficult to separate La1 and La2 from Lb2 . As the number of sampling pairs increases, it is thought that overlapping data pairs will become more common. It is therefore expected that Farhit sampling and Randomhit sampling will cause the performance to become worse. In the case of Nearhit sampling, since a3 and b3 are less likely to be close to b1 and b2, it is thought that performance degradation will be less likely to occur. Based on this reasoning, there are as many possible sampling methods as there are combinations of positive pair and negative pair sampling methods. The\nsampling methods that we actually evaluated and compared are as follows1 .\n\u2022 Randomhit-Randommiss\n\u2022 Randomhit-Nearmiss\n\u2022 Nearhit-Nearmiss\n\u2022 Farhit-Nearmiss\n\u2022 Randomhit-Boundarymiss"}, {"heading": "4 Experiments and evaluation", "text": "We performed experiments to measure the effects of the proposed method on a number of different data sets. In these experiments, supervised learning was performed on data that had already been labeled. The data labels used in these experiments are all known. When the search results are obtained, the Hamming distance between the query and data in the database is calculated, and the top search results are ordered in ascending order of distance. The acquisition rate is defined for this purpose as follows.\nAcquisition := Number of data acquired by search\nTotal number of data searched . (7)\nTo evaluate the performance, we used the precision rate and recall rate as defined below.\nPrecision :=\nNumber of data items with a com-\nmon label as the query for which\nsearch results were obtained Number of data items for which\nsearch results were obtained\n,(8)\nRecall :=\nNumber of data items with a com-\nmon label as the query for which\nsearch results were obtained Number of data items with a com-\nmon as the query among all relevant\nitems\n.(9)\nA recall-precision curve shows the variation of recall rate and precision rate with changes in the acquisition rate. Better search performance is indicated by a recall rate and precision rate with values closer to 1. In the experiments, by way of reference, we also calculated the precision rate and recall rate in similarity searches based on L2 distances using the original feature vectors.\n1 We believe that this choice is natural.\nThe remainder of this section is structured as follows. First, we confirm the benefits of M-LSH on learning with an artificial data set that we prepared. We then evaluate the performance of the evaluation functions and sampling methods considered in sections 3.3 and 3.4. For this performance evaluation, we used actual data sets instead of our prepared data set. Finally we show how our proposed method differs from existing learning methods."}, {"heading": "4.1 Experiments with an artificial data set", "text": "Using an artificial data set, we confirmed the effects of M-LSH on learning. This artificial data set consists of 300 data items sampled from a three-dimensional standard normal distribution. With the axes labeled as x, y and z, we classified the data items into two classes according to whether the x component was positive or nonpositive. As can be seen from the way in which the data is labeled, we desire a hyperplane whose normal vector ~n is ~n = (\u00b11, 0, 0). Figure 4 shows the effects of LSH and M-LSH on learning with a bit string length of 1,024. The parameters of learning with M-LSH were as follows: number of processing batches: 5, number of temporal evolution steps in batch processing: 100, number of data pairs used for learning in each batch process: 2,000, number of evaluation functions used during learning: COUNT, sampling method: Randomhit-Randommiss, with equal numbers of positive and negative pairs.\nFrom Fig. 4, we can see the following. From the scatter diagram and x component histogram of the normal vectors obtained by LSH, we can see that the normal vectors are uniformly distributed on a two-dimensional sphere. From the scatter diagram and x component histogram of the normal vectors obtained by M-LSH, we can see that most of the normal vectors are distributed in the vicinity of ~n = (\u00b11, 0, 0). Figure 5 shows the precision rates and recall rates of LSH and M-LSH. As expected from the distribution of normal vectors, Fig. 5 shows that M-LSH has a positive effect\non learning."}, {"heading": "4.2 Experimental data", "text": "In this subsection, we describe the experimental data used in the performance evaluations performed in subsections 4.3 and 4.4.\nThe experimental data was obtained from the following sources.\n\u2022 MNIST Scanned images of handwritten numerals 0\u20139 [13]. Each digit is stored as a 28 \u00d7 28-pixel 8-bit grayscale image, and is labeled with the corresponding digit 0\u20139. We used the images themselves as feature quantities. Therefore, the feature quantities had 784 dimensions.\n\u2022 Fingerprint images Fingerprint image data acquired using a fingerprint image scanner. The feature quantities consisted of the 4,096-dimensional Fourier spectra of these fingerprint images. Since fingerprints are unique to each human, the data was labeled with the names of the corresponding individuals. In other words, each feature vector was given just one label. For details, see Ref. [10].\n\u2022 Speech features A set of 200-dimensional mel frequency cepstral coefficient (MFCC) feature quantities extracted from a three-hour recording of a local government assembly published on the Internet [14]. The query was a spoken sound acquired separately. In supervised learning of speech, the contents of the speech are normally labeled with a text transcription. But instead, we treated the features with the top 0.1% shortest Euclidean distances from the queries were regarded to be in the same class. Each feature vector could have multiple labels.\n\u2022 LabelMe LabelMe data using 512-dimensional Gist feature quantities [15] extracted from image data published in Ref. [16]. The labeling was applied to the dissimilarity matrices of distributed data, and the same labels were applied to data corresponding to the topmost 50 rows of data in each row. Each feature vector can have multiple labels.\nThese data sets were selected with the following applications in mind \u2014 MNIST: handwritten number recognition, Fingerprint images: biometric identification, Speech features: speech recognition, LabelMe: automatic image classification.\nThe quality of data used in the experiments is summarized in Table 1. Here, we envisaged performing searches on data recorded in a database, with the data divided into three pairwise disjoint sets: a data set used for learning, the searched data set, and a data set for queries. The learning performance varied widely depending on the number of labels in the data set and on the cardinality of data sets having a common label. However, since the data sets were not all given unique labels, it is not possible to give a na\u0308\u0131ve definition of the label numbers. We therefore reasoned as follows. For the data actually used for learning, the average value of the number of data items having a common label as one item of data is regarded as the rough cardinality of the sets for each label. The rough number of labels is then calculated by dividing the number of data items actually used for learning by the rough cardinality of the sets for each label. This information is summarized in Table 1.\nSince data generally contains noise, noise reduction must be performed. Prior to the experiments, we subjected all the data to the following noise reduction processes. These processes are widely used as noise reduction methods. The feature quantities of the data are higher-dimensional data. Depending on the data set, each component of the data may be expressed in dif-\nferent units. Unless the feature vectors are made dimensionless, they cannot be used for the calculation of distances or angles. We therefore subjected the data to an affine transformation so that the average value of each component of the feature vector of the learning data became 0, and the standard deviation of the learning data became 1. We also performed a principal component analysis for the learning data. This was done by finding the subspace with a cumulative contribution rate of over 80%, and mapping all data to this space. After the above noise reduction process, we performed learning and search tests. Since all the methods were evaluated using data that had been subjected to this noise reduction process, this noise reduction process had no effect on the performance of each method.\nThe parameters of the M-LSH experiments were as follows. Standard deviation of proposed density distribution: 0.01, number of processing batches: 10, number of temporal evolution steps in batch processing: 100, number of data pairs used for learning in each batch process: 2,000 or 20,000."}, {"heading": "4.3 Evaluation function performance", "text": "Here, we evaluate the performance of the evaluation functions cited in subsection 3.3. A natural choice of sampling method is the simplest RandomhitRandommiss sampling method. However, as was found in subsection 4.4, the Randomhit-Randommiss sampling method has poor performance.\nTherefore, we instead used Randomhit-Nearmiss sampling, which is regarded as the next simplest sampling method after Randomhit-Randommiss sampling.\nFigure 6 shows a graph of the precision rate and recall rate for data sets with an acquisition rate of 0.1. However, since different data sets have different precision rates and recall rates, the precision rates and recall rates are scaled where the values of searches using L2 distance are 1. The number of training data pairs used for training M-LSH was 2,000. Since Fig. 6 shows the scaled precision rate or the scaled recall rate, larger values indicate better performance from the learning method.\nAlthough degraded in Fig. 6, the M-LSH performance obtained using RATIO or COSINE RATIO is more or less unchanged from that of LSH. Using COUNT, M-LSH performs better for all data sets. Using COSINE, M-LSH performs worse for all data sets.\nThe reason why M-LSH using RATIO or COSINE RATIO has almost the same performance as LSH is thought to be as follows. The evaluation function includes a parameter T that is analogous to temperature. Since we used a fixed value of T = 1 in this evaluation, the index of the evaluation function is confined to the range [0, 2] or [0, 4]. In this range, a slight change of the normal vector will not cause a large change in the value of the evaluation function. Therefore, the normal vector moves about more or less at random, so no large difference from LSH is obtained. In other words,\nin this evaluation function it can be said that T = 1 corresponds to a high temperature. To increase the performance of the evaluation function, we should use a smaller T (i.e., a lower temperature), and expand the range of the evaluation function index to make the maximum value peaks sharper. However, at this limit, it can be approximated by COUNT. For this reason, at the low temperature limit, it is thought that these two evaluation functions exhibit more or less the same performance as M-LSH when using COUNT.\nIt can be seen that COSINE performed much worse than LSH for the following reason. In COSINE, there is a gentle evaluation function gradient at all points in the region where the normal vector is defined. Therefore, the normal vectors tend to be oriented toward the point that shows a global maximum value. To see that the normal vector actually exists at a point showing the maximum value, we calculated the absolute value of the cosine between normal vectors. A larger absolute value of the cosine means that the vectors are pointing in similar directions. Figure 7 shows the absolute values of the cosines made by M-LSH normal vectors using 32-bit COUNT or COSINE values. In this figure, a matrix is calculated with the absolute values of cosines between 32-bit normal vectors as its constituent values, and these values are represented as a grayscale image. The diagonal elements are all zero. As Fig. 7 clearly shows, almost all of the normal vectors obtained with M-LSH are oriented in similar directions. In the case of COSINE, it is thought that the performance can be improved by taking a low temperature limit, as was the case for RATIO and COSINE RATIO. However, at this limit, COSINE can be approximated by COUNT. Furthermore, since COSINE requires more processing time than COUNT, there is no need to bother using COSINE.\nBased on the above calculation results and discussion, it is thought that using COUNT as the evaluation function is more appropriate from the viewpoint of processing time and performance."}, {"heading": "4.4 Evaluation of sampling methods", "text": "Here, we evaluate the performance of the sampling methods discussed in subsection 3.4. From the discussion of subsection 4.3, we use the M-LSH method with the COUNT evaluation function to evaluate the performance of the sampling methods.\nIn the same way as when evaluating the performance of the evaluation functions, we consider the scaled precision rate and scaled recall rate when the acquisition rate is 0.1. Figure 8 shows a graph of the scaled precision rate and recall rate of each sampling method in MLSH using the COUNT evaluation function with 1,024 bits (except in the batch processing where the number of sample data items used was 2,000.) To evaluate the dependence on the number of sample data items used for training, we also calculated the precision rate and recall rate with 20,000 sample data items, as shown in\nFig. 9.\nFrom Figs. 8 and 9, it can be seen that the performance of M-LSH using Randomhit-Randommiss sampling is very poor for methods other than MNIST. The performance is worse than that of the LSH method.\nThe performance of M-LSH using Farhit-Nearmiss sampling was the best for MNIST. However, it had the worst performance for LabelMe.\nIt can be seen that the performance of M-LSH with the Nearhit-Nearmiss sampling method is depends strongly on the number of training data pairs. For the speech features and LabelMe data sets, the performance improves as the number of training data pairs increases. This performance improvement is thought to be due to the low probability of there being overlapping data pairs. For MNIST, the performance decreases as the number of training data pairs increases. This effect is thought to occur in the following way. As mentioned above, the role of positive pairs is to prevent data sets with a common label from being split by hyperplanes. It is therefore desirable that positive pairs are widely distributed across data sets having a common label. Nearhit sampling creates positive pairs by choosing the closest feature vectors with a common label, so a large number of positive pairs are needed for the distribution of a data set having a common label to be satisfied with a positive pair. In particular, MNIST requires more positive pairs than other data sets because there are a great many data items that have the same label. The role of negative pairs is to\nseparate data sets having different labels. Therefore, a number of negative pairs roughly equal to the square of the number of labels is sufficient. Since the positive pairs and negative pairs were used in equal numbers in these experiments, it seems that the effect of negative pairs in separating data sets having different labels outweighed the effect of positive pairs in preventing the separation of data sets having a common label. We think this is the reason why the performance decreases as the number of training data pairs is increased.\nThe M-LSH method using Randomhit-Nearmiss sampling and Randomhit-Boundarymiss sampling performed well for all data sets, regardless of the number of training data pairs. For MNIST and fingerprint images, the performance improves as the number of training data pairs is increased. However, for the speech features and LabelMe data sets, the performance was found to decrease as the number of training data pairs is increased. This is thought to be due to an increase in the number of overlapping data pairs. No large differences could be observed between these two sampling methods. However, for the speech features and LabelMe data sets, the performance was very slightly better with M-LSH using Randomhit-Boundarymiss sampling.\nBased on these results, it is thought that the appropriate choice of sampling method depends on the properties of the data. Of the sampling methods we tried out in this study, it seems that the following choices are robust methods.\n\u2022 For data sets where each feature vector has unique label: Randomhit-Nearmiss sampling or Randomhit-Boundarymiss sampling\n\u2022 For data sets where each feature vector has multiple labels and there are not many training data pairs: Randomhit-Boundarymiss sampling\n\u2022 For data sets where each feature vector has multiple labels and there are very many training data pairs: Nearhit-Nearmiss sampling"}, {"heading": "5 Comparison with existing", "text": "learning methods\nIn this section, we compare the performance of MLSH with that of the existing learning methods LSH, MLH, and S-LSH. M-LSH uses the COUNT evaluation function and the Randomhit-Boundarymiss sampling method. The number of sample data pairs is 1,000 for both the positive pairs and negative pairs.\nFigure 10 shows the Recall-Precision curves for various different data sets. Here, the number of bits is 1,024. From these results, it can be seen that M-LSH outperforms the existing learning methods for all the data sets apart from LabelMe. In LabelMe, there are small regions where the S-LSH curve rises above the precision and recall curves for M-LSH, but it can be said that better overall performance is obtained with M-LSH.\nFrom the above results, it is concluded that the proposed M-LSH learning method has good hashing performance."}, {"heading": "6 Summary and future works", "text": "In this paper, we proposed a learning method for hyperplanes using MCMC. We also considered evaluation functions and sampling methods used in this learning method, and we evaluated their performance. As a result, we have confirmed that this proposed method exceeds the performance of existing learning methods.\nFinally, we mention the direction of future research. When using the MCMC method for learning, the ultimate positions of particles do not lie at points that maximize the evaluation function. One way in which this problem could be resolved involves recording the particle loci and finding out which point maximizes the evaluation function."}], "references": [{"title": "An optimal algorithm for approximate nearest neighbor searching fixed dimensions", "author": ["Sunil Arya", "David M. Mount", "Nathan S. Netanyahu", "Ruth Silverman", "Angela Y. Wu"], "venue": "J. ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "idistance: An adaptive b+tree based indexing method for nearest neighbor search", "author": ["H.V. Jagadish", "Beng Chin Ooi", "Kian-Lee Tan", "Cui Yu", "Rui Zhang"], "venue": "ACM Trans. Database Syst.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "A quantitative analysis and performance study for similarity-search methods in highdimensional spaces", "author": ["Roger Weber", "Hans-J\u00f6rg Schek", "Stephen Blott"], "venue": "In Proceedings of the 24rd International Conference on Very Large Data Bases,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["Piotr Indyk", "Rajeev Motwani"], "venue": "In Proceedings of the thirtieth annual ACM symposium on Theory of computing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["Moses S. Charikar"], "venue": "In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Learning reconfigurable hashing for diverse semantics", "author": ["Yadong Mu", "Xiangyu Chen", "Tat-Seng Chua", "Shuicheng Yan"], "venue": "ACM International Conference on Multimedia Retrieval,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Lost in binarization: query-adaptive ranking for similar image search with compact codes", "author": ["Yu-Gang Jiang", "Jun Wang", "Shih-Fu Chang"], "venue": "In Proceedings of the 1st ACM International Conference on Multimedia Retrieval,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Minimal loss hashing for compact binary codes", "author": ["Mohammad Norouzi", "David J. Fleet"], "venue": "In Lise Getoor and Tobias Scheffer, editors,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Localitysensitive hashing with margin based feature selection", "author": ["Makiko Konoshima", "Yui Noma"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Hyperplane arrangements and locality-sensitive hashing with lift", "author": ["Yui Noma", "Makiko Konoshima"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Monte carlo sampling methods using markov chains and their applications", "author": ["W K Hastings"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1970}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["Aude Oliva", "Antonio Torralba"], "venue": "Int. J. Comput. Vision,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Small codes and large image databases for recognition", "author": ["Antonio Torralba", "Rob Fergus", "Yair Weiss"], "venue": "Proceedings of the IEEE Conf on Computer Vision and Pattern Recognition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "For example, the literatures [1, 2] are two of them.", "startOffset": 29, "endOffset": 35}, {"referenceID": 1, "context": "For example, the literatures [1, 2] are two of them.", "startOffset": 29, "endOffset": 35}, {"referenceID": 2, "context": "Consequently, searches in higher-dimensional data using these methods end up having processing times that are similar to those of searches performed without using a special index [3].", "startOffset": 179, "endOffset": 182}, {"referenceID": 3, "context": "In a method called locality-sensitive hashing [4], the feature vectors are transformed into bit strings.", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "For this transformation, methods that involve the use of hyperplanes in feature space have been intensively studied [5\u20138].", "startOffset": 116, "endOffset": 121}, {"referenceID": 5, "context": "For this transformation, methods that involve the use of hyperplanes in feature space have been intensively studied [5\u20138].", "startOffset": 116, "endOffset": 121}, {"referenceID": 6, "context": "For this transformation, methods that involve the use of hyperplanes in feature space have been intensively studied [5\u20138].", "startOffset": 116, "endOffset": 121}, {"referenceID": 7, "context": "Studies aimed at increasing the precision of feature quantities associated with the hyperplane hashing method include the following references [9\u201311].", "startOffset": 143, "endOffset": 149}, {"referenceID": 8, "context": "Studies aimed at increasing the precision of feature quantities associated with the hyperplane hashing method include the following references [9\u201311].", "startOffset": 143, "endOffset": 149}, {"referenceID": 9, "context": "Studies aimed at increasing the precision of feature quantities associated with the hyperplane hashing method include the following references [9\u201311].", "startOffset": 143, "endOffset": 149}, {"referenceID": 9, "context": "In reference [11], an experiment is performed where the hashing of hyperplanes that do not pass through the origin is learned by learning the hashing of hyperplanes that do pass through the origin.", "startOffset": 13, "endOffset": 17}, {"referenceID": 4, "context": "In one hashing method, the B hyperplanes are set randomly [5].", "startOffset": 58, "endOffset": 61}, {"referenceID": 5, "context": "Other references such as [6\u201310] describe hashing methods that use hyperplanes.", "startOffset": 25, "endOffset": 31}, {"referenceID": 6, "context": "Other references such as [6\u201310] describe hashing methods that use hyperplanes.", "startOffset": 25, "endOffset": 31}, {"referenceID": 7, "context": "Other references such as [6\u201310] describe hashing methods that use hyperplanes.", "startOffset": 25, "endOffset": 31}, {"referenceID": 8, "context": "Other references such as [6\u201310] describe hashing methods that use hyperplanes.", "startOffset": 25, "endOffset": 31}, {"referenceID": 7, "context": "In particular, MLH [9] and S-LSH [10] are described in subsections 2.", "startOffset": 19, "endOffset": 22}, {"referenceID": 8, "context": "In particular, MLH [9] and S-LSH [10] are described in subsections 2.", "startOffset": 33, "endOffset": 37}, {"referenceID": 7, "context": "One existing learning method is Minimal Loss Hashing (MLH) [9].", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "In this subsection, we describe the concept of an existing learning method called locality-sensitive hashing with margin based feature selection (S-LSH) [10].", "startOffset": 153, "endOffset": 157}, {"referenceID": 8, "context": "S-LSH has been shown to have good learning performance in many data sets [10].", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "We use the Metropolis-Hastings algorithm for MCMC [12].", "startOffset": 50, "endOffset": 54}, {"referenceID": 8, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "\u2022 LabelMe LabelMe data using 512-dimensional Gist feature quantities [15] extracted from image data published in Ref.", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Since we used a fixed value of T = 1 in this evaluation, the index of the evaluation function is confined to the range [0, 2] or [0, 4].", "startOffset": 119, "endOffset": 125}, {"referenceID": 3, "context": "Since we used a fixed value of T = 1 in this evaluation, the index of the evaluation function is confined to the range [0, 2] or [0, 4].", "startOffset": 129, "endOffset": 135}], "year": 2013, "abstractText": "Since Hamming distances can be calculated by bitwise computations, they can be calculated with less computational load than L2 distances. Similarity searches can therefore be performed faster in Hamming distance space. The elements of Hamming distance space are bit strings. On the other hand, the arrangement of hyperplanes induce the transformation from the feature vectors into feature bit strings. This transformation method is a type of locality-sensitive hashing that has been attracting attention as a way of performing approximate similarity searches at high speed. Supervised learning of hyperplane arrangements allows us to obtain a method that transforms them into feature bit strings reflecting the information of labels applied to higher-dimensional feature vectors. In this paper, we propose a supervised learning method for hyperplane arrangements in feature space that uses a Markov chain Monte Carlo (MCMC) method. We consider the probability density functions used during learning, and evaluate their performance. We also consider the sampling method for learning data pairs needed in learning, and we evaluate its performance. We confirm that the accuracy of this learning method when using a suitable probability density function and sampling method is greater than the accuracy of existing learning methods. Keyword: Higher dimensional feature vector, Locality-sensitive hashing, Arrangement of hyperplanes, Similarity search, Markov chain Monte Carlo, Low-temperature limit", "creator": "LaTeX with hyperref package"}}}