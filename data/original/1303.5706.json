{"id": "1303.5706", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "Constraint Propagation with Imprecise Conditional Probabilities", "abstract": "An approach to reasoning with default rules where the proportion of exceptions, or more generally the probability of encountering an exception, can be at least roughly assessed is presented. It is based on local uncertainty propagation rules which provide the best bracketing of a conditional probability of interest from the knowledge of the bracketing of some other conditional probabilities. A procedure that uses two such propagation rules repeatedly is proposed in order to estimate any simple conditional probability of interest from the available knowledge. The iterative procedure, that does not require independence assumptions, looks promising with respect to the linear programming method. Improved bounds for conditional probabilities are given when independence assumptions hold.", "histories": [["v1", "Wed, 20 Mar 2013 15:29:40 GMT  (448kb)", "http://arxiv.org/abs/1303.5706v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["stephane amarger", "didier dubois", "henri prade"], "accepted": false, "id": "1303.5706"}, "pdf": {"name": "1303.5706.pdf", "metadata": {"source": "CRF", "title": "Constraint Propagation with Imprecise Conditional Probabilities", "authors": ["Stephane AMARGER", "Didier DUBOIS", "Henri PRADE", "Paul Sabatier"], "emails": ["prade]@irit.fr"], "sections": [{"heading": null, "text": "1 INTRODUCTION\nIn commonsense reasoning it is very usual to manipulate rules with exceptions. One of the most important cases of such rules consists in default statements containing explicit or implicit numerical quantifiers. Even when they are explicit these quantifiers may be only vaguely stated as for instance in the proposition \"most students are young\" ; see (Zadeh, 1985). The numerical approach interprets the linguistic term \"most\" in this example as an ill-defined numerical quantifier expressing the proportion of young people among students, in a certain context (for simplicity we assume here that 'young' has a clear-cut meaning and is not viewed as a fuzzy predicate ; see (Dubois and Prade, 1988) for preliminary results on the handling of fuzzy predicates in this framework). More generally, we may have some imprecise statement about the value of the probability of not encountering an exception, i.e. in our example, the conditional probability P(young I student) is bounded from below by some number in [0,1]. This type of incomplete statistical information is considered by Kyburg (1974) as a large part of our commonsense knowledge. P(young I student) is often also regarded as a degree of certainty that a student taken at random is indeed young.\nDifferent kinds of treatment can be imagined for rules of the kind \"if A then B with probability P(B I A).\" This can be illustrated considering the above rule and another which can be chained with it, namely, \"if B then C with probability P(C I B).\" Applying Bayes rule we have P(C I A) <': P(B n C I A) = P(C I A n B}P(B I A) (here we use the same symbol 'n' for denoting the conjunction of propositions or the intersection of the classes of items which satisfy the propositions). Then assuming irrelevance of A with respect to C in the context B, namely assuming here that P(C I A n B) = P(C I B), we obtain the lower bound P(C I B)-P(B I A) for P(C I A). But without this kind of assumption, as soon as P(C I B) * 1, nothing can be said about the value of P(C I A) which can take any value in the interval [0,1]. Indeed nothing forbids to have A n C = 0 (leading to P(C I A) = 0) as well as A>;;; C (leading to P(C I A)= I) for instance. Interestingly enough if we add some information about P(A I B) we may obtain non-trivial bounds for P(C I A) just from bounds on P(B I A), P(A I B) and P(C I B). Then more generally we may choose either i) to exploit the available knowledge on conditional probabilities for computing the best possible upper and lower bounds for some other conditional probabilities of interest, or ii) to take advantage of independence assumptions (which are perhaps hard to check) and prior probabilities for computing probability estimates (Pearl, 1988). The first approach may give no informative result, but when results are informative, they are very strong. On the contrary, the Bayesian approach always gives informative results, but these results can always be questioned by the arrival of new pieces of information. In this paper we investigate the first approach in detail.\nFormally, let X be a set of objects, A and B be two subsets of X, and Qit be a subset of values (which may reduce to a single value) expressing what is known about\nthe proportion of A's which are B's. Qit is a subinterval of the unit interval [0,1], corresponding to the default rule\n\"Qit A's are B's.\" This knowledge is understood as a constraint acting on the cardinality of B relative to A, i.e.:\nlA (1 Bl E Qt!;;;; [0,1] IAI\nwhere IAI is the cardinality of the subset A. More generally, it is equivalent to a piece of information of the\nConstraint Propagation with Imprecise Conditional Probabilities 27\nform P(B I A) E [P.(B I A), P*(B I A)] where only the two bounds P.(B I A) and P*(B I A) are known. Indeed relative cardinality is a particular case of conditional probability, where the underlying distribution is uniformly distributed over X. Thus proportions and probabilities obey the same mathematical laws and we shall use them in an interchangeable way in the following.\nWe use a network representation, as for instance the one on Figure 1, where the two directed edges between two\nnodes A and B are weighted by Q\ufffd and \ufffd. i.e. what is known of the proportions of B's which are A's and of A's which are B's. Note that in terms of conditional probabilities we assume information on both P(A I B) and P(B I A), which contrasts with Bayesian networks (Pearl, 1988). Besides P(A) will be interpreted as P(A I X) where X stands for the set of all considered objects or in logical terms corresponds to the ever-true proposition. Hence all probabilities that we handle are (bounds of) conditional probabilities in a network where cycles are allowed, and no prior probability information is required in order to start the inference process in the approach described in this paper (contrary to Quinlan ( 1983)'s INFERNO system or Baldwin (1990)'s support logic programming).\nOther works have been published, that handle probability bounds (see (Dubois et al., 1990) for a survey). However, these works always assume knowledge about unconditional probabilities (i.e. P(A) = P(A I X) in our framework) and are often oriented towards the computation of unconditional probabilities P(B). This is not true here. The reasoning systems of Bacchus (1990) aim at embedding the type of knowledge we deal with into a formal logical setting. Contrastedly our aim is to specify efficient inference algorithms.\nFigure 1 : an inference network\nIn the following sections, we are going to present computational methods that can handle imprecisely-known conditional probabilities. This work pursues an earlier investigation. In Dubois and Prade (1988), see also LeaSombe (1990), a first local pattern of reasoning, corresponding to the transitive chaining syllogism was studied. In (Dubois et al., 1990) two other local patterns enable us to estimate conditional probabilities involving conjunctions of events or contexts in their expression. A more complete set of propagation rules is presented in (Amarger et a!. 1991 ).\nAfter presenting the problem is section 2, section 3 recalls how our problem can be reduced to linear programming. Section 4 presents a generalized version of Bayes' theorem\nwhich can help improve the known bounds on an inference network in a single propagation step. Section 5 recalls the previously studied inference patterns involving conjunctions and disjunctions of two terms and discusses the handling of negation. Section 6 presents the general strategy that exploits two propagation rules in order to answer queries about conditional probabilities of interest. Section 7 illustrates the approach on an example. Section 8 discusses the handling of conjunction and disjunction in queries. Section 9 considers the introduction of independence assumptions in the chaining of conditional probabilities. In the conclusion, analogies with non monotonic reasoning are pointed out.\n2 STATEMENT OF THE PROBLEM\nWe suppose that we know some default rules containing numerical quantifiers or conditional probabilities such as\n\u2022'Ql} A's are B' s\" or \"if A then B with probability P(B I A).\"\nThe objective of our research is to answer queries like : \"what proportion of A's are C's ?,\" \"what proportion of A's and B's are C's ?,\" \"what proportion of C's are A's and B's ?,\" \"what proportion of A's or B's are C's ?,\" or \"what proportion of C's are A's or B's ? ;\" or similar queries stated in terms of conditional probabilities, from the available knowledge about the values of other proportions or conditional probabilities. This corresponds respectively to evaluate the probabilities p = P(C I A), P(C I A n B), P(A n B I C), P(C I A u B) or P(A u B I C). The possible values of p usually form an interval [p.,p*], and not just a single value, where P\u2022 is the lowest value and p * the highest value possibly taken by the conditional probability. Usually, a good local uncertainty propagation method will provide bounds that bracket [p.,p *], i.e. the deduction method will be sound. If it supplies exact bounds, it is called complete. The inference patterns we shall use in the following sections are sound and are also complete when we consider just the elementary network corresponding to the statement of the pattern, i.e. they are said to be locally complete.\nOur view of a knowledge base in this paper is thus a collection of general statements regarding a population X of objects ; these statements express in imprecise terms the proportions of objects in various subclasses of X, that belong to other subclasses. This knowledge base allows for answering queries about a given object, given a subclass to which it belongs (also called its \"reference class\" by Kyburg). To do so we just apply to this object the properties of this subclass, implicitly assuming that it is a typical element of this class. If more information become available for this object, we just change its reference class accordingly.\nComputing bounds for P(B I A) is a matter of constraint propagation, and is not based on updating a probability\n28 Amarger, Dubois, and ftade\ndistribution, contrary to Bayesian reasoning. Namely, given that we know that an object of interest is in subclass A, we certainly do not interpret this fact as P(A) = 1. Indeed adding the constraint P(A) = 1 to the knowledge base may lead to modify the set of probability measures that obey the constraints induced by probability bounds (e.g. P(C I A n B) must become equal to P(C I B)). This is because P(A) = 1 means that X - A is an empty set ; on the contrary, a \"fact\" \"x E A\" in our system just indicates that we look for the properties of members of subset A. Although in Bayesian reasoning, computing P(B I A) is the same as assuming the posterior probability of A is 1 because the probability distribution on X is unique, these two operations no longer coincide with probability bounds : one is just focusing on a reference class (what can be said about B, for mem hers of A?) while the other is knowledge updating (see Dubois and Prade, 1991b). Dubois and Prade (1991b) further discuss the difference between focusing and updating in the framework of belief functions. As for the difference between computing P(B I A) and P(B) when A is an accepted fact, this topic has been considered in the philosophical literature for a long time. See e.g. Suppes (1966).\n3 A LINEAR PROGRAMMING METHOD\nIt has been shown in (Paass, 1988) that reasoning from numerically quantified general rules may be modelled as an optimization problem. Namely, if there are n atomic symbols in the network, there are 2\" possible worlds, and we can express all constraints on conditional probabilities as linear constraints where the variables x; correspond to the unknown probabilities of possible worlds i. The calculation of bounds on an unknown conditional probability P(A I B) comes down to find the maximum and the minimum of a rational fraction whose numerator sums the probabilities X; of the possible worlds where A and B are true, and whose denominator sums the probabilities x; of the possible worlds where B is true, under the constraints induced by the already known probability bounds and to the requirement that the x;'s sum to one.\nThe same approach may be used to solve any query. As we can see we are faced with a fractional linear programming problem of the form (P),\n(P) {Opti c\u00b7tx_ x 2: 0 under Px = I, M\u00b7x \ufffd o) d-tx\nwhere li is the \"unit vector,\" c, d, x are row vectors, M is a matrix, \"Opti\" is either \"Max\" or \"Min,\" and t denotes the transposition, can be transformed into an equivalent linear program (P'). Indeed, as pointed out by (Charnes and Cooper, 1962), letting y; = xV(d\u00b7tx), we obtain :\n(P') {Opti c-ty, y 2:0 under M\u00b7y \ufffd 0, d-ty = 1)\nSo, as explained and exemplified in (Amarger et al., 1990), the calculation of bounds for P(B I A) requires that two linear programs be solved (one to compute the exact lower bound and one to compute the exact upper bound). But, even if with this method we are able to precisely compute the best bounds bracketing the conditional probability of interest, it is hard to try to provide an explanation for the obtained results in terms of the available knowledge we start with.\nThis reduction of a fractional linear programming problem induced by probability constraints to a linear programming problem has been also pointed out and used in (van der Gaag, 1990), where also local computation methods are proposed on the basis of the decomposition of the linear system into subsystems, and exploiting independence relationships when they are known. Methods based on local inference patterns may provide less precise results (although they are guaranteed to be sound), but are faster and their results easier to explain.\n4 GENERALIZED BAYES' THEOREM\nIn the framework of numerical quantifiers, because we manipulate conditional probabilities, it would be interesting to use the Bayes' theorem : VA, B, P{AIB)=P(BIA)\u00b7 P(A)IP(B) But,' in our approach we do not assume that P(A) and P(B) ai'e known. A more general identity, where only conditional probabilities appear can be established :\nProposition 1 : Generalized Bayes' theorem v A!,\u00b7 . . , Ak, p (A! lAte)= p (Ak IA!)IT p (A;\\ A;+!) ; = 1 P (A;+! I A;) when all involved quantities are positive.\nThis identity is easily proved replacing conditional probabilities P(A I B) by their expressions P(A n B)/P(B). Note that this identity tells us that given a cycle At. Az, ... , Ak, Ak+t =At in a probabilistic network, the 2.k quantities (P(Aj I Ai+t). i E ]k]} u (P(Ai+t I Aj), i E ]k]) (where )k) = ]0, k) n il'i) are not independent when positive: any 2.k - 1 of these quantities determine the remaining one. Now, because we use upper and lower probabilities, we extend this theorem as follows:\nProposition 2 : Generalized Bayes' theorem - upper/lower probabilities case. Given k sets At, A2, ... , Ak, with k > 2, the following inequalities should hold : o lower bound :\nk-1 V A1,\u00b7 \u00b7 ., Ak, P\u2022 (A1IAtc) 2: P\u2022 (Ak IA1) I1 Qi,i+l\ni::::l with: V i, j E ]k), Qi,j=P\u00b7(A;IAi)/p*(AiiA;). o upper bound ;\n\u2022 * k-1 -V A!,\u00b7 \u00b7,Ak, P (A1IAk)\ufffdP (AkiA\ufffd)IT di.i+l i= 1\nConstraint Propagation with Imprecise Conditional Probabilities 29\nA simpler version of Proposition 2 is used by Fertig and Breese (1990) for arc reversal in influence diagrams where probabilities are incompletely known. Proposition 2 is the basis of a first inference rule for tightening probability bounds in a set of ill-known conditional probabilities.\nNamely given a knowledge base 3:e = {(P*(A i I A ) , p* (Ai I Aj)), i, j E ]n]} ; we can associate to it a network G with n nodes A1, A2, ... , An and whose arcs (Ai,Aj) are weighted by d.i i + 1. Proposition 2 leads to update P*(A I B), and p*(A I B) in one step as follows\nI k\u00b7l ) P\u2022(A I B)= P\u2022(B I A)\u00b7 max . fi Q;,;+I (1) overallpathsA1, ... , A\ufffdtmG i= 1 * * with2<k\ufffdn,AI=A,Ak=B\ufffd k-l tf ) P (A I B)=P (B I A)' min . fi Yg; ;+1 (2)\nover allpathsAk, ... , A1 m G . _1 \u2022 with 2 <k S n,A1 =A.Ak =B l-\nThe second update is easily explained noticing that\nd;, ;+I = 1/g;+I, ;\u00b7 Note that these changes in probability bounds do not correspond to a revision of the knowledge, but only to constraint propagation steps ; namely the set of probability measures such that V i, j, P*(Ai I A_j) s; P(Ai I Aj) s; P*(Ai I A) never changes.\nAs it can be guessed, the propagation of the constraint expressed by Proposition 1 is achieved by computing the longest (i.e. most weighted) elementary paths from A to B and from B to A in the network G where arcs (A,B) and (B,A) have been suppressed. Here the length of the path is the product of all weights of arcs in the path. For reason of computing accuracy, it is better to compute the length of the paths using a standard (max, +) path algebra, changing di,i+l into Log di,i+l\u00b7 Then any shortest path algorithm will do. Note that the length of a circuit A1, . . . , Ak, Ak+l = A1 in G is such that d1,2 \u00b7 .d.2,3 ... .d.k.l,k \u00b7 ir:.\n< \ufffd: :::\ufffd\ufffd\ufffdis inequality re::. ( AI 1 Ak) 1 ; =I l IT p* (A;+ I I A;)- tP.(Ak I A,) i= 1 and is a consequence of Proposition 1. Hence in the network with weights of the form Log d.i, j\u2022 no circuit will be of positive length. Hence longest paths between nodes will always exist.\nThe constraint propagation steps (1) and (2) can be used as an inference rule that we shall denote BG (Bayes generalized) in the following.\n5 LOCAL INFERENCE RULES\nThe first local inference pattern, already examined in (Dubois and Prade, 1988) and in (Dubois et al., 1990),\ncorresponds to the evaluation of a missing arc in the inference network, and can be viewed as the counterpart of node removal in influence diagrams.\nThe problem solved by this pattern, also called \"quantified syllogism rule\" (QS) is the following: given bounds on P(A I B), P(B I A), P(C I B) and P(B I C), what are the bounds on P(C I A) (thus removing node B). The following bounds can be shown to be the tightest ones : lower bound : P-(C I A)= Po(B I A) max (o. 1 - 1 - P\u2022(C I B) ) (3) P-(AIB) ugper bound :\np'(C)A)= min (l.l- P.(B)A)+P.(B)A). p'(C)B), P\u00b7(A)B) P'(B IA)P'(c IB) r'(B IA)P'(c I B) [l -P\u00b7(B I c)] +P'(B IA))(4) P\u00b7(A)B )P\u00b7(B )c)' P\u00b7(AIB )P.(B )C) The application of QS to the network with nodes A, B, C for the calculation of P(C I A) is denoted QS(C, B, A) = (C, A). For a proof that these bounds are optimal see (Dubois and Prade, 1988 ; Dubois et a!., 1990).\nThis pattern can be ex tended to more than 3 nodes in sequence. It can be proved that optimality is preserved. Especially, given (A, B, C, D), it is equivalent to remove B first (computing P(C I A)), then C, or C first (computing P(D I B)), and then B, in order to get P(D I A), i.e. there is an associativity property.\nProposition 3 : QS(QS(D,C,B),A) = QS(D,QS(C,B, A)) Proof : First, consider the network {A, B, C) ; applying QS we get bounds for P(C I A) and P(A I C). Then we could think of applying QS again in order to improve bounds of P(C I B) for instance. Clearly this process will not lead to improve these bounds. Indeed if these bounds were inlproved using P(B I A), P(A I B), and the calculated bounds on P(C I A), P(A I C), it would indicate that quantities P(B I C) or P(C I B) are related to P(B I A) or P(A I B). But this is clearly not true. Similarly the knowledge about P(D I C) and P(C I D) has no influence on P(B I C) and P(C I B), hence has no influence on the optimal bounds of P(C I A) and P(A I C). The optimality of the QS rule then implies that the result of applying it on P(C I A), P(A I C), P(C I D), P(D I C) will also give optimal bounds on P(D I A) and P(A I D). The same reasoning applies if we compute P(D I B), P(B I D) first. In both cases we get optimal bounds on P(D I A) and P(A I D). Associativity then follows from optimality. Q.E.D.\nThis result could also be derived by the study of the linear program associated to the network, looking for decomposability properties of the constraint matrix.\nClearly, this property of QS is very nice and easily generalizes to a network with any number of nodes. Thus on a \"linear chain\" beginning with node A1 and ending\n30 Amarger, Dubois, and ftade\nwith node Ak, we can apply QS iteratively, from left to right, in order to evaluate P(Ak I A 1) for instance, without resorting to linear programming.\nIn (Dubois et a!., 1990) the expression of bounds on P(A n B I C) and P(C I A n B) in terms of more elementary conditional probabilities P(A I C), P(C I A), P(C I B), P(B I C), P(A I B) and P(B I A) have been established starting with a complete network with nodes A, B, C. The case of disjunction is solved in (Amarger et a!., 1991) where probabilities of the form P(A u B I C) and P(C I A u B) are explicitly obtained under the same setting. Disjunction and conjunction are addressed in Section 8 using the two rules QS and BG.\nThe case of negation is especially interesting. Indeed, given P(B I A) and P(A I B), we obviously know P(-,B I A) and P(-,A I B) where -, denotes complementation. However it is easy to verify that P(A I -,B) and P(B I -,A) remain totally unknown. It is indeed easy to check that\nP(A 1-,B) = P(A I B){( 1/p(B I A))- 1} P(B)/p(-,B) (5)\nIn other words, answering queries of the form \"How many not B's are A's\" require the knowledge of unconditional probabilities. A possible other way of dealing with negation is to introduce the closed world assumption which can be stated as follows : if sets A, B and C;, i E ]n] appear in the network, then let us assume that the universe is reduced to Au B u UieJnJ C;. In other words, we assume that the set -,A n -,B n n1e Jnl -,C; is empty, or at least that P(-,A n -,B n nieJnJ -,C;) = 0. In the trivial case where we consider the classes A and B only, it leads to P(-,A n -,B) = 0, and then P(A I -,B) = P(A I A n -,B) = 1. So, if we \"open\" the world by considering C also, then we assume P(-,A n -,B n -,C)= 0. Since -,B = [-,B n (Au C)] u [-,B n -,(Au C)]= -,B n (Au C) u (-,An -,B n -,C), then P(-,B) = P(-,B n (Au C)). Thus we change the question \"what is the value of P(A I -,B) ?\" into \"what is the value of P(A 1-,B n (Au C)) ?\". A systematic way of dealing with these questions require a proper handling of Boolean expressions in conditional probabilities.\n6 A CONSTRAINT PROPAGATION B ASED ON INFERENCE RULES\nIn the previous sections, we have presented two local inference rules, and now, the problem is to use these rules in order to perform automated reasoning with the whole network. The aim of this section is to build a reasoning strategy in order to be able to answer any simple query (i.e. a query of the form \"what is the proportion of A's which are C's? ,\" where A and C are atoms in the language). The network is supposed to be made out of simple conditional probabilities of the form P(A I B) where A and B are atomic symbols.\nGraphically, to answer a query like \"what proportion of X's are Y's ?\" is equivalent to generate the new arc <X, Y> in a network like the one of Figure 1. Our approach is local in the sense that the patterns are designed to provide answers to particular queries using local inference rules. Consequently, one can observe the influence of each piece of knowledge on the result ; global methods do not offer such a possibility. Even though a part\n_ icular pattern corresponds to an elementary network, the mference patterns can work on any network, whatever its structure, unlike the Bayesian approach which needs an acyclic network topology (e.g. directed cycles are prohibited) adapted to the propagation mechanism ; see (Lauritzen and Spiegelhalter, 1988 and Pearl, 1988).\nOf course, in practice, in order to answer a particular query, it may exist several possibilities for applying the inference patterns to the network, corresponding to different paths. Since the inference rules are sound, one can easily combine the different results provided by all the applications of rules because their intersection still provides a sound result. Indeed, let us suppose that Q1 = [P\u2022l\u00b7P*!l and Q2 = [P\u20222\u00b7P*2] are two intervals that contains the value p we want to estimate ; then we have : p E Q1 n Q2 = [max(P\u20221 ,p.2), min(p*l ,p*2)]. This generalizes to the intersection of any number of intervals; and the emptiness of the intersection would be the proof that the data we start with are not consistent.\nWe will first use a saturation strategy in order to extract as much information as we can from the network, namely, try to get probability intervals as tight as possible for all conditional probabilities P (A I B). The result is called the saturated network.\nWe are going to use two tools : rule QS (corresponding to the basic quantified syllogism) presented in Section 5., in order to add links to the network, and the generalized Bayes' theorem (rule BG) presented in Section 4.\nStep 1 : recursively apply QS, to generate the missing arcs. This step is performed until the probability intervals can no more be improved. Step 2 : recursively apply BG to improve the arcs generated by Step 1.\nThen, the general algorithm is : (a) perform Step 1 (b) perform Step 2 (c) if the probability intervals have been improved go\nto (a), otherwise stop\nNote that the two steps are very complementary. Indeed, step 1 uses an optimal rule but a local one, while step 2 uses a suboptimal method but considers more than 3-tuples of nodes.\nAnother important problem encountered in inference system is the consistency of the knowledge base. Using\nConstraint Propagation with Imprecise Conditional Probabilities 31\nthe global method presented in Section 3., if one of the two linear programs we have to solve (or both) has no solution, we can say that there is an inconsistency in the constraints of the linear programs, i.e. an inconsistency in the knowledge base. Solving only one linear program is enough to find out an inconsistency (if any) among the constraints expressing the knowledge base. If there is some inconsistency, exhibiting the Simplex array, we will be able to determine where is the inconsistency, i.e. which arcs are inconsistent. So, our system is of the following general form : (a) consistency checking by linear programming\nif an inconsistency is detected, exit (b) saturation of the network (c) answering user's queries. The considered queries are of the form P(A I B) ?.\nUsing results in established in (Dubois et al., 1990; Amarger et a!., 1991) we can also handle queries of the form P(A u B I C) ? , P(A n B I C) ? , P(C I An B) ? , . . . Of course, steps (a) and (b) may take a long time computation, but they only are performed once for all at the beginning of the session, in order to ensure that the user works with a consistent knowledge base, and to make all the information explicit.\n7 AN EXAMPLE\nIn this section, our purpose is to point out the results given by both the quantified syllogism and generalized Bayes' theorem. The algorithm we use is written in \"C\" on a Sun 3/50 workstation without arithmetical co processor and the Floyd algorithm is used to compute the longest paths (see (Gondran and Minoux, 1985) for instance). The example we use is already considered in (Dubois et a!., 1990), and is pictured in Figure 2 and, in the following, we use the incidence matrix notation to let the saturated network be more readable.\nFigure 2\nSo, using the above algorithm (the details are given in (Amarger, Dubois, Prade 1991)), we get the \"saturated\" network (the improved bounds are underlined)\u00b7\nstudent sport single young children\nstudent [1.00;1.00] ll12Q;0.90] [.QM;I.OO] [0.85;\ufffd [O.OO\u00b7,Q22] Sjl_Ort [0.40;MQJ [1.00;1.00] \ufffd;0.85] [0.90;Q22] [O.OO;QJ2] single l.QZI;Q.lQJ [0.70;0.70] [1.00;1.00] l.Q..aQ;0.80] [0.05;0.1 0] . young [@;0.35] I.QM;0.88] [0.90;\ufffd] [1.00;1.00] [0.00;0.05]\nchildren [O.OO;Q.ll2] [O.OO;.I).U] [0.00;0.05] [O.OO;Q.Q1] [1.00; 1.00]\nThe computation of the complete \"saturated\" matrix was made in 10 seconds (CPU and I/0 time).\nThe optimal solution computed by the global method presented in Section 3., and in (Amarger et a!., 1990) is exactly the same as the one computed by the \"local method\" based on QS and BG. Let us note that the \"global method\" is written in \"C\", on a Sun 3/50 workstation, without arithmetical co-processor; and the computation of each element of the \"optimal\" matrix is made in 12 seconds (CPU and 1(0 time). So, combining a locally optimal method (QS) with a global but suboptimal method (generalized Bayes' theorem), we get results as good as the ones given by a globally optimal method (Simplex based method of Section 3.), but with a much smaller computation time, in our example.\n8 CONJUNCTION AND D ISJUNCTION\nThe results involving conjunction and disjunction solved in previous papers are not general enough to be very useful in practice. Their merits are but tutorial. Especially, their extension to disjunctions and conjunctions of more than two terms look untractable in an analytic form. Even the case when only three symbols A, B and C are involved, and where bounds on the six conditional probability values involving these symbols are known, will lead to unwieldy expressions because the six values are related via the generalized Bayes' theorem.\nA more realistic approach to the problem of handling disjunctions and conjunctions is to introduce new nodes in the network, that account for the concerned con junctions and disjunctions, and apply the iterative algorithm (or linear programming) to answer the query. As an example, let us consider the query \"what is the probability of C given A and B\", where the background network includes nodes A, B, C only; (see Figure 3)\nI\nFigure 3 : Introducing a new node \"An B\"\nTo deal with this problem we create a node named A n B. A description of the conjunction in terms of conditional probabilities leads to force P \u2022 (A I A n B) = 1, P.(B I A n B)= 1, P(A n B I B) = P(A I B) and P(A n B I A) = P(B I A), and to add these arcs to the network (see Figure 3). Then the calculation of P(C I An B) can be addressed by the repeated use of the Quantified Syllogism pattern and the generalized Bayes rule in this network .\n32 Amarger, Dubois, and lhde\nIn order to catch a feeling of what kinds of results can be produced by this method, let us deal with the case when the six values P(A I C), P(C I A), P(B I C), P(C I B), P(A I B), P(B I A) are precisely known in Figure 3. Of course they obey the generalized Bayes theorem, so that only five of them need to be known. The calculation of bounds for P(C I A n B) can be performed by applying twice the syllogism rule, cancelling A between A n B and C, and cancelling B between A n B and C. Applying ( 1 ) and (3) with the following substitution : A becomes An B, B becomes A, we get\nmaj 0, 1 - 1 -P(C I A) )\ufffd P(C IAn B)\ufffd m'j 1, P(C I A) ) \\ P(B I A) u\\ P(B I A)\nSimilarly, exchanging A and B in the above inequalities, we get: max (o, 1-1-P(CIB) )\ufffdP(CIAnB)\ufffdmin ( 1, P(CIB) ) P(AIB) P(AIB) Joining these results together, we obtain mavf0,1 1-P(C IA),1 1-P(C I B) )\ufffdP(C I An B){6)\n\\ P(B I A) P(A I B) P(C IAn B)\ufffd minh, P(C I B) , P(C I A) ) (7)\n'\\ P(A I B) P(B I A)\nIt can be checked that this is exactly what has been obtained in (Dubois et a!., 1 990), i.e. when we have no knowledge about P(B I C) and P(A I C). To improve these bounds requires the use of the generalized Bayes theorem. As shown in (Dubois et a!., 1990) only the lower bound of P(C I A n B) can be improved knowing P(B I C) and P(A I C). However the following extra inequalities are not related to the generalized Bayes theorem nor to the quantified syllogisms\nP(C IAn B)\ufffd P(C I A) + P(C I B) \u00b7( 1 _ 1 ) (8) P(B I A) P(A I B) P(B I C) P(C I A n B) \ufffd P(C I B) + P(C I A) \u00b7( 1 _ 1 ) (9) P(A I B) P(B I A) P(A I C)\nThese inequalities are simple consequences of the additivity of probabilities applied to A n B n C under the form P(A n B n C) = P(A n C) + P(B n C) -P((A n C) u (B n C))\n\ufffd P(A n C) + P(B n C) - P(C)\nHence additivity is not presupposed by the description of node A n B in Figure 3. Proceeding similarly for P(A n B I C), the syllogism rule leads to the following bounds max (o.P(A I C) ( 1 + (P(B I A) -I) ) .P(B I C) ( I+ (P(A I B) -I) ))\nP(CIA) P(CIB) \ufffd P(AnB I C)\ufffd\nmin (P(A I C), P(B I C), P(A I C) P(B I A), P(B I C) P(A I B) ) P(C I A) P(C I B)\nNote that in the above expression, the two last terms in the 'min' are equal due to the generalized Bayes' theorem. Using results in (Dubois et al., 1 990), it can be checked that the upper bound is optimal while the lower bound is\nsound but not optimal. Indeed we do not recover the obvious bound, again related to additivity :\nP(A n B I C)\ufffd max(O, P(A I C) + P(B I C)- 1 ) (10)\nMore specifically, given only P(A I C)= 1 and P(B I C)= 1, the repeated use of the syllogism rule and the generalized Bayes' rule are not capable of producing P(A n B I C) = 1 (a result produced by the above bound). Indeed, if we add the node AB to represent A n B, we have to saturate the following network\nFigure4\nAll that this network tells is that AB <;;; A n B and C <;;; An B, but clearly, AB n C can be anything. Also, even assuming that P(A I B) o' 1 and P(B I A) o' 1 are known and Jetting P(AB I A) = P(B I A), P(AB I B) = P(A I B) cannot improve the lower bound of P(AB I C) using the syllogism rule, nor the generalized Bayes rule. This point indicates that some of the lower bounds already obtained in (Dubois et a!., 1 990), for the conjunction will be useful to implement, in order to improve the performance of the iterative procedure, i.e. the inequalities (8), (9) and (1 0).\nAnother poin to notice is that the constraint P(AB I A) = P(B I A) is tronger than letting P*(AB I A) = P*(B I A), P.(AB I A = P.(B I A), when only bounds on P(B I A) are know , indeed, the equality of the bounds can go along with t inequality P(AB I A) * P(B I A). Let us consider the ery about P(C I A u B). To deal with this case, we cr te a node named A u B, and arcs joining this node to the network, so as to describe the disjunction in terms of conditional probabilities namely P(A u B I A) = 1 and P(A u B I B) = 1 . The calculation of P(A I A u B) and P(B I Au B) is slightly Jess straightforward, namely P(A I AuB)= P( A) P(A)\nP(A u B) P(A) + P(B)-P(A II B) P(A I B) =----------\ufffd--\ufffd---------\nP(A I B)+ P(B I A)-P(A I B)\u00b7P(B I A)\nsince P(B)/P(A)=P(B I A)/P(AIB)\u00b7 The complete study of this case is left to the reader. A lack of optimality similar to the one encountered with conjunction will be observed.\n9 INDEPENDENCE ASSUMPTIONS\nAlthough our approach does not require independence assumptions, it should be possible to use them if they hold, in order to improve bounds. This section gives preliminary results on that point, for the syllogism rule\nConstraint Propagation with Imprecise Conditional Probabilities 33\nQS. Let us consider conditional independence relations. There are three possible ones on (A, B, C):\ni) P(B n C I A) = P(B I A) \u00b7 P(C I A) ii) P(A n C I B) = P(A I B) \u00b7 P(C I B) iii) P(A n B I C) = P(A I C) \u00b7 P(B I C)\nFirst, note that i) and iii) are symmetric with respect to each other, exchanging C and A. We shall thus just consider i) and ii). ii) has already been considered in the introduction and we shall check that we cannot do better: ii) is indeed equivalent to the irrelevance property P(C I B) = P(C I A n B). Hence the independence property can be exploited by substituting P(C I B) = P(C I A n B) in the bounds on P(C I An B) (equations (6), (7)). Only the bounds where P(C I A) appear are useful. We get (for precise values) 1 - (1- P(C I A)/P(B 1 A))\ufffd P(C I B)\ufffd P(C I A)/p(B 1 A) from which it follows : P(C IB)\u00b7 P(B IA)\ufffd P(C lA)\ufffd 1-P(B lA) + P(C IB)\u00b7 P(B lA) ( 11) the lower bound improves (3) and the upper bound improves the second term in the general upper bound (4). Particularly, when P(B I A) = 1 it can be checked that ii) entails P(C I A) = P(C I B). For bounds on P(A I C), just exchange A and C in the above inequalities, and get\nP(AI B) P(B IC)\ufffd P(A I C)\ufffd 1-P(B IC)+ P(A I B) P(B I C) (12) The above inequalities can influence P(C I A) using the generalized Bayes rule since\nP(AIC) = P(CIA) \u00b7 P(AIB)-P(BIC)t1>(BIA}P(CIB) can be substituted in ( 12) and enable to catch the inequality P(C I A)< P(B I A) P(C I B) (1-P(B I C)+P(A I B) P(B I C)) (13) P(A I B) P(B I C) that comes on top of ( 1 1) (the lower bound of ( 1 1) is obtained again this way). It improves the last term appearing in the upper bound in (4).\nLet us tum to i). It yields a new expression for P(C I A) under the from P(B n C I A)I?(B I A). Let us write it by letting P(A I B n C) appear;using the generalized Bayes rule:\nP(C I A) = P(A I B n C) \u00b7 P(C I B) P(A I B)\nNow using optimal bounds (6) and (7) on P(A I B n C), and given that P(A I C) is unknown there comes\nmax (o. I- 1-P(C I B))\ufffd P(C I A)\ufffd min(1. P(C I B))(14) P(AIB) P(AIB)\nAgain, if P(C I B) = I, we conclude that P(C I A) = 1. Moreover if P(A I B)= 1, then P(C I A)= P(C I B). The lower bound in (14) improves (3), and the upper bound may improve the third term in (4). Independence assumption iii) leads to a similar bracketting of P(A I C), just exchanging C and A in (14) :\nmax (o. 1- I-P(A I B))\ufffd P(A I C)\ufffd min(!. P(A I B) )(15) P(C I B) P(C I B)\n(15) and the generalized Bayes rule enable special bounds for P(C I A) to be found under assumption iii), namely :\nP(B I A) [I _ 1 -P(C I B )]\ufffd P(C I A)\ufffd P(B I A) (!6) P(B I C) P(A I B) P(B I C)\nAgain the lower bound in (16) improves (3), and the upper bound may improve the third term in (4).\nTo summarize, when independence assumptions are declared, namely i), ii), iii), bounds on P(C I A) given in (3) and (4) can be improved by means of ( 14), ( 1 1) and ( 13), and (16) respectively. Of course, these types of independence assumption can be more directly exploited in queries involving conjunctions or disjunctions.\n10 CONCLUSION\nThe approach proposed in this paper to handle conditional probabilities in knowledge networks presupposes assumptions that contrast with the ones underlying Bayesian networks. In Bayesian networks, a single joint probability distribution is reconstructed from the acyclic network using conditional independence assumptions, and given some a priori probabilities on the roots of the acyclic network. Here, nothing is assumed about a priori (unconditional) probabilities, no independence assumption is taken for granted, and, the more cycles there are, the more informative the network is.\nResults obtained so far indicate that the two inference rules that we use in tum, namely the syllogism rule (QS) and the generalized Bayes' theorem (BG), are powerful and can compete with a brute force linear programming approach, as regards the quality of the obtained probability bounds. Our inference technique seems to be more efficient than linear programming since each run of each step of the inference procedure is polynomial in the number of nodes in the network. However, more investigation is needed on complexity aspects, and to better grasp the distance to optimality of the inference procedure.\nIt has been indicated how to deal with conditional probabilities involving conjunctions and disjunctions of two terms, and negation of terms. However the obtained optimal bounds are rather heavy mathematical expressions for conjunctions and disjunctions, and it seems difficult to extrapolate them to more than two terms. It has been shown how to solve the problem of conjunction and disjunction by introducing auxiliary nodes in the original network. In the future, we plan to treat negation likewise and to generalize the node addition approach to the combination of more than two primitive terms.\nIn the long run, we plan to develop a computerized tool (parts of which are already implemented) that can handle a knowledge base in the form of a pair (W,/'!.) where W is a set of facts and !'!. a sets of conditional probabilities. A query Q can then be solved by computing P(Q I W) where W is the conjunction of available facts, and P(Q I W) is obtained under the form of bounds derived from the\n34 Amarger, Dubois, and lhde\nsaturated network built with \ufffd. This mode of reasoning is similar to what happens in non-monotonic logic. More specifically some of the propagation rules proposed here bear some interesting analogies with some derived inference rules in a well-behaved non-monotonic logic. For instance the BG rule corresponds to\na1 fv az, az fv U3, ... , Un\u00b7l fv Un, Un fv Ut (loop) Ut fv Un where \"\" denotes the non-monotonic consequence relation discussed in (Kraus and al, 1990). The QS rule gives a1 tv az, az tv at, az tv U3 (equivalence) Ut fv U3 the basic lower bound for P(A n B I C) (see (Amarger et al., 1990)) corresponds to\n'6 tv a, '6 tv P (right and) '(fvU/\\\ufffd\nThese analogies are no longer surprizing since such kinds of links between probabilistic reasoning and non monotonic logic have been already laid bare by (Pearl, 1988) and the authors (Dubois and Prade, 1991). But the correspondence pointed out above suggests to consider a nonmonotonic logic where primitive inference rules are the above rules, i.e.ru1es which are usually considered as derived ones. This point is worth studying in the future.\nAmong topics of interest for future research, a more detailed comparison with the Bayesian approach would be quite interesting, of course. It would allow the loss of information due to the absence of a priori probabilities to be quantified. It has been demonstrated how to allow for independence assumptions in our approach. Clearly it generates non-linear constraints in the optimization problem associated to a query. But it seems that the inference procedure can cope with these assumptions in a nicer way, just by modifying the constraint propagation rules accordingly. Another topic is the extension of our method to fuzzy quantifiers, already considered in (Dubois and Prade, 1988) for the syllogism rule.\nAcknowledgements This work is partially supported by the DRUMS project (Defeasible Reasoning and Uncertainty Management Systems), funded by the Commission of the European Communities under the ESPRIT BRA n\u00b0 3085.\nReferences S. Amarger, R. Epenoy, and S. Grihon ( 1990) Reasoning\nwith conditional probabilities. A linear programming based method. Proc. of the DRUMS Esprit Project, RP2 Workshop, Albi (France), April 1990, (published by IRIT, Toulouse (France)), pp. 154- 167.\nS. Amarger, D. Dubois, and H. Prade ( 1991) Handling imprecisely-known conditional probabilities. UNICOM \"Seminar AI and Computer Power- The Impact on Statistics\", Brune! Conf. Center, West London, 13-14 March.\nF. Bacchus ( 1990) Representing and Reasoning with Probabilistic Knowledge. Cambridge, Ma.: MIT Press.\nJ. Baldwin ( 1990) Computational models of uncertainty reasoning in expert systems. Computers and Math. with Appl. 19, 105-1 19\nA. Charnes and W.W. Cooper ( 1962) Programming with linear fractional functions. Naval Res. Logist. Quart., 9, 181-186.\nD. Dubois and H. Prade ( 1988) On fuzzy syllogisms. Comp. Intel. (Canada), 4, 171-179.\nD. Dubois and H. Prade ( 1991a) Conditional objects and non-monotonic reasoning. Proc. of the 2d. Int. Conf on Principles of Knowledge Representation and Reasoning (KR'91), Morgan Kaufmann, 175- 185\nD. Dubois and H. Prade ( 1991b) Evidence knowledge and belief functions. To appear in Int. J. of Approx. Reas.\nD. Dubois, H. Prade, and J-M. Toucas ( 1990) Inference with imprecise numerical quantifiers. In : Intelligent Systems: State of the Art and Future Directions (Z. Ras, M. Zemankova, eds.), Chichester: Ellis Horwood, 52-72.\nK.W. Fertig and J.S. Breese ( 1990) Interval influence diagrams. In : Uncertainty in Artificial Intelligence 5 (M. Henrion et a!., eds.), North-Holland,149-171.\nM. Gondran and M. Minoux ( 1985) Graphes and algorithmes. Eyrolles, Paris.\nS. Kraus, D. Lehmann, and M. Magidor ( 1990) Nonmonotonic reasoning, preferential models and cumulative logics. Artificial Intel., 44, 167-207.\nH. Kyburg ( 1974) The Logical Foundation of Statistical Inference. D. Reidel, Dordrecht.\nS.L. Lauritzen and D.J. Spiegelhalter ( 1988) Local computation with probabilities on graphical structures and their application to expert systems. J. of the Royal Statistical Society, B 50(2), 157-224.\nLea Sombe (P. Besnard, M-0. Cordier, D. Dubois, L. Farinas del Cerro, C. Froidevaux, Y. Moinard, H. Prade, C. Schwind, and P. Siegel) ( 1990) Reasoning Under Incomplete Information in Artificial Intelligence : a Comparison of Formalisms Using a Single Example. Wiley, New-York .\nG. Paass ( 1988) Probabilistic Logic. In :Non-Standard Logics for Automated Reasoning (D. Dubois, P. Smcts, A. Mamdani, H. Prade, eds.), Academic Press (London), Ch. 8, 213-251.\nJ. Pearl ( 1988) Probabilistic Reasoning in Intelligent Systems : Networks of Plausible Inference. San Mateo, Ca.: Morgan Kaufmann.\nJ.R. Quinlan ( 1983) INFERNO:a cautious approach to uncertain inference. The Comp. Res., 12, 255-269.\nP. Suppes ( 1966) Probabilistic inference and the concept of total evidence. In Aspects of Inductive Logic (J. Hintikka and P. Suppes, Eds.) North-Holland, Amsterdam, 49-65\nL.C. van der Gaag (1990) Computing probability intervals under independency constraints. Proc. of the 6th Conf on Uncertainty in Artificial Intelligence, Cambridge, Mass., July 27-29, 491-495\nL.A. Zadeh ( 1985) Syllogistic reasoning in fuzzy logic and its application to usuality and reasoning with dispositions. IEEE Trans. on Systems, Man and Cybernetics, 15(6), 745-763."}], "references": [{"title": "Reasoning with conditional probabilities. A linear programming based method", "author": ["S. Amarger", "R. Epenoy", "S. Grihon"], "venue": "Proc. of the DRUMS Esprit Project,", "citeRegEx": "Amarger et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Amarger et al\\.", "year": 1990}, {"title": "Handling imprecisely-known conditional probabilities. UNICOM \"Seminar AI and Computer Power- The Impact on Statistics", "author": ["S. Amarger", "D. Dubois", "H. Prade"], "venue": null, "citeRegEx": "Amarger et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Amarger et al\\.", "year": 1991}, {"title": "Representing and Reasoning with Probabilistic Knowledge", "author": ["F. Bacchus"], "venue": null, "citeRegEx": "Bacchus,? \\Q1990\\E", "shortCiteRegEx": "Bacchus", "year": 1990}, {"title": "Computational models of uncertainty reasoning in expert systems", "author": ["J. Baldwin"], "venue": "Computers and Math. with Appl. 19,", "citeRegEx": "Baldwin,? \\Q1990\\E", "shortCiteRegEx": "Baldwin", "year": 1990}, {"title": "Programming with linear fractional functions", "author": ["A. Charnes", "W.W. Cooper"], "venue": "Naval Res. Logist. Quart.,", "citeRegEx": "Charnes and Cooper,? \\Q1962\\E", "shortCiteRegEx": "Charnes and Cooper", "year": 1962}, {"title": "On fuzzy syllogisms", "author": ["D. Dubois", "H. Prade"], "venue": "Comp. Intel. (Canada),", "citeRegEx": "Dubois and Prade,? \\Q1988\\E", "shortCiteRegEx": "Dubois and Prade", "year": 1988}, {"title": "Conditional objects and non-monotonic reasoning", "author": ["D. Dubois", "H. Prade"], "venue": "Proc. of the 2d. Int. Conf on Principles of Knowledge Representation and Reasoning", "citeRegEx": "Dubois and Prade,? \\Q1991\\E", "shortCiteRegEx": "Dubois and Prade", "year": 1991}, {"title": "Evidence knowledge and belief functions", "author": ["D. Dubois", "H. Prade"], "venue": "To appear in Int. J. of Approx. Reas", "citeRegEx": "Dubois and Prade,? \\Q1991\\E", "shortCiteRegEx": "Dubois and Prade", "year": 1991}, {"title": "Inference with imprecise numerical quantifiers", "author": ["D. Dubois", "H. Prade", "J-M"], "venue": null, "citeRegEx": "Dubois et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Dubois et al\\.", "year": 1990}, {"title": "Interval influence diagrams", "author": ["J.S.K.W. Fertig"], "venue": "Henrion et a!.,", "citeRegEx": "Fertig,? \\Q1990\\E", "shortCiteRegEx": "Fertig", "year": 1990}, {"title": "Graphes and algorithmes", "author": ["M. Gondran", "M. Minoux"], "venue": null, "citeRegEx": "Gondran and Minoux,? \\Q1985\\E", "shortCiteRegEx": "Gondran and Minoux", "year": 1985}, {"title": "Nonmonotonic reasoning, preferential models and cumulative logics", "author": ["S. Kraus", "D. Lehmann", "M. Magidor"], "venue": "Artificial Intel.,", "citeRegEx": "Kraus et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Kraus et al\\.", "year": 1990}, {"title": "The Logical Foundation of Statistical Inference. D", "author": ["H. Kyburg"], "venue": null, "citeRegEx": "Kyburg,? \\Q1974\\E", "shortCiteRegEx": "Kyburg", "year": 1974}, {"title": "Local computation with probabilities on graphical structures and their application to expert systems", "author": ["D.J.S.L. Lauritzen"], "venue": "J. of the Royal Statistical Society, B", "citeRegEx": "Lauritzen,? \\Q1988\\E", "shortCiteRegEx": "Lauritzen", "year": 1988}, {"title": "Reasoning Under Incomplete Information in Artificial Intelligence : a Comparison of Formalisms", "author": ["Lea Sombe (P. Besnard", "M-0. Cordier", "D. Dubois", "L. Farinas del Cerro", "C. Froidevaux", "Y. Moinard", "H. Prade", "C. Schwind", "P. Siegel"], "venue": null, "citeRegEx": "Besnard et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Besnard et al\\.", "year": 1990}, {"title": "Probabilistic Logic. In :Non-Standard Logics for Automated Reasoning", "author": ["G. Paass"], "venue": "Academic Press (London), Ch", "citeRegEx": "Paass,? \\Q1988\\E", "shortCiteRegEx": "Paass", "year": 1988}, {"title": "Probabilistic Reasoning in Intelligent Systems : Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "INFERNO:a cautious approach to uncertain inference", "author": ["J.R. Quinlan"], "venue": "The Comp. Res.,", "citeRegEx": "Quinlan,? \\Q1983\\E", "shortCiteRegEx": "Quinlan", "year": 1983}, {"title": "Probabilistic inference and the concept of total evidence", "author": ["P. Suppes"], "venue": "In Aspects of Inductive Logic (J. Hintikka and P. Suppes, Eds.) North-Holland,", "citeRegEx": "Suppes,? \\Q1966\\E", "shortCiteRegEx": "Suppes", "year": 1966}, {"title": "Computing probability intervals under independency constraints", "author": ["L.C. van der Gaag"], "venue": "Proc. of the 6th Conf on Uncertainty in Artificial Intelligence,", "citeRegEx": "Gaag,? \\Q1990\\E", "shortCiteRegEx": "Gaag", "year": 1990}, {"title": "Syllogistic reasoning in fuzzy logic and its application to usuality and reasoning with dispositions", "author": ["L.A. Zadeh"], "venue": "IEEE Trans. on Systems, Man and Cybernetics,", "citeRegEx": "Zadeh,? \\Q1985\\E", "shortCiteRegEx": "Zadeh", "year": 1985}], "referenceMentions": [{"referenceID": 20, "context": "see (Zadeh, 1985).", "startOffset": 4, "endOffset": 17}, {"referenceID": 12, "context": "This type of incomplete statistical information is considered by Kyburg (1974) as a large part of our commonsense knowledge.", "startOffset": 65, "endOffset": 79}, {"referenceID": 16, "context": "hard to check) and prior probabilities for computing probability estimates (Pearl, 1988).", "startOffset": 75, "endOffset": 88}, {"referenceID": 16, "context": "Note that in terms of conditional probabilities we assume information on both P(A I B) and P(B I A), which contrasts with Bayesian networks (Pearl, 1988).", "startOffset": 140, "endOffset": 153}, {"referenceID": 3, "context": "prior probability information is required in order to start the inference process in the approach described in this paper (contrary to Quinlan ( 1983)'s INFERNO system or Baldwin (1990)'s support logic programming).", "startOffset": 171, "endOffset": 186}, {"referenceID": 8, "context": "Other works have been published, that handle probability bounds (see (Dubois et al., 1990) for a survey).", "startOffset": 69, "endOffset": 90}, {"referenceID": 2, "context": "The reasoning systems of Bacchus (1990) aim at embedding the type of knowledge we deal with into a formal logical setting.", "startOffset": 25, "endOffset": 40}, {"referenceID": 5, "context": "In Dubois and Prade (1988), see also LeaSombe (1990), a first local pattern of reasoning,", "startOffset": 3, "endOffset": 27}, {"referenceID": 5, "context": "In Dubois and Prade (1988), see also LeaSombe (1990), a first local pattern of reasoning,", "startOffset": 3, "endOffset": 53}, {"referenceID": 8, "context": "In (Dubois et al., 1990) two other local patterns enable us to estimate conditional probabilities involving conjunctions of events or contexts in their expression.", "startOffset": 3, "endOffset": 24}, {"referenceID": 5, "context": "Although in Bayesian reasoning, computing P(B I A) is the same as assuming the posterior probability of A is 1 because the probability distribution on X is unique, these two operations no longer coincide with probability bounds : one is just focusing on a reference class (what can be said about B, for mem hers of A?) while the other is knowledge updating (see Dubois and Prade, 1991b). Dubois and Prade (1991b) further discuss the difference between focusing and updating in the framework of belief functions.", "startOffset": 362, "endOffset": 413}, {"referenceID": 5, "context": "Although in Bayesian reasoning, computing P(B I A) is the same as assuming the posterior probability of A is 1 because the probability distribution on X is unique, these two operations no longer coincide with probability bounds : one is just focusing on a reference class (what can be said about B, for mem hers of A?) while the other is knowledge updating (see Dubois and Prade, 1991b). Dubois and Prade (1991b) further discuss the difference between focusing and updating in the framework of belief functions. As for the difference between computing P(B I A) and P(B) when A is an accepted fact, this topic has been considered in the philosophical literature for a long time. See e.g. Suppes (1966).", "startOffset": 362, "endOffset": 701}, {"referenceID": 15, "context": "It has been shown in (Paass, 1988) that reasoning from numerically quantified general rules may be modelled as an optimization problem.", "startOffset": 21, "endOffset": 34}, {"referenceID": 4, "context": "Indeed, as pointed out by (Charnes and Cooper, 1962), letting y; = xV(d\u00b7tx), we obtain : (P') {Opti c-ty, y 2:0 under M\u00b7y \ufffd 0, d-ty = 1) So, as explained and exemplified in (Amarger et al.", "startOffset": 26, "endOffset": 52}, {"referenceID": 0, "context": "Indeed, as pointed out by (Charnes and Cooper, 1962), letting y; = xV(d\u00b7tx), we obtain : (P') {Opti c-ty, y 2:0 under M\u00b7y \ufffd 0, d-ty = 1) So, as explained and exemplified in (Amarger et al., 1990), the calculation of bounds for P(B I A) requires that two linear programs be solved (one to compute the exact lower bound and one to compute the exact upper bound).", "startOffset": 173, "endOffset": 195}, {"referenceID": 9, "context": "A simpler version of Proposition 2 is used by Fertig and Breese (1990) for arc reversal in influence diagrams where probabilities are incompletely known.", "startOffset": 46, "endOffset": 71}, {"referenceID": 5, "context": "The first local inference pattern, already examined in (Dubois and Prade, 1988) and in (Dubois et al.", "startOffset": 55, "endOffset": 79}, {"referenceID": 8, "context": "The first local inference pattern, already examined in (Dubois and Prade, 1988) and in (Dubois et al., 1990), corresponds to the evaluation of a missing arc in the inference network, and can be viewed as the counterpart of node removal in influence diagrams.", "startOffset": 87, "endOffset": 108}, {"referenceID": 5, "context": "For a proof that these bounds are optimal see (Dubois and Prade, 1988 ; Dubois et a!., 1990).", "startOffset": 46, "endOffset": 92}, {"referenceID": 10, "context": "longest paths (see (Gondran and Minoux, 1985) for instance).", "startOffset": 19, "endOffset": 45}, {"referenceID": 0, "context": "The QS rule gives a1 tv az, az tv at, az tv U3 (equivalence) Ut fv U3 the basic lower bound for P(A n B I C) (see (Amarger et al., 1990)) corresponds to '6 tv a, '6 tv P (right and) '(fvU/\\\ufffd These analogies are no longer surprizing since such kinds of links between probabilistic reasoning and non\u00ad monotonic logic have been already laid bare by (Pearl,", "startOffset": 114, "endOffset": 136}, {"referenceID": 6, "context": "1988) and the authors (Dubois and Prade, 1991).", "startOffset": 22, "endOffset": 46}, {"referenceID": 5, "context": "Another topic is the extension of our method to fuzzy quantifiers, already considered in (Dubois and Prade, 1988) for the syllogism rule.", "startOffset": 89, "endOffset": 113}], "year": 2011, "abstractText": "An approach to reasoning with default rules where the proportion of exceptions, or more generally the probability of encountering an exception, can be at least roughly assessed is presented. It is based on local uncertainty propagation rules which provide the best bracketing of a conditional probability of interest from the knowledge of the bracketing of some other conditional probabilities. A procedure that uses two such propagation rules repeatedly is proposed in order to estimate any simple conditional probability of interest from the available knowledge. The iterative procedure, that does not require independence assumptions, looks promising with respect to the linear programming method. Improved bounds for conditional probabilities are given when independence assumptions hold.", "creator": "pdftk 1.41 - www.pdftk.com"}}}