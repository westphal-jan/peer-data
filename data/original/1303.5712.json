{"id": "1303.5712", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "Symbolic Probabilistic Inference with Continuous Variables", "abstract": "Research on Symbolic Probabilistic Inference (SPI) [2, 3] has provided an algorithm for resolving general queries in Bayesian networks. SPI applies the concept of dependency directed backward search to probabilistic inference, and is incremental with respect to both queries and observations. Unlike traditional Bayesian network inferencing algorithms, SPI algorithm is goal directed, performing only those calculations that are required to respond to queries. Research to date on SPI applies to Bayesian networks with discrete-valued variables and does not address variables with continuous values. In this papers, we extend the SPI algorithm to handle Bayesian networks made up of continuous variables where the relationships between the variables are restricted to be ?linear gaussian?. We call this variation of the SPI algorithm, SPI Continuous (SPIC). SPIC modifies the three basic SPI operations: multiplication, summation, and substitution. However, SPIC retains the framework of the SPI algorithm, namely building the search tree and recursive query mechanism and therefore retains the goal-directed and incrementality features of SPI.", "histories": [["v1", "Wed, 20 Mar 2013 15:30:11 GMT  (213kb)", "http://arxiv.org/abs/1303.5712v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kuo-chu chang", "robert fung"], "accepted": false, "id": "1303.5712"}, "pdf": {"name": "1303.5712.pdf", "metadata": {"source": "CRF", "title": "Symbolic Probabilistic Inference with Continuous Variables", "authors": ["Kuo-Chu Chang", "Robert Fung"], "emails": [], "sections": [{"heading": null, "text": "1 Introduction\nThe Bayesian networks technology provides a language for representing uncertain beliefs and inference algo rithms for drawing sound conclusions from such repre sentations. A Bayesian network is a directed, acyclic graph in which the nodes represent random variables, and the arcs between the nodes represent possible probabilistic dependence between the variables. The\n1 This work is based on research supported by WRDC under Contract F33615-90-C-1482.\nsuccess of the technology is in part due to the devel opment of efficient probabilistic inference algorithms [5, 6, 7). These algorithms have for the most part been designed to efficiently compute the posterior proba bility of each node or the result of simple arbitrary queries. They have not efficiently addressed the more general problem of answering multiple queries with re spect to differing sets of evidence.\nRecent research in Symbolic Probabilistic Inference (SPI) [2, 3) has made a significant step in this direc tion. Unlike traditional Bayesian network inference algorithms, SPI algorithm is goal directed, performing only those calculations that are required to respond to queries. In addition, SPI is incremental with respect to both queries and observations. However, the research to date on SPI applies only to Bayesian networks with discrete-valued variables.\nThere have been several inference algorithms designed to handle networks that are made up of continuous variables where the relationships between the variables are restricted to be \"linear gaussian\". The algorithms include the distributed algorithm (7] and the influence diagram approach (4, 8).\nIn this paper, we extend the SPI algorithm to perform this function-handle Bayesian networks with contin uous linear gaussian variables. We call the extension, SPI Continuous (SPIC). The framework of this algo rithm is the same as that for SPI. However, the ba sic SPI operations of multiplication, integration and substitution are quite different. Because SPIC stays within the SPI framework, the goal-directed and in crementality features of the algorithm are preserved.\nThe paper is organized as follows. Section 2 briefly summarize the SPI algorithm which includes the con struction of the SPI node tree and the recursive query processing. Section 3 describes the new algorithm with continuous variables. The representation are described as well as the \"basic\" operations. Finally, some con cluding remarks are given in Section 4.\n78 Chang and fung\n2 Overview of the SPI Algorithm\nThe SPI algorithm consists of several major process ing steps. The first step is to organize the nodes of a Bayesian network into a tree structure for query pro cessing. We call these structures SPI trees. In the second step, queries are directed to the root node of the SPI tree. The query is decomposed into queries for the node's subtrees. This recursive procedure con tinues until a particular query can be answered at the node at which it is directed. The answer is then com puted and returned to the next higher level in the SPI tree. Once a node has responses from all of its sub trees it can compute its own response to its predeces sor node. This process terminates when the root node processes all the responses from its subtrees.\nAn SPI tree is constructed by organizing the nodes of a Bayesian network into a tree structure. The only constraint on the construction process is that if there is an arc between two nodes in the original network, then one of the nodes must be a direct or indirect pre decessor of the other in the SPI tree. This constraint allows many possible SPI trees.\nThe first step in building a SPI tree is to choose the root node. This done by computing the maximum node to node distance for each node. The node that has the smallest maximum distance is chosen as the root node. This heuristic is designed to produce a. \"bushy\" SPI tree which can take advantage of dis tributed processing. The second step is to use max imum cardinality search [9] to build the tree from the root node. This step constructs the tree based on the connectivity principle and guarantees satisfaction of the tree construction constraint.\nThe general format for a query received by SPI is as a conditional probability, namely, P{XIY}, where X and Y are sub sets of nodes in the Bayesian networks. To be processed by SPI, queries of this form are first transformed into another format. The format consists of two set of nodes L and M which satisfy:\nM=(XUY)nD(L) (1) where X= MnL, Y = M\\L, and D(L) is the dimen sion of the distribution associated with the node set L. Intuitively, L represents the minimum set of node dis tributions needed to respond to the query and M is dimension of the desired response. L and M can be computed in linear time and are simple to implement [3]. Figure 1 and 2 show a simple Bayesian network and the corresponding SPI tree. For example, if the query is to find the joint probability of a1 and c2, the query being sent to the root node will be consisting of L = {a1,a2,c1,c2} and M = {a1,c2}.\nThe heart of the SPI algorithm is as follows; at any node i, a request arrives for a probability distribution represented by L and M, the algorithm responds to the request by computing the \"generalized\" distribu-\nFigure 1: An Example Network\nFigure 2: The SPI tree\nSymbolic Probabilistic Inference with Continuous Variables 79\ntion Q(M) [3]. Q(M) is obtained by multiplying the distributions in node L and summing over dimensions L \\ M. If such a distribution had already been com puted earlier and cached, it can be returned immedi ately. However, usually it will be necessary to send requests to node's successors in the SPI tree in order to compute the response. It is obvious that if a par ticular subtree has nothing to do with the query (i.e., there is no intersection), then no query will be sent to that subtree. For the same example above, the query can be obtained as,\nQ(a1,c2) = 1l\"a1 L 1l\"a01l\"c11l\"c, (2)\nwhere 1r; represents the probability distribution asso ciated with node i. There are three major operations in SPI algorithm: multiplication, summation and sub stitution. Multiplication calculates the product of two distributions; summation calculates the sum of a dis tribution over a set of variables; and substitution cal culates the result of substituting an observed value for a node into a distribution. For networks with con tinuous variables, the SPI algorithm can be applied directly. However the multiplication, summation, and substitution operations must be modified. In the next section, we will describe the corresponding multiplica tion, integration, and substitution operations for the networks with continuous variables.\n3 The SPI with Continuous Variables Algorithm\nThe continuous SPI (SPIC) algorithm requires redef inition of the SPI operations: multiplication, integra tion, and substitution. The general mechanism for the continuous SPI algorithm is basically the same as the discrete one except for the caching operation and the handling of evidence. We first describe the representa tion for conditional probability distributions in linear gaussian continuous variables and then the three op erations in detail.\n3.1 Node Representation\nA SPIC node represents a vector of continuous vari ables. SPIC restricts the conditional probability dis tribution of each node to be \"linear gaussian\". \"Linear gaussian\" distributions are the sum of a deterministic component and a probabilistic component. The de terministic component is a linear combination of the node's predecessor values. The probabilistic compo nent is restricted to be gaussian (i.e., normal) which can be specified with mean vectors and covariance ma trices. For a Bayesian network of this type, the nec essary prior information needed before any inference can be drawn are the the prior distributions of the root nodes (i.e., mean and covariance) and the links between nodes in the network.\nFor example, for a random vector represented by a node x, if it is a root node, only mean x and the corre sponding covariance matrix Q, need to be specified. If it is not a root node, and has a set of predecessor nodes x\ufffd, .. , x\ufffd, then the relation between x and its predeces sors represented by the following linear equation need to be specified,\nx = Btx{ + ... + BNxj, + w, (3)\nwhere B1, .. , BN are constant transition matrices rep resenting the relative contribution made by each of the predecessor variables to the determination of the dependent variable x, and w, is a noise vector sum marizing other factors affecting x. w, is assumed to be normally distributed with mean x and covariance Q,. For most applications, w, will have a zero mean. However this is not always true apriori and can occur when distributions are multiplied together (e.g., a root node and a non-root node).\nFor each node x in a network, the sufficient informa tion describing the node itself and the relationship to its predecessors can therefore be represented in the fol lowing form:\n( 4)\nWith this simple representation, we will then describe the multiplication, integration, and substitution oper ations.\n3.2 Multiplication\nIn this section, we describe the multiplication oper ation for SPIC. We describe when distributions can be multiplied, what the result will look like, and then describe in detail how each part of the result is com puted.\nThere is a constraint on what distributions can be mul tiplied. This constraint called \"combinability\" was de veloped in [1]. According to the theorem derived in [1], a set of nodes S is \"combinable\" (i.e., able to be ag gregated) if and only if every pair of nodes in the set is combinable. Two nodes are combinable if all nodes in the path(s) between the two nodes are in the setS. It can be shown that if the set of nodes corresponding to the distributions to be multiplied are \"combinable\", then multiplying the distributions is the same as find ing the \"joint\" distribution of those nodes, or in other words, to find the new probability representation of the \"combined\" node.\nBased on the separation principle in the SPI tree (i.e., any node separate its successors rooted from itself) (3], it can be easily shown that any distributions that will be multiplied from any query request during SPI processing are always combinable. In other words, we can transform the multiplication operation in SPI al gorithm into a node combination operation for the con tinuous nodes.\n80 Chang and fung\nThe dimension of the resulting distribution will be the sum of the dimensions of those nodes to be multiplied. For instance, two representations of nodes (variables) x1 and x2 each with dimension D(xt) and D(x2) are to be multiplied, the resulting representation will have dimension D(xt) + D(x2) , which can be interpretated as the description of the combined variable x with x1 stack over x2, i.e., x = [ :\ufffd ] . The question now is how to calculate the new probability representation of x based on the old ones of x1 and x2\u2022 It is dear that this is nothing but to identify the new predecessors xr of x and calculate the links B; between x and the new predecessors as well as to calculate the new conditional mean x and the associated covariance Qx.\nFirst of all, the new predecessors of x is just the union of the predecessors of x1 and x2 (excluding Xi or X2 when one is the direct predecessor of the other).\u00b7 The new linear relation (coefficients) between x and xf are obtained based on the old links. Conceptually, this can be accomplished by first breaking down the combined node into the original element nodes, and then find the relation between the predecessor and the element nodes individually. To do so, all paths between the predecessor and the desired element node are found and then combined together. The contribution of a path is obtained by multiplying the transition matrices of links along the path in the original network.\nThe mean and associated noise covariance matrix for the combined node is the last part of the representa tion that needs to be calculated. The first quantity that requires computation is the implicit linear rela tionship T between the two nodes that are to be mul tiplied. Based on the SPI tree structure, it can be shown that the T for two nodes is non-zero only when one node is the direct or indirect predecessor of the other. If the nodes do stand in this relation, T can be calculated by first finding all paths between the two nodes and adding the contribution from all paths. As in the process for finding the link coefficients for the new predecessors, the path contributions are obtained by multiplying together the transition matrices of links along the path. Given T, the new mean and covari ance matrix of x can be obtained as below, given that x1 is the direct or indirect predecessor of x2.\n(5)\nQ - [ Qx, X- TQ x, Q' T' ] X} TQx,T + Qx, (6)\nFor example, the results of multiplying 'll\"e1 and 'll\"e, of the previous example given in Figure 1 can be repre sented by\nC!2 = [ CC21 ]\n(7) where c12 is the \"combined node\", Txy are the links (transition matrices) between x and y, and a1 and a2 are the new predecessors. In addition to the links ob tained as above, the new conditional mean and corre sponding covariance are obtained as,\n(8)\nwhich is zero since both c-1 and c2 are zeros, and\n(9)\nIn the discrete case, multiplying distributions can be expensive since the size of the resulting distribution grows exponentially with the number of distribution (nodes) to be multiplied. However, in the linear gaus sian case, the resulting representation only increases quadratically. This is because a gaussian distribution can be sufficiently represented by a mean vector and a covariance matrix.\n3.3 Integration\nThe integration operation is relatively simple. All one has to do is to identify and keep the appropriate slots from the representation (i.e., links to predeces sors, mean, and covariance) and discard the rest of them. It can be easily shown that the reduced repre sentation precisely describes the resulting distribution after the integration. For example, for the distribution of the combined node c12 obtained from the multipli cation above. If the goal is to integrate c1 out of the joint probability representation, all one has to do is to grab the appropriate slots from the links to a, and a2, the mean vector c12, and the covariance matrix Qc12\u2022 These slots are corresponding to the c2 vari able, namely,\n(11)\nand (12)\n3.4 Substitution\nEvidence is represented in the form of exact observa tion of the values of variables. The set of variables\nSymbolic Probabilistic Inference with Continuous Variables 81\nwhich have been observed is denoted by E. One easy way of incorporating evidence is to include evidence in the query, for instance, P{XIY, E} and then substitute the observed values E* for E after the more general query is computed. Suppose the query results before the substitution of the observation is represented as\n(13)\nwith the associated covariance matrix :!::x. To \"sub stitute\" the observation E*, we first remove the link KE from the representation, then replace the mean X by X + KEE*. The covariance matrix remains the same. It can be easily shown that the new rep resentation correctly describe the results of the query, P{XIY,E = E*}. Other more efficient methods such as to do substitution before query are currently under our investigation.\n4 Conclusion\nRecent research in Symbolic Probabilistic Inference (SPI) has made a significant step in improving effi ciency of general query processing. Unlike traditional inference algorithms, SPI algorithm is goal directed, performing only those calculations that are required to respond to queries. In addition, SPI is incremental with respect to both queries and observations. In this paper, we extent the SPI algorithm to handle Bayesian networks with linear gaussian variables. We call the algorithm SPIC. The framework of this algorithm is the same as that for SPI. However, the basic opera tions of multiplication, integration and handling ob servation are quite different. The equivalent operation to the multiplication of discrete case is similar to the \"node combination\" operation in the continuous case and the equivalent operation to the summation of dis crete case is the integration operation. Because SPIC stays within the SPI framework, the goal-directed and incrementality features of the algorithm are preserved.\nIn this paper, we have only addressed the problem of continuous variables that are restricted to have linear gaussian models. Many real-world problems may require nonlinear or nongaussian models. Classi cal methods such as approximation with linearization (e.g., extended Kalman filters) or sum of gaussians de serve attention in further investigation. A version of the SPIC algorithm has been implemented, prelimi nary results show expected performance.\nReferences\n[1] K. C. Chang and R. M. Fung. Node aggregation for distributed inference in bayesian networks. In Proceedings of the 11th IJCAI, Detroit, Michigan, August 1989.\n[2] B. D'Ambrosio. Symbolic probabilistic inference in belief nets. 1990.\n[3] R. Shachter A. Del Favero and B. D'Ambrosio. Symbolic probabilistic inference: A probabilistic perspective. Proceeding of AAAI, 1990.\n[4] C. Robert Kenley. Influence Diagram Models with Continuous Variables. PhD thesis, Stanford Uni versity, Stanford, California, 1986.\n[5] S. L. Lauritzen and D. J. Spiegelhalter. Local computations with probabilities on graphical struc tures and their application in expert systems. Jour nal Royal Statistical Society B, 50, 1988.\n[6] Judea Pearl. Fusion, propagation, and structuring in belief networks. Artificial Intelligence, 29, 1986.\n[7] Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers, 1988.\n[8] Ross D. Shachter and C. Robert Kenley. Gaussian inflience giagrams. Management Science, 35, 1989.\n[9] R. E. Tarjan and M. Yannakakis. Simple linear time algorithm to test chordality of graphs. SIAM J. Computing, 13, 1984."}], "references": [{"title": "Node aggregation for distributed inference in bayesian networks", "author": ["K.C. Chang", "R.M. Fung"], "venue": "In Proceedings of the 11th IJCAI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "Symbolic probabilistic inference in belief nets", "author": ["B. D'Ambrosio"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "Symbolic probabilistic inference: A probabilistic perspective", "author": ["R. Shachter A. Del Favero", "B. D'Ambrosio"], "venue": "Proceeding of AAAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1990}, {"title": "Influence Diagram Models with Continuous Variables", "author": ["C. Robert Kenley"], "venue": "PhD thesis, Stanford Uni\u00ad versity,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1986}, {"title": "Local computations with probabilities on graphical struc\u00ad tures and their application in expert systems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "Jour\u00ad nal Royal Statistical Society B,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1988}, {"title": "Fusion, propagation, and structuring in belief networks", "author": ["Judea Pearl"], "venue": "Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1986}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1988}, {"title": "Gaussian inflience giagrams", "author": ["Ross D. Shachter", "C. Robert Kenley"], "venue": "Management Science,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "Simple linear time algorithm to test chordality of graphs", "author": ["R.E. Tarjan", "M. Yannakakis"], "venue": "SIAM J. Computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1984}], "referenceMentions": [{"referenceID": 8, "context": "The second step is to use max\u00ad imum cardinality search [9] to build the tree from the root node.", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "L and M can be computed in linear time and are simple to implement [3].", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "tion Q(M) [3].", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "This constraint called \"combinability\" was de\u00ad veloped in [1].", "startOffset": 58, "endOffset": 61}, {"referenceID": 0, "context": "According to the theorem derived in [1], a set of nodes S is \"combinable\" (i.", "startOffset": 36, "endOffset": 39}], "year": 2011, "abstractText": "Research on Symbolic Probabilistic Inference (SPI) [2, 3) has provided an algorithm for re\u00ad solving general queries in Bayesian networks. SPI applies the concept of dependency\u00ad directed backward search to probabilistic in\u00ad ference, and is incremental with respect to both queries and observations. Unlike tra\u00ad ditional Bayesian network inferencing algo\u00ad rithms, SPI algorithm is goal directed, per\u00ad forming only those calculations that are re\u00ad quired to respond to queries. Research to date on SPI applies to Bayesian networks with discrete-valued variables and does not address variables with continuous values. In this paper1, we extend the SPI algorithm to handle Bayesian networks made up of continuous variables where the relationships between the variables are restricted to be \"linear gaussian\". We call this variation of the SPI algorithm, SPI Continuous (SPIC). SPIC modifies the three basic SPI opera\u00ad tions: multiplication, summation, and sub\u00ad stitution. However, SPIC retains the frame\u00ad work of the SPI algorithm, namely building the search tree and recursive query mecha\u00ad nism and therefore retains the goal-directed and incrementality features of SPI.", "creator": "pdftk 1.41 - www.pdftk.com"}}}