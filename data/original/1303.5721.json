{"id": "1303.5721", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "Search-based Methods to Bound Diagnostic Probabilities in Very Large Belief Nets", "abstract": "Since exact probabilistic inference is intractable in general for large multiply connected belief nets, approximate methods are required. A promising approach is to use heuristic search among hypotheses (instantiations of the network) to find the most probable ones, as in the TopN algorithm. Search is based on the relative probabilities of hypotheses which are efficient to compute. Given upper and lower bounds on the relative probability of partial hypotheses, it is possible to obtain bounds on the absolute probabilities of hypotheses. Best-first search aimed at reducing the maximum error progressively narrows the bounds as more hypotheses are examined. Here, qualitative probabilistic analysis is employed to obtain bounds on the relative probability of partial hypotheses for the BN20 class of networks networks and a generalization replacing the noisy OR assumption by negative synergy. The approach is illustrated by application to a very large belief network, QMR-BN, which is a reformulation of the Internist-1 system for diagnosis in internal medicine.", "histories": [["v1", "Wed, 20 Mar 2013 15:30:56 GMT  (408kb)", "http://arxiv.org/abs/1303.5721v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["max henrion"], "accepted": false, "id": "1303.5721"}, "pdf": {"name": "1303.5721.pdf", "metadata": {"source": "CRF", "title": "Search-based Methods to Bound Diagnostic Probabilities in Very Large Belief Nets", "authors": ["Max Henrion"], "emails": ["Henrion@Sumex-AIM.Stanford.EDU"], "sections": [{"heading": null, "text": "1 INTRODUCTION Bayesian belief networks provide a tractable basis for expressing uncertain knowledge at both qualitative and quantitative levels, in a way that is formally sound and intuitively appealing. They are already being used in a wide variety of applications, including knowledge bases of up to about one thousand nodes. A major obstacle to their application for still larger applications is the limitations of available algorithms for diagnostic inference. Exact diagnostic inference in general belief networks has been shown to be NP-hard (Cooper, 1991). Hence, there is considerable interest in the development of methods that provide greater efficiency at the cost of imprecision in the results (Henrion, 1990b ).\nThere have been two main directions in which researchers have sought efficient approximate algorithms. One\napproach involves random sampling of network instantiations, also known as stochastic simulation (Henrion, 1988). The other involves search among the space of instantiations (hypotheses) to find those that are most probable. Cooper (I 984) employed this approach in Nestor, to obtain the most probable hypotheses. Peng and Reggia (1987a, I987b) and Henrion (1990a) developed more powerful admissability heuristics to prune the search tree, allowing more efficient search of BN20 networks, that is bipartite networks consisting of independent diseases, conditionally independent findings, and noisy ORs, as descnoed in Section 3. These methods are guaranteed to find the most probable composite hypotheses, and their relative probabilities (ratio of posterior probabilities of hypotheses). Peng and Reggia (1989) and Henrion (1990a) also describe methods to bound the absolute probabilities of the composite hypotheses.\nPeng and Reggia's approach to abductive reasoning is based on the notion of minimal covering sets of diseases which explain observed findings. They use logical techniques initially to identify covering sets for the given findings, and then use probabilistic methods to find the most probable hypotheses. This scheme assumes zero leaks, that is that no findings can occur \"spontaneously\" in the absence of any explicitly modelled cause. For the QMR-BN application to be described here, and indeed most medical problems, most findings have non-zero leak rates due to false positives, and so an adequate diagnosis does not necessarily all have to explain all observed findings. This makes the covering set approach inapplicable.\nShimony and Charniak (1990) describe a search-based method that finds the MAP (Maximum A-posteriori Probability) assignments to general belief networks. They show how any belief network can be converted to an equivalent weighted boolean function DAG, and that solving the best selection problem (minimum cost assignment) for this network is equivalent to finding the MAP assignment for the belief network. While the best selection problem is also NP-hard, standard best-first search can be relatively efficient in practice.\nSearch-Based Methods to Bound Diagnostic Probabilities in Very Large Belief Nets 143\nIf the results of diagnostic inference or abductive reasoning are to be used as the basis for making decisions, for example how to treat a patient, or what additional tests to order, knowing the relative probabilities of the most likely complete hypothesis is not enough. We want to know the absolute probabilities, or at least have bounds on them, and we want often want to know the marginal posterior probabilities of individual diseases, or of one or two diseases, rather than of complete assignments which include instantiations of all the other nodes.\nTo obtain bounds on the absolute probabilities, we need bounds on the relative probabilities of all the hypothesis that we have not explicitly examined in the search. That is we want to find bounds on the sum of the relative probabilities of the possible extensions of a given hypothesis. Given bounds on the relative probabilities of all hypotheses, we can compute bounds on the absolute probabilities. However, to find such bounds requires additional knowledge of properties of the network. Qualitative knowledge about influences (Wellman, 1990; Wellman & Henrion, 1991) is a useful source of information to obtain bounds, as we shall see.\nThis paper presents improvements and generalizations to the TopN algorithm. First, I will describe the QMR-BN belief network which is the application providing a context and motivation for this work on algorithm design. I then describe a generalization of the noisy-OR assumption of the BN20 networks, to negative product synergy. This forms a basis for generalized bounding theorems, including a new lower bound, that provides a significant improvement on TopN as presented in Henrion (1990b). Qualitative probabilistic analysis, using signs of influence and synergies, provides a clearer and more general basis for obtaining these. I then describe a method to obtain bounds on the posterior probability of hypotheses and for individual diseases. Finally, I present results from application to the QMR-BN network, showing progressive improvement as search is extended.\n2 QMR AND INTERNIST -1 QMR (Quick Medical Reference) is a knowledge-based system for supporting diagnosis by physicians in internal medicine (Miller et a/, 1986). It is a successor to the Internist-! system (Miller et a/, 1982). The version of the knowledge-base used here contains information for 576 diseases (of the estimated 750 diseases comprising internal medicine) and over 4000 manifestations, such as patient characteristics, medical history, symptoms, signs, and laboratory results. In this paper, these are referred to generically as findings. QMR contains over 40,000 disease-finding associations. It represents about 25 person years of effort in knowledge engineering and is one of the most comprehensive structured medical knowledge-bases currently existing.\nThe knowledge-base consists of a profile for each disease, that is, a list of the findings associated with it. Each such association between disease d and finding/ is quantified by two numbers: The evoking s trength is a number between\n0 and 5 which answers the question \"Given a patient with finding f, how strongly should I consider disease d to be its explanation?\". The frequency is a number between 1 and 5 answering the question \"How often does a patient with disease d have finding f?\". Associated with each finding f is an import, being a number between 1 and 5 answering \"To what degree is one compelled to explain the presence of finding/ in any patient?\".\n3 QMR-BN: A PROBABILISTIC INTERPRETATION OF QMR\nThe aim of this project1 is to develop a coherent probabilistic interpretation of QMR, which we call QMR BN (for Belief Network), and eventually a version with treatment decisions and cost or value models, which we call QMR-DT (for Decision Theory). The first goal is to improve the consistency of the knowledge base and to explicate the independence assumptions it incorporates. A second goal is to provide a challenging example to develop and test new algorithms for probabilistic reasoning. The current version is a reformulation of the Internist-! knowledge-base. See Henrion (1990a), Shwe et a/, (1991) and Middleton et a/, (1991) for more details.\nDiseases\nFigure 2: BN20 Belief net\nA probabilistic representation can be divided into two aspects: The framework of qualitative assumptions about dependence and indcpendences, and the quantification of the probabilities within that framework. QMR-BN currently follows INTERNIST-I and QMR in assuming that all diseases and findings are binary variables, being either present or absent, without intermediate values. The initial qualitative formulation incorporates the following assumptions, expressed by the belief net in Figure 2:\nAssumption 1 (MID): Diseases are marginally independent.\nAssumption 2 (CIF): All findings are conditionally independent of each other given any hypothesis.\n1 This project is a collaboration with Gregory Cooper, David Heckcrman, Eric Horvitz, Blackford Middleton, and Michael\nShwe.\n144 Henrion\nAssumption 3 (LNOG): The effects of multiple diseases on a common finding are combined as a Leaky Noisy OR Gate. Suppose Sdf is the link event that disease d is sufficient to cause finding f. 2 The noisy OR assumption is that finding f will occur if any link event occurs linking a present disease to f, and that these link events are independent. (This is sometimes known as causal independence.) With a leaky noisy OR an additional leak event Lt is possible, which can cause f to occur even with no explicit disease present.\nDefinition 1 (BN20): The class of bipartite belief nets conforming to Assumptions 1, 2 and 3, are termed BN20.\nSome of the findings in INTERNIST-!, such as the demographics or family history of a patient, are not actually caused by diseases, but rather circumstances or risk factors that may affect disease probabilities. These variables should rearranged for ease of assessment so that they influence the diseases rather than vice versa. Currently, we have done this with age and sex as represented in figure 3.\nFindings\nDemographic factors\nprevalence rates for each disease, a quantity with no correspondence in the INTERNIST-1/QMR knowledge base. These were estimated from data compiled by the National Center for Health Statistics on the basis of hospital discharges, conditional on the specified demographic (age and sex) categories. In summary, the qualitative independence assumptions of BN20, together with the link probabilities, leak probabilities, and disease probabilities conditional on age and sex, specify a reformulation of QMR in coherent probabilistic form.\n4 INFERENCE ALGORITHMS Given this BN20 representation, is there a tractable method for diagnostic inference? To compute the exact posterior probability of any hypothesis, we need to compute the sum of relative probabilities of all hypotheses. Since the set of complete hypotheses (disease combinations) is the powerset of the set of diseases and has cardinality of z576, this may seem a rather daunting prospect. We have explored at least three different approaches for diagnostic inference for this class of networks. These include an exact method (Quickscore), and two approximate methods, one using a forward sampling or simulation scheme, (likelihood weighting), and one using search of the hypothesis tree with probability bounding (TopN).\nThe QuickScore algorithm (Heckerman, 1989) uses an ingenious rearrangement of the summation. Its complexity is polynomial in the number of diseases but is exponential in the number of findings observed. In practice it can score cases with 12 findings in about 10 minutes (Lightspeed Pascal on a Mac IIci), but it becomes too slow if there are many more findings. Since BN20 has large numbers of intersecting loops, exact methods seem unlikely to be tractable for larger problems.\nLikelihood weighting (Shachter & Peot, 1989; Fung & Chang, 1989) is a development of logic sampling (Henrion, 1988) in which each randomly generated hypothesis is weighted by the likelihood of the observed findings conditional on the hypothesis. Further efficiency is achieved by using importance sampling, in which the sampling probabilities of diseases are iteratively adjusted to reflect the evolving estimate of their actual probabilities. The S algorithm (Shwe & Cooper, 1990) initializes the probabilities with a version of tabular Bayes (assuming mutual exclusivity of diseases) as a starting point for sampling. This version converges to reasonable estimates of the posterior probabilities in about 40,000 samples taking an average of 94 minutes for the SAM cases (on a Macintosh Ilci).\nThe TopN algorithm takes a quite different approach, searching among hypotheses, that is complete instantiations of the diseases. It relies for its efficiency on the assumption that of the vast (z576 for QMR-BN) set of possible hypotheses, only a tiny fraction of them account for most of the probability mass. Hypotheses with more than a few diseases (five or six at most) have negligible probabilities, since the improbability of that many\nSearch-Based Methods to Bound Diagnostic Probabilities in Very Large Belief Nets 145\ndiseases rapidly outweighs any possible improvement in explaining the observed findings. The second key idea is that, even though computing the absolute posterior probability of a hypothesis is intractable in general (requiring summing over all hypotheses), it is easy to compute the relative probabilities of two hypotheses (see also Cooper 1984; de Kleer & Williams, 1986; and Peng & Reggia 1987a). The third key element is an admissability heuristic to prune paths that cannot led to the most probable hypothesis (or most probable N hypotheses, hence the name TopN), so that only a small part of the space need be searched. A fourth element are some theorems that allow bounding of the sum of relative probabilities of all extensions of each hypothesis, and hence allow obtaining bounds on the absolute probabilities of hypotheses without examining them all. In the following I will give more detail on these, with some extensions and generalizations of previous results.\n5 NOTATION: I will use the common convention that lower case letters,\nsuch as d, refer to variables, with uppercase, D and D, referring to the events d=true and d=false, respectively. Analogously, if h is a set of diseases, then H denotes the event that all diseases in h are true (present), and JH[ denotes the event that all diseases in h are false (absent): 2\nH= U D, G= U D VdEh V dE g\nGiven a set of diseases, 6 = { d!, dz, ... dn), a complete hypothesis is an event that assigns a value, true or false, to every disease in 6. A partial hypothesis assigns a value to a proper subset of the diseases in 6, leaving the rest unspecified. If hc6, then H is a partial hypothesis, since diseases not in h remain unspecified. Adjacency of events denotes conjunction. So the event HG specifies that all diseases in h are present, all those in g are absent, and the rest unspecified. (We assume hng = 0.)\nUnderlining makes a complete hypothesis from a partial one, assigning absent to all diseases not specified. Thus H denotes the event that all diseases in h are present and all others in 6 absent:\nH= U Du U D VdEh Vd<\ufffd.h\n2Note that lHI is not equivalent to H. the event that at least one of the diseases in h is absent.\n6 RELATIVE PROBABILITY AND MARGINAL EXPLANATORY POWER\nWe define h0=0 as the empty set of diseases, and Ho is the corresponding event that no disease from 6 is present. The relative probability of a hypothesis H is the ratio of the posterior probability of H given findings F to the posterior probability of hypothesis Ho:\nP(H I F) P(H F) R(H) Rill) =\nP(Hol F) = P(Ho F) = Rilio) [1]\nTopN starts its search from h0, extending it by adding one disease at a time. To generate the next candidate hypothesis, it adds to the current hypothesis the disease which leads to the largest relative probability. To identify the n most probable hypotheses (hence \"TopN\"), it applies an admissibility heuristic, which abandons a search path when it provably cannot lead to any hypothesis more probable than the nth best so far. TopN's admissibility criterion is based on the concept of Marginal Explanatory Power (MEP).\nDefinition (MEP): The Marginal Explanatory Power (MEP) of a disease d with respect to a hypothesis set of diseases, h, is the ratio of the posterior probability of the extended hypothesis hud to the posterior of h alone:\nP(HDI F) R(HD) MEP(D, H) =\nP(Hl F) = R(H) [2]\nThe MEP is a measure of the increase or decrease (according to whether it is greater or less than 1) in the degree to which the hypothesis explains the findings F due to the addition of d. The use of the MEP as the basis of an admissable search heuristic depends on the following result (Henrion 1990a):\nTheorem la (declining MEP): Given a BN20 network, for any disease d, and disease sets h and g, the marginal explanatory power (MEP) of d with respect to h cannot be less than the MEP of d for any extension hug, i.e.\nMEP(D, H)\ufffd MEP(D, HG) [3]\nWhen searching for the most probable hypothesis from current hypothesis h, if MEP(D, H) \ufffd 1 then d can be eliminated as a path for exploring as an extension to H, since it cannot lead to a more probable hypothesis. It can also be eliminated as a candidate for extending other extensions of H. Thus the only diseases which need to be considered as extensions of H are those for which MEP(D, H)> 1.\n7 NEGATIVE PRODUCT SYNERGY AND THE MEP THEOREM\nIt turns out that Theorem I a does not require the leaky noisy OR assumption 3 of BN20, assumed in Henrion\n146 Henrion\n(1990a); a weaker assumption, negative product synergy will suffice. First, we define this property, and then show the more general version of the theorem.\nDefinition 2a (two cause NPS): Suppose there are two propositions, d and e, and other variable(s) x, that influence finding F according to the conditional probability distribution P(FI d ex), there is negative product synergy in the influence of d and e on J, iff\n-\nP(FIDE x) <_ P(FIDE x) ...., vx.\n- -- P(FIDE x) P(FIDE x) [4]\nThis is the condition required for disease d to \"explain away\" the evidence F, that is, given F, there is a negative influence between d and e (Henrion & Druzdzel, 1990):\n-\nP(EID F x) \ufffd P(EID F x) \"iix. [5]\nIt is simple to show that the noisy OR (with or without leaks) exhibits negative product synergy, and so gives rise to this explaining away phenomenon. Wellman and Henrion ( 1991) generalize the definition of product synergy for n-ary variables, and discuss its relation to additive synergy. Here we generalize the definition in a different way to apply where there are more than two variables which together influence another variable:\nDefinition 2b (n cause NPS): Consider a set .1. of propositions which influence finding F, as specified by conditional probability distribution P(Fit.). The influence exhibits negative product synergy, iff for any sets of\npropositions x,y,z\ufffdLl. there is negative product synergy between x andy given z, i.e.\nP(FIXYZ) P(FIXZ) P(FI.YZ) \ufffd P(FIZ) . [6]\nAssumption 4 (POS): The influence of every disease d on every finding f is positive, that is, for any set of diseases h not containing d,\n-\nP(FIDH) \ufffd P(FIDH), \"iihcll, where dE h\nSince the inequality is weak, this also allows diseases and findings to be unlinked (independent). Positive influence from disease to finding is an automatic consequence of Assumption 3, the leaky noisy ORs, but not of negative product synergy.\nWe can now define a class of bipartite belief nets that generalizes the leaky noisy OR of BN20 to positive links with negative product synergy:\nDefinition 3 (BN2NPS): A bipartite network is said to be BN2NPS if it satisfies Assumption 1 (marginally independent diseases), Assumption 2 (conditionally independent findings), Assumption 4 (positive links), and negative product synergy (NPS) in the influence of the diseases on each finding .\nWe can now obtain a generalization of Theorem 1a, which applies to BN2NPS:\nTheorem lb (declining MEP): Given a BN2NPS network, then, for any disease subsets x, y, z of .1., the complete set of diseases, the marginal explanatory power (MEP) of x with respect to z cannot be less than the MEP of x for any extension yuz, i.e.\nMEP(X, Z) \ufffd MEP(X, YZ) [7]\nProof: Taking the ratio of the two sides, and substituting the definition of MEP [2],\nMEP(X, Z) P(F XZ) P(F .YZ) MEP(X, YZ) = P(F Z) P(F XYZ)\nP(FIXZ) P(FI.YZ) P(XZ) P(.YZ) = P(FIZ) P(FIXYZ) x P(Z) P(XYZ) [S]\nFrom the definition of n cause negative product synergy [6] above, we know the first term of the produce above is \ufffd1. From the marginal independence of diseases, we know that\nP(XZ) = P(Z) fiO(D), where O(D) = \ufffd\ufffdm) dez\nExpanding P(.YZ) and P(XYZ) similarly in the second term, the top and bottom cancel out. Hence we are left with the entire ratio as \ufffdI. QED.\n8 BOUNDS ON THE PROBABILITY OF EXTENSIONS\nWe want not just to identify the most probable hypotheses using their relative probabilities, but to obtain bounds on their absolute probabilities. To do this we need to obtain bounds on the relative probabilities of all the extensions of hypotheses in the search tree, so that we can put bound on the contributions of all the hypotheses we do not examine explicitly.\nSo far we have considered only complete hypotheses, such as H. The relative probability of a partial hypothesis H is the sum of the relative probabilities of all complete extensions of H, that is all complete hypotheses in which all diseases in h are present, that is,\nR(H) = .LN.Qh R(,S) [9] We also need the relative probabilities of partial hypotheses that contain excluded diseases, such as:\nR(HG) = L\"i! s where gc::2.Qh R(,S), [I OJ where gC is the complement of g, i.e. the set of diseases\nin L1 but not in g.\nSearch-Based Methods to Bound Diagnostic Probabilities in Very Large Belief Nets 147\nThe following result gives an upper bound for the relative probability of a partial hypothesis h excluding diseases in g. It gives it in terms of the relative probability of the corresponding complete hypothesis and the MEP for candidate extension diseases d with respect to h, which are relatively easy to compute:\nTheorem 2 (UBI):\nR(HG) \ufffd R(ID fl[l+MEP(D, H)]. 'v'dehug\n[11]\nThis follows from the observation that that at most there is no overlap between the findings explained by each disease, and so the MEP(D, H) for each disease d is the same, no matter how many other diseases are in the hypothesis h it is extending. It is a generalization of Theorem 2 given in Henrion ( 1990c) for the BN20 assumptions. The complete proof relies on the Declining MEP Theorem lb, and so it also follows from the more relaxed BN2NPS assumptions.\nR(H) provides a simple lower bound (LBl) for R(HG).\nThis bound would be attained if all proper extensions S::Jh had probability R(S)=O (Henrion, 1990c).\nAn higher lower bound is given by the following:\nTheorem 3 (LB2):\n> IT 1 R(HG) _ R(ID l-P(D)\" 'v'dehug\n[12]\nThis follows from Assumption 4 of positive influences, that extending a hypothesis h by disease d cannot reduce the likelihood of evidence F, that is P(FIHD) :2: P(FI.!:D. There are often diseases d which explain nothing more than hypothesis h, that is for which P(FIHD)=P(FIH). Since these diseases are independent of the rest conditional on H, it is possible to factor out their contributions to a partial hypothesis HG thus:\nTheorem 4 (Factoring independents):\nR(HG) = R(HGW) IT 1-i(D)' 'v'dEw\nwhere w= ( d : P(FIHD)=P(FI.!:D}.\nThis allows us to remove all such independent (non explanatory) diseases, w. from the candidate list as extensions of h. while accounting for their contribution. Note that some diseases have relatively high priors (e.g. peptic ulcer with prior 1.6%) and so are not infrequently among the top ten hypotheses even if there is no specific evidence for them. Application of this result prevents them from cluttering up the search process.\nUnfortunately the upper bound UBI is not always a good guide when there are many diseases each of which can explain a lot relative to Ho, i .e. MEP(D, H0)\u00bb 1. In the beginning of the search in a case with twenty or more positive findings, UB 1 can be very large, for example overflowing an 8 byte floating point number (> 1 0300), unless computed as logs. An upper bound avoids this tendency is given by:\nTheorem 5 (UB2):\n< P(HG) - P(H) R(HG) - R(ID + P(FI Ho)P(Ho)\nwhere P(HG) - Pili)\n[13]\n= flP(D) fl[l-P(D)]- flP(D) fl[l-P(D)] dEh d\u00a3g d\u00a3h dEif\n= flP(D) [ fl[l-P(D)] - ;(Ho) ]\u00b7 lit= h lit= [ 1-P(D)] g\ndEh\nThis is based on the observation that at most any extension D to H will completely explain all findings, that is P(FIDH) \ufffd 1. This bound is complementary to UBI, with use early in the search in cases with many positive findings.\n9 SEARCH METHOD\nThe search uses a best-first approach, where \"best\" means the candidate partial hypothesis with the greatest possible contribution to uncertainty about the relative posterior probability. This uncertainty is measured as the maximum error, the difference between the lower bound 2 and the least of the upper bounds:\nMaxErr(h) = Min(UBl(h), UB2(h)) - LB2(h) [14]\nWe order the candidate hypotheses by MaxErr and select the top one as the next one to expand. This is the one for which expansion has the largest scope for reducing its contribution to the overall uncertainty about the relative probability of all unsearched hypotheses. Each time a hypothesis is expanded, this reduces the bounds on its parents. Search terminates, either when the MaxErr is less than a criterion, Pmin, expressed as a fraction of the upper bound on the total relative probability, or when the search runs out of space for the hypothesis tree. As in most best-first or A* searches, the algorithm is liable to be memory bound, running out of space before running out of time.\n10 OBTAINING ABSOLUTE PROBABILITIES\nSo far we have obtained bounds on the relative probability of a variety of partial hypotheses, including LBR(H), UBR(H) for each hypothesis H in the search tree, each\n148 Henrion\ndisease D, LBR(D), UBR(D), and Ho. Note that the partial hypothesis H0 is all extensions of the no disease hypothesis, i.e. all possible hypotheses, so P(H0) = 1.\nR(Ho) = L'v's;;;!ho R(S) \" P (S, F ) = \ufffd 'v' S;;;!ho P(Ho F)\nHence, P(F) = R(Ho) Pilio F)\nP(F) P(Ho F)\n[15]\n[16]\nThe posterior probability of any partial hypothesis H is\nP(HIF) = P(H F)\nP(F)\nSubstituting in from the definition of relative probability P(H F)= R(H) P<Bo F) and [16] we get\n..1ill!L P(HIF) = R(Ho) . [17]\nThe upper bound for this is when R(H) is at its upper bound UBR(H) and R(Ho) is lower bound LBR(Ho), but note that since the partial hypothesis H0 includes H, we need replace LBR(H) as a component of LBR(Ho) by the the upper bound of H in the denominator too. Thus, we get the upper bound on the posterior probability of H is:\nUBR(H) UBP(HIF) = LBR(Ho)-LBR(H)+UBR(H)' [181\nand similarly the lower bound is _\nLBR(H) LBP(HIF) - UBR(Ho)-UBR(H)+LBR(H)' [19]\nThe maximum total error due to probability of hypotheses not examined in the search is given by\nUBR@o)-LBR(Ho) UBR@o) [20]\nTopN also produces a \"best\" probability estimate for each hypothesis, h , defined as the ratio of the sum of the relative probabilities of all complete hypotheses actually examined that contain h, to the relative probability of all hypotheses examined, e:\nLV' gEe where g;;2h R(Q_) Best(H) = - [21]\nL'v'gEe R(Q.) This probability estimate is guaranteed to be between the lower and upper bounds on the absolute probability.\n1 1 PERFORMANCE OF TOPN: The QMR-BN research team has assembled cases for testing the performance of alternative inference algorithms. These include 16 cases abstracted from the\nScientific American Medicine (SAM) Continuing Medical Education Service. More details of the coding process are given in Shwe et a/ (1991). For analysis of timing and accuracy we examined 12 of the 16 SAM cases in which Quickscore can be run for comparison, that is cases with less than 14 positive findings. These cases have an average 9 positive and 11 negative findings. Table 4 gives results for on the performance of TopN for series of runs using a search precision (Pmin) of I0-5. The number of hypotheses examined varies from 277 to 30000. (In two cases search was cut off after 30000 hypotheses due to exhausting memory space. ) Since the distributions of hypotheses, time, and precision are highly skewed, Table 1 includes minimum, maximum and median, as well as mean values.\nTopN took an average of 18 seconds (maximum of just over a minute) for the 12 SAM cases. The S sampling algorithm was run for 40,000 samples to achieve adequate convergence for the SAM cases, taking an average of 94 minutes on a Macintosh Ilci (about three times faster than the machine used for the Quickscore and TopN runs). In some cases the maximum probability bound is at or near 1, and quite useless. But it turns out that the actual accuracy of the \"best estimate\" probabilities is very good when compared with the exact results from QuickScore, with a mean standard error between of 1.2%. Thus it appears that the bounds are highly conservative (much larger than necessary) in most cases. This finding suggests the sampled hypotheses are quite representative in terms of disease probabilities of the unsampled ones. Of course this may not always be true, but it suggests some interesting conjectures about properties of the hypothesis population. To examine the effect of computational effort on the error, the precision for terminating search Pmin was varied by factors of 10 from w-3 to I0-7. Decreasing Pm in increases the number of hypotheses explored, and decreases the maximum bound on the probability error. The computation time is approximately linear in the number of hypotheses examined, with 30,000 hypotheses taking about 65 seconds on a plain Macintosh II. Figure 3 shows the effect of increasing the number of hypotheses searched on the error bound for the 16 SAM cases. Most converge satisfactorily according to the error bound by 30,000 hypotheses, but four do not.\nFigure 3 illustrates how the uncertainty about the computed probabilities decreases as the search of the hypothesis tree is extended, that is as the cumulative probability of all the hypotheses examined approaches one. Thus TopN is an \"any time\" algorithm: If it is stopped at any point after initialization, it will give bounds on the posterior probabilities; and the longer it runs, the narrower these bounds will be. Given an estimate of the convergence rate, a meta-reasoner could select the run-time to be allocated according to the urgency of the diagnosis, the importance of precision, and the cost of computing.\nCONCLUSIONS\nThe QMR-BN belief network confronts us with the general intractability of exact algorithms for diagnostic inference. Search-based algorithms such as TopN appear a promising approximate approach for such networks. They may be seen as smarter than forward sampling techniques in that they search specifically to find the most probable instantiations. They rely on exact methods for bounding the error in the resulting probabilities instead of the statistical error estimation methods available for some sampling techniques. We have presented a variety of results that bound the relative probabilities of partial hypotheses in BN20 and BN2NPS networks. These results illustrate the value of applying methods of qualitative probabilistic analysis, based on knowledge of the signs of influences and synergies. For the QMR-BN project, and no doubt others, there remains a need to develop more general results, for example for networks with prior dependences among diseases. The generality of search-based methods for bounding probabilities remains an open question. It seems unlikely that the kind of bounding results used here will be obtainable for completely general networks, but some further generality may be obtainable from knowledge of qualitative probabilistic properties of other classes of network.\nAcknowledgements\nThis work was supported in part by the National Science Foundation under grant IRI-8807061 to Carnegie Mellon, and in part by the Rockwell International Science Center.\nReferences\nCooper, G.F. (1984) \"NESTOR: A computer-based medical diagnostic aid that integrates causal and probabilistic knowledge\", ST AN-CS-84-1031 (PhD Dissertation), Dept of Computer Science, Stanford University. Cooper, G. F. (1990). \"The computational complexity of probabilistic inference using Bayesian belief networks.\" Artificial Intelligence. 42(2-3): 393-405. de Kleer, J & Williams, B (1986) Reasoning about multiple faults, Proc 5th National Conference on AI, AAAI, Philadelphia, ppl32-139. Fung, R, & Chang, K.-C (1989) \"Weighting and integrating evidence for stochastic simulation in Bayesian networks\", in M. Henrion (ed.), Proc of Fifth Workshop on Uncertainty in AI, Windsor, Ontario, 112-117. Heckerman, D. & Miller, R.A. (1976) \"Towards a better understanding of Internist- I knowledge bases\", in MEDINFO 86, R. Salamon, B. Blum, M. Jorgenson (eds.), IFIP-IMIA, Elsevier Science, North-Holland. pp2226. Beckerman, D.E. (1989) \"A tractable inference algorithm for diagnosing multiple diseases\", in M. Henrion (ed.), Proc of Fifth Workshop on Uncertainty in AI, Windsor, Ontario, 174-181. Henrion, M. (1987) \"Uncertainty in Artificial Intelligence: Is probability epistemologically and heuristically adequate?\", in Expert Judgment and Expert Systems, J.L. Mumpower (ed.) Springer-Verlag, Berlin, pplOS-130. Henri on, M. ( 1988) \"Propagation of uncertainty by probabilistic logic sampling in Bayes' networks\", in\n150 Henrion\nUncertainty in Artificial Intelligence, Vol 2, J. Lemmer & L.N. Kana! (Eds.), North-Holland, Amsterdam. pp149164.\nHenrion, M. (1990a) \"Towards efficient probabilistic diagnosis in multiply connected belief networks\", in Influence Diagrams, Belief Nets, and Decision Analysis, R.M. Oliver & J.Q. Smith (eds.), Wiley, London.\nHenrion, M. (1990b). An introduction to algorithms for inference in belief nets. In M.H.&.R. Shachter (Eds.), Uncertainty in Artificial Intelligence 5 (pp. 129-138). Amsterdam: Elsevier, North Holland.\nHenrion, M. (1990c) Towards efficient probabilistic diagnosis with a very large knowledge base. In Proceedings of International Workshop on Principles of Diagnosis, Menlo Park, Ca.\nHorvitz, E.J., Breese, J.S., & Henrion, M. (1988), \"Decision theory in expert systems and artificial intelligence\", Int. J. of Approximate Reasoning. 2, pp247-302.\nMiddleton, B.F., M. Shwe, D.E. Heckerman, M. Henrion, E.J. Horvitz, H. Lehmann & G.F. Cooper (1991). \"Probabilistic diagnosis using a reformulation of the Intemist-1/QMR Knowledge Base: II. Evaluation of Diagnostic Performance.\" Tech Report. Knowledge Systems Laboratory, Stanford University, Ca.\nMiller, R.A., Pople, E.P., & Myers, J.D. (1982) \"Internist-1, an experimental computer-based diagnostic consultant for general internal medicine\", New England J. of Medicine, No 307, Aug 19, pp486-476.\nMiller, R.A., McNeil, M.A., Challinor, S.M., Masarie, F.E., & Myers, J.D. (1986) \"The 1nternist1/Quick Medical Reference Project -- Status Report\", The Western J. of Medicine, No 145, 6, December, pp816-822.\nPearl, J. (1986) \"Fusion, propagation, and structuring in belief networks\", Artificial Intelligence, 29, pp241-88.\nPeng, Y. & Reggia, J.A. (J987a) \"A probabilistic Causal Model for diagnostic problem solving - Part I: Integrating symbolic causal inference with numeric probabilistic inference\", IEEE Trans. on Systems, Man, and Cybernetics, Vol SMC-17, No 2, Mar/Apr, pp146-62.\nPeng, Y. & Reggia, J.A. (1987b) \"A probabilistic Causal Model for diagnostic problem solving - Part 2: Diagnostic strategy\", IEEE Trans. on Systems, Man, and Cybernetics: Special issue for diagnosis, Vol SMC-17, No 3, May, pp395-406.\nPeng, Y. & Reggia, J.A. (1989) \"A comfort measure for diagnostic problem-solving\", Information Sciences, V 47, pp149-184.\nShimony, S.E. and Charniak, E. (1990) \"A new algorithm for finding MAP assignments to belief networks\", in Proc of Sixth Conference on Uncertainty in AI, Cambridge, Ma., p98-103. Shwe, M. & G. Cooper (1990). An empirical analysis of likelihood weighting simulation on a large, multiply-\nconnected belief network. In Proceedings of Sixth Conference on Uncertainty in AI , (pp. 498-508). Cambridge MA: Association for Uncertainty in AI.\nShwe, M., B.F. Middleton, D.E. Heckerman, M. Henrion, E.J. Horvitz, H. Lehmann & G.F. Cooper (1990). \"Probabilistic diagnosis using a reformulation of the Internist-1/QMR Knowledge Base: I. The probabilistic model and inference algorithms\" Tech Report. Knowledge Systems Laboratory, Stanford University, Ca.\nWellman, M.P. and M. Henrion (1991) \"Qualitative Intercausal relations, or Explaining 'Explaining Away\"', KR-91: Principles of Knowledge Representation and Reasoning: Proceedings of the Second International Coriference, Morgan Kaufman, Menlo Park, Calif."}], "references": [{"title": "NESTOR: A computer-based medical diagnostic aid that integrates causal and probabilistic knowledge", "author": ["G.F. Cooper"], "venue": "ST AN-CS-84-1031 (PhD Dissertation),", "citeRegEx": "Cooper,? \\Q1984\\E", "shortCiteRegEx": "Cooper", "year": 1984}, {"title": "The computational complexity of probabilistic inference using Bayesian belief networks.", "author": ["G.F. Cooper"], "venue": "Artificial Intelligence", "citeRegEx": "Cooper,? \\Q1990\\E", "shortCiteRegEx": "Cooper", "year": 1990}, {"title": "Reasoning about multiple faults", "author": ["J de Kleer", "B Williams"], "venue": "Proc 5th National Conference on AI, AAAI, Philadelphia,", "citeRegEx": "Kleer and Williams,? \\Q1986\\E", "shortCiteRegEx": "Kleer and Williams", "year": 1986}, {"title": "Weighting and integrating evidence for stochastic simulation in Bayesian networks", "author": ["R Fung", "Chang", "K.-C"], "venue": "Proc of Fifth Workshop on Uncertainty in AI, Windsor,", "citeRegEx": "Fung et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Fung et al\\.", "year": 1989}, {"title": "Towards a better understanding of Internist- I knowledge bases", "author": ["D. Heckerman", "R.A. Miller"], "venue": "in MEDINFO 86,", "citeRegEx": "Heckerman and Miller,? \\Q1976\\E", "shortCiteRegEx": "Heckerman and Miller", "year": 1976}, {"title": "A tractable inference algorithm for diagnosing multiple diseases", "author": ["D.E. Beckerman"], "venue": "Proc of Fifth Workshop on Uncertainty in AI, Windsor,", "citeRegEx": "Beckerman,? \\Q1989\\E", "shortCiteRegEx": "Beckerman", "year": 1989}, {"title": "Uncertainty in Artificial Intelligence: Is probability epistemologically and heuristically adequate?\", in Expert Judgment and Expert Systems, J.L. Mumpower (ed.) Springer-Verlag, Berlin, pplOS-130", "author": ["M. Henrion"], "venue": null, "citeRegEx": "Henrion,? \\Q1987\\E", "shortCiteRegEx": "Henrion", "year": 1987}, {"title": "Propagation of uncertainty by probabilistic logic sampling in Bayes' networks", "author": ["M. Henri on"], "venue": null, "citeRegEx": "on,? \\Q1988\\E", "shortCiteRegEx": "on", "year": 1988}, {"title": "1990a) \"Towards efficient probabilistic diagnosis in multiply connected belief networks\", in Influence Diagrams, Belief Nets, and Decision Analysis", "author": ["M. Henrion"], "venue": null, "citeRegEx": "Henrion,? \\Q1990\\E", "shortCiteRegEx": "Henrion", "year": 1990}, {"title": "An introduction to algorithms for inference in belief nets. In M.H.&.R. Shachter (Eds.), Uncertainty in Artificial Intelligence 5 (pp. 129-138)", "author": ["M. Henrion"], "venue": null, "citeRegEx": "Henrion,? \\Q1990\\E", "shortCiteRegEx": "Henrion", "year": 1990}, {"title": "Towards efficient probabilistic diagnosis with a very large knowledge base", "author": ["M. Henrion"], "venue": "In Proceedings of International Workshop on Principles of Diagnosis,", "citeRegEx": "Henrion,? \\Q1990\\E", "shortCiteRegEx": "Henrion", "year": 1990}, {"title": "Decision theory in expert systems and artificial intelligence", "author": ["E.J. Horvitz", "J.S. Breese", "M. Henrion"], "venue": "Int. J. of Approximate Reasoning", "citeRegEx": "Horvitz et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Horvitz et al\\.", "year": 1988}, {"title": "Probabilistic diagnosis using a reformulation of the Intemist-1/QMR Knowledge Base: II. Evaluation of Diagnostic Performance.", "author": ["B.F. Middleton", "M. Shwe", "D.E. Heckerman", "M. Henrion", "E.J. Horvitz", "H. Lehmann", "G.F. Cooper"], "venue": "Tech Report. Knowledge", "citeRegEx": "Middleton et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Middleton et al\\.", "year": 1991}, {"title": "Internist-1, an experimental computer-based diagnostic consultant for general internal medicine", "author": ["R.A. Miller", "E.P. Pople", "J.D. Myers"], "venue": "New England J. of Medicine, No 307,", "citeRegEx": "Miller et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1982}, {"title": "The 1nternist1/Quick Medical Reference Project -- Status Report", "author": ["R.A. Miller", "M.A. McNeil", "S.M. Challinor", "F.E. Masarie", "J.D. Myers"], "venue": "The Western J. of Medicine,", "citeRegEx": "Miller et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1986}, {"title": "Fusion, propagation, and structuring in belief networks", "author": ["J. Pearl"], "venue": "Artificial Intelligence,", "citeRegEx": "Pearl,? \\Q1986\\E", "shortCiteRegEx": "Pearl", "year": 1986}, {"title": "1987b) \"A probabilistic Causal Model for diagnostic problem solving - Part 2: Diagnostic strategy", "author": ["J.A. Reggia"], "venue": "IEEE Trans. on Systems, Man, and Cybernetics, Vol SMC-17,", "citeRegEx": "Y. and Reggia,? \\Q1987\\E", "shortCiteRegEx": "Y. and Reggia", "year": 1987}, {"title": "Cybernetics: Special issue for diagnosis, Vol SMC-17, No 3, May", "author": ["J.A. Reggia"], "venue": null, "citeRegEx": "Y. and Reggia,? \\Q1989\\E", "shortCiteRegEx": "Y. and Reggia", "year": 1989}, {"title": "A new algorithm for finding MAP assignments to belief networks", "author": ["S.E. Shimony", "E. Charniak"], "venue": "Proc of Sixth Conference on Uncertainty in AI,", "citeRegEx": "Shimony and Charniak,? \\Q1990\\E", "shortCiteRegEx": "Shimony and Charniak", "year": 1990}, {"title": "An empirical analysis of likelihood weighting simulation on a large, multiply", "author": ["M. Shwe", "G. Cooper"], "venue": null, "citeRegEx": "Shwe and Cooper,? \\Q1990\\E", "shortCiteRegEx": "Shwe and Cooper", "year": 1990}, {"title": "Probabilistic diagnosis using a reformulation of the Internist-1/QMR Knowledge Base: I. The probabilistic model and inference algorithms", "author": ["M. Shwe", "B.F. Middleton", "D.E. Heckerman", "M. Henrion", "E.J. Horvitz", "H. Lehmann", "G.F. Cooper"], "venue": "Tech Report. Knowledge", "citeRegEx": "Shwe et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Shwe et al\\.", "year": 1990}, {"title": "Qualitative Intercausal relations, or Explaining 'Explaining Away\"', KR-91: Principles of Knowledge Representation and Reasoning: Proceedings of the Second International Coriference", "author": ["M.P. Wellman", "M. Henrion"], "venue": null, "citeRegEx": "Wellman and Henrion,? \\Q1991\\E", "shortCiteRegEx": "Wellman and Henrion", "year": 1991}], "referenceMentions": [{"referenceID": 0, "context": "Cooper (I 984) employed this approach in Nestor, to obtain the most probable hypotheses. Peng and Reggia (1987a, I987b) and Henrion (1990a) developed more powerful admissability heuristics to prune the search tree, allowing more efficient search of BN20 networks, that is bipartite networks consisting of independent diseases, conditionally independent findings, and noisy ORs, as descnoed in Section 3.", "startOffset": 0, "endOffset": 140}, {"referenceID": 0, "context": "Cooper (I 984) employed this approach in Nestor, to obtain the most probable hypotheses. Peng and Reggia (1987a, I987b) and Henrion (1990a) developed more powerful admissability heuristics to prune the search tree, allowing more efficient search of BN20 networks, that is bipartite networks consisting of independent diseases, conditionally independent findings, and noisy ORs, as descnoed in Section 3. These methods are guaranteed to find the most probable composite hypotheses, and their relative probabilities (ratio of posterior probabilities of hypotheses). Peng and Reggia (1989) and Henrion (1990a) also describe methods to bound the absolute probabilities of the composite hypotheses.", "startOffset": 0, "endOffset": 587}, {"referenceID": 0, "context": "Cooper (I 984) employed this approach in Nestor, to obtain the most probable hypotheses. Peng and Reggia (1987a, I987b) and Henrion (1990a) developed more powerful admissability heuristics to prune the search tree, allowing more efficient search of BN20 networks, that is bipartite networks consisting of independent diseases, conditionally independent findings, and noisy ORs, as descnoed in Section 3. These methods are guaranteed to find the most probable composite hypotheses, and their relative probabilities (ratio of posterior probabilities of hypotheses). Peng and Reggia (1989) and Henrion (1990a) also describe methods to bound the absolute probabilities of the composite hypotheses.", "startOffset": 0, "endOffset": 607}, {"referenceID": 7, "context": "Shimony and Charniak (1990) describe a search-based method that finds the MAP (Maximum A-posteriori Probability) assignments to general belief networks.", "startOffset": 4, "endOffset": 28}, {"referenceID": 6, "context": "This forms a basis for generalized bounding theorems, including a new lower bound, that provides a significant improvement on TopN as presented in Henrion (1990b). Qualitative probabilistic analysis, using signs of influence and synergies, provides a clearer and more general basis for obtaining these.", "startOffset": 147, "endOffset": 163}, {"referenceID": 6, "context": "See Henrion (1990a), Shwe et a/, (1991) and Middleton et a/, (1991) for more details.", "startOffset": 4, "endOffset": 20}, {"referenceID": 6, "context": "See Henrion (1990a), Shwe et a/, (1991) and Middleton et a/, (1991) for more details.", "startOffset": 4, "endOffset": 40}, {"referenceID": 6, "context": "See Henrion (1990a), Shwe et a/, (1991) and Middleton et a/, (1991) for more details.", "startOffset": 4, "endOffset": 68}, {"referenceID": 7, "context": "The second stage is to assign probabilities to this framework, either derived from the QMR numbers, or elsewhere. Heckerman & Miller (1986) have demonstrated a fairly reliable monotonic correspondence between the frequency numbers and P(ffd), the link probabilities of a finding f given only disease d.", "startOffset": 7, "endOffset": 140}, {"referenceID": 7, "context": "These include 16 cases abstracted from the Scientific American Medicine (SAM) Continuing Medical Education Service. More details of the coding process are given in Shwe et a/ (1991).", "startOffset": 79, "endOffset": 182}], "year": 2011, "abstractText": "Since exact probabilistic inference is intractable in general for large multiply connected belief nets, approximate methods are required. A promising approach is to use heuristic search among hypotheses (instantiations of the network) to find the most probable ones, as in the TopN algorithm. Search is based on the relative probabilities of hypotheses which are efficient to compute. Given upper and lower bounds on the relative probability of partial hypotheses, it is possible to obtain bounds on the absolute probabilities of hypotheses. Best-first search aimed at reducing the maximum error progressively narrows the bounds as more hypotheses are examined. Here, qualitative probabilistic analysis is employed to obtain bounds on the relative probability of partial hypotheses for the BN20 class of networks networks and a generalization replacing the noisy OR assumption by negative synergy. The approach is illustrated by application to a very large belief network, QMR-BN, which is a reformulation of the Internist-! system for diagnosis in internal medicine.", "creator": "pdftk 1.41 - www.pdftk.com"}}}