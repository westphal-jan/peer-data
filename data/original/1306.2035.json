{"id": "1306.2035", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2013", "title": "Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation", "abstract": "While several papers have investigated computationally and statistically efficient methods for learning Gaussian mixtures, precise minimax bounds for their statistical performance as well as fundamental limits in high-dimensional settings are not well-understood. In this paper, we provide precise information theoretic bounds on the clustering accuracy and sample complexity of learning a mixture of two isotropic Gaussians in high dimensions under small mean separation. If there is a sparse subset of relevant dimensions that determine the mean separation, then the sample complexity only depends on the number of relevant dimensions and mean separation, and can be achieved by a simple computationally efficient procedure. Our results provide the first step of a theoretical basis for recent methods that combine feature selection and clustering.", "histories": [["v1", "Sun, 9 Jun 2013 16:28:56 GMT  (26kb)", "http://arxiv.org/abs/1306.2035v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG math.ST stat.TH", "authors": ["martin azizyan", "aarti singh", "larry a wasserman"], "accepted": true, "id": "1306.2035"}, "pdf": {"name": "1306.2035.pdf", "metadata": {"source": "CRF", "title": "Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation\u2217", "authors": ["Martin Azizyan", "Aarti Singh", "Larry Wasserman"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n30 6.\n20 35\nv1 [\nst at\n.M L\n] 9\nJ un\n2 01\n1 Introduction Gaussian mixture models provide a simple framework for several machine learning problems including clustering, density estimation and classification. Mixtures are especially appealing in high dimensional problems. Perhaps the most common use of Gaussian mixtures is for clustering. Of course, the statistical (and computational) behavior of these methods can degrade in high dimensions. Inspired by the success of variable selection methods in regression, several authors have considered variable selection for clustering. However, there appears to be no theoretical results justifying the advantage of variable selection in high dimensional setting.\nTo see why some sort of variable selection might be useful, consider clustering n subjects using a vector of d genes for each subject. Typically d is much larger than n which suggests that statistical clustering methods will perform poorly. However, it may be the case that there are only a small number of relevant genes in which case we might expect better behavior by focusing on this small set of relevant genes.\nThe purpose of this paper is to provide precise bounds on clustering error with mixtures of Gaussians. We consider both the general case where all features are relevant, and the special case where only a subset of features are relevant. Mathematically, we model an irrelevant feature by requiring the mean of that feature to be the same across clusters, so that the feature does not serve to differentiate the groups. Throughout this paper, we use the probability of misclustering an observation, relative to the optimal clustering if we had known the true distribution, as our loss function. This is akin to using excess risk in classification.\nThis paper makes the following contributions:\n\u2022 We provide information theoretic bounds on the sample complexity of learning a mixture of two isotropic Gaussians in the small mean separation setting that precisely captures the dimension dependence, and matches known sample complexity requirements for some existing algorithms. This also debunks the myth that there is a gap between statistical and computational complexity of learning mixture of two isotropic Gaussians for small mean separation. Our bounds require non-standard arguments since our loss function does not satisfy the triangle inequality.\n\u2217This work is supported in part by NSF grant IIS-1116458 and NSF CAREER award IIS-1252412.\n\u2022 We consider the high-dimensional setting where only a subset of relevant dimensions determine the mean separation between mixture components and demonstrate that learning is substantially easier as the sample complexity only depends on the sparse set of relevant dimensions. This provides some theoretical basis for feature selection approaches to clustering.\n\u2022 We show that a simple computationally feasible procedure nearly achieves the information theoretic sample complexity even in high-dimensional sparse mean separation settings.\nRelated Work. There is a long and continuing history of research on mixtures of Gaussians. A complete review is not feasible but we mention some highlights of the work most related to ours.\nPerhaps the most popular method for estimating a mixture distribution is maximum likelihood. Unfortunately, maximizing the likelihood is NP-Hard. This has led to a stream of work on alternative methods for estimating mixtures. These new algorithms use pairwise distances, spectral methods or the method of moments.\nPairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001). These methods require the mean separation to increase with dimension. The first one requires the separation to be \u221a d while the latter two improve it to d1/4. To avoid this problem, Vempala and Wang (2004) introduced the idea of using spectral methods for estimating mixtures of spherical Gaussians which makes mean separation independent of dimension. The assumption that the components are spherical was removed in Brubaker and Vempala (2008). Their method only requires the components to be separated by a hyperplane and runs in polynomial time, but requires n = \u2126(d4 log d) samples. Other spectral methods include Kannan et al. (2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm.\nKalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting \u00b5 > 1, Chaudhuri et al. (2009) show that n = \u2126\u0303(d/\u00b52) samples are needed. They also provide an information theoretic bound on the necessary sample complexity of any algorithm which matches the sample complexity of their method (up to log factors) in d and \u00b5. When the mean separation is small \u00b5 < 1, they show that n = \u2126\u0303(d/\u00b54) samples are sufficient for accurate estimation. Our results for the small mean separation setting provide a matching necessary condition.\nMost of these papers are concerned with computational efficiency and do not give precise, statistical minimax upper and lower bounds. None of them deal with the case we are interested in, namely, a high dimensional mixture with sparse mean separation.\nWe should also point out that the results in different papers are not necessarily comparable since different authors use different loss functions. In this paper we use the probability of misclassifying a future observation, relative to how the correct distribution clusters the observation, as our loss function. This should not be confused with the probability of attributing a new observation to the wrong component of the mixture. The latter loss does not to tend to zero as the sample size increases. Our loss is similar to the excess risk used in classification where we compare the misclassification rate of a classifier to the misclassification rate of the Bayes optimal classifier.\nFinally, we remind the reader that our motivation for studying sparsely separated mixtures is that this provides a model for variable selection in clustering problems. There are some relevant recent papers on this problem in the high-dimensional setting. Pan and Shen (2007) use penalized mixture models to do variable selection and clustering simultaneously. Witten and Tibshirani (2010) develop a penalized version of k-means clustering. Related methods include Raftery and Dean (2006); Sun et al. (2012) and Guo et al. (2010). The applied bioinformatics literature also contains a huge number of heuristic methods for this problem. None of these papers provide minimax bounds for the clustering error or provide theoretical evidence of the benefit of using variable selection in unsupervised problems such as clustering.\n2 Problem Setup\nIn this paper, we consider the simple setting of learning a mixture of two isotropic Gaussians with equal mixing weights, given n data points X1, . . . , Xn \u2208 Rd drawn i.i.d. from a d-dimensional mixture density function\np\u03b8(x) = 1\n2 f(x;\u00b51, \u03c3\n2I) + 1\n2 f(x;\u00b52, \u03c3\n2I),\nwhere f(\u00b7;\u00b5,\u03a3) is the density of N (\u00b5,\u03a3), \u03c3 > 0 is a fixed constant, and \u03b8 := (\u00b51, \u00b52) \u2208 \u0398. We consider two classes \u0398 of parameters:\n\u0398\u03bb = {(\u00b51, \u00b52) : \u2016\u00b51 \u2212 \u00b52\u2016 \u2265 \u03bb} \u0398\u03bb,s = {(\u00b51, \u00b52) : \u2016\u00b51 \u2212 \u00b52\u2016 \u2265 \u03bb, \u2016\u00b51 \u2212 \u00b52\u20160 \u2264 s} \u2286 \u0398\u03bb.\nThe first class defines mixtures where the components have a mean separation of at least \u03bb > 0. The second class defines mixtures with mean separation \u03bb > 0 along a sparse set of s \u2208 {1, . . . , d} dimensions. Also, let P\u03b8 denote the probability measure corresponding to p\u03b8. Throughout the paper, we will use \u03c6 and \u03a6 to denote the standard normal density and distribution functions.\nFor a mixture with parameter \u03b8, the Bayes optimal classification, that is, assignment of a point x \u2208 Rd to the correct mixture component, is given by the function\nF\u03b8(x) = argmax i\u2208{1,2}\nf(x;\u00b5i, \u03c3 2I).\nGiven any other candidate assignment function F : Rd \u2192 {1, 2}, we define the loss incurred by F as\nL\u03b8(F ) = min \u03c0\nP\u03b8({x : F\u03b8(x) 6= \u03c0(F (x))})\nwhere the minimum is over all permutations \u03c0 : {1, 2} \u2192 {1, 2}. This is the probability of misclustering relative to an oracle that uses the true distribution to do optimal clustering.\nWe denote by F\u0302n any assignment function learned from the data X1, . . . , Xn, also referred to as estimator. The goal of this paper is to quantify how the minimax expected loss (worst case expected loss for the best estimator)\nRn \u2261 inf F\u0302n sup \u03b8\u2208\u0398 E\u03b8L\u03b8(F\u0302n)\nscales with number of samples n, the dimension of the feature space d, the number of relevant dimensions s, and the signal-to-noise ratio defined as the ratio of mean separation to standard deviation \u03bb/\u03c3. We will also demonstrate a specific estimator that achieves the minimax scaling.\nFor the purposes of this paper, we say that feature j is irrelevant if \u00b51(j) = \u00b52(j). Otherwise we say that feature j is relevant.\n3 Minimax Bounds\n3.1 Small mean separation setting without sparsity\nWe begin without assuming any sparsity, that is, all features are relevant. In this case, comparing the projections of the data to the projection of the sample mean onto the first principal component suffices to achieve both minimax optimal sample complexity and clustering loss.\nTheorem 1 (Upper bound). Define\nF\u0302n(x) = { 1 if xT v1(\u03a3\u0302n) \u2265 \u00b5\u0302Tnv1(\u03a3\u0302n) 2 otherwise.\nwhere \u00b5\u0302n = n\u22121 \u2211n i=1 Xi is the sample mean, \u03a3\u0302n = n \u22121 \u2211n\ni=1(Xi \u2212 \u00b5\u0302n)(Xi \u2212 \u00b5\u0302n)T is the sample covariance and v1(\u03a3\u0302n) denotes the eigenvector corresponding to the largest eigenvalue of \u03a3\u0302n. If n \u2265 max(68, 4d), then\nsup \u03b8\u2208\u0398\u03bb\nE\u03b8L\u03b8(F\u0302 ) \u2264 600max ( 4\u03c32\n\u03bb2 , 1\n)\u221a d log(nd)\nn .\nFurthermore, if \u03bb\u03c3 \u2265 2max(80, 14 \u221a 5d), then\nsup \u03b8\u2208\u0398\u03bb E\u03b8L\u03b8(F\u0302 ) \u2264 17 exp ( \u2212 n 32 ) + 9 exp ( \u2212 \u03bb 2 80\u03c32 ) .\nTheorem 2 (Lower bound). Assume that d \u2265 9 and \u03bb\u03c3 \u2264 0.2. Then\ninf F\u0302n sup \u03b8\u2208\u0398\u03bb\nE\u03b8L\u03b8(F\u0302n) \u2265 1\n500 min\n{\u221a log 2\n3\n\u03c32 \u03bb2 \u221a d\u2212 1 n , 1 4 } .\nWe believe that some of the constants (including lower bound on d and exact upper bound on \u03bb/\u03c3) can be tightened, but the results demonstrate matching scaling behavior of clustering error with d, n and \u03bb/\u03c3. Thus, we see (ignoring constants and log terms) that\nRn \u2248 \u03c32\n\u03bb2\n\u221a d\nn , or equivalently n \u2248 d \u03bb4/\u03c34 for a constant target value of Rn.\nThe result is quite intuitive: the dependence on dimension d is as expected. Also we see that the rate depends in a precise way on the signal-to-noise ratio \u03bb/\u03c3. In particular, the results imply that we need d \u2264 n.\nIn modern high-dimensional datasets, we often have d > n i.e. large number of features and not enough samples. However, inference is usually tractable since not all features are relevant to the learning task at hand. This sparsity of relevant feature set has been successfully exploited in supervised learning problems such as regression and classification. We show next that the same is true for clustering under the Gaussian mixture model.\n3.2 Sparse and small mean separation setting\nNow we consider the case where there are s < d relevant features. Let S denote the set of relevant features. We begin by constructing an estimator S\u0302n of S as follows. Define\n\u03c4\u0302n = 1 + \u03b1\n1\u2212 \u03b1 mini\u2208{1,...,d} \u03a3\u0302n(i, i),\nwhere\n\u03b1 =\n\u221a 6 log(nd)\nn +\n2 log(nd)\nn .\nNow let S\u0302n = {i \u2208 {1, . . . , d} : \u03a3\u0302n(i, i) > \u03c4\u0302n}.\nNow we use the same method as before, but using only the features in S\u0302n identified as relevant.\nTheorem 3 (Upper bound). Define\nF\u0302n(x) =\n{ 1 if xT\nS\u0302n v1(\u03a3\u0302S\u0302n) \u2265 \u00b5\u0302 T S\u0302n v1(\u03a3\u0302S\u0302n)\n2 otherwise\nwhere S\u0302n is the estimated set of relevant dimensions, xS\u0302n are the coordinates of x restricted to the dimensions in S\u0302n, and \u00b5\u0302S\u0302n and \u03a3\u0302S\u0302n are the sample mean and covariance of the data restricted to the dimensions in S\u0302n. If n \u2265 max(68, 4s), d \u2265 2 and \u03b1 \u2264 14 , then\nsup \u03b8\u2208\u0398\u03bb,s\nE\u03b8L\u03b8(F\u0302 ) \u2264 603max ( 16\u03c32\n\u03bb2 , 1\n)\u221a s log(ns)\nn + 220\n\u03c3 \u221a s\n\u03bb\n( log(nd)\nn\n) 1 4\n.\nNext we find the lower bound.\nTheorem 4 (Lower bound). Assume that \u03bb\u03c3 \u2264 0.2, d \u2265 17, and that 5 \u2264 s \u2264 d\u221214 + 1. Then\ninf F\u0302n sup \u03b8\u2208\u0398\u03bb,s\nE\u03b8L\u03b8(F\u0302n) \u2265 1\n600 min\n{\u221a 8\n45\n\u03c32 \u03bb2 \u221a s\u2212 1 n log ( d\u2212 1 s\u2212 1 ) , 1 2 } .\nWe remark again that the constants in our bounds can be tightened, but the results suggest that\n\u03c3\n\u03bb\n( s2 log d\nn\n)1/4 \u227b Rn \u227b \u03c32\n\u03bb2\n\u221a s log d\nn ,\nor n = \u2126\n( s2 log d\n\u03bb4/\u03c34\n) for a constant target value of Rn.\nIn this case, we have a gap between the upper and lower bounds for the clustering loss. Also, the sample complexity can possibly be improved to scale as s (instead of s2) using a different method. However, notice that the dimension only enters logarithmically. If the number of relevant dimensions is small then we can expect good rates. This provides some justification for feature selection. We conjecture that the lower bound is tight and that the gap could be closed by using a sparse principal component method as in Vu and Lei (2012) to find the relevant features. However, that method is combinatorial and so far there is no known computationally efficient method for implementing it with similar guarantees.\nWe note that the upper bound is achieved by a two-stage method that first finds the relevant dimensions and then estimates the clusters. This is in contrast to the methods described in the introduction which do clustering and variable selection simultaneously. This raises an interesting question: is it always possible to achieve the minimax rate with a two-stage procedure or are there cases where a simultaneous method outperforms a two-stage procedure? Indeed, it is possible that in the case of general covariance matrices (non-spherical) two-stage methods might fail. We hope to address this question in future work.\n4 Proofs of the Lower Bounds\nThe lower bounds for estimation problems rely on a standard reduction from expected error to hypothesis testing that assumes the loss function is a semi-distance, which the clustering loss isn\u2019t. However, a local triangle inequality-type bound can be shown (Proposition 2). This weaker condition can then be used to lower-bound the expected loss, as stated in Proposition 1 (which follows easily from Fano\u2019s inequality).\nThe proof techniques of the sparse and non-sparse lower bounds are almost identical. The main difference is that in the non-sparse case, we use the Varshamov\u2013Gilbert bound (Lemma 1) to construct a set of sufficiently dissimilar hypotheses, whereas in the sparse case we use an analogous result for sparse hypercubes (Lemma 2). See the appendix for complete proofs of all results.\nLemma 1 (Varshamov\u2013Gilbert bound). Let \u2126 = {0, 1}m for m \u2265 8. There exists a subset {\u03c90, ..., \u03c9M} \u2286 \u2126 such that \u03c90 = (0, ..., 0), \u03c1(\u03c9i, \u03c9j) \u2265 m8 for all 0 \u2264 i < j \u2264 M , and M \u2265 2m/8, where \u03c1 denotes the Hamming distance between two vectors (Tsybakov (2009)).\nLemma 2. Let \u2126 = {\u03c9 \u2208 {0, 1}m : \u2016\u03c9\u20160 = s} for integers m > s \u2265 1 such that s \u2264 m/4. There exist \u03c90, ..., \u03c9M \u2208 \u2126 such that \u03c1(\u03c9i, \u03c9j) > s/2 for all 0 \u2264 i < j \u2264 M , and log(M + 1) \u2265 s5 log ( m s ) (Massart (2007), Lemma 4.10).\nProposition 1. Let \u03b80, ..., \u03b8M \u2208 \u0398\u03bb (or \u0398\u03bb,s), M \u2265 2, 0 < \u03b1 < 1/8, and \u03b3 > 0. If for all 1 \u2264 i \u2264 M , KL(P\u03b8i , P\u03b80) \u2264 \u03b1 logMn , and if L\u03b8i(F\u0302 ) < \u03b3 implies L\u03b8j(F\u0302 ) \u2265 \u03b3 for all 0 \u2264 i 6= j \u2264 M and clusterings F\u0302 , then inf F\u0302n maxi\u2208[0..M ] E\u03b8iL\u03b8i(F\u0302n) \u2265 0.07\u03b3. Proposition 2. For any \u03b8, \u03b8\u2032 \u2208 \u0398\u03bb, and any clustering F\u0302 , let \u03c4 = L\u03b8(F\u0302 ) + \u221a KL(P\u03b8, P\u03b8\u2032)/2. If L\u03b8(F\u03b8\u2032) + \u03c4 \u2264 1/2, then L\u03b8(F\u03b8\u2032)\u2212 \u03c4 \u2264 L\u03b8\u2032(F\u0302 ) \u2264 L\u03b8(F\u03b8\u2032) + \u03c4. We will also need the following two results. Let \u03b8 = (\u00b50 \u2212 \u00b5/2, \u00b50 + \u00b5/2) and \u03b8\u2032 = (\u00b50 \u2212 \u00b5\u2032/2, \u00b50 + \u00b5\u2032/2) for \u00b50, \u00b5, \u00b5 \u2032 \u2208 Rd such that \u2016\u00b5\u2016 = \u2016\u00b5\u2032\u2016, and let cos\u03b2 = |\u00b5 T\u00b5\u2032| \u2016\u00b5\u20162 . Proposition 3. Let g(x) = \u03c6(x)(\u03c6(x) \u2212 x\u03a6(\u2212x)). Then 2g (\n\u2016\u00b5\u2016 2\u03c3 ) sin\u03b2 cos\u03b2 \u2264 L\u03b8(F\u03b8\u2032) \u2264 tan \u03b2\u03c0 .\nProposition 4. Let \u03be = \u2016\u00b5\u20162\u03c3 . Then KL(P\u03b8, P\u03b8\u2032) \u2264 \u03be4(1\u2212 cos\u03b2).\nProof of Theorem 2. Let \u03be = \u03bb2\u03c3 , and define \u01eb = min {\u221a log 2 3 \u03c32 \u03bb 1\u221a n , \u03bb 4 \u221a d\u22121 } . Define \u03bb20 = \u03bb\n2 \u2212 (d \u2212 1)\u01eb2. Let \u2126 = {0, 1}d\u22121. For \u03c9 = (\u03c9(1), ..., \u03c9(d \u2212 1)) \u2208 \u2126, let \u00b5\u03c9 = \u03bb0ed + \u2211d\u22121 i=1 (2\u03c9(i) \u2212 1)\u01ebei (where {ei}di=1 is the\nstandard basis for Rd). Let \u03b8\u03c9 = ( \u2212\u00b5\u03c92 , \u00b5\u03c9 2 ) \u2208 \u0398\u03bb.\nBy Proposition 4, KL(P\u03b8\u03c9 , P\u03b8\u03bd ) \u2264 \u03be4(1 \u2212 cos\u03b2\u03c9,\u03bd) where cos\u03b2\u03c9,\u03bd = 1 \u2212 2\u03c1(\u03c9,\u03bd)\u01eb 2 \u03bb2 and \u03c1 is the Hamming\ndistance, so KL(P\u03b8\u03c9 , P\u03b8\u03bd ) \u2264 \u03be4 2(d\u22121)\u01eb 2 \u03bb2 . By Proposition 3, since cos\u03b2\u03c9,\u03bd \u2265 12 ,\nL\u03b8\u03c9(F\u03b8\u03bd ) \u2264 1\n\u03c0 tan\u03b2\u03c9,\u03bd \u2264\n4\n\u03c0 \u221a d\u2212 1\u01eb \u03bb , and\nL\u03b8\u03c9(F\u03b8\u03bd ) \u2265 2g(\u03be) sin\u03b2\u03c9,\u03bd cos\u03b2\u03c9,\u03bd \u2265 \u221a 2g(\u03be)\n\u221a \u03c1(\u03c9, \u03bd)\u01eb\n\u03bb\nwhere g(x) = \u03c6(x)(\u03c6(x) \u2212 x\u03a6(\u2212x)). By Lemma 1, there exist \u03c90, ..., \u03c9M \u2208 \u2126 such that M \u2265 2(d\u22121)/8 and \u03c1(\u03c9i, \u03c9j) \u2265 d\u221218 for all 0 \u2264 i < j \u2264 M . For simplicity of notation, let \u03b8i = \u03b8\u03c9i for all i \u2208 [0..M ]. Then, for i 6= j \u2208 [0..M ],\nKL(P\u03b8i , P\u03b8j ) \u2264 \u03be4 2(d\u2212 1)\u01eb2\n\u03bb2 , L\u03b8i(F\u03b8j ) \u2264\n4\n\u03c0 \u221a d\u2212 1\u01eb \u03bb and L\u03b8i(F\u03b8j ) \u2265 1 2 g(\u03be) \u221a d\u2212 1\u01eb \u03bb .\nDefine \u03b3 = 14 (g(\u03be)\u2212 2\u03be2) \u221a d\u22121\u01eb \u03bb . Then for any i 6= j \u2208 [0..M ], and any F\u0302 such that L\u03b8i(F\u0302 ) < \u03b3,\nL\u03b8i(F\u03b8j ) + L\u03b8i(F\u0302 ) +\n\u221a KL(P\u03b8i , P\u03b8j )\n2 <\n( 4\n\u03c0 +\n1 4 (g(\u03be)\u2212 2\u03be2) + \u03be2 ) \u221a d\u2212 1\u01eb \u03bb \u2264 1 2\nbecause, for \u03be \u2264 0.1, by definition of \u01eb, ( 4\n\u03c0 +\n1 4 (g(\u03be)\u2212 2\u03be2) + \u03be2 ) \u221a d\u2212 1\u01eb \u03bb \u2264 2 \u221a d\u2212 1\u01eb \u03bb \u2264 1 2 .\nSo, by Proposition 2, L\u03b8j (F\u0302 ) \u2265 \u03b3. Also, KL(P\u03b8i , P\u03b80) \u2264 (d \u2212 1)\u03be4 2\u01eb 2 \u03bb2 \u2264 logM 9n for all 1 \u2264 i \u2264 M , because, by definition of \u01eb, \u03be4 2\u01eb 2\n\u03bb2 \u2264 log 2 72n . So by Proposition 1 and the fact that \u03be \u2264 0.1,\ninf F\u0302n max i\u2208[0..M ]\nE\u03b8iL\u03b8i(F\u0302n) \u2265 0.07\u03b3 \u2265 1\n500 min\n{\u221a log 2\n3\n\u03c32 \u03bb2 \u221a d\u2212 1 n , 1 4 }\nand to complete the proof we use sup\u03b8\u2208\u0398\u03bb E\u03b8L\u03b8(F\u0302n) \u2265 maxi\u2208[0..M ] E\u03b8iL\u03b8i(F\u0302n) for any F\u0302n. Proof of Theorem 4. For simplicity, we state this construction for \u0398\u03bb,s+1, assuming 4 \u2264 s \u2264 d\u221214 . Let \u03be = \u03bb2\u03c3 ,\nand define \u01eb = min {\u221a 8 45 \u03c32 \u03bb \u221a 1 n log ( d\u22121 s ) , 12 \u03bb\u221a s } . Define \u03bb20 = \u03bb 2 \u2212 s\u01eb2. Let \u2126 = {\u03c9 \u2208 {0, 1}d\u22121 : \u2016\u03c9\u20160 = s}. For \u03c9 = (\u03c9(1), ..., \u03c9(d \u2212 1)) \u2208 \u2126, let \u00b5\u03c9 = \u03bb0ed + \u2211d\u22121\ni=1 \u03c9(i)\u01ebei (where {ei}di=1 is the standard basis for Rd). Let \u03b8\u03c9 = ( \u2212\u00b5\u03c92 , \u00b5\u03c9 2 ) \u2208 \u0398\u03bb,s. By Lemma 2, there exist \u03c90, ..., \u03c9M \u2208 \u2126 such that log(M + 1) \u2265 s5 log ( d\u22121 s ) and \u03c1(\u03c9i, \u03c9j) \u2265 s2 for all 0 \u2264 i < j \u2264 M . The remainder of the proof is analogous to that of Theorem 2 with \u03b3 = 14 (g(\u03be)\u2212 \u221a 2\u03be2) \u221a s\u01eb \u03bb .\n5 Proofs of the Upper Bounds\nPropositions 5 and 6 below bound the error in estimating the mean and principal direction, and can be obtained using standard concentration bounds and a variant of the Davis\u2013Kahan theorem. Proposition 7 relates these errors to the clustering loss. For the sparse case, Propositions 8 and 9 bound the added error induced by the support estimation procedure. See appendix for proof details.\nProposition 5. Let \u03b8 = (\u00b50 \u2212 \u00b5, \u00b50 + \u00b5) for some \u00b50, \u00b5 \u2208 Rd and X1, ..., Xn i.i.d.\u223c P\u03b8 . For any \u03b4 > 0, we have \u2016\u00b50 \u2212 \u00b5\u0302n\u2016 \u2265 \u03c3 \u221a 2max(d,8 log 1 \u03b4 ) n + \u2016\u00b5\u2016 \u221a 2 log 1 \u03b4 n with probability at least 1\u2212 3\u03b4.\nProposition 6. Let \u03b8 = (\u00b50 \u2212 \u00b5, \u00b50 + \u00b5) for some \u00b50, \u00b5 \u2208 Rd and X1, ..., Xn i.i.d.\u223c P\u03b8 with d > 1 and n \u2265 4d. Define cos\u03b2 = |v1(\u03c32I + \u00b5\u00b5T )T v1(\u03a3\u0302n)|. For any 0 < \u03b4 < d\u22121\u221ae , if max ( \u03c32 \u2016\u00b5\u20162 , \u03c3 \u2016\u00b5\u2016 )\u221a max(d,8 log 1 \u03b4 )\nn \u2264 1160 , then with probability at least 1\u2212 12\u03b4 \u2212 2 exp ( \u2212 n20 ) ,\nsin\u03b2 \u2264 14max ( \u03c32\n\u2016\u00b5\u20162 , \u03c3 \u2016\u00b5\u2016\n)\u221a d\n\u221a 10\nn log\nd \u03b4 max\n( 1, 10\nn log\nd\n\u03b4\n) .\nProposition 7. Let \u03b8 = (\u00b50 \u2212 \u00b5, \u00b50 + \u00b5), and for some x0, v \u2208 Rd with \u2016v\u2016 = 1, let F\u0302 (x) = 1 if xT v \u2265 xT0 v, and 2 otherwise. Define cos\u03b2 = |vT\u00b5|/\u2016\u00b5\u2016. If |(x0 \u2212 \u00b50)T v| \u2264 \u03c3\u01eb1 + \u2016\u00b5\u2016\u01eb2 for some \u01eb1 \u2265 0 and 0 \u2264 \u01eb2 \u2264 14 , and if sin\u03b2 \u2264 1\u221a\n5 , then\nL\u03b8(F\u0302 ) \u2264 exp { \u22121 2 max ( 0, \u2016\u00b5\u2016 2\u03c3 \u2212 2\u01eb1 )2}[ 2\u01eb1 + \u01eb2 \u2016\u00b5\u2016 \u03c3 + 2 sin\u03b2 ( 2 sin\u03b2 \u2016\u00b5\u2016 \u03c3 + 1 )] .\nProof. Let r = \u2223\u2223\u2223 (x0\u2212\u00b50)\nT v cos \u03b2\n\u2223\u2223\u2223. Since the clustering loss is invariant to rotation and translation,\nL\u03b8(F\u0302 ) \u2264 1\n2\n\u222b \u221e\n\u2212\u221e\n1 \u03c3 \u03c6 (x \u03c3 )[ \u03a6 (\u2016\u00b5\u2016+ |x| tan\u03b2 + r \u03c3 ) \u2212 \u03a6 (\u2016\u00b5\u2016 \u2212 |x| tan\u03b2 \u2212 r \u03c3 )] dx\n\u2264 \u222b \u221e\n\u2212\u221e \u03c6(x)\n[ \u03a6 (\u2016\u00b5\u2016 \u03c3 ) \u2212 \u03a6 (\u2016\u00b5\u2016 \u2212 r \u03c3 \u2212 |x| tan\u03b2 )] dx.\nSince tan\u03b2 \u2264 12 and \u01eb2 \u2264 14 , we have r \u2264 2\u03c3\u01eb1+2\u2016\u00b5\u2016\u01eb2, and\u03a6 ( \u2016\u00b5\u2016 \u03c3 ) \u2212\u03a6 ( \u2016\u00b5\u2016\u2212r \u03c3 ) \u2264 2 ( \u01eb1 + \u01eb2 \u2016\u00b5\u2016 \u03c3 ) \u03c6 ( max ( 0, \u2016\u00b5\u20162\u03c3 \u2212 2\u01eb1 )) .\nDefining A = \u2223\u2223\u2223 \u2016\u00b5\u2016\u2212r\u03c3 \u2223\u2223\u2223,\n\u222b \u221e\n\u2212\u221e \u03c6(x)\n[ \u03a6 (\u2016\u00b5\u2016 \u2212 r \u03c3 ) \u2212 \u03a6 (\u2016\u00b5\u2016 \u2212 r \u03c3 \u2212 |x| tan \u03b2 )] dx \u2264 2 \u222b \u221e\n0\n\u222b A\nA\u2212x tan \u03b2 \u03c6(x)\u03c6(y)dydx\n= 2\n\u222b \u221e\n\u2212A sin \u03b2\n\u222b A cos \u03b2+(u+A sin \u03b2) tan \u03b2\nA cos \u03b2\n\u03c6(u)\u03c6(v)dudv \u2264 2\u03c6 (A) tan\u03b2 (A sin\u03b2 + 1)\n\u2264 2\u03c6 ( max ( 0,\n\u2016\u00b5\u2016 2\u03c3\n\u2212 2\u01eb1 )) tan\u03b2 (( 2 \u2016\u00b5\u2016 \u03c3 + 2\u01eb1 ) sin\u03b2 + 1 )\nwhere we used u = x cos\u03b2\u2212 y sin\u03b2 and v = x sin\u03b2+ y cos\u03b2 in the second step. The bound now follows easily.\nProof of Theorem 1. Using Propositions 5 and 6 with \u03b4 = 1\u221a n , Proposition 7, and the fact that (C+x) exp(\u2212max(0, x\u2212 4)2/8) \u2264 (C + 6) exp(\u2212max(0, x\u2212 4)2/10) for all C, x > 0,\nE\u03b8L\u03b8(F\u0302 ) \u2264 600max ( 4\u03c32\n\u03bb2 , 1\n)\u221a d log(nd)\nn\n(it is easy to verify that the bounds are decreasing with \u2016\u00b5\u2016, so we use \u2016\u00b5\u2016 = \u03bb2 to bound the supremum). In the d = 1 case Proposition 6 need not be applied, since the principal directions agree trivially. The bound for \u03bb\u03c3 \u2265 2max(80, 14 \u221a 5d) can be shown similarly, using \u03b4 = exp ( \u2212 n32 ) .\nProposition 8. Let \u03b8 = (\u00b50 \u2212 \u00b5, \u00b50 + \u00b5) for some \u00b50, \u00b5 \u2208 Rd and X1, ..., Xn i.i.d.\u223c P\u03b8 . For any 0 < \u03b4 < 1\u221ae such that\u221a 6 log 1\n\u03b4 n \u2264 12 , with probability at least 1\u2212 6d\u03b4, for all i \u2208 [d],\n|\u03a3\u0302n(i, i)\u2212 (\u03c32 + \u00b5(i)2)| \u2264 \u03c32 \u221a\n6 log 1\u03b4 n + 2\u03c3|\u00b5(i)|\n\u221a 2 log 1\u03b4\nn + (\u03c3 + |\u00b5(i)|)2 2 log\n1 \u03b4\nn .\nProposition 9. Let \u03b8 = (\u00b50 \u2212 \u00b5, \u00b50 + \u00b5) for some \u00b50, \u00b5 \u2208 Rd and X1, ..., Xn i.i.d.\u223c P\u03b8 . Define\nS(\u03b8) = {i \u2208 [d] : \u00b5(i) 6= 0} and S\u0303(\u03b8) = {i \u2208 [d] : |\u00b5(i)| \u2265 4\u03c3\u221a\u03b1}.\nAssume that n \u2265 1, d \u2265 2, and \u03b1 \u2264 14 . Then S\u0303(\u03b8) \u2286 S\u0302n \u2286 S(\u03b8) with probability at least 1\u2212 6n .\nProof. By Proposition 8, with probability at least 1\u2212 6n ,\n|\u03a3\u0302n(i, i)\u2212 (\u03c32 + \u00b5(i)2)| \u2264 \u03c32 \u221a 6 log(nd)\nn + 2\u03c3|\u00b5(i)|\n\u221a 2 log(nd)\nn + (\u03c3 + |\u00b5(i)|)2 2 log(nd) n\nfor all i \u2208 [d]. Assume the above event holds. If S(\u03b8) = [d], then of course S\u0302n \u2286 S(\u03b8). Otherwise, for i /\u2208 S(\u03b8), we have (1 \u2212 \u03b1)\u03c32 \u2264 \u03a3\u0302n(i, i) \u2264 (1 + \u03b1)\u03c32, so it is clear that S\u0302n \u2286 S(\u03b8). The remainder of the proof is trivial if S\u0303(\u03b8) = \u2205 or S(\u03b8) = \u2205. Assume otherwise. For any i \u2208 S(\u03b8),\n\u03a3\u0302n(i, i) \u2265 (1\u2212 \u03b1)\u03c32 + ( 1\u2212 2 log(nd)\nn\n) \u00b5(i)2 \u2212 2\u03b1\u03c3|\u00b5(i)|.\nBy definition, |\u00b5(i)| \u2265 4\u03c3\u221a\u03b1 for all i \u2208 S\u0303(\u03b8), so (1+\u03b1) 2\n1\u2212\u03b1 \u03c3 2 \u2264 \u03a3\u0302n(i, i) and i \u2208 S\u0302n (we ignore strict equality above as\na measure 0 event), i.e. S\u0303(\u03b8) \u2286 S\u0302n, which concludes the proof.\nProof of Theorem 3. Define S(\u03b8) = {i \u2208 [d] : \u00b5(i) 6= 0} and S\u0303(\u03b8) = {i \u2208 [d] : |\u00b5(i)| \u2265 4\u03c3\u221a\u03b1}. Assume S\u0303(\u03b8) \u2286 S\u0302n \u2286 S(\u03b8) (by Proposition 9, this holds with probability at least 1 \u2212 6n ). If S\u0303(\u03b8) = \u2205, then we simply have E\u03b8L\u03b8(F\u0302n) \u2264 12 .\nAssume S\u0303(\u03b8) 6= \u2205. Let cos \u03b2\u0302 = |v1(\u03a3\u0302S\u0302n) T v1(\u03a3)|, cos \u03b2\u0303 = |v1(\u03a3S\u0302n) T v1(\u03a3)|, and cos\u03b2 = |v1(\u03a3\u0302S\u0302n) T v1(\u03a3S\u0302n)| where \u03a3 = \u03c32I + \u00b5\u00b5T , and for simplicity we define \u03a3\u0302S\u0302n and \u03a3S\u0302n to be the same as \u03a3\u0302n and \u03a3 in S\u0302n, respectively, and 0 elsewhere. Then sin \u03b2\u0302 \u2264 sin \u03b2\u0303 + sin\u03b2, and\nsin \u03b2\u0303 = \u2016\u00b5\u2212 \u00b5S\u0302(\u03b8)\u2016 \u2016\u00b5\u2016 \u2264 \u2016\u00b5\u2212 \u00b5S\u0303(\u03b8)\u2016 \u2016\u00b5\u2016 \u2264 4\u03c3\n\u221a \u03b1 \u221a |S(\u03b8)| \u2212 |S\u0303(\u03b8)| \u2016\u00b5\u2016 \u2264 8 \u03c3 \u221a s\u03b1 \u03bb .\nUsing the same argument as the proof of Theorem 1, as long as the above bound is smaller than 1 2 \u221a 5 ,\nE\u03b8L\u03b8(F\u0302 ) \u2264 600max (\n\u03c32 ( \u03bb 2 \u2212 4\u03c3 \u221a s\u03b1\n)2 , 1 )\u221a s log(ns)\nn + 104\n\u03c3 \u221a s\u03b1\n\u03bb +\n3 n .\nUsing the fact L\u03b8(F\u0302 ) \u2264 12 always, and that \u03b1 \u2264 14 implies log(nd) n \u2264 1, the bound follows.\n6 Conclusion\nWe have provided minimax lower and upper bounds for estimating high dimensional mixtures. The bounds show explicitly how the statistical difficulty of the problem depends on dimension d, sample size n, separation \u03bb and sparsity level s.\nFor clarity, we have focused on the special case where there are two spherical components and the mixture weights are equal. In future work, we plan to extend the results to general mixtures of k Gaussians.\nOne of our motivations for this work is the recent interest in variable selection methods to facilitate clustering in high dimensional problems. Existing methods such as Pan and Shen (2007); Witten and Tibshirani (2010); Raftery and Dean (2006); Sun et al. (2012) and Guo et al. (2010) provide promising numerical evidence that variable selection does improve high dimensional clustering. Our results provide some theoretical basis for this idea.\nHowever, there is a gap between the results in this paper and the methodology papers mentioned above. Indeed, as of now, there is no rigorous proof that the methods in those papers outperform a two stage approach where the first stage screens for relevant features and the second stage applies standard clustering methods on the features extracted from the first stage. We conjecture that there are conditions under which simultaneous feature selection and clustering outperforms the two stage approach. Settling this questions will require the aforementioned extension of our results to the general mixture case.\nReferences\nDimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions. In Learning Theory, pages 458\u2013469. Springer, 2005.\nSanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary gaussians. In Proceedings of the thirty-third annual ACM symposium on Theory of computing, pages 247\u2013257. ACM, 2001.\nMikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 103\u2013112. IEEE, 2010.\nS Charles Brubaker and Santosh S Vempala. Isotropic pca and affine-invariant clustering. In Building Bridges, pages 241\u2013281. Springer, 2008.\nKamalika Chaudhuri, Sanjoy Dasgupta, and Andrea Vattani. Learning mixtures of gaussians using the k-means algorithm. arXiv preprint arXiv:0912.0086, 2009.\nSanjoy Dasgupta. Learning mixtures of gaussians. In Foundations of Computer Science, 1999. 40th Annual Symposium on, pages 634\u2013644. IEEE, 1999.\nG.H. Golub and C.F. Van Loan. Matrix Computations. Johns Hopkins Studies in the Mathematical Sciences. Johns Hopkins University Press, 3rd edition, 1996. ISBN 9780801854149.\nJian Guo, Elizaveta Levina, George Michailidis, and Ji Zhu. Pairwise variable selection for high-dimensional modelbased clustering. Biometrics, 66(3):793\u2013804, 2010.\nDaniel Hsu and Sham M Kakade. Learning mixtures of spherical gaussians: moment methods and spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer Science, pages 11\u201320. ACM, 2013.\nAdam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Disentangling gaussians. Communications of the ACM, 55 (2):113\u2013120, 2012.\nRavindran Kannan, Hadi Salmasian, and Santosh Vempala. The spectral method for general mixture models. In Learning Theory, pages 444\u2013457. Springer, 2005.\nPascal Massart. Concentration inequalities and model selection. 2007.\nWei Pan and Xiaotong Shen. Penalized model-based clustering with application to variable selection. The Journal of Machine Learning Research, 8:1145\u20131164, 2007.\nAdrian E Raftery and Nema Dean. Variable selection for model-based clustering. Journal of the American Statistical Association, 101(473):168\u2013178, 2006.\nLeonard J. Schulman and Sanjoy Dasgupta. A two-round variant of em for gaussian mixtures. In Proc. 16th UAI (Conference on Uncertainty in Artificial Intelligence), pages 152\u2013159, 2000.\nWei Sun, Junhui Wang, and Yixin Fang. Regularized k-means clustering of high-dimensional data and its asymptotic consistency. Electronic Journal of Statistics, 6:148\u2013167, 2012.\nA.B. Tsybakov. Introduction to Nonparametric Estimation. Springer Series in Statistics. Springer, 2009.\nSantosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal of Computer and System Sciences, 68(4):841\u2013860, 2004.\nVincent Q Vu and Jing Lei. Minimax sparse principal subspace estimation in high dimensions. arXiv preprint arXiv:1211.0373, 2012.\nDaniela M Witten and Robert Tibshirani. A framework for feature selection in clustering. Journal of the American Statistical Association, 105(490), 2010.\n7 Notation\nFor \u03b8 = (\u00b51, \u00b52) \u2208 R2\u00d7d, define\np\u03b8(x) = 1\n2 f(x;\u00b51, \u03c3\n2I) + 1\n2 f(x;\u00b52, \u03c3\n2I),\nwhere f(\u00b7;\u00b5,\u03a3) is the density of N (\u00b5,\u03a3), \u03c3 > 0 is a fixed constant. Let P\u03b8 denote the probability measure corresponding to p\u03b8. We consider two classes \u0398 of parameters:\n\u0398\u03bb = {(\u00b51, \u00b52) : \u2016\u00b51 \u2212 \u00b52\u2016 \u2265 \u03bb}\n\u0398\u03bb,s = {(\u00b51, \u00b52) : \u2016\u00b51 \u2212 \u00b52\u2016 \u2265 \u03bb, \u2016\u00b51 \u2212 \u00b52\u20160 \u2264 s} \u2286 \u0398\u03bb. Throughout this document, \u03c6 and \u03a6 denote the standard normal density and distribution functions. For a mixture with parameter \u03b8, the Bayes optimal classification, that is, assignment of a point x \u2208 Rd to the\ncorrect mixture component, is given by the function\nF\u03b8(x) = argmax i\u2208{1,2}\nf(x;\u00b5i, \u03c3 2I).\nGiven any other candidate assignment function F : Rd \u2192 {1, 2}, we define the loss incurred by F as\nL\u03b8(F ) = min \u03c0\nP\u03b8({x : F\u03b8(x) 6= \u03c0(F (x))})\nwhere the minimum is over all permutations \u03c0 : {1, 2} \u2192 {1, 2}. For X1, . . . , Xn\ni.i.d.\u223c P\u03b8 , let \u00b5\u0302n and \u03a3\u0302n be the mean and covariance of the corresponding empirical distribution. Also, for a matrix B, vi(B) and \u03bbi(B) are the i\u2019th eigenvector and eigenvalue of B (assuming B is symmetric),\narranged so that \u03bbi(B) \u2265 \u03bbi+1(B), and \u2016B\u20162 is the spectral norm.\n8 Upper bounds\n8.1 Standard concentration bounds\n8.1.1 Concentration bounds for estimating the mean\nProposition 10. Let X \u223c \u03c72d. Then for any \u01eb > 0,\nP(X > (1 + \u01eb)d) \u2264 exp { \u2212d 2 (\u01eb\u2212 log(1 + \u01eb)) } .\nIf \u01eb < 1, then\nP(X < (1\u2212 \u01eb)d) \u2264 exp { d\n2 (\u01eb+ log(1 \u2212 \u01eb))\n} .\nProof. Since EetX = (1\u2212 2t)\u2212d2 for 0 < t < 12 ,\nP(X > (1 + \u01eb)d) = P(etX > et(1+\u01eb)d)\n\u2264 e\u2212t(1+\u01eb)d(1\u2212 2t)\u2212 d2\n= exp [ \u2212t(1 + \u01eb)d+ d\n2 log\n1\n1\u2212 2t\n] .\nTo minimize the right hand side, we differentiate the exponent with respect to t to obtain the equation\n\u2212(1 + \u01eb)d+ d 1\u2212 2t = 0\nwhich can be satisfied by setting t = 12 ( 1\u2212 11+\u01eb ) < 12 (it is easy to verify that this is a global minimum). Using this value for t, the first bound follows. Also, for t > 0 and \u01eb < 1,\nP(X < (1\u2212 \u01eb)d) = P(e\u2212tX > e\u2212t(1\u2212\u01eb)d) \u2264 et(1\u2212\u01eb)d(1 + 2t)\u2212d2\n= exp [ t(1\u2212 \u01eb)d\u2212 d\n2 log(1 + 2t)\n]\nand setting t = 12\n( 1 1\u2212\u01eb \u2212 1 ) ,\nP(X < (1 \u2212 \u01eb)d) \u2264 exp [ d\n2 (\u01eb+ log(1 \u2212 \u01eb))\n] .\nProposition 11. Let Z1, ..., Zn i.i.d.\u223c N (0, Id). Then for any \u01eb > 0,\nP (\u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nZi \u2225\u2225\u2225\u2225\u2225 \u2265 \u221a (1 + \u01eb)d n ) \u2264 exp { \u2212d 2 (\u01eb\u2212 log(1 + \u01eb)) } .\nProof. Using Proposition 10,\nP (\u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nZi \u2225\u2225\u2225\u2225\u2225 \u2265 \u221a (1 + \u01eb)d n ) = P   \u2225\u2225\u2225\u2225\u2225 1\u221a n n\u2211\ni=1\nZi \u2225\u2225\u2225\u2225\u2225 2 \u2265 (1 + \u01eb)d  \n= P (X \u2265 (1 + \u01eb)d) \u2264 exp { \u2212d 2 (\u01eb \u2212 log(1 + \u01eb)) }\nwhere X \u223c \u03c72d.\n8.1.2 Concentration bounds for estimating principal direction\nProposition 12. Let Z1, ..., Zn i.i.d.\u223c N (0, Id) and \u03b4 > 0. If n \u2265 d then with probability at least 1\u2212 3\u03b4,\n\u2016\u03a3\u0302n \u2212 Id\u20162 \u22643  1 + \u221a 2 log 1\u03b4\nd\n  \u221a d\nn max\n 1,  1 + \u221a 2 log 1\u03b4\nd\n  \u221a d\nn\n \n+  1 + \u221a 8 log 1\u03b4\nd max\n( 1,\n8 log 1\u03b4 d\n)  d\nn\nwhere \u03a3\u0302n is the empirical covariance of Zi. Proof. Let Zn = 1n \u2211n i=1 Zi. Then\n\u2016\u03a3\u0302n \u2212 Id\u20162 = \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nZiZ T i \u2212 Id \u2212 ZnZ\nT n \u2225\u2225\u2225\u2225\u2225 2\n\u2264 \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nZiZ T i \u2212 Id \u2225\u2225\u2225\u2225\u2225 2 + \u2225\u2225Zn \u2225\u22252 .\nIt is well known that for any \u01eb1 > 0,\nP (\u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nZiZ T i \u2212 Id \u2225\u2225\u2225\u2225\u2225 2 \u2265 3 (1 + \u01eb1) \u221a d n max ( 1, (1 + \u01eb1) \u221a d n )) \u2264 2 exp { \u2212d\u01eb 2 1 2 } .\nUsing this along with Proposition 11, we have for any \u01eb2 > 0,\nP ( \u2016\u03a3\u0302n \u2212 Id\u20162 \u2265 3 (1 + \u01eb1) \u221a d\nn max\n( 1, (1 + \u01eb1) \u221a d\nn\n) + (1 + \u01eb2)d\nn\n)\n\u2264 2 exp { \u2212d\u01eb 2 1\n2\n} + exp { \u2212d 2 (\u01eb2 \u2212 log(1 + \u01eb2)) } .\nSetting \u01eb1 = \u221a 2 log 1 \u03b4\nd ,\nP  \u2016\u03a3\u0302n \u2212 Id\u20162 \u2265 3  1 + \u221a 2 log 1\u03b4\nd\n  \u221a d\nn max\n 1,  1 + \u221a 2 log 1\u03b4\nd\n  \u221a d\nn\n + (1 + \u01eb2)d\nn\n \n\u2264 2\u03b4 + exp { \u2212d 2 (\u01eb2 \u2212 log(1 + \u01eb2)) } \u2264 2\u03b4 + exp { \u2212d 8 \u01eb2min(1, \u01eb2) }\nand, setting \u01eb2 =\n\u221a 8 log 1\n\u03b4 d max ( 1, 8 log 1 \u03b4 d ) , with probability at least 1\u2212 3\u03b4,\n\u2016\u03a3\u0302n \u2212 Id\u20162 \u22643  1 + \u221a 2 log 1\u03b4\nd\n  \u221a d\nn max\n 1,  1 + \u221a 2 log 1\u03b4\nd\n  \u221a d\nn\n \n+  1 + \u221a 8 log 1\u03b4\nd max\n( 1,\n8 log 1\u03b4 d\n)  d\nn .\nProposition 13. Let X1, Y1, ..., Xn, Yn i.i.d.\u223c N (0, 1). Then for any \u01eb > 0,\nP (\u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\nXiYi \u2223\u2223\u2223\u2223\u2223 > \u01eb 2 ) \u2264 2 exp { \u2212n\u01ebmin(1, \u01eb) 10 } .\nProof. Let Z = XY where X,Y i.i.d.\u223c N (0, 1). Then for any t such that |t| < 1,\nEetZ = 1\u221a\n1\u2212 t2 .\nSo for 0 < t < 1,\nP\n( 1\nn\nn\u2211\ni=1\nXiYi > \u01eb\n) = P ( exp { n\u2211\ni=1\ntXiYi\n} > exp(n\u01ebt) )\n\u2264 E ( exp { n\u2211\ni=1\ntXiYi\n}) exp(\u2212n\u01ebt)\n= (E exp(tXiYi)) n exp(\u2212n\u01ebt) = (1 \u2212 t2)\u2212n2 exp(\u2212n\u01ebt) = exp\n{ \u2212n 2 ( 2\u01ebt+ log(1 \u2212 t2) )} .\nThe bound is minimized by t = 12\u01eb (\u221a 1 + 4\u01eb2 \u2212 1 ) < 1, so\nP\n( 1\nn\nn\u2211\ni=1\nXiYi > \u01eb\n) \u2264 exp\n{ \u2212n 2 h(2\u01eb) }\nwhere\nh(u) = (\u221a 1 + u2 \u2212 1 ) + log ( 1\u2212 1\nu2\n(\u221a 1 + u2 \u2212 1 )2) .\nSince h(u) \u2265 u5 min(1, u),\nP\n( 1\nn\nn\u2211\ni=1\nXiYi > \u01eb\n) \u2264 exp { \u2212n 2 2\u01eb 5 min (1, 2\u01eb) }\nand the proof is complete by noting that the distribution of XiYi is symmetric.\n8.2 Davis\u2013Kahan\nLemma 3. Let A,E \u2208 Rd\u00d7d be symmetric matrices, and u \u2208 Rd\u22121 such that\nui = vi+1(A) TEv1(A).\nIf \u03bb1(A)\u2212 \u03bb2(A) > 0 and \u2016E\u20162 \u2264\n\u03bb1(A)\u2212 \u03bb2(A) 5\nthen \u221a 1\u2212 (v1(A)T v1(A+ E))2 \u2264\n4\u2016u\u2016 \u03bb1(A)\u2212 \u03bb2(A)\n(Corollary 8.1.11 of Golub and Van Loan (1996)).\n8.3 Bounding error in estimating the mean\nProposition 14. Let \u03b8 = (\u00b50 \u2212 \u00b5, \u00b50 + \u00b5) for some \u00b50, \u00b5 \u2208 Rd and X1, ..., Xn i.i.d.\u223c P\u03b8 . For any \u03b4 > 0,\nP  \u2016\u00b50 \u2212 \u00b5\u0302n\u2016 \u2265 \u03c3 \u221a 2max(d, 8 log 1\u03b4 )\nn + \u2016\u00b5\u2016\n\u221a 2 log 1\u03b4\nn\n  \u2264 3\u03b4.\nProof. Let Z1, ..., Zn i.i.d.\u223c N (0, I) and Y1, ..., Yn i.i.d. such that P(Yi = \u22121) = P(Yi = 1) = 12 . Then for any \u01eb1, \u01eb2 > 0,\nP ( \u2016\u00b50 \u2212 \u00b5\u0302n\u2016 \u2265 \u03c3 \u221a (1 + \u01eb1)d\nn + \u2016\u00b5\u2016\u01eb2\n)\n= P (\u2225\u2225\u2225\u2225\u2225\u00b50 \u2212 1 n d\u2211\ni=1\n(\u03c3Zi + \u00b50 + \u00b5Yi) \u2225\u2225\u2225\u2225\u2225 \u2265 \u03c3 \u221a (1 + \u01eb1)d n + \u2016\u00b5\u2016\u01eb2 )\n= P (\u2225\u2225\u2225\u2225\u2225\u03c3 1 n d\u2211\ni=1\nZi + \u00b5 1\nn\nd\u2211\ni=1\nYi \u2225\u2225\u2225\u2225\u2225 \u2265 \u03c3 \u221a (1 + \u01eb1)d n + \u2016\u00b5\u2016\u01eb2 )\n\u2264 P ( \u03c3 \u2225\u2225\u2225\u2225\u2225 1 n d\u2211\ni=1\nZi \u2225\u2225\u2225\u2225\u2225+ \u2016\u00b5\u2016 \u2223\u2223\u2223\u2223\u2223 1 n d\u2211\ni=1\nYi \u2223\u2223\u2223\u2223\u2223 \u2265 \u03c3 \u221a (1 + \u01eb1)d n + \u2016\u00b5\u2016\u01eb2 )\n\u2264 P (\u2225\u2225\u2225\u2225\u2225 1 n d\u2211\ni=1\nZi \u2225\u2225\u2225\u2225\u2225 \u2265 \u221a (1 + \u01eb1)d n ) + P (\u2223\u2223\u2223\u2223\u2223 1 n d\u2211\ni=1\nYi \u2223\u2223\u2223\u2223\u2223 \u2265 \u01eb2 )\n\u2264 exp { \u2212d 2 (\u01eb1 \u2212 log(1 + \u01eb1)) } + 2 exp { \u2212n\u01eb 2 2 2 }\nwhere the last step is using Hoeffding\u2019s inequality and Proposition 11. Setting \u01eb2 = \u221a 2 log 1 \u03b4\nn ,\nP  \u2016\u00b50 \u2212 \u00b5\u0302n\u2016 \u2265 \u03c3 \u221a (1 + \u01eb1)d\nn + \u2016\u00b5\u2016\n\u221a 2 log 1\u03b4\nn\n \n\u2264 exp { \u2212d 2 (\u01eb1 \u2212 log(1 + \u01eb1)) } + 2\u03b4.\nSince \u01eb1 \u2212 log(1 + \u01eb1) \u2265 \u01eb14 min(1, \u01eb1),\nexp { \u2212d 2 (\u01eb1 \u2212 log(1 + \u01eb1)) } \u2264 exp { \u2212d 8 \u01eb1 min(1, \u01eb1) } .\nSetting\n\u01eb1 =\n\u221a 8 log 1\u03b4\nd max\n( 1,\n8 log 1\u03b4 d\n) ,\nwe have\nP  \u2016\u00b50 \u2212 \u00b5\u0302n\u2016 \u2265 \u03c3 \u221a\u221a\u221a\u221a\u221a d n  1 + \u221a 8 log 1\u03b4 d max ( 1, 8 log 1\u03b4 d ) + \u2016\u00b5\u2016 \u221a 2 log 1\u03b4 n   \u2264 3\u03b4\nand the bound follows.\n8.4 Bounding error in estimating principal direction\nProposition 15. Let \u03b8 = (\u00b50\u2212\u00b5, \u00b50+\u00b5) for some \u00b50, \u00b5 \u2208 Rd and X1, ..., Xn i.i.d.\u223c P\u03b8 . If n \u2265 d then for any \u03b4, \u03b41 > 0, with probability at least 1\u2212 5\u03b4 \u2212 2\u03b41,\n\u2016\u03a3\u0302n \u2212 (\u03c32Id + \u00b5\u00b5T )\u20162\n\u2264 3\u03c32  1 + \u221a 2 log 1\u03b4\nd\n  \u221a d\nn max\n 1,  1 + \u221a 2 log 1\u03b4\nd\n  \u221a d\nn\n \n+ \u03c32  1 + \u221a 8 log 1\u03b4\nd max\n( 1,\n8 log 1\u03b4 d\n)  d\nn\n+ 4\u03c3\u2016\u00b5\u2016 \u221a\u221a\u221a\u221a\u221a  1 + \u221a 8 log 1\u03b4\nd max\n( 1,\n8 log 1\u03b4 d\n)  d\nn + 2\u2016\u00b5\u20162 log 1\u03b41 n\nwhere \u03a3\u0302n is the empirical covariance of Xi.\nProof. We can express Xi as Xi = \u03c3Zi + \u00b5Yi + \u00b50 where Z1, ..., Zn i.i.d.\u223c N (0, Id) and Y1, ..., Yn i.i.d. such that P(Yi = \u22121) = P(Yi = 1) = 12 . Then\n\u03a3\u0302n \u2212 (\u03c32Id + \u00b5\u00b5T ) = \u03c32(\u03a3\u0302Zn \u2212 Id)\u2212 \u00b5\u00b5TY 2\n+ \u03c3\n( 1\nn\nn\u2211\ni=1\nYiZi \u2212 Y Z ) \u00b5T\n+ \u03c3\u00b5\n( 1\nn\nn\u2211\ni=1\nYiZi \u2212 Y Z )T\nwhere \u03a3\u0302Zn is the empirical covariance of Zi and Y and Z are the empirical means of Yi and Zi. So\n\u2016\u03a3\u0302n \u2212 (\u03c32Id + \u00b5\u00b5T )\u20162 \u2264 \u03c32\u2016\u03a3\u0302Zn \u2212 Id\u20162 + \u2016\u00b5\u20162Y 2\n+ 2\u03c3\u2016\u00b5\u2016 (\u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nYiZi \u2225\u2225\u2225\u2225\u2225+ |Y |\u2016Z\u2016 ) .\nBy Hoeffding\u2019s inequality,\nP ( \u2016\u00b5\u20162Y 2 \u2265\n2\u2016\u00b5\u20162 log 1\u03b41 n\n) \u2264 2\u03b41.\nSince |Y | \u2264 1 and since YiZi has the same distribution as Zi, by Proposition 11, for any \u01eb > 0,\nP ( 2\u03c3\u2016\u00b5\u2016 (\u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nYiZi \u2225\u2225\u2225\u2225\u2225+ |Y |\u2016Z\u2016 ) \u2265 4\u03c3\u2016\u00b5\u2016 \u221a (1 + \u01eb)d n )\n\u2264 2 exp { \u2212d 2 (\u01eb\u2212 log(1 + \u01eb)) } \u2264 2 exp { \u2212d 8 \u01ebmin(1, \u01eb) } .\nSetting\n\u01eb =\n\u221a 8 log 1\u03b4\nd max\n( 1,\n8 log 1\u03b4 d\n)\nwe have\nP  2\u03c3\u2016\u00b5\u2016 (\u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nYiZi \u2225\u2225\u2225\u2225\u2225+ |Y |\u2016Z\u2016 ) \u2265 4\u03c3\u2016\u00b5\u2016\n\u221a\u221a\u221a\u221a\u221a  1 + \u221a 8 log 1\u03b4\nd max\n( 1,\n8 log 1\u03b4 d\n)  d\nn\n \n\u2264 2\u03b4.\nFinally, by Proposition 12, with probability at least 1\u2212 3\u03b4,\n\u03c32\u2016\u03a3\u0302Zn \u2212 Id\u20162 \u22643\u03c32  1 + \u221a 2 log 1\u03b4\nd\n  \u221a d\nn max\n 1,  1 + \u221a 2 log 1\u03b4\nd\n  \u221a d\nn\n \n+ \u03c32  1 + \u221a 8 log 1\u03b4\nd max\n( 1,\n8 log 1\u03b4 d\n)  d\nn\nand we complete the proof by combining the three bounds.\nProposition 16. Let \u03b8 = (\u00b50 \u2212 \u00b5, \u00b50 + \u00b5) for some \u00b50, \u00b5 \u2208 Rd and X1, ..., Xn i.i.d.\u223c P\u03b8 . If n \u2265 d > 1 then for any 0 < \u03b4 \u2264 1\u221a\ne and i \u2208 [2..d], with probability at least 1\u2212 7\u03b4,\n\u2223\u2223\u2223vi(\u03c32I + \u00b5\u00b5T )T (\u03a3\u0302n \u2212 (\u03c32I + \u00b5\u00b5T ))v1(\u03c32I + \u00b5\u00b5T ) \u2223\u2223\u2223\n\u2264 \u03c32 1 2\n\u221a 10 log 1\u03b4\nn max\n( 1,\n10 log 1\u03b4 n\n) + \u03c3\u2016\u00b5\u2016 \u221a 2 log 1\u03b4\nn + (\u03c32 + \u03c3\u2016\u00b5\u2016)2 log\n1 \u03b4\nn .\nProof. Let Z1,W1, ..., Zn,Wn i.i.d.\u223c N (0, 1) and Y1, ..., Yn i.i.d. such that P(Yi = \u22121) = P(Yi = 1) = 12 . It is easy to see that the quantity of interest is equal in distribution to \u2223\u2223\u2223\u2223\u2223\u2223 1 n n\u2211\nj=1\n(\u03c3Zi \u2212 \u03c3Z)(\u03c3Wi \u2212 \u03c3W + \u2016\u00b5\u2016Yi \u2212 \u2016\u00b5\u2016Y ) \u2223\u2223\u2223\u2223\u2223\u2223\nwhere Z,W, Y are the respective empirical means. Moreover, \u2223\u2223\u2223\u2223\u2223\u2223 1 n n\u2211\nj=1\n(\u03c3Zi \u2212 \u03c3Z)(\u03c3Wi \u2212 \u03c3W + \u2016\u00b5\u2016Yi \u2212 \u2016\u00b5\u2016Y ) \u2223\u2223\u2223\u2223\u2223\u2223\n\u2264 \u03c32 \u2223\u2223\u2223\u2223\u2223\u2223 1 n n\u2211\nj=1\nZiWi \u2223\u2223\u2223\u2223\u2223\u2223 + \u03c32 \u2223\u2223Z \u2223\u2223 \u2223\u2223W \u2223\u2223+ \u03c3\u2016\u00b5\u2016 \u2223\u2223\u2223\u2223\u2223\u2223 1 n n\u2211\nj=1\nZiYi \u2223\u2223\u2223\u2223\u2223\u2223 + \u03c3\u2016\u00b5\u2016 \u2223\u2223Z \u2223\u2223 \u2223\u2223Y \u2223\u2223 .\nFrom Proposition 13, we have\nP   \u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\nZiWi \u2223\u2223\u2223\u2223\u2223 > 1 2 \u221a 10 log 1\u03b4 n max ( 1, 10 log 1\u03b4 n )  \u2264 2\u03b4;\nusing Hoeffding\u2019s inequality,\nP  |Y | \u2265 \u221a 2 log 1\u03b4\nn\n  \u2264 2\u03b4;\nand using the Gaussian tail bound, for \u03b4 \u2264 1\u221a e ,\nP  |Z| \u2265 \u221a 2 log 1\u03b4\nn\n  \u2264 \u03b4\nand the final result follows easily.\nProposition 17. Let \u03b8 = (\u00b50 \u2212 \u00b5, \u00b50 + \u00b5) for some \u00b50, \u00b5 \u2208 Rd and X1, ..., Xn i.i.d.\u223c P\u03b8 with d > 1 and n \u2265 4d. For any 0 < \u03b4 < d\u22121\u221a\ne , if\nmax\n( \u03c32\n\u2016\u00b5\u20162 , \u03c3 \u2016\u00b5\u2016\n)\u221a max(d, 8 log 1\u03b4 )\nn \u2264 1 160\nthen with probability at least 1\u2212 12\u03b4 \u2212 2 exp ( \u2212 n20 ) ,\n\u221a 1\u2212 (v1(\u03c32I + \u00b5\u00b5T )T v1(\u03a3\u0302n))2 \u2264 14max ( \u03c32\n\u2016\u00b5\u20162 , \u03c3 \u2016\u00b5\u2016\n)\u221a d \u221a\u221a\u221a\u221a10 log d \u03b4\nn max\n( 1,\n10 log d\u03b4 n\n) .\nProof. By Proposition 15 (with \u03b41 = exp ( \u2212 n20 ) ), Proposition 16 (with \u03b42 = \u03b4d\u22121 ), and Lemma 3, with probability at\nleast 1\u2212 12\u03b4 \u2212 2 exp ( \u2212 n20 ) ,\n\u221a 1\u2212 (v1(\u03c32I + \u00b5\u00b5T )T v1(\u03a3\u0302n))2\n\u2264 4 \u221a d\u2212 1\n\u2016\u00b5\u20162\n \u03c32 1\n2\n\u221a\u221a\u221a\u221a10 log d\u22121 \u03b4\nn max\n( 1,\n10 log d\u22121\u03b4 n\n) + \u03c3\u2016\u00b5\u2016 \u221a 2 log d\u22121\u03b4\nn + (\u03c32 + \u03c3\u2016\u00b5\u2016)2 log\nd\u22121 \u03b4\nn\n \nand the result follows after some simplifications.\n8.5 General result relating error in estimating mean and principal direction to clustering loss\nProposition 18. Let \u03b8 = (\u00b50 \u2212 \u00b5, \u00b50 + \u00b5) and let\nF\u0302 (x) = { 1 if xT v \u2265 xT0 v 2 otherwise\nfor some x0, v \u2208 Rd, with \u2016v\u2016 = 1. Define cos\u03b2 = |vT\u00b5|/\u2016\u00b5\u2016. If |(x0 \u2212\u00b50)T v| \u2264 \u03c3\u01eb1 + \u2016\u00b5\u2016\u01eb2 for some \u01eb1 \u2265 0 and 0 \u2264 \u01eb2 \u2264 14 , and if sin\u03b2 \u2264 1\u221a5 , then\nL\u03b8(F\u0302 ) \u2264 exp { \u22121 2 max ( 0, \u2016\u00b5\u2016 2\u03c3 \u2212 2\u01eb1 )2}[ 2\u01eb1 + \u01eb2 \u2016\u00b5\u2016 \u03c3 + 2 sin\u03b2 ( 2 sin\u03b2 \u2016\u00b5\u2016 \u03c3 + 1 )] .\nProof.\nL\u03b8(F\u0302 ) =min \u03c0\nP\u03b8({x : F\u03b8(x) 6= \u03c0(F\u0302 (x))})\n=min { P\u03b8[{x : ((x \u2212 \u00b50)T\u00b5)((x \u2212 x0)T v) \u2265 0}], P\u03b8[{x : ((x \u2212 \u00b50)T\u00b5)((x\u2212 x0)T v) \u2264 0}] } .\nWLOG assume vT\u00b5 \u2265 0 (otherwise we can simply replace v with \u2212v, which does not affect the bound). Then\nL\u03b8(F\u0302 ) = P\u03b8[{x : ((x \u2212 \u00b50)T\u00b5)((x \u2212 x0)T v) \u2264 0}] = P\u03b8[{x : ((x \u2212 \u00b50)T\u00b5)((x \u2212 \u00b50)T v \u2212 (x0 \u2212 \u00b50)T v) \u2264 0}]\n= P\u03b8\n[{ x : ( (x\u2212 \u00b50)T \u00b5\n\u2016\u00b5\u2016\n) ((x\u2212 \u00b50)T v \u2212 (x0 \u2212 \u00b50)T v) \u2264 0 }] .\nDefine \u00b5\u0306 = \u00b5\n\u2016\u00b5\u2016 ,\nx\u0306 = (x\u2212 \u00b50)T \u00b5\u0306, and\ny\u0306 = (x\u2212 \u00b50)T v \u2212 \u00b5\u0306\u00b5\u0306T v \u2016v \u2212 \u00b5\u0306\u00b5\u0306T v\u2016 \u2261 (x\u2212 \u00b50) T v \u2212 \u00b5\u0306\u00b5\u0306T v sin\u03b2\nso that\nL\u03b8(F\u0302 ) = P\u03b8 [{ x : x\u0306 ( y\u0306 sin\u03b2 + x\u0306 cos\u03b2 \u2212 (x0 \u2212 \u00b50)T v ) \u2264 0 }]\n= P\u03b8 [{x : min(0, B(y\u0306)) \u2264 x\u0306 \u2264 max(0, B(y\u0306))}]\nwhere\nB(y\u0306) = (x0 \u2212 \u00b50)T v\ncos\u03b2 \u2212 y\u0306 tan\u03b2.\nSince x\u0306 and y\u0306 are projections of x \u2212 \u00b50 onto orthogonal unit vectors, and since x\u0306 is exactly the component of x\u2212 \u00b50 that lies in the direction of \u00b5, we can integrate out all other directions and obtain\nL\u03b8(F\u0302 ) =\n\u221e\u222b\n\u2212\u221e\n\u03c6\u03c3(y\u0306)\nmax(0,B(y\u0306))\u222b\nmin(0,B(y\u0306))\n( 1\n2 \u03c6\u03c3(x\u0306+ \u2016\u00b5\u2016) +\n1 2 \u03c6\u03c3(x\u0306\u2212 \u2016\u00b5\u2016)\n) dx\u0306dy\u0306\nwhere \u03c6\u03c3 is the density of N (0, \u03c32). But,\nmax(0,B(y\u0306))\u222b\nmin(0,B(y\u0306))\n( 1\n2 \u03c6\u03c3(x\u0306+ \u2016\u00b5\u2016) +\n1 2 \u03c6\u03c3(x\u0306\u2212 \u2016\u00b5\u2016)\n) dx\u0306\n= 1\n2\nmax(0,B(y\u0306))\u222b\nmin(0,B(y\u0306))\n\u03c6\u03c3(x\u0306+ \u2016\u00b5\u2016)dx\u0306+ 1\n2\nmax(0,B(y\u0306))\u222b\nmin(0,B(y\u0306))\n\u03c6\u03c3(x\u0306 \u2212 \u2016\u00b5\u2016)dx\u0306\n= 1\n2\n( \u03a6 ( max(0, B(y\u0306)) + \u2016\u00b5\u2016\n\u03c3\n) \u2212 \u03a6 ( min(0, B(y\u0306)) + \u2016\u00b5\u2016\n\u03c3\n))\n+ 1\n2\n( \u2212\u03a6 (\u2212max(0, B(y\u0306)) + \u2016\u00b5\u2016 \u03c3 ) +\u03a6 (\u2212min(0, B(y\u0306)) + \u2016\u00b5\u2016 \u03c3 ))\n= 1\n2\n( \u03a6 (\u2016\u00b5\u2016+ |B(y\u0306)| \u03c3 ) \u2212 \u03a6 (\u2016\u00b5\u2016 \u2212 |B(y\u0306)| \u03c3 )) .\nSince the above quantity is increasing in |B(y\u0306)|, and since |B(y\u0306)| \u2264 |y\u0306| tan\u03b2 + r where\nr = \u2223\u2223\u2223\u2223 (x0 \u2212 \u00b50)T v\ncos\u03b2\n\u2223\u2223\u2223\u2223 ,\nwe have that, replacing y\u0306 by x,\nL\u03b8(F\u0302 ) \u2264 1\n2\n\u221e\u222b\n\u2212\u221e\n1 \u03c3 \u03c6 (x \u03c3 )[ \u03a6 (\u2016\u00b5\u2016+ |x| tan \u03b2 + r \u03c3 ) \u2212 \u03a6 (\u2016\u00b5\u2016 \u2212 |x| tan\u03b2 \u2212 r \u03c3 )] dx\n\u2264 \u221e\u222b\n\u2212\u221e\n1 \u03c3 \u03c6 (x \u03c3 )[ \u03a6 (\u2016\u00b5\u2016 \u03c3 ) \u2212 \u03a6 (\u2016\u00b5\u2016 \u2212 |x| tan\u03b2 \u2212 r \u03c3 )] dx\n=\n\u221e\u222b\n\u2212\u221e\n\u03c6(x) [ \u03a6 (\u2016\u00b5\u2016 \u2212 r \u03c3 ) \u2212 \u03a6 (\u2016\u00b5\u2016 \u2212 r \u03c3 \u2212 |x| tan \u03b2 )] dx\n+ [ \u03a6 (\u2016\u00b5\u2016 \u03c3 ) \u2212 \u03a6 (\u2016\u00b5\u2016 \u2212 r \u03c3 )] .\nSince tan\u03b2 \u2264 12 , we have that r \u2264 2|(x0 \u2212 \u00b50)T v| \u2264 2\u03c3\u01eb1 + 2\u2016\u00b5\u2016\u01eb2 and\n\u03a6 (\u2016\u00b5\u2016 \u03c3 ) \u2212 \u03a6 (\u2016\u00b5\u2016 \u2212 r \u03c3 ) \u2264 r \u03c3 \u03c6 ( max ( 0, \u2016\u00b5\u2016 \u2212 r \u03c3 ))\n\u2264 ( 2\u01eb1 + 2\u01eb2\n\u2016\u00b5\u2016 \u03c3\n) \u03c6 ( max ( 0, (1\u2212 2\u01eb2)\n\u2016\u00b5\u2016 \u03c3\n\u2212 2\u01eb1 )) ,\nand since \u01eb2 \u2264 14 ,\n\u03a6 (\u2016\u00b5\u2016 \u03c3 ) \u2212 \u03a6 (\u2016\u00b5\u2016 \u2212 r \u03c3 ) \u2264 2 ( \u01eb1 + \u01eb2 \u2016\u00b5\u2016 \u03c3 ) \u03c6 ( max ( 0, \u2016\u00b5\u2016 2\u03c3 \u2212 2\u01eb1 )) .\nDefining A = \u2223\u2223\u2223\u2016\u00b5\u2016\u2212r\u03c3 \u2223\u2223\u2223, \u221e\u222b\n\u2212\u221e\n\u03c6(x) [ \u03a6 (\u2016\u00b5\u2016 \u2212 r \u03c3 ) \u2212 \u03a6 (\u2016\u00b5\u2016 \u2212 r \u03c3 \u2212 |x| tan\u03b2 )] dx\n\u2264 2 \u221e\u222b\n0\nA\u222b\nA\u2212x tan \u03b2\n\u03c6(x)\u03c6(y)dydx = 2\n\u221e\u222b\n\u2212A sin \u03b2\nA cos\u03b2+(x+A sin \u03b2) tan \u03b2\u222b\nA cos\u03b2\n\u03c6(x)\u03c6(y)dydx\n\u2264 2\u03c6(A cos \u03b2) tan\u03b2 \u221e\u222b\n\u2212A sin \u03b2\n(x+A sin\u03b2)\u03c6(x)dx\n= 2\u03c6(A cos \u03b2) tan\u03b2 (A sin\u03b2\u03a6(A sin \u03b2) + \u03c6(A sin\u03b2)) \u2264 2\u03c6 (A) tan\u03b2 (A sin\u03b2 + 1) \u2264 2\u03c6 ( max ( 0,\n\u2016\u00b5\u2016 \u2212 r \u03c3\n)) tan\u03b2 ((\u2016\u00b5\u2016+ r \u03c3 ) sin\u03b2 + 1 )\nand \u221e\u222b\n\u2212\u221e\n\u03c6(x) [ \u03a6 (\u2016\u00b5\u2016 \u2212 r \u03c3 ) \u2212 \u03a6 (\u2016\u00b5\u2016 \u2212 r \u03c3 \u2212 |x| tan\u03b2 )] dx\n\u2264 2\u03c6 ( max ( 0,\n\u2016\u00b5\u2016 2\u03c3\n\u2212 2\u01eb1 )) tan\u03b2 (( 2 \u2016\u00b5\u2016 \u03c3 + 2\u01eb1 ) sin\u03b2 + 1 ) .\nSo we have that\nL\u03b8(F\u0302 ) \u2264 2 ( \u01eb1 + \u01eb2\n\u2016\u00b5\u2016 \u03c3\n) \u03c6 ( max ( 0,\n\u2016\u00b5\u2016 2\u03c3\n\u2212 2\u01eb1 ))\n+ 2\u03c6 ( max ( 0,\n\u2016\u00b5\u2016 2\u03c3\n\u2212 2\u01eb1 )) tan\u03b2 (( 2 \u2016\u00b5\u2016 \u03c3 + 2\u01eb1 ) sin\u03b2 + 1 )\n\u2264 \u03c6 ( max ( 0,\n\u2016\u00b5\u2016 2\u03c3\n\u2212 2\u01eb1 )) \u00d7\n\u00d7 [ 2\u01eb1 + 2\u01eb2\n\u2016\u00b5\u2016 \u03c3 + 4 sin\u03b2 tan\u03b2 \u2016\u00b5\u2016 \u03c3 + 4\u01eb1 sin\u03b2 tan\u03b2 + 2 tan\u03b2\n]\n\u2264 exp { \u22121 2 max ( 0, \u2016\u00b5\u2016 2\u03c3 \u2212 2\u01eb1 )2}[ 2\u01eb1 + \u01eb2 \u2016\u00b5\u2016 \u03c3 + tan\u03b2 ( 2 sin\u03b2 \u2016\u00b5\u2016 \u03c3 + 1 )] .\n8.6 Non-sparse upper bound\nTheorem 5. For any \u03b8 \u2208 \u0398\u03bb and X1, ..., Xn i.i.d.\u223c P\u03b8, let\nF\u0302 (x) = { 1 if xT v1(\u03a3\u0302n) \u2265 \u00b5\u0302Tnv1(\u03a3\u0302n) 2 otherwise,\nand let n \u2265 max(68, 4d), d \u2265 1. Then\nsup \u03b8\u2208\u0398\u03bb\nEL\u03b8(F\u0302 ) \u2264 600max ( 4\u03c32\n\u03bb2 , 1\n)\u221a d log(nd)\nn .\nFurthermore, if \u03bb\u03c3 \u2265 2max(80, 14 \u221a 5d), then\nsup \u03b8\u2208\u0398\u03bb EL\u03b8(F\u0302 ) \u2264 17 exp ( \u2212 n 32 ) + 9 exp ( \u2212 \u03bb 2 80\u03c32 ) .\nProof. Using Propositions 14 and 17 with \u03b4 = 1\u221a n , Proposition 18, and the fact that (C + x) exp(\u2212max(0, x \u2212 4)2/8) \u2264 (C + 6) exp(\u2212max(0, x\u2212 4)2/10) for all C, x > 0,\nEL\u03b8(F\u0302 ) \u2264 600max ( 4\u03c32\n\u03bb2 , 1\n)\u221a d log(nd)\nn\n(it is easy to verify that the bounds are decreasing with \u2016\u00b5\u2016, so we use \u2016\u00b5\u2016 = \u03bb2 to bound the supremum). Note that the d = 1 case must be handled separately, but results in a bound that agrees with the above.\nAlso, when \u03bb\u03c3 \u2265 2max(80, 14 \u221a 5d), using \u03b4 = exp ( \u2212 n32 ) ,\nEL\u03b8(F\u0302 ) \u226417 exp ( \u2212 n 32 ) + 9 exp ( \u2212 \u03bb 2 80\u03c32 ) .\n8.7 Estimating the support in the sparse case\nProposition 19. Let \u03b8 = (\u00b50 \u2212 \u00b5, \u00b50 + \u00b5) for some \u00b50, \u00b5 \u2208 Rd and X1, ..., Xn i.i.d.\u223c P\u03b8. For any 0 < \u03b4 < 1\u221ae such\nthat \u221a 6 log 1 \u03b4\nn \u2264 12 , with probability at least 1\u2212 6d\u03b4,\n|\u03a3\u0302n(i, i)\u2212 (\u03c32 + \u00b5(i)2)| \u2264 \u03c32 \u221a\n6 log 1\u03b4 n + 2\u03c3|\u00b5(i)|\n\u221a 2 log 1\u03b4\nn + (\u03c3 + |\u00b5(i)|)2 2 log\n1 \u03b4\nn\nfor all i \u2208 [d].\nProof. Consider any i \u2208 [d]. Let Z1, ..., Zn i.i.d.\u223c N (0, 1) and Y1, ..., Yn i.i.d. such that P(Yj = \u22121) = P(Yj = 1) = 12 . Then \u03a3\u0302n(i, i) is equal in distribution to\n1\nn\nn\u2211\nj=1\n(\u03c3Zj + \u00b5(i)Yj \u2212 \u03c3Z \u2212 \u00b5(i)Y )2\nwhere Z and Y are the respective empirical means, and\n1\nn\nn\u2211\nj=1\n(\u03c3Zj + \u00b5(i)Yj \u2212 \u03c3Z \u2212 \u00b5(i)Y )2 = 1\nn\nn\u2211\nj=1\n(\u03c3Zj + \u00b5(i)Yj) 2 \u2212 (\u03c3Z + \u00b5(i)Y )2\n= \u03c32 1\nn\nn\u2211\nj=1\nZ2j + \u00b5(i) 2 + 2\u03c3\u00b5(i)\n1\nn\nn\u2211\nj=1\nZjYj\n\u2212 \u03c32Z2 \u2212 \u00b5(i)2Y 2 \u2212 2\u03c3\u00b5(i)ZY\nSo, by Hoeffding\u2019s inequality, a Gaussian tail bound, and Proposition 10, we have that for any 0 < \u03b4 < 1\u221a e , with probability at least 1\u2212 6\u03b4,\n|\u03a3\u0302n(i, i)\u2212 (\u03c32 + \u00b5(i)2)| \u2264 \u03c32 \u221a\n6 log 1\u03b4 n + 2\u03c3|\u00b5(i)|\n\u221a 2 log 1\u03b4\nn + (\u03c3 + |\u00b5(i)|)2 2 log\n1 \u03b4\nn\nwhere we have used the fact that for \u01eb \u2208 (0, 0.5],\nmax {\u2212\u01eb+ log(1 + \u01eb), \u01eb + log(1\u2212 \u01eb)} \u2264 \u2212 \u01eb 2\n3\nand the result follows easily.\nProposition 20. Let \u03b8 = (\u00b50 \u2212 \u00b5, \u00b50 + \u00b5) for some \u00b50, \u00b5 \u2208 Rd and X1, ..., Xn i.i.d.\u223c P\u03b8 . Define\nS(\u03b8) = {i \u2208 [d] : \u00b5(i) 6= 0},\n\u03b1 =\n\u221a 6 log(nd)\nn +\n2 log(nd)\nn ,\nS\u0303(\u03b8) = {i \u2208 [d] : |\u00b5(i)| \u2265 4\u03c3\u221a\u03b1},\n\u03c4\u0302n = 1 + \u03b1\n1\u2212 \u03b1 mini\u2208[d] \u03a3\u0302n(i, i),\nand S\u0302n = {i \u2208 [d] : \u03a3\u0302n(i, i) > \u03c4\u0302n}.\nAssume that n \u2265 1, d \u2265 2, and \u03b1 \u2264 14 . Then S\u0303(\u03b8) \u2286 S\u0302n \u2286 S(\u03b8) with probability at least 1\u2212 6n . Proof. By Proposition 19, with probability at least 1\u2212 6n ,\n|\u03a3\u0302n(i, i)\u2212 (\u03c32 + \u00b5(i)2)| \u2264 \u03c32 \u221a 6 log(nd)\nn + 2\u03c3|\u00b5(i)|\n\u221a 2 log(nd)\nn + (\u03c3 + |\u00b5(i)|)2 2 log(nd) n\nfor all i \u2208 [d]. Assume the above event holds. If S(\u03b8) = [d], then of course S\u0302n \u2286 S(\u03b8). Otherwise, for i /\u2208 S(\u03b8),\n(1\u2212 \u03b1)\u03c32 \u2264 \u03a3\u0302n(i, i) \u2264 (1 + \u03b1)\u03c32\nso it is clear that S\u0302n \u2286 S(\u03b8). The remainder of the proof is trivial if S\u0303(\u03b8) = \u2205 or S(\u03b8) = \u2205. Assume otherwise. For any i \u2208 S(\u03b8),\n\u03a3\u0302n(i, i) \u2265 (1\u2212 \u03b1)\u03c32 + \u00b5(i)2 \u2212 2\u03c3|\u00b5(i)| \u221a 2 log(nd)\nn \u2212 2\u03c3|\u00b5(i)|2 log(nd) n \u2212 \u00b5(i)2 2 log(nd) n\n\u2265 (1\u2212 \u03b1)\u03c32 + ( 1\u2212 2 log(nd)\nn\n) \u00b5(i)2 \u2212 2\u03b1\u03c3|\u00b5(i)|.\nBy definition, |\u00b5(i)| \u2265 4\u03c3\u221a\u03b1 for all i \u2208 S\u0303(\u03b8), so\n(1 + \u03b1)2\n1\u2212 \u03b1 \u03c3 2 \u2264 (1\u2212 \u03b1)\u03c32 +\n( 1\u2212 2 log(nd)\nn\n) \u00b5(i)2 \u2212 2\u03b1\u03c3|\u00b5(i)| \u2264 \u03a3\u0302n(i, i)\nand i \u2208 S\u0302n (we ignore strict equality above as a measure 0 event), i.e. S\u0303(\u03b8) \u2286 S\u0302n, which concludes the proof.\n8.8 Sparse upper bound\nTheorem 6. For any \u03b8 = (\u00b50 \u2212 \u00b5, \u00b50 + \u00b5) \u2208 \u0398\u03bb,s and X1, ..., Xn i.i.d.\u223c P\u03b8 with n \u2265 max(68, 4s) and s \u2265 1, define\n\u03b1 =\n\u221a 6 log(nd)\nn +\n2 log(nd)\nn ,\n\u03c4\u0302n = 1 + \u03b1\n1\u2212 \u03b1 mini\u2208[d] \u03a3\u0302n(i, i),\nand S\u0302n = {i \u2208 [d] : \u03a3\u0302n(i, i) > \u03c4\u0302n}. Assume that d \u2265 2, and \u03b1 \u2264 14 . Let\nF\u0302n(x) =\n{ 1 if xT\nS\u0302n v1(\u03a3\u0302S\u0302n) \u2265 \u00b5\u0302 T S\u0302n v1(\u03a3\u0302S\u0302n)\n2 otherwise\nwhere \u00b5\u0302S\u0302n and \u03a3\u0302S\u0302n are the empirical mean and covariance of Xi for the dimensions in S\u0302n, and 0 elsewhere. Then\nsup \u03b8\u2208\u0398\u03bb,s\nEL\u03b8(F\u0302 ) \u2264 603max ( 16\u03c32\n\u03bb2 , 1\n)\u221a s log(ns)\nn + 220\n\u03c3 \u221a s\n\u03bb\n( log(nd)\nn\n) 1 4\n.\nProof. Define S(\u03b8) = {i \u2208 [d] : \u00b5(i) 6= 0}\nand S\u0303(\u03b8) = {i \u2208 [d] : |\u00b5(i)| \u2265 4\u03c3\u221a\u03b1},\nAssume S\u0303(\u03b8) \u2286 S\u0302n \u2286 S(\u03b8) (by Proposition 20, this holds with probability at least 1 \u2212 6n ). If S\u0303(\u03b8) = \u2205, then we simply have EL\u03b8(F\u0302n) \u2264 12 .\nAssume S\u0303(\u03b8) 6= \u2205. Let cos \u03b2\u0302 = |v1(\u03a3\u0302S\u0302n)\nT v1(\u03a3)|, cos \u03b2\u0303 = |v1(\u03a3S\u0302n)\nT v1(\u03a3)|, and\ncos\u03b2 = |v1(\u03a3\u0302S\u0302n) T v1(\u03a3S\u0302n)|\nwhere \u03a3 = \u03c32I + \u00b5\u00b5T , and \u03a3S\u0302n is the same as \u03a3 in S\u0302n, and 0 elsewhere. Then\nsin \u03b2\u0302 \u2264 sin \u03b2\u0303 + sin\u03b2. Also\nsin \u03b2\u0303 = \u2016\u00b5\u2212 \u00b5S\u0302(\u03b8)\u2016\n\u2016\u00b5\u2016\n\u2264 \u2016\u00b5\u2212 \u00b5S\u0303(\u03b8)\u2016\n\u2016\u00b5\u2016\n\u2264 4\u03c3\n\u221a \u03b1 \u221a |S(\u03b8)| \u2212 |S\u0303(\u03b8)| \u2016\u00b5\u2016\n\u2264 8\u03c3 \u221a s\u03b1\n\u03bb .\nUsing the same argument as the proof of Theorem 5, we have that as long as the above bound is smaller than 1 2 \u221a 5 ,\nEL\u03b8(F\u0302 ) \u2264 600max (\n\u03c32 ( \u03bb 2 \u2212 4\u03c3 \u221a s\u03b1\n)2 , 1 )\u221a s log(ns)\nn + 104\n\u03c3 \u221a s\u03b1\n\u03bb +\n3\nn\n\u2264 603max ( 16 \u03c32\n\u03bb2 , 1\n)\u221a s log(ns)\nn + 104\n\u03c3 \u221a s\u03b1\n\u03bb .\nHowever, when 8\u03c3 \u221a s\u03b1\n\u03bb > 1 2 \u221a 5 , the above bound is bigger than 12 , which is a trivial upper bound on the clustering\nerror, hence the bound can be stated without further conditions. Finally, since \u03b1 \u2264 14 , we must have log(nd) n \u2264 1, so \u03b1 \u2264 ( \u221a 6 + 2) \u221a log(nd)\nn , which completes the proof.\n9 Lower bounds\n9.1 Standard tools\nLemma 4. Let P0, P1, ..., PM be probability measures satisfying\n1\nM\nM\u2211\ni=1\nKL(Pi, P0) \u2264 \u03b1 logM\nwhere 0 < \u03b1 < 1/8 and M \u2265 2. Then inf \u03c8 max i\u2208[0..M ] Pi(\u03c8 6= i) \u2265 0.07\n(Tsybakov (2009)).\nLemma 5. (Varshamov\u2013Gilbert bound) Let \u2126 = {0, 1}m for m \u2265 8. Then there exists a subset {\u03c90, ..., \u03c9M} \u2286 \u2126 such that \u03c90 = (0, ..., 0),\n\u03c1(\u03c9i, \u03c9j) \u2265 m\n8 , \u2200 0 \u2264 i < j \u2264 M,\nand M \u2265 2m/8,\nwhere \u03c1 denotes the Hamming distance between two vectors (Tsybakov (2009)).\nLemma 6. Let \u2126 = {\u03c9 \u2208 {0, 1}m : \u2016\u03c9\u20160 = s} for integers m > s \u2265 1. For any \u03b1, \u03b2 \u2208 (0, 1) such that s \u2264 \u03b1\u03b2m, there exists \u03c90, ..., \u03c9M \u2208 \u2126 such that for all 0 \u2264 i < j \u2264 M ,\n\u03c1(\u03c9i, \u03c9j) > 2(1\u2212 \u03b1)s\nand log(M + 1) \u2265 cs log (m s )\nwhere c =\n\u03b1\n\u2212 log(\u03b1\u03b2) (\u2212 log\u03b2 + \u03b2 \u2212 1).\nIn particular, setting \u03b1 = 3/4 and \u03b2 = 1/3, we have that \u03c1(\u03c9i, \u03c9j) > s/2, log(M + 1) \u2265 s5 log ( m s ) , as long as s \u2264 m/4 (Massart (2007), Lemma 4.10).\n9.2 A reduction to hypothesis testing without a general triangle inequality\nProposition 21. Let \u03b80, ..., \u03b8M \u2208 \u0398\u03bb (or \u0398\u03bb,s), M \u2265 2, 0 < \u03b1 < 1/8, and \u03b3 > 0. If\nmax i\u2208[M ]\nKL(P\u03b8i , P\u03b80) \u2264 \u03b1 logM\nn\nand for all 0 \u2264 i 6= j \u2264 M and clusterings F\u0302 ,\nL\u03b8i(F\u0302 ) < \u03b3 implies L\u03b8j(F\u0302 ) \u2265 \u03b3,\nthen\ninf F\u0302n max i\u2208[0..M ]\nE\u03b8iL\u03b8i(F\u0302n) \u2265 0.07\u03b3.\nProof. Using Markov\u2019s inequality,\ninf F\u0302n max i\u2208[0..M ] E\u03b8iL\u03b8i(F\u0302n) \u2265 \u03b3 inf F\u0302n max i\u2208[0..M ] Pn\u03b8i\n( L\u03b8i(F\u0302n) \u2265 \u03b3 ) .\nDefine\u03c8\u2217(F\u0302n) = argmin i\u2208[0..M ] L\u03b8i(F\u0302n). By assumption,L\u03b8i(F\u0302n) < \u03b3 impliesL\u03b8j(F\u0302n) \u2265 \u03b3 for any j 6= i, so L\u03b8i(F\u0302n) < \u03b3 only when \u03c8\u2217(F\u0302n) = i. Hence,\nPn\u03b8i ( \u03c8\u2217(F\u0302n) = i ) \u2265 Pn\u03b8i ( L\u03b8i(F\u0302n) < \u03b3 )\nand\ninf F\u0302n max i\u2208[0..M ]\nPn\u03b8i ( L\u03b8i(F\u0302n) \u2265 \u03b3 ) \u2265 max\ni\u2208[0..M ] Pn\u03b8i\n( \u03c8\u2217(F\u0302n) 6= i )\n\u2265 inf \u03c8\u0302n max i\u2208[0..M ] Pn\u03b8i\n( \u03c8\u0302n 6= i )\n\u2265 0.07\nwhere the last step is by Lemma 4.\n9.3 Properties of the clustering error\nProposition 22. For any \u03b8, \u03b8\u2032 \u2208 \u0398\u03bb, and any clustering F\u0302 , if\nL\u03b8(F\u03b8\u2032) + L\u03b8(F\u0302 ) +\n\u221a KL(P\u03b8, P\u03b8\u2032)\n2 \u2264 1 2 ,\nthen\nL\u03b8(F\u03b8\u2032)\u2212 L\u03b8(F\u0302 )\u2212 \u221a KL(P\u03b8, P\u03b8\u2032)\n2 \u2264 L\u03b8\u2032(F\u0302 ) \u2264 L\u03b8(F\u03b8\u2032) + L\u03b8(F\u0302 ) +\n\u221a KL(P\u03b8, P\u03b8\u2032)\n2 .\nProof. WLOG assume F\u03b8 , F\u03b8\u2032 , and F\u0302 are such that, using simplified notation,\nL\u03b8(F\u03b8\u2032) = P\u03b8(F\u03b8 6= F\u03b8\u2032)\nand L\u03b8(F\u0302 ) = P\u03b8(F\u03b8 6= F\u0302 ).\nThen\nP\u03b8(F\u03b8\u2032 6= F\u0302 ) = P\u03b8 ( (F\u03b8 = F\u03b8\u2032) \u2229 (F\u03b8 6= F\u0302 ) \u222a (F\u03b8 6= F\u03b8\u2032) \u2229 (F\u03b8 = F\u0302 ) )\n= P\u03b8 ( (F\u03b8 = F\u03b8\u2032) \u2229 (F\u03b8 6= F\u0302 ) ) + P\u03b8 ( (F\u03b8 6= F\u03b8\u2032) \u2229 (F\u03b8 = F\u0302 ) ) .\nSince\n0 \u2264 P\u03b8 ( (F\u03b8 = F\u03b8\u2032) \u2229 (F\u03b8 6= F\u0302 ) ) \u2264 P\u03b8 ( F\u03b8 6= F\u0302 ) = L\u03b8(F\u0302 ),\nP\u03b8 ( (F\u03b8 6= F\u03b8\u2032) \u2229 (F\u03b8 = F\u0302 ) ) \u2264 P\u03b8 (F\u03b8 6= F\u03b8\u2032) = L\u03b8(F\u03b8\u2032),\nand\nL\u03b8(F\u03b8\u2032)\u2212 L\u03b8(F\u0302 ) = P\u03b8 (F\u03b8 6= F\u03b8\u2032)\u2212 P\u03b8(F\u03b8 6= F\u0302 ) \u2264 P\u03b8 ( (F\u03b8 6= F\u03b8\u2032) \u2229 (F\u03b8 = F\u0302 ) ) ,\nwe have that\nL\u03b8(F\u03b8\u2032)\u2212 L\u03b8(F\u0302 ) \u2264 P\u03b8(F\u03b8\u2032 6= F\u0302 ) \u2264 L\u03b8(F\u03b8\u2032) + L\u03b8(F\u0302 )\nand\nL\u03b8(F\u03b8\u2032)\u2212 L\u03b8(F\u0302 )\u2212 TV(P\u03b8, P\u03b8\u2032) \u2264 P\u03b8\u2032(F\u03b8\u2032 6= F\u0302 ) \u2264 L\u03b8(F\u03b8\u2032) + L\u03b8(F\u0302 ) + TV(P\u03b8, P\u03b8\u2032).\nIt is easy to see that if L\u03b8(F\u03b8\u2032) + L\u03b8(F\u0302 ) + TV(P\u03b8, P\u03b8\u2032) \u2264 12 , then the above bound implies\nL\u03b8(F\u03b8\u2032)\u2212 L\u03b8(F\u0302 )\u2212 TV(P\u03b8, P\u03b8\u2032) \u2264 L\u03b8\u2032(F\u0302 ) \u2264 L\u03b8(F\u03b8\u2032) + L\u03b8(F\u0302 ) + TV(P\u03b8, P\u03b8\u2032).\nThe final step is to use the fact that TV(P\u03b8, P\u03b8\u2032) \u2264 \u221a KL(P\u03b8,P\u03b8\u2032 ) 2 .\nProposition 23. For some \u00b50, \u00b5, \u00b5\u2032 \u2208 Rd such that \u2016\u00b5\u2016 = \u2016\u00b5\u2032\u2016, let\n\u03b8 = ( \u00b50 \u2212 \u00b5\n2 , \u00b50 +\n\u00b5\n2\n)\nand\n\u03b8\u2032 = ( \u00b50 \u2212 \u00b5\u2032\n2 , \u00b50 +\n\u00b5\u2032\n2\n) .\nThen\n2g (\u2016\u00b5\u2016 2\u03c3 ) sin\u03b2 cos\u03b2 \u2264 L\u03b8(F\u03b8\u2032) \u2264 1 \u03c0 tan\u03b2\nwhere cos\u03b2 = |\u00b5 T\u00b5\u2032|\n\u2016\u00b5\u20162 and g(x) = \u03c6(x)(\u03c6(x) \u2212 x\u03a6(\u2212x)). Proof. It is easy to see that\nL\u03b8(F\u03b8\u2032) = 1\n2\n\u222b\nR\n1 \u03c3 \u03c6 (x \u03c3 )( \u03a6 (\u2016\u00b5\u2016 2\u03c3 + |x| tan\u03b2 \u03c3 ) \u2212 \u03a6 (\u2016\u00b5\u2016 2\u03c3 \u2212 |x| tan\u03b2 \u03c3 )) dx.\nDefine \u03be = \u2016\u00b5\u20162\u03c3 . With a change of variables, we have\nL\u03b8(F\u03b8\u2032) = 1\n2\n\u222b\nR\n\u03c6(x) (\u03a6 (\u03be + |x| tan\u03b2) \u2212 \u03a6 (\u03be \u2212 |x| tan\u03b2)) dx\n=\n\u221e\u222b\n0\n\u03c6(x)(\u03a6(\u03be + x tan\u03b2)\u2212 \u03a6(\u03be \u2212 x tan\u03b2))dx.\nFor any a \u2264 b, \u03a6(b)\u2212 \u03a6(a) \u2264 b\u2212a\u221a 2\u03c0 , so\nL\u03b8(F\u03b8\u2032) =\n\u221e\u222b\n0\n\u03c6(x)(\u03a6(\u03be + x tan\u03b2)\u2212 \u03a6(\u03be \u2212 x tan\u03b2))dx\n\u2264 \u221e\u222b\n0\n\u03c6(x)(\u03a6(x tan \u03b2)\u2212 \u03a6(\u2212x tan\u03b2))dx\n\u2264 tan\u03b2 \u221a 2\n\u03c0\n\u221e\u222b\n0\nx\u03c6(x)dx\n= 1\n\u03c0 tan\u03b2.\nAlso,\nL\u03b8(F\u03b8\u2032) =\n\u221e\u222b\n0\n\u03c6(x)(\u03a6(\u03be + x tan\u03b2)\u2212 \u03a6(\u03be \u2212 x tan\u03b2))dx\n\u2265 2 tan\u03b2 \u221e\u222b\n0\nx\u03c6(x)\u03c6(\u03be + x tan\u03b2)dx\n= 2 tan\u03b2 1\u221a 2\u03c0\n\u221e\u222b\n0\nx 1\u221a 2\u03c0 exp\n{ \u2212x 2 + (\u03be + x tan\u03b2)2\n2\n} dx\n= 2 tan\u03b2 1\u221a 2\u03c0 exp\n{ \u2212\u03be 2\n2\n( 1\u2212 tan 2 \u03b2\n1 + tan2 \u03b2\n)} \u221e\u222b\n0\nx 1\u221a 2\u03c0 exp    \u2212 ( x+ \u03be tan \u03b21+tan2 \u03b2 )2 2 ( 1\u221a\n1+tan2 \u03b2\n)2    dx\n\u2265 2 tan\u03b2 1\u221a 2\u03c0 exp\n{ \u2212\u03be 2\n2\n} \u221e\u222b\n0\nx 1\u221a 2\u03c0 exp\n{ \u2212 (x+ \u03be sin\u03b2 cos\u03b2) 2\n2 cos2 \u03b2\n} dx\n= 2 tan\u03b2\u03c6(\u03be) [ cos2 \u03b2\u221a\n2\u03c0 exp\n{ \u2212\u03be 2 sin2 \u03b2\n2\n} \u2212 \u03be sin\u03b2 cos2 \u03b2\u03a6(\u2212\u03be sin\u03b2) ]\n= 2 sin\u03b2 cos\u03b2\u03c6(\u03be) [\u03c6(\u03be sin\u03b2) \u2212 \u03be sin\u03b2\u03a6(\u2212\u03be sin\u03b2)] \u2265 2 sin\u03b2 cos\u03b2\u03c6(\u03be) [\u03c6(\u03be) \u2212 \u03be\u03a6(\u2212\u03be)] .\n9.4 A KL divergence bound of the necessary order\nProposition 24. For some \u00b50, \u00b5, \u00b5\u2032 \u2208 Rd such that \u2016\u00b5\u2016 = \u2016\u00b5\u2032\u2016, let\n\u03b8 = ( \u00b50 \u2212 \u00b5\n2 , \u00b50 +\n\u00b5\n2\n)\nand\n\u03b8\u2032 = ( \u00b50 \u2212 \u00b5\u2032\n2 , \u00b50 +\n\u00b5\u2032\n2\n) .\nThen\nKL(P\u03b8 , P\u03b8\u2032) \u2264 \u03be4(1\u2212 cos\u03b2)\nwhere \u03be = \u2016\u00b5\u20162\u03c3 and cos\u03b2 = |\u00b5T\u00b5\u2032| \u2016\u00b5\u2016\u2016\u00b5\u2032\u2016 .\nProof. Since the KL divergence is invariant to affine transformations, it is easy to see that\nKL(P\u03b8 , P\u03b8\u2032) =\n\u222b\nR\n\u222b\nR\np1(x, y) log p1(x, y)\np2(x, y) dxdy\nwhere\np1(x, y) = 1\n2 \u03c6(x + \u03bex)\u03c6(y + \u03bey) +\n1 2 \u03c6(x\u2212 \u03bex)\u03c6(y \u2212 \u03bey),\np2(x, y) = 1\n2 \u03c6(x + \u03bex)\u03c6(y \u2212 \u03bey) +\n1 2 \u03c6(x\u2212 \u03bex)\u03c6(y + \u03bey),\n\u03bex = \u03be cos \u03b2\n2 , \u03bey = \u03be sin\n\u03b2 2 .\nSince\np1(x, y) p2(x, y) = \u03c6(x + \u03bex)\u03c6(y + \u03bey) + \u03c6(x \u2212 \u03bex)\u03c6(y \u2212 \u03bey) \u03c6(x + \u03bex)\u03c6(y \u2212 \u03bey) + \u03c6(x \u2212 \u03bex)\u03c6(y + \u03bey)\n= exp(\u2212x\u03bex \u2212 y\u03bey) + exp(x\u03bex + y\u03bey) exp(\u2212x\u03bex + y\u03bey) + exp(x\u03bex \u2212 y\u03bey)\nwe have\nlog p1(x, y)\np2(x, y) = log\ncosh(x\u03bex + y\u03bey) cosh(x\u03bex \u2212 y\u03bey) .\nFurthermore, \u222b\nR\n\u222b\nR\n1 2 \u03c6(x + \u03bex)\u03c6(y + \u03bey) log cosh(x\u03bex + y\u03bey) cosh(x\u03bex \u2212 y\u03bey) dxdy\n=\n\u222b\nR\n\u222b\nR\n1 2 \u03c6(\u2212x+ \u03bex)\u03c6(\u2212y + \u03bey) log cosh(\u2212x\u03bex \u2212 y\u03bey) cosh(\u2212x\u03bex + y\u03bey) dxdy\n=\n\u222b\nR\n\u222b\nR\n1 2 \u03c6(x \u2212 \u03bex)\u03c6(y \u2212 \u03bey) log cosh(x\u03bex + y\u03bey) cosh(x\u03bex \u2212 y\u03bey) dxdy\nso\nKL(P\u03b8, P\u03b8\u2032) =\n\u222b\nR\n\u222b\nR\n\u03c6(x \u2212 \u03bex)\u03c6(y \u2212 \u03bey) log cosh(x\u03bex + y\u03bey)\ncosh(x\u03bex \u2212 y\u03bey) dxdy\n=\n\u222b\nR\n\u222b\nR\n\u03c6(x)\u03c6(y) log cosh(x\u03bex + \u03be\n2 x + y\u03bey + \u03be 2 y)\ncosh(x\u03bex + \u03be2x \u2212 y\u03bey \u2212 \u03be2y) dxdy.\nBut for any x\n\u2212 \u222b\nR\n\u03c6(x)\u03c6(y) log cosh(x\u03bex + \u03be 2 x \u2212 y\u03bey \u2212 \u03be2y)dy\n= \u2212 \u222b\nR\n\u03c6(x)\u03c6(\u2212y) log cosh(x\u03bex + \u03be2x + y\u03bey \u2212 \u03be2y)dy\n= \u2212 \u222b\nR\n\u03c6(x)\u03c6(y) log cosh(x\u03bex + \u03be 2 x + y\u03bey \u2212 \u03be2y)dy,\nthus,\nKL(P\u03b8, P\u03b8\u2032) =\n\u222b\nR\n\u222b\nR\n\u03c6(x)\u03c6(y) log cosh(x\u03bex + \u03be\n2 x + y\u03bey + \u03be 2 y)\ncosh(x\u03bex + \u03be2x + y\u03bey \u2212 \u03be2y) dxdy\n=\n\u222b\nR\n\u03c6(z) log cosh(z\n\u221a \u03be2x + \u03be 2 y + \u03be 2 x + \u03be 2 y)\ncosh(z \u221a \u03be2x + \u03be 2 y + \u03be 2 x \u2212 \u03be2y) dz\n=\n\u222b\nR\n\u03c6(z) log cosh(\u03bez + \u03be2x + \u03be 2 y)\ncosh(\u03bez + \u03be2x \u2212 \u03be2y) dz\nsince \u03be2x + \u03be 2 y = \u03be 2. By the mean value theorem and the fact that tanh is monotonically increasing,\nlog cosh(\u03bez + \u03be2x + \u03be 2 y)\ncosh(\u03bez + \u03be2x \u2212 \u03be2y) \u2264 2\u03be2y tanh(\u03bez + \u03be2x + \u03be2y)\n= 2\u03be2y tanh(\u03bez + \u03be 2)\nfor all z. Since tanh is an odd function,\nKL(P\u03b8 , P\u03b8\u2032) \u2264 2\u03be2y \u222b\nR\n\u03c6(z) tanh(\u03bez + \u03be2)dz\n= 2\u03be2y\n\u222b\nR\n\u03c6(z)(tanh(\u03bez + \u03be2)\u2212 tanh(\u03bez))dz.\nUsing the mean value theorem again,\ntanh(\u03bez + \u03be2)\u2212 tanh(\u03bez) \u2264 \u03be2 max x\u2208[\u03bez,\u03bez+\u03be2] (1\u2212 tanh2(x))\n\u2264 \u03be2\nfor all z, so\nKL(P\u03b8, P\u03b8\u2032) \u2264 2\u03be2\u03be2y = 2\u03be4 sin2 \u03b2\n2\n= \u03be4(1\u2212 cos\u03b2).\n9.5 Non-sparse lower bound\nTheorem 7. Assume that d \u2265 9 and \u03bb\u03c3 \u2264 0.2. Then\ninf F\u0302n sup \u03b8\u2208\u0398\u03bb\nE\u03b8L\u03b8(F\u0302n) \u2265 1\n500 min\n{\u221a log 2\n3\n\u03c32 \u03bb2 \u221a d\u2212 1 n , 1 4 } .\nProof. Let \u03be = \u03bb2\u03c3 , and define\n\u01eb = min\n{\u221a log 2\n3\n\u03c32\n\u03bb 1\u221a n ,\n\u03bb\n4 \u221a d\u2212 1\n} .\nDefine \u03bb20 = \u03bb 2 \u2212 (d\u2212 1)\u01eb2. Let \u2126 = {0, 1}d\u22121. For \u03c9 = (\u03c9(1), ..., \u03c9(d\u2212 1)) \u2208 \u2126, let \u00b5\u03c9 = \u03bb0ed + \u2211d\u22121 i=1 (2\u03c9(i)\u2212\n1)\u01ebei (where {ei}di=1 is the standard basis for Rd). Let \u03b8\u03c9 = ( \u2212\u00b5\u03c92 , \u00b5\u03c9 2 ) \u2208 \u0398\u03bb.\nBy Proposition 24,\nKL(P\u03b8\u03c9 , P\u03b8\u03bd ) \u2264 \u03be4(1 \u2212 cos\u03b2\u03c9,\u03bd)\nwhere\ncos\u03b2\u03c9,\u03bd = |\u00b5T\u03c9\u00b5\u03bd | \u03bb2 = 1\u2212 2\u03c1(\u03c9, \u03bd)\u01eb 2 \u03bb2\nand \u03c1 is the Hamming distance, so\nKL(P\u03b8\u03c9 , P\u03b8\u03bd ) \u2264 \u03be4 2\u03c1(\u03c9, \u03bd)\u01eb2\n\u03bb2\n\u2264 \u03be4 2(d\u2212 1)\u01eb 2\n\u03bb2 .\nBy Proposition 23, since cos\u03b2\u03c9,\u03bd \u2265 12 ,\nL\u03b8\u03c9(F\u03b8\u03bd ) \u2264 1\n\u03c0 tan\u03b2\u03c9,\u03bd\n\u2264 2 \u03c0 sin\u03b2\u03c9,\u03bd\n\u2264 4 \u03c0 \u221a d\u2212 1\u01eb \u03bb\nand\nL\u03b8\u03c9(F\u03b8\u03bd ) \u2265 2g(\u03be) sin\u03b2\u03c9,\u03bd cos\u03b2\u03c9,\u03bd \u2265 g(\u03be) sin\u03b2\u03c9,\u03bd\n\u2265 \u221a 2g(\u03be)\n\u221a \u03c1(\u03c9, \u03bd)\u01eb\n\u03bb\nwhere g(x) = \u03c6(x)(\u03c6(x) \u2212 x\u03a6(\u2212x)). By Lemma 5, there exist \u03c90, ..., \u03c9M \u2208 \u2126 such that M \u2265 2(d\u22121)/8 and\n\u03c1(\u03c9i, \u03c9j) \u2265 d\u2212 1 8 , \u2200 0 \u2264 i < j \u2264 M.\nFor simplicity of notation, let \u03b8i = \u03b8\u03c9i for all i \u2208 [0..M ]. Then, for i 6= j \u2208 [0..M ],\nKL(P\u03b8i , P\u03b8j ) \u2264 \u03be4 2(d\u2212 1)\u01eb2\n\u03bb2 ,\nand\nL\u03b8i(F\u03b8j ) \u2264 4\n\u03c0 \u221a d\u2212 1\u01eb \u03bb\nand\nL\u03b8i(F\u03b8j ) \u2265 1\n2 g(\u03be) \u221a d\u2212 1\u01eb \u03bb .\nDefine\n\u03b3 = 1\n4 (g(\u03be)\u2212 2\u03be2) \u221a d\u2212 1\u01eb \u03bb .\nThen for any i 6= j \u2208 [0..M ], and any F\u0302 such that L\u03b8i(F\u0302 ) < \u03b3,\nL\u03b8i(F\u03b8j ) + L\u03b8i(F\u0302 ) +\n\u221a KL(P\u03b8i , P\u03b8j )\n2 <\n( 4\n\u03c0 +\n1 4 (g(\u03be)\u2212 2\u03be2) + \u03be2 ) \u221a d\u2212 1\u01eb \u03bb \u2264 1 2\nbecause, for \u03be \u2264 0.1, by definition of \u01eb, ( 4\n\u03c0 +\n1 4 (g(\u03be)\u2212 2\u03be2) + \u03be2 ) \u221a d\u2212 1\u01eb \u03bb \u2264 2 \u221a d\u2212 1\u01eb \u03bb \u2264 1 2 .\nSo, by Proposition 22,\nL\u03b8j (F\u0302 ) \u2265 L\u03b8i(F\u03b8j )\u2212 L\u03b8i(F\u0302 )\u2212 \u221a KL(P\u03b8i , P\u03b8j )\n2 \u2265 \u03b3.\nAlso,\nmax i\u2208[M ]\nKL(P\u03b8i , P\u03b80) \u2264 (d\u2212 1)\u03be4 2\u01eb2\n\u03bb2\n\u2264 logM 9n\nbecause, by definition of \u01eb,\n\u03be4 2\u01eb2 \u03bb2 \u2264 log 2 72n .\nSo by Proposition 21 and the fact that \u03be \u2264 0.1,\ninf F\u0302n max i\u2208[0..M ]\nE\u03b8iL\u03b8i(F\u0302n) \u2265 0.07\u03b3\n= 0.07 1\n4 (g(\u03be)\u2212 2\u03be2) \u221a d\u2212 1\u01eb \u03bb\n\u2265 1 500 min\n{\u221a log 2\n3\n\u03c32 \u03bb2 \u221a d\u2212 1 n , 1 4 }\nand to complete the proof we use the fact that\ninf F\u0302n sup \u03b8\u2208\u0398\u03bb E\u03b8L\u03b8(F\u0302n) \u2265 inf F\u0302n max i\u2208[0..M ] E\u03b8iL\u03b8i(F\u0302n).\n9.6 Sparse lower bound\nTheorem 8. Assume that \u03bb\u03c3 \u2264 0.2, d \u2265 17, and\n5 \u2264 s \u2264 d\u2212 1 4 + 1.\nThen\ninf F\u0302n sup \u03b8\u2208\u0398\u03bb,s\nE\u03b8L\u03b8(F\u0302n) \u2265 1\n600 min\n{\u221a 8\n45\n\u03c32 \u03bb2 \u221a s\u2212 1 n log ( d\u2212 1 s\u2212 1 ) , 1 2 } .\nProof. For simplicity, we state this proof for \u0398\u03bb,s+1, assuming 4 \u2264 s \u2264 d\u221214 . Let \u03be = \u03bb2\u03c3 , and define\n\u01eb = min\n{\u221a 8\n45\n\u03c32\n\u03bb\n\u221a 1\nn log ( d\u2212 1 s ) , 1 2 \u03bb\u221a s } .\nDefine \u03bb20 = \u03bb 2 \u2212 s\u01eb2. Let \u2126 = {\u03c9 \u2208 {0, 1}d\u22121 : \u2016\u03c9\u20160 = s}. For \u03c9 = (\u03c9(1), ..., \u03c9(d \u2212 1)) \u2208 \u2126, let \u00b5\u03c9 =\n\u03bb0ed + \u2211d\u22121 i=1 \u03c9(i)\u01ebei (where {ei}di=1 is the standard basis for Rd). Let \u03b8\u03c9 = ( \u2212\u00b5\u03c92 , \u00b5\u03c9 2 ) \u2208 \u0398\u03bb,s.\nBy Proposition 24,\nKL(P\u03b8\u03c9 , P\u03b8\u03bd ) \u2264 \u03be4(1 \u2212 cos\u03b2\u03c9,\u03bd)\nwhere\ncos\u03b2\u03c9,\u03bd = |\u00b5T\u03c9\u00b5\u03bd | \u03bb2 = 1\u2212 \u03c1(\u03c9, \u03bd)\u01eb 2 2\u03bb2\nand \u03c1 is the Hamming distance, so\nKL(P\u03b8\u03c9 , P\u03b8\u03bd ) \u2264 \u03be4 \u03c1(\u03c9, \u03bd)\u01eb2\n2\u03bb2\n\u2264 \u03be4 s\u01eb 2\n\u03bb2 .\nBy Proposition 23, since cos\u03b2\u03c9,\u03bd \u2265 12 ,\nL\u03b8\u03c9(F\u03b8\u03bd ) \u2264 1\n\u03c0 tan\u03b2\u03c9,\u03bd\n\u2264 2 \u03c0 sin\u03b2\u03c9,\u03bd \u2264 2 \u221a 2\n\u03c0\n\u221a s\u01eb\n\u03bb\nand\nL\u03b8\u03c9(F\u03b8\u03bd ) \u2265 2g(\u03be) sin\u03b2\u03c9,\u03bd cos\u03b2\u03c9,\u03bd \u2265 g(\u03be) sin\u03b2\u03c9,\u03bd\n\u2265 g(\u03be)\u221a 2\n\u221a \u03c1(\u03c9, \u03bd)\u01eb\n\u03bb\nwhere g(x) = \u03c6(x)(\u03c6(x)\u2212 x\u03a6(\u2212x)). By Lemma 6, there exist \u03c90, ..., \u03c9M \u2208 \u2126 such that log(M +1) \u2265 s5 log ( d\u22121 s ) and \u03c1(\u03c9i, \u03c9j) \u2265 s\n2 , \u2200 0 \u2264 i < j \u2264 M.\nFor simplicity of notation, let \u03b8i = \u03b8\u03c9i for all i \u2208 [0..M ]. Then, for i 6= j \u2208 [0..M ],\nKL(P\u03b8i , P\u03b8j ) \u2264 \u03be4 s\u01eb2\n\u03bb2 ,\nand\nL\u03b8i(F\u03b8j ) \u2264 2 \u221a 2\n\u03c0\n\u221a s\u01eb\n\u03bb\nand\nL\u03b8i(F\u03b8j ) \u2265 g(\u03be)\n2\n\u221a s\u01eb\n\u03bb .\nDefine\n\u03b3 = 1\n4 (g(\u03be)\u2212\n\u221a 2\u03be2)\n\u221a s\u01eb\n\u03bb .\nThen for any i 6= j \u2208 [0..M ], and any F\u0302 such that L\u03b8i(F\u0302 ) < \u03b3,\nL\u03b8i(F\u03b8j ) + L\u03b8i(F\u0302 ) +\n\u221a KL(P\u03b8i , P\u03b8j )\n2 <\n( 2 \u221a 2\n\u03c0 +\n1 4 (g(\u03be)\u2212\n\u221a 2\u03be2) + \u03be2\u221a 2\n) \u221a s\u01eb\n\u03bb \u2264 1 2\nbecause, for \u03be \u2264 0.1, by definition of \u01eb, ( 2 \u221a 2\n\u03c0 +\n1 4 (g(\u03be)\u2212\n\u221a 2\u03be2) + \u03be2\u221a 2\n) \u221a s\u01eb\n\u03bb \u2264\n\u221a s\u01eb\n\u03bb \u2264 1 2 .\nSo, by Proposition 22,\nL\u03b8j (F\u0302 ) \u2265 L\u03b8i(F\u03b8j )\u2212 L\u03b8i(F\u0302 )\u2212 \u221a KL(P\u03b8i , P\u03b8j )\n2 \u2265 \u03b3.\nAlso,\nmax i\u2208[M ]\nKL(P\u03b8i , P\u03b80) \u2264 \u03be4 s\u01eb2\n\u03bb2\n\u2264 1 18n log ( d\u2212 1 s ) s 5\n\u2264 1 9n log (( d\u2212 1 s ) s 5 \u2212 1 )\n\u2264 logM 9n\nbecause, by definition of \u01eb,\n\u03be4 s\u01eb2 \u03bb2 \u2264 s 90n log ( d\u2212 1 s ) .\nSo by Proposition 21 and the fact that \u03be \u2264 0.1,\ninf F\u0302n max i\u2208[0..M ]\nE\u03b8iL\u03b8i(F\u0302n) \u2265 0.07\u03b3\n\u2265 0.070.1 4\n\u221a s\u01eb\n\u03bb\n\u2265 1 600 min\n{\u221a 8\n45\n\u03c32 \u03bb2\n\u221a s\nn log ( d\u2212 1 s ) , 1 2\n}\nand to complete the proof we use the fact that\ninf F\u0302n sup \u03b8\u2208\u0398\u03bb,s E\u03b8L\u03b8(F\u0302n) \u2265 inf F\u0302n max i\u2208[0..M ] E\u03b8iL\u03b8i(F\u0302n)."}], "references": [{"title": "On spectral learning of mixtures of distributions", "author": ["Dimitris Achlioptas", "Frank McSherry"], "venue": "In Learning Theory,", "citeRegEx": "Achlioptas and McSherry.,? \\Q2005\\E", "shortCiteRegEx": "Achlioptas and McSherry.", "year": 2005}, {"title": "Learning mixtures of arbitrary gaussians", "author": ["Sanjeev Arora", "Ravi Kannan"], "venue": "In Proceedings of the thirty-third annual ACM symposium on Theory of computing,", "citeRegEx": "Arora and Kannan.,? \\Q2001\\E", "shortCiteRegEx": "Arora and Kannan.", "year": 2001}, {"title": "Polynomial learning of distribution families", "author": ["Mikhail Belkin", "Kaushik Sinha"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Belkin and Sinha.,? \\Q2010\\E", "shortCiteRegEx": "Belkin and Sinha.", "year": 2010}, {"title": "Isotropic pca and affine-invariant clustering", "author": ["S Charles Brubaker", "Santosh S Vempala"], "venue": "In Building Bridges,", "citeRegEx": "Brubaker and Vempala.,? \\Q2008\\E", "shortCiteRegEx": "Brubaker and Vempala.", "year": 2008}, {"title": "Learning mixtures of gaussians using the k-means algorithm", "author": ["Kamalika Chaudhuri", "Sanjoy Dasgupta", "Andrea Vattani"], "venue": "arXiv preprint arXiv:0912.0086,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "Learning mixtures of gaussians", "author": ["Sanjoy Dasgupta"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Dasgupta.,? \\Q1999\\E", "shortCiteRegEx": "Dasgupta.", "year": 1999}, {"title": "Matrix Computations. Johns Hopkins Studies in the Mathematical Sciences", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "Golub and Loan.,? \\Q1996\\E", "shortCiteRegEx": "Golub and Loan.", "year": 1996}, {"title": "Pairwise variable selection for high-dimensional modelbased clustering", "author": ["Jian Guo", "Elizaveta Levina", "George Michailidis", "Ji Zhu"], "venue": null, "citeRegEx": "Guo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2010}, {"title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions", "author": ["Daniel Hsu", "Sham M Kakade"], "venue": "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,", "citeRegEx": "Hsu and Kakade.,? \\Q2013\\E", "shortCiteRegEx": "Hsu and Kakade.", "year": 2013}, {"title": "Disentangling gaussians", "author": ["Adam Tauman Kalai", "Ankur Moitra", "Gregory Valiant"], "venue": "Communications of the ACM,", "citeRegEx": "Kalai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2012}, {"title": "The spectral method for general mixture models", "author": ["Ravindran Kannan", "Hadi Salmasian", "Santosh Vempala"], "venue": "In Learning Theory,", "citeRegEx": "Kannan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2005}, {"title": "Concentration inequalities and model selection", "author": ["Pascal Massart"], "venue": null, "citeRegEx": "Massart.,? \\Q2007\\E", "shortCiteRegEx": "Massart.", "year": 2007}, {"title": "Penalized model-based clustering with application to variable selection", "author": ["Wei Pan", "Xiaotong Shen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Pan and Shen.,? \\Q2007\\E", "shortCiteRegEx": "Pan and Shen.", "year": 2007}, {"title": "Variable selection for model-based clustering", "author": ["Adrian E Raftery", "Nema Dean"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Raftery and Dean.,? \\Q2006\\E", "shortCiteRegEx": "Raftery and Dean.", "year": 2006}, {"title": "A two-round variant of em for gaussian mixtures", "author": ["Leonard J. Schulman", "Sanjoy Dasgupta"], "venue": "In Proc. 16th UAI (Conference on Uncertainty in Artificial Intelligence),", "citeRegEx": "Schulman and Dasgupta.,? \\Q2000\\E", "shortCiteRegEx": "Schulman and Dasgupta.", "year": 2000}, {"title": "Regularized k-means clustering of high-dimensional data and its asymptotic consistency", "author": ["Wei Sun", "Junhui Wang", "Yixin Fang"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Sun et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2012}, {"title": "Introduction to Nonparametric Estimation", "author": ["A.B. Tsybakov"], "venue": null, "citeRegEx": "Tsybakov.,? \\Q2009\\E", "shortCiteRegEx": "Tsybakov.", "year": 2009}, {"title": "A spectral algorithm for learning mixture models", "author": ["Santosh Vempala", "Grant Wang"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Vempala and Wang.,? \\Q2004\\E", "shortCiteRegEx": "Vempala and Wang.", "year": 2004}, {"title": "Minimax sparse principal subspace estimation in high dimensions", "author": ["Vincent Q Vu", "Jing Lei"], "venue": "arXiv preprint arXiv:1211.0373,", "citeRegEx": "Vu and Lei.,? \\Q2012\\E", "shortCiteRegEx": "Vu and Lei.", "year": 2012}, {"title": "A framework for feature selection in clustering", "author": ["Daniela M Witten", "Robert Tibshirani"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Witten and Tibshirani.,? \\Q2010\\E", "shortCiteRegEx": "Witten and Tibshirani.", "year": 2010}, {"title": "2, where \u03c1 denotes the Hamming distance between two vectors (Tsybakov", "author": [], "venue": "Let \u03a9 = {\u03c9 \u2208 {0,", "citeRegEx": "\u2265,? \\Q2009\\E", "shortCiteRegEx": "\u2265", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Pairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001).", "startOffset": 34, "endOffset": 50}, {"referenceID": 0, "context": "Pairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001).", "startOffset": 34, "endOffset": 80}, {"referenceID": 0, "context": "Pairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001). These methods require the mean separation to increase with dimension.", "startOffset": 84, "endOffset": 108}, {"referenceID": 0, "context": "Pairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001). These methods require the mean separation to increase with dimension. The first one requires the separation to be \u221a d while the latter two improve it to d. To avoid this problem, Vempala and Wang (2004) introduced the idea of using spectral methods for estimating mixtures of spherical Gaussians which makes mean separation independent of dimension.", "startOffset": 84, "endOffset": 312}, {"referenceID": 0, "context": "Pairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001). These methods require the mean separation to increase with dimension. The first one requires the separation to be \u221a d while the latter two improve it to d. To avoid this problem, Vempala and Wang (2004) introduced the idea of using spectral methods for estimating mixtures of spherical Gaussians which makes mean separation independent of dimension. The assumption that the components are spherical was removed in Brubaker and Vempala (2008). Their method only requires the components to be separated by a hyperplane and runs in polynomial time, but requires n = \u03a9(d log d) samples.", "startOffset": 84, "endOffset": 551}, {"referenceID": 0, "context": "Pairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001). These methods require the mean separation to increase with dimension. The first one requires the separation to be \u221a d while the latter two improve it to d. To avoid this problem, Vempala and Wang (2004) introduced the idea of using spectral methods for estimating mixtures of spherical Gaussians which makes mean separation independent of dimension. The assumption that the components are spherical was removed in Brubaker and Vempala (2008). Their method only requires the components to be separated by a hyperplane and runs in polynomial time, but requires n = \u03a9(d log d) samples. Other spectral methods include Kannan et al. (2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013).", "startOffset": 84, "endOffset": 744}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013).", "startOffset": 8, "endOffset": 39}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm.", "startOffset": 8, "endOffset": 65}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components.", "startOffset": 8, "endOffset": 203}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al.", "startOffset": 8, "endOffset": 376}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians.", "startOffset": 8, "endOffset": 401}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting \u03bc > 1, Chaudhuri et al. (2009) show that n = \u03a9\u0303(d/\u03bc) samples are needed.", "startOffset": 8, "endOffset": 547}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting \u03bc > 1, Chaudhuri et al. (2009) show that n = \u03a9\u0303(d/\u03bc) samples are needed. They also provide an information theoretic bound on the necessary sample complexity of any algorithm which matches the sample complexity of their method (up to log factors) in d and \u03bc. When the mean separation is small \u03bc < 1, they show that n = \u03a9\u0303(d/\u03bc) samples are sufficient for accurate estimation. Our results for the small mean separation setting provide a matching necessary condition. Most of these papers are concerned with computational efficiency and do not give precise, statistical minimax upper and lower bounds. None of them deal with the case we are interested in, namely, a high dimensional mixture with sparse mean separation. We should also point out that the results in different papers are not necessarily comparable since different authors use different loss functions. In this paper we use the probability of misclassifying a future observation, relative to how the correct distribution clusters the observation, as our loss function. This should not be confused with the probability of attributing a new observation to the wrong component of the mixture. The latter loss does not to tend to zero as the sample size increases. Our loss is similar to the excess risk used in classification where we compare the misclassification rate of a classifier to the misclassification rate of the Bayes optimal classifier. Finally, we remind the reader that our motivation for studying sparsely separated mixtures is that this provides a model for variable selection in clustering problems. There are some relevant recent papers on this problem in the high-dimensional setting. Pan and Shen (2007) use penalized mixture models to do variable selection and clustering simultaneously.", "startOffset": 8, "endOffset": 2197}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting \u03bc > 1, Chaudhuri et al. (2009) show that n = \u03a9\u0303(d/\u03bc) samples are needed. They also provide an information theoretic bound on the necessary sample complexity of any algorithm which matches the sample complexity of their method (up to log factors) in d and \u03bc. When the mean separation is small \u03bc < 1, they show that n = \u03a9\u0303(d/\u03bc) samples are sufficient for accurate estimation. Our results for the small mean separation setting provide a matching necessary condition. Most of these papers are concerned with computational efficiency and do not give precise, statistical minimax upper and lower bounds. None of them deal with the case we are interested in, namely, a high dimensional mixture with sparse mean separation. We should also point out that the results in different papers are not necessarily comparable since different authors use different loss functions. In this paper we use the probability of misclassifying a future observation, relative to how the correct distribution clusters the observation, as our loss function. This should not be confused with the probability of attributing a new observation to the wrong component of the mixture. The latter loss does not to tend to zero as the sample size increases. Our loss is similar to the excess risk used in classification where we compare the misclassification rate of a classifier to the misclassification rate of the Bayes optimal classifier. Finally, we remind the reader that our motivation for studying sparsely separated mixtures is that this provides a model for variable selection in clustering problems. There are some relevant recent papers on this problem in the high-dimensional setting. Pan and Shen (2007) use penalized mixture models to do variable selection and clustering simultaneously. Witten and Tibshirani (2010) develop a penalized version of k-means clustering.", "startOffset": 8, "endOffset": 2311}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting \u03bc > 1, Chaudhuri et al. (2009) show that n = \u03a9\u0303(d/\u03bc) samples are needed. They also provide an information theoretic bound on the necessary sample complexity of any algorithm which matches the sample complexity of their method (up to log factors) in d and \u03bc. When the mean separation is small \u03bc < 1, they show that n = \u03a9\u0303(d/\u03bc) samples are sufficient for accurate estimation. Our results for the small mean separation setting provide a matching necessary condition. Most of these papers are concerned with computational efficiency and do not give precise, statistical minimax upper and lower bounds. None of them deal with the case we are interested in, namely, a high dimensional mixture with sparse mean separation. We should also point out that the results in different papers are not necessarily comparable since different authors use different loss functions. In this paper we use the probability of misclassifying a future observation, relative to how the correct distribution clusters the observation, as our loss function. This should not be confused with the probability of attributing a new observation to the wrong component of the mixture. The latter loss does not to tend to zero as the sample size increases. Our loss is similar to the excess risk used in classification where we compare the misclassification rate of a classifier to the misclassification rate of the Bayes optimal classifier. Finally, we remind the reader that our motivation for studying sparsely separated mixtures is that this provides a model for variable selection in clustering problems. There are some relevant recent papers on this problem in the high-dimensional setting. Pan and Shen (2007) use penalized mixture models to do variable selection and clustering simultaneously. Witten and Tibshirani (2010) develop a penalized version of k-means clustering. Related methods include Raftery and Dean (2006); Sun et al.", "startOffset": 8, "endOffset": 2410}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting \u03bc > 1, Chaudhuri et al. (2009) show that n = \u03a9\u0303(d/\u03bc) samples are needed. They also provide an information theoretic bound on the necessary sample complexity of any algorithm which matches the sample complexity of their method (up to log factors) in d and \u03bc. When the mean separation is small \u03bc < 1, they show that n = \u03a9\u0303(d/\u03bc) samples are sufficient for accurate estimation. Our results for the small mean separation setting provide a matching necessary condition. Most of these papers are concerned with computational efficiency and do not give precise, statistical minimax upper and lower bounds. None of them deal with the case we are interested in, namely, a high dimensional mixture with sparse mean separation. We should also point out that the results in different papers are not necessarily comparable since different authors use different loss functions. In this paper we use the probability of misclassifying a future observation, relative to how the correct distribution clusters the observation, as our loss function. This should not be confused with the probability of attributing a new observation to the wrong component of the mixture. The latter loss does not to tend to zero as the sample size increases. Our loss is similar to the excess risk used in classification where we compare the misclassification rate of a classifier to the misclassification rate of the Bayes optimal classifier. Finally, we remind the reader that our motivation for studying sparsely separated mixtures is that this provides a model for variable selection in clustering problems. There are some relevant recent papers on this problem in the high-dimensional setting. Pan and Shen (2007) use penalized mixture models to do variable selection and clustering simultaneously. Witten and Tibshirani (2010) develop a penalized version of k-means clustering. Related methods include Raftery and Dean (2006); Sun et al. (2012) and Guo et al.", "startOffset": 8, "endOffset": 2429}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting \u03bc > 1, Chaudhuri et al. (2009) show that n = \u03a9\u0303(d/\u03bc) samples are needed. They also provide an information theoretic bound on the necessary sample complexity of any algorithm which matches the sample complexity of their method (up to log factors) in d and \u03bc. When the mean separation is small \u03bc < 1, they show that n = \u03a9\u0303(d/\u03bc) samples are sufficient for accurate estimation. Our results for the small mean separation setting provide a matching necessary condition. Most of these papers are concerned with computational efficiency and do not give precise, statistical minimax upper and lower bounds. None of them deal with the case we are interested in, namely, a high dimensional mixture with sparse mean separation. We should also point out that the results in different papers are not necessarily comparable since different authors use different loss functions. In this paper we use the probability of misclassifying a future observation, relative to how the correct distribution clusters the observation, as our loss function. This should not be confused with the probability of attributing a new observation to the wrong component of the mixture. The latter loss does not to tend to zero as the sample size increases. Our loss is similar to the excess risk used in classification where we compare the misclassification rate of a classifier to the misclassification rate of the Bayes optimal classifier. Finally, we remind the reader that our motivation for studying sparsely separated mixtures is that this provides a model for variable selection in clustering problems. There are some relevant recent papers on this problem in the high-dimensional setting. Pan and Shen (2007) use penalized mixture models to do variable selection and clustering simultaneously. Witten and Tibshirani (2010) develop a penalized version of k-means clustering. Related methods include Raftery and Dean (2006); Sun et al. (2012) and Guo et al. (2010). The applied bioinformatics literature also contains a huge number of heuristic methods for this problem.", "startOffset": 8, "endOffset": 2451}, {"referenceID": 18, "context": "We conjecture that the lower bound is tight and that the gap could be closed by using a sparse principal component method as in Vu and Lei (2012) to find the relevant features.", "startOffset": 128, "endOffset": 146}, {"referenceID": 16, "context": ", 0), \u03c1(\u03c9i, \u03c9j) \u2265 m8 for all 0 \u2264 i < j \u2264 M , and M \u2265 2, where \u03c1 denotes the Hamming distance between two vectors (Tsybakov (2009)).", "startOffset": 114, "endOffset": 130}, {"referenceID": 11, "context": ", \u03c9M \u2208 \u03a9 such that \u03c1(\u03c9i, \u03c9j) > s/2 for all 0 \u2264 i < j \u2264 M , and log(M + 1) \u2265 s 5 log ( m s ) (Massart (2007), Lemma 4.", "startOffset": 93, "endOffset": 108}, {"referenceID": 11, "context": "Existing methods such as Pan and Shen (2007); Witten and Tibshirani (2010); Raftery and Dean (2006); Sun et al.", "startOffset": 25, "endOffset": 45}, {"referenceID": 11, "context": "Existing methods such as Pan and Shen (2007); Witten and Tibshirani (2010); Raftery and Dean (2006); Sun et al.", "startOffset": 25, "endOffset": 75}, {"referenceID": 11, "context": "Existing methods such as Pan and Shen (2007); Witten and Tibshirani (2010); Raftery and Dean (2006); Sun et al.", "startOffset": 25, "endOffset": 100}, {"referenceID": 11, "context": "Existing methods such as Pan and Shen (2007); Witten and Tibshirani (2010); Raftery and Dean (2006); Sun et al. (2012) and Guo et al.", "startOffset": 25, "endOffset": 119}, {"referenceID": 7, "context": "(2012) and Guo et al. (2010) provide promising numerical evidence that variable selection does improve high dimensional clustering.", "startOffset": 11, "endOffset": 29}, {"referenceID": 16, "context": "(Tsybakov (2009)).", "startOffset": 1, "endOffset": 17}, {"referenceID": 16, "context": "and M \u2265 2, where \u03c1 denotes the Hamming distance between two vectors (Tsybakov (2009)).", "startOffset": 69, "endOffset": 85}, {"referenceID": 11, "context": "In particular, setting \u03b1 = 3/4 and \u03b2 = 1/3, we have that \u03c1(\u03c9i, \u03c9j) > s/2, log(M + 1) \u2265 s 5 log ( m s ) , as long as s \u2264 m/4 (Massart (2007), Lemma 4.", "startOffset": 125, "endOffset": 140}], "year": 2013, "abstractText": "While several papers have investigated computationally and statistically efficient methods for learning Gaussian mixtures, precise minimax bounds for their statistical performance as well as fundamental limits in high-dimensional settings are not well-understood. In this paper, we provide precise information theoretic bounds on the clustering accuracy and sample complexity of learning a mixture of two isotropic Gaussians in high dimensions under small mean separation. If there is a sparse subset of relevant dimensions that determine the mean separation, then the sample complexity only depends on the number of relevant dimensions and mean separation, and can be achieved by a simple computationally efficient procedure. Our results provide the first step of a theoretical basis for recent methods that combine feature selection and clustering.", "creator": "LaTeX with hyperref package"}}}