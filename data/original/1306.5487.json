{"id": "1306.5487", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2013", "title": "Model Reframing by Feature Context Change", "abstract": "The feature space (including both input and output variables) characterises a data mining problem. In predictive (supervised) problems, the quality and availability of features determines the predictability of the dependent variable, and the performance of data mining models in terms of misclassification or regression error. Good features, however, are usually difficult to obtain. It is usual that many instances come with missing values, either because the actual value for a given attribute was not available or because it was too expensive. This is usually interpreted as a utility or cost-sensitive learning dilemma, in this case between misclassification (or regression error) costs and attribute tests costs. Both misclassification cost (MC) and test cost (TC) can be integrated into a single measure, known as joint cost (JC). We introduce methods and plots (such as the so-called JROC plots) that can work with any of-the-shelf predictive technique, including ensembles, such that we re-frame the model to use the appropriate subset of attributes (the feature configuration) during deployment time. In other words, models are trained with the available attributes (once and for all) and then deployed by setting missing values on the attributes that are deemed ineffective for reducing the joint cost. As the number of feature configuration combinations grows exponentially with the number of features we introduce quadratic methods that are able to approximate the optimal configuration and model choices, as shown by the experimental results.", "histories": [["v1", "Sun, 23 Jun 2013 23:36:40 GMT  (2460kb,D)", "http://arxiv.org/abs/1306.5487v1", "MSc Thesis, 126 pages"]], "COMMENTS": "MSc Thesis, 126 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["celestine-periale maguedong-djoumessi"], "accepted": false, "id": "1306.5487"}, "pdf": {"name": "1306.5487.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Maguedong Djoumessi", "Celestine Periale", "Marie Julienne"], "emails": [], "sections": [{"heading": null, "text": "Ma\u0301ster Universitario en Ingenier\u0301\u0131a del Software, Me\u0301todos Formales y Sistemas de Informacio\u0301n\nMSc in Software Engineering, Formals Methods and Information Systems\nMODEL REFRAMING BY FEATURE CONTEXT\nCHANGE\nMaguedong Djoumessi Celestine Periale\nDirigida por: / Supervised by:\nJose\u0301 Herna\u0301ndez Orallo\nValencia, Julio de 2013 / Valencia, July 2013\nar X\niv :1\n30 6.\n54 87\nv1 [\ncs .L\nG ]\n2 3\nJu n\n20 13\nIn love memory of my Mum:\nDonfack Marie Julienne\nAbstract\nMany solutions to cost-sensitive classification (and regression) rely on some or all of the following assumptions: we have complete knowledge about the cost context at training time, we can easily re-train whenever the cost context changes, and we have technique-specific methods (such as cost-sensitive decision trees) that can take advantage of that information. In this work we address the problem of selecting models and minimising joint cost (integrating both misclassification cost and test costs) without any of the above assumptions. We introduce methods and plots (such as the so-called JROC plots) that can work with any off-the-shelf predictive technique, including ensembles, such that we re-frame the model to use the appropriate subset of attributes (the feature configuration) during deployment time. In other words, models are trained with the available attributes (once and for all) and then deployed by setting missing values on the attributes that are deemed ineffective for reducing the joint cost. As the number of feature configuration combinations grows exponentially with the number of features we introduce quadratic methods that are able to approximate the optimal configuration and model choices, as shown by the experimental results.\nKeywords: test cost, misclassification cost, missing values, re-framing, ROC analysis, operating context, feature configuration, feature selection.\ni\nResumen\nMuchas de las soluciones para la clasificacio\u0301n y regresio\u0301n sensible al coste se basan en alguna de las siguientes hipo\u0301tesis: que tenemos un conocimiento completo sobre el contexto de coste en tiempo de entrenamiento, que podemos volver a entrenar con facilidad cada vez que cambia el contexto de costes, y que tenemos los me\u0301todos para una te\u0301cnica especifica (tales como a\u0301rboles de decisiones sensibles a los costes) que pueden aprovechar esa informacio\u0301n. En este trabajo se aborda el problema de la seleccio\u0301n de modelos y la minimizacio\u0301n de los costes conjuntos (integrando tanto el coste de clasificacio\u0301n erro\u0301nea como los costes de pruebas de atributos) sin ninguno de los supuestos anteriores. Introducimos me\u0301todos y gra\u0301ficos (como los gra\u0301ficos JROC) que pueden funcionar con cualquier te\u0301cnica predictiva comu\u0301n, incluyendo ensembles, de tal manera que nos adapta el modelo para el subconjunto apropiado de atributos (la configuracio\u0301n de los atributos) durante el tiempo de despliegue. En otras palabras, los modelos son entrenados con los atributos disponibles (una vez y para siempre) y luego desplegados mediante el establecimiento de valores faltantes en los atributos que se consideran ineficaces para reducir el conjunto. Como el nu\u0301mero de combinaciones de los atributos crece exponencialmente con el nu\u0301mero de atributos se introducira\u0301n me\u0301todos cuadra\u0301ticos que son capaces de aproximar la opcio\u0301n de configuracio\u0301n o\u0301ptima y el modelo o\u0301ptimo, como se muestra con los resultados experimentales.\nKeywords: costes de test de atributos, costes de clasificacio\u0301n erronea, valores faltantes, reframing, ana\u0301lisis ROC, contexto de operacio\u0301n, configuracio\u0301nn de atributos.\nii\nRemerciements\nAu terme de ce si riche parcours, je tiens a\u0300 rendre tout d\u2019abord grace a\u0300 Dieu sans qui rien n\u2019aurait e\u0301te\u0301 possible et aussi a\u0300 remercier toutes ces personnes qui ont su m\u2019accompagner et m\u2019apporter tout le soutien morale, physique et financier necessaire pour y arriver. Je pense ici a\u0300:\n\u2022 L\u2019ame de ma tre\u0301s che\u0300re maman DONFACK Marie-Julienne, qui a donne\u0301 jusqu\u2019a\u0300 la dernie\u0300re de ses forces pour que je puisse recevoir une e\u0301ducation de qualite\u0301. Merci, mille merci\nmaman!\n\u2022 Mon papa et fre\u0300res et soeurs qui ont e\u0301te\u0301 d\u2019un tres grand soutien moral a\u0300 chaque fois que j\u2019en avais besoin.\n\u2022 Mon parrain et pa\u2019a Jose Antonio Varela Ferrandis qui m\u2019a accompagne\u0301 et \u201cprotege\u0301\u201d tout au long de mon sejour a\u0300 Valencia.\n\u2022 A toutes mes mamis et papis de Benimaclet qui par leur amour mon procure\u0301 une veritable famille en Espagne.\nA tous ceux-ci ainsi qu\u2019a\u0300 tous les anonymes qui te pres ou de loin et d\u2019une manie\u0300re ou d\u2019une autre ont contribue\u0301 a\u0300 ce que je puisse arriver au terme de ce parcours, je dirige mes since\u0300res remerciements.\niii\nAgradecimientos\nAl final de este curso tan rico, quiero en primer lugar dar las gracias a Dios, sin el nada hubiera sido posible, y tambie\u0301n agradecer a todas aquellas personas que pudieron que me han acompan\u0303ado y me han dado todo el apoyo moral, fsico y financiero necesario para llegar hasta aqu\u0301\u0131. Pienso:\n\u2022 En el alma de mi muy querida madre Donfack Marie Julienne, que entrego\u0301 hasta el u\u0301ltimo extremo de su fuerza para que puediera recibir una educacio\u0301n de calidad. Gracias, mil\ngracias mama\u0301!\n\u2022 A mi padre y mis hermanos y hermanas que han sido de un gran apoyo moral cada vez que lo necesitaba.\n\u2022 Mi padrino y pa\u2019a Jose\u0301 Antonio Varela Ferrandis que me ha acompan\u0303ado y protegido durante toda mi estancia en Valencia.\n\u2022 A todos mis mamis y papis de Benimaclet que por su amor me han ofrecido una familia de verdad en Espan\u0303a.\n\u2022 A Jose\u0301 Herna\u0301ndez Orallo, que con la calidez de sus guiones, su paciencia y devocio\u0301n me ha dirigido durante todo el desarollo de mi tesis.\n\u2022 A la Universidad Politecnica de Valencia, a todos mis profesores. Con la calidez de su ensen\u0303anza, salgo de este curso grata y llena de nuevos conocimientos.\n\u2022 A mis compan\u0303eros de la universidad y de casa, que me han ayudado en adaptarme y integrarme en la cultura espan\u0303ola.\nA todos ellos, y todos los que no he nombrado que de lejos o de cerca, de una manera u otra, han contribuido a lo que puedo llegar hasta el final de este curso, dirijo mis ma\u0301s sinceros agradecimientos.\niv\nCelestine P. MAGUEDONG D.\nValencia, 2013\nv\nContents"}, {"heading": "1 Introduction 1", "text": "1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2 Preview and others approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.3 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.4 Thesis plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5"}, {"heading": "2 Previous Works 6", "text": "2.1 Machine learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.2 Features selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.3 Missing attributes: a challenge . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.3.1 Missing attribute imputation . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.4 Cost-sensitive methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.4.1 Cost-sensitive decision trees . . . . . . . . . . . . . . . . . . . . . . . . . . 9\nvi\n2.5 Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.5.1 R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.5.2 RWeka . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.5.3 Weka algorithms used in this work . . . . . . . . . . . . . . . . . . . . . . 11\n2.6 Classifier visualisation and evaluation . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.6.1 ROC Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.6.2 Classifiers evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13"}, {"heading": "3 Reframing the model with missing values on purpose 14", "text": "3.1 Definition of the approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3.2 Reframing the model with missing values on purpose . . . . . . . . . . . . . . . 17\n3.3 The MC/TC tradeoff: JROC plots . . . . . . . . . . . . . . . . . . . . . . . . . . 22"}, {"heading": "4 Approximating the JROC hull 30", "text": ""}, {"heading": "5 Experiments 36", "text": "5.1 Uniform context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n5.2 Variable context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39"}, {"heading": "6 Conclusions and future works 44", "text": "6.1 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n6.2 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\nvii\n6.3 Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n6.4 Dissemination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\nAppendices 48"}, {"heading": "A Details of the results 49", "text": "B Implementation details 53\nB.1 Function definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\nB.2 Main code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\nB.3 BMC, BTC, BJC, RND methods implementation: . . . . . . . . . . . . . . . . . 68\nB.4 Plots representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\nB.5 Experiment implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\nB.5.1 Statistical tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\nBibliography 109\nviii\nList of Tables\n5.1 Description of the datasets used in the experiments. . . . . . . . . . . . . . . . . 36\n5.2 JC results (mean and standard deviation) of the 20 results (5 values of alpha with\n4 repetitions) for each of the 5 methods (columns: Full, BMC, BTC, BJC, RND) and each of the 6 datasets (rows: 1 to 6), with the uniform context. . . . . . . . 38\n5.3 JC results (mean and standard deviation) of the 24 results (6 datasets with 4\nrepetitions) for each of the 5 methods (columns: Full, BMC, BTC, BJC, RND) and each of the 5 possible values of \u03b1 (rows 0.1 to 0.9), with the uniform context. 38\n5.4 This figure shows the JC means for the 4 repetitions for each of the 5 methods\n(Full, BMC, BTC, BJC, RND). . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n5.5 JC results (mean and standard deviation) of the 20 results (5 values of alpha with\n4 repetitions) for each of the 5 methods (columns: Full, BMC, BTC, BJC, RND) and each of the 6 datasets (rows: 1 to 6), with the variable context. . . . . . . . 41\n5.6 JC results (mean and standard deviation) of the 24 results (6 datasets with 4\nrepetitions) for each of the 5 methods (columns: Full, BMC, BTC, BJC, RND) and each of the 5 possible values of \u03b1 (rows 0.1 to 0.9), with the variable context. 41\n5.7 This figure shows the JC means for the 4 repetitions for each of the 5 methods\n(Full, BMC, BTC, BJC, RND). . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\nix\nList of Figures\n3.1 Evolution of accuracy according to attribute selection for three models: decision\ntrees, SMO and kNN. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n3.2 Evolution of MC, TC and JC according to attribute selection for a SMO (SVM)\nmodel using the uniform context \u03b8U . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n3.3 Evolution of MC, TC and JC according to attribute selection for a SMO (SVM)\nmodel using non-uniform context. . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.4 JROC plots for the three models: decision trees, SMO and IBk using the uniform\noperating context\u03b8U . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n3.5 JROC plots for the three models: decision trees, SMO and IBk using the non-\nuniform operating context. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.6 JROC plot with isometrics representation for operating condition \u03b1 = 0.03, \u03b1 =\n0.5 and \u03b1 = 0.9. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.7 Convex hulls. Up: iris dataset with the operating context \u03b81. Down: Pima Indian\ndiabetes dataset with the operating context \u03b82. . . . . . . . . . . . . . . . . . . . 29\n4.1 Hull of the combinations selected by the method BMC. Compare with Convex\nHull. Up: iris dataset with the operating context \u03b81. Down: Pima Indian diabetes dataset with the operating context \u03b82. . . . . . . . . . . . . . . . . . . . . . . . . 32\nx\n4.2 Hull of the combinations selected by the method BTC. Compare with Convex\nHull. Up: iris dataset with the operating context \u03b81. Down: Pima Indian diabetes dataset with the operating context \u03b82. . . . . . . . . . . . . . . . . . . . . . . . . 33\n4.3 Hull of the combinations selected by the method BJC. Compare with Convex\nHull. Up: iris dataset with the operating context \u03b81. Down: Pima Indian diabetes dataset with the operating context \u03b82. . . . . . . . . . . . . . . . . . . . . . . . . 34\n4.4 Hull of the combinations selected by the method RND. Compare with Convex\nHull. Up: iris dataset with the operating context \u03b81. Down: Pima Indian diabetes dataset with the operating context \u03b82. . . . . . . . . . . . . . . . . . . . . . . . . 35\nA.1 Evolution of MC, TC and JC according to attribute selection for a IBk (KNN)\nmodel using uniform context. Left: Iris dataset, Right: Diabetes dataset . . . . . 50\nA.2 Evolution of MC, TC and JC according to attribute selection for a IBk (KNN)\nmodel using non-uniform context. Left: Iris dataset, Right: Diabetes dataset . . 50\nA.3 Evolution of MC, TC and JC according to attribute selection for a Adaboost\nmodel using uniform context. Left: Iris dataset, Right: Diabetes dataset . . . . . 51\nA.4 Evolution of MC, TC and JC according to attribute selection for a Adaboost\nmodel using non-uniform context. Left: Iris dataset, Right: Diabetes dataset . . 51\nA.5 Evolution of MC, TC and JC according to attribute selection for a Bagging\nmodel using uniform context. Left: Iris dataset, Right: Diabetes dataset . . . . . 52\nA.6 Evolution of MC, TC and JC according to attribute selection for a Bagging\nmodel using non-uniform context. Left: Iris dataset, Right: Diabetes dataset . . 52\nxi"}, {"heading": "1 Introduction", "text": "The main statement of this thesis is about machine learning (ML)/data mining (DM) \u2014 especially about cost-sensitive learning\u2014 which consist in extracting useful knowledge from large and detailed collections of data. Because of the wide range of ways in which companies and institutions can easily, efficiently and cheaply collect data (e.g. web, social networks . . . ), this subject has become of increasing importance and interest. This interest has inspired a rapidly developing research field with developments both on a theoretical, as well as on a practical level with the availability of many commercial and non-commercial tools. In fact, data mining has many advantages across different industries, allowing large historical data to be used as the background for prediction. The interpretation and evaluation of patterns obtained by data mining produces new knowledge that decision-makers can use. Thus, ML/DM provide a way to extract knowledge and obtain information that can be useful for prediction and be used as a support in a decision making. Unfortunately, in data mining approaches, two important assumptions slow the good application of those technology. first the context in which data are obtained can change, for instance, when the features used for describing a patient in a hospital, can be different in one year to the other. Second, the cost to obtain those data can be very high sometimes. As a result, many approaches produce knowledge which is not well adapted to the context or that is not exhaustive. Those limitations have generated a relatively recent interest in richer and data mining approaches in order to improve, more, optimise the result when contexts change.\n1"}, {"heading": "1.1 Motivation", "text": "Reuse of learnt knowledge is of critical importance in the majority of knowledge-intensive application areas, particularly because the operating context can be expected to vary from training to deployment. In machine learning this is most commonly studied in relation to variations in class and cost skew in classification. While this is evidently useful in many practical situations, there is a clear and pressing need to generalise the notion of operating context beyond the narrow framework of skew-sensitive and cost-sensitive classification. The approach is based around the new notion of model reframing, which can be applied to inputs (features), outputs (predictions) or parts of models (patterns), in this way generalising, integrating and broadening the more traditional and diverse notions of model adjustment in machine learning and data mining. These ideas have led to the following project:\nwww.reframe-d2k.org\nOne kind of context in the above project is related to the way inputs (i.e., features) can vary from training to deployment. Among these changes, we can mention that some attributes can disappear or may have different application costs.\nFor instance, a model M can be learnt from a dataset features X = (x1, x2, . . . , xm) and output attribute Y. On deployment time, we may find that we only have a (possible much smaller) dataset of features X'= (x1', x2', . . . , xm'). If all the features in X \u2032 are also in X, there is no problem for applying M . But even in this case, some attributes may have higher costs than others on deployment time. On other occasions some features may be not present. In these cases we may use some information relating attributes in X with attributes in X'. For instance, x2 may not be present in X', but we may know that the correlation of x2 and X \u2032 is high, so one could be used instead of the other. For nominal attributes, we may have functional dependencies or association rules. Also, we may have some kind of hierarchy of X using Principal Component Analysis, or other kind of information that helps us to relate X and X', to be able to apply M with X'. This information can be used in the reframing process or can be embedded in M as a more versatile model, possibly taking a more general notion of feature, or using attribute selection lattices. A simpler approach is to consider that those features of X which are not in X \u2032 can be just processed as missing values. In fact, we can also use missing values for existing attributes if the attributes test cost is too high compared to what we gain by using the attribute.\n2"}, {"heading": "1.2 Preview and others approaches", "text": "The feature space (including both input and output variables) characterises a data mining problem [34]. In predictive (supervised) problems, the quality and availability of features determines the predictability of the dependent variable, and the performance of data mining models in terms of misclassifcation or regression error. Good features, however, are usually difficult to obtain. It is usual that many instances come with missing values, either because the actual value for a given attribute was not available or because it was too expensive (e.g., in medical domains, where attributes usually correspond to diagnostic tests). This is usually interpreted as a utility or cost-sensitive learning dilemma [43, 15], in this case between misclassification (or regression error) costs and attribute tests costs. Both misclassification cost (MC) and test cost (TC) can be integrated into a single measure, known as joint cost (JC).\nOne possible option to affront this dilemma is known as missing value imputation [46], but this approach is not usually appropriate when test costs are considered. First, expensive attributes (e.g., in diagnosis) are usually missing for many other instances as well and it is difficulty to infer from other instances or attributes. Second, imputing missing values \u201cis regarded as unnecessary for cost-sensitive learning that also considers the test costs\u201d [45].\nThe most common option is usually to train models that are able to do reasonably good predictions with the available attributes or to find a trade-off (in terms of minimising joint cost) about how many (and which) attributes need to be used. Retraining with the necessary attributes (or with feature selection) does not seem to be a good option, because for n attributes we typically have 2n possible combinations. Also, only using the training instances for which the same subset of attributes is available can bias each \u2018partial\u2019 model. As a result, one common option is to use techniques that lead to models that can use any subset of attributes. Decision trees are the usual choice [35, 45, 36] because the use of attributes can be customised in many different ways to (for example taking to consideration missing values without imputing them or considering misclassification cost). But this alternative limits the option of machine learning techniques to be used (from now limited to the decision trees). We could also try \u2014 if not already done \u2014 to design cost-sensitive versions for many other families of techniques, such as Bayesian models, neural networks, logistic regression, kernel methods, etc., with varying success. This would lead to two problems. On one hand, we would need to have a library of specific cost-sensitive algorithms for classification and regression, which would also limit our range of options and the use of the ultimate learning techniques (until cost-sensitive versions appear and are implemented). On the other hand, even assuming that this is possible, we would require\n3\nsome tools to properly select which model is better, as we do not know in advance what the misclassification (or regression error) cost and test cost configuration is. In fact, each instance may have a different subset of missing values and a different cost configuration, so this choice must be very specific.\nIn this work, we explore an alternative, more general approach using off-the-shelf machine learning methods. The procedure is simple: we use any data mining technique that accepts missing values during training and prediction and learn a predictive model with our training data as usual. Then, we evaluate the model (on a validation dataset) by exploring the lattice of attribute subsets, with a very straightforward mechanism: we set missing values on purpose for each combination in the lattice. From here, we know how well our model behaves for any attribute subset. From here, once the model needs to be deployed on unlabelled data, whenever a new instance appears (with a possibly particular cost configuration) we decide which attributes the model requires to get the lowest expected joint cost1. This is done by calculating the expected joint cost for each point in the lattice. In this sense, each prediction is associated with a possibly different operating condition, and the best attribute subset is chosen.\nInterestingly, we can use the previous approach for more than one model, and see that some models dominate for some operating conditions over the rest. This is exactly the way ROC analysis works (for classification [42, 23, 29, 17, 37] and for regression [28])."}, {"heading": "1.3 Objectives", "text": "The goal of our investigation is then to introduce new methods to make optimal choices in terms of the joint cost when using off-the-shelf data mining models. An optimal choice is understood as selecting the right model with the right subset of attributes given a cost context.\nWe will introduce graphical plots and procedures to make this selection and also to reduce the number of combinations in the lattice that need to be explored in order to make a good selection.\nIn order to improve those objectives, several sub-objectives should be accomplished:\n1Given an example with some non-missing and some missing values we may decide increase the number of non-missing values. Given a case for which we have not still retrieved any of the attributes (tests) we decide how many (and which) attributes we are going to ask for.\n4\n\u2022 A graphical evaluation of how a model improves, and most especially, degrades if we start removing attributes, finding the concept of dominance here.\n\u2022 The analysis of those context changes related to attribute (feature) costs and the introduction of solutions to create more versatile models and reframing transformations.\n\u2022 The evaluation of the previous approaches with several datasets, using common repositories or the reframe domains.\nIn this work we will focus on classification, but many of the ideas could be extended to regression as well."}, {"heading": "1.4 Thesis plan", "text": "This work is organised as follows:\n\u2022 Chapter 2: Previous Works. This chapter describes some general aspects of machine learning with some of its actual challenges, and present some others works related to the\ncurrent one.\n\u2022 Chapter 3: Reframing the model with missing values on purpose. In this chapter, a description and explanation of our approach is made.\n\u2022 Chapter 4: Approximating the JROC hull. A scaling-up of the approach described in the previous chapter is developed in this section.\n\u2022 Chapter 5: Experiments. This chapter presents all the experiments performed during this work and the result obtained, using a set of datasets of the UCI repository and different\nmachine learning methods.\nThis work discusses, in chapter 6, the conclusions obtained from the experiment results, some future work and the dissemination made.\nFinally, some appendices follow with more details about the results of chapter 5 and a description of the implementation of JROC.\n5"}, {"heading": "2 Previous Works", "text": "In this chapter, we give an overview of machine learning, and some of its difficulties and challenges are presented. we also see some previous works and approaches related to our investigation which face those difficulties. Finally, the tools used during our investigation are introduced."}, {"heading": "2.1 Machine learning", "text": "In recent times the level of technological advancement in the use of machine learning as a reliable means of retrieving information, and the ever increasing amounts of available data makes the analysis of data necessary for many areas of technological progress. Furthermore, the use of machine learning and machine learning tools has proven to help solve some complex problems, providing good solutions, in the form of classification and regression models [21].\nMachine learning is the means by which knowledge is acquired and the ability to use it. This implies that learning involves the process of finding and describing data in the form of structural patterns. For instance, data could be of client complaints in an organisation or other service options open to clients. The output of such learning could be whether a particular customer complaint was genuine or not. Another example is that the data could contain examples of customers who have switched to another service provider in the telecommunication industry and some that have not. The output of learning could be the prediction of whether a particular customer will switch to another service provider or not. There are two types of learning: supervised and unsupervised. The previous examples are supervised. In this work, we will focus on supervised learning.\n6\nSupervised learning requires the response for each instance such that it can be used by the system to guide the learning. The whole process includes the collection of data to be used for data mining, identifying the target variable, dividing up of the data into training and testing data and constructing and evaluating the model. The training data is used by the data mining algorithm to learn the data and build a model while the test data is used to evaluate the performance of the model on new data. For instance, decision trees and neural nets are two common types of supervised learning. This type of learning always requires a target variable to predict.\nHowever, some of the problems of supervised learning are overfitting [21] [44], which means that the model perfectly works on the training data but not on the evaluation (test) data. This problem can also occur independtly of overfitting. It happens when the model learns the training data with some attributes and in the test time there are some missing or new features. Thus, learning, inference, and prediction in the presence of missing data or \u201cnew\u201d data are pervasive problems in machine learning and statistical data analysis."}, {"heading": "2.2 Features selection", "text": "There are some aspects of machine learning which involve the application of datasets with a large number of features (tens or hundreds of thousands of variables available) such as text processing, gene expression, array analysis, and combinatorial chemistry. In such case the model built could be overfitted. Feature selection has in recent times become the focus of much research [26, 38], with approaches to address such a situation. Feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of choosing a subset of relevant features within the original set of features which will be used in model construction. The principal assumption when using a feature selection technique is that the data may contains many redundant or irrelevant features. A feature is redundant if it does not give enough information once given the already selected features. Irrelevant features are the ones which provide useless information for all contexts. In addition, feature selection has some others advantages when constructing predictive models: improved model interpretability, reduced training times, and enhanced generalisation by reducing overfitting.\n7"}, {"heading": "2.3 Missing attributes: a challenge", "text": "In addition to the large number of variables or attributes that machine learning often has to face, missing values is another challenge. Missing data can be introduced in many situations and domains, and for various reasons. For instance, in bioinformatics, certain regions of a gene micro-array may fail to give measurements of the underlying gene expressions due to scratches, finger prints, or manufacturing defects; or in a hospital, participants in a clinical study may drop out during the course of the study leading to missing observations at subsequent time points; and moreover, a doctor may not be able to order all applicable tests to a patient. All these examples could lead to a large number of missing data."}, {"heading": "2.3.1 Missing attribute imputation", "text": "In the past there have been many approaches to account for missing data. However, one well known attempt to deal with missing data is known as \u201cimputation\u201d [46]. In statistics, imputation is the process of replacing missing data with substituted values (when substituting for a data point, it is known as \u201cunit imputation\u201d; when substituting for a component of a data point, it is known as \u201citem imputation\u201d). Because of the fact that data can be \u201cobstructive\u201d during training and lead to the construction of a suboptimal or defective model, the concept of imputation is seen by some researchers as a way to avoid challenges involved with list-wise deletion of instances that have missing values. In practice, when one or more values are missing from a case, most statistical packages default to removing any example that has a missing value (the classifier algorithm discard all the instances which do not have all the information), which may introduce bias or affect the representation of the results (pattern). But imputation preserves all cases by replacing missing data with a probable value based on other available information. Once all missing values have been imputed, the data set can then be trained and evaluated using standard methods for complete data. Nevertheless, imputing missing values \u201cis regarded as unnecessary for cost-sensitive learning that also considers the test costs\u201d [45].\n8"}, {"heading": "2.4 Cost-sensitive methods", "text": "As a means of minimise the cost made when predicting the classification of unseen examples, there have been many works in the last decade that investigate approaches to learning or revising classification procedures that attempt to reduce the cost of misclassified examples rather than the number of misclassified examples [43]. These ideas are underpinned by the concept that in many problems, the cost of all errors (false positive or false negative) is not equal. Thus, the cost of making an error can depend upon both the predicted class of the example and the actual class of an example. Therefore, in contrast to the general case, here the main purpose is not to maximise the accuracy. For instance, in a bank, credit card fraud detection aims to maximise the total transaction amount of correctly detected frauds minus the cost to investigate all (correctly and incorrectly) detected frauds; And because undetected fraud causes a loss of the whole transaction amount, it is by far more profitable to detect frauds with high transaction amount than those whose amount is not even higher than the cost to investigate.\nTurney [43] presents an excellent survey on different types of costs in cost-sensitive learning,\namong which misclassification costs and test costs are distinguished as most important.\nMany modifications to machine learning algorithms attempt to reduce these costs, as in\nclassification rules [39], when a prior domain theory is available [15]."}, {"heading": "2.4.1 Cost-sensitive decision trees", "text": "In real practice, as we said, many real-world data sets for machine learning and data mining contain missing values and previous research regards it as a problem and attempts to impute missing values before training and testing. Some other works study this issue in cost-sensitive learning.\nIn cost-sensitive learning, which attempts to minimise the total cost of tests and misclassifications, missing data can be useful for cost reduction, so imputing missing values might be unnecessary. Some recent approaches in this field are cost-sensitive decision tree learning algorithms which should utilise only known values, using decision trees as the base learner, that consider both test costs and misclassification costs. In this case, some attributes (during the tests) might be too expensive for obtaining their values. Thus it can be cheaper not to include\n9\ntheir values, avoiding so expensive and risky tests (as in patient diagnosis for example [35, 45]). This reasoning is called: \u201cmissing is useful\u201d [45], as the fact that values actually reduce the total cost of tests and, therefore, it is not meaningful to impute their values, as they will not reduce the misclassification cost. Cost-sensitive decision trees algorithms have different approaches:\n\u2022 As values are missing for certain reasons \u2014 unnecessary and too expensive to test \u2014 an option is to replace missing data with a special value, mostly called \u201cnull\u201d in databases.\nAnd this null value will then be treated just as a regular known value in the model construction (tree building in this case) and during the test processes. Since this strategy has been proposed in machine learning [1, 12], its performance and efficiency in costsensitive learning has not been shown, since the use of the same value to all missing values (null in this case) may not be good enough since it could introduce bias.\n\u2022 The second option is called the C4.5 strategy. This approach consists in choosing, during training, an attribute by the probability of missing values of that attribute. And during\nthe test process, a test example with missing value is divided into branches according to the portions of training examples falling into those branches. The class of the test example is the weighted classification of all leaves. C4.5s missing-value strategy has been shown to be efficient in cost-sensitive learning[3].\nBoth approaches consider missing values in a way to make methods cost-sensitive. But they are not optimal in such cases where you have to deal with the cost of the features and missing features at the same time. Also, they are restricted to decision trees. This investigation brings a new vision to tackle this kind of cases. The approach we discuss in this work does not look at imputation either as a solution nor an option since it is a disadvantage for the cost-effectiveness in cost-sensitive learning. Our idea is to build a model independent approach (not based on a special technique such as decision tree, i.e, it can be applied to any technique.)\nBefore the presentation of our approach, it is necessary to present and define some tools used\nduring the process."}, {"heading": "2.5 Tools", "text": "The main tools used for the implementation of this work are R and RWeka.\n10"}, {"heading": "2.5.1 R", "text": "R is at the same time a language and an environment used for statistical computing and graphical representation about statistics results. Besides, the fact that R provides a great number of statistical methods (linear and nonlinear modeling, classical statistical tests, classification, regressions, clustering, . . . ) and graphical techniques, gives the possibility to be highly extended by integrating many packages and libraries available."}, {"heading": "2.5.2 RWeka", "text": "RWeka is an R interface to Weka, while Weka is a collection of machine learning algorithms for data mining tasks written in Java. It contains tools to pre-process data, for classification or regression, clustering, association rules, and visualisation. Weka is also recommended and used for developing new machine learning schemes. The package RWeka contains the interface code, and the Weka jar is in a separate package RWekajars. For the experiments of our work we used Weka (thus RWeka, because the implementation and execution are made in R language and environment).\nAs we will work with a learned model that we want to apply to a new cost context, we need to find a way for removing some attributes from the model, in order to reduce the Test Cost. A tricky way to do this is by setting them as \u201cmissing\u201d. In Weka the missing values are represented with a \u201c?\u201d; so for an attribute that we do not want to represent in the reframed model (maybe because of its higher test cost), we just represent the entire column with \u201c?\u201d."}, {"heading": "2.5.3 Weka algorithms used in this work", "text": "The main algorithms which we used for the experimental part of this work are:\n\u2022 SMO is a Support Vector Machine (SVM), a supervised learning model with associated learning algorithms that analyse data and recognise patterns, used for classification and\nregression. The basic SVM takes a set of input data and predicts, for each given input, which of two possible classes forms the output, making it a non-probabilistic binary linear\n11\nclassifier. But it also works for more than two classes.\n\u2022 IBk is a K-nearest neighbour (KNN), a machine learning classification algorithm. KNN is lazy (it defers computation until classification is needed) and supervised.\n\u2022 J48 is a decision tree learning algorithm that graphically displays the classification process of a given input for a given output class labels.\n\u2022 Adaboost is an ensemble method for constructing a \u201cstrong\u201d classifier as a linear combination of simple \u201cweak\u201d classifiers [4].\n\u2022 Bagging [4] is a classificcation method which consists in training a number of base learners each from a different bootstrap sample (original data partition) by calling a base learning\nalgorithm (decision trees, KNN, . . . ). After obtaining the base learners, it executes a vote and the most-voted class is predicted.\nWe also add that, for the representation and the evaluation for each model or algorithm used in our approach, we have used the ROC curves and some evaluation of classifiers methods. We see some basic ideas next."}, {"heading": "2.6 Classifier visualisation and evaluation", "text": ""}, {"heading": "2.6.1 ROC Analysis", "text": "A receiver operating characteristics (ROC) graph is a technique for visualising, organising and selecting classifiers based on their performance [17]. First, ROC graphs have been introduced to represent the trade-off between hit rates and false alarm rates. Later, it has also been demonstrated that ROC curves can also be very useful as a visualisation method for classifiers evaluation and comparison. This is due partly because of the fact that simple classification accuracy is often a poor metric for measuring performance, and partly because ROC curves have properties well adapted for domains with unbalanced class distributions and unequal classification error costs.\n12"}, {"heading": "2.6.2 Classifiers evaluation", "text": "Nowadays there is a wide range of classifiers techniques. This has led the user to one preoccupation: how can he evaluate how good is the learned model? Performance metrics are of a fundamental importance to give an consistent answer to this question. In classification, it has been shown that in data mining problems, the use of accuracy to compare classifiers is not adequate, and many works have been developed to address this problem [19]. In general, we have to choose the most adequate measure (or set of measures) for a specific application (Accuracy, F-measure, Rank Rate, Area Under the ROC Curve (AUC), Squared Error, . . . ). These metrics can be grouped in three principal groups [19]: the ones based on a threshold and qualitative understanding of error, the ones based on a probabilistic understanding of error, and the ones based on how well the model ranks the examples (where AUC is found).\n13"}, {"heading": "3 Reframing the model with missing val-", "text": "ues on purpose\nThere has been an extensive work in the past decades on how the performance of a predictive technique evolves with different feature subsets. This is the core of feature selection techniques. In fact, model performance can even be increased by using a subset of the original attributes. Also, if we think about costs, most works on minimising costs have taken this approach [35, 45, 36].\nHowever, we can also consider that the model has already been trained (with possibly all the attributes) and we may just want to apply the model with fewer available attributes, e.g., when missing values appear or when we cannot afford \u2018buying\u2019 some of the tests included in the model. It is important to say that we consider models that may have been developed by experts or by automated predictive analysis tools, or both. What we do is to reframe the model to a situation with fewer attributes."}, {"heading": "3.1 Definition of the approach", "text": "We will focus on classification problems, characterised by a multivariate input domain X, i.e., a tuple of elements of sets X1, X2, . . . , Xm, where m is the number of features or input attributes, possibly containing the null value, and a univariate output domain Y \u2282 {l1, l2, . . . , lc}, where c is the number of classes or labels of the output attribute. The domain space D is then X \u00d7 Y. Examples or instances are just pairs \u3008x, y\u3009 \u2208 D, and datasets are subsets (actually multi-sets) of D. The length of a dataset will usually be denoted by n. A crisp classification model f\u0302\n14\nis a function f\u0302 : X \u2192 Y. We just represent the true value by y and the estimated value by y\u0302. Subindices will be used when referring to more than one example in a dataset. Given an example i, the values of the m input attributes are denoted by xi,1, xi,2, . . . , xi,m.\nThroughout this work we will use several classifiers from Weka [27]. We are especially interested in using the techniques as they are, being able to use techniques that are, in principle, inattentive to the use of all the attributes, such as kernel methods, ensembles, etc. In particular, we will use SMO (a support vector machine), IBk (a k-nearest neighbour), J48 (a decision tree), Adaboost (an ensemble method with decision stumps) and Bagging (an exemple method with decision trees)presented in the previous chapter. All of them will be used with their default parameters.\nOnce this common setting for classification is set, we may wonder how models are created and deployed. In fact, models are usually learned under some contextual information but possibly deployed several times under changing conditions. Reuse of learned knowledge is of critical importance in the majority of knowledge-intensive application areas, particularly because the operating context can be expected to vary from training to deployment and we need to make the best decision according to that context [42, 23]. One kind of context is related to the way inputs (i.e., features) can vary from training to deployment. Among these changes, we can mention two important ones: attributes may not be available (missing values) or may have different test costs. Another type of context depends on how class distribution and misclassification costs affect the output variable. Note that these context changes may happen for each problem instance individually. For instance, in a medical domain, some tests may not be applicable to some patients (as can be contraindicated or risky), other tests may be more or less expensive depending on the patient (her insurance policy). Also, for the output variable, a wrong diagnosis usually has asymmetric costs, as a false negative is usually worse (and economically more expensive in the long term) than a false positive.\nThese two types of costs (test costs and misclassification costs) are highly intertwined. In fact, as Turney [43] points out, we can only rationally determine whether it is worthwhile to pay the cost of test when we know the cost of misclassification errors. If the cost of misclassification errors is much greater than the cost of tests, then it is rational to purchase all tests that seem to have some predictive value. But if the cost of misclassification errors is much less than the cost of tests, then it is not rational to purchase any tests.\nLet us define these types of cost formally:\nDefinition 1. A misclassification cost function is any function M : Y\u00d7Y\u2192 R which compares\n15\nelements in the output domain. For convenience, the first argument will be the estimated value, and the second argument the actual value.\nAs Y is a discrete set, typically we refer to M as the misclassification cost matrix. We will assume that the diagonal of the matrix is zero (i.e., \u2200y : M(y, y) = 0) and that the other elements of the matrix are greater than or equal to 0.\nWe can have a different matrix for each example, denoted by Mi. From above, we define the misclassification cost MC of an example i as MCi ,Mi(y\u0302i, yi). For a complete dataset we can just calculate the average MC as the Frobenius product between the confusion matrix and the cost matrix, divided by n.\nDefinition 2. The test cost vector is a real vector of size m, i.e., (t1, t2, . . . , tm), where m is the number of attributes. The test cost function Tj is any function as:\nTj(x) ,\n{ tj if x is not null\n0 otherwise\nWe can have a different test cost function for example, denoted by Ti,j . From above, we define the test cost TC of an example i as TCi , \u2211m j=1 Ti,j(xi,j). For a complete dataset we can just calculate the average TC as the dot product between the use vector (how many times each attribute has been used) and the test cost vector, divided by n.\nWe want to integrate both the misclassication cost and the test cost in one single measure\nof cost:\nDefinition 3. The joint cost is:\nJCi , \u03b1 \u00b7MCi + (1\u2212 \u03b1) \u00b7 TCi\nwith \u03b1 \u2208 [0, 1].\nThe value \u03b1 will be better explained later on, but clearly sets more relevance to misclassification or test costs. If \u03b1 = 1 only the misclassification cost matters, and if \u03b1 = 0 only the test cost matters. M , T and \u03b1 configure the cost context. With m attributes and c classes, there are m+ c(c\u2212 1)\u2212 1 degrees of freedom (assuming the cost matrix has a zero diagonal).\nExample 1. Consider the iris dataset [2], created by R.A. Fisher, which is composed of four attributes: SL, SW , PL and PW and three classes: setosa, versicolour and virginica.\n16\nAssume that we have an example where the test cost vector is (3, 2, 10, 5) and the misclassi-\nfication cost matrix M is defined as follows:\nsetosa versicolour virginica\nsetosa 0 20 15 versicolour 5 0 15\nvirginica 30 15 0\nwhere columns represent the actual value and rows the predicted value. Consider also that we have three models. Model 1 requires attributes SL and PL and predicts virginica, model 2 requires attributes SL and SW and predicts setosa, and model 3 requires all attributes and predicts versicolour. If the true label is versicolour, then we have JC = MC+TC = 15 + (3 + 10) = 28 for model 1, JC = MC+TC = 20 + (3 + 2) = 25 for model 2, and JC = MC+TC = 0 + (3 + 2 + 10 + 5) = 20 for model 3.\nIn the previous example, model 3 is better than the other two for this example. Of course, in general, we need to make the decision of which model to use without knowing the actual label, and that will depend on the reliability of the models and the class frequencies. This is then a decision problem, for which we need to determine the model with lowest expected cost.\nInterestingly, we may wonder what would happen if we remove attribute PW for model 3. Even though we are told that model 3 works with that attribute, it is not difficult to guess what the model would do without it. There are several ways to do it: set it to null and see what happens, or consider several values for the attribute and get the most frequently predicted class. Imagine that, by using any of these two methods, model 3 still predicts versicolour. Our cost would have been lowered down to 15.\nSo, the question we want to address in this work is not only what model to choose but also\nthe subset of attributes that we will use (\u2018buy\u2019)."}, {"heading": "3.2 Reframing the model with missing values on purpose", "text": "Re-training can be a bad choice on many occasions: when we have an expert (human-made) model, when we are using ensembles or other techniques with high training costs, when the\n17\ntraining data is no longer available, or when the cost context changes recurrently, even for each example. What can we do instead of re-training? What we do is to reframe the original model to a situation with fewer attributes, a different feature configuration . But, how do models behave when we remove attributes from them?\nFirst, we need to clarify how we can get predictions from a model that takes m attributes when we only provide m\u2032 < m. There are two possible ways of reframing a model in order to do this:\n1. Setting the attribute to null. Many models can just work with missing values for test\ninstances. However, on some occasions the model cannot take null values (e.g., logistic regression is usually one of these techniques). Nonetheless, it highly depends on the implementation of the technique (or the model).\n2. Instead, we can invent or negotiate over the attribute [6]. This means that if it is a nominal\nattribute, we can just ask the model to give a prediction for all the possible values for the attribute, get the predictions, and calculate the most frequently predicted class. If it is a numerical attribute, we can just use a sampling or discretisation and then behave similarly. If we have information about the attribute value distribution, we can also use it, as in missing value imputation.\nThis second approach is more powerful (and related to missing value imputation and feature selection). In fact, on occasions, we may even realise that we get the same prediction for whatever value of the attribute (i.e., this is said to be a non-negotiable attribute in terms of [6]) so we can clearly save the cost of getting the value for this attribute. However, we will work with the first way, as using a null value works for many DM/ML techniques and libraries, without further modification of our models. In our case, it just worked smoothly with Weka [27].\nOnce we know a simple procedure to reduce the attribute set, let us analyse how models behave. Figure 3.1 shows the evolution of accuracy1, for all the possible subsets of the iris and the diabetes dataset (the subset lattice). The models (decision trees, SMO and KNN) are learned over the whole training dataset using all the attributes. Then, some attributes are removed by setting a null value on them systematically. Up: iris dataset (with 4 attributes and hence 24 = 16 combinations). Down: Pima Indian diabetes dataset (with 8 attributes and hence 28 = 256 combinations). Models are trained on 2/3 of the data and evaluated with the rest. We see many interesting things here: First, the general pattern is to get more accuracy\n1We show accuracy, but we could show other measures such as AUC or MSE [19, 31]\n18\n19\nas more attributes are used. But, obviously, some attributes are more important than others, leading to a sawtooth picture. Second, and more interestingly, the minimum is found at the majority-class classifier, i.e., if we are not given any information about any attribute, the best thing that we can do is to predict the majority class (or the class with lowest expected loss if misclassification costs are taken into account). Third, now surprisingly, we see that for some models and problems (Figure 3.1, down), the maximum is not obtained with all the attributes. In fact, it is obtained at several other places, one of them with four attributes removed (of the possible eight).\nWe can show the specific values of MC, TC and the aggregate JC for a given context of M ,\nT and \u03b1. We will consider a \u2018uniform\u2019 operating context:\nDefinition 4. The uniform operating context \u03b8U is defined by a uniform test cost vector (1/m, 1/m, . . . , 1/m) and a uniform misclassification cost matrix \u2200y1, y2 : M(y1, y2) = c/(c\u22121) if y1 6= y2 and 0 otherwise. Also, \u03b1 = 0.5.\nThe parameters of this context have the property that given a problem whose classes are perfectly balanced, the expected MC of a random classifier is 1 and the expected TC of a classifier using all the attributes is 1. As a consequence, JC = 1. For this context, any model with JC > 1 is clearly a model to be discarded. In fact, as a random classifier does not need to use any of the attributes, any JC > 0.5 is also discardable for this context.\nFigure 3.2 shows the evolution of MC, TC and the aggregate JC for the uniform context described above. Up: iris dataset. The configuration which minimises the JC is given by the use of only attribute 4 (removing -1-2-3). Down: Pima Indian diabetes dataset. The configuration that minimises the TC is given by only two attributes (removing six). We can see that the information shown is very similar to that evolution of Figure 3.1. However, for other operating contexts, things might be different.\nNow let us consider another operating context, defined as follows for the problems \u201ciris\u201d and\n\u201cPima Indian diabetes\u201d.\nThe operating context \u03b81 for \u201ciris\u201d is just the one in example 1. The operating context \u03b82 for \u201cPima Indian diabetes\u201d is defined as a test cost vector is (2, 50, 5, 5, 20, 3, 10, 1) where the most expensive tests correspond to \u2018plasma glucose concentration\u2019, \u20182hour serum insulin\u2019 and \u2018diabetes pedigree function\u2019. The misclassification cost matrix M is defined as follows:\n20\n21\nnegative (0) positive (1)\nnegative (0) 0 200 positive (1) 50 0\nwhere columns represent the actual value and rows the predicted value. The value of \u03b1 is 0.5.\nWith these operating contexts, Figure 3.3,shows the same plots as Figure 3.2 ( Up: iris dataset using context \u03b81. The configuration which minimises the JC is given by the attribute 4. Down: Pima Indian diabetes dataset using context \u03b82. The configuration which minimises the JC is given by removing all the attributes.) ."}, {"heading": "3.3 The MC/TC tradeoff: JROC plots", "text": "The plots seen in the previous section are very informative for a given operating context. If the plots are drawn on a validation set, we will just choose the model and attribute configuration which minimises the JC. However, there are some problems with the previous plots: if we have several models, the plot gets too crowded. Also, the curves are usually too sawtooth. Finally, we need to change the curves whenever we change the operating context.\nWhile some of the above problems are difficult to solve completely, most especially because we have m + c(c \u2212 1) \u2212 1 degrees of freedom, we can see a more convenient alternative that minimises these problems. We call these JROC plots.\nDefinition 5. A JROC plot shows the test cost (TC) on the x\u2212axis and misclassification cost (MC) on the y\u2212axis.\nFigure 3.4 shows JROC plots for iris and Pima Indian diabetes. For iris, as it has four attributes, we see 24 \u00d7 3 points, 24 for each model. For diabetes, as it has eight attributes, we see 28 \u00d7 3 points, 28 fore each model. Those models and configurations which go closer to the bottom left corner are better than those that are placed on the top right area of the plot. There is always a point with 0 TC and a high misclassification cost, usually matching the majority class model. However, as mentioned earlier on, the minimum MC is not always achieve with maximum TC. In this particular case, for iris we see that decision trees and kNN (IBk) perform better, as the points which are most on the bottom left are of these models. However, for diabetes, it seems that SMO and kNN get closer to the desired bottom left corner.\n22\n23\n24\nFigure 3.5 shows a similar plot with different cost contexts. Here, we also see how the points are now located in different places. Even though the classifiers are the same, Compare to Figure 3.4, which uses a different cost context, the distribution of the points is very different. (The test cost (TC) on the x-axis and misclassification cost (MC) on the y-axis. up: iris dataset with the operating context \u03b81. Down: Pima Indian diabetes dataset with the operating context \u03b82.)\nIntentionally, we have not shown \u03b1 on the plots, so, according to definition 3, we cannot calculate JC unless this value is fixed. The following lemma shows that the same plot can be used for any value of \u03b1, with the notion of cost isometrics.\nProposition 1. Given a value of alpha the points which are connected by a line with slope 1\u2212\u03b1\u03b1 have the same JC.\nProof. From definition 3 we have that JC = \u03b1 \u00b7MC + (1\u2212\u03b1) \u00b7TC. Consequently, two points a and b have the same JC iff \u03b1 \u00b7MCa + (1\u2212 \u03b1) \u00b7 TCa = \u03b1 \u00b7MCb + (1\u2212 \u03b1) \u00b7 TCb. Operating with this equation, we get:\n\u03b1 \u00b7MCa + (1\u2212 \u03b1) \u00b7 TCa = \u03b1 \u00b7MCb + (1\u2212 \u03b1) \u00b7 TCb\nMCa + 1\u2212 \u03b1 \u03b1 \u00b7 TCa = MCb + 1\u2212 \u03b1 \u03b1 \u00b7 TCb\nMCa \u2212MCb = 1\u2212 \u03b1 \u03b1 \u00b7 (TCb \u2212 TCa) MCa \u2212MCb TCa \u2212 TCb = 1\u2212 \u03b1 \u03b1\nAs the last expression is the change in y divided by the change in x, the expression 1\u2212\u03b1\u03b1 is the slope of this line.\nIf \u03b1 = 1 only the misclassification cost matters and the slope is 0, and if \u03b1 = 0 only the test cost matters and the slope is infinite. It is clear that \u03b1 only represents one of the m+c(c\u22121)\u22121 degrees of freedom, but it is able to consider the most important one: the relative relevance between misclassification and test costs.\nAs in classical ROC analysis, if we slide an isometric line given by a value of \u03b1 from the point (0, 0) in the opposite direction (towards the top-right part of the plot), we will eventually find one point (or more) on the plot. This is the best point according to the operating condition.\nFigure 3.6 ( Up: iris dataset with the operating context \u03b81. Down: Pima Indian diabetes dataset with the operating context \u03b82.) shows three different isometrics given by operating\n25\n26\nconditions \u03b1 = 0.03, \u03b1 = 0.5 and \u03b1 = 0.9 and where they touch on the cloud of points. As we can see, different feature configurations and models are chosen for each operating condition\nFinally, if we consider all possible values of \u03b1 \u2208 [0, 1] we see that some points are never chosen. This is exactly the notion of convex hull:\nDefinition 6. A JROC convex hull of a model is the convex hull of the set of points on the JROC space that are defined using all the attribute subsets.\nFigure 3.7 shows the convex hull for each of the three models. We can also see the regions of dominance. For diabetes, IBk dominates for high values of \u03b1, while the decision tree dominates for low values of \u03b1.\nFrom here we can calculate the regions of dominance for \u03b1 and choose the best model\naccordingly, in the very same way as in ROC analysis.\n27\n28\n29"}, {"heading": "4 Approximating the JROC hull", "text": "The previous procedure allows us to determine the best model and configuration given the operating condition. We only need to calculate where all the points lie, compute the convex hull and find the one that corresponds for each possible \u03b1 in application time. While this looks easy to do, there is one big issue. As the number of attributes increase, the number of points for a model grows exponentially: a lattice of m elements has 2m nodes. For instance, for a model with 16 attributes, we would have 216 = 65536 points. Navigating the complete lattice of attribute subsets, calculate their expected TC and MC would be unfeasible. So we need to explore some ways to reduce the number of configurations that are evaluated, while still having a good approximation of the JROC hulls in order to do the correct decisions and get the optimal cost.\nWe will consider how to reduce the number of configurations from an exponential growth (O(2m)), given by a full method, to a quadratic growth (O(m2)). We consider four possible1 methods:\n\u2022 Backward MC-guided (BMC): we start with the m attributes, we evaluate with the m cases removing one attribute, and choose the best one in terms of MC, then we evaluate\nthe m\u22121 cases removing one attribute from the previous one. This has an order of O(m2). In particular, it is easy to see that it leads to exactly m2/2) points.\n\u2022 Backward TC-guided (BTC): backward incremental: as MC using TC instead. It has the same order and number of points.\n\u2022 Backward JC-guided (BJC): as MC using JC instead. It has the same order and number 1There would also be the forward versions as well. We rule these possibilites out here for the simplicity of exposition, and also because we think that the results would be similar, but they could also be considered in practice.\n30\nof points.\n\u2022 Monte Carlo (RND): a random sample over the lattice. In order to make comparison fair, we will also consider the same number of elements:\nIt is easy to show that if the misclassification cost matrix is uniform, then BTC and BJC are equivalent. If the test cost vector then BMC and BJC are equivalent. If both the misclassification cost matrix and the cost vector are uniform (i.e., the uniform operating context \u03b8U ) then BMC, BTC and BJC are equivalent.\nFigure 4.1 shows the results for the BMC method for our two datasets and operating contexts ( Up: iris dataset with the operating context \u03b81. Down: Pima Indian diabetes dataset with the operating context \u03b82). If we compare with Figure 3.7, we see that the hull are almost identical.\nSimilarly, we have the results for the BTC, BJC and RND methods on figures 4.2, 4.3 and 4.4. We note that there are m(m + 1)/2 + 1 points for each model (in each graph) instead of 2m. If we compare with Figure 3.7, we see that the hulls are much worse for BTC, and notably worse for BJC and RND. However, we see that the results for BMC are good, almost identical to Figure 3.7. Does this observation hold in general? The answer of this question (and more) are explored in the experiments.\n31\n32\n33\n34\n35"}, {"heading": "5 Experiments", "text": "In this chapter we present an implementation of our approach described in sections 3 and 4, and an analysis of results\nWe are going to explore whether the JROC plots are effective, and also whether their quadratic approximation suffers from a degradation. In order to do that, we consider six datasets of the UCI repository, with number of attributes between 4 and 11, as shown in Table 5.1. We could not use larger datasets in this first experiment because of the slowness of the Full method, as the number of elements to explore in the lattice grows exponentially.\nWe consider two different contexts: a uniform context \u03b8u and a random context where each value of the misclassification cost matrix and test cost vector are obtained by multiplying the original value of the uniform context by k, where k = e\u03b2\u00d7(k0\u22120.5), k0 is obtained as a random number between 0 and 1 using a uniform distribution, and \u03b2 is a factor of how irregular we want the vector and matrix to be. We set \u03b2 = 10 for the following experiments. Once this function is applied, the test cost vector T and the misclassification cost matrix M are normalised such\n36\nthat \u2211 T = 1 and \u2211 M = c2.\nFor each dataset of size n, we split it into a work dataset (2n/3 of the data) and the remaining data (n/3) for test. With the work dataset, we perform a split of the work dataset into two halves. We train the four models (SMO, kNN, AdaBoost and Bagging) with the first half of the data (n/3) and calculate all the points (i.e., TC and MC) according to the full method, and the BMC, BTC, BJC and RND methods with the other half. Next, we choose 5 values of \u03b1 \u2208 {0.1, 0.3, 0.5, 0.7, 0.9} and determine the best configuration of model and attribute subset for each of the five methods. We use these 5 configurations for the test set and calculate the JC.\nWe repeat the experiment 4 times. This gives us, 4\u00d75 = 20 results for each of the 5 methods for each of the 6 datasets."}, {"heading": "5.1 Uniform context", "text": "First we give the results for the uniform context. Table 5.2 shows the mean and standard deviation of the results for each dataset and method. We see that Full cannot be improved by the other methods, as it explores all the possibilities. In general, the RND method is worse than the backward methods, except for dataset 1 (the smaller one, iris, where the number of explored configurations is 4\u00d7 5 + 1 = 11 in front of a total of 16, which is not a big difference). In fact, for the big datasets, where the difference in explored configuration grows exponentially, we see that the backward methods get close to the Full methods, which gives support to these approximation.\nTable 5.3 shows the results aggregated for all datasets but showing each value of \u03b1. This means (8 datasets with 10 repetitions). In this case, we can see that the backward methods are consistently better than the RND method and are reasonable close to the Full method. The influence of \u03b1 is not particularly clear, the approximation is similar for all of them.\nFinally, we want to see the whole picture and perform a statistical test. In order to assess the significance of the experimental results we will use a custom procedure, following [33] and [22, ch.12], which in turn is mostly based on [13]. Since we will not have any baseline method, we will use a Friedman test to tell whether the difference between several methods is significant and then we will apply the Nemenyi post-hoc test. We agree with [25] that the Nemenyi test is a \u201cvery\n37\n38\nconservative procedure and many of the obvious differences may not be detected\u201d, but we prefer to be conservative given our experimental setting and the use of a 0.95 confidence level. In some result tables we will show the means (even though in many cases they are not commensurate) and in some other tables we will show the average ranks (from which the Friedman and Nemenyi tests are calculated). We will also include the critical difference for the Nemenyi test, so we will be able to simply tell whether the difference between two algorithms is significant if the difference between their average ranks is greater than the critical difference.\nTable 5.4 shows the results of several data, where we are particularly interested in knowing which of the three backward methods is best. As we can see, the three methods behave almost equally. In fact, BMC and BJC are exactly equal, which is a consequence of the use of a uniform context. The 30 rows are given by 6 datasets and 5 possible values of \u03b1 with the uniform context. The \u2018Avg\u2019 row shows the averages of the first 30 rows. Finally, the \u2018AR\u2019 row shows the average rank for each method. With these ranks the Friedman test is applied and gives a Friedman statistic of 62.51 which is greather than the critical value of 10.97. Consequently, the null hypothesis is rejected (significance level: 0.05) and we conclude that the methods do not perform equally. In order to see which methods are significantly different from the rest, we look at the critical difference for the Nemenyi post-hoc test, which is 0.2965. This means that the Full method is statistically better than the rest, and that the RND method is statistically worse than the rest, but there is no statistically significant difference between the three methods BMC, BTC and BJC."}, {"heading": "5.2 Variable context", "text": "In order to see what happens in a more realistic situation, let us see the results for the nonuniform context. Table 5.5 shows the mean and standard deviation of the results for each dataset and method. Here we see that not all backward methods are equivalently, but interestingly we see that BMC is now consistently better than RND for all datasets.\nAgain, Table 5.6 shows the results aggregated for all datasets but showing each value of \u03b1. This means (8 datasets with 10 repetitions). Now we see that the results are not especially different according to \u03b1.\nFinally, if we look at the whole picture and using a statistical test, we see in Table 5.7 that the backward methods are better than the RND method, but now we find difference between\n39\n40\n41\nthem. In fact, BTC is significantly better than BMC and BJC. The 30 rows are given by 6 datasets and 5 possible values of \u03b1 with the variable context. The \u2018Avg\u2019 row shows the averages of the first 30 rows. Finally, the \u2018AR\u2019 row shows the average rank for each method. With these ranks the Friedman test is applied and gives a Friedman statistic of 67.27 which is greater than the critical value of 10.97. Consequently, the null hypothesis is rejected (significance level: 0.05) and we conclude that the methods do not perform equally. In order to see which methods are significantly different from the rest, we look at the critical difference for the Nemenyi post-hoc test, which is 0.2965. This means that the Full method is statistically better than the rest, and that the RND method is statistically worse than the rest. In this case, we see that the BTC method is significantly better than BMC and BJC.\nAlthough some more definitive conclusions of which method is best in general would require more datasets and repetitions (although the results are significant enough here), these experiments show the potential of the backward methods.\n42\n43"}, {"heading": "6 Conclusions and future works", "text": "In this chapter, we summarise the accomplishments and discuss some interesting future work."}, {"heading": "6.1 Conclusion", "text": "Missing values and cost minimisation are important issues in machine learning. In this work we have developed a new approach that tackles the problem of cost minimisation (both misclassification and test cost) reframing an already built model using missing values on purpose. A new manner to address the cost reduction in machine learning problems. Our approach is totally model independent (can be used and works perfectly for any predictive model). Overall, our approach fulfills all the objectives fixed in the introduction:\n1. An evaluation (graphical) of how a model improves, and most especially, degrades if we\nstart removing attributes, finding the concept of dominance here. figure 3.1 shows precisely this graphical evaluation, depicting the variation of accuracy (of two datasets trained using 3 differents models) in sawteeth form when we start removing attributes.\n2. An analysis of those context changes related to attribute (feature) costs and the introduc-\ntion of solutions to create more versatile models and reframing transformations. In chapter 3, section 3.1, we define the notion of context and analyse how the operating context could change. We have also defined an approach for a reframing transformation, finding a way to build a versatile model which minimises the costs (Joint Cost), by introducing missing values on purpose. Given the exponential cost of this procedure, we introduce four new quadratic approximations in chapter 4.\n44\n3. The evaluation of the previous approaches with several datasets, using common repositories\nor the reframe domains. This point is achieved with the experiments in chapter 5, where we evaluate our approach on 6 different datasets and analyse the results obtained."}, {"heading": "6.2 Discussion", "text": "In the introduction we argued that we were looking for a flexible approach that could be used in a variety of circumstances. In fact, we were motivated by the following considerations:\n1. The method must work for any kind of predictive model, either human-made or trained\nfrom data using any off-the-shelf predictive modelling technique.\n2. Each example may have a different subset of missing values.\n3. Retraining the model for each example (using a subset of the examples with similar feature\nsubsets) is not an option (because of 2 above or other reasons).\n4. Both misclassification cost (MC) and test cost (TC) must be considered.\nWe have presented some graphical tools and an optimisation method that meets these requirements. In fact, this has to be compared to the usual approach which is specific on decision trees, with several approaches according to [45, 36]: (a) KV, a tree is rebuilt when missing values are found, (b) Null strategy: replace by an extra label (model is not rebuilt), (c) Internal node: creates nodes for examples with missing values (model is not rebuilt), and (d) C4.5 strategy: probabilistic approach (model is not rebuilt). Option (a) is infeasible if the situation 2 holds.\nAll the above options are specific to decision trees, so they are not able to take advantage of many other off-the-shelf techniques of our preferred data mining suite or machine learning library. This is an important limitation as many of the most powerful machine learning techniques used today, such as ensemble methods (using or not decision trees as base classifiers), support vector machines, etc., are much more difficult to adapt for minimising test costs.\nThe take-away message of this work is that we can use any machine learning technique, train a model on a dataset with the available attributes and possibly containing missing values,\n45\nand reframe it for a different deployment context where we have fewer available attributes, a different distribution of missing values, a different misclassification matrix and test cost vector. While the Full approach is intractable in general, we have introduced some approximations that are just quadratic, which are feasible for hundreds of attributes, which is already a high number of attributes if we are considering test costs. Also, during all the process we can explore the performance of several models using JROC curves. In fact, these curves are not specific for the methods we have introduced here; they could be used for the traditional methods used for decision trees or for the analysis of any cost context considering both MC and TC."}, {"heading": "6.3 Future work", "text": "This work opens many new avenues of future work. For instance, in section 3.2 we discuss that an alternative to the use of missing values is the use of ranges (see bullet 2). The approach presented here could also be compared or explored in combination to the mimetic technique to get models that use fewer attributes [20, 16, 9, 10, 8]. Another interesting idea would be the problem of quantification with test costs, which could be applied to both classification and regression [5, 7]. We have also been suggested1 to use decision stump ensembles, where the elements in the ensemble could be pruned a posteriori when the test cost is known.\nMore comparison with the area of feature selection could lead to a better understanding of the possibilities of reframing and better methods. For instance, the use of the attribute correlation can be used to an approximate notion of dominance (e.g., if two attributes have high correlation, the cost is expected to be related to the lowest test cost for any of them). As for the relation to other problems, we could also consider that the output domain may be null, as in abstaining classifiers [18, 41] and the notion of delegation [21] could be applied to this case. In fact, a missing value on purpose can be seen as the parallel of a reject option or abstention for the output value.\nThe notion of JROC curve could be further explored and extended. For instance, we could figure out other ways of drawing these curves, by using attribute correlation or some other order on the attributes. The issue of representing operating conditions when the the matrix and vector are not fixed could lead to more dimensions, or the inclusion of the cost matrix. At least in the case of binary classifiers we could have 3D surfaces, using, e.g., TPR, FPR and TC. As for\n1Peter Flach, personal communication\n46\nany curve representing cost for each operating condition, we wonder whether the area over the JROC curve means something, as in ROC analysis [11, 24]. Also we could ask the question of whether we can draw cost plots as in [14, 30, 32].\nFinally, there are more more ambitious ideas. We could investigate which attributes to use for each example. We could use reliability measures (especially in probabilistic classifiers) to make better decisions on whether to remove an attribute or not. We could analyse what to do when new attributes appear, using, e.g., the correlation to other attributes to derive the old attributes, or thinking about more general ways of representing the feature space. Finally, we think that there is no reason why most of the ideas introduced here could not work equally well for regression, combining the test cost with any regression loss."}, {"heading": "6.4 Dissemination", "text": "This work was presented in Brussels, in the REFRAME meeting (the complete information can be found here: http://www.reframe-d2k.org/index.php/Meetings_%26_Workshops ). Part of this report have been published as the ARXIV paper 1305.7111: \u201cTest cost and misclassification cost trade-off using reframing\u201d [40].\n47\nAppendices\n48"}, {"heading": "A Details of the results", "text": "In this appendix, the results of the experiments presented in the section 5 are detailed. In fact the experiments presented in figure 3.2 and figure 3.2 have been also performed using other machine learning methods: IBk, Bagging and Boosting (see section 2.5.3). The different results obtained are shown in the following plots.\nAll the figures A.1 A.3 A.5 A.2 A.4 and A.6 bring us to the same main conclusions: the best joint cost is not always obtained using all the features, or removing all of them. Moreover, the result vary with different machine learning methods when the dataset differs (for the iris dataset the best result is given by the Adaboost, but it is given by the SMO (SVM) for the Diabetes dataset).\n49\n50\n51\n52\nB Implementation details\nFor the implementation of the approach developed during this research, two principal sources files have been written in the R language, using the RWeka packages and executed in a R platform (see section 2.5).\nThe first one is used for the representation of all the differents plots shown previously (and more). Given a dataset or a set of datasets and a set of machine learning models (J48, IBk, and SMO in this case), this program first splits each dataset into train and test parts, then trains the train data with each model; after that it proceeds to the reframing of the model and evaluates each reframed model. the result of the accuracy is then represented in a graph, as the trade-off of misclassification and test cost, the JROC convex hull graphs. The result of the approximation of the methods defined in section 4 can also be shown.\n############################################################################## # # This is R code for calculating and drawing JROC curves (ROC curves for joint costs)\nand their convex hulls.\n# Joint costs include misclassification costs and test costs # # # This code has been developed by # Celestine-Periale Maguedong-Djoumessi, cemadj@posgrado.upv.es # Jose Hernandez-Orallo, jorallo@dsic.upv.es # UNIVERSITAT POLITECNICA DE VALENCIA, SPAIN # # ##############################################################################\n53\n######################################## # LIBRARIES ########################################\nlibrary(combinat) # combn library(RWeka)\nB.1 Function definition\nIn this section all the functions used in the main code are defined.\n######################################## # FUNCTION DEFINITION ########################################\n# This function sets to null a subset of the columns of a dataframe settonull <- function(test, vector) {\nif (is.null(vector)) {\ntest\n} else {\nfor(j in 1:length(vector)) {\ntest[,vector[j]] <- rep(\"?\",length(test[,1]))\n} test\n}\n}\n# Calculates the misclassification cost from rweka\u2019s textual output. # This function takes as input the result of an evaluation of a model on a dataset,\nand gives as result the field of misclassification cost.\nmisclascost <- function(e) {\nmylist <- strsplit(toString(e), \"\\n\") totalcoststring <- mylist[[1]][8] totalcoststringnoblanks <- gsub(\" \", \"\", totalcoststring)\n54\ntotalcoststringnoblanksonlynumber <- gsub(\"AverageCost\", \"\",\ntotalcoststringnoblanks)\ntotalcostnumber <- as.numeric(totalcoststringnoblanksonlynumber) totalcostnumber\n}\n# This calculate the leftmost intercept line for a set of points given in arrays X and\nY\nleftmost_intercept <- function(slope, X, Y) {\nbest_intercept <- Inf for (i in 1:length(X)) {\nx <- X[i] y <- Y[i] intercept <- y - slope*x if (intercept < best_intercept)\nbest_intercept <- intercept\n} best_intercept\n}\n# This calculates the leftmost intercept line for a set of points given in arrays X\nand Y.\n# But this one returns the points leftmost_intercept_xy <- function(slope, X, Y) {\nbest_intercept <- Inf for (i in 1:length(X)) {\nx <- X[i] y <- Y[i] intercept <- y - slope*x if (intercept < best_intercept) {\nbest_intercept <- intercept bestx <- x besty <- y\n}\n} c(bestx,besty)\n}\n55\nB.2 Main code\nHere, we define all the configuration needed before the experiment (definiton of the path where result will be saved, definition of the dataset(s) to be used and the operational context meaning the different costs.).\n######################################## # MAIN ########################################\n# Sets seed, so all executions do the same and the plots are comparable set.seed(2)\nWORKDIR <- \"C:/Users/periale/Desktop/tesis/images\" # Celestine\n#WORKDIR <- \"A:/__FAENA__/_TESIS Co-Dirigides/Celestine Maguedong/code\" # Jose setwd(WORKDIR)\n# Dataset selection\n#namedataset <- \"breast-cancer\"\nnamedataset <- \"diabetes\"\npath=\"\" # variable wich contains the path for the directory of the datasets if (WORKDIR==\"A:/__FAENA__/_TESIS Co-Dirigides/Celestine Maguedong/code\") { path=\"\" } else { path=\"C:/Users/periale/Desktop/datasets/UCI/\" }\nif (namedataset != \"iris\") {\nmydata <- read.arff(paste(path,namedataset,\".arff\", sep=\"\")) #mydata <- read.arff(\"diabetes.arff\")\n} else if (namedataset == \"iris\") {\ndata(iris) mydata <- iris\n}\n56\n# Gets length, number of attributes and number of classes len <- length(mydata[,1]) nattr <- length(mydata[1,]) for (i in 1:len) {\nfor (j in 1:(nattr-1)) {\nif (is.na(mydata[i,j]))\nmydata[i,j] <- \"?\"\n}\n}\nnclasses <- length(unique(mydata[,nattr]))\n# Ensure that the name of the class attribute is called \"class\" names(mydata)[nattr] <- \"class\"\n# Shuffles datasets and splits it shufindx <- sample(1:len, len) mydata <- mydata[shufindx,]\nlentrain <- trunc(len*2/3) train <- mydata[1:lentrain,] test <- mydata[(trunc(len*2/3)+1):len,] lentest <- length(test[,1])\n# Training\n# Defines test cost vector and misc cost matrix # There are several options depending on the experiment\n# Uniform, as for definition 2 testcostvector_U <- rep(1/(nattr-1), nattr-1) # nattr includes the class, so we need\nto remove 1\nmcvector_U <- matrix(rep(nclasses/(nclasses-1), nclasses^2), nclasses, nclasses) for (i in 1:nclasses)\nmcvector_U[i,i] <- 0\nalpha <- 0.5\n57\ntestcostvector_01_iris <- c(3, 2, 10, 5) mcvector_01_iris <- matrix(c(0,5,30,20,0,15,15,15,0),3,3) # In Weka, rows are actual\nvalues and columns are predicted values\ntestcostvector_02_diabetes <- c(2, 50, 5, 5, 20, 3, 10, 1) mcvector_02_diabetes <- matrix(c(0,200,50,0),2,2) # In Weka, rows are actual values\nand columns are predicted values\ncost_configuration <- \"UR\" #cost_configuration <- \"01\" # cost_configuration <- \"02\"\nif (cost_configuration == \"U\") {\ntestcostvector <- testcostvector_U mcvector <- mcvector_U\n} else if (cost_configuration == \"UR\") {\ntestcostvector <- testcostvector_U mcvector <- mcvector_U for (i in 1:length(testcostvector)) {\nk0 <- runif(1) k = exp(2*(k0-0.5)) testcostvector[i] <- testcostvector[i]*k\n} for (i in 1:length(mcvector[1,])) {\nfor (j in 1:length(mcvector[,1])) {\nk0 <- runif(1) k = exp(2*(k0-0.5)) mcvector[i,j] <- mcvector[i,j]*k\n}\n}\n} else if (cost_configuration == \"01\") {\ntestcostvector <- testcostvector_01_iris mcvector <- mcvector_01_iris\n} else if (cost_configuration == \"02\") {\ntestcostvector <- testcostvector_02_diabetes mcvector <- mcvector_02_diabetes\n}\n58\nprint(testcostvector) print(mcvector)\n# SOME OPTION TO MAKE THE PLOTS LOOK OKAY\nif (namedataset == \"diabetes\") {\nLEGEND_LOCATION <- \"bottomleft\" LEGEND_LOCATION0 <- LEGEND_LOCATION\n} else {\nLEGEND_LOCATION <- \"topright\" if (cost_configuration == \"U\") {\nLEGEND_LOCATION0 <- \"bottom\"\n} else {\nLEGEND_LOCATION0 <- LEGEND_LOCATION\n}\n}\n# ALPHAS THAT WE WILL USE ALPHAS <- c(0.03, 0.5, 0.9)\n# Training # define all the models used for the training of the dataset. models <- list() models_names <- list()\n# models <- c(models, list(J48(class ~ ., data = train))) # models_names <- c(models_names, \"DT\") # \"Tree\"\nmodels <- c(models, list(SMO(class ~ ., data = train))) models_names <- c(models_names, \"SMO\") # \"SVM\"\nmodels <- c(models, list(IBk(class ~ ., data = train))) models_names <- c(models_names, \"IBk\") # \"kNN\"\n# models <- c(models, list(AdaBoostM1(class ~ ., data = train, control =\nWeka_control(W = \"DecisionStump\"))))\n# models_names <- c(models_names, \"BstDS\")\n59\nmodels <- c(models, list(AdaBoostM1(class ~ ., data = train, control = Weka_control(W\n= list(J48, M = 30)))))\nmodels_names <- c(models_names, \"BstDT\")\n# models <- c(models, list(Bagging(class ~ ., data = train, control = Weka_control(W =\n\"DecisionStump\"))))\n# models_names <- c(models_names, \"BagDS\")\nmodels <- c(models, list(Bagging(class ~ ., data = train, control = Weka_control(W =\nlist(J48, M = 30)))))\nmodels_names <- c(models_names, \"BagDT\")\nlenmod <- length(models)\n# Initialisation of lists and structures for main loop mc1=0 mis1=0\n# These are the lists where we keep the values for TC, MC, JC and accuracy testcost <- rep(list(),lenmod) misccost <- rep(list(),lenmod) jointcost <- rep(list(),lenmod) accuracylist <- list()\nlabel <- \"-\" labelnames <- rep(list(),lenmod)\nmodifiedtestvector <- list()\n# objects use for the construction of box plot instances for the 3differents methods boxplotstrg1 <- NULL bp1 <- rep(list(), lenmod)\nbpnames <- NULL\n60\nlatticecounter <- 0\n# objects for the incremental methods\ntcpivotset <- NULL # Attributes removed for tcpivotset (originally none) tccounter <- rep(0,lenmod) # corrected tcselection <- list()\nmcpivotset1 <- NULL # Attributes removed for mcpivotset (originally none) mccounter1 <- rep(0,lenmod) # corrected mcselection <- list()\nmcpivotsetnew<-list() tcpivotsetnew<-list() jcpivotsetnew<-list()\njcpivotset1 <- NULL # Attributes removed for jcpivotset (originally none) jccounter1 <- rep(0,lenmod) # corrected jcselection <- list()\n# The outer loop is for each row in the lattice (removing i attributes) for (i in 0:(nattr-1)) { # it goes from 0 (no attributes removed) to nattr-1 (all\nattributes removed)\n# (Note that nattr includes the class, so nattr-1 is the\nactual number of attributes\ncat(\"\\n****Outer loop. Iteration: \", i, \"of \", nattr-1, \"****\\n\\n\")\n# What\u2019s the meaning of these pivots? tcpivot <- Inf mcpivot1 <- Inf jcpivot1 <- Inf\npivot <- 1 bpnames[i+1] <- i boxplotstrg1 <- NULL label<-\"_\"\n61\n# This calculates the combinations removing i attributes settonullmatrix <- t(combn(1:(nattr-1), i)) print(settonullmatrix) if (i == 0) {\nlen <- 0\n} else {\nlen <- length(settonullmatrix[,1])\n}\n# The inner loop is for each element in the row in the lattice for (j in (min(1,i):len)) {\nlatticecounter <- latticecounter + 1 cat(\"\\n ****Inner loop. Iteration: \", j, \"of \", len, \" Points in the lattice: \",\nlatticecounter, \"of\", 2^(nattr-1), \"****\\n\\n\")\ntcignore <- FALSE mcignore1 <- FALSE jcignore1 <- FALSE\nk=1 for (mod in models) {\nif (j==0) {\nmodifiedtest <- test label <- \"ALL\" myvector <- NULL\n} else {\nmyvector <- settonullmatrix[j,] modifiedtest <- settonull(test, myvector)\nnewattr <- setdiff(myvector, tcpivotset) # New attributes from the one fixed for\ntc in the previous lattice row\nif (length(newattr) != 1) {\ntcignore <- TRUE\n}\nnewattr <- setdiff(myvector, mcpivotset1) # New attributes from the one fixed for\nmc in the previous lattice row\n62\nif (length(newattr) != 1) {\nmcignore1 <- TRUE\n}\nnewattr <- setdiff(myvector, jcpivotset1) # New attributes from the one fixed for\njc in the previous lattice row\nif (length(newattr) != 1) {\njcignore1 <- TRUE\n}\n}\nif (!tcignore)\ntccounter[k] <- tccounter[k] + 1\nif (!mcignore1)\nmccounter1[k] <- mccounter1[k] + 1\nif (!jcignore1)\njccounter1[k] <- jccounter1[k] + 1\n# do whatever you need to do with modifiedtest\ne <- evaluate_Weka_classifier(mod,newdata=modifiedtest, cost= mcvector, complexity\n= TRUE,seed = 123, class = TRUE )\n# computation of the accuracy with the DT # for the first model res1 <- predict(mod, newdata = modifiedtest)\n# print(\"result with DT\") # print(e1) # summary(e1)\nhits <- 0 for (i in 1:lentest)\nif (res1[i] == modifiedtest[i,nattr])\nhits <- hits +1\naccuracy1 <- hits/ lentest\n# print(accuracy1) print(k)\n63\nif (length(accuracylist)<lenmod)\naccuracylist <- c(accuracylist, list(accuracy1)) else accuracylist[[k]] <- c(accuracylist[[k]], list(accuracy1))\nmc1 <- misclascost(e[1]) if (length(misccost)<lenmod)\nmisccost <- c(misccost, list(mc1)) else misccost[[k]] <- c(misccost[[k]], list(mc1))\n# computation of the total test cost (and creation of labels) tc <- 0 if (j==0) {\nlabel <- \"ALL\"\n} else {\nlabel <- \"\"\n} for (i in 1:(nattr-1)) {\nif (modifiedtest[1,i] != \"?\") {\ntc <- tc + testcostvector[i]\n}\n}\nif (length(testcost)<lenmod)\ntestcost <- c(testcost, list(tc)) else testcost[[k]] <- c(testcost[[k]],list(tc))\njc1 <- alpha*mc1 + (1-alpha)*tc\nif (length(jointcost)<lenmod)\njointcost <- c(jointcost,list(jc1)) else jointcost[[k]] <- c(jointcost[[k]], list(jc1))\n64\nif (!tcignore) { # tcignore == TRUE means that this point cannot be derived from\nthe previous row in the lattice\nif (length(tcselection)<lenmod) tcselection <- c(tcselection, latticecounter) else tcselection[[k]] <- c(tcselection[[k]], latticecounter) # Index of the\npoint\nif (tcpivot > tc) {\ntcpivot <- tc tcpivotsetnew <- myvector # Keeps track of the attributes of the best point for\njc\n}\n}\nif (!mcignore1) { # mcignore1 == TRUE means that this point cannot be derived from\nthe previous row in the lattice\nif (length(mcselection)<lenmod)\nmcselection <- c(mcselection, latticecounter) else mcselection[[k]] <- c(mcselection[[k]], latticecounter) # Index of the\npoint\nif (mcpivot1 > mc1) {\nmcpivot1 <- mc1 mcpivotsetnew <- myvector # Keeps track of the attributes of the best point for\njc\n}\n}\nif (!jcignore1) { # jcignore1 == TRUE means that this point cannot be derived from\nthe previous row in the lattice\nif (length(jcselection)<lenmod)\njcselection <- c(jcselection, latticecounter) else jcselection[[k]] <- c(jcselection[[k]], latticecounter) # Index of the\npoint\n65\nif (jcpivot1 > jc1) {\njcpivot1 <- jc1 jcpivotsetnew <- myvector # Keeps track of the attributes of the best point for\njc\n}\n}\nboxplotstrg1[pivot] <- jc1\npivot <- pivot+1\ncat(\"model:\", unlist(models_names[k]),\"\\n\")\ntcpivotset <- tcpivotsetnew cat(\"\\nBest TC: points explored: \", tccounter[k], \"\\n\") cat(\"Best TC: attributes removed: \", tcpivotset, \"\\n\")\nmcpivotset1 <- mcpivotsetnew cat(\"\\nBest MC: points explored: \", mccounter1[k], \"\\n\") # Remains to be adjust with\nthe others one\ncat(\"Best MC: attributes removed: \", mcpivotset1, \"\\n\")\njcpivotset1 <- jcpivotsetnew cat(\"\\nBest JC: points explored: \", jccounter1[k], \"\\n\") cat(\"Best JC: attributes removed: \", jcpivotset1, \"\\n\") k=k+1 } # end of inner loop\nfor (i in 1:(nattr-1)) {\nif (modifiedtest[1,i] == \"?\") { # computation of the labelnames of the\ncurrent modified test file.\nlabel <- paste(label,\"-\", i, sep=\"\")\n}\n}\nlabelnames <- c(labelnames, list(label))\n66\nmodifiedtestvector <- c(modifiedtestvector, list(modifiedtest)) # computation\nof the vector which contains all the differents modifiedtests\ncat(\"\\n\\n\")\nbp1 <-c(bp1,list(boxplotstrg1))\n}\n} # end of outer loop\n# The labels are converted into a vector x x=NULL for (i in 1:length(labelnames))\nx[i] <- labelnames[[i]]\n# We calculate some max and min for the plots\nmaxmisccost <- max(unlist(misccost))\nminmisccost <- min(unlist(misccost))\nmaxjointcost <- max(unlist(jointcost))\nminjointcost <- min(unlist(jointcost))\nmaxtestcost <- max(unlist(testcost)) mintestcost <- min(unlist(testcost))\n67\nmaxcost <- max(maxmisccost, maxjointcost, maxtestcost) mincost <- min(minmisccost, minjointcost, mintestcost)\nmaxaccuracy <- max(unlist(accuracylist))\nminaccuracy <- min(unlist(accuracylist))\nB.3 BMC, BTC, BJC, RND methods implementation:\nThis part of the code implements the four others approach (adding to the Full method) which approximate the JROC hull (see chapter 4): the Backward MC-guided (BMC), the Backward TC-guided (BTC), the Backward JC-guided (BJC) and Monte Carlo (RND).\nmisccosttc <- list() misccostmc <- list() misccostjc <- list() testcosttc <- list() testcostmc <- list() testcostjc <- list() misccostrnd <- list() testcostrnd <- list()\n##################### TC method incremental ########################## for (i in 1:lenmod) { misccosttc <- c(misccosttc, list(misccost[[i]][tcselection[[i]]])) testcosttc <- c(testcosttc,list(testcost[[i]][tcselection[[i]]])) }\n##################### MC method incremental ##########################\n68\nfor (i in 1:lenmod) { misccostmc <- c(misccostmc, list(misccost[[i]][mcselection[[i]]])) testcostmc <- c(testcostmc,list(testcost[[i]][mcselection[[i]]])) }\n##################### JC method incremental ##########################\nfor (i in 1:lenmod) { misccostjc <- c(misccostjc, list(misccost[[i]][jcselection[[i]]])) testcostjc <- c(testcostjc,list(testcost[[i]][jcselection[[i]]])) }\n##################### Monte Carlo method ##########################\nlatticesize <- length(modifiedtestvector) rndsamplesize <- (nattr-1)*(nattr)/2 + 1\nrndselection <- sample(1:latticesize, rndsamplesize, replace=FALSE)\n# lists for the monte carlo method. All the lists end with rnd (rnd) for (i in 1:lenmod) { misccostrnd <- c(misccostrnd, list(misccost[[i]][rndselection])) testcostrnd <- c(testcostrnd, list(testcost[[i]][rndselection])) }\nlabelnamesrnd <- labelnames[rndselection]\n69\n# The monte carlo labels are converted into a vector Y Y=NULL for (i in 1:length(labelnamesrnd))\nY[i] <- labelnamesrnd[[i]]\npoint_full<-NULL point_btc<-NULL point_bmc<-NULL point_bjc<-NULL point_rnd<-NULL\nalpha_iso<-0.5\nslope <- (1 - alpha_iso) / - alpha_iso\n# FULL METHOD\npoint_full <- leftmost_intercept_xy(slope, c(unlist(testcost)), c(unlist(misccost)) ) jcfull <- alpha_iso*point_full[2] + (1-alpha_iso)*point_full[1]\n# BMC METHOD\npoint_bmc <- leftmost_intercept_xy(slope, c(unlist(testcostmc)), c(unlist(misccostmc)\n))\njcbmc <- alpha_iso*point_bmc[2] + (1-alpha_iso)*point_bmc[1]\n# BTC METHOD\npoint_btc <- leftmost_intercept_xy(slope, c(unlist(testcosttc)), c(unlist(misccosttc)\n))\njcbtc <- alpha_iso*point_btc[2] + (1-alpha_iso)*point_btc[1]\n# BTC METHOD\npoint_bjc <- leftmost_intercept_xy(slope, c(unlist(testcostjc)), c(unlist(misccostjc)\n))\njcbjc <- alpha_iso*point_bjc[2] + (1-alpha_iso)*point_bjc[1]\n# BTC METHOD\npoint_rnd <- leftmost_intercept_xy(slope, c(unlist(testcostrnd)),\nc(unlist(misccostrnd) ))\njcrnd <- alpha_iso*point_rnd[2] + (1-alpha_iso)*point_rnd[1]\nc(jcfull, jcbmc, jcbtc, jcbjc, jcrnd) # Returns the five joint costs for the five\n70\ndifferent methods\nB.4 Plots representation\nThis last part of the code is used for the representation of the results obtained during the experiments in various graphs, which are saved as pdf files.\n################################################ #### PDF FILES ##### ################################################\n# PDF options PDFOPEN <- TRUE # If the plots are output on a PDF file PDFCLOSE <- PDFOPEN # Close the PDF file. This should match PDFOPEN except when you\nwant to draw several curves before closing.\nPDFheight= 10 # 7 is the default, so 14 makes it double higher than wide, 5\nmakes letters bigger (in proportion) for just one plot\nPDFwidth= 10 # (as above for width) 7 by default\n# Colours plot_colors <- c(\"green\", \"red\", \"blue\", \"orange\", \"yellow\", \"violet\", \"pink\") INTERCEPT_COLOUR <- \"darkgrey\"\n# Figure 1 : Accuracy vs attributes for all methods pdfname <- paste(\"accuracyall\", namedataset, \".pdf\", sep=\"\") # File name\nif (PDFOPEN) {\npdf(pdfname, height= PDFheight, width= PDFwidth)\n}\n71\npar(mar=c(8,6,5,5) + 0.1)\n# A=NULL # corrected # for (i in 1:length(accuracylist3)) # corrected # A[i] <- accuracylist3[[i]] # corrected\n#for draw the plot of accuracy for (i in 1:lenmod) {\nif (i==1) {\nplot(1:length(accuracylist[[1]]), accuracylist[[1]],ylab=\"Accuracy\",\nxlab=\"\",ylim=c(minaccuracy,maxaccuracy),lwd=3, xaxt=\"n\",cex.lab=2, type=\"l\", col=plot_colors[1])\naxis(1, at=1:length(x),srt=45, padj=1,las=2, lab=x)\n} else {\nlines(1:length(accuracylist[[i]]), accuracylist[[i]], type=\"l\",lty=i,lwd=3,\ncol=plot_colors[i])\n}\n} legend(LEGEND_LOCATION,c(unlist(models_names)), cex=1, col=plot_colors, lty=1:lenmod,\nlwd=3, bty=\"n\")\nif (PDFCLOSE) {\ndev.off()\n}\nfor(i in 1:lenmod) #for the graphs representing different cost for all methods {\n# Figure 2 (or 3) : All costs vs attributes for one method pdfname <- paste(\"costs\",models_names[i], cost_configuration, namedataset, \".pdf\",\nsep=\"\") # File name\nif (PDFOPEN) {\npdf(pdfname, height= PDFheight, width= PDFwidth)\n} par(mar=c(8,6,5,4) + 0.1)\n72\n#for the graphs representing different cost for one method\nplot(1:length(misccost[[i]]), misccost[[i]],ylab=\"\",\nxlab=\"\",ylim=c(mincost,maxcost),lwd=3, xaxt=\"n\",lty=2, type=\"l\",cex.lab=2,cex.axis=1.5, col=\"dark grey\")\naxis(1, at=1:length(x),srt=45, padj=1,las=2, lab=x) lines(1:length(jointcost[[i]]),jointcost[[i]], type=\"l\", lty=1, lwd=3, col=\"dark grey\") lines(1:length(testcost[[i]]), testcost[[i]], type=\"l\", lty=3, lwd=3, col=\"dark grey\") legend(LEGEND_LOCATION0, c(\"Joint Cost\",\"Misc. Cost\",\"Test Cost\"), cex=1.5, col=\"dark\ngrey\", lty=1:3, lwd=3, bty=\"n\")\nif (PDFCLOSE) {\ndev.off()\n}\n}\n#Figure 4 (or 5) pdfname <- paste(\"totalplot\", cost_configuration, namedataset, \".pdf\", sep=\"\") # File\nname\nif (PDFOPEN) {\npdf(pdfname, height= PDFheight, width= PDFwidth)\n} par(mar=c(5,6,5,2) + 0.1)\nfor (i in 1:lenmod) {\nif (i==1) {\nplot(testcost[[1]], misccost[[1]],ylab=\"MC\",\nxlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.lab=2,cex.axis=1.5, col=plot_colors[1])\n} else {\npoints(testcost[[i]],misccost[[i]], cex=2, pch=i,lwd=2, col=plot_colors[i])\n}\n}\nlegend(LEGEND_LOCATION, c(unlist(models_names)), cex=1.5, col=plot_colors,\npch=1:lenmod, lwd=3, bty=\"n\")\n73\nif (PDFCLOSE) {\ndev.off()\n}\n# figure 6 pdfname <- paste(\"totalplot\", cost_configuration, namedataset, \"-iso\", \".pdf\", sep=\"\")\n# File name\nif (PDFOPEN) {\npdf(pdfname, height= PDFheight, width= PDFwidth)\n} par(mar=c(5,6,5,2) + 0.1)\nfor (i in 1:lenmod) {\nif (i==1) {\nplot(testcost[[1]], misccost[[1]],ylab=\"MC\",\nxlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.lab=2,cex.axis=1.5, col=plot_colors[1])\n} else {\npoints(testcost[[i]],misccost[[i]], cex=2, pch=i,lwd=2, col=plot_colors[i])\n}\n}\nlegend(LEGEND_LOCATION, c(unlist(models_names)), cex=1.5, col=plot_colors,\npch=1:lenmod, lwd=3, bty=\"n\")\nfor (i in 1:length(ALPHAS)) {\nalpha1 <- ALPHAS[i] slope <- (1 - alpha1) / - alpha1 intercept <- leftmost_intercept(slope, c(unlist(testcost)), c(unlist(misccost)) ) abline(a=intercept,b=slope, col = INTERCEPT_COLOUR, lty=4)\n}\nif (PDFCLOSE) {\n74\ndev.off()\n}\n# figure 7 pdfname <- paste(\"totalplot\", cost_configuration, namedataset, \"-ch\", \".pdf\", sep=\"\")\n# File name\nif (PDFOPEN) {\npdf(pdfname, height= PDFheight, width= PDFwidth)\n} par(mar=c(5,6,5,2) + 0.1)\nfor (i in 1:lenmod) {\nif (i==1) {\nplot(testcost[[1]], misccost[[1]],ylab=\"MC\",\nxlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.lab=2,cex.axis=1.5, col=plot_colors[1])\n} else {\npoints(testcost[[i]],misccost[[i]], cex=2, pch=i,lwd=2, col=plot_colors[i])\n}\n}\nlegend(LEGEND_LOCATION, c(unlist(models_names)), cex=1.5, col=plot_colors,\npch=1:lenmod, lwd=3, lty=1:3, bty=\"n\")\nBIGX <- maxtestcost*10000 BIGY <- maxmisccost*10000\nfor (i in 1:lenmod) {\nX <- c(unlist(testcost[[i]]), 0, BIGX, BIGX) Y <- c(unlist(misccost[[i]]), BIGY, BIGY, 0) ch <- chull(X,Y) testcostch <- X[ch] misccostch <- Y[ch] lines(testcostch,misccostch, cex=2, lty=i, pch=i,lwd=2, col=plot_colors[i])\n}\n75\nif (PDFCLOSE) {\ndev.off()\n}\n# figure 8 pdfname <- paste(\"mcincremental\", cost_configuration, namedataset, \".pdf\", sep=\"\") #\nFile name\nif (PDFOPEN) {\npdf(pdfname, height= PDFheight, width= PDFwidth)\n} par(mar=c(5,6,5,2) + 0.1) for (i in 1:lenmod) { if(i==1) { plot(testcostmc[[1]], misccostmc[[1]],ylab=\"MC\",\nxlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.lab=2,cex.axis=1.5, col=plot_colors[1])\n} else { points(testcostmc[[i]], misccostmc[[i]], cex=2, pch=2,lwd=2, col=plot_colors[i]) } } legend(LEGEND_LOCATION, c(unlist(models_names)), cex=1.5, col=plot_colors, pch=1:3,\nlwd=3, bty=\"n\")\nBIGX <- maxtestcost*10000 BIGY <- maxmisccost*10000\nfor (i in 1:lenmod) {\n76\nX <- c(unlist(testcostmc[[i]]), 0, BIGX, BIGX) Y <- c(unlist(misccostmc[[i]]), BIGY, BIGY, 0) ch <- chull(X,Y) testcostch <- X[ch] misccostch <- Y[ch] lines(testcostch,misccostch, cex=2, lty=i, pch=i,lwd=2, col=plot_colors[i])\n}\nif (PDFCLOSE) {\ndev.off()\n}\n# figure 9 pdfname <- paste(\"jcincremental\", cost_configuration, namedataset, \".pdf\", sep=\"\") #\nFile name\nif (PDFOPEN) {\npdf(pdfname, height= PDFheight, width= PDFwidth)\n}\npar(mar=c(5,6,5,2) + 0.1) for (i in 1:lenmod) { if(i==1) { plot(testcostjc[[1]], misccostjc[[1]],ylab=\"MC\",\nxlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.lab=2,cex.axis=1.5, col=plot_colors[1])\n} else { points(testcostjc[[i]], misccostjc[[i]], cex=2, pch=2,lwd=2, col=plot_colors[i]) } } legend(LEGEND_LOCATION, c(unlist(models_names)), cex=1.5, col=plot_colors, pch=1:3,\nlwd=3, bty=\"n\")\nBIGX <- maxtestcost*10000 BIGY <- maxmisccost*10000\n77\nfor (i in 1:lenmod) {\nX <- c(unlist(testcostjc[[i]]), 0, BIGX, BIGX) Y <- c(unlist(misccostjc[[i]]), BIGY, BIGY, 0) ch <- chull(X,Y) testcostch <- X[ch] misccostch <- Y[ch] lines(testcostch,misccostch, cex=2, lty=i, pch=i,lwd=2, col=plot_colors[i])\n}\nif (PDFCLOSE) {\ndev.off()\n}\n#figure 10 pdfname <- paste(\"tcincremental\", cost_configuration, namedataset, \".pdf\", sep=\"\") #\nFile name\nif (PDFOPEN) {\npdf(pdfname, height= PDFheight, width= PDFwidth)\n} par(mar=c(5,6,5,2) + 0.1)\nfor (i in 1:lenmod) { if(i==1) { plot(testcosttc[[1]], misccosttc[[1]],ylab=\"MC\",\nxlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.lab=2,cex.axis=1.5, col=plot_colors[1])\n} else { points(testcosttc[[i]], misccosttc[[i]], cex=2, pch=2,lwd=2, col=plot_colors[i]) } } legend(LEGEND_LOCATION, c(unlist(models_names)), cex=1.5, col=plot_colors, pch=1:3,\nlwd=3, bty=\"n\")\n78\nBIGX <- maxtestcost*10000 BIGY <- maxmisccost*10000\nfor (i in 1:lenmod) {\nX <- c(unlist(testcosttc[[i]]), 0, BIGX, BIGX) Y <- c(unlist(misccosttc[[i]]), BIGY, BIGY, 0) ch <- chull(X,Y) testcostch <- X[ch] misccostch <- Y[ch] lines(testcostch,misccostch, cex=2, lty=i, pch=i,lwd=2, col=plot_colors[i])\n}\nif (PDFCLOSE) {\ndev.off()\n}\n# figure 11 pdfname <- paste(\"rnd\", cost_configuration, namedataset, \".pdf\", sep=\"\") # File name\nif (PDFOPEN) {\npdf(pdfname, height= PDFheight, width= PDFwidth)\n} par(mar=c(5,6,5,2) + 0.1) plot(testcostrnd[[1]], misccostrnd[[1]],ylab=\"MC\",\nxlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.lab=2,cex.axis=1.5, col=plot_colors[1])\npoints(testcostrnd[[1]],misccostrnd[[2]], cex=2, pch=2,lwd=2, col=plot_colors[2]) points(testcostrnd[[1]],misccostrnd[[3]], cex=2, pch=3,lwd=2, col=plot_colors[3]) legend(LEGEND_LOCATION, c(unlist(models_names)), cex=1.5, col=plot_colors, pch=1:3,\nlwd=3, bty=\"n\")\nBIGX <- maxtestcost*10000 BIGY <- maxmisccost*10000\n79\nfor (i in 1:lenmod) {\nX <- c(unlist(testcostrnd[[i]]), 0, BIGX, BIGX) Y <- c(unlist(misccostrnd[[i]]), BIGY, BIGY, 0) ch <- chull(X,Y) testcostch <- X[ch] misccostch <- Y[ch] lines(testcostch,misccostch, cex=2, lty=i, pch=i,lwd=2, col=plot_colors[i])\n}\nif (PDFCLOSE) {\ndev.off()\n}\nBOXPLOTS <- FALSE\nif (BOXPLOTS) {\n# figure boxplot pdfname <- paste(\"boxplotDT\", cost_configuration, namedataset, \".pdf\", sep=\"\") # File\nname\nif (PDFOPEN) {\npdf(pdfname, height= PDFheight, width= PDFwidth)\n} par(mar=c(5,6,5,2) + 0.1)\nboxplot(bp1,names=bpnames, ylab=\"JC\", xlab=\"Features Removed\")\n80\nif (PDFCLOSE) {\ndev.off()\n}\n# figure boxplot pdfname <- paste(\"boxplotSMO\", cost_configuration, namedataset, \".pdf\", sep=\"\") # File\nname\nif (PDFOPEN) {\npdf(pdfname, height= PDFheight, width= PDFwidth)\n} par(mar=c(5,6,5,2) + 0.1)\nboxplot(bp2,names=bpnames, ylab=\"JC\", xlab=\"Features Removed\")\nif (PDFCLOSE) {\ndev.off()\n}\n# figure boxplot pdfname <- paste(\"boxplotIBk\", cost_configuration, namedataset, \".pdf\", sep=\"\") # File\nname\nif (PDFOPEN) {\npdf(pdfname, height= PDFheight, width= PDFwidth)\n} par(mar=c(5,6,5,2) + 0.1)\nboxplot(bp3,names=bpnames, ylab=\"JC\", xlab=\"Features Removed\")\nif (PDFCLOSE) {\ndev.off()\n}\n81\n} # end if\nB.5 Experiment implementation\nThe second source implements the macro-function \u201cONE-EXPERIMENT\u201d, which performs the experiments described in section 5. In fact, this function, giving a value for \u03b1 (the isometric value used to give a certain weight to gauge the relevance to the misclassification and test cost), a dataset and a list of different machine learning methods (predictive model algoritms), computes the best features configuration, i.e., the configuration of features which gives the best Joint Cost for each of the 5 methods (Full, BMC, BTC, BJC, RND). In the implementation of this macro-function, there are some other functions previously defined which are also used, thus repeated here.\n############################################################################## # # This is R code for computing the ONE-EXPERIMENT macro-function. # Joint costs include misclassification costs and test costs # # # This code has been developed by # Celestine-Periale Maguedong-Djoumessi, cemadj@posgrado.upv.es # Jose Hernandez-Orallo, jorallo@dsic.upv.es # UNIVERSITAT POLITECNICA DE VALENCIA, SPAIN # # ##############################################################################\n######################################## # LIBRARIES ########################################\n82\nlibrary(combinat) # combn library(RWeka)\n######################################## # FUNCTION DEFINITION ########################################\n# This function sets to null a subset of the columns of a dataframe settonull <- function(test, vector) {\nif (is.null(vector)) {\ntest\n} else {\nfor(j in 1:length(vector)) {\ntest[,vector[j]] <- rep(\"?\",length(test[,1]))\n} test\n}\n}\n# Calculates the misclassification cost from rweka\u2019s textual output misclascost <- function(e) {\nmylist <- strsplit(toString(e), \"\\n\") totalcoststring <- mylist[[1]][8] totalcoststringnoblanks <- gsub(\" \", \"\", totalcoststring) totalcoststringnoblanksonlynumber <- gsub(\"AverageCost\", \"\",\ntotalcoststringnoblanks)\ntotalcostnumber <- as.numeric(totalcoststringnoblanksonlynumber) totalcostnumber\n}\n# This calculate the leftmost intercept line for a set of points given in arrays X and\nY\nleftmost_intercept <- function(slope, X, Y) {\nbest_intercept <- Inf for (i in 1:length(X)) {\nx <- X[i] y <- Y[i]\n83\nintercept <- y - slope*x if (intercept < best_intercept)\nbest_intercept <- intercept\n} best_intercept\n}\n# This calculates the leftmost intercept line for a set of points given in arrays X\nand Y.\n# But this one returns the points leftmost_intercept_xy <- function(slope, X, Y) {\nbest_intercept <- Inf for (i in 1:length(X)) {\nx <- X[i] y <- Y[i] intercept <- y - slope*x if (intercept < best_intercept) {\nbest_intercept <- intercept bestx <- x besty <- y\n}\n} c(bestx,besty)\n}\n######################################## # MAIN ########################################\n# Sets seed, so all executions do the same and the plots are comparable set.seed(2)\n# WORKDIR <- \"C:/Users/periale/Desktop/tesis/images\" # Celestine #WORKDIR <- \"E:/__FAENA__/_TESIS Co-Dirigides/Celestine Maguedong/code\" # Jose WORKDIR <- \"A:/__FAENA__/_TESIS Co-Dirigides/Celestine Maguedong/experiments\" # Jose\n84\nsetwd(WORKDIR)\n########################################################################## ############## MACRO-FUNCTION FOR ONE EXPERIMENT ######################### ##########################################################################\n# INPUTS: models_names, namedataset, alpha\nOne_experiment<- function(models_names, namedataset, alpha, VERBOSE=FALSE,\ncost_configuration) {\n# OUTPUT: # c(jcfull, jcbmc, jcbtc, jcbjc, jcrnd)\n# we DO re-train the models. lenmod <- length(models_names)\n85\npath<-\"\"\n# Dataset selection\n# namedataset <- \"iris\" # namedataset <- \"diabetes\"\nif (namedataset != \"iris\") {\nmydata <- read.arff(paste(path,namedataset,\".arff\",sep=\"\")) #mydata <- read.arff(\"diabetes.arff\")\n} else if (namedataset == \"iris\") {\ndata(iris) mydata <- iris\n}\n# Gets length, number of attributes and number of classes len <- length(mydata[,1]) nattr <- length(mydata[1,]) nclasses <- length(unique(mydata[,nattr]))\n# Ensure that the name of the class attribute is called \"class\" names(mydata)[nattr] <- \"class\"\n# Shuffles datasets and splits it shufindx <- sample(1:len, len) mydata <- mydata[shufindx,]\nlentrain <- trunc(len*2/3) train <- mydata[1:lentrain,] test <- mydata[(trunc(len*2/3)+1):len,] lentest <- length(test[,1])\n# Defines test cost vector and misc cost matrix\n86\n# There are several options depending on the experiment\n# Uniform, as for definition 2 testcostvector_U <- rep(1/(nattr-1), nattr-1) # nattr includes the class, so we need\nto remove 1\nmcvector_U <- matrix(rep(nclasses/(nclasses-1), nclasses^2), nclasses, nclasses) for (i in 1:nclasses)\nmcvector_U[i,i] <- 0\n#testcostvector <- c(2,50,5,5,20,3,10,1) #mcvector <- matrix(c(0,50,200,0),2,2)\n# cost_configuration <- \"U\" # cost_configuration <- \"01\" # cost_configuration <- \"02\"\nif (cost_configuration == \"U\") {\ntestcostvector <- testcostvector_U mcvector <- mcvector_U\n} else if (cost_configuration == \"UR\") {\ntestcostvector <- testcostvector_U mcvector <- mcvector_U for (i in 1:length(testcostvector)) {\nk0 <- runif(1)\n# k = exp(2*(k0-0.5))\nk = exp(UR_FACTOR*(k0-0.5))\ntestcostvector[i] <- testcostvector[i]*k\n}\n# normalise vector testcostvector <- testcostvector / sum(testcostvector)\nfor (i in 1:length(mcvector[1,])) {\nfor (j in 1:length(mcvector[,1])) {\nk0 <- runif(1)\n# k = exp(2*(k0-0.5))\nk = exp(UR_FACTOR*(k0-0.5)) mcvector[i,j] <- mcvector[i,j]*k\n}\n}\n87\n# normalise matrix mcvector <- mcvector / sum(mcvector)\n# print(testcostvector) # print(mcvector)\n} else if (cost_configuration == \"01\") {\ntestcostvector_01_iris <- c(3, 2, 10, 5) mcvector_01_iris <- matrix(c(0,5,30,20,0,15,15,15,0),3,3) # In Weka, rows are actual\nvalues and columns are predicted values\ntestcostvector <- testcostvector_01_iris mcvector <- mcvector_01_iris\n} else if (cost_configuration == \"02\") {\ntestcostvector_02_diabetes <- c(2, 50, 5, 5, 20, 3, 10, 1) mcvector_02_diabetes <- matrix(c(0,200,50,0),2,2) # In Weka, rows are actual values\nand columns are predicted values\ntestcostvector <- testcostvector_02_diabetes mcvector <- mcvector_02_diabetes\n}\n# Training models <- list()\nfor (i in models_names) {\nif (i == \"J48\") {\nmodels <- c(models, list(J48(class ~ ., data = train)))\n} else if (i == \"SMO\") {\nmodels <- c(models, list(SMO(class ~ ., data = train)))\n} else if (i == \"IBk\") {\nmodels <- c(models, list(IBk(class ~ ., data = train)))\n88\n} else if (i == \"BstDS\") {\nmodels <- c(models, list(AdaBoostM1(class ~ ., data = train, control =\nWeka_control(W = \"DecisionStump\"))))\n} else if (i == \"BstDT\") {\nmodels <- c(models, list(AdaBoostM1(class ~ ., data = train, control =\nWeka_control(W = list(J48, M = 30)))))\n} else if (i == \"BagDS\") {\nmodels <- c(models, list(Bagging(class ~ ., data = train, control = Weka_control(W\n= \"DecisionStump\"))))\n} else if (i == \"BagDT\") {\nmodels <- c(models, list(Bagging(class ~ ., data = train, control = Weka_control(W\n= list(J48, M = 30)))))\n} else {\ncat(\"\\n\\nERROR: model name unknown\\n\\n\") abort()\n}\n}\n# Initialisation of lists and structures for main loop mc1=0 mis1=0\n# These are the lists where we keep the values for TC, MC, JC and accuracy testcost <- rep(list(),lenmod) misccost <- rep(list(),lenmod) jointcost <- rep(list(),lenmod) accuracylist <- list()\nlabel <- \"-\" labelnames <- rep(list(),lenmod)\nmodifiedtestvector <- list()\n# objects use for the construction of box plot instances for the 3differents methods\n89\nboxplotstrg1 <- NULL bp1 <- rep(list(), lenmod)\nbpnames <- NULL\nlatticecounter <- 0\n# objects for the incremental methods\ntcpivotset <- NULL # Attributes removed for tcpivotset (originally none) tccounter <- rep(0,lenmod) # corrected tcselection <- list()\nmcpivotset1 <- NULL # Attributes removed for mcpivotset (originally none) mccounter1 <- rep(0,lenmod) # corrected mcselection <- list()\njcpivotset1 <- NULL # Attributes removed for jcpivotset (originally none) jccounter1 <- rep(0,lenmod) # corrected jcselection <- list()\nmcpivotsetnew<-list() tcpivotsetnew<-list() jcpivotsetnew<-list()\n# The outer loop is for each row in the lattice (removing i attributes) for (i in 0:(nattr-1)) { # it goes from 0 (no attributes removed) to nattr-1 (all\nattributes removed)\n# (Note that nattr includes the class, so nattr-1 is the\nactual number of attributes\nif (VERBOSE) cat(\"\\n****Outer loop. Iteration: \", i, \"of \", nattr-1, \"****\\n\\n\")\n# What\u2019s the meaning of these pivots? tcpivot <- Inf mcpivot1 <- Inf jcpivot1 <- Inf\n90\npivot <- 1 bpnames[i+1] <- i boxplotstrg1 <- NULL label<-\"_\"\n# This calculates the combinations removing i attributes settonullmatrix <- t(combn(1:(nattr-1), i)) if (VERBOSE) print(settonullmatrix) if (i == 0) {\nlen <- 0\n} else {\nlen <- length(settonullmatrix[,1])\n}\n# The inner loop is for each element in the row in the lattice for (j in (min(1,i):len)) {\nlatticecounter <- latticecounter + 1 if (VERBOSE) cat(\"\\n ****Inner loop. Iteration: \", j, \"of \", len, \" Points in the\nlattice: \", latticecounter, \"of\", 2^(nattr-1), \"****\\n\\n\")\ntcignore <- FALSE mcignore1 <- FALSE jcignore1 <- FALSE\nk=1 for (mod in models) {\nif (j==0) {\nmodifiedtest <- test label <- \"ALL\" myvector <- NULL\n} else {\nmyvector <- settonullmatrix[j,] modifiedtest <- settonull(test, myvector)\nnewattr <- setdiff(myvector, tcpivotset) # New attributes from the one fixed for\ntc in the previous lattice row\n91\nif (length(newattr) != 1) {\ntcignore <- TRUE\n}\nnewattr <- setdiff(myvector, mcpivotset1) # New attributes from the one fixed for\nmc in the previous lattice row\nif (length(newattr) != 1) {\nmcignore1 <- TRUE\n}\nnewattr <- setdiff(myvector, jcpivotset1) # New attributes from the one fixed for\njc in the previous lattice row\nif (length(newattr) != 1) {\njcignore1 <- TRUE\n}\n}\nif (!tcignore)\ntccounter[k] <- tccounter[k] + 1\nif (!mcignore1)\nmccounter1[k] <- mccounter1[k] + 1\nif (!jcignore1)\njccounter1[k] <- jccounter1[k] + 1\n# do whatever you need to do with modifiedtest\ne <- evaluate_Weka_classifier(mod,newdata=modifiedtest, cost= mcvector, complexity\n= TRUE,seed = 123, class = TRUE )\n# computation of the accuracy with the DT # for the first model res1 <- predict(mod, newdata = modifiedtest)\n# print(\"result with DT\") # print(e1) # summary(e1)\nhits <- 0 for (i in 1:lentest)\n92\nif (res1[i] == modifiedtest[i,nattr])\nhits <- hits +1\naccuracy1 <- hits/ lentest\n# print(accuracy1)\nif (VERBOSE) print(k)\nif (length(accuracylist)<lenmod)\naccuracylist <- c(accuracylist, list(accuracy1)) else accuracylist[[k]] <- c(accuracylist[[k]], list(accuracy1))\nmc1 <- misclascost(e[1]) if (length(misccost)<lenmod)\nmisccost <- c(misccost, list(mc1)) else misccost[[k]] <- c(misccost[[k]], list(mc1))\n# computation of the total test cost (and creation of labels) tc <- 0 if (j==0) {\nlabel <- \"ALL\"\n} else {\nlabel <- \"\"\n} for (i in 1:(nattr-1)) {\nif (modifiedtest[1,i] != \"?\") {\ntc <- tc + testcostvector[i]\n}\n}\nif (length(testcost)<lenmod)\ntestcost <- c(testcost, list(tc)) else testcost[[k]] <- c(testcost[[k]],list(tc))\njc1 <- alpha*mc1 + (1-alpha)*tc\nif (length(jointcost)<lenmod)\n93\njointcost <- c(jointcost,list(jc1)) else jointcost[[k]] <- c(jointcost[[k]], list(jc1))\nif (!tcignore) { # tcignore == TRUE means that this point cannot be derived from\nthe previous row in the lattice\nif (length(tcselection)<lenmod) tcselection <- c(tcselection, latticecounter) else tcselection[[k]] <- c(tcselection[[k]], latticecounter) # Index of the\npoint\nif (tcpivot > tc) {\ntcpivot <- tc tcpivotsetnew <- myvector # Keeps track of the attributes of the best point for\njc\n}\n}\nif (!mcignore1) { # mcignore1 == TRUE means that this point cannot be derived from\nthe previous row in the lattice\nif (length(mcselection)<lenmod)\nmcselection <- c(mcselection, latticecounter) else mcselection[[k]] <- c(mcselection[[k]], latticecounter) # Index of the\npoint\nif (mcpivot1 > mc1) {\nmcpivot1 <- mc1 mcpivotsetnew <- myvector # Keeps track of the attributes of the best point for\njc\n}\n}\nif (!jcignore1) { # jcignore1 == TRUE means that this point cannot be derived from\nthe previous row in the lattice\n94\nif (length(jcselection)<lenmod)\njcselection <- c(jcselection, latticecounter) else jcselection[[k]] <- c(jcselection[[k]], latticecounter) # Index of the\npoint\nif (jcpivot1 > jc1) {\njcpivot1 <- jc1 jcpivotsetnew <- myvector # Keeps track of the attributes of the best point for\njc\n}\n}\nboxplotstrg1[pivot] <- jc1\npivot <- pivot+1\nif (VERBOSE) cat(\"model:\", unlist(models_names[k]),\"\\n\") tcpivotset <- tcpivotsetnew if (VERBOSE) {\ncat(\"\\nBest TC: points explored: \", tccounter[k], \"\\n\") cat(\"Best TC: attributes removed: \", tcpivotset, \"\\n\")\n}\nmcpivotset1 <- mcpivotsetnew if (VERBOSE) {\ncat(\"\\nBest MC: points explored: \", mccounter1[k], \"\\n\") # Remains to be adjust\nwith the others one\ncat(\"Best MC: attributes removed: \", mcpivotset1, \"\\n\")\n}\njcpivotset1 <- jcpivotsetnew if (VERBOSE) {\ncat(\"\\nBest JC: points explored: \", jccounter1[k], \"\\n\")\ncat(\"Best JC: attributes removed: \", jcpivotset1, \"\\n\")\n}\n95\nk=k+1\n} # end of inner loop\nfor (i in 1:(nattr-1)) {\nif (modifiedtest[1,i] == \"?\") { # computation of the labelnames of the\ncurrent modified test file.\nlabel <- paste(label,\"-\", i, sep=\"\")\n}\n}\nlabelnames <- c(labelnames, list(label))\nmodifiedtestvector <- c(modifiedtestvector, list(modifiedtest)) # computation\nof the vector which contains all the differents modifiedtests\nif (VERBOSE) cat(\"\\n\\n\")\nbp1 <-c(bp1,list(boxplotstrg1))\n}\n} # end of outer loop\n# The labels are converted into a vector x x=NULL for (i in 1:length(labelnames))\nx[i] <- labelnames[[i]]\n# We calculate some max and min for the plots\nmaxmisccost <- max(unlist(misccost))\n96\nminmisccost <- min(unlist(misccost))\nmaxjointcost <- max(unlist(jointcost))\nminjointcost <- min(unlist(jointcost))\nmaxtestcost <- max(unlist(testcost)) mintestcost <- min(unlist(testcost))\nmaxcost <- max(maxmisccost, maxjointcost, maxtestcost) mincost <- min(minmisccost, minjointcost, mintestcost)\nmaxaccuracy <- max(unlist(accuracylist))\nminaccuracy <- min(unlist(accuracylist))\nmisccosttc <- list() misccostmc <- list() misccostjc <- list() testcosttc <- list() testcostmc <- list() testcostjc <- list() misccostrnd <- list() testcostrnd <- list()\n##################### TC method incremental ########################## for (i in 1:lenmod) { misccosttc <- c(misccosttc, list(misccost[[i]][tcselection[[i]]])) testcosttc <- c(testcosttc,list(testcost[[i]][tcselection[[i]]])) }\n97\n##################### MC method incremental ##########################\nfor (i in 1:lenmod) { misccostmc <- c(misccostmc, list(misccost[[i]][mcselection[[i]]])) testcostmc <- c(testcostmc,list(testcost[[i]][mcselection[[i]]])) }\n##################### JC method incremental ##########################\nfor (i in 1:lenmod) { misccostjc <- c(misccostjc, list(misccost[[i]][jcselection[[i]]])) testcostjc <- c(testcostjc,list(testcost[[i]][jcselection[[i]]])) }\n##################### Monte Carlo method ##########################\nlatticesize <- length(modifiedtestvector) rndsamplesize <- (nattr-1)*(nattr)/2 + 1\nrndselection <- sample(1:latticesize, rndsamplesize, replace=FALSE)\n# lists for the monte carlo method. All the lists end with rnd (rnd) for (i in 1:lenmod) { misccostrnd <- c(misccostrnd, list(misccost[[i]][rndselection])) testcostrnd <- c(testcostrnd, list(testcost[[i]][rndselection])) }\n98\nlabelnamesrnd <- labelnames[rndselection]\n# The monte carlo labels are converted into a vector Y Y=NULL for (i in 1:length(labelnamesrnd))\nY[i] <- labelnamesrnd[[i]]\npoint_full<-NULL point_btc<-NULL point_bmc<-NULL point_bjc<-NULL point_rnd<-NULL\nslope <- (1 - alpha) / - alpha\n# FULL METHOD\npoint_full <- leftmost_intercept_xy(slope, c(unlist(testcost)), c(unlist(misccost)) ) jcfull <- alpha*point_full[2] + (1-alpha)*point_full[1]\n# BMC METHOD\npoint_bmc <- leftmost_intercept_xy(slope, c(unlist(testcostmc)), c(unlist(misccostmc)\n))\njcbmc <- alpha*point_bmc[2] + (1-alpha)*point_bmc[1]\n# BTC METHOD\npoint_btc <- leftmost_intercept_xy(slope, c(unlist(testcosttc)), c(unlist(misccosttc)\n))\njcbtc <- alpha*point_btc[2] + (1-alpha)*point_btc[1]\n# BTC METHOD\npoint_bjc <- leftmost_intercept_xy(slope, c(unlist(testcostjc)), c(unlist(misccostjc)\n))\njcbjc <- alpha*point_bjc[2] + (1-alpha)*point_bjc[1]\n# BTC METHOD\npoint_rnd <- leftmost_intercept_xy(slope, c(unlist(testcostrnd)),\n99\nc(unlist(misccostrnd) ))\njcrnd <- alpha*point_rnd[2] + (1-alpha)*point_rnd[1]\nc(jcfull, jcbmc, jcbtc, jcbjc, jcrnd) # Returns the five joint costs for the five\ndifferent methods\n} ########################################################################## ############## END MACRO-FUNCTION FOR ONE EXPERIMENT ##################### ##########################################################################\n########################################################################## ############## We perform the experiments with several alphas, datasets and repetitions\nmodels_names <- list()\n# models_names <- c(models_names, \"J48\") # \"DT\" models_names <- c(models_names, \"SMO\") # \"SVM\" models_names <- c(models_names, \"IBk\") # \"kNN\" # models_names <- c(models_names, \"BstDS\") models_names <- c(models_names, \"BstDT\") # models_names <- c(models_names, \"BagDS\") models_names <- c(models_names, \"BagDT\")\n# Datasets #datasetnames <- c(\"glass\") datasetnames <- c(\"diabetes\") #datasetnames <- c(\"iris\")\n100\n#datasetnames <-\nc(\"iris\",\"breast-w\",\"breast-cancer\",\"diabetes\",\"glass\",\"balance-scale\") # vector which contains the dataset names (8 datasets at final) on which we will performed the one-experiment function\nnum_datasets <- length(datasetnames)\n# ALPHAS THAT WE WILL USE FOR THE EXPERIMENTS ALPHAS_ISO <- c(0.5) #ALPHAS_ISO <- c( 0.1, 0.3, 0.5, 0.7, 0.9) num_alphas <- length(ALPHAS_ISO)\n# Repetitions REPETITIONS <- 4\n# Methods #methods <- c(\"jcfull\", \"jcbmc\",\"jcbtc\",\"jcbjc\",\"jcrnd\") methods <- c(\"Full\", \"BMC\",\"BTC\",\"BJC\",\"RND\") num_methods <- length(methods)\n# Cost configuration # cost_configuration <- \"U\"\ncost_configuration <- \"UR\" UR_FACTOR <- 10\n# cost_configuration <- \"01\" # Only for iris # cost_configuration <- \"02\" # only for diabetes\nNUM_EXPERIMENTS <- num_alphas * num_datasets * REPETITIONS\n# Matrices mdat <- matrix(data=NA, nrow = num_methods, ncol = NUM_EXPERIMENTS, byrow = TRUE,\ndimnames = list(methods ))\ntab <- matrix(data=NA, nrow = num_datasets, ncol = num_methods, byrow = TRUE,\ndimnames = list(c(1:num_datasets),methods ))\n# matrix which will contains all the means and sd for the datasets\ntab2 <- matrix(data=NA, nrow = num_alphas, ncol = num_methods, byrow = TRUE,\n101\ndimnames = list(ALPHAS_ISO,methods )) #\nmatrix which will contains all the means and sd for the datasets\ntab3 <- matrix(data=NA, nrow = num_alphas*num_datasets, ncol = num_methods, byrow =\nTRUE,\ndimnames = list((1:(num_alphas*num_datasets)),methods ))\n# matrix which will contains all the means and sd for\nthe datasets\n# Experiments begin\ncat(\"\\n\\n\\n\\n\\n\\n#################### EXPERIMENTS BEGIN!!! ##############\\n\\n\")\nk=0\nfor (dataset in datasetnames) {\ncat(\" Dataset: \", dataset, \"\\n\") result=NULL for (i in ALPHAS_ISO) {\ncat(\" Alpha: \", i, \"\\n\") for(j in 1:REPETITIONS) {\nk<- k+1\ncat(\" Repetition: \", j, \"\\n\") cat(\" Experiment num.: \", k, \"of\", NUM_EXPERIMENTS, \"\\n\")\nresult <- One_experiment(models_names, dataset, i, VERBOSE=FALSE,\ncost_configuration) # performs the one-experiment 10 times with the same value of alpha but with different partition of the data set.\nmdat[,k]<-result # remains construct the matrix which contains the sd and medians values\n}\n}\nd=d+1\n} cat(\"\\n\\n#################### EXPERIMENTS END!!! ##############\\n\\n\")\n102\ncat(\"We now calculate matrices...\")\n# First matrix for (d in 1:num_datasets) {\nfor (j in 1: num_methods) {\nrange0 <- (d-1)*num_alphas*REPETITIONS + 1 range1 <- (d)*num_alphas*REPETITIONS my_subset <- range0:range1\nv <- as.vector(t(mdat[j,my_subset])) my_mean <- mean(v) my_sd <- sd(v) tab[d,j]<-paste(my_mean,\"+/-\",my_sd,sep=\"\")\n}\n}\n# Second matrix for (a in 1:num_alphas) {\nfor (j in 1: num_methods) {\nmy_subset <- NULL for (d in 1:num_datasets) {\nrange0 <- (d-1)*REPETITIONS*num_alphas + (a-1)*REPETITIONS + 1 range1 <- (d-1)*REPETITIONS*num_alphas + (a)*REPETITIONS my_subset <- c(my_subset, range0:range1)\n} # 1:4, 9:14 17:21 print(my_subset) v <- as.vector(t(mdat[j,my_subset])) my_mean <- mean(v) my_sd <- sd(v) tab2[a,j]<-paste(my_mean,\"+/-\",my_sd,sep=\"\")\n}\n}\n# Third matrix\nfor (i in 1:(num_alphas*num_datasets)) {\nfor (j in 1:num_methods) {\n103\nrange0 <- (i-1)*REPETITIONS + 1 range1 <- (i)*REPETITIONS my_subset <- range0:range1\nv <- as.vector(t(mdat[j,my_subset])) my_mean <- mean(v)\n# my_sd <- sd(v)\ntab3[i,j]<- my_mean # paste(my_mean,\"+/-\",my_sd,sep=\"\")\n}\n}\ntabmeans <- NULL # Means row for (j in 1:num_methods) {\ntabmeans[j] <- mean(tab3[,j])\n}\ntab3means <- rbind(tab3, Avg= tabmeans)\n#save(datasetnames,ALPHAS_ISO, models_names, cost_configuration, mdat, tab, tab2,\ntab3, tabmeans, tab3means, file=\"C:/Users/periale/Desktop/tesis/images/resultsFile\")\nsave(datasetnames,ALPHAS_ISO, models_names, cost_configuration, mdat, tab, tab2, tab3,\ntabmeans, tab3means, file=\"resultsFile\")\n# load(\"resultsFile\")\nprint(mdat) # write.matrix(mdat,file=\"C:/Users/periale/Desktop/tesis/images/mdat\", sep=\"\\t\") write.table(mdat,file=\"mdat.csv\") # mdat <- read.table(file=\"mdat.csv\")\nprint(tab) #write.matrix(tab,file=\"C:/Users/periale/Desktop/tesis/images/tab\", sep=\"\\t\") write.table(tab,file=\"tab.csv\") # tab <- read.table(file=\"tab.csv\")\nprint(tab2) #write.matrix(tab2,file=paste(\"C:/Users/periale/Desktop/tesis/images/tab2\", sep=\"\\t\")) write.csv(tab2,file=\"tab2.csv\") # tab2 <- read.csv(file=\"tab2.csv\")\n104\nB.5.1 Statistical tests\nThis part of code implemented the computation of the average ranks of the result obtained in the previous part, from which the Friedman and Nemenyi statistic tests are calculated.\n################################################ #### STATISTICAL TESTS ##### ################################################\nlibrary(xtable) # for xtable library(SuppDists) # for Friedman test (qFriedman)\n# Calculate ranks for the columns and derives the rank CalculateRanks <- function(v) {\n# Calculate the ranks ranks <- v lenv <- length(v[,1]) widthv <- length(v[1,]) for (i in 1:(lenv-1)) { # -1 to exclude the mean\nmy_row <- as.vector(t(v[i,])) # This is done in case v is not a matrix but a data\nframe\n# s <- sort(my_row, index.return=TRUE) # Doesn\u2019t work well with ties # ranks[i,s$ix] <- 1:widthv ranks[i,] <- rank(my_row)\n}\n105\nfor (j in 1:widthv) {\nranks[lenv,j] <- mean(ranks[1:(lenv-1),j])\n}\nranks\n}\n# Add the avg rank row to the matrix AddRanks <- function(v, ranks) {\nlenv <- length(ranks[,1]) vnew <- rbind(v, ranks[lenv,]) rn <- rownames(v) rn <- c(rn, \"AR\") # \"AvgRk\") rownames(vnew) <- rn\n# Return matrix vnew\n}\n# Calculate Friedman and Nemenyi tests Tests <- function(ranks, alpha) {\n# alpha <- 0.05 # significance level\nlenv <- length(ranks[,1]) # This includes the mean at the end widthv <- length(ranks[1,])\n# Calculate Friedman statistic R <- mean(ranks[lenv,]) n <- lenv-1 # Number of datasets. (-1) because the matrix also includes the average\nrow\nk <- widthv # Number of algorithms Sum1 <- n * sum((ranks[lenv,]-R)^2) Sum2 <- 1/(n*(k-1)) * sum((ranks[1:n,1:k]-R)^2) Friedman.stat <- Sum1/Sum2 #CriticalValue <- qFriedman(0.975, 3, 10) # Example: alpha= 0.05 (two-tail), k=3, n=10 CriticalValue <- qFriedman(1-(alpha/2), k, n)\ndf <- (n-1)*(k-1) # degrees of freedom\n106\n# qa <- qtukey(0.95, 3, 18) / sqrt(2) # Example: alpha: 0.05, k=3, df= 18\nqa <- qtukey(alpha, k, df) / sqrt(2)\nNemenyi_CD <- qa * sqrt((k*(k+1))/(6*n))\nif (Friedman.stat > CriticalValue) {\ntext1 <- paste(\"Friedman statistic: \", format(Friedman.stat, digits=4), \" >\nCritical Value: \", format(CriticalValue, digits=4), \" Null hypothesis rejected (significance level: \", toString(alpha), \"). Algorithms do not perform equally.\", sep=\"\")\n} else {\ntext1 <- paste(\"Friedman statistic: \", format(Friedman.stat, digits=4), \" <\nCritical Value: \", format(CriticalValue, digits=4), \" Null hypothesis not rejected (significance level: \", toString(alpha), \"). Algorithms may perform equally.\", sep=\"\")\n} text2 <- paste(\"Critical difference for the Nemenyi post-hoc test: \",\nformat(Nemenyi_CD, digits=4), sep=\"\")\nc(text1, text2)\n}\nmytab <- tab3means # mytab <- tab3means[,2:num_methods] # We eliminate the perfect method # mytab <- tab3means[,1:(num_methods-1)] # We eliminate the worst method\nranks1 <- CalculateRanks(mytab) tab3complete <- AddRanks(mytab, ranks1) resTests1 <- Tests(ranks1, 0.05) # 0.05 confidence level\ntab3complete resTests1\nTab <- xtable(tab3complete, digits=4, caption= paste(\"This figure shows the JC means\nfor the 4 repetitions for each of the 5 methods (Full, BMC, BTC, BJC, RND). The 30 rows are given by 6 datasets and 5 possible values of $alpha$. The \u2018Avg\u2019 row shows the averages of the first 30 rows. Finally, the \u2018AR\u2019 shows the average rank for each method. With these ranks the Friedman test is applied: \", paste(resTests1,\n107\ncollapse = \u2019\u2019)))\nprint(Tab, file= paste(\"\", \"table3.tex\", sep=\"\"))\n108"}], "references": [{"title": "Hydra: A noise-tolerant relational concept learning algorithm", "author": ["K.M. Ali", "M.J. Pazzani"], "venue": "Intl Joint Conf. Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1993}, {"title": "An analysis of four missing data treatment methods for supervised learning", "author": ["G. Batista", "M.C. Monard"], "venue": "Applied Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "An empirical comparison of voting classification algorithms: Bagging, boosting, and variants", "author": ["E. Kohavi R. Bauer"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Quantification via probability estimators", "author": ["A. Bella", "C. Ferri", "J. Hern\u00e1ndez-Orallo", "M.J. Ram\u0131\u0301rez-Quintana"], "venue": "In 2010 IEEE International Conference on Data Mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Ram\u0131\u0301rez-Quintana. Using negotiable features for prescription problems", "author": ["A. Bella", "C. Ferri", "J. Hern\u00e1ndez-Orallo", "M.J"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Aggregative quantification for regression", "author": ["Antonio Bella", "C\u00e8sar Ferri", "Jos\u00e9 Hern\u00e1ndez-Orallo", "Ma\u0155\u0131a Jos\u00e9 Ram\u0131\u0301rez-Quintana"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Ram\u0131\u0301rez-Quintana. Estimating the class probability threshold without training data", "author": ["R. Blanco-Vega", "C. Ferri-Ram\u0131\u0301rez", "J. Hern\u00e1ndez-Orallo", "M.J"], "venue": "ROC Analysis in Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Ram\u0131\u0301rez-Quintana. Analysing the trade-off between comprehensibility and accuracy in mimetic models", "author": ["Ricardo Blanco-Vega", "Jos\u00e9 Hern\u00e1ndez-Orallo", "M. Jos\u00e9"], "venue": "Science, 7th International Conference,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Knowledge acquisition through machine learning: minimising expert\u2019s effort", "author": ["Ricardo Blanco-Vega", "Jos\u00e9 Hern\u00e1ndez-Orallo", "M. Jos\u00e9 Ram\u0131\u0301rez-Quintana"], "venue": "Fourth International Conference on Machine Learning and Applications,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "The use of the area under the ROC curve in the evaluation of machine learning algorithms", "author": ["A.P. Bradley"], "venue": "Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "The default values approach to missing information", "author": ["C.J. Date", "H. Darwen"], "venue": "Relational Database Writings,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1989}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["J. Dem\u0161ar"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Cost Curves: An Improved Method for Visualizing Classifier Performance", "author": ["C. Drummond", "R.C. Holte"], "venue": "Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "The foundations of cost-sensitive learning", "author": ["C. Elkan"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Simple mimetic classifiers", "author": ["V. Estruch", "C. Ferri", "J. Hernandez-Orallo", "M. Ramirez-Quintana"], "venue": "Machine Learning and Data Mining in Pattern Recognition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "An introduction to ROC analysis", "author": ["T. Fawcett"], "venue": "Pattern Recognition Letters,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Cautious classifiers", "author": ["C. Ferri", "J. Hern\u00e1ndez-Orallo"], "venue": "Proceedings of the 1st International Workshop on ROC Analysis in Artificial Intelligence", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "An experimental comparison of performance measures for classification", "author": ["C. Ferri", "J. Hern\u00e1ndez-Orallo", "R. Modroiu"], "venue": "Pattern Recognition Let.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "From ensemble methods to comprehensible models", "author": ["C. Ferri", "J. Hernandez-Orallo", "M. Ramirez-Quintana"], "venue": "Discovery Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Delegating classifiers", "author": ["C\u00e9sar Ferri", "Peter A. Flach", "Jos\u00e9 Hern\u00e1ndez-Orallo"], "venue": "Machine Learning, Proceedings of the Twenty-first International Conference (ICML", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Machine Learning: The Art and Science of Algorithms that Make Sense of Data", "author": ["P. Flach"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Decision support for data mining", "author": ["P. Flach", "H. Blockeel", "C. Ferri", "J. Hern\u00e1ndez-Orallo", "J. Struyf"], "venue": "Data Mining and Decision Support,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "A coherent interpretation of AUC as a measure of aggregated classification performance", "author": ["P. Flach", "J. Hern\u00e1ndez-Orallo", "C. Ferri"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "An extension on statistical comparisons of classifiers over multiple data sets for all pairwise comparisons", "author": ["S. Gar\u0107\u0131a", "F. Herrera"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}, {"title": "The weka data mining software: an update", "author": ["Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H Witten"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Roc curves for regressions", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "Pattern Recognition, to appear,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "The 1st workshop on ROC analysis in artificial intelligence (ROCAI-2004)", "author": ["J. Hern\u00e1ndez-Orallo", "C. Ferri", "N. Lachiche", "P. Flach"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Brier curves: a new cost-based visualisation of classifier performance", "author": ["J. Hern\u00e1ndez-Orallo", "P. Flach", "C. Ferri"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "A unified view of performance metrics: Translating threshold choice into expected classification loss", "author": ["J. Hern\u00e1ndez-Orallo", "P. Flach", "C. Ferri"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "ROC curves in cost space", "author": ["J. Hern\u00e1ndez-Orallo", "P. Flach", "C. Ferri"], "venue": "Machine Learning,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Evaluating Learning Algorithms: A Classification Perspective", "author": ["N. Japkowicz", "M. Shah"], "venue": "Cambridge Univ Pr,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Feature space theory - a mathematical foundation for data mining", "author": ["H.X. Li", "L.D. Xu"], "venue": "Knowledge-based systems,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2001}, {"title": "Decision trees with minimal costs", "author": ["C.X. Ling", "Q. Yang", "J. Wang", "S. Zhang"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2004}, {"title": "A survey of cost-sensitive decision tree induction algorithms", "author": ["Susan Lomax", "Sunil Vadera"], "venue": "ACM Computing Surveys (CSUR),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Selecting features in microarray classification using ROC curves", "author": ["H. Mamitsuka"], "venue": "Pattern Recognition,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}, {"title": "Feature selection algorithms: A survey and experimental evaluation", "author": ["L.C. Molina", "L. Belanche", "\u00c0. Nebot"], "venue": "In Data Mining,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2002}, {"title": "Reducing misclassification costs", "author": ["Michael Pazzani", "Christopher Merz", "Patrick Murphy", "Kamal Ali", "Timothy Hume", "Clifford Brunk"], "venue": "In Proceedings of the Eleventh International Conference on Machine Learning,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1994}, {"title": "Test cost and misclassification cost trade-off using reframing", "author": ["Jos\u00e9 Hern\u00e1ndez-Orallo"], "venue": "arXiv preprint arXiv:1305.7111,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}, {"title": "Optimizing abstaining classifiers using ROC analysis", "author": ["T. Pietraszek"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2005}, {"title": "Better decisions through science", "author": ["J.A. Swets", "R.M. Dawes", "J. Monahan"], "venue": "Scientific American,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2000}, {"title": "Types of cost in inductive concept learning", "author": ["P. Turney"], "venue": "Canada National Research Council Publications Archive,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2000}, {"title": "Data Mining: Practical Machine Learning Tool and Technique with Java implementation", "author": ["I. Witten", "M. Frank"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2000}, {"title": "Missing is useful: missing values in cost-sensitive decision trees", "author": ["Shichao Zhang", "Zhenxing Qin", "Charles X Ling", "Shengli Sheng"], "venue": "IEEE transactions on knowledge and data engineering,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2005}, {"title": "Missing value estimation for mixed-attribute data sets. Knowledge and Data Engineering", "author": ["Xiaofeng Zhu", "Shichao Zhang", "Zhi Jin", "Zili Zhang", "Zhuoming Xu"], "venue": "IEEE Transactions on,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2011}], "referenceMentions": [{"referenceID": 32, "context": "2 Preview and others approaches The feature space (including both input and output variables) characterises a data mining problem [34].", "startOffset": 130, "endOffset": 134}, {"referenceID": 41, "context": "This is usually interpreted as a utility or cost-sensitive learning dilemma [43, 15], in this case between misclassification (or regression error) costs and attribute tests costs.", "startOffset": 76, "endOffset": 84}, {"referenceID": 13, "context": "This is usually interpreted as a utility or cost-sensitive learning dilemma [43, 15], in this case between misclassification (or regression error) costs and attribute tests costs.", "startOffset": 76, "endOffset": 84}, {"referenceID": 44, "context": "One possible option to affront this dilemma is known as missing value imputation [46], but this approach is not usually appropriate when test costs are considered.", "startOffset": 81, "endOffset": 85}, {"referenceID": 43, "context": "Second, imputing missing values \u201cis regarded as unnecessary for cost-sensitive learning that also considers the test costs\u201d [45].", "startOffset": 124, "endOffset": 128}, {"referenceID": 33, "context": "Decision trees are the usual choice [35, 45, 36] because the use of attributes can be customised in many different ways to (for example taking to consideration missing values without imputing them or considering misclassification cost).", "startOffset": 36, "endOffset": 48}, {"referenceID": 43, "context": "Decision trees are the usual choice [35, 45, 36] because the use of attributes can be customised in many different ways to (for example taking to consideration missing values without imputing them or considering misclassification cost).", "startOffset": 36, "endOffset": 48}, {"referenceID": 34, "context": "Decision trees are the usual choice [35, 45, 36] because the use of attributes can be customised in many different ways to (for example taking to consideration missing values without imputing them or considering misclassification cost).", "startOffset": 36, "endOffset": 48}, {"referenceID": 40, "context": "This is exactly the way ROC analysis works (for classification [42, 23, 29, 17, 37] and for regression [28]).", "startOffset": 63, "endOffset": 83}, {"referenceID": 21, "context": "This is exactly the way ROC analysis works (for classification [42, 23, 29, 17, 37] and for regression [28]).", "startOffset": 63, "endOffset": 83}, {"referenceID": 27, "context": "This is exactly the way ROC analysis works (for classification [42, 23, 29, 17, 37] and for regression [28]).", "startOffset": 63, "endOffset": 83}, {"referenceID": 15, "context": "This is exactly the way ROC analysis works (for classification [42, 23, 29, 17, 37] and for regression [28]).", "startOffset": 63, "endOffset": 83}, {"referenceID": 35, "context": "This is exactly the way ROC analysis works (for classification [42, 23, 29, 17, 37] and for regression [28]).", "startOffset": 63, "endOffset": 83}, {"referenceID": 26, "context": "This is exactly the way ROC analysis works (for classification [42, 23, 29, 17, 37] and for regression [28]).", "startOffset": 103, "endOffset": 107}, {"referenceID": 19, "context": "Furthermore, the use of machine learning and machine learning tools has proven to help solve some complex problems, providing good solutions, in the form of classification and regression models [21].", "startOffset": 194, "endOffset": 198}, {"referenceID": 19, "context": "However, some of the problems of supervised learning are overfitting [21] [44], which means that the model perfectly works on the training data but not on the evaluation (test) data.", "startOffset": 69, "endOffset": 73}, {"referenceID": 42, "context": "However, some of the problems of supervised learning are overfitting [21] [44], which means that the model perfectly works on the training data but not on the evaluation (test) data.", "startOffset": 74, "endOffset": 78}, {"referenceID": 24, "context": "Feature selection has in recent times become the focus of much research [26, 38], with approaches to address such a situation.", "startOffset": 72, "endOffset": 80}, {"referenceID": 36, "context": "Feature selection has in recent times become the focus of much research [26, 38], with approaches to address such a situation.", "startOffset": 72, "endOffset": 80}, {"referenceID": 44, "context": "However, one well known attempt to deal with missing data is known as \u201cimputation\u201d [46].", "startOffset": 83, "endOffset": 87}, {"referenceID": 43, "context": "Nevertheless, imputing missing values \u201cis regarded as unnecessary for cost-sensitive learning that also considers the test costs\u201d [45].", "startOffset": 130, "endOffset": 134}, {"referenceID": 41, "context": "4 Cost-sensitive methods As a means of minimise the cost made when predicting the classification of unseen examples, there have been many works in the last decade that investigate approaches to learning or revising classification procedures that attempt to reduce the cost of misclassified examples rather than the number of misclassified examples [43].", "startOffset": 348, "endOffset": 352}, {"referenceID": 41, "context": "Turney [43] presents an excellent survey on different types of costs in cost-sensitive learning, among which misclassification costs and test costs are distinguished as most important.", "startOffset": 7, "endOffset": 11}, {"referenceID": 37, "context": "Many modifications to machine learning algorithms attempt to reduce these costs, as in classification rules [39], when a prior domain theory is available [15].", "startOffset": 108, "endOffset": 112}, {"referenceID": 13, "context": "Many modifications to machine learning algorithms attempt to reduce these costs, as in classification rules [39], when a prior domain theory is available [15].", "startOffset": 154, "endOffset": 158}, {"referenceID": 33, "context": "their values, avoiding so expensive and risky tests (as in patient diagnosis for example [35, 45]).", "startOffset": 89, "endOffset": 97}, {"referenceID": 43, "context": "their values, avoiding so expensive and risky tests (as in patient diagnosis for example [35, 45]).", "startOffset": 89, "endOffset": 97}, {"referenceID": 43, "context": "This reasoning is called: \u201cmissing is useful\u201d [45], as the fact that values actually reduce the total cost of tests and, therefore, it is not meaningful to impute their values, as they will not reduce the misclassification cost.", "startOffset": 46, "endOffset": 50}, {"referenceID": 0, "context": "Since this strategy has been proposed in machine learning [1, 12], its performance and efficiency in costsensitive learning has not been shown, since the use of the same value to all missing values (null in this case) may not be good enough since it could introduce bias.", "startOffset": 58, "endOffset": 65}, {"referenceID": 10, "context": "Since this strategy has been proposed in machine learning [1, 12], its performance and efficiency in costsensitive learning has not been shown, since the use of the same value to all missing values (null in this case) may not be good enough since it could introduce bias.", "startOffset": 58, "endOffset": 65}, {"referenceID": 1, "context": "5s missing-value strategy has been shown to be efficient in cost-sensitive learning[3].", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "\u2022 Adaboost is an ensemble method for constructing a \u201cstrong\u201d classifier as a linear combination of simple \u201cweak\u201d classifiers [4].", "startOffset": 125, "endOffset": 128}, {"referenceID": 2, "context": "\u2022 Bagging [4] is a classificcation method which consists in training a number of base learners each from a different bootstrap sample (original data partition) by calling a base learning algorithm (decision trees, KNN, .", "startOffset": 10, "endOffset": 13}, {"referenceID": 15, "context": "1 ROC Analysis A receiver operating characteristics (ROC) graph is a technique for visualising, organising and selecting classifiers based on their performance [17].", "startOffset": 160, "endOffset": 164}, {"referenceID": 17, "context": "In classification, it has been shown that in data mining problems, the use of accuracy to compare classifiers is not adequate, and many works have been developed to address this problem [19].", "startOffset": 186, "endOffset": 190}, {"referenceID": 17, "context": "These metrics can be grouped in three principal groups [19]: the ones based on a threshold and qualitative understanding of error, the ones based on a probabilistic understanding of error, and the ones based on how well the model ranks the examples (where AUC is found).", "startOffset": 55, "endOffset": 59}, {"referenceID": 33, "context": "Also, if we think about costs, most works on minimising costs have taken this approach [35, 45, 36].", "startOffset": 87, "endOffset": 99}, {"referenceID": 43, "context": "Also, if we think about costs, most works on minimising costs have taken this approach [35, 45, 36].", "startOffset": 87, "endOffset": 99}, {"referenceID": 34, "context": "Also, if we think about costs, most works on minimising costs have taken this approach [35, 45, 36].", "startOffset": 87, "endOffset": 99}, {"referenceID": 25, "context": "Throughout this work we will use several classifiers from Weka [27].", "startOffset": 63, "endOffset": 67}, {"referenceID": 40, "context": "Reuse of learned knowledge is of critical importance in the majority of knowledge-intensive application areas, particularly because the operating context can be expected to vary from training to deployment and we need to make the best decision according to that context [42, 23].", "startOffset": 270, "endOffset": 278}, {"referenceID": 21, "context": "Reuse of learned knowledge is of critical importance in the majority of knowledge-intensive application areas, particularly because the operating context can be expected to vary from training to deployment and we need to make the best decision according to that context [42, 23].", "startOffset": 270, "endOffset": 278}, {"referenceID": 41, "context": "In fact, as Turney [43] points out, we can only rationally determine whether it is worthwhile to pay the cost of test when we know the cost of misclassification errors.", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "The joint cost is: JCi , \u03b1 \u00b7MCi + (1\u2212 \u03b1) \u00b7 TCi with \u03b1 \u2208 [0, 1].", "startOffset": 56, "endOffset": 62}, {"referenceID": 4, "context": "Instead, we can invent or negotiate over the attribute [6].", "startOffset": 55, "endOffset": 58}, {"referenceID": 4, "context": ", this is said to be a non-negotiable attribute in terms of [6]) so we can clearly save the cost of getting the value for this attribute.", "startOffset": 60, "endOffset": 63}, {"referenceID": 25, "context": "In our case, it just worked smoothly with Weka [27].", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": "We see many interesting things here: First, the general pattern is to get more accuracy 1We show accuracy, but we could show other measures such as AUC or MSE [19, 31] 18", "startOffset": 159, "endOffset": 167}, {"referenceID": 29, "context": "We see many interesting things here: First, the general pattern is to get more accuracy 1We show accuracy, but we could show other measures such as AUC or MSE [19, 31] 18", "startOffset": 159, "endOffset": 167}, {"referenceID": 0, "context": "As we can see, different feature configurations and models are chosen for each operating condition Finally, if we consider all possible values of \u03b1 \u2208 [0, 1] we see that some points are never chosen.", "startOffset": 150, "endOffset": 156}, {"referenceID": 31, "context": "In order to assess the significance of the experimental results we will use a custom procedure, following [33] and [22, ch.", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "12], which in turn is mostly based on [13].", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": "We agree with [25] that the Nemenyi test is a \u201cvery 37", "startOffset": 14, "endOffset": 18}, {"referenceID": 43, "context": "In fact, this has to be compared to the usual approach which is specific on decision trees, with several approaches according to [45, 36]: (a) KV, a tree is rebuilt when missing values are found, (b) Null strategy: replace by an extra label (model is not rebuilt), (c) Internal node: creates nodes for examples with missing values (model is not rebuilt), and (d) C4.", "startOffset": 129, "endOffset": 137}, {"referenceID": 34, "context": "In fact, this has to be compared to the usual approach which is specific on decision trees, with several approaches according to [45, 36]: (a) KV, a tree is rebuilt when missing values are found, (b) Null strategy: replace by an extra label (model is not rebuilt), (c) Internal node: creates nodes for examples with missing values (model is not rebuilt), and (d) C4.", "startOffset": 129, "endOffset": 137}, {"referenceID": 18, "context": "The approach presented here could also be compared or explored in combination to the mimetic technique to get models that use fewer attributes [20, 16, 9, 10, 8].", "startOffset": 143, "endOffset": 161}, {"referenceID": 14, "context": "The approach presented here could also be compared or explored in combination to the mimetic technique to get models that use fewer attributes [20, 16, 9, 10, 8].", "startOffset": 143, "endOffset": 161}, {"referenceID": 7, "context": "The approach presented here could also be compared or explored in combination to the mimetic technique to get models that use fewer attributes [20, 16, 9, 10, 8].", "startOffset": 143, "endOffset": 161}, {"referenceID": 8, "context": "The approach presented here could also be compared or explored in combination to the mimetic technique to get models that use fewer attributes [20, 16, 9, 10, 8].", "startOffset": 143, "endOffset": 161}, {"referenceID": 6, "context": "The approach presented here could also be compared or explored in combination to the mimetic technique to get models that use fewer attributes [20, 16, 9, 10, 8].", "startOffset": 143, "endOffset": 161}, {"referenceID": 3, "context": "Another interesting idea would be the problem of quantification with test costs, which could be applied to both classification and regression [5, 7].", "startOffset": 142, "endOffset": 148}, {"referenceID": 5, "context": "Another interesting idea would be the problem of quantification with test costs, which could be applied to both classification and regression [5, 7].", "startOffset": 142, "endOffset": 148}, {"referenceID": 16, "context": "As for the relation to other problems, we could also consider that the output domain may be null, as in abstaining classifiers [18, 41] and the notion of delegation [21] could be applied to this case.", "startOffset": 127, "endOffset": 135}, {"referenceID": 39, "context": "As for the relation to other problems, we could also consider that the output domain may be null, as in abstaining classifiers [18, 41] and the notion of delegation [21] could be applied to this case.", "startOffset": 127, "endOffset": 135}, {"referenceID": 19, "context": "As for the relation to other problems, we could also consider that the output domain may be null, as in abstaining classifiers [18, 41] and the notion of delegation [21] could be applied to this case.", "startOffset": 165, "endOffset": 169}, {"referenceID": 9, "context": "any curve representing cost for each operating condition, we wonder whether the area over the JROC curve means something, as in ROC analysis [11, 24].", "startOffset": 141, "endOffset": 149}, {"referenceID": 22, "context": "any curve representing cost for each operating condition, we wonder whether the area over the JROC curve means something, as in ROC analysis [11, 24].", "startOffset": 141, "endOffset": 149}, {"referenceID": 12, "context": "Also we could ask the question of whether we can draw cost plots as in [14, 30, 32].", "startOffset": 71, "endOffset": 83}, {"referenceID": 28, "context": "Also we could ask the question of whether we can draw cost plots as in [14, 30, 32].", "startOffset": 71, "endOffset": 83}, {"referenceID": 30, "context": "Also we could ask the question of whether we can draw cost plots as in [14, 30, 32].", "startOffset": 71, "endOffset": 83}, {"referenceID": 38, "context": "7111: \u201cTest cost and misclassification cost trade-off using reframing\u201d [40].", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "null(vector)) { test } else { for(j in 1:length(vector)) { test[,vector[j]] <- rep(\"?\",length(test[,1])) } test } } # Calculates the misclassification cost from rweka\u2019s textual output.", "startOffset": 98, "endOffset": 102}, {"referenceID": 0, "context": "misclascost <- function(e) { mylist <- strsplit(toString(e), \"\\n\") totalcoststring <- mylist[[1]][8] totalcoststringnoblanks <- gsub(\" \", \"\", totalcoststring)", "startOffset": 93, "endOffset": 96}, {"referenceID": 6, "context": "misclascost <- function(e) { mylist <- strsplit(toString(e), \"\\n\") totalcoststring <- mylist[[1]][8] totalcoststringnoblanks <- gsub(\" \", \"\", totalcoststring)", "startOffset": 97, "endOffset": 100}, {"referenceID": 0, "context": "# Gets length, number of attributes and number of classes len <- length(mydata[,1]) nattr <- length(mydata[1,]) for (i in 1:len) { for (j in 1:(nattr-1)) { if (is.", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "# Gets length, number of attributes and number of classes len <- length(mydata[,1]) nattr <- length(mydata[1,]) for (i in 1:len) { for (j in 1:(nattr-1)) { if (is.", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "# Ensure that the name of the class attribute is called \"class\" names(mydata)[nattr] <- \"class\" # Shuffles datasets and splits it shufindx <- sample(1:len, len) mydata <- mydata[shufindx,] lentrain <- trunc(len*2/3) train <- mydata[1:lentrain,] test <- mydata[(trunc(len*2/3)+1):len,] lentest <- length(test[,1])", "startOffset": 307, "endOffset": 311}, {"referenceID": 0, "context": "5)) testcostvector[i] <- testcostvector[i]*k } for (i in 1:length(mcvector[1,])) { for (j in 1:length(mcvector[,1])) { k0 <- runif(1) k = exp(2*(k0-0.", "startOffset": 74, "endOffset": 78}, {"referenceID": 0, "context": "5)) testcostvector[i] <- testcostvector[i]*k } for (i in 1:length(mcvector[1,])) { for (j in 1:length(mcvector[,1])) { k0 <- runif(1) k = exp(2*(k0-0.", "startOffset": 110, "endOffset": 114}, {"referenceID": 0, "context": "# This calculates the combinations removing i attributes settonullmatrix <- t(combn(1:(nattr-1), i)) print(settonullmatrix) if (i == 0) { len <- 0 } else { len <- length(settonullmatrix[,1]) }", "startOffset": 185, "endOffset": 189}, {"referenceID": 0, "context": "if (length(accuracylist)<lenmod) accuracylist <- c(accuracylist, list(accuracy1)) else accuracylist[[k]] <- c(accuracylist[[k]], list(accuracy1)) mc1 <- misclascost(e[1]) if (length(misccost)<lenmod) misccost <- c(misccost, list(mc1)) else misccost[[k]] <- c(misccost[[k]], list(mc1))", "startOffset": 166, "endOffset": 169}, {"referenceID": 0, "context": "5 slope <- (1 - alpha_iso) / - alpha_iso # FULL METHOD point_full <- leftmost_intercept_xy(slope, c(unlist(testcost)), c(unlist(misccost)) ) jcfull <- alpha_iso*point_full[2] + (1-alpha_iso)*point_full[1] # BMC METHOD point_bmc <- leftmost_intercept_xy(slope, c(unlist(testcostmc)), c(unlist(misccostmc) )) jcbmc <- alpha_iso*point_bmc[2] + (1-alpha_iso)*point_bmc[1] # BTC METHOD point_btc <- leftmost_intercept_xy(slope, c(unlist(testcosttc)), c(unlist(misccosttc) )) jcbtc <- alpha_iso*point_btc[2] + (1-alpha_iso)*point_btc[1] # BTC METHOD point_bjc <- leftmost_intercept_xy(slope, c(unlist(testcostjc)), c(unlist(misccostjc) )) jcbjc <- alpha_iso*point_bjc[2] + (1-alpha_iso)*point_bjc[1] # BTC METHOD point_rnd <- leftmost_intercept_xy(slope, c(unlist(testcostrnd)), c(unlist(misccostrnd) )) jcrnd <- alpha_iso*point_rnd[2] + (1-alpha_iso)*point_rnd[1] c(jcfull, jcbmc, jcbtc, jcbjc, jcrnd) # Returns the five joint costs for the five 70", "startOffset": 201, "endOffset": 204}, {"referenceID": 0, "context": "5 slope <- (1 - alpha_iso) / - alpha_iso # FULL METHOD point_full <- leftmost_intercept_xy(slope, c(unlist(testcost)), c(unlist(misccost)) ) jcfull <- alpha_iso*point_full[2] + (1-alpha_iso)*point_full[1] # BMC METHOD point_bmc <- leftmost_intercept_xy(slope, c(unlist(testcostmc)), c(unlist(misccostmc) )) jcbmc <- alpha_iso*point_bmc[2] + (1-alpha_iso)*point_bmc[1] # BTC METHOD point_btc <- leftmost_intercept_xy(slope, c(unlist(testcosttc)), c(unlist(misccosttc) )) jcbtc <- alpha_iso*point_btc[2] + (1-alpha_iso)*point_btc[1] # BTC METHOD point_bjc <- leftmost_intercept_xy(slope, c(unlist(testcostjc)), c(unlist(misccostjc) )) jcbjc <- alpha_iso*point_bjc[2] + (1-alpha_iso)*point_bjc[1] # BTC METHOD point_rnd <- leftmost_intercept_xy(slope, c(unlist(testcostrnd)), c(unlist(misccostrnd) )) jcrnd <- alpha_iso*point_rnd[2] + (1-alpha_iso)*point_rnd[1] c(jcfull, jcbmc, jcbtc, jcbjc, jcrnd) # Returns the five joint costs for the five 70", "startOffset": 364, "endOffset": 367}, {"referenceID": 0, "context": "5 slope <- (1 - alpha_iso) / - alpha_iso # FULL METHOD point_full <- leftmost_intercept_xy(slope, c(unlist(testcost)), c(unlist(misccost)) ) jcfull <- alpha_iso*point_full[2] + (1-alpha_iso)*point_full[1] # BMC METHOD point_bmc <- leftmost_intercept_xy(slope, c(unlist(testcostmc)), c(unlist(misccostmc) )) jcbmc <- alpha_iso*point_bmc[2] + (1-alpha_iso)*point_bmc[1] # BTC METHOD point_btc <- leftmost_intercept_xy(slope, c(unlist(testcosttc)), c(unlist(misccosttc) )) jcbtc <- alpha_iso*point_btc[2] + (1-alpha_iso)*point_btc[1] # BTC METHOD point_bjc <- leftmost_intercept_xy(slope, c(unlist(testcostjc)), c(unlist(misccostjc) )) jcbjc <- alpha_iso*point_bjc[2] + (1-alpha_iso)*point_bjc[1] # BTC METHOD point_rnd <- leftmost_intercept_xy(slope, c(unlist(testcostrnd)), c(unlist(misccostrnd) )) jcrnd <- alpha_iso*point_rnd[2] + (1-alpha_iso)*point_rnd[1] c(jcfull, jcbmc, jcbtc, jcbjc, jcrnd) # Returns the five joint costs for the five 70", "startOffset": 527, "endOffset": 530}, {"referenceID": 0, "context": "5 slope <- (1 - alpha_iso) / - alpha_iso # FULL METHOD point_full <- leftmost_intercept_xy(slope, c(unlist(testcost)), c(unlist(misccost)) ) jcfull <- alpha_iso*point_full[2] + (1-alpha_iso)*point_full[1] # BMC METHOD point_bmc <- leftmost_intercept_xy(slope, c(unlist(testcostmc)), c(unlist(misccostmc) )) jcbmc <- alpha_iso*point_bmc[2] + (1-alpha_iso)*point_bmc[1] # BTC METHOD point_btc <- leftmost_intercept_xy(slope, c(unlist(testcosttc)), c(unlist(misccosttc) )) jcbtc <- alpha_iso*point_btc[2] + (1-alpha_iso)*point_btc[1] # BTC METHOD point_bjc <- leftmost_intercept_xy(slope, c(unlist(testcostjc)), c(unlist(misccostjc) )) jcbjc <- alpha_iso*point_bjc[2] + (1-alpha_iso)*point_bjc[1] # BTC METHOD point_rnd <- leftmost_intercept_xy(slope, c(unlist(testcostrnd)), c(unlist(misccostrnd) )) jcrnd <- alpha_iso*point_rnd[2] + (1-alpha_iso)*point_rnd[1] c(jcfull, jcbmc, jcbtc, jcbjc, jcrnd) # Returns the five joint costs for the five 70", "startOffset": 690, "endOffset": 693}, {"referenceID": 0, "context": "5 slope <- (1 - alpha_iso) / - alpha_iso # FULL METHOD point_full <- leftmost_intercept_xy(slope, c(unlist(testcost)), c(unlist(misccost)) ) jcfull <- alpha_iso*point_full[2] + (1-alpha_iso)*point_full[1] # BMC METHOD point_bmc <- leftmost_intercept_xy(slope, c(unlist(testcostmc)), c(unlist(misccostmc) )) jcbmc <- alpha_iso*point_bmc[2] + (1-alpha_iso)*point_bmc[1] # BTC METHOD point_btc <- leftmost_intercept_xy(slope, c(unlist(testcosttc)), c(unlist(misccosttc) )) jcbtc <- alpha_iso*point_btc[2] + (1-alpha_iso)*point_btc[1] # BTC METHOD point_bjc <- leftmost_intercept_xy(slope, c(unlist(testcostjc)), c(unlist(misccostjc) )) jcbjc <- alpha_iso*point_bjc[2] + (1-alpha_iso)*point_bjc[1] # BTC METHOD point_rnd <- leftmost_intercept_xy(slope, c(unlist(testcostrnd)), c(unlist(misccostrnd) )) jcrnd <- alpha_iso*point_rnd[2] + (1-alpha_iso)*point_rnd[1] c(jcfull, jcbmc, jcbtc, jcbjc, jcrnd) # Returns the five joint costs for the five 70", "startOffset": 855, "endOffset": 858}, {"referenceID": 0, "context": "1) # A=NULL # corrected # for (i in 1:length(accuracylist3)) # corrected # A[i] <- accuracylist3[[i]] # corrected #for draw the plot of accuracy for (i in 1:lenmod) { if (i==1) { plot(1:length(accuracylist[[1]]), accuracylist[[1]],ylab=\"Accuracy\", xlab=\"\",ylim=c(minaccuracy,maxaccuracy),lwd=3, xaxt=\"n\",cex.", "startOffset": 206, "endOffset": 209}, {"referenceID": 0, "context": "1) # A=NULL # corrected # for (i in 1:length(accuracylist3)) # corrected # A[i] <- accuracylist3[[i]] # corrected #for draw the plot of accuracy for (i in 1:lenmod) { if (i==1) { plot(1:length(accuracylist[[1]]), accuracylist[[1]],ylab=\"Accuracy\", xlab=\"\",ylim=c(minaccuracy,maxaccuracy),lwd=3, xaxt=\"n\",cex.", "startOffset": 226, "endOffset": 229}, {"referenceID": 0, "context": "lab=2, type=\"l\", col=plot_colors[1]) axis(1, at=1:length(x),srt=45, padj=1,las=2, lab=x) } else { lines(1:length(accuracylist[[i]]), accuracylist[[i]], type=\"l\",lty=i,lwd=3, col=plot_colors[i]) } } legend(LEGEND_LOCATION,c(unlist(models_names)), cex=1, col=plot_colors, lty=1:lenmod, lwd=3, bty=\"n\") if (PDFCLOSE) { dev.", "startOffset": 32, "endOffset": 35}, {"referenceID": 0, "context": "1) for (i in 1:lenmod) { if (i==1) { plot(testcost[[1]], misccost[[1]],ylab=\"MC\", xlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "1) for (i in 1:lenmod) { if (i==1) { plot(testcost[[1]], misccost[[1]],ylab=\"MC\", xlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "5, col=plot_colors[1]) } else { points(testcost[[i]],misccost[[i]], cex=2, pch=i,lwd=2, col=plot_colors[i]) } } legend(LEGEND_LOCATION, c(unlist(models_names)), cex=1.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "1) for (i in 1:lenmod) { if (i==1) { plot(testcost[[1]], misccost[[1]],ylab=\"MC\", xlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "1) for (i in 1:lenmod) { if (i==1) { plot(testcost[[1]], misccost[[1]],ylab=\"MC\", xlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "5, col=plot_colors[1]) } else { points(testcost[[i]],misccost[[i]], cex=2, pch=i,lwd=2, col=plot_colors[i]) } } legend(LEGEND_LOCATION, c(unlist(models_names)), cex=1.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "1) for (i in 1:lenmod) { if (i==1) { plot(testcost[[1]], misccost[[1]],ylab=\"MC\", xlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "1) for (i in 1:lenmod) { if (i==1) { plot(testcost[[1]], misccost[[1]],ylab=\"MC\", xlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "5, col=plot_colors[1]) } else { points(testcost[[i]],misccost[[i]], cex=2, pch=i,lwd=2, col=plot_colors[i]) } } legend(LEGEND_LOCATION, c(unlist(models_names)), cex=1.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "1) for (i in 1:lenmod) { if(i==1) { plot(testcostmc[[1]], misccostmc[[1]],ylab=\"MC\", xlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.", "startOffset": 52, "endOffset": 55}, {"referenceID": 0, "context": "1) for (i in 1:lenmod) { if(i==1) { plot(testcostmc[[1]], misccostmc[[1]],ylab=\"MC\", xlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.", "startOffset": 69, "endOffset": 72}, {"referenceID": 0, "context": "5, col=plot_colors[1]) } else { points(testcostmc[[i]], misccostmc[[i]], cex=2, pch=2,lwd=2, col=plot_colors[i]) } } legend(LEGEND_LOCATION, c(unlist(models_names)), cex=1.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "1) for (i in 1:lenmod) { if(i==1) { plot(testcostjc[[1]], misccostjc[[1]],ylab=\"MC\", xlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.", "startOffset": 52, "endOffset": 55}, {"referenceID": 0, "context": "1) for (i in 1:lenmod) { if(i==1) { plot(testcostjc[[1]], misccostjc[[1]],ylab=\"MC\", xlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.", "startOffset": 69, "endOffset": 72}, {"referenceID": 0, "context": "5, col=plot_colors[1]) } else { points(testcostjc[[i]], misccostjc[[i]], cex=2, pch=2,lwd=2, col=plot_colors[i]) } } legend(LEGEND_LOCATION, c(unlist(models_names)), cex=1.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "1) for (i in 1:lenmod) { if(i==1) { plot(testcosttc[[1]], misccosttc[[1]],ylab=\"MC\", xlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.", "startOffset": 52, "endOffset": 55}, {"referenceID": 0, "context": "1) for (i in 1:lenmod) { if(i==1) { plot(testcosttc[[1]], misccosttc[[1]],ylab=\"MC\", xlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.", "startOffset": 69, "endOffset": 72}, {"referenceID": 0, "context": "5, col=plot_colors[1]) } else { points(testcosttc[[i]], misccosttc[[i]], cex=2, pch=2,lwd=2, col=plot_colors[i]) } } legend(LEGEND_LOCATION, c(unlist(models_names)), cex=1.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "1) plot(testcostrnd[[1]], misccostrnd[[1]],ylab=\"MC\", xlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "1) plot(testcostrnd[[1]], misccostrnd[[1]],ylab=\"MC\", xlab=\"TC\",ylim=c(minmisccost,maxmisccost),xlim=c(mintestcost,maxtestcost),pch=1,lwd=2,cex=2,cex.", "startOffset": 38, "endOffset": 41}, {"referenceID": 0, "context": "5, col=plot_colors[1]) points(testcostrnd[[1]],misccostrnd[[2]], cex=2, pch=2,lwd=2, col=plot_colors[2]) points(testcostrnd[[1]],misccostrnd[[3]], cex=2, pch=3,lwd=2, col=plot_colors[3]) legend(LEGEND_LOCATION, c(unlist(models_names)), cex=1.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "5, col=plot_colors[1]) points(testcostrnd[[1]],misccostrnd[[2]], cex=2, pch=2,lwd=2, col=plot_colors[2]) points(testcostrnd[[1]],misccostrnd[[3]], cex=2, pch=3,lwd=2, col=plot_colors[3]) legend(LEGEND_LOCATION, c(unlist(models_names)), cex=1.", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "5, col=plot_colors[1]) points(testcostrnd[[1]],misccostrnd[[2]], cex=2, pch=2,lwd=2, col=plot_colors[2]) points(testcostrnd[[1]],misccostrnd[[3]], cex=2, pch=3,lwd=2, col=plot_colors[3]) legend(LEGEND_LOCATION, c(unlist(models_names)), cex=1.", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "5, col=plot_colors[1]) points(testcostrnd[[1]],misccostrnd[[2]], cex=2, pch=2,lwd=2, col=plot_colors[2]) points(testcostrnd[[1]],misccostrnd[[3]], cex=2, pch=3,lwd=2, col=plot_colors[3]) legend(LEGEND_LOCATION, c(unlist(models_names)), cex=1.", "startOffset": 141, "endOffset": 144}, {"referenceID": 1, "context": "5, col=plot_colors[1]) points(testcostrnd[[1]],misccostrnd[[2]], cex=2, pch=2,lwd=2, col=plot_colors[2]) points(testcostrnd[[1]],misccostrnd[[3]], cex=2, pch=3,lwd=2, col=plot_colors[3]) legend(LEGEND_LOCATION, c(unlist(models_names)), cex=1.", "startOffset": 182, "endOffset": 185}, {"referenceID": 0, "context": "null(vector)) { test } else { for(j in 1:length(vector)) { test[,vector[j]] <- rep(\"?\",length(test[,1])) } test } } # Calculates the misclassification cost from rweka\u2019s textual output misclascost <- function(e) { mylist <- strsplit(toString(e), \"\\n\") totalcoststring <- mylist[[1]][8] totalcoststringnoblanks <- gsub(\" \", \"\", totalcoststring) totalcoststringnoblanksonlynumber <- gsub(\"AverageCost\", \"\", totalcoststringnoblanks) totalcostnumber <- as.", "startOffset": 98, "endOffset": 102}, {"referenceID": 0, "context": "null(vector)) { test } else { for(j in 1:length(vector)) { test[,vector[j]] <- rep(\"?\",length(test[,1])) } test } } # Calculates the misclassification cost from rweka\u2019s textual output misclascost <- function(e) { mylist <- strsplit(toString(e), \"\\n\") totalcoststring <- mylist[[1]][8] totalcoststringnoblanks <- gsub(\" \", \"\", totalcoststring) totalcoststringnoblanksonlynumber <- gsub(\"AverageCost\", \"\", totalcoststringnoblanks) totalcostnumber <- as.", "startOffset": 277, "endOffset": 280}, {"referenceID": 6, "context": "null(vector)) { test } else { for(j in 1:length(vector)) { test[,vector[j]] <- rep(\"?\",length(test[,1])) } test } } # Calculates the misclassification cost from rweka\u2019s textual output misclascost <- function(e) { mylist <- strsplit(toString(e), \"\\n\") totalcoststring <- mylist[[1]][8] totalcoststringnoblanks <- gsub(\" \", \"\", totalcoststring) totalcoststringnoblanksonlynumber <- gsub(\"AverageCost\", \"\", totalcoststringnoblanks) totalcostnumber <- as.", "startOffset": 281, "endOffset": 284}, {"referenceID": 0, "context": "# Gets length, number of attributes and number of classes len <- length(mydata[,1]) nattr <- length(mydata[1,]) nclasses <- length(unique(mydata[,nattr]))", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "# Gets length, number of attributes and number of classes len <- length(mydata[,1]) nattr <- length(mydata[1,]) nclasses <- length(unique(mydata[,nattr]))", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "# Ensure that the name of the class attribute is called \"class\" names(mydata)[nattr] <- \"class\" # Shuffles datasets and splits it shufindx <- sample(1:len, len) mydata <- mydata[shufindx,] lentrain <- trunc(len*2/3) train <- mydata[1:lentrain,] test <- mydata[(trunc(len*2/3)+1):len,] lentest <- length(test[,1])", "startOffset": 307, "endOffset": 311}, {"referenceID": 0, "context": "5)) testcostvector[i] <- testcostvector[i]*k } # normalise vector testcostvector <- testcostvector / sum(testcostvector) for (i in 1:length(mcvector[1,])) { for (j in 1:length(mcvector[,1])) { k0 <- runif(1) # k = exp(2*(k0-0.", "startOffset": 148, "endOffset": 152}, {"referenceID": 0, "context": "5)) testcostvector[i] <- testcostvector[i]*k } # normalise vector testcostvector <- testcostvector / sum(testcostvector) for (i in 1:length(mcvector[1,])) { for (j in 1:length(mcvector[,1])) { k0 <- runif(1) # k = exp(2*(k0-0.", "startOffset": 184, "endOffset": 188}, {"referenceID": 0, "context": "pivot <- 1 bpnames[i+1] <- i boxplotstrg1 <- NULL label<-\"_\" # This calculates the combinations removing i attributes settonullmatrix <- t(combn(1:(nattr-1), i)) if (VERBOSE) print(settonullmatrix) if (i == 0) { len <- 0 } else { len <- length(settonullmatrix[,1]) }", "startOffset": 259, "endOffset": 263}, {"referenceID": 0, "context": "if (res1[i] == modifiedtest[i,nattr]) hits <- hits +1 accuracy1 <- hits/ lentest # print(accuracy1) if (VERBOSE) print(k) if (length(accuracylist)<lenmod) accuracylist <- c(accuracylist, list(accuracy1)) else accuracylist[[k]] <- c(accuracylist[[k]], list(accuracy1)) mc1 <- misclascost(e[1]) if (length(misccost)<lenmod) misccost <- c(misccost, list(mc1)) else misccost[[k]] <- c(misccost[[k]], list(mc1))", "startOffset": 288, "endOffset": 291}, {"referenceID": 0, "context": "point_full<-NULL point_btc<-NULL point_bmc<-NULL point_bjc<-NULL point_rnd<-NULL slope <- (1 - alpha) / - alpha # FULL METHOD point_full <- leftmost_intercept_xy(slope, c(unlist(testcost)), c(unlist(misccost)) ) jcfull <- alpha*point_full[2] + (1-alpha)*point_full[1] # BMC METHOD point_bmc <- leftmost_intercept_xy(slope, c(unlist(testcostmc)), c(unlist(misccostmc) )) jcbmc <- alpha*point_bmc[2] + (1-alpha)*point_bmc[1] # BTC METHOD point_btc <- leftmost_intercept_xy(slope, c(unlist(testcosttc)), c(unlist(misccosttc) )) jcbtc <- alpha*point_btc[2] + (1-alpha)*point_btc[1] # BTC METHOD point_bjc <- leftmost_intercept_xy(slope, c(unlist(testcostjc)), c(unlist(misccostjc) )) jcbjc <- alpha*point_bjc[2] + (1-alpha)*point_bjc[1] # BTC METHOD point_rnd <- leftmost_intercept_xy(slope, c(unlist(testcostrnd)), 99", "startOffset": 264, "endOffset": 267}, {"referenceID": 0, "context": "point_full<-NULL point_btc<-NULL point_bmc<-NULL point_bjc<-NULL point_rnd<-NULL slope <- (1 - alpha) / - alpha # FULL METHOD point_full <- leftmost_intercept_xy(slope, c(unlist(testcost)), c(unlist(misccost)) ) jcfull <- alpha*point_full[2] + (1-alpha)*point_full[1] # BMC METHOD point_bmc <- leftmost_intercept_xy(slope, c(unlist(testcostmc)), c(unlist(misccostmc) )) jcbmc <- alpha*point_bmc[2] + (1-alpha)*point_bmc[1] # BTC METHOD point_btc <- leftmost_intercept_xy(slope, c(unlist(testcosttc)), c(unlist(misccosttc) )) jcbtc <- alpha*point_btc[2] + (1-alpha)*point_btc[1] # BTC METHOD point_bjc <- leftmost_intercept_xy(slope, c(unlist(testcostjc)), c(unlist(misccostjc) )) jcbjc <- alpha*point_bjc[2] + (1-alpha)*point_bjc[1] # BTC METHOD point_rnd <- leftmost_intercept_xy(slope, c(unlist(testcostrnd)), 99", "startOffset": 419, "endOffset": 422}, {"referenceID": 0, "context": "point_full<-NULL point_btc<-NULL point_bmc<-NULL point_bjc<-NULL point_rnd<-NULL slope <- (1 - alpha) / - alpha # FULL METHOD point_full <- leftmost_intercept_xy(slope, c(unlist(testcost)), c(unlist(misccost)) ) jcfull <- alpha*point_full[2] + (1-alpha)*point_full[1] # BMC METHOD point_bmc <- leftmost_intercept_xy(slope, c(unlist(testcostmc)), c(unlist(misccostmc) )) jcbmc <- alpha*point_bmc[2] + (1-alpha)*point_bmc[1] # BTC METHOD point_btc <- leftmost_intercept_xy(slope, c(unlist(testcosttc)), c(unlist(misccosttc) )) jcbtc <- alpha*point_btc[2] + (1-alpha)*point_btc[1] # BTC METHOD point_bjc <- leftmost_intercept_xy(slope, c(unlist(testcostjc)), c(unlist(misccostjc) )) jcbjc <- alpha*point_bjc[2] + (1-alpha)*point_bjc[1] # BTC METHOD point_rnd <- leftmost_intercept_xy(slope, c(unlist(testcostrnd)), 99", "startOffset": 574, "endOffset": 577}, {"referenceID": 0, "context": "point_full<-NULL point_btc<-NULL point_bmc<-NULL point_bjc<-NULL point_rnd<-NULL slope <- (1 - alpha) / - alpha # FULL METHOD point_full <- leftmost_intercept_xy(slope, c(unlist(testcost)), c(unlist(misccost)) ) jcfull <- alpha*point_full[2] + (1-alpha)*point_full[1] # BMC METHOD point_bmc <- leftmost_intercept_xy(slope, c(unlist(testcostmc)), c(unlist(misccostmc) )) jcbmc <- alpha*point_bmc[2] + (1-alpha)*point_bmc[1] # BTC METHOD point_btc <- leftmost_intercept_xy(slope, c(unlist(testcosttc)), c(unlist(misccosttc) )) jcbtc <- alpha*point_btc[2] + (1-alpha)*point_btc[1] # BTC METHOD point_bjc <- leftmost_intercept_xy(slope, c(unlist(testcostjc)), c(unlist(misccostjc) )) jcbjc <- alpha*point_bjc[2] + (1-alpha)*point_bjc[1] # BTC METHOD point_rnd <- leftmost_intercept_xy(slope, c(unlist(testcostrnd)), 99", "startOffset": 729, "endOffset": 732}, {"referenceID": 0, "context": "c(unlist(misccostrnd) )) jcrnd <- alpha*point_rnd[2] + (1-alpha)*point_rnd[1] c(jcfull, jcbmc, jcbtc, jcbjc, jcrnd) # Returns the five joint costs for the five different methods", "startOffset": 74, "endOffset": 77}, {"referenceID": 0, "context": "# Calculate ranks for the columns and derives the rank CalculateRanks <- function(v) { # Calculate the ranks ranks <- v lenv <- length(v[,1]) widthv <- length(v[1,]) for (i in 1:(lenv-1)) { # -1 to exclude the mean my_row <- as.", "startOffset": 136, "endOffset": 140}, {"referenceID": 0, "context": "# Calculate ranks for the columns and derives the rank CalculateRanks <- function(v) { # Calculate the ranks ranks <- v lenv <- length(v[,1]) widthv <- length(v[1,]) for (i in 1:(lenv-1)) { # -1 to exclude the mean my_row <- as.", "startOffset": 160, "endOffset": 164}, {"referenceID": 0, "context": "# Add the avg rank row to the matrix AddRanks <- function(v, ranks) { lenv <- length(ranks[,1]) vnew <- rbind(v, ranks[lenv,]) rn <- rownames(v) rn <- c(rn, \"AR\") # \"AvgRk\") rownames(vnew) <- rn # Return matrix vnew }", "startOffset": 90, "endOffset": 94}, {"referenceID": 0, "context": "05 # significance level lenv <- length(ranks[,1]) # This includes the mean at the end widthv <- length(ranks[1,]) # Calculate Friedman statistic R <- mean(ranks[lenv,]) n <- lenv-1 # Number of datasets.", "startOffset": 44, "endOffset": 48}, {"referenceID": 0, "context": "05 # significance level lenv <- length(ranks[,1]) # This includes the mean at the end widthv <- length(ranks[1,]) # Calculate Friedman statistic R <- mean(ranks[lenv,]) n <- lenv-1 # Number of datasets.", "startOffset": 108, "endOffset": 112}], "year": 2013, "abstractText": "Many solutions to cost-sensitive classification (and regression) rely on some or all of the following assumptions: we have complete knowledge about the cost context at training time, we can easily re-train whenever the cost context changes, and we have technique-specific methods (such as cost-sensitive decision trees) that can take advantage of that information. In this work we address the problem of selecting models and minimising joint cost (integrating both misclassification cost and test costs) without any of the above assumptions. We introduce methods and plots (such as the so-called JROC plots) that can work with any off-the-shelf predictive technique, including ensembles, such that we re-frame the model to use the appropriate subset of attributes (the feature configuration) during deployment time. In other words, models are trained with the available attributes (once and for all) and then deployed by setting missing values on the attributes that are deemed ineffective for reducing the joint cost. As the number of feature configuration combinations grows exponentially with the number of features we introduce quadratic methods that are able to approximate the optimal configuration and model choices, as shown by the experimental results.", "creator": "LaTeX with hyperref package"}}}