{"id": "1307.0426", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2013", "title": "An Empirical Study into Annotator Agreement, Ground Truth Estimation, and Algorithm Evaluation", "abstract": "Although agreement between annotators has been studied in the past from a statistical viewpoint, little work has attempted to quantify the extent to which this phenomenon affects the evaluation of computer vision (CV) object detection algorithms. Many researchers utilise ground truth (GT) in experiments and more often than not this GT is derived from one annotator's opinion. How does the difference in opinion affect an algorithm's evaluation? Four examples of typical CV problems are chosen, and a methodology is applied to each to quantify the inter-annotator variance and to offer insight into the mechanisms behind agreement and the use of GT. It is found that when detecting linear objects annotator agreement is very low. The agreement in object position, linear or otherwise, can be partially explained through basic image properties. Automatic object detectors are compared to annotator agreement and it is found that a clear relationship exists. Several methods for calculating GTs from a number of annotations are applied and the resulting differences in the performance of the object detectors are quantified. It is found that the rank of a detector is highly dependent upon the method used to form the GT. It is also found that although the STAPLE and LSML GT estimation methods appear to represent the mean of the performance measured using the individual annotations, when there are few annotations, or there is a large variance in them, these estimates tend to degrade. Furthermore, one of the most commonly adopted annotation combination methods--consensus voting--accentuates more obvious features, which results in an overestimation of the algorithm's performance. Finally, it is concluded that in some datasets it may not be possible to state with any confidence that one algorithm outperforms another when evaluating upon one GT and a method for calculating confidence bounds is discussed.", "histories": [["v1", "Mon, 1 Jul 2013 16:16:40 GMT  (5645kb,D)", "https://arxiv.org/abs/1307.0426v1", "23 pages"], ["v2", "Sat, 26 Oct 2013 14:39:01 GMT  (5640kb,D)", "http://arxiv.org/abs/1307.0426v2", "23 pages"], ["v3", "Tue, 26 Apr 2016 11:05:18 GMT  (11559kb,D)", "http://arxiv.org/abs/1307.0426v3", "16 pages"]], "COMMENTS": "23 pages", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["thomas a lampert", "r\\'e stumpf", "pierre gan\\c{c}arski"], "accepted": false, "id": "1307.0426"}, "pdf": {"name": "1307.0426.pdf", "metadata": {"source": "CRF", "title": "An Empirical Study into Annotator Agreement, Ground Truth Estimation, and Algorithm Evaluation", "authors": ["Thomas A. Lampert", "Andr\u00e9 Stumpf", "Pierre Gan\u00e7arski"], "emails": ["tlampert@unistra.fr)."], "sections": [{"heading": null, "text": "Index Terms\u2014Evaluation, ranking, performance, feature detection, agreement, annotation, ground truth, gold-standard ground truth, expert agreement, receiver operating characteristic analysis, precision, recall.\nI. INTRODUCTION\nThe evaluation of computer vision algorithms often requires ground truth (GT) data. The difficulty presented by this is that a gold-standard GT can be costly to obtain (if at all possible). For example, determining gold-standard GT in remote sensing experiments would typically require field surveys over large and sometimes remote areas and for medical\nManuscript received September 28, 2014; revised October 26, 2015 and March 15, 2016; accepted March 17, 2016. This work was supported by the French Research Agency through the COCLICO Project: ANR Mode\u0300les Nume\u0301eriques Program under Grant ANR-12-MN-001-COCLICO 2012\u20132016.\nT. A. Lampert and P. Ganc\u0327arski are with the ICube Laboratory, University of Strasbourg, France (e-mail: tlampert@unistra.fr).\nA. Stumpf is with the Laboratoire Image, Ville, et Environment (LIVE), University of Strasbourg, France.\n\u00a92016 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\nscans difficulties arise since it would require invasive surgery. It is therefore commonly assumed that the opinion of one (or more) annotator(s) approximates this gold-standard GT. Nevertheless, annotators rarely agree completely when giving their opinion and this disagreement can be characterised as bias\u2014the tendency of an annotator to prefer one decision over another\u2014and variance\u2014the natural variation that one annotator will have to the next (or themselves at a later date) [1]. This poses a problem when evaluating computer vision algorithms: how does the difference in annotator opinion affect an algorithm\u2019s evaluation?\nThis work intends to quantify the effects of GT variability on the design, training, and evaluation of segmentation algorithms. To this end, supervised and unsupervised algorithms are evaluated in four case studies, all of which embody typical computer vision problems: the segmentation of natural images (referred to as the Segmentation case study), the identification of fissures in aerial imagery (referred to as the Fissure case study), the identification of landslides in satellite imagery (referred to as the Landslide case study), and the identification of blood vessels in medical imagery (referred to as the Blood Vessel case study). The true GT of these data sets (the goldstandard GT) cannot be deduced from the imagery alone and annotations by human experts are used as the best available approximation. This limitation is typical in many computer vision applications such as medical imaging, remote sensing, and natural scene analysis. Furthermore, there exist many objects in these datasets that can cause false-positive and falsenegative errors, making them ideal to study annotator and detector agreements.\nSeveral previous studies have developed statistical methods for estimating the gold-standard GT from a number of annotations [1, 2, 3, 4, 5, 6, 7, 8]. Although some public datasets offer segmentations obtained from different annotators [9, 10, 11] these methods are rarely employed in real-world algorithm evaluation, where experimentation is typically limited to one annotation. Consequently, little is known about how different GTs and estimated gold standards affect the performance comparison of different algorithms.\nThrough performance evaluation, GT data often influences an algorithm\u2019s design, the choice of an algorithm\u2019s parameter values, and also influences the structure of the training data itself. It is therefore important to quantify the effect that different GTs have on reporting an algorithm\u2019s performance. Relying on one annotator\u2019s opinion allows an algorithm to learn the annotator\u2019s bias, and does not necessarily result in a model that is effective at locating the true target. This problem can be circumvented when the images are captured in\nar X\niv :1\n30 7.\n04 26\nv3 [\ncs .C\nV ]\n2 6\nA pr\n2 01\n6\ntightly controlled conditions or are synthetically generated [12] because the gold-standard GT is trivial to calculate. In remote sensing and medical imaging problems, and those concerning natural images, this is not the case.\nThe following assumptions regarding the problem\u2019s characteristics are implicitly made within this study. In computer vision problems true positive locations tend to be spatially correlated (segments tend not to be lone pixels, but a number of connected pixels) and correlated with some image properties. It is assumed that the annotators are not malicious in producing their annotation, are not producing annotations at random, and are not simply following low-level cues in the image, but are instead able to draw upon some higher-level knowledge. This allows them to distinguish between segments that belong to the negative class, but share the same low-level image properties as those segments that constitute the positive class.\nTherefore the objectives of this study are to: \u2022 empirically demonstrate any bias that results from evalu-\nating an algorithm with a single annotation; \u2022 quantify the effect that different GTs may have on the\nevaluation of multiple algorithms; \u2022 and provide a general comparison between algorithms\ndesigned to infer the gold-standard GT. The following section reviews relevant work from the literature. Section III prescribes the experimental methodology, the analysed datasets and the results are described in Section IV, and a discussion of these results is presented in Section V. Finally, Section VI presents the study\u2019s conclusions."}, {"heading": "II. RELATED WORK", "text": "In a classic study Smyth et al. [7] analyse the uncertainty of an annotator\u2019s judgement in marking volcanoes in synthetic aperture radar images of Venus. The authors assume a stochastic labelling process, to account for intra-annotator variability, and outline the probabilistic free-response ROC analysis that integrates the uncertainty of an annotator\u2019s judgement directly into the performance measure.\nMore recently a number of methods for combining multiple image annotations are proposed. These include work from the medical domain in which practitioners manually segment anatomical scans. The annotations are subsequently warped to match novel scans in order to estimate their segmentations. Kauppi et al. [4] take GTs as the intersection (consensus), fixed size neighbourhoods of the points marked by each annotator, and a combination of the two. The authors conclude that the intersection method is preferential as it results in the highest detector performance. Numerous weighted extensions to the voting framework have been proposed based upon global [13], local [14, 15, 13], semi-local [13, 16], and non-local [17] information.\nProbably the most popular gold-standard GT estimation method originating from the medical domain is proposed by Warfield et al. [8], named simultaneous truth and performance level estimation (STAPLE) in which annotator performance (measured as sensitivity and specificity) and the gold-standard GT are simultaneously estimated within a maximum-likelihood setting, the optimisation being solved using expectation-\nmaximisation (a variant for handling continuous labels has been proposed by Warfield et al. [1] and Xing et al. [18]). The same authors also propose an approach in which the bias and variance of each annotator is estimated instead of their sensitivity and specificity [1] and another variant that accounts for instabilities in the annotator performance measures [19]. Much subsequent work has concentrated on the STAPLE algorithm: removing its assumption that annotator performances are constant throughout the data [20, 21, 22], and COLLATE [23], which accounts for spatial variability in task difficulty. Landman et al. [24] point out that in research and clinical environments it is not often possible to obtain multiple annotations for the whole dataset. Extensions to handle multiple partial, but overlapping, annotations have therefore been proposed [19, 24, 25].\nKamarainen et al. [26] propose a simpler alternative to STAPLE by maximising the mutual agreement of annotator ratings. This approach avoids the use of priors, and does not introduce segments that did not appear in the original annotations. Langerak et al. [5] argue, however, that STAPLE fails when annotator uncertainty varies considerably due to the fact that the STAPLE algorithm combines all of the annotators\u2019 labellings. Instead they propose the selective and iterative method for performance level estimation (SIMPLE) in which only labels that are deemed reliable are taken into account. Li et al. [6] propose a probabilistic approach that uses level sets in which the likelihood function is inspired by the STAPLE algorithm (LSML). To overcome the susceptibility of the STAPLE algorithm to strongly diverging annotations they accept that the contribution of an annotator\u2019s judgement should be dependent upon their performance, but differently to STAPLE the energy function is constrained by a shape prior that is dependent upon the amount of detail in the annotator\u2019s marking, forming the LSMLP algorithm. Biancardi and Reeves [2] state that the STAPLE algorithm (even with the Markov random field extension) and simple voting strategies assume that the pixels are spatially independent. A novel voting procedure is introduced to overcome this. It is preceded by a distance transformation that attributes positive values to the inside of the GT segmentation\u2019s boundary, which increase towards its centre, and decreases negatively outside the segment border; thus the truth estimate from self distances (TESD) algorithm is introduced [2].\nA new direction that has recently gained interest is to combine the information derived from the manual annotations with that derived from the image to imply the location of features-of-interest. Yang and Choe [27] follow this path and propose a method that incorporates the warping error to preserve topological disagreements between the estimated gold-standard GT and the annotations. A number of extensions to the STAPLE algorithm have also been proposed [28, 29, 30] which incorporate the image\u2019s intensity values, as well as the performance of multiple experts, to transfer the labelling of one image onto that of another. Moreover, Asman and Landman [31] propose to combine a locally weighted voting strategy with information derived form the image\u2019s intensity.\nThe widely used Berkeley segmentation dataset contains five-hundred images, each having five GTs. The authors in-\nclude the level of annotator agreement within their evaluations [9], which provides a valuable reference when interpreting the results. Using the earlier Berkeley 300 database, Martin et al. [32] present a statistical analysis of the variation observed within the annotations [32]. They notice that independent annotators tend to include the same pixel in the same region, but also that the number of segments in the same image can vary by a factor of ten. The impact of GTs from different annotators on the ranking of segmentation algorithms has not yet been investigated."}, {"heading": "III. METHODOLOGY", "text": "To recapitulate, this work aims to demonstrate the effects of GT variability on the design, training, and evaluation of segmentation algorithms by studying their performance measured using single annotations, comparing multiple algorithms using different GTs, and comparing gold-standard GT inference algorithms. To achieve these aims, the methodological evaluation will be centred around three aspects: annotator agreement; the relation between annotator agreement and detector performance; and ground truths and reported detector performance. Scripts to recreate the results presented henceforth are available on-line1."}, {"heading": "A. Data", "text": "The data used in each of the case studies can be modelled as an image, I : {0, 1, . . . , X \u2212 1} \u00d7 {0, 1, . . . , Y \u2212 1} 7\u2192 R where X is the image\u2019s width and Y its height.\nFor each study N annotators have provided manual markings containing the locations of the foreground target specific to each study. All case studies are binary detection problems and each annotation has the value one where the annotator perceived the feature-of-interest to exist and zero otherwise. The result of this process is are N binary maps describing the location of the features-of-interest according to each annotator. As such, each annotator\u2019s output is modelled as a function Mn : {0, 1, . . . , X \u2212 1} \u00d7 {0, 1, . . . , Y \u2212 1} 7\u2192 {0, 1}, where 0 and 1 represent the absence and presence of the object respectively and n = 1, . . . , N ."}, {"heading": "B. Annotator Agreement", "text": "The first stage of analysis tests the level of agreement between the annotators in each case study, and exposes the image properties that promote this agreement.\nSmyth [33] presents a method for calculating the lower bound on error in a set of annotations relative to the (unknown) gold-standard ground-truth. This bound is defined to be\ne\u0304 \u2265 1 XYN Y\u22121\u2211 y=0 X\u22121\u2211 x=0 min {N \u2212A(x, y), A(x, y)} (1)\nwhere A(x, y) is the number of annotators that labelled pixel (x, y) as containing the feature-of-interest, Equation (2). The minimum of Equation (1) is reached when all annotators agree and the maximum (0.5) when the decisions are evenly split.\n1https://sites.google.com/site/tomalampert/code\nIt is therefore closely related to the entropy of the annotators\u2019 decisions. The maximum value for an acceptable level of experimental data quality suggested by the author is 10 %.\nAlso to this end, the per-pixel annotator agreement is calculated, which is simply the number of annotators that have marked each pixel, such that\nA(x, y) = N\u2211 n=1 Mn(x, y). (2)\nThe ratio of the number of pixels that are have a minimum level of agreement to the number of pixels that belong to annotated regions can therefore be calculated as follows\nA\u0302(n) = 1\n|C| X\u22121\u2211 x=0 Y\u22121\u2211 y=0 \u03c7B(x, y) (3)\nwhere B = {(x, y) | A(x, y) \u2265 n}, \u03c7B is the indicator function, C = {(x, y) | A(x, y) > 0}, and 1 \u2264 n \u2264 N is the range of values for the minimum level of agreement.\nThese functions allow for the testing of correlations between annotator agreement and different image properties\u2014a means to uncover at least part of the reason behind the variance of agreement. Each dataset presents different features, but where applicable the following will be tested: intensity, contrast, and each of the colour channels. The Pearson\u2019s r correlation coefficient will be used and, since the sample size for the analysis is extremely large, it will be tested for significance to 99 % confidence. In the case that the image is colour, intensity is calculated such that I(x, y) = 0.2989 \u00b7 R(x, y) + 0.5870 \u00b7 G(x, y) + 0.1140 \u00b7B(x, y). Image contrast in a colour image is calculated using the Michelson contrast measure within a 3\u00d7 3 local neighbourhood such that\nc(x, y) = max(i,j)\u2208Wxy L(i, j)\u2212min(i,j)\u2208Wxy L(i, j) max(i,j)\u2208Wxy L(i, j) + min(i,j)\u2208Wxy L(i, j) (4)\nwhere L(i, j) is the image\u2019s tone component, obtained by converting the colour image into the CIELAB colour space, and Wxy is the set of co-ordinates that define the neighbourhood of L(x, y). Image contrast in a grey scale image is calculated as above by substituting I(x, y) for L(x, y). For the comparison of contrast and agreement the maximum agreement within the local neighbourhood is used.\nGround truths at different levels of agreement are calculated such that\n\u03b3\u03c4 (x, y) = { 1 if 1NA(x, y) \u2265 \u03c4 , 0 otherwise, (5)\nwhere \u03c4 represents the level of annotator agreement. Additionally, a number of the gold-standard ground-truth estimation methods are evaluated. These weight annotations based upon the assumption that more reliable annotators can be identified through inter-annotator comparisons.\nTo examine the inter-annotator variability, cluster analysis using the pairwise F1-score between annotator markings is performed. The F1-score [34], calculated between participants i and j, is defined as\nFij = 2 pijrij pij + rij , (6)\nand this quantity is therefore the harmonic mean of precision (pij) and recall (rij). Note that the F1-score is robust in the presence of class-imbalance since it does not take into account true-negative classifications [34]. Hierarchical clustering is performed using Ward\u2019s minimum variance, implemented with the Lance-Williams dissimilarity update formula by linking pairs of annotations with the highest pair-wise F1-score and repeating this until all annotations are included.\nAs a principled way of identifying outliers within the group of annotations, the mean F1-score difference (1\u2212Fij) between each annotator and all other annotators is calculated. Those that have a mean difference greater than the average plus one standard deviation are labelled as outliers.\nFollowing the example of Saur et al. [35], and to highlight any individual differences between the annotators, each is compared to the group\u2019s consensus (image pixels that 50 % or more of the annotators marked as containing a relevant feature), calculated using Eq. (5) where \u03c4 = 0.5. This is achieved by calculating: Sensitivity, which measures the proportion of positives that\nare correctly identified as such; Specificity, which measures the proportion of negatives that\nare correctly identified as such; Positive Predictive Value (PPV), which measures the propor-\ntions of positives that are true positives; Negative Predictive Value (NPV), which measures the pro-\nportions of negatives that are true negatives; Cohen\u2019s kappa coefficient, which measures the inter-rater\nagreement correcting for agreement that occurs by chance."}, {"heading": "C. Relation between Agreement and Detector Performance", "text": "After analysing the properties of annotator agreement, it follows to investigate its relation to detector performance. Therefore four detectors are selected from each of the case study domains and applied to the detection problem at hand (every effort was made to select the best performing detectors within each domain). Each of the detectors is evaluated using GTs calculated at increasing levels of agreement according to Eq. (5), where \u03c4 = 1/N, 2/N, . . . , N/N .\nIt is common to measure detector performance through ROC curve analysis, however, recent literature points out that this may overestimate performance when applied to highly skewed datasets (those in which the number of positive, Np, and negative, Nn, examples are not balanced) and therefore precision-recall (P-R) curves are preferable [36, 34]. Nevertheless, precision is sensitive to the skew ratio, \u03c6 = Np/Nn. To overcome this Flach [37] proposes to analytically vary the skew ratio in the precision measure and Lampert and Ganc\u0327arski [38] to integrate this added dimension, thus forming a P\u0304-R curve. This allows P\u0304-R curves derived from GTs containing different skew ratios, i.e. GTs derived from different levels of agreement, to be compared and for a fair representation of detector performance in problems in which the skew ratio is a priori unknown. The measure is defined such that\nP\u0304 (\u03b8) = 1\n\u03c0\u20322 \u2212 \u03c0\u20321 \u222b \u03c0\u20322 \u03c0\u20321 \u03c0\u2032TP(\u03b8) \u03c0\u2032TP(\u03b8) + (1\u2212 \u03c0\u2032)\u03c6FP(\u03b8) d\u03c0\u2032 (7)\nwhere \u03b8 is a threshold on the detector\u2019s output, \u03c0\u20321 and \u03c0 \u2032 2 are the lower and upper bounds of the problem\u2019s estimated range of skew ratios, and TP(\u03b8) and FP(\u03b8) are the number of true positive and false positive detections. Interpolation between P\u0304R points [38] enables accurate area under the curve (AUCP\u0304R) measurements to be taken.\nTo assess the relation between annotator agreement and detector output two correlation coefficients will be measured (to 99 % confidence). The first being the correlation calculated within locations identified as features by any of the annotator (CCO) and the second the whole image (CCI). The first of these highlights the relation between the detector output and annotator agreement in positive feature locations. The second includes any false positive detections that the detector may make, and therefore the absolute value of these correlations in addition to the difference between them are indicative of a detector\u2019s reliability."}, {"heading": "D. Ground Truths and Reported Detector Performance", "text": "The final question that this research intends to investigate is: how great is the influence of different ground truths on an algorithm\u2019s reported performance?\nTo this end several GTs are calculated according to Eq. (5): the combined annotations where \u03c4 = 1/N , i.e. segments of interest that any annotator marked (Any-GT); the consensus of half of the annotators, or majority vote, in which \u03c4 = 0.5 (0.5-GT); and the consensus of three-quarters of the annotators, where \u03c4 = 0.75 (0.75-GT). Also included are gold-standard GT estimations calculated using STAPLE [8] (without assigning consensus votes [22]), SIMPLE [5], and LSML [6] (using 0.5-GT as an initial estimate and 1000 iterations). Furthermore, an additional GT is determined by excluding outlying annotations (these will be identified in Section III-B) and combining those remaining according to Eq. (5) with \u03c4 = 0.5 (Excl-0.5-GT).\nTwo forms of evaluation are investigated: the first being the relative detector ranking, ranked according to the area under the P\u0304-R curve; and the second being the variability observed in the absolute values of the P\u0304-R curves."}, {"heading": "IV. EXPERIMENTAL RESULTS AND ANALYSES", "text": "This section presents the results of applying the described methodology to each of the case studies included in this investigation."}, {"heading": "A. Data", "text": "The case studies presented in this section are concerned with: Image segmentation Most of the images within the Berkeley 300 (colour) dataset have been annotated by numerous different annotators. Only for a small subset of five images did the same annotators perform the segmentation (annotator IDs for the Berkeley 500 dataset are not available). These images are: 65033.jpg, 157055.jpg (Figure 1a), 385039.jpg, 368016.jpg, and 105019.jpg. Each image was concatenated to form one large image, in which X = 1595 and Y = 479, and the same process was used to form one GT for each of the annotators.\nFissures in remotely sensed images The data is obtained from the Super-Sauze landslide in the Barcelonnette basin, southern French Alps, using an unmanned aerial vehicle to obtain high resolution images. Further information regarding this dataset is present in the literature [39, 40]. An area of interest, where X = 1425 and Y = 906, was extracted from the data and is presented in Figure 1b. Very little colour information is present in this type of image and it was therefore converted to grey scale using the standard formula: I(x, y) = 0.2989\u00b7R(x, y)+0.5870\u00b7G(x, y)+0.1140\u00b7B(x, y). Thirteen annotators (N = 13) were enlisted to manually mark the pixels in the (RGB) image that formed part of a fissure. Within this section, each of these participants will be referred to as A1\u2013A13. The level of expertise ranged from expert geomorphologists familiar with the study site (2), nonexperts familiar with fissure formation and/or detection (5), and contributors without any a priori knowledge (6). Prior to the marking experiment, all the annotators were given a basic introduction to fissure characteristics. The annotators then independently marked all the pixels that they believed to form part of a fissure, taking as much time as they required (this ranged from 2 to 3 hours). The annotators were encouraged to perform the marking on a level in which they could see individual pixels clearly and zoom in and out as needed to assess the context of the area being marked. Landslides in satellite imagery The dataset is derived from Geoeye-1 satellite images with four spectral bands (blue, green, red, and near infra-red) and a nominal ground resolution of 50 cm. The image presented in Figure 1c was captured at Nova Friburgo, Brazil shortly after a major landslide event in January 2011 and covers approximately 10 km2 (X = 5960 and Y = 5960 pixels). A second image was recorded by the same satellite in May 2010 and depicts the ground conditions before the event. Five annotators (N = 5), who were all familiar with landslide mapping in remote sensing images, were asked to independently mark the outlines of the regions affected by landslide activity. To achieve this, the RGB components of the pre-event and the post-event satellite images were visualised using a natural color scheme. Detailed information regarding this dataset exists in the literature [41]. Retinal blood vessels The STructured Analysis of the Retina (STARE) dataset was used in this case study. The dataset consists of twenty colour retinal images, which for the purposes of this study are treated as a single image (X = 2800 and Y = 3025). An example image is presented in Figure 1d. A mask was formed which delineates the pixels that fall outside the retina by thesholding the intensity of the red channel at a value of 40 (the black area) and these pixels were excluded from the experiments. The dataset contains two annotations which delineate the blood vessels in the image."}, {"heading": "B. Annotator Agreement", "text": "The pixel-level annotator agreements for each case study are presented in Figure 2. To verify that these are acceptable for experimental use Smyth\u2019s lower error bound estimate, i.e. the average error rate amongst the annotators, was calculated and found to be e\u0304 \u2265 2.6611 % (Segmentation), e\u0304 \u2265 1.26 % (Fissure), e\u0304 \u2265 1.1012 % (Landslide), and e\u0304 \u2265 3.1123 % (Blood Vessel). These values are well within the 10 % limit\nthat is recommended [33] and considerably lower than the error bound of 20 % found in the volcano labelling experiment presented by the author [33], in which the signal-to-noise ratio of the features is much lower than in the presented case studies.\nThe ratio of pixels having a minimum level of annotator agreement to the number of pixels that belong to annotated regions is presented in Figure 3. For the Segmentation, Fissure and Blood Vessel case studies the ratio decreases approximately exponentially as a function of minimum annotator agreement. For the Fissure dataset the thirteen annotators agree on only approximately 0.6979 % of all of the pixels that were marked as fissures by any of the annotators. The ratio decreases most rapidly in the Segmentation case study, whereas the Landslide case study exhibits a rather linear trend. These differences are due to a combination of the geometric structure of the targeted objects, and the fact that disagreement\ntends to occur along object borders. As such, uncertainties in the outline of a feature lead to a stronger disagreement if the targeted features are only one pixel wide (Segmentation) or several pixels wide (Fissure, Blood Vessel) when compared to the rather blob-like regions exhibited in the Landslide case study. Indeed, if the outlines of the Landslide annotations are analysed, agreement also drops approximately exponentially.\nThe correlation coefficients between annotator agreement\nand image properties are presented in Table I. These offer an explanation for the relation between detector performance and annotator agreement that will be explored in the remainder of this paper, i.e. stronger image features tend to be more confidently detected by a detection algorithm, and also attract higher levels of annotator agreement. In each case study there exists at least one significant correlation between image properties and agreement: contrast in the Segmentation case study indicating that the annotators tend to agree on stronger edges; contrast and intensity in the Fissure case study indicating that dark fissures on a lighter background attract greater agreement; near-IR and red, which exhibit a strong response if vegetation is removed during a landslide because the reddish soil is exposed; and green in the Blood Vessel case study, which is the principal channel used for discrimination in many blood vessel detection studies.\nDendrograms describing the annotator pairwise F1-scores in each case study are presented in Figure 4 and the full statistics of each annotator compared to the average annotation (Eq. (5), \u03c4 = 0.5) are presented in Table II.\nThe relatively low levels of agreement in the segmentation problem are reflected in the pairwise differences in F1-scores used to form the dendrogram in Figure 4a. The differences are relatively high, ranging from 0.545 to 0.680, and one outlier is identified: A5 (the mean F1-score difference was found to be 0.6016, with a standard deviation of 0.0280, and A5 resulted in a difference of 0.6454). This annotator also results in the lowest specificity, positive predictive value, and kappa coefficient as shown in Table IIIa. The variance in the annotations are emphasised by the lowest specificities observed in all of the case studies. A dendrogram describing the Blood Vessel dataset is not included as no outliers can be identified with only two annotations. Nevertheless, the F1-score difference (1 \u2212 Fij) calculated between the two annotations was found to be 0.2583 meaning that they give fairly consistent markings. The statistics in Table IIId are not as informative as in the other case studies due to the low number of annotators and this highlights one of the issues of estimating GTs using few annotations and such statistical comparisons. Nevertheless, we can infer that A2 marked a much larger number of blood vessels compared to A1 due to A2 having a high sensitivity and A1 not (in this case the 50 % agreement GT with which these statistics are calculated contains locations that any of the annotators marked, hence the specificity and PPV being one).\nIt would be expected that more than one cluster emerges within the Fissure case study, Figure 4b, partitioning the different experience levels; however, this isn\u2019t the case and annotators of varying levels of expertise are quite homogeneously mixed. This indicates that none of the groups is overly biased in favour of one particular decision. Annotators A1, A4, and A11 are identified as falling outside of one standard deviation of the mean F1-score difference. These same annotators achieve considerably lower sensitivity when compared to the consensus (see Table IIIb). They also result in lower kappa coefficients and PPVs\u2014indicating that, when compared to the consensus, these annotators fail to identify a majority of the fissures and/or produce more \u2018false negative\u2019 and \u2018false\npositive\u2019 detections. The mean F1-score difference (1\u2212Fij) is found to be 0.5765 and the standard deviation 0.0459, these annotators fall outside this threshold having a mean difference of 0.6716, 0.6321, and 0.6287 (corresponding to A1, A4, and A11 respectively). It is illustrated by these results that all of the annotators are reliable in detecting negative instances of fissures, indicated by high specificity and negative predictive values, due to the highly skewed nature of the problem in which negative instances constitute a high proportion of the data. Highlighting the difficulty and uncertainty in detecting positive instances in this dataset, however, are low sensitivity and PPVs.\nIn the Landslide case study, each of the annotators were geographers familiar with the detection of landslides in remotely sensed imagery. This is reflected in the low inter F1score difference (1\u2212Fij), which ranges from 0.14 to 0.28 (by comparison this range was approximately 0.40 to 0.75 in the Fissure case study). Nevertheless, one outlier is identified and this is A2 (the mean difference was found to be 0.2044 and its standard deviation 0.0275, A2 resulted in a mean difference of 0.2438). This annotator also results in the lowest of the sensitivity and negative predictive values (when compared to the consensus opinion) presented in Table IIIc. On average, sensitivity, PPV and kappa are higher than in the Fissure case study, indicating that the features used for the identification of landslides are more clearly defined and understood by the annotators."}, {"heading": "C. Agreement and Detector Performance", "text": "During these case studies a number of detectors were selected and their ability to detect features in the area of interest was evaluated by calculating P\u0304-R curves: Segmentation The top four performing segmentation algorithms listed on the Berkeley dataset web page2 were selected to form part of this case study. These were: REN [42], gPbucm (UCM) [9], Global Probability of Boundary (GP) [43], and XREN [44]. The integration limits of the P\u0304-R curves were \u03c0\u20321 = 0.0000 and \u03c0 \u2032 2 = 0.0428, which were found to be \u03c0\u20321 = \u00b5 \u2212 3\u03c3 and \u03c0\u20322 = \u00b5 + 3\u03c3 where \u00b5 is the mean skew found within the Berkeley dataset and \u03c3 its standard deviation [38]. As discussed by Martin et al. [45], when evaluating segmentation algorithms it is common to loosen the definition of true-positive detections to account for deviations in detected boundary location. True-positive detections are accumulated if a detection is within a defined distance of one or more GT boundaries. In these experiments the allowed distance is taken to be the default found with the Berkeley benchmark code\u20140.0075 times the length of the image\u2019s diagonal. The images 105019.jpg and 368016.jpg are randomly selected for use as the training set and removed from this point forward. One further modification to the methodology was made to better suite the definition of segmentation. The low agreement GTs (\u03c4 = 1/N , for example) result in multiple pixel wide segmentations (as annotators may agree upon the boundary\u2019s existence, but not on its exact location), which causes an unfair\n2http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/ segbench/bench/html/algorithms.html, accessed 23rd October 2015\npenalty on the algorithm because a segmentation algorithm is designed to detect single pixel segmentation boundaries. Therefore, each GT is thinned prior to its use to reduce the boundary widths to one pixel whilst preserving any individual, low agreement markings. Fissure Current state-of-the-art linear feature detectors were selected from the literature: a linear classifier trained using 2D Gabor wavelet (elongation = 4, scales a = 2, 3, 4, 5, and frequency k0 = 3) and inverted grey-scale features (2D GWLC) [46]; Gaussian filter matching, where \u03c3 = 1 [47] (Gauss); Top-Hat transform (4 pixel radius circular structuring element); and the Centre-Surround (C-S) transform (using a 3 \u00d7 3 pixel neighbourhood) [48]. Where public source code was not available the respective authors kindly agreed to run the algorithm on the data and provide a number of outputs, calculated using a range of parameter values (to ensure that the implementations were true to the author\u2019s intentions and to allow reproducibility of the results). As the 2D GWLC method is a supervised learning algorithm a random subset of the image, 569\u00d7 362 pixels in size, was used as a training set (16 % of the image), the GT was defined according to Eq. (5) using \u03c4 = 1/N , and the training area was excluded from the test set. Within this case study the P\u0304-R integration limits were set to \u03c0\u20321 = 0.1 and \u03c0 \u2032 2 = 0.5 (from ten times as many negative as positive instances to a balanced dataset) to reflect the large range of skews that can be observed in a remote sensing application. Landslide Four popular classification algorithms were applied (due to their proven strength in real-world applications): random forest (RF) [49], support-vector machine (SVM), knearest neighbours (KNN), and a neural network (ANN). After fine scale image segmentation, 101 features describing the spectral characteristics, texture, shape, topographic variables, and neighbourhood contrast were extracted. The resulting dataset is available on-line3 and a detailed description of the feature extraction methods are given in the literature [41]. Each classifier was trained upon samples from the same randomly\n3http://eost.unistra.fr/recherche/ipgs/dgda/dgda-perso/andre-stumpf/ data-and-code/\nselected square subset covering 10 % of the area of interest. The number of trees in the RF was fixed to 500 and 10 variables were tested for the splits at each node. The SVM used a radial basis kernel having parameters C = 10 and \u03c3 = 0.004, determined through an exhaustive grid search. The ANN was a single layer network with a logistic activation function. An exhaustive grid search to optimize the weight decay function and the number of nodes resulted in values of 0.1 and 7, respectively. Likewise, a grid search for the number of nearest neighbours resulted in k = 23 for the KNN algorithm. Parameter tuning was performed through bootstrap resampling of the training data using the area under the ROC curve as a performance measure. The P\u0304-R integration limits were set to \u03c0\u20321 = 0.01 and \u03c0 \u2032 2 = 0.10 to reflect typical ratios of affected to unaffected areas after large-scale landslide triggering events [50]. Blood Vessel The four detectors selected for this case study were the Matched-Filter Response (MSF) [11], Linear Classifier (LMSE), k-nearest neighbours (KNN), and Gaussian Mixture Model (GMM). The LMSE, KNN and GMM classifiers were implemented using the MLVessel software package [46], the features were taken to be the inverted green channel, and the responses of Gabor wavelets (elongation = 4, scales a = 2, 3, 4, 5, and frequency k0 = 3) applied to the inverted green channel. The first five images of the dataset (im0001\u20135) were used exclusively for training. The P\u0304-R curve integration limits were \u03c0\u20321 = 0.023 and \u03c0 \u2032 2 = 0.235, which were found to be \u03c0\u20321 = \u00b5 \u2212 3\u03c3 and \u03c0\u20322 = \u00b5 + 3\u03c3 where \u00b5 was the mean skew found within a number of retinal image datasets and \u03c3 its standard deviation [38].\nThe P\u0304-R curves derived from these detectors are presented in Figure 5. A striking observation is that the performance of all detectors increases with annotator agreement in a predictable manner in the higher recall ranges. It was shown in Table I that there is a tenancy for annotators to agree upon more obvious image features, and these results indicate that the detectors extract similar features. Regarding the Fissure dataset, there is a large difference between the detection rate of high and low agreement fissures\u2014detection of the lower is not a trivial matter and most likely needs to be augmented with high-level information that is not exploited by the evaluated detectors. In the lower recall ranges of the Segmentation, Landslide, and Blood Vessel case studies the tendency for precision to increase with agreement is reversed. This phenomenon can be explained by analysing the correlations between annotator agreement and detector output presented in Table III.\nSeveral general tendencies can be drawn from these correlations. The detectors that exhibit a large drop between CCO and CCI also exhibit low sensitivity (i.e. produce a high falsepositive rate). For example, this is reflected in the P\u0304-R curves of the C-S detector (Figure 5h): low sensitivity dominates the low agreement ground truths (for example, \u03c4 = 1/13), but the detector results in the highest performance when using the high agreement ground truths (for example, \u03c4 = 1). The detectors that exhibit a high correlation with agreement over the whole image, and also exhibit the lowest drop in correlation between the two tests (2D GWLC, Gauss, SVM, & GMM detectors for example), have (relatively) low false positive rates and result\nin high P\u0304-R curves (Figures 5e, 5f, 5j, & 5p). A large drop in correlation, along with a low absolute correlation, is observed with the Top-Hat detector, and indeed in Fig. 5g the curves are skewed towards lower precision values. The detectors that result in the lowest drop or an increase in correlation (2D GWLC, Gauss, RF, KNN, SVM, & ANN) result in a tighter spread of P\u0304-R curves (Figures 5e, 5f, 5i, 5k, 5j, & 5l).\nThe P\u0304-R curves from the Segmentation, Landslide and Blood Vessel case studies largely follow the trend: as agreement increases, algorithm performance also increases. There is, however, a tendency for precision to be inversely proportional to agreement in lower recall ranges. This phenomenon can be explained by analysing the correlations between annotator agreement and detector outputs presented in Table III and noting that in all the cases in which this trend is observed CCI is higher than CCO. This indicates that detector outputs agree with annotator agreement within feature locations and more so over the whole image, implying that there is a relatively low FP detection rate, which at the lower recall ranges results in high precision. As the threshold on agreement increases, image locations having increasingly stronger features form the GT and these locations also result in the highest detector responses. Furthermore, high CCI values imply that as lower agreement segments are removed from the GT they are instead classified as false positive detections, thus reducing precision in the lower recall ranges as the threshold on annotator agreement is increased."}, {"heading": "D. Ground Truths and Reported Detector Performance", "text": "The detector ranks (measured as AUCP\u0304R [38]) when evaluated using different GTs were determined, and in three of the case studies (Segmentation, Fissure, and Blood Vessel) three rankings emerged, which are described in Table IV. In the Landslide case study only one emerged due to the low interannotator variance. In the Fissure, Landslide, and Blood Vessel case studies these ranks reflect the results of the correlation\nanalyses: the top ranked detectors (2D GWLC, SVM, and GMM) and the bottom ranked detectors (Top-Hat and KNN) correspond to either the highest correlations or the lowest drops in correlation observed in the previous section (see Table III). Furthermore, in the Fissure case study the ranking observed is not determined by the level of annotator expertise. This corroborates the lack of distinction between different expertise levels in the dendrogram presented in Figure 4b.\nIn the Segmentation case study, however, the algorithms with the highest correlation (UCM) and the highest CCO to CCI increase (GP) are ranked at the middle or bottom. On one hand this can be attributed to the relatively high\nannotation variance and the overall low correlation between detector output and the annotator agreement (Table III). On the other hand it should also be considered that the correlations are derived using all of the annotated pixels, while the P\u0304-R curves are calculated using GTs that were thinned to a width of one pixel and the TP rates calculated with a tolerance to small mismatches of the segmentation boundary. This not only highlights the sensitivity of evaluating algorithms using different GTs that exhibit high variance, but also illustrates how different evaluation strategies can provoke different outcomes.\nIn the Fissure case study, a majority of the individual annotations give the same ranking as obtained using the\nTABLE IV RANKINGS OF DETECTORS EVALUATED USING EACH GROUND TRUTH\n(MEASURED BY THE AREA UNDER THE P\u0304-R CURVE). (A) SEGMENTATION CASE STUDY, THE GTS THAT RESULT IN EACH RANKING ARE: RANKING #1 \u2014 BERKELEY EVALUATION FRAMEWORK (A1\u2013A5); RANKING #2 \u2014 A4, A5, ANY-GT, LSML-GT, STAPLE-GT; RANKING #3 \u2014 A1, A2, A3, 0.5-GT, 0.75-GT, EXCL-0.5-GT, SIMPLE-GT. (B) FISSURE CASE STUDY, THE GTS THAT RESULT IN EACH RANKING ARE: RANKING #1 \u2014 A2(NE), A4(FE), A6(NE), A11(FE), ANY-GT, LSML-GT, STAPLE-GT, EXCL-0.5-GT; RANKING #2 \u2014 A1(NE), A3(NE), A5(NE), A7(IE), A8(FE & IE), A9 (NE), A10(FE), A12(FE), A13(FE & IE), 0.5-GT, SIMPLE-GT; RANKING #3 \u2014 0.75-GT. (C) LANDSLIDE CASE STUDY, ALL GTS RESULT IN THE SAME RANKING. (D) BLOOD VESSEL CASE STUDY, THE GTS THAT\nRESULT IN EACH RANKING ARE: RANKING #1 \u2014 A1, 0.75-GT, SIMPLE-GT; RANKING #2 \u2014 A2, 0.5-GT/ANY-GT, STAPLE-GT;\nRANKING #3 \u2014 LSML-GT.\nPosition Ranking #1 Ranking #2 Ranking #3 1 REN REN REN 2 GP GP XREN 3 UCM XREN GP 4 XREN UCM UCM\n(a) Segmentation\nPosition Ranking #1 Ranking #2 Ranking #3 1 2D GWLC 2D GWLC C-S 2 Gauss C-S 2D GWLC 3 C-S Gauss Gauss 4 Top-Hat Top-Hat Top-Hat\n(b) Fissure\nPosition Ranking #1 1 SVM 2 RF 3 ANN 4 KNN\n(c) Landslide\nPosition Ranking #1 Ranking #2 Ranking #3 1 GMM GMM GMM 2 MSF KNN KNN 3 LMSE MSF LMSE 4 KNN LMSE MSF\n(d) Blood Vessel\nSIMPLE-GT, and 0.5-GT, however, when the 0.75-GT, AnyGT, STAPLE-GT, and LSML-GT are under consideration, the ranking changes\u2014the method of calculating the GT influences detector ranking. More importantly, the ranking derived using a 75 % voting strategy (Fissure) and LSML (Blood Vessel) are in disagreement with that obtained using the individual annotations, which contradicts what should be expected. To illustrate ranks in the Fissure case study, the P\u0304-R curves for all four detectors evaluated using the STAPLE-GT, 0.5-GT, and 0.75-GT are plotted in Figure 6, each colour represents one of the rankings presented in Table IVb.\nIn the Blood Vessel case study the ranks of the lower three detectors are not consistent. The MSF detector, for example, achieves the lowest performance in Figure 5 along with the lowest correlation with annotator agreement (Table III), however, depending upon which GT is used, this detector is placed second, third, or last.\nAn overview of the performance variations that result from\nusing different GT estimation methods and evaluation frameworks can be obtained from Figure 7, in which the P\u0304-R curves obtained using the best performing detector in each of the case studies are presented. The P\u0304-R curves for the REN segmentation algorithm (Figure 7a) exhibit the largest level of variance, a result of the high annotator variance observed in Section III-B. At the upper extreme of this variance is the methodology prescribed for evaluating segmentation algorithms upon the Berkeley datasets, which includes a tolerance for misalignments of TP detections. The 0.75-GT, 0.5-GT, and SIMPLEGT yield higher performance curves (particularly in higher recall ranges) and Any-GT relatively low performance curves when compared to the remaining GTs. The STAPLE-GT and LSML-GT curves show low p\u0304recision (when compared to the remaining curves) in the upper recall ranges, but model the mean of the individual annotation curves in the lower recall ranges. This is a consequence of the large variance observed in the annotations. The curves resulting from Excl-0.5-GT and SIMPLE-GT are very similar as they are both derived using the same principle (removing outliers and then voting).\nThe P\u0304-R curves resulting from 2D GWLC are presented in Figure 7b. The effects of the voted GTs (0.5-GT, 0.75-GT, and SIMPLE-GT) become evident: these P\u0304-R curves estimate a relatively high detector performance and seem to act as generous estimates of the upper bound of the performance derived from the individual annotations. Moreover, the AnyGT appears to act as an estimate of the lower bound of the performance derived from the individual annotations, and when sufficient annotations are available (Fissure and Landslide) the curves obtained using STAPLE-GT and LSML-GT appear to approximately model the mean of the performance obtained using the inlying individual annotations. It should be noted, however, that the LSML technique is highly dependent upon the estimate used for initialisation.\nSimilarly, in the Landslide case study (Figure 7c) 0.5-GT, 0.75-GT, and SIMPLE-GT yield P\u0304-R curves that seem to model the upper bound of the performance obtained using the individual annotations, STAPLE and LSML tend to produce\nGTs that result in P\u0304-R curves that are within the range of those obtained using the individual annotations, and Any-GT marks the lower bound of the detector\u2019s performance. Overall it can be observed that the lower annotator variance observed in this case study leads to a significantly lower P\u0304-R curve spread.\nOn the contrary, in the Blood Vessel case study (Figure 7d, due to the limited number of annotations the Any-GT, 0.5- GT, and Excl-0.5-GT are identical) the LSML-GT forms a lower bound on the reported performance. The STAPLE-GT (equal to the 0.5-GT and the Any-GT) delineates the mean of all the curves, whereas previously (but to a lesser extent in the Segmentation case study) the STAPLE-GT and LSML-GT represented an estimate of the mean of the curves obtained using the individual annotations. Once more 0.75-GT results in a higher estimate of performance than that obtained using each of the individual annotations."}, {"heading": "V. DISCUSSION", "text": "The following discussion is divided into two parts: the first summarises the results presented in the previous section and their implications, and the second presents general recommendations that can be derived from these implications."}, {"heading": "A. Summary of Results", "text": "It has been shown that the performance of classifiers and detectors increases as GTs are formed using increasingly higher agreement levels. Forming a GT using an agreement of 50 % generally increases a detector\u2019s reported performance to a range far greater than that obtained using all of the individual annotations. Kauppi et al. [4] conclude that the intersection method (consensus) is preferential as it results in the highest performance. Nevertheless, this study indicates that the method focusses on evaluating a detector against the most obvious segments in the image and provides overly optimistic performance estimates. Raising the level of agreement at which the GT is calculated increases this tendency.\nOne factor that has a stabilising effect on reported performance is low annotation variance. The Landslide dataset contains the lowest variance between annotations and this is reflected in the tight spread of the performance curves and in the stability of the detector ranking. Hence choosing any of the GTs for evaluating an algorithm would have resulted in similar reported performance. On the other end of the scale the Segmentation dataset contained the largest annotation variance, and the reported performances also exhibit the largest variance. This is in contrast to the findings of Martin et al. [32] who found a large amount of agreement between the segmented regions, but not the boundaries themselves. This also affected the gold-standard GT estimation methods, where in the other case studies the STAPLE and LSML methods typically modelled the \u2018mean\u2019 performance derived using the individual annotations, in this case study they actually resulted in the lowest performance curves. Both of these methods combine annotations using the annotator\u2019s statistical profile and given that there is a large variance in this dataset this may not be appropriate. In this situation removing the outlier annotations and performing consensus voting appears to be\nmore stable. In all but the Fissure case study this method also reported similar performances to that obtained using the STAPLE and LSML algorithms.\nBy and large, when the variance between annotations is relatively low (for example in the Landslide case study in which the F1-score differences range from 0.14 to 0.28) the STAPLE and LSML methods provide GTs that report a performance within the middle of that reported by each of the individual annotations. Nevertheless, as noted above, this is not the case when annotation variance increases or few annotations are available (as in the Blood Vessel case study) and this seems to be in line with other studies [5]. The SIMPLE algorithm was proposed to overcome these limitations when annotator uncertainty varies considerably [5] and indeed, in these situations it does seem to offer an improvement (see, for example, the Segmentation and Blood Vessel case studies). Nevertheless, when the variance in annotator agreement is not so extreme, SIMPLE seems to result in an overestimation of performance (see the Fissure dataset for example).\nThe output of all of the detectors produced medium to high correlations with annotator agreement. It can be stated that a detector\u2019s performance increases as the agreement upon the segment increases and those detectors resulting in the lowest drop in correlation (from CCO to CCI) result in a lower P\u0304-R curve spread. This seems intuitive as agreement should be higher for more obvious segments and, assuming that the detector is effective, these should also elicit the highest detector responses. This translates to increasingly higher P\u0304R curves as GTs with higher levels of agreement are used. Unexpectedly however, when the correlation between detector output and agreement increases from within segment locations (CCO) to the whole image (CCI), precision decreases in lower recall ranges. Surprisingly, this reduction in precision indicates an accurate detector\u2014as agreement increases, loweragreement segments are removed from the GT causing the detector to classify them as false positives. This could be an indication that some of the annotators have missed important segments, which the detector considers to be true positives, and providing these locations as feedback to the annotators for confirmation could be a way of improving GT reliability.\nThe image features included in this study account for a high proportion of the observed agreement (it should be kept in mind these features are not independent of each other), but capture only local, low-level information, ignoring any higher level and global queues and knowledge that the annotators exploit. Further evidence for this is provided by the agreement level GT curves, which generally show that there is a large difference between the detection rate of high and low agreement segments\u2014detection of the lower is not a trivial matter and the decision most likely needs to be augmented with high-level information that is not exploited by these detectors.\nIn all but the Landslide case study it has been shown that the rank of a detector is dependent upon the GT used for evaluation. It can therefore be stated that the variance in performance observed when evaluating two detectors using different GTs is not equal and therefore, the relative difference in performance between detectors is dependent upon the GT used for evaluation. Three different rankings were observed in\nthree of the four case studies. In one occasion the top ranked detector changed depending upon the GT, however, in most cases the top ranked detector remained constant. This is partly due to the fact that these top ranked detectors are considerably superior to the remaining and had their performance been closer this would not have been the case. The effects are most obvious in the Blood Vessel case study, in which the detector that produces the worst correlation with annotator agreement (MSF: CCO = 0.3923 and CCI = 0.3573) was placed second, third, and fourth in each of the three emergent rankings, even though it is clearly the worst performing of the evaluated detectors. Moreover, taking the 50 % or 75 % consensus GTs does not necessarily result in a detector ranking that is the consensus of the ranks obtained using the individual annotations (see, for example, Tables IVb and IVd). In fact, it can produce a ranking that has nothing in common with these individual rankings (Table IVb).\nThe largest minimum bound on error, e\u0304, was found in the Blood Vessel case study although the Segmentation and Fissure case studies produced the lowest pairwise F1 scores (in fact the agreement between the two annotators in the Blood Vessel case study is relatively high). This uncovers two peculiarities with Smyth\u2019s calculation (see Equation (1)) when used with only two, and an odd number of, annotators: the maximum of e\u0304 is reached when the maximum disagreement amongst the annotators takes place. On either side of this\nmaximum e\u0304 decreases symmetrically. First, when only two annotators are present, N = 2, any disagreement results in the maximum of the function since [N \u2212max{A(x, y), N \u2212 A(x, y)}]/N \u2208 {0, 0.5}. Secondly, when an odd number of annotators are present this term can not reach the theoretical maximum of 0.5, and therefore all disagreements contribute less than in the case of two annotators. Thus although the F1 score attests to greater agreement in the Blood Vessel case study, it receives a higher minimum bound on the error.\nFinally, as has been shown in the Segmentation case study, the evaluation framework adopted in this domain, through accounting for variances observed in the annotations, yields a very optimistic estimate of algorithm performance when compared to the traditional precision-recall evaluation framework."}, {"heading": "B. Recommendations", "text": "Comparing annotators and deciding upon outliers based solely upon inter-annotator performance is not a reliable method even though it offers reasonable modelling of\u2014what could be described as\u2014the average performance when correctly implemented (the SIMPLE, and to some extent the LSML, algorithms for example). Several counter examples can be easily proposed, such as a situation in which all but one annotator is inaccurate, a case in which the accurate annotator would be deemed an outlier and removed. Furthermore, an inaccurate annotation could in fact contain all of the true\npositive positions, but have low specificity, other annotations may have low sensitivity and therefore removing the \u2018outlier\u2019 implies discarding valuable information that may not be possible to infer using other means. As Smyth [33] states \u201cwithout knowing GT one can not make any statements about the errors of an individual labeller\u201d.\nOverly simplistic methods to utilise all of the available annotations (voting) have been shown to fail. More sensitive algorithms, such as STAPLE, take a step in the right direction. Nevertheless, these algorithms still assume that the gold-standard ground truth can be inferred by measuring the performance of annotators in relation to each other. The most promising advances have started to integrate information derived from the image into the process, and it has been shown herein that these properties do correlate with annotator agreement. Care should be taken, however, as this produces a somewhat circulatory solution in which the image features used by the detection algorithms are also used to decide upon which segments the algorithms are evaluated. Furthermore, in some domains correlation strengths between annotator agreement and image features decrease when moving from within segment locations to the whole image. Demonstrating that these properties are not uniquely tied to the segments of interest and employing this source of information risks introducing false positive locations to the inferred GT.\nIn other fields of science, progress has been made on improving the rating of annotator performance by gathering meta-data along with the annotations. The Cooke method [51] prescribes that the annotators are asked to estimate a interval of probable values along with their concrete answer, and furthermore they are also asked to answer multiple questions on topics from their field that have known answers. This information is used to weight the annotator\u2019s contribution in relation to their accuracy in this estimation and thus, has been shown to be more accurate than consensus voting [52].\nIt is clear that evaluating upon different GTs, whether these are annotations or some merging thereof, reveals different trends in the performance of classification algorithms. Synonymously, different images reveal different algorithm strengths during evaluation and, as such, large datasets are used to smooth the differences and reveal the best overall performing algorithm. However laborious it may be, the presented work implies that an algorithm should also be evaluated using different GTs. While the presented study does not offer an ultimate solution for how those GTs should be combined the described analysis framework provides a means to quantify the spread of measured performance and test whether the observed differences in performance are significant or not.\nThe variance of the annotations, and thus the variance of the algorithm\u2019s measured performance, is indicative of the number of annotations that should be collected for accurate evaluation. The Landslide case study, for example, exhibits low annotator variance and this is reflected in the spread of P\u0304-R curves, which are relatively tightly clustered. Performance bounds can therefore be reliably estimated with few annotations. The Segmentation annotations, in contrast, exhibit large variance, as do the resulting P\u0304-R curves. Under these conditions (and those in which few annotations are available, such as in the\nBlood Vessel case study) it may not be possible to state with certainty whether one algorithm outperforms another and further studies with more annotations should be conducted.\nConsidering that in all of the evaluated datasets the Any-GT and high agreement level GTs (0.5-GT or 0.75-GT) appear to model the lower and upper bounds (respectively) on the spread of measured performance, this may offer a means of measuring the performance overlap between two algorithms, which would be characteristic of the confidence that can be attributed to any measured differences in performance.\nThis approach accepts that there exists imperfections in the individual annotations, which are included in the Any-GT, but assuming that a perfect detector is created, these imperfections cause the performance to degrade and simply decreases the lower bound on performance (and therefore represents the uncertainty inherent in the problem). Furthermore, there is a high likelihood that these imperfections are removed at high agreement levels (since they are variations of individual annotators). The upper bound, therefore is stable with respect to these and the true, unknown, detector performance is contained somewhere within these bounds.\nFinally, to be able to use such an approach, and to understand annotator variance within standard evaluation datasets, it should be made possible to determine which annotations each annotator produced, and to ensure a sufficient coverage of the dataset by the same annotators."}, {"heading": "VI. CONCLUSIONS", "text": "This paper set out to quantify the effects of obtaining ground truth data from multiple annotators in a computer vision setting. It has also taken some steps towards identifying which properties of the image are related to agreement amongst the annotators. Statistical analyses of the GTs in each case study lead to the quantification of the differences between the annotations. A number of gold-standard GT estimation methods were evaluated, including removing outlier annotations, and it was found that the STAPLE and LSML algorithms find a balance between all annotations when their variance is low. Ground truths formed by taking segments that any of the annotators marked and thresholding at 50 % and 75 % agreement, tend to form lower and upper bounds on detector performance. Performance measured when using the GT derived by removing outlier annotations and then taking the consensus vote approaches that of STAPLE and LSML in all but one of the case studies. It does, however, appear to be more stable when the annotations have high variance.\nIt can be concluded that the rank of a detector is highly dependent upon which GT estimation algorithm is used. In some cases the GTs calculated by voting resulted in a detector rank that is in discordance with each of the individual annotations. The P\u0304-R curves obtained using the voted GTs also appear to be outliers when compared to those of the remaining GTs, suggesting that these commonly employed GT estimation methods overemphasise detector performance when compared to individual annotator opinion. Furthermore, under some conditions a detector whose output is poorly correlated with annotator agreement can be placed above those that have vastly better correlated outputs.\nTherefore in addition to evaluating an algorithm over a data set that contains multiple images, it is concluded that an algorithm should also be evaluated using multiple ground truths. The variance of performance that is observed using these different ground truths can then be used to quantify the confidence in the differences between detectors. In situations in which there are few annotations available, or when the interannotator variance is high, further study into the nature of the problem should be conducted as these conditions imply that it is not possible to state that one algorithm outperforms another with any confidence. Therefore, whenever possible the intrinsic uncertainties of annotator judgements should be assessed before the evaluation of detection algorithms, since measures of absolute performance and relative ranking of detectors may vary considerably according to the GT employed.\nThe possibility of estimating a detector\u2019s true performance through the variability of annotator opinion would be an interesting avenue to follow. Assuming that performances derived using different GTs are observations of a hidden variable, it may be possible to estimate its true value\u2014the gold standard performance. Much research is dedicated to inferring the gold-standard GT, however, this is a complex problem in which many assumptions need to be made, and the proposed approach may avoid some of these.\nAn additional question that is raised by this study is: which metric should be used to evaluate an estimated gold standard? Generally speaking the gold standard is unknown and therefore comparison is impossible. Restricting evaluation to individual annotations assumes high specificity and sensitivity. Removing annotations, however, assumes inability compared to the consensus, but do those removed represent true insight into the problem? One thing is clear, detector performance should not be used to evaluate an estimated gold-standard ground truth."}, {"heading": "ACKNOWLEDGEMENT", "text": "The participating annotators from LIVE, IPGS, and ICube (University of Strasbourg), and ITC (University of Twente) are gratefully acknowledged."}], "references": [{"title": "Validation of image segmentation by estimating rater bias and variance", "author": ["S. Warfield", "K. Zou", "W. Wells"], "venue": "Phil. Trans. R. Soc. A, vol. 366, no. 1874, pp. 2361\u20132375, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1874}, {"title": "TESD: A novel ground truth estimation method", "author": ["A. Biancardi", "A. Reeves"], "venue": "Medical Imaging 2009: Computer-Aided Diagnosis, vol. 7260, February 2009, pp. 72 603V\u201372 603V\u20138.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Automated analysis of radar images of Venus: Handling lack of ground truth", "author": ["M.C. Burl", "U.M. Fayyad", "P. Perona", "P. Smyth"], "venue": "ICIP, vol. 3, 1994, pp. 236\u2013240.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1994}, {"title": "Fusion of multiple expert annotations and overall score selection for medical image diagnosis", "author": ["T. Kauppi", "J.-K. Kamarainen", "L. Lensu", "V. Kalesnykiene", "I. Sorri", "H. K\u00e4lvi\u00e4inen", "H. Uusitalo", "J. Pietil\u00e4"], "venue": "Image Analysis, ser. LNCS. Springer, 2009, vol. 5575, pp. 760\u2013769.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Label fusion in atlas-based segmentation using a selective and iterative method for performance level estimation (SIMPLE)", "author": ["T. Langerak", "U. van der Heidean", "A. Kotte", "M. Viergever", "M. van Vulpen", "J. Pluim"], "venue": "IEEE Trans. Med. Imag., vol. 29, no. 12, pp. 2000\u20132008, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Estimating the ground truth from multiple individual segmentations incorporating prior pattern analysis with application to skin lesion segmentation", "author": ["X. Li", "B. Aldridge", "R. Fisher", "J. Rees"], "venue": "ISIB, 2011, pp. 1438\u20131441.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Inferring ground truth from subjective labelling of Venus images", "author": ["P. Smyth", "U. Fayyad", "M. Burl", "P. Perona", "P. Baldi"], "venue": "NIPS, 1994, pp. 1085\u20131092.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1994}, {"title": "Simultaneous truth and performance level estimation (STAPLE): An algorithm for the validation of image segmentation", "author": ["S. Warfield", "K. Zou", "W. Wells"], "venue": "IEEE Trans. Med. Imag., vol. 23, no. 7, pp. 903\u2013921, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Contour detection and hierarchical image segmentation", "author": ["P. Arbela\u00e9z", "M. Maire", "C. Fowlkes", "J. Malik"], "venue": "IEEE Trans. PAMI, vol. 33, no. 5, pp. 898\u2013916, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "The lung image database consortium (LIDC) and image database resource initiative (IDRI) : A completed reference database of lung nodules on CT scans", "author": ["A S."], "venue": "Medical Physics, vol. 38, pp. 915\u2013931, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Locating blood vessels in retinal images by piece-wise threhsold probing of a matched filter response", "author": ["A. Hoover", "V. Kouznetsova", "M. Goldbaum"], "venue": "IEEE Trans. Med. Imag., vol. 19, no. 3, pp. 203\u2013210, 2000.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "A detailed investigation into low-level feature detection in spectrogram images", "author": ["T. Lampert", "S. O\u2019Keefe"], "venue": "Pattern Recognition, vol. 44, no. 9, pp. 2076\u20132092, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "A generative model for image segmentation based on label fusion", "author": ["M. Sabuncu", "B. Yeo", "K.V. Leemput", "B. Fischl", "P. Golland"], "venue": "IEEE Trans. Med. Imag., vol. 29, no. 10, pp. 1714\u20131729, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Combination strategies in multi-atlas image segmentation: application to brain MR data", "author": ["X. Artaechevarria", "A. Munoz-Barrutia", "C.O. de Solorzano"], "venue": "IEEE Trans. Med. Imag., vol. 28, no. 8, pp. 1266\u20131277, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-atlas-based segmentation with local decision fusion\u2013application to cardiac and aortic segmentation in CT scans", "author": ["I. Isgum", "M. Staring", "A. Rutten", "M. Prokop", "M. Viergever", "B. van Ginneken"], "venue": "IEEE Trans. Med. Imag., vol. 28, no. 7, pp. 1000\u20131010, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-atlas segmentation with joint label fusion", "author": ["H. Wang", "J. Suh", "S. Das", "J. Pluta", "C. Craige", "P. Yushkevich"], "venue": "IEEE Trans. PAMI, vol. 35, no. 3, pp. 611\u2013623, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Patch-based segmentation using expert priors: Application to hippocampus and ventricle segmentation", "author": ["P. Coup\u00e9", "J. Manj\u00f3n", "V. Fonov", "J. Pruessner", "M. Robles", "D. Collins"], "venue": "NeuroImage, vol. 54, no. 2, pp. 940\u2013954, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Statistical fusion of continuous labels: identification of cardiac landmarks", "author": ["F. Xing", "S. Soleimanifard", "J. Prince", "B. Landman"], "venue": "Proc. SPIE Medical Imaging 2011: Image Processing, vol. 7962, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Incorporating priors on expert performance parameters for segmentation valida-  16  IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 25, NO. 6, MARCH 2016 tion and label fusion: a maximum a posteriori STAPLE", "author": ["O. Commowick", "S. Warfield"], "venue": "Proc. of the 13th Int. Conf. on Med. Image Comput. Comput. Assist. Interv., 2010, pp. 25\u201332.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Characterizing spatially varying performance to improve multi-atlas multi-label segmentation", "author": ["A. Asman", "B. Landman"], "venue": "Proc. of the 22nd int. conf. on Information processing in medical imaging, 2011, pp. 85\u201396.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Formulating spatially varying performance in the statistical fusion framework", "author": ["\u2014\u2014"], "venue": "IEEE Trans. Med. Imag., vol. 31, pp. 1326\u20131336, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Estimating a reference standard segmentation with spatially varying performance parameters: Local MAP STAPLE", "author": ["O. Commowick", "A. Akhondi-Asl", "S. Warfield"], "venue": "IEEE Trans. MI, vol. 31, no. 8, pp. 1593\u20131606, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust statistical label fusion through COnsensus level, Labeler Accuracy, and Truth Estimation (COLLATE)", "author": ["A. Asman", "B. Landman"], "venue": "IEEE Trans. Med. Imag., vol. 30, pp. 1179\u20131794, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Simultaneous truth and performance level estimation with incomplete, over-complete, and ancillary data", "author": ["B. Landman", "J. Bogovic", "J. Prince"], "venue": "Proc. SPIE Medical Imaging 2010: Image Processing, vol. 7623, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust statistical fusion of image labels", "author": ["B. Landman", "A. Asman", "A. Scoggins", "J. Bogovic", "F. Xing", "J. Prince"], "venue": "IEEE Trans. MI, vol. 31, no. 2, pp. 512\u2013522, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining multiple image segmentations by maximizing expert agreement", "author": ["J.-K. Kamarainen", "L. Lensu", "T. Kauppi"], "venue": "Proc. of the 3rd Int. Workshop on Machine Learning in Medical Imaging, 2012, pp. 193\u2013200.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Ground truth estimation by maximizing topological agreements in electron microscopy data", "author": ["H.-F. Yang", "Y. Choe"], "venue": "Proc. of the 7th Int. Conf. on Advances in visual computing, 2011, pp. 371\u2013380.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Non-local STAPLE: An intensity-driven multi-atlas rater model", "author": ["A. Asman", "B. Landman"], "venue": "Proc. of the 15th Int. Conf. on Med. Image Computing and Computer- Assisted Intervention, vol. 3, 2012, pp. 426\u2013434.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Non-local statistical label fusion for multi-atlas segmentation", "author": ["\u2014\u2014"], "venue": "Med. Image Anal., vol. 17, no. 2, pp. 194\u2013 208, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "iSTAPLE: improved label fusion for segmentation by combining STAPLE with image intensity", "author": ["X. Liu", "A. Montillo", "E. Tan", "J. Schenck"], "venue": "Proc. SPIE Medical Imaging 2013: Image Processing, vol. 8669, 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Simultaneous segmentation and statistical label fusion", "author": ["A. Asman", "B. Landman"], "venue": "Proc. SPIE Medical Imaging 2012: Image Processing, vol. 8314, 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["D. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "ICCV, 2001, pp. 416\u2013423.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2001}, {"title": "Bounds on the mean classification error rate of multiple experts", "author": ["P. Smyth"], "venue": "Pattern Recogn. Lett., vol. 17, no. 12, pp. 1253\u20131257, 1996.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1996}, {"title": "Learning from imbalanced data", "author": ["H. He", "E. Garcia"], "venue": "IEEE Trans. KDE, vol. 21, no. 9, pp. 1263\u20131284, 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Effect of reader ex-  perience on variability, evaluation time and accuracy of coronary plaque detection with computed tomography coronary angiography", "author": ["S. Saur", "H. Alkadhi"], "venue": "Eur. Radiol., vol. 20, no. 7, pp. 1599\u20131606, 2010.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "The relationship between precision-recall and ROC curves", "author": ["J. Davis", "M. Goadrich"], "venue": "ICML, 2006, pp. 233\u2013240.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "The geometry of ROC space: understanding machine learning metrics through ROC isometrics", "author": ["P. Flach"], "venue": "ICML, 2003, pp. 194\u2013201.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2003}, {"title": "The bane of skew: Uncertain ranks and unrepresentative precision", "author": ["T. Lampert", "P. Gan\u00e7arski"], "venue": "Machine Learning, vol. 97, no. 1\u20132, pp. 5\u201332, 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "UAV-based remote sensing of the Super- Sauze landslide: Evaluation and results", "author": ["U. Niethammer", "M. James", "S. Rothmund", "J. Travelletti", "M. Joswig"], "venue": "Eng. Geol., vol. 128, no. 1, pp. 2\u201311, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Image-based mapping of surface fissures for the investigation of landslide dynamics", "author": ["A. Stumpf", "J.-P. Malet", "N. Kerle", "U. Niethammer", "S. Rothmund"], "venue": "Geomorphology, vol. 186, pp. 12\u201327, 2013.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Active learning in the spatial domain for remote sensing image classification", "author": ["A. Stumpf", "N. Lachiche", "N. Malet", "J.-P. Malet", "N. Kerle", "A. Puissant"], "venue": "IEEE Trans. Geosci. Remote Sens., vol. PP, no. 99, pp. 1\u201316, 2013.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Discriminatively trained sparse code gradients for contour detection", "author": ["X. Ren", "L. Bo"], "venue": "NIPS, 2012, pp. 593\u2013 601.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Using contours to detect and localize junctions in natural images", "author": ["M. Maire", "P. Arbelaez", "C. Fowlkes", "J. Malik"], "venue": "IEEE Conf. CVPR, 2008, pp. 1\u20138.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-scale improves boundary detection in natural images", "author": ["X. Ren"], "venue": "ECCV, 2008, pp. 533\u2013545.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning to detect natural image boundaries using local brightness, color, and texture cues", "author": ["D. Martin", "C. Fowlkes", "J. Malik"], "venue": "IEEE Trans. PAMI, vol. 26, no. 5, pp. 530\u2013549, 2004.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2004}, {"title": "Retinal vessel segmentation using the 2- D Gabor wavelet and supervised classification", "author": ["J. Soares", "J. Leandro", "R. Cesar-Jr.", "H. Jelinek", "M. Cree"], "venue": "IEEE Trans. Med. Imag., vol. 25, no. 9, pp. 1214\u20131222, 2006.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2006}, {"title": "Multiscale line detection for landslide fissure mapping", "author": ["A. Stumpf", "T. Lampert", "J.-P. Malet", "N. Kerle"], "venue": "IGARSS. IEEE, 2012, pp. 5450\u20135453.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast centre-surround contrast modification", "author": ["V. Vonikakis", "I. Andreadis", "A. Gasteratos"], "venue": "IET Image Process., vol. 2, no. 1, pp. 19\u201334, 2008.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2008}, {"title": "Classification and regression by randomForest", "author": ["A. Liaw", "M. Wiener"], "venue": "Rnews, vol. 2, pp. 18\u201322, 2002.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2002}, {"title": "Landslide inventories and their statistical properties", "author": ["B. Malamud", "D. Turcotte", "F. Guzzetti", "P. Reichenbach"], "venue": "Earth Surf. Process. Landf., vol. 29, pp. 687\u2013711, 2004.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2004}, {"title": "Experts in Uncertainty: Opinion and Subjective Probability in Science", "author": ["R. Cooke"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1991}, {"title": "A route to more tractable expert advice", "author": ["W. Aspinall"], "venue": "Nature, vol. 463, pp. 294\u2013295, 2010.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Nevertheless, annotators rarely agree completely when giving their opinion and this disagreement can be characterised as bias\u2014the tendency of an annotator to prefer one decision over another\u2014and variance\u2014the natural variation that one annotator will have to the next (or themselves at a later date) [1].", "startOffset": 299, "endOffset": 302}, {"referenceID": 0, "context": "Several previous studies have developed statistical methods for estimating the gold-standard GT from a number of annotations [1, 2, 3, 4, 5, 6, 7, 8].", "startOffset": 125, "endOffset": 149}, {"referenceID": 1, "context": "Several previous studies have developed statistical methods for estimating the gold-standard GT from a number of annotations [1, 2, 3, 4, 5, 6, 7, 8].", "startOffset": 125, "endOffset": 149}, {"referenceID": 2, "context": "Several previous studies have developed statistical methods for estimating the gold-standard GT from a number of annotations [1, 2, 3, 4, 5, 6, 7, 8].", "startOffset": 125, "endOffset": 149}, {"referenceID": 3, "context": "Several previous studies have developed statistical methods for estimating the gold-standard GT from a number of annotations [1, 2, 3, 4, 5, 6, 7, 8].", "startOffset": 125, "endOffset": 149}, {"referenceID": 4, "context": "Several previous studies have developed statistical methods for estimating the gold-standard GT from a number of annotations [1, 2, 3, 4, 5, 6, 7, 8].", "startOffset": 125, "endOffset": 149}, {"referenceID": 5, "context": "Several previous studies have developed statistical methods for estimating the gold-standard GT from a number of annotations [1, 2, 3, 4, 5, 6, 7, 8].", "startOffset": 125, "endOffset": 149}, {"referenceID": 6, "context": "Several previous studies have developed statistical methods for estimating the gold-standard GT from a number of annotations [1, 2, 3, 4, 5, 6, 7, 8].", "startOffset": 125, "endOffset": 149}, {"referenceID": 7, "context": "Several previous studies have developed statistical methods for estimating the gold-standard GT from a number of annotations [1, 2, 3, 4, 5, 6, 7, 8].", "startOffset": 125, "endOffset": 149}, {"referenceID": 8, "context": "Although some public datasets offer segmentations obtained from different annotators [9, 10, 11] these methods are rarely employed in real-world algorithm evaluation, where experimentation is typically limited to one annotation.", "startOffset": 85, "endOffset": 96}, {"referenceID": 9, "context": "Although some public datasets offer segmentations obtained from different annotators [9, 10, 11] these methods are rarely employed in real-world algorithm evaluation, where experimentation is typically limited to one annotation.", "startOffset": 85, "endOffset": 96}, {"referenceID": 10, "context": "Although some public datasets offer segmentations obtained from different annotators [9, 10, 11] these methods are rarely employed in real-world algorithm evaluation, where experimentation is typically limited to one annotation.", "startOffset": 85, "endOffset": 96}, {"referenceID": 11, "context": "tightly controlled conditions or are synthetically generated [12] because the gold-standard GT is trivial to calculate.", "startOffset": 61, "endOffset": 65}, {"referenceID": 6, "context": "[7] analyse the uncertainty of an annotator\u2019s judgement in marking volcanoes in synthetic aperture radar images of Venus.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] take GTs as the intersection (consensus), fixed size neighbourhoods of the points marked by each annotator, and a combination of the two.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Numerous weighted extensions to the voting framework have been proposed based upon global [13], local [14, 15, 13], semi-local [13, 16], and non-local [17] information.", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "Numerous weighted extensions to the voting framework have been proposed based upon global [13], local [14, 15, 13], semi-local [13, 16], and non-local [17] information.", "startOffset": 102, "endOffset": 114}, {"referenceID": 14, "context": "Numerous weighted extensions to the voting framework have been proposed based upon global [13], local [14, 15, 13], semi-local [13, 16], and non-local [17] information.", "startOffset": 102, "endOffset": 114}, {"referenceID": 12, "context": "Numerous weighted extensions to the voting framework have been proposed based upon global [13], local [14, 15, 13], semi-local [13, 16], and non-local [17] information.", "startOffset": 102, "endOffset": 114}, {"referenceID": 12, "context": "Numerous weighted extensions to the voting framework have been proposed based upon global [13], local [14, 15, 13], semi-local [13, 16], and non-local [17] information.", "startOffset": 127, "endOffset": 135}, {"referenceID": 15, "context": "Numerous weighted extensions to the voting framework have been proposed based upon global [13], local [14, 15, 13], semi-local [13, 16], and non-local [17] information.", "startOffset": 127, "endOffset": 135}, {"referenceID": 16, "context": "Numerous weighted extensions to the voting framework have been proposed based upon global [13], local [14, 15, 13], semi-local [13, 16], and non-local [17] information.", "startOffset": 151, "endOffset": 155}, {"referenceID": 7, "context": "[8], named simultaneous truth and performance level estimation (STAPLE) in which annotator performance (measured as sensitivity and specificity) and the gold-standard GT are simultaneously estimated within a maximum-likelihood setting, the optimisation being solved using expectationmaximisation (a variant for handling continuous labels has been proposed by Warfield et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] and Xing et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "The same authors also propose an approach in which the bias and variance of each annotator is estimated instead of their sensitivity and specificity [1] and another variant that accounts for instabilities in the annotator performance measures [19].", "startOffset": 149, "endOffset": 152}, {"referenceID": 18, "context": "The same authors also propose an approach in which the bias and variance of each annotator is estimated instead of their sensitivity and specificity [1] and another variant that accounts for instabilities in the annotator performance measures [19].", "startOffset": 243, "endOffset": 247}, {"referenceID": 19, "context": "Much subsequent work has concentrated on the STAPLE algorithm: removing its assumption that annotator performances are constant throughout the data [20, 21, 22], and COLLATE [23], which accounts for spatial variability in task difficulty.", "startOffset": 148, "endOffset": 160}, {"referenceID": 20, "context": "Much subsequent work has concentrated on the STAPLE algorithm: removing its assumption that annotator performances are constant throughout the data [20, 21, 22], and COLLATE [23], which accounts for spatial variability in task difficulty.", "startOffset": 148, "endOffset": 160}, {"referenceID": 21, "context": "Much subsequent work has concentrated on the STAPLE algorithm: removing its assumption that annotator performances are constant throughout the data [20, 21, 22], and COLLATE [23], which accounts for spatial variability in task difficulty.", "startOffset": 148, "endOffset": 160}, {"referenceID": 22, "context": "Much subsequent work has concentrated on the STAPLE algorithm: removing its assumption that annotator performances are constant throughout the data [20, 21, 22], and COLLATE [23], which accounts for spatial variability in task difficulty.", "startOffset": 174, "endOffset": 178}, {"referenceID": 23, "context": "[24] point out that in research and clinical environments it is not often possible to obtain multiple annotations for the whole dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Extensions to handle multiple partial, but overlapping, annotations have therefore been proposed [19, 24, 25].", "startOffset": 97, "endOffset": 109}, {"referenceID": 23, "context": "Extensions to handle multiple partial, but overlapping, annotations have therefore been proposed [19, 24, 25].", "startOffset": 97, "endOffset": 109}, {"referenceID": 24, "context": "Extensions to handle multiple partial, but overlapping, annotations have therefore been proposed [19, 24, 25].", "startOffset": 97, "endOffset": 109}, {"referenceID": 25, "context": "[26] propose a simpler alternative to STAPLE by maximising the mutual agreement of annotator ratings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] argue, however, that STAPLE fails when annotator uncertainty varies considerably due to the fact that the STAPLE algorithm combines all of the annotators\u2019 labellings.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] propose a probabilistic approach that uses level sets in which the likelihood function is inspired by the STAPLE algorithm (LSML).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Biancardi and Reeves [2] state that the STAPLE algorithm (even with the Markov random field extension) and simple voting strategies assume that the pixels are spatially independent.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "It is preceded by a distance transformation that attributes positive values to the inside of the GT segmentation\u2019s boundary, which increase towards its centre, and decreases negatively outside the segment border; thus the truth estimate from self distances (TESD) algorithm is introduced [2].", "startOffset": 288, "endOffset": 291}, {"referenceID": 26, "context": "Yang and Choe [27] follow this path and propose a method that incorporates the warping error to preserve topological disagreements between the estimated gold-standard GT and the annotations.", "startOffset": 14, "endOffset": 18}, {"referenceID": 27, "context": "A number of extensions to the STAPLE algorithm have also been proposed [28, 29, 30] which incorporate the image\u2019s intensity values, as well as the performance of multiple experts, to transfer the labelling of one image onto that of another.", "startOffset": 71, "endOffset": 83}, {"referenceID": 28, "context": "A number of extensions to the STAPLE algorithm have also been proposed [28, 29, 30] which incorporate the image\u2019s intensity values, as well as the performance of multiple experts, to transfer the labelling of one image onto that of another.", "startOffset": 71, "endOffset": 83}, {"referenceID": 29, "context": "A number of extensions to the STAPLE algorithm have also been proposed [28, 29, 30] which incorporate the image\u2019s intensity values, as well as the performance of multiple experts, to transfer the labelling of one image onto that of another.", "startOffset": 71, "endOffset": 83}, {"referenceID": 30, "context": "Moreover, Asman and Landman [31] propose to combine a locally weighted voting strategy with information derived form the image\u2019s intensity.", "startOffset": 28, "endOffset": 32}, {"referenceID": 8, "context": "clude the level of annotator agreement within their evaluations [9], which provides a valuable reference when interpreting the results.", "startOffset": 64, "endOffset": 67}, {"referenceID": 31, "context": "[32] present a statistical analysis of the variation observed within the annotations [32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] present a statistical analysis of the variation observed within the annotations [32].", "startOffset": 85, "endOffset": 89}, {"referenceID": 32, "context": "Smyth [33] presents a method for calculating the lower bound on error in a set of annotations relative to the (unknown) gold-standard ground-truth.", "startOffset": 6, "endOffset": 10}, {"referenceID": 33, "context": "The F1-score [34], calculated between participants i and j, is defined as", "startOffset": 13, "endOffset": 17}, {"referenceID": 33, "context": "Note that the F1-score is robust in the presence of class-imbalance since it does not take into account true-negative classifications [34].", "startOffset": 134, "endOffset": 138}, {"referenceID": 34, "context": "[35], and to highlight any individual differences between the annotators, each is compared to the group\u2019s consensus (image pixels that 50 % or more of the annotators marked as containing a relevant feature), calculated using Eq.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "It is common to measure detector performance through ROC curve analysis, however, recent literature points out that this may overestimate performance when applied to highly skewed datasets (those in which the number of positive, Np, and negative, Nn, examples are not balanced) and therefore precision-recall (P-R) curves are preferable [36, 34].", "startOffset": 337, "endOffset": 345}, {"referenceID": 33, "context": "It is common to measure detector performance through ROC curve analysis, however, recent literature points out that this may overestimate performance when applied to highly skewed datasets (those in which the number of positive, Np, and negative, Nn, examples are not balanced) and therefore precision-recall (P-R) curves are preferable [36, 34].", "startOffset": 337, "endOffset": 345}, {"referenceID": 36, "context": "To overcome this Flach [37] proposes to analytically vary the skew ratio in the precision measure and Lampert and Gan\u00e7arski [38] to integrate this added dimension, thus forming", "startOffset": 23, "endOffset": 27}, {"referenceID": 37, "context": "To overcome this Flach [37] proposes to analytically vary the skew ratio in the precision measure and Lampert and Gan\u00e7arski [38] to integrate this added dimension, thus forming", "startOffset": 124, "endOffset": 128}, {"referenceID": 37, "context": "Interpolation between P\u0304R points [38] enables accurate area under the curve (AUCP\u0304R) measurements to be taken.", "startOffset": 33, "endOffset": 37}, {"referenceID": 7, "context": "Also included are gold-standard GT estimations calculated using STAPLE [8] (without assigning consensus votes [22]), SIMPLE [5], and LSML [6] (using 0.", "startOffset": 71, "endOffset": 74}, {"referenceID": 21, "context": "Also included are gold-standard GT estimations calculated using STAPLE [8] (without assigning consensus votes [22]), SIMPLE [5], and LSML [6] (using 0.", "startOffset": 110, "endOffset": 114}, {"referenceID": 4, "context": "Also included are gold-standard GT estimations calculated using STAPLE [8] (without assigning consensus votes [22]), SIMPLE [5], and LSML [6] (using 0.", "startOffset": 124, "endOffset": 127}, {"referenceID": 5, "context": "Also included are gold-standard GT estimations calculated using STAPLE [8] (without assigning consensus votes [22]), SIMPLE [5], and LSML [6] (using 0.", "startOffset": 138, "endOffset": 141}, {"referenceID": 38, "context": "Further information regarding this dataset is present in the literature [39, 40].", "startOffset": 72, "endOffset": 80}, {"referenceID": 39, "context": "Further information regarding this dataset is present in the literature [39, 40].", "startOffset": 72, "endOffset": 80}, {"referenceID": 40, "context": "Detailed information regarding this dataset exists in the literature [41].", "startOffset": 69, "endOffset": 73}, {"referenceID": 32, "context": "that is recommended [33] and considerably lower than the error bound of 20 % found in the volcano labelling experiment presented by the author [33], in which the signal-to-noise ratio of the features is much lower than in the presented case studies.", "startOffset": 20, "endOffset": 24}, {"referenceID": 32, "context": "that is recommended [33] and considerably lower than the error bound of 20 % found in the volcano labelling experiment presented by the author [33], in which the signal-to-noise ratio of the features is much lower than in the presented case studies.", "startOffset": 143, "endOffset": 147}, {"referenceID": 41, "context": "These were: REN [42], gPbucm (UCM) [9], Global Probability of Boundary (GP) [43], and XREN [44].", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "These were: REN [42], gPbucm (UCM) [9], Global Probability of Boundary (GP) [43], and XREN [44].", "startOffset": 35, "endOffset": 38}, {"referenceID": 42, "context": "These were: REN [42], gPbucm (UCM) [9], Global Probability of Boundary (GP) [43], and XREN [44].", "startOffset": 76, "endOffset": 80}, {"referenceID": 43, "context": "These were: REN [42], gPbucm (UCM) [9], Global Probability of Boundary (GP) [43], and XREN [44].", "startOffset": 91, "endOffset": 95}, {"referenceID": 37, "context": "0428, which were found to be \u03c0\u2032 1 = \u03bc \u2212 3\u03c3 and \u03c0\u2032 2 = \u03bc + 3\u03c3 where \u03bc is the mean skew found within the Berkeley dataset and \u03c3 its standard deviation [38].", "startOffset": 149, "endOffset": 153}, {"referenceID": 44, "context": "[45], when evaluating segmentation algorithms it is common to loosen the definition of true-positive detections to account for deviations in detected boundary location.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "Fissure Current state-of-the-art linear feature detectors were selected from the literature: a linear classifier trained using 2D Gabor wavelet (elongation = 4, scales a = 2, 3, 4, 5, and frequency k0 = 3) and inverted grey-scale features (2D GWLC) [46]; Gaussian filter matching, where \u03c3 = 1 [47] (Gauss); Top-Hat transform (4 pixel radius circular structuring element); and the Centre-Surround (C-S) transform (using a 3 \u00d7 3 pixel neighbourhood) [48].", "startOffset": 249, "endOffset": 253}, {"referenceID": 46, "context": "Fissure Current state-of-the-art linear feature detectors were selected from the literature: a linear classifier trained using 2D Gabor wavelet (elongation = 4, scales a = 2, 3, 4, 5, and frequency k0 = 3) and inverted grey-scale features (2D GWLC) [46]; Gaussian filter matching, where \u03c3 = 1 [47] (Gauss); Top-Hat transform (4 pixel radius circular structuring element); and the Centre-Surround (C-S) transform (using a 3 \u00d7 3 pixel neighbourhood) [48].", "startOffset": 293, "endOffset": 297}, {"referenceID": 47, "context": "Fissure Current state-of-the-art linear feature detectors were selected from the literature: a linear classifier trained using 2D Gabor wavelet (elongation = 4, scales a = 2, 3, 4, 5, and frequency k0 = 3) and inverted grey-scale features (2D GWLC) [46]; Gaussian filter matching, where \u03c3 = 1 [47] (Gauss); Top-Hat transform (4 pixel radius circular structuring element); and the Centre-Surround (C-S) transform (using a 3 \u00d7 3 pixel neighbourhood) [48].", "startOffset": 448, "endOffset": 452}, {"referenceID": 48, "context": "Landslide Four popular classification algorithms were applied (due to their proven strength in real-world applications): random forest (RF) [49], support-vector machine (SVM), knearest neighbours (KNN), and a neural network (ANN).", "startOffset": 140, "endOffset": 144}, {"referenceID": 40, "context": "The resulting dataset is available on-line3 and a detailed description of the feature extraction methods are given in the literature [41].", "startOffset": 133, "endOffset": 137}, {"referenceID": 49, "context": "10 to reflect typical ratios of affected to unaffected areas after large-scale landslide triggering events [50].", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "Blood Vessel The four detectors selected for this case study were the Matched-Filter Response (MSF) [11], Linear Classifier (LMSE), k-nearest neighbours (KNN), and Gaussian Mixture Model (GMM).", "startOffset": 100, "endOffset": 104}, {"referenceID": 45, "context": "The LMSE, KNN and GMM classifiers were implemented using the MLVessel software package [46], the features were taken to be the inverted green channel, and the responses of Gabor wavelets (elongation = 4, scales a = 2, 3, 4, 5, and frequency k0 = 3) applied to the inverted green channel.", "startOffset": 87, "endOffset": 91}, {"referenceID": 37, "context": "235, which were found to be \u03c0\u2032 1 = \u03bc \u2212 3\u03c3 and \u03c0\u2032 2 = \u03bc + 3\u03c3 where \u03bc was the mean skew found within a number of retinal image datasets and \u03c3 its standard deviation [38].", "startOffset": 163, "endOffset": 167}, {"referenceID": 37, "context": "The detector ranks (measured as AUCP\u0304R [38]) when evaluated using different GTs were determined, and in three of the case studies (Segmentation, Fissure, and Blood Vessel) three", "startOffset": 39, "endOffset": 43}, {"referenceID": 3, "context": "[4] conclude that the intersection method (consensus) is preferential as it results in the highest performance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "[32] who found a large amount of agreement between the segmented regions, but not the boundaries themselves.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Nevertheless, as noted above, this is not the case when annotation variance increases or few annotations are available (as in the Blood Vessel case study) and this seems to be in line with other studies [5].", "startOffset": 203, "endOffset": 206}, {"referenceID": 4, "context": "The SIMPLE algorithm was proposed to overcome these limitations when annotator uncertainty varies considerably [5] and indeed, in these situations it does seem to offer an improvement (see, for example, the Segmentation and Blood Vessel case studies).", "startOffset": 111, "endOffset": 114}, {"referenceID": 32, "context": "As Smyth [33] states \u201cwithout knowing GT one can not make any statements about the errors of an individual labeller\u201d.", "startOffset": 9, "endOffset": 13}, {"referenceID": 50, "context": "The Cooke method [51] prescribes that the annotators are asked to estimate a interval of probable values along with their concrete answer, and furthermore they are also asked to answer multiple questions on topics from their field that have known answers.", "startOffset": 17, "endOffset": 21}, {"referenceID": 51, "context": "This information is used to weight the annotator\u2019s contribution in relation to their accuracy in this estimation and thus, has been shown to be more accurate than consensus voting [52].", "startOffset": 180, "endOffset": 184}], "year": 2016, "abstractText": "Although agreement between annotators who mark feature locations within images has been studied in the past from a statistical viewpoint, little work has attempted to quantify the extent to which this phenomenon affects the evaluation of foreground-background segmentation algorithms. Many researchers utilise ground truth in experimentation and more often than not this ground truth is derived from one annotator\u2019s opinion. How does the difference in opinion affect an algorithm\u2019s evaluation? A methodology is applied to four image processing problems to quantify the inter-annotator variance and to offer insight into the mechanisms behind agreement and the use of ground truth. It is found that when detecting linear structures annotator agreement is very low. The agreement in a structure\u2019s position can be partially explained through basic image properties. Automatic segmentation algorithms are compared to annotator agreement and it is found that there is a clear relation between the two. Several ground truth estimation methods are used to infer a number of algorithm performances. It is found that: the rank of a detector is highly dependent upon the method used to form the ground truth; and that although STAPLE and LSML appear to represent the mean of the performance measured using individual annotations, when there are few annotations, or there is a large variance in them, these estimates tend to degrade. Furthermore, one of the most commonly adopted combination methods\u2014consensus voting\u2014 accentuates more obvious features, resulting in an overestimation of performance. It is concluded that in some datasets it is not possible to confidently infer an algorithm ranking when evaluating upon one ground truth.", "creator": "LaTeX with hyperref package"}}}