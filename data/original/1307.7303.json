{"id": "1307.7303", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jul-2013", "title": "Learning to Understand by Evolving Theories", "abstract": "In this paper, we describe an approach that enables an autonomous system to infer the semantics of a command (i.e. a symbol sequence representing an action) in terms of the relations between changes in the observations and the action instances. We present a method of how to induce a theory (i.e. a semantic description) of the meaning of a command in terms of a minimal set of background knowledge. The only thing we have is a sequence of observations from which we extract what kinds of effects were caused by performing the command. This way, we yield a description of the semantics of the action and, hence, a definition.", "histories": [["v1", "Sat, 27 Jul 2013 20:33:34 GMT  (172kb,D)", "http://arxiv.org/abs/1307.7303v1", "KRR Workshop at ICLP 2013"]], "COMMENTS": "KRR Workshop at ICLP 2013", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["martin e mueller", "madhura d thosar"], "accepted": false, "id": "1307.7303"}, "pdf": {"name": "1307.7303.pdf", "metadata": {"source": "CRF", "title": "Learning to Understand by Evolving Theories", "authors": ["Martin E. M\u00fcller", "Madhura D. Thosar"], "emails": ["m.e.mueller@acm.org,", "madhura.thosar@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "By \u201csemantics\u201d one usually refers to the meaning of something. But since there is no such thing as understanding in a computer, there is no semantics either. Yet, a robot acts (causing events) and senses the environment (identifying things). But reasoning about actions requires language and, hence, semantics. There are several prime inspirations for our work: First, John Locke\u2019s concept of learning from scratch, [1], and Bertrand Russell\u2019s \u201cTheory of knowledge\u201d, [2]. [3] also summarises several theories of cognition related to learning the semantics of an action. With a model being defined as a formal description of the observable effects of an action we add to it the fundamental problem of identifying a change in the environment to be an effect of an action. Both the environment and the innate knowledge is represented relationally as logic programs so we can describe the formation of more complex models as a learning process similar to inductive logic programming, [6]."}, {"heading": "2 Learning to Understand Actions", "text": "Understanding and learning require knowledge. In this article, we presuppose an agent to have a minimal set of \u201cknowledge\u201d as a given set of terminological formulae describing facts, properties and rules that hold within the agent\u2019s model of the world."}, {"heading": "2.1 Actions as state changes", "text": "Performing an action usually results in a change in the world. Since we do not know the world itself but only a model of it, we need to live with interpretations of partial data collected by sensors. Goal directed acting (planned, rational\nar X\niv :1\n30 7.\n73 03\nv1 [\ncs .L\nG ]\n2 7\nJu l 2\n01 3\nbehaviour) therefore requires to anticipate an action\u2019s results and we do so by assuming that if we do something, the real effects takes us into a situation where our sensors deliver results that correspond to what we wanted to achieve:\nwi Action //\n\u03b1i\nwi+1 \u03b1i+1\n\u03b4i Operator // \u03b4i+1\n(1)\nAll \u03b4i are state descriptions, which consist of sets of logic formulae (in our case, Horn clauses) over a signature \u03a3. The \u03b1i represent the interpretation of sensory data and, thus, the assignments of the variables in \u03b4i. An operator op is a function that changes \u03b4i and which anticipates a change in \u03b1. Example. Let \u03b4i |=\u03b1i Position = \u3008X,Y \u3009. Assume there is a function mv that takes as input the variable Position and two further parameters Dx and Dy such that mv(\u3008X,Y \u3009 ,Dx ,Dy) = \u3008X + Dx , Y + Dy\u3009. Then, we want \u03b4i+1 |=\u03b1i+1 Position = \u3008X,Y \u3009 where \u03b1i+1(X) = \u03b1i(X) + Dx and \u03b1i+1(Y ) = \u03b1i(Y ) + Dy . Usually, the semantics of a formula \u03d5 is determined by an interpretation function \u00b7 \u03b1: Fml\u2192 A with A being the domain of the model algebra. For all variables and constant symbols X \u03b1:= \u03b1(X). This means that in equation (1), \u03b1 actually points the other way round: The meaning of a variable is grounded in the world. The reason for us to denote the arrows the other way round is as follows: We know the algorithmic (and thus purely syntactical) definition of op (like mv) but we do not know its semantics. So by observing the change act : \u03b1i 7\u2192 \u03b1i+1 we find the meaning of op in a changed \u03b4 such that\n\u03b1i+1 = \u03b1i\u03b4 := \u03b1i \u2212 { \u3008X,\u03b1i(X)\u3009 :\nX is affected while performing act\n} (2)\n\u222a \u3008X, dX \u3009 : X is affected whileperforming act and its reading becomes dX  Note that \u201cbeing affected\u201d does not necessarily mean \u201cchanged/caused by\u201d: Not everything that happens while we act is a consequence of our acting. Therefore, observing a change in the environment does not mean it contributes to the meaning of the performed action (c.f. the frame problem and determinism, [10])."}, {"heading": "2.2 Representing States", "text": "We assume the world at a point t in time to be represented by a state snapshot. State snapshots are, basically, total functions \u03b1t which to a set of variables representing certain sensory input assign corresponding values. The set of vari-\nables is Var = VarO \u222aVarI with VarO = { X\u0307i : i \u2208 nO } being the set of (global) observable variables and VarI = {Xj : j \u2208 nI} being the set of (local) internal variables. Furthermore, every variable X \u2208 Var is assigned a type. A state\nsnapshot st at time t is defined as sequence\nst := [\u3008X,\u03b1t(X)\u3009 : X \u2208 Var] . (3)\nFor a set T = {i : i \u2208m} we have a sample\nS = {\u3008t, st\u3009 : t \u2208 T} (4)\nof state snapshots. Since T is a strict order, we can also uniquely arrange all state snapshots in a table which gives every variable X\u0307 the reading of a function (i.e. a column in the table). Using the relational database definition language, the sample can be described as\nsnapshots(t, fX\u03070 , . . . , fX\u0307nO\u22121 ,\u03b1t(X0), . . . ,\u03b1(XnO\u22121)) PK(snapshots) = t dom(fX\u0307i) = di,dom(\u03b1(Xj)) = dj\n(5)\nwhere all di, dj are the types of the according variables and t is a unique identifier (\u201cPrimary Key\u201d).1 We shall represent collected state snapshots as factual knowledge represented using simple Horn logic facts and key-value lists:\nstate(t, [x0\u2212v0, x1\u2212v1, . . . , xn\u22121\u2212vn\u22121]). (6)\nwhere all xi are variable names (hence lowercase; they correspond to the column features in the database notation in equation (5)) and all vi are the instantiations of the according variables at time t. Table 1 lists the hierarchy of variable types."}, {"heading": "2.3 Bootstrapping initial theories", "text": "Symbol grounding or lexical semantics fails to work until one agrees on a set of common, atomic meaningful symbols from which one can construct larger ones. Whatever we perceive, we need to be able to put it into words in order to reason about it. And whatever we shall learn can be learned only if we have a sufficient repository of basic categories from which we can build a desired concept.Our initial knowledge consists of a type system and a set of primitive typed relations and (arithmetic) operations. It contains the definitions of relations such as greater than, less than; arithmetic relations such as addition, subtraction, multiplication, division; spatial relations like position, orientation, proximity; and predicate logic expressions including equality. The type system can easily be adapted to the domain: Example. With only real numbers num : X and num : Y describing coordinates and suitable arithmetic operations we can define a type pos as num \u00d7 num with a new operation dist : pos\u2192 pos as\ndist(P1, P2) := sqrt((P1.1\u2212 P2.1)^2+(P1.2\u2212 P2.2)^2) 1 Therefore, equations (3) and (5) are equivalent: snapshot(t, . . .) = st.\nwhere X.i denotes the i-th component of X. This way, the background knowledge can be represented as follows: Example. First, we define the signature of the operations or relations to be implemented:\nadd to : num\u00d7 num\u2192 num left of : obj\u00d7 obj\u2192 {true, false}\nThe according definitions are straightforward:\nadd to(num : X, num : Y, num :Z) :\u21d0\u21d2 Z = X + Y. left of(pos : (X1, ), pos : (X2, )) :\u21d0\u21d2 X1 < X2."}, {"heading": "2.4 Actions", "text": "An action is something a robot does. Usually, every action is the result of a command and it causes several effects (hopefully all and only desired). A command is an instantiation of an operator (c.f. STRIPS, [13]): MOVE obj : Obj , num : x PRE { at(Obj , pos : \u2329 X\u0307, Y\u0307 \u232a ) } EFF [ power(0.8); wait(c \u00b7 x); power(0) ] DEL { at(Obj , pos : \u2329 X\u0307, Y\u0307 \u232a ) }\nADD { at(Obj , pos : \u2329 X\u0307 + cx sin Z\u0307, Y\u0307 + cx cos Z\u0307 \u232a ) }  One might state that the semantics of moving is rather simple: it is the program as it is defined in the EFF-slot of the operator definition, but our goal is to explain, what it means to do EFF. Note the different kinds of variables used in the operator definition above: As already stated, the dotted variables are global variables holding feature values that describe the world. Simple variable symbols are required for an abstract\ndefinition of the operator. MOVE works for any kind of object Obj , accepts as a parameter a numerical value x (which means the distance one shall move), and a constant c that translates distance into the period of time one travels at a determined speed. In planning, one puts the semantics into the operator definition by defining changes in the state descriptions. The operator is applicable only if PRE is satisfied and ADD and DEL describe how a state description changes after performing EFF.2 As one can see, the new position of the robot after moving is defined in terms of the previous position (X\u0307 and Y\u0307 ), the travel distance x (and a factor c) and another globally observable feature Z\u0307: the robot\u2019s orientation."}, {"heading": "2.5 Learning by Observing", "text": "If we want a robot to learn the semantics of its actions, we reverse the line of argument: MOVE means that certain variables change in a certain way. This, in general, gives rise to two learning problems:\n1. Which is the (smallest) subset V \u2286 Var that contains all relevant variables for describing the effects of an action? 2. Once we know where to look for changes, what is the underlying system in the way the values change?\nThe first questions seems simple to be answered: Collect all those variables X\u0307 with \u03b1i(X\u0307) 6= \u03b1i+1(X\u0307) for an action that is performed between i and i + 1. But this is too simple: First, the meaning of an action could be exactly to preserve a certain value rather than changing it and second there are many other reasons why a variable might be changed without any connection to performing the action. The second question is not as simple. Let V \u2286 VarO be a suitable set of observable variables. Then, the question is to find the general law that describes the change. This presupposes that operators are \u201csynchronised\u201d in the sense, that all affected variables are changed at the same time.3 With a suitable set of background knowledge (see sections 2.2- 2.4) and Horn logic as representation formalisms, Inductive Logic Programming (ILP), [15,16,17,18], is a prime candidate for learning the semantics of action."}, {"heading": "2.6 Related work", "text": "DIDO, [19], explores unfamiliar domains without any external supervision. The task is to build a representation of the domain which can be used to predict the outcomes of its motor operations with a maximum likelihood of being correct. Its\n2 Note that in this paradigm, one works on descriptions of states. In a closed, deterministic and fully observed world such a description (is assumed to) coincide with reality. In the real world, however, situations may change without a change of its description and there might be different description for one and the same world. 3 A first step towards dealing with state snapshots of asynchronous processes in the context of learning was discussed in [14].\nkey features - unsupervised inductive learning from actions and state descriptions without prior knowledge - represent the fundamental principles on which EVOLT is based on, too. The major difference between the proposed system and DIDO is that EVOLT learns by observing the environment whereas DIDO learns by the (given) effects of an action. LIVE, [20,21], uses complementary discrimination learning which is inspired by Piaget\u2019s child development theories to learn the prediction models of effects of an action. A prediction model in this setting is given by the triplet \u3008precondition, action, prediction\u3009 and states that if an action applied to a percept which satisfies a precondition then the resulting percept should satisfy the prediction. Like DIDO, HYPER [22] shall learn by exploring the domain. HYPER uses standard ILP (Aleph, [23]) to induce knowledge about movement (movability, obstacle identification, DoF). In the course of the XPERO project [4] the same ILP learner was used. One one hand, this requires a far more elaborate background knowledge which in turn introduces bias in form of model assumptions. On the other hand, full ILP requires negative examples to avoid over-generalisation. [5] focus on generating negative examples by introducing a bias that determines feature value ranges. While we deal with more or less the same problem of learning the semantics of actions, our approach differs from all these approaches as we try to learn without negative examples from scratch or, at least, with as few as possible assumptions and predetermined (and predetermining) knowledge."}, {"heading": "3 EVOLT: Evolving Theories", "text": "Learning the meaning of an action in EVOLT comprises of three stages of subsequent steps:4\n1. Identify relevant observable variables. 2. Induce a new hypothesis describing the performed action. 3. Refine already existing hypotheses.\nIn this first study and to suit the huge amount of flat data we have focused on one aspect of ILP and extended the generalisation procedure to deal with typed terms. Additionally, simple data collections leave us with positive examples only. Finally, the clause head of the target predicate is under-specified in terms of mode description as, e.g. used [24] such that there are no prior candidates for the sought clause head available. EVOLT searches for the relations exhaustively where each relation definition in background knowledge is scanned to check whether it entails the changes in the state variables caused by an action. The increase of search effort due to the brute-force method is alleviated by our use of a type system (cf. 2.2) which allows quick unification test. Nevertheless, this efficiency gain does not scale for the number of subsets of variables still grows exponentially with a linear growth\n4 Here, we use the term \u201ctheory\u201d for what one usually calls a \u201cmodel\u201d.\nin the number of variables used on a base determined by the size of a variables domain. In the following, different stages of learning are described briefly."}, {"heading": "3.1 Evolving theories by stepwise refinement", "text": "Extracting action effects from state description data We shall explain this by the following example: Each shape5 in figure 1 represents a value assigned\nto a variable X\u0307i. Assuming that at time t an action was performed and the immediate prior and posterior state snapshots are st\u22121 and st+1, we define an effect and a no-effect set\nEff t := {X \u2208 VarO : \u03b1t\u22121(X) 6= \u03b1t+1(X)} (7) Eff t := VarO \u2212 Eff t (8)\nrespectively. Imagine now that at t (that is, between t\u22121 and t+ 1) a command to perform act had been issued and all effects (\u201cEFF\u201d) came into force. Assume further that act was called by parameters a0, . . . , ak\u22121. Therefore we expect to see that actt(a0, . . . , ak\u22121) causes a certain change:\nactt(a0, . . . , ak\u22121) : \u03b1t\u22121 7\u2192 \u03b1t+1 5 Similar shapes in the same set do not necessarily represent the same value for cor-\nresponding variables. However similar shapes corresponding to the same variable in different sets represent the same value.\nwhere \u03b1t+1 = \u03b1t\u22121\u03c3 and \u03c3 is the substitution acting on \u03b1t\u22121 as defined by the operator declaration that was executed. In analogy to our example in section 2.4, we have the following Example. Let Obj = \u2032huey\u2032 and x = 45. Assume X\u0307 = 0 and Y\u0307 = 7. Then (presupposing a suitable value c), we would expect the robot Huey to end up on a position that is \u201cnortheast\u201d of its initial position:\nMOVEt( \u2032huey\u2032, 45)(X\u0307) : 0 7\u2192 5 MOVEt( \u2032huey\u2032, 45)(Y\u0307 ) : 7 7\u2192 12\nBuilding hypothesis about cause and effect Hypotheses are formulated in terms of possible linkings between action parameters ai and variables X\u0307.\nThe transition group Tran is set of triplets containing all combinations of observable variables and parameters:\nTrant := {\u2329 \u03b1t\u22121(X\u0307), ai,\u03b1t+1(X\u0307) \u232a : X\u0307 \u2208 Eff t, 0 \u2264 i < k } (9)\nIf we assume there are m effect pairs, then for k action parameters, there will be k \u00b7m number of triplets in a transition group. Since we examine only one action at a time, k is restricted by the number of parameters of only one operator which, in general, is rather small. As shown in figure 2, each member of the triplet is assigned a data type (recall section 1) which acts as a search bias to reduce the search space. The background knowledge provides the definitions of predicates that can be used by the learner to explain the relation which may exist between instantiations of variables X\u0307j caused by an action act with parameters ai. Once\nthe transition group is created, the learner examines each triplet against each definition in the background knowledge to identify a relation described by values in a given triplet by matching the predicate signature against the types in the transition group:\nThactt :=\n{\u2329 X\u0307, R \u232a : \u2329 \u03b1t\u22121(X\u0307), ai,\u03b1t+1(X\u0307) \u232a \u2208 Trant,\nR = {r : r :s1 \u00d7 s2 \u2192 s3}\n} (10)\nwhere the signature matches the types of the transition triplet.\nRefining existing theories Assume we already inferred a theory Thactt . Now, at t\u2032 > t, the transition group suggests to build a new theory Thactt\u2032 . In order to refine Thactt to meet the hypotheses of Th act t\u2032 we build the intersection which then contains only those relation/variable candidates that are part of both theories. This way, we monotonously specify the theory by ruling out all those candidates that do not support all of our observations:\nThactt+1 := Th act t \u2229 Th act t\u22121. (11)\nWhile an on-line learning method that continuously refines a theory is a very useful tool for robots that learn from scratch, the brute intersection has disadvantages, too:\n\u2013 It could be an operator does not always affect the same set of variables. For example, one could move into the y-direction only thereby leaving the x-coordinates unchanged. But a move usually affects all components of the location.\n\u2013 Forcing every affected variable to be affected every time is also prone to noise since it could be we just have an observation or quantisation error.\nThe motivation for choosing this approach despite its disadvantages was that in real life the non-determinism of the surrounding world results in a huge set of transitions pairs, and, hence, in large theories. Therefore we rule out singular observations to decrease noise by the price of over-pruning the hypothesis space. The results described in the next sections are surprisingly accurate and semantically valid and therefore support our approach to theory refinement by monotonic specialisation."}, {"heading": "3.2 Example", "text": "State descriptions The following is an example of the EVOLT representation of states (section 2.2). Each state description is represented by a predicate state spec/2 with its first argument being the time stamp t and the second one a variable list (c.f. equation (6)). For example,\nstate spec(31, [ [r pos, [9, 14]:pos], [obj num, 1:num], [obj grab, [none]:obj, 0:truthVal], [obj pos, [obst]:obj, [13,3]:pos] ]).\nEach variable is, again, represented as a list with the first list element being the name of an observable variable X\u0307 (e.g. obj con) and the remaining arguments being its value \u03b1t(X\u0307). By unification, [X\u0307|Val ], Val is a list of typed variable values (with types written postfix). This allows to assign complex values to a variable: [obj pos, [obst]: obj, [13, 3]: pos] means that \u02d9Obj Loc is of a type obj\u00d7pos. The state snapshot states that at time 31 the robot is at \u30089, 14\u3009, there is one more object, an obstacle is at position \u300813, 3\u3009, and the robot does not hold anything in its gripper.\nActions Actions are represented by a predicate action(Action id ,Time,Parameters). The first argument serves as an identifier for program internal purposes, Time identifies the time stamp t and Parameters is a list of parameters. For example, the action move forward by a distance 3 at time 32 is represented as\naction(move forward, 32, [3:dist]).\nAction-effect Theory An action theory Thactt is represented by action theory/2. Since we continuously update theories (see equation (11), we do not explicitly store the time stamp. However, multiple alternative hypotheses can be asserted to the Prolog workspace thus enabling a time-independent off-line learning on entire sets of theories.\naction theory(theory(Name), Params, relation is(Relations))\nThe first argument carries the name act of the action to be explained, the second argument is a list of parameters that were passed to act. relation is(Relations) holds pairs of observable variables X\u0307 and the candidates for operators/relations that can be used to describe parameter-dependent changes (see equation (10)). Again, we give an example for moving forward:\naction theory( move forward, [D:dist], relation is([ [ r pos,\n[ has new position( [X1, Y1]:pos, D: dist, [X2, Y2]:pos)\n]]]) ).\nIt states the trivial but true proposition that, when moving, the position changes.\nBackground Knowledge As seen in the previous example, the Thactt consists of prior defined relations and operators. Examining the background knowledge for relations satisfying the changes as recorded in Trant involves two steps. In the first step, the data type of the triplet is matched against the data type of the operator declarations. This is a simple unification problem for the applicability of respective predicates. The background knowledge predicate definitions for the example from section 2.3 are:\nadd to (X : num, Y : num, Z : num):Z = X + Y. left of ([X1, ] : pos, [X2, ] : pos):X1 < X2.\nExample. The idea is that, based on the state description for t = 31 and an according state description at t = 33 including [r pos, [9, 20]:pos]], and with background knowledge\ntravel x([X1, Y ] : pos, D : dist, [X2, Y ] : pos):orientation WE(Z) X2 is (X1 + (Z * C * D)).\nthe system deduces that carrying out a task with name move forward at t = 32 and an action parameter 3 when heading East (then, Z = 1; heading West means Z = \u22121) results in an x-aligned position change:\naction theory( move forward, [D:dist], [ travel x(\n[X1, Y1]:pos, D: dist, [X2, Y1]:pos) ]\n).\nbecause the above goal is provable with instantiating C = 2."}, {"heading": "3.3 Results", "text": "The primary goal of the experimental evaluation was to examine the applicability and behaviour of EVOLT in different environmental settings. The example of one such environmental setting is given below. The data (set of states and actions) was collected as follows: A robot was controlled to move in a simulated environment (using openRAVE). We recorded all state descriptions (by reading out the simulation system variables and representing them as facts) and also recorded all the commands that were issued to the robot (that is, the manual control commands were translated into action commands). Figure 3 depicts the task environment that was considered for an experiment where the robot is placed between two obstacles. In our case, the position is expressed using a\nthree-dimensional coordinate system (adding height to our previous code examples). We tried learning semantic descriptions of several primitive actions like\nmove forward, turn left/right, grab/drop object based on background knowledge. The following is an example of the semantics learned for turn left/right (disregarding all information about the two obstacles):\nr pos, [ change in orientation(\n[X, Y ] :pos, G :angle, [X, Y ] :pos )\n]\nmeaning that whenever an action name \u201cturn X\u201d is performed, and whenever it receives a parameter G (which is an angle), then the orientation of the robot changes, but its position remains the same."}, {"heading": "4 Conclusion", "text": ""}, {"heading": "4.1 Summary", "text": "We have proposed an approach to learn the semantics of actions by describing the observed effects in terms of atomic arithmetic/logical operations. An action is considered as an event that causes the current state to change where the state is a set of features which represent sensory inputs. The working principles of EVOLT are inspired by the Evolution of Theories paradigm, while the learning itself is implemented using principles of Inductive Logic Programming. EVOLT uses exhaustive or brute-force search to search the relations where the search is guided by the search bias in the form of typed variables. Another contribution is that, motivated by its use in robotics, our systems works on unlabelled examples.\nMoreover, EVOLT is provided with a minimal set of background knowledge which is \u201ccommon knowledge\u201d in a sense that it is not task specific and only provides atomic operations on sensory data. Even though the results of the evaluation are quite reasonable, the system is still in its adolescent stage. For example, it is assumed that all actions are executed successfully, that there are no side-effects and that the world is fully observable and deterministic. However, all of these requirements usually are not met in a real world robot scenario."}, {"heading": "4.2 Prospects", "text": "Our immediate future goal is to make the system more sophisticated by improving its data handling and extending background knowledge with additional definitions of more complex relations. Further space of improvement is given by needs for\n\u2013 a type system that allows typing of complex terms, \u2013 added time stamps to action-theory facts for sequence learning, \u2013 building transition sets on entire Eff t\u00d7k\u00d7Eff t, therefore allowing to express\nsemantic relations involving several variables, \u2013 considering as transitions \u2118(Eff t)\u00d7\u2118(k)\u00d7\u2118(Eff t) which takes the learning\nto a further level of abstraction by combining several observable values, \u2013 a more sophisticated theory refinement that goes beyond a mere collection\nof common appearances of variable assignment changes.\nWith a then growing complexity of search space we also need to introduce more sophisticated biases. Also, the dimensionality of hypothesis space (as determined by the number of observable variables) needs to be reduced, where at the same time we want to enable the system to deal with an increasing number of features. One method for a feature selection procedure in the context of relational representation of data is rough set data analysis, [6]; see also sections 2.2 and 2.5). Together with a modal logic representation form we aim at building a framework for a modal inductive logic programming setting that shall be able to infer more powerful semantic descriptions of actions. Thanks. The authors wish to thank Bjo\u0308rn Kahl for his support and expertise in robot manipulation and simulation."}], "references": [{"title": "An essay concerning human understanding", "author": ["J. Locke"], "venue": "The Pennsylvania State University", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1690}, {"title": "Theory of Knowledge: The 1913 Manuscript", "author": ["B.A.W. Russell"], "venue": "Routledge", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1992}, {"title": "Being aware: Where we think the Action is", "author": ["M.E. M\u00fcller"], "venue": "Cognition, Technology, and Work 9(2)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Xpero - learning by experimentation (d1.6)", "author": ["B. Kahl", "T. Henne", "U. K\u00f6ckemann", "C. Shahazad", "E. Prassler"], "venue": "Workpackage final report,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Towards iterative learning of autonomous robots using ilp", "author": ["N. Akhtar", "M. Fueller", "B. Kahl", "T. Henne"], "venue": "15th Intl. Conf. on Advanced Robotics (ICAR).", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Relational Knowledge Discovery", "author": ["M.E. M\u00fcller"], "venue": "Cambridge University Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Programs with commonsense", "author": ["J. McCarthy"], "venue": "Proceedings of the Teddington Conference on the Mechanization of Thought Processes. Her Majesty\u2019s Stationery Office, London", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1959}, {"title": "Making robots conscious", "author": ["J. McCarthy"], "venue": "In Furukawa, K., Michie, D., Muggleton, S., eds.: Machine Intelligence 15: Intelligent Agents. Oxford University Press, Oxford", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1996}, {"title": "On sentences which are true of direct unions of algebras", "author": ["A. Horn"], "venue": "Journal of Symbolic Logic 16(1)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1951}, {"title": "Some philosophical problems from the standpoint of artificial intelligence", "author": ["J. McCarthy", "P.J. Hayes"], "venue": "Machine Intelligence 4", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1969}, {"title": "Tree of Knowledge: Biological Roots of Human Understanding", "author": ["H.R. Maturana", "F.J. Varela"], "venue": "Shambala", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1987}, {"title": "Vehicles: Experiments in Synthetic Psychology", "author": ["V. Braitenberg"], "venue": "The MIT Press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1986}, {"title": "Strips: A new approach to the application of theorem proving to problem solving", "author": ["N.J. Nilsson", "R.E. Fikes"], "venue": "Technical Report 43, Stanford Research Institute, SRI, Menlo Park, CA", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1970}, {"title": "Relational Cognitive Structures for Intelligent Agent and Robot Control", "author": ["M.E. M\u00fcller", "F. Krebs", "F. Hielscher"], "venue": "Systems, Man, and Cybernetics (SMC-2008), IEEE", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Induction of Horn-clauses: methods and the plausible generalization algorithm", "author": ["W. Buntine"], "venue": "26", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1987}, {"title": "Towards constructive induction in first-order predicate calculus", "author": ["S. Muggleton", "W. Buntine"], "venue": "TIRM 88-03, The Turing Institute, Glasgow", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1988}, {"title": "Logical and Relational Learning", "author": ["L.D. Raedt"], "venue": "Cognitive Technologies. Springer", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning novel domains through curiosity and conjecture", "author": ["P.D. Scott", "S. Markovitch"], "venue": "In Proceedings of International Joint Conference for Artificial Intelligence, Morgan Kaufmann", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1989}, {"title": "Complementary discrimination learning with decision lists", "author": ["W.M. Shen"], "venue": "In Proceedings Tenth National Conference on Artificial Intelligence (pp. 153-158). Menlo Park, Ca, AAAI Press", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1992}, {"title": "Discovery as autonomous learning from the environment", "author": ["W.M. Shen"], "venue": "Machine Learning, Computer Science Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1994}, {"title": "An experiment in robot discovery with ilp", "author": ["G. Leban", "J. \u017dabkar", "I. Bratko"], "venue": "Proceedings of the 18th international conference on Inductive Logic Programming. ILP \u201908, Berlin, Heidelberg, Springer-Verlag", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "The Aleph Manual", "author": ["A. Srinivasan"], "venue": "University of Oxford.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Inverting entailment and Progol", "author": ["S. Muggleton"], "venue": "In Furukawa, K., Michie, D., Muggleton, S., eds.: Machine Intelligence 14. Oxford University Press", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": "There are several prime inspirations for our work: First, John Locke\u2019s concept of learning from scratch, [1], and Bertrand Russell\u2019s \u201cTheory of knowledge\u201d, [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "There are several prime inspirations for our work: First, John Locke\u2019s concept of learning from scratch, [1], and Bertrand Russell\u2019s \u201cTheory of knowledge\u201d, [2].", "startOffset": 156, "endOffset": 159}, {"referenceID": 2, "context": "[3] also summarises several theories of cognition related to learning the semantics of an action.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Both the environment and the innate knowledge is represented relationally as logic programs so we can describe the formation of more complex models as a learning process similar to inductive logic programming, [6].", "startOffset": 210, "endOffset": 213}, {"referenceID": 9, "context": "the frame problem and determinism, [10]).", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "STRIPS, [13]): \uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 MOVE obj : Obj , num : x PRE { at(Obj , pos : \u3008 \u1e8a, \u1e8e \u3009 ) }", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "4) and Horn logic as representation formalisms, Inductive Logic Programming (ILP), [15,16,17,18], is a prime candidate for learning the semantics of action.", "startOffset": 83, "endOffset": 96}, {"referenceID": 15, "context": "4) and Horn logic as representation formalisms, Inductive Logic Programming (ILP), [15,16,17,18], is a prime candidate for learning the semantics of action.", "startOffset": 83, "endOffset": 96}, {"referenceID": 16, "context": "4) and Horn logic as representation formalisms, Inductive Logic Programming (ILP), [15,16,17,18], is a prime candidate for learning the semantics of action.", "startOffset": 83, "endOffset": 96}, {"referenceID": 17, "context": "DIDO, [19], explores unfamiliar domains without any external supervision.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "3 A first step towards dealing with state snapshots of asynchronous processes in the context of learning was discussed in [14].", "startOffset": 122, "endOffset": 126}, {"referenceID": 18, "context": "LIVE, [20,21], uses complementary discrimination learning which is inspired by Piaget\u2019s child development theories to learn the prediction models of effects of an action.", "startOffset": 6, "endOffset": 13}, {"referenceID": 19, "context": "LIVE, [20,21], uses complementary discrimination learning which is inspired by Piaget\u2019s child development theories to learn the prediction models of effects of an action.", "startOffset": 6, "endOffset": 13}, {"referenceID": 20, "context": "Like DIDO, HYPER [22] shall learn by exploring the domain.", "startOffset": 17, "endOffset": 21}, {"referenceID": 21, "context": "HYPER uses standard ILP (Aleph, [23]) to induce knowledge about movement (movability, obstacle identification, DoF).", "startOffset": 32, "endOffset": 36}, {"referenceID": 3, "context": "In the course of the XPERO project [4] the same ILP learner was used.", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "[5] focus on generating negative examples by introducing a bias that determines feature value ranges.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "used [24] such that there are no prior candidates for the sought clause head available.", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "state spec(31, [ [r pos, [9, 14]:pos], [obj num, 1:num], [obj grab, [none]:obj, 0:truthVal], [obj pos, [obst]:obj, [13,3]:pos] ]).", "startOffset": 25, "endOffset": 32}, {"referenceID": 13, "context": "state spec(31, [ [r pos, [9, 14]:pos], [obj num, 1:num], [obj grab, [none]:obj, 0:truthVal], [obj pos, [obst]:obj, [13,3]:pos] ]).", "startOffset": 25, "endOffset": 32}, {"referenceID": 12, "context": "state spec(31, [ [r pos, [9, 14]:pos], [obj num, 1:num], [obj grab, [none]:obj, 0:truthVal], [obj pos, [obst]:obj, [13,3]:pos] ]).", "startOffset": 115, "endOffset": 121}, {"referenceID": 2, "context": "state spec(31, [ [r pos, [9, 14]:pos], [obj num, 1:num], [obj grab, [none]:obj, 0:truthVal], [obj pos, [obst]:obj, [13,3]:pos] ]).", "startOffset": 115, "endOffset": 121}, {"referenceID": 12, "context": "This allows to assign complex values to a variable: [obj pos, [obst]: obj, [13, 3]: pos] means that  \u0307 Obj Loc is of a type obj\u00d7pos.", "startOffset": 75, "endOffset": 82}, {"referenceID": 2, "context": "This allows to assign complex values to a variable: [obj pos, [obst]: obj, [13, 3]: pos] means that  \u0307 Obj Loc is of a type obj\u00d7pos.", "startOffset": 75, "endOffset": 82}, {"referenceID": 8, "context": "The idea is that, based on the state description for t = 31 and an according state description at t = 33 including [r pos, [9, 20]:pos]], and with background knowledge", "startOffset": 123, "endOffset": 130}, {"referenceID": 18, "context": "The idea is that, based on the state description for t = 31 and an according state description at t = 33 including [r pos, [9, 20]:pos]], and with background knowledge", "startOffset": 123, "endOffset": 130}, {"referenceID": 5, "context": "One method for a feature selection procedure in the context of relational representation of data is rough set data analysis, [6]; see also sections 2.", "startOffset": 125, "endOffset": 128}], "year": 2013, "abstractText": "In this paper, we describe an approach that enables an autonomous system to infer the semantics of a command (i.e. a symbol sequence representing an action) in terms of the relations between changes in the observations and the action instances. We present a method of how to induce a theory (i.e. a semantic description) of the meaning of a command in terms of a minimal set of background knowledge. The only thing we have is a sequence of observations from which we extract what kinds of effects were caused by performing the command. This way, we yield a description of the semantics of the action and, hence, a definition.", "creator": "LaTeX with hyperref package"}}}