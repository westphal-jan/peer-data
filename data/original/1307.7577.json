{"id": "1307.7577", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jul-2013", "title": "Safe Screening with Variational Inequalities and Its Application to Lasso", "abstract": "The model selected by sparse learning techniques usually has a few non-zero entries. Safe screening---which eliminates the features that are guaranteed to be absent for a target regularization parameter based on an existing solution---is a technique for improving the computational efficiency while maintaining the same solution. In this note, we propose an approach called Sasvi (Safe screening with variational inequalities) and take LASSO for example in the analysis. Sasvi makes use of the variational inequalities which provide the sufficient and necessary optimality conditions for the dual problem. The existing approaches can be casted as relaxed versions of the proposed Sasvi, and thus Sasvi provides a much stronger screening rule. The monotone properties of Sasvi are studied based on which a sure removal regularization parameter can be identified for each feature. In addition, the proposed Sasvi is readily extended for solving the generalized sparse linear models. Preliminary experimental results are reported.", "histories": [["v1", "Mon, 29 Jul 2013 13:45:58 GMT  (127kb,D)", "http://arxiv.org/abs/1307.7577v1", "20 pages"], ["v2", "Wed, 30 Oct 2013 15:00:04 GMT  (3312kb,D)", "http://arxiv.org/abs/1307.7577v2", "27 pages, 6 figures, 2 tables"], ["v3", "Mon, 12 May 2014 19:46:39 GMT  (3322kb,D)", "http://arxiv.org/abs/1307.7577v3", "Accepted by International Conference on Machine Learning 2014"]], "COMMENTS": "20 pages", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["jun liu 0003", "zheng zhao", "jie wang", "jieping ye"], "accepted": true, "id": "1307.7577"}, "pdf": {"name": "1307.7577.pdf", "metadata": {"source": "CRF", "title": "Safe Screening With Variational Inequalities and Its Applicaiton to LASSO", "authors": ["Jun Liu", "Zheng Zhao", "Jie Wang", "Jieping Ye"], "emails": ["jun.liu@sas.com", "zheng.zhao@sas.com", "jie.wang.ustc@asu.edu", "jieping.ye@asu.edu"], "sections": [{"heading": "1 Introduction", "text": "Sparse learning approaches usually obtain a model with only a few non-zero features and they are able to achieve an interpretable model with high prediction accuracy. Taking advantage of the nature of sparsity, screening techniques have been proposed for accelerating the computation. The strong rules proposed in [2] works very well in practice although they might mistakenly discard active features. To remedy this, safe screening techniques [1, 3, 4], which elliminates the features that are guaranteed to be absent for a target regularization parameter based on an existing solution, have been proposed. These approaches studied the dual problem and bounded a functional of the dual variable at the target regularization parameter based on an existing solution.\nar X\niv :1\n30 7.\n75 77\nv1 [\ncs .L\nG ]\nIn this note, we propose an approach called Sasvi (Safe screening with variational inequalities) and take LASSO for example in the analysis. Sasvi makes use of the variational inequalities which provide the sufficient and necessary optimality conditions for the dual problem. The existing approaches can be casted as relaxed versions of the proposed Sasvi, and thus Sasvi provides a much stronger screening rule. The monotone properties of Sasvi are studied based on which a sure removal regularization parameter can be identified for each feature. In adddition, the proposed Sasvi is readily extended for solving the generalized sparse linear models."}, {"heading": "2 Safe Screening with Variational Inequalities", "text": "For simplicity of discussion, we take LASSO as an example, and discuss the extension to the generalized linear models such as logistic regression in Section 3.\nFor any \u03bb > 0, the dual of the LASSO problem can be derived as follows:\nmin\u03b2\n{ 1\n2 \u2016X\u03b2 \u2212 y||22 + \u03bb\u2016\u03b2\u20161\n} (1)\n= min\u03b2 max\u03b8\n{ \u3008y \u2212X\u03b2, \u03bb\u03b8\u3009 \u2212 1\n2 \u2016\u03bb\u03b8||22 + \u03bb\u2016\u03b2\u20161\n} (2)\n= max\u03b8 min\u03b2 \u03bb\n{ \u3008y,\u03b8\u3009 \u2212 \u03bb\n2 \u2016\u03b8||22 \u2212 \u3008XT\u03b8,\u03b2\u3009+ \u2016\u03b2\u20161\n} (3)\n= max\u03b8:\u2016XT \u03b8\u2016\u221e\u22641 \u03bb 2\n{ \u22121\n2 \u2016\u03b8 \u2212 y \u03bb ||22 + 1 2 \u2016y \u03bb ||22 } . (4)\nEq. (1) is the (primal) Lasso problem. A dual varialbe \u03b8 is introduced in (2). It is obvious that the optimal primal variable (\u03b2\u2217) and the optimal dual variable (\u03b8\u2217) satisfy\n\u03bb\u03b8\u2217 = y \u2212X\u03b2\u2217. (5)\nBy exchanging the min and max in Eq. (2), we obtain Eq. (3). Eq. (4) is obtained using the following simple fact\nmin \u03b2 {\u2212\u03b2b+ |\u03b2|} =\n{ 0, |b| \u2264 1\n\u2212\u221e, |b| > 1 (6)\nIn addition, an analysis of Eq. (6) shows that\n|(XT\u03b8\u2217)i| < 1\u21d2 \u03b2\u2217i = 0, (7)\nwhere (XT\u03b8\u2217)i denotes the i-th component of X T\u03b8\u2217.\nThe dual problem in Eq. (4) can be rewritten as\nmin \u03b8:\u2016XT \u03b8\u2016\u221e\u22641\n1 2 \u2016\u03b8 \u2212 y \u03bb ||22. (8)\nLet \u2016XTy\u2016\u221e \u2265 \u03bb1 > \u03bb2 > 0 be two distinct regularization parameters. Let \u03b2\u22171 and \u03b2 \u2217 2 be the optimal primal variables corresponding to \u03bb1 and \u03bb2, respectively. Let \u03b8\u22171 and \u03b8 \u2217 2 be the optimal dual variables corresponding to \u03bb1 and \u03bb2, respectively. Given \u03bb1, \u03b8 \u2217 1 and \u03bb2, we aim at bounding |xTj \u03b8\u22172 | without the actual computation of \u03b8\u22172 , where xj denotes the j-th column of X. According to Eq. (7), if |xTj \u03b8\u22172 | < 1, then the j-th feature is absent for computing \u03b2\u22172 ."}, {"heading": "2.1 Variational Inequality", "text": "Lemma 1 For the constrained convex optimzation problem:\nmin x\u2208G f(x), (9)\nwith G beging convex and closed and f(\u00b7) being convex and differentiable, x\u2217 is an optimal solution if and only if\n\u3008f \u2032(x\u2217),x\u2212 x\u2217\u3009 \u2265 0,\u2200x \u2208 G. (10)\nEq. (10) is the so-called variation inequality for the problem in Eq. (9). Applying Lemma 1, we can represent the optimality conditions for \u03b8\u22171 and \u03b8 \u2217 2 as the following variational inequalities:\n\u3008\u03b8\u22171 \u2212 y\n\u03bb1 ,\u03b8 \u2212 \u03b8\u22171\u3009 \u2265 0,\u2200\u03b8 : \u2016XT\u03b8\u2016\u221e \u2264 1, (11)\n\u3008\u03b8\u22172 \u2212 y\n\u03bb2 ,\u03b8 \u2212 \u03b8\u22172\u3009 \u2265 0,\u2200\u03b8 : \u2016XT\u03b8\u2016\u221e \u2264 1. (12)\nPlugging \u03b8 = \u03b8\u22172 and \u03b8 = \u03b8 \u2217 1 into Eq. (11) and Eq. (12) respectively, we have\n\u3008\u03b8\u22171 \u2212 y\n\u03bb1 ,\u03b8\u22172 \u2212 \u03b8\u22171\u3009 \u2265 0, (13)\n\u3008\u03b8\u22172 \u2212 y\n\u03bb2 ,\u03b8\u22171 \u2212 \u03b8\u22172\u3009 \u2265 0. (14)\nFor Eq. (12), besides \u03b8 = \u03b8\u22171 , one may make use of \u03b8 = \u2212\u03b8\u22171 and \u03b8 = \u00b1 y\n\u03bbmax ,\nwhich are also feasible for the constraints. In addition, if one has multiple existing solutions, one might combine them to form tighter constraints for \u03b8\u22172 . However, these are not pursued in this note."}, {"heading": "2.2 The Proposed Sasvi", "text": "In the proposed safe screening with variational inequalities (Sasvi), we bound |xTj \u03b8\u22172 | by\nmax \u03b8:\u3008\u03b8\u22171\u2212 y \u03bb1 ,\u03b8\u2212\u03b8\u22171 \u3009\u22650,\u3008\u03b8\u2212 y \u03bb2 ,\u03b8\u22171\u2212\u03b8\u3009\u22650\n|\u3008xj ,\u03b8\u3009|. (15)\nNote that, \u03b8\u22172 belongs to the constraints defined in Eq. 15, and the maximum obtained by Eq. 15 provides an upper-bound for |xTj \u03b8\u22172 |.\nThe key of Sasvi is to make use of Eq. (13) and Eq. (14) which are the instantiations of the variational inequalities in Eq. (11) and Eq. (12) for bounding \u03b8\u22172 . Next, we discuss how to solve Eq. (15).\nDenote\n\u03b8 = \u03b8\u22171 + y \u03bb2\n2 + r,a = (\ny\n\u03bb1 \u2212 \u03b8\u22171)/2,b = (\ny\n\u03bb2 \u2212 \u03b8\u22171)/2. (16)\nFigure 1 illustrates a and b by lines EB and EC, respectively. The following theorem shows that the angle between a and b is acute.\nTheorem 1 Let y 6= 0, and \u2016XTy\u2016\u221e \u2265 \u03bb1 > \u03bb2 > 0. We have\nb 6= 0, \u3008b,a\u3009 \u2265 0, (17)\nand the equality holds if and only if \u03bb1 = \u2016XTy\u2016\u221e. In addition, if \u03bb1 < \u2016XTy\u2016\u221e then a 6= 0.\nWith the notations in Eq. (16), Eq. (15) can be rewritten as\nmax r\n|\u3008xj , \u03b8\u22171 + y \u03bb2\n2 \u3009+ \u3008xj , r\u3009|\nsubject to \u3008a, r + b\u3009 \u2264 0, \u2016r\u201622 \u2264 \u2016b\u201622. (18)\nThe objective function of Eq. 18 can be represented in the following equivalent form:\nmax ( \u3008xj , \u03b8\u22171 + y \u03bb2\n2 \u3009 \u2212 \u3008\u2212xj , r\u3009,\u2212\u3008xj ,\n\u03b8\u22171 + y \u03bb2\n2 \u3009 \u2212 \u3008xj , r\u3009\n) , (19)\nwhich indicates that Eq. (18) can be computed by maximizing \u2212\u3008\u2212xj , r\u3009 and \u2212\u3008xj , r\u3009 over the constraints in the same equation. Maximizing \u2212\u3008\u2212xj , r\u3009 and \u2212\u3008xj , r\u3009 can be computed by minimzing \u3008\u2212xj , r\u3009 and \u3008xj , r\u3009, and we consider to solve the following minimization problem\nmin r\n\u3008x, r\u3009\nsubject to \u3008a, r + b\u3009 \u2264 0, \u2016r\u201622 \u2264 \u2016b\u201622. (20)\nNext, we show that Eq. (20) admits a closed form solution. If \u03bb1 = \u2016XTy\u2016\u221e, then a = 0 and the constraint \u3008a, r + b\u3009 \u2264 0 of Eq. (20) can be removed. It is easy to get that r = \u2212x\u2016b\u20162\u2016x\u20162 minimizes Eq. (20) with the minimum function value being \u2212\u2016x\u20162\u2016b\u20162. In our following discussion, we focus on 0 < \u03bb1 < \u2016XTy\u2016\u221e and thus a 6= 0 according to Theorem 1. We define\nx\u22a5 = x\u2212 \u3008x,a\u3009 \u2016a\u201622 a (21)\ny\u22a5 = x\u2212 \u3008y,a\u3009 \u2016a\u201622 a (22)\n2 , the middle point of E and C. The dashed\nline is \u03b8 : \u3008\u03b8\u22171 \u2212 y \u03bb1 ,\u03b8 \u2212 \u03b8\u22171\u3009 = 0. EX1, EX2, EX3 and EX4 denote \u00b1xj in two subcases: the angle between EB and EX1 (EX4) is larger than the angle between EB and EC, and the angle between EB and EX2 (EX3) is smaller than the angle between EB and EC. R2 and R3 are the maximizers to Eq. (15) with EX2 and EX3 denoting \u00b1xj , and the maximizer to Eq. (15) for EX1 (EX4) denoting \u00b1xj is on the intersection between the dashed line and the ball centered at D with radius ED.\nwhich are the orthonormal projections of x and y onto the null space of a, respectively.\nThe Lagraingian of Eq. (20) can be written as\nL(r, \u03b1, \u03b2) = \u3008x, r\u3009+ \u03b1\u3008a, r + b\u3009+ \u03b2 2 (\u2016r\u201622 \u2212 \u2016b\u201622), (23)\nwhere \u03b1, \u03b2 \u2265 0 are introduced for the two inequalities, respectively. It is clear that the minimal value of Eq. (20) is lower bounded (the minimum is no less than \u2212\u2016b\u20162\u2016x\u20162 by only considering the constraint \u2016r\u201622 \u2264 \u2016b\u201622). Therefore, the optimal dual variable \u03b2 is always positive; otherwise, minimizing Eq. (23) with regard to r achieves \u2212\u221e.\nSetting the derivative with regard to r to zero, we have\nr = \u2212x\u2212 \u03b1a\n\u03b2 . (24)\nPlugging Eq. (24) into Eq. (23), we obtain the dual problem of Eq. (20):\nmax \u03b1,\u03b2 \u03b1\u3008a,b\u3009 \u2212 1 2\u03b2 \u2016x + \u03b1a\u201622 \u2212 \u03b2 2 \u2016b\u201622\nsubject to \u03b1 \u2265 0, \u03b2 \u2265 0. (25)\nFor given \u03b2, we have\n\u03b1 = max( \u03b2\u3008a,b\u3009 \u2212 \u3008x,a\u3009\n\u2016a\u201622 , 0). (26)\nWe consider two cases. In the first case, we assume that \u03b1 = 0. We have\nr = \u2212x \u03b2 , \u03b2 \u2264 \u3008x,a\u3009 \u3008a,b\u3009 . (27)\nMaking use of the complementary slackness condition (note the optimal \u03b2 does not equal to zero), we have\n\u2016r\u20162 = \u2016 \u2212x \u03b2 \u20162 = \u2016b\u20162. (28)\nThus, we have\n\u03b2 = \u2016x\u20162 \u2016b\u20162 . (29)\nIncorporating Eq. (27) and Eq. (29), we have\n\u3008b,a\u3009 \u2016b\u20162\u2016a\u20162 \u2264 \u3008x,a\u3009 \u2016x\u20162\u2016a\u20162 , (30)\nso that the angle between a and b is equal to or larger than the angle between x and a. Note that \u3008b,a\u3009 \u2265 0 according to Theorem 1. For an illustration of\na and b, please refer to EB and EC in Figure 1, respectively. For x satisfying Eq. 30, please see EX2 and EX3 in the same figure. In addition, we have\n\u3008x, r\u3009 = \u2212\u2016x\u20162\u2016b\u20162. (31)\nIn the second case, Eq. (30) does not hold. We have\n\u03b1 = \u03b2\u3008a,b\u3009 \u2212 \u3008x,a\u3009\n\u2016a\u201622 . (32)\nPlugging Eq. (32) into Eq. (24), we have\nr = \u2212x\u2016a\u2016 2 2 + \u03b2\u3008a,b\u3009a\u2212 \u3008x,a\u3009a\n\u03b2\u2016a\u201622 (33)\nSince \u2016r\u201622 = \u2016b\u201622 (note that the optimal \u03b2 does not equal to zero), we have\n\u03b2 = \u221a \u2016x\u201622\u2016a\u201622 \u2212 \u3008x,a\u30092 \u2016y\u201622\u2016a\u201622 \u2212 \u3008y,a\u30092 = \u2016x\u22a5\u20162 \u2016b\u22a5\u20162 . (34)\nIn addition, we have\n\u3008x, r\u3009 = \u2212\u2016x\u22a5\u20162\u2016y\u22a5\u20162 \u2212 \u3008a,b\u3009\u3008x,a\u3009 \u2016a\u201622 . (35)\nThe main result is given in the following theorem.\nTheorem 2 Let y 6= 0, and \u2016XTy\u2016\u221e \u2265 \u03bb1 > \u03bb2 > 0. If a 6= 0, we denote\nx\u22a5j = xj \u2212 \u3008xj ,a\u3009a \u2016a\u201622 . (36)\n1. If \u3008b,a\u3009\u2016b\u20162\u2016a\u20162 > |\u3008xj ,a\u3009| \u2016xj\u20162\u2016a\u20162 then\n\u3008xj ,\u03b8\u22172\u3009 \u2264 \u3008xj ,\u03b8\u22171\u3009+ 1 \u03bb2 \u2212 1\u03bb1 2 [\u2016x\u22a5j \u20162\u2016y\u22a5\u20162 + \u3008x\u22a5j ,y\u22a5\u3009] (37)\nand\n\u2212 \u3008xj ,\u03b8\u22172\u3009 \u2264 \u2212\u3008xj ,\u03b8\u22171\u3009+ 1 \u03bb2 \u2212 1\u03bb1 2 [\u2016x\u22a5j \u20162\u2016y\u22a5\u20162 \u2212 \u3008x\u22a5j ,y\u22a5\u3009] (38)\n2. If \u3008xj ,a\u3009 > 0 and \u3008b,a\u3009\u2016b\u20162\u2016a\u20162 \u2264 \u3008xj ,a\u3009 \u2016xj\u20162\u2016a\u20162 , then \u3008xj ,\u03b8 \u2217 2\u3009 satisfies Eq. (37),\nand \u2212 \u3008xj ,\u03b8\u22172\u3009 \u2264 \u2212\u3008xj ,\u03b8\u22171\u3009 \u2212 \u3008xj ,b\u3009+ \u2016xj\u20162\u2016b\u20162. (39)\n3 . If \u3008xj ,a\u3009 < 0 and \u3008b,a\u3009\u2016b\u20162\u2016a\u20162 \u2264 \u2212\u3008xj ,a\u3009 \u2016xj\u20162\u2016a\u20162 , then\n\u3008xj ,\u03b8\u22172\u3009 \u2264 \u3008xj ,\u03b8\u22171\u3009+ \u3008xj ,b\u3009+ \u2016xj\u20162\u2016b\u20162. (40)\nand \u2212\u3008xj ,\u03b8\u22172\u3009 satisfies Eq. (38).\n4. If a = 0, then Eq. (39) and Eq. (40) holds.\nAn illustration of Theorem 2 for different cases can be found in Fig. 1. For the first case, the feature xj has a larger angle with a = X\u03b2\u22171\u03bb1\n2 compared to the one between a and b, and the upper-bound of |xTj \u03b8\u22172 | is the sum of 1) |xTj \u03b8\u22171 |, the inner product between xj and the dual solution at the known regualarization parameter \u03bb1 and 2) 1 \u03bb2 \u2212 1\u03bb1 2 [\u2016x \u22a5 j \u20162\u2016y\u22a5\u20162\u00b1\u3008x\u22a5j ,y\u22a5\u3009], a quantity that is computed based on the orthonormal projections of x and y onto the null space of a, respectively.\nFor the second and the third cases, the feature xj has a larger angle with\na = X\u03b2\u22171\u03bb1\n2 compared to the one between a and b, and the upper-bound of |xTj \u03b8\u22172 | is the sum of 1) |xTj \u03b8\u22171 |, the inner product between xj and the dual solution at the known regualarization parameter \u03bb1 and 2) \u3008xj ,b\u3009+\u2016xj\u20162\u2016b\u20162.\nFor the fourth case with \u03bb1 = \u2016XTy\u2016\u221e, the primal variable \u03b2\u22171 is a zero vector, and \u03b8\u22171 = y \u03bb1\n. This provides the screening rule at the maximal reugularization parameter."}, {"heading": "2.3 Relationship to Existing Approaches", "text": "Figure 2 compares the Sasvi approach with the SAFE approach [1] and the DPP approach [3]. It can be observed that the constraint used by Sasvi has much smaller volume compared to both SAFE and DPP, and thus leads to tighter bound for |\u3008xj ,\u03b8\u22172\u3009|. Next, we show that both SAFE and DPP indeed can be derived from the variational inequalities in Eq. (11) and Eq. (12) followed by relaxations.\nThe SAFE approach starts with \u03b8\u22171 , the optimal solution to the dual problem in Eq. (8). Denote G(\u03b8) = 12\u2016y|| 2 2 \u2212 12\u2016\u03bb2\u03b8 \u2212 y|| 2 2. The SAFE approach makes use of the so-called \u201cdual\u201d scaling, and compute the upper-bound of the G(\u03b8) for 0 < \u03bb2 < \u03bb1 as\n\u03b3(\u03bb2) = max s:|s|\u22641 G(s\u03b8) = max s:|s|\u22641\n1 2 \u2016y||22 \u2212 1 2 \u2016s\u03bb2\u03b8\u22171 \u2212 y||22. (41)\nNote that, compared to the SAFE paper, the dual variable \u03b8 has been scaled in the formulation in Eq. (41), while the results reproduced here are identical to that of the SAFE paper. Denote s\u2217 as the optimal solution. Solving Eq. (41), we have s\u2217 = max(min( \u3008\u03b8\u22171 ,y\u3009 \u03bb2\u2016\u03b8\u22171\u20162\n, 1),\u22121) when \u03b81 6= 0. Applying Lemma 3 in the appendix, we have \u3008\u03b8\u22171 ,y\u3009 \u2265 \u03bb1\u2016\u03b8\u22171\u20162 > \u03bb1\u2016\u03b8\u22171\u20162. Thus, we have s\u2217 = 1. The SAFE approach utilizes the following equation to compute the upper-bound for |\u3008xj ,\u03b8\u22172\u3009| as\n|\u3008xj ,\u03b8\u22172\u3009| \u2264 max \u03b8:G(\u03b8)\u2265\u03b3(\u03bb2) |\u3008xj ,\u03b8\u3009|\n= max \u03b8:\u2016\u03b8\u2212 y\u03bb2 ||2\u2264\u2016s \u2217\u03b8\u22171\u2212 y \u03bb2 ||2 |\u3008xj ,\u03b8\u3009| = |\u3008xj ,y\u3009| \u03bb2 + \u2016xj\u20162 \u2225\u2225\u2225\u2225s\u2217\u03b8\u22171 \u2212 y\u03bb2 \u2225\u2225\u2225\u2225 2 .\n(42)\nIf the bound on the right hand side of Eq. (42) is less than 1, then the j-th feature can be elliminated for \u03bb2. Utilizing \u2016XT\u03b8\u22171\u2016\u221e \u2264 1 and |s\u2217| \u2264 1, we can set \u03b8 = s\u2217\u03b8\u22171 in Eq. (12) and obtain\n\u3008\u03b8\u22172 \u2212 y\n\u03bb2 , s\u2217\u03b8\u22171 \u2212 \u03b8\u22172\u3009 \u2265 0, (43)\nwhich leads to\n\u3008\u03b8\u22172\u2212 y\n\u03bb2 ,\u03b8\u22172\u2212\ny\n\u03bb2 \u3009\u2212\u3008\u03b8\u22172\u2212\ny\n\u03bb2 , s\u2217\u03b8\u22171\u2212\ny\n\u03bb2 \u3009 = \u3008\u03b8\u22172\u2212\ny\n\u03bb2 ,\u03b8\u22172\u2212\ny\n\u03bb2 +\ny\n\u03bb2 \u2212s\u2217\u03b8\u22171\u3009 \u2264 0.\n(44) Making use of \u3008\u03b8\u22172 \u2212 y \u03bb2 , s\u2217\u03b8\u22171 \u2212 y \u03bb2 \u3009 \u2264 \u2016\u03b8\u22172 \u2212 y \u03bb2 \u20162\u2016s\u2217\u03b8\u22171 \u2212 y \u03bb2 \u20162, we can obtain\n\u2016\u03b8\u22172 \u2212 y\n\u03bb2 \u20162 \u2264 \u2016s\u2217\u03b8\u22171 \u2212\ny\n\u03bb2 \u20162, (45)\nwhich is the constraint used in Eq. (42). Therefore, the SAFE approach can be treated as generating the constraint for \u03b8\u22172 using the variational inequality in Eq. (12) followed by relaxations. Note that, the ball specified by Eq. (44) has half radius of the one by Eq. (45).\nAdding Eq. (13) and Eq. (14), we have\n\u3008 y \u03bb2 \u2212 y \u03bb1 ,\u03b8\u22172 \u2212 \u03b8\u22171\u3009 \u2265 \u3008\u03b8\u22172 \u2212 \u03b8\u22171 ,\u03b8\u22172 \u2212 \u03b8\u22171\u3009. (46)\nIncorporating \u3008 y\u03bb2 \u2212 y \u03bb1 ,\u03b8\u22172 \u2212 \u03b8\u22171\u3009 \u2264 \u2016 y \u03bb2 \u2212 y\u03bb1 \u20162\u2016\u03b8 \u2217 2 \u2212 \u03b8\u22171\u20162, we have\u2225\u2225\u2225\u2225 y\u03bb2 \u2212 y\u03bb1 \u2225\u2225\u2225\u2225 2 \u2265 \u2016\u03b8\u22172 \u2212 \u03b8\u22171\u20162 , (47)\nwhich is the one used in the DPP approach [3]. Therefore, although the authors in [3] motivates the DPP approach from the viewpoint of Euclidean projection, the DPP approach can indeed be treated as generating the constraint for \u03b8\u22172 using the variational inequality in Eq. (11) and Eq. (12) followed by relaxation. Note that, the ball specified by Eq. (46) has half radius of the one by Eq. (47)."}, {"heading": "2.4 Feature Sure Removal Parameter", "text": "We begin with the study on the monotone properties of the upper-bounds established in Theorem 2.\nFor the first case, the upper-bound of |xTj \u03b8\u22172 | is monotonely decreasing with regard to \u03bb2 as\n\u2016x\u22a5j \u20162\u2016y\u22a5\u20162 + \u3008x\u22a5j ,y\u22a5\u3009\nand \u2016x\u22a5j \u20162\u2016y\u22a5\u20162 \u2212 \u3008x\u22a5j ,y\u22a5\u3009\nare non-negative and independent of \u03bb2.\nFor the fourth case, we have \u03b8\u22171 = y \u03bb1 since a = 0. Thus b = ( y\u03bb2 \u2212 \u03b8 \u2217 1)/2 =\n1 \u03bb2 \u2212 1\u03bb2 2 y. As a result,\n\u2212\u3008xj ,b\u3009+ \u2016xj\u20162\u2016b\u20162 = 1 \u03bb2 \u2212 1\u03bb1 2 [\u2212\u3008xj ,y\u3009+ \u2016xj\u20162\u2016y\u20162]\n\u3008xj ,b\u3009+ \u2016xj\u20162\u2016b\u20162 = 1 \u03bb2 \u2212 1\u03bb1 2 [\u3008xj ,y\u3009+ \u2016xj\u20162\u2016y\u20162]\nare monotonely decreasing with regard to \u03bb2. For the second and the third cases, the vector b is dependent on \u03bb2 and the upper-bounds are not necessarily monotone with regard to \u03bb2. This indicates that if \u03bb12 > \u03bb 2 2 the obtained upper-bound for \u03bb 1 2 might be less than that for \u03bb22. This somehow coincides with a property of the solution of LASSO, i.e., a feature that is absent for a given regularization parameter might be active for a smaller regularization parameter. Our following analysis focuses on the monotone properties of the established upper-bounds.\nLemma 2 Let y 6= 0, and \u2016XTy\u2016\u221e > \u03bb1 \u2265 \u03bb > 0. Denote\nf(\u03bb) = \u3008(y\u03bb \u2212 \u03b8 \u2217 1)/2,a\u3009\n\u2016(y\u03bb \u2212 \u03b8 \u2217 1)/2\u20162\u2016a\u20162\n(48)\ng(\u03bb) = \u3008(y\u03bb \u2212 \u03b8 \u2217 1)/2,y\u3009\n\u2016(y\u03bb \u2212 \u03b8 \u2217 1)/2\u20162\u2016y\u20162\n(49)\nf(\u03bb) is strictly increasing with regard to \u03bb in (0, \u03bb1]. g(\u03bb) is strictly decreasing with regard to \u03bb in (0, \u03bb1].\nThe main results are provided in the following theorem.\nTheorem 3 Let y 6= 0, and \u2016XTy\u2016\u221e \u2265 \u03bb1 > \u03bb2 > 0. Define \u03bb2a as\n\u2022 If \u03bb1 = \u2016XTy\u2016\u221e or if 0 < \u03bb1 < \u2016XTy\u2016\u221e and \u3008y,a\u3009\u2016y\u20162\u2016a\u20162 \u2265 |\u3008xj ,a\u3009| \u2016xj\u20162\u2016a\u20162 , then\nlet \u03bb2a = 0.\n\u2022 If 0 < \u03bb1 < \u2016XTy\u2016\u221e and \u3008y,a\u3009\u2016y\u20162\u2016a\u20162 < |\u3008xj ,a\u3009| \u2016xj\u20162\u2016a\u20162 , then let \u03bb2a be the unique\nvalue in (0, \u03bb1] that satisfies f(\u03bb2a) = |\u3008xj ,a\u3009| \u2016xj\u20162\u2016a\u20162 .\nDefine \u03bb2y as\n\u2022 If \u03bb1 = \u2016XTy\u2016\u221e or if 0 < \u03bb1 < \u2016XTy\u2016\u221e and \u3008y,a\u3009\u2016y\u20162\u2016a\u20162 \u2265 |\u3008xj ,y\u3009| \u2016xj\u20162\u2016a\u20162 , then\nlet \u03bb2y = \u03bb1.\n\u2022 If 0 < \u03bb1 < \u2016XTy\u2016\u221e and \u3008y,a\u3009\u2016y\u20162\u2016a\u20162 < |\u3008xj ,y\u3009| \u2016xj\u20162\u2016a\u20162 , then let \u03bb2y be the unique\nvalue in (0, \u03bb1] that satisfies g(\u03bb2y) = |\u3008xj ,y\u3009| \u2016xj\u20162\u2016y\u20162 .\nWe have the following monotone properties on the upper-bounds:\n1. If \u3008xj ,a\u3009 > 0 or if \u3008xj ,a\u3009 \u2264 0 and \u03bb2a \u2264 \u03bb2y, then the established upperbound of \u3008xj ,\u03b8\u22172\u3009 is monotonically decreasing with regard to \u03bb2 in (0, \u03bb1).\n2. If \u3008xj ,a\u3009 < 0 or if \u3008xj ,a\u3009 \u2265 0 and \u03bb2a \u2264 \u03bb2y, then the established upperbound of \u2212\u3008xj ,\u03b8\u22172\u3009 is monotonically decreasing with regard to \u03bb2 in (0, \u03bb1).\n3. If \u3008xj ,a\u3009 \u2264 0 and \u03bb2a > \u03bb2y, then the established upper-bound of \u3008xj ,\u03b8\u22172\u3009 is monotonically decreasing with regard to \u03bb2 in (0, \u03bb2y) and [\u03bb2a, \u03bb1), but monotonically increasing with regard to \u03bb2 in (\u03bb2y, \u03bb2a).\n4. If \u3008xj ,a\u3009 \u2265 0 and \u03bb2a > \u03bb2y, then the established upper-bound of \u2212\u3008xj ,\u03b8\u22172\u3009 is monotonically decreasing with regard to \u03bb2 in (0, \u03bb2y) and [\u03bb2a, \u03bb1), but monotonically increasing with regard to \u03bb2 in (\u03bb2y, \u03bb2a).\nFrom Theorem 3, one can identify the regularization parameter at which a feature is surely removed from the solution."}, {"heading": "3 Extension to Logistic Regression", "text": "The sparse logistic regression can be written as:\nmin \u03b2 m\u2211 i=1 log(1 + exp(\u2212yi\u03b2Txi)) + \u03bb\u2016\u03b2\u20161. (50)\nLet ui = \u03b2 Txi, we have\nmin \u03b2,u:ui=\u03b2Txi m\u2211 i=1 log(1 + exp(\u2212yiui)) + \u03bb\u2016\u03b2\u20161. (51)\nIntroducing the dual variable z for the equality constraints ui = \u03b2 Txi, i =\n1, 2, . . . ,m, we have\nmax z min \u03b2,u m\u2211 i=1 log(1 + exp(\u2212yiui)) + \u03bb\u2016\u03b2\u20161 + m\u2211 i=1 zi(ui \u2212 \u03b2Txi)\n= max z min u min \u03b2 { m\u2211 i=1 log(1 + exp(\u2212yiui)) + m\u2211 i=1 ziui + \u03bb\u2016\u03b2\u20161 \u2212 \u03b2T m\u2211 i=1 zixi }\n= max z min u { m\u2211 i=1 log(1 + exp(\u2212yiui)) + m\u2211 i=1 ziui + min \u03b2 (\u03bb\u2016\u03b2\u20161 \u2212 \u03b2T m\u2211 i=1 zixi) }\n= max z:\u2016 \u2211 i zixi\u2016\u221e\u2264\u03bb min u m\u2211 i=1 log(1 + exp(\u2212yiui)) + m\u2211 i=1 ziui\n= max z:\u2016 \u2211 i zixi\u2016\u221e\u2264\u03bb m\u2211 i=1 ( log( yi yi \u2212 zi ) + zi yi log( yi \u2212 zi zi ) ) .\n(52)\nLet \u03b8i = zi \u03bb . The dual problem of Eq. (50) can be written as\nmin \u03b8:\u2016XT \u03b8\u2016\u221e\u22641 \u2212 m\u2211 i=1 ( log( yi/\u03bb yi/\u03bb\u2212 \u03b8i ) + \u03b8i yi/\u03bb log( yi/\u03bb\u2212 \u03b8i \u03b8i ) ) (53)\nLet\nf(\u03b8; y\n\u03bb ) = \u2212 m\u2211 i=1 ( log( yi/\u03bb yi/\u03bb\u2212 \u03b8i ) + \u03b8i yi/\u03bb log( yi/\u03bb\u2212 \u03b8i \u03b8i ) ) (54)\nIt can be easily verified that the function f(\u03b8; y\u03bb ) is convex with regard to \u03b8. Let \u03b2\u2217 and \u03b8\u2217 be the primal and dual optimals, respectively. If |(XT\u03b8\u2217)i| < 1 then \u03b2\u2217i = 0.\nThe derivative with regard to \u03b8 can be computed as\n\u2202f(\u03b8; y\u03bb )\n\u2202\u03b8i = \u2212 1 yi/\u03bb log( yi/\u03bb\u2212 \u03b8i \u03b8i ) (55)\nAccording to Lemma 1, the optimality condition via the variational inequality is:\nm\u2211 i=1 1 yi/\u03bb log( yi/\u03bb\u2212 \u03b8\u2217i \u03b8\u2217i )(\u03b8i \u2212 \u03b8\u2217i ) \u2264 0,\u2200\u03b8 : \u2016XT\u03b8\u2016\u221e \u2264 1. (56)\nLet \u03b8\u22171 : \u03b8 \u2217 i,1, i = 1, 2, . . . ,m be the optimal solution corresponding to \u03bb1.\nThe variational inequality that ensures the optimality of \u03b8\u22171 is:\nm\u2211 i=1 1 yi/\u03bb1 log( yi/\u03bb1 \u2212 \u03b8\u2217i,1 \u03b8\u2217i,1 )(\u03b8i \u2212 \u03b8\u2217i,1) \u2264 0,\u2200\u03b8 : \u2016XT\u03b8\u2016\u221e \u2264 1. (57)\nLet \u03b8\u22172 : \u03b8 \u2217 i,2, i = 1, 2, . . . ,m be the optimal solution corresponding to \u03bb2.\nThe variational inequality that ensures the optimality of \u03b8\u22172 is:\nm\u2211 i=1 1 yi/\u03bb2 log( yi/\u03bb2 \u2212 \u03b8\u2217i,2 \u03b8\u2217i,2 )(\u03b8i \u2212 \u03b8\u2217i,2) \u2264 0,\u2200\u03b8 : \u2016XT\u03b8\u2016\u221e \u2264 1. (58)\nOne can follow the discussion for the LASSO approach in estimating the upper-bound of \u3008xj ,\u03b8\u22172\u3009 by\nmax \u03b8 |\u3008xj ,\u03b8\u3009|\nsubjet to m\u2211 i=1 1 yi/\u03bb1 log( yi/\u03bb1 \u2212 \u03b8\u2217i,1 \u03b8\u2217i,1 )(\u03b8i \u2212 \u03b8\u2217i,1) \u2264 0\nm\u2211 i=1 1 yi/\u03bb2 log( yi/\u03bb2 \u2212 \u03b8i \u03b8i )(\u03b8\u2217i,1 \u2212 \u03b8i) \u2264 0.\n(59)\nIn Eq. (59), the first constraint is a half space, and the second constraint corresponds to a closed convex set. For sake of computational convenience, one may relax the second inequality, and we leave the derivation of the screening rule based on Eq. (59) to a future work."}, {"heading": "4 Experiment", "text": "In this section, we conduct experiments to evaluate the performance of the proposed screening method, that is, Sasvi, on synthetic data sets. We compare the performance of Sasvi with the sequential SAFE rule [1] and the sequential DPP [3] which achieve the state-of-the-art performance for the Lasso problems. Notice that, all of the three methods are \u201csafe\u201d in the sense that the discarded features are guaranteed to have 0 coefficients in the true solution.\nTo measure the performance of the screening methods, we use the rejection ratio, i.e., the ratio between the number of features discarded by the screening methods and the number of features with 0 coefficients in the true solution. We report the rejection ratio of Sasvi, SAFE and DPP along a sequence of 91 tuning parameter values equally spaced on the scale of \u03bb/\u03bbmax from 1.0 to 0.1. The general experiment settings are as follows. The entries of the response vector y and the data matrix X are generated i.i.d. from a standard Gaussian distribution, and the correlation between y and each feature xk is uniformly distributed on the interval [\u2212c, c], where c \u2208 {0.3, 0.5, 0.8, 1.0}. To reduce the variability, we repeat the computation 20 times and report the average results for each experiment.\nFrom the practical point of view, people are usually more interested in the computational gain resulting from the new methods, i.e., how much speedup can we get via the new methods? To answer this question, we first apply the solver (SLEP1) with wart-start to solve the Lasso problem along the given 91 parameters and report the total running time. Then, for each parameter, we first use the screening methods to identify the inactive features and then apply the solver to solve the Lasso problem with the reduced data matrix. We repeat this process until all of the problems are solved and report the total running time (including screening). To demonstrate the efficiency, we also report the total running time of of the screening methods in discarding the inactive features for the Lasso problem with the given sequence of parameters.\nFigure 3 presents the rejection ratio of Sasvi, SAFE and DPP along the 91 parameters. We can see that the three screening methods are able to discard more inactive features when the value of c increases, i.e., the features become more correlated with the response vector. In all of the cases, Sasvi significantly outperforms SAFE and DPP in discarding inactive features for the Lasso problem.\nTable 1 demonstrates that Sasvi leads to the most computational gain among DPP and SAFE. The speedup of SLEP+Sasvi is approximately 5, 8, 12, 20 times compared with that of SLEP. Although Sasvi is the most expensive screening methods among DPP and SAFE, it is able to discard far more inactive features, leading to a greatly reduced data matrix which needs to be entered the optimization. As a result, Sasvi is the most effective screening method among DPP and SAFE in accelerating the computation.\n1http://www.public.asu.edu/~jye02/Software/SLEP/"}, {"heading": "5 Conclusion", "text": "In this note, we propose an approach called Sasvi (Safe screening with variational inequalities). Sasvi makes use of the variational inequalities which provide the sufficient and necessary optimality conditions for the dual problem. The existing approaches can be casted as relaxed versions of the proposed Sasvi, and thus Sasvi provides a much stronger screening rule. The monotone properties of Sasvi are studied based on a sure removal regularization parameter which can be identified for each feature. In adddition, the proposed Sasvi is readily extended for solving the generalized sparse linear models."}, {"heading": "6 Appendix", "text": ""}, {"heading": "6.1 Proof of Theorem 1", "text": "We begin with two technical lemmas.\nLemma 3 Let y 6= 0. If 0 < \u03bb1 \u2264 \u2016XTy\u2016\u221e, we have\n\u3008 y \u03bb1 \u2212 \u03b8\u22171 ,\u03b8\u22171\u3009 \u2265 0. (60)\nProof Since the Euclidean projection of y\u03bb1 onto {\u03b8 : \u2016X T\u03b8\u2016\u221e \u2264 1} is \u03b8\u22171 , it follows from Lemma 1 that\n\u3008\u03b8\u22171 \u2212 y\n\u03bb1 ,\u03b8 \u2212 \u03b8\u22171\u3009 \u2265 0,\u2200\u03b8 : \u2016XT\u03b8\u2016\u221e \u2264 1. (61)\nAs 0 \u2208 {\u03b8 : \u2016XT\u03b8\u2016\u221e \u2264 1}, we have Eq. (60).\nLemma 4 Let y 6= 0. If 0 < \u03bb1 \u2264 \u2016XTy\u2016\u221e, we have\n\u3008 y \u03bb1 \u2212 \u03b8\u22171 ,y\u3009 \u2265 0, (62)\nwhere the equality holds if and only if \u03bb1 = \u2016XTy\u2016\u221e.\nProof We have\n\u3008 y \u03bb1 \u2212 \u03b8\u22171 , y \u03bb1 \u3009 \u2212 \u3008 y \u03bb1 \u2212 \u03b8\u22171 ,\u03b8\u22171\u3009 = \u3008 y \u03bb1 \u2212 \u03b8\u22171 , y \u03bb1 \u2212 \u03b8\u22171\u3009 \u2265 0, (63)\nwhere the equality holds if and only if y\u03bb1 = \u03b8 \u2217 1 . Incorporating Eq. (60) in Lemma 3 and Eq. (63), we have Eq. (62). The equality holds if and only if y \u03bb1\n= \u03b8\u22171 or equivlently \u03bb1 = |XTy\u2016\u221e. Now, we are ready to prove Theorem 1. If follows from Eq. (16) and Eq. (62)\n\u3008b,a\u3009 = 1 \u03bb2 \u2212 1\u03bb1 2 \u3008( y \u03bb1 \u2212 \u03b8\u22171)/2,y\u3009+ \u2016( y \u03bb1 \u2212 \u03b8\u22171)/2\u20162 \u2265 0. (64)\nIt is clear that the equality holds if and only if y\u03bb1 = \u03b8 \u2217 1 or equivlently \u03bb1 = |XTy\u2016\u221e. In other words, if \u03bb1 < |XTy\u2016\u221e then \u3008b,a\u3009 > 0, which leads to b 6= 0 and a 6= 0."}, {"heading": "6.2 Proof of Theorem 2", "text": "We prove the four cases one by one as follows.\nIf \u3008b,a\u3009\u2016b\u20162 > |\u3008xj ,a\u3009| \u2016xj\u20162 , i.e., Eq. (30) does not hold with x = \u00b1xj , we have b 6= 0\nand a 6= 0, and\n\u3008xj ,\u03b8\u22172\u3009 \u2264 max \u03b8:\u3008\u03b8\u22171\u2212 y \u03bb1 ,\u03b8\u2212\u03b8\u22171 \u3009\u22650,\u3008\u03b8\u2212 y \u03bb2 ,\u03b8\u22171\u2212\u03b8\u3009\u22650 \u3008xj ,\u03b8\u3009\n= max r:\u3008a,r+b\u3009\u22640,\u2016r\u201622\u2264\u2016b\u201622\n\u3008xj , \u03b8\u22171 + y \u03bb2\n2 \u3009+ \u3008xj , r\u3009\n= \u3008xj , \u03b8\u22171 + y \u03bb2\n2 \u3009+ max r:\u3008a,r+b\u3009\u22640,\u2016r\u201622\u2264\u2016b\u201622 \u3008xj , r\u3009\n= \u3008xj , \u03b8\u22171 + y \u03bb2\n2 \u3009 \u2212 min r:\u3008a,r+b\u3009\u22640,\u2016r\u201622\u2264\u2016b\u201622 \u3008\u2212xj , r\u3009\n\u2264 \u3008xj ,a\u3009+ \u3008xj ,\u03b8\u22171\u3009+ 1 \u03bb2 \u2212 1\u03bb1 2 \u3008xj ,y\u3009\n+ \u221a (\u2016 \u2212 xj\u201622 \u2212\n\u3008\u2212xj ,a\u30092 \u2016a\u201622 )(\u2016b\u201622 \u2212 \u3008b,a\u30092 \u2016a\u201622 ) + \u3008a,b\u3009\u3008\u2212xj ,a\u3009 \u2016a\u201622\n= \u3008xj ,\u03b8\u22171\u3009+ 1 \u03bb2 \u2212 1\u03bb1 2 [\u3008xj ,y\u3009 \u2212 \u3008a,y\u3009 \u2016a\u201622 \u3008xj ,a\u3009] + 1 \u03bb2 \u2212 1\u03bb1 2 \u221a (\u2016xj\u201622 \u2212 \u3008xj ,a\u30092 \u2016a\u201622 )(\u2016y\u201622 \u2212 \u3008y,a\u30092 \u2016a\u201622 ).\n(65)\nThe first inequality follows from the fact that \u03b8\u22172 satisfies the constraints in Eq. (15). The first equality follows from rewriting Eq. (15) as Eq. (18). The last inequality utilies Eq. (35) which is the result for the case \u3008b,a\u3009\u2016b\u20162 > |\u3008xj ,a\u3009| \u2016xj\u20162 > \u3008\u2212xj ,a\u3009 \u2016xj\u20162 by setting x = \u2212xj . The last equality utlizes\n\u2016b\u201622 \u2212 \u3008b,a\u30092\n\u2016a\u201622 = ( 1 \u03bb2 \u2212 1\u03bb1 2 )2(\u2016y\u201622 \u2212 \u3008y,a\u30092 \u2016a\u201622 ) (66)\nand\n\u3008a,b\u3009\u3008xj ,a\u3009 \u2016a\u201622 = \u3008xj ,a\u3009(1 + \u3008a,y\u3009\n1 \u03bb2 \u2212 1\u03bb1 2\n\u2016a\u201622 ), (67)\nwhich can be derived from Eq. (16). Following a similar derivation, we have\n\u3008\u2212xj ,\u03b8\u22172\u3009 \u2264 max \u03b8:\u3008\u03b8\u22171\u2212 y \u03bb1 ,\u03b8\u2212\u03b8\u22171 \u3009\u22650,\u3008\u03b8\u2212 y \u03bb2 ,\u03b8\u22171\u2212\u03b8\u3009\u22650 \u3008\u2212xj ,\u03b8\u3009\n= max r:\u3008a,r+b\u3009\u22640,\u2016r\u201622\u2264\u2016b\u201622\n\u3008\u2212xj , \u03b8\u22171 + y \u03bb2\n2 \u3009+ \u3008\u2212xj , r\u3009\n= \u3008\u2212xj , \u03b8\u22171 + y \u03bb2\n2 \u3009+ max r:\u3008a,r+b\u3009\u22640,\u2016r\u201622\u2264\u2016b\u201622 \u3008\u2212xj , r\u3009\n= \u3008\u2212xj , \u03b8\u22171 + y \u03bb2\n2 \u3009 \u2212 min r:\u3008a,r+b\u3009\u22640,\u2016r\u201622\u2264\u2016b\u201622 \u3008xj , r\u3009\n\u2264 \u3008\u2212xj ,a\u3009+ \u3008\u2212xj ,\u03b8\u22171\u3009+ 1 \u03bb2 \u2212 1\u03bb1 2 \u3008\u2212xj ,y\u3009\n+ \u221a (\u2016xj\u201622 \u2212\n\u3008xj ,a\u30092 \u2016a\u201622 )(\u2016b\u201622 \u2212 \u3008b,a\u30092 \u2016a\u201622 ) + \u3008a,b\u3009\u3008xj ,a\u3009 \u2016a\u201622\n= \u2212\u3008xj ,\u03b8\u22171\u3009 \u2212 1 \u03bb2 \u2212 1\u03bb1 2 [\u3008xj ,y\u3009 \u2212 \u3008a,y\u3009 \u2016a\u201622 \u3008xj ,a\u3009] + 1 \u03bb2 \u2212 1\u03bb1 2 \u221a (\u2016xj\u201622 \u2212 \u3008xj ,a\u30092 \u2016a\u201622 )(\u2016y\u201622 \u2212 \u3008y,a\u30092 \u2016a\u201622 ).\n(68)\nThe last inequality utilies Eq. (35) which is the result for the case \u3008b,a\u3009\u2016b\u20162 > |\u3008xj ,a\u3009| \u2016xj\u20162 > \u3008xj ,a\u3009 \u2016xj\u20162 by setting x = xj .\nIf \u3008b,a\u3009\u2016b\u20162 \u2264 \u3008xj ,a\u3009 \u2016xj\u20162 and \u3008xj ,a\u3009 > 0, we have \u3008b,a\u3009 \u2016b\u20162 > \u3008\u2212xj ,a\u3009 \u2016xj\u20162 since \u3008b,a\u3009 \u2265 0 according to Theorem 1. Thus, Eq. (30) does not hold with x = \u2212xj , and we can get Eq. (65) for bounding \u3008xj ,\u03b8\u22172\u3009. In addition, Eq. (30) holds with x = xj , and we have\n\u3008\u2212xj ,\u03b8\u22172\u3009 \u2264 max \u03b8:\u3008\u03b8\u22171\u2212 y \u03bb1 ,\u03b8\u2212\u03b8\u22171 \u3009\u22650,\u3008\u03b8\u2212 y \u03bb2 ,\u03b8\u22171\u2212\u03b8\u3009\u22650 \u3008\u2212xj ,\u03b8\u3009\n= max r:\u3008a,r+b\u3009\u22640,\u2016r\u201622\u2264\u2016b\u201622\n\u3008\u2212xj , \u03b8\u22171 + y \u03bb2\n2 \u3009+ \u3008\u2212xj , r\u3009\n= \u3008\u2212xj , \u03b8\u22171 + y \u03bb2\n2 \u3009+ max r:\u3008a,r+b\u3009\u22640,\u2016r\u201622\u2264\u2016b\u201622 \u3008\u2212xj , r\u3009\n= \u3008\u2212xj , \u03b8\u22171 + y \u03bb2\n2 \u3009 \u2212 min r:\u3008a,r+b\u3009\u22640,\u2016r\u201622\u2264\u2016b\u201622 \u3008xj , r\u3009\n\u2264 \u3008\u2212xj ,\u03b8\u22171\u3009+ \u3008\u2212xj ,b\u3009+ \u2016xj\u20162\u2016b\u20162 = \u2212\u3008xj ,\u03b8\u22171\u3009 \u2212 \u3008xj ,b\u3009+ \u2016xj\u20162\u2016b\u20162.\n(69)\nThe last inequality utilizes Eq. (31) with x = xj .\nIf \u3008b,a\u3009\u2016b\u20162 \u2264 \u2212\u3008xj ,a\u3009 \u2016xj\u20162 and \u3008xj ,a\u3009 < 0, Eq. (30) holds with x = \u2212xj , and we have\n\u3008xj ,\u03b8\u22172\u3009 \u2264 max \u03b8:\u3008\u03b8\u22171\u2212 y \u03bb1 ,\u03b8\u2212\u03b8\u22171 \u3009\u22650,\u3008\u03b8\u2212 y \u03bb2 ,\u03b8\u22171\u2212\u03b8\u3009\u22650 \u3008xj ,\u03b8\u3009\n= max r:\u3008a,r+b\u3009\u22640,\u2016r\u201622\u2264\u2016b\u201622\n\u3008xj , \u03b8\u22171 + y \u03bb2\n2 \u3009+ \u3008xj , r\u3009\n= \u3008xj , \u03b8\u22171 + y \u03bb2\n2 \u3009+ max r:\u3008a,r+b\u3009\u22640,\u2016r\u201622\u2264\u2016b\u201622 \u3008xj , r\u3009\n= \u3008xj , \u03b8\u22171 + y \u03bb2\n2 \u3009 \u2212 min r:\u3008a,r+b\u3009\u22640,\u2016r\u201622\u2264\u2016b\u201622 \u3008\u2212xj , r\u3009\n\u2264 \u3008xj ,\u03b8\u22171\u3009+ \u3008xj ,b\u3009+ \u2016 \u2212 xj\u20162\u2016b\u20162 = \u3008xj ,\u03b8\u22171\u3009+ \u3008xj ,b\u3009+ \u2016xj\u20162\u2016b\u20162,\n(70)\nwhere the last inequality utilizes Eq. (31) with x = \u2212xj . In addition, we have \u3008b,a\u3009 \u2016b\u20162 > \u3008xj ,a\u3009 \u2016xj\u20162 since \u3008b,a\u3009 \u2265 0 according to Theorem 1 and \u3008xj ,a\u3009 < 0. Thus, Eq. (30) does not hold with x = xj , and we can get Eq. (68) for bounding \u3008xj ,\u03b8\u22172\u3009.\nIf a = 0, then Eq. (30) holds with x = \u00b1xj , and we can get Eq. (69) and Eq. (70).\nThis ends the proof of this theorem."}], "references": [{"title": "Safe feature elimination in sparse supervised learning", "author": ["Laurent El Ghaoui", "Vivian Viallon", "Tarek Rabbani"], "venue": "Pacific Journal of Optimization,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Strong rules for discarding predictors in lasso-type problems", "author": ["Robert Tibshirani", "Jacob Bien", "Jerome Friedman", "Trevor Hastie", "Noah Simon", "Jonathan Taylor", "Ryan J. Tibshirani"], "venue": "Journal of the Royal Statistical Society: Series B,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Lasso screening rules via dual polytope projection", "author": ["Jie Wang", "Binbin Lin", "Pinghua Gong", "Peter Wonka", "Jieping Ye"], "venue": "Technical report,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Learning sparse representations of high dimensional data on large scale dictionaries", "author": ["Zhen James Xiang", "Hao Xu", "Peter J. Ramadge"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}], "referenceMentions": [{"referenceID": 1, "context": "The strong rules proposed in [2] works very well in practice although they might mistakenly discard active features.", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "To remedy this, safe screening techniques [1, 3, 4], which elliminates the features that are guaranteed to be absent for a target regularization parameter based on an existing solution, have been proposed.", "startOffset": 42, "endOffset": 51}, {"referenceID": 2, "context": "To remedy this, safe screening techniques [1, 3, 4], which elliminates the features that are guaranteed to be absent for a target regularization parameter based on an existing solution, have been proposed.", "startOffset": 42, "endOffset": 51}, {"referenceID": 3, "context": "To remedy this, safe screening techniques [1, 3, 4], which elliminates the features that are guaranteed to be absent for a target regularization parameter based on an existing solution, have been proposed.", "startOffset": 42, "endOffset": 51}, {"referenceID": 0, "context": "3 Relationship to Existing Approaches Figure 2 compares the Sasvi approach with the SAFE approach [1] and the DPP approach [3].", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "3 Relationship to Existing Approaches Figure 2 compares the Sasvi approach with the SAFE approach [1] and the DPP approach [3].", "startOffset": 123, "endOffset": 126}, {"referenceID": 0, "context": "The constraint for \u03b8\u2217 2 used by the SAFE [1] approach is the ball centered at C with radius being the smallest distance from C to the points in the line segment EG.", "startOffset": 41, "endOffset": 44}, {"referenceID": 2, "context": "The constraint for \u03b8\u2217 2 used by the DPP [3] approach is the ball centered at E with radius BC.", "startOffset": 40, "endOffset": 43}, {"referenceID": 2, "context": "which is the one used in the DPP approach [3].", "startOffset": 42, "endOffset": 45}, {"referenceID": 2, "context": "Therefore, although the authors in [3] motivates the DPP approach from the viewpoint of Euclidean projection, the DPP approach can indeed be treated as generating the constraint for \u03b8\u2217 2 using the variational inequality in Eq.", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "We compare the performance of Sasvi with the sequential SAFE rule [1] and the sequential DPP [3] which achieve the state-of-the-art performance for the Lasso problems.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "We compare the performance of Sasvi with the sequential SAFE rule [1] and the sequential DPP [3] which achieve the state-of-the-art performance for the Lasso problems.", "startOffset": 93, "endOffset": 96}], "year": 2017, "abstractText": "The model selected by sparse learning techniques usually has a few non-zero entries. Safe screening\u2014which eliminates the features that are guaranteed to be absent for a target regularization parameter based on an existing solution\u2014is a technique for improving the computational efficiency while maintaining the same solution. In this note, we propose an approach called Sasvi (Safe screening with variational inequalities) and take LASSO for example in the analysis. Sasvi makes use of the variational inequalities which provide the sufficient and necessary optimality conditions for the dual problem. The existing approaches can be casted as relaxed versions of the proposed Sasvi, and thus Sasvi provides a much stronger screening rule. The monotone properties of Sasvi are studied based on which a sure removal regularization parameter can be identified for each feature. In addition, the proposed Sasvi is readily extended for solving the generalized sparse linear models. Preliminary experimental results are reported.", "creator": "LaTeX with hyperref package"}}}