{"id": "1310.4227", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2013", "title": "On Measure Concentration of Random Maximum A-Posteriori Perturbations", "abstract": "The maximum a-posteriori (MAP) perturbation framework has emerged as a useful approach for inference and learning in high dimensional complex models. By maximizing a randomly perturbed potential function, MAP perturbations generate unbiased samples from the Gibbs distribution. Unfortunately, the computational cost of generating so many high-dimensional random variables can be prohibitive. More efficient algorithms use sequential sampling strategies based on the expected value of low dimensional MAP perturbations. This paper develops new measure concentration inequalities that bound the number of samples needed to estimate such expected values. Applying the general result to MAP perturbations can yield a more efficient algorithm to approximate sampling from the Gibbs distribution. The measure concentration result is of general interest and may be applicable to other areas involving expected estimations.", "histories": [["v1", "Tue, 15 Oct 2013 23:30:52 GMT  (64kb,D)", "http://arxiv.org/abs/1310.4227v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.PR", "authors": ["francesco orabona", "tamir hazan", "anand d sarwate", "tommi s jaakkola"], "accepted": true, "id": "1310.4227"}, "pdf": {"name": "1310.4227.pdf", "metadata": {"source": "CRF", "title": "On Measure Concentration of Random Maximum A-Posteriori Perturbations", "authors": ["Francesco Orabona", "Tamir Hazan", "Anand D. Sarwate", "Tommi Jaakkola"], "emails": ["orabona@ttic.edu.", "tamir.hazan@gmail.com.", "asarwate@ttic.edu.", "tommi@csail.mit.edu."], "sections": [{"heading": "1 Introduction", "text": "Modern machine learning tasks in computer vision, natural language processing, and computational biology involve inference in high-dimensional complex models. Examples include scene understanding [Felzenszwalb and Zabih, 2011], parsing [Koo et al., 2010], and protein design [Sontag et al., 2008]. In these settings inference involves finding likely structures that fit the data, such as objects in images, parsers in sentences, or molecular configurations in proteins. Each structure corresponds to an assignment of values to random variables and the likelihood of an assignment is based on defining potential functions that account for interactions over these variables. Given the observed data, these likelihoods yield a posterior probability distribution on assignments known as the Gibbs distribution. Contemporary practice gives rise to posterior probabilities that consider potential influence of the data on the variables of the model (high signal) as well as human knowledge about the potential interactions between these variables (high coupling). The resulting posterior probability landscape is often \u201cragged\u201d; in such landscapes Markov chain Monte Carlo (MCMC) approaches to sampling from the Gibbs distribution may become prohibitively expensive. This is in contrast to\n\u2217FO, TH, and ADS contributed equally to this paper. \u2020Toyota Technological Institute at Chicago, Chicago, IL, USA, orabona@ttic.edu. \u2021Department of Computer Science, University of Haifa, Haifa, Israel, and Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA, tamir.hazan@gmail.com. \u00a7Toyota Technological Institute at Chicago, Chicago, IL, USA, asarwate@ttic.edu. \u00b6Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA, tommi@csail.mit.edu.\nar X\niv :1\n31 0.\n42 27\nv1 [\ncs .L\nG ]\n1 5\nO ct\n2 01\nthe success of MCMC approaches in other settings (e.g., Jerrum et al. [2004], Huber [2003]) where no data term (signal) exists.\nOne way around the difficulties of sampling from the Gibbs distribution is to look for the maximum a posteriori probability (MAP) structure. Substantial effort has gone into developing algorithms for recovering MAP assignments by exploiting domain-specific structural restrictions such as super-modularity [Kolmogorov, 2006] or by linear programming relaxations such as cuttingplanes [Sontag et al., 2008, Werner, 2008]. A drawback of MAP inference is that it returns a single assignment; in many contemporary models with complex potential functions on many variables, there are several likely structures, which makes MAP inference less appealing. We would like to also find these other \u201chighly probable\u201d assignments.\nRecent work has sought to leverage the current efficiency of MAP solvers to build procedures to sample from the Gibbs distribution, thereby avoiding the computational burden of MCMC methods. These works calculate the MAP structure of a randomly perturbed potential function. Such an approach effectively ignores the raggedness of the landscape that hinders MCMC. Papandreou and Yuille [2011] and Tarlow et al. [2012] have shown that randomly perturbing the potential of each structure with an independent random variable that follows the Gumbel distribution and finding the MAP assignment of the perturbed potential function provides an unbiased sample from the Gibbs distribution. Unfortunately the total number of structures, and consequently the total number of random perturbations, is exponential in the structure\u2019s dimension. Alternatively, Hazan et al. [2013] use expectation bounds on the partition function [Hazan and Jaakkola, 2012] to build a sampler for Gibbs distribution using MAP solvers on low dimensional perturbations which are only linear in the dimension of the structures.\nThe samplers based on low dimensional perturbations involve calculating expectations of the value of the MAP solution after perturbations. In this paper we give a statistical characterization of this value. In particular, we prove new measure concentration inequalities that show the expected perturbed MAP value can be estimated with high probability using only a few random samples. This is an important ingredient to construct an alternative to MCMC in the data-knowledge domain that relies on MAP solvers. The key technical challenge comes from the fact that the perturbations are Gumbel random variables. Since the Gumbel distribution is continuous, the MAP value of the perturbed potential function is unbounded and standard approaches such as McDiarmid\u2019s inequality do not apply. Instead, we derive a new Poincare\u0301 inequality for the Gumbel distribution, as well as a modified logarithmic Sobolev inequality using the approach suggested by Bobkov and Ledoux [1997], as described in the monograph of Ledoux [2001]. These results, which are of general interest, also guarantee that the deviation of the sampled mean of random MAP perturbations from their expectation has an exponential decay."}, {"heading": "2 Problem statement", "text": "Notation: Boldface will denote tuples or vectors and calligraphic script sets. For a tuple x = (x1, x2, . . . , xn), let xj:k = (xj , xj+1, . . . , xk)."}, {"heading": "2.1 The MAP perturbation framework", "text": "Statistical inference problems involve reasoning about the states of discrete variables whose configurations (assignments of values) specify the discrete structures of interest. Suppose that our\nmodel has n variables x = (x1, x2, . . . , xn) where each xi taking values in a discrete set Xi. Let X = X1 \u00d7 X2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Xn so that x \u2208 X . Let Dom(\u03b8) \u2286 X be a subset of possible configurations and \u03b8 : X \u2192 R be a potential function that gives a score to an assignment or structure x, where \u03b8(x) = \u2212\u221e for x /\u2208 Dom(\u03b8). The potential function induces a probability distribution on configurations x via the Gibbs distribution:\np(x) \u2206 =\n1 Z exp(\u03b8(x)), (1)\nZ \u2206 = \u2211 x\u2208X exp(\u03b8(x)). (2)\nThe normalization constant Z is called the partition function. Sampling from (1) is often difficult because the sum in (2) involves an exponentially large number of terms (equal to the number of discrete structures). In many cases, computing the partition function is in the complexity class #P (e.g., Valiant [1979]).\nFinding the most likely assignment of values to variables is easier. As the Gibbs distribution is typically constructed given observed data, we call this the maximum a-posteriori (MAP) prediction. Maximizing (1):\nx\u0302MAP = argmax x\u2208X \u03b8(x). (3)\nThere are many good optimization algorithms for solving (3) in cases of practical interest. Although MAP prediction is still NP-hard in general, it is often simpler than sampling from the Gibbs distribution.\nHowever, there are often several values of x whose scores \u03b8(x) are close to \u03b8(x\u0302MAP), and we would like to recover those as well. As an alternative to MCMC methods for sampling from the Gibbs distribution in (1), we can draw samples by perturbing the potential function and solving the resulting MAP problem. The MAP perturbation approach adds a random function \u03b3 : X \u2192 R to the potential function in (1) and solves the resulting MAP problem:\nx\u0302R\u2212MAP = argmax x\u2208X\n{\u03b8(x) + \u03b3(x)} . (4)\nThe random function \u03b3(\u00b7) associates a random variable to each x \u2208 X . The simplest approach to designing a perturbation function is to associate an independent and identically distributed (i.i.d.) random variable \u03b3(x) for each x \u2208 X . We can find the distribution of the randomized MAP predictor in (4) when {\u03b3(x) : x \u2208 X} are i.i.d.; in particular, suppose each \u03b3(x) a Gumbel random variable with zero mean, variance \u03c02/6, and cumulative distribution function\nG(y) = exp(\u2212 exp(\u2212(y + c))), (5)\nwhere c \u2248 0.5772 is the Euler-Mascheroni constant. The following result characterizes the distribution of the randomized predictor x\u0302R\u2212MAP in (4).\nTheorem 1. Gumbel and Lieblein [1954] Let \u0393 = {\u03b3(x) : x \u2208 X} be a collection of i.i.d. Gumbel random variables whose distribution is given by (5). Then\nlogZ = E\u0393 [ max x\u2208X {\u03b8(x) + \u03b3(x)} ] , (6)\nexp(\u03b8(x\u0302))\nZ = P\u0393\n( x\u0302 = argmax\nx\u2208X {\u03b8(x) + \u03b3(x)}\n) .\nThe max-stability of the Gumbel distribution provides a straightforward approach to generate unbiased samples from the Gibbs distribution \u2013 simply generate the perturbations in \u0393 and solve the problem in (4). However, because \u0393 contains |X | i.i.d. random variables, this approach to inference has complexity which is exponential in n."}, {"heading": "2.2 Sampling from the Gibbs distribution using low dimensional perturbations", "text": "Sampling from the Gibbs distribution is inherently tied to estimating the partition function in (2). If we could compute Z exactly, then we could sample x1 with probability proportional to\u2211\nx2,...,xn exp(\u03b8(x)), and for each subsequent dimension i, sample xi with probability proportional to \u2211\nxi+1,...,xn exp(\u03b8(x)), yielding a Gibbs sampler. However, this involves computing the partition\nfunction, which is hard. Instead, Hazan et al. [2013] use the representation in (6) to derive a family of self-reducible upper bounds on Z and then use these upper bounds in an iterative algorithm that samples from the Gibbs distribution using low dimensional random MAP perturbations. This gives a method which has complexity linear in n.\nIn the following, instead of the |X | independent random variables in (4), we define the random function \u03b3(x) in (4) as the sum of independent random variables for each coordinate xi of x:\n\u03b3(x) = n\u2211 i=1 \u03b3i(xi).\nThis function involves generating \u2211n\ni=1 |Xi| random variables for each i and xi \u2208 Xi. Let \u0393 = n\u22c3 i=1 {\u03b3i(xi) : xi \u2208 Xi}\nbe a collection of \u2211\ni |Xi| i.i.d. Gumbel random variables with distribution (5). The sampling algorithm in Algorithm 1 uses these random perturbations to draw unbiased samples from the Gibbs distribution. For a fixed x1:(j\u22121) = (x1, . . . , xj\u22121), define\nVj = max xj:n \u03b8(x) + n\u2211 i=j \u03b3i(xi)  . (7) The sampler proceeds sequentially \u2013 for each j it constructs a distribution pj(\u00b7) on Xj \u222a{r}, where r indicates a \u201crestart\u201d and attempts to draw an assignment for xj . If it draws r then it starts over again from j = 1, and if it draws an element in Xj it fixes xj to that element and proceeds to j+ 1.\nImplementing Algorithm 1 requires estimating the expectations E\u0393[Vj ] in (7). In this paper we show how to estimate E\u0393[Vj ] and bound the error with high probability by taking the sample mean of M i.i.d. copies of Vj . Specifically, we show that the estimation error decays exponentially with M . To do this we derive a new measure concentration result by proving a modified logarithmic Sobolev inequality for the product of Gumbel random variables. To do so we derive a more general result \u2013 a Poincare\u0301 inequality for log-concave distributions that may not be log-strongly concave, i.e., for which the second derivative of the exponent is not bounded away from zero.\nAlgorithm 1 Sampling with low-dimensional random MAP perturbations from the Gibbs distribution [Hazan et al., 2013]\nIterate over j = 1, ..., n, while keeping fixed x1:(j\u22121)\n1. For each xj \u2208 Xj , set pj(xj) = exp(E\u0393[Vj+1])exp(E\u0393[Vj ]) , where Vj is given by (7)\n2. Set pj(r) = 1\u2212 \u2211\nxj\u2208Xj p(xj)\n3. Sample an element in Xj \u222a{r} according to pj(\u00b7). If r is sampled then reject and restart with j = 1. Otherwise, fix the sampled element xj and continue the iterations\nOutput: x = (x1, ..., xn)"}, {"heading": "2.3 Measure concentration", "text": "We can think of the maximum value of the perturbed MAP problem as a function of the associated perturbation variables \u0393 = {\u03b3i(xi) : i \u2208 [n], xi \u2208 Xi}. There are m \u2206 = |X1| + |X2| + \u00b7 \u00b7 \u00b7 + |Xn| i.i.d. random variables in \u0393. For practical purposes, e.g., to estimate the quality of the sampling algorithm in Algorithm 1, it is important to evaluate the deviation of its sampled mean from its expectation. For notational simplicity we would only describe the deviation of the maximum value of the perturbed MAP from its expectation, namely\nF (\u0393) = V1 \u2212 E [V1] . (8)\nSince the expectation is a linear function, E [F ] = \u222b F (\u0393)d\u00b5(\u0393) = 0 is zero, with respect to any measure \u00b5 on \u0393. The deviation of F (\u0393) is dominated by its moment generating function\n\u039b(\u03bb) \u2206 = E [exp(\u03bbF )] . (9)\nThat is, for every \u03bb > 0,\nP (F (\u03b3) \u2265 r) \u2264 \u039b(\u03bb)/ exp(\u2212\u03bbr).\nMany measure concentration results such as McDiarmid\u2019s inequality rely on bounds on the variation of F (\u0393). Unfortunately, this does not hold for MAP perturbations and instead we use the logSobolev approach bound (9). Specifically, we want to construct a differential bound on the \u03bb\u2212scaled cumulant generating function:\nH(\u03bb) \u2206 =\n1 \u03bb log \u039b(\u03bb). (10)\nFirst note that that by L\u2019Ho\u0302pital\u2019s rule H(0) = \u039b \u2032(0) \u039b(0) = \u222b Fd\u00b5n = 0, so we may represent H(\u03bb) by\nintegrating its derivative: H(\u03bb) = \u222b \u03bb\n0 H \u2032(\u03bb\u0302)d\u03bb\u0302. Thus to bound the moment generating function it\nsuffices to bound H \u2032(\u03bb) \u2264 \u03b1(\u03bb) for some function \u03b1(\u03bb). A direct computation of H \u2032(\u03bb) translates this bound to\n\u03bb\u039b\u2032(\u03bb)\u2212 \u039b(\u03bb) log \u039b(\u03bb) \u2264 \u03bb2\u039b(\u03bb)\u03b1(\u03bb). (11)\nThe left side of (11) turns out to be the so-called functional entropy Ledoux [2001] of the function h = exp(\u03bbF ) with respect to a measure \u00b5:\nEnt\u00b5(h) \u2206 = \u222b h log hd\u00b5\u2212 (\u222b hd\u00b5 ) log \u222b hd\u00b5.\nUnlike McDiarmid\u2019s inequality, this approach provides measure concentration for unbounded functions, such those arising from MAP perturbations.\nA log-Sobolev inequality upper-bounds the entropy Ent\u00b5(h) in terms of an integral involving \u2016\u2207F\u20162. They are appealing to derive measure concentration results in product spaces, i.e., for functions of subsets of variables \u0393, because it is sufficient to prove a log-Sobolev inequality on a single variable function f . Given such a scalar result, the additivity property of the entropy (e.g., [Boucheron et al., 2004]) extends the inequality to functions F of many variables. In this work we derive a log-Sobolev inequality for the Gumbel distribution, by bounding the variance of a function by its derivative:\nVar\u00b5(f) \u2206 = \u222b f2d\u00b5\u2212 (\u222b fd\u00b5 )2 \u2264 C \u222b |f \u2032|2d\u00b5. (12)\nThis is called a Poincare\u0301 inequality, proven originally for the Gaussian case. We prove such an inequality for the Gumbel distribution, which then implies the log-Sobolev inequality and hence measure concentration. We then apply the result to the MAP perturbation framework."}, {"heading": "2.4 Related work", "text": "We are interested in efficient sampling from the Gibbs distribution in (1) when n is large an the model is complex due to the amount of data and the domain-specific modeling. This is often done with MCMC (cf. Koller and Friedman [2009]), which may be challenging in ragged probability landscapes. MAP perturbations use efficient MAP solvers as black box, but the statistical properties of the solutions, beyond Theorem 1, are still being studied. Papandreou and Yuille [2011] consider probability models that are defined by the maximal argument of randomly perturbed potential function, while Tarlow et al. [2012] considers sampling techniques for such models and Keshet et al. [2011] explores the generalization bounds for such models. Rather than focus on the statistics of the solution (the argmaxx) we study statistical properties of the MAP value (the maxx) of the estimate in (4).\nHazan and Jaakkola [2012] used the random MAP perturbation framework to derive upper bounds on the partition function in (2), and Hazan et al. [2013] derived the unbiased sampler in Algorithm 1. Both of these approaches involve computing an expectation over the distribution of the MAP perturbation, which can be estimated by sample averages. This paper derives new measure concentration results that bound the error of this estimate in terms of the number of samples, making Algorithm 1 practical.\nMeasure concentration has appeared in many machine learning analyses, most commonly to bound the rate of convergence for risk minimization, either via empirical risk minimization (ERM) (e.g., Bartlett and Mendelson [2003]) or in PAC-Bayesian approaches (e.g., McAllester [2003]). In these applications the function for which we want to show concentration is \u201cwell-behaved\u201d in the sense that the underlying random variables are bounded or the function satisfies some boundeddifference or self-bounded conditions conditions, so measure concentration follows from inequalities\nsuch as Bernstein [1946], Azuma-Hoeffding [Azuma, 1967, Hoeffding, 1963, McDiarmid, 1989], or Bousquet [2003]. However, in our setting, the Gumbel random variables are not bounded, and random perturbations may result in unbounded changes of the perturbed MAP value.\nThere are several results on measure concentration for Lipschitz functions of Gaussian random variables (c.f. Maurey and Pisier [1986]). In this work we use logarithmic Sobolev inequalities Ledoux [2001] and prove a new measure concentration result for Gumbel random variables. To do this we generalize a classic result of Brascamp and Lieb [1976] on Poincare\u0301 inequalities to non-strongly log-concave distributions, and also recover the concentration result of Bobkov and Ledoux [1997] for functions of Laplace random variables."}, {"heading": "3 Concentration of measure", "text": "In this section we prove the main technical results of this paper \u2013 a new Poincare\u0301 inequality for log concave distributions and the corresponding measure concentration result. We will then specialize our result to the Gumbel distribution and apply it to the MAP perturbation framework. Because of the tensorization property of the functional entropy, it is sufficient for our case to prove an inequality like (12) for functions f of a single random variable with measure \u00b5."}, {"heading": "3.1 A Poincare\u0301 inequality for log-concave distributions", "text": "Our Theorem 2 in this section generalizes a celebrated result of Brascamp and Lieb [1976, Theorem 4.1] to a wider family of log-concave distributions and strictly improves their result. For an appropriately scaled convex function Q on R, the function q(y) = exp(\u2212Q(y)) defines a density on R corresponding to a log concave measure \u00b5. Unfortunately, their result is restricted to distributions for which Q(y) is strongly convex. The Gumbel distribution with CDF (5) has density\ng(y) = exp (\u2212 (y + c+ exp(\u2212(y + c)))) , (13)\nand the second derivative of y+c+exp(\u2212(y+c)) cannot be lower bounded by any constant greater than 0, so it is not log-strongly convex.\nTheorem 2. Let \u00b5 be a log-concave measure with density q(y) = exp(\u2212Q(y)), where Q : R\u2192 R is convex function satisfying the following conditions:\n\u2022 Q has a unique minimum in a point y = a\n\u2022 Q is twice continuously differentiable in each point of his domain, except possibly in y = a\n\u2022 Q\u2032(y) 6= 0 for any y 6= a\n\u2022 limy\u2192a\u00b1 Q\u2032(y) 6= 0 or limy\u2192a\u00b1 Q\u2032\u2032(y) 6= 0\nLet f : R\u2192 R a continuous function, differentiable almost everywhere, such that\nlim y\u2192\u00b1\u221e f(y)q(y) = 0, (14)\nthen for any 0 \u2264 \u03b7 < 1, such that Q \u2032\u2032(y) |Q\u2032(y)| + \u03b7|Q \u2032(y)| 6= 0, \u2200y \u2208 R \\ {a}, we have\nVar\u00b5(f) \u2264 1\n1\u2212 \u03b7 \u222b R\n(f \u2032(y))2\nQ\u2032\u2032(y) + \u03b7(Q\u2032(y))2 q(y)dy.\nProof. The proof is based on the one in Brascamp and Lieb [1976], but it uses a different strategy in the final critical steps. We first observe that for any K \u2208 R,\nVar\u00b5(f) \u2264 \u222b R (f(y)\u2212K)2d\u00b5, (15)\nso we will focus on bounding the left-hand side of (15) for the particular choice of K = h(a).\nLet f\u0303(y) \u2206 = f(y)\u2212 f(a) and U(y) \u2206= f\u0303(y) 2q(y) Q\u2032(y) . Note that d\u00b5 = q(y)dy. We have that\nU \u2032(y) = 2f\u0303 \u2032(y)f\u0303(y)q(y)\nQ\u2032(y) \u2212 f\u0303(y)2q(y)\n( Q\u2032\u2032(y)\n(Q\u2032(y))2 + 1\n) .\nRearranging terms and integrating, we see that\u222b f\u0303(y)2q(y)dy = \u222b ( 2f\u0303 \u2032(y)f\u0303(y)\nQ\u2032(y) \u2212 f\u0303(y)\n2Q\u2032\u2032(y)\n(Q\u2032(y))2\n) q(y)dy \u2212 U(y).\nWe now consider the integral between \u2212\u221e and a (analogous reasoning holds for the one between a and +\u221e). We claim that limy\u2192a\u2212 U(y) = 0. There are two possible cases: Q\u2032(a) 6= 0 and Q\u2032(a) = 0. In the first case the claim is obvious, in the second case we have limy\u2192a\u2212 f\u0303(y)2 Q\u2032(y) = limy\u2192a\u2212 2f \u2032(y)f\u0303(y) Q\u2032\u2032(y) = 0, and anagously for the limit from the left. Using (14) too, we have\u222b a\n\u2212\u221e f\u0303(y)2q(y)dy = lim \u21920\u2212 \u222b a+ \u2212\u221e ( 2f\u0303 \u2032(y)f\u0303(y) Q\u2032(y) \u2212 f\u0303(y) 2Q\u2032\u2032(y) (Q\u2032(y))2 ) q(y)dy\n\u2264 lim \u21920\u2212 \u222b a+ \u2212\u221e ( 2|f\u0303 \u2032(y)||f\u0303(y)| |Q\u2032(y)| \u2212 f\u0303(y) 2Q\u2032\u2032(y) (Q\u2032(y))2 ) q(y)dy\n\u2264 lim \u21920\u2212 \u222b a+ \u2212\u221e ( f\u0303 \u2032(y)2 Q\u2032\u2032(y) + \u03b7(Q\u2032(y))2 + \u03b7f\u0303(y)2 ) q(y)dy,\nwhere in the second inequality we used 2\u03b1\u03b2 \u2264 \u03b12\u03b6 + \u03b2 2\u03b6, for any \u03b1, \u03b6 \u2208 R and \u03b6 > 0, with \u03b1 = |f\u0303 \u2032(y)|, \u03b2 = |f\u0303(x)|, and \u03b6 = Q \u2032\u2032(y) |Q\u2032(y)| + \u03b7|Q\n\u2032(y)|. Reasoning in the same way for the interval [a,+\u221e), reordering the terms, and using (15), we have the result.\nThe main difference between Theorem 2 and the result of Brascamp and Lieb [1976, Theorem 4.1] is that the latter requires the function Q to be strongly convex. Our result holds for non-strongly concave functions including the Laplace and Gumbel distributions. If we take \u03b7 = 0 in Theorem 2 we recover the original result of Brascamp and Lieb [1976, Theorem 4.1]. For the case \u03b7 = 1/2, Theorem 2 yields the Poincare\u0301 inequality for the Laplace distribution given in Ledoux [2001]. Like the Gumbel distribution, the Laplace distribution is not strongly log-concave and previously required an alternative technique to prove measure concentration Ledoux [2001]. The following gives a Poincare\u0301 inequality for the Gumbel distribution.\nCorollary 1. Let \u00b5 be the measure corresponding to the Gumbel distribution and q(y) = g(y) in (13). For any function f that satisfies the conditions in Theorem 2, we have\nVar\u00b5(f) \u2264 4 \u222b R (f \u2032(y))2d\u00b5. (16)\nProof. For the Gumbel distribution we have Q(y) = y + c+ exp(\u2212(y + c)) in Theorem 2, so\nQ\u2032\u2032(y) + \u03b7(Q\u2032(y))2 = e\u2212(y+c) + \u03b7(1\u2212 e\u2212(y+c))2.\nWe want an lower bound for all y. Minimizing,\ne\u2212(y+c) = 2\u03b7(1\u2212 e\u2212(y+c))e\u2212(y+c)\nor e\u2212(y+c) = 1\u2212 12\u03b7 , so the lower bound is 1\u2212 1 2\u03b7 + 1 4\u03b7 or 4\u03b7\u22121 4\u03b7 for \u03b7 > 1 2 . For \u03b7 \u2264 1 2 ,\n\u03b7 + (1\u2212 2\u03b7)e\u2212(y+c) + e\u22122(y+c) \u2265 \u03b7.\nSo min {\n4\u03b7 (4\u03b7\u22121)(1\u2212\u03b7) , 1 \u03b7(1\u2212\u03b7) } = 4 at \u03b7 = 12 , so applying Theorem 2 we obtain (16)."}, {"heading": "3.2 Measure concentration for the Gumbel distribution", "text": "In the MAP perturbations such as that in (7), we have a function of many random variables. We now derive a result based on the Corollary 1 to bound the moment generating function for random variables defined as a function of m random variables. This gives a measure concentration inequality for the product measure \u00b5m of \u00b5 on Rm, where \u00b5 corresponds to a scalar Gumbel random variable.\nTheorem 3. Let \u00b5 denote the Gumbel measure on R and let F : Rm \u2192 R be a function such that \u00b5m-almost everywhere we have \u2016\u2207F\u20162 \u2264 a2 and \u2016\u2207F\u2016\u221e \u2264 b. Furthermore, suppose that for y = (y1, . . . , ym),\nlim yi\u2192\u00b1\u221e F (y1, . . . , ym) m\u220f i=1 g(yi) = 0,\nwhere g(\u00b7) is given by (13). Then, for any r \u2265 0 and any |\u03bb| \u2264 110b , we have\nE[exp(\u03bb(F \u2212 E[F ]))] \u2264 exp(5a2\u03bb2).\nProof. For each i = 1, 2 . . . ,m, we can think of F as a scalar function fi of its i-th argument for i = 1, . . . ,m. Using Theorem 5.14 of Ledoux [2001] and Corollary 1, for any |\u03bb|b \u2264 \u03c1 \u2264 1,\nEnt\u00b5i(exp(\u03bbfi)) \u2264 2\u03bb2 ( 1 + \u03c1\n1\u2212 \u03c1\n)2 exp(2 \u221a 5\u03c1) \u222b |\u2202iF |2d\u00b5i.\nWe now use Proposition 5.13 in Ledoux [2001] to tensorize the entropy by summing over i = 1 to m:\nEnt\u00b5m(exp(\u03bbfi)) \u2264 2\u03bb2 ( 1 + \u03c1\n1\u2212 \u03c1\n)2 exp(2 \u221a 5\u03c1) \u222b m\u2211 i=1 |\u2202iF |2 exp(\u03bbF )d\u00b5m\n\u2264 2\u03bb2 ( 1 + \u03c1\n1\u2212 \u03c1\n)2 exp(2 \u221a 5\u03c1)a2 \u222b exp(\u03bbf)d\u00b5m.\nHence, choosing \u03c1 = 110 , we obtain, for any |\u03bb| \u2264 1 10b\nEnt\u00b5m(exp(\u03bbF )) \u2264 5a2\u03bb2E\u00b5m [exp(\u03bbF )]. (17)\nRecall the moment generating function in (9) and \u03bb\u2212scaled cumulant generating function in (10), and note that H(0) = E[F ]. We now use Herbst\u2019s argument Ledoux [2001]. Using (17) we have\nH \u2032(\u03bb) = Ent\u00b5m(exp(\u03bbF ))\n\u03bb2\u039b(\u03bb) \u2264 5a2. (18)\nIntegrating (18) we get H(\u03bb) \u2264 H(0) + 5a2\u03bb = E[F ] + 5a2\u03bb,\nNow, from the definition of H(\u03bb), this implies\nlogE[exp(\u03bbF )] \u2264 \u03bbE[F ] + 5a2\u03bb2 .\nWith this lemma we can now upper bound the error in estimating the average E[F ] of a function F of m i.i.d. Gumbel random variables by generating M independent samples of F and taking the sample mean.\nCorollary 2. Consider the same assumptions of Theorem 3. Let \u03b71, \u03b72, . . . , \u03b7M be M i.i.d. random variables with the same distribution as F . Then with probability at least 1\u2212 \u03b4,\n1\nM M\u2211 j=1 \u03b7j \u2212 E[F ] \u2264 max\n( 20b\nM log\n1 \u03b4 ,\n\u221a 20a2\nM log\n1\n\u03b4\n) .\nProof. From the independence assumption, using the Markov inequality, we have that\nP  M\u2211 j=1 \u03b7j \u2264ME[F ] +Mr  \u2264 exp(\u2212ME[F ]\u2212Mr) M\u220f j=1 E[exp(\u03bb\u03b7j)].\nApplying Theorem 3, we have, for any |\u03bb| \u2264 110b ,\nP  1 M M\u2211 j=1 \u03b7j \u2264 E[F ] + r  \u2264 exp(M(5a2\u03bb2 \u2212 \u03bbr)). Optimizing over \u03bb subject to |\u03bb| \u2264 110b we obtain\nexp(M(5a2\u03bb2 \u2212 \u03bbr)) \u2264 exp ( \u2212M\n20 min\n( r\nb , r2 a2\n)) .\nEquating the left side of the last inequality to \u03b4 and solving for r, we have the stated bound."}, {"heading": "3.3 Application to MAP perturbations", "text": "To apply these results to the MAP perturbation problem we must calculate the parameters in the bound given by the Corollary 2. Let F (\u0393) be the random MAP perturbation as defined in (8).\nThis is a function of m \u2206 = \u2211n\ni=1 |Xi| i.i.d. Gumbel random variables. The (sub)gradient of this\nfunction is structured and points toward the \u03b3i(xi) that relate to the maximizing assignment in x\u0302R\u2212MAP defined in (4), when \u03b3(x) = \u2211n i=1 \u03b3i(xi), that is\n\u2202F (\u0393) \u2202\u03b3i(xi) = { 1 if xi \u2208 x\u0302R\u2212MAP 0 otherwise.\nThus the gradient satisfies \u2016\u2207F\u20162 = n and \u2016\u2207F\u2016\u221e = 1 almost everywhere, so a2 = n and b = 1. Suppose we sample M i.i.d. copies \u03931,\u03932, . . . ,\u0393M copies of \u0393 and estimate the deviation from the expectation by 1M \u2211M i=1 F (\u0393i). We can apply Corollary 2 to both F and \u2212F to get the following double-sided bound with probability 1\u2212 \u03b4:\u2223\u2223\u2223\u2223\u2223 1M M\u2211 i=1 F (\u0393i) \u2223\u2223\u2223\u2223\u2223 \u2264 max ( 20 M log 2 \u03b4 , \u221a 20n M log 2 \u03b4 ) .\nThus this result gives an estimation for the MAP perturbations E [maxx {\u03b8(x) + \u2211n\ni=1 \u03b3i(xi)}] that hold in high probability.\nThis result can also be applied to estimate the quality of Algorithm 1 that samples from the Gibbs distribution using MAP solvers. Now we let F equal Vj from (7). This is a function of\nmj \u2206 = \u2211n i=j |Xi| i.i.d. Gumbel random variables whose gradient satisfies \u2016\u2207Vj\u2016 2 = n \u2212 j + 1 and \u2016\u2207Vj\u2016\u221e = 1 almost everywhere, so a 2 = n\u2212 j+ 1 and b = 1. Suppose U = Vj \u2212E [Vj ] is a random variable that measures the deviation of Vj from its expectation, and assume we sample Mj i.i.d. random variable U1, U2, . . . , UMj . We then estimate this deviation by the sample mean 1 Mj \u2211Mj i=1 Ui. Applying Corollary 2 to both Vj and \u2212Vj to get the following bound with probability 1\u2212 \u03b4:\u2223\u2223\u2223\u2223\u2223\u2223 1Mj Mj\u2211 i=1 Ui \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 max ( 20 Mj log 2 \u03b4 , \u221a 20(n\u2212 j + 1) Mj log 2 \u03b4 ) . (19)\nFor each j in Algorithm 1, we must estimate |Xj | expectations E\u0393[Vj+1], for a total at most m expectation estimates. For any > 0 we can choose {Mj : j = 1, . . . , n} so that the right side of (19) is at most for each j with probability 1 \u2212 n\u03b4. Let p\u0302j(xj) be the ratio estimated in the first step of Algorithm 1, and \u03b4\u2032 = n\u03b4. Then with probability 1 \u2212 \u03b4\u2032, for all j = 1, 2, . . . , n, exp(E[Vj+1]\u2212 ) exp(E[Vj ]+ ) \u2264 p\u0302j(xj) \u2264 exp(E[Vj+1]+ ) exp(E[Vj ]\u2212 ) , or\nexp(\u22122 ) \u2264 p\u0302j(xj) pj(xj) \u2264 exp(2 )."}, {"heading": "4 Experiments", "text": "We evaluated our approach on a 100\u00d7 100 spin glass model with n = 104 variables, for which\n\u03b8(x1, ..., xn) = \u2211 i\u2208V \u03b8i(xi) + \u2211 (i,j)\u2208E \u03b8i,j(xi, xj) .\nwhere xi \u2208 {\u22121, 1}. Each spin has a local field parameter \u03b8i(xi) = \u03b8ixi and interacts in a grid shaped graphical structure with couplings \u03b8i,j(xi, xj) = \u03b8i,jxixj . Whenever the coupling parameters\nare positive the model is called attractive since adjacent variables give higher values to positively correlated configurations. We used low dimensional random perturbations \u03b3(x) = \u2211n i=1 \u03b3i(xi).\nThe local field parameters \u03b8i were drawn uniformly at random from [\u22121, 1] to reflect high signal. The parameters \u03b8i,j were drawn uniformly from [0, c], where c \u2208 [0, 4] to reflect weak, medium and strong coupling potentials. As these spin glass models are attractive, we are able to use the graphcuts algorithm (Kolmogorov [2006]) to compute the MAP perturbations efficiently. Throughout our experiments we evaluated the expected value of F (\u0393) with 100 different samples of \u0393. We note that we have two random variables \u03b3i(xi) for each of the spins in the 100 \u00d7 100 model, thus \u0393 consists of m = 2 \u2217 104 random variables.\nFigure 1 shows the error in the sample mean 1M \u2211M\nk=1 F (\u0393k) versus the coupling strength for three different sample sizes M = 1, 5, 10. The error reduces rapidly as M increases; only 10 samples are needed to estimate the expected value of a random MAP perturbation with 104 variables. To test our measure concentration result, that ensures exponential decay, we measure the deviation of the sample mean from its expectation by using M = 1, 5, 10 samples. Figure 2 shows the histogram of the sample mean, i.e., the number of times that the sample mean has error more than r from the true mean. One can see that the decay is indeed exponential for every M , and that for larger M the decay is much faster. These show that by understanding the measure concentration properties\nof MAP perturbations, we can efficiently estimate the mean with high probability, even in very high dimensional spin-glass models."}, {"heading": "5 Conclusion", "text": "Sampling from the Gibbs distribution is important because it helps find near-maxima in posterior probability landscapes that are typically encountered in the high dimensional complex models. These landscapes are often ragged due to domain-specific modeling (coupling) and the influence of data (signal), making MCMC challenging. In contrast, sampling based on MAP perturbations ignores the ragged landscape as it directly targets the most plausible structures. In this paper we characterized the statistics of MAP perturbations.\nTo apply the low-dimensional MAP perturbation technique in practice, we must estimate the expected value of the quantities Vj under the perturbations. We derived high-probability estimates of these expectations that allow estimation with arbitrary precision. To do so we proved more general results on measure concentration for functions of Gumbel random variables and a Poincare\u0301 inequality for non-strongly log-concave distributions. These results hold in generality and may be of use in other applications.\nThe results here can be taken in a number of different directions. MAP perturbation models are related PAC-Bayesian generalization bounds, so it may be possible to derive PAC-Bayesian bounds for unbounded loss functions using our tools. Such loss functions may exclude certain configurations and are already used implicitly in computer vision applications such as interactive segmentations. More generally, Poincare\u0301 inequalities relate the variance of a function and its derivatives. Our result may suggest new stochastic gradient methods that control variance via controlling gradients. This connection between variance and gradients may be useful in the analysis of other learning algorithms and applications."}], "references": [{"title": "Weighted sums of certain dependent random variables", "author": ["K. Azuma"], "venue": "To\u0302hoku Mathematical Journal,", "citeRegEx": "Azuma.,? \\Q1967\\E", "shortCiteRegEx": "Azuma.", "year": 1967}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "JMLR, 3:463\u2013482,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2003\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2003}, {"title": "The Theory of Probabilities", "author": ["S. Bernstein"], "venue": "Gastehizdat Publishing House, Moscow,", "citeRegEx": "Bernstein.,? \\Q1946\\E", "shortCiteRegEx": "Bernstein.", "year": 1946}, {"title": "Poincar\u00e9\u2019s inequalities and Talagrand\u2019s concentration phenomenon for the exponential measure", "author": ["S. Bobkov", "M. Ledoux"], "venue": "Probability Theory and Related Fields,", "citeRegEx": "Bobkov and Ledoux.,? \\Q1997\\E", "shortCiteRegEx": "Bobkov and Ledoux.", "year": 1997}, {"title": "Concentration inequalities", "author": ["S. Boucheron", "G. Lugosi", "O. Bousquet"], "venue": "In Advanced Lectures on Machine Learning,", "citeRegEx": "Boucheron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2004}, {"title": "Concentration inequalities for sub-additive functions using the entropy method. In Stochastic inequalities and applications, pages 213\u2013247", "author": ["O. Bousquet"], "venue": null, "citeRegEx": "Bousquet.,? \\Q2003\\E", "shortCiteRegEx": "Bousquet.", "year": 2003}, {"title": "On extensions of the Brunn-Minkowski and Pr\u00e9kopa-Leindler theorems, including inequalities for log concave functions, and with an application to the diffusion equation", "author": ["H.J. Brascamp", "E.H. Lieb"], "venue": "J. Func. Analysis,", "citeRegEx": "Brascamp and Lieb.,? \\Q1976\\E", "shortCiteRegEx": "Brascamp and Lieb.", "year": 1976}, {"title": "Dynamic programming and graph algorithms in computer vision", "author": ["P.F. Felzenszwalb", "R. Zabih"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "Felzenszwalb and Zabih.,? \\Q2011\\E", "shortCiteRegEx": "Felzenszwalb and Zabih.", "year": 2011}, {"title": "Statistical theory of extreme values and some practical applications: a series of lectures", "author": ["E.J. Gumbel", "J. Lieblein"], "venue": "Number 33 in National Bureau of Standards Applied Mathematics Series. US Govt. Print. Office,", "citeRegEx": "Gumbel and Lieblein.,? \\Q1954\\E", "shortCiteRegEx": "Gumbel and Lieblein.", "year": 1954}, {"title": "On the partition function and random maximum a-posteriori perturbations", "author": ["T. Hazan", "T. Jaakkola"], "venue": "In ICML,", "citeRegEx": "Hazan and Jaakkola.,? \\Q2012\\E", "shortCiteRegEx": "Hazan and Jaakkola.", "year": 2012}, {"title": "On sampling from the Gibbs distribution with random maximum a-posteriori perturbations", "author": ["T. Hazan", "S. Maji", "T. Jaakkola"], "venue": null, "citeRegEx": "Hazan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2013}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": null, "citeRegEx": "Hoeffding.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding.", "year": 1963}, {"title": "A bounding chain for swendsen-wang", "author": ["M. Huber"], "venue": "Random Structures & Algorithms,", "citeRegEx": "Huber.,? \\Q2003\\E", "shortCiteRegEx": "Huber.", "year": 2003}, {"title": "A polynomial-time approximation algorithm for the permanent of a matrix with nonnegative entries", "author": ["M. Jerrum", "A. Sinclair", "E. Vigoda"], "venue": null, "citeRegEx": "Jerrum et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Jerrum et al\\.", "year": 2004}, {"title": "PAC-Bayesian approach for minimization of phoneme error rate", "author": ["J. Keshet", "D. McAllester", "T. Hazan"], "venue": "In ICASSP,", "citeRegEx": "Keshet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Keshet et al\\.", "year": 2011}, {"title": "Probabilistic graphical models", "author": ["D. Koller", "N. Friedman"], "venue": "MIT press,", "citeRegEx": "Koller and Friedman.,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman.", "year": 2009}, {"title": "Convergent tree-reweighted message passing for energy", "author": ["V. Kolmogorov"], "venue": "minimization. PAMI,", "citeRegEx": "Kolmogorov.,? \\Q2006\\E", "shortCiteRegEx": "Kolmogorov.", "year": 2006}, {"title": "Dual decomposition for parsing with non-projective head automata", "author": ["T. Koo", "A.M. Rush", "M. Collins", "T. Jaakkola", "D. Sontag"], "venue": "In EMNLP,", "citeRegEx": "Koo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "The Concentration of Measure Phenomenon, volume 89 of Mathematical Surveys and Monographs", "author": ["M. Ledoux"], "venue": "American Mathematical Society,", "citeRegEx": "Ledoux.,? \\Q2001\\E", "shortCiteRegEx": "Ledoux.", "year": 2001}, {"title": "Simplified PAC-Bayesian margin bounds", "author": ["D. McAllester"], "venue": "Learning Theory and Kernel Machines,", "citeRegEx": "McAllester.,? \\Q2003\\E", "shortCiteRegEx": "McAllester.", "year": 2003}, {"title": "On the method of bounded differences. In Surveys in Combinatorics, number 141 in London Mathematical Society Lecture Note Series, pages 148\u2013188", "author": ["C. McDiarmid"], "venue": null, "citeRegEx": "McDiarmid.,? \\Q1989\\E", "shortCiteRegEx": "McDiarmid.", "year": 1989}, {"title": "Perturb-and-MAP random fields: Using discrete optimization to learn and sample from energy models", "author": ["G. Papandreou", "A. Yuille"], "venue": "In ICCV,", "citeRegEx": "Papandreou and Yuille.,? \\Q2011\\E", "shortCiteRegEx": "Papandreou and Yuille.", "year": 2011}, {"title": "Probabilistic methods in the geometry of Banach spaces. In Probabilty and Analysis, Varenna (Italy) 1985, volume 1206 of Lecture Notes in Mathematics", "author": ["G. Pisier"], "venue": null, "citeRegEx": "Pisier.,? \\Q1986\\E", "shortCiteRegEx": "Pisier.", "year": 1986}, {"title": "Tightening LP relaxations for MAP using message passing", "author": ["D. Sontag", "T. Meltzer", "A. Globerson", "T. Jaakkola", "Y. Weiss"], "venue": "In UAI,", "citeRegEx": "Sontag et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sontag et al\\.", "year": 2008}, {"title": "High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft", "author": ["T. Werner"], "venue": null, "citeRegEx": "Werner.,? \\Q1979\\E", "shortCiteRegEx": "Werner.", "year": 1979}], "referenceMentions": [{"referenceID": 7, "context": "Examples include scene understanding [Felzenszwalb and Zabih, 2011], parsing [Koo et al.", "startOffset": 37, "endOffset": 67}, {"referenceID": 17, "context": "Examples include scene understanding [Felzenszwalb and Zabih, 2011], parsing [Koo et al., 2010], and protein design [Sontag et al.", "startOffset": 77, "endOffset": 95}, {"referenceID": 23, "context": ", 2010], and protein design [Sontag et al., 2008].", "startOffset": 28, "endOffset": 49}, {"referenceID": 16, "context": "Substantial effort has gone into developing algorithms for recovering MAP assignments by exploiting domain-specific structural restrictions such as super-modularity [Kolmogorov, 2006] or by linear programming relaxations such as cuttingplanes [Sontag et al.", "startOffset": 165, "endOffset": 183}, {"referenceID": 9, "context": "[2013] use expectation bounds on the partition function [Hazan and Jaakkola, 2012] to build a sampler for Gibbs distribution using MAP solvers on low dimensional perturbations which are only linear in the dimension of the structures.", "startOffset": 56, "endOffset": 82}, {"referenceID": 9, "context": ", Jerrum et al. [2004], Huber [2003]) where no data term (signal) exists.", "startOffset": 2, "endOffset": 23}, {"referenceID": 9, "context": "[2004], Huber [2003]) where no data term (signal) exists.", "startOffset": 8, "endOffset": 21}, {"referenceID": 9, "context": "[2004], Huber [2003]) where no data term (signal) exists. One way around the difficulties of sampling from the Gibbs distribution is to look for the maximum a posteriori probability (MAP) structure. Substantial effort has gone into developing algorithms for recovering MAP assignments by exploiting domain-specific structural restrictions such as super-modularity [Kolmogorov, 2006] or by linear programming relaxations such as cuttingplanes [Sontag et al., 2008, Werner, 2008]. A drawback of MAP inference is that it returns a single assignment; in many contemporary models with complex potential functions on many variables, there are several likely structures, which makes MAP inference less appealing. We would like to also find these other \u201chighly probable\u201d assignments. Recent work has sought to leverage the current efficiency of MAP solvers to build procedures to sample from the Gibbs distribution, thereby avoiding the computational burden of MCMC methods. These works calculate the MAP structure of a randomly perturbed potential function. Such an approach effectively ignores the raggedness of the landscape that hinders MCMC. Papandreou and Yuille [2011] and Tarlow et al.", "startOffset": 8, "endOffset": 1168}, {"referenceID": 9, "context": "[2004], Huber [2003]) where no data term (signal) exists. One way around the difficulties of sampling from the Gibbs distribution is to look for the maximum a posteriori probability (MAP) structure. Substantial effort has gone into developing algorithms for recovering MAP assignments by exploiting domain-specific structural restrictions such as super-modularity [Kolmogorov, 2006] or by linear programming relaxations such as cuttingplanes [Sontag et al., 2008, Werner, 2008]. A drawback of MAP inference is that it returns a single assignment; in many contemporary models with complex potential functions on many variables, there are several likely structures, which makes MAP inference less appealing. We would like to also find these other \u201chighly probable\u201d assignments. Recent work has sought to leverage the current efficiency of MAP solvers to build procedures to sample from the Gibbs distribution, thereby avoiding the computational burden of MCMC methods. These works calculate the MAP structure of a randomly perturbed potential function. Such an approach effectively ignores the raggedness of the landscape that hinders MCMC. Papandreou and Yuille [2011] and Tarlow et al. [2012] have shown that randomly perturbing the potential of each structure with an independent random variable that follows the Gumbel distribution and finding the MAP assignment of the perturbed potential function provides an unbiased sample from the Gibbs distribution.", "startOffset": 8, "endOffset": 1193}, {"referenceID": 8, "context": "Alternatively, Hazan et al. [2013] use expectation bounds on the partition function [Hazan and Jaakkola, 2012] to build a sampler for Gibbs distribution using MAP solvers on low dimensional perturbations which are only linear in the dimension of the structures.", "startOffset": 15, "endOffset": 35}, {"referenceID": 3, "context": "Instead, we derive a new Poincar\u00e9 inequality for the Gumbel distribution, as well as a modified logarithmic Sobolev inequality using the approach suggested by Bobkov and Ledoux [1997], as described in the monograph of Ledoux [2001].", "startOffset": 159, "endOffset": 184}, {"referenceID": 3, "context": "Instead, we derive a new Poincar\u00e9 inequality for the Gumbel distribution, as well as a modified logarithmic Sobolev inequality using the approach suggested by Bobkov and Ledoux [1997], as described in the monograph of Ledoux [2001]. These results, which are of general interest, also guarantee that the deviation of the sampled mean of random MAP perturbations from their expectation has an exponential decay.", "startOffset": 159, "endOffset": 232}, {"referenceID": 8, "context": "Gumbel and Lieblein [1954] Let \u0393 = {\u03b3(x) : x \u2208 X} be a collection of i.", "startOffset": 0, "endOffset": 27}, {"referenceID": 10, "context": "Instead, Hazan et al. [2013] use the representation in (6) to derive a family of self-reducible upper bounds on Z and then use these upper bounds in an iterative algorithm that samples from the Gibbs distribution using low dimensional random MAP perturbations.", "startOffset": 9, "endOffset": 29}, {"referenceID": 10, "context": "Algorithm 1 Sampling with low-dimensional random MAP perturbations from the Gibbs distribution [Hazan et al., 2013] Iterate over j = 1, .", "startOffset": 95, "endOffset": 115}, {"referenceID": 18, "context": "The left side of (11) turns out to be the so-called functional entropy Ledoux [2001] of the function h = exp(\u03bbF ) with respect to a measure \u03bc:", "startOffset": 71, "endOffset": 85}, {"referenceID": 4, "context": ", [Boucheron et al., 2004]) extends the inequality to functions F of many variables.", "startOffset": 2, "endOffset": 26}, {"referenceID": 11, "context": "Koller and Friedman [2009]), which may be challenging in ragged probability landscapes.", "startOffset": 0, "endOffset": 27}, {"referenceID": 11, "context": "Koller and Friedman [2009]), which may be challenging in ragged probability landscapes. MAP perturbations use efficient MAP solvers as black box, but the statistical properties of the solutions, beyond Theorem 1, are still being studied. Papandreou and Yuille [2011] consider probability models that are defined by the maximal argument of randomly perturbed potential function, while Tarlow et al.", "startOffset": 0, "endOffset": 267}, {"referenceID": 11, "context": "Koller and Friedman [2009]), which may be challenging in ragged probability landscapes. MAP perturbations use efficient MAP solvers as black box, but the statistical properties of the solutions, beyond Theorem 1, are still being studied. Papandreou and Yuille [2011] consider probability models that are defined by the maximal argument of randomly perturbed potential function, while Tarlow et al. [2012] considers sampling techniques for such models and Keshet et al.", "startOffset": 0, "endOffset": 405}, {"referenceID": 11, "context": "[2012] considers sampling techniques for such models and Keshet et al. [2011] explores the generalization bounds for such models.", "startOffset": 57, "endOffset": 78}, {"referenceID": 8, "context": "Hazan and Jaakkola [2012] used the random MAP perturbation framework to derive upper bounds on the partition function in (2), and Hazan et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 8, "context": "Hazan and Jaakkola [2012] used the random MAP perturbation framework to derive upper bounds on the partition function in (2), and Hazan et al. [2013] derived the unbiased sampler in Algorithm 1.", "startOffset": 0, "endOffset": 150}, {"referenceID": 1, "context": ", Bartlett and Mendelson [2003]) or in PAC-Bayesian approaches (e.", "startOffset": 2, "endOffset": 32}, {"referenceID": 1, "context": ", Bartlett and Mendelson [2003]) or in PAC-Bayesian approaches (e.g., McAllester [2003]).", "startOffset": 2, "endOffset": 88}, {"referenceID": 1, "context": "such as Bernstein [1946], Azuma-Hoeffding [Azuma, 1967, Hoeffding, 1963, McDiarmid, 1989], or Bousquet [2003].", "startOffset": 8, "endOffset": 25}, {"referenceID": 0, "context": "such as Bernstein [1946], Azuma-Hoeffding [Azuma, 1967, Hoeffding, 1963, McDiarmid, 1989], or Bousquet [2003]. However, in our setting, the Gumbel random variables are not bounded, and random perturbations may result in unbounded changes of the perturbed MAP value.", "startOffset": 26, "endOffset": 110}, {"referenceID": 0, "context": "such as Bernstein [1946], Azuma-Hoeffding [Azuma, 1967, Hoeffding, 1963, McDiarmid, 1989], or Bousquet [2003]. However, in our setting, the Gumbel random variables are not bounded, and random perturbations may result in unbounded changes of the perturbed MAP value. There are several results on measure concentration for Lipschitz functions of Gaussian random variables (c.f. Maurey and Pisier [1986]).", "startOffset": 26, "endOffset": 401}, {"referenceID": 0, "context": "such as Bernstein [1946], Azuma-Hoeffding [Azuma, 1967, Hoeffding, 1963, McDiarmid, 1989], or Bousquet [2003]. However, in our setting, the Gumbel random variables are not bounded, and random perturbations may result in unbounded changes of the perturbed MAP value. There are several results on measure concentration for Lipschitz functions of Gaussian random variables (c.f. Maurey and Pisier [1986]). In this work we use logarithmic Sobolev inequalities Ledoux [2001] and prove a new measure concentration result for Gumbel random variables.", "startOffset": 26, "endOffset": 470}, {"referenceID": 0, "context": "such as Bernstein [1946], Azuma-Hoeffding [Azuma, 1967, Hoeffding, 1963, McDiarmid, 1989], or Bousquet [2003]. However, in our setting, the Gumbel random variables are not bounded, and random perturbations may result in unbounded changes of the perturbed MAP value. There are several results on measure concentration for Lipschitz functions of Gaussian random variables (c.f. Maurey and Pisier [1986]). In this work we use logarithmic Sobolev inequalities Ledoux [2001] and prove a new measure concentration result for Gumbel random variables. To do this we generalize a classic result of Brascamp and Lieb [1976] on Poincar\u00e9 inequalities to non-strongly log-concave distributions, and also recover the concentration result of Bobkov and Ledoux [1997] for functions of Laplace random variables.", "startOffset": 26, "endOffset": 614}, {"referenceID": 0, "context": "such as Bernstein [1946], Azuma-Hoeffding [Azuma, 1967, Hoeffding, 1963, McDiarmid, 1989], or Bousquet [2003]. However, in our setting, the Gumbel random variables are not bounded, and random perturbations may result in unbounded changes of the perturbed MAP value. There are several results on measure concentration for Lipschitz functions of Gaussian random variables (c.f. Maurey and Pisier [1986]). In this work we use logarithmic Sobolev inequalities Ledoux [2001] and prove a new measure concentration result for Gumbel random variables. To do this we generalize a classic result of Brascamp and Lieb [1976] on Poincar\u00e9 inequalities to non-strongly log-concave distributions, and also recover the concentration result of Bobkov and Ledoux [1997] for functions of Laplace random variables.", "startOffset": 26, "endOffset": 752}, {"referenceID": 6, "context": "The proof is based on the one in Brascamp and Lieb [1976], but it uses a different strategy in the final critical steps.", "startOffset": 33, "endOffset": 58}, {"referenceID": 6, "context": "The main difference between Theorem 2 and the result of Brascamp and Lieb [1976, Theorem 4.1] is that the latter requires the function Q to be strongly convex. Our result holds for non-strongly concave functions including the Laplace and Gumbel distributions. If we take \u03b7 = 0 in Theorem 2 we recover the original result of Brascamp and Lieb [1976, Theorem 4.1]. For the case \u03b7 = 1/2, Theorem 2 yields the Poincar\u00e9 inequality for the Laplace distribution given in Ledoux [2001]. Like the Gumbel distribution, the Laplace distribution is not strongly log-concave and previously required an alternative technique to prove measure concentration Ledoux [2001].", "startOffset": 56, "endOffset": 478}, {"referenceID": 6, "context": "The main difference between Theorem 2 and the result of Brascamp and Lieb [1976, Theorem 4.1] is that the latter requires the function Q to be strongly convex. Our result holds for non-strongly concave functions including the Laplace and Gumbel distributions. If we take \u03b7 = 0 in Theorem 2 we recover the original result of Brascamp and Lieb [1976, Theorem 4.1]. For the case \u03b7 = 1/2, Theorem 2 yields the Poincar\u00e9 inequality for the Laplace distribution given in Ledoux [2001]. Like the Gumbel distribution, the Laplace distribution is not strongly log-concave and previously required an alternative technique to prove measure concentration Ledoux [2001]. The following gives a Poincar\u00e9 inequality for the Gumbel distribution.", "startOffset": 56, "endOffset": 656}, {"referenceID": 18, "context": "14 of Ledoux [2001] and Corollary 1, for any |\u03bb|b \u2264 \u03c1 \u2264 1, Ent\u03bci(exp(\u03bbfi)) \u2264 2\u03bb ( 1 + \u03c1 1\u2212 \u03c1 )2 exp(2 \u221a 5\u03c1) \u222b |\u2202iF |d\u03bci.", "startOffset": 6, "endOffset": 20}, {"referenceID": 18, "context": "13 in Ledoux [2001] to tensorize the entropy by summing over i = 1 to m:", "startOffset": 6, "endOffset": 20}, {"referenceID": 18, "context": "We now use Herbst\u2019s argument Ledoux [2001]. Using (17) we have H \u2032(\u03bb) = Ent\u03bcm(exp(\u03bbF )) \u03bb2\u039b(\u03bb) \u2264 5a.", "startOffset": 29, "endOffset": 43}, {"referenceID": 16, "context": "As these spin glass models are attractive, we are able to use the graphcuts algorithm (Kolmogorov [2006]) to compute the MAP perturbations efficiently.", "startOffset": 87, "endOffset": 105}], "year": 2013, "abstractText": "The maximum a-posteriori (MAP) perturbation framework has emerged as a useful approach for inference and learning in high dimensional complex models. By maximizing a randomly perturbed potential function, MAP perturbations generate unbiased samples from the Gibbs distribution. Unfortunately, the computational cost of generating so many high-dimensional random variables can be prohibitive. More efficient algorithms use sequential sampling strategies based on the expected value of low dimensional MAP perturbations. This paper develops new measure concentration inequalities that bound the number of samples needed to estimate such expected values. Applying the general result to MAP perturbations can yield a more efficient algorithm to approximate sampling from the Gibbs distribution. The measure concentration result is of general interest and may be applicable to other areas involving expected estimations.", "creator": "LaTeX with hyperref package"}}}