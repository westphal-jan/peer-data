{"id": "1310.5042", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2013", "title": "Distributional semantics beyond words: Supervised learning of analogy and paraphrase", "abstract": "There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous). One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples. A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). However, past work required hand-coding the combination function for different tasks. The main contribution of this paper is that combination functions are generated by supervised learning. We achieve state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval~2012 Task 2) and measuring compositional similarity between noun-modifier phrases and unigrams (multiple-choice paraphrase questions).", "histories": [["v1", "Fri, 18 Oct 2013 14:50:39 GMT  (30kb)", "http://arxiv.org/abs/1310.5042v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL cs.IR", "authors": ["peter d turney"], "accepted": true, "id": "1310.5042"}, "pdf": {"name": "1310.5042.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["peter.turney@nrc-cnrc.gc.ca"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 0.\n50 42\nv1 [\ncs .L\nG ]\n1 8\nO ct\nThere have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous). One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples. A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). However, past work required hand-coding the combination function for different tasks. The main contribution of this paper is that combination functions are generated by supervised learning. We achieve state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval 2012 Task 2) and measuring compositional similarity between nounmodifier phrases and unigrams (multiplechoice paraphrase questions)."}, {"heading": "1 Introduction", "text": "Harris (1954) and Firth (1957) hypothesized that words that appear in similar contexts tend to have similar meanings. This hypothesis is the foundation for distributional semantics, in which words are represented by context vectors. The similarity of two words is calculated by comparing the two corresponding context vectors (Lund et al., 1995; Landauer and Dumais, 1997; Turney and Pantel, 2010).\nDistributional semantics is highly effective for measuring the semantic similarity between individual words. On a set of eighty multiplechoice synonym questions from the test of English as a foreign language (TOEFL), a distributional approach recently achieved 100% accuracy (Bullinaria and Levy, 2012). However, it has been difficult to extend distributional semantics beyond individual words, to word pairs, phrases, and sentences.\nMoving beyond individual words, there are various types of semantic similarity to consider. Here we focus on paraphrase and analogy. Paraphrase is similarity in the meaning of two pieces of text (Androutsopoulos and Malakasiotis, 2010). Analogy is similarity in the semantic relations of two sets of words (Turney, 2008a).\nIt is common to study paraphrase at the sentence level (Androutsopoulos and Malakasiotis, 2010), but we prefer to concentrate on the simplest type of paraphrase, where a bigram paraphrases a unigram. For example, dog house is a paraphrase of kennel. In our experiments, we concentrate on noun-modifier bigrams and noun unigrams.\nAnalogies map terms in one domain to terms in another domain (Gentner, 1983). The familiar analogy between the solar system and the RutherfordBohr atomic model involves several terms from the domain of the solar system and the domain of the atomic model (Turney, 2008a).\nThe simplest type of analogy is proportional analogy, which involves two pairs of words (Turney, 2006b). For example, the pair \u3008cook, raw\u3009 is analogous to the pair \u3008decorate, plain\u3009. If we cook\na thing, it is no longer raw; if we decorate a thing, it is no longer plain. The semantic relations between cook and raw are similar to the semantic relations between decorate and plain. In the following experiments, we focus on proportional analogies.\nErk (2013) distinguished four approaches to extend distributional semantics beyond words: In the first, a single vector space representation for a phrase or sentence is computed from the representations of the individual words (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). In the second, two phrases or sentences are compared by combining multiple pairwise similarity values (Socher et al., 2011; Turney, 2012). Third, weighted inference rules integrate distributional similarity and formal logic (Garrette et al., 2011). Fourth, a single space integrates formal logic and vectors (Clarke, 2012).\nTaking the second approach, Turney (2012) introduced a dual-space model, with one space for measuring domain similarity (similarity of topic or field) and another for function similarity (similarity of role or usage). Similarities beyond individual words are calculated by functions that combine domain and function similarities of component words.\nThe dual-space model has been applied to measuring compositional similarity (paraphrase recognition) and relational similarity (analogy recognition). In experiments that tested for sensitivity to word order, the dual-space model performed significantly better than competing approaches (Turney, 2012).\nA limitation of past work with the dual-space model is that the combination functions were handcoded. Our main contribution is to show how handcoding can be eliminated with supervised learning. For ease of reference, we will call our approach SuperSim (supervised similarity). With no modification of SuperSim for the specific task (relational similarity or compositional similarity), we achieve better results than previous hand-coded models.\nCompositional similarity (paraphrase) compares two contiguous phrases or sentences (n-grams), whereas relational similarity (analogy) does not require contiguity. We use tuple to refer to both contiguous and noncontiguous word sequences.\nWe approach analogy as a problem of supervised tuple classification. To measure the relational sim-\nilarity between two word pairs, we train SuperSim with quadruples that are labeled as positive and negative examples of analogies. For example, the proportional analogy \u3008cook, raw, decorate, plain\u3009 is labeled as a positive example.\nA quadruple is represented by a feature vector, composed of domain and function similarities from the dual-space model and other features based on corpus frequencies. SuperSim uses a support vector machine (Platt, 1998) to learn the probability that a quadruple \u3008a, b, c, d\u3009 consists of a word pair \u3008a, b\u3009 and an analogous word pair \u3008c, d\u3009. The probability can be interpreted as the degree of relational similarity between the two given word pairs.\nWe also approach paraphrase as supervised tuple classification. To measure the compositional similarity beween an m-gram and an n-gram, we train the learning algorithm with (m+ n)-tuples that are positive and negative examples of paraphrases.\nSuperSim learns to estimate the probability that a triple \u3008a, b, c\u3009 consists of a compositional bigram ab and a synonymous unigram c. For instance, the phrase fish tank is synonymous with aquarium; that is, fish tank and aquarium have high compositional similarity. The triple \u3008fish, tank, aquarium\u3009 is represented using the same features that we used for analogy. The probability of the triple can be interpreted as the degree of compositional similarity between the given bigram and unigram.\nWe review related work in Section 2. The general feature space for learning relations and compositions is presented in Section 3. The experiments with relational similarity are described in Section 4, and Section 5 reports the results with compositional similarity. Section 6 discusses the implications of the results. We consider future work in Section 7 and conclude in Section 8."}, {"heading": "2 Related Work", "text": "In SemEval 2012, Task 2 was concerned with measuring the degree of relational similarity between two word pairs (Jurgens et al., 2012) and Task 6 (Agirre et al., 2012) examined the degree of semantic equivalence between two sentences. These two areas of research have been mostly independent, although Socher et al. (2012) and Turney (2012) present unified perspectives on the two tasks. We\nfirst discuss some work on relational similarity, then some work on compositional similarity, and lastly work that unifies the two types of similarity."}, {"heading": "2.1 Relational Similarity", "text": "LRA (latent relational analysis) measures relational similarity with a pair\u2013pattern matrix (Turney, 2006b). Rows in the matrix correspond to word pairs (a, b) and columns correspond to patterns that connect the pairs (\u201ca for the b\u201d) in a large corpus. This is a holistic (noncompositional) approach to distributional similarity, since the word pairs are opaque wholes; the component words have no separate representations. A compositional approach to analogy has a representation for each word, and a word pair is represented by composing the representations for each member of the pair. Given a vocabulary of N words, a compositional approach requires N representations to handle all possible word pairs, but a holistic approach requires N2 representations. Holistic approaches do not scale up (Turney, 2012). LRA required nine days to run.\nBollegala et al. (2008) answered the SAT analogy questions with a support vector machine trained on quadruples (proportional analogies), as we do here. However, their feature vectors are holistic, and hence there are scaling problems.\nHerdag\u0306delen and Baroni (2009) used a support vector machine to learn relational similarity. Their feature vectors contained a combination of holistic and compositional features.\nMeasuring relational similarity is closely connected to classifying word pairs according to their semantic relations (Turney and Littman, 2005). Semantic relation classification was the focus of SemEval 2007 Task 4 (Girju et al., 2007) and SemEval 2010 Task 8 (Hendrickx et al., 2010)."}, {"heading": "2.2 Compositional Similarity", "text": "To extend distributional semantics beyond words, many researchers take the first approach described by Erk (2013), in which a single vector space is used for individual words, phrases, and sentences (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). In this approach, given the words a and b with context vectors a\nand b, we construct a vector for the bigram ab by applying vector operations to a and b.\nMitchell and Lapata (2010) experiment with many different vector operations and find that element-wise multiplication performs well. The bigram ab is represented by c = a \u2299 b, where ci = ai \u00b7 bi. However, element-wise multiplication is commutative, so the bigrams ab and ba map to the same vector c. In experiments that test for order sensitivity, element-wise multiplication performs poorly (Turney, 2012).\nWe can treat the bigram ab as a unit, as if it were a single word, and construct a context vector for ab from occurrences of ab in a large corpus. This holistic approach to representing bigrams performs well when a limited set of bigrams is specified in advance (before building the word\u2013context matrix), but it does not scale up, because there are too many possible bigrams (Turney, 2012).\nAlthough the holistic approach does not scale up, we can generate a few holistic bigram vectors and use them to train a supervised regression model (Guevara, 2010; Baroni and Zamparelli, 2010). Given a new bigram cd, not observed in the corpus, the regression model can predict a holistic vector for cd, if c and d have been observed separately. We show in Section 5 that this idea can be adapted to train SuperSim without manually labeled data.\nSocher et al. (2011) take the second approach described by Erk (2013), in which two sentences are compared by combining multiple pairwise similarity values. They construct a variable-sized similarity matrix X, in which the element xij is the similarity between the i-th phrase of one sentence and the j-th phrase of the other. Since supervised learning is simpler with fixed-sized feature vectors, the variable-sized similarity matrix is then reduced to a smaller fixed-sized matrix, to allow comparison of pairs of sentences of varying lengths."}, {"heading": "2.3 Unified Perspectives on Similarity", "text": "Socher et al. (2012) represent words and phrases with a pair, consisting of a vector and a matrix. The vector captures the meaning of the word or phrase and the matrix captures how a word or phrase modifies the meaning of another word or phrase when they are combined. They apply this matrix\u2013vector representation to both compositions and relations.\nTurney (2012) represents words with two vectors, a vector from domain space and a vector from function space. The domain vector captures the topic or field of the word and the function vector captures the functional role of the word. This dual-space model is applied to both compositions and relations.\nHere we extend the dual-space model of Turney (2012) in two ways: Hand-coding is replaced with supervised learning and two new sets of features augment domain and function space. Moving to supervised learning instead of hand-coding makes it easier to introduce new features.\nIn the dual-space model, parameterized similarity measures provided the input values for handcrafted functions. Each task required a different set of hand-crafted functions. The parameters of the similarity measures were tuned using a customized grid search algorithm. The grid search algorithm was not suitable for integration with a supervised learning algorithm. The insight behind SuperSim is that, given appropriate features, a supervised learning algorithm can replace the grid search algorithm and the hand-crafted functions."}, {"heading": "3 Features for Tuple Classification", "text": "We represent a tuple with four types of features, all based on frequencies in a large corpus. The first type of feature is the logarithm of the frequency of a word. The second type is the positive pointwise mutual information (PPMI) between two words (Church and Hanks, 1989; Bullinaria and Levy, 2007). Third and fourth are the similarities of two words in domain and function space (Turney, 2012).\nIn the following experiments, we use the PPMI matrix from Turney et al. (2011) and the domain and function matrices from Turney (2012).1 The three matrices and the word frequency data are based on the same corpus, a collection of web pages gathered from university web sites, containing 5 \u00d7 1010 words.2 All three matrices are word\u2013context matrices, in which the rows correspond to terms (words\n1The three matrices and the word frequency data are available on request from the author. The matrix files range from two to five gigabytes when packaged and compressed for distribution.\n2The corpus was collected by Charles Clarke at the University of Waterloo. It is about 280 gigabytes of plain text.\nand phrases) in WordNet.3 The columns correspond to the contexts in which the terms appear; each matrix involves a different kind of context.\nLet \u3008x1, x2, . . . , xn\u3009 be an n-tuple of words. The number of features we use to represent this tuple increases as a function of n.\nThe first set of features consists of log frequency values for each word xi in the n-tuple. Let freq(xi) be the frequency of xi in the corpus. We define LF(xi) as log(freq(xi)+1). If xi is not in the corpus, freq(xi) is zero, and thus LF(xi) is also zero. There are n log frequency features, one LF(xi) feature for each word in the n-tuple.\nThe second set of features consists of positive pointwise mutual information values for each pair of words in the n-tuple. We use the raw PPMI matrix from Turney et al. (2011). Although they computed the singular value decomposition (SVD) to project the row vectors into a lower-dimensional space, we need the original high-dimensional columns for our features. The raw PPMI matrix has 114,501 rows and 139,246 columns with a density of 1.2%. For each term in WordNet, there is a corresponding row in the raw PPMI matrix. For each unigram in WordNet, there are two corresponding columns in the raw PPMI matrix, one marked left and the other right.\nSuppose xi corresponds to the i-th row of the PPMI matrix and xj corresponds the j-th column, marked left. The value in the i-th row and j-th column of the PPMI matrix, PPMI(xi, xj , left), is the positive pointwise mutual information of xi and xj co-occurring in the corpus, where xj is the first word to the left of xi, ignoring any intervening stop words (that is, ignoring any words that are not in WordNet). If xi (or xj) has no corresponding row (or column) in the matrix, then the PPMI value is set to zero.\nTurney et al. (2011) estimated PPMI(xi, xj , left) by sampling the corpus for phrases containing xi and then looking for xj to the left of xi in the sampled phrases (and likewise for right). Due to this sampling process, PPMI(xi, xj , left) does not necessarily equal PPMI(xj , xi, right). For example, suppose xi is a rare word and xj is a common word. With PPMI(xi, xj , left), when we sample phrases containing xi, we are relatively likely to find xj in some of\n3See http://wordnet.princeton.edu/ for information about WordNet.\nthese phrases. With PPMI(xj , xi, right), when we sample phrases containing xj , we are less likely to find any phrases containing xi. Although, in theory, PPMI(xi, xj , left) should equal PPMI(xj , xi, right), they are likely to be unequal given a limited sample.\nFrom the n-tuple, we select all of the n(n \u2212 1) pairs, \u3008xi, xj\u3009, such that i 6= j. We then generate two features for each pair, PPMI(xi, xj , left) and PPMI(xi, xj , right). Thus there are 2n(n\u22121) PPMI values in the second set of features.\nThe third set of features consists of domain space similarity values for each pair of words in the n-tuple. Domain space was designed to capture the topic of a word. Turney (2012) first constructed a frequency matrix, in which the rows correspond to terms in WordNet and the columns correspond to nearby nouns. Given a term xi, the corpus was sampled for phrases containing xi and the phrases were processed with a part-of-speech tagger, to identify nouns. If the noun xj was the closest noun to the left or right of xi, then the frequency count for the i-th row and j-th column was incremented. The hypothesis was that the nouns near a term characterize the topics associated with the term.\nThe word\u2013context frequency matrix for domain space has 114,297 rows (terms) and 50,000 columns (noun contexts, topics), with a density of 2.6%. The frequency matrix was converted to a PPMI matrix and then smoothed with SVD. The SVD yields three matrices, U, \u03a3, and V.\nA term in domain space is represented by a row vector in Uk\u03a3 p k. The parameter k specifies the number of singular values in the truncated singular value decomposition; that is, k is the number of latent factors in the low-dimensional representation of the term (Landauer and Dumais, 1997). We generate Uk and \u03a3k by deleting the columns in U and \u03a3 corresponding to the smallest singular values. The parameter p raises the singular values in \u03a3k to the power p (Caron, 2001). As p goes from one to zero, factors with smaller singular values are given more weight. This has the effect of making the similarity measure more discriminating (Turney, 2012).\nThe similarity of two words in domain space, Dom(xi, xj , k, p), is computed by extracting the row vectors in Uk\u03a3 p k\nthat correspond to the words xi and xj , and then calculating their cosine. Optimal performance requires tuning the parameters k and p for\nthe task (Bullinaria and Levy, 2012; Turney, 2012). In the following experiments, we avoid directly tuning k and p by generating features with a variety of values for k and p, allowing the supervised learning algorithm to decide which features to use.\nFrom the n-tuple, we select all 1 2 n(n \u2212 1) pairs, \u3008xi, xj\u3009, such that i < j. For each pair, we generate domain similarity features, Dom(xi, xj , k, p), where k varies from 100 to 1000 in steps of 100 and p varies from 0 to 1 in steps of 0.1. The number of k values, nk, is 10 and the number of p values, np, is 11; therefore there are 110 features, nknp, for each pair, \u3008xi, xj\u3009. Thus there are 12n(n\u22121)nknp domain space similarity values in the third set of features.\nThe fourth set of features consists of function space similarity values for each pair of words in the n-tuple. Function space was designed to capture the functional role of a word. It is similar to domain space, except the context is based on verbal patterns, instead of nearby nouns. The hypothesis was that the functional role of a word is characterized by the patterns that relate the word to nearby verbs.\nThe word\u2013context frequency matrix for function space has 114,101 rows (terms) and 50,000 columns (verb pattern contexts, functional roles), with a density of 1.2%. The frequency matrix was converted to a PPMI matrix and smoothed with SVD.\nFrom the n-tuple, we select all 1 2 n(n \u2212 1) pairs, \u3008xi, xj\u3009, such that i < j. For each pair, we generate function similarity features, Fun(xi, xj , k, p), where k and p vary as they did with domain space. Thus there are 1\n2 n(n \u2212 1)nknp function space similarity\nvalues in the fourth set of features. Table 1 summarizes the four sets of features and the size of each set as a function of n, the number of words in the given tuple. The values of nk and np (10 and 11) are considered to be constants. Table 2 shows the number of elements in the feature vector, as n varies from 1 to 6. The total number of features is O(n2). We believe that this is acceptable growth and will scale up to comparing sentence pairs.\nThe four sets of features have a hierarchical relationship. The log frequency features are based on counting isolated occurrences of each word in the corpus. The PPMI features are based on direct co-occurrences of two words; that is, PPMI is only greater than zero if the two words actually occur together in the corpus. Domain\nand function space capture indirect or higherorder co-occurrence, due to the truncated SVD (Lemaire and Denhie\u0300re, 2006); that is, the values of Dom(xi, xj , k, p) and Fun(xi, xj , k, p) can be high even when xi and xj do not actually co-occur in the corpus. We conjecture that there are yet higher orders in this hierarchy that would provide improved similarity measures.\nSuperSim learns to classify tuples by representing them with these features. SuperSim uses the sequential minimal optimization (SMO) support vector machine (SVM) as implemented in Weka (Platt, 1998; Witten et al., 2011).4 The kernel is a normalized third-order polynomial. Weka provides probability estimates for the classes by fitting the outputs of the SVM with logistic regression models."}, {"heading": "4 Relational Similarity", "text": "This section presents experiments with learning relational similarity using SuperSim. The training datasets consist of quadruples that are labeled as positive and negative examples of analogies. Table 2 shows that the feature vectors have 1,348 elements.\nWe experiment with three datasets, a collection of 374 five-choice questions from the SAT college entrance exam (Turney et al., 2003), a modified ten-choice variation of the SAT questions\n4Weka is available at http://www.cs.waikato.ac.nz/ml/weka/.\n(Turney, 2012), and the relational similarity dataset from SemEval 2012 Task 2 (Jurgens et al., 2012).5"}, {"heading": "4.1 Five-choice SAT Questions", "text": "Table 3 is an example of a question from the 374 five-choice SAT questions. Each five-choice question yields five labeled quadruples, by combining the stem with each choice. The quadruple \u3008word, language, note, music\u3009 is labeled positive and the other four quadruples are labeled negative.\nSince learning works better with balanced training data (Japkowicz and Stephen, 2002), we use the symmetries of proportional analogies to add more positive examples (Lepage and Shin-ichi, 1996). For each positive quadruple, \u3008a, b, c, d\u3009, we add three more positive quadruples, \u3008b, a, d, c\u3009, \u3008c, d, a, b\u3009, and \u3008d, c, b, a\u3009. Thus each five-choice question provides four positive and four negative quadruples.\nWe use ten-fold cross-validation to apply SuperSim to the SAT questions. The folds are constructed so that the eight quadruples from each SAT question are kept together in the same fold. To answer a question in the testing fold, the learned model assigns a probability to each of the five choices and guesses the choice with the highest probability. SuperSim achieves a score of 54.8% correct (205 out of 374).\nTable 4 gives the rank of SuperSim in the list of the top ten results with the SAT analogy questions.6 The scores ranging from 51.1% to 57.0% are not significantly different from SuperSim\u2019s score of 54.8%, according to Fisher\u2019s exact test at the 95% confidence level. However, SuperSim answers the SAT\n5The SAT questions are available on request from the author. The SemEval 2012 Task 2 dataset is available at https://sites.google.com/site/semeval2012task2/.\n6See the State of the Art page on the ACL Wiki at http://aclweb.org/aclwiki.\nquestions in a few minutes, whereas LRA requires nine days, and SuperSim learns its models automatically, unlike the hand-coding of Turney (2012)."}, {"heading": "4.2 Ten-choice SAT Questions", "text": "In addition to symmetries, proportional analogies have asymmetries. In general, if the quadruple \u3008a, b, c, d\u3009 is positive, \u3008a, d, c, b\u3009 is negative. For example, \u3008word, language, note, music\u3009 is a good analogy, but \u3008word, music, note, language\u3009 is not. Words are the basic units of language and notes are the basic units of music, but words are not necessary for music and notes are not necessary for language.\nTurney (2012) used this asymmetry to convert the 374 five-choice SAT questions into 374 tenchoice SAT questions. Each choice \u3008c, d\u3009 was expanded with the stem \u3008a, b\u3009, resulting in the quadruple \u3008a, b, c, d\u3009, and then the order was shuffled to \u3008a, d, c, b\u3009, so that each choice pair in a fivechoice question generated two choice quadruples in a ten-choice question. Nine of the quadruples are negative examples and the quadruple consisting of the stem pair followed by the solution pair is the only positive example. The purpose of the ten-choice questions is to test the ability of measures of relational similarity to avoid the asymmetric distractors.\nWe use the ten-choice questions to compare the hand-coded dual-space approach (Turney, 2012) with SuperSim. We also use these questions to perform an ablation study of the four sets of features in SuperSim. As with the five-choice questions, we use the symmetries of proportional analogies to add three more positive examples, so the training\ndataset has nine negative examples and four positive examples per question. We apply ten-fold crossvalidation to the 374 ten-choice questions.\nOn the ten-choice questions, SuperSim\u2019s score is 52.7% (Table 5), compared to 54.8% on the five-choice questions (Table 4), a drop of 2.1%. The hand-coded dual-space model scores 47.9% (Table 5), compared to 51.1% on the five-choice questions (Table 4), a drop of 3.2%. The difference between SuperSim (52.7%) and the handcoded dual-space model (47.9%) is not significant according to Fisher\u2019s exact test at the 95% confidence level. The advantage of SuperSim is that it does not need hand-coding. The results show that SuperSim can avoid the asymmetric distractors.\nTable 5 shows the impact of different subsets of features on the percentage of correct answers to the ten-choice SAT questions. Included features are marked 1 and ablated features are marked 0. The results show that the log frequency (LF) and PPMI features are not helpful (but also not harmful) for relational similarity. We also see that domain space and function space are both needed for good results."}, {"heading": "4.3 SemEval 2012 Task 2", "text": "The SemEval 2012 Task 2 dataset is based on the semantic relation classification scheme of Bejar et al. (1991), consisting of ten high-level categories of relations and seventy-nine subcategories, with paradigmatic examples of each subcategory. For instance, the subcategory taxonomic in the category class inclusion has three paradigmatic examples, flower:tulip, emotion:rage, and poem:sonnet.\nJurgens et al. (2012) used Amazon\u2019s Mechanical Turk to create the SemEval 2012 Task 2 dataset in two phases. In the first phase, Turkers expanded the paradigmatic examples for each subcategory to an average of forty-one word pairs per subcategory, a total of 3,218 pairs. In the second phase, each word pair from the first phase was assigned a prototypicality score, indicating its similarity to the paradigmatic examples. The challenge of SemEval 2012 Task 2 was to guess the prototypicality scores.\nSuperSim was trained on the five-choice SAT questions and evaluated on the SemEval 2012 Task 2 test dataset. For a given a word pair, we created quadruples, combining the word pair with each of the paradigmatic examples for its subcategory. We then used SuperSim to compute the probabilities for each quadruple. Our guess for the prototypicality score of the given word pair was the average of the probabilities. Spearman\u2019s rank correlation coefficient between the Turkers\u2019 prototypicality scores and SuperSim\u2019s scores was 0.408, averaged over the sixty-nine subcategories in the testing set. SuperSim has the highest Spearman correlation achieved to date on SemEval 2012 Task 2 (see Table 6)."}, {"heading": "5 Compositional Similarity", "text": "This section presents experiments using SuperSim to learn compositional similarity. The datasets consist of triples, \u3008a, b, c\u3009, such that ab is a nounmodifier bigram and c is a noun unigram. The triples are labeled as positive and negative examples of paraphrases. Table 2 shows that the feature vectors have 675 elements. We experiment\nwith two datasets, seven-choice and fourteen-choice noun-modifier questions (Turney, 2012).7"}, {"heading": "5.1 Noun-Modifier Questions", "text": "The first dataset is a seven-choice noun-modifier question dataset, constructed from WordNet (Turney, 2012). The dataset contains 680 questions for training and 1,500 for testing, a total of 2,180 questions. Table 7 shows one of the questions.\nThe stem is a bigram and the choices are unigrams. The bigram is composed of a head noun (world), modified by an adjective or noun (fantasy). The solution is the unigram (fairyland) that belongs to the same WordNet synset as the stem.\nThe distractors are designed to be difficult for current approaches to composition. For example, if fantasy world is represented by element-wise multiplication of the context vectors for fantasy and world (Mitchell and Lapata, 2010), the most likely guess is fantasy or world, not fairyland (Turney, 2012).\nEach seven-choice question yields seven labeled triples, by combining the stem with each choice. The triple \u3008fantasy, world, fairyland\u3009 is labeled positive and the other six triples are labeled negative.\nIn general, if \u3008a, b, c\u3009 is a positive example, then \u3008b, a, c\u3009 is negative. For example, world fantasy is not a paraphrase of fairyland. The second dataset is constructed by applying this shuffling transformation to convert the 2,180 sevenchoice questions into 2,180 fourteen-choice questions (Turney, 2012). The second dataset is designed\n7The seven-choice dataset is available at http://jair.org/papers/paper3640.html. The fourteen-choice dataset can be generated from the seven-choice dataset.\nto be difficult for approaches that are not sensitive to word order.\nTable 8 shows the percentage of the testing questions that are answered correctly for the two datasets. Because vector addition and element-wise multiplication are not sensitive to word order, they perform poorly on the fourteen-choice questions. For both datasets, SuperSim performs significantly better than all other approaches, except for the holistic approach, according to Fisher\u2019s exact test at the 95% confidence level.8\nThe holistic approach is noncompositional. The stem bigram is represented by a single context vector, generated by treating the bigram as if it were a unigram. A noncompositional approach cannot scale up to realistic applications (Turney, 2012). The holistic approach cannot be applied to the fourteenchoice questions, because the bigrams in these questions do not correspond to terms in WordNet, and hence they do not correspond to row vectors in the matrices we use (see Section 3).\nTurney (2012) found it necessary to hand-code a soundness check into all of the algorithms (vector addition, element-wise multiplication, dual-space, and holistic). Given a stem ab and a choice c, the hand-coded check assigns a minimal score to the choice if c = a or c = b. We do not need to handcode any checking into SuperSim. It learns automatically from the training data to avoid such choices."}, {"heading": "5.2 Ablation Experiments", "text": "Table 9 shows the effects of ablating sets of features on the performance of SuperSim with the fourteen-choice questions. PPMI features are the most important; by themselves, they achieve 59.7% correct, although the other features are needed to\n8The results for SuperSim are new but the other results in Table 8 are from Turney (2012).\nreach 68.0%. Domain space features reach the second highest performance when used alone (34.6%), but they reduce performance (from 69.3% to 68.0%) when combined with other features; however, the drop is not significant according to Fisher\u2019s exact test at the 95% significance level.\nSince the PPMI features play an important role in answering the noun-modifier questions, let us take a closer look at them. From Table 2, we see that there are twelve PPMI features for the triple \u3008a, b, c\u3009, where ab is a noun-modifier bigram and c is a noun unigram. We can split the twelve features into three subsets, one subset for each pair of words, \u3008a, b\u3009, \u3008a, c\u3009, and \u3008b, c\u3009. For example, the subset for \u3008a, b\u3009 is the four features PPMI(a, b, left), PPMI(b, a, left), PPMI(a, b, right), and PPMI(b, a, right). Table 10 shows the effects of ablating these subsets.\nThe results in Table 10 indicate that all three PPMI subsets contribute to the performance of SuperSim, but the \u3008a, b\u3009 subset contributes more than the other two subsets. The \u3008a, b\u3009 features help\nto increase the sensitivity of SuperSim to the order of the words in the noun-modifier bigram; for example, they make it easier to distinguish fantasy world from world fantasy."}, {"heading": "5.3 Holistic Training", "text": "SuperSim uses 680 training questions to learn to recognize when a bigram is a paraphrase of a unigram; it learns from expert knowledge implicit in WordNet synsets. It would be advantageous to be able to train SuperSim with less reliance on expert knowledge.\nPast work with adjective-noun bigrams has shown that we can use holistic bigram vectors to train a supervised regression model (Guevara, 2010; Baroni and Zamparelli, 2010). The output of the regression model is a vector representation for a bigram that approximates the holistic vector for the bigram; that is, it approximates the vector we would get by treating the bigram as if it were a unigram.\nSuperSim does not generate vectors as output, but we can still use holistic bigram vectors for training. Table 11 shows a seven-choice training question that was generated without using WordNet synsets. The choices of the form a b are bigrams, but we represent them with holistic bigram vectors; we pretend they are unigrams. We call a b bigrams pseudounigrams. As far as SuperSim is concerned, there is no difference between these pseudo-unigrams and true unigrams. The question in Table 11 is treated the same as the question in Table 7.\nWe generate 680 holistic training questions by randomly selecting 680 noun-modifier bigrams from WordNet as stems for the questions (search engine), avoiding any bigrams that appear as stems in the testing questions. The solution (search engine) is the pseudo-unigram that corresponds to the\nstem bigram. In the matrices in Section 3, each term in WordNet corresponds to a row vector. These corresponding row vectors enable us to treat bigrams from WordNet as if they were unigrams. The distractors are the component unigrams in the stem bigram (search and engine) and pseudounigrams that share a component word with the stem (search warrant, diesel engine). To construct the holistic training questions, we used WordNet as a source of bigrams, but we ignored the rich information that WordNet provides about these bigrams, such as their synonyms, hypernyms, hyponyms, meronyms, and glosses.\nTable 12 compares holistic training to standard training (that is, training with questions like Table 11 versus training with questions like Table 7). The testing set is the standard testing set in both cases. There is a significant drop in performance with holistic training, but the performance still surpasses vector addition, element-wise multiplication, and the hand-coded dual-space model (see Table 8).\nSince holistic questions can be generated automatically without human expertise, we experimented with increasing the size of the holistic training dataset, growing it from 1,000 to 10,000 questions in increments of 1,000. The performance on the fourteen-choice questions with holistic training and standard testing varied between 53.3% and 55.1% correct, with no clear trend up or down. This is not significantly different from the performance with 680 holistic training questions (54.4%).\nIt seems likely that the drop in performance with holistic training instead of standard training is due to a difference in the nature of the standard questions (Table 7) and the holistic questions (Table 11). We are currently investigating this issue. We expect to be able to close the performance gap in future work, by improving the holistic questions. However, it is possible that there are fundamental limits to holistic training."}, {"heading": "6 Discussion", "text": "SuperSim performs slightly better (not statistically significant) than the hand-coded dual-space model on relational similarity problems (Section 4), but it performs much better on compositional similarity problems (Section 5). The ablation studies suggest this is due to the PPMI features, which have no effect on ten-choice SAT performance (Table 5), but have a large effect on fourteen-choice noun-modifier paraphrase performance (Table 9).\nOne advantage of supervised learning over handcoding is that it facilitates adding new features. It is not clear how to modify the hand-coded equations for the dual-space model of noun-modifier composition (Turney, 2012) to include PPMI information.\nSuperSim is one of the few approaches to distributional semantics beyond words that has attempted to address both relational and compositional similarity (see Section 2.3). It is a strength of this approach that it works well with both kinds of similarity."}, {"heading": "7 Future Work and Limitations", "text": "Given the promising results with holistic training for noun-modifier paraphrases, we plan to experiment with holistic training for analogies. Consider the proportional analogy hard is to hard time as good is to good time, where hard time and good time are pseudo-unigrams. To a human, this analogy is trivial, but SuperSim has no access to the surface form of a term. As far as SuperSim is concerned, this analogy is much the same as the analogy hard is to difficulty as good is to fun. This strategy automatically converts simple, easily generated analogies into more complex, challenging analogies, which may be suited to training SuperSim.\nThis also suggests that noun-modifier paraphrases may be used to solve analogies. Perhaps we can evaluate the quality of a candidate analogy \u3008a, b, c, d\u3009 by searching for a term e such that \u3008b, e, a\u3009 and \u3008d, e, c\u3009 are good paraphrases. For example, consider the analogy mason is to stone as carpenter is to wood. We can paraphrase mason as stone worker and carpenter as wood worker. This transforms the analogy to stone worker is to stone as wood worker is to wood, which makes it easier to recognize the relational similarity.\nAnother area for future work is extending SuperSim beyond noun-modifier paraphrases to measuring the similarity of sentence pairs. We plan to adapt ideas from Socher et al. (2011) for this task. They use dynamic pooling to represent sentences of varying size with fixed-size feature vectors. Using fixedsize feature vectors avoids the problem of quadratic growth and it enables the supervised learner to generalize over sentences of varying length.\nSome of the competing approaches discussed by Erk (2013) incorporate formal logic. The work of Baroni et al. (2012) suggests ways that SuperSim could be developed to deal with logic.\nWe believe that SuperSim could benefit from more features, with greater diversity. One place to look for these features is higher levels in the hierarchy that we sketch in Section 3.\nOur ablation experiments suggest that domain and function spaces provide the most important features for relational similarity, but PPMI values provide the most important features for noun-modifier compositional similarity. Explaining this is another topic for future research."}, {"heading": "8 Conclusion", "text": "In this paper, we have presented SuperSim, a unified approach to analogy (relational similarity) and paraphrase (compositional similarity). SuperSim treats them both as problems of supervised tuple classification. The supervised learning algorithm is a standard support vector machine. The main contribution of SuperSim is a set of four types of features for representing tuples. The features work well with both analogy and paraphrase, with no task-specific modifications. SuperSim matches the state of the art on SAT analogy questions and substantially advances the state of the art on the SemEval 2012 Task 2 challenge and the noun-modifier paraphrase questions.\nSuperSim runs much faster than LRA (Turney, 2006b), answering the SAT questions in minutes instead of days. Unlike the dual-space model (Turney, 2012), SuperSim requires no handcoded similarity composition functions. Since there is no hand-coding, it is easy to add new features to SuperSim. Much work remains to be done, such as incorporating logic and scaling up to sentence\nparaphrases, but past work suggests that these problems are tractable.\nIn the four approaches described by Erk (2013), SuperSim is an instance of the second approach to extending distributional semantics beyond words, comparing word pairs, phrases, or sentences (in general, tuples) by combining multiple pairwise similarity values. Perhaps the main significance of this paper is that it provides some evidence in support of this general approach."}], "references": [{"title": "Mona Diab", "author": ["Eneko Agirre", "Daniel Cer"], "venue": "and Aitor Gonzalez-Agirre.", "citeRegEx": "Agirre et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "A survey of paraphrasing and textual entailment methods", "author": ["Androutsopoulos", "Prodromos Malakasiotis"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Androutsopoulos et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Androutsopoulos et al\\.", "year": 2010}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Baroni", "Zamparelli2010] Marco Baroni", "Roberto Zamparelli"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Ngoc-Quynh Do", "author": ["Marco Baroni", "Raffaella Bernardi"], "venue": "and Chung-chieh Shan.", "citeRegEx": "Baroni et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Susan E", "author": ["Isaac I. Bejar", "Roger Chaffin"], "venue": "Embretson.", "citeRegEx": "Bejar et al.1991", "shortCiteRegEx": null, "year": 1991}, {"title": "Clustering word pairs to answer analogy questions", "author": ["Bi\u00e7ici", "Yuret2006] Ergun Bi\u00e7ici", "Deniz Yuret"], "venue": "In Proceedings of the Fifteenth Turkish Symposium on Artificial Intelligence and Neural Networks (TAINN", "citeRegEx": "Bi\u00e7ici et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bi\u00e7ici et al\\.", "year": 2006}, {"title": "Yutaka Matsuo", "author": ["Danushka Bollegala"], "venue": "and Mitsuru Ishizuka.", "citeRegEx": "Bollegala et al.2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Yutaka Matsuo", "author": ["Danushka Bollegala"], "venue": "and Mitsuru Ishizuka.", "citeRegEx": "Bollegala et al.2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Extracting semantic representations from word co-occurrence statistics: A computational study", "author": ["Bullinaria", "Levy2007] John Bullinaria", "Joseph Levy"], "venue": "Behavior Research Methods,", "citeRegEx": "Bullinaria et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bullinaria et al\\.", "year": 2007}, {"title": "Extracting semantic representations from word co-occurrence statistics: Stop-lists, stemming, and SVD", "author": ["Bullinaria", "Levy2012] John Bullinaria", "Joseph Levy"], "venue": "Behavior Research Methods,", "citeRegEx": "Bullinaria et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bullinaria et al\\.", "year": 2012}, {"title": "Experiments with LSA scoring: Optimal rank and basis", "author": ["John Caron"], "venue": "In Proceedings of the SIAM Computational Information Retrieval Workshop,", "citeRegEx": "Caron.,? \\Q2001\\E", "shortCiteRegEx": "Caron.", "year": 2001}, {"title": "Word association norms, mutual information, and lexicography", "author": ["Church", "Hanks1989] Kenneth Church", "Patrick Hanks"], "venue": "In Proceedings of the 27th Annual Conference of the Association of Computational Linguistics,", "citeRegEx": "Church et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Church et al\\.", "year": 1989}, {"title": "A contexttheoretic framework for compositionality in distributional semantics", "author": ["Daoud Clarke"], "venue": "Computational Linguistics,", "citeRegEx": "Clarke.,? \\Q2012\\E", "shortCiteRegEx": "Clarke.", "year": 2012}, {"title": "Towards a semantics for distributional representations", "author": ["Katrin Erk"], "venue": "In Proceedings of the 10th International Conference on Computational Semantics (IWCS", "citeRegEx": "Erk.,? \\Q2013\\E", "shortCiteRegEx": "Erk.", "year": 2013}, {"title": "A synopsis of linguistic theory 1930\u20131955", "author": ["John Rupert Firth"], "venue": "In Studies in Linguistic Analysis,", "citeRegEx": "Firth.,? \\Q1957\\E", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "Katrin Erk", "author": ["Dan Garrette"], "venue": "and Ray Mooney.", "citeRegEx": "Garrette et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Structure-mapping: A theoretical framework for analogy", "author": ["Dedre Gentner"], "venue": "Cognitive Science,", "citeRegEx": "Gentner.,? \\Q1983\\E", "shortCiteRegEx": "Gentner.", "year": 1983}, {"title": "Peter Turney", "author": ["Roxana Girju", "Preslav Nakov", "Vivi Nastase", "Stan Szpakowicz"], "venue": "and Deniz Yuret.", "citeRegEx": "Girju et al.2007", "shortCiteRegEx": null, "year": 2007}, {"title": "A regression model of adjective-noun compositionality in distributional semantics", "author": ["Emiliano Guevara"], "venue": "In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics (GEMS", "citeRegEx": "Guevara.,? \\Q2010\\E", "shortCiteRegEx": "Guevara.", "year": 2010}, {"title": "Lorenza Romano", "author": ["Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti"], "venue": "and Stan Szpakowicz.", "citeRegEx": "Hendrickx et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Bagpack: A general framework to represent semantic relations", "author": ["Herda\u011fdelen", "Baroni2009] Ama\u00e7 Herda\u011fdelen", "Marco Baroni"], "venue": "In Proceedings of the EACL", "citeRegEx": "Herda\u011fdelen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Herda\u011fdelen et al\\.", "year": 2009}, {"title": "The class imbalance problem: A systematic study", "author": ["Japkowicz", "Stephen2002] Nathalie Japkowicz", "Shaju Stephen"], "venue": "Intelligent Data Analysis,", "citeRegEx": "Japkowicz et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Japkowicz et al\\.", "year": 2002}, {"title": "and Keith J", "author": ["David A. Jurgens", "Saif M. Mohammad", "Peter D. Turney"], "venue": "Holyoak.", "citeRegEx": "Jurgens et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Landauer and Susan T", "author": ["K Thomas"], "venue": "Dumais.", "citeRegEx": "Landauer and Dumais1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Effects of high-order co-occurrences on word semantic similarity. Current Psychology Letters: Behaviour", "author": ["Lemaire", "Denhi\u00e8re2006] Beno\u0131\u0302t Lemaire", "Guy Denhi\u00e8re"], "venue": "Brain & Cognition,", "citeRegEx": "Lemaire et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lemaire et al\\.", "year": 2006}, {"title": "Saussurian analogy: A theoretical account and its application", "author": ["Lepage", "Shin-ichi1996] Yves Lepage", "Ando Shin-ichi"], "venue": "In Proceedings of the 16th International Conference on Computational Linguistics (COLING", "citeRegEx": "Lepage et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Lepage et al\\.", "year": 1996}, {"title": "Curt Burgess", "author": ["Kevin Lund"], "venue": "and Ruth Ann Atchley.", "citeRegEx": "Lund et al.1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Wen-tau Yih", "author": ["Tomas Mikolov"], "venue": "and Geoffrey Zweig.", "citeRegEx": "Mikolov et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Mitchell", "Lapata2008] Jeff Mitchell", "Mirella Lapata"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Mitchell", "Lapata2010] Jeff Mitchell", "Mirella Lapata"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2010}, {"title": "Duluth: Measuring degrees of relational similarity with the gloss vector measure of semantic relatedness", "author": ["Ted Pedersen"], "venue": "In First Joint Conference on Lexical and Computational Semantics (*SEM),", "citeRegEx": "Pedersen.,? \\Q2012\\E", "shortCiteRegEx": "Pedersen.", "year": 2012}, {"title": "Fast training of support vector machines using sequential minimal optimization", "author": ["John C. Platt"], "venue": "In Advances in Kernel Methods: Support Vector Learning,", "citeRegEx": "Platt.,? \\Q1998\\E", "shortCiteRegEx": "Platt.", "year": 1998}, {"title": "UTD: Determining relational similarity using lexical patterns", "author": ["Rink", "Harabagiu2012] Bryan Rink", "Sanda Harabagiu"], "venue": "In First Joint Conference on Lexical and Computational Semantics (*SEM),", "citeRegEx": "Rink et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rink et al\\.", "year": 2012}, {"title": "The impact of selectional preference agreement on semantic relational similarity", "author": ["Rink", "Harabagiu2013] Bryan Rink", "Sanda Harabagiu"], "venue": "In Proceedings of the 10th International Conference on Computational Semantics (IWCS", "citeRegEx": "Rink et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rink et al\\.", "year": 2013}, {"title": "and Christopher D", "author": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng"], "venue": "Manning.", "citeRegEx": "Socher et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Christopher Manning", "author": ["Richard Socher", "Brody Huval"], "venue": "and Andrew Ng.", "citeRegEx": "Socher et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "David Pinto", "author": ["Mireya Tovar", "J. Alejandro Reyes", "Azucena Montes", "Darnes Vilari\u00f1o"], "venue": "and Saul Le\u00f3n.", "citeRegEx": "Tovar et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Turney and Michael L", "author": ["D Peter"], "venue": "Littman.", "citeRegEx": "Turney and Littman2005", "shortCiteRegEx": null, "year": 2005}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter D. Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Jeffrey Bigham", "author": ["Peter D. Turney", "Michael L. Littman"], "venue": "and Victor Shnayder.", "citeRegEx": "Turney et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Dan Assaf", "author": ["Peter D. Turney", "Yair Neuman"], "venue": "and Yohai Cohen.", "citeRegEx": "Turney et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Expressing implicit semantic relations without supervision", "author": ["Peter D. Turney"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Turney.,? \\Q2006\\E", "shortCiteRegEx": "Turney.", "year": 2006}, {"title": "Similarity of semantic relations", "author": ["Peter D. Turney"], "venue": "Computational Linguistics,", "citeRegEx": "Turney.,? \\Q2006\\E", "shortCiteRegEx": "Turney.", "year": 2006}, {"title": "The latent relation mapping engine: Algorithm and experiments", "author": ["Peter D. Turney"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney.,? \\Q2008\\E", "shortCiteRegEx": "Turney.", "year": 2008}, {"title": "A uniform approach to analogies, synonyms, antonyms, and associations", "author": ["Peter D. Turney"], "venue": "In Proceedings of the 22nd International Conference on Computational Linguistics (Coling", "citeRegEx": "Turney.,? \\Q2008\\E", "shortCiteRegEx": "Turney.", "year": 2008}, {"title": "Domain and function: A dual-space model of semantic relations and compositions", "author": ["Peter D. Turney"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney.,? \\Q2012\\E", "shortCiteRegEx": "Turney.", "year": 2012}, {"title": "WordNet sits the SAT: A knowledge-based approach to lexical analogy", "author": ["Tony Veale"], "venue": "In Proceedings of the 16th European Conference on Artificial Intelligence (ECAI", "citeRegEx": "Veale.,? \\Q2004\\E", "shortCiteRegEx": "Veale.", "year": 2004}, {"title": "and Mark A", "author": ["Ian H. Witten", "Eibe Frank"], "venue": "Hall.", "citeRegEx": "Witten et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Geoffrey Zweig", "author": ["Alisa Zhila", "Wen-tau Yih", "Christopher Meek"], "venue": "and Tomas Mikolov.", "citeRegEx": "Zhila et al.2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [], "year": 2013, "abstractText": "There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous). One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples. A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). However, past work required hand-coding the combination function for different tasks. The main contribution of this paper is that combination functions are generated by supervised learning. We achieve state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval 2012 Task 2) and measuring compositional similarity between nounmodifier phrases and unigrams (multiplechoice paraphrase questions).", "creator": "LaTeX with hyperref package"}}}