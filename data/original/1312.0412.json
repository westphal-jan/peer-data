{"id": "1312.0412", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2013", "title": "Practical Collapsed Stochastic Variational Inference for the HDP", "abstract": "Recent advances have made it feasible to apply the stochastic variational paradigm to a collapsed representation of latent Dirichlet allocation (LDA). While the stochastic variational paradigm has successfully been applied to an uncollapsed representation of the hierarchical Dirichlet process (HDP), no attempts to apply this type of inference in a collapsed setting of non-parametric topic modeling have been put forward so far. In this paper we explore such a collapsed stochastic variational Bayes inference for the HDP. The proposed online algorithm is easy to implement and accounts for the inference of hyper-parameters. First experiments show a promising improvement in predictive performance.", "histories": [["v1", "Mon, 2 Dec 2013 10:58:01 GMT  (125kb,D)", "http://arxiv.org/abs/1312.0412v1", "NIPS Workshop; Topic Models: Computation, Application, and Evaluation"]], "COMMENTS": "NIPS Workshop; Topic Models: Computation, Application, and Evaluation", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["arnim bleier"], "accepted": false, "id": "1312.0412"}, "pdf": {"name": "1312.0412.pdf", "metadata": {"source": "CRF", "title": "Practical Collapsed Stochastic Variational Inference for the HDP", "authors": ["Arnim Bleier"], "emails": ["arnim.bleier@gesis.org"], "sections": [{"heading": "1 Background", "text": "We begin by considering a model where each document d is a mixture \u03b8d of K discrete topicdistributions \u03c6k over a vocabulary of V terms. Let zdi \u2208 {1, ..,K} denote the topic of the ith word wdi \u2208 {1, .., V } in document d \u2208 {1, .., D} and place Dirichlet priors on the parameters \u03b8d, \u03c6k. We have\nzdi | \u03b8d \u223c Discrete(\u03b8d) , \u03b8d \u223c Dirichlet(\u03b1\u03c0) , wdi | zdi, {\u03c6k} \u223c Discrete(\u03c6zdi) , \u03c6k \u223c Dirichlet(\u03b2) ,\nwhere \u03c0 is the top-level distribution over topics, and \u03b1 and \u03b2 are concentration parameters. While the dimensionality of K is fixed in latent Dirichlet allocation (LDA), we want the model to determine the number of topics needed. Consequently we follow the assumptions made by the hierarchical Dirichlet process (HDP) [1] of a countable but infinite number of topics, of which only a finite number is used in the posterior. Our prior \u03c0 is constructed by a truncated sick-breaking process [2],\n\u03c0k = \u03c0\u0304k k\u22121\u220f l=1 (1\u2212 \u03c0\u0304l) , \u03c0\u0304k | \u03b3 \u223c Beta(1, \u03b3) , \u03c0\u0304T = 1 , (1)\nwhere \u03c0\u0304 are the stick proportions. T is the truncation level and not the number of topics; if set to an appropriate level (i.e. T > K) the truncated stick-breaking process is a sufficient approximation of the Dirichlet process.\nIn the reminder of this paper we start by reviewing variational batch inference for the collapsed representation of the HDP. We then introduce our proposed stochastic updates. After that an early evaluation of our algorithm is presented. We conclude with a discussion of our work and its current limitations.\nar X\niv :1\n31 2.\n04 12\nv1 [\ncs .L\nG ]\n2 D\nec 2"}, {"heading": "2 Practical Collapsed Variational Inference", "text": "In this section we review practical batch collapsed variational Bayes inference (PCVB0) proposed by Sato et al. [3] which later will be the fundament of our stochastic inference. The collapsed representation of the HDP is achieved by marginalizing over \u03b8 and \u03c6. If only zero-order information is used, the update for the variational distributions zdi over T possible topic assignments for each word is given by\nq(zdi = k) \u221d (n\u00acdidk + \u03b1\u03c0k) n\u00acdikwdi + \u03b2\nn\u00acdik. + V \u03b2 . (2)\nWhere n\u00acdidk is the number of times a word in document d has been assigned to topic k, and n \u00acdi kwdi is the number of times the term wdi has been assigned to topic k, in both cases excluding the current word di. Furthermore . is used in place of a variable to indicate that the sum over its values (i.e. nk. = \u2211 w nkw) is taken. Teh\u2019s [2] original variational Bayes inference required maintaining variance counts for \u03b1\u03c0. In a response Sato et al. [3] showed the usefulness of a lower-bound approximation for the number of tables in the Dirichlet process Chinese Restaurant representation. Leading to the update of the corpus-wide topic popularity\n\u03c0k = \u03c0\u0304k k\u22121\u220f l=1 (1\u2212 \u03c0\u0304l) , \u03c0\u0304k = uk uk + vk , (3)\nuk = 1 + \u2211 d E[1(ndk \u2265 1)] , vk = \u03b30 + T\u2211 l=k+1,d E[1(ndl \u2265 1)] , (4)\nwhere 1(.) is the indicator function and the expectation E[1(ndk \u2265 1)] = 1\u2212 \u220f i(1\u2212 q(zdi = k)). Moreover, using point estimates the updates for the hyper-parameters \u03b1 and \u03b3 are\n\u03b1 = \u2211 d,k E[1(ndk \u2265 1)]\u2211\nd[\u03a8(nd + \u03b1 old)\u2212\u03a8(\u03b1old)]\n, (5)\n\u03b3 = T \u2212 1\u2211T\u22121\nk=1 [\u03a8(uk + vk)\u2212\u03a8(vk)] , (6)\nwith nd being the number of words in document d and \u03a8(.) the digamma function."}, {"heading": "3 Proposed Updates", "text": "One of the main drawbacks of batch collapsed variational inference for the HDP are its high memory requirements. We propose to circumvent this. Following the ideas behind stochastic collapsed variational Bayesian inference (SCVB0) proposed by Foulds et al. [4] we potentially allow for data to arrive in a stream, but maintain the simplicity of the original PCVB0 schema.\nThe practical collapsed stochastic variational Bayes inference for the hierarchical Dirichlet process (PCSVB0), we propose, processes one word at a time, serially processing each word from all documents in turn. Suppose we have a guess of the current PCVB0 statistics. Next, we draw the ith word wdi of document d and compute its corresponding zdi via Equation (2). The expected number of times k appears in the document ndk, with respect to the current word, is ndq(zdi = k). Furthermore, the expected number of times nkw topic k is used by term wdi is nq(zdi = k) and zero for all other terms, with n being the total number of words in the corpus. As we process word by word we compute new zdi\u2019s, but do not store them. Consequently, we cannot subtract the current word in Equation (2) and approximate the distribution by\nq(zdi = k) \u221d (ndk + \u03b1\u03c0k) nkwdi + \u03b2\nnk. + V \u03b2 . (7)\nWith the variational distribution zdi of the current word we are able to update the expected statistics for ndk and nkw via\nndk \u2190 (1\u2212 \u03c1dt )ndk + \u03c1dtndq(zdi = k) , (8) nkw \u2190 (1\u2212 \u03c1ct)nkw + \u03c1ctnq(zdi = k)1[wdi = w] , (9)\nand \u03c1t is the step-size in update t. The parameters u and v for the the stick-breaking proportions required in the computation of Equation (3) and Equation (6) are updated after a document is processed. We re-order the sticks according to their sizes. The updates are\nuk \u2190 (1\u2212 \u03c1ht )uk + \u03c1ht (1 +DE[1(ndk \u2265 1)]) , (10)\nvk \u2190 (1\u2212 \u03c1ht )vk + \u03c1ht (\u03b3 +D T\u2211\nl=k+1\nE[1(ndl \u2265 1)]) . (11)\nFor the stochastic update of \u03b1we again assume that our entire corpus consists of the single document d repeated D times, leading to\n\u03b1\u2190 (1\u2212 \u03c1ht )\u03b1+ \u03c1ht\n(\u2211T k=1 E[1(ndk \u2265 1)]\n\u03a8(nd + \u03b1)\u2212\u03a8(\u03b1)\n) . (12)\nThis suggests an iterative procedure, altering between approximating zdi and updating the expected count statistics word by word, and updating the global topic popularity along the hyper-parameters document by document.\nAlgorithm 1 PCSVB0 HDP inference 1: Initialize nkw, ndk, \u03c0, \u03b1, \u03b3. 2: Set step-size schedule for \u03c1dt , \u03c1 c t and \u03c1 h t .\n3: repeat 4: for each document d do 5: for each word i in d do 6: Compute q(zdi = k) (Equation 7). 7: Update ndk (Equation 8). 8: Update nkw (Equation 9). 9: end for\n10: Update \u03c0k (Equation 3 with 10,11). 11: Update \u03b3 and \u03b1 (Equation 6,12). 12: end for 13: until stopping criterion is met."}, {"heading": "4 Evaluation", "text": "In this section we describe a first experimental analysis of the proposed PCSVB0 inference. We studied the predictive performance of the algorithm on the Associated Press (TREC-1) data. The dataset contains 398k tokens across 2250 documents, with a vocabulary size of 10932 unique terms. For the evaluation we compared the perplexity versus the number of documents seen for PCSVB0, SCVB0 and PCVB0. We trained the model on 80% of the documents. All held out documents were split; 70% of the tokens in each held out document were used to estimate the document parameters, the remaining 30% were used to compute the perplexity.\nWe used the step-size schedule \u03c1t = s(\u03c4+t)0.9 . For the update of nkw in iteration t the schedule \u03c1 c t was parameterized with s = 10 and \u03c4 = 1000; the schedule for ndk \u03c1dt was parameterized with s = 1 and \u03c4 = 10; and the schedule for the global stick-breaking weights and hyper-parameter\nupdates \u03c1ht was parameterized with s = 5 and \u03c4 = 100. The prior on the topic-simplex \u03b2 was set for all algorithms to \u03b2 = 0.01. The prior on the document-simplex for SCVB0 was set to \u03b1 = 0.1. Furthermore, for PCSVB0 as well as PCVB0 a truncation-level of T = 200 was used."}, {"heading": "5 Discussion", "text": "We presented a collapsed stochastic variational inference algorithm for the HDP-LDA topic model. Our algorithm is based on the application of a practical lower bound approximation of the truncated stick-breaking process to a collapsed stochastic inference scheme and simpler to implement then other uncollapsed online variational inference algorithms for the HDP[5]. Initial small-scale experiments show promising improvements of PCSVB0 in predictive performance over existing algorithms, both in terms of the rate of convergence and the found optimum. Directions for future work are the application of so called \u2018clumping\u2019 in order to perform the update only for each distinct term per document, and then to scale the update by the number of its copies. Another direction is the usage of mini-batches. Such optimizations would improve wall-clock time per iteration and allow for a fair comparison with other online variational inference algorithms for the HDP."}], "references": [{"title": "Hierarchical Dirichlet Processes", "author": ["Y.W. Teh", "M. Jordan", "M. Beal", "D. Blei"], "venue": "Journal of the American Statistical Association", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Collapsed variational inference for HDP", "author": ["Y.W. Teh", "K. Kurihara", "M. Welling"], "venue": "Advances in Neural Information Processing Systems. pp", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Practical Collapsed Variational Bayes Inference for Hierarchical Dirichlet Process", "author": ["I. Sato", "K. Kurihara", "H. Nakagawa"], "venue": "Proceedings of the 18 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet Allocation", "author": ["J. Foulds", "L. Boyles", "C. Dubois", "P. Smyth", "M. Welling"], "venue": "Proceedings of the 19 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Online Variational Inference for the Hierarchical Dirichlet Processes", "author": ["C. Wang", "J. Paisley", "D. Blei"], "venue": "Proceedings of the 14 International Conference on Artificial Intelligence and Statistic", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Consequently we follow the assumptions made by the hierarchical Dirichlet process (HDP) [1] of a countable but infinite number of topics, of which only a finite number is used in the posterior.", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "Our prior \u03c0 is constructed by a truncated sick-breaking process [2],", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "[3] which later will be the fundament of our stochastic inference.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Teh\u2019s [2] original variational Bayes inference required maintaining variance counts for \u03b1\u03c0.", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "[3] showed the usefulness of a lower-bound approximation for the number of tables in the Dirichlet process Chinese Restaurant representation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] we potentially allow for data to arrive in a stream, but maintain the simplicity of the original PCVB0 schema.", "startOffset": 0, "endOffset": 3}], "year": 2013, "abstractText": "Recent advances have made it feasible to apply the stochastic variational paradigm to a collapsed representation of latent Dirichlet allocation (LDA). While the stochastic variational paradigm has successfully been applied to an uncollapsed representation of the hierarchical Dirichlet process (HDP), no attempts to apply this type of inference in a collapsed setting of non-parametric topic modeling have been put forward so far. In this paper we explore such a collapsed stochastic variational Bayes inference for the HDP. The proposed online algorithm is easy to implement and accounts for the inference of hyper-parameters. First experiments show a promising improvement in predictive performance. 1 Background We begin by considering a model where each document d is a mixture \u03b8d of K discrete topicdistributions \u03c6k over a vocabulary of V terms. Let zdi \u2208 {1, ..,K} denote the topic of the i word wdi \u2208 {1, .., V } in document d \u2208 {1, .., D} and place Dirichlet priors on the parameters \u03b8d, \u03c6k. We have zdi | \u03b8d \u223c Discrete(\u03b8d) , \u03b8d \u223c Dirichlet(\u03b1\u03c0) , wdi | zdi, {\u03c6k} \u223c Discrete(\u03c6zdi) , \u03c6k \u223c Dirichlet(\u03b2) , where \u03c0 is the top-level distribution over topics, and \u03b1 and \u03b2 are concentration parameters. While the dimensionality of K is fixed in latent Dirichlet allocation (LDA), we want the model to determine the number of topics needed. Consequently we follow the assumptions made by the hierarchical Dirichlet process (HDP) [1] of a countable but infinite number of topics, of which only a finite number is used in the posterior. Our prior \u03c0 is constructed by a truncated sick-breaking process [2],", "creator": "LaTeX with hyperref package"}}}