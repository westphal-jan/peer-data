{"id": "1312.5770", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Consistency of Causal Inference under the Additive Noise Model", "abstract": "We analyze a family of methods for statistical causal inference from sample under the so-called Additive Noise Model. While most work on the subject has concentrated on establishing the soundness of the Additive Noise Model, the statistical consistency of the resulting inference methods has received little attention. We derive general conditions under which the given family of inference methods consistently infers the causal direction in a nonparametric setting.", "histories": [["v1", "Thu, 19 Dec 2013 22:15:40 GMT  (55kb,D)", "https://arxiv.org/abs/1312.5770v1", null], ["v2", "Mon, 23 Dec 2013 16:59:04 GMT  (55kb,D)", "http://arxiv.org/abs/1312.5770v2", null], ["v3", "Wed, 5 Feb 2014 03:37:30 GMT  (65kb,D)", "http://arxiv.org/abs/1312.5770v3", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["samory kpotufe", "eleni sgouritsa", "dominik janzing", "bernhard sch\u00f6lkopf"], "accepted": true, "id": "1312.5770"}, "pdf": {"name": "1312.5770.pdf", "metadata": {"source": "META", "title": "Consistency of Causal Inference under the Additive Noise Model", "authors": ["Samory Kpotufe", "Eleni Sgouritsa", "Dominik Janzig", "Bernhard Sch\u00f6lkopf"], "emails": ["SAMORY@TTIC.EDU", "ELENI.SGOURITSA@TUEBINGEN.MPG.DE", "DOMINIK.JANZIG@TUEBINGEN.MPG.DE", "BS@TUEBINGEN.MPG.DE"], "sections": [{"heading": "1. Introduction", "text": "Drawing causal conclusions for a set of observed variables given a sample from their joint distribution is a fundamental problem in science. Conditional-independence-based methods (Pearl, 2000; Spirtes et al., 2000) estimate a set of directed acyclic graphs, all entailing the same conditional independences, from the data. However, these methods can not distinguish between two graphs that entail the same set of conditional independences, the so-called Markov equivalent graphs. Consider for example the case of only two observed dependent random variables. Conditionalindependence-based methods can not recover the causal graph since X \u2192 Y and Y \u2192 X are Markov equivalent. An elegant basis for causal graphs is the framework of structural causal models (SCMs) (Pearl, 2000), where every observable is a function of its parents and an unobserved independent noise term. This allows us to formulate\nLong ArXiv Version.\nan assumption on function classes which lets us infer the causal direction in two-variable case.\nA special case of SCMs is the Causal Additive Noise Model (CAM) (Shimizu et al., 2006; Hoyer et al., 2009; Tillman et al., 2009; Peters et al., 2011a;b) which is given as follows: given two random variables X and Y , X is assumed to cause Y if (i) Y can be obtained as a function of X plus a noise term independent of X , but (ii) X cannot be obtained as a function of Y plus independent noise, then we infer that X causes Y . In this case, where (i) and (ii) hold simultaneously, the CAM is termed identifiable.\nInitial work on the CAM focused on establishing its theoretical soundness, i.e. understanding the class of distributions PX,Y for which the CAM is identifiable, i.e. for which (i) and (ii) hold simultaneously. Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + \u03b7 is linear, provided the independent noise \u03b7 is not Gaussian. Later, Hoyer et al. (2009), Zhang & Hyva\u0308rinen (2009) and Peters et al. (2011a) showed that the CAM is identifiable more generally even if f is nonlinear, the main technical requirements being that the marginals PX , and P\u03b7 are absolutely continuous on R, with P\u03b7 having support R. Note that Zhang & Hyva\u0308rinen (2009) also introduces a generalization of the CAM termed post-nonlinear models. Further work by Peters et al. (2011b) showed how to reduce causal inference for a network of multiple variables under the CAM to the case of two variables X and Y discussed so far, by properly extending the conditions (i) and (ii) to conditional distributions instead of marginals. Thus, the soundness of the CAM being established by these various works, the next natural question is to understand the statistical behavior of the resulting estimation procedures on ar X iv :1 31 2.\n57 70\nv3 [\ncs .L\nG ]\n5 F\nfinite samples.\nCurrent insights into this last question are mostly empirical. Various works (Shimizu et al., 2006; Hoyer et al., 2009; Peters et al., 2011a) have successfully validated procedures based on the CAM (outlined in Section 1.1 below) on a mix of artificial and real-world datasets where the causal structure to be inferred is clear. However, on the theoretical side, it remains unclear whether these procedures can infer causality from samples in general situations where the CAM is identifiable. In the particular case where the functional relation between X and Y is linear, Hyva\u0308rinen et al. (2008) proposed a successful method shown to be consistent. In a recent Arxived result appearing after our initial submission, Buhlmann et al. (2013) showed the consistency of a maximum log-likelihood approach to causal inference under the multi-variable network extension of Peters et al. (2011b).\nWhile consistency has been shown for particular procedures, in this paper we are rather interested in general conditions under which common approaches, with various algorithmic instantiations, are consistent. We derive both algorithmic and distributional conditions for statistical consistency in general situations where the CAM is identifiable. The present work focuses on the case of two real variables, allowing us to focus on the inherent difficulties of achieving consistency with the common algorithmic approaches. These difficulties, described in Section 1.2 have to do with estimating the degree of independence between noise and input, while the noise is itself estimated from the input and hence is inherently dependent on the input."}, {"heading": "1.1. Inference Methods Under the Additive Noise Model", "text": "Causal inference methods under the Additive Noise Model typically follow the meta-procedure below. Assume f and g are the best functional fits under some risk, respectively Y \u2248 f(X) and X \u2248 g(Y ):\nFit Y as a function f(X), obtain the residuals \u03b7Y,f = Y \u2212 f(X), fit X as a function g(Y ), obtain the residuals \u03b7X,g = X \u2212 g(Y ), decide X \u2192 Y if \u03b7Y,f \u22a5 X but \u03b7X,g 6\u22a5 Y , decide Y \u2192 X if the reverse holds true, abstain otherwise.\nInstantiations thus vary in the regression procedures employed for function fitting, and in the independence measures employed. Our analysis concerns procedures employing an entropy-based independence measure, which is cheaper than usual independence tests. These procedures vary in the regression and entropy estimators employed. They are presented in detail in Section 3."}, {"heading": "1.2. Towards Consistency: Main Difficulties", "text": "Assume (i) and (ii) hold so that X causes Y under the CAM. We want to detect this from sufficiently large finite samples. This is consistency in a rough sense.\nEstablishing consistency of the above meta-procedure faces many subtle difficulties. The above outlined algorithmic approach consists of four interdependent statistical estimation tasks, namely two regression problems and two independence-tests. Considered separately, the consistency of such estimation tasks is well understood, but in the present context the success of the independence tests is contingent on successful regression.\nThe main difficulty is that although we are observing X and Y , we are not observing the residuals \u03b7Y,f and \u03b7X,g , but empirical approximations \u03b7Y,fn and \u03b7X,gn obtained by estimating f and g as fn and gn on a sample of size n.\nFor now, consider just detecting that \u03b7Y,f , f unknown, is independent from X . A good estimator fn will ensure that fn and f are close, usually in an L2 sense (i.e. EX |fn(X)\u2212 f(X)|2 \u2248 0). Hence \u03b7Y,fn is close to \u03b7Y,f , but unfortunately this does not imply that \u03b7Y,fn \u22a5 X if \u03b7Y,f \u22a5 X . In fact it is easy to construct r.v.\u2019s A,B,C such thatA \u22a5 B, |B \u2212 C| < , for arbitrary , but C 6\u22a5 A. Thus, the estimate \u03b7Y,fn might be close to \u03b7Y,f , yet it might still appear dependent on X even if \u03b7Y,f is not. Complicating matters further, \u03b7Y,fn and \u03b7Y,f would only be close in an average sense (instead of close for every value of X) since fn and f are typically only close in an average sense (e.g. close in L2).\nNow consider the full causal discovery, i.e. consider also detecting that \u03b7X,g depends on Y . To achieve consistency, the independence test employed must detect more dependence between \u03b7X,gn and Y than between \u03b7Y,fn and X . This will depend on how the particular independence test is influenced by errors in the particular regression procedures employed, and the relative rates at which these various procedures converge.\nAs previously mentioned, we will consider a family of independence-tests based on comparing sums of entropies. We will handle the above difficulties and derive conditions for consistency by first understanding how the various estimated entropies converge as a function of regression convergence (L2 convergence).\nWe do not consider the question of finite-sample convergence rates for causal estimation under the CAM. In fact, it is not even clear whether it is generally possible to establish such rates. This is because it is generally possible that the Bayes best fits f(x) = E [Y |x] is smooth while g(y) = E [X|y] is not even continuous; yet it is well known that without smoothness or similar structural conditions, ar-\nbitrarily bad rates of convergence are possible in regression (see e.g. (Gyorfi et al., 2002), Theorem 3.1).\nHowever, along the way of deriving consistency, we analyze the convergence of various quantities, which appear to affect the finite-sample behavior of the meta-procedure. In particular the tails of the additive noise and the richness of the regression algorithms seem to have a strong effect on convergence. This is verified in controlled simulations. The theoretical details are discussed in Section 4."}, {"heading": "2. Preliminaries", "text": ""}, {"heading": "2.1. Setup and Notation", "text": "We letH and I denote respectively differential entropy, and mutual information (Cover et al., 1994). Given a density p we will at times use the (abuse of) notation H(p) when a r.v. is unspecified.\nThe distribution of a r.v. Z is denoted PZ , and its density when it exists is denoted pZ .\nThroughout the analysis we will be concerned with residuals from regression fits. We use the following notation. Definition 1. For a function f : R 7\u2192 R, we consider either of the residuals: \u03b7Y,f , Y \u2212f(X) and \u03b7X,f , X\u2212f(Y ).\nThe Causal Additive Noise Model is captured as follows: Definition 2 (CAM). Given r.v.\u2019sX,Y , a function f : R 7\u2192 R and a r.v. \u03b7, we write X f,\u03b7\u2212\u2212\u2192 Y if the following holds:\n(i) PX,Y is generated as X \u223c PX , and Y = f(X) + \u03b7, where the noise r.v. \u03b7 has 0 mean and \u03b7 \u22a5 X;\n(ii) for any g : R 7\u2192 R, \u03b7X,g , X \u2212 g(Y ) depends on X .\nWe write X \u2192 Y when f and \u03b7 are clear from context."}, {"heading": "3. Causal Inference Procedures", "text": ""}, {"heading": "3.1. Main Intuition", "text": "Lemma 1. Consider any absolutely continuous jointdistribution PX,Y on X,Y \u2208 R. For any two functions f, g : R 7\u2192 R we have\nH(X) +H(\u03b7Y,f ) = H(Y ) +H(\u03b7X,g)\n\u2212 {I(\u03b7X,g, Y )\u2212 I(\u03b7Y,f , X)} .\nProof. By the chain rule of differential entropy we have\nH(X,Y ) = H(X) +H(Y |X) = H(X) +H(\u03b7Y,f |X) = H(X) +H(\u03b7Y,f )\u2212 I(\u03b7Y,f , X), similarly\nH(X,Y ) = H(Y ) +H(\u03b7X,g)\u2212 I(\u03b7X,g, Y ).\nEquate the two r.h.s above and rearrange.\nNote that whenever \u03b7Y,f ,\u22a5 X , we have I(\u03b7Y,f , X) = 0. Therefore, by the above lemma, if \u03b7Y,f ,\u22a5 X then CXY , H(X)+H(\u03b7Y,f ) is smaller thanCY X , H(Y )+H(\u03b7X,g). This yields a measure of independence which is relatively cheap to estimate. In particular the test depends only on the marginal distributions of the r.v.\u2019s X,Y and functional residuals, and does not involve estimating joint distributions or conditionals, as is implicit in most independence tests. We analyze a family of procedures based on this idea. This family is given in the next subsection."}, {"heading": "3.2. Meta-Algorithm", "text": "Let {(Xi, Yi)}n1 = {(x1, y1), . . . , (xn, yn)} be a finite sample drawn from PX,Y . Let Hn(X) and Hn(Y ) be respective estimators of H(X) and H(Y ) based on the sample {(Xi, Yi)}n1 .\nWe consider the following family of inference procedures:\nGiven an i.i.d sample {(Xi, Yi)}n1 from PX,Y , let fn be returned by an algorithm which fits Y as fn(X) and gn be returned by an algorithm which fitsX as gn(Y ). LetHn denote an entropy estimator. Given a threshold parameter \u03c4n\nn\u2192\u221e\u2212\u2212\u2212\u2212\u2192 0: Decide X \u2192 Y if Hn(X) +Hn(\u03b7Y,fn) + \u03c4n \u2264 Hn(Y ) +Hn(\u03b7X,gn). Decide Y \u2192 X if Hn(Y ) +Hn(\u03b7X,gn) + \u03c4n \u2264 Hn(X) +Hn(\u03b7Y,fn). Abstain otherwise.\nThe analysis in this paper is carried with respect to the L2,PX and L2,PY functional norms defined as follows. Definition 3. For f : R 7\u2192 R, and a measure \u00b5 on R, the L2,\u00b5 norm is given as \u2016f\u20162,\u00b5 = (\u222b t f(t)2 d\u00b5(t) )1/2 .\nWe assume the internal procedures fn, gn, Hn have the following consistency properties. Assumption 1. The internal procedures are consistent:\n\u2022 Suppose EY 2 < \u221e. Let f(x) , E [Y |x]. Then \u2016fn \u2212 f\u20162,PX P\u2212\u2192 0.\n\u2022 Suppose EX2 < \u221e. Let g(y) , E [X|y]. Then \u2016gn \u2212 g\u20162,PY P\u2212\u2192 0.\n\u2022 Suppose Z has bounded variance, and has continuous density pZ such that \u2203T,C > 0, \u03b1 > 1, \u2200 |t| > T, pZ(t) \u2264 C |t|\u2212\u03b1. Then |Hn(Z)\u2212H(Z)| P\u2212\u2192 0.\nMany common nonparametric regression procedures (e.g. kernel, k-NN, Kernel-SVM, spline regressors) are consistent in the above sense (Gyorfi et al., 2002). Also the consistency of a variety of entropy estimators (e.g. plug-in entropy estimators) is well established (Beirlant et al., 1997)."}, {"heading": "4. Technical overview of results", "text": "We consider the following two versions of the above metaprocedure. The analysis (Section 5) is divided accordingly.\nDefinition 4 (Decoupled-estimation). fn and gn are learned on half of the sample {(Xi, Yi)}n1 , and the Hn (\u03b7Y,fn) andHn (\u03b7X,gn) are learned on the other half of the sample (w.l.o.g. assume n is even). Hn(X) andHn(Y ) could be learned on either half or on the entire sample.\nDefinition 5 (Coupled-estimation). All fn, gn and entropies Hn are learned on the entire sample {(Xi, Yi)}n1 .\nOur most general consistency result (Theorem 1, Section 5.1) concerns decoupled-estimation. By decoupling regression and entropy estimations, we reduce the potential of overfitting, during entropy estimation, the generalization error of regression. This generalization error could be large if the regression algorithms are too rich (e.g. ERM over large functional classes). Our simulations show that, when the regression algorithm is too rich, the variance of the causal inference is large for coupled-estimation but remains low for decoupled-estimation (Fig. 1(a)). By decreasing the richness of the class (simulated by increasing the kernel bandwidth for a kernel regressor) the source of variance shifts to the sample size, and coupled-estimation (which estimates everything on a larger sample) becomes the better procedure and tends to converge faster (Fig. 1(b)).\nFor the consistency result of Theorem 1 we make no assumption on the richness of the regression algorithms, but simply assume that they converge in L2 (Assumption 1). The main technicality is to then show that entropies of residuals are locally continuous relative to the L2 metric in both causal and anticausal directions.\nFor coupled-estimation, the main difficulty is the following. Even though the entropy estimators are consistent for a fixed distribution, the distribution of the residuals change with fn and gn, thus with every random sample (this problem is alleviated by decoupling the estimation). However, if the richness of the regression algorithms is controlled, in other words if the set of potential fn and gn is not too rich, then the entropy estimate for residuals might converge. We show in Theorem 2 (Section 5.2) that if we employ kernel regressors with properly chosen bandwidths, and kernelbased entropy estimators with sufficiently smooth kernels, then the resulting method is consistent for causal inference.\nBoth consistency results of Theorem 1 and Theorem 2 rely on tail assumptions on the additive noise \u03b7 (where X f,\u03b7\u2212\u2212\u2192 Y ). We assume an exponentially decreasing tail for the more difficult case of coupled-estimation, but need only a mild assumption of polynomially decreasing tail in the case of decoupled-estimation. Note that it is common to assume that \u03b7 has Gaussian tail, and our assumptions are milder in that respect.\nInterestingly, our analysis for Theorem 1 suggests that convergence of causal inference is likely faster if the noise \u03b7 has faster decreasing tail (see Lemma 3). This is verified in our simulations where we vary the tail of \u03b7 (Fig. 1(c))."}, {"heading": "5. Analysis", "text": ""}, {"heading": "5.1. Consistency for Decoupled-estimation", "text": "In this section we establish a general consistency result for the meta-procedure above. The main technicality consists of relating differential entropy of residuals to the L2-norms of residuals (i.e. to the error made in function estimation). We henceforth let \u03a3 denote the Lebesgue measure.\nThe analysis in this section uses the following polynomial tail assumption on \u03b7. We note that Assumption 2 satisfies the idenfiability conditions of (Zhang & Hyva\u0308rinen, 2009).\nAssumption 2 (Tail). PX,Y is generated as follows: X\nf,\u03b7\u2212\u2212\u2192 Y for some bounded function f , with bounded derivative on R. PX has bounded support, and both PX and P\u03b7 have densities pX , p\u03b7 with bounded derivatives on R. Furthermore, we assume \u03b7 has bounded variance, and p\u03b7 satisfies, for some T > 0, C > 0, and \u03b1 > 1:\n\u2200 |t| > T, p\u03b7(t) \u2264 C |t|\u2212\u03b1 . (1)\nNote that, since the unknown target functions are assumed bounded, any consistent regressor can be appropriately truncated while maintaining consistency. We therefore have the following technical assumption on the regressors.\nAssumption 3. The regression procedures return bounded functions: limn\u2192\u221emax {\u2016fn(t)\u2016\u221e , \u2016gn(t)\u2016\u221e} <\u221e. Theorem 1 (General consistency for decoupled-estimation). Suppose X f,\u03b7\u2212\u2212\u2192 Y for some f, \u03b7, and PX,Y satisfies the tail Assumption 2. Suppose fn, gn, and Hn are consistent procedures satisfying Assumption 1 and 3. Let the meta-algorithm be decoupled as in Definition 4.\nThen the probability of correctly deciding X \u2192 Y goes to 1 as n\u2192\u221e.\nTo prove the theorem, we have to understand how the estimated entropies converge as a function of the L2 error in regression estimation. We will proceed by bounding the distance between the densities p\u03b7Y,f and p\u03b7Y,f\u2032 of the residuals of functions f and f \u2032 in terms of the L2 distance between f and f \u2032 (Lemma 3); this will then be used to bound the difference in the entropy of such residuals.\nGiven Assumption 2, the following lemma establishes some useful properties of the distribution PX,Y and of the distribution of certain residuals. It is easy to verify that under our assumptions, all distributions under consideration in the lemma are absolutely continuous.\nLemma 2 (Properties of induced densities). Suppose PX,Y satisfies Assumption 2 for some f, \u03b7, and \u03b1 > 1. We then have the following: (i) pX,Y has a bounded gradient on R2, (ii) consider functions f \u2032, g : R 7\u2192 R and suppose sup |f \u2032| and sup |g| are at most T0 for some T0; then there exists T \u2032 > 0 depending on T0, and C \u2032 > 0 such that \u2200 |t| > T \u2032\n{ pX,Y (\u00b7, t), pX,Y (t, \u00b7), p\u03b7Y,f\u2032 (t), p\u03b7X,g (t) } \u2264 C \u2032 |t|\u2212\u03b1 .\nIn particular, the above holds for g(y) , E [X|Y = y].\nThe next lemma relates the density of residuals to the L2 distance between functions. Notice, as discussed in Section 4, that the Lemma suggests that the densities of residuals converge faster the sharper the tails of the noise \u03b7: the larger \u03b1, the sharper the bounds are in terms of the L2 distance between functions.\nLemma 3 (Density of residuals w.r.t. L2 distance). Suppose the joint distribution PX,Y satisfies Assumption 2 for some f, \u03b7 and \u03b1 > 1. Let g(y) , E[X|Y = y]. Consider functions f \u2032, g\u2032 : R 7\u2192 R. There exist a constant C \u2032\u2032 such that for \u2016f \u2212 f \u2032\u20162,PX and (respectively) \u2016g \u2212 g\n\u2032\u20162,PY sufficiently small, we have\nsup t\u2208R \u2223\u2223\u2223p\u03b7Y,f\u2032 (t)\u2212 p\u03b7Y,f (t)\u2223\u2223\u2223 \u2264 C \u2032\u2032 (\u2016f \u2032 \u2212 f\u20162,PX)(\u03b1\u22121)/2\u03b1 , and\nsup t\u2208R\n\u2223\u2223\u2223p\u03b7X,g\u2032 (t)\u2212 p\u03b7X,g (t)\u2223\u2223\u2223 \u2264 C \u2032\u2032 (\u2016g\u2032 \u2212 g\u20162,PY )(\u03b1\u22121)/2\u03b1 .\nProof. We start by bounding the difference between p\u03b7Y,f\u2032 (t) and p\u03b7Y,f\u2032 (t). We note that the same ideas can be used to bound the difference between p\u03b7X,g\u2032 (t) and p\u03b7X,g (t), sinceX and Y are interchangeable in the analysis from this point on. This is because what follows does not depend on how PX,Y is generated, just on the properties of the induced distributions as stated in Lemma 2.\nWe will partition the space R as follows. First, let R> denote the set\n{ x : |f(x)\u2212 f \u2032(x)| > \u221a \u2016f \u2212 f \u2032\u20162,PX } .\nWe define the following interval U \u2282 R: let T \u2032 be defined as in Lemma 2, and \u03c4 > T \u2032; we have U , [\u2212\u03c4, \u03c4 ].\nFor any t \u2208 R we have by writing residual densities in terms of the joint pX,Y (as in the proof of Lemma 2 in\nsupplementary appendix) that \u2223\u2223\u2223p\u03b7Y,f\u2032 (t)\u2212 p\u03b7(t)\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223\u222b R (pX,Y (x, t+ f \u2032(x))\u2212 pX,Y (x, t+ f(x))) dx \u2223\u2223\u2223\u2223 \u2264\n\u2223\u2223\u2223\u2223\u2223 \u222b R\\D (pX,Y (x, t+ f \u2032(x))\u2212 pX,Y (x, t+ f(x))) dx \u2223\u2223\u2223\u2223\u2223 (2)\n+ \u222b U\\R> |pX,Y (x, t+ f \u2032(x))\u2212 pX,Y (x, t+ f(x))| dx\n(3)\n+ \u2223\u2223\u2223\u2223\u222b R> (pX,Y (x, t+ f \u2032(x))\u2212 pX,Y (x, t+ f(x))) dx \u2223\u2223\u2223\u2223 . (4)\nTo bound the first term (2), let yx denote either of t+f \u2032(x) or t+ f(x), we have by Lemma 2 that\u222b \u221e \u03c4 pX,Y (x, yx) dx \u2264 \u222b \u221e \u03c4 C \u2032x\u2212\u03b1 dx \u2264 C \u2032 \u03b1\u2212 1 \u03c4\u2212(\u03b1\u22121),\nso that the first term (2) is at most 2 C \u2032 \u03b1\u22121\u03c4 \u2212(\u03b1\u22121).\nTo bound the second term (3) we recall that pX,Y has a bounded gradient on R2 (Lemma 2). Therefore there exists C0 such that for every x, y, \u2208 R, pX,Y (x, y + ) differs from pX,Y (x, y) by at most C0 \u00b7 | |. It follows that the second term (3) is at most\u222b U\\R> C0 |f \u2032(x)\u2212 f(x)| dx \u2264 2\u03c4 \u00b7 C0 \u221a \u2016f \u2212 f \u2032\u20162,PX .\nThe third term (4) is equal to\n|P (X \u2208 R>, Y = t+ f \u2032(X))\u2212 P (X \u2208 R>, Y = t+ f(X)) | \u2264 PX(R>).\nWe next bound PX (R>) while noting that \u2016f \u2212 f \u2032\u20162,PX could be 0. Let > \u2016f \u2212 f \u2032\u20162,PX . By Markov\u2019s inequality,\nPX { |f(X)\u2212 f \u2032(X)| > \u221a } \u2264 \u2016f \u2212 f \u2032\u20161,PX\u221a\n\u2264 \u2016f \u2212 f \u2032\u20162,PX\u221a .\nThus, consider a sequence of \u2192 \u2016f \u2212 f \u2032\u20162,PX , by Fatou\u2019s lemma we have PX (R>) \u2264 \u221a \u2016f \u2212 f \u2032\u20162,PX .\nCombining the above analysis we have that\u2223\u2223\u2223p\u03b7Y,f\u2032 (t)\u2212 p\u03b7(t)\u2223\u2223\u2223 \u22642 C \u2032\u03b1\u2212 1\u03c4\u2212(\u03b1\u22121) + (1 + 2\u03c4 \u00b7 C0) \u221a \u2016f \u2212 f \u2032\u20162,PX .\nNow, for \u2016f \u2212 f \u2032\u20162,PX sufficiently small, we can pick \u03c4 = O ( \u2016f \u2212 f \u2032\u20162,PX )\u22121/2\u03b1 to get the result.\nAs previously noted we can use the same ideas as above to similarly bound \u2223\u2223\u2223p\u03b7X,g\u2032 (t)\u2212 p\u03b7X,g (t)\u2223\u2223\u2223 for all t \u2208 R. It suffices to interchange X and Y in the above analysis.\nLemma 4. Let p1, p2 be two densities such that there exist T,C > 1 and \u03b1 > 1, for all |t| > T , maxi\u2208[2] pi(t) < C |t|\u2212\u03b1. Suppose supt\u2208R |p1(t)\u2212 p2(t)| < for some < min { 1/T 2, 1/(3e) } satisfying the further condition: \u2200t > 1/ \u221a , t(\u03b1\u22121)/2 > ln t. We then have for sufficiently small\n|H(p1)\u2212H(p2)| \u2264 18 \u221a ln(1/3 ) + 4C\u03b1\n\u03b1\u2212 1 (\u03b1\u22121)/4.\nProof. For simplicity of notation in what follows, let \u03c4 , 1/ \u221a . Let U , [\u2212\u03c4, \u03c4 ] and let U2> , {t \u2208 U , p2(t) > 2 }. Define \u03b3(u) = \u2212u lnu for u > 0, and \u03b3(0) = 0. We will use the fact that for the function \u03b3(\u00b7) is increasing on [0, 1/e]. We have\nH(p1) = \u222b R\\U \u03b3(p1(t)) dt+ \u222b U2> \u03b3(p1(t)) dt\n+ \u222b U\\U2> \u03b3(p1(t)) dt\n\u2264 \u222b R\\U \u03b3(p1(t)) dt+ \u222b U2> p1(t) ln 1 p1(t) dt\n+ \u03a3 (U \\ U2>) \u00b7 \u03b3(3 ), (5)\nsince for t \u2208 U \\ U2> we have\np1(t) \u2264 p2(t) + \u2264 3 \u2264 1/e.\nTo bound the first term of (5), notice that\u222b \u221e \u03c4 \u03b3(p1(t)) dt \u2264 \u222b \u221e \u03c4 \u2212Ct\u2212\u03b1 ln ( Ct\u2212\u03b1 ) dt\n\u2264 \u222b \u221e \u03c4 C\u03b1t\u2212\u03b1 ln t dt\n\u2264 \u222b \u221e \u03c4 C\u03b1t\u2212(\u03b1+1)/2 dt \u2264 2C\u03b1 \u03b1\u2212 1 \u03c4\u2212(\u03b1\u22121)/2,\nhence we have\u222b R\\U \u03b3(p1(t)) dt \u2264 C \u2032\u03c4\u2212\u03b1 \u2032 for C \u2032, \u03b1\u2032 > 0.\nNext we bound the second term of (5) as follows:\u222b U2> p1(t) ln 1 p1(t) dt\n\u2264 \u222b U2> (p2(t) + ) ln 1 p2(t)\u2212 dt\n= \u222b U2> p2(t) ln 1 p2(t)(1\u2212 /p2(t)) dt\n+ \u222b U2> ln 1 p2(t)\u2212 dt\n\u2264 H(p2) + \u222b U2> p2(t) ln 1 1\u2212 /p2(t) dt\n+ \u222b U2> ln 1 dt\n\u2264 H(p2) + \u222b U2> p2(t) ln(1 + 2 /p2(t)) dt + \u03a3 (U2>) \u00b7 \u03b3( ) \u2264 H(p2) + 2\u03a3 (U2>) \u00b7 + \u03a3 (U2>) \u00b7 \u03b3( ).\nCombining all the above, we have\nH(p1) \u2264H(p2) + 3\u03a3 (U) \u00b7 \u03b3(3 ) + C \u2032\u03c4\u2212\u03b1 \u2032\n=H(p2) + 18 \u221a ln(1/3 ) + C \u2032 \u03b1 \u2032/2.\nNotice that p1 and p2 are interchangeable in the above argument. The result therefore follows.\nWe are now ready to prove the main theorem.\nProof of Theorem 1\nLet f(x) , E [Y |x] and g(y) , E [X|y]. By Lemma 1,\nH(X) +H (\u03b7Y,f ) > H(Y ) +H (\u03b7X,g) + 8 , (6)\nfor some > 0.\nThus we detect the right direction X \u2192 Y if all quantities (a) |Hn (\u03b7Y,fn)\u2212H (\u03b7Y,f )|, (b) |Hn (\u03b7X,gn)\u2212H (\u03b7Y,g)|, (c) |Hn(X)\u2212H(X)|, and (d) |Hn(Y )\u2212H(Y )|, are at most .\nBy assumption, (c) and (d) both tend to 0 in probability. The quantities (a) and (b) are handled as follows. We only show the argument for (a), as the argument for (b) is the same. We have:\n|Hn (\u03b7Y,fn)\u2212H (\u03b7Y,f )| \u2264 |Hn (\u03b7Y,fn)\u2212H (\u03b7Y,fn)| + |H (\u03b7Y,fn)\u2212H (\u03b7Y,f )| .\nNow Hn (\u03b7Y,fn) is consistent for fn fixed (it easy to check that P\u03b7Y,fn satisfies the necessary conditions provided fn is bounded) and fn is learned on an independent sample from Hn, we have |Hn (\u03b7Y,fn)\u2212H (\u03b7Y,fn)| P\u2212\u2192 0.\nBy Lemma 3, convergence of fn i.e. \u2016fn \u2212 f\u20162,PX P\u2212\u2192 0 implies supt \u2223\u2223p\u03b7Y,fn (t)\u2212 p\u03b7Y,f (t)\u2223\u2223 P\u2212\u2192 0; this in turn implies by Lemma 4 that |H (\u03b7Y,fn)\u2212H (\u03b7Y,f )| P\u2212\u2192 0.\nThus all quantities (a)-(d) are at most with probability going to 1."}, {"heading": "5.2. Coupled Regression and Residual-entropy Estimation", "text": "Here we consider a coupled version of the meta-algorithm where fn and gn are kernel regressors. This is described in the next subsection."}, {"heading": "5.2.1. KERNEL INSTANTIATION OF THE META-ALGORITHM", "text": "Regression: Although any kernel that is 0 outside a bounded region will work for the regression, we focus here (for simplicity) on the particular case where fn and gn are box-kernel regressors defined as follows (interchange X and Y to obtain gn(y)):\nfn(x) = 1\nnx,h n\u2211 i=1 Yi1{|Xi\u2212x|<h}, (7)\nwhere nx,h = |i : |Xi \u2212 x| < h| , for a bandwidth h.\nEntropy estimation: Given a sequence = { i}ni=1, and a bandwidth \u03c3, define pn, as follows:\npn, (t) = 1\nnh n\u2211 i=1 K ( i \u2212 t \u03c3 ) ,\nwhere \u222b R K(u) du = 1, \u2223\u2223\u2223\u2223 dduK(u) \u2223\u2223\u2223\u2223 <\u221e, and K(u) = 0 for |u| \u2265 1.\nLet Y,i = Yi \u2212 fn(Xi) and X,i = Xi \u2212 gn(Yi). The residual entropy estimators are defined as:\nHn (\u03b7Y,fn) , H (pn, Y ) and Hn (\u03b7X,gn) , H (pn, X ) . (8)"}, {"heading": "5.2.2. CONSISTENCY RESULT FOR COUPLED-ESTIMATION", "text": "We abuse notation and use h and \u03c3 to denote the bandwidth parameters used to estimate either fn andHn (\u03b7Y,fn), or gn and Hn (\u03b7X,gn). We make the distinction clear whenever needed.\nThe consistency result depends on the following quantities bounded in Lemma 5. Definition 6 (Expected average excess risk). Define Rn(fn) , E 1n \u2211n i=1 |fn(Xi)\u2212 f(Xi)| and similarly\nRn(gn) , E 1n \u2211n i=1 |gn(Yi)\u2212 g(Yi)|.\nWe assume in this section that the noise \u03b7 has exponentially decreasing tail:\nDefinition 7. A r.v. Z has exponentially decreasing tail if there exists C,C \u2032 > 0 such that for all t > 0, P (|Z \u2212 EZ| > t) \u2264 Ce\u2212C\u2032t.\nThe following consistency theorem hinges on properly choosing the bandwidths parameters h and \u03c3. Essentially we want to choose h such that regression estimation is consistent, and we want to choose \u03c3 so as not to overfit regression error. If the bandwidth \u03c3 is too small relative to regression error (captured by Rn), then the entropy estimator (for the residual entropy) is only fitting this error. The conditions on \u03c3 in the Theorem are mainly to ensure that \u03c3 is not too small relative to regression error Rn.\nTheorem 2 (Coupled estimation). Suppose X f,\u03b7\u2212\u2212\u2192 Y for some f, \u03b7, and suppose PX,Y satisfies Assumption 2, and \u03b7 has exponentially decreasing tail. Let fn, gn, and Hn be defined as in Section 5.2.1, and let bothHn(X) andHn(Y ) be consistent as in Assumption 1.\nSuppose that : (i) For learning fn and Hn (\u03b7Y,fn), we use h = c1n\n\u2212\u03b1 for some c1 > 0 and 0 < \u03b1 < 1, and \u03c3 = c2n\u2212\u03b2 for some c2 > 0 and 0 < \u03b2 < min {(1\u2212 \u03b1)/4, \u03b1/2}. (ii) For learning gn and Hn (\u03b7X,gn), h satisfies h \u2192 0 and nh \u2192 \u221e, and \u03c3 satisfies \u03c3 \u2192 0, n\u03c3 \u2192 \u221e, and \u03c3 = \u2126(Rn(gn) \u2212\u03b3) for some 0 < \u03b3 < 1/2.\nThen the probability of correctly detecting X \u2192 Y goes to 1 as n\u2192\u221e.\nThe theorem relies on Lemma 5 which bounds the errors Rn for both fn and gn. Suppose X\nf,\u03b7\u2212\u2212\u2192 Y , then if f is smooth or continuously differentiable,Rn(fn)\u2192 0, and in fact we can obtain finite rates of convergence for Rn(fn), thus yielding advice on setting \u03c3. The second part of the Lemma corresponds to this situation.\nHowever, as mentioned earlier in the paper introduction, a smooth f does not ensure that g(y) , g(X|y) is smooth or even continuous, so we do not have rates for Rn(gn). We can nonetheless show that Rn(gn) would generally converge to 0, which is sufficient for there to be proper settings for \u03c3 (i.e. \u03c3 larger than the error, but also tending to 0).\nWe note that the r.v.\u2019s X and Y are interchangeable in this lemma since it does not assume X \u2192 Y . The proof is given in the supplemental appendix.\nLemma 5. Let fn be defined as in (7). Let f(x) , E [Y |x]. Suppose (i) EY 2 < \u221e and that f is bounded; h \u2192 0 and nh\u2192\u221e. Then E 1n \u2211n 1 |fn(Xi)\u2212 f(Xi)| n\u21920\u2212\u2212\u2212\u2192 0.\nSuppose further (ii) that PX has bounded support and that f is continuously differentiable; h = c1n\u2212\u03b1 for some c1 > 0 and 0 < \u03b1 < 1.\nThen we have E 1n \u2211n\n1 |fn(Xi)\u2212 f(Xi)| \u2264 c2n\u2212\u03b2 , for \u03b2 , min {(1\u2212 \u03b1)/2, \u03b1}.\nWe can now prove the theorem of this section.\nProof of Theorem 2\nLet \u0304Y,i , Yi\u2212f(Xi) and \u0304X,i , Xi\u2212g(Yi). Note that, under our conditions on \u03c3 both H (pn,\u0304Y ) and H (pn,\u0304X ) are respectively consistent estimators of H (\u03b7Y,f ) , H(\u03b7) and H (\u03b7X,g) (see e.g. (Beirlant et al., 1997)). For any two densities p, p\u2032 we write |p\u2212 p\u2032| to denote supt |p(t)\u2212 p\u2032(t)|.\nGiven the assumption that K has bounded derivative on R, there exists a constant cK such that\n|pn,\u0304Y \u2212 pn, Y | \u2264 cK \u03c32 \u00b7\n( 1\nn n\u2211 i=1 |fn(Xi)\u2212 f(Xi)| ) which implies\nE |pn,\u0304Y \u2212 pn, Y | 1/2 \u2264\n\u221a cKRn(fn)\n\u03c3 ,\nand also\n|pn,\u0304X \u2212 pn, X | \u2264 cK \u03c32 \u00b7\n( 1\nn n\u2211 i=1 |gn(Yi)\u2212 g(Yi)| ) which also implies\nE |pn,\u0304X \u2212 pn, X | 1/2 \u2264\n\u221a cKRn(gn)\n\u03c3 .\nThus by Lemma 5, we have E |pn,\u0304X \u2212 pn, X | 1/2 \u2192 0, which in turn implies by Markov\u2019s inequality that |pn,\u0304X \u2212 pn, X |\nP\u2212\u2192 0. Now since PX has bounded support, both pn, X and pn,\u0304X have bounded support, and\nhence by Lemma 4 we have \u2223\u2223H (p X,gn )\u2212H (p\u0304X,g)\u2223\u2223 P\u2212\u2192 0. Hence we also have |Hn (\u03b7X,gn)\u2212H(\u03b7X,g)| P\u2212\u2192 0.\nAgain by Lemma 5, we have that, for n sufficiently large, E |pn,\u0304Y \u2212 pn, Y |\n1/2 \u2264 Cn\u2212\u03b2/2 for some C > 0. Therefore by Markov\u2019s inequality, we have P ( |pn,\u0304Y \u2212 pn, Y | \u2265 \u221a Cn\u2212\u03b2/4 ) \u2192 0. Now, under the exponential tail assumption on the noise, all Yi samples are contained in a region of size C \u2032 log n with probability at least 1/n. Thus, since K is supported in [\u22121, 1], both pn,\u0304X and pn, Y are 0 outside a region of size C\n\u2032\u2032 log n. Let T be as in Lemma 4; for all n sufficiently large,\u221a Cn\u2212\u03b2/4 < 1/(C \u2032\u2032 log n)2 = 1/T 2. It follows by\nLemma 4 that \u2223\u2223H (p Y,fn )\u2212H (p\u0304Y,f )\u2223\u2223 P\u2212\u2192 0, and hence that |Hn (\u03b7Y,fn)\u2212H(\u03b7Y,f )| P\u2212\u2192 0.\nThe rest of the proof is similar to that of Theorem 1 by calling on Lemma 1 and using the consistency of Hn(X) and Hn(Y )."}, {"heading": "6. Final Remarks", "text": "We derived the first consistency results for an existing family of procedures for causal inference under the Additive Noise Model. We obtained mild algorithmic requirements, and various distributional tail conditions which guarantee consistency. The present work focuses on the case of two r.v.s X and Y , which captures the inherent difficulties of consistency. We believe however that the insights developed should extend to the case of random vectors under corresponding tail conditions. The details however are left for future work.\nAnother interesting multivariate situation is that of a causal network of r.v.s. as in Peters et al. (2011b) dicussed earlier. Extending our consistency results to this particular multivariate case would primarily consist of extending our distributional tail conditions to the tails of distributions resulting from conditioning on appropriate sets of variables in the network. This is however a non-trivial extension as it involves, e.g. for the convergence of conditional entropies, some additional integration steps that have to be carefully worked out.\nA possible future direction of investigation is to understand under what conditions finite sample rates can be obtained for such procedures. For reasons explained earlier, we do not believe that this is possible without less general distributional assumptions."}, {"heading": "A. Omitted figures from Section 4", "text": "Some addtional experimental results were omitted in the main paper for space, and are given in Figure 2."}, {"heading": "B. Omitted Proofs: Section 5.1", "text": "Proof of Lemma 2. Note that, by assumption, both pX and p\u03b7 are bounded. For any x, y \u2208 R, we have\npX,Y (x, y) = pX(x) \u00b7 pY |x(y) = pX(x) \u00b7 p\u03b7(y \u2212 f(x)). (9)\ntherefore ddxpX,Y (x, y) is given by\nd\ndx pX(x) \u00b7 p\u03b7(y \u2212 f(x))\u2212\nd dx f(x) \u00b7 d dx p\u03b7(y \u2212 f(x)).\nIt is clear that supx,y d dxpX,Y (x, y) < \u221e. Similarly supx,y d dypX,Y (x, y) < \u221e. Also, since p\u03b7 is bounded, we have from (9) that for |t| sufficiently large, pX,Y (t, y) = 0 independent of y. Also, since f and pX are bounded, we have, independent of x, that for |t| sufficiently large, pX,Y (x, t) < C \u2032t\u2212\u03b1 for some C \u2032 > 0.\nNext the density of the residual \u03b7Y,f \u2032 of a function f \u2032 is easily obtained as follows for any t \u2208 R.\np\u03b7Y,f\u2032 (t) = \u222b R pX(x) \u00b7 pY |x(t+ f \u2032(x)) dx\n= \u222b R pX,Y (x, t+ f \u2032(x)) dx (10)\n= \u222b R pX(x) \u00b7 p\u03b7(t+ f \u2032(x)\u2212 f(x)) dx.\nThus if f \u2032 is bounded, there exists T \u2032 > 0 such that for |t| > T \u2032,\np\u03b7Y,f\u2032 (t) \u2264 C \u2032 |t|\u2212\u03b1 \u00b7 \u222b R pX(x) dx = C \u2032 |t|\u2212\u03b1 ,\nwhere \u03b1 is the same as for the bound on p\u03b7 from the assumption.\nWe have similarly for any function g that,\np\u03b7X,g (t) = \u222b pY >0 pY (y) \u00b7 pX|y(t+ g(y)) dy\n= \u222b R pX,Y (t+ g(y), y) dy. (11)\n= \u222b R pX(t+ g(y)) \u00b7 p\u03b7(y \u2212 f(t+ g(y))) dy.\nIf g is bounded, then for t sufficiently large, pX(t+g(y)) = 0 for all y so p\u03b7X,g (t) = 0."}, {"heading": "C. Omitted Proofs: Section 5.2", "text": "We denote the random n-samples as Xn , {Xi}n1 and Y n , {Yi}n1 , throughout this section.\nWe bound Rn(fn) and Rn(gn) in Lemma 5 which makes use of Lemma 6. We note that the r.v.\u2019s X and Y are interchangeable in the two lemmas 5 and 6, since they do not assume X \u2192 Y .\nLemma 6 ((Gyorfi et al., 2002)). For any positive f : R \u2192 R such that E f < \u221e, there exists c0 such that EX,Xn { 1\nnX,h \u2211 Xi:|Xi\u2212X|<h f(Xi) } \u2264 c0E f(X).\nProof of Lemma 5. We simply have to show that E 1n \u2211n 1 |fn(Xi)\u2212 f(Xi)|\n2 n\u2192\u221e\u2212\u2212\u2212\u2212\u2192 0 since by Ho\u0308lder\u2019s inequality and Jensen\u2019s inequalities, for any \u03c6n(\u00b7),\nE 1\nn \u2211 i |\u03c6n(Xi)| \u2264E \u221a 1 n \u2211 i |\u03c6n(Xi)|2\n\u2264 \u221a E 1\nn \u2211 i |\u03c6n(Xi)|2.\nFor assumption (i), pick any > 0. We will show that for n sufficiently large, the above expectation is at most (7 + 3c0) , where c0 is as in Lemma 6. The further claim of assumption (ii) will be obtained along the way.\nFirst condition on Xn, fixing x = Xi for some Xi, and taking expectation with respect to the randomness in Y n , {Yi}n1 . We have by a standard bias-variance decomposition (see e.g. (Gyorfi et al., 2002)) that EY n|Xn |fn(x)\u2212 f(x)| 2\n\u2264 C nx,h + \u2223\u2223\u2223\u2223\u2223\u2223 1nx,h \u2211\n|Xj\u2212x|<h\nf(Xj)\u2212 f(x) \u2223\u2223\u2223\u2223\u2223\u2223 2 = Ax +Bx,\n(12)\nfor some C depending on the variance of Y .\nWe start with a bound on the first term of (12). Pick an interval S such that PX(R \\ S) < .\nConsider an (h/2)-cover Z of S such that for every z \u2208 S, the interval [z \u2212 h/2, z + h/2] is contained in S. We can pick such a Z of size at most 2\u03a3(S)/h. Note that for any x \u2208 [z \u2212 h/2, z + h/2], nx,h \u2265 nz,h/2 ,\n|{Xi : |z \u2212Xi| < h/2}|. We then have\n1\nn n\u2211 i=1 AXi = 1 n n\u2211 i=1 C nXi,h ( 1{Xi\u2208S} + 1{Xi /\u2208S} ) \u2264 1 n n\u2211 j=1 C nXi,h 1{Xi\u2208S} + 1 n n\u2211 i=1 1{Xi /\u2208S}\n\u2264 1 n \u2211 z\u2208Z \u2211 Xi:|z\u2212Xi|\u2264h/2 C nXi,h + 1 n n\u2211 i=1 1{Xi /\u2208S}\n\u2264 1 n \u2211 z\u2208Z C \u00b7 nz,h/2 nz,h/2 + 1 n n\u2211 i=1 1{Xi /\u2208S}.\nTherefore by taking expectation over Xn and letting nh sufficiently large, we have\nE 1\nn n\u2211 i=1 AXi \u2264 2C \u00b7 \u03a3(S) nh + PX(R \\ S) \u2264 2 .\nUnder assumption (ii), pick S larger than the support of PX , we have by the same equation above that for large n\nE 1\nn n\u2211 i=1 AXi \u2264 2C \u00b7 \u03a3(S) nh = 2C \u00b7 \u03a3(S) c1n1\u2212\u03b1 .\nWe now turn to the second term of (12). Under assumption (ii) the function f is Lipschitz continuous and we therefore have for some constant cf that Ax \u2264 cfh2 = cfc1n\u22122\u03b1. Combining with the bound on Ax gives the result for assumption (i).\nFor assumption (i) we proceed as follows. It is well known that bounded uniformly continuous functions are dense in L2,PX for any PX . Therefore let f\u0303 be a bounded uniformly\ncontinuous function such that \u2225\u2225\u2225f\u0303 \u2212 f\u2225\u2225\u2225\n2,PX < \u221a . Since\nh = h(n) \u2192 0, we have sup|x,x\u2032|<h \u2223\u2223\u2223f\u0303(x)\u2212 f\u0303(x\u2032)\u2223\u2223\u22232 < for n sufficiently large. The second term of the r.h.s. of\nthe above equation (12) can then be bounded as follows. If nx,h = 1, then Bx = 0. Otherwise, if nx,h > 1, we have\nBx \u2264 3\nnx,h \u2211 |Xj\u2212x|<h (\u2223\u2223\u2223f(Xj)\u2212 f\u0303(Xj)\u2223\u2223\u22232 + \u2223\u2223\u2223f\u0303(Xj)\u2212 f\u0303(x)\u2223\u2223\u22232 + \u2223\u2223\u2223f\u0303(x)\u2212 f(x)\u2223\u2223\u22232)\n\u22643 + \u2223\u2223\u2223f\u0303(x)\u2212 f(x)\u2223\u2223\u2223\n+ 3\nnx,h \u2211 |Xj\u2212x|<h \u2223\u2223\u2223f(Xj)\u2212 f\u0303(Xj)\u2223\u2223\u22232 \u22643 + ( 3\nnx,h + 1 ) \u2223\u2223\u2223f\u0303(x)\u2212 f(x)\u2223\u2223\u2223 + 3\nnx,h \u2212 1 \u2211\n|Xj\u2212x|<h,Xj 6=x \u2223\u2223\u2223f(Xj)\u2212 f\u0303(Xj)\u2223\u2223\u22232 . Therefore taking expectation over Xn, and applying Lemma 6 to the second term above for x = Xi, we have\nEBXi \u22643 + 2EX \u2223\u2223\u2223f\u0303(X)\u2212 f(X)\u2223\u2223\u22232\n+ 3c0EX \u2223\u2223\u2223f\u0303(X)\u2212 f(X)\u2223\u2223\u22232 = (5 + 3c0) ,\nso that E 1n \u2211 iBXi \u2264 (5 + 3c0) ."}], "references": [{"title": "Nonparametric entropy estimation: An overview", "author": ["Beirlant", "Jan", "Dudewicz", "Edward J", "Gy\u00f6rfi", "L\u00e1szl\u00f3", "Van der Meulen", "Edward C"], "venue": "International Journal of Mathematical and Statistical Sciences,", "citeRegEx": "Beirlant et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Beirlant et al\\.", "year": 1997}, {"title": "Cam: Causal additive models, high-dimensional order search and penalized regression", "author": ["P. Buhlmann", "J. Peters", "J. Ernest"], "venue": null, "citeRegEx": "Buhlmann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Buhlmann et al\\.", "year": 2013}, {"title": "Elements of information theory", "author": ["Cover", "Thomas M", "Thomas", "Joy A", "Kieffer", "John"], "venue": "SIAM Review,", "citeRegEx": "Cover et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cover et al\\.", "year": 1994}, {"title": "A Distribution Free Theory of Nonparametric Regression", "author": ["L. Gyorfi", "M. Kohler", "A. Krzyzak", "H. Walk"], "venue": null, "citeRegEx": "Gyorfi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gyorfi et al\\.", "year": 2002}, {"title": "Nonlinear causal discovery with additive noise models", "author": ["Hoyer", "Patrik O", "Janzing", "Dominik", "JM Mooij", "Peters", "Jonas", "Sch\u00f6lkopf", "Bernhard"], "venue": "Proceedings of Advances in Neural Processing Information Systems,", "citeRegEx": "Hoyer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hoyer et al\\.", "year": 2009}, {"title": "Causality: models, reasoning and inference, volume 29", "author": ["Pearl", "Judea"], "venue": null, "citeRegEx": "Pearl and Judea.,? \\Q2000\\E", "shortCiteRegEx": "Pearl and Judea.", "year": 2000}, {"title": "Causal inference on discrete data using additive noise models", "author": ["Peters", "Jonas", "Janzing", "Dominik", "Scholkopf", "Bernhard"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Peters et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2011}, {"title": "Identifiability of causal graphs using functional models", "author": ["Peters", "Jonas", "Mooij", "Joris", "Janzing", "Dominik", "Sch\u00f6lkopf", "Bernhard"], "venue": "Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Peters et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2011}, {"title": "A linear non-gaussian acyclic model for causal discovery", "author": ["Shimizu", "Shohei", "Hoyer", "Patrik O", "Hyv\u00e4rinen", "Aapo", "Kerminen", "Antti"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Shimizu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shimizu et al\\.", "year": 2006}, {"title": "Causation Prediction & Search 2e, volume 81", "author": ["Spirtes", "Peter", "Glymour", "Clark N", "Scheines", "Richard"], "venue": "MIT press,", "citeRegEx": "Spirtes et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Spirtes et al\\.", "year": 2000}, {"title": "Nonlinear directed acyclic structure learning with weakly additive noise models", "author": ["Tillman", "Robert", "Gretton", "Arthur", "Spirtes", "Peter"], "venue": "Proceedings of Advances in Neural Processing Information Systems,", "citeRegEx": "Tillman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tillman et al\\.", "year": 2009}, {"title": "On the identifiability of the post-nonlinear causal model", "author": ["Zhang", "Kun", "Hyv\u00e4rinen", "Aapo"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Zhang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 9, "context": "Conditional-independence-based methods (Pearl, 2000; Spirtes et al., 2000) estimate a set of directed acyclic graphs, all entailing the same conditional independences, from the data.", "startOffset": 39, "endOffset": 74}, {"referenceID": 8, "context": "Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + \u03b7 is linear, provided the independent noise \u03b7 is not Gaussian.", "startOffset": 14, "endOffset": 36}, {"referenceID": 4, "context": ", 2006; Hoyer et al., 2009; Tillman et al., 2009; Peters et al., 2011a;b) which is given as follows: given two random variables X and Y , X is assumed to cause Y if (i) Y can be obtained as a function of X plus a noise term independent of X , but (ii) X cannot be obtained as a function of Y plus independent noise, then we infer that X causes Y . In this case, where (i) and (ii) hold simultaneously, the CAM is termed identifiable. Initial work on the CAM focused on establishing its theoretical soundness, i.e. understanding the class of distributions PX,Y for which the CAM is identifiable, i.e. for which (i) and (ii) hold simultaneously. Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + \u03b7 is linear, provided the independent noise \u03b7 is not Gaussian. Later, Hoyer et al. (2009), Zhang & Hyv\u00e4rinen (2009) and Peters et al.", "startOffset": 8, "endOffset": 851}, {"referenceID": 4, "context": ", 2006; Hoyer et al., 2009; Tillman et al., 2009; Peters et al., 2011a;b) which is given as follows: given two random variables X and Y , X is assumed to cause Y if (i) Y can be obtained as a function of X plus a noise term independent of X , but (ii) X cannot be obtained as a function of Y plus independent noise, then we infer that X causes Y . In this case, where (i) and (ii) hold simultaneously, the CAM is termed identifiable. Initial work on the CAM focused on establishing its theoretical soundness, i.e. understanding the class of distributions PX,Y for which the CAM is identifiable, i.e. for which (i) and (ii) hold simultaneously. Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + \u03b7 is linear, provided the independent noise \u03b7 is not Gaussian. Later, Hoyer et al. (2009), Zhang & Hyv\u00e4rinen (2009) and Peters et al.", "startOffset": 8, "endOffset": 877}, {"referenceID": 4, "context": ", 2006; Hoyer et al., 2009; Tillman et al., 2009; Peters et al., 2011a;b) which is given as follows: given two random variables X and Y , X is assumed to cause Y if (i) Y can be obtained as a function of X plus a noise term independent of X , but (ii) X cannot be obtained as a function of Y plus independent noise, then we infer that X causes Y . In this case, where (i) and (ii) hold simultaneously, the CAM is termed identifiable. Initial work on the CAM focused on establishing its theoretical soundness, i.e. understanding the class of distributions PX,Y for which the CAM is identifiable, i.e. for which (i) and (ii) hold simultaneously. Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + \u03b7 is linear, provided the independent noise \u03b7 is not Gaussian. Later, Hoyer et al. (2009), Zhang & Hyv\u00e4rinen (2009) and Peters et al. (2011a) showed that the CAM is identifiable more generally even if f is nonlinear, the main technical requirements being that the marginals PX , and P\u03b7 are absolutely continuous on R, with P\u03b7 having support R.", "startOffset": 8, "endOffset": 903}, {"referenceID": 4, "context": ", 2006; Hoyer et al., 2009; Tillman et al., 2009; Peters et al., 2011a;b) which is given as follows: given two random variables X and Y , X is assumed to cause Y if (i) Y can be obtained as a function of X plus a noise term independent of X , but (ii) X cannot be obtained as a function of Y plus independent noise, then we infer that X causes Y . In this case, where (i) and (ii) hold simultaneously, the CAM is termed identifiable. Initial work on the CAM focused on establishing its theoretical soundness, i.e. understanding the class of distributions PX,Y for which the CAM is identifiable, i.e. for which (i) and (ii) hold simultaneously. Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + \u03b7 is linear, provided the independent noise \u03b7 is not Gaussian. Later, Hoyer et al. (2009), Zhang & Hyv\u00e4rinen (2009) and Peters et al. (2011a) showed that the CAM is identifiable more generally even if f is nonlinear, the main technical requirements being that the marginals PX , and P\u03b7 are absolutely continuous on R, with P\u03b7 having support R. Note that Zhang & Hyv\u00e4rinen (2009) also introduces a generalization of the CAM termed post-nonlinear models.", "startOffset": 8, "endOffset": 1140}, {"referenceID": 4, "context": ", 2006; Hoyer et al., 2009; Tillman et al., 2009; Peters et al., 2011a;b) which is given as follows: given two random variables X and Y , X is assumed to cause Y if (i) Y can be obtained as a function of X plus a noise term independent of X , but (ii) X cannot be obtained as a function of Y plus independent noise, then we infer that X causes Y . In this case, where (i) and (ii) hold simultaneously, the CAM is termed identifiable. Initial work on the CAM focused on establishing its theoretical soundness, i.e. understanding the class of distributions PX,Y for which the CAM is identifiable, i.e. for which (i) and (ii) hold simultaneously. Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + \u03b7 is linear, provided the independent noise \u03b7 is not Gaussian. Later, Hoyer et al. (2009), Zhang & Hyv\u00e4rinen (2009) and Peters et al. (2011a) showed that the CAM is identifiable more generally even if f is nonlinear, the main technical requirements being that the marginals PX , and P\u03b7 are absolutely continuous on R, with P\u03b7 having support R. Note that Zhang & Hyv\u00e4rinen (2009) also introduces a generalization of the CAM termed post-nonlinear models. Further work by Peters et al. (2011b) showed how to reduce causal inference for a network of multiple variables under the CAM to the case of two variables X and Y discussed so far, by properly extending the conditions (i) and (ii) to conditional distributions instead of marginals.", "startOffset": 8, "endOffset": 1252}, {"referenceID": 8, "context": "Various works (Shimizu et al., 2006; Hoyer et al., 2009; Peters et al., 2011a) have successfully validated procedures based on the CAM (outlined in Section 1.", "startOffset": 14, "endOffset": 78}, {"referenceID": 4, "context": "Various works (Shimizu et al., 2006; Hoyer et al., 2009; Peters et al., 2011a) have successfully validated procedures based on the CAM (outlined in Section 1.", "startOffset": 14, "endOffset": 78}, {"referenceID": 3, "context": ", 2006; Hoyer et al., 2009; Peters et al., 2011a) have successfully validated procedures based on the CAM (outlined in Section 1.1 below) on a mix of artificial and real-world datasets where the causal structure to be inferred is clear. However, on the theoretical side, it remains unclear whether these procedures can infer causality from samples in general situations where the CAM is identifiable. In the particular case where the functional relation between X and Y is linear, Hyv\u00e4rinen et al. (2008) proposed a successful method shown to be consistent.", "startOffset": 8, "endOffset": 505}, {"referenceID": 1, "context": "In a recent Arxived result appearing after our initial submission, Buhlmann et al. (2013) showed the consistency of a maximum log-likelihood approach to causal inference under the multi-variable network extension of Peters et al.", "startOffset": 67, "endOffset": 90}, {"referenceID": 1, "context": "In a recent Arxived result appearing after our initial submission, Buhlmann et al. (2013) showed the consistency of a maximum log-likelihood approach to causal inference under the multi-variable network extension of Peters et al. (2011b). While consistency has been shown for particular procedures, in this paper we are rather interested in general conditions under which common approaches, with various algorithmic instantiations, are consistent.", "startOffset": 67, "endOffset": 238}, {"referenceID": 3, "context": "(Gyorfi et al., 2002), Theorem 3.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Setup and Notation We letH and I denote respectively differential entropy, and mutual information (Cover et al., 1994).", "startOffset": 98, "endOffset": 118}, {"referenceID": 3, "context": "kernel, k-NN, Kernel-SVM, spline regressors) are consistent in the above sense (Gyorfi et al., 2002).", "startOffset": 79, "endOffset": 100}, {"referenceID": 0, "context": "plug-in entropy estimators) is well established (Beirlant et al., 1997).", "startOffset": 48, "endOffset": 71}, {"referenceID": 0, "context": "For entropy estimation we employ a resubstitution estimate using a kernel density estimator tuned against log-likelihood (Beirlant et al., 1997) and for regression estimator we use kernel regression (KR).", "startOffset": 121, "endOffset": 144}, {"referenceID": 0, "context": "(Beirlant et al., 1997)).", "startOffset": 0, "endOffset": 23}], "year": 2014, "abstractText": "We analyze a family of methods for statistical causal inference from sample under the socalled Additive Noise Model. While most work on the subject has concentrated on establishing the soundness of the Additive Noise Model, the statistical consistency of the resulting inference methods has received little attention. We derive general conditions under which the given family of inference methods consistently infers the causal direction in a nonparametric setting.", "creator": "LaTeX with hyperref package"}}}