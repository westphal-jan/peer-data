{"id": "1401.0514", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jan-2014", "title": "Structured Generative Models of Natural Source Code", "abstract": "We study the problem of building generative models of natural source code (NSC); that is, source code written and understood by humans. Our primary contribution is to describe a family of generative models for NSC that have three key properties: First, they incorporate both sequential and hierarchical structure. Second, we learn a distributed representation of source code elements. Finally, they integrate closely with a compiler, which allows leveraging compiler logic and abstractions when building structure into the model. We also develop an extension that includes more complex structure, refining how the model generates identifier tokens based on what variables are currently in scope. Our models can be learned efficiently, and we show empirically that including appropriate structure greatly improves the models, measured by the probability of generating test programs.", "histories": [["v1", "Thu, 2 Jan 2014 19:35:31 GMT  (415kb,D)", "https://arxiv.org/abs/1401.0514v1", null], ["v2", "Fri, 20 Jun 2014 08:12:20 GMT  (629kb,D)", "http://arxiv.org/abs/1401.0514v2", null]], "reviews": [], "SUBJECTS": "cs.PL cs.LG stat.ML", "authors": ["chris j maddison", "daniel tarlow"], "accepted": true, "id": "1401.0514"}, "pdf": {"name": "1401.0514.pdf", "metadata": {"source": "META", "title": "Structured Generative Models of Natural Source Code", "authors": ["Chris J. Maddison", "Daniel Tarlow"], "emails": ["CMADDIS@CS.TORONTO.EDU", "DTARLOW@MICROSOFT.COM"], "sections": [{"heading": "1. Introduction", "text": "Source code is ubiquitous, and a great deal of human effort goes into developing it. An important goal is to develop tools that make the development of source code easier, faster, and less error-prone, and to develop tools that are able to better understand pre-existing source code. To date this problem has largely been studied outside of machine learning. Many problems in this area do not appear to be well-suited to current machine learning technologies. Yet, source code is some of the most widely available data with many public online repositories. Additionally, massive open online courses (MOOCs) have begun to collect source code homework assignments from tens of thousands of students (Huang et al., 2013). At the same time, the software engineering community has recently observed that it is useful to think of source code as natural\u2014written by humans and meant to be understood by other humans (Hindle et al., 2012). This natural source code (NSC) has a great deal of statistical regularity that is ripe for study in machine learning.\nProceedings of the 31st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nThe combination of these two observations\u2014the availability of data, and the presence of amenable statistical structure\u2014has opened up the possibility that machine learning tools could become useful in various tasks related to source code. At a high level, there are several potential areas of contributions for machine learning. First, code editing tasks could be made easier and faster. Current autocomplete suggestions rely primarily on heuristics developed by an Integrated Development Environment (IDE) designer. With machine learning methods, we might be able to offer much improved completion suggestions by leveraging the massive amount of source code available in public repositories. Indeed, Hindle et al. (2012) have shown that even simple n-gram models are useful for improving code completion tools, and Nguyen et al. (2013) have extended these ideas. Other related applications include finding bugs (Kremenek et al., 2007), mining and suggesting API usage patterns (Bruch et al., 2009; Nguyen et al., 2012; Wang et al., 2013), as a basis for code complexity metrics (Allamanis & Sutton, 2013), and to help with enforcing coding conventions (Allamanis et al., 2014). Second, machine learning might open up whole new applications such as automatic translation between programming languages, automatic code summarization, and learning representations of source code for the purposes of visualization or discriminative learning. Finally, we might hope to leverage the large amounts of existing source code to learn improved priors over programs for use in programming by example (Halbert, 1984; Gulwani, 2011) or other program induction tasks.\nOne approach to developing machine learning tools for NSC is to improve specific one-off tasks related to source code. Indeed, much of the work cited above follows in this direction. An alternative, which we pursue here, is to develop a generative model of source code with the aim that many of the above tasks then become different forms of query on the same learned model (e.g., code completion is conditional sampling; bug fixing is model-based de-\n\u2020Work done primarily while author was an intern at Microsoft Research.\nar X\niv :1\n40 1.\n05 14\nv2 [\ncs .P\nL ]\n2 0\nJu n\n20 14\nnoising; representations may be derived from latent variables (Hinton & Salakhutdinov, 2006) or from Fisher vectors (Jaakkola & Haussler, 1998)). We believe that building a generative model focuses attention on the challenges that source code presents. It forces us to model all aspects of the code, from the high level structure of classes and method declarations, to constraints imposed by the programming language, to the low level details of how variables and methods are named. We believe building good models of NSC to be a worthwhile modelling challenge for machine learning research to embrace.\nIn Section 2 we introduce notation and motivate the requirements of our models\u2014they must capture the sequential and hierarchical nature of NSC, naturalness, and codespecific structural constraints. In Sections 3 and 4 we introduce Log-bilinear Tree-Traversal models (LTTs), which combine natural language processing models of trees with log-bilinear parameterizations, and additionally incorporate compiler-like reasoning. In Section 5 we discuss how efficiently to learn these models, and in Section 7 we show empirically that they far outperform the standard NLP models that have previously been applied to source code. As an introductory result, Fig. 1 shows samples generated by a Probabilistic Context Free Grammar (PCFG)-based model (Fig. 1 (b)) versus samples generated by the full version of our model (Fig. 1 (b)). Although these models apply to any common imperative programming language like C/C++/Java, we focus specifically on C#. This decision is based on (a) the fact that large quantities of data are readily available online, and (b) the recently released Roslyn C# compiler (MSDN, 2011) exposes APIs that allow easy access to a rich set of internal compiler data structures and processing results."}, {"heading": "2. Modelling Source Code", "text": "In this section we discuss the challenges in building a generative model of code. In the process we motivate our\nchoice of representation and model and introduce terminology that will be used throughout.\nHierarchical Representation. The first step in compilation is to lex code into a sequence of tokens, (\u03b1t)Tt=1 = \u03b1 . Tokens are strings such as \u201csum\u201d, \u201c.\u201d, or \u201cint\u201d that serve as the atomic syntactic elements of a programming language. However, representing code as a flat sequence leads to very inefficient descriptions. For example, in a C# for loop, there must be a sequence containing the tokens for, (, an initializer statement, a condition expression, an increment statement, the token ), then a body block. A representation that is fundamentally flat cannot compactly represent this structure, because for loops can be nested. Instead, it is more efficient to use the hierarchical structure that is native to the programming language. Indeed, most source code processing is done on tree structures that encode this structure. These trees are called abstract syntax trees (ASTs) and are constructed either explicitly or implicitly by compilers after lexing valid sequences of code. The leaf nodes of the AST are the tokens produced by the lexer. The internal nodes {ni}Ni=1 are specific to a compiler and correspond to expressions, statements or other high level syntactic elements such as Block or ForStatement. The children tuple Ci of an internal node ni is a tuple of nodes or tokens. An example AST is shown in Fig. 2. Note how the EqualsValueClause node has a subtree corresponding to the code = sum. Because many crucial properties of the source code can be derived from an AST, they are a primary data structure used to reason about source code. For example, the tree structure is enough to determine which variables are in scope at any point in the program. For this reason we choose the AST as the representation for source code and consider generative models that define distributions over ASTs.\nModelling Context Dependence. A PCFG seems like a natural choice for modelling ASTs. PCFGs generate ASTs from the root to the leaves by repeatedly sampling children\nAlgorithm 1 Sampling from LTTs. 1: initialize empty stack S 2: sample (n,h0)\u223c p(n,h0) 3: push n onto S 4: (i, t)\u2190 (1,1) 5: while S is not empty do 6: pop the top node n from S 7: if n is an internal node then 8: ni\u2190 n 9: sample hi \u223c p(hi |hi\u22121) 10: sample Ci \u223c p(Ci |ni,hi) 11: push n for n \u2208 REVERSED(Ci) onto S 12: i\u2190 i+1 13: else 14: \u03b1t \u2190 n 15: t\u2190 t +1 16: end if 17: end while\ntuples given a parent node. The procedure recurses until all leaves are tokens producing nodes ni in a depth-first traversal order and sampling children tuples independently of the rest of the tree. Unfortunately this independence assumption produces a weak model; Fig. 1 (a) shows samples from such a model. While basic constraints like matching of parentheses and braces are satisfied, most important contextual dependencies are lost. For example, identifier names (variable and method names) are drawn independently of each other given the internal nodes of the AST, leading to nonsense code constructions.\nThe first source of context dependence in NSC comes from the naturalness of software. People have many stylistic habits that significantly limit the space of programs that we might expect to see. For example, when writing nested for loops, it is common to name the outer loop variable i and the inner loop variable j. The second source of context dependence comes from additional constraints inherent in the semantics of code. Even if the syntax is context free, the fact that a program conforms to the grammar does not ensure that a program compiles. For example, variables must be declared before they are used.\nOur approach to dealing with dependencies beyond what a PCFG can represent will be to introduce traversal variables {hi}Ni=0 that evolve sequentially as the nodes ni are being generated. Traversal variables modulate the distribution over children tuples by maintaining a representation of context that depends on the state of the AST generated so far."}, {"heading": "3. Log-bilinear Tree-Traversal Models", "text": "LTTs are a family of probabilistic models that generate ASTs in a depth-first order (Algorithm 1). First, the stack is initialized and the root is pushed (lines 1-4). Elements are popped from the stack until it is empty. If an internal node\nni is popped (line 6), then it is expanded into a children tuple and its children are pushed onto the stack (lines 10- 11). If a token \u03b1t is popped, we label it and continue (line 14). This procedure has the effect of generating nodes in a depth-first order. In addition to the tree that is generated in a recursive fashion, traversal variables hi are updated whenever an internal node is popped (line 9). Thus, they traverse the tree, evolving sequentially, with each hi corresponding to some partial tree of the final AST. This sequential view will allow us to exploit context, such as variable scoping, at intermediate stages of the process (see Section 4).\nAlgorithm 1 produces a sequence of internal nodes (ni)Ni=1, traversal variables (hi)Ni=0, and the desired tokens {\u03b1t}Tt=1. It is defined by three distributions: (a) the prior over the root node and traversal variables, p(n,h); (b) the distribution over children tuples conditioned on the parent node and h, denoted p(C |n,h); and (c) the transition distribution for the hs, denoted p(hi |hi\u22121). The joint distribution over the elements produced by Algorithm 1 is\np(n1,h0) N\n\u220f i=1 p(Ci |ni,hi)p(hi |hi\u22121) (1)\nThus, LTTs can be viewed as a Markov model equipped with a stack\u2014a special case of a Probabilistic Pushdown Automata (PPDA) (Abney et al., 1999). Because depthfirst order produces tokens in the same order that they are observed in the code, it is particularly well-suited. We note that other traversal orders produce valid distributions over trees such as right-left or breadth-first. Because we compare to sequential models, we consider only depth-first.\nParameterizations. Most of the uncertainty in generation comes from generating children tuples. In order to avoid an explosion in the number of parameters for the children tuple distribution p(C |n,h), we use a log-bilinear form. For all distributions other than p(C |n,h) we use a simple tabular parameterization.\nThe log-bilinear form consists of a real-valued vector representation of (ni,hi) pairs, Rcon(ni,hi), a real-valued vector representation for the children tuple, Rch(Ci), and a bias term for the children, bch(Ci). These are combined via an inner product, which gives the negative energy of the children tuple\n\u2212E(Ci;ni,hi) = Rch(Ci)T Rcon(ni,hi)+bch(Ci)\nAs is standard, this is then exponentiated and normalized to give the probability of sampling the children: p(Ci |ni,hi)\u221d exp{\u2212E(Ci;ni,hi)} . We take the support over which to normalize this distribution to be the set of children tuples observed as children of nodes of type ni in the training set.\nThe representation functions rely on the notion of an R matrix that can be indexed into with objects to look up D dimensional real-valued vectors. Rx denotes the row of the R\nmatrix corresponding to any variable equal to x. For example, if ni = int and n j = int, then Rni = Rn j . These objects may be tuples and in particular (type, int) 6= int. Similarly, bx looks up a real number. In the simple variant, each unique C sequence receives the representation Rch(Ci) = RCi and bch(Ci) = bCi . The representations for (n,h) pairs are defined as sums of representations of their components. If hi is a collection of variables (hi j representing the jth variable at the ith step) then\nRcon(ni,hi) =W con0 Rni + H\n\u2211 j=1 W conj Rhi j (2)\nThe W cons are matrices (diagonal for computational efficiency) that modulate the contribution of a variable in a position-dependent way. In other variants the children tuple representations will also be defined as sums of their component representations. The log-bilinear parameterization has the desirable property that the number of parameters\ngrows linearly in the dimension of h, so we can afford to have high dimensional traversal variables without worrying about exponentially bad data fragmentation."}, {"heading": "4. Extending LTTs", "text": "The extensions of LTTs in this section allow (a) certain traversal variables to depend arbitrarily on previously generated elements of the AST; (b) annotating nodes with richer types; and (c) letting Rch be compositionally defined, which becomes powerful when combined with deterministic reasoning about variable scoping.\nWe distinguish between deterministic and latent traversal variables. The former can be computed deterministically from the current partial tree (the tree nodes and tokens that have been instantiated at step i) that has been generated while the latter cannot. To refer to a collection of both deterministic and latent traversal variables we continue to use the unqualified \u201ctraversal variables\u201d term.\nDeterministic Traversal Variables. In the basic generative procedure, traversal variables hi satisfy the first-order Markov property, but it is possible to condition on any part of the tree that has already been produced. That is, we can replace p(hi |hi\u22121) by p(hi |h0:i\u22121,n1:i,\u03b11:t) in Eq. 1. Inference becomes complicated unless these variables are deterministic traversal variables (inference is explained in Section 5) and the unique value that has support can be computed efficiently. Examples of these variables include the set of node types that are ancestors of a given node, and the last n tokens or internal nodes that have been generated. Variable scoping, a more elaborate deterministic relationship, is considered later.\nAnnotating Nodes. Other useful features may not be deterministically computable from the current partial tree. Consider knowing that a BinaryExpression will evaluate to an object of type int. This information can be encoded by letting nodes take values in the cross-product space of the node type space and the annotation space. For example, when adding type annotations we might have nodes take value (BinaryExpression, int) where before they were just BinaryExpression. This can be problematic, because the cardinality of the parent node space increases exponentially as we add annotations. Because the annotations are uncertain, this means there are more choices of node values at each step of the generative procedure, and this incurs a cost in log probabilities when evaluating a model. Experimentally we found that simply annotating expression nodes with type information led to worse log probabilities of generating held out data: the cost of generating tokens decreased because the model had access to type information, the increased cost of generating type annotations along with nodetypes outweighed the improvement.\nIdentifier Token Scoping. The source of greatest uncertainty when generating a program are children of IdentifierToken nodes. IdentifierToken nodes are very common and are parents of all tokens (e.g. variable and method names) that are not built-in language keywords (e.g., IntKeyword or EqualsToken) or constants (e.g., StringLiterals). Knowing which variables have previously been declared and are currently in scope is one of the most powerful signals when predicting which IdentifierToken will be used at any point in a program. Other useful cues include how recently the variable was declared and what the type the variable is. In this section we a scope model for LTTs.\nScope can be represented as a set of variable feature vectors corresponding to each to a variable that is in scope.1 Thus, each feature vector contains a string identifier corresponding to the variable along with other features as (key, value) tuples, for example (type,int). A variable is \u201cin scope\u201d if there is a feature vector in the scope set that has a string identifier that is the same as the variable\u2019s identifier.\nWhen sampling an identifier token, there is a two step procedure. First, decide whether this identifier token will be sampled from the current scope. This is accomplished by annotating each IdentifierToken internal node with a binary variable that has the states global or local. If local, proceed to use the local scope model defined next. If global, sample from a global identifier token model that gives support to all identifier tokens. Note, we consider the global option a necessary smoothing device, although ideally we would have a scope model complete enough to have all possible identifier tokens.\nThe scope set can be updated deterministically as we traverse the AST by recognizing patterns that correspond to when variables should be added or removed from the scope. We implemented this logic for three cases: parameters of a method, locally declared variables, and class fields that have been defined prior in the class definition. We do not include class fields defined after the current point in the code, and variables and methods available in included namespaces. This incompleteness necessitates the global option described above, but these three cases are very common and cover many interesting cases.\nGiven the scope set which contains variable feature vectors {v\u03b1} and parent node (IdentifierToken, local), the probability of selecting token child \u03b1 is proportional to p(\u03b1 |ni,hi) \u221d exp{\u2212E(\u03b1;ni,hi)}, where we normalize only over the variables currently in scope. Specifically, we\n1Technically, we view the scope as a deterministic traversal variable, but it does not contribute to Rcon.\nlet Rch(\u03b1) and bch(\u03b1) be defined as follows:\nRch(\u03b1) = V\n\u2211 u=1\nW chu Rv\u03b1u bch(\u03b1) = V\n\u2211 u=1 bv\u03b1u . (3)\nFor example, if a variable in scope has feature vector (numNodes, (type, int) , (recently-declared, 0)), then its corresponding Rch would be a context matrixmodulated sum of representations RnumNodes, R(type,int), and R(recently-declared,0). This representation will then be combined with the context representation as in the basic model. The string identifier feature numNodes is the same object as token nodes of the same string, thus they share their representation."}, {"heading": "5. Inference and Learning in LTTs", "text": "In this section we briefly consider how to compute gradients and probabilities in LTTs.\nOnly Deterministic Traversal Variables. If all traversal variables hi can be computed deterministically from the current partial tree, we use the compiler to compute the full AST corresponding to program \u03b1m. From the AST we compute the only valid setting of the traversal variables. Because both the AST and the traversal variables can be deterministically computed from the token sequence, all variables in the model can be treated as observed. Since LTTs are directed models, this means that the total log probability is a sum of log probabilities at each production, and learning decomposes into independent problems at each production. Thus, we can simply stack all productions into a single training set and follow standard gradient-based procedures for training log-bilinear models. More details will be described in Section 7, but generally we follow the NoiseContrastive Estimation (NCE) technique employed in Mnih & Teh (2012).\nLatent Traversal Variables. In the second case, we allow latent traversal variables that are not deterministically computable from the AST. In this case, the traversal variables couple the learning across different productions from the same tree. For simplicity and to allow efficient exact inference, we restrict these latent traversal variables to just be a single discrete variable at each step (although this restriction could easily be lifted if one was willing to use approximate inference). Because the AST is still a deterministic function of the tokens, computing log probabilities corresponds to running the forward-backward algorithm over the latent states in the depth-first traversal of the AST. We can formulate an EM algorithm adapted to the NCE-based learning of log-bilinear models for learning parameters. The details of this can be found in the Supplementary Material."}, {"heading": "6. Related Work", "text": "The LTTs described here are closely related to several existing models. Firstly, a Hidden Markov Model (HMM) can be recovered by having all children tuples contain a token and a Next node, or just a token (which will terminate the sequence), and having a single discrete latent traversal variable. If the traversal variable has only one state and the children distributions all have finite support, then an LTT becomes equivalent to a Probabilistic Context Free Grammar (PCFG). PCFGs and their variants are components of state-of-the-art parsers of English (McClosky et al., 2006), and many variants have been explored: internal node annotation Charniak (1997) and latent annotations Matsuzaki et al. (2005). Aside from the question of the order of the traversals, the traversal variables make LTTs special cases of Probabilistic Pushdown Automata (PPDA) (for definition and weak equivalence to PCFGs, see Abney et al. (1999)). Log-bilinear parameterizations have been applied widely in language modeling, for n-gram models (Saul & Pereira, 1997; Mnih & Hinton, 2007; Mnih & Teh, 2012) and PCFG models (Charniak, 2000; Klein & Manning, 2002; Titov & Henderson, 2007; Henderson & Titov, 2010). To be clear, our claim is not that general Tree Traversal models or the log-bilinear paremeterizations are novel; however, we believe that the full LTT construction, including the tree traversal structure, log-bilinear parameterization, and incorporation of deterministic logic to be novel and of general interest.\nThe problem of modeling source code is relatively understudied in machine learning. We previously mentioned Hindle et al. (2012) and Allamanis & Sutton (2013), which tackle the same task as us but with simple NLP models. Very recently, Allamanis & Sutton (2014) explores more sophisticated nonparametric Bayesian grammar models of source code for the purpose of learning code idioms. Liang et al. (2010) use a sophisticated non-parametric model to encode the prior that programs should factorize repeated computation, but there is no learning from existing source code, and the prior is only applicable to a functional programming language with quite simple syntax rules. Our approach builds a sophisticated and learned model and supports the full language specification of a widely used imperative programming language."}, {"heading": "7. Experimental Analysis", "text": "In all experiments, we used a dataset that we collected from TopCoder.com. There are 2261 C# programs which make up 140k lines of code and 2.4M parent nodes in the collective abstract syntax trees. These programs are solutions to programming competitions, and there is some overlap in programmers and in problems across the programs. We created training splits based on the user identity, so the set of users in the test set are disjoint from those in the training\nor validation sets (but the training and validation sets share users). The overall split proportions are 20% test, 10% validation, and 70% train. The evaluation measure that we use throughout is the log probability under the model of generating the full program. All logs are base 2. To make this number more easily interpretable, we divide by the number of tokens in each program, and report the average log probability per token.\nExperimental Protocol. All experiments use a validation set to choose hyperparameter values. These include the strength of a smoothing parameter and the epoch at which to stop training (if applicable). We did a coarse grid search in each of these parameters and the numbers we report (for train, validation, and test) are all for the settings that optimized validation performance. For the gradientbased optimization, we used AdaGrad (Duchi et al., 2011) with stochastic minibatches. Unless otherwise specified, the dimension of the latent representation vectors was set to 50. Occasionally the test set will have tokens or children tuples unobserved in the training set. In order to avoid assigning zero probability to the test set, we locally smoothed every children distribution with a default model that gives support to all children tuples. The numbers we report are a lower bound on the log probability of data under for a mixture of our models with this default model. Details of this smoothed model, along with additional experimental details, appear in the Supplementary Materials. There is an additional question of how to represent novel identifiers in the scope model. We set the representations of all features in the variable feature vectors that were unobserved in the training set to the all zeros vector.\nBaselines and Basic Log-bilinear Models. The natural choices for baseline models are n-gram models and PCFGlike models. In the n-gram models we use additive smoothing, with the strength of the smoothing hyperparameter chosen to optimize validation set performance. Similarly, there is a smoothing parameter in the PCFG-like models that is chosen to optimize validation set performance. We explored the effect of the log-bilinear parameterization in two ways. First, we trained a PCFG model that was identical to the first PCFG model but with the parameterization defined using the standard log-bilinear parameterization. This is the most basic LTT model, with no traversal variables (LTT- /0). The result was nearly identical to the standard PCFG. Next, we trained a 10-gram model with a standard log-bilinear parameterization, which is equivalent to the models discussed in (Mnih & Teh, 2012). This approach dominates the basic n-gram models, allowing longer contexts and generalizing better. Results appear in Fig. 4.\nDeterministic Traversal Variables. Next, we augmented LTT-/0 model with deterministic traversal variables that in-\nclude hierarchical and sequential information. The hierarchical information is the depth of a node, the kind of a node\u2019s parent, and a sequence of 10 ancestor history variables, which store for the last 10 ancestors, the kind of the node and the index of the child that would need to be recursed upon to reach the current point in the tree. The sequential information is the last 10 tokens that were generated.\nIn Fig. 5 we report results for three variants: hierarchy only (LTT-Hi), sequence only (LTT-Seq), and both (LTTHiSeq). The hierarchy features alone perform better than the sequence features alone, but that their contributions are independent enough that the combination of the two provides a substantial gain over either of the individuals.\nLatent Traversal Variables. Next, we considered latent traversal variable LTT models trained with EM learning. In all cases, we used 32 discrete latent states. Here, results were more mixed. While the latent-augmented LTT (LTTlatent) outperforms the LTT- /0 model, the gains are smaller than achieved by adding the deterministic features. As a baseline, we also trained a log-bilinear-parameterized standard HMM, and found its performance to be far worse than other models. We also tried a variant where we added latent traversal variables to the LTT-HiSeq model from the previous section, but the training was too slow to be practical due to the cost of computing normalizing constants in the E step. See Fig. 6.\nScope Model. The final set of models that we trained incorporate the scope model from Section 4 (LTT-HiSeqScope). The features of variables that we use are the identifier string, the type, where the variable appears in a list sorted by when the variable was declared (also known as a de Bruijn index), and where the variable appears in a list sorted by when the variable was last assigned a value.\nHere, the additional structure provides a large additional improvement over the previous best model (LTT-HiSeq). See Fig. 7.\nAnalysis. To understand better where the improvements in the different models come from, and to understand where there is still room left for improvement in the models, we break down the log probabilities from the previous experiments based on the value of the parent node. The results appear in Fig. 8. In the first column is the total log probability number reported previously. In the next columns, the contribution is split into the cost incurred by generating tokens and trees respectively. We see, for example, that the full scope model pays a slightly higher cost to generate the tree structure than the Hi&Seq model, which is due to it having to properly choose whether IdentifierTokens are drawn from local or global scopes, but that it makes up for this by paying a much smaller cost when it comes to generating the actual tokens.\nIn the Supplementary Materials, we go further into the breakdowns for the best performing model, reporting the percentage of total cost that comes from the top parent kinds. IdentifierTokens from the global scope are the largest cost (30.1%), with IdentifierTokens covered by our local scope model (10.9%) and Blocks (10.6%) next. This suggests that there would be value in extending our scope model to include more IdentifierTokens and an improved model of Block sequences.\nSamples. Finally, we qualitatively evaluate the different methods by drawing samples from the models. Samples of for loops appear in Fig. 1. To generate these samples, we ask (b) the PCFG and (c) the LTT-HiSeq-Scope model to generate a ForStatement. For (a) the LBL n-gram model, we simply insert a for token as the most recent token. We also initialize the traversal variables to reasonable values:\ne.g., for the LTT-HiSeq-Scope model, we initialize the local scope to include string[] words. We also provide samples of full source code files (CompilationUnit) from the LTT-HiSeq-Scope model in the Supplementary Material, and additional for loops. Notice the structure that the model is able to capture, particularly related to high level organization, and variable use and re-use. It also learns quite subtle things, like int variables often appear inside square brackets."}, {"heading": "8. Discussion", "text": "Natural source code is a highly structured source of data that has been largely unexplored by the machine learning community. We have built probabilistic models that capture some of the structure that appears in NSC. A key to our approach is to leverage the great deal of work that has gone into building compilers. The result is models that not only yield large improvements in quantitative measures over baselines, but also qualitatively produce far more realistic samples.\nThere are many remaining modeling challenges. One question is how to encode the notion that the point of source code is to do something. Relatedly, how do we represent and discover high level structure related to trying to accomplish such tasks? There are also a number of specific sub-problems that are ripe for further study. Our model of Block statements is naive, and we see that it is a significant contributor to log probabilities. It would be interesting to apply more sophisticated sequence models to children tuples of Blocks. Also, applying the compositional representation used in our scope model to other children tuples would interesting. Similarly, it would be interesting to extend our scope model to handle method calls. Another high level piece of structure that we only briefly experimented with is type information. We believe there to be great potential in properly handling typing rules, but we found that the simple approach of annotating nodes to actually hurt our models.\nMore generally, this work\u2019s focus was on generative modeling. An observation that has become popular in machine learning lately is that learning good generative models can be valuable when the goal is to extract features from the data. It would be interesting to explore how this might be\napplied in the case of NSC.\nIn sum, we argue that probabilistic modeling of source code provides a rich source of problems with the potential to drive forward new machine learning research, and we hope that this work helps illustrate how that research might proceed forward."}, {"heading": "Acknowledgments", "text": "We are grateful to John Winn, Andy Gordon, Tom Minka, and Thore Graepel for helpful discussions and suggestions. We thank Miltos Allamanis and Charles Sutton for pointers to related work."}, {"heading": "EM Learning for Latent Traversal Variable LTTs", "text": "Here we describe EM learning of LTTs with latent traversal variables. Consider probability of \u03b1 with deterministic traversal variables hdi and latent traversal variables h l i (where hi represents the union of {hli} and h d i ):\n\u2211 h0:N\np(n1,h0) N\n\u220f i=1 p(Ci |ni,hi)p(hli |hli\u22121)\n\u00d7p(hdi |h0:i\u22121,n1:i, ,\u03b11:t) (4)\nFirstly, the p(hdi | \u00b7) terms drop off because as above we can use the compiler to compute the AST from \u03b1 then use the AST to deterministically fill in the only legal values for the hdi variables, which makes these terms always equal to 1. It then becomes clear that the sum can be computed using the forward-backward algorithm. For learning, we follow the standard EM formulation and lower bound the data log probability with a free energy of the following form (which for brevity drops the prior and entropy terms):\nN\n\u2211 i=2 \u2211 hli ,h l i\u22121 Qi,i\u22121(hli ,h l i\u22121) logP(h l i |hli\u22121)\n+ N\n\u2211 i=1 \u2211 hli Qi(hli) logp(Ci |ni,hi) (5)\nIn the E step, the Q\u2019s are updated optimally given the current parameters using the forward backward algorithm. In the M step, given Q\u2019s, the learning decomposes across productions. We represent the transition probabilities using a simple tabular representation and use stochastic gradient updates. For the emission terms, it is again straightforward to use standard log-bilinear model training. The only difference from the previous case is that there are now K training examples for each i, one for each possible value of hli , which are weighted by their corresponding Qi(h l i). A simple way of handling this so that log-bilinear training methods can be used unmodified is to sample hli values from the corresponding Qi(\u00b7) distribution, then to add unweighted examples to the training set with hli values being given their sampled value. This can then be seen as a stochastic incremental M step.\nMore Experimental Protocol Details For all hyperparameters that were not validated over (such as minibatch size, scale of the random initializations, and learning rate), we chose a subsample of the training set and manually chose a setting that did best at optimizing the training log probabilities. For EM learning, we divided the data into databatches, which contained 10 full programs, ran forward-backward on the databatch, then created a set of minibatches on which to do an incremental M step using AdaGrad. All parameters were then held fixed throughout the experiments, with the exception that we re-optimized the parameters for the learning that required EM, and we scaled the learning rate when the latent dimension changed. Our code used properly vectorized Python for the gradient updates and a C++ implementation of the forward-backward algorithm but was otherwise not particularly optimized. Run times (on a single core) ranged from a few hours to a couple days.\nSmoothed Model In order to avoid assigning zero probability to the test set, we assumed knowledge of the set of all possible tokens, as well as all possible internal node types \u2013 information available in the Roslyn API. Nonetheless, because we specify distributions over tuples of children there are tuples in the test set with no support. Therefore we smooth every p(Ci |hi,ni) by mixing it with a default distribution pde f (Ci |hi,ni) over children that gives broad support.\np\u03c0 (Ci |hi,ni) = \u03c0 p(Ci |hi,ni)+(1\u2212\u03c0)pde f (Ci |hi,ni) (6)\nFor distributions whose children are all 1-tuples of tokens, the default model is an additively smoothed model of the empirical distribution of tokens in the train set. For other distributions we model the number of children in the tuple as a Poisson distribution, then model the identity of the children independently (smoothed additively).\nThis smoothing introduces trees other than the Roslyn AST with positive support. This opens up the possibility that there are multiple trees consistent with a given token sequence and we can no longer compute logp(\u03b1) in the manner discussed in Section 5. Still we report the log-probability of the AST, which is now a lower bound on logp(\u03b1).\nfor ( int i = 0 ; i < words . Length ; ++ i ) i = i . Replace ( \"X\" , i ) ;\nfor ( int j = 0 ; j < words . X ; j ++ ) {\nif ( j [ j ] == - 1 ) continue ;\nif ( words [ j ] != words [ j ] ) j += thisMincost ( 1 ) ;\nelse {\nj = ( j + 1 ) % 2 ;\nwords [ j + 1 ] += words [ 0 ] ;\n}\n}\nfor ( int j = words ; j < words . Pair ; ++ j )\nfor ( int i = 0 ; i < words . Length ; ++ i ) {\nisUpper ( i , i ) ;\n}\nfor ( int i = 0 ; i < words . Length ; ++ i ) {\nwords [ i , i ] = words . Replace ( \"*\" , i * 3 ) ;\n}\nfor ( int j = 360 ; j < j ; ) {\nif ( ! words . ContainsKey ( j ) ) {\nif ( words . at + \" \" + j == j ) return ume ( j , j ) ;\n} else {\nj = 100 ;\n}\n}\nfor ( int c = 0 ; c < c ; ++ c )\nfor ( int i = 0 ; i < c ; i ++ ) {\nif ( ! words [ i ] ) i = i ;\n}\nfor ( int i = 0 ; i < words . Length ; i ++ ) {\ni . Parse ( i ) ;\n}\nfor ( int i = words ; i < 360 ; ++ i ) {\nusing System ;\nusing System . Collections . Text ;\nusing System . Text . Text ;\nusing System . Text . Specialized ;\nusing kp . Specialized ;\nusing System . Specialized . Specialized ;\npublic class MaxFlow\nusing System ;\nusing System . sampling . Specialized ;\npublic class AlternatingLane\nusing System ;\nusing System . Collections . Collections ;\nusing System . Collections . Text ;\nusing System . Text . Text ;\npublic class TheBlackJackDivTwo"}], "references": [{"title": "Relating probabilistic grammars and automata", "author": ["Abney", "Steven", "McAllester", "David", "Pereira", "Fernando"], "venue": "In ACL,", "citeRegEx": "Abney et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Abney et al\\.", "year": 1999}, {"title": "Mining source code repositories at massive scale using language modeling", "author": ["Allamanis", "Miltiadis", "Sutton", "Charles"], "venue": "In MSR,", "citeRegEx": "Allamanis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Allamanis et al\\.", "year": 2013}, {"title": "Learning natural coding conventions", "author": ["Allamanis", "Miltiadis", "Barr", "Earl T", "Sutton", "Charles"], "venue": "arXiv preprint arXiv:1402.4182,", "citeRegEx": "Allamanis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Allamanis et al\\.", "year": 2014}, {"title": "Learning from examples to improve code completion systems", "author": ["Bruch", "Marcel", "Monperrus", "Martin", "Mezini", "Mira"], "venue": "In ESEC/FSE,", "citeRegEx": "Bruch et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bruch et al\\.", "year": 2009}, {"title": "Statistical parsing with a context-free grammar and word statistics", "author": ["Charniak", "Eugene"], "venue": "AAAI/IAAI, 2005:598\u2013603,", "citeRegEx": "Charniak and Eugene.,? \\Q1997\\E", "shortCiteRegEx": "Charniak and Eugene.", "year": 1997}, {"title": "A maximum-entropy-inspired parser", "author": ["Charniak", "Eugene"], "venue": "In ACL, pp", "citeRegEx": "Charniak and Eugene.,? \\Q2000\\E", "shortCiteRegEx": "Charniak and Eugene.", "year": 2000}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Automating string processing in spreadsheets using input-output examples", "author": ["Gulwani", "Sumit"], "venue": "In ACM SIGPLAN Notices,", "citeRegEx": "Gulwani and Sumit.,? \\Q2011\\E", "shortCiteRegEx": "Gulwani and Sumit.", "year": 2011}, {"title": "Programming by example", "author": ["Halbert", "Daniel Conrad"], "venue": "PhD thesis, University of California, Berkeley,", "citeRegEx": "Halbert and Conrad.,? \\Q1984\\E", "shortCiteRegEx": "Halbert and Conrad.", "year": 1984}, {"title": "Incremental sigmoid belief networks for grammar learning", "author": ["Henderson", "James", "Titov", "Ivan"], "venue": "JMLR, 11:3541\u20133570,", "citeRegEx": "Henderson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2010}, {"title": "On the naturalness of software", "author": ["Hindle", "Abram", "Barr", "Earl T", "Su", "Zhendong", "Gabel", "Mark", "Devanbu", "Premkumar"], "venue": "In ICSE,", "citeRegEx": "Hindle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hindle et al\\.", "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": "Science,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Syntactic and functional variability of a million code submissions in a machine learning MOOC", "author": ["Huang", "Jonathan", "Piech", "Chris", "Nguyen", "Andy", "Guibas", "Leonidas"], "venue": "In AIED,", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Exploiting generative models in discriminative classifiers", "author": ["Jaakkola", "Tommi", "Haussler", "David"], "venue": null, "citeRegEx": "Jaakkola et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1998}, {"title": "Fast exact inference with a factored model for natural language parsing", "author": ["Klein", "Dan", "Manning", "Christopher D"], "venue": "In NIPS, pp", "citeRegEx": "Klein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2002}, {"title": "A factor graph model for software bug finding", "author": ["Kremenek", "Ted", "Ng", "Andrew Y", "Engler", "Dawson R"], "venue": "In IJCAI,", "citeRegEx": "Kremenek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kremenek et al\\.", "year": 2007}, {"title": "Learning programs: A hierarchical Bayesian approach", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "In ICML, pp", "citeRegEx": "Liang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2010}, {"title": "Probabilistic cfg with latent annotations", "author": ["Matsuzaki", "Takuya", "Miyao", "Yusuke", "Tsujii", "Jun\u2019ichi"], "venue": "In ACL,", "citeRegEx": "Matsuzaki et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Matsuzaki et al\\.", "year": 2005}, {"title": "Effective self-training for parsing", "author": ["McClosky", "David", "Charniak", "Eugene", "Johnson", "Mark"], "venue": "In ACL, pp. 152\u2013159", "citeRegEx": "McClosky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Three new graphical models for statistical language modelling", "author": ["Mnih", "Andriy", "Hinton", "Geoffrey"], "venue": "In ICML, pp", "citeRegEx": "Mnih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2007}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Mnih", "Andriy", "Teh", "Yee Whye"], "venue": "In ICML,", "citeRegEx": "Mnih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2012}, {"title": "Graph-based pattern-oriented, contextsensitive source code completion", "author": ["Nguyen", "Anh Tuan", "Tung Thanh", "Hoan Anh", "Tamrawi", "Ahmed", "Hung Viet", "Al-Kofahi", "Jafar", "Tien N"], "venue": "In ICSE,", "citeRegEx": "Nguyen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2012}, {"title": "A statistical semantic language model for source code", "author": ["Nguyen", "Tung Thanh", "Anh Tuan", "Hoan Anh", "Tien N"], "venue": "In ESEC/FSE,", "citeRegEx": "Nguyen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2013}, {"title": "Aggregate and mixedorder Markov models for statistical language processing", "author": ["Saul", "Lawrence", "Pereira", "Fernando"], "venue": "In EMNLP,", "citeRegEx": "Saul et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Saul et al\\.", "year": 1997}, {"title": "Incremental Bayesian networks for structure prediction", "author": ["Titov", "Ivan", "Henderson", "James"], "venue": "In ICML,", "citeRegEx": "Titov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2007}, {"title": "Mining succinct and high-coverage api usage patterns from source code", "author": ["Wang", "Jue", "Dang", "Yingnong", "Zhang", "Hongyu", "Chen", "Kai", "Xie", "Tao", "Dongmei"], "venue": "In MSR,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 12, "context": "Additionally, massive open online courses (MOOCs) have begun to collect source code homework assignments from tens of thousands of students (Huang et al., 2013).", "startOffset": 140, "endOffset": 160}, {"referenceID": 10, "context": "At the same time, the software engineering community has recently observed that it is useful to think of source code as natural\u2014written by humans and meant to be understood by other humans (Hindle et al., 2012).", "startOffset": 189, "endOffset": 210}, {"referenceID": 15, "context": "Other related applications include finding bugs (Kremenek et al., 2007), mining and suggesting API usage patterns (Bruch et al.", "startOffset": 48, "endOffset": 71}, {"referenceID": 3, "context": ", 2007), mining and suggesting API usage patterns (Bruch et al., 2009; Nguyen et al., 2012; Wang et al., 2013), as a basis for code complexity metrics (Allamanis & Sutton, 2013), and to help with enforcing coding conventions (Allamanis et al.", "startOffset": 50, "endOffset": 110}, {"referenceID": 21, "context": ", 2007), mining and suggesting API usage patterns (Bruch et al., 2009; Nguyen et al., 2012; Wang et al., 2013), as a basis for code complexity metrics (Allamanis & Sutton, 2013), and to help with enforcing coding conventions (Allamanis et al.", "startOffset": 50, "endOffset": 110}, {"referenceID": 25, "context": ", 2007), mining and suggesting API usage patterns (Bruch et al., 2009; Nguyen et al., 2012; Wang et al., 2013), as a basis for code complexity metrics (Allamanis & Sutton, 2013), and to help with enforcing coding conventions (Allamanis et al.", "startOffset": 50, "endOffset": 110}, {"referenceID": 2, "context": ", 2013), as a basis for code complexity metrics (Allamanis & Sutton, 2013), and to help with enforcing coding conventions (Allamanis et al., 2014).", "startOffset": 122, "endOffset": 146}, {"referenceID": 7, "context": "Indeed, Hindle et al. (2012) have shown that even simple n-gram models are useful for improving code completion tools, and Nguyen et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 7, "context": "Indeed, Hindle et al. (2012) have shown that even simple n-gram models are useful for improving code completion tools, and Nguyen et al. (2013) have extended these ideas.", "startOffset": 8, "endOffset": 144}, {"referenceID": 0, "context": "Thus, LTTs can be viewed as a Markov model equipped with a stack\u2014a special case of a Probabilistic Pushdown Automata (PPDA) (Abney et al., 1999).", "startOffset": 124, "endOffset": 144}, {"referenceID": 18, "context": "PCFGs and their variants are components of state-of-the-art parsers of English (McClosky et al., 2006), and many variants have been explored: internal node annotation Charniak (1997) and latent annotations Matsuzaki et al.", "startOffset": 79, "endOffset": 102}, {"referenceID": 16, "context": "PCFGs and their variants are components of state-of-the-art parsers of English (McClosky et al., 2006), and many variants have been explored: internal node annotation Charniak (1997) and latent annotations Matsuzaki et al.", "startOffset": 80, "endOffset": 183}, {"referenceID": 16, "context": ", 2006), and many variants have been explored: internal node annotation Charniak (1997) and latent annotations Matsuzaki et al. (2005). Aside from the question of the order of the traversals, the traversal variables make LTTs special cases of Probabilistic Pushdown Automata (PPDA) (for definition and weak equivalence to PCFGs, see Abney et al.", "startOffset": 111, "endOffset": 135}, {"referenceID": 0, "context": "Aside from the question of the order of the traversals, the traversal variables make LTTs special cases of Probabilistic Pushdown Automata (PPDA) (for definition and weak equivalence to PCFGs, see Abney et al. (1999)).", "startOffset": 197, "endOffset": 217}, {"referenceID": 10, "context": "We previously mentioned Hindle et al. (2012) and Allamanis & Sutton (2013), which tackle the same task as us but with simple NLP models.", "startOffset": 24, "endOffset": 45}, {"referenceID": 10, "context": "We previously mentioned Hindle et al. (2012) and Allamanis & Sutton (2013), which tackle the same task as us but with simple NLP models.", "startOffset": 24, "endOffset": 75}, {"referenceID": 10, "context": "We previously mentioned Hindle et al. (2012) and Allamanis & Sutton (2013), which tackle the same task as us but with simple NLP models. Very recently, Allamanis & Sutton (2014) explores more sophisticated nonparametric Bayesian grammar models of source code for the purpose of learning code idioms.", "startOffset": 24, "endOffset": 178}, {"referenceID": 10, "context": "We previously mentioned Hindle et al. (2012) and Allamanis & Sutton (2013), which tackle the same task as us but with simple NLP models. Very recently, Allamanis & Sutton (2014) explores more sophisticated nonparametric Bayesian grammar models of source code for the purpose of learning code idioms. Liang et al. (2010) use a sophisticated non-parametric model to encode the prior that programs should factorize repeated computation, but there is no learning from existing source code, and the prior is only applicable to a functional programming language with quite simple syntax rules.", "startOffset": 24, "endOffset": 320}, {"referenceID": 6, "context": "For the gradientbased optimization, we used AdaGrad (Duchi et al., 2011) with stochastic minibatches.", "startOffset": 52, "endOffset": 72}], "year": 2014, "abstractText": "We study the problem of building generative models of natural source code (NSC); that is, source code written by humans and meant to be understood by humans. Our primary contribution is to describe new generative models that are tailored to NSC. The models are based on probabilistic context free grammars (PCFGs) and neuro-probabilistic language models (Mnih & Teh, 2012), which are extended to incorporate additional source code-specific structure. These models can be efficiently trained on a corpus of source code and outperform a variety of less structured baselines in terms of predictive log likelihoods on held-out data.", "creator": "LaTeX with hyperref package"}}}