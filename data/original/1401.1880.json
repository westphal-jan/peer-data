{"id": "1401.1880", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jan-2014", "title": "DJ-MC: A Reinforcement-Learning Agent for Music Playlist Recommendation", "abstract": "In recent years, there has been growing focus on the study of automated recommender systems. Music recommendation systems serve as a prominent domain for such works, both from an academic and a commercial perspective. A relatively well known fact in music cognition is that music is experienced in temporal context and in sequence. In this work we present DJ-MC, a reinforcement-learning based framework for music recommendation that does not recommend songs individually but rather song sequences, or playlists, based on a learned model of preferences for both individual songs and song transitions. To reduce exploration time, we initialize a model based on user feedback. This model is subsequently updated by reinforcement. We show our algorithm outperforms a more naive approach, using both real song data and real playlist data to validate our approach.", "histories": [["v1", "Thu, 9 Jan 2014 01:50:09 GMT  (862kb,D)", "http://arxiv.org/abs/1401.1880v1", null], ["v2", "Wed, 25 Mar 2015 18:40:46 GMT  (831kb,D)", "http://arxiv.org/abs/1401.1880v2", "-Updated to the most recent and completed version (to be presented at AAMAS 2015) -Updated author list. in Autonomous Agents and Multiagent Systems (AAMAS) 2015, Istanbul, Turkey, May 2015"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["elad liebman", "maytal saar-tsechansky", "peter stone"], "accepted": false, "id": "1401.1880"}, "pdf": {"name": "1401.1880.pdf", "metadata": {"source": "CRF", "title": "DJ-MC: A Reinforcement-Learning Agent for Music Playlist Recommendation", "authors": ["Elad Liebman", "Peter Stone"], "emails": ["eladlieb@cs.utexas.edu", "pstone@cs.utexas.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "In recent years, recommender systems have risen in prominence as a field of research [2]. Music has been one of the main domains in which recommender systems have been developed and tested, both academically [2, 4, 15] and commercially [4]. Among known commercial applications one may consider Pandora (and the Music Genome Project), Jango, Last.fm and many others. Some of the details of how such recommendation systems predict individual song preferences are known (machine learning and collaborative filtering are often used). However, music listeners usually listen to music over an extended session that persists over a sequence of a songs, rather than a single song in isolation. More importantly, it is a relatively well known fact in music cognition that music is experienced in temporal context and in sequence [11,18]. In other words, the enjoyment obtained from listening to a song can be heightened or decreased by its relative position in a sequence. This fact is also well known among DJs, who perceive playlist-building as a whole [9]. Indeed, some work in automated playlist building has attempted to attack this problem from the DJs perspective [9, 10, 16]. Though motivated by similar goals, there seems to be little to no actual linking between learning user preferences and holistic playlist generation. In this paper, we try to combine the two perspectives by applying a\nAppears in: Proceedings of the 13th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2014), Lomuscio, Scerri, Bazzan, Huhns (eds.), May, 5\u20139, 2014, Paris, France. Copyright c\u00a9 2014, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.\nreinforcement learning approach which considers not only individual song rewards but the pleasure generated from song transitions, and the impact of playlist history. To this end, the system plans ahead to maximize expected reward adaptively for a specific listener. Finally, we present DJ-MC, a full reinforcement learning adaptive DJ agent, that learns an estimate of the specific preferences of a given listener and employs Monte-Carlo planning to exploit the learned model, while adjusting it based on provided feedback."}, {"heading": "2. RELATED WORK", "text": "Though not much work has attempted to model playlists directly, there has been substantial research on modeling similarity between artists and between songs. Platt et al. [17] use semantic tags to learn a Gaussian process kernel function between pairs of songs. Weston et al. [22] learn an embedding in a shared space of social tags, acoustic features and artist entities by optimizing an evaluation metric for various music retrieval tasks. Aizenberg et al. [3] put forth a probabilistic approach. In their paper they model radio stations as probability distributions of items to be played, embedded in an inner-product space, using real playlist histories for training. These works have close ties with the larger body of research dealing with sequence modeling in natural language processing [12].\nHowever, most modern Internet radio services provide the user with a stream of songs rather than single, isolated recommendations. In most of these services, the user first seeds the stream of music with a list of preferred artists, genres etc. Based on this seed, a backend service then produces a sequence of songs that can be considered a playlist. How exactly this song sequence is produced is not publicly known for most commercial services. More importantly, while some details on how specific songs are selected (e.g. via collaborative filtering [19]) are known, the specifics of how a certain order is determined are not revealed, nor is it known which criteria are used.\nIn the academic literature, several recent papers have tried to tackle the issue of playlist prediction. Maillet et al. [13] approach the playlist prediction problem from a supervised binary classification perspective. Positive examples are pairs of songs that appeared in sequence in one of the training playlists, and negative examples are randomly chosen pairs which never appeared in sequence in historical data. Mcfee and Lanckriet [14] consider playlists as a natural language induced over songs, training a bigram model for transitions and observing playlists as Markov chains. In other words, in their model, the likelihood of a transition is only depen-\nar X\niv :1\n40 1.\n18 80\nv1 [\ncs .L\nG ]\n9 J\nan 2\n01 4\ndent on the acoustic and social-tagging similarity between the current song and a prospective transition candidate. In a later work, Chen et al. [8] take on a similar Markov approach, but do not rely on any acoustic or semantic information about the songs. Instead, they treat playlists as Markov chains in some latent space, and learn a metric representation (or multiple representations) for each song in that space. Subsequently, their approach captures the idea of direction in music playlists, beyond mere likelihood. In somewhat related work, Zheleva et al. [23] adapt a Latent Dirichlet Allocation model to capture music taste from listening activities across users, and identify both the groups of songs associated with the specific taste and the groups of listeners who share the same taste. Subsequently, they define a graphical model that takes listening sessions into account and captures the listening moods. This session model leads to groups of songs and groups of listeners with similar behavior across sessions.\nThe key difference between the approaches surveyed in this section and our methodology is that to the best of our knowledge, no one has attempted to model entire playlists adaptively, or build an actual agent architecture that can interact with a human listener individually. By exploiting user reinforcement to update the model for transition generation, our agent is able to respond to changes in mood and preferences in mid-session, and learn song and transition preference models for users without any prior knowledge."}, {"heading": "3. DATA", "text": "A large component of this work was extrating real world data for both songs and playlists to formalize and test our approach. In this section we present the various data sources we use to model songs and playlists."}, {"heading": "3.1 The Million Song Dataset", "text": "The Million Song Dataset [6] is a freely-available collection of audio features and metadata for a million contemporary popular music tracks. The core of the dataset is the feature analysis and metadata for one million songs, provided by The Echo Nest.1 The dataset covers 44, 745 different artists, and as the name implies, 106 different tracks. The main acoustic features provided for each song include pitches, timbre and loudness extracted for segments delimited by note onsets. The timing of the note onsets is also provided, enabling tempo analysis. An example of the audio input for a single track is provided in Figure 1."}, {"heading": "3.2 The Yes.com Archive", "text": "This playlist dataset was collected by Chen et al. [8]. The playlists and tag data were respectively crawled from Yes.com and Last.fm. Yes.com is a website that provides radio playlists from hundreds of radio stations in the United States. By using the web based API http://api.yes.com, one can retrieve the playlist record of a specified station for the last 7 days. Chen et al. collected as many playlists as possible by specifying all possible genres and getting playlists from all possible stations, harvesting data over an extended period of time from December 2010 to May 2011. This led to a dataset of 75, 262 songs and 2, 840, 553 transitions.\n3.3 The Art of the Mix Archive 1http://the.echonest.com/\nThis corpus was collected by Berenzweig et al. [5]. For this corpus, the authors gathered around 29,000 playlists from The Art of the Mix (www.artofthemix.org), a website that serves as a repository and community center for playlist hobbyists. Therefore, each playlist in this corpus was (ostensibly) generated by a real user, rather than a commercial radio DJ or a music recommendation system, thus making this corpus particularly appealing for listener modeling."}, {"heading": "3.4 Connecting Records across Data Sources", "text": "Having used multiple sources of song data, connecting between song entries from different sources has proven surprisingly more difficult than expected. First of all, most songs appearing in one data source do not appear in the other. Furthermore, there can be variation in artist and song names (for instance, \u201cThe Smashing Pumpkins\u201d vs. \u201cSmashin\u2019 Pumpkins\u201d). To further complicate things, The Art of the Mix Archive, which contains playlists made by real human users, contains many typos. To overcome these issues, we use the following scheme, somewhat similar to the procedure in [14]: for each song observed in a playlist, we take the artist name as it appears in the playlist, and all other artist names associated with it (if such an identifier exists). We then take all these potential names and split them into single words, after cleaning non-letter elements such as \u201c\u201d\u2019. We then generate a unified and extended list of candidate names, comprising all potential names as well as their broken down subparts. We then take each element and use the Echo Nest inexact search API to obtain the top 5 artist name matches for each member in the extended candidate list. Combining all results into a single result list, we then check all the songs existing in the Million Song Dataset by any of these artists. Using Levenshtein distances, we compare the retrieved artist and song names to the original song data appearing in the playlist, and search for the closest match on both fronts. If the edit distance for song name\nand artist name is less than 50% of the average distance, we return a match.\nTo validate this method, we tested it on 40 hand-labeled examples, 20 of which exist in the Million Song Dataset. Our approach retrieves 19 out of the 20 matches correctly, and does not retrieve any false positives, making it suitable for our purposes. More rigorous validation of this method is beyond the scope of this paper."}, {"heading": "4. MODELING", "text": "In this section we discuss the way in which both songs and listeners were modeled. Any vector representation of songs can be used in our framework. The algorithm is more dependent on our specific choices in representing listeners, but we believe the framework could be modified or expanded to support other listener architectures."}, {"heading": "4.1 Modeling Songs", "text": "We assume each song can be factored as a sequence of scalar features. Such scalar features should include details about the spectral fingerprint of the song, its rhythmic characteristics, its overall loudness, etc. For the scope of this work, we used the acoustic features in the Million Song Dataset representation to extract 12 meta-features, out of which 2 are 12-dimensional, resulting in a 34-dimensional feature vector:\n1. 10-th percentile of the tempo - calculated by computing a cumulative distribution of beat durations and finding its 10-th percentile.\n2. 90-th percentile of the tempo - calculated by computing a cumulative distribution of beat durations and finding its 90-th percentile.\n3. The variance in tempo - calculated as variance in beat duration.\n4. The average of the tempo - calculated as the average beat duration.\n5. 10-th percentile of the loudness - calculated by computing a cumulative distribution of maximum amplitude and finding its 10-th percentile.\n6. 90-th percentile of the loudness - calculated by computing a cumulative distribution of maximum amplitude and finding its 90-th percentile.\n7. The variance in loudness - calculated as variance in maximum amplitude.\n8. The average of the loudness - calculated as average maximum amplitude per beat.\n9. Dominant pitch - calculated as the pitch class that is most frequently highest-weighted over all beats.\n10. Dominant pitch variance - calculated as the number of times the dominant pitch changes, normalized by the number of beats.\n11. Average Timbre Weights - the average weights of the 12 basis functions used by the Echo Nest analysis tool to capture the spectro-temporal landscape of the song.\n12. Variance of Timbre Weights - the variance of the weights of the 12 basis functions used by the Echo Nest analysis tool to capture the spectro-temporal landscape of the song."}, {"heading": "4.2 Modeling Listeners", "text": "Despite an abundance of literature on the psychology of human musical perception [20], there is no canonical model of the human listening experience. For the purposes of this work we model listening as being dependant on preferences over features, and preference over feature transitions. Despite its relative simplicity, this model is fairly consistent with many of the observed properties of human perception as indicated in the literature [7, 11,18,20].\nFor each feature, we collect statistics over the entire music database, and quantize the feature into percentiles. For each percentile, the listener assigns (internally) a value representing the reward it gives (pleasure it obtains) thanks to that specific feature. For a given song vector i, the \u201cmyopic\u201d song reward is the linear sum of rewards obtained for each feature of the song individually: rewardi = \u2211 k\u2208featuresReward k i . Beyond myopic reward, there is also reward for transitions between songs, which are modeled as the linear combination of rewards for transitions between feature values. In other words, reward(i, j) = \u2211 k\u2208features reward k fi,fj , where rewardkfi,fj is the reward for transitioning from value songi[k] = fi to value songj [k] = fj in the specific feature k. To reduce complexity, we discretize each feature by percentiles. We also make the simplifying assumption that transition rewards are independent for each feature. They are not necessarily symmetric in our model.\nTo model the complex dependence of music on history, transitions are considered not only between the most recent song and the one before it, but aggregately across history, in the following way: for the t-th song, each (t \u2212 i)th song has chance 1\ni of affecting the transition reward, by\ncontributing 1 i \u00b7 rewardt,t\u2212i. It follows that each song in the history contributes ( 1 i )2 of its relative transition reward in expectation. The total reward for a new song is therefore: totalRewardi = rewardi+ \u2211 i\u2208[1...horizon] ( 1 i )2 \u00b7 reward(t, t\u2212 i) in expectation. As a consequence, the last song is a nonMarkovian state signal with respect to listener preferences.\nWe model rewards and transitions in this way mostly for the sake of tractability. Even for a moderate set of 10 features each discretized to 10 percentiles, the entire set of possible transitions is of size 1010 \u00d7 1010, which is clearly intractable. In each feature, however, there are 102 possible transitions, which makes modeling much more manageable. More complex modeling of transitions (dependent on feature pairs, for instance) is possible, but complicate things considerably. We find that this listener architecture, in which the rewards for each feature and each percentile are independent, offers a great deal of flexibility in modeling a very wide range of potential listeners. Some like music that is slow and quiet, some like music that is loud and fast, and some like all possible mixes in between, and likewise for all sorts of transitions. The purpose of our framework is to be as general and as robust as possible when dealing with a wide array of potential listeners. We believe our modeling is suitable for this purpose."}, {"heading": "4.3 Expressiveness of the Transition Model", "text": "In order for our features to have a chance at modeling dif-\nferent users\u2019 transition preferences, they must be able to capture, at least crudely, the difference between sequences which are commonly perceived as \u201cgood\u201d vs. sequences which are commonly perceived as \u201cbad\u201d. To gather evidence that the features are expressive in this way, we examine the transition profile for two families of transitions, \u201cpoor\u201d vs. \u201cfair\u201d, but using the same population of songs. We generate \u201cfair\u201d transitions by sampling pairs of songs that originally appeared in sequence. We generate \u201cpoor\u201d transitions by interleaving songs so that each one is distinctly different in character(for instance, a heavy metal track followed by a soft jazz piece). The difference between the two profiles can be seen in Figure 2. Though we would expect different profiles for different people and for different datasets, the clear differences (19 of the 34 are discriminative for this song set) suggest that our feature set will enable discriminative predictions of user preferences.\nTo clarify, we use the words \u201cpoor\u201d and \u201cfair\u201d to reflect common perceptions, but our approach in general indicates that our model is expressive enough to capture differences between separate types of transitions, regardless of how one might label them, aesthetically."}, {"heading": "4.4 Predictive Power of the Model", "text": "Additional evidence for the validity of our model should come from its predictive power. To test how predictive our models are, we took the corpus of Yes.com playlists (see Section 3.2), partitioned them into a test set and a training set, and subsequently trained a listener model using the training set. Following this step, we examined the cumulative reward our listener model yielded on the test set. We compared this reward to the cumulative reward our model obtained on randomly generated playlists. To obtain sufficient statistics, we repeated this process 50 times and averaged the results. As hoped, our model yields considerably higher reward on the test playlists compared to randomly generated ones. This is evident in Figure 3, which shows the differences in expected reward obtained by the model over real playlists compared to randomly generated ones, for varying train set sizes.\nThe exact procedure for training a listener model based on playlists is provided in Section 6.3."}, {"heading": "5. RL FRAMEWORK", "text": "In this section we discuss our reinforcement learning approach to a playlist-oriented music recommendation system. Two main principles guide the system:\n\u2022 In order to generate an effective sequence of songs we need to model not only the myopic preferences of the listener but also his transition preferences.\n\u2022 In any online playlist generation task, the extent of explicit exploration is very limited.\nThe first principle is straightforward. Regarding the second one, we note that the potential transition space, even for databases of moderate size, is extremely large. Effectively, no listener would endure a long extent of pure exploration between songs and genres (say, from Russian folk music to Japanese noise rock) until the system \u201clocks in\u201d on a passable model of his preferences. To make exploration as focused and as effective as possible, we present a model learning scheme which we dub\u201cLearning Estimates from Active Feedback\u201d, or LEAF.\nMDP Model We consider the adaptive playlist generation problem formally as a Markov Decision Process (MDP). An episodic MDP is a tuple (S,A, P,Rs, Rt, T ) where S is the set of states; A the set of actions, P : (S,A, S)\u2192 [0, 1] is the state transition probability function where P (s, a, s\u2032) denotes the probability of transitioning from state s to state s\u2032 when taking action a. Rs : S \u2192 R and Rt : S \u00d7 S \u2192 R are the state reward function and the transition reward function, respectively. T is the set of terminal states, which end the episode. In effect, the playlist history is the true state of the system in our setting. The action space is the selection of songs. At each step, we choose which should be the next song played to the listener. In order to keep the system realistic, we enforce the restriction that no song can be played twice in the same session. This restriction is reasonable since we only deal with playlists of length \u2264 50 songs (under the assumption that the average song is 3\u2212 4 minutes long, we\nlimit playlists to be \u2264 150 \u2212 200 minutes long, which is a reasonable length for a playlist session). The set of terminal states T is {S|length of S is 50}. We note that the myopic reward for each song is independent of its location in the sequence, but the choice of placement can drastically affect the transition reward to and from it."}, {"heading": "5.1 Model Learning", "text": "The exact effect of history is non-deterministic, and is unknown to the DJ agent. Because of this difficulty, and because of how large the real state space may be even when only considering single transitions and music databases of moderate size, a straightforward value-function based strategy, such as learning the state-action value function Q(s, a), is infeasible. We opted for a different approach, in which we try to generate a rough estimate of the listener\u2019s preferences, and then utilize a simple form of tree search for planning. The LEAF scheme is a means to overcome the difficulty of sufficient exploration. It is reliant on feedback from the listener, and it is designed to ask for it actively in a targeted way. The scheme has two main parts:\n1. At the first stage, the system asks the listener for his favorite N songs in the database. These are used to train a model for myopic song rewards. This step is equivalent to the seeding phase that exists in most Internet music streaming services.\n2. Then, the system applies the \u03b4-medoids algorithm, a novel method for representative selection [1]. This algorithm, once provided with a distance criterion \u03b4, returns a compact but close-fitting subset of representatives such that no sample in the dataset is more than \u03b4 away from a representatives. We use this property to select a set of representatives from the music database. \u03b4 is initialized to be the 10-th percentile of the distance histogram between all pairs of songs in the database. The systems chooses songs from the representative set and queries the listener which, out of the representatives (or a random subset of the representatives if the representative set is too large), it would like to listen to next. These observations are used to train a model for song transitions. We make the somewhat simplifying assumption that at least a large fraction of the database is known to the listener. It is possible to modify the framework so that learning at this stage is only done on a small known subset of the artists.\n5.1.1 Learning Song Preferences We begin by initializing the predicted value of each per-\ncentile for each feature to 1. This procedure can be perceived as maintaining a Bayesian uniform prior or as a Laplace\u201cadd 1\u201d form of smoothing, since each song contributes n to the overall sum of updates. We do not normalize since we only care about aggregate reward, rendering normalization unnecesary. Subsequently, to learn myopic song preferences we poll the listener for his N = 10 favorite songs in the database. The favorite songs are defined as those with the highest myopic reward. For each song in this set we go over each feature and increase the value of the corresponding percentile by 1. This step can be perceived as a crude frequentist approximation to the expected myopic reward. Pseudocode for this algorithm is presented in Algorithm 1.\nAlgorithm 1 Learning Song Preferences\n1: Input: data preferredSet = x0 . . . xm 2: for i = 0 to m do 3: for featuref \u2208 xi do 4: find closest feature percentile p for f 5: rewardfp = rewardfp + 1 6: end for 7: end for\n5.1.2 Learning Transition Preferences In the second stage, we query the listener\u2019s preferences\nregarding transitions. As before, we first initialize the predicted value of transition from percentile i to percentile j for each feature to 1. Again, this can be perceived as maintaining a Bayesian uniform prior or as a Laplace \u201cadd 1\u201d form of smoothing.\nWe wouldn\u2019t want to query transitions only for preferred songs, for instance, because that won\u2019t necessarily reveal much about the preferred transitions. For this reason we would try to explore the preferences of the listener in a targeted fashion, by presenting him with different possible transitions that encapsulate the variety in the dataset, and directly asking which of a possible set of options the listener would prefer. On the other hand, we would also like to exclude regions in the search space where expected song rewards are particularly low.\nTo accomplish both ends, we first choose a subset of 50- percent of the songs in the database, which we believe, based on our song rewards model, obtains the highest myopic reward. We do so because estimating song rewards is easier than estimating song transition rewards, so we would like to at least guarantee that we don\u2019t select songs with very low expected myopic reward just because we believe they may yield high transition rewards.\nThen, we query transition preferences over the upper median of songs by eliciting user feedback. We do so by first extracting a characteristic set of songs from the entire database, using the \u03b4-medoids algorithm. We define the 10-th percentile of the overall pairwise distances between songs in the dataset as the value of \u03b4, the threshold for selecting a new representative. Subsequently, we begin choosing songs from this representative subset, and ask the listener which, out of this set (or a random subset of the characteristic subset, depending on the parameters of the system and the representative set size) they would like to listen to next. In our model, the listener chooses the next song he would like by simulating the listening experience, including the non-deterministic history-dependent transition reward, and choosing the one with the maximal total reward. We then proceed to update the characteristics of this transition, by increasing the weight of feature percentile transitions by 1, similarly to how we updated the model for myopic preferences. The full details of the algorithm are described in Algorithm 2.\nSince the songs are selected from a set that best captures the entire database, and since the songs selected are not necessarily part of the listener\u2019s \u201cpreferred set\u201d, this approximate model proves useful, as we later show empirically."}, {"heading": "5.2 Planning Via Tree Search", "text": "Once we\u2019ve obtained estimates of song rewards and song transition rewards, we employ a simple tree-search heuristic for planning, similar to that used in [21]. As before, we\nAlgorithm 2 Learning Transition Preferences\n1: Input: representative representativeSet = \u03b4 \u2212 Medoids(full dataset, \u03b4 = 10th percentile of pairwise distances ) 2: prevSong = choose a song randomly from representativeSet 3: while playlist hasn\u2019t reached its end do 4: nextSong = argmax\ni\u2208preferredSet(totalRewardi)\n(chosen by the listener without revealing internals to DJMC)\n5: for f \u2208 featureSet do 6: find closest percentile for value of f in prevSong, p1 7: find closest percentile for value of f in nextSong, p2 8: rewardfp1,fp2 = rewardfp1,fp2 + 1 9: end for\n10: end while\nAlgorithm 3 Tree Search Planning\n1: sort songs in database by expected myopic reward, put all songs above the median in PromisingSet 2: BestTrajectory = null 3: HighestExpectedPayoff = \u2212\u221e 4: while computational power not exhausted do 5: trajectory = [] 6: for 1.. . . . planningHorizon do 7: song \u2190 selected randomly from PromisingSet (avoiding repetitions) 8: add song to trajectory 9: end for 10: expectedPayoffForTrajectory = result of simulation on models 11: if expectedPayoffForTrajectory > HighestExpectedPayoff then 12: HighestExpectedPayoff = expectedPayoffForTrajectory 13: BestTrajectory = trajectory 14: end if 15: end while 16: return BestTrajectory [0]\nchoose a subset of 50-percent of the songs in the database, which we believe, based on our song rewards model, obtain the highest myopic reward. Thus the search spaces for model learning and planning are the same. At each point, we simulate a trajectory of future songs selected at random from this \u201chigh-yield\u201d subset. We use the playlist history and our model for song rewards and transition rewards to calculate the expected payoff of the song trajectory. We repeat this process n times, finding the randomly generated trajectory which yields the highest expected payoff. We then select the first song of this trajectory to be the next song played. We use just the first song and not the whole sequence because as modeling noise accumulates, our estimates become farther off. Furthermore, as we discuss in the next subsection, we also adjust our model at each step if need be. The tree search algorithm is described in Algorithm 3."}, {"heading": "5.3 Credit Assignment for Reward", "text": "Both in real life and in our model, once a user provides\nfeedback for a transition (either explicitly or implicitly), it is in the form of a unified signal combining both the individual song reward and the transition reward. In order to update our song and transition models effectively, we need to assign credit for each component separately. We achieve this goal by computing the factors of the expected song and transition rewards in the overall expected reward, and using these factors as the weights in the update stage. In other words, given that we have observed reward rt, and we have expected song and transition rewards resong and r e transition, respectively, we update the current song feature profile by resong resong+r e transition and the current feature transition profile by\nretransition resong+r e transition ."}, {"heading": "5.4 DJ-MC: Complete Agent Architecture", "text": "Finally we describe the full architecture of the DJ agent:\n\u2022 Learning Estimates from Active Feedback (LEAF):\n1. The agent polls a preferredSet of size m of songs the listener likes best, standalone (i.e. the m songs which maximize myopic song rewards), and uses this set to train a model of song rewards.\n2. The agent calculates a pairwise distance matrix between all songs in the database, and the 10-th percentile of distances \u03b4\u2217.\n3. The agent distills a representative set representativeSet from the entire song database, using the \u03b4-medoid algorithm and the precomputed \u03b4\u2217.\n4. For k steps, the algorithm chooses songs from representativeSet, and asks the listener what song out of representativeSet (or a subset of it if the full set is too large) he would like to listen to next). Given the choices of the listener, the agent updates its model for transition rewards.\n\u2022 Planning and Model Update:\n1. For horizon\u2212k steps, we use TreeSearch to plan the next action, obtaining total reward reward.\n2. We use reward to update our song reward and transition reward models:\n\u2013 We define rincr = log(reward/avgreward). avgreward is the average of the actual rewards we\u2019ve seen thus far from the beginning of history. This factor determines the direction of the update (negative if reward < avgreward, positive if reward > avgreward. It also determines the magnitude of the update - we update more the farther we are from the expected reward.\n\u2013 Given expected song and transition rewards resong and r e transition we set weights for the\ncredit assignment:\n\u2217 wsong = resong\nresong+r e transition\n\u2217 wtransition = r e transition\nresong+r e transition\n\u2013 Update:\n\u2217 Update expected song feature-percentile rewards by wsong \u00b7 rincr \u2217 Update expected transition feature-percentile-\npercentile rewards by wtransition \u00b7 rincr\nIn the next section we discuss the effectiveness of our framework both on randomly generated listener models, and on listener models generated based on man-made music playlists."}, {"heading": "6. EXPERIMENTAL SETUP", "text": "To evaluate our results we test our algorithm both on a large set of random listeners, and on listener models trained based on real playlist data made by people (see subsection 3.3). To obtain a song corpus, at each experiment, we sample a subset of 1000 songs out of the Million Song Dataset and use it as a corpus for playlist generation."}, {"heading": "6.1 Baselines", "text": "In order to measure the improvement offered by our agent, we compare our DJ-MC algorithm against two different baselines:\n1. A random agent - an agent that chooses songs completely at random out of the dataset. The absolute baseline.\n2. A greedy agent - an agent that uses Algorithm 1 to generate an estimate of myopic rewards for songs and only uses it, disregarding transitions. The algorithm generates a model of myopic preferences, ranks all songs by expected myopic reward, and then plays them by order, starting from most highly ranked, until it reaches the end of the playlist. This method can be seen as closer to the straightforward approach of learning song rewards independently.\nWe show that our algorithm, by modeling transitions (even partially and approximately), does better than a more straightforward approach, which ignores transition rewards."}, {"heading": "6.2 Random Listeners", "text": "We first evaluate DJ-MC based on the agent\u2019s ability to adapt to purely random agents. For each listener, a random value for features and feature transitions was assigned from the range [0, 1] uniformly and independently for each feature and each feature transition. This is a challenging task as there is greater variety and unpredictability in such listeners compared to most human listeners, which can be more easily categorized. We test DJ-MC over 20 randomly generated listeners and average the results for each one of the three approaches we\u2019re testing (random, greedy, and our agent architecture). For these experiments we used a playlist length of 50 songs, a planning horizon of 10 songs ahead, a computational budget of 100 random trajectories for planning, a query size of 10 songs for song reward modeling and 10 songs for transition rewards. The results are given in Figure 4.\nThe DJ-MC agent performs consistently better than the baselines, most noticeably in the beginning of the session - knowledge of transition preferences greatly alleviates the cold start problem in planning."}, {"heading": "6.3 Modeling Listeners via Playlists", "text": "To generate potential human listeners, we use the Art of the Mix dataset, which, as previously indicated, was collected from real user submissions. Subsequently, we use kmeans clustering on the playlists (represented as artist frequency vectors), to generate 10 playlist clusters. Each such\ncluster represents a basic listener type. To generate listeners, we sample 70% of the song transition pairs in a playlist cluster, and use them to train a listener model, similarly to how DJ-MC learns a transition model for a user given examples. We repeat this procedure 20 times, first choosing a random cluster and then choosing a random transition subset to train the listener on. As before, we test DJ-MC over 20 randomly generated listeners and average the results for each one of the three approaches we\u2019re testing (random, greedy, and DJ-MC). For these experiments we used a playlist length of 50 songs, a planning horizon of 10 songs ahead, a computational budget of 100 random trajectories for planning, a query size of 10 songs for song reward modeling and 10 songs for transition rewards. The results are given in Figure 5.\nAs before the DJ-MC agent outperforms, doing considerably better from the beginning of the session."}, {"heading": "7. DISCUSSION & FUTURE WORK", "text": "In this work we presented a full DJ agent, which we call DJ-MC, meant to learn the preferences of an individual listener in a targeted way, and generate a suitable playlist adaptively. In the experimental session we show that our approach offers strict improvement over a more naive (and standard) approach, which only considers song rewards.\nThe task in this work, of modeling listeners and generating a playlist accordingly, is far from trivial. Some evidence for this difficulty is the fact that in Figures 4 and 5, the greedy, myopic heuristic offers only a limited improvement over a purely random song selector. The difficulty originates from three issues. First, the way listeners are modeled offers very little structure, and therefore the amount of learning that can be done from selecting any limited set of n \u201cfavorite\u201d songs is partial and noisy at best. Enforcing some dependence between percentiles would allow us to generalize from observations belonging to some percentile i to its neighboring percentiles i+ 1, i\u2212 1, for instance. Second, we don\u2019t know how to accurately assign credit for rewards both temporally, and also between features and feature transitions, forcing us to rely on our model for credit assignment. Third, the way in which we limit the extent of exploration and the duration of each playlist building session forces the algorithm to learn based on very little information. Still, even these noisy models offer a considerable improvement over a greedy/myopic algorithm that selects songs regardless of sequence.\nIn this work we focus on the audio properties of songs. This approach has the advantage of being able to generate pleasing playlists that are unexpected with respect to traditional genre and period classifications, social tagging data etc. However, in future work, it would be of interest to combine these two types of information sources to generate a model that considers both the intrinsic sonic features of songs as well as varied sources of metadata (e.g. tags, social data, artist co-occurrence rates, genre classification, time period etc.). In general, a more refined feature set could be obtained. Such a model might be able to capture dependencies between features, or the temporal aspect of songs, enabling a more sophisticated model of listeners. However, these refinements would make the state space for learning much larger, and may force us to use even more abstraction in the process of modeling.\nIt would also be of interest to test our framework on specific types of listeners and music corpora. Another intriguing idea is to model listeners not as static and nonstationary, but as evolving over time, i.e., have the choice of songs by the DJ affect the listener\u2019s preferences over songs and transitions. Ultimately (and most importantly), it would be of great interest to test the framework when interacting with real people. Good performance when generating playlists for real human listeners online would serve as the best evidence for the validity of our approach. However, due to the time and difficulty of such human testing, especially in listening sessions lasting 3-4 hours, it is important to first validate the approach in simulation, as we do in this paper.\nThe framework we present in this paper is not optimized - it is dependent on many parameters (such as lookahead horizon for planning, number of random trajectories generated for planning, step sizes for estimate updates in different context and so forth). There is plenty of room for experimentation and optimization that would lead the framework to the best expected performance. It is quite feasible to think of other learning approaches (regression-based, for instance) that would lead to a better estimate of listener preferences. All in all, however, we believe this work shows promise for both creating better music recommendation systems, and proving the effectiveness of a reinforcement-learning based approach in a new practical domain."}, {"heading": "8. REFERENCES", "text": "[1] Representative selection in non metric datasets. In The 31st\nInternational Conference on Machine Learning (ICML 2014) - submitted for review, 2014. [2] G. Adomavicius and A. Tuzhilin. Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions. Knowledge and Data Engineering, IEEE Transactions on, 17(6):734\u2013749, 2005. [3] N. Aizenberg, Y. Koren, and O. Somekh. Build your own music recommender by modeling internet radio streams. In Proceedings of the 21st international conference on World Wide Web, pages 1\u201310. ACM, 2012. [4] L. Barrington, R. Oda, and G. Lanckriet. Smarter than genius? human evaluation of music recommender systems. In International Symposium on Music Information Retrieval, 2009. [5] A. Berenzweig, B. Logan, D. P. Ellis, and B. Whitman. A large-scale evaluation of acoustic and subjective music-similarity measures. Computer Music Journal, 28(2):63\u201376, 2004. [6] T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and P. Lamere. The million song dataset. In ISMIR 2011: Proceedings of the 12th International Society for Music Information Retrieval Conference, October 24-28, 2011, Miami, Florida, pages 591\u2013596. University of Miami, 2011. [7] W. L. Berz. Working memory in music: A theoretical model. Music Perception, pages 353\u2013364, 1995. [8] S. Chen, J. L. Moore, D. Turnbull, and T. Joachims. Playlist prediction via metric embedding. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 714\u2013722. ACM, 2012. [9] D. Cliff. Hang the dj: Automatic sequencing and seamless mixing of dance-music tracks. HP LABORATORIES TECHNICAL REPORT HPL, (104), 2000.\n[10] M. Crampes, J. Villerd, A. Emery, and S. Ranwez. Automatic playlist composition in a dynamic music landscape. In Proceedings of the 2007 international workshop on Semantically aware document processing and indexing, pages 15\u201320. ACM, 2007. [11] J. B. Davies and J. B. Davies. The psychology of music. Hutchinson London, 1978. [12] D. Jurafsky, J. H. Martin, A. Kehler, K. Vander Linden, and N. Ward. Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition, volume 2. MIT Press, 2000. [13] F. Maillet, D. Eck, G. Desjardins, P. Lamere, et al. Steerable playlist generation by learning song similarity from radio station playlists. In ISMIR, pages 345\u2013350, 2009. [14] B. McFee and G. R. Lanckriet. The natural language of playlists. In ISMIR, pages 537\u2013542, 2011. [15] J. O\u2019Donovan and B. Smyth. Trust in recommender systems. In Proceedings of the 10th international conference on Intelligent user interfaces, pages 167\u2013174. ACM, 2005. [16] N. Oliver and L. Kreger-Stickles. Papa: Physiology and purpose-aware automatic playlist generation. In Proc. 7th Int. Conf. Music Inf. Retrieval, pages 250\u2013253, 2006. [17] J. C. Platt. Fast embedding of sparse similarity graphs. In Advances in Neural Information Processing Systems, page None, 2003. [18] C. E. Seashore. Psychology of music. New York: Dover Publications, 1938. [19] X. Su and T. M. Khoshgoftaar. A survey of collaborative filtering techniques. Advances in artificial intelligence, 2009:4, 2009. [20] S.-L. Tan, P. Pfordresher, and R. Harre\u0301. Psychology of music: From sound to significance. Psychology Press, 2010. [21] D. Urieli and P. Stone. A learning agent for heat-pump thermostat control. In Proceedings of the 12th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), May 2013. [22] J. Weston, S. Bengio, and P. Hamel. Multi-tasking with joint semantic spaces for large-scale music annotation and retrieval. Journal of New Music Research, 40(4):337\u2013348, 2011. [23] E. Zheleva, J. Guiver, E. Mendes Rodrigues, and N. Milic\u0301-Frayling. Statistical models of music-listening sessions in social media. In Proceedings of the 19th international conference on World wide web, pages 1019\u20131028. ACM, 2010."}], "references": [{"title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions", "author": ["G. Adomavicius", "A. Tuzhilin"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, 17(6):734\u2013749", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Build your own music recommender by modeling internet radio streams", "author": ["N. Aizenberg", "Y. Koren", "O. Somekh"], "venue": "Proceedings of the 21st international conference on World Wide Web, pages 1\u201310. ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Smarter than genius? human evaluation of music recommender systems", "author": ["L. Barrington", "R. Oda", "G. Lanckriet"], "venue": "International Symposium on Music Information Retrieval", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "A large-scale evaluation of acoustic and subjective music-similarity measures", "author": ["A. Berenzweig", "B. Logan", "D.P. Ellis", "B. Whitman"], "venue": "Computer Music Journal, 28(2):63\u201376", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "The million song dataset", "author": ["T. Bertin-Mahieux", "D.P. Ellis", "B. Whitman", "P. Lamere"], "venue": "ISMIR 2011: Proceedings of the 12th International Society for Music Information Retrieval Conference, October 24-28, 2011, Miami, Florida, pages 591\u2013596. University of Miami", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Working memory in music: A theoretical model", "author": ["W.L. Berz"], "venue": "Music Perception, pages 353\u2013364", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1995}, {"title": "Playlist prediction via metric embedding", "author": ["S. Chen", "J.L. Moore", "D. Turnbull", "T. Joachims"], "venue": "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 714\u2013722. ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Hang the dj: Automatic sequencing and seamless mixing of dance-music tracks", "author": ["D. Cliff"], "venue": "HP LABORATORIES TECHNICAL REPORT HPL, (104)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Automatic playlist composition in a dynamic music landscape", "author": ["M. Crampes", "J. Villerd", "A. Emery", "S. Ranwez"], "venue": "Proceedings of the 2007 international workshop on Semantically aware document processing and indexing, pages 15\u201320. ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "The psychology of music", "author": ["J.B. Davies", "J.B. Davies"], "venue": "Hutchinson London", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1978}, {"title": "Speech and language processing: An introduction to natural language processing", "author": ["D. Jurafsky", "J.H. Martin", "A. Kehler", "K. Vander Linden", "N. Ward"], "venue": "computational linguistics, and speech recognition, volume 2. MIT Press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "et al", "author": ["F. Maillet", "D. Eck", "G. Desjardins", "P. Lamere"], "venue": "Steerable playlist generation by learning song similarity from radio station playlists. In ISMIR, pages 345\u2013350", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "The natural language of playlists", "author": ["B. McFee", "G.R. Lanckriet"], "venue": "ISMIR, pages 537\u2013542", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Trust in recommender systems", "author": ["J. O\u2019Donovan", "B. Smyth"], "venue": "In Proceedings of the 10th international conference on Intelligent user interfaces,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Papa: Physiology and purpose-aware automatic playlist generation", "author": ["N. Oliver", "L. Kreger-Stickles"], "venue": "Proc. 7th Int. Conf. Music Inf. Retrieval, pages 250\u2013253", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast embedding of sparse similarity graphs", "author": ["J.C. Platt"], "venue": "Advances in Neural Information Processing Systems, page None", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Psychology of music", "author": ["C.E. Seashore"], "venue": "New York: Dover Publications", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1938}, {"title": "A survey of collaborative filtering techniques", "author": ["X. Su", "T.M. Khoshgoftaar"], "venue": "Advances in artificial intelligence, 2009:4", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Psychology of music: From sound to significance", "author": ["S.-L. Tan", "P. Pfordresher", "R. Harr\u00e9"], "venue": "Psychology Press", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "A learning agent for heat-pump thermostat control", "author": ["D. Urieli", "P. Stone"], "venue": "In Proceedings of the 12th International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Multi-tasking with joint semantic spaces for large-scale music annotation and retrieval", "author": ["J. Weston", "S. Bengio", "P. Hamel"], "venue": "Journal of New Music Research, 40(4):337\u2013348", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Statistical models of music-listening sessions in social media", "author": ["E. Zheleva", "J. Guiver", "E. Mendes Rodrigues", "N. Mili\u0107-Frayling"], "venue": "Proceedings of the 19th international conference on World wide web, pages 1019\u20131028. ACM", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "In recent years, recommender systems have risen in prominence as a field of research [2].", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "Music has been one of the main domains in which recommender systems have been developed and tested, both academically [2, 4, 15] and commercially [4].", "startOffset": 118, "endOffset": 128}, {"referenceID": 2, "context": "Music has been one of the main domains in which recommender systems have been developed and tested, both academically [2, 4, 15] and commercially [4].", "startOffset": 118, "endOffset": 128}, {"referenceID": 13, "context": "Music has been one of the main domains in which recommender systems have been developed and tested, both academically [2, 4, 15] and commercially [4].", "startOffset": 118, "endOffset": 128}, {"referenceID": 2, "context": "Music has been one of the main domains in which recommender systems have been developed and tested, both academically [2, 4, 15] and commercially [4].", "startOffset": 146, "endOffset": 149}, {"referenceID": 9, "context": "More importantly, it is a relatively well known fact in music cognition that music is experienced in temporal context and in sequence [11,18].", "startOffset": 134, "endOffset": 141}, {"referenceID": 16, "context": "More importantly, it is a relatively well known fact in music cognition that music is experienced in temporal context and in sequence [11,18].", "startOffset": 134, "endOffset": 141}, {"referenceID": 7, "context": "This fact is also well known among DJs, who perceive playlist-building as a whole [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "Indeed, some work in automated playlist building has attempted to attack this problem from the DJs perspective [9, 10, 16].", "startOffset": 111, "endOffset": 122}, {"referenceID": 8, "context": "Indeed, some work in automated playlist building has attempted to attack this problem from the DJs perspective [9, 10, 16].", "startOffset": 111, "endOffset": 122}, {"referenceID": 14, "context": "Indeed, some work in automated playlist building has attempted to attack this problem from the DJs perspective [9, 10, 16].", "startOffset": 111, "endOffset": 122}, {"referenceID": 15, "context": "[17] use semantic tags to learn a Gaussian process kernel function between pairs of songs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] learn an embedding in a shared space of social tags, acoustic features and artist entities by optimizing an evaluation metric for various music retrieval tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[3] put forth a probabilistic approach.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "These works have close ties with the larger body of research dealing with sequence modeling in natural language processing [12].", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "via collaborative filtering [19]) are known, the specifics of how a certain order is determined are not revealed, nor is it known which criteria are used.", "startOffset": 28, "endOffset": 32}, {"referenceID": 11, "context": "[13] approach the playlist prediction problem from a supervised binary classification perspective.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Mcfee and Lanckriet [14] consider playlists as a natural language induced over songs, training a bigram model for transitions and observing playlists as Markov chains.", "startOffset": 20, "endOffset": 24}, {"referenceID": 6, "context": "[8] take on a similar Markov approach, but do not rely on any acoustic or semantic information about the songs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[23] adapt a Latent Dirichlet Allocation model to capture music taste from listening activities across users, and identify both the groups of songs associated with the specific taste and the groups of listeners who share the same taste.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The Million Song Dataset [6] is a freely-available collection of audio features and metadata for a million contemporary popular music tracks.", "startOffset": 25, "endOffset": 28}, {"referenceID": 6, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "To overcome these issues, we use the following scheme, somewhat similar to the procedure in [14]: for each song observed in a playlist, we take the artist name as it appears in the playlist, and all other artist names associated with it (if such an identifier exists).", "startOffset": 92, "endOffset": 96}, {"referenceID": 18, "context": "Despite an abundance of literature on the psychology of human musical perception [20], there is no canonical model of the human listening experience.", "startOffset": 81, "endOffset": 85}, {"referenceID": 5, "context": "Despite its relative simplicity, this model is fairly consistent with many of the observed properties of human perception as indicated in the literature [7, 11,18,20].", "startOffset": 153, "endOffset": 166}, {"referenceID": 9, "context": "Despite its relative simplicity, this model is fairly consistent with many of the observed properties of human perception as indicated in the literature [7, 11,18,20].", "startOffset": 153, "endOffset": 166}, {"referenceID": 16, "context": "Despite its relative simplicity, this model is fairly consistent with many of the observed properties of human perception as indicated in the literature [7, 11,18,20].", "startOffset": 153, "endOffset": 166}, {"referenceID": 18, "context": "Despite its relative simplicity, this model is fairly consistent with many of the observed properties of human perception as indicated in the literature [7, 11,18,20].", "startOffset": 153, "endOffset": 166}, {"referenceID": 19, "context": "Once we\u2019ve obtained estimates of song rewards and song transition rewards, we employ a simple tree-search heuristic for planning, similar to that used in [21].", "startOffset": 154, "endOffset": 158}], "year": 2017, "abstractText": "In recent years, there has been growing focus on the study of automated recommender systems. Music recommendation systems serve as a prominent domain for such works, both from an academic and a commercial perspective. A relatively well known fact in music cognition is that music is experienced in temporal context and in sequence. In this work we present DJ-MC, a reinforcement-learning based framework for music recommendation that does not recommend songs individually but rather song sequences, or playlists, based on a learned model of preferences for both individual songs and song transitions. To reduce exploration time, we initialize a model based on user feedback. This model is subsequently updated by reinforcement. We show our algorithm outperforms a more naive approach, using both real song data and real playlist data to validate our approach.", "creator": "LaTeX with hyperref package"}}}