{"id": "1401.3426", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Networks of Influence Diagrams: A Formalism for Representing Agents' Beliefs and Decision-Making Processes", "abstract": "This paper presents Networks of Influence Diagrams (NID), a compact, natural and highly expressive language for reasoning about agents beliefs and decision-making processes. NIDs are graphical structures in which agents mental models are represented as nodes in a network; a mental model for an agent may itself use descriptions of the mental models of other agents. NIDs are demonstrated by examples, showing how they can be used to describe conflicting and cyclic belief structures, and certain forms of bounded rationality. In an opponent modeling domain, NIDs were able to outperform other computational agents whose strategies were not known in advance. NIDs are equivalent in representation to Bayesian games but they are more compact and structured than this formalism. In particular, the equilibrium definition for NIDs makes an explicit distinction between agents optimal strategies, and how they actually behave in reality.", "histories": [["v1", "Wed, 15 Jan 2014 04:38:59 GMT  (530kb)", "http://arxiv.org/abs/1401.3426v1", null]], "reviews": [], "SUBJECTS": "cs.GT cs.AI", "authors": ["yaakov gal", "avi pfeffer"], "accepted": false, "id": "1401.3426"}, "pdf": {"name": "1401.3426.pdf", "metadata": {"source": "CRF", "title": "Networks of Influence Diagrams: A Formalism for Representing Agents\u2019 Beliefs and Decision-Making Processes", "authors": ["Ya\u2019akov Gal", "Avi Pfeffer"], "emails": ["gal@csail.mit.edu", "avi@eecs.harvard.edu"], "sections": [{"heading": null, "text": "highly expressive language for reasoning about agents\u2019 beliefs and decision-making processes. NIDs are graphical structures in which agents\u2019 mental models are represented as nodes in a network; a mental model for an agent may itself use descriptions of the mental models of other agents. NIDs are demonstrated by examples, showing how they can be used to describe conflicting and cyclic belief structures, and certain forms of bounded rationality. In an opponent modeling domain, NIDs were able to outperform other computational agents whose strategies were not known in advance. NIDs are equivalent in representation to Bayesian games but they are more compact and structured than this formalism. In particular, the equilibrium definition for NIDs makes an explicit distinction between agents\u2019 optimal strategies, and how they actually behave in reality."}, {"heading": "1. Introduction", "text": "In recent years, decision theory and game theory have had a profound impact on the design of intelligent systems. Decision theory provides a mathematical language for single-agent decision-making under uncertainty, whereas game theory extends this language to the multiagent case. On a fundamental level, both approaches provide a definition of what it means to build an intelligent agent, by equating intelligence with utility maximization. Meanwhile, graphical languages such as Bayesian networks (Pearl, 1988) have received much attention in AI because they allow for a compact and natural representation of uncertainty in many domains that exhibit structure. These formalisms often lead to significant savings in representation and in inference time (Dechter, 1999; Cowell, Lauritzen, & Spiegelhater, 2005).\nRecently, a wide variety of representations and algorithms have augmented graphical languages to be able to represent and reason about agents\u2019 decision-making processes. For the single-agent case, influence diagrams (Howard & Matheson, 1984) are able to represent and to solve an agent\u2019s decision making problem using the principles of decision theory. This representation has been extended to the multi-agent case, in which decision problems are solved within a game-theoretic framework (Koller & Milch, 2001; Kearns, Littman, & Singh, 2001).\nThe focus in AI so far has been on the classical, normative approach to decision and game theory. In the classical approach, a game specifies the actions that are available to the agents, as well as their utilities that are associated with each possible set of agents\u2019\nc\u00a92008 AI Access Foundation. All rights reserved.\nactions. The game is then analyzed to determine rational strategies for each of the agents. Fundamental to this approach are the assumptions that the structure of the game, including agents\u2019 utilities and their actions, is known to all of the agents, that agents\u2019 beliefs about the game are consistent with each other and correct, that all agents reason about the game in the same way, and that all agents are rational in that they choose the strategy that maximizes their expected utility given their beliefs.\nAs systems involving multiple, autonomous agents become ubiquitous, they are increasingly deployed in open environments comprising human decision makers and computer agents that are designed by or represent different individuals or organizations. Examples of such systems include on-line auctions, and patient care-delivery systems (MacKie-Mason, Osepayshivili, Reeves, & Wellman, 2004; Arunachalam & Sadeh, 2005). These settings are challenging because no assumptions can be made about the decision-making strategies of participants in open environments. Agents may be uncertain about the structure of the game or about the beliefs of other agents about the structure of the game; they may use heuristics to make decisions or they may deviate from their optimal strategies (Camerer, 2003; Gal & Pfeffer, 2003b; Rajarshi, Hanson, Kephart, & Tesauro, 2001).\nTo succeed in such environments, agents need to make a clear distinction between their own decision-making models, the models others may be using to make decisions, and the extent to which agents deviate from these models when they actually make their decisions. This paper contributes a language, called Networks of Influence Diagrams (NID), that makes explicit the different mental models agents may use to make their decisions. NIDs provide for a clear and compact representation with which to reason about agents\u2019 beliefs and their decision-making processes. It allows multiple possible mental models of deliberation for agents, with uncertainty over which models agents are using. It is recursive, so that the mental model for an agent may itself contain models of the mental models of other agents, with associated uncertainty. In addition, NIDs allow agents\u2019 beliefs to form cyclic structures, of the form, \u201cI believe that you believe that I believe,...\u201d, and this cycle is explicitly represented in the language. NIDs can also describe agents\u2019 conflicting beliefs about each other. For example, one can describe a scenario in which two agents disagree about the beliefs or behavior of a third agent.\nNIDs are a graphical language whose building blocks are Multi Agent Influence Diagrams (MAID) (Koller & Milch, 2001). Each mental model in a NID is represented by a MAID, and the models are connected in a (possibly cyclic) graph. Any NID can be converted to an equivalent MAID that will represent the subjective beliefs of each agent in the game.\nWe provide an equilibrium definition for NIDs that combines the normative aspects of decision-making (what agents should do) with the descriptive aspects of decision-making (what agents are expected to do). The equilibrium makes an explicit distinction between two types of strategies: Optimal strategies represent agents\u2019 best course of action given their beliefs over others. Descriptive strategies represent how agents may deviate from their optimal strategy. In the classical approach to game theory, the normative aspect (what agents should do) and the descriptive aspect (what analysts or other agents expect them to do), have coincided. Identification of these two aspects makes sense when an agent can do no better than optimize its decisions relative to its own model of the world. However, in open environments, it is important to consider the possibility that an agent is deviating from its rational strategy with respect to its model.\nNIDs share a relationship with the Bayesian game formalism, commonly used to model uncertainty over agents\u2019 payoffs in economics (Harsanyi, 1967). In this formalism, there is a type for each possible payoff function an agent may be using. Although NIDs are representationally equivalent to Bayesian games, we argue that they are a more compact, succinct and natural representation. Any Bayesian game can be converted to a NID in linear time. Any NID can be converted to a Bayesian game, but the size of the Bayesian game may be exponential in the size of the NID.\nThis paper is a revised and expanded version of previous work (Gal & Pfeffer, 2003a, 2003b, 2004), and is organized as follows: Section 2 presents the syntax of the NID language, and shows how they build on MAIDs in order to express the structure that holds between agents\u2019 beliefs. Section 3 presents the semantics of NIDs in terms of MAIDs, and provides an equilibrium definition for NIDs. Section 4 provides a series of examples illustrating the representational benefits of NIDs. It shows how agents can construct belief hierarchies of each other\u2019s decision-making in order to represent agents\u2019 conflicting or incorrect belief structures, cyclic belief structures and opponent modeling. It also shows how certain forms of bounded rationality can be modeled by making a distinction between agents\u2019 models of deliberation and the way they behave in reality. Section 5 demonstrates how NIDs can model \u201cI believe that you believe\u201d type reasoning in practice. It describes a NID that was able to outperform the top programs that were submitted to a competition for automatic rockpaper-scissors players, whose strategy was not known in advance. Section 6 compares NIDs to several existing formalisms for describing uncertainty over decision-making processes. It provides a linear time algorithm for converting Bayesian games to NIDs. Finally, Section 7 concludes and presents future work."}, {"heading": "2. NID Syntax", "text": "The building blocks of NIDs are Bayesian networks (Pearl, 1988), and Multi Agent Influence Diagrams (Koller & Milch, 2001). A Bayesian network is a directed acyclic graph in which each node represents a random variable. An edge between two nodes X1 and X2 implies that X1 has a direct influence on the value of X2. Let Pa(Xi) represent the set of parent nodes for Xi in the network. Each node Xi contains a conditional probability distribution (CPD) over its domain for any value of its parents, denoted P (Xi | Pa(Xi)). The topology of the network describes the conditional independence relationships that hold in the domain \u2014 every node in the network is conditionally independent of its non-descendants given its parent nodes. A Bayesian network defines a complete joint probability distribution over its random variables that can be decomposed as the product of the conditional probabilities of each node given its parent nodes. Formally,\nP (X1, . . . , Xn) = n\u220f\ni=1\nP (Xi | Pa(Xi))\nWe illustrate Bayesian networks through the following example.\nExample 2.1. Consider two baseball team managers Alice and Bob whose teams are playing the late innings of a game. Alice, whose team is hitting, can attempt to advance a runner by instructing him to \u201csteal\u201d a base while the next pitch is being delivered. A successful\nsteal will result in a benefit to the hitting team and a loss to the pitching team, or it may result in the runner being \u201cthrown out\u201d, incurring a large cost to the hitting team and a benefit to the pitching team. Bob, whose team is pitching, can instruct his team to throw a \u201cpitch out\u201d, thereby increasing the probability that a stealing runner will be thrown out. However, throwing a pitch out incurs a cost to the pitching team. The decisions whether to steal and pitch out are taken simultaneously by both team managers. Suppose that the game is not tied, that is either Alice\u2019s or Bob\u2019s team is leading in score, and that the identity of the leading team is known to Alice and Bob when they make their decision.\nSuppose that Alice and Bob are using pre-specified strategies to make their decisions described as follows: when Alice is leading, she instructs a steal with probability 0.75, and Bob calls a pitch out with probability 0.90; when Alice is not leading, she instructs a steal with probability 0.65, and Bob calls a pitch out with probability 0.50. There are six random variables in this domain: Steal and PitchOut represent the decisions for Alice and Bob; ThrownOut represents whether the runner was thrown out; Leader represents the identity of the leading team; Alice and Bob represent the utility functions for Alice and Bob. Figure 1 shows a Bayesian network for this scenario.\nThe CPD associated with each node in the network represents a probability distribution over its domain for any value of its parents. The CPDs for nodes Leader, Steal, PitchOut, and ThrownOut in this Bayesian network are shown in Table 1. For example, the CPD for ThrownOut, shown in Table 1d, represents the conditional probability distribution P (ThrownOut | Steal, PitchOut). According to the CPD, when Alice instructs a runner to steal a base there is an 80% chance to get thrown out when Bob calls a pitch out and a 60% chance to get thrown out when Bob remains idle. The nodes Alice and Bob have deterministic CPDs, assigning a utility for each agent for any joint value of the parent nodes Leader, Steal, PitchOut and ThrownOut. The utility for Alice is shown in Table 2. The utility for Bob is symmetric and assigns the negative value assigned by Alice\u2019s utility for the same value of the parent nodes. For example, when Alice is leading, and she instructs a runner to steal a base, Bob instructs a pitch out, and the runner is thrown out, then Alice incurs a utility of \u221260, while Bob incurs a utility of 60.1\n1. Note that when Alice does not instruct to steal base, the runner cannot be thrown out, and the utility for both agents is not defined for this case."}, {"heading": "2.1 Multi-agent Influence Diagrams", "text": "While Bayesian networks can be used to specify that agents play specific strategies, they do not capture the fact that agents are free to choose their own strategies, and they cannot be analyzed to compute the optimal strategies for agents. Multi-agent Influence Diagrams (MAID), address these issues by extending Bayesian networks to strategic situations, where agents must choose the values of their decisions to maximize their own utilities, contingent on the fact that other agents are choosing the values of their decisions to maximize their own utilities. A MAID consists of a directed graph with three types of nodes: Chance nodes, drawn as ovals, represent choices of nature, as in Bayesian networks. Decision nodes, drawn as rectangles, represent choices made by agents. Utility nodes, drawn as diamonds, represent agents\u2019 utility functions. Each decision and utility node in a MAID is associated with a particular agent. There are two kinds of edges in a MAID: Edges leading to chance and utility nodes represent probabilistic dependence, in the same manner as edges in a Bayesian network. Edges leading into decision nodes represent information that is available to the agents at the time the decision is made. The domain of a decision node represents the choices that are available to the agent making the decision. The parents of decision nodes are called informational parents. There is a total ordering over each agent\u2019s decisions, such that earlier decisions and their informational parents are always informational parents of later decisions. This assumption is known as perfect recall or no forgetting. The CPD of a chance node specifies a probability distribution over its domain for each value of the parent nodes, as in Bayesian networks. The CPD of a utility node represents a deterministic function that assigns a probability of 1 to the utility incurred by the agent for any value of the parent nodes.\nIn a MAID, a strategy for decision node Di maps any value of the informational parents, denoted as pai, to a choice for Di. Let Ci be the domain of Di. The choice for the decision can be any value in Ci. A pure strategy for Di maps each value of the informational parents to an action ci \u2208 Ci. A mixed strategy for Di maps each value of the informational parents to a distribution over Ci. Agent \u03b1 is free to choose any mixed strategy for Di when it makes that decision. A strategy profile for a set of decisions in a MAID consists of strategies specifying a complete plan of action for all decisions in the set.\nThe MAID for Example 2.1 is shown in Figure 2. The decision nodes Steal and PitchOut represent Alice\u2019s and Bob\u2019s decisions, and the nodes Alice and Bob represent their utilities. The CPDs for the chance node Leader and ThrownOut are as described in Tables 1a and 1d.\nA MAID definition does not specify strategies for its decisions. These need to be computed or assigned by some process. Once a strategy exists for a decision, the relevant decision node in the MAID can be converted to a chance node that follows the strategy. This chance node will have the same domain and parent nodes as the domain and informational parents for the decision node in the MAID. The CPD for the chance node will equal the strategy for the decision. We then say that the chance node in the Bayesian network implements the strategy in the MAID. A Bayesian network represents a complete strategy profile for the MAID if each strategy for a decision in the MAID is implemented by a relevant chance node in the Bayesian network. We then say that the Bayesian network implements that strategy profile. Let \u03c3 represent the strategy profile that implements all\ndecisions in the MAID. The distribution defined by this Bayesian network is denoted by P \u03c3.\nAn agent\u2019s utility function is specified as the aggregate of its individual utilities; it is the sum of all of the utilities incurred by the agent in all of the utility nodes that are associated with the agent.\nDefinition 2.2. Let E be a set of observed nodes in the MAID representing evidence that is available to \u03b1 and let \u03c3 be a strategy profile for all decisions. Let U(\u03b1) be the set of all utility nodes belonging to \u03b1. The expected utility for \u03b1 given \u03c3 and E is defined as\nU\u03c3(\u03b1 | E) = \u2211\nU\u2208U(\u03b1)\nE\u03c3[U | E] = \u2211\nU\u2208U(\u03b1)\n\u2211\nu\u2208Dom(U)\nP \u03c3(u | E) \u00b7 u\nSolving a MAID requires computing an optimal strategy profile for all of the decisions, as specified by the Nash equilibrium for the MAID, defined as follows.\nDefinition 2.3. A strategy profile \u03c3 for all decisions in the MAID is a Nash equilibrium if each strategy component \u03c3i for decision Di belonging to agent \u03b1 in the MAID is one that maximizes the utility achieved by the agent, given that the strategy for other decisions is \u03c3\u2212i.\n\u03c3i \u2208 argmax \u03c4i\u2208\u2206Si\nU \u3008\u03c4i,\u03c3\u2212i\u3009(\u03b1) (1)\nThese equilibrium strategies specify what each agent should do at each decision given the available information at the decision. When the MAID contains several sequential decisions, the no-forgetting assumption implies that these decisions can be taken sequentially by the agent, and that all previous decisions are available as observations when the agent reasons about its future decisions.\nAny MAID has at least one Nash equilibrium. Exact and approximate algorithms have been proposed for solving MAIDs efficiently, in a way that utilizes the structure of the network (Koller & Milch, 2001; Vickrey & Koller, 2002; Koller, Meggido, & von Stengel,\n1996; Blum, Shelton, & Koller, 2006). Exact algorithms for solving MAIDs decompose the MAID graph into subsets of interrelated sub-games, and then proceed to find a set of equilibria in these sub-games that together constitute a global equilibrium for the entire game. In the case that there are multiple Nash equilibria, these algorithms will select one of them, arbitrarily. The MAID in Figure 2 has a single Nash equilibrium, which we can obtain by solving the MAID: When Alice is leading, she instructs her runner to steal a base with probability 0.2, and remain idle with probability 0.8, while Bob calls a pitch out with probability 0.3, and remains idle with probability 0.7. When Bob is leading, Alice instructs a steal with probability 0.8, and Bob calls a pitch out with probability 0.5.\nThe Bayesian network that implements the Nash equilibrium strategy profile for the MAID can be queried to predict the likelihood of interesting events. For example, we can query the network in Figure 2 and find that the probability that the stealer will get thrown out, given that agents\u2019 strategies follow the Nash equilibrium strategy profile, is 0.57.\nAny MAID can be converted to an extensive form game \u2014 a decision tree in which each vertex is associated with a particular agent or with nature. Splits in the tree represent an assignment of values to chance and decision nodes in the MAID; leaves of the tree represent the end of the decision-making process, and are labeled with the utilities incurred by the agents given the decisions and chance node values that are instantiated along the edges in the path leading to the leaf. Agents\u2019 imperfect information regarding the actions of others are represented by the set of vertices they cannot tell apart when they make a particular decision. This set is referred to as an information set. Let D be a decision in the MAID belonging to agent \u03b1. There is a one-to-one correspondence between values of the informational parents of D in the MAID and the information sets for \u03b1 at the vertices representing its move for decision D."}, {"heading": "2.2 Networks of Influence Diagrams", "text": "To motivate NIDs, consider the following extension to Example 2.1.\nExample 2.4. Suppose there are experts who will influence whether or not a team should steal or pitch out. There is social pressure on the managers to follow the advice of the experts, because if the managers\u2019 decision turns out to be wrong they can assign blame to the experts. The experts suggest that Alice should call a steal, and Bob should call a pitch out. This advice is common knowledge between the managers. Bob may be uncertain as to whether Alice will in fact follow the experts and steal, or whether she will ignore them and play a best-response with respect to her beliefs about Bob. To quantify, Bob believes that with probability 0.7, Alice will follow the experts, while with probability 0.3, Alice will play best-response. Alice\u2019s beliefs about Bob are symmetric to Bob\u2019s beliefs about Alice: With probability 0.7 Alice believes Bob will follow the experts and call a pitch out, and with probability 0.3 Alice believes that Bob will play the best-response strategy with respect to his beliefs about Alice. The probability distribution for other variables in this example remains as shown in Table 1.\nNIDs build on top of MAIDs to explicitly represent this structure. A Network of Influence Diagrams (NID) is a directed, possibly cyclic graph, in which each node is a MAID. To avoid confusion with the internal nodes of each MAID, we will call the nodes of a NID blocks. Let D be a decision belonging to agent \u03b1 in block K, and let \u03b2 be any agent. (In\nparticular, \u03b2 may be agent \u03b1 itself.) We introduce a new type of node, denoted Mod[\u03b2, D] with values that range over each block L in the NID. When Mod[\u03b2, D] takes value L, we say that agent \u03b2 in block K is modeling agent \u03b1 as using block L to make decision D. This means that \u03b2 believes that \u03b1 may be using the strategy computed in block L to make decision D. For the duration of this paper, we will refer to a node Mod[\u03b2, D] as a \u201cMod node\u201d when agent \u03b2 and decision D are clear from context.\nA Mod node is a chance node just like any other; it may influence, or be influenced by other nodes of K. It is required to be a parent of the decision D but it is not an informational parent of the decision. This is because an agent\u2019s strategy for D does not specify what to do for each value of the Mod node. Every decision D will have a Mod[\u03b2, D] node for each agent that makes a decision in block K, including agent \u03b1 itself that owns the decision. If the CPD of Mod[\u03b2, D] assigns positive probability to some block L, then we require that D exists in block L either as a decision node or as a chance node. If D is a chance node in L, this means that \u03b2 believes that agent \u03b1 is playing like an automaton in L, using a fixed, possibly mixed strategy for D; if D is a decision node in L, this means that \u03b2 believes \u03b1 is analyzing block L to determine the course of action for D. For presentation purposes, we also add an edge K \u2192 L to the NID, labeled {\u03b2, D}.\nWe can represent Example 2.4 in the NID described in Figure 3. There are three blocks in this NID. The Top-level block, shown in Figure 3a, corresponds to an interaction between Alice and Bob in which they are free to choose whether to steal base or call a pitch out, respectively. This block is identical to the MAID of Figure 2, except that each decision node includes the Mod nodes for all of the agents. Block S, presented in Figure 3b, corresponds to a situation where Alice follows the expert recommendation and instructs her player to steal.\nIn this block, the Steal decision is replaced with a chance node, which assigns probability 1 to true for any value of the informational parent Leader. Similarly, block P, presented in Figure 3c, corresponds to a situation where Bob instructs his team to pitch out. In this block, the PitchOut decision is replaced with a chance node, which assigns probability 1 to true for any value of the informational parent Leader.\nThe root of the NID is the Top-level block, which in this example corresponds to reality. The Mod nodes in the Top-level block capture agents\u2019 beliefs over their decision-making processes. The node Mod[Bob, Steal] represents Bob\u2019s belief about which block Alice is using to make her decision Steal. Its CPD assigns probability 0.3 to the Top-level block, and 0.7 to block S. Similarly, the node Mod[Alice, PitchOut] represents Alice\u2019s beliefs about which block Bob is using to make the decision PitchOut. Its CPD assigns probability 0.3 to the Top-level block, and 0.7 to block P. These are shown in Table 3.\nAn important aspect of NIDs is that they allow agents to express uncertainty about the block they themselves are using to make their own decisions. The node Mod[Alice, Steal] in the Top-level block represents Alice\u2019s beliefs about which block Alice herself is using to make her decision Steal. In our example, the CPD of this node assigns probability 1 to the Top-level. Similarly, the node Mod[Bob, PitchOut] represents Bob\u2019s beliefs about which block he is using to make his decision PitchOut, and assigns probability 1 to the Top-level block. Thus, in this example, both Bob and Alice are uncertain about which block the other agent is using to make a decision, but not about which block they themselves are using.\nHowever, we could also envision a situation in which an agent is unsure about its own decision-making. We say that if Mod[\u03b2, D] at block K equals some block L $= K, and \u03b2 owns decision D, then agent \u03b2 is modeling itself as using block L to make decision D. In Section 3.2 we will show how this allows to capture interesting forms of bounded rational behavior. We do impose the requirement that there exists no cycle in which each edge includes a label {\u03b1, D}. In other words, there is no cycle in which the same agent is modeling itself at each edge. Such a cycle is called a self-loop. This is because the MAID representation for a NID with a self-loop will include a cycle between the nodes representing the agent\u2019s beliefs about itself at each block of the NID.\nIn future examples, we will use the following convention: If there exists a Mod[\u03b1, D] node at block K (regardless of whether \u03b1 owns the decision) and the CPD of Mod[\u03b1, D] assigns probability 1 to block K, we will omit the node Mod[\u03b1, D] from the block description. In the Top-level block of Figure 3a, this means that both nodes Mod[Alice, Steal] and Mod[Bob, PitchOut], currently appearing as dashed ovals, will be omitted."}, {"heading": "3. NID Semantics", "text": "In this section we provide semantics for NIDs in terms of MAIDs. We first show how a NID can be converted to a MAID. We then define a NID equilibrium in terms of a Nash equilibrium of the constructed MAID."}, {"heading": "3.1 Conversion to MAIDs", "text": "The following process converts each block K in the NID to a MAID fragment OK , and then connects them to form a MAID representation of the NID. The key construct in this process is the use of a chance node DK\u03b1 in the MAID to represent the beliefs of agent \u03b1 regarding the action that is chosen for decision D at block K. The value of D\u03b1 depends on the block used by \u03b1 to model decision D, as determined by the value of the Mod[\u03b1, D] node.\n1. For each block K in the NID, we create a MAID OK . Any chance or utility node N in block K that is a descendant of a decision node in K is replicated in OK , once for each agent \u03b1, and denoted NK\u03b1 . If N is not a descendant of a decision node in K, it is copied to OK and denoted NK . In this case, we set NK\u03b1 = NK for any agent \u03b1.\n2. If P is a parent of N in K, then PK\u03b1 will be made a parent of NK\u03b1 in OK . The CPD of NK\u03b1 in OK will be equal to the CPD of N in K.\n3. For each decision D in K, we create a decision node BR[D]K in OK , representing the optimal action for \u03b1 for this decision. If N is a chance or decision node which is an informational parent of D in K, and D belongs to agent \u03b1, then NK\u03b1 will be made an informational parent of BR[D]K in OK .\n4. We create a chance node DK\u03b1 in OK for each agent \u03b1. We make Mod[\u03b1, D]K a parent of DK\u03b1 . If decision D belongs to agent \u03b1, then we make BR[D]K a parent of DK\u03b1 . If decision D belongs to agent \u03b2 $= \u03b1, then we make DK\u03b2 a parent of DK\u03b1 .\n5. We assemble all the MAID fragments OK into a single MAID O as follows: We add an edge DL\u03b1 \u2192 DK\u03b2 where L $= K if L is assigned positive probability by Mod[\u03b2, D]K , and \u03b1 owns decision D. Note that \u03b2 may be any agent, including \u03b1 itself.\n6. We set the CPD of DK\u03b1 to be a multiplexer. If \u03b1 owns D then the CPD of DK\u03b1 assigns probability 1 to BR[D]K when Mod[\u03b1, D]K equals K, and assigns probability 1 to DL\u03b1 when Mod[\u03b1, D]K equals L $= K. If \u03b2 $= \u03b1 owns D then the CPD of DK\u03b1 assigns probability 1 to DK\u03b2 when Mod[\u03b1, D]\nK equals K, and assigns probability 1 to DL\u03b2 when Mod[\u03b1, D]K equals L $= K.\nTo explain, Step 1 of this process creates a MAID fragment OK for each NID block. All nodes that are ancestors of decision nodes \u2014 representing events that occur prior to the decisions \u2014 are copied to OK . However, events that occur after decisions are taken may depend on the actions for those decisions. Every agent in the NID may have its own beliefs about these actions and the events that follow them, regardless of whether that agent owns the decision. Therefore, all of the descendant nodes of decisions are duplicated for each agent in OK . Step 2 ensures that if any two nodes are connected in the original block K, then\nthe nodes representing agents\u2019 beliefs in OK are also connected. Step 3 creates a decision node in OK for each decision node in block K belonging to agent \u03b1. The informational parents for the decision in OK are those nodes that represent the beliefs of \u03b1 about its informational parents in K. Step 4 creates a separate chance node in OK for each agent \u03b1 that represents its belief about each of the decisions in K. If \u03b1 owns the decision, this node depends on the decision node belonging to \u03b1. Otherwise, this node depends on the beliefs of \u03b1 regarding the action of agent \u03b2 that owns the decision. In the case that \u03b1 models \u03b2 as using a different block to make the decisions, Step 5 connects between the MAID fragments of each block. Step 6 determines the CPDs for the nodes representing agents\u2019 beliefs about each other\u2019s decisions. The CPD ensures that the block that is used to model a decision is determined by the value of the Mod node. The MAID that is obtained as a result of this process is a complete description of agents\u2019 beliefs over each other\u2019s decisions.\nWe demonstrate this process by converting the NID of Example 2.4 to its MAID representation, shown in Figure 4. First, MAID fragments for the three blocks Top-level, P, and S are created. The node Leader appearing in blocks Top-level, P, and S is not a descendant of any decision. Following Step 1, it is created once in each of the MAID fragments, giving the nodes LeaderTL, LeaderP and LeaderS . Similarly, the node Steal in block S and the node PitchOut in block P are created once in each MAID fragment, giving the nodes StealS and PitchOutP . Also in Step 1, the nodes Mod[Alice, Steal]TL, Mod[Bob, Steal]TL, Mod[Alice, PitchOut]TL and Mod[Bob, PitchOut]TL are added to the MAID fragment for the Top-level block.\nStep 3 adds the decision nodes BRTL[Steal] and BRTL[PitchOut] to the MAID fragment for the Top-level block. Step 4 adds the chance nodes PitchOutTLBob, PitchOut TL Alice, Steal TL Alice and StealTLBob to the MAID fragment for the Top-level block. These nodes represent agents\u2019 beliefs in this block about their own decisions or the decisions of other agents. For example, PitchOutTLBob represents Bob\u2019s beliefs about its decision whether to pitch out, while PitchOutTLAlice represents Alice\u2019s beliefs about Bob\u2019s beliefs about this decision. Also following Step 4, edges BRTL[PitchOut] \u2192 PitchOutTLBob and StealTLAlice \u2192 StealTLBob are added to the MAID fragment for the Top-level block. These represent Bob\u2019s beliefs over its own decision at the block. An edge StealTLAlice \u2192 StealTLBob is added to the MAID fragment to represent Bob\u2019s beliefs over Alice\u2019s decision at the Top-level block. There are also nodes representing Alice\u2019s beliefs about her and Bob\u2019s decisions in this block.\nIn Step 5, edges StealS \u2192 StealTLBob and PitchOutP \u2192 PitchoutTLAlice are added to the MAID fragment for the Top-level block. This is to allow Bob to reason about Alice\u2019s decision in block S, and for Alice to reason about Bob\u2019s decision in block P. This action unifies the MAID fragments into a single MAID. The parents of StealTLBob are Mod[Bob, Steal]TL, Steal S and StealTLAlice. Its CPD is a multiplexer node that determines Bob\u2019s prediction about Alice\u2019s action: If Mod[Bob, Steal]TL equals S, then Bob believes Alice to be using block S, in which her action is to follow the experts and play strategy StealS . If Mod[Bob, Steal]TL equals the Top-level block, then Bob believes Alice to be using the Top-level block, in which Alice\u2019s action is to respond to her beliefs about Bob. The situation is similar for Alice\u2019s decision StealTLAlice and the node Mod[Alice, Steal]TL with the following exception: When Mod[Alice, Steal]TL equals the Top-level block, then Alice\u2019s action follows her decision node BRTL[Steal].\nIn the Appendix, we prove the following theorem.\nTheorem 3.1. Converting a NID into a MAID will not introduce a cycle in the resulting MAID.\nAs this conversion process implies, NIDs and MAIDs are equivalent in their expressive power. However, NIDs provide several advantages over MAIDs. A NID block structure makes explicit agents\u2019 different beliefs about decisions, chance variables and utilities in the world. It is a mental model of the way agents reason about decisions in the block. MAIDs do not distinguish between the real world and agents\u2019 mental models of the world or of each other, whereas NIDs have a separate block for each mental model. Further, in the MAID, nodes simply represent chance, decision or utilities, and are not inherently interpreted in terms of beliefs. A DK\u03b1 node in a MAID representation for a NID does not inherently represent agent \u03b1\u2019s beliefs about how decision D is made in mental model K, and the ModK for agent \u03b1 does not inherently represent which mental model is used to make a decision. Indeed, there are no mental models defined in a MAID. In addition, there is no relationship in a MAID between descendants of decisions NK\u03b1 and NK\u03b2 , so there is no sense in which they represent the possibly different beliefs of agents \u03b1 and \u03b2 about N .\nTogether with the NID construction process described above, a NID is a blueprint for constructing a MAID that describes agents\u2019 mental models. Without the NID, this process becomes inherently difficult. Furthermore, the constructed MAID may be large and unwieldy compared to a NID block. Even for the simple NID of Example 2.4, the MAID of Figure 4 is complicated and hard to understand."}, {"heading": "3.2 Equilibrium Conditions", "text": "In Section 2.1, we defined pure and mixed strategies for decisions in MAIDs. In NIDs, we associate the strategies for decisions with the blocks in which they appear. A pure strategy for a decision D in a NID block K is a mapping from the informational parents of D to an action in the domain of D. Similarly, a mixed strategy for D is a mapping from the informational parents of D to a distribution over the domain of D. A strategy profile for a NID is a set of strategies for all decisions at all blocks in the NID.\nTraditionally, an equilibrium for a game is defined in terms of best response strategies. A Nash equilibrium is a strategy profile in which each agent is doing the best it possibly can, given the strategies of the other agents. Classical game theory predicts that all agents will play a best response. NIDs, on the other hand, allow us to describe situations in which an agent deviates from its best response by playing according to some other decision-making process. We would therefore like an equilibrium to specify not only what the agents should do, but also to predict what they actually do, which may be different.\nA NID equilibrium includes two types of strategies. The first, called a best response strategy, describes what the agents should do, given their beliefs about the decision-making processes of other agents. The second, called an actually played strategy, describes what agents will actually do according to the model described by the NID. These two strategies are mutually dependent. The best response strategy for a decision in a block takes into account the agent\u2019s beliefs about the actually played strategies of all the other decisions. The actually played strategy for a decision in a block is a mixture of the best response for the decision in the block, and the actually played strategies for the decision in other blocks.\nDefinition 3.2. Let N be a NID and let M be the MAID representation for N. Let \u03c3 be an equilibrium for M. Let D be a node belonging to agent \u03b1 in block K of N. Let the parents of D be Pa. By the construction of the MAID representation detailed in Section 3.1, the parents of BR[D]K in M are PaK\u03b1 and the domains of Pa and Pa K \u03b1 are the same. Let \u03c3BR[D]K (pa) denote the mixed strategy assigned by \u03c3 for BR[D] K when PaK\u03b1 equals pa. The best response strategy for D in K, denoted \u03b8KD (pa), defines a function from values of Pa to distributions over D that satisfy\n\u03b8KD (pa) \u2261 \u03c3BR[D]K (pa)\nIn other words, the best response strategy is the same as the MAID equilibrium when the corresponding parents take on the same values.\nDefinition 3.3. Let P \u03c3 denote the distribution that is defined by the Bayesian network that implements \u03c3. The actually played strategy for decision D in K that is owned by agent \u03b1, denoted \u03c6KD(pa), specifies a function from values of Pa to distributions over D that satisfy\n\u03c6KD(pa) \u2261 P \u03c3(DK\u03b1 | pa)\nNote here, that DK\u03b1 is conditioned on the informational parents of decision D rather than its own parents. This node represents the beliefs of \u03b1 about decision K. Therefore, the actually played strategy for D in K represents \u03b1\u2019s belief about D in K, given the informational parents of D.\nDefinition 3.4. Let \u03c3 be a MAID equilibrium. The NID equilibrium corresponding to \u03c3 consists of two strategy profiles \u03b8 and \u03c6, such that for every decision D in every block K, \u03b8KD is the best response strategy for D in K, and \u03c6 K D is the actually played strategy for D in K.\nFor example, consider the constructed MAID for our baseball example in Figure 4. The best response strategies in the NID equilibrium specify strategies for the nodes Steal and PitchOut in the Top-level block that belong to Alice and Bob respectively. For an equilibrium \u03c3 of the MAID, the best response strategy for Steal in the Top-level block is the strategy specified by \u03c3 for BRTL[Steal]. Similarily, the best response strategy for Pitchout in the Top-level block is the strategy specified by \u03c3 for BRTL[Pitchout]. The actually played strategy for Steal in the Top-level is equal to the conditional probability distribution over StealTLAlice given the informational parent Leader\nTL. Similarly, the actually played strategy for Pitchout is equal to the conditional probability distribution over PitchoutTLBob given the informational parent LeaderTL. Solving this MAID yields the following unique equilibrium: In the NID Top-level block, the CPD for nodes Mod[Alice, Steal] and Mod[Bob, Pitchout] assigns probability 1 to the Top-level block, so the actually played and best response strategies for Bob and Alice are equal and specified as follows: If Alice is leading, then Alice steals base with probability 0.56 and Bob pitches out with probability 0.47. If Bob is leading, then Alice never steals base and Bob never pitches out. It turns out that because the experts may instruct Bob to call a pitch out, Alice is considerably less likely to steal base, as compared to her equilibrium strategy for the MAID of Example 2.1, where none of the managers considered the possibility that the other was being advised by experts. The case is similar for Bob.\nA natural consequence of this definition is that the problem of computing NID equilibria reduces to that of computing MAID equilibria. Solving the NID requires to convert it to its MAID representation and solving the MAID using exact or approximate solution algorithms. The size of the MAID is bounded by the size of a block times the number of blocks times the number of agents. The structure of the NID can then be exploited by a MAID solution algorithm (Koller & Milch, 2001; Vickrey & Koller, 2002; Koller et al., 1996; Blum et al., 2006)."}, {"heading": "4. Examples", "text": "In this section, we provide a series of examples demonstrating the benefits of NIDs for describing and representing uncertainty over decision-making processes in a wide variety of domains."}, {"heading": "4.1 Irrational Agents", "text": "Since the challenge to the notion of perfect rationality as the foundation of economic systems presented by Simon (1955), the theory of bounded rationality has grown in different\ndirections. From an economic point of view, bounded rationality dictates a complete deviation from the utility maximizing paradigm, in which concepts such as \u201coptimization\u201d and \u201cobjective functions\u201d are replaced with \u201csatisficing\u201d and \u201cheuristics\u201d (Gigerenzer & Selten, 2001). These concepts have recently been formalized by Rubinstein (1998). From a traditional AI perspective, an agent exhibits bounded rationality if its program is a solution to the constrained optimization problem brought about by limitations of architecture or computational resources (Russell & Wefald, 1991). NIDs serve to complement these two prevailing perspectives by allowing to control the extent to which agents are behaving irrationally with respect to their model.\nIrrationality is captured in our framework by the distinction between best response and actually played strategies. Rational agents always play a best response with respect to their models. For rational agents, there is no distinction between the normative behavior prescribed for each agent in each NID block, and the descriptive prediction of how the agent actually would play when using that block. In this case, the best response and actually played strategies of the agents are equal. However, in open systems, or when people are involved, we may need to model agents whose behavior differs from their best response strategy. In other words, their best response strategies and actually played strategies are different. We can capture agent \u03b1 behaving (partially) irrationally about its decision D\u03b1 in block K by setting the CPD of Mod[\u03b1, D\u03b1] to assign positive probability to some block L $= K.\nThere is a natural way to express this distinction in NIDs through the use of the Mod node. If D\u03b1 is a decision associated with agent \u03b1, we can use Mod[\u03b1, D\u03b1] to describe which block \u03b1 actually uses to make the decision D\u03b1. In block K, if Mod[\u03b1, D\u03b1] is equal to K with probability 1, then it means that within K, \u03b1 is making the decision according to its beliefs in block K, meaning that \u03b1 will be rational; it will play a best response to the strategies of other agents, given its beliefs. If, however, Mod[\u03b1, D\u03b1] assigns positive probability to some block L other than K, it means that there is some probability that \u03b1 will not play a best response to its beliefs in K, but rather play a strategy according to some other block L. In this case, we say \u03b1 self-models at block K. The introduction of actually played strategies into the equilibrium definition represents another advantage of NIDs over MAIDs, in that they explicitly represent strategies for agents that may deviate from their optimal strategies.\nIn some cases, making a decision may lead an agent to behave irrationally by viewing the future in a considerably more positive light than is objectively likely. For example, a person undergoing treatment for a disease may believe that the treatment stands a better chance of success than scientifically plausible. In the psychological literature, this effect is referred to as motivational bias or positive illusion (Bazerman, 2001). As the following example shows, NIDs can represent agents\u2019 motivational biases in a compelling way, by making Mod nodes depend on the outcome of decision nodes.\nExample 4.1. Consider the case of a toothpaste company whose executives are faced with two sequential decisions: whether to place an advertisement in a magazine for their leading brand, and whether to increase production of the brand. Based on past analysis, the executives know that without advertising, the probability of high sales for the brand in the next quarter will be 0.5. Placing the advertisement costs money, but the probability of high sales will rise to 0.7. Increasing production of the brand will contribute to profit\nif sales are high, but will hurt profit if sales are low due to the high cost of storage space. Suppose now that the company executives wish to consider the possibility of motivational bias, in which placing the advertisement will inflate their beliefs about sales to be high in the next quarter to probability 0.9. This may lead the company to increase the production of the brand when it is not warranted by the market and consequently, suffer losses. The company executives wish to compute their best possible strategy for their two decisions given the fact that they attribute a motivational bias.\nA NID describing this situation is shown in Figure 5c. The Top-level block in Figure 5a shows the situation from the point of view of reality. It includes two decisions, whether to advertise (Advertise) and whether to increase the supply of the brand (Increase). The node Sales represents the amount of sales for the brand after the decision of whether to advertise, and the node Profit represents the profit for the company, which depends on the nodes Advertise, Increase and Sales. The CPD of Sales in the Top-level block assigns probability 0.7 to high if Advertise is true and 0.5 to high if Advertise is false, as described in Table 4a. The utility values for node Profit are shown in Table 4.1. For example, when the company advertises the toothpaste, increases its supply, and sales are high, it receives a reward of 70; when the company advertises the toothpaste, does not increase its supply, and sales are low, it receives a reward of \u221240. Block Bias, described in Figure 5b, represents the company\u2019s biased model. Here, the decision to advertise is replaced by an automaton chance node that assigns probability 1 to Advertise = true. The CPD of Sales in block Bias assigns probability 0.9 to high if Advertise is true and 0.5 to high if Advertise is false, as described in Table 4b. In the Top-level block, we have the following:\n1. The node Mod[Company, Advertise] assigns probability 1 to the Top-level block.\n2. The decision node Advertise is a parent of the node Mod[Company, Increase].\n3. The node Mod[Company, Increase] assigns probability 1 to block Bias when Advertise is true, and assigns probability 0 to block Bias when Advertise is false.\nIntuitively, Step 1 captures the company\u2019s beliefs that it is not biased before it makes the decision to advertise. Step 2 allows the company\u2019s uncertainty about whether it is biased to depend on the decision to advertise. Note that this example shows when it is necessary for a decision node to depend on an agent\u2019s beliefs about a past decision. Step 3 captures the company\u2019s beliefs that it may use block Bias to make its decision whether to increase supply, in which it is over confident about high sales.\nSolving this NID results in the following unique equilibrium: In block Bias, the company\u2019s actually played and best response strategy is to increase supply, because this is its optimal action when it advertises and sales are high. In block Top-level, we have the following: If the company chooses not to advertise, it will behave rationally, and its best response and actually played strategy will be not to increase supply; if the company chooses to advertise, its actually played strategy will be to use block Bias in which it increases supply, and its best response strategy will be not to increase supply. Now, the expected utility for the company in the Top-level block is higher when it chooses not to advertise. Therefore, its best response strategies for both decisions are not to advertise nor to increase supply. Interestingly, if the company was never biased, it can be shown using backwards induction\nthat its optimal action for the first decision is to advertise. Thus, by reasoning about its own possible irrational behavior for the second decision, the company revised its strategy for the first decision.\nExample 4.2. Consider the following extension to Example 2.4. Suppose that there are now two successive pitches, and on each pitch the managers have an option to steal or pitch out. If Bob pitches out on the first pitch, his utility for pitching out on the second pitch (regardless of Alice\u2019s action) decreases by 20 units because he has forfeited two pitches. Bob believes that with probability 0.3, he will succumb to social pressure during the second pitch and call a pitch out. Bob would like to reason about this possibility when making the decision for the first pitch.\nIn this example, each manager is faced with a sequential decision problem: whether to steal or pitch out in the first and second pitch. The strategy for the second pitch is relevant to the strategy for the first pitch for each agent. Now, each of the managers, if they were rational, could use backward induction to compute optimal strategies for the first pitch, by working backwards from the second pitch. However, this is only a valid procedure if the managers behave rationally on the second pitch. In the example above, Bob knows that he will be under strong pressure to pitch out on the second pitch and he wishes to take this possibility into account, while making his decision for the first pitch.\nWe can model this situation in a NID as follows. The Top-level block of the NID is shown in Figure 6a. Here, the decision nodes Steal1 and PitchOut1 represent the decisions for Alice and Bob in the first pitch, and the nodes Steal2 and Pitchout2 represent the decisions for Alice and Bob in the second pitch. The nodes Leader, Steal1, PitchOut1 and ThrownOut1 are all informational parents of the decision nodes Steal2 and PitchOut2. For expository convenience, we have not included the edges leading from node Leader to the utility nodes in the block. Block L, shown in Figure 6b, describes a model for the second pitch in which Bob is succumbing to social pressure and pitches out, regardless of who is leading. This is represented by having the block include a chance node PitchOut2 which equals true with probability 1 for each value of Leader. The node Mod[Bob, PitchOut2] will assign probability 0.3 to block L, and 0.7 probability to the Top-level block, as shown in Table 4.1. The node Mod[Bob, PitchOut2] is not displayed in the Top-level block. By our convention, this implies that its CPD assigns probability 1 to the Top-level block, in which Bob is reasoning about the possibility of behaving irrationally with respect to the second pitch. In this way, we have captured the fact that Bob may behave irrationally with respect to the second pitch, and that he is reasoning about this possibility when making the decision for the first pitch.\nThere is a unique equilibrium for this NID. Both agents behave rationally for their first decision so their actually played and best response strategies are equal, and specified as follows: Alice steals a base with probability 0.49 if she is leading, and never steals a base if Bob is leading. Bob pitches out with probability 0.38 if Alice is leading and pitches out with probability 0.51 if Bob is leading. In the second pitch, Alice behaves rationally, and her best response and actually played strategy are as follows: steal base with probability 0.42 if Alice is leading and never steal base if Bob is leading. Bob may behave irrationally in the second pitch: His best response strategy is to pitch out with probability 0.2 if Alice is leading, and pitch out with probability 0.52 if Bob is leading; his actually played strategy is to pitch out with probability 0.58 if Alice is leading, and with probability 0.71 if Bob is leading. Note that because Bob is reasoning about his possible irrational behavior in the second pitch, he is less likely to pitch out in the first pitch as compared to the case in which Bob is completely rational (Example 2.4)."}, {"heading": "4.2 Conflicting Beliefs", "text": "In traditional game theory, agents\u2019 beliefs are assumed to be consistent with a common prior distribution, meaning that the beliefs of agents about each other\u2019s knowledge is expressed as a posterior probability distribution resulting from conditioning a common prior on each agent\u2019s information state. One consequence of this assumption is that agents\u2019 beliefs can differ only if they observe different information (Aumann & Brandenburger, 1995). This result led to theoretic work that attempted to relax the common prior assumption. Myerson (1991) showed that a game with inconsistent belief structure that is finite can be converted to a new game with consistent belief structures by constructing utility functions that are equivalent to the original game in a way that they both assign the same expected utility to the agents. However, this new game will include beliefs and utility functions that are fundamentally different to the original game exhibiting the inconsistent belief structure. For a summary of the economic and philosophical ramifications of relaxing the common prior assumption, see the work of Morris (1995) and Bonanno and Nehring (1999).\nOnce we have a language that allows us to talk about different mental models that agents have about the world, and different beliefs that they have about each other and about the structure of the game, it is natural to relax the common prior assumption within NIDs while preserving the original structure of the game.\nExample 4.3. Consider the following extension to the baseball scenario of Example 2.1. The probability that the runner is thrown out depends not only on the decisions of both managers, but also on the speed of the runner. Suppose a fast runner will be thrown out with 0.4 probability when Bob calls a pitch out, and with 0.2 probability when Bob does not call a pitch out. A slow runner will be thrown out with 0.8 probability when Bob calls a pitch out, and with 0.6 probability when Bob does not call a pitch out.\nNow, Bob believes the runner to be slow, but is unsure about Alice\u2019s beliefs regarding the speed of the runner. With probability 0.8, Bob believes that Alice thinks that the stealer is fast, and with probability 0.2 Bob believes that Alice thinks that the stealer is slow. Assume that the distributions for other variables in this example are as described in Table 1.\nIn this example, Bob is uncertain whether Alice\u2019s beliefs about the speed of the runner conflict with his own. NIDs allow to express this in a natural fashion by having two blocks that describe the same decision-making process, but differ in the CPD that they assign to the speed of the runner. Through the use of the Mod node, NIDs can specify agents\u2019 conflicting beliefs about which of the two blocks is used by Alice to make her decision, according to Bob\u2019s beliefs. The NID and blocks for this scenario are presented in Figure 7.\nIn the Top-level block, shown in Figure 7a, Bob and Alice decide whether to pitch out or to steal base, respectively. This block is identical in structure to the Top-level block of the previous example, but it has an additional node Speed that is a parent of node ThrownOut, representing the fact that the speed of the runner affects the probability that the runner is thrown out.\nThe Top-level corresponds to Bob\u2019s model, in which the runner is slow. The CPD of the node Speed assigns probability 1 to slow in this block, as shown in Table 7a. Block L, shown in Figure 7b, represents an identical decision-making process as in the Top-level block, except that the CPD of Speed is different: it assigns probability 1 to fast, as shown in Table 7b. The complete NID is shown in Figure 7c. Bob\u2019s uncertainty in the Toplevel block over Alice\u2019s decision-making process is represented by the node Mod[Bob, Steal], whose CPD is shown in Table 7c. With probability 0.8, Alice is assumed to be using block L, in which the speed of the runner is fast. With probability 0.2, Alice is assumed to be using the Top-level block, in which the speed of the runner is slow. Note that in the\nTop-level block, the nodes Mod[Alice, Steal], Mod[Alice, PitchOut] and Mod[Bob, PitchOut] are not displayed. By the convention introduced earlier, all these nodes assign probability 1 to the Top-level block and have been omitted from the Top-level block of Figure 7a. Interestingly, this implies that Alice knows the runner to be slow, even though Bob believes that Alice believes the runner is fast. When solving this NID, we get a unique equilibrium. Both agents are rational, so their best response and actually played strategies are equal, and specified as follows: In block L, the runner is fast, so Alice always steals base, and Bob always calls a pitch out. In the Top-level block, Bob believes that Alice uses block L with high probability, in which she seals a base. In the Top-level block the speed of the runner is slow and will likely be thrown out. Therefore, Bob does not pitch out in order to maximize its utility given its beliefs about Alice. In turn, Alice does not steal base at the Top-level block because the speed of the runner is slow at this block."}, {"heading": "4.3 Collusion and Alliances", "text": "In a situation where an agent is modeling multiple agents, it may be important to know whether those agents are working together in some fashion. In such situations, the models of how the other agents make their decisions may be correlated, due to possible collusion.\nExample 4.4. A voting game involves 3 agents Alice, Bob, and Carol, who are voting one of them to be chairperson of a committee. Alice is the incumbent, and will be chairperson if the vote ends in a draw. Each agent would like itself to be chairperson, and receives utility 2 in that case. Alice also receives a utility of 1 if she votes for the winner but loses the election, because she wants to look good. Bob and Carol, meanwhile, dislike Alice and receive utility -1 if Alice wins.\nIt is in the best interests of agents Bob and Carol to coordinate, and both vote for the same person. If Bob and Carol do indeed coordinate, it is in Alice\u2019s best interest to vote for the person they vote for. However, if Bob and Carol mis-coordinate, Alice should vote for herself to remain the chairperson. In taking an opponent modeling approach, Alice would like to have a model of how Bob and Carol are likely to vote. Alice believes that with probability 0.2, Bob and Carol do not collude; with probability 0.3, Bob and Carol collude to vote for Bob; with probability 0.4, Bob and Carol collude to vote for Carol. Also, Alice believes that when they collude, both agents might renege and vote for themselves with probability 0.1.\nThis example can easily be captured in a NID. The Top-level block is shown in Figure 8. There is a node Collude, which will have three possible values: none indicating no collusion;\nBob and Carol indicating collusion to vote for Bob or Carol respectively. The decision nodes A, B, C represent the decisions for Alice, Bob and Carol, respectively. The CPD for Collude is presented in Table 8a. The nodes Mod[Alice, B] and Mod[Alice, C], whose CPD is shown in Table 8b and 8c respectively, depend on Collude. If Collude is none, Mod[Alice, B] will assign probability 1 to the Top-level block. If Collude is Bob, Mod[Alice, B] will equal a block B describing an automaton in which Bob and Carol both vote for Bob. If Collude is Carol, Mod[Alice, B] will equal a block C, in which Bob and Carol vote for Carol with probability 0.9, and block B with probability 0.1. This accounts for the possibility that when Bob and Carol have agreed to vote for Carol, Bob might renege. The CPD for Mod[Alice, B] is similar, and is described in Table 8b. The CPD for Mod[Alice, C] is symmetric, and is described in Table 8c.\nIn the unique NID equilibrium for this example, all agents are rational so their actually played and best response strategies are equal. In the equilibrium, Alice always votes for Carol because she believes that Bob and Carol are likely to collude and vote for Carol.\nIn turn, Carol votes for herslef or for Bob with probability 0.5, and Bob always votes for himself. By reneging, Bob gives himself a chance to win the vote, in the case that Carol votes for him.\nMoving beyond this example, one of the most important issues in multi-player games is alliances. When players form an alliance, they will act for the benefit of the alliance rather than purely for their own self-interest. Thus an agent\u2019s beliefs about the alliance structure affects its models of how other agents make their decisions. When an agent has to make a decision in such a situation, it is important to be able to model its uncertainty about the alliance structure."}, {"heading": "4.4 Cyclic Belief Structures", "text": "Cyclic belief structures are important in game theory, where they are used to model agents who are symmetrically modeling each other. They are used to describe an infinite regress of \u201cI think that you think that I think...\u201d reasoning. Furthermore, cyclic belief structures can be expressed in economic formalisms, like Bayesian games, so it is vital to allow them in NIDs in order for NIDs to encompass Bayesian games. Cyclic belief structures can naturally be captured in NIDs by including a cycle in the NID graph.\nExample 4.5. Recall Example 4.3, in which Alice and Bob had conflicting beliefs about the speed of the runner. Suppose that Bob believes that the runner is slow, and that with probability 0.8, Alice believes that the runner is fast, and is modeling Bob as reasoning about Alice\u2019s beliefs, and so on...\nWe model this scenario using the cyclic NID described in Figure 9c. In the Top-level block, shown in Figure 9b, Bob believes the runner to be slow and is modeling Alice as using block L to make her decision. In block L, Alice believes the runner to be fast, and is modeling Bob as using the Top-level block to make his decision. Bob\u2019s beliefs about Alice in the Top-level block are represented by the CPD of node Mod[Bob, Steal], shown in Table 9c, which assigns probability 1 to block L.\nIn block L, the CPD of Speed, shown in Table 9b assigns probability 1 to fast. Alice\u2019s beliefs about Bob in block L are represented by the CPD of node Mod[Alice, PitchOut], shown in Table 9d, which assigns probability 1 to block L. In the Top-level block, the CPD of Speed assigns probability 1 to slow, shown in Table 4.4a. The NID equilibrium for this scenario is as follows. In both blocks L and Top-level, Alice does not steal base, and Bob does not pitch out, regardless of who is leading."}, {"heading": "5. Application: Opponent Modeling", "text": "In some cases, agents use rules, heuristics, patterns or tendencies when making decisions. One of the main approaches to game playing with imperfect information is opponent modeling, in which agents try to learn the patterns exhibited by other players and react to their model of others. NIDs provide a solid, coherent foundation for opponent modeling.\nExample 5.1. In the game of RoShamBo (commonly referred to as Rock-Paper-Scissors), players simultaneously choose one of rock, paper, or scissors. If they choose the same item, the result is a tie; otherwise rock crushes scissors, paper covers rock, or scissors cut paper, as shown in Table 10.\nThe game has a single Nash equilibrium in which both players play a mixed strategy over {rock, paper, scissors} with probability {13 , 1 3 , 1 3}. If both players do not deviate from their equilibrium strategy, they are guaranteed an expected payoff of zero. In fact, it is easy to verify that a player who always plays his equilibrium strategy is guaranteed to get an expected zero payoff regardless of the strategy of his opponent. In other words, sticking to the equilibrium strategy guarantees not to lose a match in expectation, but it also guarantees not to win it!\nHowever, a player can try and win the game if the opponents are playing suboptimally. Any suboptimal strategy can be beaten, by predicting the next move of the opponent and then employing a counter-strategy. The key to predicting the next move is to model the strategy of the opponent, by identifying regularities in its past moves.\nNow consider a situation in which two players play repeatedly against each other. If a player is able to pick up the tendencies of a suboptimal opponent, it might be able to defeat it, assuming the opponent continues to play suboptimally. In a recent competition (Billings, 2000), programs competed against each other in matches consisting of 1000 games of RoShamBo. As one might expect, Nash equilibrium players came in the middle of the pack because they broke even against every opponent. It turned out that the task of modeling the opponent\u2019s strategy can be surprisingly complex, despite the simple structure of the game itself. This is because sophisticated players will attempt to counter-model their opponents, and will hide their own strategy to avoid detection. The winning program, called Iocaine Powder (Egnor, 2000), did a beautiful job of modeling its opponents on multiple levels. Iocaine Powder considered that its opponent might play randomly, according to some heuristic, or it might try to learn a pattern used by Iocaine Powder, or it might play a strategy designed to counter Iocaine Powder learning its pattern, or several other possibilities."}, {"heading": "5.1 A NID for Modeling Belief Hierarchies", "text": "Inspired by \u201cIocaine Powder\u201d, we constructed a NID for a player that is playing a match of RoShamBo and is trying to model his opponent. Suppose that Bob wishes to model Alice\u2019s play using a NID. The block Top-level of the NID, shown in Figure 10a, is simply a MAID depicting a RoShamBo round between Bob and Alice. Both players have access to a predictor P, an algorithm that is able to predict the next move in a sequence as a probability distribution over the possible moves. The only information available to the predictor is the history of past moves for Alice and Bob.\nAlice may be ignoring P, and playing the Nash Equilibrium strategy. Bob has several alternative models of Alice\u2019s decision. According to block Automaton, shown in Figure 10c, Alice always follows the signal P. In block B1, shown in Figure 10b, Bob is modeling Alice as using block Automaton to make her decision. This is achieved by setting the CPD of Mod[Bob, Alice] in block B1 to assign probability 1 to Automaton. We can analyze the NID rooted at block B1 to determine Bob\u2019s best response to Alice. For example, if Bob thinks, based on the history, that P is most likely to tell Alice to play rock, then Bob would play paper. Let us denote this strategy as BR(P).\nHowever, Alice can also model Bob by assigning probability 1 to Mod[Alice, Bob] in block A1. In this way, Alice is reasoning about Bob modeling Alice as following the predictor P.\nWhen we analyze the NID originating in block A1, shown in Figure 10a, we will determine Alice\u2019s best-response to Bob\u2019s model of her as well as Bob\u2019s best-response to his model of Alice. Since Alice believes that Bob plays BR(P) as a result of Bob\u2019s belief that Alice plays according to P, she will therefore play a best response to BR(P), thereby double-guessing Bob. Alice\u2019s strategy in block A1 is denoted as BR(BR(P)). Following our example, in block A1 Alice does not play rock at all, but scissors, in order to beat Bob\u2019s play of paper. Similarly, in block B2, Bob models Alice as using block A1 to make her decisions, and in block A2, Alice models Bob as using block B2 to make his decision. Therefore, solving the NID originating in block B2 results in a BR(BR(BR(P))) strategy for Bob. This would prompt Bob to play rock in B2 in our example, in order to beat scissors. Lastly, solving the NID originating in block A2 results in a BR(BR(BR(BR(P)))) strategy for Alice. This would prompt Alice to play paper in block A2, in order to beat rock. Thus, we have shown that for every instance of the predictor P, Alice might play one of the three possible strategies. Any pure strategy can only choose between rock, paper, or scissors for any given P, so this reasoning process terminates.\nThe entire NID is shown in Figure 10d. In block Top-level, Bob models Alice as using one of several possible child blocks: block Automaton, in which Alice follows her predictor; block A1, in which Alice is second-guessing her predictor; or block A2, in which Alice is triple-guessing her predictor. Bob\u2019s uncertainty over Alice\u2019s decision-making processes is captured in the Mod[Bob, Alice] node in block Top-level. Analyzing the Top-level block of this NID will extract Bob\u2019s best response strategy given his beliefs about Alice\u2019s decisionmaking processes.\nTo use this NID in practice, it is necessary to compute the MAID equilibrium and extract Bob\u2019s best-response strategy at the Top-level block. To this end, we need to estimate the values of the NID parameters, represented by the unknown CPDs at each of its blocks, and solve the NID. These parameters include Mod[Bob, Alice], representing Bob\u2019s beliefs in the Top-level block regarding which block Alice is using; and node P, representing the distributions governing the signals for Alice and Bob, respectively.2 To this end, we use an on-line version of the the EM algorithm that was tailored for NIDs. We begin with random parameter assignments to the unknown CPDs. We then revise the estimate over the parameters of the NID given the observations at each round. Then Bob plays the bestresponse strategy of the MAID representation for the NID given the current parameter setting. Interleaving learning and using the NID to make a decision helps Bob to adapt to Alice\u2019s possibly changing strategy."}, {"heading": "5.2 Empirical Evaluation", "text": "We evaluated the NID agent against the ten top contestants from the first automatic RoShamBo competition. All of these agents used an opponent modeling approach, that is, they learned some signal of their opponent\u2019s play based on the history of prior rounds. Contestants can be roughly classified according to three dimensions: the type of signal used (probabilistic vs. deterministic); the type of reasoning used (pattern vs. meta-reasoners); and, their degree of exploration versus exploitation of their model. Probabilistic agents\n2. Technically, the CPDs for the nodes representing prior history are also missing. However, they are observed at each decision-making point in the interaction and their CPDs do not affect players\u2019 utilities.\nestimated a distribution over the strategies of their opponents while deterministic agents predicted their opponents\u2019 next move with certainty. Pattern reasoners directly modeled their opponents as playing according to some rule or distribution, and did not reason about the possibility that their opponents were modeling themselves. In contrast, meta-reasoners attempted to double- or triple-guess their opponents\u2019 play. Exploitative agents played a best response to their model of their opponents, while explorative agents deviated, under certain conditions, from their best response strategy to try and learn different behavioral patterns of their opponents. Iocaine Powder used the strategy of reverting to the Nash equilibrium when it was losing. Because this made it impossible to evaluate whether our NID model could learn Iocaine powder\u2019s reasoning process, we turned off this strategy. Also, we limited all contestants\u2019 strategies to depend on the last 100 rounds of play, in order to allow a fair comparison with the NID agent that only used four rounds of play. We did not limit them to four rounds because they were not originally designed to use such a short history. Our purpose was to show that explicitly reasoning and learning about mental models can make a difference, and not to optimize learning the model of the signal.\nFigure 11 shows the performance of the RoShamBo NID when playing 10 matches of 3,000 rounds with each contestant. The overall standings were determined by ordering the total scores for each contestant in all rounds played (+1 for winning a round against a contestant by the NID player; \u22121 for losing a round; 0 for ties). Therefore, it was important for each player to maximize its win against the weaker opponents, and minimize its loss to stronger opponents. The x-axis includes the contestant number while the y-axis describes the difference between the average score of the RoShamBo NID and the contestant; error bars indicate a single standard deviation difference.\nAs shown by the figure, the RoShamBo NID was able to defeat all contestant in all matches, including a version of Iocaine Powder. The best performance for the NID was achieved when playing pattern reasoners that used deterministic signals (Contestants 3, 5 and 6). Each of these contestants directly predicted their opponents\u2019 play as a function of the history, without reasoning about their opponents\u2019 model of themselves. Consequently, it was difficult for them to detect change in the strategies of adaptive opponents, such as the RoShamBo NID. In addition, the use of deterministic signals made it harder for these contestants to capture probabilistic players like the NID algorithm.\nThe RoShamBo NID also outperformed those contestants that attempted to trick their opponents, by reasoning about the possibility that the opponents are double- and tripleguessing their model (Contestants 4 and 1). This shows that the NID was able to determine the level of reasoning employed by its opponents."}, {"heading": "6. Relationship with Economic Models", "text": "In this section, we describe the relationship between NIDs and several existing formalisms for representing uncertainty over decision-making processes. NIDs share a close relationship with Bayesian games (Harsanyi, 1967), a game-theoretic framework for representing uncertainty over players\u2019 payoffs. Bayesian games capture the beliefs agents have about each other as well as define an equilibrium that assigns a best response strategy for each agent given its beliefs. Bayesian games are quite powerful in their ability to describe belief hierarchies and cyclic belief structures.\nIn a Bayesian game, each agent has a discrete type embodying its private information. Let N be a set of agents. For each agent i a Bayesian game includes a set of possible types Ti, a set of possible actions Ci, a conditional distribution pi and a utility function ui. Let T = \u00d7i\u2208NTi and let C = \u00d7i\u2208NCi. For each agent i, let T\u2212i = \u00d7j &=iTj denote the set of all possible types other than those of agent i. The probability distribution pi is a function from ti to \u2206T\u2212i, that is, pi(.|ti) specifies for each type ti \u2208 Ti a joint distribution over the types of the other agents. The utility function ui is a function from C \u00d7 T to the real numbers. It is a standard assumption that the game, including agents\u2019 strategies, utilities and type distributions, is common knowledge to all agents.\nThe solution concept most commonly associated with Bayesian games is a Bayesian Nash equilibrium. This equilibrium maps each type to a mixed strategy over its actions that is the agent\u2019s best response to the strategies of the other agents, given its beliefs about their types. Notice that in a Bayesian game, an agent\u2019s action can depend on its own types but not on the types of the other agents, because they are unknown to that agent when it analyzes the game. It is assumed that each agent knows its own type, and that this type subsumes all of the agent\u2019s private information before the game begins. Because the types of other agents are unknown, each agent maximizes its expected utility given its distribution over other types.\nLet N\u2212i denote all of the agents in the Bayesian game apart from agent i. Let \u03c3i(.|ti) denote a random strategy for agent i given that its type is ti. A Bayesian Nash equilibrium\nis any mixed strategy profile \u03c3 such that for any agent i and type ti \u2208 Ti we have\n\u03c3i(.|ti) \u2208 argmax\u03c4\u2208\u2206Ci \u2211\nt\u2212i\u2208T\u2212i pi(t\u2212i|ti)\u00b7\u2211 c\u2208C (\u220f j\u2208N\u2212i \u03c3j(cj |tj) ) \u03c4(ci)ui(t, c)\n(2)\nBayesian games have been used extensively for modeling interaction in which agents have private information, such as auction mechanisms (Myerson, 1991) and they can be used to express uncertainty over agents\u2019 decision-making models. In general, Bayesian games are just as expressive as NIDs. As we show, any Bayesian game can be converted into a NID in time and space linear in the size of the Bayesian game. Conversely, any NID can be converted into a Bayesian game, because any NID can be converted to a MAID, which can in turn be converted to an extensive form game. The extensive form game can be converted to a normal form game which is a trivial Bayesian game with only one type per agent. However, in the worst case, the size of the extensive form game will be exponential in the number of informational parents for decision nodes in the MAID, and the size of the normal form game will be exponential in the size of the extensive form game. Of course, this is a brute force conversion; more compact conversions may be possible.\nWe now consider more formally the question of whether Bayesian games can be represented by NIDs. The idea is to align each type in a Bayesian game with a decision in a NID block. The resulting best response strategy for the decision in the NID equilibrium will equal the Bayes Nash equilibrium strategy for the type.\nDefinition 6.1. Let B be a Bayesian game and N a NID. We say that N is equivalent to B if there exists an injective mapping f from types in B to (block,agent) pairs in N , such that the following conditions hold:\n1. For any Bayesian Nash equilibrium \u03c3 of B, there exists a NID equilibrium of N , such that for every type ti, if f maps ti to (K,\u03b1), the best-response and actually-played strategies for \u03b1 in K are equal to \u03c3i(.|ti).\n2. For any NID equilibrium of N , there exists a Bayesian Nash equilibrium \u03c3 of B such for every (K,\u03b1) in the image of f , \u03c3i(.|ti) where ti = f\u22121(K,\u03b1) is equal to the best-response and actually-played strategies for \u03b1 in K.\nThe following theorem is proved in Appendix 8.\nTheorem 6.2. Every Bayesian game can be represented by an equivalent NID whose size is linear in the size of the Bayesian game.\nIn this section, we will use the term Bayesian games to specify a representation that includes type distributions and utility functions that are presented explicitly. NIDs enjoy the same advantages over fully specified Bayesian games that graphical models typically enjoy over unstructured representations. In general, NIDs may be exponentially more compact than Bayesian games because Bayesian games require, for every type of every agent, a full joint distribution over the types of all other agents. In addition, the utility function in a Bayesian game specifies a utility for each joint combination of types and actions of every player. These distributions and utility functions are exponential in the number of players. In NIDs, because they are based on MAIDs, the type distributions can be decomposed\ninto a product of small conditional distributions, and the utility functions can be additively decomposed into a sum of small functions that depend only on a small number of actions.\nIn addition, Bayesian games are representationally obscure. First, types in Bayesian games are atomic entities that capture all the information available to an agent in a single variable. A type is used to capture both an agent\u2019s beliefs about the way the world works (including its preferences), and its private information. For example, in poker, both the player\u2019s beliefs about the other player\u2019s tendency to bluff and her knowledge of what cards she has received are captured by a type. We believe that these two aspects are fundamentally different; one describes the actual state of the world and the other describes what is going on in a player\u2019s head. Conflating these two aspects leads to confusion. In NIDs, the two aspects are differentiated. Private information about the world is represented by informational parents, whereas mental models are represented by blocks.\nSecond, a type in a Bayesian game does not decompose different aspects of information into variables. Thus in poker, the hand must be represented by a single variable, whereas in NIDs it can be represented by different variables representing each of the cards. A final point is that in Bayesian games all of the uncertainty must be folded into the utility functions and the distribution over agents\u2019 types. Consider the scenario in which two agents have conflicting beliefs about a chance variable, such as in Example 4.3. In a NID, there will be a separate block for each possible mental model that differs in the CPD assignments for the chance variable. In contrast, each type in the Bayesian game would sum over the distribution over the chance variable. Looking at the Bayesian game, we would not know whether the reason for the different utility functions is because the agent has different beliefs about the chance variable, or whether it is due to different preferences of the agent.\nNIDs also exhibit a relationship with more recent formalisms for games of awareness, in which agents may be unaware of other players\u2019 strategies or of the structure of the game (Halpern & Rego, 2006; Feinberg, 2004). A game description in this formalism shows how players\u2019 awareness about each other\u2019s strategies changes over time. A game of awareness includes a set of extensive form game descriptions, called augmented games, that represent an analyst\u2019s beliefs about the world, as well as separate descriptions for each game that may become true according to agents\u2019 subjective beliefs. The analyst\u2019s augmented game is considered to be the actual description of reality, while each subjective augmented game can differ from the analyst\u2019s game in agents\u2019 utility functions, their decisions, and the strategies available to agents at each of their decisions. A history for an agent in an augmented game is a sub-path in the tree leading to a node in which the agent makes a move. Awareness is modeled by a function that maps an agent-history pair in one augmented game to another augmented game which the agent considers possible given the history. Uncertainty over agents\u2019 awareness in an augmented game can be quantified by having nature choose a move in the tree leading to agents\u2019 information sets. The definition of Nash equilibrium is extended to include a set of strategies for each agent-game pairthat the agent considers to be possible, given a history and the best-response strategies used by other agents in the augmented game. This formalism can capture an analyst\u2019s model about agents\u2019 awareness as well as agents\u2019 model about their own, or other agents\u2019 awareness.\nThere are fundamental differences between NIDs and games of awareness. First, like Bayesian games, the equilibrium conditions for this representation do not allow for agents to deviate from their best-response strategies. Second, they require the presence of a modeler\nagent, that in reality, is modeling its uncertainty about levels of awareness of other agents. NIDs allow for such a modeler agent, but they do not require it. This allows to capture situations where no agent has certain knowledge of reality, such as in the Baseball NID of Example 2.4. Third, each augmented game of awareness is represented as an extensive form game, that as we have shown above, may be exponentially larger than the MAID used to represent each decision-making model in a NID. Lastly, agents\u2019 awareness over each other\u2019s strategies is just one type of reasoning that can be captured by a NID. Other types of reasoning processes were described in Section 4.\nLastly, Gmytrasiewicz and Durfee (2001) have developed a framework for representing uncertainty over decision-making using a tree structure in which the nodes consist of payoff matrices for a particular agent. Like Bayesian games, uncertainty is folded into the payoff matrices. Each agent maintains its own tree, representing its model of the decision-making processes used by other agents. Like traditional representations, this language assumes that all agents behave rationally. In addition, it assumes that each agent believes others to use a fixed strategy, that is folded into the environment."}, {"heading": "7. Conclusion", "text": "We have presented a highly expressive language for describing agents\u2019 beliefs and decisionmaking processes in games. Our language is graphical. A model in our language is a network of interrelated models, where each mental model itself is a graphical model of a game. An agent in one mental model may believe that another agent (or possibly itself) uses a different mental model to make decisions; it may have uncertainty about which mental model is used. We presented semantics for our language in terms of multi-agent influence diagrams. We analyzed the relationship between our language and Bayesian games. They are equally expressive, but NIDs may be exponentially more compact.\nWe showed how our language can be used to describe agents who play irrationally, in the sense that their actual play does not correspond to the best possible response given their beliefs about the world and about other agents. This is captured by a novel equilibrium concept that captures the interaction between what agents should do and what they actually do. We also showed how to express situations in which agents have conflicting beliefs, including situations in which the agents do not have a common prior distribution over the state of the world. Finally, we showed how to capture cyclic reasoning patterns, in which agents engage in infinite chains of \u201cI think that you think that I think...\u201d reasoning.\nA vital question is the use of our language to learn about agents\u2019 behavior and reasoning processes. As we have shown, our language can be used to learn non-stationary strategies in rock-paper-scissors. In other work, we have shown how models that were inspired by NIDs can learn people\u2019s play in negotiation games (Gal, Pfeffer, Marzo, & Grosz, 2004; Gal & Pfeffer, 2006). The focus of our continuing work will be to develop a general method for learning models in NIDs."}, {"heading": "Acknowledgments", "text": "Thank you very much for the useful comments provided by the anonymous reviewers and editor. Thanks to Barbara Grosz and Whitman Richards for their invaluable guidance.\nThanks to Adam Juda for reading a prior draft of this work. This work was supported by an NSF Career Award IIS-0091815 and AFOSR under contract FA9550-05-1-0321."}, {"heading": "8. Appendix A", "text": "Theorem 3.1: Converting a NID into a MAID will not introduce a cycle in the resulting MAID.\nProof. First, let us ignore the edges added by step 5 of the construction, and focus on the MAID fragment OK constructed from a single block K. Since the block is acyclic, we can number the nodes of the block with integers in topological order. We now number the nodes of OK as follows. For a node N\u03b1 that derives from a chance or utility node N in K, N\u03b1 gets the same number as N . A node BR[D]K gets the same number as D. A node DK\u03b1 , where \u03b1 owns D, gets the same number as D plus 1/3. A node DK\u03b1 , where \u03b1 does not own D, gets the same number as D plus 2/3. By construction, if P is a parent of N in OK , P has a lower number than N .\nNow let us consider the entire constructed MAID O. Suppose, by way of contradiction, that there is a cycle in O. It follows from the above argument that it must consist entirely of edges between fragments added by step 5. Since all such edges emanate from a node DK\u03b1 where \u03b1 owns D, and end at a node DL\u03b1 , all nodes in the cycle must refer to the same decision D, and must belong to the agent who owns D. Thus the cycle must be of the form DK1\u03b1 , . . . , D Kn \u03b1 , D K1 \u03b1 where \u03b1 owns D. Since an edge has been added from DKi\u03b1 to DKi+1 in O, \u03b1 must be modeling itself in block Ki as using block Ki+1 to make decision D. Therefore there is a self-loop in the NID, which is a contradiction.\nTheorem 6.2: Every Bayesian game can be represented as an equivalent NID whose size is linear in the size of the Bayesian game.\nProof. Given a Bayesian game B, we construct a NID N as follows. The set of agents in N is equal to the set of agents in B. For each type ti of agent i in B there is a corresponding block in N labeled ti. The block ti contains a decision node Dj and utility node Uj for every agent j. Dj has no informational parents. The domain of Dj is the set of choices Cj for agent j in B. We add a new chance node Qi in block ti whose domain is the set T\u2212i. Each node Mod[i, Dj ] where j $= i will have the node Qi as a parent. The parents of Ui are all the decision nodes as well as the node Qi. For an agent j $= i, Uj has only the parent Dj . For each agent j we define a distinguished action c\u2217j \u2208 Cj .\nWe set the CPD for nodes in ti as follows:\n1. The CPD of Mod[i, Di] assigns probability 1 to ti.\n2. The CPD of Qi assigns probability pi(t\u2212i | ti), as defined in B, for each type profile t\u2212i \u2208 T\u2212i.\n3. The CPD of a node Mod[i, Dj ] where j $= i assigns probability 1 to block tj when the jth element of the value of the parent node Qi equals tj . This projects the probability distribution Qi in B to the node Mod[i, Dj ] representing i\u2019s beliefs about which block agent j is using in the NID.\n4. The CPD of Ui assigns probability 1 to ui(t, c), as defined in B, given that Qi equals t, and D equals c.\n5. The CPD of Uj assigns probability 1 to utility 1 when Dj = c\u2217j , and probability 1 to 0 otherwise.\n6. The CPD of Mod[j, Dk], for all k, when j $= i, assigns probability 1 to ti.\nOur construction is accompanied by the injective mapping f that maps a type ti to the (block,agent) pair (ti, i).\nLet M be the constructed MAID for N. To prove condition 1 of Definition 6.1, let \u03c4 be a Bayes Nash equilibrium of B. For each agent, \u03c4i is a conditional probability distribution \u03c4i(\u00b7 | ti). We define the strategy profile \u03c3 in M as follows. \u03c3BR[Di]ti = \u03c4i(\u00b7 | ti) for decisions owned by agent i, and \u03c3BR[Dj ]ti assigns probability 1 to c \u2217 j when j $= i.\nWe claim the following:\n1. \u03c3 is a MAID equilibrium in M, according to Definition 2.3.\n2. In the resulting NID equilibrium, the best response strategy for i in ti is \u03c4i(\u00b7 | ti).\n3. In the resulting NID equilibrium, the actually played strategy is the same as the best response strategy.\nClaim 3 is true because Mod[i, Di] assigns probability 1 to ti. Note that there are no informational parents in N. Therefore, by the definition of NID equilibrium, the best response strategy \u03b8tiDi = \u03c3BR[Di,ti] = \u03c4i(\u00b7 | ti). Therefore, Claim 2 is true.\nTo prove Claim 1, note first that in block ti, the utility node Uj , where j $= i, is fully determined by Dj , because Dj is the sole parent of Uj . Also, player j is not self-modeling at Dj , because the CPD of node Mod[j, Dj ] assigns probability 1 to ti. The same holds in M: the decision node BR[Dj ]ti is the sole parent of U tij . Therefore, in any equilibrium for M, the strategy for BR[Dj ]ti will assign probability 1 to the distinguished action c\u2217j that causes U tij to be 1.\nIn block ti, the CPD of Mod[i, Dj ] assigns probability 0 to ti. This means that player j is not using block ti to make its decision, according to i\u2019s beliefs. Therefore, BR[Dj ]ti is independent of U tii , and the equilibrium strategies for BR[Di]\nti are independent of the distinguished action chosen for the BR[Di]tj .\nBy the definition of MAID equilibrium of Definition 2.3, the strategy profile \u03c3 is an equilibrium if each \u03c3i maximizes EU\u03c3(i). We need to show that maximizing this is equivalent to maximizing the right hand side of Equation 2. There is a utility U tii and decision node BR[Di]ti in every block ti. Let ctii denote a choice for agent i at decision BR[Di]\nti in block ti. Let t\u2032i denote a block corresponding to a different type t \u2032 i for agent i. Let c t\u2032i i be a choice for the agent in decision BR[Di]t \u2032 i at block t\u2032i and c\u2212i all the choices for other decisions BR[D\u2212i]ti . By the construction of M, U tii is d-separated from BR[Di] t\u2032i given BR[Di]ti and BR[D\u2212i]ti . As a result, we can optimize U tii separately from all other utility nodes belonging to agent i, considering only BR[Di]ti . We then get that the utility for i in M given the\nstrategy profile \u03c3 can be written as\nE\u03c3[U tii ] = \u2211\nc ti i\n\u03c3tii (c ti i )\n\u2211\nc\u2212i\n\u03c3\u2212i(c\u2212i) \u2211\nu ti i\nP \u03c3(U tii = ui | c ti i , c\u2212i) \u00b7 u ti i (3)\nWe now condition on agent i\u2019s beliefs about the decisions of other agents in block ti. Let Mod[i,D\u2212i]ti denote the set of nodes Mod[i, Dj ]ti where j $= i, and let the tuple t\u2212i refer to the block label profile for blocks T\u2212i. We now obtain\n\u2211\nc ti i\n\u03c3tii (c ti i )\n\u2211\nc\u2212i\n\u03c3\u2212i(c\u2212i) \u2211\nt\u2212i\nP (Mod[i,D\u2212i]ti = t\u2212i) \u2211\nu ti i\n(utii | c ti i , c\u2212i, t\u2212i) \u00b7 u ti i (4)\nNow observe that the role of Mod[i,D\u2212i]ti is to determine which choices for decisions BR[D\u2212i]ti are relevant for the utility of player i. In particular, if Mod[i, Dj ]ti is equal to tj , then it is j\u2019s choice in block tj that player i needs to consider when it makes decision BR[Di]ti . Let c t\u2212i i denote the relevant choices for BR[D\u2212i]\nti when Mod[i,D\u2212i]ti = t\u2212i. Since other choice variables are irrelevant, we can marginalize them out and obtain\n\u2211 c ti i \u03c3tii (c ti i ) \u2211 c t\u2212i \u2212i \u03c3\u2212i(c t\u2212i \u2212i ) \u2211 t\u2212i\nP (Mod[i,D\u2212i]ti = t\u2212i) (5) \u2211\nu ti i\nP \u03c3(U tii = ui | c ti i , c t\u2212i \u2212i ) \u00b7 u ti i\nRearranging terms, we rewrite Equation 5.\n\u2211 t\u2212i P (Mod[i,D\u2212i]ti = t\u2212i) \u2211 c (\u220f j &=i \u03c3 tj j (c tj j ) ) (6)\n\u03c3tii (c ti i ) \u2211 u\nti i\nP \u03c3(U tii = ui | c ti i , c t\u2212i \u2212i ) \u00b7 u ti i\nBy our construction, P (Mod[i,D\u2212i]ti = t\u2212i) is pi(t\u2212i | ti) as defined in B, \u03c3 tj j (c tj j ) is \u03c4j(cj | tj) as defined in B, and \u2211\nu ti i\nP \u03c3(U tii = ui | c ti i , c t\u2212i \u2212i ) \u00b7 u ti i is ui(t, c). We therefore get\n\u2211\nt\u2212i\npi(t\u2212i | ti) \u2211\nc\n  \u220f\nj &=i \u03c4j(cj | tj)\n\n \u03c4i(ci | ti)ui(t, c) (7)\nTherefore \u03c3 is a MAID equilibrium of M if and only if \u03c4 is a Bayesian Nash equilibrium of B. Claim 1 is established and therefore Condition 1 of Definition 6.1 is satisfied.\nFinally, to prove Condition 2, given a NID equilibrium of N we construct a MAID equilibrium \u03c3 for M by copying the best response strategies, and then construct strategies \u03c4 for B in exactly the reverse manner to above. The previous reasoning applies in reverse to show that \u03c4 is a Bayes Nash equilibrium of B and the best response and actually played strategies for N are equal to \u03c4 ."}], "references": [{"title": "The supply chain trading agent competition", "author": ["R. Arunachalam", "N.M. Sadeh"], "venue": "Electronic Commerce Research and Applications,", "citeRegEx": "Arunachalam and Sadeh,? \\Q2005\\E", "shortCiteRegEx": "Arunachalam and Sadeh", "year": 2005}, {"title": "Epistemic conditions for", "author": ["R. Aumann", "A. Brandenburger"], "venue": "Nash equilibrium. Econometrica,", "citeRegEx": "Aumann and Brandenburger,? \\Q1995\\E", "shortCiteRegEx": "Aumann and Brandenburger", "year": 1995}, {"title": "Judgment in Managerial Decision Making", "author": ["M. Bazerman"], "venue": "Wiley Publishers.", "citeRegEx": "Bazerman,? 2001", "shortCiteRegEx": "Bazerman", "year": 2001}, {"title": "The first international RoShamBo programming competition", "author": ["D. Billings"], "venue": "International Computer Games Association Journal, 23 (1), 3\u20138.", "citeRegEx": "Billings,? 2000", "shortCiteRegEx": "Billings", "year": 2000}, {"title": "A continuation method for Nash equilibria in structured games", "author": ["B. Blum", "C.R. Shelton", "D. Koller"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Blum et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2006}, {"title": "How to make sense of the common prior assumption under incomplete information", "author": ["G. Bonanno", "K. Nehring"], "venue": "International Journal of Game Theory,", "citeRegEx": "Bonanno and Nehring,? \\Q1999\\E", "shortCiteRegEx": "Bonanno and Nehring", "year": 1999}, {"title": "Probabilistic Networks and Expert Systems", "author": ["R.G. Cowell", "S.L. Lauritzen", "D.J. Spiegelhater"], "venue": null, "citeRegEx": "Cowell et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Cowell et al\\.", "year": 2005}, {"title": "Bucket elimination: A unifying framework for reasoning", "author": ["R. Dechter"], "venue": "Artificial Intelligence, 113 (1-2), 41\u201385.", "citeRegEx": "Dechter,? 1999", "shortCiteRegEx": "Dechter", "year": 1999}, {"title": "Iocaine Powder", "author": ["D. Egnor"], "venue": "International Computer Games Association Journal, 23 (1), 3\u20138.", "citeRegEx": "Egnor,? 2000", "shortCiteRegEx": "Egnor", "year": 2000}, {"title": "Subjective reasoning \u2014 games with unawareness", "author": ["Y. Feinberg"], "venue": "Tech. rep. 1875, Stanford University.", "citeRegEx": "Feinberg,? 2004", "shortCiteRegEx": "Feinberg", "year": 2004}, {"title": "A language for modeling agents\u2019 decision making processes in games", "author": ["Y. Gal", "A. Pfeffer"], "venue": "In Proc. 2nd International Joint Conference on Autonomous Agents and Multi-agent Systems (AAMAS)", "citeRegEx": "Gal and Pfeffer,? \\Q2003\\E", "shortCiteRegEx": "Gal and Pfeffer", "year": 2003}, {"title": "A language for opponent modeling in repeated games", "author": ["Y. Gal", "A. Pfeffer"], "venue": "In Workshop on Game Theory and Decision Theory, AAMAS", "citeRegEx": "Gal and Pfeffer,? \\Q2003\\E", "shortCiteRegEx": "Gal and Pfeffer", "year": 2003}, {"title": "Reasoning about rationality and belief", "author": ["Y. Gal", "A. Pfeffer"], "venue": "In Proc. 3rd International Joint Conference on Autonomous Agents and Multi-agent Systems (AAMAS)", "citeRegEx": "Gal and Pfeffer,? \\Q2004\\E", "shortCiteRegEx": "Gal and Pfeffer", "year": 2004}, {"title": "Predicting people\u2019s bidding behavior in negotiation", "author": ["Y. Gal", "A. Pfeffer"], "venue": "In Proc. 5th International Joint Conference on Autonomous Agents and Multi-agent Systems (AAMAS)", "citeRegEx": "Gal and Pfeffer,? \\Q2006\\E", "shortCiteRegEx": "Gal and Pfeffer", "year": 2006}, {"title": "Learning social preferences in games", "author": ["Y. Gal", "A. Pfeffer", "F. Marzo", "B. Grosz"], "venue": "In Proc. 19th National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Gal et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2004}, {"title": "Bounded Rationality: The Adaptive Toolbox", "author": ["G. Gigerenzer", "R. Selten"], "venue": null, "citeRegEx": "Gigerenzer and Selten,? \\Q2001\\E", "shortCiteRegEx": "Gigerenzer and Selten", "year": 2001}, {"title": "Rational communication in multi-agent environments", "author": ["P. Gmytrasiewicz", "E.H. Durfee"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Gmytrasiewicz and Durfee,? \\Q2001\\E", "shortCiteRegEx": "Gmytrasiewicz and Durfee", "year": 2001}, {"title": "Extensive games with possibly unaware players", "author": ["J. Halpern", "L. Rego"], "venue": "In Proc. 5th International Joint Conference on Autonomous Agents and Multi-agent Systems (AAMAS)", "citeRegEx": "Halpern and Rego,? \\Q2006\\E", "shortCiteRegEx": "Halpern and Rego", "year": 2006}, {"title": "Games with incomplete information played by \u2019Bayesian\u2019 players", "author": ["J.C. Harsanyi"], "venue": "Management Science, 14, 159\u2013182, 320\u2013334, 486\u2013502.", "citeRegEx": "Harsanyi,? 1967", "shortCiteRegEx": "Harsanyi", "year": 1967}, {"title": "Influence diagrams", "author": ["R.A. Howard", "J.E. Matheson"], "venue": "In Readings on the Principles and Applications of Decision Analysis,", "citeRegEx": "Howard and Matheson,? \\Q1984\\E", "shortCiteRegEx": "Howard and Matheson", "year": 1984}, {"title": "Graphical models for game theory", "author": ["M. Kearns", "M. Littman", "S. Singh"], "venue": "In Proc. 17th Conference on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "Kearns et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2001}, {"title": "Efficient computation of equilibria for extensive two-person games", "author": ["D. Koller", "N. Meggido", "B. von Stengel"], "venue": "Games and Economic Behavior,", "citeRegEx": "Koller et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Koller et al\\.", "year": 1996}, {"title": "Multi-agent influence diagrams for representing and solving games", "author": ["D. Koller", "B. Milch"], "venue": "In Proc. 17th International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "Koller and Milch,? \\Q2001\\E", "shortCiteRegEx": "Koller and Milch", "year": 2001}, {"title": "Price prediction strategies for market-based scheduling", "author": ["J.K. MacKie-Mason", "A. Osepayshivili", "D.M. Reeves", "M.P. Wellman"], "venue": "In Proc. of 18th International Conference on Automated Planning and Scheduling", "citeRegEx": "MacKie.Mason et al\\.,? \\Q2004\\E", "shortCiteRegEx": "MacKie.Mason et al\\.", "year": 2004}, {"title": "The common prior assumption in economic theory", "author": ["S. Morris"], "venue": "Economic Philosophy, pp. 227\u2013253.", "citeRegEx": "Morris,? 1995", "shortCiteRegEx": "Morris", "year": 1995}, {"title": "Game Theory", "author": ["R. Myerson"], "venue": "Harvard University Press.", "citeRegEx": "Myerson,? 1991", "shortCiteRegEx": "Myerson", "year": 1991}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": "Morgan Kaufmann.", "citeRegEx": "Pearl,? 1988", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Agent-human interactions in the continuous double auction", "author": ["D. Rajarshi", "J.E. Hanson", "J.O. Kephart", "G. Tesauro"], "venue": "In Proc. 17th International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "Rajarshi et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Rajarshi et al\\.", "year": 2001}, {"title": "Modeling Bounded Rationality", "author": ["A. Rubinstein"], "venue": "MIT Press.", "citeRegEx": "Rubinstein,? 1998", "shortCiteRegEx": "Rubinstein", "year": 1998}, {"title": "Do the Right Thing: Studies in Limited Rationality", "author": ["S. Russell", "E. Wefald"], "venue": null, "citeRegEx": "Russell and Wefald,? \\Q1991\\E", "shortCiteRegEx": "Russell and Wefald", "year": 1991}, {"title": "A behavioral model of rational choice", "author": ["H.A. Simon"], "venue": "Quarterly Journal of Economics, 69, 99\u2013118.", "citeRegEx": "Simon,? 1955", "shortCiteRegEx": "Simon", "year": 1955}, {"title": "Multi-agent algorithms for solving graphical games", "author": ["D. Vickrey", "D. Koller"], "venue": "In Proc. 18th National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Vickrey and Koller,? \\Q2002\\E", "shortCiteRegEx": "Vickrey and Koller", "year": 2002}], "referenceMentions": [{"referenceID": 26, "context": "Meanwhile, graphical languages such as Bayesian networks (Pearl, 1988) have received much attention in AI because they allow for a compact and natural representation of uncertainty in many domains that exhibit structure.", "startOffset": 57, "endOffset": 70}, {"referenceID": 7, "context": "These formalisms often lead to significant savings in representation and in inference time (Dechter, 1999; Cowell, Lauritzen, & Spiegelhater, 2005).", "startOffset": 91, "endOffset": 147}, {"referenceID": 18, "context": "NIDs share a relationship with the Bayesian game formalism, commonly used to model uncertainty over agents\u2019 payoffs in economics (Harsanyi, 1967).", "startOffset": 129, "endOffset": 145}, {"referenceID": 26, "context": "The building blocks of NIDs are Bayesian networks (Pearl, 1988), and Multi Agent Influence Diagrams (Koller & Milch, 2001).", "startOffset": 50, "endOffset": 63}, {"referenceID": 21, "context": "The structure of the NID can then be exploited by a MAID solution algorithm (Koller & Milch, 2001; Vickrey & Koller, 2002; Koller et al., 1996; Blum et al., 2006).", "startOffset": 76, "endOffset": 162}, {"referenceID": 4, "context": "The structure of the NID can then be exploited by a MAID solution algorithm (Koller & Milch, 2001; Vickrey & Koller, 2002; Koller et al., 1996; Blum et al., 2006).", "startOffset": 76, "endOffset": 162}, {"referenceID": 30, "context": "Since the challenge to the notion of perfect rationality as the foundation of economic systems presented by Simon (1955), the theory of bounded rationality has grown in different", "startOffset": 108, "endOffset": 121}, {"referenceID": 2, "context": "In the psychological literature, this effect is referred to as motivational bias or positive illusion (Bazerman, 2001).", "startOffset": 102, "endOffset": 118}, {"referenceID": 27, "context": "These concepts have recently been formalized by Rubinstein (1998). From a traditional AI perspective, an agent exhibits bounded rationality if its program is a solution to the constrained optimization problem brought about by limitations of architecture or computational resources (Russell & Wefald, 1991).", "startOffset": 48, "endOffset": 66}, {"referenceID": 23, "context": "Myerson (1991) showed that a game with inconsistent belief structure that is finite can be converted to a new game with consistent belief structures by constructing utility functions that are equivalent to the original game in a way that they both assign the same expected utility to the agents.", "startOffset": 0, "endOffset": 15}, {"referenceID": 23, "context": "For a summary of the economic and philosophical ramifications of relaxing the common prior assumption, see the work of Morris (1995) and Bonanno and Nehring (1999).", "startOffset": 119, "endOffset": 133}, {"referenceID": 5, "context": "For a summary of the economic and philosophical ramifications of relaxing the common prior assumption, see the work of Morris (1995) and Bonanno and Nehring (1999). Once we have a language that allows us to talk about different mental models that agents have about the world, and different beliefs that they have about each other and about the structure of the game, it is natural to relax the common prior assumption within NIDs while preserving the original structure of the game.", "startOffset": 137, "endOffset": 164}, {"referenceID": 3, "context": "In a recent competition (Billings, 2000), programs competed against each other in matches consisting of 1000 games of RoShamBo.", "startOffset": 24, "endOffset": 40}, {"referenceID": 8, "context": "The winning program, called Iocaine Powder (Egnor, 2000), did a beautiful job of modeling its opponents on multiple levels.", "startOffset": 43, "endOffset": 56}, {"referenceID": 18, "context": "NIDs share a close relationship with Bayesian games (Harsanyi, 1967), a game-theoretic framework for representing uncertainty over players\u2019 payoffs.", "startOffset": 52, "endOffset": 68}, {"referenceID": 25, "context": "Bayesian games have been used extensively for modeling interaction in which agents have private information, such as auction mechanisms (Myerson, 1991) and they can be used to express uncertainty over agents\u2019 decision-making models.", "startOffset": 136, "endOffset": 151}, {"referenceID": 9, "context": "NIDs also exhibit a relationship with more recent formalisms for games of awareness, in which agents may be unaware of other players\u2019 strategies or of the structure of the game (Halpern & Rego, 2006; Feinberg, 2004).", "startOffset": 177, "endOffset": 215}, {"referenceID": 16, "context": "Lastly, Gmytrasiewicz and Durfee (2001) have developed a framework for representing uncertainty over decision-making using a tree structure in which the nodes consist of payoff matrices for a particular agent.", "startOffset": 8, "endOffset": 40}], "year": 2008, "abstractText": "This paper presents Networks of Influence Diagrams (NID), a compact, natural and highly expressive language for reasoning about agents\u2019 beliefs and decision-making processes. NIDs are graphical structures in which agents\u2019 mental models are represented as nodes in a network; a mental model for an agent may itself use descriptions of the mental models of other agents. NIDs are demonstrated by examples, showing how they can be used to describe conflicting and cyclic belief structures, and certain forms of bounded rationality. In an opponent modeling domain, NIDs were able to outperform other computational agents whose strategies were not known in advance. NIDs are equivalent in representation to Bayesian games but they are more compact and structured than this formalism. In particular, the equilibrium definition for NIDs makes an explicit distinction between agents\u2019 optimal strategies, and how they actually behave in reality.", "creator": null}}}