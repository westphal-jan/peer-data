{"id": "1401.6124", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2014", "title": "Iterative Universal Hash Function Generator for Minhashing", "abstract": "Minhashing is a technique used to estimate the Jaccard Index between two sets by exploiting the probability of collision in a random permutation. In order to speed up the computation, a random permutation can be approximated by using an universal hash function such as the $h_{a,b}$ function proposed by Carter and Wegman. A better estimate of the Jaccard Index can be achieved by using many of these hash functions, created at random. In this paper a new iterative procedure to generate a set of $h_{a,b}$ functions is devised that eliminates the need for a list of random values and avoid the multiplication operation during the calculation. The properties of the generated hash functions remains that of an universal hash function family. This is possible due to the random nature of features occurrence on sparse datasets. Results show that the uniformity of hashing the features is maintaned while obtaining a speed up of up to $1.38$ compared to the traditional approach.", "histories": [["v1", "Thu, 23 Jan 2014 19:03:38 GMT  (18kb)", "http://arxiv.org/abs/1401.6124v1", "6 pages, 4 tables, 1 algorithm"]], "COMMENTS": "6 pages, 4 tables, 1 algorithm", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["fabricio olivetti de franca"], "accepted": false, "id": "1401.6124"}, "pdf": {"name": "1401.6124.pdf", "metadata": {"source": "META", "title": "Iterative Universal Hash Function Generator for Minhashing", "authors": [], "emails": ["olivetti@ieee.org"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 1.\n61 24\nv1 [\ncs .L\nG ]\n2 3\nJa n\nMinhashing is a technique used to estimate the Jaccard Index between two sets by exploiting the probability of collision in a random permutation. In order to speed up the computation, a random permutation can be approximated by using an universal hash function such as the ha,b function proposed by Carter and Wegman. A better estimate of the Jaccard Index can be achieved by using many of these hash functions, created at random. In this paper a new iterative procedure to generate a set of ha,b functions is devised that eliminates the need for a list of random values and avoid the multiplication operation during the calculation. The properties of the generated hash functions remains that of an universal hash function family. This is possible due to the random nature of features occurrence on sparse datasets. Results show that the uniformity of hashing the features is maintaned while obtaining a speed up of up to 1.38 compared to the traditional approach.\nKeywords:"}, {"heading": "1. Introduction", "text": "Many tasks in Machine Learning require the calculation of similarity for every pair of objects in the data set. Some examples of such tasks are data clustering and nearest neighbors searching. The need for such calculation may be demanding for large data sets with many features leading to the so called curse of dimensionality [1]. This occurs when the objects of a data set are described on a higher dimension, typically with hundreds of features, adding one more dimension to the computational complexity, i.e., a O(n) becomes O(n.d), where n is the number of objects and d the number of features. Also, a higher dimension may also lead to a loss of precision when calculating the similarities between two object since this may only be perceived on a small subset of features and thus masked by all the others, that could be considered random noise.\nOne way to deal with such problem is to transform the\nEmail address: olivetti@ieee.org (Fabr\u0131\u0301cio Olivetti de Franc\u0327a)\ndata into a low dimension representation while preserving enough information in way that similar objects remains similar after the transformation. Some methods used to reduce the dimensionality of a data set are Feature Selection [2], Feature Extraction [3] and Probabilistic Dimension Reduction [4].\nThe Feature Selection approach tries to find the subset of the features that are most relevant to the task being performed. This can be done by removing redundant (highly correlated) variables and irrelevant variables that brings no information whatsoever. These are usually measured by the Entropy, Information Gain and Correlation measures. Since this is a combinatorial problem, it is mostly solved with meta-heuristic approaches [5, 6].\nFeature Extraction refers to the creation of a smaller features set based on the (non-)linear combination of the original features. This is usually performed by using matrix decomposition techniques, creating a map between the original features set to the transformed one. More recently there has been a focus on Neural Network Feature Extraction through Deep Learning techniques [7].\nProbabilistic Dimension Reduction is performed ex-\nPreprint submitted to Elsevier January 25, 2014\nploiting the probability of two objects being very similar. This is done by means of specially designed hash functions that has a high probability of collision when hashing objects with similarity above a certain threshold. One of these techniques is called Locality-Sensitive Hashing [8, 9] and was successfuly applied to different applications including text and image retrieval. The main advantage of this technique is that the computational complexity approximates O(n).\nOne of such technique is called Minhashing [10], and it was specifically crafted to approximate the Jaccard Index between two sets. This is based on the fact that, given a random permutation of the features set, the probability that the first feature of two objects are the same is equal to the Jaccard Index between those two. In order to become computationally feasible, the random permutation is approximated by using an universal hash function.\nSince a good estimation through probabilistic procedures require a sample of considerable size, there is a need to automatically create universal hash functions at random. One of such approaches, as devised by Carter and Wegman [11], requires that two random values are drawn from an uniform distribution for each function. Given that N hash functions are created, there will be a total of 2N random values. This leads to a requirement of additional O(2N) space complexity and N multiplication operations.\nThe need for an optimization is justified since this procedure is often used on very large datasets [12] and with algorithms that requires a very large number of those hash functions [13] where a small speed up might imply on an economy of hours of processing.\nIn this paper it will be described an iterative procedure for generating any number of hash functions without the need of generating random numbers. This is done by exploiting the randomness of the features occurrence on sparse data sets. The speed up of this method will be experimently measured as well as the uniformity properties. In Section 2 it will be explained how minahshing works and the general algorithm. Section 3 will explain the proposed modifications, describe the pseudo-algorithm and show some initiall results regarding the uniformity of this approach. The performance and properties of such modification will be analysed and explored during the experimental setup of Section 4. Finally, in Section 5 it will be given some concluding remarks."}, {"heading": "2. Min-wise Independent Permutations Locality Sensitive Hashing", "text": "When the features of the studied data set can be described as sets, as it is the case on many semi-structure data (i.e., text documents), the similarity between two objects can be calculated through the Jaccard Index or Jaccard similarity:\nJ = |O1 \u2229 O2| |O1 \u222a O2| , (1)\nwhere O1 and O2 are the two objects compared and it returns a value between 0 and 1, with the latter meaning the two objects are equal.\nLet us illustrate this concept by using an example of comparing two text documents:\nd1 = \u201dThe cat ate the mouse\u201d d2 = \u201dThe mouse ate the cheese\u201d\nOne data strcuture used to represent these documents is an ordered set of terms contained in a document, or the bag-of-words representations:\nd1 = {ate, cat, mouse, the} d2 = {ate, cheese, mouse, the}\nThe Jaccard similarity between these two sets will be:\nJ(d1, d2) = |d1 \u2229 d2| |d1 \u222a d2| = 3 5 . (2)\nSince the intersection and union operations can be costly to compute, it can become very time consuming to perform pairwise similarity computation on a large data set with objects represented as a large set of features. As such, it is desirable to find a way to estimate the Jaccard index and reduce the objects representation to a smaller dimension.\nThe technique known as Min-wise independent permutations locality sensitive hashing (MinHash) [8], allows to quickly estimate the similarity of two sets approximating the Jaccard Index. It is based on the probability that, given a random permutation of two sets, the first element of both sets will be the same with a probability equal to their Jaccard Index.\nFollowing the same example, suppose the ordered set of terms is shuffled following a permutation \u03c0 and the documents are represented by:\nd1 = {the, mouse, cat, ate} d2 = {the, mouse, cheese, ate}\nThe probability that the first element of these sets are the same is equal to the ratio between the number of common elements and the number of total distinct elements, or:\nP(d1\u03c00 = d2 \u03c0 0) = |d1 \u2229 d2| |d1 \u222a d2| = 3 5 = J(d1, d2). (3)\nThis probability can be estimated by reshuffling the ordered set with N different permutations and counting the number Nequal of times that the first element are equal to both sets:\nP(d1\u03c00 = d2 \u03c0 0) \u2248 Nequal N \u2248 J(d1, d2). (4)\nSo, it is possible to estimate the Jaccard Index between two objects, described by a total of M features, by calculating N Minhashes, with N different permutations, for each document. Notice that the permutation must follow an uniform distribution.\nGiven that N < M, the cost of comparing two documents is reduced by a factor of NM . Since the cost of generating a permutations is O(M), the total computational cost will be O(N.(M + D.M\u0304)) where D is the number of documents and M\u0304 is the average number of features per document. One way to avoid this additional cost is by using an universal hash function such as one of those proposed in [11]:\nh(x) = a \u00b7 x + b mod P, (5)\nwhere a and b are randomly chosen with uniform distribution, and P is a large prime number. The variable x is the value to be hashed, i.e., a number associated with a given feature. The prime number should be at least as large as the number of features. This hash function will map each feature to an index in the range [0, P[. The random values will ensure that those indeces generate a random permutation of the feature set.\nNotice that with this function the application of MinHash is straightforward. For each object j, simply find, for each hash function i, the feature x that has the minimum value for hi(x):\nmhi(o j) = arg min x\n{ hi(x) | \u2200x \u2208 o j }\n(6)\nSince the probability that the minimum indices of two objects are the same, given a hash function, equals the Jaccard Index as well The complexity with this approach becomes O(N.D.M\u0304) with usually M\u0304 << M on sparse data sets."}, {"heading": "3. Iterative Procedure", "text": "By inspecting Eq. 5, the purpose of the random values a and b is to ensure uniformity of permutation given a sequence of features indeces. In other words, if we have M features, in M different permutations, each feature is expected to have the minimum hash value once.\nIn many real world applications the set of features describing each object is sparse (small percentage of the full feature set) and presents a randomness following a probability distribution. The randomness of features occurrence can be exploited to avoid the random values calculation. Let us first define the i-th hash function, of a sequence of N functions, as:\nhi = (a + i) \u00b7 x + i \u00b7 b mod P. (7)\nNotice that this is still the same universal hash function as before, holding the same properties. If we subtract hi\u22121 from hi in modulus P, we obtain:\n\u2206h = hi \u2212 hi\u22121 = [(a + i) \u00b7 x + i \u00b7 b mod P\n\u2212 (a + i \u2212 1) \u00b7 x + (i \u2212 1) \u00b7 b mod P]\n= (x + b) mod P,\nwhen P > a, b. So, given a value x to be hashed, and the value of the first hash function h0(x), we can obtain any number of hash functions by sequentially summing up \u2206h. This procedure is described in Alg. 1.\nThe main difference between the random approach and the iterative one is that the latter avoid the multiplication operation. This reduce the computational complexity in about O(N \u00b7 b \u00b7 logb \u00b7 loglogb) (but this varies depending\nAlgorithm 1: Iterative universal hash function generator. Data: the value x to be hashed, initial values for a\nand b, a large prime number P and the number N of hash functions.\nResult: vector H with the n hash values of x\nH[0] = a \u2217 x; \u2206h = b + x mod P; for i\u2190 1 to N do\nH[i] = H[i \u2212 1] + \u2206h mod P;\non the multiplication algorithm used). Also, as stated before, the space complexity is reduced from O(2N) to O(2) regarding the list of random numbers.\nNotice that this iterative procedure can also be simplified to a function H(i) = a\u00b7x+i\u00b7\u2206h mod P, so we can still paralellize the calculation of each H[i] if so is required. Additionally, when the algorithm is run on a distributed framework (i.e., MapReduce) the overhead of sending a vector of random numbers of size 2N throughout all of the machines is eliminated. If not for a significant speedup, then for a concise and clearer code.\nSince the rationale for the proof of universality of this hash function is the same as the random approach, in the next section it will be provided some empirical experiments in order to test the validity of these claims. It will be compared, between the random and the iterative approach, the uniformity of buckets distribution, the uniformity of choosing each feature independetly from the Minhashing procedure, the Jaccard estimation error and the computational time to generate a set of either hash functions."}, {"heading": "4. Experimental Results", "text": "This section will be devided into two subsections: the first one will test the uniformity of distribution of hashed values to m buckets and that the probabiliy of a value x be chosen as a Minahsh is uniform; the second will show that the estimation error of the Jaccard Index is similar between both approaches with a slightly advantage for the iterative proceudre and, finally, the obtained speed up."}, {"heading": "4.1. Uniformity of Distribution", "text": "The uniformity of an universal hash function states that, given a hash function with m buckets, the probability of a value x being assigned to a bucket with a random hash function h is proportional to 1/m. To verify if the family of Iterative Hash functions has this property, it was performed 100 independent experiments with a different value x randomly assigned in the range [0, P[, where P = 7, 757 was the prime number chosen, to be hashed in one of 100 different buckets. To evaluate the uniformity it was performed a \u03c7-squared test for each experiment with \u03b1 = 0.05 with 1, 000 hash functions. The choice for this number of hash functions is due to a recomendation that the expected count for each bucket should be 5 to 10 in order for the \u03c7-squared test to return an acurate response.\nSimilarly, for each experiment it was generated 100 random key values and the Minhash for each of the 1, 000 hash functions was calculated. The uniformity of the probability that a given key will be chosen as the Minhash was also tested with the \u03c7-squared test with \u03b1 = 0.05. Table 1 reports the percentage of the 100 experiments where the \u03c7-squared test pointed to a likely uniformity.\nFrom these results we can see that both family of hash functions are likely to generate an uniform distribution of buckets and a uniform distribution of choice for the Minhash. In order to simulate a more realistic scenario, where we can have thousands of possible features (i.e., text mining), the same tests were performed but with 5, 000 buckets and features and 50, 000 hash functions. The results are reported on Table 2.\nAs we can see, these results just confirms the previous experiment that both set of hash functions likely have the uniformity property and belongs to the universal family of hash functions."}, {"heading": "4.2. Hash Function Performance", "text": "Next the processing time for both approaches were compared in order to quantify the speedup obtained with the iterative hash function. For this end it was performed 100 independent experiments to generate and apply 100, 000 hash functions for a subset of 500 documents from the 20 Newsgroups dataset [14] and 10, 000 hash functions for a subset of 3, 891 documents from the Classic dataset [15]. This code was written in C++ with Boost Library and compiled with g++ 4.7.3 using just the optmization flag -O2 on an Intel i5 2.5GHz using a single core. The Operational System used was the Mint 15 Linux Distribution and the source code for all of the reported experiments can be found at https://github.com/folivetti/HBLCoClust/.\nIn Table 3 the average and standard-deviation of the time taken by every experiments are reported, it is possible to see that there is a speedup of 1.38, on average, for the 20-Newsgroup dataset and 1.25 for the Classic dataset, also a t-paired test was performed with \u03b1 = 0.05 which confirmed that the difference is statistically significant for both experiments. As the results point out, this is a significant improvement on the computational time that can chiefly benefit the computation of datasets with millions of objects. If, for example, an experiment using the traditional random approach takes 10 hours on a very large dataset, with the iterative procedure we can expect about 7 hours of processing.\nFinally, in order to see if the Jaccard coefficient estimation of both hash approaches are equivalent, the Minhashing estimation was performed on the same dataset with each object being represented by 5, 10, 15 hashes and the\nMean Absolute Error was calculate between each estimation and the real Jaccard Index.\nThe results from Table 4 shows a small difference of the mean absolute error obtained by each approach. It should be noticed that a t-test performed on each experiment revealed that the p-value for the experiments with 10 and 15 hashes were below 0.05 which means that, althought just slightly different, the means are unlikely the same. The p-value obtained on the 5 hashes experiments was much higher than 0.05, though."}, {"heading": "5. Conclusion", "text": "This paper proposed an optimization to the random universal hash function generator commonly used to estimate the Jaccard Coefficient between two sets. The proposal eliminates the need of generating two large lists of random values and the repetition of a multiplication operation throughout the calculation. This is done by changing the function creation to an iterative procedure where a given hash function hi(x) depends on the previously generated function plus an increment.\nAs a first concern, it was tested whether this iterative procedure keeps the universality properties of the random hash functions. Some experiments indicates that this hash function has uniformity of buckets allocation and Minhashing distribution, as such, it can be used as an universal hash function.\nNext, the speed up obtained with this procedure was measured and the average gain was between 1.25 to 1.38 from the time measured with the random approach. Additionally, it was shown, with another experiment, that the estimation error regarding the Jaccard Index, remained pratically the same, with a tiny advantage to the iterative approach.\nThese results show that the iterative procedure can have a significant impact when mining very large datasets and\nwhen using hundreds of hash functions. And, since it is just a simple modification, it can be easily adapted onto many existing source codes that already uses such hash functions."}], "references": [{"title": "On bias", "author": ["J.H. Friedman"], "venue": "variance, 0/1loss, and the curse-of-dimensionality, Data mining and knowledge discovery 1 (1) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "The Journal of Machine Learning Research 3 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Feature extraction: foundations and applications", "author": ["I. Guyon"], "venue": "Vol. 207, Springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Tutorial, Dimensionality reduction the probabilistic way, ICML2008 Tutorial", "author": ["I.N.D. Lawrence"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Feature subset selection by means of a bayesian artificial immune system", "author": ["P.A. Castro", "F.J. Von Zuben"], "venue": "in: Hybrid Intelligent Systems, 2008. HIS\u201908. Eighth International Conference on, IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Metaheuristics for feature selection: application to sepsis outcome prediction", "author": ["S.M. Vieira", "L.F. Mendon\u00e7a", "G.J. Farinha", "J.M. Sousa"], "venue": "in: Evolutionary Computation (CEC), 2012 IEEE Congress on, IEEE", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "R", "author": ["S. Har-Peled", "P. Indyk"], "venue": "Motwani, Approximate nearest neighbor: Towards removing the curse of dimensionality., Theory OF Computing 8 (1) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "On the resemblance and containment of documents", "author": ["A. Broder"], "venue": "in: Compression and Complexity of Sequences 1997. Proceedings", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Universal classes of hash functions", "author": ["J. Carter", "M.N. Wegman"], "venue": "Journal of Computer and System Sciences 18 (2) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1979}, {"title": "Locality sensitive hashing for similarity search using mapreduce on large scale data", "author": ["R. Szmit"], "venue": "in: Language Processing and Intelligent Information Systems, Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable overlapping co-clustering of word-document data", "author": ["F.O.D. Franca"], "venue": "in: Machine Learning and Applications (ICMLA), 2012 11th International Conference on, Vol. 1, IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Newsweeder: Learning to filter netnews", "author": ["K. Lang"], "venue": "in: Proceedings of the Twelfth International Conference on Machine Learning", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": "The need for such calculation may be demanding for large data sets with many features leading to the so called curse of dimensionality [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 1, "context": "Some methods used to reduce the dimensionality of a data set are Feature Selection [2], Feature Extraction [3] and Probabilistic Dimension Reduction [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "Some methods used to reduce the dimensionality of a data set are Feature Selection [2], Feature Extraction [3] and Probabilistic Dimension Reduction [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "Some methods used to reduce the dimensionality of a data set are Feature Selection [2], Feature Extraction [3] and Probabilistic Dimension Reduction [4].", "startOffset": 149, "endOffset": 152}, {"referenceID": 4, "context": "Since this is a combinatorial problem, it is mostly solved with meta-heuristic approaches [5, 6].", "startOffset": 90, "endOffset": 96}, {"referenceID": 5, "context": "Since this is a combinatorial problem, it is mostly solved with meta-heuristic approaches [5, 6].", "startOffset": 90, "endOffset": 96}, {"referenceID": 6, "context": "One of these techniques is called Locality-Sensitive Hashing [8, 9] and was successfuly applied to different applications including text and image retrieval.", "startOffset": 61, "endOffset": 67}, {"referenceID": 7, "context": "One of such technique is called Minhashing [10], and it was specifically crafted to approximate the Jaccard Index between two sets.", "startOffset": 43, "endOffset": 47}, {"referenceID": 8, "context": "One of such approaches, as devised by Carter and Wegman [11], requires that two random values are drawn from an uniform distribution for each function.", "startOffset": 56, "endOffset": 60}, {"referenceID": 9, "context": "The need for an optimization is justified since this procedure is often used on very large datasets [12] and with algorithms that requires a very large number of those hash functions [13] where a small speed up might imply on an economy of hours of processing.", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "The need for an optimization is justified since this procedure is often used on very large datasets [12] and with algorithms that requires a very large number of those hash functions [13] where a small speed up might imply on an economy of hours of processing.", "startOffset": 183, "endOffset": 187}, {"referenceID": 6, "context": "The technique known as Min-wise independent permutations locality sensitive hashing (MinHash) [8], allows to quickly estimate the similarity of two sets approximating the Jaccard Index.", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "One way to avoid this additional cost is by using an universal hash function such as one of those proposed in [11]:", "startOffset": 110, "endOffset": 114}, {"referenceID": 11, "context": "For this end it was performed 100 independent experiments to generate and apply 100, 000 hash functions for a subset of 500 documents from the 20 Newsgroups dataset [14] and 10, 000 hash functions for a subset of 3, 891 documents from the Classic dataset [15].", "startOffset": 165, "endOffset": 169}], "year": 2014, "abstractText": "Minhashing is a technique used to estimate the Jaccard Index between two sets by exploiting the probability of collision in a random permutation. In order to speed up the computation, a random permutation can be approximated by using an universal hash function such as the ha,b function proposed by Carter and Wegman. A better estimate of the Jaccard Index can be achieved by using many of these hash functions, created at random. In this paper a new iterative procedure to generate a set of ha,b functions is devised that eliminates the need for a list of random values and avoid the multiplication operation during the calculation. The properties of the generated hash functions remains that of an universal hash function family. This is possible due to the random nature of features occurrence on sparse datasets. Results show that the uniformity of hashing the features is maintaned while obtaining a speed up of up to 1.38 compared to the traditional approach.", "creator": "LaTeX with hyperref package"}}}