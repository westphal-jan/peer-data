{"id": "1401.6131", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Controlling Complexity in Part-of-Speech Induction", "abstract": "We consider the problem of fully unsupervised learning of grammatical (part-of-speech) categories from unlabeled text. The standard maximum-likelihood hidden Markov model for this task performs poorly, because of its weak inductive bias and large model capacity. We address this problem by refining the model and modifying the learning objective to control its capacity via para- metric and non-parametric constraints. Our approach enforces word-category association sparsity, adds morphological and orthographic features, and eliminates hard-to-estimate parameters for rare words. We develop an efficient learning algorithm that is not much more computationally intensive than standard training. We also provide an open-source implementation of the algorithm. Our experiments on five diverse languages (Bulgarian, Danish, English, Portuguese, Spanish) achieve significant improvements compared with previous methods for the same task.", "histories": [["v1", "Thu, 16 Jan 2014 05:20:08 GMT  (499kb)", "http://arxiv.org/abs/1401.6131v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["jo\\~ao v gra\\c{c}a", "kuzman ganchev", "luisa coheur", "fernando pereira", "ben taskar"], "accepted": false, "id": "1401.6131"}, "pdf": {"name": "1401.6131.pdf", "metadata": {"source": "CRF", "title": "Controlling Complexity in Part-of-Speech Induction", "authors": ["Jo\u00e3o V. Gra\u00e7a", "Kuzman Ganchev", "Lu\u00edsa Coheur", "Fernando Pereira", "Ben Taskar"], "emails": ["JOAO.GRACA@L2F.INESC-ID.PT", "KUZMAN@GOOGLE.COM", "LUISA.COHEUR@L2F.INESC-ID.PT", "PEREIRA@GOOGLE.COM", "TASKAR@CIS.UPENN.EDU"], "sections": [{"heading": null, "text": "egories from unlabeled text. The standard maximum-likelihood hidden Markov model for this task performs poorly, because of its weak inductive bias and large model capacity. We address this problem by refining the model and modifying the learning objective to control its capacity via parametric and non-parametric constraints. Our approach enforces word-category association sparsity, adds morphological and orthographic features, and eliminates hard-to-estimate parameters for rare words. We develop an efficient learning algorithm that is not much more computationally intensive than standard training. We also provide an open-source implementation of the algorithm. Our experiments on five diverse languages (Bulgarian, Danish, English, Portuguese, Spanish) achieve significant improvements compared with previous methods for the same task."}, {"heading": "1. Introduction", "text": "Part-of-speech (POS) categories are elementary building blocks for the syntactic analysis of text that play an important role in many natural-language-processing tasks, from machine translation to information extraction. While English and a handful of other languages are fortunate enough to have comprehensive POS-annotated corpora such as the Penn Treebank (Marcus, Marcinkiewicz, & Santorini, 1993), most of the worlds\u2019 languages have extremely limited linguistic resources. It is unrealistic to expect annotation efforts to catch up with the explosion of unlabeled electronic text anytime soon. This lack of supervised data will likely persist in the near future because of the investment required for accurate linguistic annotation: it took two years to annotate 4,000 sentences with syntactic parse trees for the Chinese Treebank (Hwa, Resnik, Weinberg, Cabezas, & Kolak, 2005) and four to seven years to annotate 50,000 sentences across a range of languages (Abeill\u00e9, 2003).\nc\u00a92011 AI Access Foundation. All rights reserved.\nSupervised learning of taggers from POS-annotated training text is a well-studied task, with several methods achieving near-human tagging accuracy (Ratnaparkhi, 1996; Toutanova, Klein, Manning, & Singer, 2003; Shen, Satta, & Joshi, 2007). However, POS induction \u2013 where one does not have access to a labeled corpus \u2013 is a more difficult task with much room for improvement. In recent literature, POS induction has been used to refer to two different tasks. For the first one, in addition to raw text, we are given a dictionary containing the possible tags for each word and the goal is to disambiguate the tags of a particular word occurrence (Merialdo, 1994). For the second task, we are given raw text, but no dictionary is provided; the goal is to cluster words that have the same grammatical behavior. In this work, we target this latter, more challenging, unsupervised POS induction task.\nRecent work on this task typically relies on distributional or morphological features, since words with the same grammatical function tend to occur in similar contexts and to have common morphology (Brown, deSouza, Mercer, Pietra, & Lai, 1992; Sch\u00fctze, 1995; Clark, 2003). However, those statistical regularities are not enough to overcome several challenges. First, the algorithm has to decide how many clusters to use for broad syntactic categories (for instance, whether to distinguish between plural and singular nouns). Second, category size distribution tends to be uneven. For example, the vast majority of the word types are open class (nouns, verbs, adjectives), and even among open class categories, there are many more nouns than adjectives. This runs contrary to the learning biases in commonly-used statistical models. A common failure of those models is to clump several rare categories together and split common categories.\nFor individual word types, a third challenge arises from ambiguity in grammatical role and word sense. Many words can take on different POS tags in different occurrences, depending on the context of occurrence (the word run can be either a verb or a noun). Some approaches assume (for computational and statistical simplicity) that each word can only have one tag, aggregating all local contexts through distributional clustering (Sch\u00fctze, 1995). While this one-tag-per-word assumption is clearly wrong, across many languages for which we have annotated corpora, such methods perform competitively with methods that can assign different tags to the same word in different contexts (Lamar, Maron, Johnson, & Bienenstock, 2010). This is partly due to the typical statistical dominance of one of the tags for a word, especially if the corpus includes a single genre, such as news. The other reason is that less restrictive models do not encode the useful bias that most words typically take on a very small number of tags.\nMost approaches that do not make the one-tag-per-word assumption take the form of a hidden Markov model (HMM) where the hidden states represent word classes and the observations are word sequences (Brown et al., 1992; Johnson, 2007). Unfortunately, standard HMMs trained to maximize likelihood perform poorly, since the learned hidden classes do not align well with true POS tags. Besides the potential model estimation errors due to non-convex optimization involved in training, there is a more pernicious problem. Typical maxima of likelihood do not align well with maxima of POS tag accuracy (Smith & Eisner, 2005; Gra\u00e7a, Ganchev, Pereira, & Taskar, 2009), suggesting serious mismatch between model and data.\nIn this work, we significantly reduce that modeling mismatch by combining three ideas:\n\u2022 The standard HMM treats words as atomic units, without using orthographic and morphological information. That information is critical to generalization in many languages (Clark, 2003). To address this problem, we reparameterize the standard HMM by replacing the multinomial emission distributions by maximum-entropy models (similar to the work of Berg-\nKirkpatrick, Bouchard-C\u00f4t\u00e9, DeNero, & Klein, 2010 and Gra\u00e7a, 2010). This allows the use of orthographic and morphological features in the emission model. Moreover, the standard HMM model has a very large number of parameters: the number of tags times the number of word types. This presents an extremely rich model space capable of fitting irrelevant correlations in the data. To address this problem we dramatically reduce the number of parameters of the model by discarding features with small support in the corpus, that is, those involving rare words or word parts.\n\u2022 The HMM model allows a high level of ambiguity for the tags of each word. As a result, when maximizing the marginal likelihood, common words typically tend to be associated with every tag with some non-trivial probability (Johnson, 2007). However, a natural property of POS categories across many languages and annotation standards is that each word only has a small number of allowed tags. To address this problem we use the posterior regularization (PR) framework (Gra\u00e7a, Ganchev, & Taskar, 2007; Ganchev, Gra\u00e7a, Gillenwater, & Taskar, 2010) to constrain the ambiguity of word-tag associations via a sparsity-inducing penalty on the model posteriors (Gra\u00e7a et al., 2009).\nWe show that each of the proposed extensions improves the standard HMM performance, and moreover, that the gains are nearly additive. The improvements are significant across different metrics previously proposed for this task. For instance, for the 1-Many metric, our method attains a 10.4% average improvement over the regular HMM. We also compare the proposed method with eleven previously proposed approaches. For all languages but English and all metrics except 1-1, our method achieves the best published results. Furthermore, our method appears the most stable across different testing scenarios and always shows competitive results. Finally, we show how the induced tags can be used to improve the performance of a supervised POS tagging system in a limited labeled data scenario. Our open-source software for POS induction and evaluation is available at http://code.google.com/p/pr-toolkit/.\nThis paper is organized as follows. Section 2 describes the basic HMM for POS induction and its maximum-entropy extension. Section 3 describes standard EM and our sparsity-inducing estimation method. Section 4 presents a comprehensive survey of previous fully unsupervised POS induction methods. In Section 5 we provide a detailed experimental evaluation of our method. Finally, in Section 6, we summarize our results and suggest ideas for future work."}, {"heading": "2. Models", "text": "The model for all our experiments is based on a first order HMM. We denote the sequence of words in a sentence as boldface x and the sequence of hidden states which correspond to partof-speech tags as boldface y. For a sentence of length l, we have thus l hidden state variables yi \u2208 {1, . . . , J}, 1 \u2264 i \u2264 l where J is the number of possible POS tags, and l observation variables xi \u2208 {1, . . . , V }, 1 \u2264 i \u2264 l, where V is the number of word types. To simplify notation, we assume that every tag sequence is prefixed with the conventional start tag y0 = start, allowing us to write as p(y1|y0) the initial state probability of the HMM.\nThe probability of a sentence x along with a particular hidden state sequence y is given by:\np(x,y) = l\u220f i=1 pt(yi | yi\u22121)po(xi | yi), (1)\nwhere po(xi | yi) is the probability of observing word xi given that we are in state yi (emission probability), and pt(yi | yi\u22121) is the probability of being in state yi, given that the previous hidden state was yi\u22121 (transition probability)."}, {"heading": "2.1 Multinomial Emission Model", "text": "Standard HMMs use multinomial emission and transition probabilities. That is, for a generic word xi and tag yi, the observation probability po(xi | yi) and the transition probability pt(yi | yi\u22121) are multinomial distributions. In the experiments we refer to this model simply as the HMM. This model has a very large number of parameters because of the large number of word types (see Table 1). A common convention we follow is to lowercase words as well as to map words occurring only once in the corpus to a special token \u2018unk\u2019."}, {"heading": "2.2 Maximum Entropy Emission Model", "text": "In this work, we use a simple modification of the HMM model discussed in the previous section: we represent conditional probability distributions as maximum entropy (log-linear) models. Specifically, the emission probability is expressed as:\npo(x|y) = exp(\u03b8 \u00b7 f(x, y))\u2211 x\u2032 exp(\u03b8 \u00b7 f(x\u2032, y))\n(2)\nwhere f(x, y) is a feature function, x ranges over all word types, and \u03b8 are the model parameters. We will refer to this model as HMM+ME. In addition to word identity, features include orthographyand morphology-inspired cues such as presence of capitalization, digits, and common suffixes. The feature sets are described in Section 5. The idea of replacing the multinomial models of an HMM by maximum entropy models is not new and has been applied before in different domains (Chen, 2003), as well as in POS induction (Berg-Kirkpatrick et al., 2010; Gra\u00e7a, 2010). A key advantage of this representation is that it allows for a much tighter control over the expressiveness of the model. For many languages it is helpful to exclude word identity features for rare words in order to constrain the model and force generalization across words with similar features. Unlike mapping all rare words to the \u2018unk\u2019 token in the multinomial setting, the maxent model still captures some information about the word through the other features. Moreover, we can reduce the number of parameters even further by using lowercase word identities while still keeping the case information by using a case feature. Table 1 shows the number of features we used for different corpora. Note that the reduced feature set has an order of magnitude fewer parameters than the multinomial model."}, {"heading": "3. Learning", "text": "In Section 5 we describe experiments comparing the HMM model to the ME model under three learning scenarios: maximum likelihood training using the EM algorithm (Dempster, Laird, & Rubin, 1977) for both HMM and HMM+ME, gradient-based likelihood optimization for the HMM+ME model, and PR with sparsity constraints (Gra\u00e7a et al., 2009) for both HMM and HMM+ME. This section describes all three learning algorithms.\nIn the following, we denote the whole corpus, a list of sentences, by X = (x1,x2, . . . ,xN ) and the corresponding tag sequences by Y = (y1,y2, . . . ,yN )."}, {"heading": "3.1 Maximum Likelihood with EM", "text": "Standard HMM training seeks model parameters \u03b8 that maximize the log-likelihood of the observed data: Log-Likelihood: L(\u03b8) = log \u2211 Y p\u03b8(X,Y) (3)\nwhere X is the whole corpus. Since the model assumes independence between sentences ,\nlog \u2211 Y p\u03b8(X,Y) = N\u2211 n=1 log \u2211 yn p\u03b8(x n,yn), (4)\nbut we use the corpus notation for consistency with Section 3.3. Because of the latent variables Y, the log-likelihood function for the HMM model is not convex in the model parameters, and the model is fitted using the EM algorithm. EM maximizes L(\u03b8) via block-coordinate ascent on a lower bound F (q, \u03b8) using an auxiliary distribution over the latent variables q(Y) (Neal & Hinton, 1998). By Jensen\u2019s inequality, we define a lower-bound F (q, \u03b8) as:\nL(\u03b8) = log \u2211 Y q(Y) p\u03b8(X,Y) q(Y) \u2265 \u2211 Y q(Y) log p\u03b8(X,Y) q(Y) = F (q, \u03b8). (5)\nWe can rewrite F (q, \u03b8) as:\nF (q, \u03b8) = \u2211 Y q(Y) log(p\u03b8(X)p\u03b8(Y|X))\u2212 \u2211 Y q(Y) log q(Y) (6)\n= L(\u03b8)\u2212 \u2211 Y q(Y) log q(Y) p\u03b8(Y|X) (7) = L(\u03b8)\u2212KL(q(Y)||p\u03b8(Y|X)). (8)\nUsing this interpretation, we can view EM as performing coordinate ascent on F (q, \u03b8). Starting from an initial parameter estimate \u03b80, the algorithm iterates two block-coordinate ascent steps until a convergence criterion is reached:\nE : qt+1 = argmax q F (q, \u03b8t) = argmin q KL(q(Y) \u2016 p\u03b8t(Y | X)) (9)\nM : \u03b8t+1 = argmax \u03b8 F (qt+1, \u03b8) = argmax \u03b8 Eqt+1 [log p\u03b8(X,Y)] (10)\nThe E-step corresponds to maximizing Eq. 8 with respect to q and the M-step corresponds to maximizing Eq. 6 with respect to \u03b8. The EM algorithm is guaranteed to converge to a local maximum of L(\u03b8) under mild conditions (Neal & Hinton, 1998). For an HMM POS tagger, the E-Step computes the posteriors p\u03b8t(y|x) over the latent variables (POS tags) given the observed variables (words) and current parameters \u03b8t for each sentence. This is accomplished by the forwardbackward algorithm for HMMs. The EM algorithm together with the forward-backward algorithm for HMMs is usually referred to as the Baum\u2013Welch algorithm (Baum, Petrie, Soules, & Weiss, 1970).\nThe M step uses qt+1 (qt+1n are the posteriors for a given sentence) to \u201cfill in\u201d the values of tags Y and estimate parameters \u03b8t+1. Since the HMM model is locally normalized and the features used\nonly depend on the tag and word identities and not on the particular position where they occur, the optimization decouples in the following way:\nEqt+1 [log p\u03b8(X,Y)] = N\u2211 n=1 Eqt+1n [log ln\u220f i=1 pt(y n i | yni\u22121)po(xni | yni )] (11)\n= N\u2211 n=1 ln\u2211 i=1 ( Eqt+1n log pt(y n i | yni\u22121) +Eqt+1n log po(x n i | yni ) ) (12)\nFor the multinomial emission model, this optimization is particularly easy and simply involves normalizing (expected) counts for each parameter. For the maximum-entropy emission model parameterized as in Equation 2, there is no closed form solution so we need to solve an unconstrained optimization problem. For each possible hidden tag value y we have to solve two problems: estimate the emission probabilities po(x|y) and estimate the transition probabilities pt(y\u2032|y), where the gradient for each one of those is given by\n\u2202Eqt+1 [log p\u03b8(X,Y)]\n\u2202\u03b8 = Eqt+1\n[ f(X,Y)\u2212Ep\u03b8(X\u2032|Y)[f(X \u2032,Y)] ] , (13)\nwhich is similar to the gradient in supervised ME models, except for the expectation over all Y under qt+1(Y) instead of observed Y. The optimization is done using L-BFGS with Wolfe\u2019s rule line search (Nocedal & Wright, 1999)."}, {"heading": "3.2 Maximum Likelihood with Direct Gradient", "text": "While likelihood is traditionally optimized with EM, Berg-Kirkpatrick et al. (2010) find that for the HMM with the maximum entropy emission model, higher likelihood and better accuracy can be achieved by with a gradient-based likelihood-optimization method. They use L-BFGS in their experiments. The derivative of the likelihood is,\n\u2202L(\u03b8) \u2202\u03b8 = \u2202 \u2202\u03b8 log p\u03b8(X) =\n1\np\u03b8(X)\n\u2202\n\u2202\u03b8 p\u03b8(X) =\n1\np\u03b8(X)\n\u2202\n\u2202\u03b8 \u2211 Y p\u03b8(X,Y) (14)\n= \u2211 Y 1 p\u03b8(X) \u2202 \u2202\u03b8 p\u03b8(X,Y) = \u2211 Y p\u03b8(X,Y) p\u03b8(X) \u2202 \u2202\u03b8 log p\u03b8(X,Y) (15)\n= \u2211 Y p\u03b8(Y|X) \u2202 \u2202\u03b8 log p\u03b8(X,Y), (16)\nwhich is exactly the same as the derivative of the M-Step. Here in Equation 14 we apply the chain rule to take the derivative of log p\u03b8(X), while in Equation 15 we apply the chain rule in the reverse direction. The biggest difference between the EM procedure and direct gradient is that for EM we fix the counts on the E-Step and optimize the ME model using those counts. When directly optimizing the likelihood we need to recompute the counts for each parameter setting, which can be expensive. Appendix A gives a more detailed discussion of both methods."}, {"heading": "3.3 Controlling Tag Ambiguity with PR", "text": "One problem with unsupervised HMM POS tagging is that the maximum likelihood objective may encourage tag distributions that allow many different tags for a word in a given context. We do not\nfind that in actual text with linguist-designed tags, because tags are designed to be informative about the word\u2019s grammatical role. In the following paragraphs we describe a measure of tag ambiguity proposed by Gra\u00e7a et al. (2009) that we will attempt to control. It is easier to understand this measure with hard tag assignments, so we start with that and thene extend the discussion to distributions over tags.\nConsider a word such as \u201cstock\u201d. Intuitively, we would like all occurrences of \u201cstock\u201d to be tagged with a small subset of all possible tags (noun and verb, in this case). For a hard assignment of tags to the entire corpus, Y, we could count how many different tags are used in Y for occurrences of the word \u201cstock.\u201d\nIf instead of a single tagging of the corpus, we have a distribution q(Y) over assignments, we need to generalize this ambiguity measure. Instead of asking was a particular tag ever used for the word \u201cstock\u201d, we would ask what is the maximum probability with which a particular tag was used for the word \u201cstock\u201d. Then instead of counting the number of tags, we would sum these probabilities.\nAs motivation, Figure 1 shows the distribution of tag ambiguity across words for two corpora. As we see from Figure 1, when we train using the EM procedure described in Section 3.1, the HMM and ME models grossly overestimates the tag ambiguity of almost all words. However when the same models are trained using PR to penalize the tag ambiguity, both models (HMM+Sp, HMM+ME+Sp) achieve a tag ambiguity closer to the truth.\nMore formally, Gra\u00e7a et al. (2009) define this measure in terms of constraint features \u03c6(X,Y). Constraint feature \u03c6wvj(X,Y) takes on value 1 if the jth occurrence of word typew in X is assigned to tag v in the tag assignment Y. Consequently, the probability that the jth occurrence of word w has tag v under the label distribution q(Y) is Eq[\u03c6wvj(X,Y)]. The ambiguity measurement for word type w becomes:\nAmbiguity Penalty for word type w: \u2211 v max j Eq(Y) [\u03c6wvj(X,Y)] . (17)\nThis sum of maxima is also called the `1/`\u221e mixed norm. For brevity we use the norm notation ||Eq[\u03c6w]||1/\u221e. For computational reasons, we do not add a penalty term based on the ambiguity of the model distribution p\u03b8(Y|X), but instead introduce an auxiliary distribution q(Y) which\nmust be close to p\u03b8 but also must have low ambiguity. Our modified objective becomes\nmax \u03b8,q L(\u03b8)\u2212KL(q(Y)||p\u03b8(Y|X))\u2212 \u03c3 \u2211 w ||Eq[\u03c6w(X,Y)]||1/\u221e . (18)\nGra\u00e7a et al. (2009) optimize this objective using an algorithm very similar to EM. The added complexity of implementing their algorithm lies only in computing the Kullback-Leibler projection in a modified E-Step. However, this computation involves choosing a distribution over exponentially many objects (label assignments). Luckily, Gra\u00e7a et al. (2009) show that the dual formulation for the E-Step is more manageable. This is given by:\nmax \u03bb\u22650\n\u2212 log (\u2211 Y p\u03b8(Y|X) exp(\u2212\u03bb \u00b7 \u03c6(X,Y)) ) s. t. \u2211 j \u03bbwvj \u2264 \u03c3 (19)\nwhere \u03bb is the vector of dual parameters \u03bbwvj , one for each \u03c6wvj . The projected distribution is then given by: q(Y) \u221d p\u03b8(Y|X) exp (\u2212\u03bb \u00b7 \u03c6(X,Y)). Note that when p\u03b8 is given by an HMM, q for each sentence can be expressed as\nq(yn) \u221d I\u220f i=1 pt(y n i | yni\u22121)qo(xni | yni ), (20)\nwhere qo(xi|yi) = po(xi|yi) exp(\u2212\u03bbxiyij) act as modified (unnormalized) emission probabilities. The objective of Equation 19 is just the negative sum of the log probabilities of all the sentences under q plus a constant. We can compute this by running forward-backward on the corpus, similar to the E-Step in normal EM. The gradient of the objective is also computed using the forwardbackward algorithm. Note that the objective in Eq. 19 is concave with respect to \u03bb and can be optimized using a variety of methods. We perform the dual optimization by projected gradient, using the fast simplex projection algorithm for \u03bb described by Bertsekas, Homer, Logan, and Patek (1995). In our experiments we found that taking a few projected gradient steps was not enough, and performing the optimization until convergence helps the results."}, {"heading": "4. Related Work", "text": "POS tags place words into classes that share some commonalities as to what other (classes of) words they cooccur with. Therefore, it is natural to ask whether word clustering methods based on word context distributions might be able to recover the word classification inherent in a POS tag set. Several influential methods, most notably mutual-information clustering (Brown et al., 1992), have been used to cluster words according to how their immediately contiguous words are distributed. Although those methods were not explicitly designed for POS induction, the resulting clusters capture some syntactic information (see also Martin, Liermann, & Ney, 1998, for a different method with a similar objective). Clark (2003) refined the distributional clustering approach by adding morphological and word frequency information, to obtain clusters that more closely resemble POS tags.\nOther forms of distributional clustering go beyond the immediate neighbors of a word to represent a whole vector of coocurrences with the target word within a text window, and compare those vectors using some suitable metric, such as cosine similarity. However, these wider-range similarities have problems in capturing more local regularities. For instance, an adjective and a noun might\nlook similar if the noun tends to be used in noun-noun compounds; similarly, two adjectives with different semantics or selectional preferences might be used with different contexts. Moreover, this problem is aggravated with data sparsity. As an example, infrequent adjectives that modify different nouns tend to have completely disjoint context vectors (but even frequent words like \u201ca\u201d and \u201can\u201d might have completely different context vectors, since these articles are used in disjoint right contexts). To alleviate these problems, Sch\u00fctze (1995) used frequency cutoffs, singular-value decomposition of co-occurrence matrices, and approximate co-clustering through two stages of SVD, with the clusters from the first stage used instead of individual words to provide vector representations for the second-stage clustering.\nLamar, Maron and Johnson (2010) have recently revised the two-stage SVD model of Sch\u00fctze (1995) and achieve close to state-of-the-art performance. The revisions are relatively small, but touch several important aspects of the model: singular vectors are scaled by their singular values to preserve the geometry of the original space; latent descriptors are normalized to unit length; and cluster centroids are computed as a weighted average of their constituent vectors based on the word frequency, so that rare and common words are treated differently and centroids are initialized in a deterministic manner.\nA final class of approaches \u2013 which include the work in this paper \u2013 uses a sequence model, such as an HMM, to represent the probabilistic dependencies between consecutive tags. In these approaches, each observation corresponds to a particular word and each hidden state corresponds to a cluster. However, as noted by Clark (2003) and Johnson (2007), using maximum likelihood training for such models does not achieve good results: maximum likelihood training tends to result in very ambiguous distributions for common words, in contradiction with the rather sparse wordtag distribution. Several approaches have been proposed to mitigate this problem. Freitag (2004) clusters the most frequent words using a distributional approach and co-clustering. To cluster the remaining (infrequent) words, the author trains a second-order HMM where the emission probabilities for the frequent words are fixed to the clusters found earlier and emission probabilities for the remaining words are uniform.\nSeveral studies propose using Bayesian inference with an improper Dirichlet prior to favor sparse model parameters and hence indirectly reduce tag ambiguity (Johnson, 2007; Gao & Johnson, 2008; Goldwater & Griffiths, 2007). This was further refined by Moon, Erk, and Baldridge (2010) by representing explicitly the different ambiguity patterns of function and content words. Lee, Haghighi, and Barzilay (2010) take a more direct approach to reducing tag ambiguity by explicitly modeling the set of possible tags for each word type. Their model first generates a tag dictionary that assigns mass to only one tag for each word type to reflect lexicon sparsity. This dictionary is then used to constrain a Dirichlet prior from which the emission probabilities are drawn by only having support for word-tag pairs in the dictionary. Then a token-level HMM using those emission parameters and transition parameters draw from a symmetric Dirichlet prior are used for tagging the entire corpus. The authors also show improvements by using morphological features when creating the dictionary. Their system achieves state-of-art results for several languages. It should be noted that a common issue with the above sparsity-inducing approaches is that sparsity is imposed at the parameter level, the probability of word given tag, while the desired sparsity is at the posterior level, the probability of tag given word. Gra\u00e7a et al. (2009) use the PR framework to penalize ambiguous posteriors distributions of words given tokens, which achieves better results than the Bayesian sparsifying Dirichlet priors.\nMost recently, Berg-Kirkpatrick et al. (2010) and Gra\u00e7a (2010) proposed replacing the multinomial distributions of the HMM by maximum entropy (ME) distributions. This allows the use of features to capture morphological information, and achieve very promising results. Berg-Kirkpatrick et al. (2010) also find that optimizing the likelihood with L-BFGS rather than EM leads to substantial improvements, which we show not to be the case beyond English.\nWe also note briefly POS induction methods that rely on a prior tag dictionary indicating for each word type what POS tags it can have. The POS induction task is then, for each word token in the corpus, to disambiguate between the possible POS tags, as described by Merialdo (1994). Unfortunately, the availability of a large manually-constructed tag dictionary is unrealistic and much of the later work tries to reduce the required dictionary size in different ways, by generalizing from a small dictionary with only a handful of entries (Smith & Eisner, 2005; Haghighi & Klein, 2006; Toutanova & Johnson, 2007; Goldwater & Griffiths, 2007). However, although this approach greatly simplifies the problem \u2013 most words can only have one tag and, furthermore, the cluster-tag mappings are predetermined, thus removing an extra level of ambiguity \u2013 the accuracy of such methods is still significantly behind supervised methods. To address the remaining ambiguity by imposing additional sparsity, Ravi and Knight (2009) minimize the number of possible tag-tag transitions in the HMM via a integer program. Finally, Snyder, Naseem, Eisenstein, and Barzilay (2008) jointly train a POS induction system over parallel corpora in several languages, exploiting the fact that different languages present different ambiguities."}, {"heading": "5. Experiments", "text": "In this section we present encouraging results validating the proposed method in six different testing scenarios according to different metrics. The highlights are:\n\u2022 A maximum-entropy emission model with a Markov transition model trained with the ambiguity penalty improves over the regular HMM in all cases with an average improvement of 10.4% (according to the 1-Many metric).\n\u2022 When compared against a broad range of recent POS induction systems, our method produces the best results for all languages except English. Furthermore, the method seems less sensitive to particular test conditions than previous methods.\n\u2022 The induced clusters are useful features in training supervised POS taggers, improving test accuracy as much or more than the clusters learned by competing methods."}, {"heading": "5.1 Corpora", "text": "In our experiments we test several POS induction methods on five languages with the help of manually POS-tagged corpora for those languages. Table 1 summarizes characteristics of the test corpora: the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993) (we consider both the 17-tag version of Smith & Eisner, 2005 (En17) and the 45-tag version (En45)); the Bosque subset of the Portuguese Floresta Sinta(c)tica Treebank (Afonso, Bick, Haber, & Santos, 2002) (Pt); the Bulgarian BulTreeBank (Simov et al., 2002) (Bg) (with only the 12 coarse tags); the Spanish corpus from the Cast3LB treebank (Civit & Mart\u00ed, 2004) (Es); and the Danish Dependency Treebank (DDT) (Kromann, Matthias T., 2003) (Dk)."}, {"heading": "5.2 Experimental Setup", "text": "We compare our work with two kinds of methods: those that induce a single cluster for each word type (type-level tagging), and those that allow different tags on different occurrences of a word type (token-level tagging). For type-level tagging, we use two standard baselines, BROWN and CLARK, as described by Brown et al. (1992)1 and Clark (2003)2. Following Headden, McClosky, and Charniak (2008), we trained the CLARK system with both 5 and 10 hidden states for the letter HMM and ran it for 10 iterations; the BROWN system was run according with the instructions accompanying the code. We also ran the recently proposed LDC system (Lamar, Maron, & Bienenstock, 2010)3, with the configuration described in their paper for PTB45 and PTB17, and the PTB17 configuration for the other corpora. It should be noted that we did not carry out our experiments with the SVD2 system (Lamar, Maron and Johnson, 2010), since SVD2 is superseded by LDC according to its authors.\nFor token-level tagging, we experimented with the feature-rich HMM as presented by BergKirkpatrick et al. (2010), trained both using EM training (BK+EM) and direct gradient (BK+DG), using the configuration provided by the authors4. We report results from the type-level HMM (TLHMM) (Lee et al., 2010) when applicable, since we were not able to run that system. Moreover, we compared those systems against our own implementation of various HMM-based approaches: the HMM with a multinomial emission probabilities (Section 2.1), the HMM with maximumentropy emission probabilities (Section 2.2) trained with EM (HMM+ME), trained by direct gradient (HMM+ME+DG), and trained using PR with the ambiguity penalty, as described in Section 3.3 (HMM+Sp for multinomial emissions, and HMM+ME+Sp for maximum-entropy emissions). In addition, we also compared to a multinomial HMM with a sparsifying Dirichlet prior on the parameters (HMM+VB) trained using variational Bayes (Johnson, 2007).\nFollowing standard practice, for the multinomial HMMs that do not use morphological information, we lowercase the corpora and replace unique words by a special unknown token, as this improves the multinomial HMM results by decreasing the number of parameters and eliminating\n1. Implementation: http://www.cs.berkeley.edu/~pliang/software/brown-cluster-1.2.zip 2. Implementation: http://www.cs.rhul.ac.uk/home/alexc/pos2.tar.gz 3. Implementation provided by Lamar, Maron and Bienenstock (2010). 4. Implementation provided by Berg-Kirkpatrick et al. (2010).\nvery rare words (mostly nouns). Since the maximum-entropy emission models have access to morphological features, these preprocessing steps do not improve performance and we do not perform them in that case.\nAt the start of EM, we randomly initialize all of our implementations of HMM-based models from the same posteriors, obtained by running the E-step of the HMM model with a set of random parameters: close to uniform with random uniform \u201cjitter\u201d of 0.01. This means that for each random seed, the initialization is identical for all models.\nFor EM and variational Bayes training, we train the model for 200 iterations, since we found that typically most models tend to converge by iteration 100. For the HMM+VB model we fix the transition prior5 \u03b1 to 0.001 and test an emission prior \u03b1 equal to 0.1 and 0.001, corresponding to the best values reported by Johnson (2007).\nFor PR training, we initialize with 30 EM iterations and then run for 170 iterations of PR, following Gra\u00e7a et al. (2009). We used the results that worked best for English (En17) (Gra\u00e7a et al., 2009), regularizing only words that occur at least 10 times, with \u03c3 = 32, and use the same configuration for all the other scnenarios. This setting was not specifically tuned for the test languages, and might not be optimal for every language. Setting such parameters in an unsupervised manner is a difficult task and we do not address it here (Gra\u00e7a, 2010 discusses more experiments with different values of those parameters).\nWe obtain hard assignments using posterior decoding, where for each position we pick the label with highest posterior probability, since this showed small but consistent improvements over Viterbi decoding. For all experiments that required random initialization of the parameters we report the average of 5 random seeds.\nAll experiments were run using the number of true tags as the number of clusters, with results obtained in the test set portion of each corpus. We evaluate all systems using four common metrics for POS induction: 1-Many mapping, 1-1 mapping (Haghighi & Klein, 2006), variation of information (VI) (Meila\u0306, 2007), and validity measure (V) (Rosenberg & Hirschberg, 2007). These metrics are described in detail in Appendix B."}, {"heading": "5.3 HMM+ME+Sp Performance", "text": "This section compares the gains from using a feature-rich representation with those from the ambiguity penalty, as described in Section 3.3. Experiments show that having a feature-rich representation always improves performance, and that having an ambiguity penalty also always improves performance. Then, we will see that the improvements from the two methods combine additively, suggesting that they address independent aspects of POS induction.\nWe use two different feature sets: the large feature set is that of Berg-Kirkpatrick et al. (2010), while the reduced feature set was described by Gra\u00e7a (2010). We apply count-based feature selection to both the identity and suffix features. Specifically, we only add identity features for words occurring at least 10 times and suffix features for words occurring at least 20 times. We also add a punctuation feature. In what follows, we refer to the large feature set as feature set 1 and the reduced feature set as 2. The total number of features for each model and language is given in Table 1. The results of these experiments are summarized in Table 2.\nTable 2 shows the results for 10 training methods across six corpora and four evaluation metrics, resulting in 240 experimental conditions. To simplify the discussion, we focus on the 1-Many metric\n5. The transition prior does not significantly affect the results, and we do not report results with different values.\n(top left tab of Table 2), and just observe that the conclusions hold for the other three evaluation metrics also. From Table 2 we can conclude the following:\n\u2022 Adding a penalty for high word-tag ambiguity improves the performance of the multinomial HMM. The multinomial HMM trained with EM (line 1 in Table 2) is always worse than the multinomial HMM trained with PR and an ambiguity penalty, by 6.5% on average (line 2 in Table 2).\n\u2022 The feature-rich maximum entropy HMMs (lines 3-6 in Table 2) almost always perform better than the multinomial HMM. This is true for both feature sets and both regularization strengths used, with an average increase of 6.4%. The exceptions are possibly due to suboptimal regularization.\n\u2022 Adding a penalty for high word-tag ambiguity to the maximum-entropy HMM improves performance. In almost all cases, comparing lines 3-6 to lines 7-10 in Table 2, the sparsity constraints improve performance (average improvement of 1.6%). The combined system al-\nmost always outperforms the multinomial HMM trained using the ambiguity penalty with an average improvement of 1.6%. For every corpus the best performance is achieved by the model with an ambiguity penalty and maximum-entropy emission probabilities.\n\u2022 For every language except English with 17 tags and a particular feature configuration, reducing the feature set by excluding rare features improves performance on average by 2.3% (lines 5-6 are better than lines 3-4 in Table 2).\n\u2022 Regularizing the maximum-entropy model is more important when there are many features and when we do not have a word-tag ambiguity penalty. Lines 3-4 of Table 2 have the maximum-entropy HMM with many features, and we see that having a tight parameter prior almost always out-performs having a looser prior. By contrast, looking at lines 9-10 of Table 2 we see that when we have an ambiguity penalty and fewer features a looser prior is almost always better than a tighter parameter prior. This was observed also by Gra\u00e7a (2010).\nIt is very encouraging to see that the improvements of using a feature-rich model are additive with the effects of penalizing tag-ambiguity. This is especially surprising since we did not optimize the strength of the tag-ambiguity penalty for the maximum-entropy emission HMM, but rather used a value reported by Gra\u00e7a et al. (2009) to work for the multinomial emission HMM. Experiments reported by Gra\u00e7a (2010) show that tuning this parameter can further improve performance. Nevertheless, both methods regularize the objective in different ways and their interaction should be accounted for. It would be interesting to use L1 regularization on the ME models, instead of L22 regularization together with a feature count cutoff. This way the model could learn which features to discard, instead of requiring a predefined parameter that depends on the particular corpus characteristics.\nAs reported by Berg-Kirkpatrick et al. (2010), the way in which the objective is optimized can have a big impact on the overall results. However, due to the non-convex objective function it is unclear which optimization method works better and why. We briefly analyze this question in Appendix A and leave it as an open question for future work."}, {"heading": "5.4 Error Analysis", "text": "Figure 2 shows the distribution of true tags and clusters for both the HMM model (left) and the HMM+ME+Sp model (right) on the En17 corpus. Each bar represents a cluster, labeled by the tag assigned to it after performing the 1-Many mapping. The colors represent the number of words with the corresponding true tag. To reduce clutter, true tags that were never used to label a cluster are grouped into \u201cOthers\u201d.\nWe observe that both models split common tags such as \u201cnouns\u201d into several hidden states. This splitting accounts for many of the errors in both models. By using 5 states for nouns instead of 7, HMM+ME+Sp is able to use more states for adjectives. Another improvement comes from a better grouping of prepositions. For example \u201cto\u201d is grouped with punctuation by the HMM while for HMM+ME+Sp it is correctly mapped to prepositions. Although this should be the correct behavior, it actually hurts, since the tagset has a special tag \u201cTO\u201d and all occurrences of the word \u201cto\u201d are incorrectly assigned, resulting in the loss of 2.2% accuracy. In contrast, HMM has a state mapped to the tag \u201cTO\u201d but the word \u201cto\u201d comprises only one fifth of that state. The most common error made by HMM+ME+Sp is to include the word \u201cThe\u201d with the second noun induced tag in Figure 2 (Right). This induced tag contains mostly capitalized nouns and pronouns, which often\nprecede nouns of other induced tags. We suspect that the capitalization feature is the cause of this error.\nThe better performance of feature-based models on Portuguese relative to English may be due to the ability of features to better represent the richer morphology of Portuguese. Figure 3 shows the induced clusters for Portuguese. The HMM+ME+Sp model improves over HMM for all tags except for adjectives. Both models have trouble distinguishing nouns from adjectives. The reduced accuracy for adjectives for HMM+ME+Sp is explained by the mapping of a single cluster containing most of the adjectives to adjectives by the HMM model and to nouns in the HMM+ME+Sp model. Removing the noun-adjective distinction, as suggested by Zhao and Marcus (2009), would increase performance of both models by about 6%. Another qualitative difference we observed was that the HMM+ME+Sp model used a single induced cluster for proper nouns rather than spreading them across different clusters."}, {"heading": "5.5 State-of-the-Art Comparison", "text": "We now compare our best POS induction system (based on the settings in line 10 of Table 2), to other recent systems. Results are summarized in Table 3. As we have previously done with Table 2, we focus the discussion on the 1-Many evaluation metric, as results are qualitatively the same for the VI and V metrics, while the 1-1 metric shows more variance across languages.\nLines 1-3 in Table 3 show clustering algorithms based on the information gain on various metrics. BROWN wins 5/6 times (in the scenarios with fewer clusters) over the CLARK system, despite the fact that CLARK uses morphology. Comparing lines 1-3 of Table 3 to line 4, we see that the LDC system is particularly strong for En17 where it achieves state-of-the-art results, but behaves worse than the BROWN system for every other corpus.\nFor HMMs with multinomial emissions (lines 5-8 of Table 3), both maximum likelihood training (HMM) and parameter sparsity (HMM+VB) perform worse than adding an ambiguity penalty (HMM+Sp). This holds for other evaluation metrics, with the exception of 1-1. This confirms previous results by Gra\u00e7a et al. (2009). Comparing the models in lines 5-8 to those in lines 1-3, we see\nthat the best HMM (HMM+Sp) performs comparably with the best clustering (BROWN), with one model winning for 3 languages and the other for the remaining 3.\nThe feature rich HMMs (BK+EM and BK+DG) perform very well, achieving results that are better than HMM+Sp for 4 of 6 tests. Even though both optimize the same objective, they achieve different results on different corpora. We explore the training procedure in more detail in Appendix A, comparing also our implementation to that of Berg-Kirkpatrick et al. (2010). For brevity, Table 3 contains only the results from the implementation of Berg-Kirkpatrick et al. (2010). Our implementation produces comparable, but not quite identical results.\nLines 11-12 of Table 3 display the two methods that attempt to control tag ambiguity and have a feature-rich representation to capture morphological information. The results for TLHMM are taken from Lee et al. (2010), so we do not report results for the En17 and Bg corpora. Also, because we were not able to rerun the experiments for TLHMM, we were not able to compute the information-theoretic metrics. Consequently, the comparison for TLHMM is slightly less complete than for the other methods. Both TLHMM and HMM+ME+Sp perform competitively or better than the other systems. This is not surprising since they have the ability to model morphological regularity while also penalizing high ambiguity. Comparing TLHMM with HMM+ME+Sp, we see that HMM+ME+Sp performs better on the 1-Many metric. In contrast, TLHMM performs better on 1-1. One possible explanation is that the underlying model in TLHMM is a Bayesian HMM with sparsifying Dirichlet priors. As noted by Gra\u00e7a et al. (2009), models trained in this way tend to have a cluster distribution that more closely resemble the true POS distribution (some clusters with lots of words and some with few words) which favors the 1-1 metric (a description of the particularity of the 1-1 metric is discussed in Appendix B).\nTo summarize, for all non-English languages and all metrics except 1-1, the HMM+ME+Sp system performs better than all the other systems. For English, BK+DG wins for the 45-tag corpus, while LDC wins for the 17-tag corpus. The HMM+ME+Sp system is fairly robust, performing well on all corpora and best on several of them, which allow us to conclude that it is not tuned to any particular corpus or evaluation metric.\nThe performance of HMM+ME+Sp is tightly related to the performance of the underlying HMM+ME system. In Appendix A we present a discussion about the performance of different optimization methods for HMM+ME. We compare our HMM+ME implementation to that of BK+EM and BK+DG and show that there are some significant differences in performance. However, its not clear by the results which one is better, and why it performs better in a given situation.\nAs mentioned by Clark (2003), morphological information is particularly useful for rare words. Table 4 compares different models\u2019 accuracy for words according to their frequency. We compare clustering models based on information gain with and without morphological information (BROWN,CLARK), a distributional information-based model (LDC), and the feature rich HMM with tag ambiguity control (HMM+ME+Sp). As expected we see that systems using morphology do better on rare words. Moreover these systems improve over almost all categories except very common words (words occurring more than 50 times). Comparing HMM+ME+Sp against CLARK, we see that even for the condition where CLARK overall works better (En45), it still performs worse for rare words than HMM+ME+Sp."}, {"heading": "5.6 Using the Clusters", "text": "As a further comparison of the different POS induction methods, we experiment with a simple semisupervised scheme where we use the learned clusters as features in a supervised POS tagger. The basic supervised model has the same features as the HMM+ME model, except that we use all word identities and suffixes regardless of frequency. We trained the supervised model using averaged perceptron for a number of iterations chosen as follows: split the training set into 20% for development and 80% for training and pick the number of iterations \u03bd to optimize accuracy on the development set. Finally, trained on the full training set using \u03bd iterations and report results on a 500 sentence test set.\nWe augmented the standard features with the learned hidden state for the current token, for each unsupervised method (BROWN,CLARK,LDC, HMM+ME+Sp). Figure 4 shows the average accuracy of the supervised model as we varied the type of unsupervised features. The average is taken over 10 random samples for the training set at each training set size. We can see from Figure 4 that using sem-supervised features from any of the models improves performance even if we have 500 labeled sentences. Moreover, we see that HMM+ME+Sp either performs as well or better than the other models."}, {"heading": "6. Conclusion", "text": "In this work we investigated the task of fully unsupervised POS induction in five different languages. We identified and proposed solutions for three major problems of the simple hidden Markov model that has been used extensively for this task: i) treating words atomically, ignoring orthographic and morphological information \u2013 which we addressed by replacing multinomial word distributions by small maximum-entropy models; ii) an excessive number of parameters that allows models to fit irrelevant correlations \u2013 which we adressed by discarding parameters with small support in the corpus; iii) a training regime (maximum likelihood) that allows very high word ambiguity \u2013 which we addressed by training using the PR framework with a word ambiguity penalty. We show that all these solutions improve the model performance and that the improvements are additive. Comparing against the regular HMM we achieve an impressive improvement of 10.4% on average.\nWe also compared our system against the main competing systems and show that our approach performs better in every language except English. Moreover, our approach performs well across languages and learning conditions, even when hyperparameters are not tuned to the conditions. When the induced clusters are used as features in a semi-supervised POS tagger trained with a small amount of supervised data, we show significant improvements. Moreover, the clusters induced by our system always perform as well as or better than the clusters produced by other systems."}, {"heading": "Acknowledgments", "text": "Jo\u00e3o V. Gra\u00e7a was supported by a fellowship from Funda\u00e7\u00e3o para a Ci\u00eancia e Tecnologia (SFRH/ BD/ 27528/ 2006) and by FCT project CMU-PT/HuMach/0039/2008 and by FCT (INESC-ID multiannual funding) through the PIDDAC Program funds. Kuzman Ganchev was partially supported by NSF ITR EIA 0205448. Ben Taskar was partially supported by the DARPA CSSG 2009 Award and the ONR 2010 Young Investigator Award. Lu\u00edsa Coheur was partially supported by FCT (INESC-ID multiannual funding) through the PIDDAC Program funds."}, {"heading": "Appendix A. Unsupervised Optimization", "text": "Berg-Kirkpatrick et al. (2010) describe the feature-rich HMM and show that training this model using direct gradient rather than EM can lead to better results. However, they only report results for the En45 corpus. Table 5 compares their implementation of both training regimes (BK+EM, BK+DG) on the different languages. Comparing the two training regimes, we see that there is no clear winner. BK+EM wins in 3 cases (Bg,En17,Dk) and loses on the other three.\nIt is also not clear how to predict which method is more suitable. In a follow up discussion 6 the authors propose that the difference arises from when each algorithm starts to fine-tune the weights of rare features relative to when it trains the weights of common features such as short suffixes. In the case of direct gradient training, at the start of optimization, the weights of common features change more rapidly because weight gradient is proportional to feature frequency. As training progresses, more weight is transferred to the rarer features. In contrast, for EM training, the optimization is done to completion on each M-Step, so even in the first iterations of EM where the counts are mostly random, the rarer features get a lot of the weight mass. This prevents the model from generalizing, and optimization terminates at a local maximum closer to the starting point. To allow EM to use common features for longer we tried some small experiments where we initially had very permissive stopping criteria for the M-step. After a few EM iterations with permissive stopping criteria, we require stricter stopping criteria. This tended to improve EM, but we did not find a principled method of setting a schedule for the convergence criteria on the M-step. Furthermore, these small experiments do not explain why direct gradient is only better than EM for some languages while being worse on others.\nA related study (Salakhutdinov et al., 2003) compares the convergence rate of EM and direct gradient training, and identifies conditions when EM achieves Newton-like behavior, and when it achieves first-order convergence. The conditions are based on the amount of missing information, which in this case can be approximated by the number of hidden states. Potentially, this difference can also lead to different local maxima, mainly due to the non-local nature of the line search procedure of gradient based methods. In fact, looking at the results, DG training seems to work better on the corpora that have a higher number of hidden states (En45, Es) and work worse on corpora with fewer hidden states (Bg,En17).\nAlso in Table 5 we compare our implementation of the HMM+ME model to the implementation of Berg-Kirkpatrick et al. (2010), using the same conditions (regularization parameter, feature set, convergence criteria, initialization) and observe significant differences in results. Communication and code-comparison revealed small implementation differences: we use a bias feature while they do not; for the same random seed, our parameters are initialized differently than theirs; we have\n6. http://www.cs.berkeley.edu/~tberg/gradVsEM/main.html\ndifferent implementations of the optimization algorithm; and a different number of iterations. For some corpora these differences result in better performance for their implementation, while for other corpora our implementation gets better results. We leave these details as well as a better understanding of the differences between each optimization procedure as future work, since this is not the main focus of the present paper."}, {"heading": "Appendix B. Evaluation Metrics", "text": "To compare the performance of the different models one needs to evaluate the quality of the induced clusters. Several evaluation metrics for clustering have been proposed in previous work. The metrics we use to evaluate can be divided into two types (Reichart & Rappoport, 2009): mapping-based and information theoretic. Mapping based metrics require a post-processing step to map each cluster to a POS tag and then evaluate accuracy as for supervised POS tagging. Information-theoretic (IT) metrics compare the induced clusters directly with the true POS tags.\n1-Many mapping and 1-1 mapping (Haghighi & Klein, 2006) are two widely-used mapping metrics. In the 1-Many mapping, each hidden state is mapped to the tag with which it cooccurs the most. This means that several hidden states can be mapped to the same tag, and some tags might not be used at all. The 1-1 mapping greedily assigns each hidden state to a single tag. In the case where the number of tags and hidden states is the same, this will give a 1-1 correspondence. A major drawback of the latter mapping is that it fails to express all the information of the hidden states. Typically, unsupervised models prefer to explain very frequent tags with several hidden states, and combine some very rare tags. For example the Pt corpus has 3 tags that occur only once in the corpus. Grouping these together but subdividing nouns still provides a lot of information about the true tag assignments. However, this would not be captured by the 1-1 mapping. This metric tends to favor systems that produce an exponential distribution on the size of each induced cluster independent of the clusters\u2019 true quality, and it does not correlate well with the information theoretic metrics (Gra\u00e7a et al., 2009). Nevertheless, the 1-Many mapping also has drawbacks, since it can only distinguish clusters based on their most frequent tag. So, having a cluster split almost evenly\nbetween nouns and adjectives, or having a cluster with the same number of nouns, but a mixture of words with different tags gives the same 1-Many accuracy.\nThe information-theoretic measures we use for evaluation are variation of information (VI) (Meila\u0306, 2007) and validity-measure (V) (Rosenberg & Hirschberg, 2007). Both are based on the entropy and conditional entropy of the tags and induced clusters. VI has desirable geometric properties \u2013 it is a metric and is convexly additive (Meila\u0306, 2007). However, the range of VI values is dataset-dependent (VI lies in [0, 2 logN ] whereN is the number of POS tags) which does not allow a comparison across datasets with different N . The validity-measure (V) is also an entropy-based measure and always lies in the range [0, 1], but does not satisfy the same geometric properties as VI. It has been reported to give a high score when a large number of clusters exist, even if these are of low quality (Reichart & Rappoport, 2009). Other information-theoretic measures have been proposed that better handle different numbers of clusters, for instance NVI (Reichart & Rappoport, 2009). However, in this work all testing conditions will be on the same corpora with the same number of clusters so that problem does not exist. Christodoulopoulos, Goldwater, and Steedman (2010) present an extensive comparison between evaluation metrics. In related work Maron, Lamar, and Bienenstock (2010) present another empirical study about metrics and conclude that the VI metric can produce results that contradict the true quality of the induced clustering, by giving very high scores to very simple baseline systems, for instance assigning the same label to all words. They also point out several problems with the 1-1 metric some of which we explained previously. Since metric comparison is not the focus of this work we will compare all methods using the four metrics described in this section."}], "references": [{"title": "Treebanks: Building and Using Parsed Corpora", "author": ["A. Abeill\u00e9"], "venue": null, "citeRegEx": "Abeill\u00e9,? \\Q2003\\E", "shortCiteRegEx": "Abeill\u00e9", "year": 2003}, {"title": "Floresta Sinta(c)tica: a treebank for Portuguese", "author": ["S. Afonso", "E. Bick", "R. Haber", "D. Santos"], "venue": "In Proc. LREC,", "citeRegEx": "Afonso et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Afonso et al\\.", "year": 2002}, {"title": "A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains", "author": ["L. Baum", "T. Petrie", "G. Soules", "N. Weiss"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Baum et al\\.,? \\Q1970\\E", "shortCiteRegEx": "Baum et al\\.", "year": 1970}, {"title": "Painless unsupervised learning with features", "author": ["T. Berg-Kirkpatrick", "A. Bouchard-C\u00f4t\u00e9", "J. DeNero", "D. Klein"], "venue": "In Proc. NAACL", "citeRegEx": "Berg.Kirkpatrick et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2010}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "P.V. deSouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Conditional and joint models for grapheme-to-phoneme conversion", "author": ["S. Chen"], "venue": "In Proc. ECSCT", "citeRegEx": "Chen,? \\Q2003\\E", "shortCiteRegEx": "Chen", "year": 2003}, {"title": "Two decades of unsupervised POS induction: How far have we come", "author": ["C. Christodoulopoulos", "S. Goldwater", "M. Steedman"], "venue": "In Proc. EMNLP,", "citeRegEx": "Christodoulopoulos et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Christodoulopoulos et al\\.", "year": 2010}, {"title": "Building cast3lb: A spanish treebank", "author": ["M. Civit", "M. Mart\u00ed"], "venue": "Research on Language & Computation,", "citeRegEx": "Civit and Mart\u00ed,? \\Q2004\\E", "shortCiteRegEx": "Civit and Mart\u00ed", "year": 2004}, {"title": "Combining distributional and morphological information for part of speech induction", "author": ["A. Clark"], "venue": "In Proc. EACL", "citeRegEx": "Clark,? \\Q2003\\E", "shortCiteRegEx": "Clark", "year": 2003}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Toward unsupervised whole-corpus tagging", "author": ["D. Freitag"], "venue": "In Proc. COLING. Association for Computational Linguistics", "citeRegEx": "Freitag,? \\Q2004\\E", "shortCiteRegEx": "Freitag", "year": 2004}, {"title": "Posterior regularization for structured latent variable models", "author": ["K. Ganchev", "J. Gra\u00e7a", "J. Gillenwater", "B. Taskar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ganchev et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ganchev et al\\.", "year": 2010}, {"title": "A comparison of Bayesian estimators for unsupervised hidden Markov model POS taggers", "author": ["J. Gao", "M. Johnson"], "venue": "In In Proc. EMNLP,", "citeRegEx": "Gao and Johnson,? \\Q2008\\E", "shortCiteRegEx": "Gao and Johnson", "year": 2008}, {"title": "A fully Bayesian approach to unsupervised part-of-speech tagging", "author": ["S. Goldwater", "T. Griffiths"], "venue": "In In Proc. ACL,", "citeRegEx": "Goldwater and Griffiths,? \\Q2007\\E", "shortCiteRegEx": "Goldwater and Griffiths", "year": 2007}, {"title": "Parameter vs. posterior sparisty in latent variable models", "author": ["J. Gra\u00e7a", "K. Ganchev", "F. Pereira", "B. Taskar"], "venue": "In Proc. NIPS", "citeRegEx": "Gra\u00e7a et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gra\u00e7a et al\\.", "year": 2009}, {"title": "Expectation maximization and posterior constraints", "author": ["J. Gra\u00e7a", "K. Ganchev", "B. Taskar"], "venue": null, "citeRegEx": "Gra\u00e7a et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gra\u00e7a et al\\.", "year": 2007}, {"title": "Posterior Regularization Framework: Learning Tractable Models with Intractable Constraints", "author": ["Gra\u00e7a", "J. a. d. A. V"], "venue": "Ph.D. thesis, Universidade Te\u0301cnica de Lisboa, Instituto Superior Te\u0301cnico", "citeRegEx": "Gra\u00e7a and V.,? \\Q2010\\E", "shortCiteRegEx": "Gra\u00e7a and V.", "year": 2010}, {"title": "Prototype-driven learning for sequence models", "author": ["A. Haghighi", "D. Klein"], "venue": "In Proc. HTLNAACL. ACL", "citeRegEx": "Haghighi and Klein,? \\Q2006\\E", "shortCiteRegEx": "Haghighi and Klein", "year": 2006}, {"title": "Evaluating unsupervised part-of-speech tagging for grammar induction", "author": ["III Headden", "W. P", "D. McClosky", "E. Charniak"], "venue": "In Proc. COLING,", "citeRegEx": "Headden et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Headden et al\\.", "year": 2008}, {"title": "Bootstrapping parsers via syntactic projection across parallel texts", "author": ["R. Hwa", "P. Resnik", "A. Weinberg", "C. Cabezas", "O. Kolak"], "venue": "Special Issue of the Journal of Natural Language Engineering on Parallel Texts,", "citeRegEx": "Hwa et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hwa et al\\.", "year": 2005}, {"title": "Why doesn\u2019t EM find good HMM POS-taggers", "author": ["M. Johnson"], "venue": "In In Proc. EMNLP-CoNLL", "citeRegEx": "Johnson,? \\Q2007\\E", "shortCiteRegEx": "Johnson", "year": 2007}, {"title": "The Danish Dependency Treebank and the underlying linguistic theory", "author": ["Kromann", "Matthias T"], "venue": "In Second Workshop on Treebanks and Linguistic Theories (TLT),", "citeRegEx": "Kromann and T.,? \\Q2003\\E", "shortCiteRegEx": "Kromann and T.", "year": 2003}, {"title": "Latent-descriptor clustering for unsupervised POS induction", "author": ["M. Lamar", "Y. Maron", "E. Bienenstock"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lamar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lamar et al\\.", "year": 2010}, {"title": "SVD and clustering for unsupervised POS tagging", "author": ["M. Lamar", "Y. Maron", "M. Johnson", "E. Bienenstock"], "venue": "In Proceedings of the ACL 2010 Conference: Short Papers,", "citeRegEx": "Lamar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lamar et al\\.", "year": 2010}, {"title": "Simple type-level unsupervised POS tagging", "author": ["Y.K. Lee", "A. Haghighi", "R. Barzilay"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lee et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2010}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["M. Marcus", "M. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Evaluation criteria for unsupervised POS induction", "author": ["Y. Maron", "M. Lamar", "E. Bienenstock"], "venue": null, "citeRegEx": "Maron et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Maron et al\\.", "year": 2010}, {"title": "Algorithms for bigram and trigram word clustering", "author": ["S. Martin", "J. Liermann", "H. Ney"], "venue": "In Speech Communication,", "citeRegEx": "Martin et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Martin et al\\.", "year": 1998}, {"title": "Comparing clusterings\u2014an information based distance", "author": ["M. Meil\u0103"], "venue": "J. Multivar. Anal.,", "citeRegEx": "Meil\u0103,? \\Q2007\\E", "shortCiteRegEx": "Meil\u0103", "year": 2007}, {"title": "Tagging English text with a probabilistic model", "author": ["B. Merialdo"], "venue": "Computational linguistics,", "citeRegEx": "Merialdo,? \\Q1994\\E", "shortCiteRegEx": "Merialdo", "year": 1994}, {"title": "Crouching Dirichlet, hidden Markov model: Unsupervised POS tagging with context local tag generation", "author": ["T. Moon", "K. Erk", "J. Baldridge"], "venue": "In Proc. EMNLP,", "citeRegEx": "Moon et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Moon et al\\.", "year": 2010}, {"title": "A new view of the EM algorithm that justifies incremental, sparse and other variants", "author": ["R.M. Neal", "G.E. Hinton"], "venue": "Learning in Graphical Models,", "citeRegEx": "Neal and Hinton,? \\Q1998\\E", "shortCiteRegEx": "Neal and Hinton", "year": 1998}, {"title": "A maximum entropy model for part-of-speech tagging", "author": ["A. Ratnaparkhi"], "venue": "In Proc. EMNLP. ACL", "citeRegEx": "Ratnaparkhi,? \\Q1996\\E", "shortCiteRegEx": "Ratnaparkhi", "year": 1996}, {"title": "Minimized models for unsupervised part-of-speech tagging", "author": ["S. Ravi", "K. Knight"], "venue": null, "citeRegEx": "Ravi and Knight,? \\Q2009\\E", "shortCiteRegEx": "Ravi and Knight", "year": 2009}, {"title": "The NVI clustering evaluation measure", "author": ["R. Reichart", "A. Rappoport"], "venue": "In Proc. CONLL", "citeRegEx": "Reichart and Rappoport,? \\Q2009\\E", "shortCiteRegEx": "Reichart and Rappoport", "year": 2009}, {"title": "V-measure: A conditional entropy-based external cluster evaluation measure", "author": ["A. Rosenberg", "J. Hirschberg"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "Rosenberg and Hirschberg,? \\Q2007\\E", "shortCiteRegEx": "Rosenberg and Hirschberg", "year": 2007}, {"title": "Optimization with EM and expectationconjugate-gradient", "author": ["R. Salakhutdinov", "S. Roweis", "Z. Ghahramani"], "venue": "In Proc. ICML,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2003}, {"title": "Distributional part-of-speech tagging", "author": ["H. Sch\u00fctze"], "venue": "In Proc. EACL,", "citeRegEx": "Sch\u00fctze,? \\Q1995\\E", "shortCiteRegEx": "Sch\u00fctze", "year": 1995}, {"title": "Guided learning for bidirectional sequence classification", "author": ["L. Shen", "G. Satta", "A. Joshi"], "venue": "In Proc. ACL,", "citeRegEx": "Shen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2007}, {"title": "Building a Linguistically Interpreted Corpus of Bulgarian: the BulTreeBank", "author": ["K. Simov", "P. Osenova", "M. Slavcheva", "S. Kolkovska", "E. Balabanova", "D. Doikoff", "K. Ivanova", "A. Simov", "E. Simov", "M. Kouylekov"], "venue": "In Proc. LREC", "citeRegEx": "Simov et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Simov et al\\.", "year": 2002}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["N. Smith", "J. Eisner"], "venue": "In Proc. ACL. ACL", "citeRegEx": "Smith and Eisner,? \\Q2005\\E", "shortCiteRegEx": "Smith and Eisner", "year": 2005}, {"title": "Unsupervised multilingual learning for POS tagging", "author": ["B. Snyder", "T. Naseem", "J. Eisenstein", "R. Barzilay"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Snyder et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snyder et al\\.", "year": 2008}, {"title": "A Bayesian LDA-based model for semi-supervised part-ofspeech tagging", "author": ["K. Toutanova", "M. Johnson"], "venue": "In Proc. NIPS,", "citeRegEx": "Toutanova and Johnson,? \\Q2007\\E", "shortCiteRegEx": "Toutanova and Johnson", "year": 2007}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C. Manning", "Y. Singer"], "venue": null, "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "A simple unsupervised learner for POS disambiguation rules given only a minimal lexicon", "author": ["Q. Zhao", "M. Marcus"], "venue": "In Proc. EMNLP", "citeRegEx": "Zhao and Marcus,? \\Q2009\\E", "shortCiteRegEx": "Zhao and Marcus", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "This lack of supervised data will likely persist in the near future because of the investment required for accurate linguistic annotation: it took two years to annotate 4,000 sentences with syntactic parse trees for the Chinese Treebank (Hwa, Resnik, Weinberg, Cabezas, & Kolak, 2005) and four to seven years to annotate 50,000 sentences across a range of languages (Abeill\u00e9, 2003).", "startOffset": 366, "endOffset": 381}, {"referenceID": 32, "context": "Supervised learning of taggers from POS-annotated training text is a well-studied task, with several methods achieving near-human tagging accuracy (Ratnaparkhi, 1996; Toutanova, Klein, Manning, & Singer, 2003; Shen, Satta, & Joshi, 2007).", "startOffset": 147, "endOffset": 237}, {"referenceID": 29, "context": "For the first one, in addition to raw text, we are given a dictionary containing the possible tags for each word and the goal is to disambiguate the tags of a particular word occurrence (Merialdo, 1994).", "startOffset": 186, "endOffset": 202}, {"referenceID": 37, "context": "Recent work on this task typically relies on distributional or morphological features, since words with the same grammatical function tend to occur in similar contexts and to have common morphology (Brown, deSouza, Mercer, Pietra, & Lai, 1992; Sch\u00fctze, 1995; Clark, 2003).", "startOffset": 198, "endOffset": 271}, {"referenceID": 8, "context": "Recent work on this task typically relies on distributional or morphological features, since words with the same grammatical function tend to occur in similar contexts and to have common morphology (Brown, deSouza, Mercer, Pietra, & Lai, 1992; Sch\u00fctze, 1995; Clark, 2003).", "startOffset": 198, "endOffset": 271}, {"referenceID": 37, "context": "Some approaches assume (for computational and statistical simplicity) that each word can only have one tag, aggregating all local contexts through distributional clustering (Sch\u00fctze, 1995).", "startOffset": 173, "endOffset": 188}, {"referenceID": 4, "context": "Most approaches that do not make the one-tag-per-word assumption take the form of a hidden Markov model (HMM) where the hidden states represent word classes and the observations are word sequences (Brown et al., 1992; Johnson, 2007).", "startOffset": 197, "endOffset": 232}, {"referenceID": 20, "context": "Most approaches that do not make the one-tag-per-word assumption take the form of a hidden Markov model (HMM) where the hidden states represent word classes and the observations are word sequences (Brown et al., 1992; Johnson, 2007).", "startOffset": 197, "endOffset": 232}, {"referenceID": 8, "context": "That information is critical to generalization in many languages (Clark, 2003).", "startOffset": 65, "endOffset": 78}, {"referenceID": 20, "context": "As a result, when maximizing the marginal likelihood, common words typically tend to be associated with every tag with some non-trivial probability (Johnson, 2007).", "startOffset": 148, "endOffset": 163}, {"referenceID": 14, "context": "To address this problem we use the posterior regularization (PR) framework (Gra\u00e7a, Ganchev, & Taskar, 2007; Ganchev, Gra\u00e7a, Gillenwater, & Taskar, 2010) to constrain the ambiguity of word-tag associations via a sparsity-inducing penalty on the model posteriors (Gra\u00e7a et al., 2009).", "startOffset": 261, "endOffset": 281}, {"referenceID": 5, "context": "The idea of replacing the multinomial models of an HMM by maximum entropy models is not new and has been applied before in different domains (Chen, 2003), as well as in POS induction (Berg-Kirkpatrick et al.", "startOffset": 141, "endOffset": 153}, {"referenceID": 3, "context": "The idea of replacing the multinomial models of an HMM by maximum entropy models is not new and has been applied before in different domains (Chen, 2003), as well as in POS induction (Berg-Kirkpatrick et al., 2010; Gra\u00e7a, 2010).", "startOffset": 183, "endOffset": 227}, {"referenceID": 14, "context": "In Section 5 we describe experiments comparing the HMM model to the ME model under three learning scenarios: maximum likelihood training using the EM algorithm (Dempster, Laird, & Rubin, 1977) for both HMM and HMM+ME, gradient-based likelihood optimization for the HMM+ME model, and PR with sparsity constraints (Gra\u00e7a et al., 2009) for both HMM and HMM+ME.", "startOffset": 312, "endOffset": 332}, {"referenceID": 3, "context": "While likelihood is traditionally optimized with EM, Berg-Kirkpatrick et al. (2010) find that for the HMM with the maximum entropy emission model, higher likelihood and better accuracy can be achieved by with a gradient-based likelihood-optimization method.", "startOffset": 53, "endOffset": 84}, {"referenceID": 14, "context": "In the following paragraphs we describe a measure of tag ambiguity proposed by Gra\u00e7a et al. (2009) that we will attempt to control.", "startOffset": 79, "endOffset": 99}, {"referenceID": 14, "context": "In the following paragraphs we describe a measure of tag ambiguity proposed by Gra\u00e7a et al. (2009) that we will attempt to control. It is easier to understand this measure with hard tag assignments, so we start with that and thene extend the discussion to distributions over tags. Consider a word such as \u201cstock\u201d. Intuitively, we would like all occurrences of \u201cstock\u201d to be tagged with a small subset of all possible tags (noun and verb, in this case). For a hard assignment of tags to the entire corpus, Y, we could count how many different tags are used in Y for occurrences of the word \u201cstock.\u201d If instead of a single tagging of the corpus, we have a distribution q(Y) over assignments, we need to generalize this ambiguity measure. Instead of asking was a particular tag ever used for the word \u201cstock\u201d, we would ask what is the maximum probability with which a particular tag was used for the word \u201cstock\u201d. Then instead of counting the number of tags, we would sum these probabilities. As motivation, Figure 1 shows the distribution of tag ambiguity across words for two corpora. As we see from Figure 1, when we train using the EM procedure described in Section 3.1, the HMM and ME models grossly overestimates the tag ambiguity of almost all words. However when the same models are trained using PR to penalize the tag ambiguity, both models (HMM+Sp, HMM+ME+Sp) achieve a tag ambiguity closer to the truth. More formally, Gra\u00e7a et al. (2009) define this measure in terms of constraint features \u03c6(X,Y).", "startOffset": 79, "endOffset": 1448}, {"referenceID": 4, "context": "Several influential methods, most notably mutual-information clustering (Brown et al., 1992), have been used to cluster words according to how their immediately contiguous words are distributed.", "startOffset": 72, "endOffset": 92}, {"referenceID": 4, "context": "Several influential methods, most notably mutual-information clustering (Brown et al., 1992), have been used to cluster words according to how their immediately contiguous words are distributed. Although those methods were not explicitly designed for POS induction, the resulting clusters capture some syntactic information (see also Martin, Liermann, & Ney, 1998, for a different method with a similar objective). Clark (2003) refined the distributional clustering approach by adding morphological and word frequency information, to obtain clusters that more closely resemble POS tags.", "startOffset": 73, "endOffset": 428}, {"referenceID": 20, "context": "Several studies propose using Bayesian inference with an improper Dirichlet prior to favor sparse model parameters and hence indirectly reduce tag ambiguity (Johnson, 2007; Gao & Johnson, 2008; Goldwater & Griffiths, 2007).", "startOffset": 157, "endOffset": 222}, {"referenceID": 32, "context": "To alleviate these problems, Sch\u00fctze (1995) used frequency cutoffs, singular-value decomposition of co-occurrence matrices, and approximate co-clustering through two stages of SVD, with the clusters from the first stage used instead of individual words to provide vector representations for the second-stage clustering.", "startOffset": 29, "endOffset": 44}, {"referenceID": 16, "context": "Lamar, Maron and Johnson (2010) have recently revised the two-stage SVD model of Sch\u00fctze (1995) and achieve close to state-of-the-art performance.", "startOffset": 17, "endOffset": 32}, {"referenceID": 16, "context": "Lamar, Maron and Johnson (2010) have recently revised the two-stage SVD model of Sch\u00fctze (1995) and achieve close to state-of-the-art performance.", "startOffset": 17, "endOffset": 96}, {"referenceID": 8, "context": "However, as noted by Clark (2003) and Johnson (2007), using maximum likelihood training for such models does not achieve good results: maximum likelihood training tends to result in very ambiguous distributions for common words, in contradiction with the rather sparse wordtag distribution.", "startOffset": 21, "endOffset": 34}, {"referenceID": 8, "context": "However, as noted by Clark (2003) and Johnson (2007), using maximum likelihood training for such models does not achieve good results: maximum likelihood training tends to result in very ambiguous distributions for common words, in contradiction with the rather sparse wordtag distribution.", "startOffset": 21, "endOffset": 53}, {"referenceID": 8, "context": "However, as noted by Clark (2003) and Johnson (2007), using maximum likelihood training for such models does not achieve good results: maximum likelihood training tends to result in very ambiguous distributions for common words, in contradiction with the rather sparse wordtag distribution. Several approaches have been proposed to mitigate this problem. Freitag (2004) clusters the most frequent words using a distributional approach and co-clustering.", "startOffset": 21, "endOffset": 370}, {"referenceID": 8, "context": "However, as noted by Clark (2003) and Johnson (2007), using maximum likelihood training for such models does not achieve good results: maximum likelihood training tends to result in very ambiguous distributions for common words, in contradiction with the rather sparse wordtag distribution. Several approaches have been proposed to mitigate this problem. Freitag (2004) clusters the most frequent words using a distributional approach and co-clustering. To cluster the remaining (infrequent) words, the author trains a second-order HMM where the emission probabilities for the frequent words are fixed to the clusters found earlier and emission probabilities for the remaining words are uniform. Several studies propose using Bayesian inference with an improper Dirichlet prior to favor sparse model parameters and hence indirectly reduce tag ambiguity (Johnson, 2007; Gao & Johnson, 2008; Goldwater & Griffiths, 2007). This was further refined by Moon, Erk, and Baldridge (2010) by representing explicitly the different ambiguity patterns of function and content words.", "startOffset": 21, "endOffset": 980}, {"referenceID": 8, "context": "However, as noted by Clark (2003) and Johnson (2007), using maximum likelihood training for such models does not achieve good results: maximum likelihood training tends to result in very ambiguous distributions for common words, in contradiction with the rather sparse wordtag distribution. Several approaches have been proposed to mitigate this problem. Freitag (2004) clusters the most frequent words using a distributional approach and co-clustering. To cluster the remaining (infrequent) words, the author trains a second-order HMM where the emission probabilities for the frequent words are fixed to the clusters found earlier and emission probabilities for the remaining words are uniform. Several studies propose using Bayesian inference with an improper Dirichlet prior to favor sparse model parameters and hence indirectly reduce tag ambiguity (Johnson, 2007; Gao & Johnson, 2008; Goldwater & Griffiths, 2007). This was further refined by Moon, Erk, and Baldridge (2010) by representing explicitly the different ambiguity patterns of function and content words. Lee, Haghighi, and Barzilay (2010) take a more direct approach to reducing tag ambiguity by explicitly modeling the set of possible tags for each word type.", "startOffset": 21, "endOffset": 1106}, {"referenceID": 8, "context": "However, as noted by Clark (2003) and Johnson (2007), using maximum likelihood training for such models does not achieve good results: maximum likelihood training tends to result in very ambiguous distributions for common words, in contradiction with the rather sparse wordtag distribution. Several approaches have been proposed to mitigate this problem. Freitag (2004) clusters the most frequent words using a distributional approach and co-clustering. To cluster the remaining (infrequent) words, the author trains a second-order HMM where the emission probabilities for the frequent words are fixed to the clusters found earlier and emission probabilities for the remaining words are uniform. Several studies propose using Bayesian inference with an improper Dirichlet prior to favor sparse model parameters and hence indirectly reduce tag ambiguity (Johnson, 2007; Gao & Johnson, 2008; Goldwater & Griffiths, 2007). This was further refined by Moon, Erk, and Baldridge (2010) by representing explicitly the different ambiguity patterns of function and content words. Lee, Haghighi, and Barzilay (2010) take a more direct approach to reducing tag ambiguity by explicitly modeling the set of possible tags for each word type. Their model first generates a tag dictionary that assigns mass to only one tag for each word type to reflect lexicon sparsity. This dictionary is then used to constrain a Dirichlet prior from which the emission probabilities are drawn by only having support for word-tag pairs in the dictionary. Then a token-level HMM using those emission parameters and transition parameters draw from a symmetric Dirichlet prior are used for tagging the entire corpus. The authors also show improvements by using morphological features when creating the dictionary. Their system achieves state-of-art results for several languages. It should be noted that a common issue with the above sparsity-inducing approaches is that sparsity is imposed at the parameter level, the probability of word given tag, while the desired sparsity is at the posterior level, the probability of tag given word. Gra\u00e7a et al. (2009) use the PR framework to penalize ambiguous posteriors distributions of words given tokens, which achieves better results than the Bayesian sparsifying Dirichlet priors.", "startOffset": 21, "endOffset": 2125}, {"referenceID": 3, "context": "Most recently, Berg-Kirkpatrick et al. (2010) and Gra\u00e7a (2010) proposed replacing the multinomial distributions of the HMM by maximum entropy (ME) distributions.", "startOffset": 15, "endOffset": 46}, {"referenceID": 3, "context": "Most recently, Berg-Kirkpatrick et al. (2010) and Gra\u00e7a (2010) proposed replacing the multinomial distributions of the HMM by maximum entropy (ME) distributions.", "startOffset": 15, "endOffset": 63}, {"referenceID": 3, "context": "Most recently, Berg-Kirkpatrick et al. (2010) and Gra\u00e7a (2010) proposed replacing the multinomial distributions of the HMM by maximum entropy (ME) distributions. This allows the use of features to capture morphological information, and achieve very promising results. Berg-Kirkpatrick et al. (2010) also find that optimizing the likelihood with L-BFGS rather than EM leads to substantial improvements, which we show not to be the case beyond English.", "startOffset": 15, "endOffset": 299}, {"referenceID": 3, "context": "Most recently, Berg-Kirkpatrick et al. (2010) and Gra\u00e7a (2010) proposed replacing the multinomial distributions of the HMM by maximum entropy (ME) distributions. This allows the use of features to capture morphological information, and achieve very promising results. Berg-Kirkpatrick et al. (2010) also find that optimizing the likelihood with L-BFGS rather than EM leads to substantial improvements, which we show not to be the case beyond English. We also note briefly POS induction methods that rely on a prior tag dictionary indicating for each word type what POS tags it can have. The POS induction task is then, for each word token in the corpus, to disambiguate between the possible POS tags, as described by Merialdo (1994). Unfortunately, the availability of a large manually-constructed tag dictionary is unrealistic and much of the later work tries to reduce the required dictionary size in different ways, by generalizing from a small dictionary with only a handful of entries (Smith & Eisner, 2005; Haghighi & Klein, 2006; Toutanova & Johnson, 2007; Goldwater & Griffiths, 2007).", "startOffset": 15, "endOffset": 733}, {"referenceID": 3, "context": "Most recently, Berg-Kirkpatrick et al. (2010) and Gra\u00e7a (2010) proposed replacing the multinomial distributions of the HMM by maximum entropy (ME) distributions. This allows the use of features to capture morphological information, and achieve very promising results. Berg-Kirkpatrick et al. (2010) also find that optimizing the likelihood with L-BFGS rather than EM leads to substantial improvements, which we show not to be the case beyond English. We also note briefly POS induction methods that rely on a prior tag dictionary indicating for each word type what POS tags it can have. The POS induction task is then, for each word token in the corpus, to disambiguate between the possible POS tags, as described by Merialdo (1994). Unfortunately, the availability of a large manually-constructed tag dictionary is unrealistic and much of the later work tries to reduce the required dictionary size in different ways, by generalizing from a small dictionary with only a handful of entries (Smith & Eisner, 2005; Haghighi & Klein, 2006; Toutanova & Johnson, 2007; Goldwater & Griffiths, 2007). However, although this approach greatly simplifies the problem \u2013 most words can only have one tag and, furthermore, the cluster-tag mappings are predetermined, thus removing an extra level of ambiguity \u2013 the accuracy of such methods is still significantly behind supervised methods. To address the remaining ambiguity by imposing additional sparsity, Ravi and Knight (2009) minimize the number of possible tag-tag transitions in the HMM via a integer program.", "startOffset": 15, "endOffset": 1468}, {"referenceID": 3, "context": "Most recently, Berg-Kirkpatrick et al. (2010) and Gra\u00e7a (2010) proposed replacing the multinomial distributions of the HMM by maximum entropy (ME) distributions. This allows the use of features to capture morphological information, and achieve very promising results. Berg-Kirkpatrick et al. (2010) also find that optimizing the likelihood with L-BFGS rather than EM leads to substantial improvements, which we show not to be the case beyond English. We also note briefly POS induction methods that rely on a prior tag dictionary indicating for each word type what POS tags it can have. The POS induction task is then, for each word token in the corpus, to disambiguate between the possible POS tags, as described by Merialdo (1994). Unfortunately, the availability of a large manually-constructed tag dictionary is unrealistic and much of the later work tries to reduce the required dictionary size in different ways, by generalizing from a small dictionary with only a handful of entries (Smith & Eisner, 2005; Haghighi & Klein, 2006; Toutanova & Johnson, 2007; Goldwater & Griffiths, 2007). However, although this approach greatly simplifies the problem \u2013 most words can only have one tag and, furthermore, the cluster-tag mappings are predetermined, thus removing an extra level of ambiguity \u2013 the accuracy of such methods is still significantly behind supervised methods. To address the remaining ambiguity by imposing additional sparsity, Ravi and Knight (2009) minimize the number of possible tag-tag transitions in the HMM via a integer program. Finally, Snyder, Naseem, Eisenstein, and Barzilay (2008) jointly train a POS induction system over parallel corpora in several languages, exploiting the fact that different languages present different ambiguities.", "startOffset": 15, "endOffset": 1611}, {"referenceID": 25, "context": "Table 1 summarizes characteristics of the test corpora: the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993) (we consider both the 17-tag version of Smith & Eisner, 2005 (En17) and the 45-tag version (En45)); the Bosque subset of the Portuguese Floresta Sinta(c)tica Treebank (Afonso, Bick, Haber, & Santos, 2002) (Pt); the Bulgarian BulTreeBank (Simov et al.", "startOffset": 109, "endOffset": 130}, {"referenceID": 39, "context": ", 1993) (we consider both the 17-tag version of Smith & Eisner, 2005 (En17) and the 45-tag version (En45)); the Bosque subset of the Portuguese Floresta Sinta(c)tica Treebank (Afonso, Bick, Haber, & Santos, 2002) (Pt); the Bulgarian BulTreeBank (Simov et al., 2002) (Bg) (with only the 12 coarse tags); the Spanish corpus from the Cast3LB treebank (Civit & Mart\u00ed, 2004) (Es); and the Danish Dependency Treebank (DDT) (Kromann, Matthias T.", "startOffset": 245, "endOffset": 265}, {"referenceID": 24, "context": "We report results from the type-level HMM (TLHMM) (Lee et al., 2010) when applicable, since we were not able to run that system.", "startOffset": 50, "endOffset": 68}, {"referenceID": 20, "context": "In addition, we also compared to a multinomial HMM with a sparsifying Dirichlet prior on the parameters (HMM+VB) trained using variational Bayes (Johnson, 2007).", "startOffset": 145, "endOffset": 160}, {"referenceID": 4, "context": "For type-level tagging, we use two standard baselines, BROWN and CLARK, as described by Brown et al. (1992)1 and Clark (2003)2.", "startOffset": 88, "endOffset": 108}, {"referenceID": 4, "context": "For type-level tagging, we use two standard baselines, BROWN and CLARK, as described by Brown et al. (1992)1 and Clark (2003)2.", "startOffset": 88, "endOffset": 126}, {"referenceID": 4, "context": "For type-level tagging, we use two standard baselines, BROWN and CLARK, as described by Brown et al. (1992)1 and Clark (2003)2. Following Headden, McClosky, and Charniak (2008), we trained the CLARK system with both 5 and 10 hidden states for the letter HMM and ran it for 10 iterations; the BROWN system was run according with the instructions accompanying the code.", "startOffset": 88, "endOffset": 177}, {"referenceID": 4, "context": "For type-level tagging, we use two standard baselines, BROWN and CLARK, as described by Brown et al. (1992)1 and Clark (2003)2. Following Headden, McClosky, and Charniak (2008), we trained the CLARK system with both 5 and 10 hidden states for the letter HMM and ran it for 10 iterations; the BROWN system was run according with the instructions accompanying the code. We also ran the recently proposed LDC system (Lamar, Maron, & Bienenstock, 2010)3, with the configuration described in their paper for PTB45 and PTB17, and the PTB17 configuration for the other corpora. It should be noted that we did not carry out our experiments with the SVD2 system (Lamar, Maron and Johnson, 2010), since SVD2 is superseded by LDC according to its authors. For token-level tagging, we experimented with the feature-rich HMM as presented by BergKirkpatrick et al. (2010), trained both using EM training (BK+EM) and direct gradient (BK+DG), using the configuration provided by the authors4.", "startOffset": 88, "endOffset": 858}, {"referenceID": 3, "context": "Implementation provided by Berg-Kirkpatrick et al. (2010).", "startOffset": 27, "endOffset": 58}, {"referenceID": 14, "context": "We used the results that worked best for English (En17) (Gra\u00e7a et al., 2009), regularizing only words that occur at least 10 times, with \u03c3 = 32, and use the same configuration for all the other scnenarios.", "startOffset": 56, "endOffset": 76}, {"referenceID": 28, "context": "We evaluate all systems using four common metrics for POS induction: 1-Many mapping, 1-1 mapping (Haghighi & Klein, 2006), variation of information (VI) (Meil\u0103, 2007), and validity measure (V) (Rosenberg & Hirschberg, 2007).", "startOffset": 153, "endOffset": 166}, {"referenceID": 18, "context": "001, corresponding to the best values reported by Johnson (2007). For PR training, we initialize with 30 EM iterations and then run for 170 iterations of PR, following Gra\u00e7a et al.", "startOffset": 50, "endOffset": 65}, {"referenceID": 14, "context": "For PR training, we initialize with 30 EM iterations and then run for 170 iterations of PR, following Gra\u00e7a et al. (2009). We used the results that worked best for English (En17) (Gra\u00e7a et al.", "startOffset": 102, "endOffset": 122}, {"referenceID": 3, "context": "We use two different feature sets: the large feature set is that of Berg-Kirkpatrick et al. (2010), while the reduced feature set was described by Gra\u00e7a (2010).", "startOffset": 68, "endOffset": 99}, {"referenceID": 3, "context": "We use two different feature sets: the large feature set is that of Berg-Kirkpatrick et al. (2010), while the reduced feature set was described by Gra\u00e7a (2010). We apply count-based feature selection to both the identity and suffix features.", "startOffset": 68, "endOffset": 160}, {"referenceID": 13, "context": "This is especially surprising since we did not optimize the strength of the tag-ambiguity penalty for the maximum-entropy emission HMM, but rather used a value reported by Gra\u00e7a et al. (2009) to work for the multinomial emission HMM.", "startOffset": 172, "endOffset": 192}, {"referenceID": 13, "context": "This is especially surprising since we did not optimize the strength of the tag-ambiguity penalty for the maximum-entropy emission HMM, but rather used a value reported by Gra\u00e7a et al. (2009) to work for the multinomial emission HMM. Experiments reported by Gra\u00e7a (2010) show that tuning this parameter can further improve performance.", "startOffset": 172, "endOffset": 271}, {"referenceID": 3, "context": "As reported by Berg-Kirkpatrick et al. (2010), the way in which the objective is optimized can have a big impact on the overall results.", "startOffset": 15, "endOffset": 46}, {"referenceID": 44, "context": "Removing the noun-adjective distinction, as suggested by Zhao and Marcus (2009), would increase performance of both models by about 6%.", "startOffset": 57, "endOffset": 80}, {"referenceID": 14, "context": "This confirms previous results by Gra\u00e7a et al. (2009). Comparing the models in lines 5-8 to those in lines 1-3, we see", "startOffset": 34, "endOffset": 54}, {"referenceID": 3, "context": "We explore the training procedure in more detail in Appendix A, comparing also our implementation to that of Berg-Kirkpatrick et al. (2010). For brevity, Table 3 contains only the results from the implementation of Berg-Kirkpatrick et al.", "startOffset": 109, "endOffset": 140}, {"referenceID": 3, "context": "We explore the training procedure in more detail in Appendix A, comparing also our implementation to that of Berg-Kirkpatrick et al. (2010). For brevity, Table 3 contains only the results from the implementation of Berg-Kirkpatrick et al. (2010). Our implementation produces comparable, but not quite identical results.", "startOffset": 109, "endOffset": 246}, {"referenceID": 3, "context": "We explore the training procedure in more detail in Appendix A, comparing also our implementation to that of Berg-Kirkpatrick et al. (2010). For brevity, Table 3 contains only the results from the implementation of Berg-Kirkpatrick et al. (2010). Our implementation produces comparable, but not quite identical results. Lines 11-12 of Table 3 display the two methods that attempt to control tag ambiguity and have a feature-rich representation to capture morphological information. The results for TLHMM are taken from Lee et al. (2010), so we do not report results for the En17 and Bg corpora.", "startOffset": 109, "endOffset": 537}, {"referenceID": 3, "context": "We explore the training procedure in more detail in Appendix A, comparing also our implementation to that of Berg-Kirkpatrick et al. (2010). For brevity, Table 3 contains only the results from the implementation of Berg-Kirkpatrick et al. (2010). Our implementation produces comparable, but not quite identical results. Lines 11-12 of Table 3 display the two methods that attempt to control tag ambiguity and have a feature-rich representation to capture morphological information. The results for TLHMM are taken from Lee et al. (2010), so we do not report results for the En17 and Bg corpora. Also, because we were not able to rerun the experiments for TLHMM, we were not able to compute the information-theoretic metrics. Consequently, the comparison for TLHMM is slightly less complete than for the other methods. Both TLHMM and HMM+ME+Sp perform competitively or better than the other systems. This is not surprising since they have the ability to model morphological regularity while also penalizing high ambiguity. Comparing TLHMM with HMM+ME+Sp, we see that HMM+ME+Sp performs better on the 1-Many metric. In contrast, TLHMM performs better on 1-1. One possible explanation is that the underlying model in TLHMM is a Bayesian HMM with sparsifying Dirichlet priors. As noted by Gra\u00e7a et al. (2009), models trained in this way tend to have a cluster distribution that more closely resemble the true POS distribution (some clusters with lots of words and some with few words) which favors the 1-1 metric (a description of the particularity of the 1-1 metric is discussed in Appendix B).", "startOffset": 109, "endOffset": 1305}, {"referenceID": 3, "context": "We explore the training procedure in more detail in Appendix A, comparing also our implementation to that of Berg-Kirkpatrick et al. (2010). For brevity, Table 3 contains only the results from the implementation of Berg-Kirkpatrick et al. (2010). Our implementation produces comparable, but not quite identical results. Lines 11-12 of Table 3 display the two methods that attempt to control tag ambiguity and have a feature-rich representation to capture morphological information. The results for TLHMM are taken from Lee et al. (2010), so we do not report results for the En17 and Bg corpora. Also, because we were not able to rerun the experiments for TLHMM, we were not able to compute the information-theoretic metrics. Consequently, the comparison for TLHMM is slightly less complete than for the other methods. Both TLHMM and HMM+ME+Sp perform competitively or better than the other systems. This is not surprising since they have the ability to model morphological regularity while also penalizing high ambiguity. Comparing TLHMM with HMM+ME+Sp, we see that HMM+ME+Sp performs better on the 1-Many metric. In contrast, TLHMM performs better on 1-1. One possible explanation is that the underlying model in TLHMM is a Bayesian HMM with sparsifying Dirichlet priors. As noted by Gra\u00e7a et al. (2009), models trained in this way tend to have a cluster distribution that more closely resemble the true POS distribution (some clusters with lots of words and some with few words) which favors the 1-1 metric (a description of the particularity of the 1-1 metric is discussed in Appendix B). To summarize, for all non-English languages and all metrics except 1-1, the HMM+ME+Sp system performs better than all the other systems. For English, BK+DG wins for the 45-tag corpus, while LDC wins for the 17-tag corpus. The HMM+ME+Sp system is fairly robust, performing well on all corpora and best on several of them, which allow us to conclude that it is not tuned to any particular corpus or evaluation metric. The performance of HMM+ME+Sp is tightly related to the performance of the underlying HMM+ME system. In Appendix A we present a discussion about the performance of different optimization methods for HMM+ME. We compare our HMM+ME implementation to that of BK+EM and BK+DG and show that there are some significant differences in performance. However, its not clear by the results which one is better, and why it performs better in a given situation. As mentioned by Clark (2003), morphological information is particularly useful for rare words.", "startOffset": 109, "endOffset": 2484}, {"referenceID": 36, "context": "A related study (Salakhutdinov et al., 2003) compares the convergence rate of EM and direct gradient training, and identifies conditions when EM achieves Newton-like behavior, and when it achieves first-order convergence.", "startOffset": 16, "endOffset": 44}, {"referenceID": 3, "context": "Table 5: EM vs direct gradient from Berg-Kirkpatrick et al. (2010) implementation compared with our implementaion of EM of the HMM with maximum-entropy emission probabilities.", "startOffset": 36, "endOffset": 67}, {"referenceID": 14, "context": "This metric tends to favor systems that produce an exponential distribution on the size of each induced cluster independent of the clusters\u2019 true quality, and it does not correlate well with the information theoretic metrics (Gra\u00e7a et al., 2009).", "startOffset": 225, "endOffset": 245}, {"referenceID": 28, "context": "The information-theoretic measures we use for evaluation are variation of information (VI) (Meil\u0103, 2007) and validity-measure (V) (Rosenberg & Hirschberg, 2007).", "startOffset": 91, "endOffset": 104}, {"referenceID": 28, "context": "VI has desirable geometric properties \u2013 it is a metric and is convexly additive (Meil\u0103, 2007).", "startOffset": 80, "endOffset": 93}, {"referenceID": 28, "context": "The information-theoretic measures we use for evaluation are variation of information (VI) (Meil\u0103, 2007) and validity-measure (V) (Rosenberg & Hirschberg, 2007). Both are based on the entropy and conditional entropy of the tags and induced clusters. VI has desirable geometric properties \u2013 it is a metric and is convexly additive (Meil\u0103, 2007). However, the range of VI values is dataset-dependent (VI lies in [0, 2 logN ] whereN is the number of POS tags) which does not allow a comparison across datasets with different N . The validity-measure (V) is also an entropy-based measure and always lies in the range [0, 1], but does not satisfy the same geometric properties as VI. It has been reported to give a high score when a large number of clusters exist, even if these are of low quality (Reichart & Rappoport, 2009). Other information-theoretic measures have been proposed that better handle different numbers of clusters, for instance NVI (Reichart & Rappoport, 2009). However, in this work all testing conditions will be on the same corpora with the same number of clusters so that problem does not exist. Christodoulopoulos, Goldwater, and Steedman (2010) present an extensive comparison between evaluation metrics.", "startOffset": 92, "endOffset": 1165}, {"referenceID": 28, "context": "The information-theoretic measures we use for evaluation are variation of information (VI) (Meil\u0103, 2007) and validity-measure (V) (Rosenberg & Hirschberg, 2007). Both are based on the entropy and conditional entropy of the tags and induced clusters. VI has desirable geometric properties \u2013 it is a metric and is convexly additive (Meil\u0103, 2007). However, the range of VI values is dataset-dependent (VI lies in [0, 2 logN ] whereN is the number of POS tags) which does not allow a comparison across datasets with different N . The validity-measure (V) is also an entropy-based measure and always lies in the range [0, 1], but does not satisfy the same geometric properties as VI. It has been reported to give a high score when a large number of clusters exist, even if these are of low quality (Reichart & Rappoport, 2009). Other information-theoretic measures have been proposed that better handle different numbers of clusters, for instance NVI (Reichart & Rappoport, 2009). However, in this work all testing conditions will be on the same corpora with the same number of clusters so that problem does not exist. Christodoulopoulos, Goldwater, and Steedman (2010) present an extensive comparison between evaluation metrics. In related work Maron, Lamar, and Bienenstock (2010) present another empirical study about metrics and conclude that the VI metric can produce results that contradict the true quality of the induced clustering, by giving very high scores to very simple baseline systems, for instance assigning the same label to all words.", "startOffset": 92, "endOffset": 1278}], "year": 2011, "abstractText": "We consider the problem of fully unsupervised learning of grammatical (part-of-speech) categories from unlabeled text. The standard maximum-likelihood hidden Markov model for this task performs poorly, because of its weak inductive bias and large model capacity. We address this problem by refining the model and modifying the learning objective to control its capacity via parametric and non-parametric constraints. Our approach enforces word-category association sparsity, adds morphological and orthographic features, and eliminates hard-to-estimate parameters for rare words. We develop an efficient learning algorithm that is not much more computationally intensive than standard training. We also provide an open-source implementation of the algorithm. Our experiments on five diverse languages (Bulgarian, Danish, English, Portuguese, Spanish) achieve significant improvements compared with previous methods for the same task.", "creator": "TeX"}}}