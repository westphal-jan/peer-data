{"id": "1402.0915", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2014", "title": "Learning Ordered Representations with Nested Dropout", "abstract": "In this paper, we study ordered representations of data in which different dimensions have different degrees of importance. To learn these representations we introduce nested dropout, a procedure for stochastically removing coherent nested sets of hidden units in a neural network. We first present a sequence of theoretical results in the simple case of a semi-linear autoencoder. We rigorously show that the application of nested dropout enforces identifiability of the units, which leads to an exact equivalence with PCA. We then extend the algorithm to deep models and demonstrate the relevance of ordered representations to a number of applications. Specifically, we use the ordered property of the learned codes to construct hash-based data structures that permit very fast retrieval, achieving retrieval in time logarithmic in the database size and independent of the dimensionality of the representation. This allows codes that are hundreds of times longer than currently feasible for retrieval. We therefore avoid the diminished quality associated with short codes, while still performing retrieval that is competitive in speed with existing methods. We also show that ordered representations are a promising way to learn adaptive compression for efficient online data reconstruction.", "histories": [["v1", "Wed, 5 Feb 2014 00:41:58 GMT  (1153kb)", "http://arxiv.org/abs/1402.0915v1", "11 pages, 5 figures. Submitted for publication"]], "COMMENTS": "11 pages, 5 figures. Submitted for publication", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["oren rippel", "michael a gelbart", "ryan p adams"], "accepted": true, "id": "1402.0915"}, "pdf": {"name": "1402.0915.pdf", "metadata": {"source": "META", "title": "Learning Ordered Representations with Nested Dropout", "authors": ["Oren Rippel", "Michael A. Gelbart", "Ryan P. Adams"], "emails": ["RIPPEL@MATH.MIT.EDU", "MGELBART@SEAS.HARVARD.EDU", "RPA@SEAS.HARVARD.EDU"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 2.\n09 15\nv1 [\nst at\n.M L\n] 5\nF eb"}, {"heading": "1. Introduction", "text": "The automatic discovery of representations is an increasingly important aspect of machine learning, motivated by a variety of considerations. For example, feature extraction is often a critical first step for supervised learning procedures. Representation learning enables one to avoid explicit feature engineering; indeed, approaches to deep feature\nlearning have often found representations that outperform their hand-crafted counterparts (e.g., Lecun & Bengio, 1995; Hinton & Salakhutdinov, 2006; Vincent et al., 2010; Coates et al., 2011). In other situations, unsupervised representation learning is useful for finding low-dimensional manifolds for visualization (e.g., Tenenbaum et al., 2000; Roweis & Saul, 2000; Van der Maaten & Hinton, 2008). There has also been increasing interest in exploiting such representations for information retrieval, leveraging the ability of unsupervised learning to discover compact and concise codes that can be used to build efficient data structures (e.g., Weiss et al., 2008; Salakhutdinov & Hinton, 2009; Krizhevsky & Hinton, 2011).\nOne frustration associated with current representation learning techniques, however, is redundancy from nonidentifiability in the resulting encoder/decoder. That is, under standard models such as autoencoders, restricted Boltzmann machines, and sparse coding, any given solution is part of an equivalence class of solutions that are equally optimal. This class emerges from the invariance of the models to various transformations of the parameters. Permutation is one clear example of such a transformation, leading to a combinatorial number of equivalent representations for a given dataset and architecture. There exist many more kinds of redundancies as well; the optimality of an autoencoder solution is preserved under any invertible linear transformation of the innermost set of weights (Bourlard & Kamp, 1987). This degeneracy also poses a difficulty when comparing experiments, due to the lack of repeatability: a solution attained by the optimization procedure is extremely sensitive to the choice of initialization.\nThis large number of equivalent representations has an advantage, however: it provides flexibility in architecture design. This freedom allows us to impose desirable structural constraints on the learned representations, without compromising their expressiveness. These constraints can imbue a number of useful properties, including the elimination of\npermutation non-identifiability. In this work we propose one such structural constraint: we specify a priori the quantity of information encapsulated in each dimension of the representation. This choice allows us to order the representation dimensions according to their information content.\nThe intuition behind our proposed approach to learning ordered representations is to train models such that the information contained in each dimension of the representation decreases as a function of the dimension index, following a pre-specified decay function. To this end, we introduce the nested dropout algorithm. As with the original dropout formulation (Hinton et al., 2012), nested dropout applies a stochastic mask over models. However, instead of imposing an independent distribution over each individual unit in a model, it assigns a distribution over nested subsets of representation units. More specifically, given a representation space of dimension K , we define a distribution pB(\u00b7) over the representation index subsets Sb = {1, . . . , b}, b = 1, . . . ,K. This has the property that if the j-th unit appears in a particular mask, then so do all \u201cearlier\u201d units 1, . . . , j \u2212 1, allowing the j-th unit to depend on them. This nesting leads to an inherent ordering over the representation dimensions. The distribution pB(\u00b7) then governs the information capacity decay by modulating the relative frequencies of these masks.\nWe motivate such ordered representations in several ways:\nIdentifiability As discussed above, many current representation learning techniques suffer from nonidentifiability of the solutions. We can remedy this by introducing strict representation ordering, which enforces distinguishability. We rigorously demonstrate this for the simple case of a semi-linear autoencoder. We prove that the application of nested dropout leads to a significant reduction in the solution space complexity without harming the solution quality. Under an additional weak constraint, we further prove that the model has a single and unique global optimum. We show that this solution is exactly the set of eigenvalues of the covariance matrix of the data, ordered by eigenvalue magnitude. This demonstrates exact equivalence between semi-linear nested dropout autoencoders and principal component analysis (PCA).\nFast retrieval Current information retrieval procedures suffer from an intrinsic tradeoff between search speed and quality: representation dimensionality and dataset size must be sacrificed to gain search tractability (Grauman & Fergus (2013) offers an excellent overview of modern retrieval procedures). Given a query datum, a na\u0131\u0308ve brute force retrieval based on Hamming distance requires a linear scan of the database, which has complexity O (KN) where K is the code length and N the database size. Semantic hashing (Salakhutdinov & Hinton, 2009) retrieves\nexamples within a Hamming neighborhood of radius R by directly scanning through all memory locations associated with them. This results in retrieval time complexity O( (\nK R\n)\n). While this is independent of the database size, it grows rapidly in K and therefore is computationally prohibitive even for codes tens of bits long; code length of 50 bits, for example, requires a petabyte of memory be addressed. Moreover, as the code length increases, it becomes very likely that many queries will not find any neighbors for any feasible radii. Locality sensitive hashing (Datar et al., 2004) seeks to preserve distance information by means of random projections; however, this can lead to very inefficient codes for high input dimensionality.\nBy imposing an ordering on the information represented in a deep model, we can learn hash functions that permit efficient retrieval. Because the importance of each successive coding dimension decays as we move through the ordering, we can naturally construct a binary tree data structure on the representation to capture a coarse-to-fine notion of similarity. This allows retrieval in time that is logarithmic with the dataset size and independent of the representation space dimensionality: the retrieval procedure adaptively selects the minimum number of code bits required for resolution. This enables very fast retrieval on large databases without sacrificing representation quality: we are able to consider codes hundreds of times longer than currently feasible with existing retrieval methods. For example, we perform retrieval on a dataset of a million entries of code length 2048 in an average time of 200\u00b5s per query\u201415,000 times faster than a linear scan or semantic hashing.\nAdaptive compression Ordered representations can also be used for \u201ccontinuous-degradation\u201d lossy compression systems: they give rise to a continuous range of bitrate/quality combinations, where each additional bit corresponds to a small incremental increase in quality. This property can in principle be applied to problems such as video streaming. The representation only needs to be encoded a single time; then, users of different bandwidths can be adaptively sent codes of different length that exactly match their bitrates. The inputs can then be reconstructed optimally for the users\u2019 channel capacities."}, {"heading": "2. Ordering with nested dropout", "text": "Dropout (Hinton et al., 2012) is a regularization technique for neural networks that adds stochasticity to the architecture during training. At each iteration, unbiased coins are flipped independently for each unit in the network, determining whether it is \u201cdropped\u201d or not. Every dropped unit is deleted from the network for that iteration, and an optimization step is taken with respect to the resulting network.\nNested dropout diverges from this in two main ways. First,\nwe only drop units in the representation space. Second, instead of flipping independent coins for different units, we instead assign a prior distribution pB(\u00b7) over the representation indices 1, . . . ,K . We then sample an index b \u223c pB(\u00b7) and drop units b+ 1, . . . ,K. The sampled units then form nested subsets: if unit j appears in a network sample, then so do units 1, . . . , j \u2212 1. This nesting results in an inherent importance ranking of the representation dimensions, as a particular unit can always rely on the presence of its predecessors. For pB(\u00b7) we select a geometric distribution: pB(b) = \u03c1b\u22121(1\u2212 \u03c1). We make this choice due to the exponential decay of this distribution and its memoryless property (see Section 4.4).\nOur architecture resembles an autoencoder in its parametric composition of an encoder and a decoder. We are given a set of N training examples {yn}Nn=1 lying in space Y \u2286 RD . We then transform the data into the representation space X \u2286 RK via a parametric transformation f\u0398 : Y \u2192 X. We denote this function as the encoder, and label the representations as {xn}Nn=1 \u2282 X. The decoder map g\u03a8 : X \u2192 Y then reconstructs the inputs from their representations as {y\u0302n}Nn=1.\nA single nested dropout sample Let us assume that we sample some b \u223c pB(\u00b7) and drop the last K \u2212 b representation units; we refer to this case as the b-truncation. This structure is equivalent to an autoencoder with a representation layer of dimension b. For a given representation x \u2208 RK , we define x\u2193b as the truncation of vector x, namely a copy of x where the last K \u2212 b units are removed, or are equivalently set to 0.\nDenoting the reconstruction of the b-truncation as y\u0302\u2193b = g\u03a8(f\u0398(y)\u2193b), the reconstruction cost function associated with a b-truncation is then\nC\u2193b(\u0398,\u03a8) = 1\nN\nN \u2211\nn=1\nL (yn, y\u0302n\u2193b) . (1)\nIn this work, we take the reconstruction loss L(\u00b7, \u00b7) to be the L2 norm. Although we write this cost as a function of the full parametrization (\u0398,\u03a8), due to the truncation only a subset of the parameters will contribute to the objective.\nThe nested dropout problem Given our distribution pB(\u00b7), we consider the mixture of the different btruncation objectives:\nC(\u0398,\u03a8)=EB [C\u2193b(\u0398,\u03a8)]=\nK \u2211\nb=1\npB(b)C\u2193b(\u0398,\u03a8). (2)\nWe formulate the nested dropout problem as the optimization of this mixture with respect to the model parameters:\n(\u0398\u2217,\u03a8\u2217) = argmin \u0398,\u03a8 C(\u0398,\u03a8) . (3)"}, {"heading": "2.1. Interpretation", "text": "Nested dropout has a natural interpretation in terms of information content in representation units. It was shown by Vincent et al. (2010) that training an autoencoder corresponds to maximizing a lower bound on the mutual information I(y;x) between the input data and their representations. Specifically, the objective of the b-truncation problem can be written in the form\nC\u2193b(\u0398,\u03a8) \u2248 Ey [ \u2212 log pY |X\u2193b (y |f\u0398(y)\u2193b ; \u03a8) ] (4)\nwhere we assume our data are sampled from the true distribution pY (\u00b7). The choice pY |X (y |x ; \u03a8) = N (y ; g\u03a8(x), \u03c3 2 ID), for example, leads to the familiar autoencoder L2 reconstruction penalty.\nNow, define I\u0303b(y;x) := \u2212C\u2193b(\u0398,\u03a8) \u2264 I(y;x) as the approximation of the true mutual information which we maximize for a given b. Then we can write the (negative) nested dropout problem in the form of a telescopic sum:\n\u2212C(\u0398,\u03a8) = K \u2211\nb=1\npB(b)I\u0303b(y;x) (5)\n= I\u03031(y;x) + K \u2211\nb=2\n[FB(K)\u2212 FB(b \u2212 1)]\u2206b ,\nwhere FB(b) = \u2211b b\u2032=1 pB(b \u2032) is the cumulative distribution function of pB(\u00b7) , and \u2206b := I\u0303b(y;x) \u2212 I\u0303b\u22121(y;x) is the marginal information gained from increasing the representation dimensionality from b units to b + 1.\nThis formulation provides a connection between the nested dropout objective and the optimal distribution of information across the representation dimensions. Note that the coefficients FB(K)\u2212 FB(b) of the marginal mutual information are positive and monotonically decrease as a function of b regardless of the choice of distribution pB(\u00b7). This establishes the ordering property intuitively sought by the nested dropout idea. We also see that if for some b we have pB(b) = 0, i.e., index b has no support under pB(\u00b7), then the ordering of representation dimensions b and b\u2212 1 no longer matters. If we set pB(1) = 0, . . . , pB(K \u2212 1) = 0 and pB(K) = 1, we recover the original order-free autoencoder formulation for K latent dimensions. In order to achieve strict ordering, then, the only assumption we must make is that pB(\u00b7) has support over all representation indices. Indeed, this will be a sufficient condition for our proofs in Section 3.2. Equation (5) informs us of how our prior choice of pB(\u00b7) dictates the optimal information allocation per unit."}, {"heading": "3. Exact recovery of PCA", "text": "In this section, we apply nested dropout to a semi-linear autoencoder. This model has a linear or a sigmoidal encoder,\nand a linear decoder. The relative simplicity of this case allows us to rigorously study the ordering property implied by nested dropout.\nFirst, we show that the class of optimal solutions of the nested dropout autoencoder is a subset of the class of optimal solutions of a standard autoencoder. This means that introducing nested dropout does not sacrifice the quality of the autoencoder solution. Second, we show that equipping an autoencoder with nested dropout significantly constrains its class of optimal solutions. We characterize these restrictions. Last, we show that under an additional orthonormality constraint, the model features a single, unique solution that is exactly the set of K eigenvectors with the largest magnitudes arising from the covariance matrix of the inputs, ordered by decreasing eigenvalue magnitude. Hence this recovers the PCA solution exactly. This is in contrast to a standard autoencoder, which recovers the PCA solution up to an invertible linear map."}, {"heading": "3.1. Problem definitions and prior results", "text": "The standard linear autoencoder problem Given our inputs, we apply the linear encoder f\u0398(y) := \u2126y + \u03c9 with parameters \u2126 \u2208 RK\u00d7D and bias vector \u03c9 \u2208 RK for K \u2264 D. Our proofs further generalize to sigmoidal nonlinearities applied to the output of the encoder, but we omit these for clarity. The decoder map g\u03a8 : X \u2192 Y is similarly taken to be g\u03a8(x) := \u0393x+ \u03b3 with parameters \u0393 \u2208 RD\u00d7K and \u03b3 \u2208 RD. We also define the design matrices Y and X whose columns consist of the observations and their representations, respectively.\nThe reconstruction of each datum is then defined as the composition of the encoder and decoder maps. Namely, y\u0302n = \u0393(\u2126yn + \u03c9) + \u03b3 \u2200n = 1, . . . , N . A semi-linear autoencoder seeks to minimize the reconstruction cost\nC(\u0398,\u03a8) =\nN \u2211\nn=1\n\u2016yn \u2212 g\u03a8(f\u0398(yn))\u2016 2 (6)\n= \u2016Y \u2212 (\u0393(\u2126Y + \u03c9) + \u03b3)\u20162F (7)\nwhere by \u2016\u00b7\u2016F we denote the Frobenius matrix norm. From this point on, without loss of generality we assume that \u03c9 = 0, \u03b3 = 0, and that the data is zero-centered. All our results hold otherwise, but with added shifting constants.\nA single b-truncation problem We continue to consider the b-truncation problem, where the last K \u2212 b units of the representation are dropped. As before, for a given representation x \u2208 RK , we define x\u2193b to be the truncation of vector x. Defining the truncation matrix Jm\u2192n \u2208 Rn\u00d7m as [Jm\u2192n]ab = \u03b4ab, then x\u2193b = JK\u2192bx. The decoder is then written as g\u03a8\u2193b(x\u2193b) = \u0393\u2193bx\u2193b, where we write \u0393\u2193b = \u0393J T K\u2192b in which the last K \u2212 b columns of \u0393 are\nremoved. The reconstruction cost function associated with a b-truncation is then\nC\u2193b(\u0398\u2193b,\u03a8\u2193b) = \u2016Y \u2212 \u0393\u2193bX\u2193b\u2016 2 F . (8)\nWe define (\u0398\u2217\u2193b,\u03a8 \u2217 \u2193b) = argmin\u0398\u2193b,\u03a8\u2193b C\u2193b(\u0398\u2193b,\u03a8\u2193b) to be an optimal solution of the b-truncation problem; we label the corresponding optimal cost as C\u2217\u2193b. Also, let V Y = Y Y T be (proportional to) the empirical covariance matrix of {yn}Nn=1 with eigendecompositionV Y = Q\u03a3\n2QT , where\u03a32 is the diagonal matrix constituting of the eigenvalues arranged in decreasing magnitude order, and Q the orthonormal matrix of the respective eigenvectors. Similarly, let R be the orthonormal eigenvector matrix of Y TY , arranged by decreasing order of eigenvalue magnitude.\nThe b-truncation problem exactly corresponds to the original semi-linear autoencoder problem, where the representation dimension is taken to be b in the first place. As such, we can apply known results about the form of the solution of a standard autoencoder. It was proven in Bourlard & Kamp (1987) that this optimal solution must be of the form\nX\u2217b = T b\u03a3\u2193bR T \u0393\u2217b = Q\u2193bT \u22121 b (9)\nwhere T b \u2208 Rb\u00d7b is an invertible matrix, \u03a3\u2193b = JK\u2192b\u03a3 \u2208 Rb\u00d7D the matrix with the b largestmagnitude eigenvalues, and Q\u2193b = QJ T K\u2192b \u2208 R D\u00d7b the matrix with the b corresponding eigenvectors. This result was established for an autoencoder of representation dimension b; we reformulated the notation to suit the nested dropout problem we define in the next subsection.\nIt can be observed from Equation (9) that the semi-linear autoencoder has a strong connection to PCA. An autoencoder discovers the eigenvectors of the empirical covariance matrix of {yn}Nn=1 corresponding to its b eigenvalues of greatest magnitude; however, this is up to to an invertible linear transformation. This class includes rotations, scalings, reflections, index permutations, and so on. This nonidentifiability has an undesirable consequence: it begets a huge class of optimal solutions.\nThe nested dropout problem We now introduce the nested dropout problem. Here, we assign the distribution b \u223c pB(\u00b7) as a prior over b-truncations. For our proofs to hold our only assumption about this distribution is that it has support over the entire index set, i.e., pB(b) > 0, \u2200b = 1, . . . ,K. To that end, we seek to minimize the nested dropout cost function, which we define as\nthe mixture of the K truncated models under pB(\u00b7):\nC(\u0398,\u03a8) = EB\n[\n\u2016Y \u2212 \u0393\u2193bX\u2193b\u2016 2 F\n]\n(10)\n=\nK \u2211\nb=1\npB(b) \u2016Y \u2212 \u0393\u2193bX\u2193b\u2016 2 F . (11)"}, {"heading": "3.2. The nested dropout problem recovers PCA exactly", "text": "Below we provide theoretical justification for the claims made in the beginning of this section. All of the proofs can be found in the appendix of this paper.\nTheorem 1. Every optimal solution of the nested dropout problem is necessarily an optimal solution of the standard autoencoder problem.\nDefinition. We define matrix T \u2208 RK\u00d7K to be commutative in its truncation and inversion if each of its leading principal minors JK\u2192bTJ T K\u2192b, b = 1, . . . ,K is invertible, and the inverse of each of its leading principal minors is equal to the leading principal minor of the inverse T\u22121, namely\nJK\u2192bT \u22121JTK\u2192b = (JK\u2192bTJ T K\u2192b) \u22121 . (12)\nThe below theorem, combined with Lemma 1, establishes tight constraints on the class of optimal solutions of the nested dropout problem. For example, an immediate corollary of this is that T cannot be a permutation matrix, as for such a matrix there must exist some leading principal minor that is not invertible.\nTheorem 2. Every optimal solution of the nested dropout problem must be of the form\nX\u2217 = T\u03a3RT \u0393\u2217 = QT\u22121 , (13)\nfor some matrixT \u2208 RK\u00d7K that is commutative in its truncation and inversion.\nDenote the column and row submatrices, respectively as Ab = [T1b, . . . , T(b\u22121),b] T and Bb = [Tb1, . . . , Tb,(b\u22121)]T . Lemma 1. Let T \u2208 RK\u00d7K be commutative in its truncation and inversion. Then all the diagonal elements of T are nonzero, and for each b = 2, . . . ,K , either Ab = 0 or Bb = 0.\nIn the result below we see that nested dropout coupled with an orthonormality constraint effectively eliminates nonidentifiability. The added constraint pins down any possible rotations and scalings.\nTheorem 3. Under the orthonormality constraint \u0393T\u0393 = IK , the nested dropout problem features a unique global optimum, and this solution is exactly the set of the K top eigenvectors of the covariance of Y , ordered by eigenvalue magnitude. Namely, X\u2217 = \u03a3RT ,\u0393\u2217 = Q."}, {"heading": "4. Training deep models with nested dropout", "text": "In this section we discuss our extension of the nested dropout approach to deep architectures. Specifically, we applied this to deep autoencoders having tens of millions of parameters, which we trained on the 80 Million Tiny Images (80MTI) dataset (Torralba et al., 2008) on a cluster of GPUs. Training models with nested dropout introduces a number of unconventional technical challenges. In the proceeding sections we describe these challenges, and discuss strategies to overcome them.\nWe first describe our general architecture and optimization setup. The 80MTI are 79,302,017 color images of size 32\u00d7 32. We pre-processed the data by subtracting from each pixel its mean and normalizing by its variance across the dataset. We optimize our models with the nonlinear conjugate gradients algorithm and select step sizes using a strong Wolfe conditions line search. For retrieval-related tasks, we seek to produce binary representations. In light of this we use rectified linear units for all nonlinarities in our encoder, as we find this leads to better binarized representation (see Subsection 4.3). Glorot et al. (2011) features an in-depth discussion and motivation of rectified linear units. We train for 2 epochs on minibatches of size 10,000. We inject noise to promote robustness, as in (Vincent et al., 2010); namely, with probability 0.1 we independently corrupt input elements to 0. For all layers other than the representation layer, we apply standard dropout with probability 0.2. At each iteration, we sample nested dropout truncation indices for each example in our minibatch, and take a step with respect to the corresponding network mask."}, {"heading": "4.1. Unit sweeping for decaying gradients", "text": "By the virtue of the decaying distribution pB(\u00b7), it becomes increasingly improbable to sample higher representation indices during training. As such, we encounter a phenomenon where gradient magnitudes vanish as a function of representation unit index. This curvature pathology, in its raw formulation, means that training representation units of higher index can be extremely slow.\nIn order to combat this effect, we develop a technique we call unit sweeping. The idea stems from the observation that the covariance of two latent units sharply decreases as a function of the of the difference of their indices. When pB(\u00b7) is a geometric distribution, for example, the probability of observing both units i and j given that one of them is observed is P[b\u2265max(i, j) | b\u2265min(i, j)] = P[b\u2265|i\u2212j|] = \u03c1\u2212|i\u2212j| by the memoryless property of the distribution. In other words, a particular latent unit becomes exponentially desensitized to values of units of higher index. As such, this unit will eventually converge during its training. Upon convergence, then, this unit can be fixed in place and its associated gradients can be omitted. Loosely speaking, this elimination reduces the \u201ccondition number\u201d of the optimization. Applying this iteratively, we sweep through the latent units, fixing each once it converges. In Figure 1 we compare filters from training a nested dropout model with and without unit sweeping."}, {"heading": "4.2. Adaptive regularization coefficients", "text": "The gradient decay as a function of representation index poses a difficulty for regularization. In particular, the ratio of the magnitudes of the gradients of the reconstruction and the regularization vanishes as a function of the index. Therefore, a single regularization term such as \u03bb\n\u2211K k=1 \u2016\u2126k\u2016L1 would not be appropriate for nested dropout, since the regularization gradient would dominate the high-index gradients. As such, the regularization must be decoupled as a function of representation index. For weight decay, for example, this would of the form \u2211K\nk=1 \u03bbk \u2016\u2126k\u2016L1 . Choosing the coefficients \u03bbk manually is challenging, and to that end we assign them adaptively. We do this by fixing in advance the ratio between the magnitude of the reconstruction gradient and the regularization gradient, and choosing the \u03bbk to satisfy this ratio requirement. This corresponds to fixing the relative contributions of the terms at each step in the optimization procedure."}, {"heading": "4.3. Code binarization", "text": "For the task of retrieval, we would like to obtain binary representations. Several binarization methods have been proposed in prior work (Salakhutdinov & Hinton, 2009; Krizhevsky & Hinton, 2011). We have empirically\nachieved good performance by tying the weights of the encoder and decoder, and thresholding at the representation layer. Although the gradient itself cannot propagate past this threshold, some signal does: the encoder can be trained since it is linked to the decoder, and its modifications are then reflected in the objective. To attain fixed marginal distributions over the binarized representation units, i.e., xk \u223c Bern(\u03b2) for k = 1, . . . ,K, we compute the \u03b2 quantile for each unit, and use this value for thresholding."}, {"heading": "4.4. Code invariance", "text": "We attain better retrieval results when we demand code invariance. That is, when we require that similar examples map to similar codes. Inspired by Rifai et al. (2011b;a), we perform this regularization stochastically by perturbing each input yn in our minibatch with some small \u03b5n \u223c N (0, \u03b5\u0304ID), and introduce penalty term\nI = 1\nN\nN \u2211\nn=1\n\u2016f\u0398(yn + \u03b5n)\u2212 f\u0398(yn)\u2016 2\n\u2016\u03b5n\u2016 2 . (14)\nIt can be shown via Taylor expansion that this corresponds to a stochastic formulation of the regularization of the Frobenius norm of the Jacobian of f\u0398(\u00b7)."}, {"heading": "5. Retrieval with ordered binary codes", "text": "In this section we discuss how ordered representations can be exploited to construct data structures that permit fast retrieval while at the same time allowing for very long codes."}, {"heading": "5.1. Binary tree on the representation space", "text": "The ordering property, coupled with the ability to control information capacity decay across representation units, motivates the construction of a binary tree over large data sets. Each node in this tree contains pointers to the set of examples that share the same path down the tree up to that point. Guaranteeing that this tree is balanced is not feasible, as this is equivalent to completely characterizing the joint distribution over the representation space. However, by the properties of the training algorithm, we are able to\nfix the marginal distributions of all the representation bits as xk \u223c Bern(\u03b2) for some hyperparameter \u03b2 \u2208 (0, 1).\nConsistent with the training procedure, we encode our database as X = {xn}Nn=1 \u2286 {0, 1}\nK , xn = f\u0398(yn). We then construct a binary tree on the resulting codes.\nGiven a query y\u0304, we first encode it as x\u0304 = f\u0398(y\u0304n). We then conduct retrieval by traveling down the binary tree with each decision determined by the next bit of x\u0304. We define the b-truncated Hamming neighborhood of x\u0304 as the set of all examples whose codes share the first b bits of x\u0304:\nNHb (x\u0304) = { y \u2208 Y : \u2016x\u2193b \u2212 x\u0304\u2193b\u2016H = 0 } . (15)\nIt is clear that NHb+1(x\u0304) \u2286 N H b (x\u0304) \u2200b = 1, . . . ,K\u22121. Our retrieval procedure then corresponds to iterating through this family of nested neighborhoods. We expect the cardinality of these to decay approximately exponentially as a function of index. We terminate the retrieval procedure when \u2223\n\u2223NHb (x\u0304) \u2223\n\u2223 < R for some pre-specified terminal neighborhood cardinality, R \u2208 N. It outputs the set NHb\u22121(x\u0304).\nAssuming marginals xk \u223c Bern(\u03b2) and neglecting dependence between the xk, this results in expected retrieval time O( logN/RH(Bern(\u03b2))) where H(Bern(\u03b2)) is the Bernoulli entropy. If \u03b2 = 12 , for example, this reduces to the balanced tree travel time O(logN/R). This retrieval time is logarithmic in the database size N , and independent of the representation space dimensionality K . If one wishes to retrieve a fixed fraction of the dataset, this renders the retrieval complexity also independent of the dataset size.\nIn many existing retrieval methods, the similarity of two examples is measured by their Hamming distance. Here, similarity is rather measured by the number of leading bits they share. This is consistent with the training procedure, which produces codes with this property by demanding reconstructive ability under code truncation variation."}, {"heading": "5.2. Empirical results", "text": "We empirically studied the properties of the resulting codes and data structures in a number of ways. First, we applied ordered retrieval to a toy problem where we trained a tiny 2-16-32-16-2 autoencoder on 2D synthetic pinwheel data (Figure 2). Here we can visualize the nesting of neighborhood families for different queries. Note that, as expected, the nested neighborhood boundaries are orthogonal to the direction of local variation of the data. This follows from the model\u2019s reconstruction loss function.\nWe then trained on 80MTI a binarized nested dropout autoencoder with layer widths 3072-2048-1024-512-1024- 2048-3072 with L1 weight decay and invariance regularization (see Section 4.4). We chose pB(\u00b7) \u223c Geom(0.97) and the binarization quantile \u03b2 = 0.2.\nEmpirical retrieval speeds for various models are shown in Figure 3. We performed retrieval by measuring Hamming distance in a linear scan over the database, and by means of semantic hashing for a number of radii. We also performed ordered retrieval for a number of terminal neighborhood cardinalities. Although semantic hashing is independent of the database size, for a radius greater than 2 it requires more time than a brute force linear scan even for very short codes. In addition, as the code length increases, it becomes very likely that many queries will not find any neighbors for any feasible radii. It can be seen that ordered retrieval carries a very small computational cost which is independent of the code length. Note that each multiplicative variation in the terminal neighborhood size R, from 2 to 32 to 512, leads to a constant shift downward on the logarithmic scale plot. This observation is consistent with our earlier analysis that the retrieval time increases logarithmically with N/R.\nIn Figure 4, we show retrieval results for varying terminal neighborhood sizes. As we decrease the terminal neighborhood size, the similarity of the retrieved data to the query increases. As more bits are added to the representation in the process of retrieval, the resolution of the query increases, and thus it is better resolved from similar images."}, {"heading": "6. Adaptive compression", "text": "Another application of ordered representations is continuous-degradation lossy compression systems. By \u201ccontinuous-degradation\u201d we mean that the message can be decoded for any number, b, of bits received, and that the reconstruction error L(y, y\u0302\u2193b) decreases monotonically with b. Such representations give rise to a continuous (up to a single bit) range of bitrate-quality combinations, where each additional bit corresponds to a small incremental increase in quality.\nThe continuous-degradation property is appealing in many situations. First, consider, a digital video signal that is broadcast to recipients with varying bandwidths. Assume further that the probability distribution over bandwidths for the population, pB(\u00b7), is known or can be estimated, and that a recipient with bandwidth b receives only the first b bits of the transmission. We can then pose the following problem: what broadcast signal minimizes the expected distortion over the population? This is formulated as\n(\u0398\u2217,\u03a8\u2217) = argmin \u0398,\u03a8 EB [L (yn, y\u0302n\u2193b)] . (16)\nThis is precisely the optimization problem solved by our model; Equation (16) is simply a rewriting of Equation (3). This connection gives rise to an interpretation of pB(\u00b7), which we have set to the geometric distribution in our experiments. In particular, pB(\u00b7) can be interpreted as the distribution over recipient bandwidths such that the system minimizes the expected reconstruction error.\nThis intuition in principle applies as well to online video streaming, in which the transmitted signal is destined for only a single recipient. Given that different recipients have different bandwidths, it is acceptable to lower the image quality in order to attain real-time video buffering. Currently, one may specify in advance a small number of fixed encodings for various bandwidths: for example, YouTube offers seven different definitions (240p, 360p, 480p, 720p, 1080p, 1440p, and 2160p), and automatically selects one of these to match the viewer\u2019s bitrate. Ordered representations\noffer the ability to fully utilize the recipient\u2019s bandwidth by truncating the signal to highest possible bitrate. Instead of compressing a handful of variants, one needs only to compute the ordered representation once in advance, and truncate it to the appropriate length at transmission time. If this desired bitrate changes over time, the quality could be correspondingly adjusted in a smooth fashion. As above, given a distribution pB(\u00b7) of bandwidths, finding the ordered representation that solves Equation (16) minimizes the expected distortion over a population of recipients."}, {"heading": "6.1. Empirical results", "text": "In Figure 5(a), we qualitatively evaluate continuousdegradation lossy compression with ordered representations. We trained a single-layer 3072-1024-3072 autoencoder with nested dropout on CIFAR-10, and produced reconstructions for different code lengths. Each column represents a different image and each row represents a different code length. As the code length increases (downwards in the figure), the reconstruction quality increases. The images second-to-bottom row look very similar to the original uncompressed images in the bottom row (24576 bits each).\nFigure 5(b) shows ordered representation reconstruction rates as a function of code length for different approaches to the problem. In addition to the above, we also trained a standard autoencoder with the same architecture but without nested dropout. On this we applied 2 different truncation approaches. The first is a simple truncation on the unordered bits. The second is Optimal Brain Damage truncation (LeCun et al., 1990), which removes units in decreasing order of their influence on the reconstruction objective, measured in terms of the first and second order terms in its Taylor expansion. This is a clever way of ordering units, but is disjoint from the training procedure and is only applied retroactively. We also compare with JPEG compres-\nsion. We use the libjpeg library and vary the JPEG quality parameter. Higher quality parameters result in larger file sizes and lower reconstruction error. Note that JPEG is not suited for the 32x32 pixel images we use in this study; its assumptions about the spectra of natural images are violated by such highly down sampled images which have lost significant low frequency content. When the quality is extended to its maximum, the loss is approximately 0.0003, in the same units (not shown)."}, {"heading": "7. Discussion and future work", "text": "We have presented a novel technique for learning representations in which the dimensions have a known ordering. This procedure is exactly equivalent to PCA for shallow autoencoders, but can generalize to deep networks as well. This enables learned representations of data that are adaptive in the sense that they can be truncated with the assurance that the shorter codes contain as much information as possible. Such codes are of interest in applications such as retrieval and compression.\nThe ordered representation retrieval approach can also be used for efficient supervised learning. Namely, it allows performing k-nearest-neighbors on very long codes in logarithmic time in their cardinality. This idea can be combined with various existing approaches to metric learning of kNN and binarized representations (Norouzi et al., 2012; Salakhutdinov & Hinton, 2007; Weinberger & Saul, 2009). The purely unsupervised approaches we have described here have not been empirically competitive with state of the art supervised methods from deep learning. We are optimistic that nested dropout can be meaningfully combined with supervised learning, but leave that for future work.\nWe also note that ordered representations provide insight into how one might practically train models with an infinite number of latent dimensions, in the spirit of Bayesian nonparametric methods. For example, the distribution pB(\u00b7) can be chosen to have infinite support, while having finite mean and variance. Finally, the nesting idea can be generalized to more complicated dependency structures, such as those described in Tarlow et al. (2012).\nAcknowledgements We are grateful to Hugo Larochelle for insightful conversations. This work was partially funded by DARPA Young Faculty Award N66001-12-14219."}, {"heading": "Appendix A. Proofs for Section 3.2", "text": "Theorem 1. Every optimal solution of the nested dropout problem is necessarily an optimal solution of the standard autoencoder problem.\nProof. Let the nested dropout autoencoder be of latent dimension K . Recall that the nested dropout objective function in Equation (11) is a strictly positive mixture of the K different b-truncation problems. As described in Subsection 3.1, an optimal solution to each b-truncation must be of the form X\u2217b = T b\u03a3\u2193bR T ,\u0393\u2217b = Q\u2193bT \u22121 b for some invertible transformation T b. We note that the PCA decomposition is a particular optimal solution for each b that is given for the choice T b = Ib. As such, the PCA decomposition exactly minimizes every term in the nested dropout mixture, and therefore must be a global solution of the nested dropout problem. This means that every optimal solution of the nested dropout problem must exactly minimize every term in the nested dropout mixture. In particular, one of these terms corresponds to the K-truncation problem, which is in fact the original autoencoder problem.\nDenote T \u2193b = JK\u2192bTJ T K\u2192b as the b-th leading principal minor and its its bottom right corner as tb = Tbb. Lemma 1. Let T \u2208 RK\u00d7K be commutative in its truncation and inversion. Then all the diagonal elements of T are nonzero, and for each b = 2, . . . ,K , either Ab = 0 or Bb = 0.\nProof. We have detT \u2193b = detT \u2193b\u22121 det(tb \u2212 BbT \u22121 \u2193b\u22121Ab) 6= 0 since T \u2193b\u22121 is invertible. Since T \u2193b\u22121 is also invertible, then tb \u2212 BbT \u22121 \u2193b\u22121Ab 6= 0. As such, we write T \u2193b in terms of blocks T \u2193b\u22121,Ab,Bb, tb, and apply blockwise matrix inversion to find that T\u22121\u2193b\u22121 = T\u22121\u2193b\u22121 + T \u22121 \u2193b\u22121Ab(tb \u2212BbT \u22121 \u2193b\u22121Ab) \u22121BbT \u22121 \u2193b\u22121 which reduces to AbBb = 0. Now, assume by contradiction that tb = 0. This means that either bottom row or the rightmost column of T \u2193b must be all zeros, which contradicts with the invertibility of T \u2193b.\nTheorem 2. Every optimal solution of the nested dropout problem must be of the form\nX\u2217 = T\u03a3RT (17)\n\u0393\u2217 = QT\u22121 , (18)\nfor some matrix T \u2208 RK\u00d7K that is commutative in its truncation and inversion.\nProof. Consider an optimal solution X\u2217,\u0393\u2217 of the nested dropout problem. For each b-truncation, as established in the proof of Theorem 1, it must hold that\nX\u2217b = T bJK\u2192b\u03a3R T (19)\n\u0393\u2217b = QJ T K\u2192bT \u22121 b . (20)\nHowever, it must also be true that Xb = X\u2193b,\u0393b = \u0393\u2193b by the definition of the nested dropout objective in Equation (11). The first equation thus gives that T bJK\u2192b =\nJK\u2192bTK , and therefore T b = JK\u2192bTKJ T K\u2192b = T \u2193b. This establishes the fact that the optimal solution for each b-truncation problem simply draws the b-th leading principal minor from the same \u201cglobal\u201d matrix T := TK . The second equation implies that for every b, it holds that JK\u2192bT \u22121JTK\u2192b = (JK\u2192bTJ T K\u2192b)\n\u22121 and as such T is commutative in its truncation and inversion.\nTheorem 3. Under the orthonormality constraint \u0393T\u0393 = IK , there exists a unique optimal solution for the nested dropout problem, and this solution is exactly the set of the K top eigenvectors of the covariance of Y , ordered by eigenvalue magnitude. Namely, X\u2217 = \u03a3RT ,\u0393\u2217 = Q.\nProof. The orthonormality constraint implies (T\u22121Q)TQT\u22121 = IK which gives T\nT = T\u22121. Hence every row and every column must have unit norm. We also have have that for every b = 1, . . . ,K\nT T\u2193b = (JK\u2192bTJ T K\u2192b) T (21)\n= JK\u2192bT TJTK\u2192b (22) = JK\u2192bT \u22121JTK\u2192b (23) = (JK\u2192bTJ T K\u2192b) \u22121 (24) = T\u22121\u2193b (25)\nwhere in the last equation we applied Lemma 1 to Theorem 2. As such, every leading principal minor is also orthonormal. For the sake of contradiction, assume there exist some m,n,m 6= n such that Tmn 6= 0. Without loss of generality assume m < n. Then\n\u2211n\u22121 p=1 T 2 mp < 1, but this violates\nthe orthonormality of T n\u22121. Thus it must be that the diagonal elements of T are all identically 1, and therefore T = IK . The result follows."}], "references": [{"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["H. Bourlard", "Y. Kamp"], "venue": "Manuscript M217,", "citeRegEx": "Bourlard and Kamp,? \\Q1987\\E", "shortCiteRegEx": "Bourlard and Kamp", "year": 1987}, {"title": "An analysis of singlelayer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "In Proc. of AISTATS,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["Datar", "Mayur", "Immorlica", "Nicole", "Indyk", "Piotr", "Mirrokni", "Vahab S"], "venue": "In Proceedings of the Twentieth Annual Symposium on Computational Geometry,", "citeRegEx": "Datar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Datar et al\\.", "year": 2004}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In Proc. of AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Learning binary hash codes for large-scale image search. In Machine Learning for Computer Vision, volume 411 of Studies in Computational Intelligence, pp. 49\u201387", "author": ["Grauman", "Kristen", "Fergus", "Rob"], "venue": null, "citeRegEx": "Grauman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Grauman et al\\.", "year": 2013}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G E Hinton", "Salakhutdinov", "R R"], "venue": "Science,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Using very deep autoencoders for content-based image retrieval", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey E"], "venue": "In ESANN,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2011}, {"title": "Convolutional Networks for Images, Speech and Time Series, pp. 255\u2013258", "author": ["Lecun", "Yann", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Lecun et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1995}, {"title": "Optimal brain damage", "author": ["LeCun", "Yann", "Denker", "John S", "Solla", "Sara A"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "LeCun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1990}, {"title": "Hamming distance metric learning", "author": ["Norouzi", "Mohammad", "Fleet", "David", "Salakhutdinov", "Ruslan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Norouzi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2012}, {"title": "Higher order contractive auto-encoder", "author": ["Rifai", "Salah", "Mesnil", "Gr\u00e9goire", "Vincent", "Pascal", "Muller", "Xavier", "Bengio", "Yoshua", "Dauphin", "Yann", "Glorot"], "venue": "In ECML/PKDD,", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["Rifai", "Salah", "Vincent", "Pascal", "Muller", "Xavier", "Glorot", "Bengio", "Yoshua"], "venue": "In Proc. of ICML,", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Roweis", "Sam T", "Saul", "Lawrence K"], "venue": null, "citeRegEx": "Roweis et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Roweis et al\\.", "year": 2000}, {"title": "Learning a nonlinear embedding by preserving class neighborhood structure", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey"], "venue": "In Proc. of AISTATS,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2007}, {"title": "Fast exact inference for recursive cardinality models", "author": ["Tarlow", "Daniel", "Swersky", "Kevin", "Zemel", "Richard S", "Adams", "Ryan P", "Frey", "Brendan"], "venue": "In 28th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Tarlow et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tarlow et al\\.", "year": 2012}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Tenenbaum", "Joshua B", "de Silva", "Vin", "Langford", "John C"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["Torralba", "Antonio", "Fergus", "Robert", "Freeman", "William T"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Torralba et al\\.,? \\Q1958\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 1958}, {"title": "Visualizing data using t-SNE", "author": ["Van der Maaten", "Laurens", "Hinton", "Geoffrey"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Weinberger", "Kilian Q", "Saul", "Lawrence K"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Weinberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}, {"title": "Spectral hashing", "author": ["Weiss", "Yair", "Torralba", "Antonio", "Fergus", "Robert"], "venue": "In NIPS, pp. 1753\u20131760,", "citeRegEx": "Weiss et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 19, "context": "Representation learning enables one to avoid explicit feature engineering; indeed, approaches to deep feature learning have often found representations that outperform their hand-crafted counterparts (e.g., Lecun & Bengio, 1995; Hinton & Salakhutdinov, 2006; Vincent et al., 2010; Coates et al., 2011).", "startOffset": 200, "endOffset": 301}, {"referenceID": 1, "context": "Representation learning enables one to avoid explicit feature engineering; indeed, approaches to deep feature learning have often found representations that outperform their hand-crafted counterparts (e.g., Lecun & Bengio, 1995; Hinton & Salakhutdinov, 2006; Vincent et al., 2010; Coates et al., 2011).", "startOffset": 200, "endOffset": 301}, {"referenceID": 6, "context": "As with the original dropout formulation (Hinton et al., 2012), nested dropout applies a stochastic mask over models.", "startOffset": 41, "endOffset": 62}, {"referenceID": 2, "context": "Locality sensitive hashing (Datar et al., 2004) seeks to preserve distance information by means of random projections; however, this can lead to very inefficient codes for high input dimensionality.", "startOffset": 27, "endOffset": 47}, {"referenceID": 6, "context": "Dropout (Hinton et al., 2012) is a regularization technique for neural networks that adds stochasticity to the architecture during training.", "startOffset": 8, "endOffset": 29}, {"referenceID": 19, "context": "It was shown by Vincent et al. (2010) that training an autoencoder corresponds to maximizing a lower bound on the mutual information I(y;x) between the input data and their representations.", "startOffset": 16, "endOffset": 38}, {"referenceID": 19, "context": "We inject noise to promote robustness, as in (Vincent et al., 2010); namely, with probability 0.", "startOffset": 45, "endOffset": 67}, {"referenceID": 3, "context": "Glorot et al. (2011) features an in-depth discussion and motivation of rectified linear units.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "The second is Optimal Brain Damage truncation (LeCun et al., 1990), which removes units in decreasing order of their influence on the reconstruction objective, measured in terms of the first and second order terms in its Taylor expansion.", "startOffset": 46, "endOffset": 66}, {"referenceID": 10, "context": "This idea can be combined with various existing approaches to metric learning of kNN and binarized representations (Norouzi et al., 2012; Salakhutdinov & Hinton, 2007; Weinberger & Saul, 2009).", "startOffset": 115, "endOffset": 192}, {"referenceID": 15, "context": "Finally, the nesting idea can be generalized to more complicated dependency structures, such as those described in Tarlow et al. (2012).", "startOffset": 115, "endOffset": 136}], "year": 2014, "abstractText": "In this paper, we study ordered representations of data in which different dimensions have different degrees of importance. To learn these representations we introduce nested dropout, a procedure for stochastically removing coherent nested sets of hidden units in a neural network. We first present a sequence of theoretical results in the simple case of a semi-linear autoencoder. We rigorously show that the application of nested dropout enforces identifiability of the units, which leads to an exact equivalence with PCA. We then extend the algorithm to deep models and demonstrate the relevance of ordered representations to a number of applications. Specifically, we use the ordered property of the learned codes to construct hash-based data structures that permit very fast retrieval, achieving retrieval in time logarithmic in the database size and independent of the dimensionality of the representation. This allows codes that are hundreds of times longer than currently feasible for retrieval. We therefore avoid the diminished quality associated with short codes, while still performing retrieval that is competitive in speed with existing methods. We also show that ordered representations are a promising way to learn adaptive compression for efficient online data reconstruction.", "creator": "LaTeX with hyperref package"}}}