{"id": "1402.4678", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2014", "title": "When Learners Surpass their Sources: Mathematical Modeling of Learning from an Inconsistent Source", "abstract": "We present a new algorithm to model and investigate the learning process of a learner mastering a set of grammatical rules from an inconsistent source. The compelling interest of human language acquisition is that the learning succeeds in virtually every case, despite the fact that the input data are formally inadequate to explain the success of learning. Our model explains how a learner can successfully learn from or even surpass its imperfect source without possessing any additional biases or constraints about the types of patterns that exist in the language. We use the data collected by Singleton and Newport (2004) on the performance of a 7-year boy Simon, who mastered the American Sign Language (ASL) by learning it from his parents, both of whom were imperfect speakers of ASL. We show that the algorithm possesses a frequency-boosting property, whereby the frequency of the most common form of the source is increased by the learner. We also explain several key features of Simon's ASL.", "histories": [["v1", "Tue, 18 Feb 2014 02:18:10 GMT  (1392kb,D)", "http://arxiv.org/abs/1402.4678v1", "21 pages, 5 figures"]], "COMMENTS": "21 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yelena mandelshtam", "natalia komarova"], "accepted": false, "id": "1402.4678"}, "pdf": {"name": "1402.4678.pdf", "metadata": {"source": "CRF", "title": "When learners surpass their models: mathematical modeling of learning from an inconsistent source", "authors": ["Yelena Mandelshtam", "Natalia L. Komarova"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The ability of children to \u201cimprove\u201d the language of their parents has been widely documented. One famous example comes from the studies of the origins and development of the Nicaraguan Sign Language (Senghas, 1995; Senghas et al., 1997; Senghas & Coppola, 2001). This language was spontaneously developed by deaf children in a number of schools in Western Nicaragua in the 1970s and 1980s. Further, the phenomenon of creolization of pidgin languages has been studied (Andersen, 1983; Thomason & Kaufman, 1991; Sebba, 1997). In the course of one generation, a language is created from a multilingually-derived, limited communication system, where different people only have rudimentary knowledge of each other\u2019s native languages.\nar X\niv :1\n40 2.\n46 78\nv1 [\ncs .C\nL ]\n1 8\nFe b\n20 14\nIt is argued that humans have an ability to improve on the (possibly inconsistent) linguistic input that they receive. The phenomenon of language regularization has been studied in the context of historical linguistics (Kroch, 1989; Kroch & Taylor, 1997; Pearl & Weinberg, 2007). Language over-regularization in children has also received much attention (Marcus et al., 1992; Marcus, 1995; Marchman et al., 1997; Plunkett & Juola, 1999).\nA unique example of a language regularization by a single child is reported in Singleton & Newport (2004). In this paper, the authors analyzed the language of a deaf boy (whom they named Simon). The unique situation of Simon was that his parents were his only sources of American Sign Language (ASL). Because Simon\u2019s parents were not native speakers of ASL (they both learned it after the age of 15), the language that they spoke to Simon had many inconsistencies. However, in a study of Simon, his parents, and other native deaf children, Simon greatly outperformed both of his parents, although he was still somewhat behind the native speakers in certain aspects.\nThe phenomenon of language regularization showcased by Simon was studied in depth by Elissa Newport and her colleagues, in a number of elegant experiments performed with adult and children learners. In Hudson Kam & Newport (2009) and Hudson Kam & Newport (2005), artificial miniature language acquisition from an inconsistent source was studied. The participants learn the language by listening to sentences of the language, which are presented in an inconsistent fashion (allowing for a probabilistic usage of several forms). The structure and complexity of the probabilistic input varies from experiment to experiment. The goal is to assess what kinds of input are most consistent with the tendency of adults and children to regularize. The authors also evaluate the differences in the learning patterns between adults and children. It was shown that children achieved higher degrees of regularization than adults, and that the degree of regularization varied in a predictable fashion depending on the structure of the source.\nNewport and colleagues used the terms \u201cfrequency boosting\u201d and \u201cfrequency matching\u201d to describe the amount of regularization exhibited by learners. Let us suppose that the \u201cteacher\u201d (or the source of the linguistic input) is inconsistent, such that it probabilistically uses several forms of a certain rule. Frequency boosting is the ability of a language learner to increase the frequency of usage of a particular form compared to the source. Frequency matching happens when the learner reproduces the same frequency of usage as the source.\nMathematical modeling of frequency boosting has been performed by Reali & Griffiths (2009). By using the iterated learning model (see Kirby (1999, 2001); Brighton (2002); Smith et al. (2003); Kirby et al. (2004)) with rational Bayesian agents (Griffiths & Kalish, 2007), it was shown that language regularization can be achieved in the course of several generations of learners. This is an important mechanism that has also been documented by Smith & Wonnacott (2010), which studied the gradual, cumulative population-level processes giving rise to language regularity.\nIn this paper we provide a mathematical framework that allows us to model regularization achieved within a single generation. We create a model (of the reinforcement-learner type) that allows us to study Simon\u2019s learning behavior, as well as other situations of language regularization in adults and children.\nThis rest of the paper is organized as follows. In Section 2 we motivate the discussion of learning algorithms, and introduce a particular algorithm of the reinforcement type. Section 3 studies properties of this algorithm analytically and numerically. In particular, its frequency boosting property is demonstrated, and the speed of convergence investigated. Section 4 uses the algorithm to discuss the performance of Simon in Singleton & Newport (2004). Section 5 contains discussion and conclusions."}, {"heading": "2 The learning alorithm", "text": ""}, {"heading": "2.1 Learning algorithms, a description of the concept", "text": "In creating a mathematical model, it is assumed that the learner has to master a number of different grammatical rules. It can be further assumed that they are independent of each other, that is, that the learning of one rule does not depend on the state of knowledge of the rest of the rules (this is a simplification). Because of the latter assumption, different rules will be considered separately, which simplifies the picture.\nThis study will concentrate on the process of learning of one rule. In general, each rule could have multiple variants (forms). The input in this context is a number of applications of the rules. Of course, total linguistic input will contain sentences which do not contain applications of the given rule. Such sentences do not contain any information regarding the rule, and thus should be ignored. Thus the relevant part of the input can be presented\nas a string of numbers, each number representing a given form being used. So, the ith number written in the string of numbers represents the ith form that the learner was exposed to. For example, if a given rule has two forms, a particular linguistic input may be 1212222212112, that is, the first time the learner was exposed to form 1, then the second time to form 2, etc. The task of the learner is to evaluate the input and create an output, which reflects the input in some way.\nThere may be different algorithms which lead to different results of learning, even if the same input is used. An example is the following algorithm (Algorithm A1): Consider the first application of the rule, and ignore all the rest of the information. If this algorithm is implemented, the learner will always learn the form of the rule that (s)he first encounters. This is not a realistic algorithm, but an example of a valid, well defined learning procedure.\nAnother example is Algorithm A2: Use all the input received. Count how many times forms 1 and 2 were used. Then use forms 1 and 2 randomly, with the same proportion.\nAlgorithms A1 and A2 defined above are in some sense opposites of each other. Algorithm A1 leads to perfect consistency of learning, that is, the learner will always use the same rule, regardless of the degree of inconsistency of the source. Algorithm A2 will retain the degree of inconsistency of the source. Also, the two algorithms have very different computational requirements for the learner: Algorithm 1 only needs to retain the information from the first application of the rule, while Algorithm 2 needs to remember and analyze a whole string of input.\nAlgorithm A1 has a serious flaw. Consider the following input: 2111111111111, that is, the first sentence contains form 2, and the rest will contain form 1. Algorithm A1 will result in form 2, whereas the source is almost perfectly consistent (except for one application) in using form 1. Since one of the goals is to learn a language somewhat close to that of the source, Algorithm A1 does not do a good job. It learns (consistently) a wrong rule. It can also be seen in the following way: Algorithm A1 is not robust in terms of slight errors of the source. Even if the source is completely consistent, the algorithm must allow for occasional errors (or imperfections in communication). The first application of the rule in 2111111111111 could be just the result of bad communication (noise). Algorithm A1 has no way to buffer that. Algorithm A2 does not have this problem, because for a sufficiently long string of input, errors will make a negligible effect on the result.\nNeither of the algorithms exhibit a frequency boosting property consistently. That is, Algorithm A1 \u201cboosts\u201d the frequency of the form that happened to be used first by the source (which may or may not correspond to the more frequent form). Algorithm 2 has a frequency matching property by construction.\nIn oder to describe the frequency boosting/regularization behavior observed in human learners, it is desirable to design a learning algorithm which meets the following intuitive requirements:\n1. Learn the rule which is close to that of the source. If the source is consistent, then it should be the same rule with a high probability.\n2. Improve the consistency of the source.\n3. Be robust with respect to errors of communication/noise.\n4. Be computationally inexpensive.\nIn the next section we introduce an algorithm with these properties."}, {"heading": "2.2 Formulation of the algorithm", "text": "The key idea of the algorithm presented in this work is that the states that are being modified by the probabilistic input are themselves probabilities.\nSuppose a rule only has two forms, form 1 and form 2. At each instance of time, the learner is characterized by two numbers, X1 \u2265 0 and X2 \u2265 0, each corresponding to probability of the usage of the corresponding form of the rule (we further impose X1 + X2 > 0). In particular, the probability of the learner to use form 1 is given by X1/(X1+X2), and the probability of the learner to use form 2 is given by X2/(X1 +X2). One can see that if X1 = 0, then form 2 is always used (the learner\u2019s language is consistent with respect to the rule in question). Similarly, with X2 = 0, only form 1 is used.\nEach time an instance of the rule application is received, the learner updates the values X1 and X2. There are many ways in which this can be done, see reinforcement-learning models (Sutton & Barto, 1998; Norman, 1972). Reinforcement models have played an important role in modeling many aspects of cognitive and neurological processes, see e.g. Maia (2009); Lee et al. (2012). This work presents a novel algorithm that satisfies regulations outlined above. Let us set X1 +X2 = L, where L is a given positive integer.\nIf form 1 was used by the source, then the learner updates in the following way: if X1 < L then\nX1 \u2192 X1 + 1, X2 \u2192 X2 \u2212 1,\notherwise, no change (X1 \u2192 X1, X2 \u2192 X2). If form 2 was used, then the learner updates as follows: if X2 < L then\nX1 \u2192 X1 \u2212 1, X2 \u2192 X2 + 1,\notherwise no change. This algorithm can be easily generalized to several forms of the same rule. Suppose that there are in total M different ways in which the rule can be used (forms 1, ...,M). Then, at each instance of time, the learner is characterized by a vector of M nonnegative numbers, X = [X1, ..., XM ], such that \u2211M i=1Xi = L; we say that the vector X/L belongs to an M -dimensional simplex. After each exposure to the rule usage, the values Xi are updated. If form j is used by the source, the learner applies the following update rule:\nif i 6= j,Xi \u2192 Xi \u2212 \u03b4\u2212i , \u03b4\u2212i =\n{ s\nM\u22121 , if Xi > s\nM\u22121 Xi, otherwise\nif i = j,Xj \u2192 Xj + \u03b4+, \u03b4+ = \u2211 i 6=j \u03b4\u2212i . (1)\nHere the parameter 0 < s < L defines the increment of a learning update. Clearly, multiplying s and L by the same number does not change the algorithm, so it will be assumed that s = 1."}, {"heading": "3 Properties of the learning algorithm", "text": "An important application in our context is to consider a teacher-learner pair. In general, the teacher does not necessarily have to be one person, it could be a number of people. The only requirement is that the statistics of the source do not change in time. To be more precise, suppose that the source is characterized by nonnegative numbers \u03bd1, ..., \u03bdM , with \u2211M i=1 \u03bdi = 1. These are interpreted as probabilities of the source to emit each of the M forms of the rule. Thus, the source can be thought of as being just one agent (whose values of Xi are \u201cfrozen,\u201d that is, not updated), or it could be a collection of\nsuch non-updating agents. The situation with the teacher-learner pair can be used to model the language acquisition of a child from one or two parents and other adults whose language is frozen. In particular, it can be applied to the situation of Simon who learned only from his two parents. It is also directly applicable to the experiments in which adults or children are learning from an artificial/inconsistent source, such as in papers Hudson Kam & Newport (2009, 2005)."}, {"heading": "3.1 Analytical Results", "text": "Let us consider the case M = 2, or two alternative forms of the rule. We denote by \u03bd the probability of the source to use form 1; form 2 is used with probability 1\u2212 \u03bd. The reinforcement learner algorithm described above can be studied as a Markov chain with states 0, 1/L, 2/L...1, where the constant L appears in the description of the algorithm, Section 2.2. The states are probabilities, that is, if the Markov Chain is in state i/L, the probability that the learner will use form 1 is i/L. This chain can be modeled by the distribution vector [X0, X1, ..., XL], at any point in time, where Xi represents the probability that the chain is in state i/L. The distribution vector is updated from step to step by multiplying by the transition matrix, A. In this case, the transition matrix will only have elements \u03bd - the probability that the source uses form 1, meaning that the chain will move to the next state, 1\u2212 \u03bd = \u00b5 - the probability that the source uses form 2, meaning that the chain will move to the previous state, and 0, since for all other cases, it is impossible to move directly from one state to another one. We have\nA =  \u00b5 \u03bd 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 \u00b5 0 \u03bd 0 \u00b7 \u00b7 \u00b7 0 0 \u00b5 0 \u03bd \u00b7 \u00b7 \u00b7 0 ... 0 . . . 0 . . . ... ...\n. . . 0 \u03bd 0 \u00b7 \u00b7 \u00b7 0 0 \u00b5 \u03bd  Let \u03a0 be the stationary distribution vector of the matrix A. No matter what the original distribution was, as the number of steps goes to infinity, the vector will become infinitely close to the stationary distribution.\nCall P the weighted average of the states, or the probability that a learner will use the correct form after a very large number of sentences given by the\nsource. P can be found by the formula\nP (L, \u03bd) = L\u2211 i=0 i L \u03a0i. (2)\nWe have the following result.\nTheorem 3.1. For all L, the expected frequency of the learner in the quasisteady state is given by\nP (L, \u03bd) = 1 + 1\nL\n( L+ 1\n\u03bbL+1 \u2212 1 \u2212 1 \u03bb\u2212 1\n) , (3)\nwhere \u03bb = \u03bd/(1\u2212 \u03bd).\nProof: In order to find P , it is necessary to find \u03a0 first. The stationary distribution \u03a0 satisfies\n[\u03a00,\u03a01,\u03a02....,\u03a0L]  \u00b5 \u03bd 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 \u00b5 0 \u03bd 0 \u00b7 \u00b7 \u00b7 0 0 \u00b5 0 \u03bd \u00b7 \u00b7 \u00b7 0 ... 0 . . . 0 . . . ... ...\n. . . 0 \u03bd 0 \u00b7 \u00b7 \u00b7 0 0 \u00b5 \u03bd\n = [\u03a00,\u03a01,\u03a02....,\u03a0L]\nwhich is equivalent to a system of equations: \u00b5\u03a00 + \u00b5\u03a01 = \u03a00 \u03bd\u03a00 + \u00b5\u03a02 = \u03a01 ... \u03bd\u03a0L\u22122 + \u00b5\u03a0L = \u03a0L\u22121\n\u03bd\u03a0L\u22121 + \u03bd\u03a0L = \u03a0L\nThe first equation is solved to get \u03a01 = 1\u2212\u00b5 \u00b5 \u03a00. Because 1\u2212 \u00b5 = \u03bd, this can be re-written as: \u03a01 = \u03bd \u00b5 \u03a00. Using the new value for \u03a01, the next equation is solved to obtain \u03a02 = \u03bd\u2212\u03bd\u00b5 \u00b52 \u03a00, which can also be written as \u03bd2 \u00b52 . To solve for the general term, induction is used with base case \u03a01 = \u03bd \u00b5 \u03a00. Assume \u03a0i = \u03bdi\n\u00b5i \u03a00 and \u03a0i+1 =\n\u03bdi+1 \u00b5i+1 \u03a00, then\n\u03bd( \u03bdi\n\u00b5i )\u03a00 + \u00b5(\u03a0i+2) =\n\u03bdi+1 \u00b5i+1 \u03a00.\nThis is solved to get:\n\u03a0i+2 = \u03bdi+2\n\u00b5i+2 \u03a00.\nSo, by induction, each term of the stationary distribution has the form,\n\u03a0m = \u03bdm\n\u00b5m \u03a00, m = 0, ..., L.\nThe last equation in the system is used to confirm this. Therefore, the stationary state of the matrix is:\n[\u03a00, \u03bd\n\u00b5 \u03a00,\n\u03bd2 \u00b52 \u03a00, ..., \u03bdL \u00b5L \u03a00], (4)\nwhich is also the distribution of probabilities for the learner to be in state values 0/L, 1/L..., L/L as n(the number of steps) goes to \u221e.\nBecause the vector is a distribution of probabilities, its components add up to one. Thus, it is possible to solve for \u03a00:\n\u03a00 + \u03bd\n\u00b5 \u03a00 + ...+\n\u03bdL \u00b5L \u03a00 = 1\nor\n\u03a00( 1\u2212 \u03bd \u00b5 L+1\n1\u2212 \u03bd \u00b5\n) = 1.\nIt is convenient to introduce a new parameter, \u03bb = \u03bd \u00b5 , obtaining\n\u03a00 = 1\u2212 \u03bb\n1\u2212 \u03bbL+1 (5)\nNow, to find the weighted average, P (L, \u03bd), it is necessary to use formula (2). By (4), (5), it is equal to:\nP (L, \u03bd) = 1\u2212 \u03bb\nL(1\u2212 \u03bbL+1) L\u2211 i=0 i\u03bbi.\nSolve and obtain equation (3).\nThe following theorem rigorously establishes the boosting effect in the case with two forms.\nTheorem 3.2. For all L and \u03bd > 1/2, P (L, \u03bd) > \u03bd.\nProof: We need to prove that the expression in (3) is greater than \u03bd, that is, that P (L, \u03bd)/\u03bd > 1.\nNote that \u03bd can be expressed through \u03bb as \u03bd = \u03bb 1+\u03bb\n. Therefore, it is sufficient to prove the inequality:\n1 + 1\nL\n( L+ 1\n\u03bbL+1 \u2212 1 \u2212 1 \u03bb\u2212 1\n) > \u03bb\n1 + \u03bb ,\nwhich, with some algebraic manipulation, taking into consideration the fact that \u03bb > 1, can be transformed into\nL\u03bbL+1 + \u03bb+ L\u03bb+ 1 > L+ L\u03bbL + \u03bbL + \u03bbL+1.\nNow, introduce functions\nf(\u03bb) = L\u03bbL+1 + \u03bb+ L\u03bb+ 1 and g(\u03bb) = L+ L\u03bbL + \u03bbL + \u03bbL+1.\nIt follows that f(1) = 2L + 2 = g(1), and also f \u2032(\u03bb) = L(L + 1)\u03bbL + 1 + L and g\u2032(\u03bb) = L2\u03bbL\u22121 + L\u03bbL\u22121 + (L + 1)\u03bbL. When \u03bb = 1, we have f \u2032(\u03bb) = (L+ 1)2 = g\u2032(\u03bb). The second derivatives of f and g are:\nf \u2032\u2032(\u03bb) = L2(L+1)\u03bbL\u22121 and g\u2032\u2032(\u03bb) = L2(L\u22121)\u03bbL\u22122+L(L\u22121)\u03bbL\u22122+L(L+1)\u03bbL\u22121.\nSince by definition the state space (L) is greater than one, and \u03bb > 1, it follows that (L\u22121)\u03bb > L\u22121. Thus L2(L+1)\u03bbL\u22121 > L(L+1)(L+\u03bb\u22121)\u03bbL\u22122, which implies f \u2032\u2032(\u03bb) > g\u2032\u2032(\u03bb) for \u03bb > 1.\nTherefore, because f \u2032\u2032(\u03bb) > g\u2032\u2032(\u03bb) for \u03bb > 1, and f \u2032(1) = g\u2032(1) and f(1) = g(1) we have f(\u03bb) > g(\u03bb) for all \u03bb > 1, by the fundamental theorem of calculus. This means that\nP (L, \u03bd) > \u03bd, \u03bd > 1/2,\nthat is, the algorithm has the boosting property.\nThe following theorem shows that as the state space gets sufficiently large, the boosting effect is maximized, resulting in a perfectly consistent output.\nTheorem 3.3. For all \u03bd > 1/2, lim L\u2192\u221e P (L, \u03bd) = 1.\nProof: By evaluating the limit as L \u2192 \u221e in equation (3) by standard methods under the assumption that \u03bb > 1, we obtain the desired result."}, {"heading": "3.2 Numerical Simulations", "text": "The reinforcement learner algorithm contains only one parameter, L. It is more intuitive to talk about the quantity\ns = 1\nL ,\nthe increment of learning. This is the amount by which the probabilities change at each step, following the source\u2019s input. The analytical results reported in the previous section can be summarized as follows. In the case where M = 2 (two forms of the rule), as the number of steps increases, the learner converges to a quasi-stationary state, where it is characterized by the frequency of form 1 given by equation (3). This frequency is higher than the frequency of the source, \u03bd (the frequency boosting effect). The frequency boosting effect becomes more pronounced as s\u2192 0, that is, for small values of the increment.\nTo find out numerically whether the reinforcement learner algorithm possesses a source boosting property for M > 2, and also to investigate other properties of the algorithm, probability vector components of the learner were computed numerically as a function of various parameters. Repeated teacher-learner interactions were simulated, and after each interaction, the\ncurrent probability vector of the learner was recorded. A Fortran computer code written computed the output frequency, i.e. the probability that the learner would use the correct form. The output frequency was computed as a function of time (the number of \u201csentences\u201d learned from the source), the size of the increment, and input frequency.\nFigure 1(a) shows the agreement of the analytical prediction (3) with the numerical simulations, in the case of two forms, M = 2. We can see from figure 1(b) that as s increases, the boosting property becomes weaker. Figure 2 numerically demonstrates the existence of the boosting property for higher numbers of M . As in the M = 2 case, the frequency boosting effect becomes more pronounced as s\u2192 0.\nNext, the dependence of the convergence time on the input frequency was studied. Figure 3(a) shows some typical runs for several values of the learning increment, s. It is clear thas as s increases, the algorithm converges faster. This was investigated formally in figure 3(b). The convergence time was calculated by computing output probabilities taking a running average over 200 steps to smooth out the curve, and then by defining the time to converge as the first step, n, such that the output frequency at n steps is within .001 of the analytically calculated expected output frequency. All of the data was averaged over 200 trials. To investigate how the convergence time depends on the increment size, figure 3(b) plots the convergence time as a function of s. We can see that convergence time decreases as s increases. The same trend holds for multi-form inputs, see figure 4(a) where we plot the current frequency of learners for M = 3, for four typical runes with different increments s. Figure 4(b) plots the convergence time as a function of the input frequency for M = 2. It decreases with increasing input frequency.\nTo summarize, as the increment s decreases, the boosting effect increases, but the speed of convergence decreases. Further, the speed of convergence increases with the input frequency."}, {"heading": "4 Applications: Simon", "text": "The numerical and the analytical studies both showed that the reinforcement learner algorithm possesses a source boosting property. In particular, when there are two forms, the learner boosts the one that is used more than 1/2 of the time, and more generally, if there are M forms, the learner boosts the dominant form, even if it is used with a frequency barely greater than\n1/M . This result explains that it is possible for a learner to surpass its source without possessing any innate sense of grammar. The study of the dependence of the convergence time on the input frequency or the state space allows to determine how many input sentences are required to achieve the boosting under various learning scenarios. These observations can help one to explain certain learning phenomena, such as the reason why those learning from an inconsistent input learn more slowly than those learning from a consistent input, and by how much the speed is slowed down.\nIn particular, these results are in agreement with the observational studies of (Singleton & Newport, 2004). Simon was tested on 7 movements, which were split into 3 categories: handshape, motion, and location. Figure 5(a) presents some results from Singleton and Newport\u2019s study on Simon\u2019s performance compared to that of his parents, categorized by different morphemes (different rules). Simon\u2019s results surpassed those of his parents in both motion and location (which were both in the 65-75% range) by about 20%, boosting the score to the score of the native learners. Thus, Simon greatly outpreformed his parents, even though they were his only sources. This can be explained by the boosting effect described here. Figure 5(b) presents the same data as in part (a) of the figure, plotted in a different way. Percentage use for the parents (the horizontal coordinates of the dots) were computed by taking the mean of the mother\u2019s and the father\u2019s frequency of use (that is, we asumed that Simon was equally exposed to both parents\u2019 input). The vertuical coordinate is Simon\u2019s percentage of use. The two solid lines represent the fits from our model based on M = 2 and M = 3 forms. Although these fits show that the theory can give results roughly in the ball park of the observations, we must note that the data shown in figure 5 are averages of performance over several morphemes in each class, and further data need to be used to inform more precise model parameterization. It is more instructive to look at individual examples of Simon\u2019s performance, which is done next.\nAn interesting question that was addressed in the paper was why Simon\u2019s performance was characterized by a high degree of frequency boosting for motion and location morphemes, but his frequency boosting was much more modest and even nonexistent for handshape morphemes. Several hypotheses were suggested. Here we provide some mathematical foundation for those ideas.\nOne hypothesis was that \u201cSimon\u2019s parents do not always use the target ASL form as their most frequent response\u201d. It is clear from figure 5(a) that\nfor Motion/Location morphemes, the parents\u2019 most frequent rule variant was indeed the correct one, consistent with the standard ASL. A frequency boosting as described in the previous sections would increase the frequency of that form, leading to a larger percentage of correct usage for Simon. This is consistent with the observations.\nOn the other hand, for handshape morphemes, the situation appears to be more complex. From figure 5(a) we can see that with both types of handshape morphemes, the parents\u2019 performance was particularly poor in terms of the frequency of usage of the correct ASL morphemes. But Simon\u2019s performance is very different for Central Object (CO) and Secondary Object (SO) morphemes. In the case of CO morphemes, Simon does not appear to boost frequency at all, while for SO we can see a considerable degree of frequency boosting.\nTo explain this, we will first take a closer look at individual CO morphemes. It turns out that because both parents use the correct form less than 59% of the time, Simon\u2019s frequency boosing in this case resulted in a decrease in the frequency of the correct variant, and instead in the boosting of an alternative, incorrect form. This made Simon\u2019s usage more consistent than his parents\u2019, but not more correct in the sense of being closer to\nthe standard ASL. The parents\u2019s most consistent sign for \u201cvehicle\u201d was the so-called \u201cB-edge\u2019 which was an incorrect form for ASL, see Singleton & Newport (2004). The father and the mother used it 67% and 47% of the time, which is 57% of the total time if Simon\u2019s input consists of equal proportions of father\u2019s and mother\u2019s speech. Simon\u2019s frequency of this incorrect sign is 73%, which is a result of frequency boosting.\nThis explanation accounts for some instances where Simon\u2019s performance was not closer to the correct ASL than his parents, without being inconsistent with his frequency boosting tendency. However, this hypothesis is not enough to explain all the data.\nCorrect Secondary Object (SO) morphemes were poorly represented in Simon\u2019s parents\u2019 speech. They use the correct forms only 43% and 37% of the time. In contrast to that, Simon\u2019s speech contains 59% of the correct variant. That is, Simon is performing frequency boosting of the variant which is used less that 50% of the time. Another, and more specific example of this is the particular sign for Plane (this sign belongs to the CO category), which his parents used correctly 44% and 11% (28% on average) of the time, while Simon boosted this frequency up to 67%. Such boosting is not possible if we use the M = 2 model of teacher-learner interactions, where there is only one alternative (incorrect) variant of the rule. If there is more than one incorrect form, each of which has a lower frequency than the correct one, then frequency boosting is possible for the correct variant.\nAnother interesting suggestion that apears in the paper is that Simon\u2019s learning happens on a slower scale than that of children learning from native signers. We have checked this hypothesis and found that this is indeed consistent with our algorithm. As demonstrated in figure 4(b), the speed of convergence of the algorithm correlates with the source consistency. If the most frequent variant has a higher value of \u03bd (the source\u2019s dominant frequency), the convergence of the learner will be higher compared to relatively lower frequencies of the source\u2019s leading variant. This mechanism suggests that even though Simon was making progress in frequency boosting his parents\u2019 input, it would take him a longer time to reach the same consistency level than it would take for children learning from native signers. This is consistent with the suggestion that \u201cSimon may still, at age 7, be performing like much younger NN (Native of Native) children on handshape morphology\u201d.\nFinally, the paper suggests that the complexity of the input plays an important role for the speed of learning and accounts for the lack of Simon\u2019s performance when it comes to certain types of morphemes. In particular, it\nis noted that handshape morphemes of the ASL verbs of motion come in two forms, the simpler semantic classifier and the more complex size-and-shape specifier (SASS). Simon performs more poorly when it comes to SASS type morphemes. In our model, the complexity of the rules may influence the speed at which a rule is learned, if we relate it to the increment of learning, s. We can speculate that more complex rules are characterized by smaller increments of learning, thus leading to a larger lag for Simon\u2019s speed of learning compared to that of the children learning from a consistent input.\nAnother way to interpret the influence of rule complexity is to note that in the example given by the paper, the complex SASS morphemes consist of several morphemes, one for shape, one for size etc. Therefore, learning such a morpheme can be represented as learning several separate rules. If in each of the rules Simon is slightly below the performance of the NN children of his age, the multiplicative effect will make his overall scoring for SASS morphemes even lower."}, {"heading": "5 Conclusions", "text": "This paper presents a model that explains the frequency boosting effect observed in children learning a language. It provides a simple algorithm of the reinforcement type that represents successful learning without the learner possessing any explicit innate biases.\nThe boosting property of the algorithm was proved analytically in the case of two forms of the rule, and demonstrated numerically for larger numbers of forms. Convergence speed and its dependence on the parameters and the source complexity was also studied.\nFinally, the findings were discussed in the context of the study Singleton & Newport (2004), to demonstrate that the simple model is capable of explaining several key features of Simon\u2019s performance and learning behavior."}], "references": [{"title": "Pidginization and Creolization as Language Acqui", "author": ["R.W. Andersen"], "venue": null, "citeRegEx": "Andersen,? \\Q1983\\E", "shortCiteRegEx": "Andersen", "year": 1983}, {"title": "Compositional syntax from cultural transmission", "author": ["H. sition. ERIC. Brighton"], "venue": null, "citeRegEx": "Brighton,? \\Q2002\\E", "shortCiteRegEx": "Brighton", "year": 2002}, {"title": "Language evolution by iterated learning with bayesian agents", "author": ["T.L. Griffiths", "M.L. Kalish"], "venue": "Cognitive Science", "citeRegEx": "Griffiths and Kalish,? \\Q2007\\E", "shortCiteRegEx": "Griffiths and Kalish", "year": 2007}, {"title": "Regularizing unpredictable variation: The roles of adult and child learners in language formation and change", "author": ["C.L. Hudson Kam", "E.L. Newport"], "venue": "Language Learning and Development", "citeRegEx": "Kam and Newport,? \\Q2005\\E", "shortCiteRegEx": "Kam and Newport", "year": 2005}, {"title": "Getting it right by getting it wrong: When learners change languages", "author": ["C.L. Hudson Kam", "E.L. Newport"], "venue": "Cognitive Psychology", "citeRegEx": "Kam and Newport,? \\Q2009\\E", "shortCiteRegEx": "Kam and Newport", "year": 2009}, {"title": "From ug to universals: Linguistic adaptation through iterated learning", "author": ["S. Kirby", "K. Smith", "H. Brighton"], "venue": "Studies in Language", "citeRegEx": "Kirby et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kirby et al\\.", "year": 2004}, {"title": "Function, Selection, and Innateness: The Emergence of Language Universals: The Emergence of Language Universals", "author": ["S. Kirby"], "venue": null, "citeRegEx": "Kirby,? \\Q1999\\E", "shortCiteRegEx": "Kirby", "year": 1999}, {"title": "Spontaneous evolution of linguistic structure-an iterated learning model of the emergence of regularity and irregularity", "author": ["S. Kirby"], "venue": "Evolutionary Computation, IEEE Transactions on", "citeRegEx": "Kirby,? \\Q2001\\E", "shortCiteRegEx": "Kirby", "year": 2001}, {"title": "Verb movement in old and middle english: Dialect variation and language contact. In: Parameters of morphosyntactic change pp. 297\u2013325", "author": ["A. Kroch", "A. Taylor"], "venue": null, "citeRegEx": "Kroch and Taylor,? \\Q1997\\E", "shortCiteRegEx": "Kroch and Taylor", "year": 1997}, {"title": "Reflexes of grammar in patterns of language change. Language variation and change", "author": ["A. Kroch"], "venue": null, "citeRegEx": "Kroch,? \\Q1989\\E", "shortCiteRegEx": "Kroch", "year": 1989}, {"title": "Neural basis of reinforcement learning and decision making", "author": ["D. Lee", "H. Seo", "M.W. Jung"], "venue": "Annual review of neuroscience", "citeRegEx": "Lee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2012}, {"title": "Reinforcement learning, conditioning, and the brain: Successes and challenges. Cognitive, Affective", "author": ["T.V. Maia"], "venue": "Behavioral Neuroscience", "citeRegEx": "Maia,? \\Q2009\\E", "shortCiteRegEx": "Maia", "year": 2009}, {"title": "Overregularization in english plural and past tense inflectional morphology: A response to marcus", "author": ["V.A. Marchman", "K. Plunkett", "J. Goodman"], "venue": "Journal of Child Language", "citeRegEx": "Marchman et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Marchman et al\\.", "year": 1997}, {"title": "Overregularization in language acquisition. Monographs of the Society for research in child development", "author": ["G.F. Marcus", "S. Pinker", "M. Ullman", "M. Hollander", "T.J. Rosen", "F. Xu", "H. Clahsen"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1992}, {"title": "Children\u2019s overregularization of english plurals: a quantitative analysis", "author": ["G.F. Marcus"], "venue": "Journal of Child Language", "citeRegEx": "Marcus,? \\Q1995\\E", "shortCiteRegEx": "Marcus", "year": 1995}, {"title": "Markov Processes and Learning Models", "author": ["M. Norman"], "venue": null, "citeRegEx": "Norman,? \\Q1972\\E", "shortCiteRegEx": "Norman", "year": 1972}, {"title": "Input filtering in syntactic acquisition: Answers from language change modeling", "author": ["L. Pearl", "A. Weinberg"], "venue": "Language Learning and Development", "citeRegEx": "Pearl and Weinberg,? \\Q2007\\E", "shortCiteRegEx": "Pearl and Weinberg", "year": 2007}, {"title": "A connectionist model of english past tense and plural morphology", "author": ["K. Plunkett", "P. Juola"], "venue": "Cognitive Science", "citeRegEx": "Plunkett and Juola,? \\Q1999\\E", "shortCiteRegEx": "Plunkett and Juola", "year": 1999}, {"title": "The evolution of frequency distributions: Relating regularization to inductive biases through iterated learning", "author": ["F. Reali", "T.L. Griffiths"], "venue": null, "citeRegEx": "Reali and Griffiths,? \\Q2009\\E", "shortCiteRegEx": "Reali and Griffiths", "year": 2009}, {"title": "Contact languages: Pidgins and creoles", "author": ["M. Sebba"], "venue": null, "citeRegEx": "Sebba,? \\Q1997\\E", "shortCiteRegEx": "Sebba", "year": 1997}, {"title": "Children creating language: How nicaraguan sign language acquired a spatial grammar", "author": ["A. Senghas", "M. Coppola"], "venue": "Psychological Science", "citeRegEx": "Senghas and Coppola,? \\Q2001\\E", "shortCiteRegEx": "Senghas and Coppola", "year": 2001}, {"title": "Argument structure in nicaraguan sign language: The emergence of grammatical devices", "author": ["A. Senghas", "M. Coppola", "E.L. Newport", "T. Supalla"], "venue": "Proceedings of the 21st Annual Boston University Conference on Language Development", "citeRegEx": "Senghas et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Senghas et al\\.", "year": 1997}, {"title": "The development of nicaraguan sign language via the language acquisition process", "author": ["A. Senghas"], "venue": "Proceedings of the 19th Annual Boston University Conference on Language Development pp", "citeRegEx": "Senghas,? \\Q1995\\E", "shortCiteRegEx": "Senghas", "year": 1995}, {"title": "When learners surpass their models: The acquisition of american sign language from inconsistent input", "author": ["J.L. Singleton", "E.L. Newport"], "venue": "Cognitive Psychology 49(4),", "citeRegEx": "Singleton and Newport,? \\Q2004\\E", "shortCiteRegEx": "Singleton and Newport", "year": 2004}, {"title": "Eliminating unpredictable variation", "author": ["K. Smith", "E. Wonnacott"], "venue": null, "citeRegEx": "Smith and Wonnacott,? \\Q2010\\E", "shortCiteRegEx": "Smith and Wonnacott", "year": 2010}, {"title": "Iterated learning: a framework", "author": ["K. Smith", "S. Kirby", "H. Brighton"], "venue": null, "citeRegEx": "Smith et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2003}, {"title": "Language contact, creolization", "author": ["S.G. Press. Thomason", "T. Kaufman"], "venue": null, "citeRegEx": "Thomason and Kaufman,? \\Q1991\\E", "shortCiteRegEx": "Thomason and Kaufman", "year": 1991}], "referenceMentions": [{"referenceID": 22, "context": "One famous example comes from the studies of the origins and development of the Nicaraguan Sign Language (Senghas, 1995; Senghas et al., 1997; Senghas & Coppola, 2001).", "startOffset": 105, "endOffset": 167}, {"referenceID": 21, "context": "One famous example comes from the studies of the origins and development of the Nicaraguan Sign Language (Senghas, 1995; Senghas et al., 1997; Senghas & Coppola, 2001).", "startOffset": 105, "endOffset": 167}, {"referenceID": 0, "context": "Further, the phenomenon of creolization of pidgin languages has been studied (Andersen, 1983; Thomason & Kaufman, 1991; Sebba, 1997).", "startOffset": 77, "endOffset": 132}, {"referenceID": 19, "context": "Further, the phenomenon of creolization of pidgin languages has been studied (Andersen, 1983; Thomason & Kaufman, 1991; Sebba, 1997).", "startOffset": 77, "endOffset": 132}, {"referenceID": 9, "context": "The phenomenon of language regularization has been studied in the context of historical linguistics (Kroch, 1989; Kroch & Taylor, 1997; Pearl & Weinberg, 2007).", "startOffset": 100, "endOffset": 159}, {"referenceID": 13, "context": "Language over-regularization in children has also received much attention (Marcus et al., 1992; Marcus, 1995; Marchman et al., 1997; Plunkett & Juola, 1999).", "startOffset": 74, "endOffset": 156}, {"referenceID": 14, "context": "Language over-regularization in children has also received much attention (Marcus et al., 1992; Marcus, 1995; Marchman et al., 1997; Plunkett & Juola, 1999).", "startOffset": 74, "endOffset": 156}, {"referenceID": 12, "context": "Language over-regularization in children has also received much attention (Marcus et al., 1992; Marcus, 1995; Marchman et al., 1997; Plunkett & Juola, 1999).", "startOffset": 74, "endOffset": 156}, {"referenceID": 9, "context": "The phenomenon of language regularization has been studied in the context of historical linguistics (Kroch, 1989; Kroch & Taylor, 1997; Pearl & Weinberg, 2007). Language over-regularization in children has also received much attention (Marcus et al., 1992; Marcus, 1995; Marchman et al., 1997; Plunkett & Juola, 1999). A unique example of a language regularization by a single child is reported in Singleton & Newport (2004). In this paper, the authors analyzed the language of a deaf boy (whom they named Simon).", "startOffset": 101, "endOffset": 425}, {"referenceID": 9, "context": "The phenomenon of language regularization has been studied in the context of historical linguistics (Kroch, 1989; Kroch & Taylor, 1997; Pearl & Weinberg, 2007). Language over-regularization in children has also received much attention (Marcus et al., 1992; Marcus, 1995; Marchman et al., 1997; Plunkett & Juola, 1999). A unique example of a language regularization by a single child is reported in Singleton & Newport (2004). In this paper, the authors analyzed the language of a deaf boy (whom they named Simon). The unique situation of Simon was that his parents were his only sources of American Sign Language (ASL). Because Simon\u2019s parents were not native speakers of ASL (they both learned it after the age of 15), the language that they spoke to Simon had many inconsistencies. However, in a study of Simon, his parents, and other native deaf children, Simon greatly outperformed both of his parents, although he was still somewhat behind the native speakers in certain aspects. The phenomenon of language regularization showcased by Simon was studied in depth by Elissa Newport and her colleagues, in a number of elegant experiments performed with adult and children learners. In Hudson Kam & Newport (2009) and Hudson Kam & Newport (2005), artificial miniature language acquisition from an inconsistent source was studied.", "startOffset": 101, "endOffset": 1215}, {"referenceID": 9, "context": "The phenomenon of language regularization has been studied in the context of historical linguistics (Kroch, 1989; Kroch & Taylor, 1997; Pearl & Weinberg, 2007). Language over-regularization in children has also received much attention (Marcus et al., 1992; Marcus, 1995; Marchman et al., 1997; Plunkett & Juola, 1999). A unique example of a language regularization by a single child is reported in Singleton & Newport (2004). In this paper, the authors analyzed the language of a deaf boy (whom they named Simon). The unique situation of Simon was that his parents were his only sources of American Sign Language (ASL). Because Simon\u2019s parents were not native speakers of ASL (they both learned it after the age of 15), the language that they spoke to Simon had many inconsistencies. However, in a study of Simon, his parents, and other native deaf children, Simon greatly outperformed both of his parents, although he was still somewhat behind the native speakers in certain aspects. The phenomenon of language regularization showcased by Simon was studied in depth by Elissa Newport and her colleagues, in a number of elegant experiments performed with adult and children learners. In Hudson Kam & Newport (2009) and Hudson Kam & Newport (2005), artificial miniature language acquisition from an inconsistent source was studied.", "startOffset": 101, "endOffset": 1247}, {"referenceID": 1, "context": "By using the iterated learning model (see Kirby (1999, 2001); Brighton (2002); Smith et al.", "startOffset": 62, "endOffset": 78}, {"referenceID": 1, "context": "By using the iterated learning model (see Kirby (1999, 2001); Brighton (2002); Smith et al. (2003); Kirby et al.", "startOffset": 62, "endOffset": 99}, {"referenceID": 1, "context": "By using the iterated learning model (see Kirby (1999, 2001); Brighton (2002); Smith et al. (2003); Kirby et al. (2004)) with rational Bayesian agents (Griffiths & Kalish, 2007), it was shown that language regularization can be achieved in the course of several generations of learners.", "startOffset": 62, "endOffset": 120}, {"referenceID": 1, "context": "By using the iterated learning model (see Kirby (1999, 2001); Brighton (2002); Smith et al. (2003); Kirby et al. (2004)) with rational Bayesian agents (Griffiths & Kalish, 2007), it was shown that language regularization can be achieved in the course of several generations of learners. This is an important mechanism that has also been documented by Smith & Wonnacott (2010), which studied the gradual, cumulative population-level processes giving rise to language regularity.", "startOffset": 62, "endOffset": 376}, {"referenceID": 1, "context": "By using the iterated learning model (see Kirby (1999, 2001); Brighton (2002); Smith et al. (2003); Kirby et al. (2004)) with rational Bayesian agents (Griffiths & Kalish, 2007), it was shown that language regularization can be achieved in the course of several generations of learners. This is an important mechanism that has also been documented by Smith & Wonnacott (2010), which studied the gradual, cumulative population-level processes giving rise to language regularity. In this paper we provide a mathematical framework that allows us to model regularization achieved within a single generation. We create a model (of the reinforcement-learner type) that allows us to study Simon\u2019s learning behavior, as well as other situations of language regularization in adults and children. This rest of the paper is organized as follows. In Section 2 we motivate the discussion of learning algorithms, and introduce a particular algorithm of the reinforcement type. Section 3 studies properties of this algorithm analytically and numerically. In particular, its frequency boosting property is demonstrated, and the speed of convergence investigated. Section 4 uses the algorithm to discuss the performance of Simon in Singleton & Newport (2004). Section 5 contains discussion and conclusions.", "startOffset": 62, "endOffset": 1243}, {"referenceID": 15, "context": "There are many ways in which this can be done, see reinforcement-learning models (Sutton & Barto, 1998; Norman, 1972).", "startOffset": 81, "endOffset": 117}, {"referenceID": 10, "context": "Maia (2009); Lee et al.", "startOffset": 0, "endOffset": 12}, {"referenceID": 10, "context": "Maia (2009); Lee et al. (2012). This work presents a novel algorithm that satisfies regulations outlined above.", "startOffset": 13, "endOffset": 31}], "year": 2014, "abstractText": "We present a new algorithm to model and investigate the learning process of a learner mastering a set of grammatical rules from an inconsistent source. The compelling interest of human language acquisition is that the learning succeeds in virtually every case, despite the fact that the input data are formally inadequate to explain the success of learning. Our model explains how a learner can successfully learn from or even surpass its imperfect source without possessing any additional biases or constraints about the types of patterns that exist in the language. We use the data collected by Singleton & Newport (2004) on the performance of a 7-year boy Simon, who mastered the American Sign Language (ASL) by learning it from his parents, both of whom were imperfect speakers of ASL. We show that the algorithm possesses a frequency-boosting property, whereby the frequency of the most common form of the source is increased by the learner. We also explain several key features of Simon\u2019s ASL.", "creator": "LaTeX with hyperref package"}}}