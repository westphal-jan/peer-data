{"id": "1402.5988", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2014", "title": "Incremental Learning of Event Definitions with Inductive Logic Programming", "abstract": "Event Recognition systems rely on properly engineered knowledge bases of event definitions to infer occurrences of events in time. The manual development of such knowledge is a tedious and error-prone task, thus event-based applications may benefit from automated knowledge construction techniques, %developed in the field of machine learning. such as Inductive Logic Programming (ILP), which combines machine learning with the declarative and formal semantics of First-Order Logic. However, learning temporal logical formalisms, which are typically utilized by logic-based Event Recognition systems is a challenging task, which most ILP systems cannot fully undertake. In addition, event-based data are usually massive and collected at different times and under various circumstances. Ideally, systems that learn from temporal data should be able to operate in an incremental mode, that is, revise prior constructed knowledge in the face of new evidence. Most ILP systems are batch learners, in the sense that in order to account for new evidence they have no alternative but to forget past knowledge and learn from scratch. Given the increased inherent complexity of ILP and the volumes of real-life temporal data, this results to algorithms that scale poorly. In this work we present an incremental method for learning and revising event-based knowledge, in the form of Event Calculus programs. The proposed algorithm relies on abductive-inductive learning and comprises an scalable clause refinement methodology, based on a compressive summarization of clause coverage in a stream of examples. We present an empirical evaluation of our approach on real and synthetic data from a video surveillance application.", "histories": [["v1", "Mon, 24 Feb 2014 21:22:51 GMT  (449kb,D)", "https://arxiv.org/abs/1402.5988v1", null], ["v2", "Sat, 22 Nov 2014 13:24:29 GMT  (347kb,D)", "http://arxiv.org/abs/1402.5988v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["nikos katzouris", "alexander artikis", "george paliouras"], "accepted": false, "id": "1402.5988"}, "pdf": {"name": "1402.5988.pdf", "metadata": {"source": "CRF", "title": "Incremental Learning of Event Definitions with Inductive Logic Programming", "authors": ["Nikos Katzouris", "Alexander Artikis", "George Paliouras"], "emails": ["nkatz@iit.demokritos.gr", "a.artikis@iit.demokritos.gr", "paliourg@iit.demokritos.gr"], "sections": [{"heading": null, "text": "Nikos Katzouris National Center for Scientific Research \u201cDemokritos\u201d and National University of Athens E-mail: nkatz@iit.demokritos.gr\nAlexander Artikis University of Piraeus and National Center for Scientific Research \u201cDemokritos\u201d E-mail: E-mail: a.artikis@iit.demokritos.gr\nGeorge Paliouras National Center for Scientific Research \u201cDemokritos\u201d E-mail: paliourg@iit.demokritos.gr\nar X\niv :1\n40 2.\n59 88\nv2 [\ncs .L\nG ]\n2 2\nN ov\n2 01"}, {"heading": "1 Introduction", "text": "The growing amounts of temporal data collected during the execution of various tasks within organizations are hard to utilize without the assistance of automated processes. Event Recognition (Etzion and Niblett, 2010; Luckham, 2001; Luckham and Schulte, 2008) refers to the automatic detection of event occurrences within a system. From a sequence of low-level events (for example sensor data) an event recognition system recognizes high-level events of interest, that is, events that satisfy some pattern. Event recognition systems with a logic-based representation of event definitions, such as the Event Calculus (Kowalski and Sergot, 1986), are attracting significant attention in the event processing community for a number of reasons, including the expressiveness and understandability of the formalized knowledge, their declarative, formal semantics (Paschke, 2005; Artikis et al, 2012) and their ability to handle rich background knowledge. Using logic programs in particular, has an extra advantage, due to the close connection between logic programming and machine learning in the field of Inductive Logic Programming (ILP) (Muggleton and Raedt, 1994; Lavrac and Dzeroski, 1993). However, such applications impose challenges that make most ILP systems inappropriate.\nSeveral logical formalisms which incorporate time and change employ nonmonotonic operators as a means for representing commonsense phenomena (Mueller, 2006). Normal logic programs with Negation as Failure (NaF) in particular are a prominent non-monotonic formalism. Most ILP learners cannot handle NaF at all, or lack a robust NaF semantics (Sakama, 2000; Ray, 2009). Another problem that often arises when dealing with events, is the need to infer implicit or missing knowledge, for instance the indirect effects of events, or possible causes of observed events. In ILP the ability to reason with missing, or indirectly observable knowledge is called non-Observational Predicate Learning (non-OPL) (Muggleton, 1995). This is a task that most ILP systems have difficulty to handle, especially when combined with NaF in the background knowledge (Ray, 2006). One way to address this problem is through the combination of ILP with Abductive Logic Programming (ALP) (Denecker and Kakas, 2002; Kakas and Mancarella, 1990; Kakas et al, 1993). Abduction in logic programming is usually given a non-monotonic semantics (Eshghi and Kowalski, 1989) and in addition, it is by nature an appropriate framework for reasoning with incomplete knowledge. Although it has a long history in the literature (Ade and Denecker, 1995), only recently has this combination brought about systems such as XHAIL (Ray, 2009), TAL (Corapi et al, 2010) and ASPAL (Corapi et al, 2011b; Athakravi et al, 2013) that may be used for the induction of event-based knowledge.\nThe above three systems which, to the best of our knowledge, are the only ILP learners that address the aforementioned learnability issues, are batch learners, in the sense that all training data must be in place prior to the initiation of the learning process. This is not always suitable for event-oriented learning tasks, where data is often collected at different times and under various circumstances, or arrives in streams. In order to account for new training examples, a batch learner has no alternative but to re-learn a hypothesis from scratch. The cost is poor scalability when \u201clearning in the large\u201d (Dietterich et al, 2008) from a growing set of data. This is particularly true in the case of temporal data, which usually come in large volumes. Consider for instance data which span a large period of time, or sensor data transmitted at a very high frequency.\nAn alternative approach is learning incrementally, that is, processing training instances when they become available, and altering previously inferred knowledge to fit new observations, instead of discarding it and starting from scratch. This process, also known as Theory Revision (Wrobel, 1996), exploits previous computations to speed-up the learning, since revising a hypothesis is generally considered more efficient than learning it from scratch (Biba et al, 2008; Esposito et al, 2000; Cattafi et al, 2010). Numerous theory revision systems have been proposed in the literature \u2013 see (Esposito et al, 2000) for a review \u2014 however their applicability in non-monotonic domains is limited (Corapi et al, 2008). This issue is addressed by recent approaches to theory revision as non-monotonic ILP (Corapi et al, 2008; Maggi et al, 2011; Corapi et al, 2011a), where a non-monotonic learner is used to extract a set of prescriptions, which can in turn be interpreted into a set of syntactic transformations on the theory at hand. However, scaling to the large volumes of today\u2019s datasets or handling streaming data remains an open issue, and the development of scalable algorithms for theory revision has been identified as an important research direction (Muggleton et al, 2012b). As historical data grow over time, it becomes progressively harder to revise knowledge, so that it accounts both for new evidence and past experience. One direction towards scaling theory revision systems is the development of techniques for reducing the need for reconsulting the whole history of accumulated experience, while updating existing knowledge.\nThis is the direction we take in this work. We build on the ideas of nonmonotonic ILP and use XHAIL as the basis for a scalable, incremental learner for the induction of event definitions in the form of Event Calculus theories. XHAIL has been used for the induction of action theories (Sloman and Lupu, 2010; Alrajeh et al, 2010, 2011, 2012, 2009). Moreover, in (Corapi et al, 2008) it has been used for theory revision in an incremental setting, revising hypotheses with respect to a recent, user-defined subset of the perceived experience. In contrast, the learner we present here performs revisions that account for all examples seen so far. We describe a compressive \u201cmemory\u201d structure, incorporated in the learning process, which reduces the need for reconsulting past experience in response to a revision. Using this structure, we propose a method which, given a stream of examples, a theory which accounts for them and a new training instance, requires at most one pass over the examples in order to revise the initial theory, so that it accounts for both past and new evidence.We evaluate empirically our approach on real and synthetic data from an activity recognition application and a transport management application. Our results indicate that our approach is significantly more efficient than XHAIL, without compromising predictive accuracy, and scales adequately to large data volumes.\nThe rest of this paper is structured as follows. Section 2 provides a brief overview of abductive and inductive logic programming. In section 3 we present the Event Calculus dialect that we employ, describe the domain of activity recognition that we use as a running example and show how event definitions may be learnt using XHAIL. In Section 4 we present our proposed method, prove its correctness and present the details of its abductive-inductive mechanism. In Section 5 we discuss some theoretical and practical implications of our approach. In Section 6 we present the experimental evaluation, and finally in Sections 7 and 8 we discuss related work and draw our main conclusions."}, {"heading": "2 Background", "text": "We assume a first-order language as in (Lloyd, 1987) where not in front of literals denotes Negation as Failure (NaF). We call a logic program Horn if it is NaF-free and normal otherwise. For more details on the basic terminology and conventions of logic programming used in this work see Appendix A. We define the entailment relation between normal logic programs in terms of the stable model semantics (Gelfond and Lifschitz, 1988) and in particular its credulous version, under which program \u03a01 entails program \u03a02, denoted by \u03a01 \u03a02, if at least one stable model of \u03a01 is a stable model of \u03a02. Following Prolog\u2019s convention, throughout this paper, predicates and ground terms in logical formulae start with a lower case letter, while variable terms start with a capital letter.\nInductive Logic Programming (ILP) is a subfield of machine learning based on logic programming. Given a set of positive and negative examples represented as logical facts, an ILP algorithm derives a set of non-ground rules which discriminate between the positive and the negative examples, potentially taking into account some background knowledge. Definition 1 provides a formal account.\nDefinition 1 (ILP) An ILP task is a triplet ILP (B,E,M) where B is a normal logic program, E = E+ \u222a E\u2212 is a set of ground literals called positive (E+) and negative (E\u2212) examples and M is a set of clauses called language bias. A normal logic program H is called an inductive hypothesis for the ILP task if H \u2286M and B \u222aH covers the examples, that is, B \u222aH E+ and B \u222aH 2 E\u2212.\nThe language bias mentioned in Definition 1 reduces the complexity of an ILP task by imposing syntactic restrictions on hypotheses that may be learnt. A commonly used language bias in ILP, also employed in this work is mode declarations (Muggleton, 1995). A mode declaration is a template literal that can be placed either in the head or the body of a hypothesis clause and contains special placemarkers for variables and ground terms. A set of mode declarations M defines a language L(M), called mode language. A clause is in L(M) if it is constructed from head and body mode declarations by replacing variable placemarkers by actual variable symbols and ground placemarkers by ground terms. Formal definitions for mode declarations and the mode language are provided in Appendix A. ILP algorithms that use mode declarations work by using L(M) as a search space for clauses, trying to optimize an objective function which takes into account example coverage and hypothesis size. Typically, the search space L(M) is structured via \u03b8-subsumption.\nDefinition 2 (\u03b8-subsumption) Clause C \u03b8-subsumes clause D, denoted C D, if there exists a substitution \u03b8 such that head(C)\u03b8 = head(D) and body(C)\u03b8 \u2286 body(D), where head(C ) and body(C ) denote the head and the body of clause C respectively. Program \u03a01 \u03b8-subsumes program \u03a02 if for each clause C \u2208 \u03a01 there exists a clause D \u2208 \u03a02 such that C D.\n\u03b8-subsumption provides a syntactic notion of generality (Dz\u030ceroski, 2010) which may be used to search for clauses based on their example coverage. Clause C is more general than clause D (resp. D is more specific than C) if C D, in which case the examples covered by D are a subset of the examples covered by C. The generality order between clauses is naturally extended to hypotheses via \u03b8-subsumption between programs.\nGiven an ILP task ILP(B ,E ,M ), a hypothesis H is called incomplete if B\u222aH does not cover some positive examples from E and inconsistent if it covers some negative examples. An inductive hypothesis for the ILP task, that is, a hypothesis that is both complete and consistent, is called correct. An incomplete hypothesis H can be made complete by generalization, that is, a set of syntactic transformations that aim to increase example coverage, and may include the addition of new clauses, or the removal of literals from existing clauses. Similarly, an inconsistent hypothesis can be made consistent by specialization, a process that aims to restrict example coverage and may include removal of clauses from the hypothesis, or addition of new literals to existing clauses in the hypothesis. Theory revision is the process of acting upon a hypothesis by means of syntactic transformations (generalization and specialization operators), in order to change the answer set of the hypothesis (Wrobel, 1996; Esposito et al, 2000), that is, the examples it accounts for. Theory revision is at the core of incremental ILP systems. In an incremental setting, examples are provided over time. A learner induces a hypothesis from scratch, from the first available set of examples, and treats this hypothesis as a revisable background theory in order to account for new examples."}, {"heading": "3 Event Calculus and Machine Learning for Event Recognition", "text": "The Event Calculus (Kowalski and Sergot, 1986) is a temporal logic for reasoning about events and their effects. It is a formalism that has been successfully used in numerous event recognition applications (Paschke, 2005; Artikis et al, 2014; Chaudet, 2006; Cervesato and Montanari, 2000). The ontology of the Event Calculus comprises time points, i.e. integers of real numbers; fluents, i.e. properties which have certain values in time; and events, i.e. occurrences in time that may affect fluents and alter their value. The domain-independent axioms of the formalism incorporate the common sense law of inertia, according to which fluents persist over time, unless they are affected by an event. We call the Event Calculus dialect used in this work Simplified Discrete Event Calculus (SDEC). As its name\nimplies, it is a simplified version of the Discrete Event Calculus, a dialect which is equivalent to the classical Event Calculus when time ranges over integer domains (Mueller, 2008).\nThe building blocks of SDEC and its domain-independent axioms are presented in Table 1. The first axiom in Table 1 states that a fluent F holds at time T if it has been initiated at the previous time point, while the second axiom states that F continues to hold unless it is terminated. initiatedAt/2 and terminatedAt/2 are defined in an application-specific manner. Examples will be presented shortly.\n3.1 Running example: Activity recognition\nThroughout this paper we use the task of activity recognition, as defined in the CAVIAR1 project, as a running example. The CAVIAR dataset consists of videos of a public space, where actors walk around, meet each other, browse information displays, fight and so on. These videos have been manually annotated by the CAVIAR team to provide the ground truth for two types of activity. The first type corresponds to low-level events, that is, knowledge about a person\u2019s activities at a certain time point (for instance walking, running, standing still and so on). The second type corresponds to high-level events, activities that involve more than one person, for instance two people moving together, fighting, meeting and so on. The aim is to recognize high-level events by means of combinations of low-level events and some additional domain knowledge, such as a person\u2019s position and direction at a certain time point.\n1 http://homepages.inf.ed.ac.uk/rbf/CAVIARDATA1/\nLow-level events are represented in SDEC by streams of ground happensAt/2 atoms (see Table 2), while high-level events and other domain knowledge are represented by ground holdsAt/2 atoms. Streams of low-level events together with domain-specific knowledge will henceforth constitute the narrative, in ILP terminology, while knowledge about high-level events is the annotation. Table 2 presents an annotated stream of low-level events. We can see for instance that the person id1 is inactive at time 999, her (x, y) coordinates are (201, 432) and her direction is 270\u25e6. The annotation for the same time point informs us that id1 and id2 are not moving together. Fluents express both high-level events and input information, such as the coordinates of a person. We discriminate between inertial and statically defined fluents. The former should be inferred by the Event Calculus axioms, while the latter are provided with the input.\nGiven such a domain description in the language of SDEC, the aim of machine learning addressed in this work is to automatically derive the Domain-Specific Axioms, that is, the axioms that specify how the occurrence of low-level events affects the truth values of the fluents that represent high-level events, by initiating or terminating them. Thus, we wish to learn initiatedAt/2 and terminatedAt/2 definitions from positive and negative examples from the narrative and the annotation.\nHenceforth, we use the term \u201cexample\u201d to encompass anything known true at a specific time point. We assume a closed world, thus anything that is not explicitly given is considered false (to avoid confusion, in the tables throughout the paper we state both negative and positive examples). An example\u2019s time point will also serve as reference. For instance, three different examples e999, e1000 and e1001 are presented in Table 2. According to the annotation, an example is either positive or negative w.r.t. a particular high-level event. For instance, e1000 in Table 2 is a negative example for the moving high-level event, while e1001 is a positive example.\n3.2 Learning and Revising Event Definitions\nLearning event definitions in the form of domain-specific Event Calculus axioms with ILP poses several challenges. Note first, that the learning problem presented in Section 3.1 requires non-Observational Predicate Learning (non-OPL) (Muggleton, 1995), meaning that instances of target predicates (initiatedAt/2 and terminatedAt/2) are not provided with the supervision. Using abduction to obtain the missing instances is a solution. Abduction is a form of logical inference that seeks to extract a set of explanations that make a set of observations true. In Abductive Logic Programming (ALP) the observations are represented by a set of queries, and one derives explanations for these observations in the form of ground facts that make the queries succeed. Definition 3 provides a formal account.\nDefinition 3 (ALP) An ALP task is a triplet ALP (B,A,G) where B is a normal logic program, A is a set of predicates called abducibles and G is a set of ground queries called goals. A set of ground atoms \u2206 is called an abductive explanation for the ALP task if the predicate of each atom in \u2206 appears in A and B \u222a\u2206 G.\nUsing ALP, the missing supervision for the learning problem of Section 3.1 can be obtained by abducing a set of ground initiatedAt/2 and terminatedAt/2 atoms as explanations for the conjunction of the holdsAt/2 literals of the annotation (see Table 2). In principle, several explanations are possible for a given set of observations.\nTo avoid redundant explanations, ALP reasoners are typically biased towards minimal explanations. For instance, the atom initiatedAt(moving(id1 , id2 ), 1000 ) is a minimal abductive explanation for the holdsAt/2 literals in Table 2.\nSeveral systems have been proposed that combine ILP with abductive reasoning. These systems use abduction to obtain missing knowledge, necessary to explain the provided examples, and then employ standard ILP techniques to construct hypotheses. However most of these systems cannot be used for learning Event Calculus programs. Some of these abductive-inductive systems are restricted to Horn logic (HAIL (Ray et al, 2003), IMPARO (Kimber et al, 2009)). Others can handle negation, but their use of abduction is limited. For instance INTHELEX (Esposito et al, 2000) uses abduction only to generate facts that might be missing from the description of an example, and is otherwise restricted to OPL. PROGOL5 (Muggleton and Bryant, 2000), ALEPH2 and ALECTO (Moyle, 2003) support some form of abductive reasoning but lack the full power of ALP. As a result, they cannot reason abductively with negated atoms (Ray, 2006).\nNaF is responsible for two more shortcomings of traditional ILP approaches w.r.t. normal logic programs. First, as explained in (Ray, 2006), the standard set cover approach on which most ILP systems rely, is essentially unsound in the presence of NaF, meaning that it may return hypotheses that do not cover all the examples. Because of NaF and its non-monotonicity, newly inferred clauses may be invalidated by past examples. At the same time the learner has no way to detect that, because in a set cover approach, designed to operate under the monotonicity of Horn logic, past examples are retracted from memory once they are covered by a clause.\nThe second shortcoming concerns theory revision, and is related to the standard \u03b8-subsumption-based heuristics used in Horn logic, which are known to be inapplicable in general in the case of normal logic programs (Fogel and Zaverucha, 1998). ILP systems construct clauses either in a bottom-up, or a top-down manner, i.e. searching for more general or more specific hypotheses respectively, in a space ordered by \u03b8-subsumption. This is an acceptable strategy to guide the search in Horn logic, because in this case, \u201cmoving up\u201d the subsumption lattice, i.e. from specific to general, increases example coverage, while \u201cmoving down\u201d, from general to specific, restricts example coverage. This does not always hold in normal logic programs, where generalizing (resp. specializing) a single clause in a hypothesis may result in less (resp. more) examples covered by the hypothesis. As a result, revising a hypothesis in a clause-by-clause manner using subsumption to guide the search, cannot be used in full clausal logic. We illustrate the case with a simple example.\nExample 1 Consider the following annotated narrative related to the fighting high-level event from CAVIAR:\nNarrative : happensAt(abrupt(id1), 1). happensAt(abrupt(id2), 1). holdsAt(close(id1, id2, 23), 1). happensAt(walking(id1), 2). happensAt(abrupt(id2), 2). holdsAt(close(id1, id2, 23), 2). Annotation : not holdsAt(fighting(id1, id2), 1). holdsAt(fighting(id1, id2), 2). holdsAt(fighting(id1, id2), 3).\n2 http://web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/\nwhere close(X,Y,D) is a statically defined fluent which states that the Euclidean distance between persons X and Y is less than threshold D. Consider also the clauses:\nC1 = initiatedAt(fighting(X,Y ), T )\u2190 happensAt(abrupt(X), T ), not happensAt(inactive(Y ), T ), holdsAt(close(X,Y, 23), T ).\nC2 = terminatedAt(fighting(X,Y ), T )\u2190 happensAt(walking(X), T ).\nC\u20322 = terminatedAt(fighting(X,Y ), T )\u2190 happensAt(walking(X), T ), not holdsAt(close(X,Y, 23), T ).\nClause C1 states that fighting between two persons id1 and id2 is initiated if one of them exhibits an abrupt behavior, the other is not inactive and their distance is less than 23 pixel positions on the video frame. Clause C2 states that fighting is terminated between two people if one of them walks. Clause C\u20322 is a specialization of C2 and dictates that fighting between two persons is terminated when one of them walks away. Consider two hypotheses H1, H2 where H1 = {C1, C2} and H2 = {C1, C\u20322}. Observe that SDEC \u222a H1 is an incomplete hypothesis, because it does not cover the positive example holdsAt(fighting(id1, id2), 3). Indeed, by means of clause C2 the fluent fighting(id1 , id2 ) is terminated at time 2, and thus it does not hold at time 3. On the other hand, hypothesis SDEC \u222aH2 does cover the positive example at time 3 because clause C\u20322 does not terminate the fighting fluent at time 2. We thus have that hypothesis H2, though more specific than H1, covers more examples.\nRecently, a number of hybrid ILP-ALP systems have been proposed, that are able to overcome the aforementioned shortcomings. XHAIL is one such system, which is at the basis of our approach to learning event definitions from streams of event-based knowledge. We next give a detailed account of XHAIL."}, {"heading": "3.2.1 The XHAIL System", "text": "XHAIL constructs hypotheses in a three-phase process. Given an ILP task ILP(B ,E ,M ), the first two phases return a ground program K, called Kernel Set of E, such that B \u222aK E. The first phase generates the heads of K\u2019s clauses by abductively deriving from B a set \u2206 of instances of head mode declaration atoms, such that B\u222a\u2206 E. The second phase generates K, by saturating each previously abduced atom with instances of body declaration atoms that deductively follow from B \u222a\u2206.\nExample 2 Table 3 presents the process of hypothesis generation by XHAIL, using an example from CAVIAR\u2019s fighting high-level event. The input consists of\nexamples in the from of narrative and annotation, a set of mode declarations and the axioms of SDEC as background knowledge. Mode declarations specify atoms that are allowed in the heads of clauses and literals that are allowed in the bodies of clauses, by being input to the modeh/1 and modeb/1 predicates respectively. Variable and ground placemarkers are indicated by terms of the form +type and #type respectively. Variables in the mode declarations shown in Table 3 are either of type pid , representing the id of a person, or of type time. The only ground term that is allowed in generated literals is of type dist , representing the Euclidean distance between persons.\nThe annotation says that fighting between persons id1 and id2 holds at time 1 and it does not hold at times 2 and 3, hence it is terminated at time 1. Respectively, fighting between persons id3 and id4 holds at time 3 and does not hold at times 1 and 2, hence it is initiated at time 2. XHAIL obtains these explanations for the holdsAt/2 literals of the annotation abductively, using the modeh atoms in the mode declarations as abducible predicates. In its first phase, it derives the two ground atoms in \u22061, presented in Phase 1 of Table 3. In its second phase, XHAIL forms a Kernel Set, as presented in Phase 2 of Table 3, by generating one clause from each abduced atom in \u22061, using this atom as the head, and body literals that deductively follow from SDEC \u222a\u22061 as the body of the clause.\nThe Kernel Set is a multi-clause version of the Bottom Clause, a concept widely used by inverse entailment systems like PROGOL and ALEPH. These systems construct hypotheses one clause at a time, using a positive example as a \u201cseed\u201d, from which a most-specific Bottom Clause is generated by inverse entailment (Muggleton, 1995). A \u201cgood\u201d, in terms of some heuristic function, hypothesis clause is then constructed by a search in the space of clauses that subsume the Bottom Clause. In contrast, the Kernel Set is generated from all positive examples at once, and XHAIL performs a search in the space of theories that subsume it, in order to arrive at a \u201cgood\u201d hypothesis. This is necessary due to the difficulties mentioned in Section 3.2, related to the non-monotonicity of NaF, which are typical of systems that learn one clause at a time. Another important difference between the Kernel Set and the Bottom Clause is that the latter is constructed by a seed example that must be provided by the supervision, while the former can also utilize atoms that are derived abductively from the background knowledge, allowing to successfully address non-OPL problems mentioned in Section 3.2.\nIn order to utilize the Kernel Set as a search space, it first needs to be variabilized. Variabilization is a process that turns each ground clause in the Kernel Set to a clause in the mode language L(M) (Definition 11 of Appendix A), where M denotes the input mode declarations. To do so, each term in a Kernel Set clause that corresponds to a variable, as indicated by the mode declarations, is replaced by an actual variable, while each term that corresponds to a ground term is retained intact.\nExample 3 [Example 2 continued] In Table 3 the variabilized Kernel Set Kv is presented in Phase 2. All variable placemarkers in the mode declarations indicate input (+) variables, meaning that the corresponding variable should either appear in the head of the clause, or be an output (\u2212) variable in some preceding body literal. In the absence of output variable placemarkers in the mode declarations of Table 3, each variable that appears in the body of a clause C \u2208 Kv, also appears in the head of C. Note also that the ground term that represents a distance threshold in the close/3 predicate has been preserved during the variabi-\nlization process, since it replaces a ground placemarker in the corresponding mode declaration.\nThe third phase of XHAIL functionality concerns the actual search for a hypothesis. Contrary to other inverse entailment systems like PROGOL and ALEPH, which rely on a heuristic search, XHAIL performs a complete search in the space of theories that subsume Kv in order to ensure soundness of the generated hypothesis. This search is biased by minimality, i.e. preference towards hypotheses with fewer literals. A hypothesis is thus constructed by dropping as many literals and clauses from Kv as possible, while correctly accounting for all the examples. To this end, Kv is subject to a syntactic transformation of its clauses, which involves two new predicates try/3 and use/2 (see Phase 3 of Table 3).\nFor each clause Ci \u2208 Kv and each body literal \u03b4ji \u2208 Ci, a new atom v(\u03b4 j i ) is generated, as a special term that contains the variables that appear in \u03b4ji . The new atom is wrapped inside an atom of the form try(i , j , v(\u03b4ji )). An extra atom use(i , 0 ) is added to the body of Ci and two new clauses try(i , j , v(\u03b4 j i ))\u2190 use(i , j ), \u03b4 j i and try(i , j , v(\u03b4ji ))\u2190 not use(i , j ) are generated, for each body literal \u03b4 j i \u2208 Ci. All these clauses are put together into a program UKv as in Table 3. UKv serves as a \u201cdefeasible\u201d version of Kv from which literals and clauses may be selected in order to construct a hypothesis that accounts for the examples. This is realized by solving an ALP task with use/2 as the only abducible predicate, as in Phase 3 of Table 3. As explained in (Ray, 2009), the intuition is as follows: In order for the head atom of clause Ci \u2208 UKv to contribute towards the coverage of an example, each of its try(i , j , v(\u03b4ji )) atoms must succeed. By means of the two rules added for each such atom, this can be achieved in two ways: Either by assuming not use(i, j), or by satisfying \u03b4ji and abducing use(i, j). A hypothesis clause is constructed by the head atom of the i-th clause Ci of Kv, if use(i, 0) is abduced, and the j-th body literal of Ci, for each abduced use(i, j) atom. All other clauses and literals from Kv are discarded. The bias towards hypotheses with fewer literals is realized by means of abducing a minimal set of use/2 atoms.\nExample 4 [Example 2 continued] \u22062 presented next to the ALP task of Phase 3 in Table 3 is a minimal abductive explanation for this ALP task. use(1 , 0 ) and use(2 , 0 ) correspond to the head atoms of the two Kv clauses, while use(1 , 3 ) and use(2 , 2 ) correspond respectively to their third and second body literal. The output hypothesis in Table 3 is constructed by these literals, while all other literals and clauses from Kv are discarded.\nTo sum up, XHAIL provides an appropriate framework for learning event definitions in the form of Event Calculus programs. However, a serious obstacle that prevents XHAIL from being widely applicable as a machine learning system for event recognition is scalability. XHAIL scales poorly, partly because of the increased computational complexity of adbuction, which lies at the core of its functionality, and partly because of the combinatorial complexity of learning whole theories, which may result in an intractable search space. In what follows, we use the XHAIL machinery to develop an incremental algorithm that scales to large volumes of sequential data, typical of event-based applications."}, {"heading": "4 ILED: Incremental Learning of Event Definitions", "text": "We begin the presentation of our approach, which we call ILED (Incremental Learning of Event Definitions), by defining the incremental setting we assume and elaborating on the main challenges that stem from this setting. We then present the basic ideas that allow to address these challenges and proceed with a detailed description of the method.\nDefinition 4 (Incremental Learning) We assume an ILP task ILP(SDEC, E ,M ), where E is a database of examples, called historical memory, storing examples presented over time. Initially E = \u2205. At time n the learner is presented with a hypothesis Hn such that SDEC \u222a Hn E , in addition to a new set of examples wn. The goal is to revise Hn to a hypothesis Hn+1, so that SDEC \u222aHn+1 E \u222a wn.\nA main challenge of adopting a full memory approach is to scale it up to a growing size of experience. This is in line with a key requirement of incremental learning where \u201cthe incorporation of experience into memory during learning should be computationally efficient, that is, theory revision must be efficient in fitting new incoming observations\u201d (Langley, 1995; Mauro et al, 2005). In the stream processing literature, the number of passes over a stream of data is often used as a measure of the efficiency of algorithms (Li et al, 2004; Li and Lee, 2009). In this spirit, the main contribution of ILED, in addition to scaling up XHAIL, is that it adopts a \u201csingle-pass\u201d theory revision strategy, that is, a strategy that requires at most one pass over E in order to compute Hn+1 from Hn.\nA single-pass revision strategy is far from trivial. For instance, the addition of a new clause C in response to a set of new examples wn implies that Hn must be checked throughout E . In case C covers some negative examples in E it should be specialized, which in turn may affect the initial coverage of C in wn. If the specialization results in the rejection of positive examples in wn, extra clauses must be generated and added to Hn, in order to retrieve the lost positives, and these clauses should be again checked for correctness in E . This process continues until a hypothesis Hn+1 is found, that accounts for all the examples in E \u222awn. In general, this requires several passes over the historical memory.\nSince experience may grow over time to an extent that is impossible to maintain in the working memory, we follow an external memory approach (Biba et al, 2008). This implies that the learner does not have access to all past experience as a whole, but to independent sets of training data, in the form of sliding windows. Sliding windows should be sufficiently large to capture the temporal dependencies between the data, as imposed by the SDEC axioms, which make the truth value of a fluent at time T depend on what happens at T\u22121. We thus assume that sliding windows consist of at least two consecutive examples. For instance, the data in Table 2 may be considered as part of two windows, or as part of a single window.\nILED\u2019s high-level strategy is presented in Algorithm 1. At time n, ILED is presented with a hypothesis Hn that accounts for the historical memory so far, and a new example window wn. If the hypothesis at hand covers the new window then it is returned as is (line 12), otherwise ILED starts the process of revising Hn (line 3). Revision operators that retract knowledge, such as the deletion of clauses or antecedents are excluded, due to the exponential cost of backtracking in the historical memory (Badea, 2001). The supported revision operators are thus:\nAlgorithm 1 iled(SDEC,M,Hn, wn) Input: The axioms of SDEC, mode declarations M, a hypothesis Hn such that SDEC \u222aHn E and an example window wn. Output: A hypothesis Hn+1 such that SDEC \u222aHn+1 E \u222a wn 1: if SDEC \u222aHn 2 wn then 2: let Kwnv be a (variabilized) Kernel Set of wn 3: let \u3008RetainedClauses,RefinedClauses,NewClauses\u3009 \u2190 revise(SDEC, Hn,Kwnv , wn) 4: let H \u2032 \u2190 Hkeep \u222a RefinedClauses \u222aNewClauses 5: if NewClauses 6= \u2205 then 6: for each wi \u2208 E, 0 \u2264 i \u2264 n\u2212 1 do 7: if SDEC \u222aH\u2032 2 wi then 8: let \u3008RetainedClauses,RefinedClauses, \u2205\u3009 \u2190 revise(SDEC, H\u2032, \u2205, wi) 9: let H \u2032 \u2190 RetainedClauses \u222a RefinedClauses 10: let Hn+1 \u2190 H \u2032 11: else 12: let Hn+1 \u2190 Hn 13: let E \u2190 E \u222a wn 14: Return Hn+1\n\u2013 Addition of new clauses. \u2013 Refinement of existing clauses, i.e. replacement of an existing clause with one\nor more specializations of that clause.\nTo treat incompleteness we add initiatedAt clauses and refine terminatedAt clauses, while to treat inconsistency we add terminatedAt clauses and refine initiatedAt clauses.\nGiven a running hypothesis Hn and a new window wn, the goal of Algorithm 1 is to retain the preservable clauses of Hn intact, refine its revisable clauses and, if necessary, generate a set of new clauses that account for new examples in the incoming window wn. Definition 5 provides a formal account for preservable and revisable clauses.\nDefinition 5 (Revisable and Preservable Parts of a Hypothesis) Let H be a hypothesis, C \u2208 H a clause and w an example window. We say that C is revisable w.r.t w if SDEC \u222a C covers some negative examples or disproves some positive examples in w. Otherwise, we say that C is preservable w.r.t. w.\nRevisions are implemented via the revise function (see line 3 of Algorithm 1). Figure 1 illustrates this function with a simple example. New clauses are generated by generalizing a Kernel Set of the incoming window, as shown in Figure 1, where a terminatedAt/2 clause is generated from the new window wn. Moreover, to facilitate refinement of existing clauses, each clause in the running hypothesis is associated with a memory of the examples it covers throughout E , in the form of a \u201cbottom program\u201d, which we call support set. The support set is constructed gradually, as new example windows arrive. It serves as a refinement search space, as shown in Figure 1, where the single clause in the running hypothesis Hn is refined w.r.t. the incoming window wn into two specializations. Each such specialization results by adding to the initial clause one antecedent from the two support set clauses which are presented in Figure 1. The revised hypothesis Hn+1 is constructed from the refined clauses and the new ones, along with the preserved clauses of Hn, if any (line 4, Algorithm 1).\nThe historical memory is reconsulted only when new clauses are generated from the Kernel Set of the new window wn (see line 5 of Algorithm 1). The new clauses are checked on each example window in E and refined if necessary. At this step there is no need to generate new clauses, but only to ensure that the ones generated at the new window wn are consistent throughout E . This is why in line 8 of Algorithm 1, NewClauses and the Kernel Set K are both empty in the result and the arguments of the revise function respectively.\nThere are two key features of ILED that contribute towards its scalability: First, the re-processing of past experience is necessary only in the case where new clauses are generated and is redundant in the case where a revision consists of refinements of existing clauses. Second, as shown by the iteration of lines 6-9 of Algorithm 1, re-processing of past experience requires a single pass over the historical memory, meaning that it suffices to \u201cre-see\u201d each past window exactly once to ensure that the output revised hypothesis Hn+1 is complete & consistent w.r.t. the entire historical memory. These properties of ILED are due to the support set, which we present in detail.\n4.1 Support Set\nThe intuition behind the support set stems from the XHAIL methodology. Given a set of examples E, XHAIL learns a hypothesis by generalizing a Kernel Set K of these examples. E may be too large to process in one go and a possible solution\nis to partition E in smaller example sets E1, . . . , En and try to learn a hypothesis that accounts for the whole of E, by gradually revising an initial hypothesis H1 acquired from E1. In this process of progressive revisions, a compressive memory of \u201csmall\u201d Kernel sets of E1, . . . , En may be used as a surrogate for the fact that one is not able to reason with the whole Kernel Set K. This is the role of the support set.\nBy means of this memory, and as far as clause refinement is concerned, ILED is able to repair problems locally, i.e. in a single example window, without affecting coverage in the parts of the historical memory where the clause under refinement has been previously checked and is preservable. In more detail, given a hypothesis clause C and a window w where C must be refined, and denoting by Epr (C ), the part of E where we know that C is preservable, ILED refines C so that its refinement covers all positive examples that C covers in Epr (C ), making the task of checking Epr (C ) in response to the refinement redundant.\nIn order to formally define the properties of the proposed memory structure, we use the notions of a depth-bound mode language and most-specific clause. Intuitively, given a set of mode declarations M and a non-negative integer i, a clause C is in the depth-bounded mode language Li(M) if it is in the mode language L(M) and additionally, its length is bound by i. A clause C in Li(M) is most-specific if it does not \u03b8-subsume any other clause in Li(M). These notions are formally defined in Definitions 13 and 14 in Appendix A. Definition 6 provides some additional notation that we henceforth use.\nDefinition 6 (Notation) Let E be the historical memory, M a set of mode declarations, Li(M) the depth-bound mode language of M for some non-negative integer i, H \u2208 Li(M) a running hypothesis and C \u2208 H a hypothesis clause. We use the following notation:\n(i) covE(C) = {e \u2208 E | SDEC\u222aC e}, i.e. covE(C) denotes the coverage of clause C in the historical memory.\n(ii) Given E \u2286 E , Li(M,E) = {D \u2208 Li(M) | SDEC\u222aD E}, i.e. Li(M,E) denotes the fragment of the depth-bound mode language Li(M) that covers a given set of examples E.\nDefinition 7 defines formally the properties of the support set.\nDefinition 7 (Support Set) Let E , M and Li(M) be as in Definition 6, Hn \u2208 Li(M) be as in the Incremental Learning setting (Definition 4) and let C \u2208 H. The support set of C is a program C .supp with the following properties:\n(i) C D for each D \u2208 C .supp. (ii) Each D \u2208 C .supp is a most-specific clause of Li(M, covE(C)). (iii) covE(C .supp) = covE(C ).\nProperties (i) and (ii) of Definition 7 imply that clause C and its support set C .supp define a space S of specializations of C, each of which is bound by a most-specific specialization, among those that cover the positive examples that C covers, and up to a maximal clause length. In other words, for every D \u2208 S there is a Cs \u2208 C .supp so that C D Cs and Cs covers at least one example from covE(C ). Property (iii) of Definition 7 ensures that space S contains refinements of clause C that collectively preserve that coverage of C in the historical memory.\nAlgorithm 2 Support set construction and maintenance\n1: let wn /\u2208 E be an example window, Hn a current hypothesis and H \u2032n = NewClauses \u222a RefinedClauses \u222a RetainedClauses a revision of Hn, generated in wn, where NewClauses,RefinedClauses and RetainedClauses are as described in Algorithm 1. 2: for each C \u2208 H\u2032n do 3: if C \u2208 NewClauses then 4: C .supp \u2190 {D \u2208 K | C D}, where K is the variabilized Kernel Set of wn from which NewClauses is generated. 5: else if C \u2208 RefinedClauses then 6: C .supp \u2190 {D \u2208 Cparent .supp | C D}, where Cparent is the \u201cancestor\u201d clause of C, i.e. the clause from which C results by specialization. 7: else 8: let ewnC be the true positives that C covers in wn, if C is an initiatedAt clause, or the true negatives that C covers, if it is a terminatedAt clause. 9: if SDEC \u222a C .supp 2 ewnC then\n10: let K be a variabilized Kernel Set of wn. 11: C .supp \u2190 C .supp \u222aK\u2032, where K\u2032 \u2286 K, such that SDEC \u222aK\u2032 ewnC\nThe purpose of C .supp is thus to serve as a search space for refinements RC of clause C for which C RC C .supp holds. In this way, clause C may be refined w.r.t. a window wn, avoiding the overhead of re-testing the refined program on E . However, to ensure that the support set can indeed be used as a refinement search space, one must ensure that C .supp will always contain such a refinement RC , i.e. a preservable program w.r.t. a given window wn, that may replace C in case that latter is revisable w.r.t. wn. Proposition 1 shows that this is indeed the case.\nProposition 1 Let Hn \u2208 Li(M) be as in the Incremental Learning setting (Definition 4), i.e. SDEC \u222aHn E, and wn be an example window. Assume also that there exists a hypothesis Hn+1 \u2208 Li(M), such that SDEC \u222a Hn+1 E \u222a wn, and that a clause C \u2208 Hn is revisable w.r.t. window wn. Then C .supp contains a refinement RC of C, which is preservable w.r.t. wn.\nProof Assume, towards contradiction, that each each refinement RC of C, contained in C .supp is revisable w.r.t. wn. It then follows that C .supp itself is revisable w.r.t. wn, i.e. it either covers some negative examples, or it disproves some positive examples in wn. Let e1 \u2208 wn be such an example that C .supp fails to satisfy, and assume for simplicity that a single clause Cs \u2208 C .supp is responsible for that. By definition, Cs covers at least one positive example e2 from E and furthermore, it is a most-specific clause, within Li(M), with that property. It then follows that e1 and e2 cannot both be accounted for, under the given language bias Li(M), i.e. there exists no hypothesis Hn+1 \u2208 Li(M) such that SDEC \u222aHn+1 E \u222awn, which contradicts our assumption. Hence C .supp is preservable w.r.t. wn and it thus contains a refinement RC of C, which is preservable w.r.t. wn.\nThe construction of the support set, presented in Algorithm 2, is a process that starts when C is added in the running hypothesis and continues as long as new example windows arrive. While this happens, clause C may be refined or retained, and its support set is updated accordingly. The details of Algorithm 2 are presented in Example 5, which also demonstrates how ILED processes incoming examples and revises hypotheses. Example 5 Consider the annotated examples and running hypothesis related to the fighting high-level event from the activity recognition application shown in\nTable 4. We assume that ILED starts with an empty hypothesis and an empty historical memory, and that w1 is the first input example window. The currently empty hypothesis does not cover the provided examples, since in w1 fighting between persons id1 and id2 is initiated at time 10 and thus holds at time 11. Hence ILED starts the process of generating an initial hypothesis. In the case of an empty hypothesis, ILED reduces to XHAIL and operates on a Kernel Set of w1 only, trying to induce a minimal program that accounts for the examples in w1. The variabilized Kernel Set in this case will be the single-clause program K1 presented in Table 4 generated from the corresponding ground clause. Generalizing this Kernel Set yields a minimal hypothesis that covers w1. One such hypothesis is clause C shown in Table 4. ILED stores w1 in E and initializes the support set of the newly generated clause C as in line 3 of Algorithm 2, by selecting from K1 the clauses that are \u03b8-subsumed by C, in this case, K1\u2019s single clause.\nWindow w2 arrives next. In w2, fighting is initiated at time 20 and thus holds at time 21. The running hypothesis correctly accounts for that and thus no re-\nvision is required. However, C .supp does not cover w2 and unless proper actions are taken, property (iii) of Definition 7 will not hold once w2 is stored in E . ILED thus generates a new Kernel Set K2 from window w2, as presented in Table 4, and updates C .supp as shown in lines 7-11 of Algorithm 2. Since C \u03b8-subsumes K2, the latter is added to C .supp, which now becomes C .supp = {K1,K2}. Now covE(C .supp) = covE(C ), hence in effect, C .supp is a summarization of the coverage of clause C in the historical memory.\nWindow w3 arrives next, which has no positive examples for the initiation of fighting. The running hypothesis is revisable in window w3, since clause C covers a negative example at time 31, by means of initiating the fluent fighting(id1 , id2 ) at time 30. To address the issue, ILED searches C .supp, which now serves as a refinement search space, to find a refinement RC that rejects the negative example, and moreover RC C .supp. Several choices exist for that. For instance, the following program\ninitiatedAt(fighting(X,Y ), T )\u2190 happensAt(active(X), T ), happensAt(abrupt(Y ), T ).\ninitiatedAt(fighting(X,Y ), T )\u2190 happensAt(active(X), T ), happensAt(kicking(Y ), T ).\nis such a refinement RC , since it does not cover the negative example in w3 and subsumes C .supp. ILED however is biased towards minimal theories, in terms of the overall number of literals and would prefer the more compressed refinement C1, shown in Table 4, which also rejects the negative example in w3 and subsumes C .supp. Clause C1 replaces the initial clause C in the running hypothesis. The hypothesis now becomes complete and consistent throughout E . Note that the hypothesis was refined by local reasoning only, i.e. reasoning within window w3 and the support set, avoiding costly look-back in the historical memory. The support set of the new clause C1 is initialized (line 5 of Algorithm 2), by selecting the subset of the support set of its parent clause that is \u03b8-subsumed by C1. In this case C1 C .supp = {K1 ,K2}, hence C1 .supp = C .supp.\nAs shown in Example 5, the support set of a clause C is a compressed enumeration of the examples that C covers throughout the historical memory. It is compressed because it is expected to encode many examples with a single variabilized clause. In contrast, a ground version of the support set would be a plain enumeration of examples, since in the general case, it would require one ground clause per example. The main advantage of the \u201clifted\u201d character of the support set over a plain enumeration of the examples is that it requires much less memory to encode the necessary information, an important feature in large-scale (temporal) applications. Moreover, given that training examples are typically characterized by heavy repetition, abstracting away redundant parts of the search space results in a memory structure that is expected to grow in size slowly, allowing for fast search that scales to large amount of historical data.\n4.2 Implementing Revisions\nAlgorithm 3 presents the details of the revise function from Algorithm 1. The input consists of SDEC as background knowledge, a running hypothesis Hn, an example window wn and a variabilized Kernel Set K wn v of wn. The clauses of K wn v and Hn are subject to the GeneralizationTansformation and the RefinementTransformation\nAlgorithm 3 revise(SDEC, Hn, wn,K wn v ) Input: The axioms of SDEC, a running hypothesis Hn an example window wn and a variabilized Kernel Set Kwnv of wn. Output: A revised hypothesis H \u2032n\n1: let U (K wnv ,Hn )\u2190 GeneralizationTransformation(K wnv ) \u222a RefinementTransformation(Hn ) 2: let \u03a6 be the abductive task \u03a6 = ALP(SDEC \u222aU (K wnv ,Hn ), {use/2 , use/3},wn ) 3: if \u03a6 has a solution then 4: let \u2206 be a minimal solution of \u03a6\n5:\nlet NewClauses = {\u03b1i \u2190 \u03b41i \u2227 . . . \u2227 \u03b4ni | \u03b1i is the head of the i\u2212th clause Ci \u2208 Kwnv and \u03b4ji is the j\u2212th body literal of Ci and use(i , 0 ) \u2208 \u2206 and use(i , j ) \u2208 \u2206, 1 \u2264 j \u2264 n }\n6: let RefinedClauses = { head(Ci )\u2190 body(Ci ) \u2227 \u03b4 j ,k1 i \u2227 . . . \u2227 \u03b4 j ,km i | Ci \u2208 Hn and use(i , j , kl ) \u2208 \u2206, 1 \u2264 l \u2264 m, 1 \u2264 j \u2264 |Ci .supp| } 7: let RetainedClauses = {Ci \u2208 Hn | use(i , j , k) /\u2208 \u2206 for any j , k} 8: let RefinedClauses = ReduceRefined(NewClauses,RefinedClauses,RetainedClauses) 9: else\n10: Return No Solution 11: Return \u3008RetainedClauses,RefinedClauses,NewClauses\u3009\nrespectively, presented in Table 5. The former is the transformation discussed in Section 3.2.1, that turns the Kernel Set into a defeasible program, allowing to construct new clauses from the Kernel Set select, in order to cover the examples. The RefinementTransformation aims at the refinement of the clauses of Hn using their support sets. It involves two fresh predicates, exception/3 and use/3. For each clause Di \u2208 Hn and for each of its support set clauses \u0393 ji \u2208 Di.supp, one new clause head(Di)\u2190 body(Di) \u2227 not exception(i , j , v(head(Di))) is generated, where v(head(Di)) is a term that contains the variables of head(Ci). Then an additional clause exception(i , j , v(head(Di)))\u2190 use(i , j , k) \u2227 not \u03b4j ,ki is generated, for each body literal \u03b4j,ki \u2208 \u0393 j i .\nThe syntactically transformed clauses are put together in a program U(Kwnv , Hn) (line 1 of Algorithm 3), which is used as a background theory along with SDEC. A minimal set of use/2 and use/3 atoms is abduced as a solution to the abductive task \u03a6 in line 2 of Algorithm 3. Abduced use/2 atoms are used to construct a set of NewClauses, as discussed in Section 3.2.1 (line 5 of Algorithm 3). These new clauses account for some of the examples in wn, which cannot be covered by existing clauses in Hn. The abduced use/3 atoms indicate clauses of Hn that must be refined. From these atoms, a refinement RDi is generated for each incorrect clause Di \u2208 Hn, such that Di RDi Di.supp (line 6 of Algorithm 3). Clauses that lack a corresponding use/3 atom in the abductive solution are retained (line 7 of Algorithm 3).\nThe intuition behind refinement generation is as follows: Assume that clause Di \u2208 Hn covers negative examples or disproves positive examples in window wn. To prevent that, the negation of the exception atom that is added to the body of Di during the RefinementTransformation, must fail to be satisfied, hence the exception atom itself must be satisfied. This can be achieved in several ways by means of the extra clauses generated by the RefinementTransformation. These clauses provide definitions for the exception atom, namely one for each body literal in each clause of Di.supp. From these rules one can satisfy the exception atom by satisfying the complement of the corresponding support set literal and abduc-\ning the accompanying use/3 atom. In this way, each incorrect clause Di \u2208 Hn and each \u0393 ji \u2208 Di .supp correspond to a set of abduced use/3 atoms of the form use(i, j, k1), . . . , use(i, j, kn). These atoms indicate that a specialization of Di may be generated by adding to the body of Di the literals \u03b4 j,k1 i , . . . , \u03b4 j,kn i from \u0393 j i . Then a refinement RDi such that Di RDi Di .supp may be generated by selecting one specialization of clause Di from each support set clause in Di .supp.\nExample 6 Table 6 presents the process of ILED\u2019s refinement. The annotation lacks positive examples and the running hypothesis consists of a single clause C, with a support set of two clauses. Clause C is inconsistent since it entails two negative examples, namely holdsAt(fighting(id1 , id2 ), 2 ) and holdsAt(fighting(id3 , id4 ), 3 ). The program that results by applying the RefinementTransformation to the support set of clause C is presented in Table 6, along with a minimal abductive explanation of the examples, in terms of use/3 atoms. Atoms use(1, 1, 2) and use(1, 1, 3) correspond respectively to the second and third body literals of the first support set clause, which are added to the body of clause C, resulting in the first specialization presented in Table 6. The third abduced atom use(1, 2, 2) corresponds to the second body literal of the second support set clause, which results in the second specialization in Table 6. Together, these specializations form a refinement of clause C that subsumes C .supp.\nMinimal abductive solutions imply that the running hypothesis is minimally revised. Revisions are minimal w.r.t. the length of the clauses in the revised hypothesis, but are not minimal w.r.t. the number of clauses, since the refinement strategy described above may result in refinements that include redundant clauses: Selecting randomly one specialization from each support set clause to generate a refinement of a clause is sub-optimal, since there may exist other refinements with fewer clauses that also subsume the whole support set, as Example 5 demonstrates. To avoid unnecessary increase at the hypothesis size, the generation of refinements is followed by a \u201creduction\u201d step (line 8 of Algorithm 3). The ReduceRefined function works as follows. For each refined clause C, it first generates all possible refinements from C .supp. This can be realized with the abductive refinement technique described above. The only difference is that the abductive solver is instructed to find all abductive explanations in terms of use/3 atoms, instead of one. Once all refinements are generated, ReduceRefined searches the revised\nhypothesis, augmented with all refinements of clause C, to find a reduced set of refinements of C that subsume C .supp.\n4.3 Soundness and Single-Pass Theory Revision\nIn this section we prove the correctness of ILED (Algorithm 1) and show that it requires at most one pass over the historical memory to revise an input hypothesis.\nProposition 2 (Soundness and Single-pass Theory Revision) Assume the incremental learning setting described in Definition 4. ILED (Algorithm 1) requires at most one pass over E to compute Hn+1 from Hn.\nProof For simplicity and without loss of generality, we assume that when a new example window wn arrives, ILED revises Hn by (a) refining an single clause C \u2208 Hn or (b) adding a new clause C\u2032.\nIn case (a), clause C is replaced by a refinementRC such that C RC C .supp. By property (iii) of the support set definition (Definition 7), RC covers all positive examples that C covers in E , hence for the hypothesis Hn+1 = (Hn r C) \u222a RC it holds that SDEC \u222a Hn+1 E and furthermore SDEC \u222a Hn+1 wn. Hence SDEC \u222a Hn+1 E \u222a wn, from which soundness for Hn+1 follows. In this case Hn+1 is constructed from Hn in a single step, i.e. by reasoning within wn without re-seeing other windows from E .\nIn case (b), Hn is revised w.r.t. wn to a hypothesis H \u2032 n = Hn \u222a C\u2032, where C\u2032 is a new clause that results from the generalization of a Kernel Set of wn. In response to the new clause addition, each window in E must be checked and C\u2032 must be refined if necessary, as shown in line 5 of Algorithm 1. Let Etested denote the fragment of E that has been tested at each point in time. Initially, i.e. once C\u2032 is generated from wn, it holds that Etested = wn. At each window that is tested, clause C\u2032 may (i) remain intact, (ii) be refined, or (iii) one of its refinements may be further refined. Assume that wk, k < n is the first window where the new clause C\u2032 must be refined. At this point, Etested = {wi \u2208 E | k < i \u2264 n}, and it holds that C\u2032 is preservable in Etested , since C\u2032 has not yet been refined. In wk, clause C\u2032 is replaced by a refinement RC\u2032 such that C\n\u2032 RC \u2032 C \u2032.supp. RC\u2032 is preservable in Etested , since it is a refinement of a preservable clause, and furthermore, it covers all positive examples that C\u2032 covers in wn, by means of the properties of the support set. Hence the hypothesis H \u2032\u2032n = (H \u2032 n r C\u2032) \u222a RC\u2032 is complete & consistent w.r.t. Etested . The same argument shows that if RC\u2032 is further refined later on (case (iii) above), the resulting hypothesis remains complete an consistent w.r.t. Etested . Hence, when all windows have been tested, i.e. when Etested = E , the resulting hypothesis Hn+1 is complete & consistent w.r.t. E \u222a wn and furthermore, each window in E has been re-seen exactly once, thus Hn+1 is computed with a single pass over E ."}, {"heading": "5 Discussion", "text": "Non-monotonic ILP, and XHAIL in particular, have some important properties, by means of which they extend traditional ILP systems. As briefly discussed in Section 3.2, these properties are related to some challenging issues that occur when learning normal logic programs, which non-monotonic ILP addresses in a robust and elegant way. We next discuss which of these properties are preserved by ILED and which are sacrificed as a trade-off for efficiency, while briefly indicating directions for improvement in future work.\nLike XHAIL, ILED aims for soundness, that is, hypotheses which cover all given examples. XHAIL ensures soundness by generalizing all examples in one go. In contrast, ILED preserves a memory of past experience for which newly acquired knowledge must account. Soundness imposes restrictions on the tasks on which ILED may be applied. In particular, we assume that the supervision is correct (i.e. it contains no contradictions or missing knowledge) and the domain is stationary, in the sense that knowledge already induced remains valid w.r.t. future instances, and retracting clauses or literals from the hypothesis at hand is never\nnecessary in order to account for new incoming example windows. ILED terminates in case its computations result in a dead-end, returning no solution. This results in treating cases such as concept drift (Esposito et al, 2004), as noise. It is possible to relax the requirement for soundness and aim at an implementation that best-fits the training instances. Handling noise and concept drift are promising extensions of ILED.\nXHAIL is a state-of-the-art system among its Inverse Entailment-based peer algorithms, in terms of completeness. That is, the hypotheses computable by XHAIL form a superset of those computable by other prominent Inverse Entailment systems like PROGOL and ALEPH (Ray, 2009). Although ILED preserves XHAIL\u2019s soundness, it does not preserve its completeness properties, due to the fact that ILED operates incrementally to gain efficiency. Thus there are cases where a hypothesis can be discovered by XHAIL, but be missed by ILED. As an example, consider cases where a target hypothesis captures long-term temporal relations in the data, as for instance, in the following clause:\ninitiatedAt(moving(X,Y ), T )\u2190 happensAt(walking(Y ), T1), T1 < T.\nIn such cases, if the parts of the data that are connected via a long-range temporal relation are given in different windows, ILED has no way to correlate these parts in order to discover the temporal relation. However, one can always achieve XHAIL\u2019s functionality by increasing appropriately ILED\u2019s window size.\nAn additional trade-off for efficiency is that not all of ILED\u2019s revisions are fully evaluated on the historical memory. For example, a new clause generated by a Kernel Set of an incoming window w is selected randomly among a set of possible choices, which are equally good locally, i.e. in window w, but their quality may substantially differ globally. For instance, selecting a particular clause in order to cover a new example, may result in a large number of refinements and an unnecessarily lengthy hypothesis, as compared to one that may have been obtained by selecting a different initial clause. On the other hand, fully evaluating all possible choices throughout E requires extensive inference in E . Thus simplicity and compression of hypotheses in ILED has been sacrificed for efficiency.\nIn ILED, a large part of the theorem proving effort that is involved in clause refinement reduces to computing subsumption between clauses, which is a hard task. Moreover, just as the historical memory grows over time, so do (in the general case) the support sets of the clauses in the running hypothesis, increasing the cost of computing subsumption. However, as in principle the largest part of a search space is redundant and the support set focuses only on its interesting parts, one would not expect that the support set will grow to a size that makes subsumption computation less efficient than inference over the entire E . Moreover, the length of Kernel Set clauses (hence that of support clauses) is restricted by the size of incoming sliding windows. Smaller windows result to smaller clauses, making the computation of subsumption relations tractable. In addition, a number of optimization techniques have been developed over the years and several generic subsumption engines have been proposed (Maloberti and Sebag, 2004; Kuzelka and Zelezny, 2008; Santos and Muggleton, 2010), some of which are able to efficiently compute subsumption relations between clauses comprising thousands of literals and hundreds of distinct variables.\nThe basic idea behind ILED is to compress examples via Bottom Clause-like structures, in order to facilitate clause refinement, while learning a hypothesis incrementally. We see the idea behind the support set as being generic enough to be applied to any Inverse Entailment system that uses Bottom Clauses to guide the search, in order to provide support for more efficient clause refinement. In that case, the use of the support set should be modified accordingly to comply with the search method adopted by each system. For instance, in the work presented here, the support set works with XHAIL\u2019s search procedure, a minimality-driven, full search in the space of theories that subsume the Kernel Set, designed to address the nonmonotonicity of normal logic programs. Different settings may be developed. For example, once the requirement for soundness is abandoned in an effort to address noise, a heuristic search strategy could be adopted, like for example PROGOL\u2019s A\u2217-like search. Different settings would require changes to the way the support set works."}, {"heading": "6 Experimental evaluation", "text": "In this section, we present experimental results from two real-world applications: Activity recognition, using real data from the benchmark CAVIAR video surveillance dataset3, as well as large volumes of synthetic CAVIAR data; and City Transport Management (CTM) using data from the PRONTO4 project.\nPart of our experimental evaluation aims to compare ILED with XHAIL. To achieve this aim we had to implement XHAIL, because the original implementation was not publicly available until recently (Bragaglia and Ray, 2014). All experiments were conducted on a 3.2 GHz Linux machine with 4 GB of RAM. The algorithms were implemented in Python, using the Clingo5 Answer Set Solver (Gebser et al, 2012) as the main reasoning component, and a Mongodb6 NoSQL database for the historical memory of the examples. The code and datasets used in these experiments can be downloaded from http://cer.iit.demokritos.gr/ ILED/experiments.\n6.1 Activity Recognition\nIn activity recognition, our goal is to learn definitions of high-level events, such as fighting, moving and meeting, from streams of low-level events like walking, standing, active and abrupt, as well as spatio-temporal knowledge. We use the benchmark CAVIAR dataset for experimentation. Details on the CAVIAR dataset and more information about activity recognition applications may be found in (Artikis et al, 2010). Consider for instance the following definition of the fighting high-level event:\n3 http://homepages.inf.ed.ac.uk/rbf/CAVIARDATA1/ 4 http://www.ict-pronto.org/ 5 http://potassco.sourceforge.net/ 6 http://www.mongodb.org/\ninitiatedAt(fighting(X,Y ), T )\u2190 happensAt(active(X), T ), not happensAt(inactive(Y ), T ), holdsAt(close(X,Y, 23), T ). (1)\ninitiatedAt(fighting(X,Y ), T )\u2190 happensAt(abrupt(X), T ), not happensAt(inactive(Y ), T ), holdsAt(close(X,Y, 23), T ). (2)\nterminatedAt(fighting(X,Y ), T )\u2190 happensAt(walking(X), T ), not holdsAt(close(X,Y, 23), T ). (3) terminatedAt(fighting(X,Y ), T )\u2190 happensAt(running(X), T ), not holdsAt(close(X,Y, 23), T ). (4)\nClause (1) dictates that a period of time for which two persons X and Y are assumed to be fighting is initiated at time T if one of these persons is active, the other one is not inactive and their distance is smaller than 23 pixel positions. Clause (2) states that fighting is initiated between two people when one of them moves abruptly, the other is not inactive, and the two persons are sufficiently close. Clauses (3) and (4) state that fighting is terminated between two people when one of them walks or runs away from the other.\nCAVIAR contains noisy data mainly due to human errors in the annotation (List et al, 2005; Artikis et al, 2010). Thus, for the experiments we manually selected a noise-free subset of CAVIAR. The resulting dataset consists of 1000 examples (that is, data for 1000 distinct time points) concerning the high-level events moving, meeting and fighting. These data, selected from different parts of the CAVIAR dataset, were combined into a continuous annotated stream of narrative atoms, with time ranging from 0 to 1000.\nIn addition to the real data, we generated synthetic data on the basis of the manually-developed CAVIAR event definitions described in (Artikis et al, 2010). In particular, streams of low-level events concerning four different persons were created randomly and were then classified using the rules of (Artikis et al, 2010). The final dataset was obtained by generating negative supervision via the closed world assumption and appropriately pairing the supervision with the narrative. The generated data consists of approximately 105 examples, which amounts to 100 MB of data.\nThe synthetic data is much more complex than the real CAVIAR data. This is due to two main reasons: First, the synthetic data includes significantly more initiations and terminations of a high-level event, thus much larger learning effort is required to explain it. Second, in the synthetic dataset more than one high-level event may be initiated or terminated at the same time point. This results in Kernel Sets with more clauses, which are hard to generalize simultaneously."}, {"heading": "6.1.1 ILED vs XHAIL", "text": "The purpose of this experiment was to assess whether ILED can efficiently generate hypotheses comparable in size and predictive quality to those of XHAIL. To this end, we compared both systems on real and synthetic data using 10-fold cross validation with replacement. For the real data, 90% of randomly selected examples, from the total of 1000 were used for training, while the remaining 10% was retained for testing. At each run, the training data were presented to ILED in example windows of sizes 10, 50, 100. The data were presented in one batch to XHAIL. For\nthe synthetic data, 1000 examples were randomly sampled at each run from the dataset for training, while the remaining data were retained for testing. Similar to the real data experiments, ILED operated on windows of sizes of 10, 50, 100 examples and XHAIL on a single batch.\nTable 7 presents the experimental results. Training times are significantly higher for XHAIL, due to the increased complexity of generalizing Kernel Sets that account for the whole set of the presented examples at once. These Kernel Sets consisted, on average, of 30 to 35 16-literal clauses, in the case of the real data, and 60 to 70 16-literal clauses in the case of the synthetic data. In contrast, ILED had to deal with much smaller Kernel Sets. The complexity of abductive search affects ILED as well, as the size of the input windows grows. ILED handles the learning task relatively well (in approximately 30 seconds) when the examples are presented in windows of 50 examples, but the training time increases almost 15 times if the window size is doubled.\nConcerning the size of the produced hypothesis, the results show that in the case of real CAVIAR data, the hypotheses constructed by ILED are comparable in size with a hypothesis constructed by XHAIL. In the case of synthetic data, the hypotheses returned by both XHAIL and ILED were significantly more complex. Note that for ILED the hypothesis size decreases as the window size increases. This is reflected in the number of revisions that ILED performs, which is significantly smaller when the input comes in larger batches of examples. In principle, the richer the input, the better the hypothesis that is initially acquired, and consequently, the less the need for revisions in response to new training instances. There is a tradeoff between the window size (thus the complexity of the abductive search) and the number of revisions. A small number of revisions on complex data (i.e. larger windows) may have a greater total cost in terms of training time, as compared to a greater number of revisions on simpler data (i.e. smaller windows). For example, in the case of window size 100 for the real CAVIAR data, ILED performs 5 revisions on average and requires significantly more time than in the case of a window size 50, where it performs 9 revisions on average. On the other hand, training times for windows of size 50 are slightly better than those obtained when the examples are presented in smaller windows of size 10. In this case, the \u201cunit cost\u201d of performing revisions w.r.t a single window are comparable between windows of size 10 and 50. Thus the overall cost in terms of training time is determined by the total number of revisions, which is greater in the case of window size 10.\nConcerning predictive quality, the results indicate that ILED\u2019s precision and recall scores are comparable to those of XHAIL. For larger input windows, precision and recall are almost the same as those of XHAIL. This is because ILED produces better hypotheses from larger input windows. Precision and recall are smaller in the case of synthetic data for both systems, because the testing set in this case is much larger and complex than in the case of real data."}, {"heading": "6.1.2 ILED Scalability", "text": "The purpose of this experiment was to assess the scalability of ILED. The experimental setting was as follows: Sets of examples of varying sizes were randomly sampled from the synthetic dataset. Each such example set was used as a training set in order to acquire an initial hypothesis using ILED. Then a new window which did not satisfy the hypothesis at hand was randomly selected and presented to ILED, which subsequently revised the initial hypothesis in order to account for both the historical memory (the initial training set) and the new evidence. For historical memories ranging from 103 to 105 examples, a new training window of size 10, 50 and 100 was selected from the whole dataset. The process was repeated ten times for each different combination of historical memory and new window size. Figure 2 presents the average revision times. The revision times for new window sizes of 10 and 50 examples are very close and therefore omitted to avoid clutter. The results indicate that revision time grows polynomially in the size of the historical memory.\n6.2 City Transport Management\nIn this section we present experimental results from the domain of City Transport Management (CTM). We use data from the PRONTO7 project. In PRONTO,\n7 http://www.ict-pronto.org/\nthe goal was to inform the decision-making of transport officials by recognising high-level events related to the punctuality of a public transport vehicle (bus or tram), passenger/driver comfort and safety. These high-level events were requested by the public transport control centre of Helsinki, Finland, in order to support resource management. Low-level events were provided by sensors installed in buses and trams, reporting on changes in position, acceleration/deceleration, in-vehicle temperature, noise level and passenger density. At the time of the project, the available datasets included only a subset of the anticipated low-level event types as some low-level event detection components were not functional. For the needs of the project, therefore, a synthetic dataset was generated. The synthetic PRONTO data has proven to be considerably more challenging for event recognition than the real data, and therefore we chose the former for evaluating ILED (Artikis et al, 2014). The CTM dataset contains 5 \u00b7 104 examples, which amount approximately to 70 MB of data.\nIn contrast to the activity recognition application, the manually developed high-level event definitions of CTM that were used to produce the annotation for learning, form a hierarchy. In these hierarchical event definitions, it is possible to define a function level that maps all high-level events to non-negative integers as follows: A level-1 event is defined in terms of low-level events (input data) only. An level-n event is defined in terms of at least one level-n\u22121 event and a possibly empty set of low-level events and high-level events of level below n\u22121. Hierarchical definitions are significantly more complex to learn as compared to non-hierarchical ones. This is because initiations and terminations of events in the lower levels of the hierarchy appear in the bodies of event definitions in the higher levels of the hierarchy, hence all target definitions must be learnt simultaneously. As we show in the experiments, this has a striking effect on the required learning effort. A solution for simplifying the learning task is to utilize knowledge about the domain (the hierarchy), learn event definitions separately, and use the acquired theories from lower levels of the event hierarchy as non-revisable background knowledge when learning event definitions for the higher levels. Below is a fragment of the CTM event hierarchy:\ninitiatedAt(punctuality(Id , nonPunctual), T )\u2190 happensAt(stopEnter(Id, StopId, late), T ).\n(5)\ninitiatedAt(punctuality(Id , nonPunctual), T )\u2190 happensAt(stopLeave(Id, StopId, early), T ).\n(6)\nterminatedAt(punctuality(Id , nonPunctual), T )\u2190 happensAt(stopEnter(Id, StopId, early), T ).\n(7)\nterminatedAt(punctuality(Id , nonPunctual), T )\u2190 happensAt(stopEnter(Id, StopId, scheduled), T ).\n(8)\ninitiatedAt(drivingQuality(Id , low), T )\u2190 initiatedAt(punctuality(Id, nonPunctual), T ), holdsAt(drivingStyle(Id, unsafe), T ).\n(9)\ninitiatedAt(drivingQuality(Id , low), T )\u2190 initiatedAt(drivingStyle(Id, unsafe), T ), holdsAt(punctuality(Id, nonPunctual), T ).\n(10)\nterminatedAt(drivingQuality(Id , low), T )\u2190 terminatedAt(punctuality(Id, nonPunctual), T ).\n(11)\nterminatedAt(drivingQuality(Id , low), T )\u2190 terminatedAt(drivingStyle(Id, unsafe), T ).\n(12)\nClauses (5) and (6) state that a period of time for which vehicle Id is said to be non-punctual is initiated if it enters a stop later, or leaves a stop earlier than the scheduled time. Clauses (7) and (8) state that the period for which vehicle Id is said to be non-punctual is terminated when the vehicle arrives at a stop earlier than, or at the scheduled time. The definition of non-punctual vehicle uses two low-level events, stopEnter and stopLeave.\nClauses (9)-(12) define low driving quality. Essentially, driving quality is said to be low when the driving style is unsafe and the vehicle is non-punctual. Driving quality is defined in terms of high-level events (we omit the definition of driving style to save space). Therefore, the bodies of the clauses defining driving quality include initiatedAt/2 and terminatedAt/2 literals."}, {"heading": "6.2.1 ILED vs XHAIL", "text": "In this experiment, we tried to learn simultaneously definitions for all target concepts, a total of nine interrelated high-level events, seven of which are level-1, one is level-2 and one is level-3. According to the employed language bias, each such high-level event must be learnt, while at the same time it may be present in the body of another high-level event in the form of (potentially negated) holdsAt/2, initiatedAt/2, or terminatedAt/2 predicate. The total number of low-level events involved is 22.\nWe used tenfold cross validation with replacement, on small amounts of data, due to the complexity of the learning task. In each run of the cross validation, we randomly sampled 20 examples from the CTM dataset, 90% of which was used for training and 10% was retained for testing. This example size was selected after experimentation, in order for XHAIL to be able to perform in an acceptable time frame. Each sample consisted of approximately 150 atoms (narrative and annotation). The examples were given to ILED in windows of granularity 5 and 10, and to XHAIL in one batch. Table 8 presents the average training times, hypothesis size, number of revisions, precision and recall.\nILED took on average 1-2 hours to complete the learning task, for windows of 5 and 10 examples, while XHAIL required more than 4 hours on average to learn hypotheses from batches of 20 examples. Compared to activity recognition, the learning setting requires larger Kernel Set structures that are hard to reason with. An average Kernel Set generated from a batch of just 20 examples consisted of approximately 30-35 clauses, with 60-70 literals each.\nLike the activity recognition experiments, precision and recall scores for ILED are comparable to those of XHAIL, with the latter being slightly better. Unlike the activity recognition experiments, precision and recall had a large diversity\nbetween different runs. Due to the complexity of the CTM dataset, the constructed hypotheses had a large diversity, depending on the random samples that were used for training. For example, some high-level event definitions were unnecessarily lengthy and difficult to be understood by a human expert. On the other hand, some level-1 definitions could in some runs of the experiment, be learnt correctly even from a limited amount of data. Such definitions are fairly simple, consisting of one initiation and one termination rule, with one body literal in each case.\nThis experiment demonstrates several limitations of learning in large and complex applications. The complexity of the domain increases the intensity of the learning task, which in turn makes training times forbidding, even for small amount of data such as 20 examples (approximately 150 atoms). This forces one to process small sets of examples at at time, which in complex domains like CTM, results to over-fitted theories and rapid increase in hypothesis size."}, {"heading": "6.2.2 Learning With Hierarchical Bias", "text": "In an effort to improve the experimental results, we utilized domain knowledge about the event hierarchy in CTM and attempted to learn high-level events in different levels separately. To do so, we had to learn a complete definition from the entire dataset for a high-level event, before utilizing it as background knowledge in the learning process of a higher-level event. To facilitate the learning task further, we also used expert knowledge about the relation between specific low-level and high-level events, excluding from the language bias mode declarations which were irrelevant to the high-level event that is being learnt at each time.\nThe experimental setting was therefore as follows: Starting from the level-1 target events, we processed the whole CTM dataset in windows of 10, 50 and 100 examples with ILED. Each high-level event was learnt independently of the others. Once complete definitions for all level-1 high-level events were constructed, they were added to the background knowledge. Then we proceeded with learning the definition for the single level-2 event. Finally, after successfully constructing the\nlevel-2 definition, we performed learning in the top-level of the hierarchy, using the previously constructed level-1 and level-2 event definitions as background knowledge. We did not attempt a comparison with XHAIL, since due to the amounts of data in CTM, the latter is not able to operate on the entire dataset.\nTable 9 presents the results. For level-1 events, scores are presented as minimummaximum pairs. For instance, the training times for level-1 events with windows of 10 examples, ranges from 4.46 to 4.88 minutes. Levels 2 and 3 have just one definition each, therefore Table 9 presents the respective scores from each run. Training times, hypothesis sizes and overall numbers of revisions are comparable for all levels of the event hierarchy. Level-1 event definitions were the easiest to acquire, with training times ranging approximately between 4.50 to 7 minutes. This was expected since clauses in level-1 definitions are significantly simpler than level-2 and level-3 ones. The level-2 event definition was the hardest to construct with training times ranging between 8 and 10 minutes, while a significant number of revisions was required for all window granularities. The definition of this highlevel event (drivingStyle) is relatively complex, in contrast to the simpler level-3 definition, for which training times are comparable to the ones for level-1 events.\nThe largest parts of training times were dedicated to checking an already correct definition against the part of the dataset that had not been processed yet. That is, for all target events, ILED converged to a complete definition relatively quickly, i.e. in approximately 1.5 to 3 minutes after the initiation of the learning process. From that point on, the extra time was spent on testing the hypothesis against the new incoming data.\nWindow granularity slightly affects the produced hypothesis for all target highlevel events. Indeed, the definitions constructed with windows of 10 examples are slightly larger than the ones constructed with larger window sizes of 50 and 100 examples. Notably, the definitions constructed with windows of granularity 50 and 100, were found concise, meaningful and very close to the actual hand-crafted rules that were utilized in PRONTO."}, {"heading": "7 Related work", "text": "A thorough review of the drawbacks of state-of-the-art ILP systems with respect to non-monotonic domains, as well as the deficiencies of existing approaches to learning Event Calculus programs can be found in (Ray, 2009; Sakama, 2005, 2001; Otero, 2001, 2003). The main obstacle, common to many learners which combine ILP with some form of abduction, like PROGOL5 (Muggleton and Bryant, 2000), ALECTO (Moyle, 2003), HAIL (Ray et al, 2003) and IMPARO (Kimber et al, 2009), is that they cannot perform abduction through negation and are thus essentially limited to Observational Predicate Learning.\nTAL (Corapi et al, 2010) is a top-down non-monotonic learner which is able to solve the same class of problems as XHAIL. It obtains a top theory by appropriately mapping the ILP problem at hand to a corresponding ALP instance, so that solutions to the latter may be translated to solutions for the initial ILP problem. Recently, the main ideas behind TAL were employed in the ASPAL system (Corapi et al, 2011b), an inductive learner which relies on Answer Set Programming as a unifying abductive-inductive framework. ASPAL obtains a top theory of skeleton rules by forming all possible clause structures that may be formed from the mode\ndeclarations. Each such structure is complemented by a set of properly formed abducible predicates. Abductive reasoning on a proper meta-level representation of the original ILP problem returns a set of such abducibles, which, due to their construction, allow to hypothesize on how variables and constants in the skeleton rules are linked together. Thus, the abduced atoms are prescriptions on how variable and constant terms in the original skeleton rules should be handled in order to obtain a hypothesis. This way, ASPAL may induce all possible hypotheses w.r.t to a certain ILP problem, as well as optimal ones, by computing minimal sets of abducibles.\nThe combination of ILP with ALP has recently been applied to meta-interpretive learning (MIL), a learning framework, where the goal is to obtain hypotheses in the presence of a meta-interpreter. The latter is a higher-order program, hypothesizing about predicates or even rules of the domain. Given such background knowledge and sets of positive and negative examples, MIL uses abduction w.r.t. the meta-interpreter to construct first-order hypotheses. MIL can be realized both in Prolog and in Answer Set Programming, and it has been implemented in the METAGOL system (Muggleton et al, 2014). Application examples involve learning definite clause grammars (Muggleton et al, 2014), discovering relations between entities and learning simple robot action strategies (Muggleton and Lin, 2013). MIL is an elegant framework, able to address difficult problems that are under-explored in traditional ILP, like handling predicate invention and learning mutually recursive programs. However, it has a number of important drawbacks. First, its expressivity is significantly limited, as IML is currently restricted to dyadic Datalog, i.e. Datalog where the arity of each predicate is at most two. As noted in (Muggleton et al, 2014), constructing meta-interpreters for richer fragments of first-order logic is not a straight-forward task and requires careful mathematical analysis. Second, given the increased computational complexity of higher-order reasoning, scaling to large volumes of data is a potential bottleneck for MIL.\nIn the non-monotonic setting, traditional ILP approaches that cover the examples sequentially cannot ensure soundness and completeness (Sakama, 2005). To deal with this issue, non-monotonic learners like XHAIL, TAL and ASPAL generalize all available examples in one go. The disadvantage of this approach, however, is poor scalability. A recent advancement which addresses the issue of scalability in non-monotonic ILP is presented in (Athakravi et al, 2013). This approach combines the top-down, meta-level learning of TAL and ASPAL, with theory revision as \u201cnon-monotonic ILP\u201d (Corapi et al, 2008), to address the \u201cgrounding bottleneck\u201d in ASPAL\u2019s functionality. The top theory derived by ASPAL, as a starting point for its search, is based on combinations of the available mode declarations and grows exponentially with the length of its clauses. Thus, obtaining a ground program from this top theory is often very expensive and can cause a learning task to become intractable (Athakravi et al, 2013). RASPAL, the system proposed in (Athakravi et al, 2013), addresses this issue by imposing bounds on the length of the top theory. Partial hypotheses of specified clause length are iteratively obtained in a refinement loop. At each iteration of this loop, the partial hypothesis obtained from the previous refinement step is further refined using theory revision as described in (Corapi et al, 2008). The process continues until a complete and consistent hypothesis is obtained. The authors show that this approach results in shorter ground programs and derives a complete and consistent hypothesis, if\none is derivable from the input data. An important difference between RASPAL and our approach is that the former addresses scalability as related to application domains, which may require a complex language bias, while our approach scales to potentially simpler, but massive volumes of sequential data, typical in temporal applications.\nTAL, ASPAL and RASPAL are top-down learners. In the work presented here, XHAIL, being a bottom-up non-monotonic learning system was the natural choice as the basis of our approach, since we intended to provide a clause refinement search bias by means of most-specific clauses, as in (Duboc et al, 2009). In that work, the Theory Revision system FORTE (Richards and Mooney., 1995) is enhanced by porting PROGOL\u2019s bottom set construction routine to its functionality, towards a more efficient refinement operator. The resulting system, FORTE MBC, works as follows: When a clause C must be refined, FORTE MBC uses mode declarations and an inverse entailment search in the background knowledge to construct a bottom clause from a positive example covered by C. It then searches for antecedents within the bottom clause. As in the case of ILED, the constrained search space results in a more efficient clause refinement process. However FORTE MBC (like FORTE itself) learns Horn theories and does not support non-Observational Predicate Learning, thus it cannot be used for the revision of Event Calculus programs. In addition, it cannot operate on an empty hypothesis (i.e. it cannot induce a hypothesis from scratch). Another important difference between FORTE MBC and ILED is the way that the former handles a potential incompleteness which may result from the specialization of a clause. In particular, once a clause is specialized, FORTE MBC checks again the whole database of examples. If some positive examples have become unprovable due to the specialization, FORTE MBC picks a different positive example covered by the initial, inconsistent clause C, constructs a new bottom clause and searches for a new specialization of clause C. The process continues until the original coverage in the example database is restored. In contrast, by means of the support set, the specializations performed by ILED preserve prior coverage in the historical memory, thus saving inference effort significantly.\nAs mentioned in (Duboc et al, 2009), there is a renewed interest in scaling Theory Revision systems and applications in the last few years, due to the availability of large-scale domain knowledge in various scientific disciplines (Dietterich et al, 2008; Muggleton et al, 2012a). Temporal and stream data are no exception and there is a need for scalable Theory Revision techniques in event-based domains. However, most Theory Revision systems, such as the systems described in (Richards and Mooney, 1991; Quinlan, 1990; Wogulis and Pazzani, 1993) limit their applicability to Horn theories.\nA well-known theory revision system is INTHELEX (Esposito et al, 2000). It is a fully incremental system that learns/revises Datalog theories and has been used in the study of several aspects of incremental learning. In particular, order effects in some simple learning tasks with ILP are discussed in (Mauro et al, 2004, 2005), and concept drift in (Esposito et al, 2004). In (Biba et al, 2008) the authors present an approach towards scaling INTHELEX. In contrast to most ILP systems that keep all examples in the main memory, (Biba et al, 2008) follows an external memory implementation, which is the approach adopted by ILED. Moreover, in that work the authors associate clauses in the theory at hand with examples they cover, via a relational schema. Thus, when a clause is refined, only the examples that were previously covered by this clause are checked. Similarly, when a clause\nis generalized, only the negative examples are checked again. The scalable version of INTHELEX presented in (Biba et al, 2008) maintains alternative versions of the hypothesis at each step, allowing to backtrack to previous states. In addition, it keeps in memory several statistics related to the examples that the system has already seen, such as the number of refinements that each example has caused, a \u201crefinement history\u201d of each clause, etc.\nOn the other hand, INTHELEX has some limitations that make it inappropriate for inducing/revising Event Calculus programs for event recognition applications. First, the restriction of its input language to Datalog limits its applicability to richer, relational event domains. For instance, complex relations between entities cannot be easily expressed in INTHELEX. Second, the use of background knowledge is limited, excluding for instance auxiliary clauses that may be used for spatio-temporal reasoning during learning time. Third, although INTHELEX uses abduction for the completion of imperfect input data, it relies on Observational Predicate Learning, meaning that it is not able to reason with predicates which are not directly observable in the examples. Therefore it cannot be used for learning event definitions."}, {"heading": "8 Conclusions", "text": "We presented an incremental ILP system, ILED, for machine learning knowledge bases for event recognition, in the form of Event Calculus theories. ILED combines techniques from non-monotonic ILP and in particular, the XHAIL algorithm, with theory revision. It acquires an initial hypothesis from the first available piece of data, and revises this hypothesis as new data arrive. Revisions account for all accumulated experience. The main contribution of ILED is that it scales-up XHAIL to large volumes of sequential data with a time-like structure, typical of event-based applications. By means of a compressive memory structure that supports clause refinement, ILED has a scalable, single-pass revision strategy, thanks to which the cost of theory revision grows as a tractable function of the perceived experience. In this work, ILED was evaluated on an activity recognition application and a transport management application. The results indicate that ILED is significantly more efficient than XHAIL, without compromising the quality of the generated hypothesis in terms of predictive accuracy and hypothesis size. Moreover, ILED scales adequately to large data volumes which XHAIL cannot handle. Future work concerns mechanisms for handling noise and concept drift."}, {"heading": "Acknowledgements", "text": "This work is partly funded by the EU project SPEEDD (FP7 619435). We would like to thank the reviewers of the Machine Learning Journal for their valuable comments on the first version of the paper."}, {"heading": "Appendix A Notions from (Inductive) Logic Programming", "text": "Definition 8 (Basic notions from Logic Programming (Lloyd, 1987)) A term is a constant, a variable, or an expression of the form f(a1, . . . , an) where f is a function symbol and a1, . . . , an are terms. A term substitution is a function from the set of terms to itself. An atom is an expression of the form p(a1, . . . , an) where p is a predicate symbol and a1, . . . , an are terms. A literal is either an atom a (positive literal) or its negation not a (negative literal). A clause C is an expression of the form a\u2190 b1, . . . , bn where a is an atom and b1, . . . , bn are literals. a is called the head of clause C, and {b1, . . . , bn} is called the body of the clause. A fact is a clause of the form a \u2190 true and an integrity constraint is a clause of the form false \u2190 a. A logic program is a collection of clauses. A clause or a logic program is Horn if it contains no negated literals and normal otherwise.\nDefinition 9 (Interpretations and models (Gelfond and Lifschitz, 1988)) Given a logic program \u03a0 an Herbrand interpretation I is a subset of the set of all possible groundings of \u03a0. I satisfies a literal a (resp. not a) iff a \u2208 I (resp. a /\u2208 I). I satisfies a set of ground atoms iff it satisfies each one of them and it satisfies a ground clause iff it satisfies the head, or does not satisfy at least one body literal. I is a Herbrand model of \u03a0 iff it satisfies every ground instance of every clause in \u03a0 and it is a minimal model iff no strict subset of I is a model of \u03a0. I is a stable model of \u03a0 iff it is a minimal model of the Horn program that results from the ground instances of \u03a0 after the removal of all clauses with a negated literal not satisfied by I, and all negative literals from the remaining clauses.\nDefinition 10 (Mode Declarations (Muggleton, 1995)) A mode declaration is either a head or body declaration, respectively, modeh(s) and modeb(s), where s is called a schema. A schema s is a ground literal containing placemarkers. A placemarker is either +type (input) \u2212type (output) or #type (ground), where type is a constant. The distinction between input and output terms in mode declarations is that any input term in a body literal must be an input term in the head, or an output term in some preceding body literal.\nDefinition 11 (Mode language (Muggleton, 1995)) A set M of mode declarations defines a language L(M). A clause C is in L(M) if it results from the declarations in M by replacing input and output placemarkers by variables and replacing ground placemarkers with ground terms. In particular C \u2208 L(M) iff its head atom (respectively each of its body literals) is constructed from the schema s in a modeh(s) atom (resp. in a modeb(s) atom) in M as follows:\n\u2013 By replacing an output (\u2212) placemarker by a new variable. \u2013 By replacing an input (+) placemarker by a variable that appears in the head\natom, or in a previous body literal. \u2013 By replacing a ground (#) placemarker by a ground term.\nA hypothesis H is in L(M) iff C \u2208 L(M) for each C \u2208 H.\nDefinition 12 (Variable depth (Muggleton, 1995)) Let C be a clause and X a variable symbol. The depth d(X) of X is defined recursively as follows:\nd(X) = { 0 if Xis in the head of C (minY \u2208VXd(Y )) + 1 else\nwhere VX are the variable symbols that appear in all literals in the body of C in which X also appears.\nDefinition 13 (Depth-bound mode language (Muggleton, 1995)) Let M be a set of mode declarations, i a non-negative integer and C a clause. C is in the depth-bounded mode language Li(M) iff C \u2208 L(M) (see Definition 11) and for each variable symbol X that appears in C it holds that d(X) \u2264 i (see Definition 12). A hypothesis H is in Li(M) iff C \u2208 Li(M) for each C \u2208 H.\nDefinition 14 (Most-specific clause relative to a set of examples) Let Li(M) be the depth-bounded mode language as in Definition 13, E a set of examples and B some background theory. Let Li(M,E) = {C \u2208 Li(M) | B \u222a C E}. A clause \u22a5 \u2208 Li(M,E) is most-specific, relative to E, iff it does not \u03b8-subsume any other clause in Li(M,E)."}], "references": [{"title": "AILP: Abductive inductive logic programming", "author": ["H Ade", "M Denecker"], "venue": null, "citeRegEx": "Ade and Denecker,? \\Q1995\\E", "shortCiteRegEx": "Ade and Denecker", "year": 1995}, {"title": "Deriving non-zeno behaviour models from goal models using ilp", "author": ["D Alrajeh", "J Kramer", "A Russo", "S Uchitel"], "venue": "Formal Aspects of Computing", "citeRegEx": "Alrajeh et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Alrajeh et al\\.", "year": 2010}, {"title": "An inductive approach for modal transition system refinement", "author": ["D Alrajeh", "J Kramer", "A Russo", "S Uchitel"], "venue": "In: International Conference on Logic Programming", "citeRegEx": "Alrajeh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Alrajeh et al\\.", "year": 2011}, {"title": "Behaviour recognition from video content: A logic programming approach", "author": ["A Artikis", "A Skarlatidis", "G Paliouras"], "venue": "International Journal on Artificial Intelligence Tools", "citeRegEx": "Artikis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Artikis et al\\.", "year": 2010}, {"title": "Logic-based event recognition. The Knowledge Engineering Review", "author": ["A Artikis", "A Skarlatidis", "F Portet", "G Paliouras"], "venue": null, "citeRegEx": "Artikis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Artikis et al\\.", "year": 2012}, {"title": "An event calculus for event recognition", "author": ["A Artikis", "M Sergot", "G Paliouras"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "citeRegEx": "Artikis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Artikis et al\\.", "year": 2014}, {"title": "Learning through hypothesis refinement using answer set programming", "author": ["D Athakravi", "D Corapi", "K Broda", "A Russo"], "venue": "Proceedings of the 23rd International Conference of Inductive Logic Programming (ILP", "citeRegEx": "Athakravi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Athakravi et al\\.", "year": 2013}, {"title": "A refinement operator for theories", "author": ["L Badea"], "venue": "Inductive Logic Programming,", "citeRegEx": "Badea,? \\Q2001\\E", "shortCiteRegEx": "Badea", "year": 2001}, {"title": "Improving scalability in ilp incremental systems", "author": ["M Biba", "TMA Basile", "S Ferilli", "F Esposito"], "venue": null, "citeRegEx": "Biba et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Biba et al\\.", "year": 2008}, {"title": "Nonmonotonic learning in large biological networks", "author": ["S Bragaglia", "O Ray"], "venue": "Proc. 24th Int. Conf. on Inductive Logic Programming", "citeRegEx": "Bragaglia and Ray,? \\Q2014\\E", "shortCiteRegEx": "Bragaglia and Ray", "year": 2014}, {"title": "Incremental declarative process mining. Smart Information and Knowledge Management pp", "author": ["M Cattafi", "E Lamma", "F Riguzzi", "S Storari"], "venue": null, "citeRegEx": "Cattafi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cattafi et al\\.", "year": 2010}, {"title": "A calculus of macro-events: Progress report", "author": ["I Cervesato", "A Montanari"], "venue": "Temporal Representation and Reasoning,", "citeRegEx": "Cervesato and Montanari,? \\Q2000\\E", "shortCiteRegEx": "Cervesato and Montanari", "year": 2000}, {"title": "Extending the event calculus for tracking epidemic spread", "author": ["H Chaudet"], "venue": "Artificial Intelligence in Medicine", "citeRegEx": "Chaudet,? \\Q2006\\E", "shortCiteRegEx": "Chaudet", "year": 2006}, {"title": "Learning rules from user behaviour", "author": ["D Corapi", "O Ray", "A Russo", "A Bandara", "E Lupu"], "venue": "Second International Workshop on the Induction of Process Models", "citeRegEx": "Corapi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Corapi et al\\.", "year": 2008}, {"title": "Inductive logic programming as abductive search", "author": ["D Corapi", "A Russo", "EC Lupu"], "venue": "Technical Communications of the 26th International Conference on Logic Programming (ICLP)", "citeRegEx": "Corapi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Corapi et al\\.", "year": 2010}, {"title": "Norm refinement and design through inductive learning. In: Coordination, Organizations, Institutions, and Norms in Agent Systems", "author": ["D Corapi", "M De Vos", "J Padget", "A Russo", "K Satoh"], "venue": null, "citeRegEx": "Corapi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Corapi et al\\.", "year": 2011}, {"title": "Inductive logic programming in answer set programming", "author": ["D Corapi", "A Russo", "E Lupu"], "venue": null, "citeRegEx": "Corapi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Corapi et al\\.", "year": 2011}, {"title": "Abduction in logic programming. Computational Logic: Logic Programming and Beyond", "author": ["M Denecker", "A Kakas"], "venue": null, "citeRegEx": "Denecker and Kakas,? \\Q2002\\E", "shortCiteRegEx": "Denecker and Kakas", "year": 2002}, {"title": "Structured machine learning: the next ten years", "author": ["TG Dietterich", "P Domingos", "L Getoor", "S Muggleton", "P Tadepalli"], "venue": "Machine Learning", "citeRegEx": "Dietterich et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dietterich et al\\.", "year": 2008}, {"title": "Using the bottom clause and mode declarations in FOL theory revision from examples. Machine Learning", "author": ["AL Duboc", "A Paes", "G Zaverucha"], "venue": null, "citeRegEx": "Duboc et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Duboc et al\\.", "year": 2009}, {"title": "Relational data mining", "author": ["S D\u017eeroski"], "venue": null, "citeRegEx": "D\u017eeroski,? \\Q2010\\E", "shortCiteRegEx": "D\u017eeroski", "year": 2010}, {"title": "Abduction compared with negation by failure", "author": ["K Eshghi", "R Kowalski"], "venue": "In: 6th International Conference on Logic Programming", "citeRegEx": "Eshghi and Kowalski,? \\Q1989\\E", "shortCiteRegEx": "Eshghi and Kowalski", "year": 1989}, {"title": "Multistrategy theory revision: Induction and abduction in inthelex", "author": ["F Esposito", "G Semeraro", "N Fanizzi", "S Ferilli"], "venue": "Machine Learning", "citeRegEx": "Esposito et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Esposito et al\\.", "year": 2000}, {"title": "Incremental learning and concept drift in inthelex", "author": ["F Esposito", "S Ferilli", "N Fanizzi", "TMA Basile", "ND Mauro"], "venue": "Intelligent Data Analysis", "citeRegEx": "Esposito et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Esposito et al\\.", "year": 2004}, {"title": "Event processing in action", "author": ["O Etzion", "P Niblett"], "venue": null, "citeRegEx": "Etzion and Niblett,? \\Q2010\\E", "shortCiteRegEx": "Etzion and Niblett", "year": 2010}, {"title": "Normal programs and multiple predicate learning", "author": ["L Fogel", "G Zaverucha"], "venue": "Inductive Logic Programming,", "citeRegEx": "Fogel and Zaverucha,? \\Q1998\\E", "shortCiteRegEx": "Fogel and Zaverucha", "year": 1998}, {"title": "Answer set solving in practice", "author": ["M Gebser", "R Kaminski", "B Kaufmann", "T Schaub"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning", "citeRegEx": "Gebser et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gebser et al\\.", "year": 2012}, {"title": "The stable model semantics for logic programming", "author": ["M Gelfond", "V Lifschitz"], "venue": "In: International Conference on Logic Programming,", "citeRegEx": "Gelfond and Lifschitz,? \\Q1988\\E", "shortCiteRegEx": "Gelfond and Lifschitz", "year": 1988}, {"title": "Generalised stable models: A semantics for abduction", "author": ["A Kakas", "P Mancarella"], "venue": "In: ninth European Conference on Artificial Intelligence", "citeRegEx": "Kakas and Mancarella,? \\Q1990\\E", "shortCiteRegEx": "Kakas and Mancarella", "year": 1990}, {"title": "Abductive logic programming", "author": ["A Kakas", "R Kowalski", "F Toni"], "venue": "Journal of Logic and Computation", "citeRegEx": "Kakas et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Kakas et al\\.", "year": 1993}, {"title": "Induction on failure: Learning connected horn theories", "author": ["T Kimber", "K Broda", "A Russo"], "venue": "Logic Programming and Nonmonotonic Reasoning,", "citeRegEx": "Kimber et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kimber et al\\.", "year": 2009}, {"title": "A logic-based calculus of events. New Generation Computing", "author": ["R Kowalski", "M Sergot"], "venue": null, "citeRegEx": "Kowalski and Sergot,? \\Q1986\\E", "shortCiteRegEx": "Kowalski and Sergot", "year": 1986}, {"title": "A restarted strategy for ecient subsumption testing", "author": ["O Kuzelka", "F Zelezny"], "venue": "Fundamenta Informaticae", "citeRegEx": "Kuzelka and Zelezny,? \\Q2008\\E", "shortCiteRegEx": "Kuzelka and Zelezny", "year": 2008}, {"title": "Learning in Humans and Machines: Towards an Interdisciplinary Learning Science, Elsevier, chap Order Effects in Incremental Learning", "author": ["P Langley"], "venue": null, "citeRegEx": "Langley,? \\Q1995\\E", "shortCiteRegEx": "Langley", "year": 1995}, {"title": "Inductive Logic Programming: Techniques and Applications", "author": ["N Lavrac", "S Dzeroski"], "venue": null, "citeRegEx": "Lavrac and Dzeroski,? \\Q1993\\E", "shortCiteRegEx": "Lavrac and Dzeroski", "year": 1993}, {"title": "Mining frequent itemsets over data streams using efficient window sliding techniques. Expert Systems with Applications", "author": ["HF Li", "SY Lee"], "venue": null, "citeRegEx": "Li and Lee,? \\Q2009\\E", "shortCiteRegEx": "Li and Lee", "year": 2009}, {"title": "An efficient algorithm for mining frequent itemsets over the entire history of data streams", "author": ["HF Li", "SY Lee", "MK Shan"], "venue": "Proc. of First International Workshop on Knowledge Discovery in Data Streams", "citeRegEx": "Li et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Li et al\\.", "year": 2004}, {"title": "Performance evaluating the evaluator", "author": ["T List", "J Bins", "J Vazquez", "RB Fisher"], "venue": "Visual Surveillance and Performance Evaluation of Tracking and Surveillance,", "citeRegEx": "List et al\\.,? \\Q2005\\E", "shortCiteRegEx": "List et al\\.", "year": 2005}, {"title": "Foundations of Logic Programming", "author": ["J Lloyd"], "venue": null, "citeRegEx": "Lloyd,? \\Q1987\\E", "shortCiteRegEx": "Lloyd", "year": 1987}, {"title": "The Power of Events: An Introduction to Complex Event Processing in Distributed Enterprise Systems", "author": ["D Luckham"], "venue": null, "citeRegEx": "Luckham,? \\Q2001\\E", "shortCiteRegEx": "Luckham", "year": 2001}, {"title": "Event processing glossary version 1.1", "author": ["D Luckham", "R Schulte"], "venue": "Event Processing Technical Society", "citeRegEx": "Luckham and Schulte,? \\Q2008\\E", "shortCiteRegEx": "Luckham and Schulte", "year": 2008}, {"title": "Revising process models through inductive learning", "author": ["FM Maggi", "D Corapi", "A Russo", "E Lupu", "G Visaggio"], "venue": "Business Process Management Workshops,", "citeRegEx": "Maggi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maggi et al\\.", "year": 2011}, {"title": "Fast theta-subsumption with constraint satisfaction algorithms", "author": ["J Maloberti", "M Sebag"], "venue": "Machine Learning", "citeRegEx": "Maloberti and Sebag,? \\Q2004\\E", "shortCiteRegEx": "Maloberti and Sebag", "year": 2004}, {"title": "Avoiding order effects in incremental learning", "author": ["ND Mauro", "F Esposito", "S Ferilli"], "venue": "TB", "citeRegEx": "Mauro et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Mauro et al\\.", "year": 2005}, {"title": "An investigation into theory completion techniques in inductive logic", "author": ["S Moyle"], "venue": "PhD thesis,", "citeRegEx": "Moyle,? \\Q2003\\E", "shortCiteRegEx": "Moyle", "year": 2003}, {"title": "Commonsense Reasoning", "author": ["E Mueller"], "venue": null, "citeRegEx": "Mueller,? \\Q2006\\E", "shortCiteRegEx": "Mueller", "year": 2006}, {"title": "Event calculus. Handbook of Knowledge Representation 3 of FAI:671\u2013708", "author": ["E Mueller"], "venue": null, "citeRegEx": "Mueller,? \\Q2008\\E", "shortCiteRegEx": "Mueller", "year": 2008}, {"title": "Inverse entailment and progol", "author": ["S Muggleton"], "venue": "New Generation Computing", "citeRegEx": "Muggleton,? \\Q1995\\E", "shortCiteRegEx": "Muggleton", "year": 1995}, {"title": "Theory completion using inverse entailment", "author": ["S Muggleton", "C Bryant"], "venue": "In: International Conference on Inductive Logic Programming,", "citeRegEx": "Muggleton and Bryant,? \\Q2000\\E", "shortCiteRegEx": "Muggleton and Bryant", "year": 2000}, {"title": "Meta-interpretive learning of higher-order dyadic datalog: Predicate invention revisited", "author": ["S Muggleton", "D Lin"], "venue": "Proceedings of the Twenty-Third international joint conference on Artificial Intelligence,", "citeRegEx": "Muggleton and Lin,? \\Q2013\\E", "shortCiteRegEx": "Muggleton and Lin", "year": 2013}, {"title": "Inductive logic programming: Theory and methods", "author": ["S Muggleton", "LD Raedt"], "venue": "Journal of Logic Programming", "citeRegEx": "Muggleton and Raedt,? \\Q1994\\E", "shortCiteRegEx": "Muggleton and Raedt", "year": 1994}, {"title": "2012a) Chess revision: acquiring the rules of chess variants through fol theory revision from examples", "author": ["S Muggleton", "A Paes", "VS Costa", "G Zaverucha"], "venue": "Inductive Logic Programming", "citeRegEx": "Muggleton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Muggleton et al\\.", "year": 2012}, {"title": "A (2012b) ILP turns 20 - biography and future challenges", "author": ["S Muggleton", "LD Raedt", "D Poole", "I Bratko", "P Flach", "K Inoue", "Srinivasan"], "venue": null, "citeRegEx": "Muggleton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Muggleton et al\\.", "year": 2012}, {"title": "A (2014) Meta-interpretive learning: application to grammatical inference. Machine Learning", "author": ["SH Muggleton", "D Lin", "N Pahlavi", "Tamaddoni-Nezhad"], "venue": null, "citeRegEx": "Muggleton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Muggleton et al\\.", "year": 2014}, {"title": "Induction of stable models", "author": ["RP Otero"], "venue": "Inductive Logic Programming,", "citeRegEx": "Otero,? \\Q2001\\E", "shortCiteRegEx": "Otero", "year": 2001}, {"title": "Induction of the effects of actions by monotonic methods", "author": ["RP Otero"], "venue": "Inductive Logic Programming,", "citeRegEx": "Otero,? \\Q2003\\E", "shortCiteRegEx": "Otero", "year": 2003}, {"title": "Eca-ruleml: An approach combining eca rules with temporal interval-based kr event logics and transactional update logics", "author": ["A Paschke"], "venue": null, "citeRegEx": "Paschke,? \\Q2005\\E", "shortCiteRegEx": "Paschke", "year": 2005}, {"title": "Learning logical definitions from relations", "author": ["JR Quinlan"], "venue": "Machine Learning", "citeRegEx": "Quinlan,? \\Q1990\\E", "shortCiteRegEx": "Quinlan", "year": 1990}, {"title": "Using abduction for induction of normal logic programs. In: ECAI06 Workshop on Abduction and Induction in Articial Intelligence and Scientic Modelling", "author": ["O Ray"], "venue": null, "citeRegEx": "Ray,? \\Q2006\\E", "shortCiteRegEx": "Ray", "year": 2006}, {"title": "Nonmonotonic abductive inductive learning", "author": ["O Ray"], "venue": "Journal of Applied Logic", "citeRegEx": "Ray,? \\Q2009\\E", "shortCiteRegEx": "Ray", "year": 2009}, {"title": "Hybrid abductive inductive learning: A generalisation of progol", "author": ["O Ray", "K Broda", "A Russo"], "venue": "In: International Conference in Inductive Logic Programming (ILP),", "citeRegEx": "Ray et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ray et al\\.", "year": 2003}, {"title": "Automated refinement of first-order horn clause domain theories. Machine Learning", "author": ["B Richards", "R Mooney"], "venue": null, "citeRegEx": "Richards and Mooney,? \\Q1995\\E", "shortCiteRegEx": "Richards and Mooney", "year": 1995}, {"title": "First order theory revision", "author": ["BL Richards", "RJ Mooney"], "venue": "In: 8th International Workshop on Machine Learning,", "citeRegEx": "Richards and Mooney,? \\Q1991\\E", "shortCiteRegEx": "Richards and Mooney", "year": 1991}, {"title": "Inverse entailment in nonmonotonic logic programs", "author": ["C Sakama"], "venue": null, "citeRegEx": "Sakama,? \\Q2000\\E", "shortCiteRegEx": "Sakama", "year": 2000}, {"title": "Subsumer: A prolog theta-subsumption", "author": ["J Santos", "S Muggleton"], "venue": "ACM Transactions on Computational Logic", "citeRegEx": "Santos and Muggleton,? \\Q2010\\E", "shortCiteRegEx": "Santos and Muggleton", "year": 2010}, {"title": "A methodology for evaluating theory revision", "author": ["J Wogulis", "M Pazzani"], "venue": null, "citeRegEx": "Wogulis and Pazzani,? \\Q1993\\E", "shortCiteRegEx": "Wogulis and Pazzani", "year": 1993}, {"title": "First order theory refinement. In: Raedt LD (ed) Advances", "author": ["S Wrobel"], "venue": "Intelligence IJCAI,", "citeRegEx": "Wrobel,? \\Q1996\\E", "shortCiteRegEx": "Wrobel", "year": 1996}], "referenceMentions": [{"referenceID": 24, "context": "Event Recognition (Etzion and Niblett, 2010; Luckham, 2001; Luckham and Schulte, 2008) refers to the automatic detection of event occurrences within a system.", "startOffset": 18, "endOffset": 86}, {"referenceID": 39, "context": "Event Recognition (Etzion and Niblett, 2010; Luckham, 2001; Luckham and Schulte, 2008) refers to the automatic detection of event occurrences within a system.", "startOffset": 18, "endOffset": 86}, {"referenceID": 40, "context": "Event Recognition (Etzion and Niblett, 2010; Luckham, 2001; Luckham and Schulte, 2008) refers to the automatic detection of event occurrences within a system.", "startOffset": 18, "endOffset": 86}, {"referenceID": 31, "context": "Event recognition systems with a logic-based representation of event definitions, such as the Event Calculus (Kowalski and Sergot, 1986), are attracting significant attention in the event processing community for a number of reasons, including the expressiveness and understandability of the formalized knowledge, their declarative, formal semantics (Paschke, 2005; Artikis et al, 2012) and their ability to handle rich background knowledge.", "startOffset": 109, "endOffset": 136}, {"referenceID": 56, "context": "Event recognition systems with a logic-based representation of event definitions, such as the Event Calculus (Kowalski and Sergot, 1986), are attracting significant attention in the event processing community for a number of reasons, including the expressiveness and understandability of the formalized knowledge, their declarative, formal semantics (Paschke, 2005; Artikis et al, 2012) and their ability to handle rich background knowledge.", "startOffset": 350, "endOffset": 386}, {"referenceID": 50, "context": "Using logic programs in particular, has an extra advantage, due to the close connection between logic programming and machine learning in the field of Inductive Logic Programming (ILP) (Muggleton and Raedt, 1994; Lavrac and Dzeroski, 1993).", "startOffset": 185, "endOffset": 239}, {"referenceID": 34, "context": "Using logic programs in particular, has an extra advantage, due to the close connection between logic programming and machine learning in the field of Inductive Logic Programming (ILP) (Muggleton and Raedt, 1994; Lavrac and Dzeroski, 1993).", "startOffset": 185, "endOffset": 239}, {"referenceID": 45, "context": "Several logical formalisms which incorporate time and change employ nonmonotonic operators as a means for representing commonsense phenomena (Mueller, 2006).", "startOffset": 141, "endOffset": 156}, {"referenceID": 63, "context": "Most ILP learners cannot handle NaF at all, or lack a robust NaF semantics (Sakama, 2000; Ray, 2009).", "startOffset": 75, "endOffset": 100}, {"referenceID": 59, "context": "Most ILP learners cannot handle NaF at all, or lack a robust NaF semantics (Sakama, 2000; Ray, 2009).", "startOffset": 75, "endOffset": 100}, {"referenceID": 47, "context": "In ILP the ability to reason with missing, or indirectly observable knowledge is called non-Observational Predicate Learning (non-OPL) (Muggleton, 1995).", "startOffset": 135, "endOffset": 152}, {"referenceID": 58, "context": "This is a task that most ILP systems have difficulty to handle, especially when combined with NaF in the background knowledge (Ray, 2006).", "startOffset": 126, "endOffset": 137}, {"referenceID": 17, "context": "One way to address this problem is through the combination of ILP with Abductive Logic Programming (ALP) (Denecker and Kakas, 2002; Kakas and Mancarella, 1990; Kakas et al, 1993).", "startOffset": 105, "endOffset": 178}, {"referenceID": 28, "context": "One way to address this problem is through the combination of ILP with Abductive Logic Programming (ALP) (Denecker and Kakas, 2002; Kakas and Mancarella, 1990; Kakas et al, 1993).", "startOffset": 105, "endOffset": 178}, {"referenceID": 21, "context": "Abduction in logic programming is usually given a non-monotonic semantics (Eshghi and Kowalski, 1989) and in addition, it is by nature an appropriate framework for reasoning with incomplete knowledge.", "startOffset": 74, "endOffset": 101}, {"referenceID": 0, "context": "Although it has a long history in the literature (Ade and Denecker, 1995), only recently has this combination brought about systems such as XHAIL (Ray, 2009), TAL (Corapi et al, 2010) and ASPAL (Corapi et al, 2011b; Athakravi et al, 2013) that may be used for the induction of event-based knowledge.", "startOffset": 49, "endOffset": 73}, {"referenceID": 59, "context": "Although it has a long history in the literature (Ade and Denecker, 1995), only recently has this combination brought about systems such as XHAIL (Ray, 2009), TAL (Corapi et al, 2010) and ASPAL (Corapi et al, 2011b; Athakravi et al, 2013) that may be used for the induction of event-based knowledge.", "startOffset": 146, "endOffset": 157}, {"referenceID": 66, "context": "This process, also known as Theory Revision (Wrobel, 1996), exploits previous computations to speed-up the learning, since revising a hypothesis is generally considered more efficient than learning it from scratch (Biba et al, 2008; Esposito et al, 2000; Cattafi et al, 2010).", "startOffset": 44, "endOffset": 58}, {"referenceID": 38, "context": "We assume a first-order language as in (Lloyd, 1987) where not in front of literals denotes Negation as Failure (NaF).", "startOffset": 39, "endOffset": 52}, {"referenceID": 27, "context": "We define the entailment relation between normal logic programs in terms of the stable model semantics (Gelfond and Lifschitz, 1988) and in particular its credulous version, under which program \u03a01 entails program \u03a02, denoted by \u03a01 \u03a02, if at least one stable model of \u03a01 is a stable model of \u03a02.", "startOffset": 103, "endOffset": 132}, {"referenceID": 47, "context": "A commonly used language bias in ILP, also employed in this work is mode declarations (Muggleton, 1995).", "startOffset": 86, "endOffset": 103}, {"referenceID": 20, "context": "\u03b8-subsumption provides a syntactic notion of generality (D\u017eeroski, 2010) which may be used to search for clauses based on their example coverage.", "startOffset": 56, "endOffset": 72}, {"referenceID": 66, "context": "Theory revision is the process of acting upon a hypothesis by means of syntactic transformations (generalization and specialization operators), in order to change the answer set of the hypothesis (Wrobel, 1996; Esposito et al, 2000), that is, the examples it accounts for.", "startOffset": 196, "endOffset": 232}, {"referenceID": 31, "context": "The Event Calculus (Kowalski and Sergot, 1986) is a temporal logic for reasoning about events and their effects.", "startOffset": 19, "endOffset": 46}, {"referenceID": 56, "context": "It is a formalism that has been successfully used in numerous event recognition applications (Paschke, 2005; Artikis et al, 2014; Chaudet, 2006; Cervesato and Montanari, 2000).", "startOffset": 93, "endOffset": 175}, {"referenceID": 12, "context": "It is a formalism that has been successfully used in numerous event recognition applications (Paschke, 2005; Artikis et al, 2014; Chaudet, 2006; Cervesato and Montanari, 2000).", "startOffset": 93, "endOffset": 175}, {"referenceID": 11, "context": "It is a formalism that has been successfully used in numerous event recognition applications (Paschke, 2005; Artikis et al, 2014; Chaudet, 2006; Cervesato and Montanari, 2000).", "startOffset": 93, "endOffset": 175}, {"referenceID": 46, "context": "implies, it is a simplified version of the Discrete Event Calculus, a dialect which is equivalent to the classical Event Calculus when time ranges over integer domains (Mueller, 2008).", "startOffset": 168, "endOffset": 183}, {"referenceID": 47, "context": "1 requires non-Observational Predicate Learning (non-OPL) (Muggleton, 1995), meaning that instances of target predicates (initiatedAt/2 and terminatedAt/2) are not provided with the supervision.", "startOffset": 58, "endOffset": 75}, {"referenceID": 48, "context": "PROGOL5 (Muggleton and Bryant, 2000), ALEPH and ALECTO (Moyle, 2003) support some form of abductive reasoning but lack the full power of ALP.", "startOffset": 8, "endOffset": 36}, {"referenceID": 44, "context": "PROGOL5 (Muggleton and Bryant, 2000), ALEPH and ALECTO (Moyle, 2003) support some form of abductive reasoning but lack the full power of ALP.", "startOffset": 55, "endOffset": 68}, {"referenceID": 58, "context": "As a result, they cannot reason abductively with negated atoms (Ray, 2006).", "startOffset": 63, "endOffset": 74}, {"referenceID": 58, "context": "First, as explained in (Ray, 2006), the standard set cover approach on which most ILP systems rely, is essentially unsound in the", "startOffset": 23, "endOffset": 34}, {"referenceID": 25, "context": "The second shortcoming concerns theory revision, and is related to the standard \u03b8-subsumption-based heuristics used in Horn logic, which are known to be inapplicable in general in the case of normal logic programs (Fogel and Zaverucha, 1998).", "startOffset": 214, "endOffset": 241}, {"referenceID": 47, "context": "These systems construct hypotheses one clause at a time, using a positive example as a \u201cseed\u201d, from which a most-specific Bottom Clause is generated by inverse entailment (Muggleton, 1995).", "startOffset": 171, "endOffset": 188}, {"referenceID": 59, "context": "As explained in (Ray, 2009), the intuition is as follows: In order for the head atom of clause Ci \u2208 UKv to contribute towards the coverage of an example, each of its try(i , j , v(\u03b4 i )) atoms must succeed.", "startOffset": 16, "endOffset": 27}, {"referenceID": 33, "context": "This is in line with a key requirement of incremental learning where \u201cthe incorporation of experience into memory during learning should be computationally efficient, that is, theory revision must be efficient in fitting new incoming observations\u201d (Langley, 1995; Mauro et al, 2005).", "startOffset": 248, "endOffset": 282}, {"referenceID": 35, "context": "In the stream processing literature, the number of passes over a stream of data is often used as a measure of the efficiency of algorithms (Li et al, 2004; Li and Lee, 2009).", "startOffset": 139, "endOffset": 173}, {"referenceID": 7, "context": "Revision operators that retract knowledge, such as the deletion of clauses or antecedents are excluded, due to the exponential cost of backtracking in the historical memory (Badea, 2001).", "startOffset": 173, "endOffset": 186}, {"referenceID": 59, "context": "That is, the hypotheses computable by XHAIL form a superset of those computable by other prominent Inverse Entailment systems like PROGOL and ALEPH (Ray, 2009).", "startOffset": 148, "endOffset": 159}, {"referenceID": 42, "context": "In addition, a number of optimization techniques have been developed over the years and several generic subsumption engines have been proposed (Maloberti and Sebag, 2004; Kuzelka and Zelezny, 2008; Santos and Muggleton, 2010), some of which are able to efficiently compute subsumption relations between clauses comprising thousands of literals and hundreds of distinct variables.", "startOffset": 143, "endOffset": 225}, {"referenceID": 32, "context": "In addition, a number of optimization techniques have been developed over the years and several generic subsumption engines have been proposed (Maloberti and Sebag, 2004; Kuzelka and Zelezny, 2008; Santos and Muggleton, 2010), some of which are able to efficiently compute subsumption relations between clauses comprising thousands of literals and hundreds of distinct variables.", "startOffset": 143, "endOffset": 225}, {"referenceID": 64, "context": "In addition, a number of optimization techniques have been developed over the years and several generic subsumption engines have been proposed (Maloberti and Sebag, 2004; Kuzelka and Zelezny, 2008; Santos and Muggleton, 2010), some of which are able to efficiently compute subsumption relations between clauses comprising thousands of literals and hundreds of distinct variables.", "startOffset": 143, "endOffset": 225}, {"referenceID": 9, "context": "To achieve this aim we had to implement XHAIL, because the original implementation was not publicly available until recently (Bragaglia and Ray, 2014).", "startOffset": 125, "endOffset": 150}, {"referenceID": 59, "context": "A thorough review of the drawbacks of state-of-the-art ILP systems with respect to non-monotonic domains, as well as the deficiencies of existing approaches to learning Event Calculus programs can be found in (Ray, 2009; Sakama, 2005, 2001; Otero, 2001, 2003).", "startOffset": 209, "endOffset": 259}, {"referenceID": 48, "context": "The main obstacle, common to many learners which combine ILP with some form of abduction, like PROGOL5 (Muggleton and Bryant, 2000), ALECTO (Moyle, 2003), HAIL (Ray et al, 2003) and IMPARO (Kimber et al, 2009), is that they cannot perform abduction through negation and are thus essentially limited to Observational Predicate Learning.", "startOffset": 103, "endOffset": 131}, {"referenceID": 44, "context": "The main obstacle, common to many learners which combine ILP with some form of abduction, like PROGOL5 (Muggleton and Bryant, 2000), ALECTO (Moyle, 2003), HAIL (Ray et al, 2003) and IMPARO (Kimber et al, 2009), is that they cannot perform abduction through negation and are thus essentially limited to Observational Predicate Learning.", "startOffset": 140, "endOffset": 153}, {"referenceID": 49, "context": "Application examples involve learning definite clause grammars (Muggleton et al, 2014), discovering relations between entities and learning simple robot action strategies (Muggleton and Lin, 2013).", "startOffset": 171, "endOffset": 196}, {"referenceID": 62, "context": "However, most Theory Revision systems, such as the systems described in (Richards and Mooney, 1991; Quinlan, 1990; Wogulis and Pazzani, 1993) limit their applicability to Horn theories.", "startOffset": 72, "endOffset": 141}, {"referenceID": 57, "context": "However, most Theory Revision systems, such as the systems described in (Richards and Mooney, 1991; Quinlan, 1990; Wogulis and Pazzani, 1993) limit their applicability to Horn theories.", "startOffset": 72, "endOffset": 141}, {"referenceID": 65, "context": "However, most Theory Revision systems, such as the systems described in (Richards and Mooney, 1991; Quinlan, 1990; Wogulis and Pazzani, 1993) limit their applicability to Horn theories.", "startOffset": 72, "endOffset": 141}], "year": 2014, "abstractText": "Event recognition systems rely on properly engineered knowledge bases of event definitions to infer occurrences of events in time. The manual development of such knowledge is a tedious and error-prone task, thus event-based applications may benefit from automated knowledge construction techniques, such as Inductive Logic Programming (ILP), which combines machine learning with the declarative and formal semantics of First-Order Logic. However, learning temporal logical formalisms, which are typically utilized by logic-based Event Recognition systems is a challenging task, which most ILP systems cannot fully undertake. In addition, event-based data is usually massive and collected at different times and under various circumstances. Ideally, systems that learn from temporal data should be able to operate in an incremental mode, that is, revise prior constructed knowledge in the face of new evidence. Most ILP systems are batch learners, in the sense that in order to account for new evidence they have no alternative but to forget past knowledge and learn from scratch. Given the increased inherent complexity of ILP and the volumes of real-life temporal data, this results to algorithms that scale poorly. In this work we present an incremental method for learning and revising event-based knowledge, in the form of Event Calculus programs. The proposed algorithm relies on abductive-inductive learning and comprises a scalable clause refinement methodology, based on a compressive summarization of clause coverage in a stream of examples. We present an empirical evaluation of our approach on real and synthetic data from activity recognition and city transport applications. Nikos Katzouris National Center for Scientific Research \u201cDemokritos\u201d and National University of Athens E-mail: nkatz@iit.demokritos.gr Alexander Artikis University of Piraeus and National Center for Scientific Research \u201cDemokritos\u201d E-mail: E-mail: a.artikis@iit.demokritos.gr George Paliouras National Center for Scientific Research \u201cDemokritos\u201d E-mail: paliourg@iit.demokritos.gr ar X iv :1 40 2. 59 88 v2 [ cs .L G ] 2 2 N ov 2 01 4 2 Nikos Katzouris et al.", "creator": "LaTeX with hyperref package"}}}