{"id": "1405.2798", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2014", "title": "Two-Stage Metric Learning", "abstract": "In this paper, we present a novel two-stage metric learning algorithm. We first map each learning instance to a probability distribution by computing its similarities to a set of fixed anchor points. Then, we define the distance in the input data space as the Fisher information distance on the associated statistical manifold. This induces in the input data space a new family of distance metric with unique properties. Unlike kernelized metric learning, we do not require the similarity measure to be positive semi-definite. Moreover, it can also be interpreted as a local metric learning algorithm with well defined distance approximation. We evaluate its performance on a number of datasets. It outperforms significantly other metric learning methods and SVM.", "histories": [["v1", "Mon, 12 May 2014 15:18:15 GMT  (287kb,D)", "http://arxiv.org/abs/1405.2798v1", "Accepted for publication in ICML 2014"]], "COMMENTS": "Accepted for publication in ICML 2014", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["jun wang 0017", "ke sun 0001", "fei sha", "st\u00e9phane marchand-maillet", "alexandros kalousis"], "accepted": true, "id": "1405.2798"}, "pdf": {"name": "1405.2798.pdf", "metadata": {"source": "META", "title": "Two-Stage Metric Learning", "authors": ["Jun Wang", "Ke Sun", "Fei Sha", "Stephane Marchand-Maillet"], "emails": ["JUN.WANG@UNIGE.CH", "KE.SUN@UNIGE.CH", "FEISHA@USC.EDU", "STEPHANE.MARCHAND-MAILLET@UNIGE.CH", "ALEXANDROS.KALOUSIS@HESGE.CH"], "sections": [{"heading": "1. Introduction", "text": "Distance measures play a crucial role in many machine learning tasks and algorithms. Standard distance metrics, e.g. Euclidean, cannot address in a satisfactory manner the multitude of learning problems, a fact that led to the development of metric learning methods which learn problemspecific distance measure directly from the data (Weinberger & Saul, 2009; Wang et al., 2012; Jain et al., 2010).\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nOver the last years various metric learning algorithms have been shown to perform well in different learning problems, however, each comes with its own set of limitations.\nLearning the distance metric with one global linear transformation is called single metric learning (Weinberger & Saul, 2009; Davis et al., 2007). In this approach the distance computation is equivalent to applying on the learning instances a learned linear transformation followed by a standard distance metric computation in the projected space. Since the discriminatory power of the input features might vary locally, this approach is often not flexible enough to fit well the distance in different regions.\nLocal metric learning addresses this limitation by learning in each neighborhood one local metric (Noh et al., 2009; Wang et al., 2012). When the local metrics vary smoothly in the feature space, learning local metrics is equivalent to learning the Riemannian metric on the data manifold (Hauberg et al., 2012). The main challenge here is that the geodesic distance endowed by the Riemannian metric is often computationally very expensive. In practice, it is approximated by assuming that the geodesic curves are formed by straight lines and the local metric does not change along these lines (Noh et al., 2009; Wang et al., 2012). Unfortunately, the approximation does not satisfy the symmetric property and therefore the result is a nonmetric distance.\nKernelized Metric Learning (KML) achieves flexibility in a different way (Jain et al., 2010; Wang et al., 2011). In KML learning instances are first mapped into the Reproducing-Kernel Hilbert Space (RKHS) by a kernel\nar X\niv :1\n40 5.\n27 98\nv1 [\ncs .L\nG ]\nfunction and then a global Mahalanobis metric is learned in the RKHS space. By defining the distance in the input feature space as the Mahalanobis distance in the RKHS space, KML is equivalent to learning a flexible non-linear distance in the input space. However, its main limitation is that the kernel matrix induced by the kernel function must be Positive Semi-Definite (PSD). Although Non-PSD kernel could be transformed into PSD kernel (Chen & Ye, 2008; Ying et al., 2009), the new PSD kernel nevertheless cannot keep all original similarity information.\nIn this paper, we propose a novel two-stage metric learning algorithm, Similarity-Based Fisher Information Metric Learning (SBFIML). It first maps instances from the data manifold into finite discrete distributions by computing their similarities to a number of predefined anchor points in the data space. Then, the Fisher information distance on the statistical manifold is used as the distance in the input feature space. This induces a new family of Riemannian distance metric in the input data space with two important properties. First, the new Riemannian metric is robust to density variation in the original data space. Without such robustness, an objective function can be easily biased towards data regions the density of which is low and thus dominates learning of the objective function. Second, the new Riemannian metric has largest distance discrimination on the manifold of anchor points and no distance in the directions being orthogonal to the manifold. So, the effect of locally irrelevant dimensions of anchor points is removed. To the best of our knowledge, this is the first metric learning algorithm that has these two important properties.\nSBFIML is flexible and general; it can be applied to different types of data spaces with various non-negative similarity functions. Comparing to KML, SBFIML does not require the similarity measure to form a PSD matrix. Moreover, SBFIML can be interpreted as a local metric learning algorithm. Compared to the previous local metric learning algorithms which produce a non-metric distance (Noh et al., 2009; Wang et al., 2012), the distance approximation in SBFIML is a well defined distance function with a closed form expression. We evaluate SBFIML on a number of datasets. The experimental results show that it outperforms in a statistically significant manner both metric learning methods and SVM."}, {"heading": "2. Preliminaries", "text": "We are given a number of learning instances {x1, . . . ,xn}, where each instance xTi \u2208 X is a d-dimensional vector, and a vector of associated class labels y = (y1, . . . , yn)T , yi \u2208 {1, . . . , c}. We assume that the input feature space X is a smooth manifold. Different learning problems can have very different types of data manifolds with possibly different dimensionality. The most commonly used manifold in\nmetric learning is the Euclidean space Rd (Weinberger & Saul, 2009). The probability simplex space Pd\u22121 has also been explored (Lebanon, 2006; Cuturi & Avis, 2011; Kedem et al., 2012).\nWe propose a general two-stage metric learning algorithm which can learn a flexible distance in different types of X data manifolds, e.g. Euclidean, probability simplex, hypersphere, etc. Concretely, we first map instances fromX onto the statistical manifold S through a similarity-based differential map, which computes their non-negative similarities to a number of predefined anchor points. Then we define the Fisher information distance as the distance on X . We have chosen to do so, since this induces a new family of Riemannian distance metric which enjoys interesting properties: 1) The new Riemannian metric is robust to density variations in the original data space, which can be produced for example by different intrinsic variabilities of the learning instances in the different categories. Distance learning over this new metric is hence robust to density variation. 2) The new Riemannian distance metric has largest distance discrimination on the manifold of the anchor points and has no distance in the directions being orthogonal to that manifold. So, the new distance metric can remove the effect of locally irrelevant dimensions of the anchor point manifold, see Figure 1 for more detials. In the remainder of this section, we will briefly introduce the necessary terminology and concepts. More details can be found in the monographs (Lee, 2002; Amari & Nagaoka, 2007).\nStatistical Manifold. We denote byMn a n-dimensional smooth manifold. For each point p on Mn, there exists at least one smooth coordinate chart (U , \u03d5) which defines a coordinate system to points on U , where U is an open subset ofMn containing p and \u03d5 : U \u2212\u2192 \u0398 is a smooth coordinate map \u03d5(p) = \u03b8 \u2208 \u0398 \u2282 Rn. \u03b8 is the coordinate of p defined by \u03d5.\nA statistical manifold is a smooth manifold whose points are probability distributions. Given a n-dimensional statistical manifold Sn, we denote by p(\u03be|\u03b8) a probability distribution in Sn, where \u03b8 = (\u03b81, . . . , \u03b8n) \u2208 \u0398 \u2282 Rn is the coordinate of p(\u03be|\u03b8) under some coordinate map \u03d5 and \u03be is the random variable of the p(\u03be|\u03b8) distribution taking values from some set \u039e. Note that, all the probability distributions in Sn share the same set \u039e.\nIn this paper, we are particularly interested in the ndimensional statistical manifoldPn, whose points are finite discrete distributions, denoted by Pn = {p(\u03be|\u03b8 = (\u03b81, . . . , \u03b8n)) : n\u2211 i=1 \u03b8i < 1,\u2200i, \u03b8i > 0} (1)\nwhere \u03be is the discrete random variable taking values in the set \u039e = {1, . . . , n + 1} and \u03b8 \u2208 \u0398 \u2282 Rn is called the m-affine coordinate (Amari & Nagaoka, 2007). The\nprobability mass of p(\u03be|\u03b8) is p(\u03be = i) = \u03b8i if i 6= n + 1, otherwise p(\u03be = n+ 1) = 1\u2212 \u2211n k=1 \u03b8k.\nFisher Information Metric. The Fisher information metric is a Riemannian metric defined on statistical manifolds and endows a distance between probability distributions (Radhakrishna Rao, 1945). The explicit form of the Fisher information metric at p(\u03be|\u03b8) is a n \u00d7 n positive definite symmetric matrix GFIM (\u03b8), the (i, j) element of which is defined by:\nGijFIM (\u03b8) = \u222b \u039e \u2202 log p(\u03be|\u03b8) \u2202\u03b8i \u2202 log p(\u03be|\u03b8) \u2202\u03b8j p(\u03be|\u03b8)d\u03be (2)\nwhere the above integral is replaced with a sum if \u039e is discrete. The following lemma gives the explicit form of the Fisher information metric on Pn. Lemma 1. On the statistical manifold Pn, the Fisher information metric GFIM (\u03b8) at p(\u03be|\u03b8) with coordinate \u03b8 is\nGijFIM (\u03b8) = 1\n\u03b8i \u03b4ij +\n1 1\u2212 \u2211n k=1 \u03b8k ,\u2200i, j \u2208 {1, . . . , n} (3)\nwhere \u03b4ij = 1 if i = j, otherwise \u03b4ij = 0.\nProperties of Fisher Information Metric. The Fisher information metric enjoys a number of interesting properties. First, the Fisher information metric is the unique Riemannian metric induced by all f -divergence measures, such as the Kullback-Leibler (KL) divergence and the \u03c72 divergence (Amari & Cichocki, 2010). All these divergences converge to the Fisher information distance as the two probability distributions are approaching each other. Another important property of the Fisher information metric from a metric learning perspective is that the distance it endows can be approximated by the Hellinger distance, the cosine distance and all f -divergence measures (Kass & Vos, 2011). More importantly, when Sn is the statistical manifold of finite discrete distributions, e.g. Pn, the cosine distance is exactly equivalent to the Fisher information distance (Lebanon, 2006; Lee et al., 2007).\nPullback Metric. LetMn and Nm be two smooth manifolds and TpMn be the tangent space ofMn at p \u2208 Mn. Given a differential map f : Mn \u2212\u2192 Nm and a Riemannian metric G on Nm, the differential map f induces a pullback metric G\u2217 at each point p onMn defined by:\n\u3008v1,v2\u3009G\u2217(p) = \u3008Dpf(v1), Dpf(v2)\u3009G(f(p)) (4)\nwhere Dpf : TpMn \u2212\u2192 Tf(p)Nm is the differential of f at point p \u2208 Mn, which maps tangent vectors v \u2208 TpMn to tangent vectors Dpf(v) \u2208 Tf(p)Nm.\nGiven the coordinate systems \u0398 and \u0393 of U \u2282 Mn and U \u2032 \u2282 Nm respectively, defined by some smooth coordinate maps \u03d5U and \u03d5U \u2032 respectively, then the explicit form of\nthe pullback metric at point p \u2208 U \u2282 Mn with coordinate \u03b8 = \u03d5U (p) is:\nG\u2217(\u03b8) = JTG(\u03b3)J (5)\nwhere \u03b3 = \u03d5U \u2032(f(p)) is the coordinate of the f(p) \u2208 U \u2032 \u2282 Nm and J is the Jacobian matrix of the function \u03d5U \u2032 \u25e6 f \u25e6 \u03d5\u22121U : \u0398 \u2212\u2192 \u0393 at point \u03b8. Since G is a Riemannian metric, the pullback metric G\u2217 is in general at least a PSD metric.\nThe following lemma gives the relation between the geodesic distances onMn and Nm. Lemma 2. Let G\u2217 be the pullback metric of a Riemannian metric G induced by a differential map f :Mn \u2212\u2192 Nm, dG\u2217(p\n\u2032, p) be the geodesic distance on Mn endowed by G\u2217 and dG(f(p\u2032), f(p)) the geodesic distance on Nm endowed by G, then, it holds limp\u2032\u2192p dG(f(p \u2032),f(p))\ndG\u2217 (p\u2032,p) = 1\nThe proof of Lemma 2 is provided in the appendix. In addition to approximating dG\u2217(p\u2032, p) directly onMn by assuming that the geodesic curve is formed by straight lines as previous local metric learning algorithms do (Noh et al., 2009; Wang et al., 2012), Lemma 2 allows us to also approximate it with dG(f(p\u2032), f(p)) on Nm. Note that, both approximations have the same asymptotic convergence result."}, {"heading": "3. Similarity-Based Fisher Information Metric Learning", "text": "We will now present our two-stage metric learning algorithm, SBFIML. In the following, we will first present how to define the similarity-based differential map f : X \u2212\u2192 P and then how to learn the Fisher information distance."}, {"heading": "3.1. Similarity-Based Differential Map", "text": "Given a number of anchor points {z1, . . . ,zn}, zi \u2208 X , we denote by s = (s1, . . . , sn) : X \u2212\u2192 R+\nn the differentiable similarity function. Each sk : X \u2212\u2192 R+ component is a differentiable function the output of which is a nonnegative similarity between some input instance xi and the anchor point zk. Based on the similarity function s we define the similarity-based differential map f as:\nf(xi) = p(\u03be|( s1(xi)\u2211n k=1 sk(xi) , . . . , sn\u22121(xi)\u2211n k=1 sk(xi) ))(6)\n= (s\u03041(xi), . . . , s\u0304n\u22121(xi))\nwhere f(xi) is a finite discrete distribution on manifold Pn\u22121. From now on, for simplicity, we will denote f(xi) by pi(\u03be). The probability mass of the kth outcome is given by: pi(\u03be = k) = s\u0304k(xi) =\nsk(xi)\u2211n k=1 sk(xi)\n. In order for f to be a valid differential map, the similarity function s must satisfy \u2211 k sk(xi) > 0, \u2200xi \u2208 X . This family of differential maps is very general and can be applied to any X space\nwhere a non-negative differentiable similarity function s can be defined. The finite discrete distribution representation, pi(\u03be), of learning instance, xi, can be intuitively seen as an encoding of its neighborhood structure defined by the similarity function s. Note that, the idea of mapping instances onto the statistical manifold P has been previously studied in manifold learning, e.g. SNE (Hinton & Roweis, 2002) and t-SNE (Van der Maaten & Hinton, 2008).\nAkin to the appropriate choice of the kernel function in a kernel-based method, the choice of an appropriate similarity function s is also crucial for SBFIML. In principle, an appropriate similarity function s should be a good match for the geometrical structure of the X data manifold. For example, for data lying on the probability simplex space, i.e. X = Pd\u22121, the similarity functions defined either on Rd or on Pd\u22121 can be used. However, the similarity function on Pd\u22121 is more appropriate, because it exploits the geometrical structure of Pd\u22121, which, in contrast, is ignored by the similarity function on Rd (Kedem et al., 2012).\nThe set of anchor points {z1, . . . ,zn} can be defined in various ways. Ideally, anchor points should be similar to the given learning instances xi, i.e. anchor points follow the same distribution as that of learning instances. Empirically, we can use directly training instances or cluster centers, the latter established by clustering algorithms. Similar to the current practice in kernel methods we will use in SBFIML as anchors points all the training instances.\nSimilarity Functions on Rd. We can define the similarity on Rd in various ways. In this paper we will investigate two types of differentiable similarity functions. The first one is based on the Gaussian function, defined as:\nsk(xi) = exp(\u2212 \u2016xi \u2212 zk\u201622\n\u03c3k ) (7)\nwhere \u2016 \u00b7 \u20162 is the L2 norm. \u03c3k controls the size of the neighborhood of the anchor point zk, with large values producing large neighborhoods. Note that the different \u03c3ks could be set to different values; if all of them are equal, this similarity function is exactly the Gaussian kernel. The second type of similarity function that we will look at is:\nsk(xi) = 1\u2212 1\n\u03c0 arccos( xTi zk \u2016xi\u20162 \u00b7 \u2016zk\u20162 ) (8)\nwhich measures the normalized angular similarity between xi and zk. This similarity function can be explained as we first projecting all points from Rd to the hypersphere and then applying the angular similarity to points on a hypersphere. As a result, this similarity function is useful for data which approximately lie on a hypersphere. Note that this similarity function is also a valid kernel function (Honeine & Richard, 2010).\nOne might say we can also achieve nonlinearity by mapping instances into the proximity space Q using the following similarity-based map g : X \u2212\u2192 Q:\ng(x) = (s1(x), . . . , sn(x)) (9)\nWe now compare our similarity-based map f , equation 6 against the similarity-based map g, equation 9, in two aspects, namely representation robustness and pullback metric analysis.\nRepresentation Robustness. Compared to the representation induced by the similarity-based map g, equation 9, our representation induced by the similarity-based map f , equation 6, is more robust to density variations in original data space, i.e. the density of the learning instances varies significantly between different regions. This can be explained by the fact that the finite discrete distribution is essentially a representation of the neighborhood structure of a learning instance normalized by a \u201dscaling\u201d factor, the sum of similarities of the learning instance to the anchor points. Hence the distance implied by the finite discrete distribution representation is less sensitive to the density variations of the different data regions. This is an important property. Without such robustness, an objective function based on raw distances can be easily biased towards data regions the density of which is low and thus dominates learning of the objective function. One example of this kind of objective is that of LMNN (Weinberger & Saul, 2009), which we will also use later in SBFIML to learn the Fisher information distance.\nPullback Metric Analysis. We also show how the two approaches differ by comparing the pullback metrics induced by the two similarity-based maps f and g. In doing so, we first need to specify the Riemannian metrics GQ in the proximity space Q and GP on the statistical manifold Pn\u22121. Following the work of similarity-based learning (Chen et al., 2009), we use the Euclidean metric as the GQ in the proximity space Q. On the statistical manifold Pn\u22121 we use the Fisher information metric GFIM defined in equation 3 as GP . To simplify our analysis, we assume X = Rd. However, note that this analysis can be generalized to other manifolds, e.g. Pd\u22121. We use the standard Cartesian coordinate system for points in Rd and Q and use m-affine coordinate system, equation 1, for points on Pn\u22121.\nThe pullback metric induced by these two differential maps are given in the following lemma.\nLemma 3. In Rd, at x with Cartesian coordinate, the form of the pullback metric G\u2217Q(x) of the Euclidean metric induced by the differential map g of equation 9 is:\nG\u2217Q(x) = \u2207g(x)\u2207g(x)T = n\u2211 i=1 \u2207si(x)\u2207si(x)T (10)\nwhere the vector \u2207si(x) of size d \u00d7 1 is the differential of ith similarity function si(x). The form of the pullback metric G\u2217P(x) of the Fisher information metric induced by the differential map f of equation 6 is:\nG\u2217P(x) = n\u2211 i=1 1 s\u0304i(x) (\u2207s\u0304i(x)\u2207s\u0304i(x)T ) (11)\nwhere\u2207s\u0304i(x) = s\u0304i(x) (\u2207 log(si(x))\u2212 E (\u2207 log(si(x)))) and the expectation of\u2207 log(si(x)) is E(\u2207 log(si(x))) =\u2211n k=1 s\u0304k(x)\u2207 log(si(x)) .\nGaussian Similarity Function. The form of pullback metrics G\u2217Q(x) and G \u2217 P(x) depends on the explicit form of the similarity function si(x). We now study their differences using the Gaussian similarity function with kernel width \u03c3, equation 7. We first show the difference between G\u2217Q(x) and G\u2217P(x) by comparing their m largest eigenvectors, the directions in which metrics have the largest distance discrimination.\nThe m largest eigenvectors UQ(x) of G\u2217Q(x) are:\nUQ(x) = arg max UTU=I\ntr(UTG\u2217Q(x)U) (12)\n= arg max UTU=I m\u2211 k=1 n\u2211 i=1 4 \u03c32 (uTk si(x)(x\u2212 zi))2\nwhere tr(\u00b7) is the trace norm and uk is the kth column of matrix U. The m largest eigenvectors UP(x) of the pullback metric G\u2217P(x) are:\nUP(x) = arg max UTU=I\ntr(UTG\u2217P(x)U) (13)\n= arg max UTU=I m\u2211 k=1 n\u2211 i=1 4s\u0304i(x) \u03c32 (uTk (zi \u2212 E(zi))2\nwhere E(zi) = \u2211n k=1 s\u0304k(x)zk\nWe see one key difference between UP(x) and UQ(x). In equation 13, UP(x) are the directions which maximize the sum of expected variance of uTk zi, k \u2208 {1, . . . ,m}, with respected to its expected mean. In contrast, the directions of UQ(x) in equation 12 maximize the sum of the unweighted \u201dvariance\u201d of uTk si(x)(x\u2212zi), k \u2208 {1, . . . ,m},\nwithout centralization. Their difference can be intuitively compared to the difference of doing local PCA with or without centralization. Therefore, UP(x) is closer to the principle directions of local anchor points. Second, since G\u2217P(x) = \u2211n i=1 4s\u0304i(x) \u03c32 (zi \u2212 E(zi))(zi \u2212 E(zi))\nT , it is also easy to show that G\u2217P(x) has no distance in the orthogonal directions of the affine subspace spanned by the weighted anchor points of s\u0304i(x)zi. So, G\u2217P(x) removes the effect of locally irrelevant dimensions to the anchor point manifold.\nTo show the differences of pullback metrics G\u2217Q(x) and G\u2217P(x) intuitively, we visualize their equi-distance curves in Figure 1, where the Guassian similarity function, euqation 7, is used to define the similarity maps in equations 9 and 6. As shown in Figure 1, we see that the pullback metric G\u2217P(x) emphasizes more the distance along the principle direction of the local anchor points than the pullback metric G\u2217Q(x). Furthermore, in Figure 1(b) we see that G\u2217P(x) has a zero distance in the direction being orthogonal to the manifold of anchor points, the straight line which the (green) anchor points lie on. Therefore, G\u2217P(x) is more discriminative on the manifold of the anchor points. To explore the effect of these differences, we also experimentally compare these two approaches in section 4 and the results show that learning the Fisher information distance on P outperforms in a significant manner learning Mahalanobis distance in proximity space Q."}, {"heading": "3.2. Large Margin Fisher Information Metric Learning", "text": "By applying on the learning instances the differential map f of equation (6) we map them on the statistical manifold Pn\u22121. We are now ready to learn the Fisher information distance from the data.\nDistance Parametrization. As discussed in section 2, the Fisher information distance on Pn\u22121 can be exactly computed by the cosine distance (Lebanon, 2006; Lee et al., 2007):\ndFIM (p i,pj) = 2 arccos( \u221a pi T\u221a pj) (14)\nwhere pi is the probability mass vector of the finite discrete\ndistribution pi(\u03be). To parametrize the Fisher information distance, we apply on the probability mass vector pi a linear transformation L. The intuition is that, the effect of the optimal linear transformation L is equivalent to locating a set of hidden anchor points such that the data\u2019s similarity representation is the same as the transformed representation. Thus the parametric Fisher information distance is defined as:\ndFIM (Lp i,Lpj) = 2 arccos( \u221a Lpi T\u221a Lpj)(15)\ns.t. L \u2265 0, \u2211 i Lij = 1,\u2200j\nL has size k \u00d7 n. k is the number of hidden anchor points. To speedup the learning process, in practice we often learn a low rank linear transformation matrix L with small k. The constraints L \u2265 0 and \u2211 i Lij = 1,\u2200j are added to ensure that each Lpi is still a finite discrete distribution on the manifold Pk\u22121.\nLearning. We will follow the large margin metric learning approach of (Weinberger & Saul, 2009) and define the optimization problem of learning L as:\nmin L\n\u2211 ijk\u2208C(i,j,k) [ ijk]+ + \u03b1 \u2211 i,j\u2192i dFIM (Lp i,Lpj)(16)\ns.t. L \u2265 0\u2211 i Lij = 1; \u2200j\nijk = dFIM (Lp i,Lpj) + \u03b3 \u2212 dFIM (Lpi,Lpk)\nwhere \u03b1 is a parameter that balances the importance of the two terms. Unlike LMNN (Weinberger & Saul, 2009), the margin parameter \u03b3 is added in the large margin triplet constraints following the work of (Kedem et al., 2012), since the cosine distance is not linear with LTL. The large margin triplet constraints C(i, j, k) for each instance xi are generated using its k1 same-class nearest neighbors and its k2 different-class nearest neighbors in the X space and constraining the distance of each instance to its k2 different class neighbors to be larger than those to its k1 same class neighbors with \u03b3 margin. In the objective function of (16) the matrix L is learned by minimizing the sum of the hinge losses and the sum of the pairwise distances of each instance to its k1 same-class nearest neighbors.\nOptimization. Since the cosine distance defined in equation (14) is not convex, the optimization problem (16) is not convex. However, the constraints on matrix L are linear and we can solve this problem using a projected subgradient method. At each iteration, the main computation is the sub-gradient computation with complexity O(mnk), wherem is the number of large margin triplet constraints. n and k are the dimensions of the L matrix. The simplex projection operator on matrix L can be efficiently computed\nwith complexity O(nk log(k)) (Duchi et al., 2008). Note that, learning distance metric on P has been previously studied by Riemannian Metric Learning (RML) (Lebanon, 2006) and \u03c72-LMNN (Kedem et al., 2012). In \u03c72-LMNN, a symmetric \u03c72 distance on P is learned with large margin idea similar to problem 16. SBFIML differs from \u03c72LMNN in that it uses the cosine distance to measure the distance onP . As described in section 2, the cosine distance is exactly equivalent to the Fisher information distance on P , while the \u03c72 distance is only an approximation. In contrast to SBFIML and \u03c72-LMNN, the work of RML focuses on unsupervised Fisher information metric learning. More importantly, both RML and \u03c72-LMNN can only be applied in problems in which the input data lie on P , while SBFIML can be applied to general data manifolds via the similaritybased differential map. Finally, note that SBFIML can also be applied to problems where we only have access to the pairwise instance similarity matrix, since it needs only the probability mass of finite discrete distributions as its input.\nLocal Metric Learning View of SBFIML. SBFIML can also be interpreted as a local metric learning algorithm. SBFIML defines the local metric on X as the pullback metric of the Fisher information metric induced by the following similarity-based parametric differential map fL : X \u2212\u2192 Pk\u22121:\nfL(xi) = L \u00b7 pi, s.t. L > 0, \u2211 i Lij = 1,\u2200j (17)\nwhere as before pi is the probability mass vector of the finite discrete distribution pi(\u03be) defined in equation (6). SBFIML learns the local metric by learning the parameters of fL. The explicit form of the pullback metric G\u2217 can be computed according to the equation (5). Given the pullback metric we can approximate the geodesic distance on X by assuming that the geodesic curves are formed by straight lines as local metric learning methods (Noh et al., 2009; Wang et al., 2012) do, which would result in a nonmetric distance. However, Lemma 2 allows us to approximate the geodesic distance on X by the Fisher information distance on Pk\u22121. SBFIML follows the latter approach. Compared to the non-metric distance approximation, this new distance is a well defined distance function which has a closed form expression. Furthermore, this new distance approximation has the same asymptotic convergence result as the non-metric distance approximation."}, {"heading": "4. Experiments", "text": "We will evaluate the performance of SBFIML on ten datasets from the UCI Machine Learning and mldata1 repositories. The details of these datasets are reported in the first column of Table 1. All datasets are preprocessed by standardizing the input features. We compare\n1http://mldata.org/.\nSBFIML against three metric learning baseline methods: LMNN (Weinberger & Saul, 2009)2, KML (Wang et al., 2011)3, GLML (Noh et al., 2009), and PLML (Wang et al., 2012). The former two learn a global Mahalanobis metric in the input feature space Rd and the RKHS space respectively, and the last two learn smooth local metrics in Rd. In addition, we also compare SBFIML against Similaritybased Mahalanobis Metric Learning (SBMML) to see the difference of pullback metrics G\u2217Q(x), equation 10, and G\u2217P(x), equation 11. SBMML learns a global Mahalanobis metric in the proximity space Q. Similar to SBFIML, the metric is learned by optimizing the problem 16, in which the cosine distance is replaced by Mahalanobis distance. The constraints on L in problem 16 are also removed. To see the difference between the cosine distance used in SBFIML and the \u03c72 distance used in \u03c72 LMNN, we compare SBFIML against \u03c72 LMNN. Note that, both methods solve exactly the same optimization problem 16 but with different distance computations. Finally, we also compare SBFIML against SVM for binary classification problems and against multi-class SVMs for multiclass classification problems. In multi-class SVMs, we use the oneagainst-all strategy to determine the class label.\nKML, SBMML and \u03c72 LMNN learn a n \u00d7 n PSD matrix and are thus computationally expensive for datasets with large number of instances. To speedup the learning process, similar to SBFIML, we can learn a low rank transformation matrix L of size k \u00d7 n. For all methods, KML, SBMML, \u03c72 LMNN and SBFMIL, we set k = 0.1n in all experiments. The matrix L in KML and SBMML was initialized by clipping the n\u00d7n identity matrix into the size of k\u00d7n. In a similar manner, in \u03c72 LMNN and SBFIML the matrix L was initialized by applying on the initialization matrix L in KML a simplex projector which ensures the constraints in problem (16) are satisfied.\nThe LMNN has one hyper-parameter \u00b5 (Weinberger &\n2http://www.cse.wustl.edu/\u223ckilian/code/code.html. 3http://cui.unige.ch/\u223cwangjun/.\nSaul, 2009). We set it to its default value \u00b5 = 1. As in (Noh et al., 2009), GLML uses the Gaussian distribution to model the learning instances of a given class. The hyper-parameters of PLML was set following (Wang et al., 2012). The SBFIML has two hyper-parameters \u03b1 and \u03b3. Following LMNN (Weinberger & Saul, 2009), we set the \u03b1 parameter to 1. We select the margin parameter \u03b3 from {0.0001, 0.001, 0.01, 0.1} using a 4-fold inner Cross Validation (CV). The selection of an appropriate similarity function is crucial for SBFIML. We choose the similarity function with a 4-fold inner CV from the angular similarity, equation (8), and the Gaussian similarity in equation (7). We examine two types of Gaussian similarity. In the first we set all \u03c3k to \u03c3 which is selected from {0.5\u03c4, \u03c4, 2\u03c4}, \u03c4 was set to the average of all pairwise distances. In the second we set the \u03c3k for each anchor point zk separately; the \u03c3k was set by making the entropy of the conditional distribution p(xi|zk) = sk(xi)\u2211n\ni=1 sk(xi) equal to log(nc) (Hinton &\nRoweis, 2002), where n is the number of training instances and c was selected from {0.8, 0.9, 0.95}.\nSince \u03c72 LMNN and SBFIML apply different distance parametrizations to solve the same optimization problem, the parameters of \u03c72 LMNN are set in exactly the same way as SBFIML, except that the margin parameter \u03b3 of \u03c72 LMNN was selected from {10\u22128, 10\u22126, 10\u22124, 10\u22122}, because \u03c72 LMNN uses the squared \u03c72 distance (Kedem et al., 2012). The best similarity map for \u03c72 LMNN is also selected using a 4-fold inner CV from the same similarity function set as that of SBFIML.\nAkin to SBFIML, the performance of KML and SVM depends heavily on the selection of the kernel. We select automatically the best kernel with a 4-fold inner CV. The kernels are chosen from the linear, the set of polynomial (degree 2,3 and 4), the angular similarity, equation (8), and the Gaussian kernels with widths {0.5\u03c4, \u03c4, 2\u03c4}, as in SBFIML \u03c4 was set to the average of all pairwise distances. In addition, we also select the margin parameter \u03b3 of KML from {0.01, 0.1, 1, 10, 100}. The C parameter of SVM was se-\nlected from {0.01, 0.1, 1, 10, 100}. SBMML does not have any constraints on the similarity function, thus we select its similarity function with a 4-fold inner CV from a set which includes all kernel and similarity functions used in SBFIML and KML. As in KML, we select the margin parameter \u03b3 of SBMML from {0.01, 0.1, 1, 10, 100}. For all methods, except GLML and SVM which do not involve triplet constraints, the triplet constraints are constructed using three same-class and ten different-class nearest neighbors for each learning instance. Finally, we use the 1- NN rule to evaluate the performance of the different metric learning methods.\nTo estimate the classification accuracy we used 5 times 10- fold CV. The statistical significance of the differences were tested using Student\u2019s t-test with a p-value of 0.05. In order to get a better understanding of the relative performance of the different algorithms for a given dataset we used a simple ranking schema in which an algorithm A was assigned one point if it was found to have a statistically significantly better accuracy than another algorithm B, 0.5 points if the two algorithms did not have a significant difference, and zero points if A was found to be significantly worse than B.\nResults. In Table 1 we report the accuracy results. We see that SBFIML outperforms in a statistical significant manner the single metric learning method LMNN and the local metric learning methods, GLML and PLML, in seven, eight and six out of ten datasets respectively. When we compare it to KML and SBMML, which learn a Mahalanobis metric in the RKHS and proximity space, respectively, we see that it is significantly better than KML and SBMML in four datasets and significantly worse in one dataset. Compared to \u03c72 LMNN, SBFIML outperforms \u03c72-LMNN on eight datasets, being statistically significant better on three, and it never loses in statistical significant manner. Finally, compared to SVM, we see that SBFIML is significantly better in two datasets and significantly worse in one dataset. In terms of the total score, SBFIML achieves the best predictive performance with 50 point, followed by SVM ,which scores 46.5 point, and \u03c72-LMNN with 42 point. The local metric learning method GLML is the one that performs the worst. A potential explanation for the poor performance of GLML could be that its Gaussian distribution assumption is not that appropriate for the datasets we experimented with.\nTo provide a better understanding of the predictive per-\nformance difference between SBFIML, SBMML, and \u03c72 LMNN, we applied them on five large datasets. To speedup the learning process, we use as anchor points 20% of randomly selected training instances. Moreover, the parameter k of low rank transformation matrix L was reduced to k = 0.05n, where n is the number of anchor points. The kernel function and similarity map was selected using 4-fold inner CV. The classification accuracy of Isolet and Pendigits are estimated by the default train and test split, for other three datasets we used 10-fold cross-validation. The statistical significance of difference were tested with McNemar\u2019s test with p-value of 0.05.\nThe accuracy results are reported in Table 2. We see that SBFIML achieves statistical significant better accuracy than SBMML on the two datasets, Splice and Pendigits. When compare it to \u03c72 LMNN, we see it is statistical significant better on one dataset, Pendigits. In terms of total score, SBFIML achieves the best score, 6.5 points, followed by \u03c72 LMNN."}, {"heading": "5. Conclusion", "text": "In this paper we present a two-stage metric learning algorithm SBFIML. It first maps learning instances onto a statistical manifold via a similarity-based differential map and then defines the distance in the input data space by the Fisher information distance on the statistical manifold. This induces a new family of distance metrics in the input data space with two important properties. First, the induced metrics are robust to density variations in the original data space and second they have largest distance discrimination on the manifold of the anchor points. Furthermore, by learning a metric on the statistical manifold SBFIML can learn distances on different types of input feature spaces. The similarity-based map used in SBFIML is natural and flexible; unlike KML it does not need to be PSD. In addition SBFIML can be interpreted as a local metric learning method with a well defined distance approximation. The experimental results show that it outperforms in a statistical significant manner both metric learning methods and SVM."}, {"heading": "Acknowledgments", "text": "Jun Wang was partially funded by the Swiss NSF (Grant 200021-122283/1). Ke Sun is partially supported by the Swiss State Secretariat for Education, Research and Innovation (SER grant number C11.0043). Fei Sha is supported by DARPA Award #D11AP00278 and ARO Award #W911NF-12-1-0241\nAppendix Proof of Lemma 2.\nProof. Let \u03b8 be the coordinate of p \u2208 U \u2282 Mn under some smooth coordinate map \u03d5U and \u03b3 be the coordinate of f(p) \u2208 U \u2032 \u2282 Nm under some smooth coordinate map \u03d5U \u2032 . Since p\u2032 approaches p, we have \u03b8\u2032 = \u03b8 + d\u03b8, where \u03b8\u2032 is the coordinate of p\u2032 under the coordinate map \u03d5U and d\u03b8 is an infinitesimal small change approaching 0. Furthermore, since f : Mn \u2212\u2192 Nm is a differential map, the function \u03d5U \u2032 \u25e6 f \u25e6 \u03d5\u22121U : \u0398 \u2212\u2192 \u0393, which we will denote by g, is also differentiable. According to the Taylor expansion, we have \u03b3\u2032 = g(\u03b8\u2032) = g(\u03b8 + d\u03b8) = g(\u03b8) + 5g(\u03b8)d\u03b8 + Rg(d\u03b8,\u03b8) = \u03b3 + Jd\u03b8 + Rg(d\u03b8,\u03b8), where \u03b3\u2032 is the coordinate of f(p\u2032) under the coordinate map \u03d5U \u2032 , J is the Jacobian matrix of the function g at point \u03b8 and Rg(d\u03b8,\u03b8) is the remainder term of linear approximation. Finally, according to the definition of pullback metric, we have limd\u03b8\u21920 dG(\u03b3)(\u03b3 \u2032,\u03b3)\ndG\u2217(\u03b8)(\u03b8 \u2032,\u03b8) =\nlimd\u03b8\u21920 (Jd\u03b8+Rg(d\u03b8,\u03b8))\nTG(\u03b3)(Jd\u03b8+Rg(d\u03b8,\u03b8))\nd\u03b8TJTG(\u03b3)Jd\u03b8 = 1. This\nends the proof."}], "references": [{"title": "Information geometry of divergence functions", "author": ["S. Amari", "A. Cichocki"], "venue": "Bulletin of the Polish Academy of Sciences: Technical Sciences,", "citeRegEx": "Amari and Cichocki,? \\Q2010\\E", "shortCiteRegEx": "Amari and Cichocki", "year": 2010}, {"title": "Methods of information geometry, volume 191", "author": ["S. Amari", "H. Nagaoka"], "venue": "Amer Mathematical Society,", "citeRegEx": "Amari and Nagaoka,? \\Q2007\\E", "shortCiteRegEx": "Amari and Nagaoka", "year": 2007}, {"title": "Training svm with indefinite kernels", "author": ["Chen", "Jianhui", "Ye", "Jieping"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Chen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2008}, {"title": "Similarity-based classification: Concepts and algorithms", "author": ["Chen", "Yihua", "Garcia", "Eric K", "Gupta", "Maya R", "Rahimi", "Ali", "Cazzanti", "Luca"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "Ground metric learning", "author": ["M. Cuturi", "D. Avis"], "venue": "arXiv preprint arXiv:1110.2306,", "citeRegEx": "Cuturi and Avis,? \\Q2011\\E", "shortCiteRegEx": "Cuturi and Avis", "year": 2011}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "In ICML,", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "Efficient projections onto the l 1-ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "In ICML,", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "A geometric take on metric learning", "author": ["Hauberg", "Sren", "Freifeld", "Oren", "Black", "Michael"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hauberg et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hauberg et al\\.", "year": 2012}, {"title": "Stochastic neighbor embedding", "author": ["G. Hinton", "S. Roweis"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Hinton and Roweis,? \\Q2002\\E", "shortCiteRegEx": "Hinton and Roweis", "year": 2002}, {"title": "The angular kernel in machine learning for hyperspectral data classification", "author": ["P. Honeine", "C. Richard"], "venue": "In Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS),", "citeRegEx": "Honeine and Richard,? \\Q2010\\E", "shortCiteRegEx": "Honeine and Richard", "year": 2010}, {"title": "Inductive regularized learning of kernel functions", "author": ["P. Jain", "B. Kulis", "I. Dhillon"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Jain et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2010}, {"title": "Geometrical foundations of asymptotic inference, volume 908", "author": ["R.E. Kass", "P.W. Vos"], "venue": null, "citeRegEx": "Kass and Vos,? \\Q2011\\E", "shortCiteRegEx": "Kass and Vos", "year": 2011}, {"title": "Non-linear metric learning", "author": ["Kedem", "Dor", "Tyree", "Stephen", "Weinberger", "Kilian", "Sha", "Fei", "Lanckriet", "Gert"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Kedem et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kedem et al\\.", "year": 2012}, {"title": "Metric learning for text documents", "author": ["G. Lebanon"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Lebanon,? \\Q2006\\E", "shortCiteRegEx": "Lebanon", "year": 2006}, {"title": "Introduction to smooth manifolds, volume 218", "author": ["J.M. Lee"], "venue": null, "citeRegEx": "Lee,? \\Q2002\\E", "shortCiteRegEx": "Lee", "year": 2002}, {"title": "Dimensionality reduction and clustering on statistical manifolds", "author": ["S.M. Lee", "A.L. Abbott", "P.A. Araman"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Lee et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2007}, {"title": "Generative local metric learning for nearest neighbor classification", "author": ["Y.K. Noh", "B.T. Zhang", "D.D. Lee"], "venue": null, "citeRegEx": "Noh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Noh et al\\.", "year": 2009}, {"title": "Information and accuracy attainable in the estimation of statistical parameters", "author": ["C. Radhakrishna Rao"], "venue": "Bulletin of the Calcutta Mathematical Society,", "citeRegEx": "Rao,? \\Q1945\\E", "shortCiteRegEx": "Rao", "year": 1945}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Laurens", "Hinton", "Geoffrey"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Metric learning with multiple kernels", "author": ["J. Wang", "H. Do", "A. Woznica", "A. Kalousis"], "venue": "Advances in Neural Information Processing Systems. MIT Press,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Weinberger and Saul,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul", "year": 2009}, {"title": "Analysis of svm with indefinite kernels", "author": ["Y. Ying", "C. Campbell", "M. Girolami"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Ying et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ying et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 10, "context": "Euclidean, cannot address in a satisfactory manner the multitude of learning problems, a fact that led to the development of metric learning methods which learn problemspecific distance measure directly from the data (Weinberger & Saul, 2009; Wang et al., 2012; Jain et al., 2010).", "startOffset": 217, "endOffset": 280}, {"referenceID": 5, "context": "Learning the distance metric with one global linear transformation is called single metric learning (Weinberger & Saul, 2009; Davis et al., 2007).", "startOffset": 100, "endOffset": 145}, {"referenceID": 16, "context": "Local metric learning addresses this limitation by learning in each neighborhood one local metric (Noh et al., 2009; Wang et al., 2012).", "startOffset": 98, "endOffset": 135}, {"referenceID": 7, "context": "When the local metrics vary smoothly in the feature space, learning local metrics is equivalent to learning the Riemannian metric on the data manifold (Hauberg et al., 2012).", "startOffset": 151, "endOffset": 173}, {"referenceID": 16, "context": "In practice, it is approximated by assuming that the geodesic curves are formed by straight lines and the local metric does not change along these lines (Noh et al., 2009; Wang et al., 2012).", "startOffset": 153, "endOffset": 190}, {"referenceID": 10, "context": "Kernelized Metric Learning (KML) achieves flexibility in a different way (Jain et al., 2010; Wang et al., 2011).", "startOffset": 73, "endOffset": 111}, {"referenceID": 19, "context": "Kernelized Metric Learning (KML) achieves flexibility in a different way (Jain et al., 2010; Wang et al., 2011).", "startOffset": 73, "endOffset": 111}, {"referenceID": 21, "context": "Although Non-PSD kernel could be transformed into PSD kernel (Chen & Ye, 2008; Ying et al., 2009), the new PSD kernel nevertheless cannot keep all original similarity information.", "startOffset": 61, "endOffset": 97}, {"referenceID": 16, "context": "Compared to the previous local metric learning algorithms which produce a non-metric distance (Noh et al., 2009; Wang et al., 2012), the distance approximation in SBFIML is a well defined distance function with a closed form expression.", "startOffset": 94, "endOffset": 131}, {"referenceID": 13, "context": "The probability simplex space Pd\u22121 has also been explored (Lebanon, 2006; Cuturi & Avis, 2011; Kedem et al., 2012).", "startOffset": 58, "endOffset": 114}, {"referenceID": 12, "context": "The probability simplex space Pd\u22121 has also been explored (Lebanon, 2006; Cuturi & Avis, 2011; Kedem et al., 2012).", "startOffset": 58, "endOffset": 114}, {"referenceID": 14, "context": "More details can be found in the monographs (Lee, 2002; Amari & Nagaoka, 2007).", "startOffset": 44, "endOffset": 78}, {"referenceID": 13, "context": "P, the cosine distance is exactly equivalent to the Fisher information distance (Lebanon, 2006; Lee et al., 2007).", "startOffset": 80, "endOffset": 113}, {"referenceID": 15, "context": "P, the cosine distance is exactly equivalent to the Fisher information distance (Lebanon, 2006; Lee et al., 2007).", "startOffset": 80, "endOffset": 113}, {"referenceID": 16, "context": "In addition to approximating dG\u2217(p, p) directly onM by assuming that the geodesic curve is formed by straight lines as previous local metric learning algorithms do (Noh et al., 2009; Wang et al., 2012), Lemma 2 allows us to also approximate it with dG(f(p), f(p)) on N.", "startOffset": 164, "endOffset": 201}, {"referenceID": 12, "context": "However, the similarity function on Pd\u22121 is more appropriate, because it exploits the geometrical structure of Pd\u22121, which, in contrast, is ignored by the similarity function on R (Kedem et al., 2012).", "startOffset": 180, "endOffset": 200}, {"referenceID": 3, "context": "Following the work of similarity-based learning (Chen et al., 2009), we use the Euclidean metric as the GQ in the proximity space Q.", "startOffset": 48, "endOffset": 67}, {"referenceID": 13, "context": "As discussed in section 2, the Fisher information distance on Pn\u22121 can be exactly computed by the cosine distance (Lebanon, 2006; Lee et al., 2007):", "startOffset": 114, "endOffset": 147}, {"referenceID": 15, "context": "As discussed in section 2, the Fisher information distance on Pn\u22121 can be exactly computed by the cosine distance (Lebanon, 2006; Lee et al., 2007):", "startOffset": 114, "endOffset": 147}, {"referenceID": 12, "context": "Unlike LMNN (Weinberger & Saul, 2009), the margin parameter \u03b3 is added in the large margin triplet constraints following the work of (Kedem et al., 2012), since the cosine distance is not linear with LL.", "startOffset": 133, "endOffset": 153}, {"referenceID": 6, "context": "The simplex projection operator on matrix L can be efficiently computed with complexity O(nk log(k)) (Duchi et al., 2008).", "startOffset": 101, "endOffset": 121}, {"referenceID": 13, "context": "Note that, learning distance metric on P has been previously studied by Riemannian Metric Learning (RML) (Lebanon, 2006) and \u03c7-LMNN (Kedem et al.", "startOffset": 105, "endOffset": 120}, {"referenceID": 12, "context": "Note that, learning distance metric on P has been previously studied by Riemannian Metric Learning (RML) (Lebanon, 2006) and \u03c7-LMNN (Kedem et al., 2012).", "startOffset": 132, "endOffset": 152}, {"referenceID": 16, "context": "Given the pullback metric we can approximate the geodesic distance on X by assuming that the geodesic curves are formed by straight lines as local metric learning methods (Noh et al., 2009; Wang et al., 2012) do, which would result in a nonmetric distance.", "startOffset": 171, "endOffset": 208}, {"referenceID": 19, "context": "SBFIML against three metric learning baseline methods: LMNN (Weinberger & Saul, 2009)2, KML (Wang et al., 2011)3, GLML (Noh et al.", "startOffset": 92, "endOffset": 111}, {"referenceID": 16, "context": ", 2011)3, GLML (Noh et al., 2009), and PLML (Wang et al.", "startOffset": 15, "endOffset": 33}, {"referenceID": 16, "context": "As in (Noh et al., 2009), GLML uses the Gaussian distribution to model the learning instances of a given class.", "startOffset": 6, "endOffset": 24}, {"referenceID": 12, "context": "Since \u03c7 LMNN and SBFIML apply different distance parametrizations to solve the same optimization problem, the parameters of \u03c7 LMNN are set in exactly the same way as SBFIML, except that the margin parameter \u03b3 of \u03c7 LMNN was selected from {10\u22128, 10\u22126, 10\u22124, 10\u22122}, because \u03c7 LMNN uses the squared \u03c7 distance (Kedem et al., 2012).", "startOffset": 306, "endOffset": 326}], "year": 2014, "abstractText": "In this paper, we present a novel two-stage metric learning algorithm. We first map each learning instance to a probability distribution by computing its similarities to a set of fixed anchor points. Then, we define the distance in the input data space as the Fisher information distance on the associated statistical manifold. This induces in the input data space a new family of distance metric with unique properties. Unlike kernelized metric learning, we do not require the similarity measure to be positive semi-definite. Moreover, it can also be interpreted as a local metric learning algorithm with well defined distance approximation. We evaluate its performance on a number of datasets. It outperforms significantly other metric learning methods and SVM.", "creator": "LaTeX with hyperref package"}}}