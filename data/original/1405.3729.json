{"id": "1405.3729", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2014", "title": "Building a Classification Model for Enrollment In Higher Educational Courses using Data Mining Techniques", "abstract": "Data Mining is the process of extracting useful patterns from the huge amount of database and many data mining techniques are used for mining these patterns. Recently, one of the remarkable facts in higher educational institute is the rapid growth data and this educational data is expanding quickly without any advantage to the educational management. The main aim of the management is to refine the education standard; therefore by applying the various data mining techniques on this data one can get valuable information. This research study proposed the \"classification model for the student's enrollment process in higher educational courses using data mining techniques\". Additionally, this study contributes to finding some patterns that are meaningful to management.", "histories": [["v1", "Thu, 15 May 2014 02:53:44 GMT  (3712kb)", "http://arxiv.org/abs/1405.3729v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["priyanka saini"], "accepted": false, "id": "1405.3729"}, "pdf": {"name": "1405.3729.pdf", "metadata": {"source": "CRF", "title": "Building a Classification Model for Enrollment In Higher Educational Courses using Data Mining Techniques", "authors": ["Priyanka Saini", "BANASTHALI VIDYAPITH"], "emails": [], "sections": [{"heading": null, "text": "Classification model for the Student\u2019s Enrollment process in higher educational courses using data mining techniques\nData Mining is the process of extracting useful patterns from the huge amount of database and many data mining techniques are used for mining these patterns. Recently, one of the remarkable facts in higher educational institute is the rapid growth data and this educational data is expanding quickly without any advantage to the educational management. The main aim of the management is to refine the education standard; therefore by applying the various data mining techniques on this data one can get valuable information.\nThis research study proposed the \u201cclassification model for the student\u2019s enrollment process in higher educational courses using data mining techniques\u201d. Additionally, this study contributes to finding some patterns that are meaningful to management.\nTable of Contents\nCertificate 2\nAcknowledgement 3\nAbstract 4\nList of Figures 7\nList of Tables 8\nChapters\n1. Introduction 9\n1.1 Data Mining 9\n1.2 Need of Current Research 12\n1.3 Research Goal 12\n1.4 Research Contribution 13\n1.5 Resear Motivation 13\n1.6 Thesis Organization 14\n2. Related Work 15\n3. Research Methodology 18\n3.1 Classification for MCA student data 18\n3.1.1 Data Used 18\n3.3.2 Used Tool 19\n3.3.4 Flowchart of process 20\n3.3.5 Results 21\n3.3.6 Conclusion 63\n3.3.7 Testing 63\n3.2 Data mining application in advertisement management 67 3.2.1 Data Used 67\n3.2.2 Analysis of Support and Confidence Value 69\n3.2.3 Cosine value Analysis 70\n3.2.4 Apriori Algorithm Implementation 71\n3.2.5 Conclusion 75\n3.3 Classification of student data using data mining technique 76 3.3.1 Data Used 76\n3.3.2 Results 76\n3.3.3 Conclusion and Future Work 77\n4. References 78\nList of Figures\nFigure 1.1: Intersection of technologies yielding to data mining\nFigure 1.2: Data Mining in Education Figure 3.1: WEKA 3.6.9 Interface Figure 3.2: The WEKA .CSV file for MCA data Figure 3.3: The WEKA .CSV file for MCA data after transformation Figure 3.4: Load the final mca.csv file in WEKA Figure 3.5: Apply Attribute Selection in the final mca.csv file Figure 3.6: Choose ID3 Decision Tree Classification Algorithm from classify tab\nFigure 3.7: Start the classification by click on start tab Figure 3.8: The Run information output\nFigure 3.9: The Classifier Model Figure 3.10: Evaluation on Training set\nFigure 3.11: Detailed Accuracy by class Figure 3.12: Cross Tabulation between PG Grade and UG Medium Figure 3.13: Cross Tabulation between PG Grade and University Standard Figure 3.14: Cross Tabulation between PG Grade and UG Stream Figure 3.15: Cross Tabulation between PG Grade and XII Medium Figure 3.16: Final Generated ID3 decision tree for MCA student performance\nFigure 3.17: Selecting a classifier from WEKA explorer Figure 3.18: Open the test set file Figure 3.19: Selecting more options Figure 3.20: Test set prediction with unknown actual classification Figure 3.21: Advertisement method\u2019s links\nFigure 3.22: Analysis chart that shows different advertisement methods (answered given in questionnaires)\nFigure 3.23: Analysis chart that shows different advertisement methods (according to data mining analysis)\nList of Tables Table 3.1 : Correlation between Mathematics Grade in XII & PG Grade Table 3.2: Correlation between UG Grade & PG Grade Table 3.3: Correlation between UG Stream & PG Grade Table 3.4: Correlation between X Per & PG Grade Table 3.5: Correlation between XII Grade & PG Grade Table 3.6: Correlation between UG Med & PG Grade Table 3.7: UG Med * PG Grade Cross tabulation Table 3.8: University Standard * PG Grade Cross Tabulation Table 3.9: XII Medium * PG Grade Cross Tabulation Table 3.10: Different advertisement methods Table 3.11: Different combination of advertisements Table 3.12: Support and confidence analysis of different relations Table 3.13: Cosine analysis of different relations Table 3.14: Advertisement Methods data set Table 3.15: Generated Candidate item set C1 Table 3.16: Generated frequent item sets L1 Table 3.17: Generated Candidate set C2 Table 3.18: Candidate set C2 with Support Count Table 3.19: Generated frequent item sets L2 Table 3.20: Generated Candidate set C3 Table 3.21: Frequent item set L2 Table 3.22: Frequent item sets Table 3.23: Student Attribute Description\nCHAPTER 1 INTRODUCTION\n1.1 Data Mining"}, {"heading": "History of Data Mining", "text": "Data mining was presented in the 1990s.Data mining roots are traced along three family lines: Statistics, Artificial Intelligence (AI), and Machine Learning.\nStatistics founds many technologies such as regression analysis, standard distribution, standard variance, standard deviation, cluster analysis, discriminate analysis. The data mining is built on these technologies and these technologies are helpful to study data and data relationships also. Data mining classification algorithms use statistical method to create decision tree and rules to validate machine learning based model. Artificial Intelligence is based on heuristics and it is used to apply human thought processing to statistical issues. Data Mining adopts ideas from artificial intelligence and pattern recognition fields such as signal processing or visualization techniques. Machine Learning is combination of Statistics and AI. It is a evolution of artificial intelligence, because it mixes Artificial intelligence heuristics with advanced statistical analysis. Figure 1.1 shows intersection of data mining with various related technologies.\nFigure 1.1: Intersection of technologies yielding to data mining"}, {"heading": "Data Mining Definition", "text": "A knowledge discovery process in databases consists of these steps:\nStatistics\nArtificial Intelligence,\nPattern Recognition Data Mining\n Understanding the requirements  Selecting a target data set  Integrating the data set  Data cleaning and preprocessing  Model building  Select data mining algorithms  Evaluating the results  Testing the results  Using this discovered knowledge"}, {"heading": "Working of Data mining", "text": "Data mining is the process of analyzing and summarizing data into useful information. Data mining software is an analytical tool that allows analyzing, categorizing and summarizing the data. Data mining process has five elements that can be summarized as:\n Extraction, transformation, and loading transaction data onto the data warehouse system.\n Storing and managing the data in a database system(multidimensional).\n Providing data access facility to business analysts and professionals.\n Analyzing the data (by application software)\n Present the data in a graph or table."}, {"heading": "Data Mining Techniques", "text": " Classification: Classification is the best & popular approach. Classification is the process of finding a set of models or functions that describe and distinguish data classes or concepts, for the purpose of being able to use the model to predict the class of objects whose class label is unknown, Unlike a classification model, the purpose of prediction model is to determine the future outcome rather than the current behaviour. Its output can be categorical or numeric value. It is a supervised learning because the classes are determined before examining the data.\n Clustering: Clustering is best suitable for finding groups of similar data items. In Clustering, a set of data items is partitioned into a set of classes such that similar characteristics items are grouped together.\n Association analysis: It is used to discover relationships between attributes and items such as the presence of one pattern implies the presence of another pattern. Association Rule is a popular technique for market basket analysis\nbecause all possible combinations of interesting product groupings can be explored .\n Decision Tree: It is a tree structure like a flow-chart, in which the rectangular boxes are called the node. Each node represents a set of records from the original data set. Internal node is a node that has a child and leaf (terminal) node is nodes that don\u2019t have children. Root node is a topmost node. The decision tree is used for finding the best way to distinguish a class from another class. There are five mostly & commonly used algorithms for decision tree: - ID3, CART, CHAID, C4.5 algorithm and J48."}, {"heading": "Data Mining Advantages", "text": "Data mining is so desired because of its many benefits. We have many cheaper techniques to collect and manage the data, but they are a few techniques for extracting useful knowledge from this data. Data mining has various advantages in many fields.\nMarketing and Retailing: Marketers can make policy to fulfill the customer needs and understand their purchasing behaviors with the help of data mining.\nBanking: Financial institutions can get help of data mining in credit and loan information. A credit card issuer can detect fraud credit card transaction.\nResearchers: Using data mining techniques researchers extract the useful knowledge by analyzing their data and precede their research work.\nEducation: Data mining is very useful in educational institutes because there is a large unused collected data and this data can be used in a proper way using data mining.\nFigure 1.2 : Data Mining in Education\n1.2 Need of Current Research\nEducational Systems\nData Mining\nTo design, plan To use, communicate\nAcademicians Students\nTo show discovered knowledge\nTo show Recommendations\nCourse information, Academic data\nThe educational data contains not only grades, exams or enrollment data but also other information. Attributes obtained from this data analysis are considered to be valuable."}, {"heading": "Educational Data Mining", "text": "Educational data mining (EDM) combines data mining and knowledge discovery methods into educational setting.EDM explores the row data from educational environment to useful data that can be used to making decisions and solve the problems."}, {"heading": "A. Data Mining Techniques in Educational Systems", "text": "In the educational systems, data mining techniques can search useful patterns and these useful patterns can be used for many purpose."}, {"heading": "Predicting Student Performance:", "text": "In Educational Systems, using data mining techniques student performance, score values are predicted and these values can be numerical or categorical. Regression analysis finds relationship between a dependent (numerical) and one or more independent variable (numerical).Classification technique is used to classify individual items. Different data mining techniques such as neural network, rule based system, regression analysis, correlation analysis are applied on educational data."}, {"heading": "Grouping Students:", "text": "Clustering and Classification data mining techniques are used to build groups of students based on their characteristics, performance, etc. Different clustering algorithms are Kmean, hierarchical, model-based clustering."}, {"heading": "Enrollment Management:", "text": "Enrollment management is required in educational system to shape the enrollment strategy of an institution and fulfill all goals. It is a set of activities such as marketing, retention program and admission process.\n1.3 Research Goal The goal of this thesis can be split as following.\n Concept: Understand the concept of data analysis and how these methods can be applied on educational data such as classification analysis.\n Method: After the initial research, select a data mining classification method .\n Evaluate: Find a method that are able to correctly classify a student data set of with a success rate of at least 70% . For this requirement decision tree method is used here for building a classification model that provide 69.69% model accuracy\n1.4 Research Contribution This project aims at developing a classification model for Enrollment in Higher Education Courses using data mining techniques. This model helps us to fulfill these objectives:\nClassification Analysis on student data\nHelpful to predict student performance from past academic performance, by which-\n1) We can identify suitable student stream in a particular course and can make a good proper counseling strategy for a particular course.\n2) It can also suggest that which course is applicable for student according to his skills."}, {"heading": "Advertisement Management", "text": "It is useful to discover some meaningful pattern for institute such as select a best advertisement medium using some data mining methods.\nClassification of Student\u2019s data Using Data Mining Techniques for Training & Placement Purpose. 1.5 Research Motivation In India, there is largest no. of educational institutes, so it is second largest in the world after United States. There is more competition between all institutes for attracting students to get enrollment in their institutes so they focus on strength of students not quality of education at the time of enrollment.\nToday Admission process of institutes has become very critical. There are many problems at the time of admission in institutes because many students apply for courses but seats are limited, so there is no proper seat allocation of courses to the students so students are unable to get enroll in their interested courses. Some students have good marks but they get admission in other course (that is not according to their subjects) due to limited seats.\nSo there is a proper attention is needed in admission process. Every year huge amount of student data is recorded in database however this data is not put in proper form. There is a requirement of data mining that handle these challenges & overcome them. Then there is enough information for better planning, evaluation and decision making. Data mining will extract hidden information from student enrollment database, this information will be meaningful for institutes.Then a better & mined knowledge is present in database that can be use directly, there is no extra requirement.\nThe motive behind in this paper is based on classification model for enrollment in higher educational courses using data mining techniques. This is useful for predicting the students that are interested to take admission in higher study course, this is also useful to select a good student for a particular course & also useful to give guidelines to student about selection of course. By this study we will find some meaningful pattern that can be useful for institutes.\n1.6 Thesis Organization The organization of the rest of this thesis is as follows:\n Chapter 2 presents the related work\n Chapter 3 discusses the research methodology that presents results, conclusion and future directions about the work done in the thesis\n Chapter4 discusses the References.\nCHAPTER 2 RELATED WORK\nData mining extract hidden patterns from the student database that provides help to university in education. The educational data mining is a recent and popular area, there are many journals, ongoing books, workshops [1]. Delavari and Beikzadeh [2] give knowledge to use data mining methods in Higher learning institutions and define how data mining can be applied to the educational data. R. R. Kabra and R. S. Bichkar [3] define that a model can be created using student\u2019s past-academic performance with the help of decision tree algorithm and this model can predict student\u2019s performance in the first year of engineering exam. Oladipupo and oyelade [4] show their study in which student\u2019s failure patterns are identified using association rule data mining technique. Their study trims down failure rate and improves academic performance. Zlatko J. Kovacic [5] show a case study for student\u2019s success prediction using educational data mining. For classify successful and unsuccessful students the CHAID and CART algorithm were applied on enrollment data of open polytechnic (New Zealand). Nguyen et al. [6] predict the performance of undergraduate and postgraduate students at two different institutes using decision tree and Bayesian network and this is used to find and helping failing students and determines scholarship. In this result, decision tree gives better accuracy than Bayesian network. Hijazi and Naqvi [7] show a case study on student performance. In this study 300 student\u2019s sample is taken from colleges of Punjab university(Pakistan) and they determine that high correlation is present between some factor (mother\u2019s education factor and student\u2019s family income factor) and student performance. T.Miranda Lakshmi, A.Martin, R.Mumtaj Begum and Dr.V.Prasanna Venkatesan [8] conduct a case study on student\u2019s qualitative data using decision tree algorithms to identify the effect of qualitative data in the performance of the student. Mohammed M. Abu Tair, Alaa M. El-Halees [9] conduct a case study on graduate students\u2019 data using data mining techniques to improve performance and extract useful knowledge from this data. Sunita B.Aher, L.M.R.J. lobo [10] conduct a comparative study to predict course selection using association rule algorithms.\nUmesh Kumar Pandey and S. Pal [11] administrate a study in which 600 student\u2019s data sample is taken from Dr. M.L. Awadh University\u2019s colleges ,Faizabad and found that whether new student will perform or not. Bhise R. B, Thorat S.S and Supekar A.K. [12] performed data mining process on the student\u2019s database using clustering- K-mean algorithm. K.Shanmuga Priya and A.V.Senthil Kumar [13] use a classification method that helps to improve the performance & extract the knowledge from student\u2019s final semester marks. Varun Kumar and Anupama Chadha [14] used association rule technique to improve the performance of postgraduate students. They focus on many factors like student\u2019s interest, teaching methodologies, curriculum design using association rule mining and these factors can affect post graduate student\u2019s performance. Abeer Badr El Din Ahmed and Ibrahim Sayed Elaraby [15] use decision tree technique for data classification that is helpful for predicting the student\u2019s final grade. In 1986 J.R Quinlan summarizes an approach and describes ID3 and this was the first research work on ID3 algorithm [16].Anand Bahety implemented the ID3 algorithm on the \u201cPlay Tennis\u201d database and classified whether the weather is suitable for playing tennis or not?.Their results concluded that ID3 doesn\u2019t works well in continuous attributes but gives good results for missing values [17]. Mary Slocum gives the implementation of the ID3 in the medical area. She transforms the data into information to make a decision and performed the task of collecting and cleaning the data .Entropy and Information Gain concepts are used in this study [18]. Kumar Ashok (et.al) performed the id3 algorithm classification on the \u201ccensus 2011 of India\u201d data to improving or implementing a policy for right people. The concept of information theory is used here. In the decision tree a property on the basis of calculation is selected as the root of the tree and this process\u2019s steps are repeated [19]. Sonika Tiwari used the ID3 algorithm for detecting Network Anomalies with horizontal portioning based decision tree and applies different clustering algorithms. She checks the network anomalies from the decision tree then she discovers the comparative analysis of different clustering algorithms and existing id3 decision tree [20]. Yadav, Bharadwaj and Pal [21] obtained the students data such as attendance, seminar, assignment marks and class test to predict the end semester performance using three algorithms ID3decision tree, C4.5 and CART and result shows that CART gives better result for classification of data.\nPandey and Pal [22] show their study using association rule analysis to find the student interest of choosing class language. In this paper they use seven different interestingness measures. Their result concluded that student has shown their interest in mix mode class language. Bharadwaj and Pal [23] use the classification decision tree technique to evaluate student\u2019 end semester performance, this study helps to identify the dropouts and students who require special attention and teacher advising. AI-Radaideh. et al [24] presents a classification based model for student performance prediction using ID3 algorithm,C4.5 and Na\u00efve Bayes algorithm but decision tree had better results. K.S. Priya and A.V.S. kumar [25] use a classification approach that extracts the knowledge from student end semester marks.\nData Mining can be used in educational field to enhance our understanding of learning process to focus on extracting and identifying the variables of the learning process of students as described by Alaa el- Halees [26]. Han and Kamber [27] describes data mining software that allow the users to analyze data from different views, and summarize these relationships which are identified during the mining process. Divakar, R.C Jain [28] applied four classification methods on student academic data i.e Decision tree (ID3), Multilayers perceptron, Decision table & Na\u00efve Bayes classification method. Shaeela Ayesha, Tasleem Mustafa, Ahsan Raza Sattar, and M. Inayat Khan [29] applied K-mean clustering to analyze learning behavior of students which will help the tutor to improve the performance of students and reduce the dropout ratio to a significant level.D\u2019Mello [30] studied on bored and frustrated student.\nCHAPTER 3 RESEARCH METHODOLOGY\n3.1 CLASSIFICATION FOR MCA STUDENT DATA In recent years, Indian higher educational institutes grow rapidly. There is more competition between institutes for attracting students to get enrollment in their institutes. The admission process is conducted every year at the institute and it results in the recording of large amounts of data. But, in most of the cases this data is not properly utilized (or analyzed) and results in wastage of what would otherwise be one of the most precious assets of the institutes. By applying the various data mining techniques on this data one can get valuable information and predictions can be done for the betterment of the admission process. This study presents data mining techniques for the enrollment process in MCA stream. These methods will help to improve the overall performance of the admission process at higher educational institutes.\n3.1.1 Data Used (Data Collection & Data Preprocessing)  The MCA student data include: Category, Father Qualification, Mother\nQualification, XII Medium, XII Stream, Mathematics Grade in XII, XII Grade, UG Stream, UG Medium, UG Grade, PG Grade.\n Some attributes are selected using attribute selection measure, these attributes are Mathematics Grade in XII, XII Grade, UG Stream, UG Grade, PG Grade.\n The next step of our application focuses on transforming these data in order to be\nused in Weka, a data mining specialized software. Since ID3 algorithm can only work with categorical variables, we have to adapt the data. With this intention we will make some transformations:\n The Mathematics Grade in XII values will be encoded like: A\u2013 (80-89), B \u2013 (70-\n79), C \u2013 (60-69), D \u2013 (50-59), E \u2013 (40-49), F<=35, not applicable\n The XII Grade values will be normalized, after normalization each ones having attached a Grade A\u2013 (80-89), B \u2013 (70-79), C \u2013 (60-69), D \u2013 (50-59), E \u2013 (40-49)\nAfter Normalization if XII Per <=0 THEN E if XII Per <=0.25 THEN D\nif XII Per <=0.487 THEN C if XII Per <=0.75 THEN B Else A\n The UG Grade values will be normalized, after normalization each ones having\nattached a Grade A\u2013 (80-89), B \u2013 (70-79), C \u2013 (60-69), D \u2013 (50-59), E \u2013 (40-49) After Normalization\nif UG Per <=0 THEN E if UG Per <=0.11 THEN D if UG Per <=0.41 THEN C if UG Per <=0.70 THEN B Else A\n The PG Grade values will be normalized, after normalization each ones having attached a Grade A\u2013 (80-89), B \u2013 (70-79), C \u2013 (60-69), D \u2013 (50-59), E \u2013 (40-49) After Normalization\nif PG Per <=0 THEN E if PG Per <=0.23 THEN D\nif PG Per <=0.46 THEN C if PG Per <=0.74 THEN B Else A\n3.1.2 Used Tool WEKA:-\nFor the purposes of this study, we select WEKA (Waikato Environment for Knowledge Analysis) software that was developed at the University of Waikato in New Zealand. WEKA tool supports to a wider range of algorithms & very large data sets. The Weka (pronounced Waykuh) workbench contains a collection of visualization tools & algorithms. WEKA is open source software issued under the GNU General Public License. It contains tools for data pre-processing, classification, regression, clustering, association rules, and visualization. The original non-java version of Weka was a TCL/TK, but the recent java based version is Weka 3(1997), is now used in many different application areas, in particular for education & research.\nWeka\u2019s main user interface is Explorer. The Experimenter is also there by which we can compare weka\u2019s machine learning algorithms\u2019 performance. The Explorer interface has many panels by which we can access to main components of workbench. The Visualization tab allows visualizing a 2-D plot of the current working relation, it is very useful. In this study WEKA toolkit 3.6.9 is used for constructing the decision tree.\nFigure 3.1: WEKA 3.6.9 Interface"}, {"heading": "SPSS", "text": "SPSS (Statistical Package for the Social Sciences) has now been in development for more than thirty years. Originally developed as a programming language for conducting statistical analysis, it has grown into a complex and powerful application with now uses both a graphical and a syntactical interface and provides dozens of functions for managing, analyzing, and presenting data. SPSS Inc. continues its tradition of regularly enhancing this family of powerful but easy-to-use statistical software products with the release of SPSS 16.0. In this study SPSS 16.0 is used for performing correlation between student\u2019s attributes & final grade (PG Grade).\n3.1.3 Flowchart of process Step 1: Data Collection Step 2: Data preprocessing (attribute selection, transformation, normalization) Step 3: Use the Data mining WEKA tool Step 4: Apply the ID3 decision tree classification technique on data Step 5: Evaluate the Results Step 6: Perform classification on new data using testing Step 7: Evaluate the accuracy of the model\n3.1.4 Results\na) Classification Analysis After this step, in order to generate the decision tree based on the ID3 algorithm in Weka, we must load our data first, followed by the proper selection of the algorithm from the list provided by this software and then press the Start button. The output generated has the next sections: o Run information o Classifier model o Evaluation on training set o Detailed Accuracy By Class\nFigure 3.2: The Weka .csv file for MCA data\nFigure 3.3: The Weka .CSV file for MCA data after the transformation\nFigure 3.4: Load the final mca.csv file in WEKA\nFigure 3.5: Apply Attribute Selection in the final mca.csv file\nFigure 3.7: Start the Classification by click on Start Tab The run information part contains general information about the scheme used, the number of instances (99) and attributes (7) as well as the attributes names as presented in Figure 3.7. === Run information === Scheme:weka.classifiers.trees.Id3 Relation: final mca-weka.filters.supervised.attribute.AttributeSelectionEweka.attributeSelection.CfsSubsetEval-Sweka.attributeSelection.BestFirst -D 1 -N 5 Instances: 99 Attributes: 5 Mathematics Grade in XII XII Grade UGStream UG Grade PG Grade Test mode:10-fold cross-validation\nFigure 3.8: The Run Information output\nThe second part of the output is represented by the ID3 decision tree (Figure 3.8) === Classifier model (full training set) === Id3\nUGStream = BSC(IT): D UGStream = BSC(Math): A UGStream = BSC(CS) | Mathematics Grade in XII = F: D | Mathematics Grade in XII = A: B | Mathematics Grade in XII = B | | XII Grade = C: D | | XII Grade = B: B | | XII Grade = A: B | | XII Grade = D: null | | XII Grade = E: null | Mathematics Grade in XII = C: B | Mathematics Grade in XII = D: null UGStream = BCA | Mathematics Grade in XII = F | | XII Grade = C: C | | XII Grade = B: null | | XII Grade = A | | | UG Grade = B: C | | | UG Grade = C: D | | | UG Grade = A: null | | | UG Grade = D: null | | XII Grade = D: C | | XII Grade = E: C | Mathematics Grade in XII = A: B | Mathematics Grade in XII = B | | UG Grade = B: B | | UG Grade = C: B | | UG Grade = A: C | | UG Grade = D: null | Mathematics Grade in XII = C | | UG Grade = B: B | | UG Grade = C | | | XII Grade = C: C | | | XII Grade = B: null | | | XII Grade = A: null | | | XII Grade = D: D | | | XII Grade = E: null | | UG Grade = A: null | | UG Grade = D: null | Mathematics Grade in XII = D | | XII Grade = C: null | | XII Grade = B: C | | XII Grade = A: null | | XII Grade = D: D\n| | XII Grade = E: null UGStream = BSC | XII Grade = C | | Mathematics Grade in XII = F: C | | Mathematics Grade in XII = A: null | | Mathematics Grade in XII = B: null | | Mathematics Grade in XII = C: D | | Mathematics Grade in XII = D: null | XII Grade = B: C | XII Grade = A: C | XII Grade = D | | Mathematics Grade in XII = F: D | | Mathematics Grade in XII = A: null | | Mathematics Grade in XII = B: null | | Mathematics Grade in XII = C: C | | Mathematics Grade in XII = D: null | XII Grade = E: null UGStream = BSC(Biotech): C UGStream = BSC(PCM): A Time taken to build model: 0 seconds\nFigure 3.9: The Classifier Model Beside this, Weka provides some complementary information about the percent of correctly as well as incorrectly classified instances. In this example, out of a total of 99 instances, only 67 have been correctly classified meaning 67.6768 %. This summary is presented in figure 3.9, along with some important statistical parameters. Kappa statistic:-a measure of agreement between two individuals, with a 0.661 value. Mean absolute error:-a quantity used to measure how close forecasts or predictions are to the eventual outcomes. Root mean squared error:-a good measure of the model\u2019s accuracy. Root relative squared error:-the average of the actual values. Relative absolute error:-similar to the relative squared error. === Stratified cross-validation === === Summary === Correctly Classified Instances 67 67.6768 %\nIncorrectly Classified Instances 22 22.2222 % Kappa statistic 0.661 Mean absolute error 0.1171 Root mean squared error 0.2935 Relative absolute error 35.5222 % Root relative squared error 72.5685 % UnClassified Instances 10 10.101 % Total Number of Instances 99\nFigure 3.10: Evaluation on training set\nThe forth part of the output, presented in Figure 3.10, contains information regarding the detailed accuracy by class. === Detailed Accuracy By Class === TP Rate FP Rate Precision Recall F-Measure ROC Area Class 0.783 0.091 0.75 0.783 0.766 0.867 D 0.794 0.036 0.931 0.794 0.857 0.96 A 0.889 0.141 0.615 0.889 0.727 0.835 B 0.429 0.053 0.6 0.429 0.5 0.665 C 0.753 0.074 0.768 0.753 0.751 0.864\n=== Confusion Matrix === a b c d <-- classified as 18 0 1 4 | a = D 0 27 7 0 | b = A 0 2 16 0 | c = B 6 0 2 6 | d = C Figure 3.11: Detailed Accuracy By Class Accuracy= (Total number of corrected prediction)/Total number of instances=67/99=67.67%\nTP Rate (True positive rate):-is the proportion of positive cases that were correctly identified, as calculated using the equation:TP=Correctly classified instance/Total instance For Class D:- 18/(18+0+1+4)=0.78 For Class A:- 27/(27+7)=0.794 For Class B:- 16/(2+16)=0.888 For Class C:- 6/(6+2+6)=0.428\nWeighted Average\nFP Rate (False positive rate):- is the proportion of negatives cases that were incorrectly classified as positive Precision(P) :- is the proportion of the predicted positive cases that were correct, as calculated using the equation:P= Correctly classified instance/Total predicted instance For Class D:- 18/(18+6)=0.75 For Class A:- 27/(27+2)=0.93 For Class B:- 16/(1+7+16+2)=0.615 For Class C:- 6/(10+6)=0.6 Recall:- TP Rate and Recall are same F-measure:- F-Measure is a measure of a test's accuracy and is determined using the formula:\n(2 * TP Rate * Precision) / (TP Rate + Precision)\nFor Class D:-(2*0.783*.75)/(.783+.75)=0.766 For Class A:-(2*0.794*.931)/(.794+.931)=0.85706 For Class B:-(2*0.889*.615)/(.889+.615)=0.727 For Class C:-(2*0.429*.6)/(.429+.6)=0.5 ROC (Receiver Operating Characteristic Area) \u2013 The ROC curve is given by the TP Rate and FP Rate. The area under the ROC Curve (AUC) is a method of measuring the performance of the ROC curve. If AUC is 1 then the prediction is perfect; if it is 0.5 then the prediction is random. Analyzing our output we conclude that even if the prediction in this case is not perfect, it is not random as well. The best prediction is for class A \u2013 0.96 and the \u201cweak\u201d prediction is 0.665 for class C. Between this extremes are the values 0.867 for class D and 0.835 for class B. Weighted Average:An average in which each quantity to be averaged is assigned a weight. These weightings determine the relative importance of each quantity on the average. For TP Rate:Sum((TP Rate of Class* number of instance with that class))/Sum(number of instances with that class) =((0.783*23)+(.794*34)+(.889*18)+(.429*14))/89=0.7529 For FP Rate:Sum((FP Rate of Class* number of instance with that class))/Sum(number of instances with that class) =((0.091*23)+(.036*34)+(.141*18)+(.053*14))/89=0.0741\nFor Precision:Sum((Precision of Class* number of instance with that class))/Sum(number of instances with that class) =((0.75*23)+(.931*34)+(.615*18)+(.6*14))/89=0.768 For Recall:Sum((Recall of Class* number of instance with that class))/Sum(number of instances with that class) =((0.783*23)+(.794*34)+(.889*18)+(.429*14))/89=0.7529 For F-Measure:Sum((F-Measure of Class* number of instance with that class))/Sum(number of instances with that class) =((0.766*23)+(.857*34)+(.727*18)+(.5*14))/89=0.751 For ROC Area:Sum((ROC Area of Class* number of instance with that class))/Sum(number of instances with that class)=((0.867*23)+(.96*34)+(.835*18)+(.665*14))/89=0.86428 b) Correlation Analysis\nTable 3.1: Correlation between Mathematics Grade in XII & PG Grade Correlations\nMathematics Grade in XII PG Grade\nMathematics Grade in XII Pearson Correlation\n1 .629**\nSig. (2-tailed) .000 N 99 99\nPG Grade Pearson Correlation\n.629** 1\nSig. (2-tailed) .000\nN 99 99 **. Correlation is significant at the 0.01 level (2-tailed).\nMathematics Grade in XII with Mathematics Grade in XII\nMathematics Grade in XII with PG Grade\nPG Grade with Mathematics Grade in XII PG Grade with PG Grade\nIn the matrix, there are four correlations \u201cMathematics Grade in XII with Mathematics Grade in XII\u201d and \u201cPG Grade with PG Grade\u201d will be of course 1 (perfect positive correlation).\nMathematics Grade in XII with PG Grade and PG Grade with Mathematics Grade in XII correlations tell us three things:\n The Pearson correlation is .629**, as there is no minus sign preceding the\ncorrelation coefficient means that the relationship between XII Grade and PG\nGrade is positive. There are two asterisks after the correlation coefficient means\ncorrelation is significant at the 0.01 level(2-tailed).\n The significance level or p-value is .000, low value indicate a low probability of finding a relationship between these variables.\n N is the number of cases that contains data and this is 99.\nTable 3.2: Correlation between UG Grade & PG Grade\nCorrelations UG Grade PG Grade\nUG Grade Pearson Correlation\n1 .498**\nSig. (2-tailed) .000 N 99 99\nPG Grade Pearson Correlation\n.498** 1\nSig. (2-tailed) .000\nN 99 99 **. Correlation is significant at the 0.01 level (2-tailed).\nTable 3.3: Correlation between UG Stream & PG Grade Correlations\nUGStream PG Grade\nUG Stream\nPearson Correlation 1 .443** Sig. (2-tailed) .000 N 99 99\nPG Grade Pearson Correlation .443** 1 Sig. (2-tailed) .000\nN 99 99 **. Correlation is significant at the 0.01 level (2-tailed).\nTable 3.4: Correlation Between X Per & PG Grade\nCorrelations X Per PG Grade\nX Per Pearson Correlation\n1 .290**\nSig. (2-tailed) .004 N 99 99\nPG Grade Pearson Correlation\n.290** 1\nSig. (2-tailed) .004\nN 99 99 **. Correlation is significant at the 0.01 level (2-tailed).\nTable 3.5: Correlation Between XII Grade & PG Grade Correlations\nXII\nGrade PG Grade\nXII Grade Pearson Correlation\n1 .124\nSig. (2-tailed) .222 N 99 99\nPG Grade Pearson Correlation\n.124 1\nSig. (2-tailed) .222\nN 99 99\nXII Grade with PG Grade and PG Grade with XII Grade correlations tell us three\nthings:\n The Pearson correlation is .124, as there is no minus sign preceding the\ncorrelation coefficient means that the relationship between XII Grade and PG\nGrade is positive. In other words, students who score high in XII Grade will tend\nto score high in PG Grade.\n The significance level or p-value is .222.\n N is 99 (the number of cases that contains data).\nTable 3.6: Correlation Between UG Med & PG Grade\nCorrelations UG Med PG Grade\nUG Med Pearson Correlation\n1 .118\nSig. (2-tailed) .246 N 99 99\nPG Grade Pearson Correlation\n.118 1\nSig. (2-tailed) .246\nN 99 99\nc) Cross Tabulation Analysis\nPG Grade UG Medium(English) UG Medium(Hindi) A 27 7 B 18 5 C 18 1 D 20 3\nFigure 3.12: Cross Tabulation between PG Grade & UG Medium\nTable 3.7: UG Med * PG Grade Cross tabulation\nCount\nPG Grade\nTotal A B C D\nUG Med English 27 18 18 20 83\nHindi 7 5 1 3 16 Total 34 23 19 23 99\nMCA PG Grade with University Standard PG Grade University Std\n(Good) University Std (Very Good)\nUniversity Std (Excellent)\nA 10 16 8 B 8 10 5 C 7 8 4 D 13 5 5\nFigure 3.13: Cross Tabulation between PG Grade & University Standard\nTable 3.8: University Standard * PG Grade Cross Tabulation\nCount\nPG Grade\nTotal A B C D\nUniversity Standard Good 10 8 7 13 38\nVery Good 16 10 8 5 39\nExcellent 8 5 4 5 22 Total 34 23 19 23 99\nMCA PG Grade with Under Graduation Stream\nPG Grade University Stream BSC(Math) University Stream BCA\nUniversity Stream BSC\nA 27 7 0 B 10 13 0 C 0 13 6 D 11 5 7\nFigure 3.14: Cross Tabulation between PG Grade & UG Stream\nMCA PG Grade with XII Medium\nPG Grade XII Medium-English XII Medium-Hindi A 16 18 B 16 7 C 8 11 D 14 9\nFigure 3.15: Cross Tabulation between PG Grade & XII Medium\nTable 3.9: XII Medium * PG Grade Cross Tabulation\nCount\nPG Grade\nTotal A B C D\nXII Medium English 16 16 8 14 54\nHindi 18 7 11 9 45 Total 34 23 19 23 99\nd) ID3 Decision Tree Creation\nSteps of ID3 decision tree creation for MCA student performance\nStep 1: the dataset S of 99 instances with grades 34 \u201cA\u201d, 23 \u201cB\u201d, 19 \u201cC\u201d and 23 \u201cD\u201d.\nEntropy (S) = -(34/99) Log2 (34/99) \u2013 (23/99) Log2 (23/99) -(19/99) Log2 (19/99) \u2013 (23/99) Log2 (23/99) = 1.965031\nSTEP 2: Attribute XII Grade\nXII Grade value can be A,B,C,D and E.\nXII Grade=A is of occurrence 17 (3 of the examples are \u201cB\u201d, 3 of the examples are \u201cC\u201d,11 of the examples are \u201cD\u201d\nXII Grade=B is of occurrence 48 ( 28-A, 16-B, 4-C)\nXII Grade=C is of occurrence 23 (6-A, 4-B, 6-C, 7-D)\nXII Grade=D is of occurrence 10 (5-C,5-D)\nXII Grade=E is of occurrence 1 (1-C)\nEntropy(SXA)= -(3/17) x log2(3/17) \u2013 (3/17) x log2(3/17) \u2013(11/17) x log2(11/17) =1.289609\nEntropy(SXB)= -(28/48) x log2(28/48) \u2013 (16/48) x log2(16/48) \u2013(4/48) x log2(4/48) =1.280672\nEntropy(SXC)= -(6/23) x log2(6/23) \u2013(4/23) x log2(4/23)\u2013(6/23) x log2(6/23) \u2013(7/23) x log2(7/23)= 1.972647\nEntropy (SXD) = - (5/10) x log2 (5/10) \u2013 (5/10) x log2 (5/10) =1\nEntropy (SXE) = - (1/1) x log2 (1/1) = 0\nGain (S, XII Grade) = Entropy (S) \u2013 (17/99) x Entropy (SXA) - (48/99) x Entropy (SXB) - (23/99) x Entropy (SXC)-(10/99) x Entropy (SXD) - (1/99) x Entropy (SXE)\nGain(S,XII Grade)= 1.965031-(17/99) x 1.289609- (48/99) x 1.280672- (23/99) x 1.972647 - (10/99) x 1- (1/99) x 0=0.563349283\nSTEP 3: Attribute Mathematic Grade in XII\nIts value can be A,B,C,D,E and F\nMathematics Grade in XII =A is of occurrence 21 (13-A,8-B)\nMathematics Grade in XII =B is of occurrence 29 (14-A, 11-B, 3-C, 1-D)\nMathematics Grade in XII =C is of occurrence 12 (2-A, 4-B, 2-C, 4-D)\nMathematics Grade in XII =D is of occurrence 4 (1-C, 3-D )\nMathematics Grade in XII =E is of occurrence 0\nMathematics Grade in XII =F is of occurrence 33 (5-A, 13-C, 15-D)\nEntropy(SMA)= -(13/21) x log2(13/21) \u2013 (8/21) x log2(8/21)= 0.958711\nEntropy(SMB)= -(14/29) x log2(14/29) \u2013 (11/29) x log2(11/29) -(3/29) x log2(3/29) \u2013 (1/29) x log2(1/29) =1.543787\nEntropy(SMC)= -(2/12) x log2(2/12) \u2013 (4/12) x log2(4/12) -(2/12) x log2(2/12) \u2013 (4/12) x log2(4/12)= 1.918295\nEntropy (SMD)= -(1/4) x log2(1/4) \u2013 (3/4) x log2(3/4) =0.811278\nEntropy(SMF)= -(5/33) x log2(5/33) \u2013 (13/33) x log2(13/33)- (15/33) x log2(15/33)= 1.458978\nGain (S, Mathematics Grade in XII) = Entropy (S) \u2013 (21/99) x Entropy (SMA) - (29/99) x Entropy (SMB) - (12/99) x Entropy (SMC)-(4/99) x Entropy (SMD) - (33/99) x Entropy (SMF)\nGain (S, Mathematics Grade in XII) = 1.965031 \u2013 (21/99) x 0.958711- (29/99) x 1.543787- (12/99) x 1.918295-(4/99) x 0.811278- (33/99) x 1.458978=0.557822111\nSTEP 4: Attribute UG Stream\nIts value can be BSC(Math),BSC(PCM),BSC(IT),BSC(CS),BSC(Biotech),BSC,BCA\nUG Stream =BSC(Math) is of occurrence 23 (23-A)\nUG Stream =BSC(PCM) is of occurrence 4 (4-A)\nUG Stream =BSC(IT) is of occurrence 2 (2-D)\nUG Stream =BSC(CS) is of occurrence 21 (10-B, 11-D)\nUG Stream =BSC(Biotech) is of occurrence 1 (1-C)\nUG Stream =BSC is of occurrence 10 (5-C,5-D)\nUG Stream =BCA is of occurrence 38 (7-A, 13-B, 13-C, 5-D)\nEntropy (SBSC (M)) = -(23/23)xlog2(23/23)=0\nEntropy (SBSC (PCM)) = -(4/4)xlog2(4/4)=0\nEntropy (SBSC (IT)) = -(2/2)xlog2(2/2)=0\nEntropy (SBSC (CS)) = -(10/21)xlog2(10/21) -(11/21)xlog2(11/21)= 0.998364\nEntropy (SBSC(Biotech))= -(1/1)xlog2(1/1) =0\nEntropy (SBSC) = -(5/10)xlog2(5/10) -(5/10)xlog2(5/10)= 1\nEntropy (SBCA) = -(7/38)xlog2(7/38) -(13/38)xlog2(13/38) -(13/38)xlog2(13/38) - (5/38)xlog2(5/38)= 1.893387\nGain (S,UG Stream)= Entropy (S)-(23/99) x Entropy (SBSC(M)) - (4/99) x Entropy (SBSC(PCM)) - (2/99) x Entropy (BSC(IT))-(21/99) x Entropy (BSC(CS))-(1/99) x Entropy (SBSC(Biotech))-(10/99) x Entropy (SBSC)- (38/99) x Entropy (SBCA)\nGain (S,UG Stream)= 1.965031-(23/99) x 0-(4/99) x 0 - (2/99) x 0 -(21/99) x 0.998364- (1/99) x 0 -(10/99) x1- (38/99) x 1.893387=0.925492111\nSTEP 5: Attribute UG Grade\nIts value can be A , B, C, D and E.\nUG Grade =A is of occurrence 2 (1-A,1-C)\nUG Grade =B is of occurrence 44 (19-A,19-B,4-C,2-D)\nUG Grade =C is of occurrence 43 (14-A,3-B,13-C,13-D)\nUG Grade =D is of occurrence 10 (1-B,1-C,8-D)\nEntropy (SUA)= -(1/2)xlog2(1/2) -(1/2)xlog2(1/2)= 1\nEntropy (SUB)= -(19/44)xlog2(19/44) -(19/44)xlog2(19/44) -(4/44)xlog2(4/44) - (2/44)xlog2(2/44)= 1.563494\nEntropy (SUC)= -(14/43)xlog2(14/43) -(3/43)xlog2(3/43) -(13/43)xlog2(13/43) - (13/43)xlog2(13/43)= 1.838607\nEntropy (SUD)= -(1/10)xlog2(1/10) -(1/10)xlog2(1/10) -(8/10)xlog2(8/10)= 0.921928\nGain (S,UG Grade)= Entropy (S)-(2/99) x Entropy (SUA) - (44/99) x Entropy (SUB) - (43/99) x Entropy (SUC)-(10/99) x Entropy (SUD)\nGain (S,UG Grade)= 1.965031-(2/99) x1- (44/99) x 1.563494- (43/99) x 1.838607- (10/99) x 0.921928=0.35823184\nSTEP 6: Find which attribute is the root node and rank the attribute according to information gain shown in Table\nGain (S,UG Stream) 0.925492111 Gain (S, XII Grade) 0.563349283 Gain(S, Mathematics Grade in XII) 0.557822111 Gain (S,UG Grade) 0.35823184\nGain (S, UG Stream)= 0.925492111 is highest\nTherefore UG Stream is root node\nSTEP 7: Find which attribute is next decision node.\nSince Entropy (SBSC(M))=0 so no classification\nUG Stream->BSC(Math):A (No further classification)\nSince Entropy (SBSC(IT))=0 so no classification\nUG Stream->BSC (IT):D (No further classification)\nSince Entropy (SBSC(Biotech))=0 so no classification\nUG Stream BSC(IT) BSC(M)\nBSC(CS) BCA\nBSC(Bio)\nBSC(PCM)\nBSC\nUG Stream->BSC(Biotech):C (No further classification)\nSince Entropy (SBSC(PCM))=0 so no classification\nUG Stream->BSC(PCM):A (No further classification)\nSince\nEntropy (SBSC (CS)) = 0.998364\nEntropy (SBSC) = -(5/10)xlog2(5/10) -(5/10)xlog2(5/10)= 1\nEntropy (SBCA) = -(7/38)xlog2(7/38) -(13/38)xlog2(13/38) -(13/38)xlog2(13/38) - (5/38)xlog2(5/38)= 1.893387\nWith UG Stream= BSC (CS)\n1 .For finding the next node in branch of BSC (CS) , calculate the gain value of BSC(CS) with other nodes (XII Grade, Mathematics Grade in XII and UG Grade)\na) Gain(SBSC(CS)),XII Grade) = ?\nXII Grade=A is of occurrence 13(0-A,3-B, 0-C, 10-D)\nXII Grade=B is of occurrence 6 (0-A, 6-B)\nXII Grade=C is of occurrence 2(1-D,1-B)\nUG Stream BSC(I T)\nBSC(M\nBSC(CS )\nBCA\nBSC(Biotec h)\nBSC(PC M)\nBSC"}, {"heading": "Class D", "text": "Class A\nClass A\nClass C\nXII Grade=D is of occurrence 0\nEntropy (SBSC(CS)XA)= -(3/13)xlog2(3/13) \u2013(10/13)xlog2(10/13)= 0.77934984\nEntropy (SBSC(CS)XB)= -(6/6)xlog2(6/6)=0\nEntropy (SBSC(CS)XC)= -(1/2)xlog2(1/2) \u2013(1/2)xlog2(1/2)= 1\nGain(SBSC(CS)),XIIGrade) = Entropy(SBSC(CS))-(13/21)*Entropy(SBSC(CS)XA)(6/21)* Entropy (SBSC(CS)XB)-(2/21)* Entropy (SBSC(CS)XC)\nGain(SBSC(CS)),XIIGrade)= 0.998364 \u2013(13/21)* 0.779349-(6/21)*0-(2/21)* 1=0.42067176\nb) Gain (SBSC (CS)), Mathematics Grade in XII) = ?\nMathematics Grade in XII =A is of occurrence 7(7-B)\nMathematics Grade in XII =B is of occurrence 3(2-B,1-D)\nMathematics Grade in XII =C is of occurrence 1(1-B)\nMathematics Grade in XII =F is of occurrence 10(10-D)\nEntropy (SBSC(CS)MA)= -(7/7)xlog2(7/7) =0\nEntropy (SBSC(CS)MB)= -(2/3)xlog2(2/3)-(1/3)xlog2(1/3) =0.918296\nEntropy (SBSC(CS)MC)= -(1/1)xlog2(1/1) =0\nEntropy (SBSC(CS)MF)= -(10/10)xlog2(10/10) =0\nGain(SBSC(CS)),Mathematics Grade in XII)=\nEntropy(SBSC(CS))-(7/21)*Entropy(SBSC(CS)MA)-(3/21)* Entropy (SBSC(CS)MB)(1/21)* Entropy (SBSC(CS)MC)-(10/21)*Entropy(SBSC(CS)MF)\nGain(SBSC(CS)),Mathematics Grade in XII) =0.998364-(7/21)*0-(3/21)* 0.918296- (1/21)*0-(10/21)*0=0.867179\nc) Gain (SBSC (CS), UG Grade) = ?\nUG Grade =A is of occurrence 0\nUG Grade =B is of occurrence 7(7-B)\nUG Grade =C is of occurrence 7 (2-B,5-D)\nUG Grade =D is of occurrence 7(1-B,6-D)\nEntropy (SBSC(CS)UA)= 0,Entropy (SBSC(CS)UB)= 0\nEntropy (SBSC(CS)UC)= -(2/7)xlog2(2/7)-(5/7)xlog2(5/7)= 0.863120569\nEntropy (SBSC(CS)UD)= -(1/7)xlog2(1/7)-(6/7)xlog2(6/7)= 0.591672779\nGain(SBSC(CS),UG Grade) =Entropy(SBSC(CS))-(7/21)*Entropy(SBSC(CS)UB)(7/21)* Entropy (SBSC(CS)UC)-(7/21)* Entropy (SBSC(CS)UD)\nGain(SBSC(CS),UG Grade)=\n0.998364-(7/21)*0-(7/21)*0.863120569-(7/21)* 0.591672779=0.513432884\nGain(SBSC(CS),Mathematics Grade in XII) is greater than others attribute, so Mathematics Grade in XII is the next node in BSC(CS) branch.\nIt\u2019s values are A,B,C,D,E,F\nSince, Entropy (SBSC(CS) MA), Entropy (SBSC(CS)MC), Entropy (SBSC(CS)MD)=0 (No classification required)\nUG Stream->BSC(CS)->Mathematics Grade in XII->A:B (No further classification)\nUG Stream->BSC(CS)->Mathematics Grade in XII->C:B (No further classification)\nUG Stream->BSC(CS)->Mathematics Grade in XII->F:D (No further classification)\nUG Stream BSC(I T)\nBSC(M )\nBSC(C S)\nBCA\nBSC(Biotec h)\nBSC(PC M)\nBSC\nClass D\nClass A\nClass A\nClass C Mathematics Grade in XII\nBut Entropy (SBSC(CS)MB)= 0.918296\nFor finding the next node in branch of BSC(CS)->Mathematics Grade in XII->B , calculate the gain value of BSC(CS)MB with other nodes (XII Grade,and UG Grade)\nGain (SBSC(CS)MB),XII Grade) = ?\nXII Grade=A is of occurrence 1(1-B)\nXII Grade=B is of occurrence 1(1-B)\nXII Grade=C is of occurrence 1(1-D)\nEntropy(SBSC(CS)MBXA)=0,Entropy(SBSC(CS)MBXB)=0, Entropy(SBSC(CSMB)XC)=0\nGain(SBSC(CS)MB),XII Grade)\n=Entropy(SBSC(CS)MB)-(1/3)*Entropy(SBSC(CS)XA)-(1/3)*Entropy(SBSC(CS)XB)-(1/3)* Entropy (SBSC(CS)XC)\nGain(SBSC(CS)MB),XII Grade) =0.918296-(1/3)*0-(1/3)*0-(1/3)* 0=0.918296\nGain (SBSC (CS) MB),UG Grade) = ?\nUG Grade =A,D are of occurrence 0\nUG Grade =B is of occurrence 2(2-B)\nUG Stream BSC(I T)\nBSC(M\nBSC(CS )\nBCA\nBSC(Biotech )\nBSC(PC M)\nBSC\nClass D\nClass A\nClass A\nClass C Mathematics Grade in XII\nA\nClass B\nClass B\nC\nClass D\nF\nUG Grade =C is of occurrence 1(1-D)\nEntropy (SBSC(CS)MBUA),\nEntropy (SBSC(CS)MBUB),Entropy (SBSC(CS)MBUC)= 0,\nEntropy (SBSC(CS)MBUD)= 0\nGain(SBSC(CS)MB),UGGrade)\n=Entropy(SBSC(CS)MB)-(0/3)*Entropy(SBSC(CS)MBUA)-(2/3)* Entropy (SBSC(CS)MBUB)(0/3)* Entropy (SBSC(CS)MBUC)-(0/3)* Entropy (SBSC(CS)MBUD)\nGain(SBSC(CS)MB),UG Grade) =0.918296\nGain(SBSC(CS)MB),XII Grade) & Gain(SBSC(CS)MB),UG Grade) are equal =0.918296\nBut gain (S,XII Grade) is greater than gain(S,UG Grade) so next node is XII grade in branch\nUG Stream->BSC(CS)->Mathematics Grade in XII->B->XII Grade\nXII Grade values are A, B , C, D\nEntropy(SBSC(CS)MBXA)=0,\nEntropy(SBSC(CS)MBXB)=0,Entropy(SBSC(CS)MBXC)=0 so there is no classification\nUG Stream->BSC(CS)->Mathematics Grade in XII->B->XII Grade->A:B (No further classification)\nUG Stream->BSC(CS)->Mathematics Grade in XII->B->XII Grade->B:B (No further classification)\nUG Stream->BSC(CS)->Mathematics Grade in XII->B->XII Grade->C:D (No further classification).\nSo stop the process here\nWith UG Stream=BCA\n2 .For finding the next node in branch of BCA , calculate the gain value of BCA with other nodes (XII Grade, Mathematics Grade in XII and UG Grade)\na) Gain (SBCA,XII Grade) = ?\nXII Grade=A is of occurrence 3( 2-C, 1-D)\nXII Grade=B is of occurrence 19( 6-A, 10-B,3-C)\nXII Grade=C is of occurrence 7( 1-A, 3-B,3-C)\nXII Grade=D is of occurrence 8( 4-C, 4-D)\nXII Grade=E is of occurrence 1( 1-C)\nEntropy (SBCAXA)= -(2/3)xlog2(2/3)-(1/3)xlog2(1/3)= 0.918296\nEntropy (SBCAXB)= -(6/19)xlog2(6/19)-(10/19)xlog2(10/19) -(3/19)xlog2(3/19)= 1.432983\nEntropy (SBCAXC)= -(1/7)xlog2(1/7)-(3/7)xlog2(3/7) -(3/7)xlog2(3/7)= 1.08263751\nEntropy (SBCAXD)= -(4/8)xlog2(4/8)-(4/8)xlog2(4/8)=1\nEntropy (SBCAXE)= -(1/1)xlog2(1/1)=0\nUG Stream BSC(I T)\nBSC(M\nBSC(C S)\nBCA\nBSC(Biotec h)\nBSC(PC M)\nBSC\nClass D\nClass A\nClass A\nClass C Mathematics Grade in XII\nA\nClass B\nClass B\nC\nClass D\nF B\nXII Grade\nA B C\nClass Class Class\nGain(SBCA,XII Grade) =Entropy(SBCA)-(3/38)*Entropy(SBCAXA)-(19/38)* Entropy (SBCAXB)-(7/38)* Entropy (SBCAXC)-(8/38)* Entropy (SBCAXD)- (1/38)* Entropy (SBCAXE)\nGain(SBCA,XII Grade) =1.893387-(3/38)* 0.918296-(19/38)* 1.432983-(7/38)* 1.082637-(8/38)*1- (1/38)* 0=0.694439\nb) Gain ( SBCA, Mathematics Grade in XII) = ?\nMathematics Grade values are A,B,C,D,E and F\nMathematics Grade in XII =A is of occurrence 1( 1-B)\nMathematics Grade in XII =B is of occurrence 17( 6-A,9-B,2-C)\nMathematics Grade in XII =C is of occurrence 6( 1-A,3-B,1-C,1-D)\nMathematics Grade in XII =D is of occurrence 4(1-C,3-D)\nMathematics Grade in XII =F is of occurrence 10( 9-C,1-D)\nEntropy (SBCAMA)= -(1/1)xlog2(1/1)=0\nEntropy (SBCAMB)= -(6/17)xlog2(6/17)-(9/17)xlog2(9/17) -(2/17)xlog2(2/17)= 1.37928049\nEntropy (SBCAMC)= -(1/6)xlog2(1/6)-(3/6)xlog2(3/6) -(1/6)xlog2(1/6) - (1/6)xlog2(1/6)= 1.79248125\nEntropy (SBCAMD)= -(1/4)xlog2(1/4) -(3/4)xlog2(3/4)= 0.81127812\nEntropy (SBCAMF)= -(9/10)xlog2(9/10) -(1/10)xlog2(1/10)= 0.46899559\nGain(SBCA, Mathematics Grade in XII) =Entropy(SBCA)(1/38)*Entropy(SBCAMA)-(17/38)* Entropy (SBCAMB)-(6/38)* Entropy (SBCAMC)(4/38)* Entropy (SBCAMD)- (10/38)* Entropy (SBCAMF)\nGain(SBCA, Mathematics Grade in XII) =1.893387-(1/38)*0-(17/38)*1.379280- (6/38)* 1.792481-(4/38)* 0.811278- (10/38)* 0.468995=0.78449995\nc) Gain( SBCA, UG Grade) = ?\nUG Grade =A is of occurrence 1(1-C)\nUG Grade =B is of occurrence 23(7-A,12-B,4-C)\nUG Grade =C is of occurrence 11(1-B,7-C,3-D)\nUG Grade =D is of occurrence 3(1-C,2-D)\nEntropy (SBCAUA)= -(1/1)xlog2(1/1)=0\nEntropy (SBCAUB)= -(7/23)xlog2(7/23) -(12/23)xlog2(12/23) -(4/23)xlog2(4/23)= 1.45090828\nEntropy (SBCAUC)= -(1/11)xlog2(1/11) -(7/11)xlog2(7/11) -(3/11)xlog2(3/11)= 1.24067053\nEntropy (SBCAUD)= -(1/3)xlog2(1/3) -(2/3)xlog2(2/3)= 0.91829583\nGain(SBCA,UG Grade) =Entropy(SBCA)-(1/38)*Entropy(SBCAUA)-(23/38)* Entropy (SBCAUB)-(11/38)* Entropy (SBCAUC)-(3/38)* Entropy (SBCAUD)\nGain(SBCA,UG Grade) =1.893387-(1/38)*0-(23/38)* 1.450908-(11/38)* 1.240670- (3/38)* 0.918295=0.58356755\nGain( SBCA, Mathematics Grade in XII) is greater than Gain(SBCA,XII Grade) and Gain( SBCA, UG Grade) ) so next node is Mathematics grade in XII in branch\nUG Stream->BCA->Mathematics Grade in XII\nMathematics Grade in XII values are A,B,C,D,E and F\nSince Entropy (SBCAMA)=0 no classification\nUG Stream->BCA->Mathematics Grade in XII->A: B (no further classification)\nEntropy (SBCAMB)= 1.37928049\nEntropy (SBCAMC)= 1.79248125\nEntropy (SBCAMD)= 0.81127812\nEntropy (SBCAMF)= 0.46899559\n1) For SBCAMB (UG Stream->BCA->Mathematics Grade in XII->B :?)\nFor finding the next node in branch UG Stream->BCA->Mathematics Grade in XII>B:? , Calculate the gain value of SBCAMB with other nodes (XII Grade, UG Grade).\na) Gain (SBCAMB, XII Grade) = ?\nXII Grade=A is of occurrence 0\nXII Grade=B is of occurrence 17( 6-A,10-B,1-C)\nXII Grade=C is of occurrence 0\nXII Grade=D is of occurrence\nEntropy (SBCAMBXB)= -(6/17)xlog2(6/17) -(10/17)xlog2(10/17) -(1/17)xlog2(1/17)= 1.221047\nGain(SBCAMB,XII Grade) =Entropy(SBCAMB)-(17/17)*Entropy(SBCAMBXB)\nGain(SBCAMB,XII Grade) =1.37928049-(17/17)* 1.221047=0.15823349\nb) Gain (SBCAMB, UG Grade) = ?\nUG Grade=A is of occurrence 1(1-C)\nUG Grade=B is of occurrence 15(6-A,8-B,1-C)\nUG Grade=C is of occurrence 1 (1-B)\nUG Grade=D is of occurrence 0(null)\nUG Grade=E is of occurrence 0(null)\nUG Grade=F is of occurrence 0(null)\nEntropy (SBCAMBUA)= -(1/1)xlog2(1/1) =0\nEntropy (SBCAMBUB)= -(6/15)xlog2(6/15) -(8/15)xlog2(8/15) -(1/15)xlog2(1/15)= 1.272905\nEntropy (SBCAMBUC)= -(1/1)xlog2(1/1) =0\nGain(SBCAMB,UG Grade) = Entropy(SBCAMB )-(15/17)* Entropy (SBCAMBUB)\nGain(SBCAMB,UG Grade) =1.379280-(15/17)* 1.272905=0.25612853\nGain(SBCAMB,UG Grade) is greater than Gain(SBCAMB,XII Grade) so UG Grade is next node in branch UG Stream->BCA->Mathematics Grade in XII->B->UG Grade\nFor finding the next node in branch UG Stream->BCA->Mathematics Grade in XII>B->UG Grade->? , Calculate the gain value of it\u2019s with other nodes (XII Grade).\nUG Grade values are A, B, C, D and E\nEntropy (SBCAMBUA), Entropy (SBCAMBUC) =0\nEntropy (SBCAMBUB) = 1.272905\nGain (SBCAMBUB, XII Grade) =?\nXII Grade values are A, B, C, D, E\nXII Grade=A,C,D,E,F are of occurrence 0\nXII Grade=B is of occurrence 15(6-A8-B, 1-C)\nEntropy(SBCAMBUBXB)= 1.272905\nGain(SBCAMBUB,XII Grade) = Entropy (SBCAMBUB)-(15/15)* Entropy (SBCAMBUBXB)\nGain(SBCAMBUB,XII Grade) =1.272905-(15/15)* 1.272905=0 (no classification)\n2) For SBCAMC (UG Stream->BCA->Mathematics Grade in XII->C :?)\nFor finding the next node in branch UG Stream->BCA->Mathematics Grade in XII>C:? , Calculate the gain value of SBCAMC with other nodes (XII Grade, UG Grade).\na) Gain (SBCAMC, XII Grade) = ?\nXII Grade=A,B are of occurrence 0\nXII Grade=C is of occurrence 5(1-A,3-B,1-C)\nXII Grade=D is of occurrence 1(1-D)\nEntropy(SBCAMCXC)= -(1/5)xlog2(1/5) -(3/5)xlog2(3/5) -(1/5)xlog2(1/5)= 1.370951\nEntropy(SBCAMCXD)= -(1/1)xlog2(1/1)=0\nGain(SBCAMC,XIIGrade)=Entropy(SBCAMC)-(5/6)*Entropy(SBCAMCXC)-(1/6)* Entropy(SBCAMCXD)\nGain(SBCAMC,XII Grade)= 1.79248125 -(5/6)* 1.370951-(1/6)* 0=0.650022\nb) Gain (SBCAMC, UG Grade) = ?\nUG Grade values are A,B,C,D and E\nUG Grade=A,D,E are of occurrence 0\nUG Grade=B is of occurrence 4(1-A,3-B)\nUG Grade=C is of occurrence 2(1-C,1-D)\nEntropy(SBCAMCUB)= -(1/4)xlog2(1/4) -(3/4)xlog2(3/4)= 0.811278\nEntropy(SBCAMCUC)= -(1/2)xlog2(1/2) -(1/2)xlog2(1/2)=1\nGain (SBCAMC, UG Grade) = Entropy (SBCAMC)-(4/6)* Entropy (SBCAMCUB) - (2/6)* Entropy (SBCAMCUC)\nGain (SBCAMC, UG Grade) =1.79248125-(4/6)* 0.811278-(2/6)* 1=0.918296\nGain(SBCAMC,UG Grade) is greater than Gain(SBCAMC,XII Grade) so UG Grade is next node in branch\nUG Stream->BCA->Mathematics Grade in XII->C->UG Grade\nFor finding the next node in branch UG Stream->BCA->Mathematics Grade in XII>C->UG Grade->? , Calculate the gain value of it\u2019s with other nodes (XII Grade).\nUG Grade values are A, B, C, D and E\nEntropy (SBCAMCUA), Entropy (SBCAMCUD), Entropy (SBCAMCUE) =0\nEntropy(SBCAMCUB)=0.811278\nGain(SBCAMCUB,XII Grade)=?\nXII Grade values are A, B, C, D, E\nXII Grade=A,B are of occurrence 0\nXII Grade=C is of occurrence 4(1-A,3-B)\nEntropy(SBCAMCUBXC)= -(1/4)xlog2(1/4) -(3/4)xlog2(3/4)= 0.811278\nGain (SBCAMCUB,XII Grade) = Entropy (SBCAMCUB)-(4/4)* Entropy (SBCAMCUBXC)\nGain (SBCAMCUB,XII Grade) =0.811278-(4/4)* 0.811278=0 no classification\nUG Stream->BCA->Mathematics Grade in XII->C->UG Grade->B:B (no further classification)\nEntropy (SBCAMCUC)=1\nGain (SBCAMCUC, XII Grade) =?, XII Grade values are A, B, C, D, E\nXII Grade=A,B are of occurrence 0\nXII Grade=C is of occurrence 1(1-C)\nXII Grade=D is of occurrence 1(1-D)\nEntropy(SBCAMCUCXC)= -(1/1)xlog2(1/1)=0,Entropy(SBCAMCUCXD)= - (1/1)xlog2(1/1)=0\nGain (SBCAMCUC, XII Grade) = Entropy (SBCAMCUC)-(1/2)* Entropy (SBCAMCUCXC)-(1/2)*Entropy(SBCAMCUCXD)\nGain (SBCAMCUC, XII Grade) =1-(1/2)*0-(1/2)*0=1 classification is required\nSince Entropy(SBCAMCUCXC) and Entropy(SBCAMCUCXD)= 0 no classification further\nUG Stream->BCA->Mathematics Grade in XII->C->UG Grade->C->XII Grade>C:C (no further classification)\nUG Stream->BCA->Mathematics Grade in XII->C->UG Grade->C->XII Grade>D:D (no further classification)\n2) For SBCAMD (UG Stream->BCA->Mathematics Grade in XII->D :?)\nFor finding the next node in branch UG Stream->BCA->Mathematics Grade in XII>D:? , Calculate the gain value of SBCAMD with other nodes (XII Grade, UG Grade).\na) Gain (SBCAMD, XII Grade) = ?\nXII Grade values are A, B, C, D, E\nXII Grade=A,C,E are of occurrence 0\nXII Grade=B is of occurrence 1(1-C)\nXII Grade=D is of occurrence 3(3-D)\nEntropy(SBCAMDXB)= -(1/1)xlog2(1/1)=0\nEntropy(SBCAMDXD)= -(3/3)xlog2(3/3)=0\nGain (SBCAMD, XII Grade) = Entropy (SBCAMD)-(1/4)* Entropy (SBCAMDXB)(3/4)*Entropy(SBCAMDXD)\nGain (SBCAMD, XII Grade) =0.81127812 -(1/4)*0-(3/4)*0=0.811278\nb) Gain (SBCAMD, UG Grade) = ?\nUG Grade values are A,B,C,D and E\nUG Grade=A,B are of occurrence 0\nUG Grade=C is of occurrence 2(1-D,1-C)\nUG Grade=D is of occurrence 2(2-D)\nEntropy(SBCAMDUC)= -(1/2)xlog2(1/2) -(1/2)xlog2(1/2)=1\nEntropy(SBCAMDUD)= -(2/2)xlog2(2/2)=0\nGain (SBCAMD, UG Grade) = Entropy (SBCAMD)-(2/4)* Entropy (SBCAMDUC)(2/4)*Entropy(SBCAMDUD)\nGain (SBCAMD, UG Grade) = 0.81127812-(2/4)*1-(2/4)*0=0.311278\nGain (SBCAMD, XII Grade) is greater than Gain (SBCAMD, UG Grade) so XII Grade is next node in branch\nUG Stream->BCA->Mathematics Grade in XII->D->XII Grade\nXII Grade values are A,B,C,D,E\nEntropy(SBCAMDXB)= -(1/1)xlog2(1/1)=0 , Entropy(SBCAMDXD)= -(3/3)xlog2(3/3)=0 (no further classification)\nUG Stream->BCA->Mathematics Grade in XII->D-> XII Grade->B:C (no further classification)\nUG Stream->BCA->Mathematics Grade in XII->D-> XII Grade->D:D (no further classification)\n2) For SBCAMF (UG Stream->BCA->Mathematics Grade in XII->F :?)\nFor finding the next node in branch UG Stream->BCA->Mathematics Grade in XII>F:? , Calculate the gain value of SBCAMF with other nodes (XII Grade, UG Grade).\na) Gain (SBCAMF, XII Grade) = ?\nXII Grade values are A, B, C, D, E\nXII Grade=A is of occurrence 3(2-C,1-D)\nXII Grade=B is of occurrence 0\nXII Grade=C is of occurrence 2(2-C)\nXII Grade=D is of occurrence 4(4-C)\nXII Grade=E is of occurrence 1(1-C)\nEntropy(SBCAMFXA)= -(2/3)xlog2(2/3) -(1/3)xlog2(1/3)= 0.918296\nEntropy(SBCAMFXC)= -(2/2)xlog2(2/2)=0,Entropy(SBCAMFXD)= -(4/4)xlog2(4/4)=0\nEntropy(SBCAMFXE)= -(1/1)xlog2(1/1)=0\nGain (SBCAMF, XII Grade)\n= Entropy (SBCAMF)-(3/10)* Entropy (SBCAMFXA)-(2/10)*Entropy(SBCAMFXC) - (4/10)* Entropy (SBCAMFXD)-(1/10)*Entropy(SBCAMFXE)\nGain (SBCAMF, XII Grade) =0.46899559-(3/10)*0.918296-(2/10)*0-(4/10)*0- (1/10)*0=0.193507\nb) Gain (SBCAMF, UG Grade) = ?\nUG Grade values are A,B,C,D and E\nUG Grade=A,E are of occurrence 0\nUG Grade=B is of occurrence 3(3-C)\nUG Grade=C is of occurrence 6(5-C,1-D)\nUG Grade=D is of occurrence 1(1-C)\nEntropy(SBCAMFUB)= -(3/3)xlog2(3/3)=0\nEntropy(SBCAMFUC)= -(5/6)xlog2(5/6) -(1/6)xlog2(1/6)= 0.650022\nEntropy(SBCAMFUD)= -(1/1)xlog2(1/1)=0\nGain (SBCAMF, UG Grade) = Entropy (SBCAMF)-(3/10)* Entropy (SBCAMFUB)(6/10)*Entropy(SBCAMFUC) -(1/10)* Entropy (SBCAMFUD)\nGain (SBCAMF, UG Grade) =0.46899559-(3/10)*0-(6/10)* 0.650022 -(1/10)* 0= 0.078982\nGain (SBCAMF, XII Grade) is greater than Gain (SBCAMF, UG Grade) so XII grade is next node in branch\nUG Stream->BCA->Mathematics Grade in XII->F-> XII Grade\nFor finding the next node in branch UG Stream->BCA->Mathematics Grade in XII>F->XII Grade->? , Calculate the gain value of it\u2019s with other nodes (UG Grade).\nXII Grade values are A,B,C,D,E\nEntropy(SBCAMFXA)= 0.918296\nEntropy(SBCAMFXC),Entropy(SBCAMFXD),Entropy(SBCAMFXE)= 0\n(No further classification )\nGain(SBCAMFXA,UG Grade)=?\nUG Grade values are A,B,C,D and E\nUG Grade=A,D,E are of occurrence 0 (null)\nUG Grade=B is of occurrence 1(1-C)\nUG Grade=C is of occurrence 2(1-C, 1-D)\nEntropy(SBCAMFXAUB)= -(1/1)xlog2(1/1)=0\nEntropy(SBCAMFXAUC)= -(1/2)xlog2(1/2) -(1/2)xlog2(1/2)=1\nGain(SBCAMFXA,UGGrade)=\nEntropy(SBCAMFXA)-(1/3)*Entropy(SBCAMFXAUB)-(2/3)*Entropy(SBCAMFXAUC)\nGain(SBCAMFXA,UG Grade)= 0.918296-(1/3)*0-(2/3)*1= 0.251629\nSince Entropy(SBCAMFXAUB)= 0 (no further classification)\nUG Stream->BCA->Mathematics Grade in XII->F-> XII Grade->A->UG Grade>B:C (no further classification)\nEntropy(SBCAMFXAUC)= 1 there is classification required but there is no remaining attribute so stop the process\nUG Stream->BCA->Mathematics Grade in XII->F-> XII Grade->A->UG Grade>C:D (no further classification)\nWith UG Stream=BSC\n3. For finding the next node in branch of BSC , calculate the gain value of BSC with other nodes (XII Grade, Mathematics Grade in XII and UG Grade)\na) Gain (SBSC, XII Grade) = ?\nXII Grade values are A, B, C, D, E\nXII Grade=A is of occurrence 1(1-C)\nXII Grade=B is of occurrence 1(1-C)\nXII Grade=C is of occurrence 6 (2-C,4-D)\nXII Grade=D is of occurrence 2(1-C,1-D)\nXII Grade=E is of occurrence 0 (null)\nEntropy(SBSCXA)= -(1/1)xlog2(1/1)=0,Entropy(SBSCXB)= -(1/1)xlog2(1/1)=0\nEntropy(SBSCXC)= -(2/6)xlog2(2/6) -(4/6)xlog2(4/6)= 0.918296\nEntropy(SBSCXD)= -(1/2)xlog2(1/2) -(1/2)xlog2(1/2)=1\nGain(SBSC,XIIGrade)= Entropy(SBSC)-(1/10)*Entropy(SBSCXA)-(1/10)*Entropy(SBSCXB)-(6/10)*Entropy (SBSCXC)-(2/10)*Entropy(SBSCXD) Gain(SBSC,XIIGrade)=1-(1/10)*0-(1/10)*0-(6/10)* 0.918296-(2/10)*1=0.249022\nb) Gain (SBSC, Mathematics Grade in XII) = ?\nMathematics Grade values are A,B,C,D,E and F\nMathematics Grade in XII =A,D,E are of occurrence 0 (null)\nMathematics Grade in XII =B is of occurrence 1( 1-C)\nMathematics Grade in XII =C is of occurrence 4( 1-C,3-D)\nMathematics Grade in XII =F is of occurrence 5( 3-C,2-D)\nEntropy(SBSCMB)= -(1/1)xlog2(1/1)=0\nEntropy(SBSCMC)= -(1/4)xlog2(1/4)-(3/4)xlog2(3/4)= 0.811278\nEntropy(SBSCMF)= -(3/5)xlog2(3/5)-(2/5)xlog2(2/5)= 0.970951\nGain (SBSC, Mathematics Grade in XII) =\nEntropy(SBSC)-(1/10)*Entropy(SBSCMB)-(4/10)*Entropy(SBSCMC)-(5/10)*Entropy(SBSCMF)\nGain (SBSC, Mathematics Grade in XII) =1-(1/10)*0-(4/10)* 0.811278-(5/10)* 0.970951=0.190013\nc) Gain (SBSC,UG Grade) = ?\nUG Grade values are A,B,C,D and E\nUG Grade=A,B,D,E are of occurrence 0 (null)\nUG Grade=C is of occurrence 10 (5-C,5-D)\nEntropy(SBSCUC)= -(5/10)xlog2(5/10)-(5/10)xlog2(5/10)=1\nGain (SBSC,UG Grade) = Entropy (SBSC)-(10/10)* Entropy (SBSCUC)\nGain (SBSC,UG Grade) =1-(10/10)*1=0\nGain(SBSC,XII Grade) is greater than Gain (SBSC, Mathematics Grade in XII) & Gain(SBSC,UG Grade) so XII Grade is next node in branch :- UG Stream->BSC->XII Grade\nFor finding the next node in branch of UG Stream->BSC->XII Grade->? , calculate the gain value of it\u2019s with other nodes (Mathematics Grade in XII and UG Grade)\nXII Grade values are A, B, C, D, E\nEntropy (SBSCXA) and Entropy (SBSCXB)= 0( No further classification)\nEntropy (SBSCXC)= 0.918296\nEntropy (SBSCXD)= 1\n1) For SBSCXC\nFor finding the next node in branch of UG Stream->BSC->XII Grade->C->? , calculate the gain value of it\u2019s with other nodes (Mathematics Grade in XII and UG Grade)\na) Gain(SBSCXC, Mathematics Grade in XII)=?\nMathematics Grade values are A,B,C,D,E and F\nMathematics Grade in XII =A,B,D,E are of occurrence 0 (null)\nMathematics Grade in XII =C is of occurrence 3 (3-D)\nMathematics Grade in XII =F is of occurrence 3 (2-C,1-D)\nEntropy(SBSCXCMC)= -(3/6)xlog2(3/6)= 0.5\nEntropy(SBSCXCMF)= -(2/3)xlog2(2/3)-(1/3)xlog2(1/3) = 0.918296\nGain(SBSCXC, Mathematics Grade in XII)=\nEntropy(SBSCXC)-(3/6)* Entropy(SBSCXCMC)-(3/6)*Entropy(SBSCXCMF)\nGain(SBSCXC, Mathematics Grade in XII)=\n0.918296-(3/6)* 0.5-(3/6)* 0.918296=0.209148\nb) Gain(SBSCXC,UG Grade)=?\nUG Grade values are A,B,C,D and E\nUG Grade=A,B,D,E are of occurrence 0 (null)\nUG Grade=C is of occurrence 6 (2-C,4-D)\nEntropy(SBSCXCUC)= -(2/6)xlog2(2/6)-(4/6)xlog2(4/6)= 0.918296\nGain(SBSCXC,UG Grade)= Entropy(SBSCXC)-(6/6)* Entropy(SBSCXCUC)\nGain(SBSCXC,UG Grade)= 0.918296-(6/6)* 0.918296=0\nGain(SBSCXC, Mathematics Grade in XII) is highest than Gain(SBSCXC,UG Grade) so Mathematics Grade in XII is next node in branch\nUG Stream->BSC->XII Grade->C->Mathematics Grade in XII\nFor finding the next node in branch of UG Stream->BSC->XII Grade->C>Mathematics Grade in XII->?, calculate the gain value of it\u2019s with other node (UG Grade)\nMathematics Grade values are A,B,C,D,E and F\nMathematics Grade in XII =A,B,D,E are of occurrence 0 (null)\nEntropy(SBSCXCMC)= 0.5\nEntropy(SBSCXCMF)= 0.918296\nFor SBSCXCMC\nFor finding the next node in branch of UG Stream->BSC->XII Grade->C>Mathematics Grade in XII->C->?, calculate the gain value of it\u2019s with other node (UG Grade)\nGain(SBSCXCMC,UG Grade)= ?\nUG Grade values are A,B,C,D and E\nUG Grade=A,B,D,E are of occurrence 0 (null)\nUG Grade=C is of occurrence 3 (3-D)\nEntropy(SBSCXCMCUC)= -(3/3)xlog2(3/3)=0 (No further classification)\nGain(SBSCXCMC,UG Grade)=\nEntropy(SBSCXCMC)-(3/3)* Entropy(SBSCXCMCUC)\nGain(SBSCXCMC,UG Grade)= 0.5-(3/3)*0=0.5\nSince Entropy(SBSCXCMCUC)= 0 (no further classification)\nUG Stream->BSC->XII Grade->C->Mathematics Grade in XII->C: D (no further classification)\nFor SBSCXCMF\nFor finding the next node in branch of UG Stream->BSC->XII Grade->C>Mathematics Grade in XII->F->?, calculate the gain value of it\u2019s with other node (UG Grade)\nGain(SBSCXCMF,UG Grade)= ?\nUG Grade values are A,B,C,D and E\nUG Grade=A,B,D,E are of occurrence 0 (null)\nUG Grade=C is of occurrence 3 (2-C,1-D)\nEntropy(SBSCXCMFUC)= -(2/3)xlog2(2/3) -(1/3)xlog2(1/3)= 0.918296\nGain(SBSCXCMF,UG Grade)=\nEntropy(SBSCXCMF)-(3/3)* Entropy(SBSCXCMFUC)\nGain(SBSCXCMF,UG Grade)=\n0.918296-(3/3)* 0.918296=0 (No further classification)\nUG Stream->BSC->XII Grade->C->Mathematics Grade in XII->F: C (no further classification)\n2) For SBSCXD\nFor finding the next node in branch of UG Stream->BSC->XII Grade->D->? , calculate the gain value of it\u2019s with other nodes (Mathematics Grade in XII and UG Grade)\nSince Entropy(SBSCXD)= 1\na) Gain(SBSCXD, Mathematics Grade in XII)=?\nMathematics Grade values are A,B,C,D,E and F\nMathematics Grade in XII =A,B,D,E are of occurrence 0 (null)\nMathematics Grade in XII =C is of occurrence 1 (1-C)\nMathematics Grade in XII =F is of occurrence 1 (1-D)\nEntropy(SBSCXDMC)= -(1/1)xlog2(1/1)=0,Entropy(SBSCXDMF)= -(1/1)xlog2(1/1)=0 (No further classification)\nGain(SBSCXD, Mathematics Grade in XII)=\nEntropy(SBSCXD)-(1/2)* Entropy(SBSCXDMC)-(1/2)* Entropy(SBSCXDMF)\nGain(SBSCXD, Mathematics Grade in XII)= 1-(1/2)*0-(1/2)* 0=1\nb) Gain(SBSCXD,UG Grade)=?\nUG Grade values are A,B,C,D and E\nUG Grade=A,B,D,E are of occurrence 0 (null)\nUG Grade=C is of occurrence 2 (1-C,1-D)\nEntropy(SBSCXDUC)= -(1/2)xlog2(1/2) -(1/2)xlog2(1/2)=1\nGain(SBSCXD,UG Grade)= Entropy(SBSCXD)-(2/2)* Entropy(SBSCXDUC)\nGain(SBSCXD,UG Grade)= 1-(2/2)* 1=0\nGain(SBSCXD, Mathematics Grade in XII) is higher than Gain(SBSCXD,UG Grade) so Mathematics Grade is next node in branch UG Stream->BSC->XII Grade->D>Mathematics Grade in XII\nFor finding the next node in branch of UG Stream->BSC->XII Grade->D>Mathematics Grade in XII->? , calculate the gain value of it\u2019s with other node (UG Grade)\nMathematics Grade in XII =A,B,D,E are of occurrence 0 (null)\nEntropy(SBSCXDMC),Entropy(SBSCXDMF)= 0 (No further classification)\nUG Stream->BSC->XII Grade->D->Mathematics Grade in XII->C: C (No further classification)\nUG Stream->BSC->XII Grade->D->Mathematics Grade in XII->F: D (No further classification)\nFigure 3.16: Final generated ID3 Decision tree for MCA student performance\nC F Class C\nClass D\nD\nMathematics Grade in XII\nF\nClass C Class D\nC C B A\nClass B\nMathematics Grade in XII\nD\nB D Class C Class D\nXII Grade\nB\nClass B Class B\nC A\nUG Grade\nClass C\nF\nClass C\nC Class C\nD\nClass C\nE\nClass C\nA\nUG Grade\nB\nClass D\nC\nXII Grade\nXII Grade\nA B C\nClass B Class D Class B\nB A\nClass B Class B\nC\nClass D\nF\nMathematics Grade in XII\nC\nXII Grade\nC D\nClass C Class D\nUG Grade\nClass B\nB\nUG Stream BSC(IT)\nBSC(M)\nBSC(CS)\nBCA\nBSC(Biotech)\nBSC(PCM)\nBSC\nClass D\nClass A\nClass A\nClass C\nA\nClass C B\nClass C\nXII Grade\nC\nMathematics Grade in XII\n3.1.5 Conclusion In this study, a reasonably accurate model has been built that helps academicians and administrators to predict student\u2019s enrollment in MCA course. This study examines that student\u2019s performance(past academic)can be used to construct a model using classification with a decision tree algorithm (ID3 and J48 decision tree algorithm).The concept of classification with Decision Tree helps to extracting the knowledge from the past academic data. In the confusion matrix of ID3 and J48, it is shown that out of four actual categories, the accuracy of D class is 78.3% and 69.6% respectively that means this model is successfully identifying the which student doesn\u2019t perform better in MCA. So, there is a need for proper counseling for the students that are going to select in MCA course. This study also helps students in selecting the course for admission according to his skills and academics. It results in that B.Sc. students with Mathematics and BCA stream students performed better in MCA but B.Sc. without Mathematics stream students did not perform well.\n3.1.6 Testing We can make predictions for a test set, whether that set contains valid class values or not. The output will contain both the actual and predicted class.\nClassifying new data\nnew instance:- {B, A, BSC(Math), ?}.\n(The class attribute is ? because you don\u2019t know the classification.)\nDo the following:\nSTEP1: Guess a value for ? (or set it at random), say B, i.e. {B, A, BSC(Math), B}\nSTEP 2: Then create a test file and include the above instance . STEP 3: Click on the Choose button and choose a classifier (trees->ID3 decision tree algorithm).\nFigure 3.17 : selecting a classifier from WEKA Explorer\nSTEP 4: open the test set file using supplied test set\nFigure 3.18:Open the test set file\nSTEP 5: Select more options for classifier evaluation\nI. Click on More options\nFigure 3.19: Selecting more options\nII. Click on some evaluation options as selected in figure\nSTEP 6: Start the classification\nFigure 3.20: Test set prediction with unknown actual classification"}, {"heading": "3.2 DATA MINING APPLICATION IN", "text": ""}, {"heading": "ADVERTISEMENT MANAGEMENT OF HIGHER", "text": "EDUCATIONAL INSTITUTES In recent years, Indian higher educational institute\u2019s competition grows rapidly for attracting students to get enrollment in their institutes. To attract students educational institutes select a best advertisement method. There are different advertisements available in the market but a selection of them is very difficult for institutes. This study is helpful for institutes to select a best advertisement medium using some data mining methods. 3.2.1 Data Used In this study the data is collected through the questionnaire survey at the institute. There are 500 questionnaires are collected. This questionnaire includes student personnel and academic information. This study is helpful for finding a better advertisement for institutes so this questionnaire includes a important question \u201cHow They knows about this university\u201d?\nTable 3.10 shows different advertisement methods that are available in questionnaire\u2019s field. Table 3.11 shows different combination of advertisements occurrences that occur while responding the questionnaire field.\nTable 3.10: Different advertisement methods\nAdvertising methods Codes Answers\nFriends FR 309 Family FA 17 Internet Search IS 24 Online Advertising OA 8 Newspaper NE 8 Others OT 5\nTable 3.11: Different combination of advertisements\nRelations Occurrences Friends and Family(FRFA) 10 Friends and Internet Search (FRIS) 6 Friends and Online Advertising(FROA) 2 Friends and Newspaper (FRNE) 3 Friends and Others(FROT) 0 Family and Internet Search(FAIS) 0 Family and Online Advertising(FAOA) 0 Family and Newspaper (FANE) 7 Family and Others(FAOT) 0 Internet Search and Online Advertising(ISOA) 4 Internet Search and Newspaper(ISNE) 2 Internet Search and Others(ISOT) 0 Online Advertising and Newspaper(OANE) 2 Online Advertising 0\nand Others(OAOT) Newspaper and Others(NEOT)\n0\nTotal counts(support)\n36\n3.2.2 Analysis of Support and Confidence Value The support value of a data item indicates the percentage of database transaction in which that data item appears. Confidence value measures the rule\u2019s strength. A small support and large confidence value are meaningful. Support and confidence analysis is shown in Table 3.12.Support and Confidence values are calculated as:\nSupport= Occurrences/Total Support Confidence=Support(X,Y)/Support(X)\nTable 3.12: Support and confidence analysis of different relations\nRelations Support = occurrences/Total Support\nConfidence = support(X,Y) / support(X)\nFA->FR 0.28 0.59 IS->FR 0.17 0.25 OA->FR 0.06 0.25 NE->FR 0.08 0.375 Friends and Others 0 - Family and Internet Search 0 - Family and Online Advertising 0 - NE->FA 0.19 0.875 Family and Others 0 - IS->OA 0.11 0.17 NE->IS 0.06 0.25 Internet Search and Others 0 - OA->NE 0.06 0.25 Online Advertising and Others 0 -\nFigure 3.21: Advertisement method\u2019s links\n3.2.3 Cosine Value Analysis Cosine value occurs between 0 and 1.If its value is close to 1 then a good correlation occurs between X and Y. Cosine analysis formula defined as: Cosine(X->Y) = p(X,Y) / sqrt(p(X)*p(Y))\nTable 3.13: Cosine analysis of different relations\nRelations Cosine value=(p(X,Y)/sqrt(p(X)*p(Y)))\nFA->FR 0.13 IS->FR 0.06 OA->FR 0.04 NE->FR 0.06 NE->FA 0.60 IS->OA 0.28 NE->IS 0.14 OA->NE 0.25 From table 3.12 result it can conclude that 28% students have answered with family and friends. Since 28% is largest support then it is a better advertisement method and its confidence value is 59% shows that if 59% of the time family occurs then friends also occurs in response. Figure 3.21 concluded that the all advertisement methods are linked to either friend advertisement or newspaper advertisement medium. In figure 3.21 it is shown that friends and newspaper have five edges. In table 3.13, cosine values are calculated for all advertisement mediums and it concludes that\nFRFA FRIS FROA FRNE ISOA ISNE FANE OANE\nFR FA IS OA NE\nnewspaper \u2192 family has good relation to each other but others relations are not so close to each other. 3.2.4 Apriori algorithm implementation\nTable 3.14: Advertisement Methods data set\nRelations Code Occurrences(Support count) Friends FR 309 Family FA 17 Internet Search IS 24 Online Advertising OA 8 Newspaper NE 8 Others OT 5 Friends and Family FRFA 10 Friends and Internet Search FRIS 6\nFriends and Online Advertising FROA 2 Friends and Newspaper FRNE 3 Family and Newspaper FANE 7 Internet Search and Online Advertising ISOA 4 Internet Search and Newspaper ISNE 2 Online Advertising and Newspaper OANE 2\nStep 1: In this step the table 5 is scanned by algorithm and obtained information generates candidate item set C1 (shown in table 6). Step 2: In this step, compare C1 candidate item set with minimum support count that generate frequent item set L1\nScan Database for count of each candidate item set\nApriori algorithm C1 Join L1 transformation\nTable 3.15: Generated Candidate item set C1\nCode Occurrences (Support count) FR 309 FA 17 IS 24 OA 8 NE 8 OT 5 Minimum support count=5 Step 3: In step 3, L1 Join L1 provides L2 frequent item set and candidate item set C2 it is called joining process.\nCode Occurrences (Support count) FR 309 FA 17 IS 24 OA 8 NE 8 OT 5 L1 Join L1\nCandidate item set C2 generation by L1 Join L1 transformation\nStep 4: In this step the table 3.19 is scanned by algorithm and compare C2 candidate item set with minimum support count that generate frequent item set L2\n= >Minimum support (5)\nTable 3.16: Generated frequent item sets L1\nCode Occurrences (Support count) FR 309 FA 17 IS 24 OA 8 NE 8 OT 5\nL1*L1 L1-1 L1-2 FR,FA FR FA FR,IS FR IS FR,OA FR OA FR,NE FR NE FR,OT FR OT FA,IS FA IS FA,OA FA OA FA,NE FA NE FA,OT FA OT IS,OA IS OA IS,NE IS NE IS,OT IS OT OA,NE OA NE OA,OT OA OT NE,OT NE OT\nTable 3.18: Candidate set C2 with Support Count\nCode Support Count FR,FA 10 FR,IS 6 FR,OA 2 FR,NE 3 FR,OT 0 FA,IS 0 FA,OA 0 FA,NE 7 FA,OT 0 IS,OA 4 IS,NE 2 IS,OT 0 OA,NE 2 OA,OT 0 NE,OT 0\nStep 5: In step 5, L2 Join L2 provides L3 frequent item set and candidate item set C3 (shown in table 11).\nCode Support count FR,FA 10 FR,IS 6 FA,NE 7 L2 Join L2\nCandidate item set C3 generation by L2 Join L2 transformation\nStep 6:\n The subsets of (FR,FA,IS) are (FR,FA),(FR,IS)and(FA,IS).(FA,IS) is not a member of L2. It is not a frequent item set. Therefore, remove (FR,FA,IS) from C3.  The subsets of (FR,FA,NE) are (FR,FA),(FA,NE)and(FR,NE).(FR,NE) is not a member of L2. It is not a frequent item set. Therefore, remove (FR,FA,NE) from C3.\n= > Minimum support (5)\nTable 3.19: Generated frequent item\nsets L2\nCode Support count FR,FA 10 FR,IS 6 FA,NE 7\nTable 3.20: Generated Candidate set C3\nL2*L2 L2-1 L2-2 FR,FA,IS FR,FA FR,IS FR,FA,NE FR,FA FA,NE FR,IS,FA,NE FR,IS FA,NE\n The subsets of (FR,IS,FA,NE) are  (FR,IS),(FR,FA),(FR,NE),(IS,FA),(IS,NE)and(FA,NE) .(FR,NE),(IS,FA),(IS,NE)\nare not a members of L2. It is not a frequent item set. Therefore, remove (FR, IS, FA, NE) from C3.\n These item sets are pruned according to pruning property because its item sets are not frequent ,C3 is null, so final frequent item set is L2.If the minimum confidence is 50% then Confidence (FA->FR) and Confidence (NE->FA) is maximum so these are the frequent item sets (shown in table 3.22).\nFigure 3.22: Analysis chart that shows different advertisement methods(answered given in questionnaires)\n0 50\n100\n150 200 250 300 350\nFriends Family Internet Search Online Advertising\nNewspaper Others\nTable 3.21: Frequent item set L2\nCode Support Count\nConfidence\nFRFA 10 0.59(59%) FRIS 6 0.25(25%) FANE 7 0.875(87%)\nTable 3.22: Frequent item sets\nCode Confidence FRFA 0.59(59%) FANE 0.875(87%)\nFigure 3.23: Analysis chart that shows different advertisement methods (according to data mining analysis)\n3.2.5 Conclusion This study looks for a better advertisement method. Fig.3.22 shows that most students have answered with friends (309 times) in the questionnaire but in this study data mining techniques are applied and this analysis concluded that the Family and Newspaper could be the best advertising method. Table 3.10 shows that most students have answered with friends (309 times) but with data mining technique\u2019s analysis it is concluded that the Friends and Family, Family and Newspaper could be the best advertisement methods because these advertisement methods are linked with all advertisement methods. After applying apriori algorithm on the data set, Table 3.22 shows frequent item sets (FRFA,FANE) that concluded that Friends and Family, Family and Newspaper is the best advertisement methods. This study is helpful for educational institutes to making a good advertisement strategy that attracts student effectively.\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\n1\nFriends and Family Internet Search and\nFriends Online Advertising and Friends Newspaper and Friends Newspaper and Family Internet Search and Others\n3.3 Classification of Student\u2019s data Using Data Mining Techniques for Training & Placement Purpose. Data Mining is new approach for technical education. Technical institute like engineering & other can use data mining techniques for analysis of different performances in student\u2019s qualifications. In our work, we collected enrolled student\u2019s data from engineering institute that have different information like 10th, 12th, B.tech passing percentage and then apply decision tree method for classifying students academics performance for Training & placement department can be identify the final grade of student for placement purpose. In future this study will be help to develop new approaches of data mining techniques in technical education. 3.3.1 Data Used\nTable 3.23: Student Attribute Description\nAttribute Possible Values Branch CS/IT 10th Per { First > 60%,Second > 45 & < 60 %,Third > 35 & < 45 % } 12th Per { First > 60%,Second > 45 & < 60 %,Third > 35 & < 45 % } B.Tech Per { First > 60%,Second > 45 & < 60 %,Third > 35 & < 45 % } Final Grade { Excellent, Good, Average }\n3.3.2 Classification Rules IF 10th % =\u201cFirst\u201d AND 12th % =\u201cFirst\u201d AND B.Tech % = \u201cFirst\u201d THEN Final_Grade = \u201cExcellent\u201d IF 10th % =\u201cSecond\u201d AND 12th % =\u201cFirst\u201d AND B.Tech % = \u201cFirst\u201d THEN Final_Grade = \u201cGood\u201d IF 10th % =\u201cThird\u201d AND 12th % =\u201cFirst\u201d AND B.Tech % =\u201cFirst\u201d THEN Final_Grade = \u201cAverage\u201d IF 10th % =\u201cFirst\u201d AND 12th % =\u201cSecond\u201d AND B.Tech % =\u201cFirst\u201d THEN Final_Grade = \u201cGood\u201d IF 10th % =\u201cSecond\u201d AND 12th % =\u201cSecond\u201dAND B.Tech %= \u201cFirst\u201d THEN Final_Grade = \u201cAverage\u201d IF 10th % =\u201cThird\u201d AND 12th % =\u201cSecond\u201d AND B.Tech %= \u201cFirst\u201d THEN Final_Grade = \u201cAverage\u201d IF 10th % =\u201cFirst\u201d AND 12th % =\u201cFirst\u201d AND B.Tech % =\u201cSecond\u201d THEN Final_Grade = \u201cAverage\u201d IF 10th % =\u201cSecond\u201d AND 12th % =\u201cFirst\u201d AND B.Tech % =\u201cSecond\u201d THEN Final_Grade = \u201cAverage\u201d\nIF 10th % =\u201cThird\u201d AND 12th % =\u201cFirst\u201d AND B.Tech % =\u201cSecond\u201d THEN Final_Grade = \u201cAverage\u201d IF 10th % =\u201cFirst\u201d AND 12th % =\u201cSecond\u201d AND B.Tech % =\u201cSecond\u201d THEN Final_Grade = \u201cAverage\u201d IF 10th % =\u201cSecond\u201d AND 12th % =\u201cSecond\u201dAND B.Tech %= \u201cSecond\u201d THEN Final_Grade = \u201cAverage\u201d 3.3.3 Conclusion & Future Work In this work we make use of data mining process in a student\u2019s database using classification data mining techniques (decision tree method). The information generated after the analysis of data mining techniques on student\u2019s data base is helpful for executives for training & placement department of engineering colleges. This work classifies the categories of student\u2019s performance in their academic qualifications. For future work, this study will be helpful for institutions and industries. We can be generating the information after implementing the others data mining techniques like clustering, Predication and Association rules etc on different eligibility criteria of industry recruitment for students.\nCHAPTER 4 REFERENCES\n[1] C. Romero and S. Ventura, \u201cEducational data mining: A survey from 1995 to 2005\u201d.Expert Systems with Applications 33 (2007) 135-146.\n[2] Delavari N, Beikzadeh M.R. \u201cData Mining Application in Higher Learning Institutions \u201cInformatics in Education, 2008, Vol. 7,No. 1, 31-54.\n[3] R. R. Kabra and R. S. Bichkar, \u201cPerformance Prediction of Engineering Students using Decision Trees \u201cInternational Journal of Computer Applications (0975 \u2013 8887) Volume 36- No.11, December 2011.\n[4] Oladipupo O.O. and Oyelade O. J., Knowledge Discovery from Student\u2019s Result Repository: Association Rule Mining Approach. International Journal of Computer Science & Security (IJCSS), Volume (4) : Issue (2).\n[5] Zlatko J.Kovacic, \u201cEarly Prediction of Student Success: Mining Students Enrollment Data\u201d, Proceedings of Informing Science & IT Education Conference (InSITE) 2010.\n[6] Nguyen Thai Nghe, Paul Janecek, and Peter Haddawy, \u201cA Comparative Analysis of Techniques for Predicting Academic Performance\u201d, In Proceedings of the 37th ASEE/IEEE Frontiers in Education Conference. Pp. 7-12, 2007.\n[7] Syed Tahir Hijazi, and S. M. M. Raza Naqvi, Factors affecting students\u2019 performance: A Case Of Private Colleges. Bangladesh e-Journal of Sociology.Volume3.Number1, January 2006.\n[8] T.Miranda Lakshmi, A.Martin, R.Mumtaj Begum and Dr.V.Prasanna Venkatesan, An Analysis on Performance of Decision Tree Algorithms using Student\u2019s Qualitative Data. I.J.Modern Education and Computer Science, 2013, 5, 18-27.\n[9] Mohammed M. Abu Tair, Alaa M. El-Halees, Mining Educational Data to Improve Students\u2019 Performance: A Case Study.International Journal of Information and Communication Technology Research.Volume 2 No. 2, February 2012.\n[10] Sunita B.Aher, L.M.R.J. lobo, \u201cA Comparative study of classification algorithms\u201d,International Journal of Information Technology and Knowledge Management, July-December 2012, Volume 5, N0.2, pp 239-243.\n[11] Umesh Kumar Pandey and S. Pal, \u201cData Mining: A prediction of performer or underperformer using classification\u201d,(IJCSIT) International Journal of Computer Science and Information Technologies, Vol. 2 (2) , 2011,686-690.\n[12] Bhise R.B, Thorat S.S, Supekar A.K, \u201cImportance of Data Mining in Higher Education System\u201d, IOSR Journal Of Humanities And Social Science (IOSR-JHSS) ISSN: 2279-0837, ISBN: 2279-0845. Volume 6, Issue 6 (Jan. - Feb. 2013), PP 18-21.\n[13] K.Shanmuga Priya and A.V.Senthil Kumar,\u201d Improving the Student\u2019s Performance Using Educational Data Mining\u201d, Int. J. Advanced Networking and Applications Volume: 04 Issue: 04 Pages:1680-1685 (2013) ISSN : 0975-0290.\n[14] Varun Kumar, Anupama Chadha, \u201cMining Association Rules in Student\u2019s Assessment Data\u201d, IJCSI International Journal of Computer Science Issues, Vol. 9, Issue 5, No 3, September 2012.\n[15] Abeer Badr El Din Ahmed and Ibrahim Sayed Elaraby, \u201cData Mining: A prediction for Student's Performance Using Classification Method\u201d, World Journal of Computer Application and Technology 2(2): 43-47, 2014.\n[16] Quinlan, J.R. 1986, Induction of Decision trees, Machine Learning. [17] Anand Bahety,\u2019 Extension and Evaluation of ID3 \u2013 Decision Tree Algorithm\u2019. University of Maryland, College Park. [18] Mary Slocum,\u201f,Decision making using ID3,RIVIER ACADEMIC JOURNAL, VOLUME 8, NUMBER 2, FALL 2012. [19] Kumar Ashok, Taneja H C, Chitkara Ashok K and Kumar Vikas,\u2019 Classification of Census Using Information Theoretic Measure Based ID3 Algorithm\u2019 . Int. Journal of Math. Analysis, Vol. 6, 2012, no. 51, 2511 \u2013 2518. [20] Sonika Tiwari and Prof. Roopali Soni,\u2019 Horizontal partitioning ID3 algorithm A new approach of detecting network anomalies using decision tree\u2019, International Journal of Engineering Research & Technology (IJERT)Vol. 1 Issue 7, September \u2013 2012. [21] S. K. Yadav, B.K. Bharadwaj and S. Pal, \u201cData Mining Applications: A comparative study for Predicting Student\u2019s Performance\u201d, International Journal of Innovative Technology and Creative Engineering (IJITCE), Vol. 1, No. 12, pp. 13-19, 2011. [22]. U. K. Pandey and S. Pal, \u201cA Data Mining view on Class Room Teaching Language\u201d, IJCSI, Vol 8 issue 2 Maerch 2011 pg 277-282, ISSN 1694-0814.\n[23]. B.K. Bharadwaj and S. Pal. \u201cMining Educational Data to Analyze Students\u2019 Performance\u201d, International Journal of Advance Computer Science and Applications (IJACSA), Vol. 2, No. 6, pp. 63-69, 2011. [24] Q. A. AI-Radaideh, E. W. AI-Shawakfa, and M. I.AI-Najjar,\u201cMining student data using decision trees\u201d,International Arab Conference on Information Technology (ACIT\u20192006), Yarmouk University, Jordan, 2006. [25] K.Shanmuga Priya and A.V.Senthil Kumar,\u201d Improving the Student\u2019s Performance Using Educational Data Mining\u201d, Int. J. Advanced Networking and Applications Volume: 04 Issue: 04 Pages:1680-1685 (2013) ISSN : 0975-0290.\n[26] Alaa el-Halees \u201cMining students data to analyze e-Learning behavior: A Case Study\u201d, 2009. [27] J. Han and M. Kamber, \u201cData Mining: Concepts and Techniques,\u201dMorgan Kaufmann, 2000. [28] Varsha, Anuj, Divakar, R.C Jain , \u201cResult analysis using classification techniques\u201d, International Journal of Computer Applications (0975-8887) Volume 1-No. 22,2010. [29] S. Ayesha, T. Mustafa, A.R. Sattar, and M.I.Khan, Data Mining Model for Higher Education System, European Journal of ScientificResearch, Vol.43, No.1, pp.24-29, 2010. [30] D'mello, S.K., Craig, S.D., Witherspoon, A.W., McDaniel, B.T. and Graesser, A.C., \u201cAutomatic Detection of Learner\u2019s Affect from Conversational Cues.\u201d User Modeling and User- Adapted Interaction vol 18. pp. 45-80, 2008."}], "references": [{"title": "Educational data mining: A survey from 1995 to 2005\u201d.Expert Systems with Applications", "author": ["C. Romero", "S. Ventura"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Data Mining Application in Higher Learning Institutions", "author": ["N Delavari", "M.R. Beikzadeh"], "venue": "\u201cInformatics in Education,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Bichkar, \u201cPerformance Prediction of Engineering Students using Decision Trees \u201cInternational Journal of Computer Applications", "author": ["R.S.R.R. Kabra"], "venue": "Volume", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Early Prediction of Student Success: Mining Students Enrollment Data", "author": ["Zlatko J.Kovacic"], "venue": "Proceedings of Informing Science & IT Education Conference (InSITE)", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "A Comparative Analysis of Techniques for Predicting Academic Performance", "author": ["Nguyen Thai Nghe", "Paul Janecek", "Peter Haddawy"], "venue": "In Proceedings of the 37th ASEE/IEEE Frontiers in Education Conference. Pp", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Factors affecting students\u2019 performance: A Case Of Private Colleges", "author": ["Syed Tahir Hijazi", "S.M.M. Raza Naqvi"], "venue": "Bangladesh e-Journal of Sociology.Volume3.Number1,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "An Analysis on Performance of Decision Tree Algorithms using Student\u2019s Qualitative Data", "author": ["T.Miranda Lakshmi", "A.Martin", "R.Mumtaj Begum", "Dr.V.Prasanna Venkatesan"], "venue": "I.J.Modern Education and Computer Science,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "El-Halees, Mining Educational Data to Improve Students\u2019 Performance: A Case Study.International Journal of Information and Communication Technology Research.Volume 2 No", "author": ["Mohammed M. Abu Tair", "Alaa M"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "lobo, \u201cA Comparative study of classification algorithms\u201d,International", "author": ["Sunita B.Aher", "L.M.R.J"], "venue": "Journal of Information Technology and Knowledge Management,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Improving the Student\u2019s Performance Using Educational Data Mining", "author": ["K.Shanmuga Priya", "A.V.Senthil Kumar"], "venue": "Int. J. Advanced Networking and Applications Volume:", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Mining Association Rules in Student\u2019s Assessment Data", "author": ["Varun Kumar", "Anupama Chadha"], "venue": "IJCSI International Journal of Computer Science Issues,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Elaraby, \u201cData Mining: A prediction for Student's Performance Using Classification Method", "author": ["Abeer Badr El Din Ahmed", "Ibrahim Sayed"], "venue": "World Journal of Computer Application and Technology", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Induction of Decision trees, Machine Learning", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1986}, {"title": "Classification of Census Using Information Theoretic Measure Based ID3 Algorithm", "author": ["Kumar Ashok", "Taneja H C", "Chitkara Ashok K", "Kumar Vikas"], "venue": "Int. Journal of Math. Analysis,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Horizontal partitioning ID3 algorithm A new approach of detecting network anomalies using decision tree", "author": ["Sonika Tiwari", "Prof. Roopali Soni"], "venue": "International Journal of Engineering Research & Technology (IJERT)Vol. 1 Issue", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Data Mining Applications: A comparative study for Predicting Student\u2019s Performance", "author": ["S.K. Yadav", "B.K. Bharadwaj", "S. Pal"], "venue": "International Journal of Innovative Technology and Creative Engineering (IJITCE), Vol. 1, No. 12, pp. 13-19", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "A Data Mining view on Class Room Teaching Language\u201d, IJCSI, Vol 8 issue 2 Maerch", "author": ["U.K. Pandey", "S. Pal"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Mining Educational Data to Analyze Students", "author": ["B.K. Bharadwaj", "S. Pal"], "venue": "Performance\u201d, International Journal of Advance Computer Science and Applications (IJACSA),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "and M", "author": ["Q.A. AI-Radaideh", "E.W. AI-Shawakfa"], "venue": "I.AI-Najjar,\u201cMining student data using decision trees\u201d,International Arab Conference on Information Technology ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Improving the Student\u2019s Performance Using Educational Data Mining", "author": ["K.Shanmuga Priya", "A.V.Senthil Kumar"], "venue": "Int. J. Advanced Networking and Applications Volume:", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Data Mining: Concepts and Techniques,\u201dMorgan", "author": ["J. Han", "M. Kamber"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2000}, {"title": "T", "author": ["S. Ayesha"], "venue": "Mustafa, A.R. Sattar, and M.I.Khan, Data Mining Model for Higher Education System, European Journal of ScientificResearch, Vol.43, No.1, pp.24-29", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatic Detection of Learner\u2019s Affect from Conversational Cues.", "author": ["S.K. D'mello", "S.D. Craig", "A.W. Witherspoon", "B.T. McDaniel", "A.C. Graesser"], "venue": "User Modeling and User- Adapted Interaction", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "The educational data mining is a recent and popular area, there are many journals, ongoing books, workshops [1].", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "Delavari and Beikzadeh [2] give knowledge to use data mining methods in Higher learning institutions and define how data mining can be applied to the educational data.", "startOffset": 23, "endOffset": 26}, {"referenceID": 2, "context": "Bichkar [3] define that a model can be created using student\u2019s past-academic performance with the help of decision tree algorithm and this model can predict student\u2019s performance in the first year of engineering exam.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "Kovacic [5] show a case study for student\u2019s success prediction using educational data mining.", "startOffset": 8, "endOffset": 11}, {"referenceID": 4, "context": "[6] predict the performance of undergraduate and postgraduate students at two different institutes using decision tree and Bayesian network and this is used to find and helping failing students and determines scholarship.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Hijazi and Naqvi [7] show a case study on student performance.", "startOffset": 17, "endOffset": 20}, {"referenceID": 6, "context": "Prasanna Venkatesan [8] conduct a case study on student\u2019s qualitative data using decision tree algorithms to identify the effect of qualitative data in the performance of the student.", "startOffset": 20, "endOffset": 23}, {"referenceID": 7, "context": "El-Halees [9] conduct a case study on graduate students\u2019 data using data mining techniques to improve performance and extract useful knowledge from this data.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "lobo [10] conduct a comparative study to predict course selection using association rule algorithms.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "Senthil Kumar [13] use a classification method that helps to improve the performance & extract the knowledge from student\u2019s final semester marks.", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "Varun Kumar and Anupama Chadha [14] used association rule technique to improve the performance of postgraduate students.", "startOffset": 31, "endOffset": 35}, {"referenceID": 11, "context": "Abeer Badr El Din Ahmed and Ibrahim Sayed Elaraby [15] use decision tree technique for data classification that is helpful for predicting the student\u2019s final grade.", "startOffset": 50, "endOffset": 54}, {"referenceID": 12, "context": "R Quinlan summarizes an approach and describes ID3 and this was the first research work on ID3 algorithm [16].", "startOffset": 105, "endOffset": 109}, {"referenceID": 13, "context": "In the decision tree a property on the basis of calculation is selected as the root of the tree and this process\u2019s steps are repeated [19].", "startOffset": 134, "endOffset": 138}, {"referenceID": 14, "context": "She checks the network anomalies from the decision tree then she discovers the comparative analysis of different clustering algorithms and existing id3 decision tree [20].", "startOffset": 166, "endOffset": 170}, {"referenceID": 15, "context": "Yadav, Bharadwaj and Pal [21] obtained the students data such as attendance, seminar, assignment marks and class test to predict the end semester performance using three algorithms ID3decision tree, C4.", "startOffset": 25, "endOffset": 29}, {"referenceID": 16, "context": "15 Pandey and Pal [22] show their study using association rule analysis to find the student interest of choosing class language.", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "Bharadwaj and Pal [23] use the classification decision tree technique to evaluate student\u2019 end semester performance, this study helps to identify the dropouts and students who require special attention and teacher advising.", "startOffset": 18, "endOffset": 22}, {"referenceID": 18, "context": "et al [24] presents a classification based model for student performance prediction using ID3 algorithm,C4.", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "kumar [25] use a classification approach that extracts the knowledge from student end semester marks.", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "Han and Kamber [27] describes data mining software that allow the users to analyze data from different views, and summarize these relationships which are identified during the mining process.", "startOffset": 15, "endOffset": 19}, {"referenceID": 21, "context": "Inayat Khan [29] applied K-mean clustering to analyze learning behavior of students which will help the tutor to improve the performance of students and reduce the dropout ratio to a significant level.", "startOffset": 12, "endOffset": 16}, {"referenceID": 22, "context": "D\u2019Mello [30] studied on bored and frustrated student.", "startOffset": 8, "endOffset": 12}], "year": 2014, "abstractText": null, "creator": null}}}