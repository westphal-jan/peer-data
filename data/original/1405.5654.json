{"id": "1405.5654", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2014", "title": "Machine Translation Model based on Non-parallel Corpus and Semi-supervised Transductive Learning", "abstract": "Although the parallel corpus has an irreplaceable role in machine translation, its scale and coverage is still beyond the actual needs. Non-parallel corpus resources on the web have an inestimable potential value in machine translation and other natural language processing tasks. This article proposes a semi-supervised transductive learning method for expanding the training corpus in statistical machine translation system by extracting parallel sentences from the non-parallel corpus. This method only requires a small amount of labeled corpus and a large unlabeled corpus to build a high-performance classifier, especially for when there is short of labeled corpus. The experimental results show that by combining the non-parallel corpus alignment and the semi-supervised transductive learning method, we can more effectively use their respective strengths to improve the performance of machine translation system.", "histories": [["v1", "Thu, 22 May 2014 07:58:56 GMT  (321kb)", "http://arxiv.org/abs/1405.5654v1", "15 pages"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lijiang chen"], "accepted": false, "id": "1405.5654"}, "pdf": {"name": "1405.5654.pdf", "metadata": {"source": "META", "title": "Machine Translation Model based on ", "authors": [], "emails": [], "sections": [{"heading": null, "text": "translation, its scale and coverage is still beyond the actual needs. Non-parallel corpus\nresources on the web have an inestimable potential value in machine translation and\nother natural language processing tasks. This article proposes a semi-supervised\ntransductive learning method for expanding the training corpus in statistical machine\ntranslation system by extracting parallel sentences from the non-parallel corpus. This\nmethod only requires a small amount of labeled corpus and a large unlabeled corpus\nto build a high-performance classifier, especially for when there is short of labeled\ncorpus. The experimental results show that by combining the non-parallel corpus\nalignment and the semi-supervised transductive learning method, we can more\neffectively use their respective strengths to improve the performance of machine\ntranslation system.\nKey Words: Machine Translation; Non-parallel Corpus; Semi-supervised\nTransductive Learning; Bilingual Alignment"}, {"heading": "1 Introduction", "text": "For most of pairs of languages, the available parallel corpus is very limited. Even with many bilingual corpora, the language pair of English and French is also confined in specific areas, such as political speech. So when training corpus in a particular field for a machine translation system, it is hard to translate the text in different fields.\nIn order to solve this problem, we turned to a comparable non-parallel corpus. Comparable Corpus contains two corpora in different language that are not parallel in the strict sense, but they are related, and often convey the same information. Such as Xinhua News, CNN, the BBC, which often contain sentence pairs that can be used as the translations.\nNon-parallel corpus is also known as comparable corpus formed by the texts of two languages with some similarities. Such as: News belong to different sites on the same day and have the same theme both in Chinese and English. Non-parallel corpus often has texts in two languages which may be the relevant reports of the same event written by different writers using there native languages, so they are comparable. In the strict sense, non-parallel corpus is not parallel, but the messages that the two texts delivered is relevant, and the words in sentences may have different orders. For it is easy to obtain from the Internet, the non-parallel corpus has been used instead of the parallel corpus as the training corpus of machine translation. Dragos Stefan Munteanu,\net al (2005) studied how to align sentences of the non-parallel corpus for a machine translation system. They first obtain the candidate translations from the non-parallel corpus by the method of information retrieval, and then using the maximum entropy classifier to accurately determine whether a sentence pair is corresponding translations. The experimental results show that the data obtained can effectively improve the accuracy of statistical machine translation.\nBased on the assumption that the information provided by the non-parallel\ncorpus can improve the accuracy of machine translation, we propose a semi-supervised transductive learning framework. \u201cTransductive\u201d means we repeat to translate the source language in the non-parallel corpus, then according to the translation results, we select the corresponding translations from the target language and source language sentences together to form parallel sentences in order to improve the accuracy of statistical machine translation. This approach also allows the machine translation system to adapt to texts of a new areas."}, {"heading": "2 Sentence alignment of bilingual corpus", "text": "Before applying bilingual corpus to machine translation, we has to determine the corresponding paragraphs, sentences, or even phrases, single words of the source and target languages which are mutual translations. This process is known as the \"alignment\". In general, the unit for collecting corporal is the chapter, so the first step of processing bilingual corpora is to align sentence.\nIn the earlier researches of sentence alignment, we generally compare the length of sentences in two languages. The basic idea of this method is that the long sentence of the source language corresponds to the long sentences of the target language, and the short sentence of the source language corresponds to the short sentence of the target language. There are proportional relationships between the sentence length of two languages. Calculating the length of the sentence has two methods: measure the number of characters or the number of words. Both methods achieved good alignment results in the Canadian Hansards bilingual corpus of British and French. Peter F. Brown (1991) uses the word as the unit to calculate the length. He selected 90% sentences of the Canadian Hansards corpus that is easier to handle. The alignment accuracy rate achieved to 99.4%. William A. Gale (1993) used characters as the calculation unit of length. The alignment accuracy achieved to 98% on all the sentences of Canadian Hansards corpus. And for the 80% part of it which is easier to handle, the alignment accuracy achieved to 99.6%. But the alignment method based on the length is not suitable for all languages. For two languages (such as English and Chinese) in different linguistic families, the results are not satisfactory that the translations are likely to be missed (Liu Xin, 1995). Dekai Wu (1994) aligned Hong Kong Hansards Chinese-English bilingual corpus with this method. The accuracy rate is only 86.4%.\nAnother sentence alignment method is based on the vocabulary. Martin Kay, et\nal (1993) proposed that if the distribution of the two words is the same, then they are the corresponding unit. That is if a word occurs in the vast majority of a sentence, its\ncorresponding word must also appear in the corresponding sentence. They use American Articles as the corpus. After four iterations, 96% of the sentences will be aligned. Stanley F. Chen et al (1993) adopted the EM iterative algorithm. According to the word-to-word translation model, they use the dynamic programming method to determine the optimal alignment mode. With the emergence of bilingual dictionaries, bilingual dictionaries are used to directly align sentences. Yang Muyun(2002) accomplishes the bilingual sentence alignment method based on bilingual dictionaries in English and Chinese, and the accuracy rate achieves 93%. Takehito Utsuro (1994) combined bilingual dictionaries and statistical methods to build a bilingual alignment framework. In addition, Dekai Wu (1994) proposed a hybrid-aligned technology by combining vocabulary and length. He tested his system in the Hong Kong Hansards Chinese-English bilingual corpus, and the accuracy rate achieves 92.1%."}, {"heading": "3 semi-supervised machine learning methods", "text": "Machine Learning is the core of the artificial intelligence research, which usually refers to that through the induction and synthesis approach the computer simulates the human learning ability, constructs its models and algorithms to acquire new knowledge or skills from the known sample, and re-organizes existing learning structure so as to continuously improve their performance. Although the mechanism of human learning is not yet clear, by the induction of calculation model, the system can automatically make adaptive changes to complete the complex tasks which only human beings can complete originally.\nMachine learning includes the supervised learning and the unsupervised learning. In traditional supervised machine learning, the learning algorithm training the model on the labeled sample set given by the outside world, inductive parameters from the sample set, and label the unknown data set based on the knowledge learned (by Trevor Hastie, 2005). Most of the classification methods are supervised machine learning. However, in many applications, labeling the sample set often requires a lot of artificial and financial resources, and the process is tedious and consistency that the quality can not be guaranteed. Because the model overfits the training data, the traditional supervised learning methods are often unable to build classifiers to meet the actual demand. Different from the supervised learning, unsupervised learning methods do not need data with labels, and the number of classes to learn may not know in advance, such as clustering and the EM algorithm.\nSemi-supervised learning is a kind of area between supervised learning and supervised learning. It requires a small labeled training data set, and an unlabeled training data set. The labeling algorithm is as follows (Ian H. Witten, 2006):\n(1) use of labeled data to train a classifier; (2) the classifier is applied to unlabeled data, gives each element a category\naccording to the class probability;\n(3) adding the new annotation data to the training set; (4) use the new training set to train a new classifier; (5) iterate until convergence."}, {"heading": "4 Related Research", "text": "Unsupervised learning method was first applied to the word alignment. Chris\nCallison -Burch, et al (2004) use unsupervised learning on a sentence-aligned parallel texts to acquire a word alignment model, and also use the unsupervised method to train another model with a small artificial word-aligned corpus. They combine the two models as word alignment statistical data, reduce the alignment error rate and improve the performance of the machine translation system. Alexander Fraser, et al (2006) use a semi-supervised learning methods on the basis of IBM model 4: use the discriminant step to improve the quality of word alignment in a small corpus that are manually aligned, apply the estimated step and maximize step alternatively in a large-scale training corpus. This method improves the translation accuracy rate of the phrase-based statistical machine translation system.\nCo-training method is a weakly supervised learning method. Chris\nCallison-Burch (2002) introduces co-training to statistical machine translation. This approach requires a variety of source language aligned with the same target language sentence as a small amount of manually annotated initial data. They use the initial small-scale manually annotated data to obtain the large-scale annotated corpus by iteration. For source language sentence of each language, they use different translation models to obtain the target language candidates, and then select the best from the candidates as the final translation.\nNicola Ueffing (2007) proposes an transductive and adaptive training model for\nstatistical machine translation. They use a phrase-based machine translation system to translate the source language sentences, translations with a confidence score above the threshold are added to the training set. \"Transductive\" means repeatedly translating sentence in development set and test set, and using the generated translations to gradually increase the performance of statistical machine translation system. The model selection step discards bad translations, and enhances the quality of the phrase translation table. Their experiments show that using the selection can obtain better results than adding all the translations to the system."}, {"heading": "5 System framework", "text": "In this paper, we propose a semi-supervised transductive learning method based on non-parallel corpus. Instead of using monolingual corpus of the source language (Nicola Ueffing, 2007), we use the non-parallel bilingual corpus that can be obtained free from the Internet. Transductive learning is a semi-supervised learning method. When lack of parallel corpus, we can expand parallel corpus gradually by iteration in order to improve the system performance. Transductive learning translates the monolingual corpus by the current model and selects the translations with higher score. The selected translations are combined with their source language sentences to form a sentence pair which are added to the training corpus. Then the expanded\ncorpus is used to train a new model to get better translations. After a dozen or even hundreds of iterations, the system performance will be gradually improved, and ultimately converge. In transductive learning, how to choose the translations with a higher accuracy rate is the key to success. Ueffing scores the translations by the priori probabilities of words and phrases and the target language model. This is a self-learning method. By discarding low-scoring translation, we can further improve the quality of the translation model.\n(1) Input: a sentence-aligned parallel training set L (2) Input: bilingual non-parallel corpus U (3) Input: number of iterations R , the value of the bestn  list\n(4) {}:1 T / / Additional bilingual training corpus\n(5) 0:i / / number of iterations (6) repeat\n(7) Training Steps: ),(: 1 )(  i i TLEstimate / / )(i for the system parameters\n(8) {}:iX / / the generated translation set of this iteration\n(9) for each source language sentence Us\n(10) Labeling Step: use )(i decoding s, compare with the corresponding target\nlanguage text of non-parallel corpus, and get N sentences with the best scores.\n(11)         N nnn i nnii ststXX 1 )( ))|(,,(: \n(12) end\n(13) Scoring Step: )(: ii XScoreS  / / give a score to each sentence in X\n(14) Selecting Steps: ),(: iii SXSelectT  / / select a subset of X including good\ntranslations. (15) 1:  ii (16) until Ri \nAlgorithm 1: semi-supervised transductive learning of non-parallel corpus\nOn the basis of Nicola Ueffing (2007), we propose a transductive learning method on non-parallel corpus. Different form Nicola Ueffing (2007), we compare the translations of the model with the target language in non-parallel corpus, and select correct ones adding to the training corpus. The selection process is no longer dependent on the language model and the priori probability. Instead it uses the translations of the current model as a scoring scale to align source language sentences and target language sentences in non-parallel corpus of in order to obtain new parallel sentences. This approach can take full advantage of the translation results of the\ncurrent model. Moreover, the parallel sentences added the training corpus to come from the corpus itself, so the translations have the higher accuracy in favor of the system performance improvement. Transductive Learning on non-parallel corpus for the first time consider the sentence alignment and the translation system as a unified whole. Sentence alignment is based on translations produced by the translation model, and updating the translation model is based on results of sentence alignment. The specific algorithm is shown in Algorithm 1."}, {"heading": "6 System Implementation", "text": ""}, {"heading": "6.1 The method for candidate parallel sentence acquisition", "text": "Through the search engines of some portals, we can find large quantities of the corresponding Chinese and English news by several designed keywords. However, without the corresponding relationship of web pages in URL, when aligning chapters, the news cannot be directly regarded as a non-parallel corpus. Referring to Dragos Stefan Munteanu (2005), we firstly segment the Chinese web pages, and then convert each Chinese word into the corresponding English translation by bilingual dictionaries to generate English query documents.\nWe use Lemur information retrieval tools (http://www.lemurproject.org/) to retrieve the English documents, and return the first 20 English documents corresponding to a Chinese document. These results are returned as candidate non-parallel corpus for transductive learning."}, {"heading": "6.2 Scoring Steps", "text": "According to the training model of the current step, we decode the source language texts in non-parallel. By comparing the acquired translations and the target language in non-parallel corpus, the similarity degrees are calculated as the score to decide whether a target language sentence is the corresponding translation. Since the training data are little at the start, the translation accuracy is not high, and the similarity of the translation and target translation is low, we considered the following factors synthetically: (1) the ratio between the translation length and the target language sentence length (2) the number of the same or similar words between the translation and the target language sentence (3) the morphological changes of English words (4) proper nouns (place or names) matched (5) number matched\nAccording to the importance, all these factors are added with weights as the total\nscore."}, {"heading": "6.3 Selecting Step", "text": "By comparing with the translations of candidate parallel corpus, the selected\nEnglish sentences have to meet the following two conditions to compose the sentence pairs with translations.\n(1) the score must be higher than the preset threshold; (2) the score must be the highest in all matching sentences. The threshold setting directly impacts on the quantity and quality of translations.\nMoreover, since the number of matched translation sentences may change with the length of the sentence, for different lengths of English sentences, we set a different threshold."}, {"heading": "6.4 non-parallel corpus example", "text": "From Xinhua news sites, we get a total of 994 Chinese-English texts. The following lists one of them.\n\u3010text\u3011\nChinese\uff0825 sentences\uff09\uff1a 1. \u4e9a\u6d32 \u80a1\u5e02 \u53d7 \u7f8e \u9f13\u821e \u661f\u671f\u4e8c \u5f3a\u52b2 \u53cd\u5f39 \u3002 2. \u4e9a\u6d32 \u8bc1\u5238 \u5e02\u573a \u8fde\u7eed \u7b2c\u4e8c \u5929 \u80a1\u5e02 \u4e0a\u626c \u3002 3. \u6295\u8d44\u8005 \u4e50\u89c2 \u5730 \u8ba4\u4e3a \uff0c \u5404\u56fd \u653f\u5e9c \u7684 \u6551 \u5e02 \u52aa\u529b \u5c06 \u633d\u6551 \u5904\u4e8e \u75c5\n\u6001 \u7684 \u5168\u7403 \u91d1\u878d \u4f53\u7cfb \u3002\n4. \u65e5\u672c \u80a1\u5e02 \u66b4\u6da8 \u767e\u5206\u4e4b 14 \u3002 5\uff0e \u5728 \u6fb3\u5927\u5229\u4e9a \uff0c \u6295\u8d44\u8005 \u53d7\u5230 \u9646 \u514b \u6587 \u603b\u7406 \u7528 \u6570\u5341\u4ebf \u7f8e\u5143 \u52a0\u5f3a \u7ecf\n\u6d4e \u7684 \u8ba1\u5212 \u7684 \u9f13\u821e \u3002\n6. \u661f\u671f\u4e8c \uff0c \u65e5\u672c \u80a1\u5e02 \u662f \u4e9a\u6d32 \u5404\u5730 \u80a1\u5e02 \u4e2d \u4e0a\u6da8 \u5e45\u5ea6 \u6700 \u5927 \u7684 \uff0c\n\u65e5\u7ecf\u6307\u6570 \u66b4\u6da8 \u767e\u5206\u4e4b 14 \uff0c \u4e00 \u5171 \u6da8 \u4e86 1100\u767e \u591a \u70b9 \u3002\n7. \u4e0a \u661f\u671f\u4e94 \uff0c \u65e5\u672c \u80a1\u5e02 \u66b4\u8dcc \u5c06\u8fd1 \u767e\u5206\u4e4b 10 \u3002 8. \u4eca\u5929 \u662f \u6709\u53f2\u4ee5\u6765 \u65e5\u672c \u80a1\u5e02 \u6da8\u5e45 \u6700 \u5927 \u7684 \u4e00 \u5929 \u3002 9. \u6295\u8d44\u8005 \u7684 \u4e50\u89c2 \u60c5\u7eea \u5728 \u6574\u4e2a \u4e1c\u4e9a \u5230\u5904 \u53ef\u89c1 \u3002"}, {"heading": "10. \u5728 \u97e9\u56fd \u3001 \u83f2\u5f8b\u5bbe \u548c \u5370\u5ea6\u5c3c\u897f\u4e9a \uff0c \u80a1\u5e02 \u90fd \u7a9c \u5347 \u767e\u5206\u4e4b 6 \u4ee5", "text": "\u4e0a \uff0c \u6fb3\u5927\u5229\u4e9a \u548c \u9999\u6e2f \u7684 \u5173\u952e \u6307\u6570 \u90fd \u4e0a\u5347 \u4e86 \u767e\u5206\u4e4b 3 \u4ee5\u4e0a \u3002"}, {"heading": "11. \u4e0a\u661f\u671f \u6700\u540e \u4e24 \u5929 \uff0c \u6fb3\u5927\u5229\u4e9a \u80a1\u5e02 \u76f4\u7ebf \u4e0b\u8dcc \uff0c \u653f\u5e9c \u51b3\u5b9a \u7528 70", "text": "\u591a \u4ebf \u7f8e\u5143 \u9632\u6b62 \u7ecf\u6d4e \u8870\u9000 \uff0c \u5bfc\u81f4 \u80a1\u5e02 \u56de\u5347 \u3002"}, {"heading": "12. \u8fd9\u4e9b \u8d44\u91d1 \u5c06 \u6765\u81ea \u9884\u7b97 \u76c8\u4f59 \uff0c \u8fd9\u4e9b \u76c8\u4f59 \u4e3b\u8981 \u662f \u9760 \u91c7\u77ff \u4e1a \u7684", "text": "\u7e41\u8363 \u79ef\u7d2f \u8d77\u6765 \u7684 \u3002"}, {"heading": "13. \u6fb3\u5927\u5229\u4e9a \u628a \u5927\u91cf \u77ff\u7269 \u51fa\u53e3 \u5230 \u4e2d\u56fd \u548c \u5370\u5ea6 \u3002", "text": "14. \u4e00\u4e9b \u7ecf\u6d4e\u5b66\u5bb6 \u8bf4 \uff0c \u523a\u6fc0 \u7ecf\u6d4e \u8ba1\u5212 \u53ef\u80fd \u4e0d\u8db3\u4ee5 \u9632\u6b62 \u6fb3\u5927\u5229\u4e9a \u6ed1\n\u5165 \u8870\u9000 \u3002\n15. \u5728 \u8fd9 \u4e4b\u524d \uff0c \u6fb3\u5927\u5229\u4e9a \u7ecf\u5386 \u4e86 17 \u5e74 \u7684 \u8fde\u7eed \u589e\u957f \u3002"}, {"heading": "16. \u6fb3\u5927\u5229\u4e9a \u603b\u7406 \u9646 \u514b \u6587 \u53d1\u8868 \u5168\u56fd \u8bb2\u8bdd \uff0c \u4ed6 \u5bf9 \u672a\u6765 \u8868\u793a \u4e50", "text": "\u89c2 \uff0c \u4f46\u662f \u4ed6 \u8bf4 \uff0c \u5fc5\u987b \u7acb\u5373 \u91c7\u53d6 \u884c\u52a8 \u3002\n17. \u201c \u5168\u7403 \u91d1\u878d \u5371\u673a \u8fdb\u5165 \u4e86 \u4e00\u4e2a \u65b0 \u7684 \u3001 \u5371\u9669 \u7684 \u3001 \u7834\u574f\u6027 \u7684 \u9636\n\u6bb5 \u3002\n18. \u5371\u673a \u5df2\u7ecf \u6ce2\u53ca \u5b9e\u9645 \u7684 \u7ecf\u6d4e \u9886\u57df \uff0c \u589e\u957f \u548c \u5c31\u4e1a \u90fd \u53d7\u5230 \u5f71\n\u54cd \u3002"}, {"heading": "19. \u8fd9 \u5c31\u662f \u4e3a\u4ec0\u4e48 \u653f\u5e9c \u51b3\u5b9a \u53ca\u65e9 \u679c\u65ad \u884c\u52a8 \uff0c \u91c7\u53d6 \u9488\u5bf9 \u672a\u6765 \u7684 \u7ecf", "text": "\u6d4e \u5b89\u5168 \u6218\u7565 \u3002"}, {"heading": "20. \u5b83 \u5c06 \u5e2e\u52a9 \u6211\u4eec \u5de9\u56fa \u672a\u6765 \u7684 \u7ecf\u6d4e \u589e\u957f \uff0c \u4e3a \u5343\u5bb6\u4e07\u6237 \u7684 \u672a\u6765", "text": "\u63d0\u4f9b \u5b9e\u9645 \u7684 \u652f\u6301 \u3002"}, {"heading": "21. \u4e9a\u6d32 \u80a1\u5e02 \u56de\u5347 \u4e4b\u524d \uff0c \u7f8e\u56fd \u80a1\u5e02 \u9053 \u743c \u65af \u6307\u6570 \u661f\u671f\u4e00 \u4e0a\u5347 \u4e86", "text": "\u767e\u5206\u4e4b 11 \u4ee5\u4e0a \u3002"}, {"heading": "22. \u8fd9 \u662f 1933\u5e74 \u4ee5\u6765 \u5347\u5e45 \u6700 \u5927 \u7684 \u4e00 \u5929 \u3002", "text": ""}, {"heading": "23. \u6295\u8d44\u8005 \u5bf9 \u7f8e\u56fd \u653f\u5e9c \u5411 \u94f6\u884c \u4f53\u7cfb \u6ce8\u8d44 \u4f5c\u51fa \u53cd\u5e94 \u3002", "text": ""}, {"heading": "24. \u6ce8\u8d44 \u662f \u4e3a\u4e86 \u523a\u6fc0 \u8d37\u6b3e \u3002", "text": ""}, {"heading": "25. \u7531\u4e8e \u5168\u7403 \u4fe1\u8d37 \u51bb\u7ed3 \uff0c \u9020\u6210 \u73b0\u91d1 \u6765\u6e90 \u67af\u7aed \uff0c \u94f6\u884c \u65e0\u6cd5 \u8d37\u6b3e \u3002", "text": "English\uff0815 sentences\uff09\uff1a 1. Asian stocks rally as optimism grows over global rescue plans 2. Asian stock markets have risen for a second consecutive day on optimism that\ngovernment rescue efforts will heal the stricken global financial system .\n3. Japanese stocks soared by 14 percent . 4. In Australia , investors were encouraged by prime minister Kevin Rudd \u2019 s plan to\nspend billions of dollars to strengthen the economy .\n5. Japan lead the gains in Asian markets tuesday as the benchmark Nikkei 225 index\nsurged by more than 1,100 points , or 14 percent - a stunning reversal after plunging nearly 10 percent friday .\n6. It was the Nikkei \u2019 s biggest single-day gain in history . 7. The positive mood has been repeated across the region . 8. Stock prices in South Korea , the Philippines and Indonesia jumped by more than\nsix percent , and key indexes in Australia and Hong Kong rallied more than three percent . 9. The recovery in Australian stocks , which went into freefall at the end of last\nweek , followed a decision by the government to spend more than $ 7 billion to try to stave off recession . 10. The funds will come from the budget surplus , which has been amassed largely on\nthe back of a mining boom that has seen vast quantities of australian minerals exported to China and India . 11. Some economists say the stimulus package may not be enough to prevent\nAustralia slipping into a recession after 17 years of uninterrupted growth .\n12. In a nationwide address , Australian Prime Minister Kevin Rudd was optimistic\nabout the future , but said that immediate action was needed .\n13. \" The global financial crisis has entered into a new , dangerous and damaging\nphase , one which goes to the real economy , growth and jobs , \" said Mr. Rudd .\n\" And that is why the government has decided to act decisively and early on the question of this economic security strategy for the future ; an economic security strategy to help underpin positive economic growth into the future and to provide practical support for households . \" 14. The recovery of Asian stock markets followed a surge in the dow jones industrial\naverage , which gained more than 11 percent monday - its biggest one-day gain since 1933 . 15. Investors were reacting to efforts by the U.S. government to inject capital into\nbanking system to stimulate lending , which has dried up in the global credit meltdown .\nThe above report is a non-parallel text. The contents of English and Chinese versions describe a same news event. However the English and Chinese sentences are not strictly one-to-one correspondent, and sometimes an English sentence corresponds to a number of Chinese sentences. The corresponding relations are shown in Table 1. In the report, The Chinese part has a total of 25 sentences, while the English part has 15. Based on text analyzing, there are two reasons for different numbers of sentencese between English part and Chinese part: (1) the contents in Chinese and English are not entirely consistent (2) there are some long sentences in the English part. However, in the above texts, there are a number of corresponding pairs of parallel sentences, such as:\n\uff081\uff09 \u6295\u8d44\u8005 \u7684 \u4e50\u89c2 \u60c5\u7eea \u5728 \u6574\u4e2a \u4e1c\u4e9a \u5230\u5904 \u53ef\u89c1 \u3002\nThe positive mood has been repeated across the region.\n\uff082\uff09 \u65e5\u672c \u80a1\u5e02 \u66b4\u6da8 \u767e\u5206\u4e4b 14 \u3002\nJapanese stocks soared by 14 percent .\nIn these sentence pairs, some are corresponding in the strict sense, such as (2);\nbut some are only roughly close in meaning, such as (1). However, whether or not they are the parallel sentence in the strict sense, as long as with the high similarity, they can be used as a useful supplement for machine translation system training corpus. In the sentences, there are some corresponding phrases, which can be added to the phrase translation table of the machine translation system.\n(1):\n(2):"}, {"heading": "6.5 The Acquisition of parallel sentence pairs", "text": "In semi-supervised transductive system on non-parallel corpus, we have to filter out the truly parallel sentences from the candidate parallel sentence pairs. For example, the sentence \u201c\u7f8e\u56fd \u603b\u7edf \u5e03\u4ec0 \u8bf4 \uff0c \u7f8e\u56fd \u6b63\u5728 \u8ddf \u5176\u5b83 \u56fd \u5bb6 \u5408\u4f5c \u6062\u590d \u56fd\u9645 \u91d1\u878d \u5e02\u573a \u7684 \u529b\u91cf \u548c \u7a33\u5b9a \u3002\u201d is translated by the initial translation system as:\nThe president bush (former us president) said, of the american wrestling with the\nother countries to work together to international financial market-place of the power\nand stabilization.\nThe translations \u201cpresident bush\u201d \u201cother countries\u201d \u201dinternational\nfinancial\u201d \u201dand\u201d appear in the corresponding English sentence. Moreover, the translation sentence and the English sentence are all included in the corresponding text of the news. Therefore, we can determine that they are likely to be corresponding sentence pair.\nWe compare the translation with the English sentence in non-parallel corpus, and\nfigure out the number of the same n-gram words. The higher the number, the higher the coefficient is. Then the result is divided by the length factor, to get the sentence similarity score.\nl e n g t h\n212211 C\n\n \u4e13\u6709\u540d\u8bcd\u6570\u5b57\u53e5\u5b50\u76f8\u4f3c\u5ea6   CCCC nn\n(2.1)\nAmong them, 1 \u3001 2 \u2026 n are adjustment parameters for 1-gram, 2-gram ...\nn-gram language model which can be adjusted according to actual situation\n( n  21 ). 1 And 2 are the adjustment parameters of the numbers and\nproper nouns. length Is the length factor.\n\u4e50\u89c2 <\u2014\u2014> positive \u60c5\u7eea <\u2014\u2014> mood \u5230\u5904\u53ef\u89c1 <\u2014\u2014> has been repeated across\n\u65e5\u672c <\u2014\u2014> Japanese \u80a1\u5e02 <\u2014\u2014> stocks \u66b4\u6da8 <\u2014\u2014> soared \u767e\u5206\u4e4b <\u2014\u2014> percent"}, {"heading": "7 Experimental Results", "text": "Because there are less non-parallel corpus that can be obtained from the Internet, so\nwe only carry out the experiment of small-scale transductive learning. The initial\ncorpus of the experiments is 1000 sentences selected from the aligned bilingual\ncorpus parallel produced by Institute of Computing Technology, Chinese Academy of\nSciences. Non-parallel corpus contains 994 Chinese-English news obtained from the\nXinhua net, including 12,326 Chinese sentences and 283,058 English sentences. In\naddition, the parallel corpus contains 187,014 translation dictionaries. Language\nmodel uses a large-scale English monolingual corpus, which contains 113,723 English\nsentences and 1,301,274 English words.\nTable 2 The evaluation results of transductive learning on non-parallel\ncorpus-based\nIteration parallel\ncorpus size\nNIST2008 CWMT2008\nNIST BLEU NIST BLEU\n0 (baseline) 1000 4.1758 6.63 4.3050 6.24\n1 1175 4.1439 6.67 4.3111 5.91\n2 1303 4.0772 6.93 4.3542 6.39\n3 1388 4.0836 7.18 4.3117 6.26\n4 1455 3.9948 7.05 4.0275 6.21\n5 1588 4.0683 7.11 4.3355 6.41\n6 1545 4.0175 7.07 4.2388 6.58\n7 1573 4.0228 7.05 4.3093 6.46\n8 1622 4.0245 6.93 4.2611 6.47\n9 1763 3.9580 6.93 4.3099 6.39\n10 1653 4.0693 7.01 4.3359 6.54\nThe parameter training corpus is Chinese-English translation corpus provided by\nNIST2008 evaluation, including a total of 691 Chinese sentences. Each Chinese\nsentence corresponds to four English translations. for the evaluation of system\nperformance, we not only use NIST2008 test set, but also the English test corpus\nproduced by the 4 th machine translation seminar (CWMT2008), in which there are\n1006 Chinese sentences.\nTable 2 shows the experimental results of the semi-supervised transductive\nlearning on non-parallel corpus after 10 iterations (including the initial system).After\n10 iterations, the size of the parallel corpus gradually expand, and the number of the\nsentence increases from the 1000 to 1653. Among them, 653 parallel sentence pairs\nare obtained by the transductive learning. The BLEU scores of the two test sets are\nimproved significantly. On NIST2008 test set, the BLEU score is 0.38, with an\nincrease of 5.73%. The NIST score and BLEU score of CWMT2008 are also\nsignificantly increased by 0.0309 and 0.30 respectively, and the increase rate is 0.72%\nand 4.81% respectively. However, the best result obtained by the experiment is not in\nthe 10 th round, but in the 2 nd , 3 rd and 6 th rounds. The reason for this phenomenon is\ndue to the parallel corpus obtained by the transductive learning may not be necessarily\nbeneficial for improving the performance of the system, which also contains some\nnon-corresponding sentences. For example in section 8, the sentence \"\u7f8e\u56fd \u4e0e \u6d2a\u90fd\n\u62c9\u65af \u591a\u5e74 \u6765 \u5173\u7cfb \u5bc6\u5207 \u3002\" is translated as \"U.S. embassy in Honduras many\nyears in. the close relations.\" The System mismatches it with the English sentence\n\"U.S. embassy in Honduras calls for peace through dialogue\", for the original\ntranslation and the English sentence have 4 fully matching words (including proper\nnouns), and the length of the sentence is short.\nSynthesizing all the experimental results, except for the NIST score on\nNIST2008, the BLEU score on NIST2008 and the BLEU score and the NIST score on\nCWMT2008 has improved significantly. The experimental results show that in the\ncase of small training corpus the semi-supervised transductive learning can extract\nparallel sentence pairs from the non-parallel corpus, gradually increase the size of the\ntraining data, so as to improve the performance of the machine translation systems."}, {"heading": "8 the significance and limitations for", "text": "non-parallel corpus used in machine\ntranslation\nThe use of non-parallel corpus opens up a new idea for training data acquisition\nin statistical machine translation. The semi-supervised transductive learning methods\non non-parallel can gradually increase the amount of parallel sentences for machine\ntranslation systems in the absence of a ready-made training corpus. Different from\nadding sentence pairs with higher score to the training corpus by Nicola Ueffing et al\n(2007), the translations with high similarities are better for improving performance of\nthe machine translation system. Some parallel sentence pairs contains many valuable\ncorresponding translations, which is same as the parallel corpus requiring a lot of\nmanpower and resources.\nHowever, there are still some difficulties and shortcomings in practical\napplications for non-parallel corpus:\n(1) In non-parallel corpus, real parallel sentence pairs are relatively small,\nincreasing the difficulty to automatically obtain from the system translations. From\n994 non-parallel corpora, we can only extract 763 pairs of parallel sentences, in which\nsome are not in strict sense of the corresponding. Therefore, enabling the machine\ntranslation system performance improved significantly requires a lot of non-parallel\ncorpus.\n(2)When extracting the corresponding parallel sentence pairs from the\nnon-parallel corpus, due to the defects of the initial translation, the system will map\nsome sentence pairs which are in some sense similar, but not parallel sentence pairs.\nThis makes a lot of noise in the training corpus, is bad for us to improve the\ntranslation system performance.\n(3) The non-parallel corpus online which can be take advantage of is limited to\nnews. Other areas of non-parallel corpus is almost invisible. So the use of non-parallel\ncorpus has strict field restrictions.\nEven so, for constructing the parallel corpus is time-consuming and laborious,\nthe prospect of non-parallel corpus is still very impressive. Extracting parallel\nsentence pairs from non-parallel corpus can be used for machine translation systems\nresources acquisition as a new direction. At present, the demand for bilingual corpus\nis the growing, and the use of the bilingual corpus has spread to word segmentation,\nPOS tagging, parsing, information retrieval and text classification. Therefore, we\nshould make best use of the resources available online to obtain more parallel corpus.\nNon-parallel corpus is not readily available bilingual corpus. Researchers can use the\ncomputer automatic or semi-automatic method to turn the non-parallel corpus into a\nbilingual corpus. We believe that this approach is helpful to training data acquisition\nfor machine translation, and can reduce the workload and time for constructing the\nparallel corpus.\nReference\n[1] Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving Machine\nTranslation Performance by Exploiting Non-Parallel Corpora. Computational\nLinguistics, 31(4):478-504.\n[2] Peter F. Brown, Jennifer C. Lai, and Robert L. Mercer. 1991. Aligning Sentences\nin Parallel Corpora. In: Proceedings of the 29 th Annual Meeting of the\nAssociation for Computational Linguistics (ACL), pages 169-176.\n[3] William A. Gale and Kenneth W. Church. 1991. A Program for Aligning\nSentence in Bilingual Corpora. In: Proceedings of 29 th Annual Meeting of the\nAssociation for Computational Linguistics (ACL), pages 177-184.\n[4] \u5218\u6615\uff0c\u5468\u660e\uff0c\u9ec4\u660c\u5b81\uff081995\uff09\uff0c\u57fa\u4e8e\u957f\u5ea6\u7b97\u6cd5\u7684\u4e2d-\u82f1\u53cc\u8bed\u6587\u672c\u5bf9\u9f50\u7684\u8bd5\u9a8c\uff0c\n\u8ba1\u7b97\u8bed\u8a00\u5b66\u8fdb\u5c55\u4e0e\u5e94\u7528\uff08\u9648\u7acb\u4e3a\uff0c\u8881\u7426\u4e3b\u7f16\uff09\uff0c\u7b2c 62\uff5e67\u9875\u3002\n[5] Dekai Wu. 1994. Aligning a Parallel English-Chinese Corpus Statistically with\nLexical Criteria. In: Proceedings of the 32 th Annual Conference of the\nAssociation for Computational Linguistics (ACL), pages 80-87.\n[6] Martin Kay and Martin R\u00f6scheisen. 1993. Text-translation Alignment.\nComputational Linguistics, 19 (1): 121-142.\n[7] Stanley F. Chen. 1993. Aligning Sentences in Bilingual Corpora Using Lexical\nInformation. In: Proceedings of 31st Annual Meeting of the Association for\nComputational Linguistics (ACL), pages 9-16.\n[8] Yang Muyun, Li Sheng, Zhao Tiejun, Lu Yajuan and Liu Zhanyi. 2002. A\nResearch on Bilingual Dictionary Based Sentence Alignment for Chinese English\nParallel Corpus. High Technology Letter, 8(1):8-11.\n[9] Takehito Utsuro, IIiroshi Ikeda, Masaya Yamane, Yuji Matsumoto and Makoto\nNagao. 1994. Bilingual Text Matching Using Bilingual Dictionary and Statistics.\nIn: Proceeding of the 15 th International Conference on Computational Linguistics,\npages 1076-1082.\n[10] Trevor Hastie, Robert Tibshirani, Jeroma Friedman \u8457\uff0c\u8303\u660e\uff0c\u67f4\u7389\u6885\u7b49\u8bd1\n\uff082005\uff09\uff0c\u7edf\u8ba1\u5b66\u4e60\u57fa\u7840\u2014\u2014\u6570\u636e\u6316\u6398\u3001\u63a8\u7406\u4e0e\u9884\u6d4b\uff0c\u5317\u4eac\uff1a\u7535\u5b50\u5de5\u4e1a\u51fa\u7248\n\u793e\u3002\n[11] Ian H. Witten, Eibe Frank \u8457\uff0c\u8463\u7433\uff0c\u90b1\u6cc9\u7b49\u8bd1\uff082006\uff09\uff0c\u6570\u636e\u6316\u6398\u5b9e\u7528\u673a\u5668\n\u5b66\u4e60\u6280\u672f\uff0c\u5317\u4eac:\u673a\u68b0\u5de5\u4e1a\u51fa\u7248\u793e\uff0c\u7b2c 241\uff5e243\u9875\u3002\n[12] Chris Callison-burch, David Talbot and Miles Osborne. 2004. Statistical Machine\nTranslation with Word- and Sentence-aligned Parallel Corpora. In: Proceedings\nof the 42 nd Annual Meeting of the Association for Computational Linguistics\n(ACL), pages 175-182.\n[13] Alexander Fraser and Daniel Marcu. 2006. Semi-supervised Training for\nStatistical Word Alignment. In: Proceedings of the 44 th Annual Meeting of the\nAssociation for Computational Linguistics (ACL), pages 769-776.\n[14] Nicola Ueffing, Gholamreza Haffari and Anoop Sarkar. 2007. Transductive\nLearning for Statistical Machine Translation. In Proceedings of the 45 th Annual\nMeeting of the Association of Computational Linguistics (ACL), pages 25-32."}], "references": [{"title": "Improving Machine Translation Performance by Exploiting Non-Parallel Corpora", "author": ["Dragos Stefan Munteanu", "Daniel Marcu"], "venue": "Computational Linguistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Aligning Sentences in Parallel Corpora", "author": ["Peter F. Brown", "Jennifer C. Lai", "Robert L. Mercer"], "venue": "Proceedings of the 29  th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1991}, {"title": "A Program for Aligning Sentence in Bilingual Corpora", "author": ["William A. Gale", "Kenneth W. Church"], "venue": "Proceedings of 29  th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1991}, {"title": "Aligning a Parallel English-Chinese Corpus Statistically with Lexical Criteria", "author": ["Dekai Wu"], "venue": "Proceedings of the 32  th Annual Conference of the Association for Computational Linguistics (ACL),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Aligning Sentences in Bilingual Corpora Using Lexical Information", "author": ["Stanley F. Chen"], "venue": "Proceedings of 31st Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "A Research on Bilingual Dictionary Based Sentence Alignment for Chinese English Parallel Corpus", "author": ["Yang Muyun", "Li Sheng", "Zhao Tiejun", "Lu Yajuan", "Liu Zhanyi"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Bilingual Text Matching Using Bilingual Dictionary and Statistics", "author": ["Takehito Utsuro", "IIiroshi Ikeda", "Masaya Yamane", "Yuji Matsumoto", "Makoto Nagao"], "venue": "Proceeding of the 15  th International Conference on Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "Statistical Machine Translation with Word- and Sentence-aligned Parallel Corpora", "author": ["Chris Callison-burch", "David Talbot", "Miles Osborne"], "venue": "Proceedings of the 42 nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Semi-supervised Training for Statistical Word Alignment", "author": ["Alexander Fraser", "Daniel Marcu"], "venue": "Proceedings of the 44 th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Transductive Learning for Statistical Machine Translation", "author": ["Nicola Ueffing", "Gholamreza Haffari", "Anoop Sarkar"], "venue": "In Proceedings of the 45 th Annual Meeting of the Association of Computational Linguistics (ACL),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "[1] Dragos Stefan Munteanu and Daniel Marcu.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Peter F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] William A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] Dekai Wu.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[7] Stanley F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[8] Yang Muyun, Li Sheng, Zhao Tiejun, Lu Yajuan and Liu Zhanyi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[9] Takehito Utsuro, IIiroshi Ikeda, Masaya Yamane, Yuji Matsumoto and Makoto Nagao.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[12] Chris Callison-burch, David Talbot and Miles Osborne.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[13] Alexander Fraser and Daniel Marcu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[14] Nicola Ueffing, Gholamreza Haffari and Anoop Sarkar.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "Although the parallel corpus has an irreplaceable role in machine translation, its scale and coverage is still beyond the actual needs. Non-parallel corpus resources on the web have an inestimable potential value in machine translation and other natural language processing tasks. This article proposes a semi-supervised transductive learning method for expanding the training corpus in statistical machine translation system by extracting parallel sentences from the non-parallel corpus. This method only requires a small amount of labeled corpus and a large unlabeled corpus to build a high-performance classifier, especially for when there is short of labeled corpus. The experimental results show that by combining the non-parallel corpus alignment and the semi-supervised transductive learning method, we can more effectively use their respective strengths to improve the performance of machine translation system.", "creator": "Microsoft\u00ae Office Word 2007"}}}