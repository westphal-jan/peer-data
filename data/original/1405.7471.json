{"id": "1405.7471", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2014", "title": "Effect of Different Distance Measures on the Performance of K-Means Algorithm: An Experimental Study in Matlab", "abstract": "K-means algorithm is a very popular clustering algorithm which is famous for its simplicity. Distance measure plays a very important rule on the performance of this algorithm. We have different distance measure techniques available. But choosing a proper technique for distance calculation is totally dependent on the type of the data that we are going to cluster. In this paper an experimental study is done in Matlab to cluster the iris and wine data sets with different distance measures and thereby observing the variation of the performances shown.", "histories": [["v1", "Thu, 29 May 2014 05:59:26 GMT  (283kb)", "http://arxiv.org/abs/1405.7471v1", "6 pages, 11 figures, Clustering, K Means"]], "COMMENTS": "6 pages, 11 figures, Clustering, K Means", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mr dibya jyoti bora", "dr anil kumar gupta"], "accepted": false, "id": "1405.7471"}, "pdf": {"name": "1405.7471.pdf", "metadata": {"source": "CRF", "title": "Effect of Different Distance Measures on the Performance of K-Means Algorithm: An Experimental Study in Matlab", "authors": ["Dibya Jyoti Bora", "Anil Kumar Gupta"], "emails": [], "sections": [{"heading": "1. INTRODUCTION:", "text": "Clustering is an unsupervised study. The main aim of clustering is to divide a dataset into some different subsets (known as \u2018clusters\u2019) such that data into a particular subset sharing similar properties while data in different subset showing different properties from data in another subset. Means to say that clustering should satisfy the two properties [1][2]: 1. High Cohesive Property and 2. Low Coupling Property. Clustering algorithms are mainly divided into two types based on developed cluster properties: hierarchical and partitional. The hierarchical methods, in general try to decompose the dataset of n objects into a hierarchy of groups [1]. This hierarchical decomposition can be represented by a tree structure diagram called as a dendrogram [3]; whose root node represents the whole dataset and each leaf node is a single object of the dataset. The clustering results can be obtained by cutting the dendrogram at different level. There are two general approaches for the hierarchical method: agglomerative and divisive [2][3][4]. Agglomerative approach is a bottom up approach starting with n-leaf nodes, letting them as individual clusters, moving upwards towards the root with some merging criteria. While divisive hierarchical clustering technique is a top down approach, starting from the root node gradually splitting the data into different clusters downwards based on the properties of the data. While in case of partitional clustering, k partitions of the datasets with n objects are created, where each partition represents a cluster, where k<= n. It tries to divide the data into subsets or partitions based on some evaluation criteria. K-Means is a very popular partitional clustering algorithm. In this paper, first of all the K-Means algorithm is\ndiscussed and then different distance measurement techniques for K-means algorithm are mentioned. After that we go through experiments for implementing K-Means with different distance measurement techniques in Matlab. Based on the results of the experiments, we finally come to a conclusion about the performance of K-Means algorithm with respect to different distance measurement techniques."}, {"heading": "2. K \u2013MEANS ALGORITHM:", "text": "The K-Means [5] is one of the famous partition clustering algorithm [ 6][7][8]. It takes the input parameter k, the number of clusters, and partitions a set of n objects into k clusters so that the resulting intra-cluster similarity is high but the inter-cluster similarity is low. The main idea is to define k centroids, one for each cluster. These centroids should be placed in a cunning way because of different location causes different results. So, the better choice is to place them as much as possible far away from each other. The next step is to take each point belonging to a given data set and associate it to the nearest centroid. When no point is pending, the first step is completed and an early groupage is done. At this point we need to re-calculate k new centroids. After we have these k new centroids, a new binding has to be done between the same data set points and the nearest new centroid. A loop has been generated. As a result of this loop we may notice that the k centroids change their location step by step until no more changes are done. In other words centroids do not move any more. Finally, this algorithm aims at minimizing an objective function, in this case a squared error function. The objective function\n, The Formal Algorithm [8] is :"}, {"heading": "1. Select K points as initial centroids.", "text": ""}, {"heading": "2. Repeat.", "text": ""}, {"heading": "3. Form k clusters by assigning all points to the closest centroid.", "text": ""}, {"heading": "4. Recompute the centroid of each cluster.Until the centroids do not change", "text": "www.ijcsit.com 2501\nA diagrammatic view[9] of the K-Means algorithm is :\nFigure (1): K-Means Algorithm"}, {"heading": "3. DISTANCE MEASUREMENTS IN K-MEANS ALGORITHMS:", "text": "In K-Means algorithm, we calculate the distance between each point of the dataset to every centroid initialized. Based on the values found, points are assigned to the centroid with minimum distance. Hence, this distance calculation plays the vital role in the clustering algorithm. As we know, distance between two points can be computed with different techniques available, so, our main aim is to pick up a proper technique from the available ones. But, in choosing such techniques, some important points to be noted such as: the property of the data and the dimension of the dataset. In this experiment, we take \u201cCityblock\u201d,\u201dEuclidean\u201d,\u201dCosine\u201d and \u201cCorrelation\u201d-these distance measurement techniques for distance calculations in the K-Means algorithm. Description about each technique is mentioned below:"}, {"heading": "3.1 City Block (Manhattan):", "text": "The city block distance [10][11] two point a and b with k dimensions is defined as:\nThe name City block distance (also referred to as Manhattan distance) [11] is explained if we consider two points in the xy-plane. The shortest distance between the two points is along the hypotenuse, which is the Euclidean distance. The City block distance is instead calculated as the distance in x plus the distance in y, which is similar to the way we move in a city (like Manhattan) where we have to move around the buildings instead of going straight through."}, {"heading": "3.2 Euclidean distance:", "text": "The Euclidean distance between two points, a and b, with k dimensions is calculated as[12][13]:\nEuclidean (and squared Euclidean) distances are usually computed from raw data, and not from standardized data. One advantage of this method is that the distance between any two objects is not affected by the addition of new objects to the analysis, which may be outliers [13]. However, the distances can be greatly affected by differences in scale among the dimensions from which the distances are computed. For example, if one of the dimensions denotes a measured length in centimeters, and you then convert it to millimeters (by multiplying the values by 10), the resulting Euclidean can be greatly affected (i.e., biased by those dimensions which have a larger scale), and consequently, the results of cluster analyses may be very different. Generally, it is good practice to transform the dimensions so that they have similar scales [13]."}, {"heading": "3.3 Cosine Distance:", "text": "The cosine distance between two points is one minus the cosine of the included angle between points (treated as vectors). Given an m-by-n data matrix X, which is treated as m (1-by-n) row vectors x1, x2, ..., xm, the cosine distances between the vector xs and xt are defined as follows[14]:\n3.4 Correlation Distance: Distance correlation is a measure of dependence between random vectors [15]. Given an m-by-n data matrix X, which is treated as m (1-by-n) row vectors x1, x2, ..., xm, the correlation distances between the vector xs and xt are defined as follows[14]:"}, {"heading": "4. EXPERIMENTS:", "text": "For our experiments, we have chosen two different datasets: iris and wine datasets. Iris dataset The Iris flower data set or Fisher's Iris data set (some times also known as Anderson's Iris data) is a multivariate data set introduced by Sir Ronald Fisher (1936) as an example of discriminant analysis. The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters[16]. Table(1) gives details of the iris dataset[16]:\nwww.ijcsit.com 2502\nData Set Characteristics: Multivariate Number of Instances: 150 Area: Life\nAttribute Characteristics: Real Number of Attributes: 4 Date Donated 1988-07-01\nAssociated Tasks: Classification Missing Values? No Number of Web Hits: 548538\nTable (1): Iris dataset\nData Set Characteristics: Multivariate Number of Instances: 178 Area: Physical\nAttribute Characteristics: Integer, Real Number of Attributes: 13 Date Donated 1991-07-01\nAssociated Tasks: Classification Missing Values? No Number of Web Hits: 328827\nTable (2) Wine dataset The wine dataset [17] is the result of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. Table (2) gives details of the wine dataset [17]: We have \u201ckmeans\u201d function to perform K-means clustering in Matlab [18]. The function kmeans performs K-Means clustering, using an iterative algorithm that assigns objects to clusters so that the sum of distances from each object to its cluster centroid, over all clusters, is a minimum. k means returns an n-by-1 vector IDX containing the cluster indices of each point. When X is a vector, kmeans treats it as an n-by-1 data matrix, regardless of its orientation. 4.1 Experiments with Iris dataset: We specify the K (number of clusters) as 3. Results for running K-Means algorithm for iris dataset with different distance measures are mentioned below: (A) City Block distance: iter phase num sum 1 1 150 1679 2 1 5 1651 3 1 1 1649 4 2 0 1649 Best total sum of distances = 1649 (B) Euclidean distance: iter phase num sum 1 1 150 14762.1 2 1 10 14360.7 3 1 31 8975.18 4 1 13 7989.84 5 1 3 7941.63 6 1 2 7899.12 7 2 0 7897.88 Best total sum of distances = 7897.88 (C) Cosine Distance: iter phase num sum 1 1 150 0.161957 2 1 1 0.161835 3 2 0 0.161835 Best total sum of distances = 0.161835 (D) Correlation Distance: iter phase num sum 1 1 150 4.61083 2 1 50 0.463321 3 1 4 0.371327 4 1 1 0.370903 5 2 0 0.370903 Best total sum of distances = 0.370903 Figure (2.1) Cluster figure for City block distance\nFigure (2.2) Cluster figure for Euclidean distance\nwww.ijcsit.com 2503\nFigure (2.3) Cluster figure for Cosine distance\nFigure (2.4) Cluster figure for Correlation distance"}, {"heading": "4.2 Experiment with Wine dataset:", "text": "The results found when running K-Means on Wine datasets with different measures are mentioned below: (A) City Block Distance: iter phase num sum 1 1 13 155074 2 1 4 148285 3 1 2 146085 4 1 2 121215 5 1 1 23214.8 6 2 0 23214.8 Best total sum of distances = 23214.8 (B) Euclidean distance: iter phase num sum 1 1 13 860498 2 1 2 67196 3 2 0 67196 Best total sum of distances = 67196\n(C) Cosine Distance: iter phase num sum 1 1 13 0.321896 2 1 1 0.296767 3 1 1 0.281313 4 2 1 0.277909 5 2 0 0.273201 Best total sum of distances = 0.273201 (D) Correlation Distance: iter phase num sum 1 1 13 5.19803 2 1 1 4.8358 3 1 1 4.23239 4 1 1 3.59637 5 1 1 3.25178 6 2 0 3.19287 Best total sum of distances = 3.19287\nFigure (3.1) Cluster figure for City block distance\nFigure (3.2) Cluster figure for Euclidean distance\nwww.ijcsit.com 2504\nFigure (3.3) Cluster figure for Cosine distance\nFigure (3.4) Cluster figure for Correlation distance\nSo, from the experiments, we have found different results (in terms of best total sum of distances and clusters\u2019 figures) for clustering the same dataset with respect to different distance measures. Also, we have noted down the time taken for each experiment. Table (3) and Table (4) show the respective time taken for the dataset iris and wine respectively:\nCity Euclidean Cosine Correlation 0.069 0.079 0.091 0.085 Table (4): Time with respect to different distance\nmeasures for Iris dataset\nCity Euclidean Cosine Correlation 0.078 0.082 0.089 0.086 Table (5): Time with respect to different distance\nmeasures for Wine dataset\nFigure (4): Comparisons Of different distance measures\nin terms of computation time for iris dataset\nFigure (5): Comparisons Of different distance measures\nin terms of computation time for wine dataset\nCONCLUSION: On the basis of results of the experiments done, we have found that city block distance shows better performance for both the datasets in terms of less computation time. While cosine takes more computation time in comparison to other distance measures for both the data sets. But, when we refer to silhouette plots of the clusters, then we must admit that \u201ccorrelation\u201d distance measures show a better interpretation of the clustered data. We have performed our experiments on two different datasets (i.e. iris and wine) only to observe the performance with respect to datasets with different number of attributes. Iris dataset has four attributes, while wine dataset has thirteen attributes. While choosing a proper distance measure, we must also keep an eye on the number of attributes involved in that dataset as otherwise it will result a high time complexity if a wrong one is chosen. In our future work, we will consider another different distance measures for K-Means algorithm with respect to a big dataset and perform a comparison among them, thereby, try to propose a good one for the task of clustering big data set. Also, we will try to extend our study for another partition clustering algorithms like K-Medoids, CLARA and CLARANS.\nwww.ijcsit.com 2505\nREFERENCES: [1] A.K. Jain, M. N. Murty, P. J. Flynn, \u201cData Clustering: A Review\u201d,\nACM Computing Surveys, vol. 31, pp. 264-323, Sep. 1999. [2] P. Berkhin. (2001) \u201cSurvey of Clustering Data Mining Techniques\u201d\n[Online]. Available: http://www.accure.com/products/rp_cluster_review.pdf. [3] Neha Soni, Amit Ganatra, \u201cCategorization of Several Clustering Algorithms from Different Perspective: A Review\u201d, International Journal of Advanced Research in Computer Science and Software Engineering, Volume 2, Issue 8, August 2012, pp 63-68 [4] J. Han , M. Kamber, Data Mining, Morgan Kaufmann Publishers, 2001. [5] MacQueen, J. B. (1967). \"Some Methods for classification and Analysis of Multivariate Observations\",Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability 1. University of California Press, 2009-04-07, pp. 281\u2013297. [6] Steinhaus, H., \"Sur la division des corps mat\u00e9riels en parties\". Bull. Acad. Polon. Sci.(in French) 4 (12),1957, pp. 801\u2013804 [7] Lloyd., S. P. \"Least squares quantization in PCM\". IEEE Transactions on Information Theory 28 (2), 1982, pp. 129\u2013137 [8] Shaeela Ayesha, Tasleem Mustafa, Ahsan Raza Sattar & M.Inayat Khan, \u201cData Mining Model for Higher Education System \u201c,European Journal of Scientific Research, ISSN 1450-216X Vol.43 No.1 ,2010, pp.27 [9] http://people.revoledu.com/kardi/tutorial/kMean/ [10] http://planetmath.org/CityBlockMetric [11] http://stn.spotfire.com/spotfire_client_help/hc/hc_city_block_distance.htm [12] Elena Deza , Michel Marie Deza, \u201cEncyclopedia of Distances\u201d,\nSpringer, 2009, page 94, [13] http://www.statsoft.com/textbook/cluster-analysis/, March 2, 2011 [14] http://www.mathworks.in/help/stats/pdist.html [15] G\u00e1bor J. Sz\u00e9kely, Maria L. Rizzo, and Nail K. Bakirov,\u201d Measuring\nand testing dependence by correlation of distances\u201d, Ann. Statist. Volume 35, Number 6 ,2007, 2313-2817\n[16] http://archive.ics.uci.edu/ml/datasets/Iris [17] http://archive.ics.uci.edu/ml/datasets/Wine [18] http://www.mathworks.in/help/stats/examples/cluster-analysis.html\nwww.ijcsit.com 2506"}], "references": [{"title": "Data Clustering: A Review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM Computing Surveys,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Survey of Clustering Data Mining Techniques", "author": ["P. Berkhin"], "venue": "http://www.accure.com/products/rp_cluster_review.pdf", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Categorization of Several Clustering Algorithms from Different Perspective: A Review", "author": ["Neha Soni", "Amit Ganatra"], "venue": "International Journal of Advanced Research in Computer Science and Software Engineering,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Some Methods for classification and Analysis of Multivariate Observations\",Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability 1", "author": ["J.B. MacQueen"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1967}, {"title": "Sur la division des corps mat\u00e9riels en parties", "author": ["H. Steinhaus"], "venue": "Bull. Acad. Polon. Sci.(in French)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1957}, {"title": "Least squares quantization in PCM", "author": ["S.P. Lloyd"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1982}, {"title": "Data Mining Model for Higher Education System \u201c,European", "author": ["Shaeela Ayesha", "Tasleem Mustafa", "Ahsan Raza Sattar", "M.Inayat Khan"], "venue": "Journal of Scientific Research, ISSN 1450-216X", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Measuring and testing dependence by correlation of distances", "author": ["G\u00e1bor J. Sz\u00e9kely", "Maria L. Rizzo", "Nail K. Bakirov"], "venue": "Ann. Statist. Volume 35, Number", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Means to say that clustering should satisfy the two properties [1][2]: 1.", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "Means to say that clustering should satisfy the two properties [1][2]: 1.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "The hierarchical methods, in general try to decompose the dataset of n objects into a hierarchy of groups [1].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "decomposition can be represented by a tree structure diagram called as a dendrogram [3]; whose root node represents the whole dataset and each leaf node is a single object of the dataset.", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "There are two general approaches for the hierarchical method: agglomerative and divisive [2][3][4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "There are two general approaches for the hierarchical method: agglomerative and divisive [2][3][4].", "startOffset": 92, "endOffset": 95}, {"referenceID": 3, "context": "K \u2013MEANS ALGORITHM: The K-Means [5] is one of the famous partition clustering algorithm [ 6][7][8].", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "K \u2013MEANS ALGORITHM: The K-Means [5] is one of the famous partition clustering algorithm [ 6][7][8].", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "K \u2013MEANS ALGORITHM: The K-Means [5] is one of the famous partition clustering algorithm [ 6][7][8].", "startOffset": 92, "endOffset": 95}, {"referenceID": 6, "context": "K \u2013MEANS ALGORITHM: The K-Means [5] is one of the famous partition clustering algorithm [ 6][7][8].", "startOffset": 95, "endOffset": 98}, {"referenceID": 6, "context": "The Formal Algorithm [8] is :", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "4 Correlation Distance: Distance correlation is a measure of dependence between random vectors [15].", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "[1] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Neha Soni, Amit Ganatra, \u201cCategorization of Several Clustering Algorithms from Different Perspective: A Review\u201d, International Journal of Advanced Research in Computer Science and Software Engineering, Volume 2, Issue 8, August 2012, pp 63-68 [4] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] MacQueen, J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] Steinhaus, H.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "801\u2013804 [7] Lloyd.", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "[8] Shaeela Ayesha, Tasleem Mustafa, Ahsan Raza Sattar & M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[15] G\u00e1bor J.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "K-means algorithm is a very popular clustering algorithm which is famous for its simplicity. Distance measure plays a very important rule on the performance of this algorithm. We have different distance measure techniques available. But choosing a proper technique for distance calculation is totally dependent on the type of the data that we are going to cluster. In this paper an experimental study is done in Matlab to cluster the iris and wine data sets with different distance measures and thereby observing the variation of the performances shown. KeywordsClustering, K Means, Iris, Wine, Matlab", "creator": "PScript5.dll Version 5.2.2"}}}