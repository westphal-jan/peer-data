{"id": "1406.2673", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2014", "title": "Mondrian Forests: Efficient Online Random Forests", "abstract": "Ensembles of randomized decision trees, usually referred to as random forests, are widely used for classification and regression tasks in machine learning and statistics. Random forests achieve competitive predictive performance and are computationally efficient to train and test, making them excellent candidates for real-world prediction tasks. The most popular random forest variants (such as Breiman's random forest and extremely randomized trees) operate on batches of training data. Online methods are now in greater demand. Existing online random forests, however, require more training data than their batch counterpart to achieve comparable predictive performance. In this work, we use Mondrian processes (Roy and Teh, 2009) to construct ensembles of random decision trees we call Mondrian forests. Mondrian forests can be grown in an incremental/online fashion and remarkably, the distribution of online Mondrian forests is the same as that of batch Mondrian forests. Mondrian forests achieve competitive predictive performance comparable with existing online random forests and periodically re-trained batch random forests, while being more than an order of magnitude faster, thus representing a better computation vs accuracy tradeoff.", "histories": [["v1", "Tue, 10 Jun 2014 19:34:51 GMT  (435kb,D)", "http://arxiv.org/abs/1406.2673v1", null], ["v2", "Mon, 16 Feb 2015 14:57:52 GMT  (594kb,D)", "http://arxiv.org/abs/1406.2673v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["balaji lakshminarayanan", "daniel m roy", "yee whye teh"], "accepted": true, "id": "1406.2673"}, "pdf": {"name": "1406.2673.pdf", "metadata": {"source": "CRF", "title": "Mondrian Forests: Efficient Online Random Forests", "authors": ["Balaji Lakshminarayanan", "Daniel M. Roy"], "emails": ["balaji@gatsby.ucl.ac.uk."], "sections": [{"heading": "1 Introduction", "text": "Despite being introduced over a decade ago, random forests remain one of the most popular machine learning tools due in part to their accuracy, scalability, and robustness in real-world classification tasks [3]. (We refer to [5] for an excellent recent survey of random forests.) In this paper, we introduce a novel type of random forest\u2014called Mondrian forests (MF), due to the fact that the underlying tree structure of each classifier in the ensemble is a so-called Mondrian process. Using the properties of Mondrian processes, we present an efficient online algorithm that agrees with its batch counterpart at each iteration. Not only are online Mondrian forests faster and more accurate than recent proposals for online random forest methods, but they nearly match the accuracy of state-of-the-art batch random forest methods trained on the same dataset.\nThe paper is organized as follows: In Section 2, we describe our approach at a high-level, and in Sections 3, 4, and 5, we describe the tree structures, label model, and incremental updates/predictions in more detail. We discuss related work in Section 6, demonstrate the excellent empirical performance of MF in Section 7, and conclude in Section 8 with a discussion about future work.\n\u2217Corresponding author. Email address: balaji@gatsby.ucl.ac.uk.\nar X\niv :1\n40 6.\n26 73\nv1 [\nst at\n.M L\n] 1\n0 Ju"}, {"heading": "2 Approach", "text": "Given N labeled examples (x1, y1), . . . , (xN , yN ) \u2208 RD \u00d7 Y as training data, our task is to predict labels y \u2208 Y for unlabeled test points x \u2208 RD. We will focus on multi-class classification where Y := {1, . . . ,K}, however, it is possible to extend the methodology to other supervised learning tasks such as regression. Let X1:n := (x1, . . . ,xn), Y1:n := (y1, . . . , yn), and D1:n := (X1:n, Y1:n).\nA Mondrian forest classifier is constructed much like a random forest: Given training data D1:N , we sample an independent collection T1, . . . , TM of so-called Mondrian trees, which we will describe in the next section. The prediction made by each Mondrian tree Tm is a distribution pTm(y|x,D1:N ) over the class label y for a test point x. The prediction made by the Mondrian forest is the average\n1\nM M\u2211 m=1 pTm(y|x,D1:N ) (1)\nof the individual tree predictions. As M \u2192\u221e, the average converges at the standard rate to the expectation\nET\u223cMT(\u03bb,D1:N )[ pT (y|x,D1:N )] +O(M \u22121/2), (2)\nwhere MT (\u03bb,D1:N ) is the distribution of a Mondrian tree. As the limiting expectation does not depend on M , we would not expect to see overfitting behavior as M increases. A similar observation was made by Breiman in his seminal article [2] introducing random forests. Note that Eq. (2) is ensemble model combination, not Bayesian model averaging.\nIn the online learning setting, the training examples are presented one after another in a sequence of trials. Mondrian forests excel in this setting: at iteration n+ 1, each Mondrian tree T \u223c MT (\u03bb,D1:n) is updated to incorporate the next labeled example (xn+1, yn+1) by sampling an extended tree T \u2032 from a distribution MTx(\u03bb, T,Dn+1). Using properties of the Mondrian process, we can choose a probability distribution MTx such that T \u2032 = T on D1:n and T \u2032 is distributed according to MT (\u03bb,D1:n+1), i.e.,\nT \u223c MT (\u03bb,D1:n) T \u2032 | T,D1:n+1 \u223c MTx(\u03bb, T,Dn+1)\nimplies T \u2032 \u223c MT (\u03bb,D1:n+1) . (3)\nTherefore, the distribution of Mondrian trees trained on a dataset in an incremental fashion is the same as that of Mondrian trees trained on the same dataset in a batch fashion, irrespective of the order in which the data points are observed. To the best of our knowledge, none of the existing online random forests have this property. Moreover, we can sample from MTx(\u03bb, T,Dn+1) efficiently: the complexity scales with the depth of the tree, which is typically logarithmic in n.\nWhile treating the online setting as a sequence of larger and larger batch problems is normally computationally prohibitive, this approach can be achieved efficiently with Mondrian forests. In the following sections, we define the Mondrian tree distribution MT (\u03bb,D1:N ), the label distribution pT (y|x,D1:N ), and the update distribution MTx(\u03bb, T,Dn+1)."}, {"heading": "3 Mondrian trees", "text": "For our purposes, a decision tree on RD will be a hierarchical, binary partitioning of RD and a rule for predicting the label of test points given training data. More carefully, a rooted, strictly-binary tree is a finite tree T such that every node in T is either a leaf or internal\nnode, and every node is the child of exactly one parent node, except for a distinguished root node, represented by , which has no parent. Let leaves(T) denote the set of leaf nodes in T. For every internal node j \u2208 T \\ leaves(T), there are exactly two children nodes, represented by left(j) and right(j). To each node j \u2208 T, we associate a block Bj \u2208 RD of the input space as follows: We let B := RD. Each internal node j \u2208 T \\ leaves(T) is associated with a split( \u03b4j , \u03bej ) , where \u03b4j \u2208 {1, 2, . . . , D} denotes the dimension of the split and \u03bej denotes the location of the split along dimension \u03b4j . We then define\nBleft(j) := {x \u2208 Bj : x\u03b4j \u2264 \u03bej} and Bright(j) := {x \u2208 Bj : x\u03b4j > \u03bej}. (4)\nWe may write Bj = ( `j1, uj1 ] \u00d7 . . .\u00d7 ( `jD, ujD ] , where `jd and ujd denote the `ower and upper limit, respectively, of the rectangular block Bj along dimension d. Put `j = {`j1, `j2, . . . , `jD} and uj = {uj1, uj2, . . . , ujD}. The decision tree structure is represented by the tuple T = (T, \u03b4, \u03be). We refer to Figure 1(a) for a simple illustration of a decision tree.\nLet parent(j) denote the parent of node j. Let N(j) denote the indices of training data points at node j, i.e., N(j) = {n \u2208 {1, . . . , N} : xn \u2208 Bj}. Let DN(j) = {XN(j), YN(j)} denote the features and labels of training data points at node j. Let `xjd and u x jd denote the lower and upper limit of training data points (hence the superscript x) respectively in node j along dimension d. Let Bxj = ( `xj1, u x j1 ] \u00d7 . . .\u00d7 ( `xjD, u x jD ] \u2286 Bj denote the smallest rectangle that encloses the training data points in node j."}, {"heading": "3.1 Mondrian process distribution over decision trees", "text": "Mondrian processes, introduced by Roy and Teh [14], are families {Mt : t \u2208 [0,\u221e)} of random, hierarchical binary partitions of RD such that Mt is a refinement of Ms whenever t > s.1 Mondrian processes are natural candidates for the partition structure of random decision trees, but Mondrian processes on RD are, in general, infinite structures that we cannot represent all at once. Because we only care about the partition on a finite set of observed data, we introduce Mondrian trees, which are restrictions of Mondrian processes to a finite set of points. A Mondrian tree T can be represented by a tuple (T, \u03b4, \u03be, \u03c4 ), where (T, \u03b4, \u03be) is a decision tree, \u03c4 = {\u03c4j}j\u2208T, and \u03c4j \u2265 0 denotes the time of the split associated with node j. The time of split increases with depth, i.e., \u03c4j > \u03c4parent(j). We abuse notation and define \u03c4parent( ) = 0.\n1Roy and Teh [14] studied the distribution of {Mt : t \u2264 \u03bb} and refered to \u03bb as the budget. See [13, Chp. 5] for more details. We will refer to t as time, not be confused with discrete time in the online learning setting.\nGiven a non-negative lifetime parameter \u03bb and training data D1:n, the generative process for sampling Mondrian trees from MT (\u03bb,D1:n) is described in the following two algorithms: Algorithm 1 SampleMondrianTree ( \u03bb,D1:n\n) 1: Initialize: T = \u2205, leaves(T) = \u2205, \u03b4 = \u2205, \u03be = \u2205, \u03c4 = \u2205, N( ) = {1, 2, . . . , n} 2: SampleMondrianBlock ( ,DN( ), \u03bb ) . Algorithm 2\nAlgorithm 2 SampleMondrianBlock ( j,DN(j), \u03bb ) 1: Add j to T 2: For all d, set `xjd = min(XN(j),d), u x jd = max(XN(j),d) . dimension-wise min and max\n3: Sample E from exponential distribution with rate \u2211 d(u x jd \u2212 `xjd) 4: if \u03c4parent(j) + E < \u03bb then . j is an internal node 5: Set \u03c4j = \u03c4parent(j) + E 6: Sample split dimension \u03b4j , choosing d with probability proportional to u x jd \u2212 `xjd 7: Sample split location \u03bej uniformly from interval [` x j\u03b4j , uxj\u03b4j ] 8: Set N(left(j)) = {n \u2208 N(j) : Xn,\u03b4j \u2264 \u03bej} and N(right(j)) = {n \u2208 N(j) : Xn,\u03b4j > \u03bej} 9: SampleMondrianBlock ( left(j),DN(left(j)), \u03bb\n) 10: SampleMondrianBlock ( right(j),DN(right(j)), \u03bb\n) 11: else . j is a leaf node 12: Set \u03c4j = \u03bb and add j to leaves(T)\nThe procedure starts with the root node and recurses down the tree. In Algorithm 2, we first compute the `x and u x i.e. the lower and upper limits of B x , the smallest rectangle enclosing XN( ). We sample E from an exponential distribution whose rate is the so-called linear dimension of Bx , given by \u2211 d(u x d \u2212 `x d). Since \u03c4parent( ) = 0, E + \u03c4parent( ) = E. If E \u2265 \u03bb, the time of split is not within the lifetime \u03bb; hence, we assign to be a leaf node and the procedure halts. (Since E[E] = 1/ (\u2211 d(u x jd \u2212 `xjd) ) , bigger rectangles are less likely to be leaf nodes.) Else, is an internal node and we sample a split (\u03b4 , \u03be ) in B x from the uniform split distribution in Bx . More precisely, we first sample the dimension \u03b4 , taking the value d with probability proportional to ux d \u2212 `x d, and then sample the split location \u03be uniformly from the interval [`x \u03b4 , u x \u03b4\n]. The procedure then recurses along the left and right children. Mondrian trees differ from standard decision trees (e.g. CART, C4.5) in the following: (i) the splits are sampled independent of the labels YN(j); (ii) every node j is associated with a split time \u03c4j ; (iii) the lifetime parameter \u03bb controls the total number of splits (similar to the maximum depth parameter for standard decision trees); (iv) the split represented by an internal node j holds only within Bxj and not the whole of Bj . No commitment is made in Bj \\Bxj . Figure 1 illustrates the difference between Mondrian trees and decision trees.\nConsider the family of distributions MT (\u03bb, F ), where F ranges over all possible finite sets of data points. Due to the fact that these distributions are derived from that of a Mondrian process on RD restricted to a set F of points, the family MT (\u03bb, \u00b7) will be projective. Intuitively, projectivity implies that the tree distributions possess a type of self-consistency in distribution. In words, if we sample a Mondrian tree T from MT (\u03bb, F ) and then restrict the tree T to a subset F \u2032 \u2286 F of points, then the restricted tree T \u2032 has distribution MT (\u03bb, F \u2032). This property follows from a similar property of Mondrian processes [13, 14]. Most importantly, projectivity gives us a consistent way to extend a Mondrian tree on a data set D1:n to a larger data set D1:n+1. We exploit this property to incrementally grow a Mondrian tree: even though MT (\u03bb,D1:n) is defined on RD, we instantiate the Mondrian tree just on the regions where we have observed training data points so far; upon observing Dn+1, we extend the Mondrian by sampling from the conditional Mondrian distribution, referred to as MTx(\u03bb, T,Dn+1) in (3), unveiling the Mondrian tree only where we have observed training data."}, {"heading": "4 Label distribution: model, hierarchical prior, and pre-", "text": "dictive posterior\nSo far, our discussion has been focused only on the tree structure. In this section, we focus on the label distribution pT (y|x,D1:N ). Intuitively, we want the label distribution at a node to be a smoothed estimate of the empirical distribution of labels at a node. We achieve this smoothing via a hierarchical Bayesian approach within each tree. For each tree T , we introduce latent parameters G which specify a distribution over y at each node, denoted by pT (y|x,G). Next, we define a hierarchical prior pT (G) that encourages label distribution at a node to be similar to that of its parent. Finally, we discuss how the likelihood and the hierarchical prior are combined to obtain the label distribution pT (y|x,D1:N ).\nLet leaf(x) denote the unique leaf node in T such that x \u2208 Bleaf(x). As is common in the decision tree literature, we assume that the probability of labels within each block is independent of X given the tree structure T . Let Gj denote the distribution of labels at node j and G = {Gj : j \u2208 T} denote the set of label distributions at all the nodes in the tree. Given a tree T , the likelihood for x is defined by the label distribution at the node leaf(x), i.e., pT (y|x,G) = Gleaf(x). In this paper, we focus on the case of categorical labels taking values in the set {1, . . . ,K}. Hence, Gj = [Gj,1, Gj,2, . . . , Gj,K ] is the discrete distribution, where Gj,k is the probability of label k at node j.\nWe model the collection Gj , for j \u2208 T , as a hierarchy of normalized stable processes (NSP). A NSP prior is a distribution over distributions and is a special case of the Pitman-Yor process (PYP) prior where the concentration parameter is taken to zero.2 The discount parameter d \u2208 (0, 1) controls how much the samples vary around the base distribution; if Gj \u223c NSP(d,H), then E[Gjk] = Hk and Var[Gjk] = (1 \u2212 d)Hk(1 \u2212Hk). We use a hierarchical NSP (HNSP) prior over Gj as follows:\nG |H \u223c NSP(d , H), and Gj |Gparent(j) \u223c NSP(dj , Gparent(j)). (5)\nThis hierarchical prior was first proposed by Wood et al. [19]. Here we set dj = exp{\u2212\u03b3(\u03c4j \u2212 \u03c4parent(j))}, and the base distribution H to be the uniform distribution over the K labels.\nGiven training data D1:N , the predictive distribution pT (y|x,D1:N ) is obtained by integrating over G, i.e., pT (y|x,D1:N ) = EG\u223cpT (G|D1:N )[pT (y|x,G)] = EG\u223cpT (G|D1:N )[Gleaf(x),y] = Gleaf(x),y, where the posterior pT (G|D1:N ) \u221d pT (G) \u220fN n=1 pT (yn|xn,G). Posterior inference in the HNSP, i.e., computation of the posterior means Gleaf(x), is a special case of posterior inference in hierarchical PYP (HPYP). In particular, Teh [17] considers the HPYP with multinomial likelihood (in the context of language modeling). The model considered here is a special case of [17]. Exact inference is intractable and hence we resort to approximations. In particular, we use a fast approximation known as the interpolated Kneser-Ney (IKN) smoothing [17], a popular technique for smoothing probabilities in language modeling [10]. The IKN approximation in [17] can be extended in a straightforward fashion to the online setting, and the computational complexity of adding a new training instance is linear in the depth of the tree. We refer the reader to Appendix A for further details."}, {"heading": "5 Online training and prediction", "text": "In this section, we describe the distribution MTx(\u03bb, T,Dn+1) that incrementally adds the data point Dn+1 to a tree T . These updates are based on the conditional Mondrian algorithm [14], specialized to a finite set of points. In general, one or more of the following three operations\n2Taking the discount parameter to zero leads to a Dirichlet process. Hierarchies of NSPs admit more tractable approximations than hierarchies of Dirichlet processes, hence our choice here.\nAt iteration 1, we have two training data points, labeled as a, b. Figures 2(a) and 2(g) show the partition and tree structure of the Mondrian tree. Note that even though there is a split x2 > 0.23 at time t = 2.42, we commit this split only within Bxj (shown by the gray rectangle).\nAt iteration 2, a new data point c is added. Algorithm 3 starts with the root node and recurses down the tree. Algorithm 4 checks if the new data point lies within Bx by computing the additional extent e` and eu. In this case, c does not lie within Bx . Let Rab and Rabc respectively denote the small gray rectangle (enclosing a, b) and big gray rectangble (enclosing a, b, c) in Figure 2(b). While extending the Mondrian from Rab to Rabc, we could either introduce a new split in Rabc outside Rab or extend the split in Rab to the new range. To choose between these two options, we sample the time of this new split: we first sample E from an exponential distribution whose rate is the sum of the additional extent, i.e., \u2211 d(e ` d + e u d), and set the time of the new split to E + \u03c4parent( ). If E+ \u03c4parent( ) \u2264 \u03c4 , this new split in Rabc can precede the old split in Rab and a split is sampled in Rabc outside Rab. In Figures 2(c) and 2(h), E + \u03c4parent( ) = 1.01 + 0 \u2264 2.42, hence a new split x1 > 0.75 is introduced. The farther a new data point x is from Bxj , the higher the rate \u2211 d(e ` d + e u d), and\nsubsequently the higher the probability of a new split being introduced, since E[E] = 1/ (\u2211\nd(e ` d+e u d ) ) .\nA new split in Rabc is sampled such that it is consistent with the existing partition structure in Rab (i.e., the new split cannot slice through Rab).\nIn the final iteration, we add data point d. In Figure 2(d), the data point d lies within the extent of the root node, hence we traverse to the left side of the root and update Bxj of the internal node containing {a, b} to include d. We could either introduce a new split or extend the split x2 > 0.23. In Figure 2(e), we extend the split x2 > 0.23 to the new extent, and traverse to the leaf node in Figure 2(h) containing b. In Figures 2(f) and 2(i), we sample E = 1.55 and since \u03c4parent(j) + E = 2.42 + 1.55 = 3.97 \u2264 \u03bb =\u221e, we introduce a new split x1 > 0.47.\nmay be executed while introducing a new data point: (i) introduction of a new split \u2018above\u2019 an existing split, (ii) extension of an existing split to the updated extent of the block and (iii) splitting an existing leaf node into two children. To the best of our knowledge, existing online decision trees use just the third operation, and the first two operations are unique to Mondrian trees. The complete pseudo-code for incrementally updating a Mondrian tree T with new data D according to MTx(\u03bb, T,D) is described in the following two algorithms. Figure 2 walks through the algorithms on a toy dataset.\nAlgorithm 3 ExtendMondrianTree(T, \u03bb,D) 1: Input: Tree T = (T, \u03b4, \u03be, \u03c4 ), new training instance D = (x, y) 2: ExtendMondrianBlock(T, \u03bb, ,D) . Algorithm 4\nAlgorithm 4 ExtendMondrianBlock(T, \u03bb, j,D) 1: Set e` = max(`xj \u2212 x, 0) and eu = max(x\u2212 uxj , 0) . e` = eu = 0D if x \u2208 Bxj 2: Sample E from exponential distribution with rate \u2211 d(e ` d + e u d)\n3: if \u03c4parent(j) + E < \u03c4j then . introduce new parent for node j 4: Sample split dimension \u03b4, choosing d with probability proportional to e`d + e u d 5: Sample split location \u03be uniformly from interval [uxj,\u03b4, x\u03b4] if x\u03b4 > u x j,\u03b4 else [x\u03b4, ` x j,\u03b4]. 6: Insert a new node \u0303 just above node j in the tree, and a new leaf j\u2032\u2032, sibling to j, where 7: \u03b4\u0303 = \u03b4, \u03be\u0303 = \u03be, \u03c4\u0303 = \u03c4parent(j) + E, ` x \u0303 = min(` x j ,x), u x \u0303 = max(u x j ,x) 8: j\u2032\u2032 = left(\u0303) iff x\u03b4\u0303 \u2264 \u03be\u0303 9: SampleMondrianBlock ( j\u2032\u2032,D, \u03bb\n) 10: else 11: Update `xj \u2190 min(`xj ,x),uxj \u2190 max(uxj ,x) . update extent of node j 12: if j /\u2208 leaves(T) then . return if j is a leaf node, else recurse down the tree 13: if x\u03b4j \u2264 \u03bej then child(j) = left(j) else child(j) = right(j) 14: ExtendMondrianBlock(T, \u03bb, child(j),D) . recurse on child containing D\nIn practice, random forest implementations stop splitting a node when all the labels are identical and assign it to be a leaf node. To make our MF implementation comparable, we \u2018pause\u2019 a Mondrian block when all the labels are identical; if a new training instance lies within Bj of a paused leaf node j and has the same label as the rest of the data points in Bj , we continue pausing the Mondrian block. We \u2018un-pause\u2019 the Mondrian block when there is more than one unique label in a block. Algorithms 9 and 10 in the appendix discuss versions of SampleMondrianBlock and ExtendMondrianBlock for paused Mondrians.\nPrediction using Mondrian tree Let x denote a test data point. If x is already \u201ccontained\u201d in the tree T , i.e., if x \u2208 Bxj for some leaf j \u2208 leaves(T ), then the prediction is taken to be Gleaf(x). Otherwise, we somehow need to incorporate x. One choice is to extend T by sampling T \u2032 from MTx(\u03bb, T,x) as described in Algorithm 3, and set the prediction to Gj , where j \u2208 leaves(T \u2032) is the leaf node containing x. A particular extension T \u2032 might lead to an overly confident prediction; hence, we average over every possible extension T \u2032. This integration can be carried out analytically and the computational complexity is linear in the depth of the tree. We refer the reader to Appendix B for further details."}, {"heading": "6 Related work", "text": "The literature on random forests is vast and we do not attempt to cover it comprehensively; we provide a brief review here and refer to [5] and [7] for a recent review of random forests in\nbatch and online settings respectively. Classic decision tree induction procedures choose the best split dimension and location from all candidate splits at each node by optimizing some suitable quality criterion (e.g. information gain) in a greedy manner. In a random forest, the individual trees are randomized to de-correlate their predictions. The most common strategies for injecting randomness are (i) bagging [1] and (ii) randomly subsampling the set of candidate splits within each node.\nTwo popular random forest variants in the batch setting are Breiman-RF [2] and Extremely randomized trees (ERT) [9]. Breiman-RF uses bagging and furthermore, at each node, a random k-dimensional subset of the original D features is sampled. ERT chooses a k dimensional subset of the features and then chooses one split location each for the k features randomly (unlike Breiman-RF which considers all possible split locations along a dimension). ERT does not use bagging. When k = 1, the ERT trees are totally randomized and the splits are chosen independent of the labels; hence the ERT-1 method is very similar to MF in the batch setting in terms of tree induction. (Note that unlike ERT, MF uses HNSP to smooth predictive estimates and allows a test point to branch off into its own node.) Perfect random trees (PERT), proposed by Cutler and Zhao [6] for classification problems, produce totally randomized trees similar to ERT-1, although there are some slight differences [9].\nExisting online random forests [7, 15] start with an empty tree and grow the tree incrementally. Every leaf of every tree maintains a list of k candidate splits and associated quality scores. When a new data point is added, the scores of the candidate splits at the corresponding leaf node are updated. To reduce the risk of choosing a sub-optimal split based on noisy quality scores, additional hyper parameters such as the minimum number of data points at a leaf node before a decision is made and the minimum threshold for the quality criterion of the best split, are used to assess \u2018confidence\u2019 associated with a split. Once these criteria are satisfied at a leaf node, the best split is chosen (making this node an internal node) and its two children are the new leaf nodes (with their own candidate splits), and the process is repeated. These methods could be memory inefficient for deep trees due to the high cost associated with maintaining candidate quality scores for the fringe of potential children [7].\nThere has been some work on incremental induction of decision trees, e.g. incremental CART [4], ITI [18], VFDT [8] and dynamic trees [16], but to the best of our knowledge, these are focused on learning decision trees and have not been generalized to online random forests. We do not compare MF to incremental decision trees, since random forests are known to outperform single decision trees."}, {"heading": "7 Empirical evaluation", "text": "The purpose of these experiments is to evaluate the predictive performance (test accuracy) of MF as a function of (i) fraction of training data and (ii) training time. We divide the training data into 100 mini-batches and we compare the performance of online random forests (MF, ORF-Saffari) to batch random forests (Breiman-RF, ERT-k, ERT-1) which are trained on the same fraction of the training data. Our scripts are implemented in Python. We implemented the ORF-Saffari algorithm as well as ERT in Python for timing comparisons. The scripts will be made publicly available. We did not implement the ORF-Denil algorithm since its performance is very similar to ORF-Saffari [7] and the computational complexity of the ORF-Denil algorithm is worse than that of ORF-Saffari. We used the Breiman-RF implementation in scikit-learn [12].3\n3The scikit-learn implementation uses highly optimized C code, hence we do not compare our runtimes with the scikit-learn implementation. The ERT implementation in scikit-learn achieves very similar test accuracy as our ERT implementation, hence we do not report those results here.\nWe evaluate on four of the five datasets used in [15] \u2014 we excluded the mushroom dataset as even very simple logical rules achieve > 99% accuracy on this dataset.4 We re-scaled the datasets such that each feature takes on values in the range [0, 1] (by subtracting the min value along that dimension and dividing by the range along that dimension, where range = max\u2212min).\nAs common in the random forest literature, we set the number of trees M = 100. For Mondrian forests, we set \u03bb =\u221e, \u03b3 = 10D. For ORF-Saffari, we set num epochs = 20 (number of passes through the training data) and set the other hyper parameters to the values used in [15]. For Breiman-RF and ERT, the hyper parameters are set to default values. We repeat each algorithm with five random initializations and report the mean performance. The results are shown in Figure 3. (The * in Breiman-RF* indicates scikit-learn implementation.)\nComparing test accuracy vs fraction of training data on usps, satimages and letter datasets, we observe that MF achieves accuracy very close to the batch RF versions (BreimanRF, ERT-k, ERT-1) trained on the same fraction of the data. MF significantly outperforms ORF-Saffari trained on the same fraction of training data. In batch RF versions, the same training data can be used to evaluate candidate splits at a node and its children. However, in the online RF versions (ORF-Saffari and ORF-Denil), incoming training examples are used to evaluate candidate splits just at a current leaf node and new training data are required to evaluate candidate splits every time a new leaf node is created. Saffari et al. [15] recommend multiple passes through the training data to increase the effective number of training samples. In a realistic streaming data setup, where training examples cannot be stored for multiple passes, MF would require significantly fewer examples than ORF-Saffari to achieve the same accuracy.\nComparing test accuracy vs training time on usps, satimages and letter datasets, we observe that MF is at least an order of magnitude faster than re-trained batch versions and ORF-Saffari. For ORF-Saffari, we plot test accuracy at the end of every additional pass; hence it contains additional markers compared to the top row which plots results after a single pass. Re-training batch RF using 100 mini-batches is unfair to MF; in a streaming data setup where the model is updated when a new training instance arrives, MF would be significantly faster than the re-trained batch versions. Assuming trees are balanced after adding each data point, it can be shown that computational complexity of MF scales as O(N logN) whereas the re-trained batch RF scales as O(N2 logN) (Appendix C).\nIt is remarkable that choosing splits independent of labels achieves competitive classification performance. This phenomenon has been observed by others as well\u2014for example, Cutler and Zhao [6] demonstrate that their PERT classifier (which is similar to batch version of MF) achieves test accuracy comparable to Breiman-RF on many real world datasets. However, in the presence of irrelevant features, methods which choose splits independent of labels (MF, ERT-1) perform worse than Breiman-RF and ERT-k (but still better than ORF-Saffari) as indicated by the results on the dna dataset. We trained MF and ERT-1 using just the most relevant 60 attributes amongst the 180 attributes5\u2014these results are indicated as MF\u2020 and ERT-1\u2020 in Figure 3. We observe that, as expected, filtering out irrelevant features significantly improves performance of MF and ERT-1."}, {"heading": "8 Discussion", "text": "We have introduced Mondrian forests, a new random forest variant which can be trained incrementally in an efficient manner. MF significantly outperforms existing online random forests in terms of training time as well as number of training instances required to achieve a\n4https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.names 5https://www.sgi.com/tech/mlc/db/DNA.names\nparticular test accuracy. Remarkably, MF achieves competitive test accuracy to batch random forests trained on the same fraction of the data. MF is unable to handle lots of irrelevant features (since splits are chosen independent of the labels)\u2014one way to use labels to guide splits is via recently proposed Sequential Monte Carlo algorithm for decision trees [11]. The computational complexity of MF is linear in the number of dimensions (since rectangles are represented explicitly) which could be expensive for high dimensional data; we will address this limitation in future work. Random forests have been tremendously influential in machine learning for a variety of tasks; hence lots of other interesting extensions of this work are possible, e.g. MF for regression, theoretical bias-variance analysis of MF, extensions of MF that use hyperplane splits instead of axis-aligned splits."}, {"heading": "Acknowledgments", "text": "We would like to thank Charles Blundell, Gintare Dziugaite, Creighton Heaukulani, Jose\u0301 Miguel Herna\u0301ndez-Lobato, Maria Lomeli, Alex Smola, Heiko Strathmann, and Srini Turaga for helpful discussions and feedback on drafts. BL gratefully acknowledges generous funding from the Gatsby Charitable Foundation. This research was carried out in part while DMR held a Research Fellowship at Emmanuel College, Cambridge, with funding also from a Newton International Fellowship through the Royal Society. YWT\u2019s research leading to these results has received funding from the European Research Council under the European Union\u2019s Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617411."}, {"heading": "A Posterior inference and prediction using the HNSP", "text": "Recall that we use a hierarchical Bayesian approach to specify a smooth label distribution pT (y|x,D1:N ) for each tree T . The label prediction at a test point x will depend on where x falls relative to the existing data in the tree T . In this section, we assume that x lies within one of the leaf nodes in T , i.e., x \u2208 Bxleaf(x), where leaf(x) \u2208 leaves(T ). If x does not lie within any of the leaf nodes in T , i.e., x /\u2208 \u222aj\u2208leaves(T )Bxj , one could extend the tree by sampling T \u2032 from MTx(\u03bb, T,x), such that x lies within a leaf node in T \u2032 and apply the procedure described below using the extended tree T \u2032. Appendix B describes this case in more detail.\nGiven training data D1:N , a Mondrian tree T and the hierarchical prior over G, the predictive label distribution pT (y|x,D1:N ) is obtained by integrating over G, i.e.\npT (y|x,D1:N ) = EG\u223cpT (G|D1:N )[pT (y|x,G)] = EG\u223cpT (G|D1:N )[Gleaf(x),y] = Gleaf(x),y. (6)\nHence, the prediction is given by Gleaf(x), the posterior mean at leaf(x). The posterior mean Gleaf(x) can be computed using existing techniques, which we review in the rest of this section. Posterior inference in the HNSP is a special case of posterior inference in hierarchical PYP (HPYP). Teh [17] considers the HPYP with multinomial likelihood (in the context of language modeling)\u2014the model considered here (HNSP with multinomial likelihood) is a special case of [17]. Hence, we just sketch the high level picture and refer the reader to [17] for further details. We first describe posterior inference given N data points D1:N (batch setting), and later explain how to adapt inference to the online setting. Finally, we describe the computation of the predictive posterior distribution.\nBatch setting\nPosterior inference is done using the Chinese restaurant process representation, wherein every node of the decision tree is a restaurant; the training data points are the customers seated in the tables associated with the leaf node restaurants; these tables are in turn customers at the tables in their corresponding parent level restaurant; the dish served at each table is the class label. Exact inference is intractable and hence we resort to approximations. In particular, we use the approximation known as the interpolated Kneser-Ney (IKN) smoothing, a popular smoothing technique for language modeling [10]. The IKN smoothing can be interpreted as an approximate inference scheme for the HPYP, where the number of tables serving a particular dish in a restaurant is at most one [17]. More precisely, if cj,k denotes the number of customers at restaurant j eating dish k and tabj,k denotes the number of tables at restaurant j serving dish k, the IKN approximation sets tabj,k = min(cj,k, 1). The counts cj,k and tabj,k can be computed in a single bottom-up pass as follows: for every leaf node j \u2208 leaves(T), cj,k is simply the number of training data points with label k at node j; for every internal node j \u2208 T \\ leaves(T), we set cj,k = tableft(j),k + tabright(j),k. For a leaf node j, this procedure is summarized in Algorithm 5. (Note that this pseudocode just serves as a reference; in practice, these counts are updated in an online fashion, as described in Algorithm 6.)\nPosterior inference: online setting\nIt is straightforward to extend inference to the online setting. Adding a new data point (x, y) affects only the counts along the path from the root to the leaf node of that data point. We update the counts in a bottom-up fashion, starting at the leaf node containing the data point,\nAlgorithm 5 InitializePosteriorCounts(j)\n1: For all k, set cjk = #{i \u2208 N(j) : yi = k} 2: Initialize j\u2032 = j 3: while True do 4: if j\u2032 /\u2208 leaves(T) then 5: For all k, set cj\u2032k = tableft(j\u2032),k + tabright(j\u2032),k\n6: For all k, set tabj\u2032k = min(cj\u2032k, 1) . IKN approximation 7: if j\u2032 = then 8: return 9: else 10: j\u2032 \u2190 parent(j\u2032)\nleaf(x). Due to the nature of the IKN approximation, we can stop at the internal node j where cj,y = 1 and need not traverse up till the root. This procedure is summarized in Algorithm 6.\nAlgorithm 6 UpdatePosteriorCounts(j, y)\n1: cjy \u2190 cjy + 1 2: Initialize j\u2032 = j 3: while True do 4: if tabj\u2032y = 1 then . none of the counts above need to be updated 5: return 6: else 7: if j\u2032 /\u2208 leaves(T) then 8: cj\u2032y = tableft(j\u2032),y + tabright(j\u2032),y"}, {"heading": "9: tabj\u2032y = min(cj\u2032y, 1) . IKN approximation", "text": "10: if j\u2032 = then 11: return 12: else 13: j\u2032 \u2190 parent(j\u2032)\nPredictive posterior computation Given the counts cj,k and table assignments tabj,k, the predictive probability (i.e., posterior mean) at node j can be computed recursively as follows:\nGjk =\n{ cj,k\u2212djtabj,k\ncj,\u00b7 + djtabj,\u00b7 cj,\u00b7 Gparent(j),k cj,\u00b7 > 0,\nGparent(j),k cj,\u00b7 = 0, (7)\nwhere cj,\u00b7 = \u2211 k cj,k, tabj,\u00b7 = \u2211 k tabj,k, and dj := exp ( \u2212\u03b3(\u03c4j \u2212 \u03c4parent(j)) ) is the \u201cdiscount\u201d for node j, defined in Section 4. Informally, the discount interpolates between the counts c and the prior. If the discount dj \u2248 1, then Gj is more like its parent Gparent(j). If dj \u2248 0, then Gj weights the counts more. These predictive probabilities can be computed in a single top-down pass as shown in Algorithm 7."}, {"heading": "B Prediction using Mondrian tree", "text": "Let x denote a test data point. We are interested in the predictive probability of y at x, denoted by pT (y|x,D1:N ). As in typical decision trees, the process involves a top-down tree traversal, starting from the root. If x is already \u201ccontained\u201d in the tree T , i.e., if x \u2208 Bxj\nAlgorithm 7 ComputePosteriorPredictiveDistribution ( T,G ) 1: . Description of top-down pass to compute posterior predictive distribution given by (7) 2: . Gjk denotes the posterior probability of y = k at node j 3: Initialize the ordered set J = { } 4: while J not empty do 5: Pop the first element of J 6: if j = then 7: Gparent( ) = H\n8: Set d = exp ( \u2212\u03b3(\u03c4j \u2212 \u03c4parent(j)) ) 9: For all k, set Gjk = c \u22121 j,\u00b7 ( cj,k \u2212 d tabj,k + d tabj,\u00b7 Gparent(j),k\n) 10: if j /\u2208 leaves(T) then 11: Append left(j) and right(j) to the end of the ordered set J\nfor some leaf j \u2208 leaves(T ), then the prediction is taken to be Gleaf(x), which is computed as described in Appendix A. Otherwise, we somehow need to incorporate x. One choice is to extend T by sampling T \u2032 from MTx(\u03bb, T,D1:n,x) as described in Algorithm 3, and set the prediction to Gj , where j \u2208 leaves(T \u2032) is the leaf node containing x. A particular extension T \u2032 might lead to an overly confident prediction; hence, we average over every possible extension T \u2032. This expectation can be carried out analytically, using properties of the Mondrian process, as we show below.\nLet ancestors(j) denote the set of all ancestors of node j. Let path(j) = {j} \u222a ancestors(j), that is, the set of all nodes along the ancestral path from j to the root. Recall that leaf(x) is the unique leaf node in T such that x \u2208 Bleaf(x). If the test point x \u2208 Bxleaf(x) (i.e., x lies within the \u2018gray rectangle\u2019 at the leaf node), it can never branch off; else, it can branch off at one or more points along the path from the root to leaf(x). More precisely, if x lies outside Bxj at node j, the probability that x will branch off into its own node at node j, denoted by 6 psj(x), is equal to the probability that a split exists in Bj outside B x j , which is\npsj(x) = 1\u2212 exp ( \u2212\u2206j\u03b7j(x) ) , where \u03b7j(x) = \u2211 d ( max(xd \u2212 uxjd, 0) + max(`xjd \u2212 xd, 0) ) ,\nand \u2206j = \u03c4j \u2212 \u03c4parent(j). Note that psj(x) = 0 if x lies within Bxj (i.e., if `xjd \u2264 xd \u2264 uxjd for all d). The probability of x not branching off before reaching node j is given by\u220f j\u2032\u2208ancestors(j)(1\u2212 psj\u2032(x)).\nIf x \u2208 Bxleaf(x), the prediction is given by Gleaf(x). If there is a split in Bj outside B x j , let \u0303 denote the new parent of j and child(\u0303) denote the child node containing just the test data point,; in this case, the prediction is Gchild(\u0303). Averaging over the location where the test point branches off, we obtain\npT (y|x,D1:N ) = \u2211\nj\u2208path(leaf(x)) ( \u220f j\u2032\u2208ancestors(j) (1\u2212 psj\u2032(x)) ) Fj(x), (8)\nwhere\nFj(x) = p s j(x)E\u2206\u0303 [ Gchild(\u0303) ] + 1[j = leaf(x)](1\u2212 psj(x))Gleaf(x). (9)\nThe second term in Fj(x) needs to be computed only for the leaf node leaf(x) and is simply the posterior mean of Gleaf(x) weighted by 1\u2212 psleaf(x)(x). The posterior mean of Gleaf(x), given\n6The superscript s in psj(x) is used to denote the fact that this split \u2018separates\u2019 the test data point x into its own leaf node.\nby Gleaf(x), can be computed using (7). The first term in Fj(x) is simply the posterior mean of Gchild(\u0303), averaged over \u2206\u0303, weighted by p s j(x). Since no labels are observed in child(\u0303), cchild(\u0303),\u00b7 = 0, hence from (7), we have Gchild(\u0303) = G\u0303. We compute G\u0303 using (7). We average over \u2206\u0303 due to the fact that the discount in (7) for the node \u0303 depends on \u03c4\u0303 \u2212 \u03c4parent(\u0303) = \u2206\u0303. To average over all valid split times \u03c4\u0303, we compute expectation w.r.t. \u2206\u0303 which is distributed according to a truncated exponential with rate \u03b7j(x), truncated to the interval [0,\u2206j ].\nThe procedure for computing pT (y|x,D1:N ) for any x \u2208 RD is summarized in Algorithm 8.\nAlgorithm 8 Predict ( T,x ) 1: . Description of prediction using a Mondrian tree, given by (8) 2: Initialize j = and pNotSeparatedYet = 1 3: Initialize s = 0K . s is K-dimensional vector where sk = pT (y = k|x,D1:N ) 4: while True do 5: Set \u2206j = \u03c4j \u2212 \u03c4parent(j) and \u03b7j(x) = \u2211 d ( max(xd \u2212 uxjd, 0) + max(`xjd \u2212 xd, 0)\n) 6: Set psj(x) = 1\u2212 exp ( \u2212\u2206j\u03b7j(x)\n) 7: if psj(x) > 0 then 8: . Let x branch off into its own node child(\u0303), creating a new node \u0303 which is the\nparent of j and child(\u0303). Gchild(\u0303) = G\u0303 from (7) since cchild(\u0303),\u00b7 = 0. 9: Compute expected discount d\u0304 = E\u2206[exp(\u2212\u03b3\u2206)] where \u2206 is drawn from a truncated\nexponential with rate \u03b7j(x), truncated to the interval [0,\u2206j ]. 10: For all k, set c\u0303,k = tab\u0303,k = min(cj,k, 1) 11: For all k, set G\u0303k = c \u22121 \u0303,\u00b7 ( c\u0303,k \u2212 d\u0304 tab\u0303,k + d\u0304 tab\u0303,\u00b7 Gparent(\u0303),k ) . Algorithm 7 and\n(9) 12: For all k, update sk \u2190 sk + pNotSeparatedYet psj(x)G\u0303k 13: if j \u2208 leaves(T) then 14: For all k, update sk \u2190 sk + pNotSeparatedYet(1\u2212 psj(x))Gjk . Algorithm 7 and (9) 15: return y\u0302 = argmaxk sk 16: else 17: pNotSeparatedYet \u2190 pNotSeparatedYet(1\u2212 psj(x)) 18: if x\u03b4j \u2264 \u03bej then j \u2190 left(j) else j \u2190 right(j) . recurse to the child where x lies"}, {"heading": "C Computational complexity", "text": "We discuss the computational complexity associated with a single Mondrian tree. The complexity of a forest is simply M times that of a single tree; however, this computation can be trivially parallelized since there is no interaction between the trees. Assume that the N data points are processed one by one. Assuming the data points form a balanced binary tree after each update, the computational cost of processing the nth data point is at most O(log n) (add the data point into its own leaf, update posterior counts for HNSP in bottom-up pass from leaf to root). The overall cost to process N data points is O( \u2211N n=1 log n) = O(logN !), which for large N tends to O(N logN) (using Stirling approximation for the factorial function). For offline RF and ERT, the expected complexity with n data points is O(n log n). The complexity of the re-trained version is O( \u2211N n=1 n log n) = O(log \u220fN n=1 n\nn), which for large N tends to O(N2 logN) (using asymptotic expansion of the hyper factorial function)."}, {"heading": "D Pseudocode for paused Mondrians", "text": "Algorithm 9 SampleMondrianBlock ( j,DN(j), \u03bb ) version that depends on labels\n1: Add j to T 2: \u2200d, set `xjd = min(XN(j),d), uxjd = max(XN(j),d) . dimension-wise min and max 3: if AllLabelsIdentical(YN(j)) then 4: Set \u03c4j = \u03bb . pause Mondrian 5: else 6: Sample E from exponential distribution with rate \u2211 d(ujd \u2212 `jd) 7: Set \u03c4j = \u03c4parent(j) + E\n8: if \u03c4j < \u03bb then 9: Sample \u03b4j with probability of choosing d proportional to u x jd \u2212 `xjd 10: Sample split location \u03bej along dimension \u03b4j from an uniform distribution over U [`xjd, uxjd] 11: Set N(left(j)) = {n \u2208 N(j) : xn\u03b4j \u2264 \u03bej} and N(right(j)) = {n \u2208 N(j) : xn\u03b4j > \u03bej} 12: SampleMondrianBlock ( left(j),DN(left(j)), \u03bb\n) 13: SampleMondrianBlock ( right(j),DN(right(j)), \u03bb\n) 14: else 15: Set \u03c4j = \u03bb and add j to leaves(T) . j is a leaf node 16: InitializePosteriorCounts(j) . Algorithm 5\nAlgorithm 10 ExtendMondrianBlock(T, \u03bb, j,D) version that depends on labels 1: if AllLabelsIdentical(YN(j)) then . paused Mondrian leaf 2: Update extent `xj \u2190 min(`xj ,x),uxj \u2190 max(uxj ,x) 3: Append D to DN(j) . append x to XN(j) and y to YN(j) 4: if y = unique(YN(j)) then 5: UpdatePosteriorCounts(j, y) . Algortithm 6 6: return . continue pausing 7: else 8: Remove j from leaves(T) 9: SampleMondrianBlock ( j,DN(j), \u03bb ) . un-pause Mondrian\n10: else 11: Set e` = max(`xj \u2212 x, 0) and eu = max(x\u2212 uxj , 0) . e` = eu = 0D if x \u2208 Bxj 12: Sample E \u223c Exp( \u2211 d(e ` d + e u d)) 13: if E + \u03c4parent(j) < \u03c4j then . introduce new parent for node j 14: Create new Mondrian block \u0303 where `x\u0303 = min(` x j ,x) and u x \u0303 = max(u x j ,x) 15: Sample \u03b4\u0303 with Pr(\u03b4\u0303 = d) proportional to e ` d + e u d 16: if x\u03b4\u0303 > u x j,\u03b4\u0303 , then sample \u03be\u0303 from U [uxj,\u03b4\u0303 , x\u03b4\u0303 ], else sample \u03be\u0303 from U([x\u03b4\u0303 , ` x j,\u03b4\u0303\n]) 17: if j = then . set \u0303 as the new root 18: \u2190 \u0303 19: else . set \u0303 as child of parent(j) 20: if j = left(parent(j)), then left(parent(j))\u2190 \u0303, else right(parent(j))\u2190 \u0303 21: if x\u03b4\u0303 > \u03be\u0303 then 22: Set left(\u0303) = j and SampleMondrianBlock ( right(\u0303),D, \u03bb ) . create new leaf for x 23: else 24: Set right(\u0303) = j and SampleMondrianBlock ( left(\u0303),D, \u03bb ) . create new leaf for x\n25: else 26: Update `xj \u2190 min(`xj ,x),uxj \u2190 max(uxj ,x) . update extent of node j 27: if j /\u2208 leaves(T) then . return if j is a leaf node, else recurse down the tree 28: if x\u03b4j \u2264 \u03bej then child(j) = left(j) else child(j) = right(j) 29: ExtendMondrianBlock(T, \u03bb, child(j),D) . recurse on child containing x"}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "<lb>Ensembles of randomized decision trees, usually referred to as random forests, are<lb>widely used for classification and regression tasks in machine learning and statistics.<lb>Random forests achieve competitive predictive performance and are computationally<lb>efficient to train and test, making them excellent candidates for real-world prediction<lb>tasks. The most popular random forest variants (such as Breiman\u2019s random forest and<lb>extremely randomized trees) operate on batches of training data. Online methods are<lb>now in greater demand. Existing online random forests, however, require more training<lb>data than their batch counterpart to achieve comparable predictive performance. In<lb>this work, we use Mondrian processes (Roy and Teh, 2009) to construct ensembles of<lb>random decision trees we call Mondrian forests. Mondrian forests can be grown in an<lb>incremental/online fashion and remarkably, the distribution of online Mondrian forests<lb>is the same as that of batch Mondrian forests. Mondrian forests achieve competitive<lb>predictive performance comparable with existing online random forests and periodically<lb>re-trained batch random forests, while being more than an order of magnitude faster,<lb>thus representing a better computation vs accuracy tradeoff.", "creator": "LaTeX with hyperref package"}}}