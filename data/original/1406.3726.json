{"id": "1406.3726", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2014", "title": "Evaluation of Machine Learning Techniques for Green Energy Prediction", "abstract": "We evaluate the following Machine Learning techniques for Green Energy (Wind, Solar) Prediction: Bayesian Inference, Neural Networks, Support Vector Machines, Clustering techniques (PCA). Our objective is to predict green energy using weather forecasts, predict deviations from forecast green energy, find correlation amongst different weather parameters and green energy availability, recover lost or missing energy (/ weather) data. We use historical weather data and weather forecasts for the same.", "histories": [["v1", "Sat, 14 Jun 2014 13:08:30 GMT  (5kb)", "http://arxiv.org/abs/1406.3726v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ankur sahai"], "accepted": false, "id": "1406.3726"}, "pdf": {"name": "1406.3726.pdf", "metadata": {"source": "CRF", "title": "Evaluation of Machine Learning Techniques for Green Energy Prediction", "authors": ["Ankur Sahai"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n37 26\nv1 [\ncs .L\nG ]\n1 4\nJu n\n20 14\nEvaluation of Machine Learning Techniques for Green Energy Prediction\nAnkur Sahai University of Mainz, Germany"}, {"heading": "1 Objective", "text": "We evaluate Machine Learning techniques for Green energy (wind, solar and biomass) prediction based on weather forecasts. Weather is constituted by multiple attributes: temperature, cloud cover, wind speed / direction which are discrete random variables. One of our objectives is to predict the weather based on the previous weather data. Additionally we are interested in finding correlation (dependencies in order to reduce the dimensionality of the data set) between these variables, predicting missing data predict deviations in weather forecasts (for job scheduling within the green control center), finding clusters within the data (constituted by closely related variables e.g. PCA that can be used to remove redundant variables), classification, finding (non-linear using SVMs) regression models, training artificial neural networks based on the historical data so that they can be used for prediction in the future."}, {"heading": "2 Machine learning techniques", "text": "We analyze the following machine learning techniques:\n\u2022 Bayesian inference\n\u2022 Neural Networks\n\u2022 Support Vector Machines\n\u2022 Clustering technique: PCA"}, {"heading": "2.1 Bayesian learning", "text": "Bayesian learning is based on using previous information to predict the future. It is based on the Bayes? rule\np(X/Y) = P(X \u2227 Y)/P(Y) (1)\n= P(Y/X) \u2217 P(X)/P(Y) (2)\n=\u21d2 p(X/Y) \u221d P(X) (3)\n=\u21d2 p(X/Y) \u221d (1/P(Y)) (4)\nIf we consider the set of all the events that are correlated to X (i.e. that X depends upon) say Y1, \u00b7 \u00b7 \u00b7 , Yn, then from (1) we get:\nX = (X \u2227 Y1) \u2228 \u00b7 \u00b7 \u00b7 \u2228 (X \u2227 Yn) (5)\n=\u21d2 p(X) = p(X \u2227 Y1) + \u00b7 \u00b7 \u00b7 + p(X \u2227 Yn) (6)\np(X) = p(X/Y1) \u2217 p(Y1) + \u00b7 \u00b7 \u00b7 + p(X/Yn) \u2217 p(Yn) (7)\np(X) = n\u2211\ni=1\np(X/Yi) \u2217 p(Yi) (8)\nBayesian networks use a directed acyclic graph for the different random variables (represented using nodes) and their conditional dependencies (represented using edges) and uses inference algorithm to make prediction. Different graphical techniques like belief propagation or expectation maximization can be used to spread the information within the Bayesian network. Expectation maximization algorithms are used to model scenarios, which consist of latent variables, and consist of two steps. The structure of a dynamic Bayesian network changes with time.\nOne simple example is a Hidden Markov Model. Markov model is a graph where each node corresponds to a state and has an associated transition probability to the neighboring nodes (which measure the probability with which the state transition will happen to the neighboring nodes). In the Hidden Markov Model some states are hidden as in the transition probability for these nodes (how the system behaves when in the state corresponding to these nodes) is not known.\nBelief propagation includes algorithm like sum-product message passing. This technique is based on using the information stored in the neighboring nodes along with conditional probability to pass message within the network. It can also be represented using a Linear programming formulation. If the input variable follows a distribution such as Gaussian, it becomes much easier to use Bayesian inference. One of the disadvantages is that it is computationally hard although it is conceptually simple.\nThis technique can also be used to recover the missing parameters / variables based on the previous value (prior distribution) and correlation with other parameters."}, {"heading": "2.2 Clustering techniques", "text": "These techniques are used to identify closely related data points in any given data set. K-means clustering is a commonly studied example where the data points have to be grouped into K clusters such that the mean distance between the clusters is minimized. These techniques can be used to identify patterns in a weather plot say in a temperature vs. cloud cover plot where, region with higher density of points can be identified (which can be used to infer when the temperature is of certain value then it is more likely that the cloud cover is of certain value).\nPrincipal Component Analysis (PCA) is one such technique that is based on generating linearly uncorrelated variable (principal components) from a data set of multiple correlated variables."}, {"heading": "2.3 Binary Decision Trees", "text": "This can be used for scheduling the jobs based on deviation between the predicted and actual weather conditions. One can calculate different schedules based upon the different weather forecast / prediction and store it in a binary decision tree. Later on depending on the deviation from the actual weather data one can use the rules stored in the nodes of the decision tree to select one of the schedules."}, {"heading": "2.4 Support Vector Machines", "text": "SVM is a supervised machine learning technique, which is used for binary classification of the input into different sets after training the model with previous data. It can\nwork in multiple dimensions represented by different variables. SVM works by building a model which is used for classification.\nIt is based on a kernel method which is used for classification / regression, reducing dimensionality or clustering. Further they can also be used for nonlinearizing the regression and avoiding over / underfitting. This is important because it is not possible to find a linear fit for many of the observed data in real world. In a simple linear regression techniques such as minimizing least square error the decision to select a regression function for the input data is based on fitting a line that minimizes the total square error for the entire data-set. However, for a data set that is very unevenly distributed the linear regression may be not be the best suited technique. Kernel techniques also make it easier to apply linear methods in higher dimensions. Another advantage of Kernel methods is that they allow comparison of two objects consisting of multiple attributes.\nAnother perspective to understanding SVMs is to view it as a Minimization (optimization) problem where the goal is to minimize the average distance between the two clusters and the hyperplane which is equivalent to identifying a hyperplane which has the maximum distance from the nearest data point from both the clusters. LibSVM and SVMLight are two software libraries that can be used for SVM."}, {"heading": "2.5 Artificial Neural Networks", "text": "This technique is based on the simulation of a biological neural network, which consists of artificial neurons. By training the artificial neurons over a period of time (with historical data) one can predict future output. It is based on studying how the neurons connect (represented by connection weights) and pass information to each other for different inputs. Neurons collect the input from neighboring neurons and based on the magnitude of this input decide whether to fire or not. There can be multiple layers in a ANN e.g. input, hidden and output layer.\nAdditionally we suggest the following techniques: Gibbs sampling, Ridge regression, Mean field value, RBF kernel."}, {"heading": "3 Conclusion", "text": "Bayesian inference can be used to predict the values of missing variables. Clustering techniques can be used to identify similarities between points in a data set in addition to removing redundancy. Binary decision tree can be used for dynamic scheduling (based on rules stored on nodes) within the green control center. SVMs and ANNs have to be analyzed further on if they can be useful for weather prediction."}], "references": [{"title": "Guttirez, Bayesian networks for probabilistic weather prediction", "author": ["Antonio S. Cofino", "Rafael Cano", "Carmen Sordo", "Jose M"], "venue": "In Proceedings of the 15th European Conference on Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Warm season lightning probability prediction for Canada and the northern United States", "author": ["Burrows", "William R", "Colin Price", "Laurence J. Wilson"], "venue": "Wea. Forecasting,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Calibrated Probabilistic Mesoscale Weather Field Forecasting: The Geostatistical Output Perturbation Method", "author": ["Yulia Gel", "Adrian E. Raftery", "Tilmann Gneiting", "Claudia Tebaldi", "Doug Nychka", "William Briggs", "Mark S. Roulston", "Veronica J. Berrocal"], "venue": "Journal of the American Statistical Association ,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "NUMERICAL WEATHER PREDICTION: A vision of the future, Weather", "author": ["M.E. McIntyre"], "venue": "Blackwell Publishing Ltd. 1477-8696,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1988}, {"title": "GreenSlot: scheduling energy consumption in green datacenters", "author": ["Inigo Goiri", "Ryan Beauchea", "Kien Le", "Thu D. Nguyen", "Md. E. Haque", "Jordi Guitart", "Jordi Torres", "Ricardo Bianchini"], "venue": "Proceedings of 2011 International Conference for High Performance Computing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "GreenHadoop: Leveraging Green Energy in Data- Processing Frameworks", "author": ["Inigo Goiri", "Kien Le", "Thu D. Nguyen", "Jordi Guitart", "Jordi Torres", "Ricardo Bianchini"], "venue": "European Conference on Computer Systems (EuroSys", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Predicting Solar Generation from Weather Forecasts Using Machine Learning", "author": ["Navin Sharma", "Pranshu Sharma", "David Irwin", "Prashant Shenoy"], "venue": "Proceedings of the Second IEEE International Conference on Smart Grid Communications (SmartGridComm),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}], "referenceMentions": [], "year": 2014, "abstractText": "We evaluate Machine Learning techniques for Green energy (wind, solar and biomass) prediction based on weather forecasts. Weather is constituted by multiple attributes: temperature, cloud cover, wind speed / direction which are discrete random variables. One of our objectives is to predict the weather based on the previous weather data. Additionally we are interested in finding correlation (dependencies in order to reduce the dimensionality of the data set) between these variables, predicting missing data predict deviations in weather forecasts (for job scheduling within the green control center), finding clusters within the data (constituted by closely related variables e.g. PCA that can be used to remove redundant variables), classification, finding (non-linear using SVMs) regression models, training artificial neural networks based on the historical data so that they can be used for prediction in the future.", "creator": "LaTeX with hyperref package"}}}