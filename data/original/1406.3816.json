{"id": "1406.3816", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2014", "title": "Simultaneous Model Selection and Optimization through Parameter-free Stochastic Learning", "abstract": "Stochastic gradient descent algorithms for training linear and kernel predictors are gaining more and more importance, thanks to their scalability. While various methods have been proposed to speed up their convergence, the model selection phase is often ignored. In fact, in theoretical works most of the time assumptions are made, for example, on the prior knowledge of the norm of the optimal solution, while in the practical world validation methods remain the only viable approach. In this paper, we propose a new kernel-based stochastic gradient descent algorithm that performs model selection while training, with no parameters to tune, nor any form of cross-validation. The algorithm builds on recent advancement in online learning theory for unconstrained settings, to estimate over time the right regularization in a data-dependent way. Optimal rates of convergence are proved under standard smoothness assumptions on the target function, using the range space of the fractional integral operator associated with the kernel.", "histories": [["v1", "Sun, 15 Jun 2014 13:34:27 GMT  (139kb,D)", "http://arxiv.org/abs/1406.3816v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["francesco orabona"], "accepted": true, "id": "1406.3816"}, "pdf": {"name": "1406.3816.pdf", "metadata": {"source": "CRF", "title": "Simultaneous Model Selection and Optimization through Parameter-free Stochastic Learning", "authors": ["Francesco Orabona"], "emails": ["francesco@orabona.com"], "sections": [{"heading": "1 Introduction", "text": "Stochastic Gradient Descent (SGD) algorithms are gaining more and more importance in the Machine Learning community as efficient and scalable machine learning tools. There are two possible ways to use a SGD algorithm: to optimize a batch objective function, e.g. [23], or to directly optimize the generalization performance of a learning algorithm, in a stochastic approximation way [20]. The second use is the one we will consider in this paper. It allows learning over streams of data, coming Independent and Identically Distributed (IID) from a stochastic source. Moreover, it has been advocated that SGD theoretically yields the best generalization performance in a given amount of time compared to other more sophisticated optimization algorithms [6].\nYet, both in theory and in practice, the convergence rate of SGD for any finite training set critically depends on the step sizes used during training. In fact, often theoretical analysis assumes the use of optimal step sizes, rarely known in reality, and in practical applications wrong step sizes can result in arbitrary bad performance. While in finite hypothesis spaces simple optimal strategies are known [2], in infinite dimensional spaces the only attempts to solve this problem achieve convergence only in the realizable case, e.g. [25], or assume prior knowledge of intrinsic (and unknown) characteristic of the problem [24, 34, 33, 31, 29]. The only known practical and theoretical way to achieve optimal rates in infinite Reproducing Kernel Hilbert Space (RKHS) is to use some form of cross-validation to select the step size that corresponds to a form of model selection [26, Chapter 7.4]. However, cross-validation techniques would result in a slower training procedure partially neglecting the advantage of the stochastic training. A notable exception is the algorithm in [21], that keeps the step size constant and uses the number of epochs on the training set as a regularization procedure. Yet, the number of epochs is decided through the use of a validation set [21].\nar X\niv :1\n40 6.\n38 16\nv1 [\ncs .L\nG ]\nNote that the situation is exactly the same in the batch setting where the regularization takes the role of the step size. Even in this case, optimal rates can be achieved only when the regularization is chosen in a problem dependent way [11, 32, 27, 16].\nOn a parallel route, the Online Convex Optimization (OCO) literature studies the possibility to learn in a scenario where the data are not IID [36, 9]. It turns out that this setting is strictly more difficult than the IID one and OCO algorithms can also be used to solve the corresponding stochastic problems [8]. The literature on OCO focuses on the adversarial nature of the problem and on various ways to achieve adaptivity to its unknown characteristics [1, 13].\nThis paper is in between these two different worlds: We extend tools from OCO to design a novel stochastic parameter-free algorithm able to obtain optimal finite sample convergence bounds in infinite dimensional RKHS. This new algorithm, called Parameter-free STOchastic Learning (PiSTOL), has the same complexity as the plain stochastic gradient descent procedure and implicitly achieves the model selection while training, with no parameters to tune nor the need for cross-validation. The core idea is to change the step sizes over time in a data-dependent way. As far as we know, this is the first algorithm of this kind to have provable optimal convergence rates.\nThe rest of the paper is organized as follows. After introducing some basic notations (Sec. 2), we will explain the basic intuition of the proposed method (Sec. 3). Next, in Sec. 4 we will describe the PiSTOL algorithm and its regret bounds in the adversarial setting and in Sec. 5 we will show its convergence results in the stochastic setting. The detailed discussion of related work is deferred to Sec. 6. Finally, we show some empirical results and draw the conclusions in Sec. 7."}, {"heading": "2 Problem Setting and Definitions", "text": "Let X \u2282 Rd a compact set and HK the RKHS associated to a Mercer kernel K : X \u00d7 X \u2192 R implementing the inner product \u3008\u00b7 , \u00b7\u3009K . The inner product is defined so that it satisfies the reproducing property, \u3008K(x, \u00b7) , f(\u00b7)\u3009K = f(x).\nPerformance is measured w.r.t. a loss function ` : R \u2192 R+. We will consider L-Lipschitz losses, that is |`(x) \u2212 `(x\u2032)| \u2264 L|x \u2212 x\u2032|, \u2200x, x\u2032 \u2208 R, and H-smooth losses, that is differentiable losses with the first derivative H-Lipschitz. Note that a loss can be both Lipschitz and smooth. A vector x is a subgradient of a convex function ` at v if `(u)\u2212 `(v) \u2265 \u3008u\u2212 v,x\u3009 for any u in the domain of `. The differential set of ` at v, denoted by \u2202`(v), is the set of all the subgradients of ` at v. 1(\u03a6) will denote the indicator function of a Boolean predicate \u03a6.\nIn the OCO framework, at each round t the algorithm receives a vector xt \u2208 X , picks a ft \u2208 HK , and pays `t(ft(xt)), where `t is a loss function. The aim of the algorithm is to minimize the regret, that is the difference between the cumulative loss of the algorithm, \u2211T t=1 `t(ft(xt)), and the cumulative loss of an\narbitrary and fixed competitor h \u2208 HK , \u2211T t=1 `t(h(xt)).\nFor the statistical setting, let \u03c1 a fixed but unknown distribution on X \u00d7 Y , where Y = [\u22121, 1]. A training set {xt, yt}Tt=1 will consist of samples drawn IID from \u03c1. Denote by f\u03c1(x) := \u222b Y yd\u03c1(y|x) the regression function, where \u03c1(\u00b7|x) is the conditional probability measure at x induced by \u03c1. Denote by \u03c1X the marginal probability measure on X and let L2\u03c1X be the space of square integrable functions with respect to \u03c1X , whose norm is denoted by \u2016f\u2016L2\u03c1X := \u221a\u222b X f\n2(x)d\u03c1X . Note that f\u03c1 \u2208 L2\u03c1X . Define the `-risk of f , as E`(f) := \u222b X\u00d7Y `(yf(x))d\u03c1. Also, define f ` \u03c1(x) := arg mint\u2208R \u222b Y `(yt)d\u03c1(y|x), that gives the optimal `-risk, E`(f `\u03c1) = inff\u2208L2\u03c1X E `(f). In the binary classification case, define the misclassification risk of f as R(f) := P (y 6= sign(f(x))). The infimum of the misclassification risk over all measurable f will be called Bayes risk and fc := sign(f\u03c1), called the Bayes classifier, is such thatR(fc) = inff\u2208L2\u03c1X R(f).\nLet LK : L2\u03c1X \u2192 HK the integral operator defined by (LKf)(x) = \u222b X K(x, x \u2032)f(x\u2032)d\u03c1X (x \u2032). There exists an orthonormal basis {\u03a61,\u03a62, \u00b7 \u00b7 \u00b7 } of L2\u03c1X consisting of eigenfunctions of LK with corresponding non-negative eigenvalues {\u03bb1, \u03bb2, \u00b7 \u00b7 \u00b7 } and the set {\u03bbi} is finite or \u03bbk \u2192 0 when k \u2192\u221e [12, Theorem 4.7].\nSinceK is a Mercer kernel,LK is compact and positive. Therefore, the fractional power operatorL \u03b2 K is well defined for any \u03b2 \u2265 0. We indicate its range space by\nFigure 1: L2\u03c1X , HK , and L \u03b2 K(L 2 \u03c1X ) spaces, with 0 < \u03b21 < 12 < \u03b22.\nL\u03b2K(L2\u03c1X ) := { f = \u221e\u2211 i=1 ai\u03a6i : \u2211 i:ai 6=0 a2i\u03bb \u22122\u03b2 i <\u221e } . (1)\nBy the Mercer\u2019s theorem, we have that L 1 2\nK(L2\u03c1X ) = HK , that is every function f \u2208 HK can be written as L 1 2\nKg for some g \u2208 L2\u03c1X , with \u2016f\u2016K = \u2016g\u2016L2\u03c1X . On the other hand, by definition of the orthonormal basis, L0K(L2\u03c1X ) = L2\u03c1X . Thus, the smaller \u03b2 is, the bigger this space of the functions will be,1 see Fig. 1. This space has a key role in our analysis. In particular, we will assume that f `\u03c1 \u2208 L\u03b2K(L2\u03c1X ) for \u03b2 > 0, that is\n\u2203g \u2208 L2\u03c1X : f `\u03c1 = L \u03b2 K(g). (2)"}, {"heading": "3 A Gentle Start: ASGD, Optimal Step Sizes, and the Perceptron", "text": "We want to investigate the problem of training a predictor, f\u0304T , on the training set {xt, yt}Tt=1 in a stochastic way, using each sample only once, to have E`(f\u0304T ) converge to E`(f `\u03c1). For the square loss, `(x) = (1\u2212x)2, the Averaged Stochastic Gradient Descent (ASGD) in Algorithm 1 has been proposed as a fast stochastic algorithm to train predictors [35]. ASGD simply goes over all the samples once, updates the predictor with the gradients of the losses, and returns the averaged solution. For ASGD with constant step size 0 < \u03b7 \u2264 14 , it is immediate to show2 that\nE[E`(f\u0304T )] \u2264 inf h\u2208HK E`(h) + \u2016h\u20162K (\u03b7T )\u22121 + 4\u03b7. (3)\nThis result shows the link between step size and regularization: In expectation, the `-risk of the averaged predictor will be close to the `-risk of the best regularized function in HK . Moreover, the amount of regularization depends on the step size used. From (3), one might be tempted to choose \u03b7 = O(T\u2212 12 ). With this choice, when the number of samples goes to infinity, ASGD would converge to the performance of the best predictor in HK at a rate of O(T\u2212 1 2 ), only if the infimum infh\u2208HK E`(h) is attained by a function inHK . Note that even with a universal kernel we only have E`(f `\u03c1) = infh\u2208HK E`(h) but there is no guarantee that the infimum is attained [26].\nOn the other hand, there is a vast literature examining the general case when (2) holds [11, 24, 34, 32, 7, 4, 33, 27, 16, 31, 29]. Under this assumption, this infimum is attained only when \u03b2 \u2265 12 , yet it is possible to prove convergence for \u03b2 > 0. In fact, when (2) holds it is known that minh\u2208HK [ E`(h) + \u2016h\u20162K (\u03b7T )\u22121 ] \u2212 E`(f `\u03c1) = O((\u03b7T )\u22122\u03b2) [12, Proposition 8.5]. Hence, it was observed in [33] that setting \u03b7 = O(T\u2212 2\u03b2 2\u03b2+1 )\nin (3), we obtain E[E`(f\u0304T )]\u2212 E`(f `\u03c1) = O ( T\u2212 2\u03b2 2\u03b2+1 ) , that is the optimal rate [33, 27]. Hence, the setting \u03b7 = O(T\u2212 12 ) is optimal only when \u03b2 = 12 , that is f `\u03c1 \u2208 HK . In all the other cases, the convergence rate of ASGD to the optimal `-risk is suboptimal. Unfortunately, \u03b2 is typically unknown to the learner.\nOn the other hand, using the tools to design self-tuning algorithms, e.g. [1, 13], it may be possible to design an ASGD-like algorithm, able to self-tune its step size in a data-dependent way. Indeed, we would\n1The case that \u03b2 < 1 implicitly assumes that HK is infinite dimensional. If HK has finite dimension, \u03b2 is 0 or 1. See also the discussion in [27].\n2For completeness, the proof is in the Appendix.\nAlgorithm 1 Averaged SGD. Parameters: \u03b7 > 0 Initialize: f1 = 0 \u2208 HK for t = 1, 2, . . . do\nReceive input vector xt \u2208 X Predict with y\u0302t = ft(xt) Update ft+1 = ft + \u03b7yt`\u2032(yty\u0302t)k(xt, \u00b7)\nend for Return f\u0304T = 1T \u2211T t=1 ft\nAlgorithm 2 The Kernel Perceptron. Parameters: None Initialize: f1 = 0 \u2208 HK for t = 1, 2, . . . do\nReceive input vector xt \u2208 X Predict with y\u0302t = sign(ft(xt)) Suffer loss 1(y\u0302t 6= yt) Update ft+1 = ft + yt1(y\u0302t 6= yt)k(xt, \u00b7)\nend for\nlike an algorithm able to select the optimal step size in (3), that is\nE[E`(f\u0304T )] \u2264 inf h\u2208HK E`(h) + min \u03b7>0 \u2016h\u20162K (\u03b7T )\u22121 + 4\u03b7 = inf h\u2208HK E`(h) + 4 \u2016h\u2016K T\u2212 1 2 . (4)\nIn the OCO setting, this would correspond to a regret bound of the form O(\u2016h\u2016K T 1 2 ). An algorithm that has this kind of guarantee is the Perceptron algorithm [22], see Algorithm 2. In fact, for the Perceptron it is possible to prove the following mistake bound [9]:\nNumber of Mistakes \u2264 inf h\u2208HK T\u2211 t=1 `h(yth(xt)) + \u2016h\u20162K + \u2016h\u2016K \u221a\u221a\u221a\u221a T\u2211 t=1 `h(yth(xt)), (5)\nwhere `h is the hinge loss, `h(x) = max(1 \u2212 x, 0). The Perceptron algorithm is similar to SGD but its behavior is independent of the step size, hence, it can be thought as always using the optimal one. Unfortunately, we are not done yet: While (5) has the right form of the bound, it is not a regret bound, rather only a mistake bound, specific for binary classification. In fact, the performance of the competitor h is measured with a different loss (hinge loss) than the performance of the algorithm (misclassification loss). For this asymmetry, the convergence when \u03b2 < 12 cannot be proved. Instead, we need an online algorithm whose regret bound scales asO(\u2016h\u2016K T 1 2 ), returns the averaged solution, and, thanks to the equality in (4), obtains a convergence rate which would depend on\nmin \u03b7>0 \u2016h\u20162K (\u03b7T )\u22121 + \u03b7. (6)\nThe r.h.s of (6) has exactly the same form of the expression in (3), but with a minimum over \u03b7. Hence, we can expect it to always have the optimal rate of convergence. In the next section, we will present such algorithm."}, {"heading": "4 PiSTOL: Parameter-free STOchastic Learning", "text": "In this section we describe the PiSTOL algorithm. The pseudo-code is in Algorithm 3. The algorithm builds on recent advancement in unconstrained online learning [28, 18, 15]. It is very similar to a SGD algorithm [35], the main difference being the computation of the solution based on the past gradients, in line 4. Note that the calculation of \u2016gt\u20162K can be done incrementally, hence, the computational complexity is the same as ASGD in a RKHS, Algorithm 1. For the PiSTOL algorithm we have the following regret bound.3\n3All the proofs are in Appendix.\nAlgorithm 3 PiSTOL: Parameter-free STOchastic Learning. 1: Parameters: a, b, L > 0 2: Initialize: g0 = 0 \u2208 HK , \u03b10 = aL 3: for t = 1, 2, . . . do 4: Set ft = gt\u22121 b\u03b1t\u22121 exp ( \u2016gt\u22121\u20162K 2\u03b1t\u22121\n) 5: Receive input vector xt \u2208 X 6: Adversarial setting: Suffer loss `t(ft(xt)) 7: Receive subgradient st \u2208 \u2202`t(ft(xt)) 8: Update gt = gt\u22121 \u2212 stk(xt, \u00b7) and \u03b1t = \u03b1t\u22121 + a|st| \u2016k(xt, \u00b7)\u2016K 9: end for\n10: Statistical setting: Return f\u0304T = 1T \u2211T t=1 ft\nTheorem 1. Assume that the sequence of xt satisfies \u2016k(xt, \u00b7)\u2016K \u2264 1 and the losses `t are convex and L-Lipschitz. Let a > 0 such that a \u2265 2.25L. Then, for any h \u2208 HK , the following bound on the regret holds for the PiSTOL algorithm\nT\u2211 t=1 [`t(ft(xt))\u2212 `t(h(xt))] \u2264\u2016h\u2016K \u221a\u221a\u221a\u221a2a(L+ T\u22121\u2211 t=1 |st| ) log ( \u2016h\u2016K \u221a aLT b + 1 ) + b\u03c6 ( a\u22121L ) log (1 + T ) ,\nwhere \u03c6(x) := x2 exp( x2 )(x+1)+2 1\u2212x exp( x2 )\u2212x\n( exp ( x 2 ) (x+ 1) + 2 ) .\nThis theorem shows that PiSTOL has the right dependency on \u2016h\u2016K and T that was outlined in Sec. 3 and its regret bound is also optimal up to \u221a log log T terms [18]. Moreover, Theorem 1 improves on the results in [18, 15], obtaining an almost optimal regret that depends on the sum of the absolute values of the gradients, rather than on the time T . This is critical to obtain a tighter bound when the losses areH-smooth, as shown in the next Corollary.\nCorollary 1. Under the same assumptions of Theorem 1, if the losses `t are also H-smooth, then4\nT\u2211 t=1 [`t(ft(xt))\u2212 `t(h(xt))] = O\u0303 max \u2016h\u2016 43K T 13 , \u2016h\u2016K T 14 ( T\u2211 t=1 `t(h(xt)) + 1 ) 1 4   .\nThis bound shows that, if the cumulative loss of the competitor is small, the regret can grow slower than\u221a T . It is worse than the regret bounds for smooth losses in [9, 25] because when the cumulative loss of the competitor is equal to 0, the regret still grows as O\u0303 ( \u2016f\u2016 4 3\nK T 1 3\n) instead of being constant. However, the\nPiSTOL algorithm does not require the prior knowledge of the norm of the competitor function h, as all the ones in [9, 25] do.\nIn the Appendix, we also show a variant of PiSTOL for linear kernels with almost optimal learning rate for each coordinate. Contrary to other similar algorithms, e.g. [13], it is a truly parameter-free one."}, {"heading": "5 Convergence Results for PiSTOL", "text": "In this section we will use the online-to-batch conversion to study the `-risk and the misclassification risk of the averaged solution of PiSTOL. We will also use the following definition: \u03c1 has Tsybakov noise exponent\n4For brevity, the O\u0303 notation hides polylogarithmic terms.\nq \u2265 0 [30] iff there exist cq > 0 such that\nPX({x \u2208 X : \u2212s \u2264 f\u03c1(x) \u2264 s}) \u2264 cqsq, \u2200s \u2208 [0, 1]. (7)\nSetting \u03b1 = qq+1 \u2208 [0, 1], and c\u03b1 = cq + 1, condition (7) is equivalent [32, Lemma 6.1] to:\nPX(sign(f(x)) 6= fc(x)) \u2264 c\u03b1(R(f)\u2212R(f\u03c1))\u03b1, \u2200f \u2208 L2\u03c1X . (8)\nThese conditions allow for faster rates in relating the expected excess misclassification risk to the expected `-risk, as detailed in the following Lemma that is a special case of [3, Theorem 10].\nLemma 1. Let ` : R\u2192 R+ be a convex loss function, twice differentiable at 0, with `\u2032(0) < 0, `\u2032\u2032(0) > 0, and with the smallest zero in 1. Assume condition (8) is verified. Then for the averaged solution f\u0304T returned by PiSTOL it holds\nE[R(f\u0304T )]\u2212R(fc) \u2264 (\n32 c\u03b1 C\n( E[E`(f\u0304T )]\u2212 E`(f `\u03c1) )) 12\u2212\u03b1 , C = min { \u2212`\u2032(0), (` \u2032(0))2\n`\u2032\u2032(0)\n} .\nThe results in Sec. 4 give regret bounds over arbitrary sequences. We now assume to have a sequence of training samples (xt, yt)Tt=1 IID from \u03c1. We want to train a predictor from this data, that minimizes the `-risk. To obtain such predictor we employ a so-called online-to-batch conversion [8]. For a convex loss `, we just need to run an online algorithm over the sequence of data (xt, yt)Tt=1, using the losses `t(x) = `(ytx), \u2200t = 1, \u00b7 \u00b7 \u00b7 , T . The online algorithm will generate a sequence of solutions ft and the online-to-batch conversion can be obtained with a simple averaging of all the solutions, f\u0304T = 1T \u2211T t=1 ft, as for ASGD. The average regret bound of the online algorithm becomes a convergence guarantee for the averaged solution [8]. Hence, for the averaged solution of PiSTOL, we have the following Corollary that is immediate from Corollary 1 and the results in [8].\nCorollary 2. Assume that the samples (xt, yt)Tt=1 are IID from \u03c1, and `t(x) = `(ytx). Then, under the assumptions of Corollary 1, the averaged solution of PiSTOL satisfies\nE[E`(f\u0304T )] \u2264 inf h\u2208HK\nE`(h) + O\u0303 ( max { \u2016h\u2016 4 3\nK T \u2212 23 , \u2016h\u2016K T\u2212 3 4\n( TE`(h) + 1 ) 1 4 }) .\nHence, we have a O\u0303(T\u2212 23 ) convergence rate to the \u03c6-risk of the best predictor in HK , if the best predictor has \u03c6-risk equal to zero, and O\u0303(T\u2212 12 ) otherwise. Contrary to similar results in literature, e.g. [25], we do not have to restrict the infimum over a ball of fixed radius in HK and our bounds depends on O\u0303(\u2016h\u2016K) rather than O(\u2016h\u2016 2 K), e.g. [35]. The advantage of not restricting the competitor in a ball is clear: The performance is always close to the best function inHK , regardless of its norm. The logarithmic terms are exactly the price we pay for not knowing in advance the norm of the optimal solution. For binary classification using Lemma 1, we can also prove a O\u0303(T\u2212 12(2\u2212\u03b1) ) bound on the excess misclassification risk in the realizable setting, that is if f `\u03c1 \u2208 HK .\nIt would be possible to obtain similar results with other algorithms, as the one in [25], using a doublingtrick approach [9]. However, this would result most likely in an algorithm not useful in any practical application. Moreover, the doubling-trick itself would not be trivial, for example the one used in [28] achieves a suboptimal regret and requires to start from scratch the learning over two different variables, further reducing its applicability in any real-world application.\nAs anticipated in Sec. 3, we now show that the dependency on O\u0303(\u2016h\u2016K) rather than onO(\u2016h\u2016 2 K) gives us the optimal rates of convergence in the general case that f `\u03c1 \u2208 L\u03b2K(L2\u03c1X ), without the need to tune any parameter. This is our main result.\nTheorem 2. Assume that the samples (xt, yt)Tt=1 are IID from \u03c1, (2) holds for \u03b2 \u2264 12 , and `t(x) = `(ytx). Then, under the assumptions of Corollary 1, the averaged solution of PiSTOL satisfies\n\u2022 If \u03b2 \u2264 13 then E[E`(f\u0304T )]\u2212 E`(f `\u03c1) \u2264 O\u0303 ( max { (E`(f `\u03c1) + 1/T ) \u03b2 2\u03b2+1T\u2212 2\u03b2 2\u03b2+1 , T\u2212 2\u03b2 \u03b2+1 }) .\n\u2022 If 13 < \u03b2 \u2264 12 , then E[E`(f\u0304T )]\u2212 E`(f `\u03c1)\n\u2264 O\u0303 ( max { (E`(f `\u03c1) + 1/T ) \u03b2 2\u03b2+1T\u2212 2\u03b2 2\u03b2+1 , (E`(f `\u03c1) + 1/T ) 3\u03b2\u22121 4\u03b2 T\u2212 1 2 , T\u2212 2\u03b2 \u03b2+1 }) .\n10 1\n10 2\n10 3\n10 4\n10 5\n10 6\n10 7\n10 \u22123\n10 \u22122\n10 \u22121\n10 0\n10 1\nT\nB o u n d\nExcess \u2113-risk bound\nE\u2113(f\u2113\u03c1) = 0 E\u2113(f\u2113\u03c1) = 0.1 E\u2113(f\u2113\u03c1) = 1\nFigure 2: Upper bound on the excess `-risk of PiSTOL for \u03b2 = 1 2 .\nThis theorem guarantees consistency w.r.t. the `-risk. We have that the rate of convergence to the optimal `-risk is O\u0303(T\u2212 3\u03b22\u03b2+1 ), if E`(f `\u03c1) = 0, and O\u0303(T\u2212 2\u03b2 2\u03b2+1 ) otherwise. However, for any finite T the rate of convergence is O\u0303(T\u2212 2\u03b2\u03b2+1 ) for any T = O(E`(f `\u03c1)\u2212 \u03b2+1 2\u03b2 ). In other words, we can expect a first regime at faster convergence, that saturates when the number of samples becomes big enough, see Fig. 2. This is particularly important because often in practical applications the features and the kernel are chosen to have good performance that is low optimal `-risk. Using Lemma 1, we have that the excess misclassification risk is O\u0303(T\u2212 2\u03b2 (2\u03b2+1)(2\u2212\u03b1) ) if E`(f `\u03c1) 6= 0, and O\u0303(T\u2212 2\u03b2\n(\u03b2+1)(2\u2212\u03b1) ) if E`(f `\u03c1) = 0. It is also worth noting that, being the algorithm designed to work in the adversarial setting, we expect its performance to be robust to small deviations from the IID scenario.\nAlso, note that the guarantees of Corollary 2 and Theorem 2 hold simultaneously. Hence, the theoretical performance of PiSTOL is always better than both the ones of SGD with the step sizes tuned with the knowledge of \u03b2 or with the agnostic choice \u03b7 = O(T\u2212 12 ). In the Appendix, we also show another convergence result assuming a different smoothness condition.\nRegarding the optimality of our results, lower bounds for the square loss are known [27] under assumption (2) and further assuming that the eigenvalues of LK have a polynomial decay, that is\n(\u03bbi)i\u2208N \u223c i\u2212b, b \u2265 1. (9)\nCondition (9) can be interpreted as an effective dimension of the space. It always holds for b = 1 [27] and this is the condition we consider that is usually denoted as capacity independent, see the discussion in [33, 21]. In the capacity independent setting, the lower bound isO(T\u2212 2\u03b22\u03b2+1 ), that matches the asymptotic rates in Theorem 2, up to logarithmic terms. Even if we require the loss function to be Lipschitz and smooth, it is unlikely that different lower bounds can be proved in our setting. Note that the lower bounds are worst case w.r.t. E`(f `\u03c1), hence they do not cover the case E`(f `\u03c1) = 0, where we get even better rates. Hence, the optimal regret bound of PiSTOL in Theorem 1 translates to an optimal convergence rate for its averaged solution, up to logarithmic terms, establishing a novel link between these two areas."}, {"heading": "6 Related Work", "text": "The approach of stochastically minimizing the `-risk of the square loss in a RKHS has been pioneered by [24]. The rates were improved, but still suboptimal, in [34], with a general approach for locally Lipschitz loss functions in the origin. The optimal bounds, matching the ones we obtain for E`(f `\u03c1) 6= 0, were obtained for \u03b2 > 0 in expectation by [33]. Their rates also hold for \u03b2 > 12 , while our rates, as the ones in [27], saturate at \u03b2 = 12 . In [29], high probability bounds were proved in the case that 1 2 \u2264 \u03b2 \u2264 1. Note that, while in the range \u03b2 \u2265 12 , that implies f\u03c1 \u2208 HK , it is possible to prove high probability bounds [29, 27, 4, 7], the range 0 < \u03b2 < 12 considered in this paper is very tricky, see the discussion in [27]. In this range no\nhigh probability bounds are known without additional assumptions. All the previous approaches require the knowledge of \u03b2, while our algorithm is parameter-free. Also, we obtain faster rates for the excess `-risk, when E`(f `\u03c1) = 0. Another important difference is that we can use any smooth and Lipschitz loss, useful for example to generate sparse solutions, while the optimal results in [33, 29] are specific for the square loss.\nFor finite dimensional spaces and self-concordant losses, an optimal parameter-free stochastic algorithm has been proposed in [2]. However, the convergence result seems specific to finite dimension.\nThe guarantees obtained from worst-case online algorithms, for example [25], have typically optimal convergence only w.r.t. the performance of the best inHK , see the discussion in [33]. Instead, all the guarantees on the misclassification loss w.r.t. a convex `-risk of a competitor, e.g. the Perceptron\u2019s guarantee, are inherently weaker than the presented ones. To see why, assume that the classifier returned by the algorithm after seeing T samples is fT , these bounds are of the form ofR(fT ) \u2264 E`(h)+O(T\u2212 1 2 (\u2016h\u20162K +1)). For simplicity, assume the use of the hinge loss so that easy calculations show that f `\u03c1 = fc and E`(f `\u03c1) = 2R(fc). Hence, even in the easy case that fc \u2208 HK , we have R(fT ) \u2264 2R(fc) + O(T\u2212 1 2 (\u2016fc\u20162K + 1)), i.e. no convergence to the Bayes risk. In the batch setting, the same optimal rates were obtained by [4, 7] for the square loss, in high probability, for \u03b2 > 12 . In [27], using an additional assumption on the infinity norm of the functions in HK , they give high probability bounds also in the range 0 < \u03b2 \u2264 12 . The optimal tuning of the regularization parameter is achieved by cross-validation. Hence, we match the optimal rates of a batch algorithm, without the need to use validation methods.\nIn Sec. 3 we saw that the core idea to have the optimal rate was to have a classifier whose performance is close to the best regularized solution, where the regularizer is \u2016h\u2016K . Changing the regularization term from the standard \u2016h\u20162K to \u2016h\u2016 q K with q \u2265 1 is not new in the batch learning literature. It has been first proposed for classification by [5], and for regression by [16]. Note that, in both cases no computational methods to solve the optimization problem were proposed. Moreover, in [27] it was proved that all the regularizers of the form \u2016h\u2016qK with q \u2265 1 gives optimal convergence rates bound for the square loss, given an appropriate setting of the regularization weight. In particular, [27, Corollary 6] proves that, using the square loss and under assumptions (2) and (9), the optimal weight for the regularizer \u2016h\u2016qK is T \u2212 2\u03b2+q(1\u2212\u03b2)\n2\u03b2+2/b . This implies a very important consequence, not mentioned in that paper: In the the capacity independent setting, that is b = 1, if we use the regularizer \u2016h\u2016K , the optimal regularization weight is T\u2212 1 2 , independent of the exponent of the range space (1) where f\u03c1 belongs. Moreover, in the same paper it was argued that \u201cFrom an algorithmic point of view however, q = 2 is currently the only feasible case, which in turn makes SVMs the method of choice\u201d. Indeed, in this paper we give a parameter-free efficient procedure to train predictors with smooth losses, that implicitly uses the \u2016h\u2016K regularizer. Thanks to this, the regularization parameter does not need to be set using prior knowledge of the problem."}, {"heading": "7 Discussion", "text": "Borrowing from OCO and statistical learning theory tools, we have presented the first parameter-free stochastic learning algorithm that achieves optimal rates of convergence w.r.t. the smoothness of the optimal predictor. In particular, the algorithm does not require any validation method for the model selection, rather it automatically self-tunes in an online and data-dependent way.\nEven if this is mainly a theoretical work, we believe that it might also have a big potential in the applied world. Hence, as a proof of concept on the potentiality of this method we have also run few preliminary experiments, to compare the performance of PiSTOL to an SVM using 5-folds cross-validation to select the regularization weight parameter. The experiments were repeated with 5 random shuffles, showing the average and standard deviations over three datasets.5 The latest version of LIBSVM was used to train the SVM [10]. We have that PiSTOL closely tracks the performance of the tuned SVM when a Gaussian kernel is used. Also, contrary to the common intuition, the stochastic approach of PiSTOL seems to have an advantage over the tuned SVM when the number of samples is small. Probably, cross-validation is a poor approximation of the generalization performance in that regime, while the small sample regime does not affect at all the analysis of PiSTOL. Note that in the case of News20, a linear kernel is used over the vectors of size 1355192. The finite dimensional case is not covered by our theorems, still we see that PiSTOL seems to converge at the same rate of SVM, just with a worse constant. It is important to note that the total time the 5-folds cross-validation plus the training with the selected parameter for the SVM on 58000 samples of SensIT Vehicle takes \u223c 6.5 hours, while our unoptimized Matlab implementation of PiSTOL less than 1 hour, \u223c 7 times faster. The gains in speed are similar on the other two datasets.\nThis is the first work we know of in this line of research of stochastic adaptive algorithms for statistical learning, hence many questions are still open. In particular, it is not clear if high probability bounds can be obtained, as the empirical results hint, without additional hypothesis. Also, we only proved convergence w.r.t. the `-risk, however for \u03b2 \u2265 12 we know that f `\u03c1 \u2208 HK , hence it would be possible to prove the stronger convergence results on\n\u2225\u2225fT \u2212 f `\u03c1\u2225\u2225K , e.g. [29]. Probably this would require a major change in the proof techniques used. Finally, it is not clear if the regret bound in Theorem 1 can be improved to depend on the squared gradients. This would result in a O\u0303(T\u22121) bound for the excess `-risk for smooth losses when E`(f `\u03c1) = 0 and \u03b2 = 12 .\n5Datasets available at http://www.csie.ntu.edu.tw/\u02dccjlin/libsvmtools/datasets/. The precise details to replicate the experiments are in the Appendix."}, {"heading": "A Per-coordinate Variant of PiSTOL", "text": "Recently a number of algorithms with a different step size for each coordinate have been proposed, e.g. [14, 13]. The motivation is to take advantage of the sparsity of the features and, at the same time, to have a slower decaying step size for rare features. However, till now this adaptation has considered only the gradients and not to the norm of the competitor. Here we close this gap.\nAs shown in [14], these kind of algorithms can be very easily designed and analyzed just running an independent copy of the algorithm on each coordinate. Hence, we have the following corollary.\nCorollary 3. Assume the kernelK is the linear one. Also, assume that the sequence of xt satisfies \u2016xt\u2016\u221e \u2264 1 and the losses `t are convex and L-Lipschitz. Let a > 0 such that a \u2265 2.25L, and b = 1d . Then, for any u \u2208 Rd, running a different copy of Algorithm 3 for each coordinate, the following regret bound holds\nT\u2211 t=1 [ `t(w > t xt)\u2212 `t(u>xt) ] \u2264\u2016u\u2016\u221e d\u2211 i=1 \u221a\u221a\u221a\u221a2a(L+ T\u22121\u2211 t=1 |si,t| ) log ( d \u2016u\u2016\u221e \u221a aLT + 1 ) + \u03c6 ( a\u22121L ) log (1 + T ) ,\nwhere \u03c6(x) := x2 exp( x2 )(x+1)+2 1\u2212x exp( x2 )\u2212x\n( exp ( x 2 ) (x+ 1) + 2 ) .\nUp to logarithmic terms, this regret bound is very similar to the one of AdaGrad [13], with two importance differences. Using our notation, AdaGrad depends \u2211T\u22121 t=1 s 2 i,t rather than \u2211T\u22121 t=1 |si,t|. In the case of Lipschitz losses and binary features, these two dependencies are essentially equivalent. The second and more important difference is that AdaGrad depends on \u2016u\u20162\u221e instead of \u2016u\u2016\u221e, or in alternative it assumes the knowledge of the (unknown) \u2016u\u2016\u221e to tune its step size."}, {"heading": "B Convergence in L1\u03c1X", "text": "Define \u2016f\u2016L1\u03c1X := \u222b X |f(x)|d\u03c1X . We now use the the standard assumption on the behavior of the approximation error in L1\u03c1X , see, e.g., [34].\nTheorem 3. Assume that the samples (xt, yt)Tt=1 are IID from \u03c1 and `t(x) = `(ytx). If for some 0 < \u03b2 \u2264 1 and C > 0, the pair (\u03c1,K) satisfies\ninf f\u2208HK \u2225\u2225f \u2212 f `\u03c1\u2225\u2225L1\u03c1X + \u03b3 \u2016f\u20162K \u2264 C\u03b3\u03b2 , \u2200\u03b3 > 0 (10) then, under the assumptions of Theorem 1, the averaged solution of PiSTOL satisfies\nE[E`(f\u0304T )]\u2212 E`(f `\u03c1) \u2264 O\u0303 ( T\u2212 \u03b2 \u03b2+1 ) .\nThis Theorem improves over the result in [34], where the worse bound O ( T \u2212 \u03b2 2(\u03b2+1) ) ,\u2200 > 0, was\nproved using the prior knowledge of \u03b2. See [26] for a discussion on the condition (10)."}, {"heading": "C Details about the Empirical Results", "text": "For the sake of the reproducibility of the experiments, we report here the exact details. The loss used by PiSTOL in all the experiments is a smoothed version of the hinge loss:\n`(x) =  0 x \u2265 1 (1\u2212 x)2 0 < x < 1 1\u2212 2x x \u2264 0.\nFor the SVM we used the hinge loss. The parameters of PiSTOL were the same in all the experiments: a = 0.25, L = 2, \u03b2 = \u221a 2aLT . The a9a dataset is composed by 32561 training samples and 16281 for testing, the dimension of the features is 123. The Gaussian kernel is\nK(x,x\u2032) = exp ( \u2212\u03b3 \u2016x\u2212 x\u2032\u201622 ) ,\nwhere \u03b3 was fixed to 0.04, as done in [19]. The \u201cC\u201d parameter of the SVM was tuned with cross-validation over the range {2\u22121, 20, 21, 22, 23}. The SensIT Vehicle dataset is a 3-class dataset composed by 78823 training samples and 19705 for testing. A binary classification task was built using the third class versus the other two, to have a very balanced problem. For the amount of time taken by LIBSVM to train a model, we only used a maximum of 58000 training samples. The parameter \u03b3 in the Gaussian kernel is 0.125, again as in in [19]. The range of the \u201cC\u201d parameter of the SVM was {20, 21, 22, 23, 24, 25, 26}. The news20.binary dataset is composed by 19996 samples with dimension 1355191, and normalized to have L2 norm equal to 1. The test set was composed by 10000 samples drawn randomly from the training samples. The range of the \u201cC\u201d parameter of the SVM was {21, 22, 23, 24}."}, {"heading": "D Proofs", "text": ""}, {"heading": "D.1 Additional Definitions", "text": "Given a closed and convex function h : HK \u2192 [\u2212\u221e,+\u221e], its Fenchel conjugate h\u2217 : HK \u2192 [\u2212\u221e,+\u221e] is defined as h\u2217(g) = supf\u2208HK ( \u3008f , g\u3009K \u2212 h(f) ) .\nD.2 Proof of (3) From [35], it is possible to extract the following inequality\nE[E`(f\u0304T )] \u2264 inf h\u2208HK\n(1\u2212 2\u03b7)\u22121 [ E`(h) + \u2016h\u2016 2 K\n2\u03b7T ] \u2264 inf h\u2208HK [ E`(h) + \u2016h\u2016 2 K 2\u03b7T ] + ((1\u2212 2\u03b7)\u22121 \u2212 1)E`(0).\nUsing the elementary inequalities (1\u2212 2\u03b7)\u22121 \u2212 1 \u2264 4\u03b7, \u22000 < \u03b7 \u2264 14 , we have\nE[E`(f\u0304T )] \u2264 inf h\u2208HK\nE`(h) + \u2016h\u2016 2 K\n\u03b7T + 4\u03b7."}, {"heading": "D.3 Proof of Theorem 1", "text": "In this section we prove the regret bound in the adversarial setting. The key idea is of the proof is to design a time-varying potential function. Some of the ideas in the proof are derived from [18, 15].\nIn the proof of Theorem 1 we also use the following technical lemmas.\nLemma 2. Let a, b, c \u2208 R, a, c \u2265 0. \u2022 if b > 0, then\nexp\n( 2a b+ b2\n2c\n) \u2264 1 + a b\nc + b2 2c\n( (a+ b)2\nc + 1\n) exp ( 2a b+ b2\n2c\n) .\n\u2022 if b \u2264 0, then exp\n( 2a b+ b2\n2c\n) \u2264 1 + a b\nc + b2 2c\n( a2 + b2\nc + 1\n) exp ( b2\n2c\n) .\nProof. Consider the function g(b) = exp ( (a+b)2\n2c\n) . Using a second order Taylor expansion around 0 we\nhave\ng(b) = exp\n( a2\n2c\n) + ab\nc exp\n( a2\n2c\n) + ( (a+ \u03be)2\nc + 1\n) exp ( (a+ \u03be)2\n2c\n) b2\n2c (11)\nfor some \u03be between 0 and b. Note that r.h.s of (11) is a convex function w.r.t. \u03be, so it is maximized when \u03be = 0 or \u03be = b. Hence, the first inequality is obtained using upper bounding \u03be with b, and (a + \u03be)2 with a2 + b2 in the second case.\nLemma 3. [15, Lemma 14] Define \u03a8(g) = b exp \u2016g\u2016 2 K\n2\u03b1 , for \u03b1, b > 0. Then\n\u03a8\u2217(f) \u2264 \u2016f\u2016K\n\u221a 2\u03b1 log (\u221a \u03b1 \u2016f\u2016K b + 1 ) \u2212 b.\nLemma 4. For all \u03b4, x1, . . . , xT \u2208 R+, we have T\u2211 t=1 xt \u03b4 + \u2211t i=1 xi \u2264 ln (\u2211T t=1 xt \u03b4 + 1 ) .\nProof. Define vt = \u03b4+ \u2211t i=1 xi. The concavity of the logarithm implies ln b \u2264 ln a+ b\u2212aa for all a, b > 0. Hence we have T\u2211 t=1 xt \u03b4 + \u2211t i=1 xi = T\u2211 t=1 xt vt = T\u2211 t=1 vt \u2212 vt\u22121 vt \u2264 T\u2211 t=1 ln vt vt\u22121 = ln vT v0 = ln \u03b4 + \u2211T t=1 at \u03b4 .\nWe are now ready to prove Theorem 1. Differently from the proof methods in [15], here the potential functions will depend explicitly on the sum of the past gradients, rather than simple on the time.\nProof of Theorem 1. Without loss of generality and for simplicity, the proof uses b = 1. For a time-varying function \u03a8\u2217t : HK \u2192 R, let \u2206t = \u03a8\u2217t (\u2016gt\u2016K)\u2212\u03a8\u2217t\u22121(\u2016gt\u22121\u2016K). Also define gt = \u2211t i=1 kt, with kt \u2208 H.\nThe Fenchel-Young inequality states that \u03a8(f)+\u03a8\u2217(g) \u2265 \u3008f , g\u3009K for all f, g \u2208 HK . Hence, it implies that, for any sequence of kt \u2208 HK and any h \u2208 HK , we have\nT\u2211 t=1 \u2206t = \u03a8 \u2217 T (\u2016gT \u2016K)\u2212\u03a8\u22170(\u2016g0\u2016K) \u2265 \u3008h , gT \u3009K \u2212\u03a8T (\u2016h\u2016K)\u2212\u03a8\u22170(\u2016g0\u2016K)\n= \u2212\u03a8T (\u2016h\u2016K) + T\u2211 t=1 \u3008h , kt\u3009K \u2212\u03a8\u22170(\u2016g0\u2016K).\nHence, using the definition of gT , we have\nT\u2211 t=1 \u3008h\u2212 ft , kt\u3009K \u2264 \u03a8T (\u2016h\u2016K) + \u03a8\u22170(\u2016g0\u2016K) + T\u2211 t=1 ( \u03a8\u2217t (\u2016gt\u2016K)\u2212\u03a8\u2217t\u22121(\u2016gt\u22121\u2016K)\u2212 \u3008ft , kt\u3009K ) .\nWe now use the notation in Algorithm 3, and set kt = \u2212\u2202`t(ft)k(xt, \u00b7), and \u03a8\u2217t (x) = b exp( x 2 2\u03b1t ). Observe that, by the hypothesis on `t, we have \u2016kt\u2016K \u2264 L. Observe that, with the choice of \u03b1t, we have the following inequalities that will be used often in the proof:\n\u2022 \u2016gt\u2016K\u03b1t \u2264 \u2016\u2211ti=1 ki\u2016K \u03b1t \u2264 1a . \u2022 \u2016gt\u22121\u2016K\u2016kt\u2016K\u03b1t \u2264 \u2016kt\u2016K \u2016\u2211t\u22121i=1 ki\u2016K \u03b1t \u2264 \u2016kt\u2016Ka \u2264 La .\n\u2022 2\u2016gt\u22121\u2016K\u2016kt\u2016K+\u2016kt\u2016 2 K\n2\u03b1t \u2264 \u2016kt\u2016K \u2016\u2211t\u22121i=1 ki\u2016K+\u2016kt\u2016K \u03b1t \u2264 \u2016kt\u2016K \u2016\u2211ti=1 ki\u2016K \u03b1t \u2264 \u2016kt\u2016Ka \u2264 La .\nWe have \u03a8\u2217t (\u2016gt\u2016K)\u2212\u03a8\u2217t\u22121(\u2016gt\u22121\u2016K)\u2212 \u3008ft , kt\u3009K = exp ( \u2016gt\u20162K 2\u03b1t ) \u2212 exp ( \u2016gt\u22121\u20162K 2\u03b1t\u22121 ) \u2212 \u3008ft , kt\u3009K\n= exp\n( \u2016gt\u22121\u20162K\n2\u03b1t\n)[ exp ( 2 \u3008gt\u22121 , kt\u3009K + \u2016kt\u2016 2 K\n2\u03b1t\n) \u2212 (1 + \u3008gt\u22121 , kt\u3009K\n\u03b1t\u22121 ) exp\n( a \u2016kt\u2016K \u2016gt\u22121\u2016 2 K\n2\u03b1t\u03b1t\u22121\n)] (12)\nConsider the max of the r.h.s. of the last equality w.r.t. \u3008gt\u22121 , kt\u3009K . Being a convex function of \u3008gt\u22121 , kt\u3009K , the maximum is achieved at the border of the domain. Hence, \u3008gt\u22121, kt\u3009 = ct \u2016gt\u22121\u2016K \u2016kt\u2016K where ct = 1 or \u22121. We will analyze the two case separately.\nCase positive: Consider the case that ct = 1. Considering only the expression in parenthesis in (12),\nwe have\nexp\n( 2 \u2016gt\u22121\u2016K \u2016kt\u2016K + \u2016kt\u2016 2 K\n2\u03b1t\n) \u2212 ( 1 + \u2016gt\u22121\u2016K \u2016kt\u2016K\n\u03b1t\u22121\n) exp ( a \u2016kt\u2016K \u2016gt\u22121\u2016 2 K\n2\u03b1t\u03b1t\u22121\n)\n\u2264 1 + \u2016gt\u22121\u2016K \u2016kt\u2016K \u03b1t\n\u2212 ( 1 + \u2016gt\u22121\u2016K \u2016kt\u2016K\n\u03b1t\u22121\n) exp ( a \u2016kt\u2016K \u2016gt\u22121\u2016 2 K\n2\u03b1t\u03b1t\u22121\n)\n+ \u2016kt\u20162K\n2\u03b1t exp\n( 2 \u2016gt\u22121\u2016K \u2016kt\u2016K + \u2016kt\u2016 2 K\n2\u03b1t\n)( (\u2016gt\u22121\u2016K + \u2016z\u2016K)2\n\u03b1t + 1\n) (13)\n\u2264 1 + a \u2016kt\u2016K L\na exp\n( L\na ) \u2016gt\u22121\u20162K 2\u03b12t + \u2016kt\u20162K 2\u03b1t exp ( L a )( 2L a + 1 ) \u2212 exp ( a \u2016kt\u2016K \u2016gt\u22121\u2016 2 K 2\u03b1t\u03b1t\u22121 ) ,\nwhere in the first inequality we used the first statement of Lemma 2. We now use the fact that A := L a exp ( L a ) < 1 and the elementary inequality exp(x) \u2265 x+ 1, to have\n1 + a \u2016kt\u2016K L\na exp\n( L\na ) \u2016gt\u22121\u20162K 2\u03b12t + \u2016kt\u20162K 2\u03b1t exp ( L a )( 2L a + 1 ) \u2212 exp ( a \u2016kt\u2016K \u2016gt\u22121\u2016 2 K 2\u03b1t\u03b1t\u22121 )\n\u2264 (A\u2212 1) a \u2016kt\u2016K \u2016gt\u22121\u2016 2 K 2\u03b1t\u03b1t\u22121 + \u2016kt\u20162K 2\u03b1t exp\n( L\na\n)( 2L\na + 1\n) (14)\n\u2264 (A\u2212 1) a \u2016kt\u2016K \u2016gt\u22121\u2016 2 K 2\u03b1t\u03b1t\u22121 + \u2016kt\u2016K L 2\u03b1t exp\n( L\na\n)( 2L\na + 1\n) . (15)\nThis quantity is non-positive iff \u2016gt\u22121\u2016 2 K\n\u03b1t\u22121 \u2265 A1\u2212A ( 2L a + 1 ) .\nWe now consider the case of A1\u2212A ( 2L a + 1 ) > \u2016gt\u22121\u2016 2 \u03b1t\u22121 \u2265 \u2016gt\u22121\u2016 2 \u03b1t . In this case, from (14), we have\nf\u2217t (\u2016gt\u2016K)\u2212 f\u2217t\u22121(\u2016gt\u22121\u2016K)\u2212 \u3008ft , kt\u3009K \u2264 exp (\nA\n2(1\u2212A)\n( 2L\na + 1\n)) exp ( L\na\n)( 2L\na + 1 ) \u2016kt\u20162K 2\u03b1t .\n(16) Case negative: Now consider the case that ct = \u22121. So we have\nexp\n( \u2016gt\u22121\u20162K\n2\u03b1t\n)[ exp ( \u22122 \u2016gt\u22121\u2016K \u2016kt\u2016K + \u2016kt\u2016 2 K\n2\u03b1t\n) + (\u2016gt\u22121\u2016K \u2016kt\u2016K \u03b1t\u22121 \u2212 1 ) exp ( a \u2016kt\u2016K \u2016gt\u22121\u2016 2 K 2\u03b1t\u03b1t\u22121 )]\n\u2264 exp ( \u2016gt\u22121\u20162K\n2\u03b1t\n)[ 1\u2212 \u2016gt\u22121\u2016K \u2016kt\u2016K\n\u03b1t + (\u2016gt\u22121\u2016K \u2016kt\u2016K \u03b1t\u22121 \u2212 1 ) exp ( a \u2016kt\u2016K \u2016gt\u22121\u2016 2 K 2\u03b1t\u03b1t\u22121 )\n+ \u2016kt\u20162K\n2\u03b1t exp\n( \u2016kt\u20162K\n2\u03b1t\n)( \u2016gt\u22121\u20162K + \u2016z\u2016 2 K\n\u03b1t + 1\n)] ,\nwhere in the inequality we used the second statement of Lemma 2. Considering again only the expression\nin the parenthesis we have\n1\u2212 \u2016gt\u22121\u2016K \u2016kt\u2016K \u03b1t + \u2016kt\u20162K 2\u03b1t exp\n( \u2016kt\u20162K\n2\u03b1t\n)( \u2016gt\u22121\u20162K + \u2016kt\u2016 2 K\n\u03b1t + 1 ) (\u2016gt\u22121\u2016K \u2016kt\u2016K\n\u03b1t\u22121 \u2212 1 ) exp ( a \u2016kt\u2016K \u2016gt\u22121\u2016 2 K 2\u03b1t\u03b1t\u22121 )\n\u2264 a \u2016gt\u22121\u2016K \u2016kt\u2016 2 K \u03b1t\u03b1t\u22121 + \u2016kt\u20162K 2\u03b1t exp\n( L\n2a )(\u2016gt\u22121\u20162K \u03b1t + L a + 1 )\n+ (\u2016gt\u22121\u2016K \u2016kt\u2016K \u03b1t\u22121 \u2212 1 )( exp ( a \u2016kt\u2016K \u2016gt\u22121\u2016 2 K 2\u03b1t\u03b1t\u22121 ) \u2212 1 )\n\u2264 \u2016gt\u22121\u2016 2 K \u2016kt\u2016 2 K\n2\u03b1t\u03b1t\u22121 exp\n( L\n2a\n) + \u2016kt\u20162K\n2\u03b1t\n( exp ( L\n2a\n)( L\na + 1\n) + 2 ) + ( L a \u2212 1 ) a \u2016kt\u2016K \u2016gt\u22121\u2016 2 K\n2\u03b1t\u03b1t\u22121\n\u2264 \u2016kt\u2016 2 K\n2\u03b1t\n( exp ( L\n2a\n)( L\na + 1\n) + 2 ) + ( L\na exp\n( L\n2a\n) + L a \u2212 1 ) a \u2016kt\u2016K \u2016gt\u22121\u2016 2 K 2\u03b1t\u03b1t\u22121 . (17)\nWe have that this quantity is non-positive if \u2016gt\u22121\u2016 2 K \u03b1t\u22121 \u2265 \u2016kt\u2016Ka exp( L2a )( L a+1)+2\n1\u2212La exp( L 2a )\u2212 L a\n. Hence we now consider\nthe case that \u2016gt\u22121\u2016 2 K \u03b1t\u22121 < \u2016kt\u2016K a exp( L2a )( L a+1)+2\n1\u2212La exp( L 2a )\u2212 L a\n.\nFrom (17) we have\n\u03a8\u2217t (\u2016gt\u2016K)\u2212\u03a8\u2217t\u22121(\u2016gt\u22121\u2016K)\u2212 \u3008ft , kt\u3009K \u2264 exp ( L\n2a\nexp ( L 2a ) ( L a + 1 ) + 2\n1\u2212 La exp ( L 2a ) \u2212 La\n) \u2016kt\u20162K\n2\u03b1t\n( exp ( L\n2a\n)( L\na + 1\n) + 2 ) (18)\nPutting together (16) and (19), we have\n\u03a8\u2217t (\u2016gt\u2016K)\u2212\u03a8\u2217t\u22121(\u2016gt\u22121\u2016K)\u2212 \u3008ft , kt\u3009K \u2264 a\nL \u03c6\n( L\na ) \u2016kt\u20162 \u03b1t . (19)\nUsing the definition of \u03c6(La ) and summing over time we have\nT\u2211 t=1 ( \u03a8\u2217t (\u2016gt\u2016K)\u2212\u03a8\u2217t\u22121(\u2016gt\u22121\u2016K)\u2212 \u3008ft , kt\u3009K ) \u2264 a L \u03c6 ( L a ) T\u2211 t=1 \u2016kt\u20162K \u03b1t\n\u2264 \u03c6 ( L\na ) T\u2211 t=1 \u2016kt\u2016K L+ \u2211t i=1 \u2016ki\u2016K \u2264 \u03c6 ( L a ) log ( 1 + \u2211t i=1 \u2016ki\u2016K L ) \u2264 \u03c6 ( L a ) log (1 + T ) , (20)\nwhere in the third inequality we used Lemma 4. Using (D.3), (20), and the definition of subgradient, we have\nT\u2211 t=1 `t(ft(xt))\u2212 T\u2211 t=1 `t(h(xt)) \u2264 T\u2211 t=1 \u2202`t(ft(xt)) (h(xt)\u2212 ft(xt)) = T\u2211 t=1 \u3008h\u2212 ft , kt\u3009K\n\u2264 \u03a8T (\u2016h\u2016K) + \u03a8\u22170(\u2016g0\u2016K) + T\u2211 t=1 ( \u03a8\u2217t (\u2016gt\u2016K)\u2212\u03a8\u2217t\u22121(\u2016gt\u22121\u2016K)\u2212 ft(zt) ) \u2264 \u03a8T (\u2016h\u2016K) + \u03a8\u22170(\u2016g0\u2016K) + \u03c6 ( L\na\n) log (1 + T ) .\nUsing Lemma 3 completes the proof."}, {"heading": "D.4 Proof of Corollary 1", "text": "We first state the technical results, used in the proofs.\nLemma 5. [25, Lemma 2.1] For an H-smooth function ` : R\u2192 R+, we have (`\u2032(x))2 \u2264 4Hf(x). Lemma 6. [12, Lemma 7.2] Let c1, c2, \u00b7 \u00b7 \u00b7 , cl > 0 and s > q1 > q2 > \u00b7 \u00b7 \u00b7 > ql\u22121 > 0. Then the equation\nxs \u2212 c1xq1 \u2212 c2xq2 \u2212 \u00b7 \u00b7 \u00b7 \u2212 cl\u22121xql\u22121 \u2212 cl = 0 has a unique positive solution x\u2217. In addition,\nx\u2217 \u2264 max { (lc1) 1 s\u2212q1 , (lc2) 1 s\u2212q2 , \u00b7 \u00b7 \u00b7 , (lcl\u22121) 1 s\u2212ql\u22121 , (lcl) 1 s } .\nLemma 7. Let a, b, c > 0 and 0 < \u03b1 < 1. Then the inequality\nx\u2212 a(x+ b)\u03b1 \u2212 c \u2264 0 implies x \u2264 amax{(2a) \u03b11\u2212\u03b1 , (2(b+ c))\u03b1}+ c. Proof. Denote by y = x+ b, so consider the function f(y) = y\u2212 ay\u03b1 \u2212 b\u2212 c. Applying Lemma 6 we get that the h(y) = 0 has a unique positive solution y\u2217 and\ny\u2217 \u2264 max { (2a) 1 1\u2212\u03b1 , 2(b+ c) } .\nMoreover, the inequality h(y) \u2264 0 is verified for y = 0, and limy\u2192+\u221e h(y) = +\u221e, so we have h(y) \u2264 0 implies y \u2264 y\u2217. We also have\ny\u2217 = a(y\u2217)\u03b1 \u2212 b\u2212 c \u2264 amax { (2a) \u03b1 1\u2212\u03b1 , (2(b+ c))\u03b1 } + b+ c.\nSubstituting back x we get the stated bound.\nProof of Corollary 1. Using Cauchy-Schwarz inequality and Lemma 5, we have\nL+ T\u22121\u2211 t=1 |st| \u2016k(xt, \u00b7)\u2016K \u2264 \u221a T \u221a\u221a\u221a\u221aL2 + T\u22121\u2211 t=1 s2t \u2016k(xt, \u00b7)\u20162K \u2264 \u221a T \u221a\u221a\u221a\u221aL2 + 4H T\u22121\u2211 t=1 `t(ft(xt))\n\u2264 \u221a T \u221a\u221a\u221a\u221aL2 + 4H T\u2211 t=1 `t(ft(xt)).\nDenote by Loss = \u2211T t=1 `t(ft(xt)) and Loss \u2217 = \u2211T t=1 `t(h(xt)). Plugging last inequality in Theorem 1, we obtain\nLoss\u2212 Loss\u2217\n\u2264 ( L2\n4H + Loss\n) 1 4\n\u2016f\u2016K T 1 4 \u221a\u221a\u221a\u221a2a\u221a4H log(\u2016f\u2016K \u221aaLT b + 1 ) + b\u03c6 ( L a ) log (1 + T ) .\nDenote by C = \u221a 2a \u221a 4H log ( \u2016f\u2016K \u221a aLT b + 1 ) . Using Lemma 7 we get\nLoss\u2212 Loss\u2217 \u2264 b\u03c6 ( L\na\n) log (1 + T )\n+ \u2016h\u2016K T 1 4C max\n{ \u2016h\u2016 1 3\nK T 1 12 (2C) 1 3 , 2 1 4\n( Loss\u2217 + b\u03c6 ( L\na\n) log (1 + T ) + L2\n4H\n) 1 4 } ."}, {"heading": "D.5 Proof of Lemma 1", "text": "Proof. For any f \u2208 L2\u03c1X , define Xf = {x \u2208 X : sign(f) 6= fc}. It is easy to verify that\nR(f)\u2212R(fc) = \u222b Xf |f\u03c1(x)|d\u03c1X (x)\n= \u222b Xf |f\u03c1(x)|1(f\u03c1(x) > )d\u03c1X (x) + \u222b Xf |f\u03c1(x)|1(f\u03c1(x) \u2264 )d\u03c1X (x).\nUsing condition (8), we have\nR(f)\u2212R(fc) \u2264 1 \u222b Xf |f\u03c1(x)|2d\u03c1X (x) + \u222b Xf d\u03c1X (x) (21)\n\u2264 1 \u222b Xf |f\u03c1(x)|2d\u03c1X (x) + c\u03b1(R(f)\u2212R(fc))\u03b1. (22)\nUsing Lemma 10.10 in [12] and proceeding as in the proof of Theorem 10.5 in [12], we have\u222b Xf |f\u03c1(x)|2d\u03c1X (x) \u2264 1 C ( E`(f)\u2212 E`(f `\u03c1) ) . (23)\nHence we have\nR(f)\u2212R(fc) \u2264 1\nC\n( E`(f)\u2212 E`(f `\u03c1) ) + c\u03b1(R(f)\u2212R(fc))\u03b1. (24)\nOptimizing over we get\nR(f)\u2212R(fc) \u2264 2 \u221a c\u03b1 C \u221a E`(f)\u2212 E`(f `\u03c1)(R(f)\u2212R(fc)) \u03b1 2 , (25)\nthat is\nR(f)\u2212R(fc) \u2264 (\n4 c\u03b1 C\n( E`(f)\u2212 E`(f `\u03c1) )) 12\u2212\u03b1 . (26)\nAn application of Jensen\u2019s inequality concludes the proof."}, {"heading": "D.6 Proof of Theorem 2", "text": "We need the following technical results.\nLemma 8. [12, Lemma 10.7] Let p, q > 1 be such that 1p + 1 q = 1. Then\nab \u2264 1 q aq\u03b7q + 1 p bp\u03b7\u2212q, \u2200a, b, \u03b7 > 0.\nLemma 9. Let a, b, p, q \u2265 0. Then\nmin x\u22650\naxp + bx\u2212q \u2264 2a qq+p b pq+p ,\nand the argmin is ( pa qb )\u2212 1q+p .\nProof. Equating the first derivative to zero we have\npaxp\u22121 = qbx\u2212q\u22121.\nThat is the optimal solution satisfies\nx =\n( pa\nqb\n)\u2212 1q+p .\nSubstituting this expression into the min we have\na\n( pa\nqb\n)\u2212 pq+p + b ( pa\nqb\n) q q+p\n= a q q+p b p q+p\n(( p\nq\n)\u2212 pq+p + ( p\nq\n) q q+p ) \u2264 2a qq+p b pq+p .\nThe next lemma is the same of [12, Proposition 8.5], but it uses f `\u03c1 instead of f\u03c1.\nLemma 10. Let X \u2282 Rd be a compact domain and K a Mercer kernel such that f `\u03c1 lies in the range of L\u03b2K , with 0 < \u03b2 \u2264 12 , that is f `\u03c1 = L \u03b2 K(g) for some g \u2208 L2\u03c1X . Then\nmin f\u2208HK \u2225\u2225f \u2212 f `\u03c1\u2225\u22252L2\u03c1X + \u03b3 \u2016f\u20162K \u2264 \u03b32\u03b2 \u2016g\u20162L2\u03c1X . Proof. It is enough to use [12, Theorem 8.4] with H = L2\u03c1X , s = 1, A = L 1 2 K and a = f ` \u03c1 .\nThe next Lemma is needed for the proof of Lemma 12 that is a stronger version of [12, Corollary 10.14] because it needs only smoothness rather than a bound on the second derivative.\nLemma 11. [17, Lemma 1.2.3] Let f be continuous differentiable on a set Q \u2282 R, and its first derivative is H-Lipschitz on Q. Then\n|f(x)\u2212 f(y)\u2212 f \u2032(x)(y \u2212 x)| \u2264 H 2 (y \u2212 x)2.\nLemma 12. Assume ` is H-smooth. Then, for any f \u2208 L2\u03c1X , we have\nE`(f)\u2212 E`(f `\u03c1) \u2264 H\n2 \u2225\u2225f \u2212 f `\u03c1\u2225\u22252L2\u03c1X . Proof. Denoting by gx(f(x)) = \u222b Y `(f(x), y)d\u03c1(y|x), we have that\nE`(f)\u2212 E`(f `\u03c1) = \u222b X gx(f(x))\u2212 gx(f `\u03c1(x))d\u03c1X . (27)\nWe now use the Lemma 11 to have\ngx(f(x))\u2212 gx(f `\u03c1(x)) \u2264 g\u2032x(f `\u03c1(x))(f(x)\u2212 f `\u03c1(x)) + H\n2 (y\u0302)(f(x)\u2212 f `\u03c1(x))2\n= H\n2 (f(x)\u2212 f `\u03c1(x))2, (28)\nwhere in the equality we have used the fact that f `\u03c1(x) is by definition the minimizer of the function g \u2032 x. Putting together (27) and (28) we have\nE`(f)\u2212 E`(f `\u03c1) \u2264 H\n2 \u222b X (f(x)\u2212 f `\u03c1(x))2d\u03c1X .\nProof of Theorem 2. We will first get rid of the norm inside the logarithmic term. This will allow us to have a bound that depends only on norm of g. Let L(f) = \u2016f\u2016K \u221a 2\u03b1 log (\u221a \u03b1\u2016f\u2016K b + 1 ) + q(f). Denote by h\u2217 = arg minf\u2208HK L(f). Hence, we\nhave\n\u2016h\u2217\u2016K \u221a 2\u03b1 \u221a \u03b1\u2016h\u2217\u2016K b\u221a\n\u03b1\u2016h\u2217\u2016K b + 1\n\u2264 \u2016h\u2217\u2016K \u221a\u221a\u221a\u221a2\u03b1 \u221a\u03b1\u2016h\u2217\u2016Kb\u221a \u03b1\u2016h\u2217\u2016K b + 1 \u2264 L(h\u2217) \u2264 L(0) = q(0). (29)\nSolving the quadratic inequality and using the elementary inequality \u221a a+ b \u2264 \u221aa+ b\n2 \u221a a , we have\n\u221a \u03b1 \u2016h\u2217\u2016K \u2264 2\u2212 1 2 q(0) + b. (30)\nSo we have min f\u2208HK\nL(f) = min f\u2208HK ,\u2016f\u2016K\u2264 2 \u2212 1 2 q(0)+b\u221a \u03b1 L(f) (31)\nWe now use this result in the regret bound of Theorem 1, to have\nmin h\u2208HK T\u2211 t=1 `t(h(xt)) + \u2016h\u2016K \u221a\u221a\u221a\u221a2a(L+ T\u22121\u2211 t=1 |st| ) log ( \u2016h\u2016K \u221a aLT b + 1 )\n\u2264 min h\u2208HK T\u2211 t=1 `t(h(xt)) + \u2016h\u2016K \u221a\u221a\u221a\u221a2a(L+ T\u22121\u2211 t=1 |st| ) log ( 2\u2212 1 2 `(0)T b + \u03c6 (a\u22121L) log (1 + T ) + 2 ) .\nReasoning as in the proof of Corollary 1, and denoting byC = 2 \u221a a \u221a H log ( 2\u2212\n1 2 `(0)T b + \u03c6 (a \u22121L) log (1 + T ) + 2 ) and B = b\u03c6 ( L a ) log (1 + T ) + L 2 4H , we get\nT\u2211 t=1 `t(ft(xt)) \u2264 min h\u2208HK T\u2211 t=1 `t(h(xt)) + b\u03c6 ( L a ) log (1 + T )\n+ \u2016h\u2016K T 1 4C max \u2016h\u2016 13K T 112 (2C) 13 , 2 14 ( T\u2211 t=1 `t(h(xt)) +B ) 1 4  . Dividing everything by T , taking the expectation of the two sides and using Jensen\u2019s inequality we have\nE[E`(f\u0304T )] \u2264 E [ 1\nT T\u2211 t=1 E`(ft) ] \u2264 min h\u2208HK E`(h) + b T \u03c6 ( L a ) log (1 + T )\n+ \u2016h\u2016K T\u2212 3 4C max\n{ \u2016h\u2016 1 3\nK T 1 12 (2C) 1 3 , 2 1 4\n( TE`(h) +B ) 1 4 } .\nWe now need to upper bound the terms in the max. Using Lemma 8, we have that, for any \u03b7, \u03b3 > 0\n2 1 4C \u2016h\u2016K T\u2212 3 4 (TE`(h) +B) 14\n\u2264 1 2\n( \u03b7\n1 2 \u2016h\u20162K + \u03b7\u2212 1 2C2T\u2212 3 2 2 1 2 (TE`(h) +B) 12 ) \u2264 1\n2\n( \u03b7\n1 2 \u2016h\u20162K +\n1 2 \u03b3\u22121\u03b7\u22121C4T\u22122 + \u03b3T\u22121(TE`(h) +B)\n) , (32)\nand 2 1 3C 4 3 \u2016h\u2016 4 3 K T \u2212 23 \u2264 2\n3\n( \u03b7\n3 2 \u2016h\u20162K + \u03b7\u22123C4T\u22122\n) . (33)\nConsider first (33). Observe that from Lemma 12 and Lemma 10, we have\nmin h\u2208HK min \u03b7>0 E`(h) + 2 3 (\u03b7 3 2 \u2016h\u20162K + C4T\u22122\u03b7\u22123)\n= min h\u2208HK min \u03b7>0 E`(h)\u2212 E`(f `\u03c1) + E`(f `\u03c1) +\n2 3 (\u03b7 3 2 \u2016h\u20162K + C4T\u22122\u03b7\u22123)\n\u2264 min h\u2208HK min \u03b7>0\nH\n2 \u2225\u2225h\u2212 f `\u03c1\u2225\u22252L2\u03c1X + E`(f `\u03c1) + 23(\u03b7 32 \u2016h\u20162K + C4T\u22122\u03b7\u22123) = min h\u2208HK min \u03b7>0 E`(f `\u03c1) + 2 3 C4T\u22122\u03b7\u22123 + H 2 (\u2225\u2225h\u2212 f `\u03c1\u2225\u22252L2\u03c1X + 43H \u03b7 32 \u2016h\u20162K )\n\u2264 min \u03b7>0 E`(f `\u03c1) +\n( 2\n3\n)2\u03b2 ( H\n2 )1\u22122\u03b2 \u03b73\u03b2 \u2016g\u20162L2\u03c1X + 2 3 C4T\u22122\u03b7\u22123\n\u2264 E`(f `\u03c1) + 2 ( 2\n3\n) 3\u03b2 \u03b2+1\n\u2016g\u2016 2 \u03b2+1\nL2\u03c1X\n( H\n2\n) 1\u22122\u03b2 \u03b2+1 (\nC4T\u22122 ) \u03b2 \u03b2+1 . (34)\nConsider now (32). Reasoning in a similar way we have\nmin h\u2208HK min \u03b7>0 min \u03b3>0 E`(h) + 1 2\n( \u03b7\n1 2 \u2016h\u20162K +\n1 2 \u03b3\u22121\u03b7\u22121C4T\u22122 + \u03b3T\u22121(TE`(h) +B) ) = min h\u2208HK min \u03b7>0 min \u03b3>0 ( 1 + 1 2 \u03b3 )( E`(h)\u2212 E`(f `\u03c1) + 1 2 ( 1 + 12\u03b3 )\u03b7 12 \u2016h\u20162K ) + 1 4 \u03b3\u22121\u03b7\u22121C4T\u22122\n+ 1\n2 \u03b3T\u22121\n( B + TE`(f `\u03c1) ) + E`(f `\u03c1)\n\u2264 min h\u2208HK min \u03b7>0 min \u03b3>0\nH\n2\n( 1 + 1\n2 \u03b3 )(\u2225\u2225h\u2212 f `\u03c1\u2225\u22252L2\u03c1X + 1H (1 + 12\u03b3)\u03b7 12 \u2016h\u20162K ) + 1 4 \u03b3\u22121\u03b7\u22121C4T\u22122\n+ 1\n2 \u03b3T\u22121\n( B + TE`(f `\u03c1) ) + E`(f `\u03c1)\n\u2264 min \u03b7>0 min \u03b3>0\n1 2 H1\u22122\u03b2\n( 1 + 1\n2 \u03b3 )1\u22122\u03b2 \u03b7\u03b2 \u2016g\u20162L2\u03c1X + 1 4 \u03b3\u22121\u03b7\u22121C4T\u22122 + 1 2 \u03b3T\u22121 ( B + TE`(f `\u03c1) ) + E`(f `\u03c1)\n\u2264 min \u03b3>0\n( H ( 1 + 1\n2 \u03b3\n)) 1\u22122\u03b2 \u03b2+1\n\u2016g\u2016 2 1+\u03b2\nL2\u03c1X (4\u03b3)\n\u2212 \u03b2\u03b2+1 C 4\u03b2 \u03b2+1T\u2212 2\u03b2 \u03b2+1 +\n1 2 \u03b3T\u22121\n( B + TE`(f `\u03c1) ) + E`(f `\u03c1) .\nWe now use the elementary inequality 1 + x \u2264 max(2, 2x),\u2200x \u2265 0, to study separately\nmin \u03b3>0\n(2H) 1\u22122\u03b2 \u03b2+1 \u2016g\u2016 2 1+\u03b2\nL2\u03c1X (4\u03b3)\n\u2212 \u03b2\u03b2+1 C 4\u03b2 \u03b2+1T\u2212 2\u03b2 \u03b2+1 +\n1 2 \u03b3T\u22121\n( B + TE`(f `\u03c1) ) , (35)\nand min \u03b3>0 (\u03b3H) 1\u22122\u03b2 \u03b2+1 \u2016g\u2016 2 1+\u03b2 L2\u03c1X (4\u03b3) \u2212 \u03b2\u03b2+1 C 4\u03b2 \u03b2+1T\u2212 2\u03b2 \u03b2+1 + 1 2 \u03b3T\u22121 ( B + TE`(f `\u03c1) ) . (36)\nFor (35), from Lemma 9, we have\nmin \u03b3>0\n(2H) 1\u22122\u03b2 \u03b2+1 \u2016g\u2016 2 1+\u03b2\nL2\u03c1X (4\u03b3)\n\u2212 \u03b2\u03b2+1 C 4\u03b2 \u03b2+1T\u2212 2\u03b2 \u03b2+1 +\n1 2 \u03b3T\u22121\n( B + TE`(f `\u03c1) ) \u2264 2 (2H) 1\u22122\u03b2 2\u03b2+1 \u2016g\u2016 2 2\u03b2+1\nL2\u03c1X 4\u2212 \u03b2 2\u03b2+1C 4\u03b2 2\u03b2+1T\u2212 2\u03b2 2\u03b2+1\n( 1\n2 T\u22121\n( B + TE`(f `\u03c1) )) \u03b22\u03b2+1 . (37)\nOn the other hand, for (36), for \u03b2 < 13 , we have that the minimum over \u03b3 is 0. For \u03b2 > 1 3 , from Lemma 9, we have\nmin \u03b3>0\n(\u03b3H) 1\u22122\u03b2 \u03b2+1 \u2016g\u2016 2 1+\u03b2\nL2\u03c1X (4\u03b3)\n\u2212 \u03b2\u03b2+1 C 4\u03b2 \u03b2+1T\u2212 2\u03b2 \u03b2+1 +\n1 2 \u03b3T\u22121\n( B + TE`(f `\u03c1) ) = min\n\u03b3>0 2\u2212 2\u03b2 \u03b2+1H 1\u22122\u03b2 \u03b2+1 \u2016g\u2016 2 1+\u03b2 L2\u03c1X \u03b3 1\u22123\u03b2 \u03b2+1 C 4\u03b2 \u03b2+1T\u2212 2\u03b2 \u03b2+1 + 1 2 \u03b3T\u22121\n( B + TE`(f `\u03c1) ) \u2264 2 12H 1\u22122\u03b24\u03b2 \u2016g\u2016 2 4\u03b2\nL2\u03c1X CT\u2212 1 2\n( 1\n2 T\u22121\n( B + TE`(f `\u03c1) )) 3\u03b2\u221214\u03b2 . (38)\nPutting together (34), (37), and (38), we have the stated bound."}, {"heading": "D.7 Proof of Theorem 3", "text": "Proof. From the proof of Theorem 2, we have that\nT\u2211 t=1 `t(ft(xt)) \u2264 inf h\u2208HK T\u2211 t=1 `t(h(xt)) + \u2016h\u2016K \u221a\u221a\u221a\u221a2aLT log(2\u2212 12 `(0)T b + \u03c6 (a\u22121L) log (1 + T ) + 2 ) + b\u03c6 ( a\u22121L ) log (1 + T ) .\nDividing everything by T , taking the expectation of the two sides and using Jensen\u2019s inequality we have E[E`(f\u0304T )] \u2264 E [ 1\nT T\u2211 t=1\nE`(ft) ]\n\u2264 inf h\u2208HK E`(h) + \u2016h\u2016K T\u2212 1 2 \u221a\u221a\u221a\u221a2aL log(2\u2212 12 `(0)T b + \u03c6 (a\u22121L) log (1 + T ) + 2 )\n+ b T \u03c6 ( a\u22121L ) log (1 + T ) .\nDenote by D = \u221a 2aL log ( 2\u2212\n1 2 `(0)T b + \u03c6 (a \u22121L) log (1 + T ) + 2\n) . Using the Lipschitzness of the loss,\nwe have E`(h)\u2212 E`(f `\u03c1) \u2264 L \u2225\u2225h\u2212 f `\u03c1\u2225\u2225L1\u03c1X , so\ninf h\u2208HK\nE`(h) +D \u2016h\u2016K T\u2212 1 2\n\u2264 inf h\u2208HK min \u03b7>0 L (\u2225\u2225h\u2212 f `\u03c1\u2225\u2225L1\u03c1X + \u03b72L \u2016h\u20162K ) + E`(f `\u03c1) + 1 2 D2T\u22121\u03b7\u22121\n\u2264 min \u03b7>0 E`(f `\u03c1) + CL1\u2212\u03b22\u2212\u03b2\u03b7\u03b2 +\n1 2 D2T\u22121\u03b7\u22121\n\u2264 E`(f `\u03c1) + C 1 1+\u03b2 (2L) 1\u2212\u03b2 1+\u03b2D 2\u03b2 \u03b2+1T\u2212 \u03b2 \u03b2+1 ,\nwhere in the last inequality we used Lemma 9."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "<lb>Stochastic gradient descent algorithms for training linear and kernel predictors are gaining more and<lb>more importance, thanks to their scalability. While various methods have been proposed to speed up their<lb>convergence, the model selection phase is often ignored. In fact, in theoretical works most of the time<lb>assumptions are made, for example, on the prior knowledge of the norm of the optimal solution, while in<lb>the practical world validation methods remain the only viable approach. In this paper, we propose a new<lb>kernel-based stochastic gradient descent algorithm that performs model selection while training, with no<lb>parameters to tune, nor any form of cross-validation. The algorithm builds on recent advancement in<lb>online learning theory for unconstrained settings, to estimate over time the right regularization in a data-<lb>dependent way. Optimal rates of convergence are proved under standard smoothness assumptions on the<lb>target function, using the range space of the fractional integral operator associated with the kernel.", "creator": "TeX"}}}