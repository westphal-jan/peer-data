{"id": "1406.5752", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2014", "title": "Divide-and-Conquer Learning by Anchoring a Conical Hull", "abstract": "We reduce a broad class of machine learning problems, usually addressed by EM or sampling, to the problem of finding the $k$ extremal rays spanning the conical hull of a data point set. These $k$ \"anchors\" lead to a global solution and a more interpretable model that can even outperform EM and sampling on generalization error. To find the $k$ anchors, we propose a novel divide-and-conquer learning scheme \"DCA\" that distributes the problem to $\\mathcal O(k\\log k)$ same-type sub-problems on different low-D random hyperplanes, each can be solved by any solver. For the 2D sub-problem, we present a non-iterative solver that only needs to compute an array of cosine values and its max/min entries. DCA also provides a faster subroutine for other methods to check whether a point is covered in a conical hull, which improves algorithm design in multiple dimensions and brings significant speedup to learning. We apply our method to GMM, HMM, LDA, NMF and subspace clustering, then show its competitive performance and scalability over other methods on rich datasets.", "histories": [["v1", "Sun, 22 Jun 2014 19:16:20 GMT  (4847kb,D)", "http://arxiv.org/abs/1406.5752v1", "26 pages, long version, in updating"]], "COMMENTS": "26 pages, long version, in updating", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["tianyi zhou", "jeff a bilmes", "carlos guestrin"], "accepted": true, "id": "1406.5752"}, "pdf": {"name": "1406.5752.pdf", "metadata": {"source": "CRF", "title": "Divide-and-Conquer Learning by Anchoring a Conical Hull", "authors": ["Tianyi Zhou", "Jeff Bilmes", "Carlos Guestrin"], "emails": ["guestrin}@uw.edu"], "sections": [{"heading": "1 Introduction", "text": "Expectation-maximization (EM) [14], sampling methods [18], and matrix factorization [27, 35] are three algorithms commonly used to produce maximum likelihood (or maximum a posteriori (MAP)) estimates of models with latent variables/factors, and thus are used in a wide range of applications such as clustering, topic modeling, collaborative filtering, structured prediction, feature engineering, and time series analysis. However, their learning procedures rely on alternating optimization/updates between parameters and latent variables, which suffer from local optima. Hence, their quality greatly depends on initialization and on using a large number of iterations for proper convergence [34].\nThe method of moments [32, 7, 23], in contrast, solves m equations by relating the first m moments of observation x \u2208 Rp to the m model parameters, and thus yields a consistent estimator with a global solution. In practice, however, sample moments usually suffer from unbearably large variance, which easily leads to the failure of final estimation, especially when m or p is large. Although recent spectral methods [10, 24, 20, 1] reduces m to 2 or 3 when estimating O(p) m parameters [2] by relating the eigenspace of lower-order moments to parameters in a matrix form up to column scale, the variance of sample moments is still sensitive to large p or data noise, which may result in poor estimation. Moreover, although spectral method using SVDs or tensor decomposition evidently simplifies learning, the computation can still be expensive for big data. In addition, recovering a parameter matrix with uncertain column scale might not be feasible for some applications.\nIn this paper, we reduce the learning in a rich class of models (e.g., matrix factorization and latent variable model) to finding the extreme rays of a conical hull from a finite set of real data points. This is obtained by applying a general separability assumption to either the data matrix in matrix factorization or the 2nd/3rd order moments in latent variable models. Separability posits that a set of n points, as rows of matrix X , can be represented by X = FXA, where the rows(bases) in XA are a\nar X\niv :1\n40 6.\n57 52\nv1 [\nst at\n.M L\n] 2\n2 Ju\nn 20\nsubset A \u2282 V = [n] of rows in X , which are called \u201canchors\u201d and are interesting to various models when |A| = k n. This property was introduced in [15] to establish the uniqueness of non-negative matrix factorization (NMF) under simplex constraints, and was later [26, 19] extended to non-negative constraints. We generalize it further to the model X = FYA for two (possibly distinct) finite sets of points X and Y , and build a new theory for the identifiability of A. This generalization enables us to apply it to more general models (ref. Table 1) besides NMF. More interestingly, it leads to a learning method with much higher tolerance to the variance of sample moments or data noise, a unique global solution, and a more interpretable model.\nAnother primary contribution of this paper is a distributed learning scheme, \u201cdivide-and-conquer anchoring (DCA)\u201d, for finding an anchor set A such that X = FYA by solving same-type subproblems on merely O(k log k) random drawn low-dimensional (low-D) hyperplanes. Each subproblem is of the form of (X\u03a6) = F \u00b7 (Y \u03a6) with random projection matrix \u03a6, and can easily be handled by most solvers due to the low dimension. This is based on the observation that the geometry of the original conical hull is partially preserved after a random projection. We analyze the probability of success for each sub-problem to recover part of A, and then study the number of sub-problems for recovering the whole A with high probability (w.h.p.). In particular, we propose an ultrafast non-iterative solver for sub-problems on the 2D plane, which requires computing an array of cosines and its max/min values, and thus results in learning algorithms with speedups of tens to hundreds of times. DCA improves multiple aspects of algorithm design since: 1) its idea of divide-and-conquer randomization gives rise to distributed learning that can reduce the original problem to multiple extremely low-D sub-problems that are much easier and faster to solve, and 2) it provides a fast subroutine checking if a point is covered by a conical hull, which can be embedded into other solvers.\nWe apply both the conical hull anchoring model and DCA to five learning models: Gaussian mixture models (GMM) [37], hidden Markov models (HMM) [6], latent Dirichlet allocation (LDA) [8], NMF [27], and subspace clustering (SC) [16]. The resulting models and algorithms show significant improvement in efficiency. On generalization performance, they consistently outperform spectral methods and matrix factorization, and are comparable to or even better than EM and sampling.\nIn the following, we will first generalize the separability assumption and minimum conical hull problem risen from NMF in \u00a7 2, and then show how to reduce more general learning models to a (general) minimum conical hull problem in \u00a7 3. \u00a7 4 presents a divide-and-conquer learning scheme that can quickly locate the anchors of the conical hull by solving the same problem in multiple extremely low-D spaces. Comprehensive experiments and comparison can be found in \u00a7 5."}, {"heading": "2 General Separability Assumption and Minimum Conical Hull Problem", "text": "The original separability property [15] is defined on the convex hull of a set of data points, namely that each point can be represented as a convex combination of certain subsets of vertices that define the convex hull. Later works on separable NMF [26, 19] extend it to the conical hull case, which replaced convex with conical combinations. Given the definition of (convex) cone and conical hull, the separability assumption can be defined both geometrically and algebraically. Definition 1 (Cone & conical hull). A (convex) cone is a non-empty convex set that is closed with respect to conical combinations of its elements. In particular, cone(R) can be defined by its k generators (or rays) R = {ri}ki=1 such that\ncone(R) = { \u2211k\ni=1 \u03b1iri | ri \u2208 R,\u03b1i \u2208 R+ \u2200i}. (1)\nIn the following, let X be a matrix with n columns and indexed by the integers [n], and XA be a subset of columns for A \u2286 [n]. Definition 2 (Separability assumption). All the data points in X are covered in a finitely generated and pointed cone whose generators are a subset A \u2286 [n] of data points. That is,\n\u2203A \u2286 [n] s.t. \u2200i \u2208 [n], Xi \u2208 cone (XA) , XA = {xi}i\u2208A. (2) An equivalent algebraic form is X = FXA,\u03a0F = [ Ik F \u2032 ] , where |A| = k, Ik is a k-by-k identity\nmatrix, F \u2032 \u2208 R(n\u2212k)\u00d7k+ , and \u03a0 is a row permutation matrix.\nThe above algebraic form gives rise to an NMF model X = FXA. The cone cone(XA) is finitely generated because all the elements in X are conical combinations of a finite set XA. It is also pointed\nbecause its non-negativity does not allow it containing both x and \u2212x. According to basic rules [30], a finitely generated and pointed cone C possesses a finite and unique set of extreme rays R, and C = cone(R) is the conical hull generated by these extreme rays R. When only one point in X is on each extreme ray, the separability assumption guarantees the uniqueness of NMF solution XA and F . Moreover, when X consists of real data points, they are often in practice interpretable since they constitute the \u201cessential\u201d set that uses actual data to express itself rather than artificial basis/factors.\nBased on separability assumption in Definition 2, we define the minimum conical hull problem, and offer several formulations for how it can be solved, including a novel formulation that utilizes submodular optimization. Each of these, in later sections of the paper, will be compared in various ways against the baseline methods established in this section. Definition 3 (Minimum Conical Hull Problem). Given a X having an index set V = [n] of its rows, minimum conical hull problem finds the subset of rows that define the same cone as all the rows. That is, find A \u2208 2V that solves:\nmin A\u2282V\n|A|, s.t., cone(XA) = cone(X). (3)\nwhere cone(XA) is the cone induced by the rows A of X .\nWe note that without the separability assumption, there is no non-trivial set A \u2282 V that is feasible. As mentioned above, there is a weakly polynomial time algorithm that in the non-negative case solves the problem via backward removal. Definition 4. A nonnegative matrix XA is simplicial if no row in XA can be represented in the convex hull of the remaining rows in XA.\nLemma 1 (Lemma 5.3 from [5]). If a nonnegative matrix X has a separable factorization WXA of inner-dimension at most k then there is one in which XA is simplicial.\nIn the proof of this lemma, Arora proposes a procedure that starting at A = V , one-by-one, removes rows violating the simpliciality of XA. This is using linear programming, where for each row in XA we test if it can be represented by convex combination of remaining rows. The rows that cannot be represented by other rows are selected as the \u201cloners\u201d (anchors). Theorem 1 (Theorem 5.4 in [5]). There is an algorithm that runs in time [weakly] polynomial in n, m, and k and given a matrix X outputs a separable factorization with inner dimension at most k (if one exists).\nThe simplicial XA obtained by the algorithm is the solution of our minimum conical hull problem. Theorem 2. A simplicial XA satisfying X = FXA, Fi,j \u2265 0 \u2200{i, j} is the solution of (3). Such XA can be achieved by Arora\u2019s algorithm that runs in time polynomial in n, m, and k.\nProof. It is easy to verify that XA obtained by Arora\u2019s exact algorithm [5] is a feasible solution fulfilling the constraint in (8). However, suppose (for contradiction) XA does not define the minimum conical hull, i.e., there exists a simplicial XB with |B| < |A| and X = F \u2032 XB . So we have\nXA = CXB , XB = DXA \u2192 XA = (CD)XA. (4)\nIf U = CD, for arbitrary row XjA in XA, we have\nXjA = U j \\jXA\\j + U j jX j A. (5)\nWe cannot make U jj = 1\u2200j, because in this case CD = I|A|, which leads to contradiction with the fact that CD has inner dimension of |B|. Therefore, we can always find at least one j such that U j\\j is not all-zero vector and\nXjA = U j\\jXA\\j\n1\u2212 U jj , (6)\nwhich violates the constraint that XA is simplicial and thus causes contradiction. So we cannot find a simplicial XB with |B| < |A| and X = W \u2032 XB , which implies A is the solution of minimum conical hull problem (8).\nLastly, we see how Equation (3) can be seen as a submodular cover problem [38]. Define cover(XA) to be the size of the largest set B \u2286 V of rows of X that lies within cone(XA). That is,\ncover(XA) = |{v \u2208 V : \u2203f \u2208 R|A|+ with Xv = f>XA}|. (7)\nDefinition 5. For a nonnegative matrix X with a ground set V of all its row indexes, the minimum conical hull problem is to solve:\nmin A\u2282V\n|A|, s.t., cover(A) = cover(V ). (8)\nIt can be verified that the constraint in Definition 5 equals to the constraint to F in Definition 2, and that f(A) = cover(XA\u222aI), where XI is any set of linearly independent row vectors, is submodular [17]. Therefore, we can apply efficient greedy algorithm for submodular cover to the minimum conical hull problem with approximation guarantee."}, {"heading": "2.1 General Separability Assumption and General Minimum Conical Hull Problem", "text": "By generalizing the separability assumption, we obtain a general minimum conical hull problem that can reduce more general learning models besides NMF, e.g., latent variable models and matrix factorization, to finding a set of \u201canchors\u201d on the extreme rays of a conical hull. At first, we only need F to be non-negative, and do not require XA or X to be non-negative. Because a pointed cone cone(XA) after rotation is still a pointed cone with unchanged anchor set A. Secondly, to ensure cone(XA) to be finitely generated, we only need the points in XA to be selected from a finite set, which could be another set Y and is not necessary to be X itself. Lastly, extra constraint can be imposed to the coefficients F \u2032 to encourage particular structures among data points. Definition 6 (General separability assumption). All the n data points(rows) in X are covered in a finitely generated and pointed cone (i.e., if x \u2208 cone(YA) then \u2212x 6\u2208 cone(YA)) whose generators form a subset A \u2286 [m] of data points in Y such that @i 6= j, YAi = a \u00b7 YAj . Geometrically, it says\n\u2200i \u2208 [n], Xi \u2208 cone (YA) , YA = {yi}i\u2208A. (9) An equivalent algebraic form is X = FYA,\u03a0F = [ Ik F \u2032 ] , where |A| = k, Ik is a k-by-k identity\nmatrix, F \u2032 \u2208 S \u2286 R(n\u2212k)\u00d7k+ , and \u03a0 is a row permutation matrix.\nWhen X = Y and S = R(n\u2212k)\u00d7k+ , it degenerates to the original separability assumption given in [3]. We generalize the minimum conical hull problem from [3]. Under the general separability assumption, it aims to find the anchor set A from the points in Y rather than X . Definition 7 (General Minimum Conical Hull Problem). Given a finite set of points X and a set Y having an index set V = [m] of its rows, the general minimum conical hull problem finds the subset of rows in Y that define a super-cone for all the rows in X . That is, find A \u2208 2V that solves:\nmin A\u2282V\n|A|, s.t., cone(YA) \u2287 cone(X). (10)\nwhere cone(YA) is the cone induced by the rows A of Y .\nWhen X = Y , it degenerates to the original minimum conical hull problem defined in [3]. A critical question of the general one is whether/when the solution A is unique. When X = Y and X = FXA, which is the case for matrix factorization and latent variable model with nonzero off-diagonal entries in D in \u00a7 3.3, by following the analysis of the separability assumption in [3],we can prove that A is unique and identifiable given X . However, when X 6= Y and X = FYA, it is clear that there could be multiple legal choices of A (e.g., there could be multiple layers of conical hulls containing a set of points X in the center). Fortunately, when the rows of Y are rank-one matrices after vectorization (concatenating all columns to a long vector), which is the common case in most latent variable models with diagonal D in \u00a7 3.3, A can be uniquely determined if the number of rows in X exceeds 2. Lemma 2 (Identifiability). IfX = FYA with the additional structure Ys = vec(Osi \u2297Osj ) whereOi is a pi \u00d7 k matrix and Osi is its sth column, under the general separability assumption in Definition 6, two (non-identical) rows in X are sufficient to exactly recover the unique A, Oi and Oj .\nProof. Two non-identical rows of X can be represented as two sum mixtures of rank-one matrices in YA with weights a 6= b, i.e.,\nX1 = vec( \u2211k s=1 a\n(s)Osi \u2297Osj ) = vec(OiDiag(a)OTj ), X2 = vec( \u2211k s=1 b (s)Osi \u2297Osj ) = vec(OiDiag(b)OTj ). (11)\nIn the following proof, we temporarily use X1 and X2 to denote themselves before vectorization. Given SVD of X1 as X1 = Ui\u03a3UTj , Oi and Oj can be represented in the following forms.\nOi = Ui\u03a3 1/2VDiag(a\u22121/2), Oj = Uj\u03a3 1/2V \u2212TDiag(a1/2). (12)\nNote the decomposition of X1 in (11) will stay the same when we change V in (12) to any other non-singular matrix. Thus a single row in X cannot uniquely identify Oi and Oj . By further given X2, we have\n\u03a3\u22121/2UTi X2Uj\u03a3 \u22121/2 = \u03a3\u22121/2UTi OiDiag(b)O T j Uj\u03a3 \u22121/2 = VDiag(b./a)V \u22121. (13)\nThe second equality is obtained by applying (12). Let the (unique) eigendecomposition of VDiag(b./a)V \u22121 to be W\u039bWT , we have V = WDiag(c). Substituting it into (12) yields\nOi = Ui\u03a3 1/2WDiag(c \u00b7 / \u221a a), Oj = Uj\u03a3 1/2W\u2212TDiag( \u221a a \u00b7 /c). (14)\nTherefore, we can determine each element s \u2208 A by checking if \u2203t, Ys = vec(Osi \u2297 Osj ) = vec [ (Ui\u03a3 1/2W )(t) \u2297 (Uj\u03a31/2W\u2212T )(t) ]\nfor each s \u2208 [n] in the ground set. Since all the quantities in the rank-one matrix on the right hand side is uniquely fixed for each t \u2208 [k], each s selected from the ground set is unique.\nIn applications such as latent variable model, as stated in \u00a7 3.3, the set of n rank-one matrices in Y is generated by computing xi \u2297 xj for n data points or observations, where xi is the ith group of features. The general minimum conical hull problem then equals to selecting k data points. Since in this case the columns of Oi and Oj are respectively estimated as xi and xj of the k selected data points, in order to further guarantee the uniqueness of the k selected data points rather than only the k out products xi \u2297 xj , we need to avoid the case when xi and xj in two different data points generate the same xi \u2297 xj . According to (14), we have to assume @{s \u2208 A, t 6= s, \u03b4 6= 0} satisfying {xi}t = \u03b4{xi}s and {xj}t = {xj}s/\u03b4, where {xi}s denotes feature xi for sth data point. Note this assumption is much weaker than limiting {x}t 6= \u03b4{x}s in original separable NMF. In above proof, we need a 6= b to ensure the identifiability of A. In practice, this inequality can be achieved by drawing random vector \u03b7 from a continuous distribution, as we did in (16) within \u00a7 3. Then the inequality holds with probability 1."}, {"heading": "3 Minimum Conical Hull Problem for General Learning Models", "text": "In this section, we discuss how to reduce the learning of general models such as matrix factorization and latent variable model to the (general) minimum conical hull problem. Five examples are given in Table 1 to show how this general technique can be applied to specific models."}, {"heading": "3.1 Matrix Factorization", "text": "Besides NMF, we consider more general matrix factorization (MF) models that can operate on negative features and specify a complicated structure of F . The MF X = FW is a deterministic latent variable model where F and W are deterministic latent factors. By assigning a likelihood p(Xi,j |Fi, (WT )j) and priors p(F ) and p(W ), its optimization model can be derived from maximum likelihood or MAP estimate. The resulting object is usually a loss function `(\u00b7) of X \u2212 FW plus regularization terms for F and W , i.e., min `(X,FW ) +RF (F ) +RW (W ).\nSimilar to separable NMF, minimizing the objective of general MF can be reduced to a minimum conical hull problem that selects the subset A with X = FXA. In this setting, RW (W ) = \u2211k i=1 g(Wi) where g(w) = 0 if w = Xi for some i and g(w) = \u221e otherwise. This is equivalent to applying a prior p(Wi) with finite support set on the rows of X to each row of W . In addition, the regularization of F can be transformed to geometric constraints between points in X and in XA. Since Fi,j is the conical combination weight of XAj in recovering Xi, a large Fi,j intuitively indicates a small angle between XAj and Xi, and vice verse. For example, the sparse and graph Laplacian prior for rows of F in subspace clustering can be reduced to \u201ccone clustering\u201d finding A."}, {"heading": "3.2 Example: Subspace Clustering", "text": "Subspace clustering (SC) assumes that the data points in each cluster exist in a low-Dimensional subspace. The subspaces defining different clusters are assumed to be distinguishable, e.g., with large principle angles between each other [36]. SC outperforms traditional clustering methods in various tasks such as motion segmentation. Most existing subspace clustering methods [16, 36], relies on spectral clustering to the sparse representations of data points, which are obtained by finding a sparse C with diag(C) = 0 in model X = CX . This usually requires a series of time costly lasso-type optimization. Then (spectral) clustering to rows of C guarantees to provide clustering labels which can lead to reliable estimation of the k subspaces [36].\nUnder the general separability assumption, SC model can be reduced to general minimum conical hull problem X = FXA once we impose an additional block diagonal constraint F \u2032 \u2208 S to the nonnegative F \u2032 (S in Definition 6), in particular, \u03a0F = [Diag(Ik1 , . . . , Ikk); Diag(F \u2032 1, . . . , F \u2032 k)], where F \u2032i \u2208 R (ni\u2212ki)\u00d7ki + , \u2211k i=1 ni = n, and \u03a0 is a row-permutation matrix. So the ni points in cluster i have coefficient matrix Fi in the subspace spanned by ki \u201canchors\u201d associated with Iki in\n\u03a0F , i.e., \u2200i \u2208 [k], Xi = FiXAi ,\u03a0iFi = [ Iki F \u2032i ] , where Xi \u2208 Rni\u00d7p are the points in cluster i, and\nA = \u228ek i=1Ai, the disjoint union of all the anchors from the k clusters. Hence, given clustering labels, our method is equivalent to applying a minimum conical hull problem model to points in each cluster.\nOur goal is to find out the k groups of anchors and simultaneously separate the points covered inside the associated k cones {cone(XAi)}i\u2208[k]. In other words, we reduce SC to a \u201ccone clustering\u201d problem separate k cones of data points. Each cone covers all the points in one cluster, and the faces spanned by its anchors separate these points from those covered by other cones.\nSeparable NMF is a special case of separable SC when k = 1. When k > 1, the cone clustering problem might be difficult to solve in high-dim space. But by applying the DCA scheme proposed in the next section, we will show that this problem can be reduced to several cone clustering problems on low-D hyperplanes, each can be solved by efficient existing clustering method."}, {"heading": "3.3 Latent Variable Model", "text": "Different from deterministic MF, we build a system of equations from the moments of probabilistic latent variable models, and then formulate it as a general minimum conical hull problem, rather than directly solve it. Let the generalization model be h \u223c p(h;\u03b1) and x \u223c p(x|h; \u03b8), where h is a latent variable, x stands for observation, and {\u03b1, \u03b8} are parameters. In a variety of graphical models such as GMMs and HMMs, we need to model conditional independence between groups of features. This is also known as the multi-view assumption. W.l.o.g., we assume that x is composed of three groups(views) of features {xi}i\u2208[3] such that \u2200i 6= j, xi \u22a5 xj |h. We further assume the dimension k of h is smaller than pi, the dimension of xi. Since the goal is learning {\u03b1, \u03b8}, decomposing the moments of x rather than the data matrix X can help us get rid of the latent variable h and thus avoid\nalternating minimization between {\u03b1, \u03b8} and h. When E(xi|h) = hTOTi (linearity assumption), the second and third order moments can be written in the form of matrix operator.{\nE (xi \u2297 xj) = E[E(xi|h)\u2297 E(xj |h)] = OiE(h\u2297 h)OTj , E (xi \u2297 xj \u00b7 \u3008\u03b7, xl\u3009) = Oi [E(h\u2297 h\u2297 h)\u00d73 (Ol\u03b7)]OTj ,\n(15)\nwhere A\u00d7n U denotes the n-mode product of a tensor A by a matrix U , \u2297 is the outer product, and the operator parameter \u03b7 can be any vector. We will mainly focus on the models in which {\u03b1, \u03b8} can be exactly recovered from conditional mean vectors {Oi}i\u2208[3] and E(h\u2297 h)1, because they cover most popular models such as GMM and HMM in real applications.\nThe left hand sides (LHS) of both equations in (15) can be directly estimated from training data, while their right hand sides (RHS) can be written in a unified matrix form OiDOTj with Oi \u2208 Rpi\u00d7k and D \u2208 Rk\u00d7k. By using different \u03b7, we can obtain 2 \u2264 q \u2264 pl + 1 independent equations, which compose a system of equations for Oi and Oj . Given the LHS, we can obtain the column spaces of Oi and Oj , which respectively equal to the column and row space of OiDOTj , a low-rank matrix when pi > k. In order to further determine Oi and Oj , our discussion falls into two types of D.\nWhen D is a diagonal matrix. This happens when \u2200i 6= j,E(hihj) = 0. A common example is that h is a label/state indicator such that h = ei for class/state i, e.g., h in GMM and HMM. In this case, the two D matrices in RHS of (15) are{\nE(h\u2297 h) = Diag( \u2212\u2212\u2212\u2192 E(h2i )), E(h\u2297 h\u2297 h)\u00d73 (Ol\u03b7) = Diag( \u2212\u2212\u2212\u2192 E(h3i ) \u00b7Ol\u03b7),\n(16)\nwhere \u2212\u2212\u2212\u2192 E(hti) = [E(ht1), . . . ,E(htk)]. So either matrix in LHS of (15) can be written as a sum of k rank-one matrices, i.e., \u2211k s=1 \u03c3 (s)Osi \u2297Osj , where Osi is the sth column of Oi.\nThe general separability assumption posits that the set of k rank-one basis matrices constructing the RHS of (15) is a unique subset A \u2286 [n] of the n samples of xi \u2297 xj constructing the left hand sides, i.e., Osi \u2297Osj = [xi \u2297 xj ]As = XAs,i \u2297XAs,j , the outer product of xi and xj in (As)th data point. Therefore, by applying q \u2212 1 different \u03b7 to (15), we obtain the system of q equations in the following form, where Y t is the estimate of the LHS of tth equation from training data.\n\u2200t \u2208 [q], Y (t) = k\u2211 s=1 \u03c3t,s[xi \u2297 xj ]As \u21d4 [vec(Y (t))]t\u2208[q] = \u03c3[vec(Xt,i \u2297Xt,j)]t\u2208A. (17)\nThe right equation in (17) is an equivalent matrix representation of the left one. Its LHS is a q \u00d7 pipj matrix, and its RHS is the product of a q \u00d7 k matrix \u03c3 and a k \u00d7 pipj matrix. By letting X \u2190 [vec(Y (t))]t\u2208[q], F \u2190 \u03c3 and Y \u2190 [vec(Xt,i \u2297 Xt,j)]t\u2208[n], we can fit (17) to X = FYA in Definition 6. Therefore, learning {Oi}i\u2208[3] is reduced to selecting k rank-one matrices from {Xt,i \u2297 Xt,j}t\u2208[n] indexed by A, and defining the extreme rays of a conical hull covering the q matrices {Y (t)}t\u2208[q]. Given the anchor set A, we have O\u0302i = XA,i and O\u0302j = XA,j by assigning real data points indexed by A to the columns of Oi and Oj . Given Oi and Oj , \u03c3 can be estimated by solving (17). In many models, a few rows of \u03c3 are sufficient to recover \u03b1.\nIn (17), the dimension of each matrix after vectorization is pipj . In practice, this could lead to computational burden. Moreover, real data could suffers from missing features, which make computing all entries of Xt,i \u2297Xt,j impossible. However, thanks to the matrix completion research [9, 12], when k min{pi, pj}, we can retain merely m = O(max{pi, pj}k log2(pi + pj)) pipj entries in the vectorization of each matrix from {Y (t)}t\u2208[q] and {Xt,i \u2297Xt,j}t\u2208[n] in (17). W.h.p., the true A can still be successfully recovered from such partial information.\nWhen D is a symmetric matrix with nonzero off-diagonal entries. This happens in \u201cadmixture\u201d models, e.g., h can be a general binary vector h \u2208 {0, 1}k or a vector on the probability simplex, and\n1Note our method can also handle more complex models that violate the linearity assumption and need higher order moments for parameter estimation. By replacing xi in (15) with vec(xi\u2297n), the vectorization of the nth tensor power of xi, Oi can contain nth order moments for p(xi|h; \u03b8). However, since higher order moments are either not necessary or difficult to estimate due to high sample complexity, we will not study them in this paper.\nthe conditional mean E(xi|h) is a mixture of columns in Oi. The most well known example is LDA, in which each document is generated by multiple topics.\nWe apply the general separability assumption by only using the first equation in (15), and treating the matrix in its LHS as X in X = FXA. When the data are extremely sparse, which is common in text data, selecting the rows of the denser second order moment as bases is a more reasonable and effective assumption compared to sparse data points. In this case, the p rows of F contain k unit vectors {ei}i\u2208[k]. This leads to a natural assumption of \u201canchor word\u201d for LDA [4]. When the data is not sure to be sparse, as we will show in the example of Kalman filter, we apply a \u201cbilateral separability assumption\u201d to both matrices on LHS of (15). It is weaker than the one used for diagonal D case, but much stronger than the one used for above LDA case. In particular, we apply the general separability assumption X = FYA to both the matrix on LHS of each equation in (15) and its transpose, i.e., the k column of Oi are selected from n instances of xi, while the k columns of Oj are selected from n instances of xj . In theory, it is hard to analyze the uniqueness of Oi and Oj under mild condition in this case, but it works pretty well in practice, and usually does ensure unique solution."}, {"heading": "3.4 Example: Multi-view Mixture Model", "text": "Mixture model (MM) p(x) = \u2211k j=1 \u03c3jp(x; \u03b8\nj) is a latent variable model broadly used in unsupervised learning including clustering, where k is the number of clusters and is normally much less than the number of features p in x. w.l.o.g., we assume the number of views to be 3. Under multi-view assumption x = {xi}i\u2208[3] with \u2200i 6= j, xi \u22a5 xj |h, MM generates an observation x by firstly drawing a label indicator h \u223c \u03c3 \u2208 \u2206k\u22121 from {ei}i\u2208[k] and then drawing features of different views xi \u223c p(xi| \u2211k j=1 hj\u03b8 j i ) independently. When h = ej , i.e., x belongs to class/cluster j, xi is drawn from p(xi|\u03b8ji ), where \u03b8 j i is the distribution parameter for view i in cluster j. We mainly focus on recovering the mean of p(xi|\u03b8ji ) for all {i, j}2, which is the learning goal of a majority number of mixture models in practice.\nOne example of MM is Gaussian MM (GMM), in which p(xi|\u03b8ji ) is Gaussian N ((O j i ) T ,\u03a3ji ), where Oji is the j th column of Oi \u2208 Rpi\u00d7k. Thus we have E(xi|h) = hTOTi , which is consistent to the linearity assumption. This GMM exactly fits the latent variable model we presented in \u00a7 3.3. Since h \u2208 {ei}i\u2208[k], E(hihj) = 0, which implies the two D matrices are diagonal. So GMM falls into the first type of latent variable model in \u00a7 3.3. By (15) and (17), given data matrix X where Xt,i is view i of data point t and is the tth row of Xi, learning GMM can be reduced to solving a general conical hull problem in the form of X = FYA by letting\nX \u2190 1 n\n[ vec[XT1 X2]; vec[X T 1 Diag(X3\u03b7t)X2]t\u2208[q] ] , Y \u2190 [vec(Xt,1 \u2297Xt,2)]t\u2208[n]. (18)\nIn practical algorithm, we can further apply the \u201cmatrix completion\u201d trick at the end of \u00a7 3.3 to the columns of X and Y in (18) by randomly sampling a subset \u2126 \u2286 [p1]\u00d7 [p2], i.e.,\nX \u2190 1 n\n[ vec[(XT1 X2)\u2126]; vec[(X T 1 Diag(X3\u03b7t)X2)\u2126]t\u2208[q] ] , Y \u2190 [vec((Xt,1 \u2297Xt,2))\u2126]t\u2208[n].\n(19) The identifiability of A in Lemma 2 still holds w.h.p. in this case. However, the resulting model requires much less computations than the \u201cfull matrix\u201d model.\nEmpirically (and shown in \u00a7 5), we find out that our method performs appealingly even when applying the 3-view assumption to data that do not have multi-view features by randomly splitting all features into 3 groups. Obviously, some feature correlation information is ignored in this case, but the rest correlations between views captured by our method are usually sufficient to produce a reliable estimation of the parameters. In addition, the Gaussian distribution assumption is only a special case satisfying the linearity assumption E(xi|h) = hTOTi made in our model. So DCA can be actually applied to more general mixture models.\n2However, as we mentioned below (15), it is possible to recover covariance or higher moments for each p(xi|\u03b8ji ) by using higher order sample moments of xi."}, {"heading": "3.5 Example: Hidden Markov Model", "text": "Hidden Markov model (HMM) is a latent variable model broadly used to analyze sequential data and time series. HMM can be depicted as a Markov chain of hidden states {ht}t\u2208[T ] (ht = ei if the state is i), each ht generates the observation xt at time t. Markov chain property implies \u2200j 6= t, xt \u22a5 xj |ht, so multi-view assumption holds. The generalization process is h1 \u223c \u03c0 \u2208 \u2206k\u22121, ht \u223c Tht\u22121 \u2208 \u2206k\u22121 and xt \u223c p(xt|ht, O), where T \u2208 Rk\u00d7k is the transition matrix such that Ti,j = p(ht = ei|ht\u22121 = ej), and O \u2208 Rp\u00d7k is the emission matrix such that E(xt|ht) = hTt OT . This is in consistency with our linearity assumption. Normally, the number of states k p, the dimension of observation x. Given one or several sequences of observations {xt}t\u2208[T ], the goal of learning HMM is to estimate parameters {O, T}. HMM can be converted to a special case of MM by integrating out the two hidden states before and after the current one. In particular, w.l.o.g., for each triple of observations {x1, x2, x3} and the corresponding hidden states {h1, h2, h3}, let h1 \u223c \u03c0, by integrating out h1 and h2, we have [2] E(x2|h2) = hT2 OT \u2192 hT2 OT2 , h2 \u223c T\u03c0, E(x1|h2) = \u2211 h1 E(x1|h1) \u00b7 p(h2|h1)p(h1)p(h2) = h T 2 [ ODiag(\u03c0)TTDiag((T\u03c0)\u22121) ]T \u2192 hT2 OT1 , E(x3|h2) = \u2211 h3\nE(x3|h3)p(h3|h2) = hT2 [OT ]T \u2192 hT2 OT3 . (20)\nTherefore, we can obtain a system of equations in the same form of (15) with diagonal D, i.e.,{ E (x2 \u2297 x3) = ODiag(T\u03c0)[OT ]T , E (x2 \u2297 x3 \u00b7 \u3008\u03b7, x1\u3009) = ODiag(T\u03c0 \u00b7O1\u03b7)[OT ]T , (21)\nA data matrix X = [X1, X2, X3] with Xi \u2208 Rn\u00d7p whose rows are all the triples {x1, x2, x3} can be built from available sequences of observations. General separability assumption posits that the conditional means E(x2|h2) and E(x3|h2), i.e., the columns in O and OT , are selected from real instances of observations. So we can fit the problem of learning emission matrix O in HMM to a conical hull problem X = FYA by using the same formulas (18) or (19) for GMM.\nWe will show how the transition matrix T can be immediately recovered given the anchor set A in \u00a7 4."}, {"heading": "3.6 Example: Kalman Filter", "text": "When extending the discrete latent state h in HMM to more general continuous latent variable, we can obtain a linear dynamical system (LDS), which has been widely used in filtering and smoothing of time series data.\nht = Tht\u22121 + wt, xt = h T t O T + vt, (22)\nwhere ht and xt are hidden (continuous) state and the associated observation (output) respectively at time t, while wt and vt are noise terms independent to Tht and Oht respectively. Hence, when distributions p(wt) and p(vt) are symmetric, we have linearity E(ht|ht\u22121) = Tht\u22121 and E(xt|ht) = hTt O\nT , which are analogous to HMM. Similar to HMM, the multi-view or conditional independence assumption automatically holds for such Markov typed model, and it is also reasonable to assume that k p. The goal of learning is to estimate {O, T} and the distribution parameters \u0398 for p(wt) and p(vt).\nCompared to HMM, the only difference here is that h changes from disjoint discrete state to kD continuous variable. In our approach, this will lead to different latent variable moments and thus discrepant matrix D in \u00a7 3.3. Therefore, reducing (22) to the general minimum conical hull problem exactly follows the same procedures for HMM in \u00a7 3.5 except replacing h2 \u223c T\u03c0 to h2 \u223c p(Th1 + w2), i.e., E(x2|h2) = hT2 OT \u2192 hT2 OT2 , h2 \u223c p(Th1 + w2), E(x1|h2) = \u2211 h1 E(x1|h1) \u00b7 p(h2|h1)p(h1)p(h2) = h T 2 [ ODiag(\u03c0)TTDiag((T\u03c0)\u22121) ]T \u2192 hT2 OT1 , E(x3|h2) = \u2211 h3\nE(x3|h3)p(h3|h2) = hT2 [OT ]T \u2192 hT2 OT3 . (23)\nTherefore, we can obtain a system of equations in the same form of (15), i.e.,{ E (x2 \u2297 x3) = OE(h2 \u2297 h2)[OT ]T , E (x2 \u2297 x3 \u00b7 \u3008\u03b7, x1\u3009) = O[E(h2 \u2297 h2 \u2297 h2)\u00d73 (O1\u03b7)][OT ]T ,\n(24)\nIn most cases, the square matrix D in both equations given in (24) is hardly to be diagonal, even when both E(h1\u2297h1) and E(w2\u2297w2) are diagonal. This falls into the second type of D discussed in \u00a7 3.5. However, since the observations are usually not sparse, different from LDA in \u00a7 3.7 where we directly apply original separability assumption to the moment matrices, we apply a \u201cbilateral separability assumption\u201d to both matrices on LHS of (24). It is weaker than the one used for diagonal D case, but much stronger than the one used for LDA case. In particular, we apply the general separability assumption to both the matrix on LHS of each equation in (24) and its transpose, i.e., the k column of O are selected from n instances of x2, while the k columns of OT are selected from n instances of x3. To fit the notations in X = FYA, let{\nX \u2190 1n [ XT2 X3; [X T 2 Diag(X1\u03b7t)X3]t\u2208[q] ] , Y \u2190 X2,\nX \u2190 1n [ XT2 X3; [X T 2 Diag(X1\u03b7t)X3]t\u2208[q] ]T , Y \u2190 X3.\n(25)\nAfter achieving O and OT , square matrix T can be immediately determined by solving a linear equation.\nWhen the noise terms in this system are randomly drawn from Gaussians, (22) leads to the infamous Kalman filter model."}, {"heading": "3.7 Example: Latent Dirichlet Allocation", "text": "Latent Dirichlet Allocation (LDA) is a latent variable model that is widely applied to bag-of-words features for text and vision data in order to extract semantic topics, whose effectiveness has been proved in clustering and classification tasks. It generates the jth word xj \u2208 {ei}i\u2208[p] in a document by firstly drawing a topic proportion h \u223c Dir(\u03b1) (Dirichlet distribution with parameter \u03b1 = {\u03b1i}i\u2208[k]) for the document, then drawing a topic zj \u223c h from {ei}i\u2208[k] with associated probability \u03b2 = zTj OT over p words in vocabulary, and drawing word xj \u223c \u03b2 at last. The topic probability matrix O stores the conditional probabilities Oi,j = p(x = ei|z = ej). Given h, different words in a document are generated independently, so the multi-view assumption \u2200j 6= t, xt \u22a5 xj |h holds. Since E(xj |h) = \u2211 z E(xj |z)p(z|h) = hTOT , the linearity assumption is satisfied on LDA. Normally, the number of topics k p, the number of words in vocabulary. Given the bag-of-words features of n documents X \u2208 Zn\u00d7p+ , where Xi,j denotes the number of times the jth word appears in the ith document, the goal of learning LDA is to estimate O and \u03b1.\nWe consider the two-gram statistics, w.l.o.g., the word-word co-occurrence matrix for the first two words x1 and x2 in a document, i.e.,\nE(x1 \u2297 x2) = E [E(x1|h)\u2297 E(x2|h)] = OE(h\u2297 h)OT , (26)\nwhere E(h \u2297 h) is the topic-topic covariance matrix, which is exactly the D matrix for the first equation in (15). Since LDA is an \u201cadmixture\u201d model that allows multiple topics in one document, this D matrix has nonzero off-diagonal entries, and thus LDA falls into the second type of latent variable model studied in \u00a7 3.3. Therefore, under separability assumption, learning O in LDA is reduced to conical hull problem X = FXA, where X here is the word-word co-occurrence matrix in (26), and O is F after column normalization (to the probability simplex). In order to take advantage of all word pairs rather than just the first one, the word-word co-occurrence matrix E(x1 \u2297 x2) is estimated by averaging over all word pairs in all documents, i.e., the X in X = FXA can be estimated from the bag-of-words feature matrix X as\nX \u2190 X\u0304T X\u0304 \u2212Diag ( 1T X\u0302 ) , X\u0302i,j \u2190\nXi,j mi(mi \u2212 1)\n, X\u0304i,j \u2190 Xi,j\u221a\nmi(mi \u2212 1) ,m\u2190 X1. (27)\nWe will show how the parameter O and \u03b1 can be immediately recovered given the anchor set A in \u00a7 4."}, {"heading": "4 Algorithms for Minimum Conical Hull Problem", "text": ""}, {"heading": "4.1 Divide-and-Conquer Anchoring (DCA) for General Minimum Conical Hull Problem", "text": "According to the above section, parameter learning of MF and latent variable model can be reduced to finding the anchor set A for a conical hull such that X = FYA. In this section, we will focus on a novel distributed learning scheme that spans different dimensions in developing algorithms for X = FYA. The proposed divide-and-conquer anchoring (DCA) in Algorithm 1 decomposes the conical hull problem to multiple (much easier) sub-problems in extremely low dimensions, which can be solved in parallel. The first DCA algorithm was proposed in [39] only for separable NMF, but this paper will largely extend the idea to much richer class of problems and algorithm designs.\nThe key insights of DCA come from two observations on the geometry of the convex cone. First, projecting a conical hull to a lower-D hyperplane partially preserves its geometry. This enables us to distribute the original problem to a few much smaller sub-problems, each handled by a solver to minimum conical hull problem. Secondly, there exists an ultrafast anchoring algorithm for subproblem on 2D plane, which only picks two anchor points based on their angles to an axis without iterative optimization or greedy pursuit. This results in a significantly efficient DCA algorithm that can be solely used, or embedded as a subroutine checking if a point is covered in a conical hull."}, {"heading": "4.2 Distributing Conical Hull Problem to Sub-problems in Low Dimensions", "text": "Due to the convexity of cone, a low-D projection of a conical hull is still a conical hull that covers the projections of the same points covered in the original conical hull, and generated by the projections of a subset of anchors on the extreme rays of the original conical hull. Lemma 3. For arbitrary point x \u2208 cone(YA) \u2282 Rp, where A is the index set of the k anchors (generators) selected from Y , for any \u03a6 \u2208 Rp\u00d7d with d \u2264 p, we have\n\u2203A\u0303 \u2286 A : x\u03a6 \u2208 cone(YA\u0303\u03a6), (28) Since merely a subset of A remains as anchors after projection, solving a minimum conical hull problem on a single low-D hyperplane rarely returns all the anchors in A. However, the whole set A can be recovered from the anchors detected on multiple low-D hyperplanes. By sampling the projection matrix \u03a6 from a random ensemble M, it can be proved that w.h.p. solving only s = O(ck log k) sub-problems are sufficient to find all anchors in A. This is the bound for the worst case, i.e., the constant c only changes with the probability that the most \u201cflat\u201d anchor on the conical hull surface is still an anchor on the low-D hyperplane after projection. However, it is interesting that when the anchor is too \u201cflat\u201d, it is very close to the face spanned by its adjacent anchors, and thus the failure of detecting it still leads to a sufficiently similar conical hull. In other words, this property indicates robustness to losing unimportant anchors.\nAlgorithm 1 DCA(X,Y, k,M) Input: Two sets of points (rows) X \u2208 Rn\u00d7p and Y \u2208 Rm\u00d7p in matrix forms (ref. Table 1 to see X and Y for different models), number of latent factors/variables k, random matrix ensemble M; Output: Anchor set A \u2286 [m] such that \u2200i \u2208 [n], Xi \u2208 cone(YA); Divide Step (in parallel): for i = 1\u2192 s := O(k log k) do\nRandomly draw a matrix \u03a6 \u2208 Rp\u00d7d from M; Solve sub-problem such as A\u0303t = MCH(X\u03a6, Y \u03a6) by any solver, e.g., (37);\nend for Conquer Step: \u2200i \u2208 [m], compute g\u0302(Yi) = (1/s) \u2211s t=1 1A\u0303t(Yi); Return A as index set of the k points with the largest g\u0302(Yi).\nFor the special case of NMF when X = FXA, the above result was proved in [39]. However, the analysis cannot be trivially extended to the general conical hull problem when X = FYA (see Left plot of Figure 1). A critical reason is that the converse of Lemma 3 does not hold: the uniqueness of the anchor set A\u0303 on low-D hyperplane could be violated, because non-anchors in Y may have non-zero probability to be projected as low-D anchors. Fortunately, we can achieve a unique A\u0303 by defining a \u201cminimal conical hull\u201d on a low-D hyperplane. Then Proposition 1 reveals when w.h.p such A\u0303 is a subset of A.\nDefinition 8 (Minimal conical hull). Given two sets of points(rows) X and Y , the conical hull spanned by anchors (generators) YA is the minimal conical hull covering all points in X iff\n\u2200{i, j, s} \u2208 { i, j, s | i \u2208 AC = [m] \\A, j \u2208 A, s \u2208 [n], Xs \u2208 cone(YA) \u2229 cone(Yi\u222a(A\\j)) } (29)\nwe have X\u0302sYi > X\u0302sYj , where x\u0302y denotes the angle between two vectors x and y. The solution of minimal conical hull is denoted by A = MCH(X,Y ).\nIt is easy to verify that the minimal conical hull is unique, and the general minimum conical hull problem X = FYA under general separability assumption (which leads to the identifiability of A) is a special case of A = MCH(X,Y ). In DCA, on each low-D hyperplane Hi, the associated sub-problem aims to find the anchor set A\u0303i = MCH(X\u03a6i, Y \u03a6i). The following proposition gives the probability of A\u0303i \u2286 A in a sub-problem solution. Proposition 1 (Probability of success in sub-problem). As defined in the right plot of Figure 1, Ai \u2208 A signifies an anchor point in YA, Ci \u2208 X signifies a point in X \u2208 Rn\u00d7p, Bi \u2208 AC signifies a non-anchor point in Y \u2208 Rm\u00d7p, the green ellipse marks the intersection hyperplane between cone(YA) and the unit sphere Sp\u22121, the superscript \u00b7\u2032 denotes the projection of a point on the intersection hyperplane. Define d-dim (d \u2264 p) hyperplanes {Hi}i\u2208[4] such that A\u20323A\u20322 \u22a5 H1, A \u2032 1A \u2032 2 \u22a5 H2, B\u20321A\u20322 \u22a5 H3, B\u20321C \u20321 \u22a5 H4, let \u03b1 = H\u03021H2 be the angle between hyperplanes\nH1 and H2, \u03b2 = H\u03023H4 be the angle between H3 and H4. If H with associated projection matrix \u03a6 \u2208 Rp\u00d7d is a d-dim hyperplane uniformly drawn from the Grassmannian manifold Gr(d, p), and A\u0303 = MCH(X\u03a6, Y \u03a6) is the solution of minimal conical hull problem MCH(X\u03a6, Y \u03a6), we have\nPr(B1 \u2208 A\u0303) = \u03b2\n2\u03c0 ,Pr(A2 \u2208 A\u0303) = \u03b1\u2212 \u03b2 2\u03c0 . (30)\nProof. This proposition can be immediately proved by using the right plot of Figure 1. When rotating H from H1 to H2 by angle \u03b1, A2 will be identified as an anchor point of the minimal conical hull, except when H is between H4 and H3, in which region the non-anchor point B1 will be identified as an anchor point. Since the probability of being anchor point is proportional to the corresponding angle, we have Pr(B1 \u2208 A\u0303) = \u03b2/(2\u03c0) and Pr(A2 \u2208 A\u0303) = (\u03b1\u2212 \u03b2)/(2\u03c0).\nIt can be further verified that the angles \u03b1 and \u03b2 can be computed from the data points shown in the plot such that\n\u03b1 = arccos ( (A\u20322 \u2212A\u20321)T (A\u20322 \u2212A\u20323) \u2016A\u20322 \u2212A\u20321\u2016\u2016A\u20322 \u2212A\u20323\u2016 ) , \u03b2 = arccos ( (B\u20321 \u2212A\u20322)T (B\u20321 \u2212 C \u20321) \u2016B\u20321 \u2212A\u20322\u2016\u2016B\u20321 \u2212 C \u20321\u2016 ) . (31)\nThus \u03b1 and \u03b2 can be computed as constants for specific {A1, A2, A3, B1, C1}. Remarks:\nIt is obvious that a large Pr(A2 \u2208 A\u0303)\u2212 Pr(B1 \u2208 A\u0303) = (\u03b1\u2212 2\u03b2)/(2\u03c0) leads to a large probability for the success of a sub-problem in recovering a subset of A, i.e., A\u0303 \u2286 A. The robustness to unimportant \u201cflat\u201d anchors still holds for this general model. When increasing an interior angle associated to vertex A\u20322 of the polygon on the green intersection hyperplane, A2 turns to be a \u201cflat\u201d anchor, and angle \u03b1 in the right plot of Figure 1 will become small, so does the probability of detecting A2 as anchor on a low-D hyperplane. The above proof also indicates robustness to data noise. When a non-anchor point\u2019s projection on the intersection hyperplane B\u2032i is close to the polygon defined by all vertexes A\u2032i, the probability of Bi to be falsely identified as a anchor in a sub-problem will increase. But since Bi is close to the true conical hull, such false non-anchor point still leads to a good approximation of the true conical hull. Therefore, even our analysis aims at precisely recovery of A under noiseless assumption of data points, failure in recovering A can still provide a good approximate of cone(YA).\nAlthough Proposition 1 generally assumes that \u03a6 is uniformly drawn from a Grassmannian manifold, empirically we can sample \u03a6 from a rich class of random matrix ensembles M, e.g., Gaussian random matrix ensemble, the ensemble composed of standard unit vectors ei (i.e., random feature selection) or real data vectors, and various sparse random matrix ensemble, which can bring evident acceleration to projection X\u03a6 and Y \u03a6.\nAccording to Proposition 1, when \u03b1 > 2\u03b2 for all tuples {A1, A2, A3, B1, C1}, the k points in Y with the largest Pr(i \u2208 A\u0303) compose the unique true anchor set A. Since Pr(i \u2208 A\u0303) cannot be exactly known, DCA compares its unbiased estimator g\u0302(Yi) = (1/s) \u2211s t=1 1A\u0303t(Yi) of all points Yi in Y , where the number of sub-problems s is the sample size of the estimator. Therefore, by using Chernoff bound, we can obtain the probability bound for the success of DCA in finding the true A. Theorem 3 (Probability bound). Following the same notations in Proposition 1, suppose p\u2217\u2217 = min{A1,A2,A3,B1,C1}(\u03b1 \u2212 2\u03b2) \u2265 c/k > 0. It holds with probability at least 1\u2212 k exp ( \u2212 cs3k ) that DCA successfully identifies all the k anchor points in A, where s is the number of sub-problems solved in DCA.\nProof. We introduce two binary random variables \u03beti = 1A\u0303t(Ai) and \u03ba t j = 1A\u0303t(Bj) indicating whether Ai \u2208 A\u0303t and Bj \u2208 A\u0303t, respectively. According to Proposition 1, we have E(\u03beit) = Pr(Ai \u2208 A\u0303t) = \u03b1\u2212 \u03b2,E(\u03ba j t ) = Pr(Bj \u2208 A\u0303t) = \u03b2. (32)\nDCA compares g\u0302(Ai) and g\u0302(Bj), i.e.,\ng\u0302(Ai) = 1\ns s\u2211 t=1 \u03beti , g\u0302(Bj) = 1 s s\u2211 t=1 \u03batj . (33)\nThe true anchor Ai is identified as an anchor by DCA in the conquer step iff g\u0302(Ai) > max Bj g\u0302(Bj). By applying Chernoff bound to random variable g\u0302(Ai) \u2212 g\u0302(Bj) (randomness is due to random hyperplane H), for any \u03b4 \u2208 [0, 1], Ai and Bj , we have\nPr ( s\u2211 t=1 (\u03beti \u2212 \u03batj) < (1\u2212 \u03b4)sp\u2217i ) \u2264 exp ( \u2212\u03b4 2sp\u2217i 2 + \u03b4 ) , (34)\nwhere p\u2217i = min Bj (\u03b1\u2212 2\u03b2).\nLet \u03b4 = 1 and f(Ai) = g\u0302(Ai)\u2212max Bj g\u0302(Bj), by (33), we have Pr(f(Ai) = 0) \u2264 exp ( \u2212sp \u2217 i\n3\n) . (35)\nThis yields\nPr( min Ai\u2208A\nf(Ai) > 0) = 1\u2212 Pr(\u222aAi\u2208Af(Ai) = 0) \u2265 1\u2212 \u2211 Ai\u2208A Pr(f(Ai) = 0)\n\u2265 1\u2212 \u2211 Ai\u2208A exp ( \u2212 sp \u2217 i 3 ) \u2265 1\u2212 k exp ( \u2212 sp \u2217\u2217 3 ) .\n(36)\nSince p\u2217\u2217 = minAi p \u2217 i \u2265 c/k, this completes the proof.\nGiven Theorem 3, we can immediately achieve the following corollary about the number of subproblems that guarantee success of DCA in finding A. Corollary 1 (Number of sub-problems). With probability 1 \u2212 \u03b4, DCA can correctly recover the anchor set A by solving \u2126( 3kc log k \u03b4 ) sub-problems.\nWhen k p, i.e., the number of latent variables/factors are much less than the features or dimension of data, which is common in many learning models, DCA can learn the model parameters by solving an extremely small number of sub-problems in parallel, no matter what solver chosen for sub-problem. Thus it provides a significantly efficient learning scheme.\nRemarks:\nIt is worth noting that although DCA uses random projection to reduce the problem size, it is different from the random projection methods based on Johnson-Lindenstrauss (JL) Lemma [22] or its variants. Because DCA allows to project the data into extremely low-D subspace in which JL lemma does not hold, but solving sub-problems on multiple times of such random projections can still recover the true solution w.h.p. In contrast, the JL Lemma based methods have to project the data into a single yet much higher dimensional subspace to gain high probability in recovering the original solution. The idea of divide-and-conquer randomization in DCA is more preferred in developing randomized algorithm, because 1) the complexity of solving a sub-problem is usually super-linear in data dimension; and 2) parallelizing the sub-problems in DCA gives further speedup."}, {"heading": "4.3 Anchoring on 2D Plane", "text": "DCA provides a fast unified distributed learning scheme that can invoke any minimal conical hull problem solver as subroutine to solve the sub-problems. Although there exists several solvers for X = FXA for NMF, most of them depend on expensive iterative algorithms derived from optimization or greedy pursuit. Moreover, there is rarely known algorithm addressing the general model X = FYA. Although DCA can invoke any solver for the sub-problem on any low-D hyperplane, an ultrafast solver for the 2D sub-problem always shows high accuracy in locating anchors when embedded into DCA. Its motivation comes from the geometry of conical hull on a 2D plane, which is a special case of a d-dim hyperplane H in the sub-problem of DCA. It leads to a non-iterative algorithm for A = MCH(X,Y ) on the 2D plane. It only requires computing n+m cosine values, finding the min/max of the n values, and comparing the remaining m ones with the min/max value.\nAccording to the left plot of Figure 1, the two anchors YA\u0303\u03a6 on a 2D plane have the min/max (among points in Y \u03a6 ) angle (to either axis) that is larger/smaller than all angles of points in X\u03a6, respectively. This leads to the following closed form of A\u0303.\nA\u0303 = {arg min i\u2208[m] ( \u0302(Yi\u03a6)\u03d5\u2212max j\u2208[n] \u0302(Xj\u03a6)\u03d5)+, arg min i\u2208[m] (min j\u2208[n] \u0302(Xj\u03a6)\u03d5\u2212 \u0302(Yi\u03a6)\u03d5)+}, (37)\nwhere (x)+ = x if x \u2265 0 and\u221e otherwise, and \u03d5 can be either the vertical or horizontal axis on a 2D plane. By plugging (37) in DCA as the solver for s sub-problems on random 2D planes, we can obtain an extremely fast learning algorithm.\nNote for the special case when X = Y , (37) degenerates to finding the two points in X\u03a6 with the smallest and largest angles to an axis \u03d5, i.e., A\u0303 = {arg mini\u2208[n] \u0302(Xi\u03a6)\u03d5, arg maxi\u2208[n] \u0302(Xi\u03a6)\u03d5}. This is used in matrix factorization and the latent variable model with nonzero off-diagonal D."}, {"heading": "4.4 DCA as Subroutine of Other Methods", "text": "Within lots of algorithms finding conical hull or other problems, testing whether a point Xi from X is covered in the (minimal) conical hull of Y , or equivalently, if Xi \u2208 cone(Y ), dominates the\ncomputation per step. In previous works, the testing needs to compute the conical combination coefficients by solving a linear programming. DCA provides a much faster off-the-shelf subroutine which can be easily invoked by other methods [5] to gain a significant acceleration.\nIn particular, the tth sub-problem in DCA turns to test if the 2D projection Xi\u03a6 is covered in cone(Y \u03a6), this requires to compute\nti = 1(Xi\u03a6 < minY \u03a6) + 1(Xi\u03a6 > maxY \u03a6). (38)\nThe conquer step in DCA becomes{ Xi \u2208 cone(Y ), if \u2211s t=1 t i = 0\nXi 6\u2208 cone(Y ), if \u2211s t=1 t i > 0,\n(39)"}, {"heading": "4.5 Examples", "text": "In this section, we will present five examples of using general conical hull problem in \u00a7 3 and DCA in Algorithm 1 to develop scalable novel learning algorithms for five popular latent variable and matrix factorization models. Although finding the anchor set A plays a major role in all algorithms, in practice each one also needs extra preprocessing/post-processing steps, which will be highlighted in the following."}, {"heading": "4.5.1 DCA for Multi-view Mixture Model", "text": "Given X and Y in (18) or (19) from \u00a7 3.4, applying DCA(X,Y, k,M) in Algorithm 1 to the X and Y in GMM is able to find out the anchor set A w.h.p., and thus O\u03021 = XA,1, O\u03022 = XA,2, and O\u03023 = XA,3. Therefore, DCA learns GMM by assigning k real data instances to the mean vectors of the k components. This is reasonable because we can usually find a real data instance sufficiently close to the true mean in each cluster when n is large enough. This results in a more interpretable GMM because the centroid of each cluster is no longer an artificial averaging, but a representative real data instance."}, {"heading": "4.5.2 DCA for Hidden Markov Model", "text": "GivenX and Y in (18) or (19) from \u00a7 3.4, and following notations in \u00a7 3.5, after obtaining O\u0302 = XA,2, O\u0302T = XA,3 by running A =DCA(X,Y, k,M), we can immediately recover transition matrix T by solving linear equation OT = XA,3 with simplex constraints to the columns of T . Since T is small k \u00d7 k matrix, there are lots of standard solvers that can quickly attain T ."}, {"heading": "4.5.3 DCA for Latent Dirichlet Allocation", "text": "Given X in (27) from \u00a7 3.7, after running A = DCA(X,X, k,M), solving the system of linear equations X = FXA under constraints \u2200i \u2208 [k], FAi,Ai = 1, FAi,t\u2208[k]\\Ai = 0 and nonnegativity Fi,j \u2265 0 gives F . Since each column of O is on a (topic) probability simplex \u2206p\u22121, column-wise normalization of F gives us the estimate to O, i.e., \u2200i \u2208 [k], O\u0302i = Fi/\u2016Fi\u20161. Then \u03b1 can be recovered from E(h\u2297h), i.e., the covariance of Dir(\u03b1) that is achieved by solving (26) given O\u0302.\nRecently, we surprisingly discover that the above procedure equals to a Bayes learning algorithm proposed in [4], whose major idea is to solve an NMF under simplex constraint by a greedy pursuit typed algorithm. Thus it can be seen as a special case of our conical hull model.\nComparing to the specific greedy algorithm developed for LDA in [4], our method provides a unified scheme that can reduce more general models besides LDA to a conical hull problem, and the proposed DCA leads to a significantly efficient algorithm. In addition, DCA has much faster speed and easier implementation. This is because 1) Limited by the simplex constraint in NMF, the Bayes learning method decomposes the row-normalized X in (27), thus it needs to additionally compute p(w2 = j|w1 = i) and p(w1 = i) for normalization, and \u2211 i p(z = et|x = ei)p(x = ei) after NMF; and 2)The greedy algorithm in [4] finds the anchors of convex hull is slower than DCA using parallel and randomized strategy.\nThe equivalence between [4] and our method can be established by the following theorem. In order to make the comparison clear, we map all the notations in our method to those used in [4] such that Q \u2190 E(x1 \u2297 x2), V \u2190 A, A \u2190 OT , vt denotes the index of the anchor word for topic t, A\u0304t,i \u2190 At,i/At,vt , wi = j \u21d4 xi = ej , zi = j \u21d4 zi = ej . Then we use the notations in [4] throughout the theorem and its proof below. Theorem 4. Solving the conical hull problem Q = A\u0304TQV proposed in this paper for LDA equals to Bayes learning of p(z1 = t|w1 = i) by NMF proposed in [4], i.e.,\nQi,j = k\u2211 t=1 A\u0304t,iQvt,j \u21d4  p(w2 = j|w1 = i) = k\u2211 t=1 p(z1 = t|w1 = i) \u00b7 p(w2 = j|z1 = t),\np(w1 = i|z1 = t) = p(z1=t|w1=i)p(w1=i)\u2211 i \u2032 p(z1=t|w1=i\u2032 )p(w1=i\u2032 ) .\n(40)\nProof. From the perspective of Bayes learning, the basic law of conditional probability gives us\np(w2 = j|w1 = i) = k\u2211 t=1 p(z1 = t|w1 = i)p(w2 = j|z1 = t). (41)\nSince At,i = p(z1 = t|w1 = i), given p(w2 = j|w1 = i) and p(w2 = j|z1 = t), solving the linear equations (41) over all {i, j} pairs gives the unique solution of A when k \u2264 p. To relate this Bayes learning to our conical hull problem with form X = FXA, multiplying both sides of (41) by p(w1 = i) yields\np(w2 = j|w1 = i)p(w1 = i) = k\u2211 t=1 p(z1 = t|w1 = i)p(w1 = i) p(w1 = vt) \u00b7 p(w2 = j|z1 = t)p(w1 = vt).\n(42) The definition of anchor word p(z1 = t|w1 = vt) = 1 leads to\np(w2 = j|w1 = vt) = p(w2 = j|z1 = t)p(z1 = t|w1 = vt) = p(w2 = j|z1 = t), (43)\nso the equation in (42) equals to\np(w1 = i, w2 = j) = k\u2211 t=1 p(z1=t|w1=i)p(w1=i) p(w1=vt) \u00b7 p(w2 = j|w1 = vt)p(w1 = vt)\n= k\u2211 t=1 p(z1=t|w1=i)p(w1=i) p(z1=t) \u00b7 p(z1=t)p(w1=vt) \u00b7 p(w1 = vt, w2 = j)\n= k\u2211 t=1 p(w1=i|z1=t) p(w1=vt|z1=t) \u00b7 p(w1 = vt, w2 = j).\n(44)\nThe last equality is due to Bayes\u2019 rule and the definition of anchor word p(z1 = t|w1 = vt) = 1, i.e., p(z1=t|w1=i)p(w1=i)\np(z1=t) = p(w1 = i|z1 = t),\np(z1=t) p(w1=vt) = p(z1=t)p(z1=t|w1=vt)p(w1=vt) = 1 p(w1=vt|z1=t) . (45)\nSubstitute Qi,j = p(w1 = i, w2 = j) and A\u0304t,i = At,i/At,vt = p(w1 = i|z1 = t)/p(w1 = vt|z1 = t) into the above equations (44), we achieve the model in the same form as conical hull problem\nQi,j = k\u2211 t=1 At,i At,vt \u00b7Qvt,j . (46)\nSince A can be uniquely recovered as row-normalized A\u0304, and the above reasoning is reversible (due to all the equalities), the equivalence between conical hull problem and the Bayes learning [4] given in (40) holds."}, {"heading": "4.5.4 DCA for Non-negative Matrix Factorization", "text": "The conical hull problem for NMF uses model X = FXA. DCA for NMF has been proposed in [39]."}, {"heading": "4.5.5 DCA for Subspace Clustering", "text": "Comparing to existing SC algorithms relying on expensive lasso-type optimizations and spectral clustering requiring SVD, a slightly modified DCA is able to provide a significantly more efficient algorithm. In particular, we change each sub-problem of A = DCA(X,X,K = \u2211k i=1 ki,M) to a separable SC of X\u03a6 on a low-D hyperplane, which can be solved by any available separable SC solver. A simple but effective one is to sample \u03a6 \u2208 Rp\u00d72, project X to an 2D plane, apply mean shift clustering algorithm [13] to the n-array of angles \u0302(Xi\u03a6)\u03d5, and add to A\u0303 the two points with the maximal and minimal angle in each cluster. The reason for using mean shift is 1) it is fast and provides highly reliable clustering result on 1D values; and 2) the number of clusters can be automatically determined in it.\nFollowing all notations in \u00a7 3.2, after obtaining A, we have to extract the anchors Ai in A for different cluster i. We use the fact that two anchors in the same cluster must keep lying in the same cluster on any low-D hyperplane (but the converse does not hold). Thus we can build a graph Laplacian from similarity matrixG \u2208 RK\u00d7K such thatGi,j = #(XA(i)\u03a6 andXA(j)\u03a6 in the same cluster), and spectral clustering [31] is able to give us the k clusters of anchors {Ai}i\u2208[k]."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 DCA for Non-negative Matrix Factorization on Synthetic Data", "text": "The experimental comparison results are shown in Figure 2. Greedy algorithms SPA, XRAY and SFO achieves the best accuracy and smallest recovery error when the noise level is above 0.2, but XRAY and SFO are the slowest two. SPA is slightly faster but still much slower than DCA. DCA with different number of sub-problems shows slightly less accuracy and larger error than greedy algorithms, but the difference is acceptable. Considering its significant acceleration, DCA offers an advantageous trade-off. LP-test [5] has the exact solution guarantee, but it is not robust to noise, and too slow in speed. Therefore, DCA provides a much faster and more practical NMF algorithm with comparable performance to the best ones."}, {"heading": "5.2 DCA for Gaussian Mixture Models on Synthetic Dataset", "text": "We thoroughly evaluate DCA-GMM on synthetic data generated with different variance level and noise level. The higher of these two levels, the harder the clustering task is. The results are reported in Figure 3, the detailed procedure generating data and evaluation metrics are given in the caption. On all metrics, DCA-GMM shows a phase transition property, i.e., the algorithm will overwhelmingly success below a curve of noise and variance level. This property verifies the robustness of DCA-GMM to data noise and variance within a cluster. In addition, when increasing the number of sub-problems (layers from bottom to top), the accuracy of DCA-GMM soon saturates on a value close to 1, this indicates that a small number of sub-problems in DCA-GMM is sufficient to produce a promising clustering result, which is highly preferred in practice. Moreover, the time cost of DCA-GMM is significantly small and thus exhibits its competitive efficiency. Furthermore, error is more robust to data noise than accuracy, because most false anchors detected in the noise case are close to the true ones, which leads to small error.\n5.3 DCA for Gaussian Mixture Model on Image Dataset\n.\nThe experimental comparison results are shown in Figure 4. DCA consistently outperforms other methods on accuracy on lots of datasets, and shows 20 \u2212 2000 times of acceleration in speed. By increasing the number of sub-problems, the accuracy of DCA improves. Note the pixels of face/handwritten digit/object images always exceed 1000, and thus results in slow computation of pairwise distances required by other clustering methods. DCA exhibits the fastest speed because the number of sub-problems s = O(k log k) does not depend on the feature dimension, and thus merely 171 2D random projections are sufficient for obtaining a promising clustering result. Spectral method performs poorer than DCA due to the large variance of sample moment. Because DCA uses the\nseparability assumption as regularization in estimating the eigenspace of the moment, the variance is reduced."}, {"heading": "5.4 DCA for Hidden Markov Model on Stock Price and Motion Capture Data", "text": "The experimental comparison results for stock price modeling and motion segmentation are shown in Figure 5 and Figure 6, respectively. In the former one, DCA always achieves slightly lower but comparable likelihood compared to Baum-Welch (EM) method, while spectral method performs worse and unstably. DCA shows significant speed advantage compared to other methods, and thus is more preferable in practice.\nIn the latter one, we evaluate the likelihood and prediction accuracy on both the training and the test set, so the regularization caused by separability assumption leads to the highest test accuracy and fastest speed of DCA. Note that the time cost of Baum-Welch method does not keeping increasing with the number of training samples in a constant speed. This is because the method uses an adaptive stop criterion, i.e., stop the optimization when the likelihood on the training data increases too slow.\nSince we cannot randomly select observations in a sequence for training due to the sequential property of the data, and due to the randomness in DCA, It is normal that the accuracy curve is not smooth. However, it is not hard to see that DCA usually achieves the highest accuracy given different number of training observations."}, {"heading": "5.5 DCA for Latent Dirichlet Allocation on Text Dataset", "text": "The experimental comparison results for topic modeling are shown in Figure 7. Compared to both traditional EM and the sampling method, DCA not only achieves both the smallest perplexity (highest likelihood) on the test set and the highest speed, but also the most stable performance when increasing the number of topics. In addition, the \u201canchor word\u201d achieved by DCA provides more interpretable topics than other methods."}, {"heading": "5.6 DCA for Subspace Clustering on Synthetic Dataset", "text": "We thoroughly evaluate DCA-subspace clustering on synthetic data generated with different noise level and maximum span angle, which is the maximum angle between two anchors of two conical hulls. Hence, large maximum span angle leads to a hard clustering problem. The results are reported in Figure 8, the detailed procedure generating data and evaluation metrics are given in the caption. On all metrics, DCA-subspace clustering shows a phase transition property, i.e., the algorithm will overwhelmingly success below a curve of noise level and maximum span angle. This property verifies the robustness of DCA-subspace clustering to data noise and overlapping between cluster. In addition, when increasing the number of sub-problems (layers from bottom to top), the precision/recall of DCA-subspace clustering soon saturates on a value close to 1, this indicates that a small number of sub-problems is sufficient to produce a promising clustering result, which is highly preferred in practice. Moreover, the time cost of DCA-subspace clustering is significantly small and thus exhibits its competitive efficiency. Furthermore, error is more robust to data noise than precision/recall, because most false anchors detected in the noise case are close to the true ones, which leads to small error.\nIt is worth noting that the same metric for all anchors and for cluster sensitive case shows different behaviors in the region of \u201clarge maximum span angle, low noise level\u201d. In particular, the precision/recall in cluster sensitive case decreases in this region, because although small noise level improves the probability of successfully identifying the anchors, the overlapping between cones caused by the large maximum span angle will lead to wrong assignment of anchors to clusters."}, {"heading": "5.7 DCA for Subspace Clustering on Image and Motion Capture Dataset", "text": "The experimental comparison results for subspace clustering on object image dataset COIL-100 are shown in Figure 9. DCA provides a much more practical algorithm in speed that can achieve comparable mutual information but more than 1000 times speedup than the state-of-the-art SC algorithms [11, 16, 28, 36].\nWe also apply DCA-subspace clustering to a sequence of motion capture data that cannot be analyzed by existing subspace clustering methods due to their high computational complexity. DCA-subspace clustering aims to find several anchor frames for each cluster such that they can reconstruct most of the frames in the same cluster as their conical combinations. According to the results exhibited in Figure 9, the anchor frames in each of the 8 detected clusters summarize one kind of motion on critical positions. So DCA provides a more interpretable subspace clustering results than other methods which usually define each cluster by several artificial bases. In addition, the reconstruction error for each cluster is small, and indicates that the selected anchor frames in each cluster are expressive and successfully summarize the associated motion. It also is worth noting that DCA-subspace clustering\n10 20 30 40 50 60 70 80 90 100 103\n104\n105\nNumber of Topics\nPe rp\nle xi\nty\nDCA LDA(s=678) DCA LDA(s=2034) DCA LDA(s=5424) DCA LDA(s=9493) EM variational Gibbs sampling Spectral method\n10 20 30 40 50 60 70 80 90 100 100\n101\n102\n103\n104\nNumber of Topics\nC PU\ns ec\non ds\nDCA LDA(s=678) DCA LDA(s=2034) DCA LDA(s=5424) DCA LDA(s=9493) EM variational Gibbs sampling Spectral method\n5 13 22 30 38 47 55 63 72 80 2000\n2200\n2400\n2600\n2800\n3000\n3200\n3400\n3600\n3800\nNumber of Topics\nPe rp\nle xi\nty\nDCA LDA(s=801) DCA LDA(s=2001) DCA LDA(s=3336) DCA LDA(s=5070) EM variational Gibbs sampling Spectral method\n5 13 22 30 38 47 55 63 72 80 10\u22121\n100\n101\n102\n103\n104\nNumber of Topics\nC PU\ns ec\non ds\nDCA LDA(s=801) DCA LDA(s=2001) DCA LDA(s=3336) DCA LDA(s=5070) EM variational Gibbs sampling Spectral method\n10 20 30 40 50 60 70 80 90 100 1000\n1500\n2000\n2500\n3000\n3500\nNumber of Topics\nPe rp\nle xi\nty\nDCA LDA(SPA) DCA LDA(XRAY) DCA LDA(SFO) DCA LDA(LP\u2212test) DCA LDA(s=358) EM variational Gibbs sampling Spectral method\n10 20 30 40 50 60 70 80 90 100 10\u22122\n10\u22121\n100\n101\n102\n103\nNumber of Topics\nC PU\ns ec\non ds\nCone LDA(SPA) Cone LDA(XRAY) Cone LDA(SFO) Cone LDA(LP\u2212test) DCA LDA(s=358) EM variational Gibbs sampling Spectral method\n10 20 30 40 50 60 70 80 90 100 2000\n3000\n4000\n5000\n6000\n7000\n8000\n9000\n10000\n11000\nNumber of Topics\nPe rp\nle xi\nty\nDCA LDA(s=31) DCA LDA(s=76) DCA LDA(s=153) DCA LDA(s=275) EM variational Gibbs sampling Spectral method\n10 20 30 40 50 60 70 80 90 100 10\u22121\n100\n101\n102\n103\n104\n105\nNumber of Topics\nC PU\ns ec\non ds\nDCA LDA(s=31) DCA LDA(s=76) DCA LDA(s=153) DCA LDA(s=275) EM variational Gibbs sampling Spectral method\nFigure 8: DCA-subspace clustering on synthetic data. On a 30 \u00d7 30 grid of different noise level and maximum span angle between anchors defining different cones, for each pair, we randomly generate data of k = 4 clusters (cones) with 500 samples of dimension 300 and 10 extreme rays (anchors) per cluster. In particular, the conical combination coefficients of each point (corresponding to the 10 anchors) in each cluster are drawn from a uniform distribution between 0 and 1, and then Gaussian noises of magnitude equal to the noise level are added to the points. We run DCA-subspace clustering using different number of sub-problems, and report their performance by precision, recall and error of the total 40 anchors (ignore the wrong assignment of true anchor to a wrong cluster), and those metrics sensitive to clusters. The CPU seconds are reported too. Each point on each 3D plot is a result of averaging 10 random trials in the same setting, and its height is the value of metric. Each 3D plot includes layers of surfaces associated with different number of sub-problems, we also report the top layer as a 2D plot below each 3D plot.\nonly costs 2.7s to obtain the results, which is much less than the time costs of most the state-of-the-art approaches."}, {"heading": "6 Conclusion", "text": "In this paper, we propose a general scheme that can reduce the parameter learning for a broad class of models, e.g., matrix factorization and latent variable model, to a geometric problem that aims to find a limited number of extreme rays so called \u201canchors\u201d of a conical hull from a finite set of real data points. Compared to EM and sampling, which are the dominating parameter learning methods nowadays, our approach avoids alternating updating between parameter and latent variables, does not require iterative procedures, and provide a global solution guarantee based on the identifiability of the anchor set. By generalizing the separability assumption X = FXA for unique NMF to a more adaptive case X = FYA, we propose a general minimum conical hull problem to formulate the reduced problem, and give rigorous theoretical analysis to the identifiability and uniqueness of its solution, as well as its interesting connections to other problems such as submodular set cover problem. As examples, we give the details of how to reduce learning NMF, subspace clustering, GMM, HMM, Kalman filter and LDA to the general minimum conical hull problem.\nWe further show that a novel idea of divide-and-conquer randomization leads to a significant efficient algorithm scheme for general minimum conical hull problem. In this \u201cdivide-and-conquer anchoring (DCA)\u201d scheme, the original anchor finding task is distributed to multiple same-type sub-problems, each of which aims to find anchors (of the minimal conical hull) on a low-dimensional hyperplane, where the random projections of all data points lie in. Different from other randomized algorithms, each sub-problem in DCA only guarantees to recover a subset of anchors. This weaker requirement allows us to project the data points into extremely low-D hyperplane. But due to randomness, combining the anchors found in all sub-problems gives an accurate estimate of the true anchor set w.h.p.. Rigorous analysis shows that the number of sub-problem to achieve such probabilistic guarantee is merelyO(k log k), where k is the number of true anchors. Since we can apply any solver to the sub-problem, DCA provides a unified scheme solving general minimum conical hull problem. In addition, since most existing solvers have super-linear time complexity w.r.t. the data dimension, the algorithm generated by DCA invoking a solver is much faster than the solver itself. Furthermore, we show that DCA can be also used as a subroutine in other methods to provide an usually faster test checking if a point is covered by a conical hull or not.\nIn the special case when the hyperplane in each sub-problem is a 2D plane, we develop an ultrafast solver that precisely identifies the two 2D anchors by only computing an array of cosine values and finding the max/min values in a sub-array. Compared to the solvers relying on iterative optimization or sampling, our solver is simpler in implementation and faster in speed. Therefore, plugging it into DCA scheme produces a significantly effective DCA algorithm, which is later applied to all examples of learning algorithm design for specific models.\nComprehensive experiments on rich datasets and thorough comparison to the state-of-the-art algorithms for different learning tasks promisingly justify the significant improvement in speed and robustness brought by our approach. In particular, DCA algorithms for different specific models usually show tens to thousands times acceleration, and better generalization performance on test sets. Moreover, the anchors selected from real data points often provide more interpretable models and convincing explanations, which are preferred in real applications."}, {"heading": "6.1 Future Works and Discussions", "text": "Although we present both the general minimum conical hull problem formulation for general learning models, and the unified scheme of DCA for solving the problem with detailed examples on popular specific models, there are several interesting and important potential extended topics of our method.\n\u2022 In order to break the linearity assumption E(x|h) = hTOT and generalize distribution p(x|h) to even non-parametric forms, we can consider to embed the joint distribution of {xi}i\u2208[3] into a reproducing kernel Hilbert space (RKHS). Hence we can instead assume E(x|h) = f(h), where function f(\u00b7) is an point in RKHS. Accordingly, the moments of finite feature vectors in (15) becomes moment operators of feature functions (or infinite feature vectors). By using the kernel trick introduced by reproducing property, the learning problem can be solved in n-D space in the same way as methods proposed in this paper. Note the same trick can be used to model and solve kernel matrix factorization in functional space too.\n\u2022 In higher order graphical models (e.g., n-gram models, higher order MRF and CRF) engaging more complicated structures, we usually parameterize the models with higher order (conditional) moments of p(x|h), rather than conditional mean. By using the trick replacing xi in (15) with vec(xi\u2297n), the vectorization of the nth tensor power of xi, columns of O stores the conditional moments and can be recovered by using our approach in this paper.\n\u2022 In hierarchical graphical model (e.g., Bayesian networks) where x \u223c p(x|h1) and ht \u223c p(ht|ht\u22121), we can apply our method to each layer of the model in a bottom-up learning manner and learn the parameters for each layer sequentially by chain rule. A very related work [2] has shown this possibility for linear Bayesian networks. However, the estimation bias can be propagated throughout the learning process and leads to poor estimation of the parameters in higher layers. The general separability assumption in our method is capable to provide effective regularizations reducing the increasing bias.\n\u2022 The paper also provides an interesting potential solver to semi-definite programming (SDP) when the matrix variable X has a low-rank penalty/constraint, which is exactly the case for many popular machine learning models. The essential idea is to represent X by a weighted sum of k rank-one matrices [25], each of which is generated by a real data point. In optimization, we can either randomly select multiple rank-one matrices according to some probability, or select them in a greedy fashion according to certain score. In most situations, the probability or score is proportional to the probability of being anchors, and hence can be quickly obtained by solving a general minimum conical hull problem by DCA. This optimization approach is able to produce a more interpretable low-rank solution with faster speed.\nWe believe that inspired by the new insights of this paper in both problem formulation and algorithm design, the learning process of various machine learning models can be largely simplified and significantly accelerated. In addition, the learning results can become more convincing and explainable even for users outside machine learning community."}], "references": [{"title": "A spectral algorithm for latent dirichlet allocation", "author": ["A. Anandkumar", "D.P. Foster", "D. Hsu", "S. Kakade", "Y. Liu"], "venue": "NIPS,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "A method of moments for mixture models and hidden markov models", "author": ["A. Anandkumar", "D. Hsu", "S.M. Kakade"], "venue": "COLT,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Supplemental material", "author": ["A. Anonymous"], "venue": "Submitted to NIPS,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["S. Arora", "R. Ge", "Y. Halpern", "D.M. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "M. Zhu"], "venue": "ICML,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Computing a nonnegative matrix factorization - provably", "author": ["S. Arora", "R. Ge", "R. Kannan", "A. Moitra"], "venue": "STOC,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical inference for probabilistic functions of finite state Markov chains", "author": ["L.E. Baum", "T. Petrie"], "venue": "Annals of Mathematical Statistics, 37:1554\u20131563,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1966}, {"title": "Polynomial learning of distribution families", "author": ["M. Belkin", "K. Sinha"], "venue": "FOCS,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Maching Learning Research (JMLR), 3:993\u20131022,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational Mathematics, 9:717\u2013772,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Full reconstruction of markov models on evolutionary trees: Identifiability and consistency", "author": ["J.T. Chang"], "venue": "Mathematical Biosciences, 137(1):51\u201373,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Spectral curvature clustering (scc)", "author": ["G. Chen", "G. Lerman"], "venue": "International Journal of Computer Vision (IJCV), 81(3):317\u2013330,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Coherent matrix completion", "author": ["Y. Chen", "S. Bhojanapalli", "S. Sanghavi", "R. Ward"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Mean shift, mode seeking, and clustering", "author": ["Y. Cheng"], "venue": "IEEE Transactions on Pattern Analysis and Maching Intelligence (TPAMI), 17(8):790\u2013799,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1995}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society, Series B, 39(1):1\u201338,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1977}, {"title": "When does non-negative matrix factorization give a correct decomposition into parts", "author": ["D. Donoho", "V. Stodden"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Sparse subspace clustering", "author": ["E. Elhamifar", "R. Vidal"], "venue": "CVPR,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Submodular Functions and Optimization", "author": ["S. Fujishige"], "venue": "Elsevier,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 6(6):721\u2013741,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1984}, {"title": "Fast and robust recursive algorithmsfor separable nonnegative matrix factorization", "author": ["N. Gillis", "S.A. Vavasis"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 36(4):698\u2013714,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "A spectral algorithm for learning hidden markov models", "author": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "COLT,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Effective split-merge monte carlo methods for nonparametric models of sequential data", "author": ["M.C. Hughes", "E.B. Fox", "E.B. Sudderth"], "venue": "NIPS,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["W. Johnson", "J. Lindenstrauss"], "venue": "Conference in modern analysis and probability (New Haven, Conn., 1982), volume 26 of Contemporary Mathematics, pages 189\u2013206.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1984}, {"title": "Efficiently learning mixtures of two gaussians", "author": ["A.T. Kalai", "A. Moitra", "G. Valiant"], "venue": "STOC,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "The spectral method for general mixture models", "author": ["R. Kannan", "H. Salmasian", "S. Vempala"], "venue": "COLT,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Random conic pursuit for semidefinite programming", "author": ["A. Kleiner", "A. Rahimi", "M.I. Jordan"], "venue": "NIPS,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast conical hull algorithms for near-separable nonnegative matrix factorization", "author": ["A. Kumar", "V. Sindhwani", "P. Kambadur"], "venue": "ICML,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, 401:788\u2013791,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "Robust subspace segmentation by low-rank representation", "author": ["G. Liu", "Z. Lin", "Y. Yu"], "venue": "ICML,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Least squares quantization in pcm", "author": ["S. Lloyd"], "venue": "IEEE Transactions on Information Theory (TIT), 28(2):129\u2013 137,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1982}, {"title": "Lecture Notes: Introduction to Linear Optimization", "author": ["A. Nemirovski"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Andrew Y. Ng", "Michael I. Jordan", "Yair Weiss"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2001}, {"title": "Contributions to the mathematical theory of evolution", "author": ["K. Pearson"], "venue": "Philosophical Transactions of the Royal Society of London. A, 185:71\u2013110,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1894}, {"title": "Fast collapsed gibbs sampling for latent dirichlet allocation", "author": ["I. Porteous", "D. Newman", "A. Ihler", "A. Asuncion", "P. Smyth", "M. Welling"], "venue": "SIGKDD, pages 569\u2013577,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Mixture Densities, Maximum Likelihood and the Em Algorithm", "author": ["R.A. Redner", "H.F. Walker"], "venue": "SIAM Review, 26(2):195\u2013239,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1984}, {"title": "Probabilistic matrix factorization", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "NIPS,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust subspace clustering", "author": ["M. Soltanolkotabi", "E. Elhamifar", "E.J. Cand\u00e8s"], "venue": "arXiv:1301.2603,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Statistical Analysis of Finite Mixture Distributions", "author": ["D.M. Titterington", "A.F.M. Smith", "U.E. Makov"], "venue": "Wiley, New York,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1985}, {"title": "An analysis of the greedy algorithm for the submodular set covering problem", "author": ["Laurence A. Wolsey"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1982}, {"title": "Divide-and-conquer anchoring for near-separable nonnegative matrix factorization and completion in high dimensions", "author": ["T. Zhou", "W. Bian", "D. Tao"], "venue": "ICDM,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 13, "context": "Expectation-maximization (EM) [14], sampling methods [18], and matrix factorization [27, 35] are three algorithms commonly used to produce maximum likelihood (or maximum a posteriori (MAP)) estimates of models with latent variables/factors, and thus are used in a wide range of applications such as clustering, topic modeling, collaborative filtering, structured prediction, feature engineering, and time series analysis.", "startOffset": 30, "endOffset": 34}, {"referenceID": 17, "context": "Expectation-maximization (EM) [14], sampling methods [18], and matrix factorization [27, 35] are three algorithms commonly used to produce maximum likelihood (or maximum a posteriori (MAP)) estimates of models with latent variables/factors, and thus are used in a wide range of applications such as clustering, topic modeling, collaborative filtering, structured prediction, feature engineering, and time series analysis.", "startOffset": 53, "endOffset": 57}, {"referenceID": 26, "context": "Expectation-maximization (EM) [14], sampling methods [18], and matrix factorization [27, 35] are three algorithms commonly used to produce maximum likelihood (or maximum a posteriori (MAP)) estimates of models with latent variables/factors, and thus are used in a wide range of applications such as clustering, topic modeling, collaborative filtering, structured prediction, feature engineering, and time series analysis.", "startOffset": 84, "endOffset": 92}, {"referenceID": 34, "context": "Expectation-maximization (EM) [14], sampling methods [18], and matrix factorization [27, 35] are three algorithms commonly used to produce maximum likelihood (or maximum a posteriori (MAP)) estimates of models with latent variables/factors, and thus are used in a wide range of applications such as clustering, topic modeling, collaborative filtering, structured prediction, feature engineering, and time series analysis.", "startOffset": 84, "endOffset": 92}, {"referenceID": 33, "context": "Hence, their quality greatly depends on initialization and on using a large number of iterations for proper convergence [34].", "startOffset": 120, "endOffset": 124}, {"referenceID": 31, "context": "The method of moments [32, 7, 23], in contrast, solves m equations by relating the first m moments of observation x \u2208 R to the m model parameters, and thus yields a consistent estimator with a global solution.", "startOffset": 22, "endOffset": 33}, {"referenceID": 6, "context": "The method of moments [32, 7, 23], in contrast, solves m equations by relating the first m moments of observation x \u2208 R to the m model parameters, and thus yields a consistent estimator with a global solution.", "startOffset": 22, "endOffset": 33}, {"referenceID": 22, "context": "The method of moments [32, 7, 23], in contrast, solves m equations by relating the first m moments of observation x \u2208 R to the m model parameters, and thus yields a consistent estimator with a global solution.", "startOffset": 22, "endOffset": 33}, {"referenceID": 9, "context": "Although recent spectral methods [10, 24, 20, 1] reduces m to 2 or 3 when estimating O(p) m parameters [2] by relating the eigenspace of lower-order moments to parameters in a matrix form up to column scale, the variance of sample moments is still sensitive to large p or data noise, which may result in poor estimation.", "startOffset": 33, "endOffset": 48}, {"referenceID": 23, "context": "Although recent spectral methods [10, 24, 20, 1] reduces m to 2 or 3 when estimating O(p) m parameters [2] by relating the eigenspace of lower-order moments to parameters in a matrix form up to column scale, the variance of sample moments is still sensitive to large p or data noise, which may result in poor estimation.", "startOffset": 33, "endOffset": 48}, {"referenceID": 19, "context": "Although recent spectral methods [10, 24, 20, 1] reduces m to 2 or 3 when estimating O(p) m parameters [2] by relating the eigenspace of lower-order moments to parameters in a matrix form up to column scale, the variance of sample moments is still sensitive to large p or data noise, which may result in poor estimation.", "startOffset": 33, "endOffset": 48}, {"referenceID": 0, "context": "Although recent spectral methods [10, 24, 20, 1] reduces m to 2 or 3 when estimating O(p) m parameters [2] by relating the eigenspace of lower-order moments to parameters in a matrix form up to column scale, the variance of sample moments is still sensitive to large p or data noise, which may result in poor estimation.", "startOffset": 33, "endOffset": 48}, {"referenceID": 1, "context": "Although recent spectral methods [10, 24, 20, 1] reduces m to 2 or 3 when estimating O(p) m parameters [2] by relating the eigenspace of lower-order moments to parameters in a matrix form up to column scale, the variance of sample moments is still sensitive to large p or data noise, which may result in poor estimation.", "startOffset": 103, "endOffset": 106}, {"referenceID": 14, "context": "This property was introduced in [15] to establish the uniqueness of non-negative matrix factorization (NMF) under simplex constraints, and was later [26, 19] extended to non-negative constraints.", "startOffset": 32, "endOffset": 36}, {"referenceID": 25, "context": "This property was introduced in [15] to establish the uniqueness of non-negative matrix factorization (NMF) under simplex constraints, and was later [26, 19] extended to non-negative constraints.", "startOffset": 149, "endOffset": 157}, {"referenceID": 18, "context": "This property was introduced in [15] to establish the uniqueness of non-negative matrix factorization (NMF) under simplex constraints, and was later [26, 19] extended to non-negative constraints.", "startOffset": 149, "endOffset": 157}, {"referenceID": 36, "context": "We apply both the conical hull anchoring model and DCA to five learning models: Gaussian mixture models (GMM) [37], hidden Markov models (HMM) [6], latent Dirichlet allocation (LDA) [8], NMF [27], and subspace clustering (SC) [16].", "startOffset": 110, "endOffset": 114}, {"referenceID": 5, "context": "We apply both the conical hull anchoring model and DCA to five learning models: Gaussian mixture models (GMM) [37], hidden Markov models (HMM) [6], latent Dirichlet allocation (LDA) [8], NMF [27], and subspace clustering (SC) [16].", "startOffset": 143, "endOffset": 146}, {"referenceID": 7, "context": "We apply both the conical hull anchoring model and DCA to five learning models: Gaussian mixture models (GMM) [37], hidden Markov models (HMM) [6], latent Dirichlet allocation (LDA) [8], NMF [27], and subspace clustering (SC) [16].", "startOffset": 182, "endOffset": 185}, {"referenceID": 26, "context": "We apply both the conical hull anchoring model and DCA to five learning models: Gaussian mixture models (GMM) [37], hidden Markov models (HMM) [6], latent Dirichlet allocation (LDA) [8], NMF [27], and subspace clustering (SC) [16].", "startOffset": 191, "endOffset": 195}, {"referenceID": 15, "context": "We apply both the conical hull anchoring model and DCA to five learning models: Gaussian mixture models (GMM) [37], hidden Markov models (HMM) [6], latent Dirichlet allocation (LDA) [8], NMF [27], and subspace clustering (SC) [16].", "startOffset": 226, "endOffset": 230}, {"referenceID": 14, "context": "2 General Separability Assumption and Minimum Conical Hull Problem The original separability property [15] is defined on the convex hull of a set of data points, namely that each point can be represented as a convex combination of certain subsets of vertices that define the convex hull.", "startOffset": 102, "endOffset": 106}, {"referenceID": 25, "context": "Later works on separable NMF [26, 19] extend it to the conical hull case, which replaced convex with conical combinations.", "startOffset": 29, "endOffset": 37}, {"referenceID": 18, "context": "Later works on separable NMF [26, 19] extend it to the conical hull case, which replaced convex with conical combinations.", "startOffset": 29, "endOffset": 37}, {"referenceID": 29, "context": "According to basic rules [30], a finitely generated and pointed cone C possesses a finite and unique set of extreme rays R, and C = cone(R) is the conical hull generated by these extreme rays R.", "startOffset": 25, "endOffset": 29}, {"referenceID": 4, "context": "3 from [5]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 4, "context": "4 in [5]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 4, "context": "It is easy to verify that XA obtained by Arora\u2019s exact algorithm [5] is a feasible solution fulfilling the constraint in (8).", "startOffset": 65, "endOffset": 68}, {"referenceID": 37, "context": "Lastly, we see how Equation (3) can be seen as a submodular cover problem [38].", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "It can be verified that the constraint in Definition 5 equals to the constraint to F in Definition 2, and that f(A) = cover(XA\u222aI), where XI is any set of linearly independent row vectors, is submodular [17].", "startOffset": 202, "endOffset": 206}, {"referenceID": 2, "context": "When X = Y and S = R + , it degenerates to the original separability assumption given in [3].", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "We generalize the minimum conical hull problem from [3].", "startOffset": 52, "endOffset": 55}, {"referenceID": 2, "context": "When X = Y , it degenerates to the original minimum conical hull problem defined in [3].", "startOffset": 84, "endOffset": 87}, {"referenceID": 2, "context": "3, by following the analysis of the separability assumption in [3],we can prove that A is unique and identifiable given X .", "startOffset": 63, "endOffset": 66}, {"referenceID": 2, "context": "More details are given in [3].", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "Model X in conical hull problem Y in conical hull problem k in conical hull problem NMF data matrixX \u2208 Rn\u00d7p + Y := X # of factors SC data matrixX \u2208 Rn\u00d7p Y := X # of basis from all clusters GMM [vec[X 1 X2]; vec[X T 1 Diag(X3\u03b7t)X2]t\u2208[q]]/n [vec(Xt,1 \u2297Xt,2)]t\u2208[n] # of components/clusters HMM [vec[X 2 X3]; vec[X T 2 Diag(X1\u03b7t)X3]t\u2208[q]]/n [vec(Xt,2 \u2297Xt,3)]t\u2208[n] # of hidden states LDA word-word co-occurrence matrixX \u2208 Rp\u00d7p + Y := X # of topics Algo Each sub-problem in DCA Post-processing afterA := \u22c3 i \u00c3 i Interpretation of anchors indexed byA NMF \u00c3 = MCH(X\u03a6, X\u03a6), can be solved by (37) solving F inX = FXA basisXA are real data points SC \u00c3 =anchors of clusters achieved by meanshift( \u0302 (X\u03a6)\u03c6) clustering anchorsXA cluster i is a cone cone(XAi ) GMM \u00c3 = MCH(X\u03a6, Y\u03a6), can be solved by (37) N/A centers [XA,i]i\u2208[3] from real data HMM \u00c3 = MCH(X\u03a6, Y\u03a6), can be solved by (37) solving T inOT = XA,3 emission matrixO = XA,2 LDA \u00c3 = MCH(X\u03a6, X\u03a6), can be solved by (37) col-normalize {F : X = FXA} anchor word for topic i (topic prob.", "startOffset": 809, "endOffset": 812}, {"referenceID": 35, "context": ", with large principle angles between each other [36].", "startOffset": 49, "endOffset": 53}, {"referenceID": 15, "context": "Most existing subspace clustering methods [16, 36], relies on spectral clustering to the sparse representations of data points, which are obtained by finding a sparse C with diag(C) = 0 in model X = CX .", "startOffset": 42, "endOffset": 50}, {"referenceID": 35, "context": "Most existing subspace clustering methods [16, 36], relies on spectral clustering to the sparse representations of data points, which are obtained by finding a sparse C with diag(C) = 0 in model X = CX .", "startOffset": 42, "endOffset": 50}, {"referenceID": 35, "context": "Then (spectral) clustering to rows of C guarantees to provide clustering labels which can lead to reliable estimation of the k subspaces [36].", "startOffset": 137, "endOffset": 141}, {"referenceID": 2, "context": ", we assume that x is composed of three groups(views) of features {xi}i\u2208[3] such that \u2200i 6= j, xi \u22a5 xj |h.", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "We will mainly focus on the models in which {\u03b1, \u03b8} can be exactly recovered from conditional mean vectors {Oi}i\u2208[3] and E(h\u2297 h)1, because they cover most popular models such as GMM and HMM in real applications.", "startOffset": 112, "endOffset": 115}, {"referenceID": 2, "context": "Therefore, learning {Oi}i\u2208[3] is reduced to selecting k rank-one matrices from {Xt,i \u2297 Xt,j}t\u2208[n] indexed by A, and defining the extreme rays of a conical hull covering the q matrices {Y }t\u2208[q].", "startOffset": 26, "endOffset": 29}, {"referenceID": 8, "context": "However, thanks to the matrix completion research [9, 12], when k min{pi, pj}, we can retain merely m = O(max{pi, pj}k log(pi + pj)) pipj entries in the vectorization of each matrix from {Y }t\u2208[q] and {Xt,i \u2297Xt,j}t\u2208[n] in (17).", "startOffset": 50, "endOffset": 57}, {"referenceID": 11, "context": "However, thanks to the matrix completion research [9, 12], when k min{pi, pj}, we can retain merely m = O(max{pi, pj}k log(pi + pj)) pipj entries in the vectorization of each matrix from {Y }t\u2208[q] and {Xt,i \u2297Xt,j}t\u2208[n] in (17).", "startOffset": 50, "endOffset": 57}, {"referenceID": 3, "context": "This leads to a natural assumption of \u201canchor word\u201d for LDA [4].", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "Under multi-view assumption x = {xi}i\u2208[3] with \u2200i 6= j, xi \u22a5 xj |h, MM generates an observation x by firstly drawing a label indicator h \u223c \u03c3 \u2208 \u2206k\u22121 from {ei}i\u2208[k] and then drawing features of different views xi \u223c p(xi| \u2211k j=1 hj\u03b8 j i ) independently.", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": ", for each triple of observations {x1, x2, x3} and the corresponding hidden states {h1, h2, h3}, let h1 \u223c \u03c0, by integrating out h1 and h2, we have [2] \uf8f4\uf8f2\uf8f4\uf8f3 E(x2|h2) = h2 O \u2192 h2 O 2 , h2 \u223c T\u03c0, E(x1|h2) = \u2211 h1 E(x1|h1) \u00b7 p(h2|h1)p(h1) p(h2) = h T 2 [ ODiag(\u03c0)TTDiag((T\u03c0)\u22121) ]T \u2192 h2 O 1 , E(x3|h2) = \u2211 h3 E(x3|h3)p(h3|h2) = h2 [OT ] \u2192 h2 O 3 .", "startOffset": 147, "endOffset": 150}, {"referenceID": 38, "context": "The first DCA algorithm was proposed in [39] only for separable NMF, but this paper will largely extend the idea to much richer class of problems and algorithm designs.", "startOffset": 40, "endOffset": 44}, {"referenceID": 38, "context": "For the special case of NMF when X = FXA, the above result was proved in [39].", "startOffset": 73, "endOffset": 77}, {"referenceID": 3, "context": "Define d-dim (d \u2264 p) hyperplanes {Hi}i\u2208[4] such that A3A2 \u22a5 H1, A \u2032 1A \u2032 2 \u22a5 H2, B\u2032 1A2 \u22a5 H3, B\u2032 1C \u2032 1 \u22a5 H4, let \u03b1 = \u01241H2 be the angle between hyperplanes H1 and H2, \u03b2 = \u01243H4 be the angle between H3 and H4.", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "By applying Chernoff bound to random variable \u011d(Ai) \u2212 \u011d(Bj) (randomness is due to random hyperplane H), for any \u03b4 \u2208 [0, 1], Ai and Bj , we have", "startOffset": 116, "endOffset": 122}, {"referenceID": 21, "context": "Remarks: It is worth noting that although DCA uses random projection to reduce the problem size, it is different from the random projection methods based on Johnson-Lindenstrauss (JL) Lemma [22] or its variants.", "startOffset": 190, "endOffset": 194}, {"referenceID": 4, "context": "DCA provides a much faster off-the-shelf subroutine which can be easily invoked by other methods [5] to gain a significant acceleration.", "startOffset": 97, "endOffset": 100}, {"referenceID": 3, "context": "Recently, we surprisingly discover that the above procedure equals to a Bayes learning algorithm proposed in [4], whose major idea is to solve an NMF under simplex constraint by a greedy pursuit typed algorithm.", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "Comparing to the specific greedy algorithm developed for LDA in [4], our method provides a unified scheme that can reduce more general models besides LDA to a conical hull problem, and the proposed DCA leads to a significantly efficient algorithm.", "startOffset": 64, "endOffset": 67}, {"referenceID": 3, "context": "This is because 1) Limited by the simplex constraint in NMF, the Bayes learning method decomposes the row-normalized X in (27), thus it needs to additionally compute p(w2 = j|w1 = i) and p(w1 = i) for normalization, and \u2211 i p(z = et|x = ei)p(x = ei) after NMF; and 2)The greedy algorithm in [4] finds the anchors of convex hull is slower than DCA using parallel and randomized strategy.", "startOffset": 291, "endOffset": 294}, {"referenceID": 3, "context": "The equivalence between [4] and our method can be established by the following theorem.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "In order to make the comparison clear, we map all the notations in our method to those used in [4] such that Q \u2190 E(x1 \u2297 x2), V \u2190 A, A \u2190 O , vt denotes the index of the anchor word for topic t, \u0100t,i \u2190 At,i/At,vt , wi = j \u21d4 xi = ej , zi = j \u21d4 zi = ej .", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "Then we use the notations in [4] throughout the theorem and its proof below.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "Solving the conical hull problem Q = \u0100QV proposed in this paper for LDA equals to Bayes learning of p(z1 = t|w1 = i) by NMF proposed in [4], i.", "startOffset": 136, "endOffset": 139}, {"referenceID": 3, "context": "Since A can be uniquely recovered as row-normalized \u0100, and the above reasoning is reversible (due to all the equalities), the equivalence between conical hull problem and the Bayes learning [4] given in (40) holds.", "startOffset": 190, "endOffset": 193}, {"referenceID": 38, "context": "DCA for NMF has been proposed in [39].", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "A simple but effective one is to sample \u03a6 \u2208 Rp\u00d72, project X to an 2D plane, apply mean shift clustering algorithm [13] to the n-array of angles \u0302 (Xi\u03a6)\u03c6, and add to \u00c3 the two points with the maximal and minimal angle in each cluster.", "startOffset": 114, "endOffset": 118}, {"referenceID": 30, "context": "Thus we can build a graph Laplacian from similarity matrixG \u2208 RK\u00d7K such thatGi,j = #(XA(i)\u03a6 andXA(j)\u03a6 in the same cluster), and spectral clustering [31] is able to give us the k clusters of anchors {Ai}i\u2208[k].", "startOffset": 148, "endOffset": 152}, {"referenceID": 4, "context": "LP-test [5] has the exact solution guarantee, but it is not robust to noise, and too slow in speed.", "startOffset": 8, "endOffset": 11}, {"referenceID": 4, "context": "LP-test is the backward removal algorithm from [5].", "startOffset": 47, "endOffset": 50}, {"referenceID": 28, "context": "Baselines: K-means [29], EM algorithm, spectral method.", "startOffset": 19, "endOffset": 23}, {"referenceID": 10, "context": "DCA provides a much more practical algorithm in speed that can achieve comparable mutual information but more than 1000 times speedup than the state-of-the-art SC algorithms [11, 16, 28, 36].", "startOffset": 174, "endOffset": 190}, {"referenceID": 15, "context": "DCA provides a much more practical algorithm in speed that can achieve comparable mutual information but more than 1000 times speedup than the state-of-the-art SC algorithms [11, 16, 28, 36].", "startOffset": 174, "endOffset": 190}, {"referenceID": 27, "context": "DCA provides a much more practical algorithm in speed that can achieve comparable mutual information but more than 1000 times speedup than the state-of-the-art SC algorithms [11, 16, 28, 36].", "startOffset": 174, "endOffset": 190}, {"referenceID": 35, "context": "DCA provides a much more practical algorithm in speed that can achieve comparable mutual information but more than 1000 times speedup than the state-of-the-art SC algorithms [11, 16, 28, 36].", "startOffset": 174, "endOffset": 190}, {"referenceID": 20, "context": "The motion for each frame is manually labeled by the authors of [21].", "startOffset": 64, "endOffset": 68}, {"referenceID": 32, "context": "Baselines: EM algorithm for variational method, Gibbs sampling [33], spectral method.", "startOffset": 63, "endOffset": 67}, {"referenceID": 2, "context": "\u2022 In order to break the linearity assumption E(x|h) = hO and generalize distribution p(x|h) to even non-parametric forms, we can consider to embed the joint distribution of {xi}i\u2208[3] into a reproducing kernel Hilbert space (RKHS).", "startOffset": 179, "endOffset": 182}, {"referenceID": 1, "context": "A very related work [2] has shown this possibility for linear Bayesian networks.", "startOffset": 20, "endOffset": 23}, {"referenceID": 24, "context": "The essential idea is to represent X by a weighted sum of k rank-one matrices [25], each of which is generated by a real data point.", "startOffset": 78, "endOffset": 82}], "year": 2014, "abstractText": "We reduce a broad class of machine learning problems, usually addressed by EM or sampling, to the problem of finding the k extremal rays spanning the conical hull of a data point set. These k \u201canchors\u201d lead to a global solution and a more interpretable model that can even outperform EM and sampling on generalization error. To find the k anchors, we propose a novel divide-and-conquer learning scheme \u201cDCA\u201d that distributes the problem to O(k log k) same-type sub-problems on different low-D random hyperplanes, each can be solved by any solver. For the 2D sub-problem, we present a non-iterative solver that only needs to compute an array of cosine values and its max/min entries. DCA also provides a faster subroutine for other methods to check whether a point is covered in a conical hull, which improves algorithm design in multiple dimensions and brings significant speedup to learning. We apply our method to GMM, HMM, LDA, NMF and subspace clustering, then show its competitive performance and scalability over other methods on rich datasets.", "creator": "LaTeX with hyperref package"}}}