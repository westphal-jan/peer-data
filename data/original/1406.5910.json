{"id": "1406.5910", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2014", "title": "Multi-utility Learning: Structured-output Learning with Multiple Annotation-specific Loss Functions", "abstract": "Structured-output learning is a challenging problem; particularly so because of the difficulty in obtaining large datasets of fully labelled instances for training. In this paper we try to overcome this difficulty by presenting a multi-utility learning framework for structured prediction that can learn from training instances with different forms of supervision. We propose a unified technique for inferring the loss functions most suitable for quantifying the consistency of solutions with the given weak annotation. We demonstrate the effectiveness of our framework on the challenging semantic image segmentation problem for which a wide variety of annotations can be used. For instance, the popular training datasets for semantic segmentation are composed of images with hard-to-generate full pixel labellings, as well as images with easy-to-obtain weak annotations, such as bounding boxes around objects, or image-level labels that specify which object categories are present in an image. Experimental evaluation shows that the use of annotation-specific loss functions dramatically improves segmentation accuracy compared to the baseline system where only one type of weak annotation is used.", "histories": [["v1", "Mon, 23 Jun 2014 14:06:24 GMT  (1860kb,D)", "http://arxiv.org/abs/1406.5910v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["roman shapovalov", "dmitry vetrov", "anton osokin", "pushmeet kohli"], "accepted": false, "id": "1406.5910"}, "pdf": {"name": "1406.5910.pdf", "metadata": {"source": "CRF", "title": "Multi-utility Learning: Structured-output Learning with Multiple Annotation-specific Loss Functions", "authors": ["Roman Shapovalov", "Dmitry Vetrov", "Anton Osokin"], "emails": ["shapovalov@graphics.cs.msu.ru", "vetrovd@yandex.ru", "anton.osokin@gmail.com", "pkohli@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Training structured-output classifiers is a challenging problem; not only because of the associated computational burden, but also due to difficulties in obtaining the ground-truth labelling for training data: in problems like semantic image segmentation the structured label may comprise thousands\nar X\niv :1\n40 6.\n59 10\nv1 [\ncs .C\nV ]\nof scalars, so annotation of large datasets requires a lot of human effort. In contrast, it is much easier to obtain a weak annotation of an image, i.e. some statistic of the image labelling. This may take various forms: an image-level label that indicates presence or counts the number of pixels of a particular object category like \u2018sky\u2019 or \u2018water\u2019, a set of objects\u2019 bounding boxes\u2014rectangles that tightly bound object instances\u2019 segmentations, or a set of seeds\u2014the pixels that have to take the specified labels (Fig. 1). More broadly, weakly-supervised learning may be useful in many training problems where the input is obtained by crowdsourcing. For example, some part of a training set for object detection may be of low quality, meaning that the bounding boxes are not tight. In the document tagging problem, low-quality ground truth may miss some tags of the documents. It is preferable to model those biases in the annotation explicitly.\nAs for semantic segmentation, different types of annotations help not only to overcome logistic difficulties, but also to characterize certain categories better. For example, many object categories (i.e. \u2018things\u2019 in terms of Heitz and Koller [6]) are better described by bounding-box annotations, while the background categories (i.e. \u2018stuff\u2019 [6])\u2014which tend to fill significant parts of an image\u2014 by image-level labels.\nA number of researchers have recognized the importance of weak annotations for learning semantic segmentation. However, most of these methods only use image-level labels. For example, Vezhnevets et al. [22, 23] use a multi-image probabilistic graphical model to propagate image-level annotations across different training images. In this paper, we present a framework for learning structured classification from the mixture of fully and weakly annotated instances. Our framework can employ different types of weak annotations, even for a single instance.\nOur work extends recent research on using latent-variable structural support vector machines (LV-SSVM) for weakly-supervised learning [3, 8, 11] by incorporating annotation-specific loss functions, which measure the inconsistency of some labelling predicted by the algorithm with the ground-truth weak annotation. We define those loss functions such that each of them returns an estimate of the expected Hamming loss w.r.t. all possible labellings consistent with the corresponding weak annotation. Due to this definition, the loss functions specific to different annotation types have the same scale. Our framework thus requires only one coefficient, which balances the relative impact of the loss functions for fully labelled and weakly annotated data, since the latter are typically less informative. We empirically show that balancing between these two kinds of loss functions can improve labelling performance.\nA number of key technical challenges arise while learning an LV-SSVM model with multiple annotation-specific loss functions. These include solution of the loss-augmented and annotationconsistent inference problems. The former involves finding the labelling that satisfies the current model and deviates from the annotation the most, while the latter involves finding the best labelling that is consistent with the weak annotation. We show how to solve these optimization problems for various loss functions using efficient optimization algorithms.\nRelation to previous work. Our work is most closely related to the work of Kumar et al. [8], who use a sequential method to learn semantic segmentation from different types of annotations. Their method starts by training LV-SSVM with a loss function defined on partial labellings; it performs loss-augmented inference using carefully initialized iterated conditional modes (ICM). Once this model is trained, they infer the partial labellings for weakly-annotated images that are consistent with their bounding-box or image-level annotations. The model is then re-trained by considering those solutions as the true partial labellings for the training instances. Unlike Kumar et al. [8], at the training stage we minimize our annotation-specific loss functions simultaneously. In this regard, our framework does not require neither fully nor partially labelled images, which are essential for the first stage of their algorithm. Furthermore, our loss functions allow us to use powerful graph cut based algorithms for solving the loss-augmented and annotation-consistent inference problems, instead of using an ICM-based inference. Finally, we use different types of weak annotations.\nFor some of the loss functions we use, the loss-augmented inference problems cannot be decomposed to the individual variables. This relates us to the recent work on supervised learning with non-decomposable loss functions [13, 16]. Pletscher and Kohli [13] use a higher-order loss function that penalizes the difference in the area of the target category between binary segmentations. They show how to use graph cuts for efficient exact loss-augmented inference. Tarlow and Zemel [16]\nuse message-passing inference in SSVM training with three different higher-order loss functions: PASCAL VOC loss, bounding box fullness loss, and local border convexity loss.\nOur contributions:\n\u2022 we propose an LV-SSVM based multi-utility learning framework, which simultaneously minimizes different annotation-specific loss functions, and a unified technique for establishing loss functions for weak annotation of different types;\n\u2022 we apply our framework to define the loss functions for training semantic segmentation that are specific to the following weak annotation types and their combinations: imagelevel labels, bounding boxes, and objects\u2019 seeds;\n\u2022 we propose efficient inference algorithms required for LV-SSVM training with these loss functions."}, {"heading": "2 Latent-variable SSVM", "text": ""}, {"heading": "2.1 Structured-output learning", "text": "Structured-output learning attempts to learn a mapping H from the space of features X to the space of all possible labellings Y . In what follows, we consider only the mappings that can be expressed as maximization of a discriminant function F that depends linearly on its parameters w:\nH(x) = argmax y\u2208Y F (x,y; w) = argmax y\u2208Y\nw\u1d40\u03a8(x,y), (1)\nwhere vector function \u03a8(x,y) denotes so-called generalized features of instance x \u2208 X and labelling y. \u03a8(x,y) is defined in a problem-specific way, while the weights w are learned from the training data. We address a wide class of so-called labelling problems, where the structured label is a vector of discrete variables: Y = KV , where K = {1, . . . ,K}. Its length V may vary for individual instances.\nThe goal of supervised structured-output learning is to obtain the most appropriate weights w given the set of features and ground-truth labels of training instances: {(xn,yn)}Nn=1, yn \u2208 Yn. Here Yn is a set of possible labellings compatible with the n-th instance. In this paper we follow the maxmargin formulation of structured-output learning (also called structural support vector machine, SSVM) [17, 20, 7]:\nmin w,\u03be\u22650\n1 2 w\u1d40w + C N N\u2211 n=1 \u03ben, (2)\ns.t. F (xn,yn; w) \u2265 max y\u0304\u2208Yn\n( F (xn, y\u0304; w) + \u2206(y\u0304,yn) ) \u2212 \u03ben, \u2200n, (3)\nwhere \u2206(y\u0304,yn) is the loss of some labelling y\u0304 = {y\u0304i}Vi=1 with respect to the ground truth labelling yn = {yni } Vn i=1. Let c n i be some cost associated with the i-th variable in the labelling of the n-th instance. The commonly used loss function is the weighted Hamming distance:\n\u2206(y\u0304,yn) = \u2211 i\u2208Vn cni [y\u0304i 6= yni ], 1 (4)\nThis loss function is decomposable w.r.t. the individual variables. It often implies that lossaugmented inference, i.e. maximization in (3), is no more difficult than the maximization of discriminant function F (x,y; w). In some cases it is possible to use higher-order loss functions that cannot be decomposed w.r.t. the individual variables [13, 16, 4].\nProblem (2)\u2013(3) is convex and can be solved by the cutting-plane method [20, 7]. This method replaces the constraint (3) with a bunch of linear constraints and then iteratively approximates the feasible polytope by adding the most violated constraint. Such constraint is determined in each iteration by running the loss-augmented inference in (3).\n1We use the Iverson bracket notation: [e] = 1 if the logical expression e is true, and [e] = 0 otherwise"}, {"heading": "2.2 Learning with weak annotations", "text": "Consider the case when in addition to N fully-labelled objects, train set contains M weaklyannotated ones: {(xm, zm)}N+Mm=N+1. Hereinafter we assume that the weak annotation zm defines a subset of full labellings L(zm) \u2282 Y that are consistent with it, and thus zm is less informative than an individual full labelling ym. Examples of such weak annotations for the image segmentation problem are (1) bounding boxes of the segments of a given label; (2) a value of some global statistic (area, average intensity, number of connected components etc.) for the segments of a given label; (3) subsets of superpixels that belong to a given label (seeds).\nWe now generalize the standard SSVM formulation to make it handle both fully and weakly annotated data simultaneously. Our multi-utility SSVM is formally defined as follows:\nmin w,\u03be\u22650,\u03b7\u22650\n1 2 w\u1d40w +\nC\nN +M\n( N\u2211\nn=1\n\u03ben + \u03b1 M\u2211 m=1 \u03b7m\n) , (5)\ns.t. F (xn,yn; w) \u2265 max y\u0304\u2208Yn (F (xn, y\u0304; w) + \u2206(y\u0304,yn))\u2212 \u03ben, \u2200n, (6)\nmax y\u2208L(zm) F (xm,y; w) \u2265 max y\u0304\u2208Ym (F (xm, y\u0304; w) + K(y\u0304, zm))\u2212 \u03b7m,\u2200m. (7)\nNote that for M = 0 the above formulation degenerates to the standard SSVM formulation, while for N = 0 it reduces to the latent-variable SSVM [24]. Note also that the full labelling yn can be seen as a degenerate weak annotation, where L(zm) = {yn}. Therefore, Problem (5)\u2013(7) is almost equivalent to LV-SSVM, but it contains the slack balancing coefficient \u03b1. Ignoring this coefficient may hurt the performance of multi-utility learning, as we show in Section 4.2. In order to perform the optimization, in addition to the loss-augmented inference in (6), we should also be able to perform the weak-loss augmented inference in (7), as well as the annotation-consistent inference in the left-hand side of (7).\nOptimization problem (5)\u2013(7) is not convex and thus hard. We follow Yu and Joachims [24] and use the concave-convex procedure (CCCP) [25] to solve it approximately."}, {"heading": "3 Weak annotation for semantic image segmentation", "text": "Semantic image segmentation aims to assign category labels to image pixels. We assume that an image is represented as a set of superpixels, i.e. groups of co-located pixels similar by appearance. Consider a graph G = (V, E). Its nodes V correspond to superpixels of the image. The set of edges E represents a neighborhood system on V that includes the pairs of nodes that correspond to all adjacent superpixels. Let xi \u2208 Rd be a vector of superpixel features associated with some node i \u2208 V , xij \u2208 Re be a vector of superpixel interaction features for the edge connecting nodes i and j, and x = \u2295 i\u2208V xi \u2295 \u2295 (i,j)\u2208E xij be their concatenation. The value of each variable yi corresponds to the label of the i-th superpixel. We use the following discriminant function F :\nF (x,y; w) = w\u1d40\u03a8(x,y) = \u2211 i\u2208V K\u2211 k=1 [yi = k](x \u1d40 i w u k) + \u2211 (i,j)\u2208E [yi = yj ](x \u1d40 ijw p), (8)\nwhere w = \u2295K\nk=1 w u k \u2295 wp is a vector of the model parameters, and wuk \u2208 Rd, wp \u2208 Re . The\nsummands in the first and the second terms are called unary and pairwise potentials, respectively. We restrict pairwise weights wp and pairwise features xij to be nonnegative and thus obtain an associative discriminative function (with only attractive pairwise potentials) [17]. Maximizing F (x,y; w) w.r.t. y is known to be NP-hard, but efficient approximate algorithms exist, e.g. \u03b1-expansion [2].\nWe use the weighted Hamming loss (4) for fully-labelled images, where ci is the number of pixels in the corresponding superpixel, so the loss function estimates the number of mislabelled image pixels.2 To use some type of weak annotations for training, we need to define the annotationspecific loss function that allows loss-augmented inference and annotation-consistent inference. The\n2In practice, ground-truth labelling of a superpixel may contain several labels; in this case the number of incorrectly inferred pixels is added to the loss. We ignore this case to ease the notation, but all the algorithms still work in that case.\nformer should be efficient, since it is performed in the inner loop of training and thus is typically a bottleneck. We show how to define and combine them for the annotations of the following types: image-level labels, bounding boxes around objects, and objects\u2019 seeds."}, {"heading": "3.1 Image-level labels", "text": "We start by defining loss functions K(y, z) for some arbitrary labelling y and ground-truth weak annotation z. In this subsection we assume that z is a set of labels used in the ground-truth image labelling (for the image in Fig. 1, z = {\u2018sky\u2019, \u2018tree\u2019, \u2018plain\u2019, \u2018grass\u2019}). We cannot compute the Hamming loss (4) if the full labelling is unknown for one of its arguments. Let\u2019s instead define a proxy loss function, that is symmetric and does not compare labels of any superpixels directly:\n\u2206il(y, y\u0304) = \u2211 i\u2208V ci[@j \u2208 V : yj = y\u0304i \u2228 @j \u2208 V : y\u0304j = yi]. (9)\nIt penalizes all the superpixels that have been given any label that lacks in the annotation y\u0304, as well as superpixels which have ground truth labels that lack in y. Unfortunately, the ground-truth labelling y\u0304 is unknown. If we knew the areas Sk of each label k \u2208 z, we could derive the following upper bound on (9):\nKil(y, z; {Sk}k\u2208z) = \u2211 k 6\u2208z \u2211 i\u2208V ci[yi = k] + \u2211 k\u2208z Sk \u220f i\u2208V [yi 6= k]. (10)\nThis upper bound is tight up to a factor of 2. The first term penalizes the pixels labelled with wrong labels, while the second term penalizes ignoring the labels from z.\nSince we do not know the areas Sk, the best we can do is to assume K(y, z) to be the expectation of (10) taken over all full labellings consistent with z. If there are enough fully-labelled images, the areas Sk can be estimated. Otherwise we assume the uniform distribution over the feasible full labellings y \u2208 z and get\nKil(y, z) = \u2211 k 6\u2208z \u2211 i\u2208V ci[yi = k] + \u2211 k\u2208z \u2211 i\u2208V ci |z| \u220f i\u2208V [yi 6= k]. (11)\nHaving defined the loss function Kil, we need to provide algorithms for inference problems in (7). For annotation-consistent inference maxy\u2208zm F (xm,y; w) we use \u03b1-expansion over the labels from zm only. Note that this may result in an inconsistent labelling: some labels from zm may miss in y. We have tried an heuristic algorithm for making it strictly consistent with z by changing one node per missing label, but observed no significant difference in practice.\nThe loss-augmented inference is now not decomposable to unary and pairwise factors. To work this around, we derive:\nmax y\u0304\u2208Ym (F (xm, y\u0304; w) +Kil(y\u0304, zm)) =\nmax y\u0304\u2208Ym\n( F (xm, y\u0304; w) +\n\u2211 k 6\u2208z \u2211 i\u2208V ci[y\u0304i = k]\u2212 \u2211 k\u2208z \u2211 i\u2208V ci |z| [\u2203i : y\u0304i = k] ) + const. (12)\nThe last maximization is the standard MRF inference problem with label costs. We use the efficient modification of \u03b1-expansion for accounting label costs [4]."}, {"heading": "3.2 Bounding boxes", "text": "It is convenient to annotate instances in an image with tight bounding boxes (Fig. 1c). On the other hand, they do not give much information for background categories. Therefore, we consider the annotation that consists of both bounding boxes and image-level labels. For example, annotation of an image may contain the bounding-box locations of cars and pedestrians, and additionally state that there are buildings, road, and sky in the image. We assume that within a certain image each category can be defined either with image-level labels, or with bounding boxes, though the type of annotation for a category may vary from image to image (see Section 4.3 for an example where it can be useful).\nWe model weak annotation z of an image as a pair (zil, zbb) of image-level and bounding box annotations. The latter is a set of bounding boxes with associated category labels: zbb = {zi}, with the following functions defined: label(zi), which defines the associated category label, and box(zi) = [left(zi), right(zi)] \u00d7 [top(zi), bottom(zi)] that defines the extent of the bounding box. The set of labels K is partitioned into three subsets w.r.t. the weak annotation z: the labels that are defined with bounding boxes (Kb = \u22c3 z\u2208zbb label(z)), those that are present somewhere else in the image (Kp = zil), and those that are absent (Ka = K \\ (Kb \u222a Kp)). Nodes V are also partitioned: Vk = \u22c3 z\u2208zbb:label(z)=k box(z) is the union of pixel indices in the bounding boxes corresponding to\nthe label k \u2208 Kb, and V0 = V \\ \u22c3\nk\u2208Kb Vk. We now define the combined loss function as:\nKil-bb(y, z) = \u2211 k\u2208Ka \u2211 i\u2208V ci [yi = k] + \u2211 k\u2208Kp \u03c3k \u220f i\u2208V [yi 6= k] +\n\u03b2 \u2211 z\u2208zbb ( bottom(z)\u2211 p=top(z) \u03bdzp right(z)\u220f q=left(z) V((p, q); y, label(z)) + right(z)\u2211 q=left(z) \u03c9zq bottom(z)\u220f p=top(z) V((p, q); y, label(z)) )\n+ \u2211 k\u2208Kb \u2211 i\u2208V0 ci [yi = k] . (13)\nThe first two terms are almost the same as in (11), but the estimate of the category area in the second term does not include the pixels within the bounding boxes: \u03c3k = (\u2211 i\u2208V0 ci ) /|zil|. The third term penalizes \u2018empty\u2019 rows and columns in the bounding boxes, i.e. those rows and columns that do not contain pixels of a target category at all. The violation function V is defined as:\nV(p; y, k) = {\n1, if map(y)p 6= k, 0, otherwise.\n(14)\nHere map(y) is the classification map induced by the superpixel labelling y. Coefficients \u03bdzp and \u03c9 z q allow us to assign the penalty for the corresponding row or column being empty, depending on its position in the bounding box. One can learn the category-specific profiles of \u03bdz and \u03c9z when the full labelling is abundant enough, but we use uniform profiles assuming that half of a bounding box is occupied by the object on average: \u03bdzp = ( right(z) \u2212 left(z) ) /2, \u03c9zq = ( bottom(z) \u2212 top(z) ) /2. Note that this makes the loss an estimate on the number of mislabelled pixels (similar to the imagelevel label loss (11)), so the value coefficient \u03b2 = 1 should work well (we show in Section 4.3 that it really does). We have also tried linearly decreasing loss used by Kumar et al. [8], but it did not affect the performance significantly. The last term penalizes the bounding-box labels outside of bounding boxes.\nWe have shown in the previous section how to account for the two initial terms in the loss-augmented inference. The last term is decomposable w.r.t. superpixels. The third term is a sum over the higherorder cliques of the following form. For each bounding box z, each row and each column generates a clique of nodes corresponding to the superpixels that intersect that row/column. We treat them the same way as the image-level loss: we modify \u03b1-expansion with label costs [4] to penalize each clique of superpixels, which contains at least one superpixel labelled with label(z). There is a technical difficulty with the superpixels that cross the bounding box border: it is unclear if their labelling with label(z) should be penalized. We adopted the following strategy: shrink the bounding box to allow some margin, and treat all superpixels that intersect the shrunk bounding box (and only them) as insiders. We set the margin width equal to 6% of the corresponding bounding box dimension.\nDuring the annotation-consistent inference, we need to infer a labelling that has objects only in bounding boxes of the corresponding category labels, and they should fill those bounding boxes tightly, i.e. touch upon all four sides of the bounding box shrunk to allow a 6% margin (Lempitsky et al. [9] showed that this definition is natural). The first condition is easy to satisfy: we can suppress certain labels outside of bounding boxes by using infinite unary potentials. To provide tightness, we use a variation of the pinpointing algorithm [9], adapted for the multi-class segmentation. First, segmentation is performed without the tightness constraints. Then, until those constraints are satisfied, one of the superpixels changes its unary potential, and expansion move is performed. In our implementation, we select the superpixel with the highest relative potential for label(z) that has not been assigned this label yet, and assign it the infinite potential for label(z) to guarantee that it will change\nits label. This procedure is finite because at each iteration at least one superpixel within box(z) switches to label(z). In contrast to Lempitsky et al. [9], we do not perform further dilation, since it is unclear, which label we should use for expansion move(s); neither of the heuristics we tried improved the result significantly. We also found that initialization of the latent variables in LV-SSVM matters: we obtained the best results when initially all superpixels within box(z) were initialized with label(z). Note that Kumar et al. [8] used a different criterion during the annotation-consistent inference: they penalize the empty rows and columns within bounding boxes (the opposite to what we do in loss-augmented inference). Note that their heuristic does not guarantee the tightness of the resulting segmentation."}, {"heading": "3.3 Objects\u2019 seeds", "text": "Another form of a weak annotation natural for the object categories is the seed annotation (Fig. 1d). In general, for a segment of some category, a seed is a subset of its pixels. We consider a particular case, where only one pixel, persumably close to the segment center, is labelled. During the annotation-consistent inference, we require the superpixel where this point is located to have the fixed seed label.\nWe now model the weak annotation z as a pair (zil, zos), where zos is a set of 2D points with the corresponding labels: (p, k). The seed centrality assumption allows us to set the Gaussian penalty for inferring any non-seed label in the neighbourhood of each seed, which brings us to the following loss function:\nKil-os(y, z) = \u2211 k\u2208ka \u2211 i\u2208V ci [yi = k] + \u2211 k\u2208kp \u03c3k \u220f i\u2208V [yi 6= k] +\n\u03b2 \u2211\n(p\u2032,k\u2032) \u2208zos\n\u2211 p\u2208I V (p; y, k\u2032) exp ( \u2212\u03c0\u2016p\u2212 p \u2032\u20162 \u03c4k\u2032 ) . (15)\nHere the first two terms are the same as in the image-level label loss. The inner sum in the third term is taken over all image pixels I . The form of the Gaussian is defined in such a way that the penalty for misclassification of the central pixel p\u2032 is 1, and whenever no superpixels of the label k\u2032 are found, the penalty is equal to the estimated area of the label k\u2032 w.r.t. all labellings consistent with the weak annotation; specifically,\n\u03c4k\u2032 =\n\u2211 i\u2208V ci\n(|zil|+ #Lab(zos)) \u00b7 #Obj(zos, k\u2032) . (16)\nHere #Lab(zos) is the number of different labels in zos, and #Obj(zos, k\u2032) is the number of seeds of the label k\u2032 in zos. Loss (15) is decomposable to factors, so the loss-augmented inference is trivial."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets and metrics", "text": "We test the proposed framework on two datasets: MSRCv23 [15, 22] and SIFT-flow4 [10, 18, 23]. MSRC contains 276 training and 256 test images that are fully labelled using 23 category labels; significant part of pixels remains unlabelled. SIFT-flow is a more challenging dataset: it is a subset of the LabelMe database [19], which contains 2488 training and 200 test images; they are labelled to 33 categories using crowd-sourcing.\nFor MSRC, we obtain superpixels using the original implementation of the gPb edge detector [1]. The unary features are the following: a histogram of SIFT [12] visual words built using a dictionary of size 512 by hard assignment of the descriptors to the bins; a histogram of the RGB colors on a dictionary of size 128; a histogram of locations over a uniform 6\u00d76 grid. We L2-normalize the joint feature vector and map it into a higher-dimensional space where the inner product approximates the \u03c72-kernel in the original space (the dimensionality of the space triples after the transformation) [21].\n3http://research.microsoft.com/en-us/projects/objectclassrecognition/ 4http://people.csail.mit.edu/celiu/LabelTransfer/code.html\nWe use pairwise factors over the pairs of the superpixels that share a common border and use the following 4 pairwise features: exp(\u2212cij/10), exp(\u2212cij/40), exp(\u2212cij/100), 1. Here cij is the strength of the boundary between segments i and j returned by the gPb.\nFor SIFT-flow, we follow Vezhnevets et al. [23] and obtain superpixels and features using the code by Tighe and Lazebnik [18]. It runs graph-based segmentation of Felzenszwalb and Huttenlocher [5] followed by feature extraction. The unary features include shape, location, texture, color and appearance feature vectors, some of which are also computed over dilated superpixel masks to capture the context: 3115 unary features in total. We also transform this vector with a \u03c72-kernel approximation, which triples its size. We use pairwise factors over the pairs of superpixels that share a common border and the pairwise features computed as distances between groups of superpixels\u2019 features (\u03c72 distance in case of histograms, Euclidean otherwise), 26 features in total.\nQuality measures. We use two standard measures of segmentation quality: accuracy and per-class recall. The accuracy is defined as the rate of correctly labelled pixels of the test set. The per-class recall is the number of correctly labelled pixels of each category divided by the true total area of that category, averaged over categories. Following the previous work [22, 14], we exclude the pixels of rare categories (\u2018horse\u2019 and \u2018mountain\u2019) from recall computation for MSRC, but include the \u2018other\u2019 label, see Section 4.2. Similarly, we exclude rare categories (\u2018cow\u2019, \u2018desert\u2019, \u2018moon\u2019, and \u2018sun\u2019) from SIFT-flow recall computation."}, {"heading": "4.2 Image-level labels", "text": "Generating weak annotation. We obtain image-level labels automatically from full labellings by enumerating the unique labels for each image. Each MSRC image typically features one or several objects of some target category (e.g. \u2018sign\u2019, \u2018cow\u2019, \u2018car\u2019) on top of some background. Not every background category falls into the used labels, so it may remain unlabelled. Thus, some images contain only one category label. In this case the image-level label unambiguously defines the full labelling. To avoid this knowledge (unrealistic in the real-world setting), we could model the \u2018other\u2019 label, which contains anything but the labelled 23 categories. However, the labellings typically have uncertain borders between segments of different labels, i.e. the borders are unlabelled too (Fig. 1b). If we modelled those boundaries as a separate category, it would hurt the segmentation performance. Instead, we want to model this \u2018other\u2019 label only for unlabelled regions, not for the boundaries. We use the following heuristic criterion for obtaining image-level labels: if an image contains only one label, or at least 30% of its pixels are unlabelled, we include them to the image-level label as the \u2018other\u2019 label.\nVarying the full-labelling rate. In our basic setting we have a (possibly empty) part of the training set fully labelled, while the rest of the images have only image-level labels. We generate those\nsubsets using the Metropolis\u2013Hastings sampling, trying to make the distribution of their label counts approximate that of the whole training set. Fig. 2a shows the accuracy and per-class recall of the test set segmentation for various full labelling rates in comparison to the fully-supervised setting.5 In the most common scenario\u2014when less than 20% of the training set is fully labelled\u2014the weaklyannotated subset provides a stable 10\u201315% improvement both in terms of the accuracy and mean per-class recall.\nBalancing the loss functions. When the training set consists of both weak annotations and full labellings, the coefficient \u03b1 from (5) needs to be set. We discovered that its optimal value was lower than 1 (Fig. 2b shows the dependency of performance on \u03b1). We speculate that this is because we are more certain about the strong loss, so it should contribute to the objective more. Thus, for all the other experiments we set \u03b1 = 0.1.\nSIFT-flow results. On the SIFT-flow dataset, we compare fully-supervised learning with weaklysupervised at one point, i.e. when only 256 training images are fully labelled, and the rest 2232 images have only image-level labels (Table 1). This weakly-learned model loses to the fully-supervised one only 2% in the accuracy and 4% in the per-class recall. Note that our model is on par with Vezhnevets et al. [23], who also reached 21% on that dataset with the same superpixels and features. The difference is they used only image-level annotation, while we used about 10% fully labelled images. However, their model is substantially more complicated: they use extremely-randomized hashing forest for non-linear feature transform, learn objectness and image-level priors, and connect superpixels of different images within the multi-image model. Since the LV-SSVM optimization problem is not convex, the algorithm may get stuck at local minima. We initialize the parameters of LV-SSVM by the parameters of the SSVM trained on the fully-labelled part of the dataset, if there is one."}, {"heading": "4.3 Adding bounding boxes and seeds", "text": "Generating weak annotation. We generate two more annotations for the MSRC training data to test additional annotation-specific loss functions. Similar to image-level labels, we generate them from the full labelling. Tight bounding boxes and object seeds are good for description of the object (\u2018thing\u2019) categories, while do not add much information beyond image-level labels for the background (\u2018stuff\u2019) categories. We divide the list of categories into two parts: background, which includes \u2018grass\u2019, \u2018sky\u2019, \u2018mountain\u2019, \u2018water\u2019, \u2018road\u2019, and \u2018other\u2019; and objects, which includes all other categories. There are two ambivalent categories\u2014\u2018building\u2019 and \u2018tree\u2019\u2014which can instantiate either a target object of a photograph, or background. We used the following heuristic for each image: consider tree and building as background iff there are other objects in the image. We enhanced the image-level labelling with either tight bounding boxes or object seeds for segments of object categories only. For the other categories, only image-level labels were available. To generate seeds,\n5http://shapovalov.ro/data/MSRC-weak-train-masks.zip\nfor each segment we took its pole of inaccessibility\u2014the point that maximizes its distance transform map.\nResults. Table 2 summarizes the results. When the full labelling is unavailable, both object seed and bounding box annotations give significant improvement over just image-level labels. Bounding boxes notably increase per-class recall: they help to better learn \u2018thing\u2019 categories, which are numerous and typically have smaller area. Overall, learning with bounding boxes only 5% inferior to learning on fully labelled data both in terms of the accuracy and per-class recall. Object seed annotation gave more modest increase in performance, though is easier to obtain. We used the value \u03b2 = 1 to balance the impact of image-level vs. bounding box (or seed) loss functions: they seem to provide equal contribution to the objective function; Fig. 2c supports that hypothesis.\nComparison to Kumar et al. [8]. Unfortunately, we cannot directly compare to Kumar et al. [8] since the type of input data for their framework is unorthodox. They use two different datasets to obtain segmentation maps (partial labellings) for the foreground and background categories, respectively. Our framework does not support this kind of annotation: we believe that it is easier to obtain segmentation for background and foreground categories using the same set of images. This eliminates the need to use the latent-variable SSVM for training the basic model; instead the global minimum of SSVM objective can be found efficiently. Also, when both image-level labels and bounding boxes (or seeds) are known for each weakly-annotated image, both background and foreground partial labellings can be inferred, and using latent-variable SSVM after adding weakly-annotated data is not necessary again. Thus, when given the data we use, the method of Kumar et al. [8] could look like this:\n\u2022 train SSVM using the fully-labelled part of the training set, \u2022 use the trained model to infer the labelling of all images consistent with the weak annota-\ntion,\n\u2022 train SSVM using the hallucinated labelling obtained in the previous step.\nThis method is similar to running one outer iteration of our training algorithm, but it has one important difference: the loss function in the second SSVM. While our method uses the weak loss function, the modified method of Kumar et al. [8] uses the strong loss function w.r.t. the hallucinated labelling. To compare the methods, we use the MSRC training set with 5 fully-labelled images and the rest annotated with bounding boxes and image-level labels (row 4, column 2 in Table 2, excluding headers) to train both described modifications: with the weak bounding-box loss function (13), and with the strong loss function (4) (still different from the loss function of Kumar et al. [8]). The segmentation maps and numerical results in Fig. 3 show that the proposed simultaneous minimization of loss functions is superior both in terms of accuracy and per-class recall."}, {"heading": "5 Conclusion", "text": "We presented the framework for learning structural classification from different types of annotations by minimizing annotation-specific loss functions. We applied it to semantic image segmentation by introducing weak loss functions for for image-level, bounding box, and object seed annotations. Usage of weakly-annotated training data consistently improves the labelling. The results on the semantic segmentation datasets show that the joint annotation where background is given by imagelevel labels, and objects are given by bounding boxes, is the best trade-off between segmentation quality and annotation effort."}], "references": [{"title": "Contour detection and hierarchical image segmentation", "author": ["P. Arbel\u00e1ez", "M. Maire", "C. Fowlkes", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Structured output learning with indirect supervision", "author": ["Chang", "M.-W", "V. Srikumar", "D. Goldwasser", "D. Roth"], "venue": "In International Conference on Machine Learning", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Fast Approximate Energy Minimization with Label Costs", "author": ["A. Delong", "A. Osokin", "H.N. Isack", "Y. Boykov"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Efficient Graph-Based Image Segmentation", "author": ["P.F. Felzenszwalb", "D.P. Huttenlocher"], "venue": "International Journal of Computer Vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Learning spatial context: Using stuff to find things", "author": ["G. Heitz", "D. Koller"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Cutting-plane training of structural SVMs", "author": ["T. Joachims", "T. Finley", "C. Yu"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Learning specific-class segmentation from diverse data", "author": ["M.P. Kumar", "H. Turki", "D. Preston", "D. Koller"], "venue": "In IEEE International Conference on Computer Vision,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Image segmentation with a bounding box prior", "author": ["V. Lempitsky", "P. Kohli", "C. Rother", "T. Sharp"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Rapid and accurate large-scale coestimation of sequence alignments and phylogenetic trees", "author": ["K. Liu", "S. Raghavan", "S. Nelesen", "C.R. Linder", "T. Warnow"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Structured Learning from Partial Annotations", "author": ["X. Lou", "F.A. Hamprecht"], "venue": "In International Conference on Machine Learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Distinctive Image Features from Scale-Invariant Keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Learning low-order models for enforcing high-order statistics", "author": ["P. Pletscher", "P. Kohli"], "venue": "In International Conference on Artificial Intelligence and Statistics", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Semantic texton forests for image categorization and segmentation", "author": ["J. Shotton", "M. Johnson", "R. Cipolla"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Textonboost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation", "author": ["J. Shotton", "J. Winn", "C. Rother", "A. Criminisi"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Structured Output Learning with High Order Loss Functions", "author": ["D. Tarlow", "R.S. Zemel"], "venue": "In International Conference on Artificial Intelligence and Statistics", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Learning associative Markov networks", "author": ["B. Taskar", "V. Chatalbashev", "D. Koller"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "SuperParsing: Scalable Nonparametric Image Parsing with Superpixels", "author": ["J. Tighe", "S. Lazebnik"], "venue": "In European Conference on Computer", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "LabelMe: Online Image Annotation and Applications", "author": ["A. Torralba", "B.C. Russel", "J. Yuen"], "venue": "Proceedings of the IEEE,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Efficient Additive Kernels via Explicit Feature Maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Weakly Supervised Semantic Segmentation with a Multi-Image Model", "author": ["A. Vezhnevets", "V. Ferrari", "J.M. Buhmann"], "venue": "In IEEE International Conference on Computer", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Weakly Supervised Structured Output Learning for Semantic Segmentation", "author": ["A. Vezhnevets", "V. Ferrari", "J.M. Buhmann"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Learning structural SVMs with latent variables", "author": ["Yu", "C.-N. J", "T. Joachims"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "The concave-convex procedure (CCCP)", "author": ["A. Yuille", "A. Rangarajan"], "venue": "In NIPS", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}], "referenceMentions": [{"referenceID": 14, "context": "Figure 1: Types of annotation for an image from the MSRC dataset [15]", "startOffset": 65, "endOffset": 69}, {"referenceID": 5, "context": "\u2018things\u2019 in terms of Heitz and Koller [6]) are better described by bounding-box annotations, while the background categories (i.", "startOffset": 38, "endOffset": 41}, {"referenceID": 5, "context": "\u2018stuff\u2019 [6])\u2014which tend to fill significant parts of an image\u2014 by image-level labels.", "startOffset": 8, "endOffset": 11}, {"referenceID": 21, "context": "[22, 23] use a multi-image probabilistic graphical model to propagate image-level annotations across different training images.", "startOffset": 0, "endOffset": 8}, {"referenceID": 22, "context": "[22, 23] use a multi-image probabilistic graphical model to propagate image-level annotations across different training images.", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "Our work extends recent research on using latent-variable structural support vector machines (LV-SSVM) for weakly-supervised learning [3, 8, 11] by incorporating annotation-specific loss functions, which measure the inconsistency of some labelling predicted by the algorithm with the ground-truth weak annotation.", "startOffset": 134, "endOffset": 144}, {"referenceID": 7, "context": "Our work extends recent research on using latent-variable structural support vector machines (LV-SSVM) for weakly-supervised learning [3, 8, 11] by incorporating annotation-specific loss functions, which measure the inconsistency of some labelling predicted by the algorithm with the ground-truth weak annotation.", "startOffset": 134, "endOffset": 144}, {"referenceID": 10, "context": "Our work extends recent research on using latent-variable structural support vector machines (LV-SSVM) for weakly-supervised learning [3, 8, 11] by incorporating annotation-specific loss functions, which measure the inconsistency of some labelling predicted by the algorithm with the ground-truth weak annotation.", "startOffset": 134, "endOffset": 144}, {"referenceID": 7, "context": "[8], who use a sequential method to learn semantic segmentation from different types of annotations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8], at the training stage we minimize our annotation-specific loss functions simultaneously.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "This relates us to the recent work on supervised learning with non-decomposable loss functions [13, 16].", "startOffset": 95, "endOffset": 103}, {"referenceID": 15, "context": "This relates us to the recent work on supervised learning with non-decomposable loss functions [13, 16].", "startOffset": 95, "endOffset": 103}, {"referenceID": 12, "context": "Pletscher and Kohli [13] use a higher-order loss function that penalizes the difference in the area of the target category between binary segmentations.", "startOffset": 20, "endOffset": 24}, {"referenceID": 15, "context": "Tarlow and Zemel [16]", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": "In this paper we follow the maxmargin formulation of structured-output learning (also called structural support vector machine, SSVM) [17, 20, 7]:", "startOffset": 134, "endOffset": 145}, {"referenceID": 19, "context": "In this paper we follow the maxmargin formulation of structured-output learning (also called structural support vector machine, SSVM) [17, 20, 7]:", "startOffset": 134, "endOffset": 145}, {"referenceID": 6, "context": "In this paper we follow the maxmargin formulation of structured-output learning (also called structural support vector machine, SSVM) [17, 20, 7]:", "startOffset": 134, "endOffset": 145}, {"referenceID": 12, "context": "the individual variables [13, 16, 4].", "startOffset": 25, "endOffset": 36}, {"referenceID": 15, "context": "the individual variables [13, 16, 4].", "startOffset": 25, "endOffset": 36}, {"referenceID": 3, "context": "the individual variables [13, 16, 4].", "startOffset": 25, "endOffset": 36}, {"referenceID": 19, "context": "Problem (2)\u2013(3) is convex and can be solved by the cutting-plane method [20, 7].", "startOffset": 72, "endOffset": 79}, {"referenceID": 6, "context": "Problem (2)\u2013(3) is convex and can be solved by the cutting-plane method [20, 7].", "startOffset": 72, "endOffset": 79}, {"referenceID": 23, "context": "Note that for M = 0 the above formulation degenerates to the standard SSVM formulation, while for N = 0 it reduces to the latent-variable SSVM [24].", "startOffset": 143, "endOffset": 147}, {"referenceID": 23, "context": "We follow Yu and Joachims [24] and use the concave-convex procedure (CCCP) [25] to solve it approximately.", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "We follow Yu and Joachims [24] and use the concave-convex procedure (CCCP) [25] to solve it approximately.", "startOffset": 75, "endOffset": 79}, {"referenceID": 16, "context": "We restrict pairwise weights w and pairwise features xij to be nonnegative and thus obtain an associative discriminative function (with only attractive pairwise potentials) [17].", "startOffset": 173, "endOffset": 177}, {"referenceID": 1, "context": "\u03b1-expansion [2].", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "We use the efficient modification of \u03b1-expansion for accounting label costs [4].", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "[8], but it did not affect the performance significantly.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "We treat them the same way as the image-level loss: we modify \u03b1-expansion with label costs [4] to penalize each clique of superpixels, which contains at least one superpixel labelled with label(z).", "startOffset": 91, "endOffset": 94}, {"referenceID": 8, "context": "[9] showed that this definition is natural).", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "To provide tightness, we use a variation of the pinpointing algorithm [9], adapted for the multi-class segmentation.", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "[9], we do not perform further dilation, since it is unclear, which label we should use for expansion move(s); neither of the heuristics we tried improved the result significantly.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] used a different criterion during the annotation-consistent inference: they penalize the empty rows and columns within bounding boxes (the opposite to what we do in loss-augmented inference).", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "We test the proposed framework on two datasets: MSRCv23 [15, 22] and SIFT-flow4 [10, 18, 23].", "startOffset": 56, "endOffset": 64}, {"referenceID": 21, "context": "We test the proposed framework on two datasets: MSRCv23 [15, 22] and SIFT-flow4 [10, 18, 23].", "startOffset": 56, "endOffset": 64}, {"referenceID": 9, "context": "We test the proposed framework on two datasets: MSRCv23 [15, 22] and SIFT-flow4 [10, 18, 23].", "startOffset": 80, "endOffset": 92}, {"referenceID": 17, "context": "We test the proposed framework on two datasets: MSRCv23 [15, 22] and SIFT-flow4 [10, 18, 23].", "startOffset": 80, "endOffset": 92}, {"referenceID": 22, "context": "We test the proposed framework on two datasets: MSRCv23 [15, 22] and SIFT-flow4 [10, 18, 23].", "startOffset": 80, "endOffset": 92}, {"referenceID": 18, "context": "SIFT-flow is a more challenging dataset: it is a subset of the LabelMe database [19], which contains 2488 training and 200 test images; they are labelled to 33 categories using crowd-sourcing.", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": "For MSRC, we obtain superpixels using the original implementation of the gPb edge detector [1].", "startOffset": 91, "endOffset": 94}, {"referenceID": 11, "context": "The unary features are the following: a histogram of SIFT [12] visual words built using a dictionary of size 512 by hard assignment of the descriptors to the bins; a histogram of the RGB colors on a dictionary of size 128; a histogram of locations over a uniform 6\u00d76 grid.", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "We L2-normalize the joint feature vector and map it into a higher-dimensional space where the inner product approximates the \u03c7-kernel in the original space (the dimensionality of the space triples after the transformation) [21].", "startOffset": 223, "endOffset": 227}, {"referenceID": 22, "context": "[23] and obtain superpixels and features using the code by Tighe and Lazebnik [18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[23] and obtain superpixels and features using the code by Tighe and Lazebnik [18].", "startOffset": 78, "endOffset": 82}, {"referenceID": 4, "context": "It runs graph-based segmentation of Felzenszwalb and Huttenlocher [5] followed by feature extraction.", "startOffset": 66, "endOffset": 69}, {"referenceID": 21, "context": "Following the previous work [22, 14], we exclude the pixels of rare categories (\u2018horse\u2019 and \u2018mountain\u2019) from recall computation for MSRC, but include the \u2018other\u2019 label, see Section 4.", "startOffset": 28, "endOffset": 36}, {"referenceID": 13, "context": "Following the previous work [22, 14], we exclude the pixels of rare categories (\u2018horse\u2019 and \u2018mountain\u2019) from recall computation for MSRC, but include the \u2018other\u2019 label, see Section 4.", "startOffset": 28, "endOffset": 36}, {"referenceID": 22, "context": "[23], who also reached 21% on that dataset with the same superpixels and features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] applied to three images from the MSRC test set", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] since the type of input data for their framework is unorthodox.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] could look like this: \u2022 train SSVM using the fully-labelled part of the training set, \u2022 use the trained model to infer the labelling of all images consistent with the weak annotation, \u2022 train SSVM using the hallucinated labelling obtained in the previous step.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] uses the strong loss function w.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8]).", "startOffset": 0, "endOffset": 3}], "year": 2014, "abstractText": "Structured-output learning is a challenging problem; particularly so because of the difficulty in obtaining large datasets of fully labelled instances for training. In this paper we try to overcome this difficulty by presenting a multi-utility learning framework for structured prediction that can learn from training instances with different forms of supervision. We propose a unified technique for inferring the loss functions most suitable for quantifying the consistency of solutions with the given weak annotation. We demonstrate the effectiveness of our framework on the challenging semantic image segmentation problem for which a wide variety of annotations can be used. For instance, the popular training datasets for semantic segmentation are composed of images with hard-to-generate full pixel labellings, as well as images with easy-to-obtain weak annotations, such as bounding boxes around objects, or image-level labels that specify which object categories are present in an image. Experimental evaluation shows that the use of annotation-specific loss functions dramatically improves segmentation accuracy compared to the baseline system where only one type of weak annotation is used.", "creator": "LaTeX with hyperref package"}}}