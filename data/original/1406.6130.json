{"id": "1406.6130", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2014", "title": "Generalized Mixability via Entropic Duality", "abstract": "Mixability is a property of a loss which characterizes when fast convergence is possible in the game of prediction with expert advice. We show that a key property of mixability generalizes, and the exp and log operations present in the usual theory are not as special as one might have thought. In doing this we introduce a more general notion of $\\Phi$-mixability where $\\Phi$ is a general entropy (\\ie, any convex function on probabilities). We show how a property shared by the convex dual of any such entropy yields a natural algorithm (the minimizer of a regret bound) which, analogous to the classical aggregating algorithm, is guaranteed a constant regret when used with $\\Phi$-mixable losses. We characterize precisely which $\\Phi$ have $\\Phi$-mixable losses and put forward a number of conjectures about the optimality and relationships between different choices of entropy.", "histories": [["v1", "Tue, 24 Jun 2014 03:31:16 GMT  (231kb,D)", "http://arxiv.org/abs/1406.6130v1", "20 pages, 1 figure. Supersedes the work inarXiv:1403.2433[cs.LG]"]], "COMMENTS": "20 pages, 1 figure. Supersedes the work inarXiv:1403.2433[cs.LG]", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mark d reid", "rafael m frongillo", "robert c williamson", "nishant mehta"], "accepted": false, "id": "1406.6130"}, "pdf": {"name": "1406.6130.pdf", "metadata": {"source": "CRF", "title": "Generalized Mixability via Entropic Duality", "authors": ["Mark D. Reid", "Rafael M. Frongillo", "Robert C. Williamson", "Nishant Mehta"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The combination or aggregation of predictions is central to machine learning. Traditional Bayesian updating can be viewed as a particular way of aggregating information that takes account of prior information. Notions of \u201cmixability\u201d which play a key role in the setting of prediction with expert advice offer a more general way to aggregate by taking into account a loss function to evaluate predictions. As shown by Vovk [1], his more general \u201caggregating algorithm\u201d reduces to Bayesian updating when log loss is used. However there is an implicit design variable in mixability that to date has not been fully exploited. The aggregating algorithm makes use of a distance between the current distribution and a prior which serves as a regularizer. In particular the aggregating algorithm uses the KL-divergence. We consider the general setting of an arbitrary loss and an arbitrary regularizer (in the form of a Bregman divergence) and show that we recover the core technical result of traditional mixability: if a loss is mixable in our generalized sense then there is a generalized aggregating algorithm which can be guaranteed to have constant regret. The generalized aggregating algorithm is developed by optimizing the bound that defines our new notion of mixability. Our approach relies heavily on dual representations of entropy functions defined on the probability simplex\nar X\niv :1\n40 6.\n61 30\nv1 [\ncs .L\nG ]\n2 4\n(hence the title). By doing so we gain new insight into why the original mixability argument works and a broader understanding of when constant regret guarantees are possible."}, {"heading": "1.1 Mixability in Prediction With Expert Advice Games", "text": "A prediction with expert advice game is defined by its loss, a collection of experts that the player must compete against, and a fixed number of rounds. Each round the experts reveal their predictions to the player and then the player makes a prediction. An observation is then revealed to the experts and the player and all receive a penalty determined by the loss. The aim of the player is to keep its total loss close to that of the best expert once all the rounds have completed. The difference between the total loss of the player and the total loss of the best expert is called the regret and is the typically the focus of the analysis of this style of game. In particular, we are interested in when the regret is constant, that is, independent of the number of rounds played.\nMore formally, let X denote a set of possible observations and let A denote a set of actions or predictions the experts and player can perform. A loss ` : A \u2192 RX assigns the penalty `x(a) to predicting a \u2208 A when x \u2208 X is observed. The set of experts is denoted \u0398 and the set of distributions over \u0398 is denoted \u2206\u0398. In each round t = 1, . . . , T , each expert \u03b8 \u2208 \u0398 makes a prediction at\u03b8 \u2208 A. These are revealed to the player who makes a prediction a\u0302t \u2208 A. Once observation xt \u2208 X is revealed the experts receive loss `xt(at\u03b8) and the player receives loss `xt(a\u0302\nt). The aim of the player is to minimize its regret Regret(T ) := LT \u2212 min\u03b8 LT\u03b8 where LT := \u2211T t=1 `xt(a\u0302 t)\nand LT\u03b8 = \u2211T t=1 `xt(a t \u03b8). We will say the game has constant regret if there exists a player who can always make predictions that guarantee Regret(T ) \u2264 R`,\u0398 for all T and all expert predictions {at\u03b8}Tt=1 where R`,\u0398 is a constant that may depend on ` and \u0398.\nIn [2, 3], Vovk showed that if the loss for a game satisfies a condition called mixability then a player making predictions using the aggregating algorithm (AA) will achieve constant regret.\nDefinition 1 (Mixability and the Aggregating Algorithm). Given \u03b7 > 0, a loss ` : A \u2192 RX is \u03b7-mixable if, for all expert predictions a\u03b8 \u2208 A, \u03b8 \u2208 \u0398 and all mixture distributions \u00b5 \u2208 \u2206\u0398 over experts there exists a prediction a\u0302 \u2208 A such that for all outcomes x \u2208 X we have\n`x(a\u0302) \u2264 \u2212\u03b7\u22121 log \u2211\n\u03b8\u2208\u0398\nexp (\u2212\u03b7`x(a\u03b8))\u00b5\u03b8. (1)\nThe aggregating algorithm starts with a mixture \u00b50 \u2208 \u2206\u0398 over experts. In round t, experts predict at\u03b8 and the player predicts the a\u0302\nt \u2208 A guaranteed by the \u03b7-mixability of ` so that (1) holds for \u00b5 = \u00b5t\u22121 and a\u03b8 = at\u03b8. Upon observing x\nt, the mixture \u00b5t \u2208 \u2206\u0398 is set so that \u00b5t\u03b8 \u221d \u00b5t\u22121\u03b8 e\u2212\u03b7`xt (a t \u03b8).\nMixability can be seen as a weakening of exp-concavity (see [4, \u00a73.3]) that requires just enough of the loss to ensure constant regret.\nTheorem 1 (Mixability implies constant regret [3]). If a loss ` is \u03b7-mixable then the aggregating algorithm will achieve Regret(T ) = \u03b7\u22121 log |\u0398|."}, {"heading": "1.2 Contributions", "text": "The key contributions of this paper are as follows. We provide a new general definition (Definition 2) of mixability and an induced generalized aggregating algorithm (Definition 3) and show (Theorem 2) that prediction with expert advice using a \u03a6-mixable loss and the associated generalized aggregating algorithm is guaranteed to have constant regret. The proof illustrates that the log and exp functions that arise in the classical aggregating algorithm are themselves not special, but rather it is a translation invariant property of the convex conjugate of and entropy \u03a6 defined on a probability simplex that is the crucial property that leads to constant regret.\nWe characterize (Theorem 4) for which entropies \u03a6 there exists \u03a6-mixable losses via the Legendre property. We show that \u03a6-mixability of a loss can be expressed directly in terms of the Bayes risk associated with the loss (Definition 4 and Theorem 3), reflecting the situation that holds for classical mixability [5]. As part of this analysis we show that proper losses are quasi-convex (Lemma 6) which, to the best of our knowledge appears to be a new result."}, {"heading": "1.3 Related Work", "text": "The starting point for mixability and the aggregating algorithm is the work of [3, 2]. The general setting of prediction with expert advice is summarized in [4, Chapters 2 and 3]. There one can find a range of results that study different aggregation schemes and different assumptions on the losses (exp-concave, mixable). Variants of the aggregating algorithm have been studied for classically mixable losses, with a trade-off between tightness of the bound (in a constant factor) and the computational complexity [6]. Weakly mixable losses are a generalization of mixable losses. They have been studied in [7] where it is shown there exists a variant of the aggregating algorithm that achieves regret C \u221a T for some constant C. Vovk [1, in \u00a72.2] makes the observation that his Aggregating Algorithm reduces to Bayesian mixtures in the case of the log loss game. See also the discussion in [4, page 330] relating certain aggregation schemes to Bayesian updating.\nThe general form of updating we propose is similar to that considered by Kivinen and Warmuth [8] who consider finding a vector w minimizing d(w, s) + \u03b7L(yt, w \u00b7xt) where s is some starting vector, (xt, yt) is the instance/label observation at round t and L is a loss. The key difference between their formulation and ours is that our loss term is (in their notation) w \u00b7 L(yt, xt) \u2013 i.e., the linear combination of the losses of the xt at yt and not the loss of their inner product. Online methods of density estimation for exponential families are discussed in [9, \u00a73] where the authors compare the online and offline updates of the same sequence and make heavy use of the relationship between the KL divergence between members of an exponential family and an associated Bregman divergence between the parameters of those members. The analysis of mirror descent [10] shows that it achieves constant regret when the entropic regularizer is\nused. However, there is no consideration regarding whether similar results extend to other entropies defined on the simplex.\nWe stress that the idea of the more general regularization and updates is hardly new. See for example the discussion of potential based methods in [4] and other references later in the paper. The key novelty is the generalized notion of mixability, the name of which is justified by the key new technical result \u2014 a constant regret bound assuming the general mixability condition achieved via a generalized algorithm which can be seen as intimately related to mirror descent. Crucially, our result depends on some properties of the conjugates of potentials defined over probabilities that do not hold for potential functions defined over more general spaces."}, {"heading": "2 Generalized Mixability and Aggregation via Convex Duality", "text": "In this section we introduce our generalizations of mixability and the aggregating algorithm. One feature of our approach is the way the generalized aggregating algorithm falls out of the definition of generalized mixability as the minimizer of the mixability bound. Our approach relies on concepts and results from convex analysis. Terms not defined below can be found in a reference such as [11]."}, {"heading": "2.1 Definitions and Notation", "text": "A convex function \u03a6 : \u2206\u0398 \u2192 R is called an entropy (on \u2206\u0398) if it is proper (i.e., \u2212\u221e < \u03a6 6= +\u221e), convex1, and lower semi-continuous. In the following example and elsewhere we use 1 to denote the vector 1\u03b8 = 1 for all \u03b8 \u2208 \u0398 so that |\u0398|\u221211 \u2208 \u2206\u0398 is the uniform distribution over \u0398. Example 1 (Entropies). The (negative) Shannon entropy H(\u00b5) := \u2211 \u03b8 \u00b5\u03b8 log\u00b5\u03b8;\nthe quadratic entropy Q(\u00b5) := \u2211 \u03b8(\u00b5 \u2212 |\u0398|\u221211)2; the Tsallis entropies S\u03b1(\u00b5) :=\n\u03b1\u22121 (\u2211\n\u03b8 \u00b5 \u03b1+1 \u03b8 \u2212 1 ) for \u03b1 \u2208 (\u22121, 0) \u222a (0,\u221e); and the Re\u0301nyi entropies R\u03b1(\u00b5) =\n\u03b1\u22121 ( log \u2211 \u03b8 \u00b5 \u03b1+1 \u03b8 ) , for \u03b1 \u2208 (\u22121, 0). We note that both Tsallis and Re\u0301nyi entropies limit to Shannon entropy \u03b1\u2192 0 (cf. [12, 13]). Let \u3008\u00b5, v\u3009 denote the inner product between \u00b5 \u2208 \u2206\u0398 and v \u2208 \u2206\u2217\u0398, the dual space of \u2206\u0398. The Bregman divergence associated with a suitably differentiable entropy \u03a6 on \u2206\u0398 is given by\nD\u03a6(\u00b5, \u00b5 \u2032) = \u03a6(\u00b5)\u2212 \u03a6(\u00b5\u2032)\u2212 \u3008\u00b5\u2212 \u00b5\u2032,\u2207\u03a6(\u00b5\u2032)\u3009 (2)\nfor all \u00b5 \u2208 \u2206\u0398 and \u00b5\u2032 \u2208 ri(\u2206\u0398), the relative interior of \u2206\u0398. Given an entropy \u03a6 : \u2206\u0398 \u2192 R, we define its entropic dual to be \u03a6\u2217(v) := sup\u00b5\u2208\u2206\u0398 \u3008\u00b5, v\u3009 \u2212 \u03a6(\u00b5) where v \u2208 \u2206\u2217\u0398, i.e., the dual space to \u2206\u0398. Note that one could also write the supremum over R\u0398 by setting \u03a6(\u00b5) = +\u221e for \u00b5 /\u2208 \u2206\u0398 so that \u03a6\u2217 is just the usual convex dual\n1 While the information theoretic notion of Shannon entropy as a measure of uncertainty is concave, it is convenient for us to work with convex functions on the simplex which can be thought of as certainty measures.\n(cf. [11]). Thus, all of the standard results about convex duality also hold for entropic duals provided some care is taken with the domain of definition. We note that although the regular convex dual ofH defined over all of R\u0398 is v 7\u2192\u2211\u03b8 exp(v\u03b8\u22121) its entropic dual is H\u2217(v) = log \u2211 \u03b8 exp(v\u03b8).\nFor differentiable \u03a6, it is known [11] that the supremum defining \u03a6\u2217 is attained at \u00b5 = \u2207\u03a6\u2217(v). That is,\n\u03a6\u2217(v) = \u3008\u2207\u03a6\u2217(v), v\u3009 \u2212 \u03a6(\u2207\u03a6\u2217(v)). (3)\nA similar result holds for \u03a6 by applying this result to \u03a6\u2217 and using \u03a6 = (\u03a6\u2217)\u2217. We will make repeated use of two easy established properties of entropic duals (see Appendix A.1 for proof).\nLemma 1. If \u03a6 is an entropy over \u2206\u0398 and \u03a6\u03b7 := \u03b7\u22121\u03a6 denotes a scaled version of \u03a6 then 1) for all \u03b7 > 0 we have \u03a6\u2217\u03b7(v) = \u03b7\n\u22121\u03a6\u2217(\u03b7v); and 2) the entropic dual \u03a6\u2217 is translation invariant \u2013 i.e., for all v \u2208 \u2206\u2217\u0398 and \u03b1 \u2208 R we have \u03a6\u2217(v+\u03b11) = \u03a6\u2217(v)+\u03b1 and hence for differentiable \u03a6\u2217 we have\u2207\u03a6\u2217(v + \u03b11) = \u2207\u03a6\u2217(v).\nThe translation invariance if \u03a6\u2217 is central to our analysis. It is what ensures our \u03a6-mixability inequality (4) \u201ctelescopes\u201d when it is summed. The proof of the original mixability result (Theorem 1) uses a similar telescoping argument that works due to the interaction of log and exp terms in Definition 1. Our results show that this telescoping property is not due to any special properties of log and exp, but rather because of the translation invariance of the entropic dual of Shannon entropy, H . The following analysis generalizes that of the original work on mixability precisely because this property holds for the dual of any entropy."}, {"heading": "2.2 \u03a6-Mixability and the Generalized Aggregating Algorithm", "text": "For convenience, we will use A \u2208 A\u0398 to denote a collection of expert predictions and A\u03b8 \u2208 A to denote the prediction of expert \u03b8. Abusing notation slightly, we will write `(A) \u2208 RX\u00d7\u0398 for the matrix of loss values [`x(A\u03b8)]x,\u03b8, and `x(A) = [`x(A\u03b8)]\u03b8 \u2208 R\u0398 for the vector of losses for each expert \u03b8 on outcome x.\nDefinition 2 (\u03a6-mixability). Let \u03a6 be an entropy on \u2206\u0398. A loss ` : A \u2192 RX is \u03a6mixable if for all A \u2208 A\u0398, all \u00b5 \u2208 \u2206\u0398, there exists an a\u0302 \u2208 A such that for all x \u2208 X\n`x(a\u0302) \u2264 Mix\u03a6`,x(A,\u00b5) := inf \u00b5\u2032\u2208\u2206\u0398 \u3008\u00b5\u2032, `x(A)\u3009+D\u03a6(\u00b5\u2032, \u00b5). (4)\nThe term on the right-hand side of (4) has some intuitive appeal. Since \u3008\u00b5\u2032, A\u3009 = E\u03b8\u223c\u00b5\u2032 [`x(A\u03b8)] (i.e., the expected loss of an expert drawn at random according to \u00b5\u2032) we can view the optimization as a trade off between finding a mixture \u00b5\u2032 that tracks the expert with the smallest loss upon observing outcome x and keeping \u00b5\u2032 close to \u00b5, as measured by D\u03a6. In the special case when \u03a6 is Shannon entropy, ` is log loss, and expert predictions A\u03b8 \u2208 \u2206X are distributions over X such an optimization is equivalent to Bayesian updating [14].\nTo see that \u03a6-mixability is indeed a generalization of Definition 1, we make use of an alternative form for the right-hand side of the bound in the \u03a6-mixability definition\nthat \u201chides\u201d the infimum inside \u03a6\u2217. As shown in Appendix A.1 this is a straightforward consequence of (3).\nLemma 2. The mixability bound\nMix\u03a6`,x(A,\u00b5) = \u03a6 \u2217(\u2207\u03a6(\u00b5))\u2212 \u03a6\u2217(\u2207\u03a6(\u00b5)\u2212 `x(A)). (5)\nHence, for \u03a6 = \u03b7\u22121H we have Mix\u03a6`,x(A,\u00b5) = \u2212\u03b7\u22121 log \u2211 \u03b8 exp(\u2212\u03b7`x(A\u03b8))\u00b5\u03b8 which is the bound in Definition 1.\nWe now define a generalization of the Aggregating Algorithm of Definition 1 that very naturally relates to our definition of \u03a6-mixability: starting with some initial distribution over experts, the algorithm repeatedly incorporates the information about the experts\u2019 performances by finding the minimizer \u00b5\u2032 in (4).\nDefinition 3 (Generalized Aggregating Algorithm). The algorithm begins with a mixture distribution \u00b50 \u2208 \u2206\u0398 over experts. On round t, after receiving expert predictions At \u2208 A\u0398, the generalized aggregating algorithm (GAA) predicts any a\u0302 \u2208 A such that `x(a\u0302) \u2264 Mix\u03a6`,x(At, \u00b5t\u22121) for all x which is guaranteed to exist by the \u03a6-mixability of `. After observing xt \u2208 X , the GAA updates the mixture \u00b5t\u22121 \u2208 \u2206\u0398 by setting\n\u00b5t := arg min \u00b5\u2032\u2208\u2206\u0398\n\u2329 \u00b5\u2032, `xt(A t) \u232a +D\u03a6(\u00b5 \u2032, \u00b5t\u22121). (6)\nWe now show that this updating process simply aggregates the per-expert losses `x(A) in the dual space \u2206\u2217\u0398 with \u2207\u03a6(\u00b50) as the starting point. The GAA is therefore closely related to mirror descent techniques [10].\nLemma 3. The GAA updates \u00b5t in (6) satisfy\u2207\u03a6(\u00b5t) = \u2207\u03a6(\u00b5t\u22121)\u2212 `xt(At) for all t and so\n\u2207\u03a6(\u00b5T ) = \u2207\u03a6(\u00b50)\u2212 T\u2211\nt=1\n`xt(A t). (7)\nThe proof is given in Appendix A.1. Finally, to see that the above is indeed a generalization of the Aggregating Algorithm from Definition 1 we need only apply Lemma 3 and observe that for \u03a6 = \u03b7\u22121H we have\u2207\u03a6(\u00b5) = \u03b7\u22121(log(\u00b5) + 1) and so log\u00b5t = log\u00b5t\u22121 \u2212 \u03b7`xt(At). Exponentiating this vector equality element-wise gives \u00b5t\u03b8 \u221d \u00b5t\u22121\u03b8 exp(\u2212\u03b7`xt(At\u03b8))."}, {"heading": "3 Properties of \u03a6-mixability", "text": "In this section we establish a number of key properties for \u03a6-mixability, the most important of these being that \u03a6-mixability implies constant regret. We also show that \u03a6-mixability is not a vacuous concept for \u03a6 other than Shannon entropy by showing that any Legendre \u03a6 has \u03a6-mixable losses and that this is a necessary condition for such losses to exist."}, {"heading": "3.1 \u03a6-mixability Implies Constant Regret", "text": "Theorem 2. If ` : A \u2192 RX is \u03a6-mixable then there is a family of strategies parameterized by \u00b5 \u2208 \u2206\u0398 which, for any sequence of observations x1, . . . , xT \u2208 X and sequence of expert predictions A1, . . . , AT \u2208 A\u0398, plays a sequence a\u03021, . . . , a\u0302T \u2208 A such that for all \u03b8 \u2208 \u0398\nT\u2211\nt=1\n`xt(a\u0302 t) \u2264\nT\u2211\nt=1\n`xt(A t \u03b8) +D\u03a6(\u03b4\u03b8, \u00b5). (8)\nThe proof is in Appendix A.2 and is a straight-forward consequence of Lemma 2 and the translation invariance of \u03a6\u2217. The standard notion of mixability is recovered when \u03a6 = 1\u03b7H for \u03b7 > 0 andH the Shannon entropy on \u2206\u0398. In this case, Theorem 1 is obtained as a corollary for \u00b5 = |\u0398|\u221211, the uniform distribution over \u0398. A compelling feature of our result is that it gives a natural interpretation of the constant D\u03a6(\u03b4\u03b8, \u03c0) in the regret bound: if \u03c0 is the initial guess as to which expert is best before the game starts, the \u201cprice\u201d that is paid by the player is exactly how far (as measured by D\u03a6) the initial guess was from the distribution that places all its mass on the best expert.\nThe following example computes mixability bounds for the alternative entropies introduced in \u00a72.1. They will be discussed again in \u00a74.2 below.\nExample 2. Consider games with K = |\u0398| experts and \u00b5 = K\u221211. For the (negative) Shannon entropy, the regret bound from Theorem 2 is DH(\u03b4\u03b8, \u00b5) = logK. For quadratic entropy the regret bound is DQ(\u03b4\u03b8, \u00b5) = 1 \u2212 2(K\u22121)K2 . For the family of Tsallis entropies the regret bound given by DS\u03b1(\u03b4\u03b8,K\n\u221211) = \u03b1\u22121(1 \u2212 K\u2212\u03b1). For the family of Re\u0301nyi entropies the regret bound becomes DR\u03b1(\u03b4\u03b8,K \u221211) = logK.\nA second, easily established result concerns the mixability of scaled entropies. The proof is by observing that in (4) the only term in the definition of Mix\u03a6\u03b7`,x involving \u03b7 is D\u03a6\u03b7 = 1 \u03b7D\u03a6. The quantification over A,\u00b5, a\u0302, \u00b5\n\u2032 and x in the original definition have been translated into infima and suprema.\nLemma 4. The function M(\u03b7) := infA,\u00b5 supa\u0302 inf\u00b5\u2032,x Mix \u03a6\u03b7 `,x(A,\u00b5) \u2212 `x(a\u0302) is nonincreasing.\nThis implies that there is a well-defined maximal \u03b7 > 0 for which a given loss ` is \u03a6\u03b7-mixable since \u03a6\u03b7-mixability is equivalent to M(\u03b7) \u2265 0. We will call this maximal \u03b7 the \u03a6-mixability constant for ` and denote it \u03b7(`,\u03a6) := sup{\u03b7 > 0 : M(\u03b7) \u2265 0}. This constant is central to the discussion in Section 4.3 below."}, {"heading": "3.2 \u03a6-Mixability of Proper Losses and Their Bayes Risks", "text": "Entropies are known to be closely related to the Bayes risk of what are called proper losses or proper scoring rules [15, 16]. Here, the predictions are distributions over outcomes, i.e., points in \u2206X . To highlight this we will use p, p\u0302 and P instead of a, a\u0302 and A to denote actions. If a loss ` : \u2206X \u2192 RX is used to assign a penalty `x(p\u0302) to a\nprediction p\u0302 upon outcome x it is said to be proper if its expected value under x \u223c p is minimized by predicting p\u0302 = p. That is, for all p, p\u0302 \u2208 \u2206X we have\nEx\u223cp [`x(p\u0302)] = \u3008p, `(p\u0302)\u3009 \u2265 \u3008p, `(p)\u3009 =: \u2212F `(p)\nwhere \u2212F ` is the Bayes risk of ` and is necessarily concave [5], thus making F ` : \u2206X \u2192 R convex and thus an entropy. The correspondence also goes the other way: given any convex function F : \u2206X \u2192 R we can construct a unique proper loss [17]. The following representation can be traced back to [18] but is expressed here using convex duality.\nLemma 5. If F : \u2206X \u2192 R is a differentiable entropy then the loss `F : \u2206X \u2192 R defined by `F (p) := F \u2217(\u2207F (p))1\u2212\u2207F (p) (9) is proper.\nIt is straight-forward to show that the proper loss associated with the negative Shannon entropy \u03a6 = H is the log loss, that is, `H(\u00b5) := \u2212 (log\u00b5(\u03b8))\u03b8\u2208\u0398.\nThis connection between losses and entropies lets us define the \u03a6-mixability of a proper loss strictly in terms of its associated entropy. This is similar in spirit to the result in [5] which shows that the original mixability (for \u03a6 = H) can be expressed in terms of the relative curvature of Shannon entropy and the loss\u2019s Bayes risk. We use the following definition to explore the optimality of Shannon mixability in Section 4.3 below.\nDefinition 4. An entropy F : \u2206X \u2192 R is \u03a6-mixable if\nsup P,\u00b5\nF \u2217 ({ \u03a6\u2217(\u2207\u03a6(\u00b5)\u2212 `Fx (P )) } x \u2212 \u03a6\u2217(\u2207\u03a6(\u00b5))1 ) \u2264 0 (10)\nwhere `F is as in Lemma 5 and the supremum is over expert predictions P \u2208 \u2206\u0398X and mixtures over experts \u00b5 \u2208 \u2206\u0398.\nAlthough this definition appears complicated due to the handling of vectors in RX and R\u0398, it has a natural interpretation in terms of risk measures from mathematical finance [19]. Given some convex function \u03b1 : \u2206X \u2192 R, its associated risk measure is its dual \u03c1(v) := supp\u2208\u2206X \u3008p,\u2212v\u3009 \u2212 \u03b1(p) = \u03b1\u2217(\u2212v) where v is a position meaning vx is some monetary value associated with outcome x occurring. Due to its translation invariance, the quantity \u03c1(v) is often interpreted as the amount of \u201ccash\u201d (i.e., outcome independent value) an agent would ask for to take on the uncertain position v. Observe that the risk \u03c1F for when \u03b1 = F satisfies \u03c1F \u25e6 `F = 0 so that `F (p) is always a \u03c1F -risk free position. If we now interpret \u00b5\u2217 = \u2207\u03a6(\u00b5) as a position over outcomes in \u0398 and \u03a6\u2217 as a risk for \u03b1 = \u03a6 the term { \u03a6\u2217(\u00b5\u2217 \u2212 `Fx (P )) } x \u2212 \u03a6\u2217(\u00b5\u2217)1 can be seen as the change in \u03c1\u03a6 risk when shifting position \u00b5\u2217 to \u00b5\u2217 \u2212 `Fx (P ) for each possible outcome x. Thus, the mixability condition in (10) can be viewed as a requirement that a \u03c1F -risk free change in positions over \u0398 always be \u03c1F -risk free.\nThe following theorem shows that the entropic version of \u03a6-mixability Definition 4 is equivalent to the loss version in Definition 2 in the case of proper losses. Its proof can\nbe found in Appendix A.3 and relies on Sion\u2019s theorem and the facts that proper losses are quasi-convex. This latter fact appears to be new so we state it here as a separate lemma and prove it in Appendix A.1.\nLemma 6. If ` : \u2206X \u2192 R is proper then p\u2032 7\u2192 \u3008p, `(p\u2032)\u3009 is quasi-convex for all p \u2208 \u2206X . Theorem 3. If ` : \u2206X \u2192 RX is proper and has Bayes risk \u2212F then F is an entropy and ` is \u03a6-mixable if and only if F is \u03a6-mixable.\nThe entropic form of mixability in (10) shares some similarities with expressions for the classical mixability constants given in [20] for binary outcome games and in [5] for general games. Our expression for the mixability is more general than the previous two being both for binary and non-binary outcomes and for general entropies. It is also arguably more efficient since the optimization in [5] for non-binary outcomes requires inverting a Hessian matrix at each point in the optimization."}, {"heading": "3.3 Characterizing and Comparing \u03a6-mixability", "text": "Although Theorem 2 recovers the already known constant regret bound for Shannonmixable losses, it is natural to ask whether the result is vacuous or not for other entropies. That is, do there exist \u03a6-mixable losses for \u03a6 other than Shannon entropy? If so, do such \u03a6-mixable losses exist for any entropy \u03a6? The next theorem answers both of these questions, showing that the existence of \u201cnon-trivial\u201d \u03a6-mixable losses is intimately related to the behaviour of an entropy\u2019s gradient at the simplex\u2019s boundary. Specifically, an entropy \u03a6 is said to be Legendre [21] if: a) \u03a6 is strictly convex in int(\u2206\u0398); and b) \u2016\u2207\u03a6(\u00b5)\u2016 \u2192 \u221e as \u00b5\u2192 \u00b5b for any \u00b5b on the boundary of \u2206\u0398.\nWe will say a loss is non-trivial if there exist distinct actions which are optimal for distinct outcomes (see A.4 for formal definition). This, for example, rules out constant losses \u2013 i.e., `(a) = k \u2208 RX for all a \u2208 A \u2013 are easily2 seen to be \u03a6-mixable for any \u03a6. For technical reasons we will further restrict our attention to curved losses by which we mean those losses with strictly concave Bayes risks. We conjecture that the following theorem also holds for non-curved losses.\nTheorem 4. There exist non-trivial, curved \u03a6-mixable losses if and only if the entropy \u03a6 is Legendre.\nThe proof is in Appendix A.4. From this result we can deduce that there are no Q-mixable losses. Also, since it is easy to show the derivatives \u2207S\u03b1 and \u2207R\u03b1 are unbounded for \u03b1 \u2208 (0, 1), the entropies S\u03b1 and R\u03b1 are Legendre. Thus there exist S\u03b1and R\u03b1-mixable losses when \u03b1 \u2208 (\u22121, 0)."}, {"heading": "4 Conclusions and Open Questions", "text": "The main purpose of this work was to shed new light on mixability by casting it within the broader notion of \u03a6-mixability. We showed that the constant regret bounds enjoyed by mixability losses are due to the translation invariance of entropic duals, and\n2 The inequality in (4) reduces to 0 \u2264 inf\u00b5\u2032 D\u03a6(\u00b5\u2032, \u00b5) which is true for all Bregman divergences.\nso are also enjoyed by any \u03a6-mixable loss. The definitions and technical machinery presented here allow us to ask precise questions about entropies and the optimality of their associated aggregating algorithms."}, {"heading": "4.1 Are All Legendre Entropies \u201cEquivalent\u201d?", "text": "Since Theorem 4 shows the existence of \u03a6-mixable losses, a natural question concerns the relationship between the sets of losses that are mixable for different choices of \u03a6. For example, are there losses that are H-mixable but not S\u03b1-mixable, or vice-versa? We conjecture that essentially all Legendre entropies \u03a6 have the same \u03a6-mixable losses up to a scaling factor.\nConjecture 1. Let \u03a6 be a entropy on \u2206\u0398 and ` be a \u03a6-mixable loss. If \u03a8 is a Legendre entropy on \u2206\u0398 then there exists an \u03b7 > 0 such that ` is \u03b7\u22121\u03a8-mixable.\nSome intuition for this conjecture is derived from observing that Mix\u03a8\u03b7`,x = \u03b7 \u22121 Mix\u03a8\u03b7`,x\nand that as \u03b7 \u2192 0 the function \u03b7` behaves like a constant loss and will therefore be mixable. This means that scaling up Mix\u03a8\u03b7`,x by \u03b7\n\u22121 should make it larger than Mix\u03a6`,x. However, some subtlety arises in ensuring that this dominance occurs uniformly."}, {"heading": "4.2 Asymptotic Behaviour", "text": "There is a lower bound due to Vovk [3] for general losses ` which shows that if one is allowed to vary the number of rounds T and the number of experts K = |\u0398|, then no regret bound can be better than the optimal regret bound obtained by Shannon mixability. Specifically, for a fixed loss ` with optimal Shannon mixability constant \u03b7`, suppose that for some \u03b7\u2032 > \u03b7` we have a regret bound of the form (logK)/\u03b7\u2032 as well as some strategy L for the learner that supposedly satisfies this regret bound. Vovk\u2019s lower bound shows, for this \u03b7\u2032 and L, that there exists an instantiation of the prediction with expert advice game with T large enough and K roughly exponential in T (and both are still finite) for which the alleged regret bound will fail to hold at the end of the game with non-zero probability. The regime in which Vovk\u2019s lower bound holds suggests that the best achievable regret with respect to the number of experts grows as logK. Indeed, there is a lower bound for general losses ` that shows the regret of the best possible algorithm on games using ` must grow like \u2126(log2K) [20].\nThe above lower bound arguments apply when the number of experts is large (i.e., exponential in the number of rounds) or if we consider the dynamics of the regret bound as K grows. This leaves open the question of the best possible regret bound for moderate and possibly fixedK which we formally state in the next section. This question that serves as a strong motivation for the study of generalized mixability considered here. Note also that the above lower bounds are consistent with the fact that there cannot be non-trivial, \u03a6-mixable losses for non-Legendre \u03a6 (e.g., the quadratic entropy Q) since the growth of the regret bound as a function of K (cf. Example 2) is less than logK and hence violates the above lower bounds."}, {"heading": "4.3 Is There An \u201cOptimal\u201d Entropy?", "text": "Since we believe that \u03a6-mixability for Legendre \u03a6 yield the same set of losses, we can ask whether, for a fixed loss `, some \u03a6 give better regret bounds than others. These bounds depend jointly on the largest \u03b7 such that ` is \u03a6\u03b7-mixable and the value of D\u03a6(\u03b4\u03b8, \u00b5). We can define the optimal regret bound one can achieve for a particular loss ` using the generalized aggregating algorithm with \u03a6\u03b7 := 1\u03b7\u03a6 for some \u03b7 > 0. This allows us to compare entropies on particular losses, and we can say that an entropy dominates another if its optimal regret bound is better for all losses `. Recalling the definition of the maximal \u03a6-mixability constant from Lemma 4, we can determine a quantity of more direct interest: the best regret bound one can obtain using a scaled copy of \u03a6. Recall that if ` is \u03a6-mixable, then the best regret bound one can achieve from the generalized aggregating algorithm is inf\u00b5 sup\u03b8D\u03a6(\u03b4\u03b8, \u00b5). We can therefore define the best regret bound for ` on a scaled version of \u03a6 to be R`,\u03a6 := \u03b7(`,\u03a6)\n\u22121 inf\u00b5 sup\u03b8D\u03a6(\u03b4\u03b8, \u00b5) which simply corresponds to the regret bound for the entropy \u03a6\u03b7(`,\u03a6). Note a crucial property of R`,\u03a6, which will be very useful in comparing entropies: R`,\u03a6 = R`,\u03b1\u03a6 for all \u03b1 > 0. (This follows from the observation that \u03b7(`, \u03b1\u03a6) = \u03b7(`,\u03a6)/\u03b1.) That is, R`,\u03a6 is independent of the particular scaling we choose for \u03a6.\nWe can now use R`,\u03a6 to define a scale-invariant relation over entropies. Define \u03a6 \u2265` \u03a8 if R`,\u03a6 \u2264 R`,\u03a8, and \u03a6 \u2265\u2217 \u03a8 if \u03a6 \u2265` \u03a8 for all losses `. In the latter case we say \u03a6 dominates \u03a8. By construction, if one entropy dominates another its regret bound is guaranteed to be tighter and therefore its aggregating algorithm will achieve better worst-case regret. As discussed above, one natural candidate for a universally dominant entropy is the Shannon entropy.\nConjecture 2. For all choices of \u0398, the negative Shannon entropy dominates all other entropies. That is, H \u2265\u2217 \u03a6 for all \u0398 and all convex \u03a6 on \u2206\u0398.\nAlthough we have not been able to prove this conjecture we were able to collect some positive evidence in the form of Table 1. Here, we took the entropic form of \u03a6-mixability from Definition 4 and implemented3 it as an optimization problem in the language R and computed \u03b7(`F ,\u03a6) for F and \u03a6 equal to the entropies introduced in Example 1 for two expert games with two outcomes. The maximal \u03b7 (and hence the optimal regret bounds) for each pair was found doing a binary search for the zero-crossing of M(\u03b7) from Lemma 4 and then applying the bounds from Example 2. Although we were expecting the dominant entropy for each loss `F to be its \u201cmatching\u201d entropy (i.e., \u03a6 = F ), as can be seen from the table the optimal regret bound for every loss was obtained in the column forH . However, one interesting feature for these matching cases is that the optimal \u03b7 (shown in parentheses) is always equal to 1.\nConjecture 3. Suppose |X| = |\u0398| so that \u2206\u0398 = \u2206X . Given a Legendre \u03a6 : \u2206\u0398 \u2192 R and its associated proper loss `\u03a6 : \u2206X \u2192 RX , the maximal \u03b7 such that `\u03a6 is \u03b7\u22121\u03a6mixable is \u03b7 = 1.\nWe conjecture that this pattern will hold for matching entropies and losses for larger numbers of experts and outcomes and hope to test or prove this in future work.\n3 In order to preserve anonymity the code will not be made available until after publication."}, {"heading": "Acknowledgments", "text": "We would like to thank Matus Telgarsky for help with restricted duals, Brendan van Rooyen for noting that there are no quadratic mixable losses, and Harish Guruprasad for identifying a flaw in an earlier \u201cproof\u201d of the quasi-convexity of proper losses. Mark Reid is supported by an ARC Discovery Early Career Research Award (DE130101605) and part of this work was developed while he was visiting Microsoft Research. NICTA is funded by the Australian Government and as an ARC ICT Centre of Excellence."}, {"heading": "A Appendix", "text": "A.1 Proof of Lemmas Proof of Lemma 1. To show 1) we observe that (\u03b7\u22121\u03a6)\u2217(v) = supp \u3008v, p\u3009\u2212\u03b7\u22121\u03a6(p) = \u03b7\u22121 supp \u3008\u03b7v, p\u3009 \u2212 \u03a6(p) = \u03b7\u22121\u03a6\u2217(\u03b7v). For 2), we note that the definition of the dual implies \u03a6\u2217(v+\u03b11) = sup\u00b5\u2208\u2206\u0398 \u3008\u00b5, v + \u03b11\u3009\u2212\u03a6(\u00b5) = sup\u00b5\u2208\u2206\u0398 \u3008\u00b5, v\u3009\u2212\u03a6(\u00b5)+\u03b1 = \u03a6\u2217(v) + \u03b1 since \u3008\u00b5,1\u3009 = 1. Taking derivatives of both sides gives the final part of the lemma.\nProof of Lemma 2. By definition \u03a6\u2217(\u2207\u03a6(\u00b5) \u2212 v) = sup\u00b5\u2032\u2208\u2206\u0398 \u3008\u00b5\u2032,\u2207\u03a6(\u00b5)\u2212 v\u3009 \u2212 \u03a6(\u00b5\u2032) and using (3) gives \u03a6\u2217(\u2207\u03a6(\u00b5)) = \u3008\u00b5,\u2207\u03a6(\u00b5)\u3009 \u2212 \u03a6(\u00b5). Subtracting the former from the latter gives \u3008\u00b5,\u2207\u03a6(\u00b5)\u3009\u2212\u03a6(\u00b5)\u2212 [ sup\u00b5\u2032\u2208\u2206\u0398 \u3008\u00b5\u2032,\u2207\u03a6(\u00b5)\u2212 v\u3009 \u2212 \u03a6(\u00b5\u2032) ] which, when rearranged gives inf\u00b5\u2032\u2208\u2206\u0398 \u03a6(\u00b5 \u2032) \u2212 \u03a6(\u00b5) \u2212 \u3008\u2207\u03a6(\u00b5), \u00b5\u2032 \u2212 \u00b5\u3009 + \u3008\u00b5\u2032, v\u3009 establishing the result. When \u03a6 = H \u2013 i.e., \u03a6 is the (negative) Shannon entropy \u2013 we have that \u2207\u03a6(\u00b5) = log\u00b5 + 1, that \u03a6\u2217(v) = log \u2211 \u03b8 exp(v\u03b8), and so \u2207\u03a6\u2217(v) = exp(v)/ \u2211 \u03b8 exp(v\u03b8), where log and exp are interpreted as acting point-wise on the vector \u00b5. By Lemma 1, \u03a6\u2217(\u2207\u03a6(\u00b5)) = \u03a6\u2217(log\u00b5+1) = \u03a6\u2217(log(\u00b5))+1 = 1 since \u03a6\u2217(log(\u00b5\u03b8)) = log \u2211 \u03b8 \u00b5\u03b8 =\n0. Similarly, \u03a6\u2217(\u2207\u03a6(\u00b5)\u2212`x(A)) = \u03a6\u2217(log(\u00b5)\u2212`x(A))+1 = log \u2211 \u03b8 \u00b5\u03b8 exp(\u2212`x(A))+ 1. Substituting this into Lemma 2 and applying the second part of Lemma 1 shows that Mix\u03b7\n\u22121H `,x (A,\u00b5) = \u2212\u03b7\u22121 log \u2211 \u03b8 exp(\u2212\u03b7`x(A\u03b8)), recovering the right-hand side of\nthe inequality in Definition 1.\nProof of Lemma 5. By eq. (3) we have F \u2217(\u2207F (p)) = \u3008p,\u2207F (p)\u3009 \u2212 F (p), giving us \u2329 p, `F (p\u2032) \u232a \u2212 \u2329 p, `F (p) \u232a = ( \u3008p\u2032,\u2207F (p\u2032)\u3009 \u2212 F (p\u2032)\u2212 \u3008p,\u2207F (p\u2032)\u3009 )\n\u2212 ( \u3008p,\u2207F (p)\u3009 \u2212 F (p)\u2212 \u3008p,\u2207F (p)\u3009 )\n= DF (p, p \u2032),\nfrom which propriety follows.\nProof of Lemma 3. By considering the LagrangianL(\u00b5, a) = \u3008\u00b5, `xt(A)\u3009+D\u03a6(\u00b5, \u00b5t\u22121)+ \u03b1(\u3008\u00b5,1\u3009 \u2212 1) and setting its derivative to zero we see that the minimizing \u00b5t must satisfy \u2207\u03a6(\u00b5t) = \u2207\u03a6(\u00b5t\u22121) \u2212 `xt(At) \u2212 \u03b1t1 where \u03b1t \u2208 R is the dual variable at step t. For convex \u03a6, the functions \u2207\u03a6\u2217 and \u2207\u03a6 are inverses [11] so \u00b5t = \u2207\u03a6\u2217(\u2207\u03a6(\u00b5t\u22121) \u2212 `xt(At) \u2212 at1) = \u2207\u03a6\u2217(\u2207\u03a6(\u00b5t\u22121) \u2212 `xt(At)) by the translation invariance of \u03a6\u2217 (Lemma 1). This means the constants \u03b1t are arbitrary and can be ignored. Thus, the mixture updates satisfy the relation in the lemma and summing over t = 1, . . . , T gives (7).\nProof of Lemma 6. Let n = |X| and fix an arbitrary p \u2208 \u2206X . The function fp(q) = \u3008p, `(q)\u3009 is quasi-convex if its \u03b1 sublevel sets F\u03b1p := {q \u2208 \u2206X : \u3008p, `(q)\u3009 \u2264 \u03b1} are convex for all \u03b1 \u2208 R. Let g(p) := infq fp(q) and fix an arbitrary \u03b1 > g(p) so that F\u03b1p 6= \u2205. Let Q\u03b1p := {v \u2208 Rn : \u3008p, v\u3009 \u2264 \u03b1} so F\u03b1p = {q \u2208 \u2206X : `(q) \u2208 Q\u03b1p }.\nDenote by h\u03b2q := {v : \u3008v, q\u3009 = \u03b2} the hyperplane in direction q \u2208 \u2206X with offset \u03b2 \u2208 R and by H\u03b2q := {v : \u3008v, q\u3009 \u2265 \u03b2} the corresponding half-space. Since ` is proper, its superprediction set S` = {\u03bb \u2208 Rn : \u2203q \u2208 \u2206X\u2200x \u2208 X\u03bbx \u2265 `x(q)} (see [17, Prop. 17]) is supported at x = `(q) by the hyperplane hg(q)q and furthermore since S` is convex, S` = \u22c2 q\u2208\u2206X H g(q) q .\nLet V \u03b1p :=\n\u22c2\nv\u2208`(\u2206X)\u2229Q\u03b1p\nH g(`\u22121(v)) `\u22121(v) =\n\u22c2\nq\u2208F\u03b1p\nHg(q)q\n(see figure 1). Since V \u03b1p is the intersection of halfspaces it is convex. Note that a given half-spaceHg(q)q is supported by exactly one hyperplane, namely h g(q) q . Thus the set of hyperplanes that support V \u03b1p is {hg(q)q : q \u2208 F\u03b1p } If u \u2208 F\u03b1p then there is a hyperplane in direction u that supports V \u03b1p and its offset is given by\n\u03c3V \u03b1p (u) := infv\u2208V \u03b1p \u3008u, v\u3009 = g(p) > \u2212\u221e\nwhereas if u 6\u2208 F\u03b1p then for all \u03b2 \u2208 R, h\u03b2u does not support V \u03b1p and hence \u03c3V \u03b1p (u) = \u2212\u221e. Thus we have shown\n( u 6\u2208W\u03b1p ) \u21d4 ( \u03c3V \u03b1p (u) = \u2212\u221e ) .\nObserve that \u03c3V \u03b1p (u) = \u2212sV \u03b1p (\u2212u) where sC(u) = supv\u2208C \u3008u, v\u3009 is the support function of a set C. It is known [22, Theorem 5.1] that the \u201cdomain of definition\u201d of a support function {u \u2208 Rn : sC(u) < +\u221e} for a convex set C is always convex. Thus G\u03b1p := {u \u2208 \u2206X : \u03c3V \u03b1p (u) > \u2212\u221e} = {u \u2208 Rn : \u03c3V \u03b1p (u) > \u2212\u221e} \u2229 \u2206X is always convex because it is the intersection of convex sets. Finally by observing that\nG\u03b1p = {p \u2208 \u2206X : `(p) \u2208 `(\u2206X) \u2229Q\u03b1p } = F\u03b1p\nwe have shown that F\u03b1p is convex. Since p \u2208 \u2206X and \u03b1 \u2208 R were arbitrary we have thus shown that fp is quasi-convex for all p \u2208 \u2206X .\nA.2 Proof of Theorem 2 Proof of Theorem 2. Applying Lemma 2 to the assumption that ` is \u03a6-mixable means that for \u00b5 equal to the updates \u00b5t from Definition 3 and At equal to the expert predictions at round t, there must exist an a\u0302t \u2208 \u2206X such that\n`xt(a\u0302 t) \u2264 \u03a6\u2217(\u2207\u03a6(\u00b5t\u22121))\u2212 \u03a6\u2217(\u2207\u03a6(\u00b5t\u22121)\u2212 `xt(At))\nVERNET, WILLIAMSON, REID\n`1(q)\n` 2 (q\n)\nS`\nz = `(q)\npQap\nVap\nq\n`(Dn)\nh L(q)q =\n{x : x \u00b7q =\nL(q)}\nFigure 9: Illustration of proof of quasi-convexity of continuous proper losses (see text).\nthe hyperplane in direction q 2 Dn with offset b 2 R and by\nHbq := {x : x0 \u00b7q b}\nthe corresponding half-space. Since ` is proper, S` is supported at x = `(q) by the hyperplane hL(q)q and furthermore since S` is convex, S` = T q2Dn H L(q) q .\nLet Vap :=\n\\\nx2`(Dn)\\Qap HL(` 1(x)) ` 1(x) =\n\\\nq2Fap HL(q)q\n(see figure 9). Since Vap is the intersection of halfspaces it is convex. Note that a given halfspace HL(q)q is supported by exactly one hyperplane, namely h L(q) q . Thus the set of hyperplanes that support Vap is {hL(q)q : q 2 Fap } If u 2 Fap then there is a hyperplane in direction u that supports Vap and its offset is given by\nsVap (u) := infx2Vap u0 \u00b7 x = L(p) > \u2022\nwhereas if u 62 Fap then for all b 2 R, hbu does not support Vap and hence sVap (u) = \u2022. Thus we have shown\nu 62Wap , \u21e3 sVap (u) = \u2022 \u2318 .\nObserve that sVap (u) = sVap ( u) where sC(u) = supx2C u0 \u00b7 x is the support function of a set C. It is known (Valentine, 1964, Theorem 5.1) that the \u201cdomain of definition\u201d of a support function\n44\nFigure 1: Visualization of construction in proof of Lemma 6.\nfor all xt \u2208 X . Summing these bounds over t = 1, . . . , T gives T\u2211\nt=1\n`xt(p t) \u2264\nT\u2211\nt=1\n\u03a6\u2217(\u2207\u03a6(\u00b5t\u22121))\u2212 \u03a6\u2217(\u2207\u03a6(\u00b5t\u22121)\u2212 `xt(At))\n=\u03a6\u2217(\u2207\u03a6(\u00b50))\u2212 \u03a6\u2217(\u2207\u03a6(\u00b5T )) (11)\n= inf \u00b5\u2032\u2208\u2206\u0398\n\u2329 \u00b5\u2032, T\u2211\nt=1\n`xT (A t) \u232a +D\u03a6(\u00b5 \u2032, \u00b50) (12)\n\u2264 \u2329 \u00b5\u2032, T\u2211\nt=1\n`xt(A t) \u232a +D\u03a6(\u00b5 \u2032, \u00b50) for all \u00b5\u2032 \u2208 \u2206\u0398 (13)\nLine (11) above is because \u2207\u03a6(\u00b5t) = \u2207\u03a6(\u00b5t\u22121) \u2212 `xt(At) by Lemma 3 and the series telescopes. Line (12) is obtained by applying (6) from Lemma 3 and matching equations (5) and (4). Setting \u00b5\u2032 = \u03b4\u03b8 and noting \u3008\u03b4\u03b8, `(At)\u3009 = `xt(At\u03b8) gives the required result.\nA.3 Proof of Theorem 3 We first establish a general reformulation of \u03a6-mixability that holds for arbitrary ` by converting the quantifiers in the definition of \u03a6-mixability from Lemma 2 for ` into an expression involving infima and suprema. We then further refine this by assuming\n` = `F is proper (and thus quasi-convex) and has Bayes risk F .\ninf A,\u00b5 sup a\u0302 inf x\n\u03a6\u2217(\u2207\u03a6(\u00b5))\u2212 \u03a6\u2217(\u2207\u03a6(\u00b5)\u2212 `Fx (A))\u2212 `Fx (a\u0302) \u2265 0\n\u21d0\u21d2 inf A,\u00b5 sup a\u0302 inf p\n\u2329 p, { \u03a6\u2217(\u2207\u03a6(\u00b5))\u2212 \u03a6\u2217(\u2207\u03a6(\u00b5)\u2212 `Fx (P )) } x \u232a \u2212 \u2329 p, `Fx (p\u0302) \u232a \u2265 0\n(14)\nwhere the term in braces is a vector in RX . The infimum over x is switched to an infimum over distributions over p \u2208 \u2206X because the optimization over p will be achieved on the vertices of the simplex as it is just an average over random variables over X .\nFrom here on we assume that ` = `F is proper and adjust our notation to emphasis that actions a\u0302 = p\u0302 and A = P are distributions. Note that the new expression is linear \u2013 and therefore convex in p \u2013 and, by Lemma 6, we know `F is quasi-convex and so the function being optimized in (14) is quasi-concave in p\u0302. We can therefore apply Sion\u2019s theorem to swap infp and supp\u0302 which means ` F is \u03a6-mixable if and only if\ninf P,\u00b5 inf p sup p\u0302\n\u2329 p, { \u03a6\u2217(\u2207\u03a6(\u00b5))\u2212 \u03a6\u2217(\u2207\u03a6(\u00b5)\u2212 `Fx (P )) } x \u232a \u2212 \u2329 p, `Fx (p\u0302) \u232a \u2265 0\n\u21d0\u21d2 inf P,\u00b5 inf p\n\u03a6\u2217(\u2207\u03a6(\u00b5))\u2212 \u2329 p, { \u03a6\u2217(\u2207\u03a6(\u00b5)\u2212 `Fx (P )) } x ) \u232a + F (p) \u2265 0\n\u21d0\u21d2 inf P,\u00b5\n\u03a6\u2217(\u2207\u03a6(\u00b5))\u2212 F \u2217( { \u03a6\u2217(\u2207\u03a6(\u00b5)\u2212 `Fx (P )) } x ) \u2265 0\nThe second line above is obtained by recalling that, by the definition of `F , its Bayes risk is F . We now note that the inner infimum over p passes through \u03a6\u2217(\u2207\u03a6(\u00b5)) so that the final two terms are just the convex dual forF evaluated at { \u03a6\u2217(\u2207\u03a6(\u00b5)\u2212 `Fx (P )) } x\n. Finally, by translation invariance of F \u2217 we can pull the \u03a6\u2217(\u03c0\u2217) term inside F \u2217 to simplify further so that the loss `F with Bayes risk F is \u03a6-mixable if and only if\ninf P,\u00b5 \u2212F \u2217\n({ \u03a6\u2217(\u2207\u03a6(\u00b5)\u2212 `Fx (P )) } x \u2212 \u03a6\u2217(\u2207\u03a6(\u00b5))1 ) \u2265 0.\nApplying Lemma 5 to write `F in terms of F and passing the sign through the infimum and converting it to a supremum gives the required result.\nA.4 Proof of Theorem 4 We will make use the following formulation of mixability,\nM(\u03b7) := inf A\u2208A, \u03c0\u2208\u2206\u0398 sup a\u0302\u2208A inf \u00b5\u2208\u2206\u0398, x\u2208X\n\u3008\u00b5, `x(A)\u3009+ 1\n\u03b7 D\u03a6(\u00b5, \u03c0)\u2212 `x(a\u0302), (15)\nso that ` is \u03a6\u03b7-mixable if and only if M(\u03b7) \u2265 0. We call a loss ` nontrivial if there exist x\u2217, x\u2032 and a\u2217, a\u2032 such that\na\u2032 \u2208 arg min{`x\u2217(a) : `x\u2032(a) = inf a\u2208A `x\u2032(a)} and inf a\u2208A `x\u2217(a) = `x\u2217(a \u2217) < `x\u2217(a \u2032) .\n(16) Intuitively, this means that there exist distinct actions which are optimal for different outcomes x\u2217, x\u2032. Note that in particular, among all optimum actions for x\u2032, a\u2032 has the lowest loss on x\u2217.\nLemma 7. Suppose ` has a strictly concave Bayes risk L. Then given any distinct \u00b5\u2217, \u00b5\u2032 \u2208 \u2206\u0398, there is some A \u2208 A and x\u2217, x\u2032 \u2208 X such that for all a\u0302 \u2208 A we have at least one of the following:\n\u3008\u00b5\u2217, `x\u2217(A)\u3009 < `x\u2217(a\u0302) , \u3008\u00b5\u2032, `x\u2032(A)\u3009 < `x\u2032(a\u0302) . (17)\nProof. Let \u03b8\u2217 be an expert such that \u03b1 := \u00b5\u2217\u03b8\u2217 > \u00b5 \u2032 \u03b8\u2217 =: \u03b2, which exists as \u00b5 \u2217 6= \u00b5\u2032. Pick arbitrary x\u2217, x\u2032 \u2208 X and let p\u2217, p\u2032 \u2208 \u2206X with support only on {x\u2217, x\u2032} and p\u2217x\u2217 = \u03b1/(\u03b1+\u03b2), p \u2032 x\u2217 = (1\u2212\u03b1)/(2\u2212\u03b1\u2212\u03b2). Now let a\u2217 = arg mina\u2208A Ex\u223cp\u2217 [`x(a)], a\u2032 = arg mina\u2208A Ex\u223cp\u2032 [`x(a)], and set A such that A\u03b8\u2217 = a\u2217 and A\u03b8 = a\u2032 for all other \u03b8 \u2208 \u0398.\nNow suppose there is some a\u0302 \u2208 A violating eq. (17). Then in particular, 1 2 (`x\u2217(a\u0302) + `x\u2032(a\u0302)) \u2264 12 (\u3008\u00b5\u2217, `x\u2217(A)\u3009+ \u3008\u00b5\u2032, `x\u2032(A)\u3009)\n= 12 (\u03b1`x\u2217(a \u2217) + (1\u2212 \u03b1)`x\u2217(a\u2032) + \u03b2`x\u2032(a\u2217) + (1\u2212 \u03b2)`x\u2032(a\u2032))\n= \u03b1+\u03b22\n( \u03b1\n\u03b1+\u03b2 `x\u2217(a \u2217) + \u03b2\u03b1+\u03b2 `x\u2032(a\n\u2217) )\n+ 2\u2212\u03b1\u2212\u03b22\n( 1\u2212\u03b1\n2\u2212\u03b1\u2212\u03b2 `x\u2217(a \u2032) + 1\u2212\u03b22\u2212\u03b1\u2212\u03b2 `x\u2032(a\n\u2032) )\n= \u03b1+\u03b22 L(p \u2217) + ( 1\u2212 \u03b1+\u03b22 ) L(p\u2032) .\nLetting p\u0304 \u2208 \u2206X with p\u0304x\u2217 = p\u0304x\u2032 = 1/2, observe that p\u0304 = \u03b1+\u03b22 p\u2217 + (1\u2212 \u03b1+\u03b2 2 )p \u2032. But by the above calculation, we have L(p\u0304) \u2264 \u03b1+\u03b22 L(p\u2217)+(1\u2212 \u03b1+\u03b2 2 )L(p \u2032), thus violating strict concavity of L.\nNon-Legendre =\u21d2 no nontrivial mixable ` with strictly convex Bayes risk: To show that no non-constant \u03a6-mixable losses exist, we must exhibit a \u03c0 \u2208 \u2206\u0398 and an A \u2208 A such that for all a\u0302 \u2208 A we can find a \u00b5 \u2208 \u2206\u0398 and x \u2208 X satisfying \u3008\u00b5, `x(A)\u3009+ 1\u03b7D\u03a6(\u00b5, \u03c0)\u2212 `x(a\u0302) < 0. Since \u03a6 is non-Legendre it must either (1) fail strict convexity, or (2) have a point on the boundary with bounded derivative; we will consider each case separately.\n(1) Assume that \u03a6 is not strictly convex; then we have some \u00b5\u2217 6= \u00b5\u2032 such that D\u03a6(\u00b5\n\u2217, \u00b5\u2032) = 0. By Lemma 7 with these two distributions, we have some A and x\u2217, x\u2032 such that for all a\u0302, either (i) \u3008\u00b5\u2217, `x\u2217(A)\u3009 < `x\u2217(a\u0302) or (ii) \u3008\u00b5\u2032, `x\u2032(A)\u3009 < `x\u2032(a\u0302). We set \u03c0 = \u00b5\u2032; in case (i) we take \u00b5 = \u00b5\u2217 and x = x\u2217, and in (ii) we take \u00b5 = \u00b5\u2032 and x = x\u2032, but as 1\u03b7D\u03a6(\u00b5, \u03c0) = 0 in both cases, we have M(\u03b7) < 0 for all \u03b7.\n(2) Now assume instead that we have some \u00b5\u2032 on the boundary of \u2206\u0398 with bounded \u2016\u2207\u03a6(\u00b5\u2032)\u2016 = C < \u221e. Because \u00b5\u2032 is on the boundary of \u2206\u0398 there is at least one expert \u03b8\u2217 \u2208 \u0398 for which \u00b5\u2032\u03b8\u2217 = 0. Pick x\u2217, x\u2032, a\u2217, a\u2032 from the definition of nontrivial, eq. (16). In particular, note that `x\u2217(a\u2217) < `x\u2217(a\u2032). Let \u03c0 = \u00b5\u2032 and A \u2208 A such that A\u03b8\u2217 = a\n\u2217 and A\u03b8 = a\u2032 for all other \u03b8. Now suppose a\u0302 \u2208 A has `x\u2032(a\u0302) > `x\u2032(a\u2032). Then taking \u00b5 = \u03c0 puts all weights on experts predicting a\u2032 while keeping D\u03a6(\u00b5, \u03c0) = 0, so choosing x = x\u2032 gives M(\u03b7) < 0 for all \u03b7. Otherwise, `x\u2032(a\u0302) = `x\u2032(a\u2032), which by eq. (16) implies `x\u2217(a\u0302) \u2265 `x\u2217(a\u2032).\nLet \u00b5\u03b1 = \u03c0+\u03b1(\u03b4\u03b8\u2217 \u2212 \u03c0), where \u03b4\u03b8\u2217 denotes the point distribution on \u03b8\u2217. Calculating, we have\nM(\u03b7) = \u3008\u00b5\u03b1, `x\u2217(A)\u3009+ 1\u03b7D\u03a6(\u00b5\u03b1, \u03c0)\u2212 `x\u2217(a\u0302) = (1\u2212 \u03b1)`x\u2217(a\u2032) + \u03b1`x\u2217(a\u2217) + 1\u03b7D\u03a6(\u00b5\u03b1, \u03c0)\u2212 `x\u2217(a\u0302) \u2264 (1\u2212 \u03b1)`x\u2217(a\u0302) + \u03b1`x\u2217(a\u2217) + 1\u03b7D\u03a6(\u00b5\u03b1, \u03c0)\u2212 `x\u2217(a\u0302) = \u03b1(`x\u2217(a \u2217)\u2212 `x\u2217(a\u0302)) + 1\u03b7Df (\u03b1, 0),\nwhere f(\u03b1) = \u03a6(\u00b5\u03b1) = \u03a6(\u03c0+\u03b1(\u03b4\u03b8\u2217 \u2212\u03c0)). As\u2207\u03c0\u03a6 is bounded, so is f \u2032(0). Now as lim \u21920Df (x+ , x)/ = 0 for any scalar convex f with bounded f \u2032(x) (see e.g. [21, Theorem 24.1] and [23]), we see that for any c > 0 we have some \u03b1 > 0 such that Df (\u03b1, 0) < c\u03b1. Taking c = \u03b7(`x\u2217(a\u0302)\u2212 `x\u2217(a\u2217)) > 0 then gives M(\u03b7) < 0.\nLegendre =\u21d2 \u2203mixable `: Assuming \u03a6 is Legendre, we need only show that some non-constant ` is \u03a6-mixable. As\u2207\u03c0\u03a6 is infinite on the boundary, \u03c0 must be in the relative interior of \u2206\u0398; otherwise D\u03a6(\u00b5, \u03c0) =\u221e for \u00b5 6= \u03c0.\nTake A = \u2206X and `(p, x) = \u2016p \u2212 \u03b4x\u20162 to be the 2-norm squared loss. Now for all \u00b5 in the interior of \u2206\u0398 and P \u2208 \u2206\u0398X , we have \u3008\u00b5, `x(P )\u3009 = \u2211 \u03b8 \u00b5\u03b8\u2016P\u03b8 \u2212 \u03b4x\u20162 \u2265\n\u2016p\u0304 \u2212 \u03b4x\u20162 by convexity, where p\u0304 = \u2211 \u03b8 \u00b5\u03b8P\u03b8. In fact, as \u00b5 is in the interior, this inequality is strict, and remains so if replace \u00b5 by \u00b5\u2032 with \u2016\u00b5\u2032 \u2212 \u00b5\u2016 < for some sufficiently small. Now for all \u00b5, P the algorithm can take p\u0302 = p\u0304, and we can always choose \u03b7 = infx,\u00b5\u2032:\u2016\u00b5\u2032\u2212\u00b5\u2016= D\u03a6(\u00b5\u2032, \u00b5)/( `max) > 0, so either \u2016\u00b5 \u2212 \u03c0\u2016 < in which case we are fine by the above, or \u00b5 is far enough away that the D\u03a6 term dominates the algorithm\u2019s loss. (Here `max is just maxp,x `x(p), which is bounded, and D\u03a6(\u00b5\u2032, \u00b5) > 0 as \u03a6 is strictly convex.) So if \u03a6 is Legendre, squared loss is \u03a6-mixable."}], "references": [{"title": "Competitive on-line statistics", "author": ["Volodya Vovk"], "venue": "International Statistical Review,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Aggregating strategies", "author": ["Volodya Vovk"], "venue": "Proceedings of the Third Annual Workshop on Computational Learning Theory (COLT),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "A game of prediction with expert advice", "author": ["Volodya Vovk"], "venue": "In Proceedings of the Eighth Annual Conference on Computational Learning Theory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "Prediction, learning, and games", "author": ["Nicolo Cesa-Bianchi"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Mixability is bayes risk curvature relative to log loss", "author": ["Tim van Erven", "Mark D Reid", "Robert C Williamson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Averaging expert predictions", "author": ["Jyrki Kivinen", "Manfred K Warmuth"], "venue": "In Computational Learning Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "The weak aggregating algorithm and weak mixability", "author": ["Yuri Kalnishkan", "Michael V. Vyugin"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["Jyrki Kivinen", "Manfred K Warmuth"], "venue": "Information and Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Relative loss bounds for on-line density estimation with the exponential family of distributions", "author": ["Katy S Azoury", "Manfred K Warmuth"], "venue": "Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Mirror descent and nonlinear projected subgradient methods for convex optimization", "author": ["Amir Beck", "Marc Teboulle"], "venue": "Operations Research Letters,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Fundamentals of convex analysis", "author": ["J.B. Hiriart-Urruty", "C. Lemar\u00e9chal"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Comparison of shannon, renyi and tsallis entropy used in decision trees", "author": ["Tomasz Maszczyk", "W\u0142odzis\u0142aw Duch"], "venue": "In Artificial Intelligence and Soft Computing\u2013", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "R\\\u2019enyi divergence and kullback-leibler divergence", "author": ["Tim Van Erven", "Peter Harremo\u00ebs"], "venue": "arXiv preprint arXiv:1206.2459,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Bayesian conditionalisation and the principle of minimum information", "author": ["Peter M Williams"], "venue": "British Journal for the Philosophy of Science,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1980}, {"title": "The geometry of proper scoring rules", "author": ["A Philip Dawid"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Strictly proper scoring rules, prediction, and estimation", "author": ["Tilmann Gneiting", "Adrian E Raftery"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Composite multiclass losses", "author": ["Elodie Vernet", "Robert C Williamson", "Mark D Reid"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Elicitation of personal probabilities and expectations", "author": ["Leonard J Savage"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1971}, {"title": "Stochastic finance, volume 27 of de gruyter studies", "author": ["Hans F\u00f6llmer", "Alexander Schied"], "venue": "in mathematics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Sequential prediction of individual sequences under general loss functions", "author": ["David Haussler", "Jyrki Kivinen", "Manfred K Warmuth"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Convex analysis", "author": ["R.T. Rockafellar"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "As shown by Vovk [1], his more general \u201caggregating algorithm\u201d reduces to Bayesian updating when log loss is used.", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": "In [2, 3], Vovk showed that if the loss for a game satisfies a condition called mixability then a player making predictions using the aggregating algorithm (AA) will achieve constant regret.", "startOffset": 3, "endOffset": 9}, {"referenceID": 2, "context": "In [2, 3], Vovk showed that if the loss for a game satisfies a condition called mixability then a player making predictions using the aggregating algorithm (AA) will achieve constant regret.", "startOffset": 3, "endOffset": 9}, {"referenceID": 2, "context": "Theorem 1 (Mixability implies constant regret [3]).", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "We show that \u03a6-mixability of a loss can be expressed directly in terms of the Bayes risk associated with the loss (Definition 4 and Theorem 3), reflecting the situation that holds for classical mixability [5].", "startOffset": 205, "endOffset": 208}, {"referenceID": 2, "context": "The starting point for mixability and the aggregating algorithm is the work of [3, 2].", "startOffset": 79, "endOffset": 85}, {"referenceID": 1, "context": "The starting point for mixability and the aggregating algorithm is the work of [3, 2].", "startOffset": 79, "endOffset": 85}, {"referenceID": 5, "context": "Variants of the aggregating algorithm have been studied for classically mixable losses, with a trade-off between tightness of the bound (in a constant factor) and the computational complexity [6].", "startOffset": 192, "endOffset": 195}, {"referenceID": 6, "context": "They have been studied in [7] where it is shown there exists a variant of the aggregating algorithm that achieves regret C \u221a T for some constant C.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "The general form of updating we propose is similar to that considered by Kivinen and Warmuth [8] who consider finding a vector w minimizing d(w, s) + \u03b7L(yt, w \u00b7xt) where s is some starting vector, (xt, yt) is the instance/label observation at round t and L is a loss.", "startOffset": 93, "endOffset": 96}, {"referenceID": 9, "context": "The analysis of mirror descent [10] shows that it achieves constant regret when the entropic regularizer is", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "See for example the discussion of potential based methods in [4] and other references later in the paper.", "startOffset": 61, "endOffset": 64}, {"referenceID": 10, "context": "Terms not defined below can be found in a reference such as [11].", "startOffset": 60, "endOffset": 64}, {"referenceID": 11, "context": "[12, 13]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "[12, 13]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 10, "context": "[11]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "For differentiable \u03a6, it is known [11] that the supremum defining \u03a6\u2217 is attained at \u03bc = \u2207\u03a6\u2217(v).", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "In the special case when \u03a6 is Shannon entropy, ` is log loss, and expert predictions A\u03b8 \u2208 \u2206X are distributions over X such an optimization is equivalent to Bayesian updating [14].", "startOffset": 174, "endOffset": 178}, {"referenceID": 9, "context": "The GAA is therefore closely related to mirror descent techniques [10].", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "Entropies are known to be closely related to the Bayes risk of what are called proper losses or proper scoring rules [15, 16].", "startOffset": 117, "endOffset": 125}, {"referenceID": 15, "context": "Entropies are known to be closely related to the Bayes risk of what are called proper losses or proper scoring rules [15, 16].", "startOffset": 117, "endOffset": 125}, {"referenceID": 4, "context": "That is, for all p, p\u0302 \u2208 \u2206X we have Ex\u223cp [`x(p\u0302)] = \u3008p, `(p\u0302)\u3009 \u2265 \u3008p, `(p)\u3009 =: \u2212F (p) where \u2212F ` is the Bayes risk of ` and is necessarily concave [5], thus making F ` : \u2206X \u2192 R convex and thus an entropy.", "startOffset": 146, "endOffset": 149}, {"referenceID": 16, "context": "The correspondence also goes the other way: given any convex function F : \u2206X \u2192 R we can construct a unique proper loss [17].", "startOffset": 119, "endOffset": 123}, {"referenceID": 17, "context": "The following representation can be traced back to [18] but is expressed here using convex duality.", "startOffset": 51, "endOffset": 55}, {"referenceID": 4, "context": "This is similar in spirit to the result in [5] which shows that the original mixability (for \u03a6 = H) can be expressed in terms of the relative curvature of Shannon entropy and the loss\u2019s Bayes risk.", "startOffset": 43, "endOffset": 46}, {"referenceID": 18, "context": "Although this definition appears complicated due to the handling of vectors in R and R, it has a natural interpretation in terms of risk measures from mathematical finance [19].", "startOffset": 172, "endOffset": 176}, {"referenceID": 19, "context": "The entropic form of mixability in (10) shares some similarities with expressions for the classical mixability constants given in [20] for binary outcome games and in [5] for general games.", "startOffset": 130, "endOffset": 134}, {"referenceID": 4, "context": "The entropic form of mixability in (10) shares some similarities with expressions for the classical mixability constants given in [20] for binary outcome games and in [5] for general games.", "startOffset": 167, "endOffset": 170}, {"referenceID": 4, "context": "It is also arguably more efficient since the optimization in [5] for non-binary outcomes requires inverting a Hessian matrix at each point in the optimization.", "startOffset": 61, "endOffset": 64}, {"referenceID": 20, "context": "Specifically, an entropy \u03a6 is said to be Legendre [21] if: a) \u03a6 is strictly convex in int(\u2206\u0398); and b) \u2016\u2207\u03a6(\u03bc)\u2016 \u2192 \u221e as \u03bc\u2192 \u03bcb for any \u03bcb on the boundary of \u2206\u0398.", "startOffset": 50, "endOffset": 54}, {"referenceID": 2, "context": "There is a lower bound due to Vovk [3] for general losses ` which shows that if one is allowed to vary the number of rounds T and the number of experts K = |\u0398|, then no regret bound can be better than the optimal regret bound obtained by Shannon mixability.", "startOffset": 35, "endOffset": 38}, {"referenceID": 19, "context": "Indeed, there is a lower bound for general losses ` that shows the regret of the best possible algorithm on games using ` must grow like \u03a9(log2K) [20].", "startOffset": 146, "endOffset": 150}], "year": 2014, "abstractText": "Mixability is a property of a loss which characterizes when fast convergence is possible in the game of prediction with expert advice. We show that a key property of mixability generalizes, and the exp and log operations present in the usual theory are not as special as one might have thought. In doing this we introduce a more general notion of \u03a6-mixability where \u03a6 is a general entropy (i.e., any convex function on probabilities). We show how a property shared by the convex dual of any such entropy yields a natural algorithm (the minimizer of a regret bound) which, analogous to the classical aggregating algorithm, is guaranteed a constant regret when used with \u03a6-mixable losses. We characterize precisely which \u03a6 have \u03a6-mixable losses and put forward a number of conjectures about the optimality and relationships between different choices of entropy.", "creator": "LaTeX with hyperref package"}}}