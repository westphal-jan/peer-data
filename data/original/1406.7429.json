{"id": "1406.7429", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2014", "title": "Comparison of SVM Optimization Techniques in the Primal", "abstract": "This paper examines the efficacy of different optimization techniques in a primal formulation of a support vector machine (SVM). Three main techniques are compared. The dataset used to compare all three techniques was the Sentiment Analysis on Movie Reviews dataset, from kaggle.com.", "histories": [["v1", "Sat, 28 Jun 2014 18:59:44 GMT  (14kb)", "http://arxiv.org/abs/1406.7429v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jonathan katzman", "diane duros"], "accepted": false, "id": "1406.7429"}, "pdf": {"name": "1406.7429.pdf", "metadata": {"source": "CRF", "title": "Comparison of SVM Optimization Techniques in the Primal", "authors": ["Diane Duros"], "emails": ["dduros1@gmail.com", "jonathan.d.katzman@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n74 29\nv1 [\ncs .L\nG ]\n2 8\nJu n\nThis paper examines the efficacy of different optimization techniques in a primal formulation of a support vector machine (SVM). Three main techniques are compared. The dataset used to compare all three techniques was the Sentiment Analysis on Movie Reviews dataset, from kaggle.com."}, {"heading": "1 Introduction", "text": "Most SVM literature states the primal optimization, then proceeds to the dual formulation without providing significant detail on training an SVM using the primal optimization problem. Learning an SVM is typically viewed as a constrained quadratic programming problem.\nOur goal is to analyze three different primal optimization methods in the context of a large, text-based dataset. Given a training set {(xi, yi)}1\u2264i\u2264n, xi \u2208 R, yi \u2208 {1,\u22121}, the primal optimization problem is:\nminw,b || w ||2 +C n \u2211\ni=1\n\u03bePi\nunder constraints yi(w \u00b7 xi + b) > 1; \u03bei, \u03bei > 0 (1)\nWe use a hard-margin SVM, where C = 0 - that is, we would like every data point to be outside of the margin around the hyperplane. The values \u03bei allow for some slack, but by setting C to 0, we remove this possibility.\nThe optimization algorithms we consider are gradient descent, Newton\u2019s method, and the Pegasos algorithm, which is an application of a stochastic sub gradient method. Other algorithms that are related to these are the NORMA algorithm[3], which is another application of stochastic gradient descent, and SGD-QN [1], which combines stochastic gradient descent with a quasi-Newton method. Other previous work in Newton\u2019s method [2] has used the USPS dataset, which is significantly smaller than our data set."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Data", "text": "We retrieved our data from the problem \u201cSentiment Analysis on Movie Reviews,\u201d from kaggle.com. The data originates from the Rotten Tomatoes dataset and consists of phrases that have been assigned sentiment labels, where the sentiment labels are { 0: negative, 1: somewhat negative, 2: neutral, 3: somewhat positive, 4: positive }.\nInitially, we collapsed the labels to binary labels, where an original label of 3 or 4 was considered positive, and 2 or below became a negative label. Later, we created a MulticlassSVM to handle the non-binary labels.\nIn addition to collapsing the labels, we also had a number of options for processing our data. We ignored punctuation such as \u201c,\u201d, determining that it was unlikely to contribute significantly to the sentiment while appearing often in both positive and negative phrases. We also ignored upper/lower case distinctions, which collapsed our corpus from \u223c18000\nwords to \u223c16000 words. When we compare our data to that of the USPS dataset, USPS has 7291 data instances with 256 features, so our data is at least an order of magnitude larger than this."}, {"heading": "2.2 Features", "text": "We generated features based on a bag of words model, where a phrase is represented as the multiset of its words. There are two options: binary features (where the feature indicates the existence of a word in a phrase) and continuous features (where the feature indicates the frequency of a word in a phrase).\nWe have as many features for each instance as there are words in the corpus (18226), but each feature vector is sparse, as we can see in table 2.1. Since there\u2019s an average of 6.85 words per phrase (data instance), the other 18220 elements of the feature vector will be 0. We did not implement feature selection to only utilize the most significant features, as we believe that feature selection would not be able to maintain a balance between reducing the number of features and maintaining information for each phrase (if we removed too many words, many phrases would have most, or maybe all, of their words removed, becoming useless)."}, {"heading": "3 SVMs", "text": "We chose to approach this data set with SVMs, because they provide a framework for various optimization methods, as well as generalizing to multiclass data. This also allowed us to build upon work that we\u2019d done in class, using the gradient descent method as a baseline to compare with our other optimization methods.\nSVMs are binary linear classifiers. In training, they create a hyperplane between two classes of data, where the hyperplane maximizes the margin between the two classes. The main assumption that\nwe have made about our data is that it is linearly separable. Since our data is text-based, one example that could cause non-linear separability is sarcasm: suppose a reviewer uses a \u201cpositive\u201d word sarcastically.\nOur optimizers return the coefficients of the hyperplane that maximizes the margin, where we have as many coefficients as we do features. The general objective function is of the form f(w) = L(w) + r(w), where L(w) is a convex measure of loss and r(w) is a convex regularization, so our optimization goal is to minimize the loss function.\nBasis To calculate a basis, we take all data points that constitute support vectors (that is, whose margins are less than one), and calculate their average distance from the hyperplane described by the weights w. We use the average distance, because it is a more robust measure for implementation; if we summed these values, we would overfit the data."}, {"heading": "3.1 Multi-class SVM", "text": "In order to classify data into 5 classes, we needed to expand our SVM to handle more than just binary labels. Since two-class problems are easier to solve, we will generate multiple pairwise SVMs for multi-class classification, using the \u201cone against one\u201d method [6].\nTo enforce ordinal ranking (0 < 1 < 2 < 3 < 4), instead of generating pairwise SVMs between all pairs, we only created pairwise SVMs for label pairs { (0,1), (1,2), (2,3), (3,4) }. This way, we never attempt to classify between classes that aren\u2019t directly related via the ranking inequality.\nOnce each pairwise SVM is trained, according to a user defined optimization method, there is an additional training step. For prediction in the binary SVM, we compute E(W ), where W represents the weights trained by the SVM. If E(W ) \u2265 0, we classify the example as positive, otherwise, it is negative. For the multiclass SVM, we compute E(W) for each pairwise SVM, and store the values in S. Then, we compute P (label | S) = P (label,S)\nP (S) , that is, we count the number of times the true label occurs with the array S. Then, when we predict an instance, we calculate S, then determine which label is most likely (has the highest probability).\nThis introduces the problem that, if we have never seen the true label and S together, we will never be\nable to predict it. In addition, if we train on a restricted dataset, it is possible that we will not see all possible values of S. In this case, if we encounter a value of S that we didn\u2019t see in training, we classify based on the overall probability of the labels, given the training data (the probability is the proportion of times we\u2019ve seen each label in training data to the number of all training instances.).\nFor this multi-class SVM, we need to achieve accuracy above 0.2 (1/5) to improve upon random chance, whereas we need to have accuracy greater than 0.5 for the binary SVM."}, {"heading": "4 Primal Optimization Methods", "text": "Many SVM packages optimize the dual form of the SVM, but we chose to explore different methods for optimizing the primal problem.\nFor a linear SVM, both the primal and the dual are convex quadratic programs, so an exact solution exists. Chapelle shows not only do the primal and dual optimization methods reach the same result, but that when an approximate solution is desired, primal optimization is superior[2]. The dual program solves for a vector that is as long as the number of training instances, and the primal program solves for a vector that is as long as the number of features. As seen in table 2.1, we have 150,000 instances, but only 18,000 features, so the primal problem will be more efficient to solve for our data set.\nWe examined three different optimization methods: gradient descent, Newton\u2019s approximation, and stochastic subgradient (where we used the Pegasos algorithm). Since the hinge loss is nondifferentiable, Chapelle considered smooth loss functions instead of the hinge loss[2], while the Pegasos algorithm[8] uses sub-gradients."}, {"heading": "4.1 Gradient Descent", "text": "We use gradient descent as a baseline with which to compare our two more complex methods. Gradient descent is a classical optimization technique, since the gradient of a function points in the direction of greatest increase, and therefore the negative gradient points in the direction of greatest decrease. It uses the update step:\nw\u2032 = w \u2212 \u03b7 \u25bd L(x, y;w) (2)\nwhere \u03b7 is the learning rate, L(x, y;w) is the loss function for a data point given the current hyperplane coefficients w. We used a default step size (learning rate) of .001. We sacrificed runtime and used a small step size so that we wouldn\u2019t overshoot and miss the minimum. In this method, we use the entire training set to compute the gradient.[5]\nLoss function The loss function used is quadratic loss ( which is differentiable everywhere unlike the hinge loss), the L2 penalization of training errors,\nL(yi, f(xi)) = max(0, 1 \u2212 yif(xi))2\nRuntime Gradient descent is linear in the number of training instances, iterations, and number of features, which in this case is quite large."}, {"heading": "4.2 Newton\u2019s Approximation", "text": "As seen in Chapelle, we can write (1) as an unconstrained optimization function:\n\u03bb\u03b2TK\u03b2 + n \u2211\ni=1\nL(yi,K T i \u03b2) (3)\nwhere \u03bb is a regularization parameter, L is the loss function, and Ki is the ith column of a kernel K.\nLoss Function The loss function used is quadratic loss, the L2 penalization of training errors,\nL(yi, f(xi)) = max(0, 1 \u2212 yif(xi))2\nIf the loss on a point xi is nonzero, then xi is a support vector. The gradient of (3) with respect to \u03b2 is \u25bd = 2(\u03bbK\u03b2 +KI0(K\u03b2 \u2212 Y )) and the Hession is\nH = 2(\u03bbK +K0K)\nWe can combine these to see that after the Newton update step1,\n\u03b2 = (\u03bbIn + I 0K)\u22121I0Y (4)\nIf Ksv is the sub matrix corresponding to the support vectors, then since \u03bbIn + I0K = 0, the final update is\n\u03b2 = (\u03bbInsv +Ksv) \u22121Ysv 0\n(5)\n1This assumes that K is invertible.\nWhen we attempted to run our implementation of this method on our full training set, we realized that it would not terminate in a reasonable amount of time for this project. Since the algorithm initially restricts data to 1000 samples, then recursively trains on double the data until finally training on the full dataset, there is a cubic increase in time if the number of support vectors increases with size of the dataset, which it does. On our data, it would take 9 recursive calls to fully train the model.\nSince we could not train this model on our full dataset, we created a restricted dataset of 1000 instances for evaluation purposes. We also restricted the number of iterations to 5, since Chapelle claims that the algorithm should converge to the solution within 5 iterations.\nThis method approximates the inverse Hessian, and uses it to scale the gradient at each iteration. Methods like this tend to be avoided for large-scale, batch problems, because, as we have discovered, this method does not scale well to large data.\nRuntime The runtime depends on the complexity of one Newton step, O(nnsv + n3sv), since we converge in a constant number of iterations.\nRBF Kernel We used a radial basis function (RBF) kernel for this method, because that is the original kernel used by Chapelle.\nK(x, x\u2032) = exp(\u2212|| x\u2212 x \u2032 ||22\n2\u03c32 )\nAfter getting poor performance from this method, we further examined our kernel, and determined that we should not have used an RBF kernel for this problem. The RBF kernel acts as a Gaussian filter, which are often used for smoothing images, so the RBF kernel is a filter that selects smooth solutions.2\n2http://charlesmartin14.wordpress.com/2012/02/06/kernels part 1/\nelse sv \u2190 {1, . . . , n} repeat \u03b2sv \u2190 (Ksv + \u03bbInxv)\u22121Ysv Other components of \u03b2 \u2190 0 sv \u2190 indices i such that yi[K\u03b2]i < 1 until sv has not changed\nThis doesn\u2019t make sense in the context of text data, because you can\u2019t view the data in terms of signals with a frequency domain. This kernel was logical for the USPS dataset Chapelle used, because it is an image dataset. A better kernel for us would have been the linear kernel."}, {"heading": "4.3 Stochastic Subgradient", "text": "Stochastic gradient descent is significantly faster than methods like gradient descent that use the true gradient at each iteration. Instead, it approximates the gradient on fewer training examples. The stochastic sub gradient method we implemented is the Pegasos algorithm by Shalev-Shwartz et al., which is a stochastic gradient descent method that also has a projection step. The objective to be minimized is\nminw \u03bb 2 || w ||2 + 1 m \u2211\n(x,y)\u2208X L(w; (x, y)) (6)\nAt each iteration, we select a random subset of training examples, and update the weight vector with the subgradient of the objective function evaluated on the subset. Then we project the vector onto a sphere with radius 1/ \u221a \u03bb, because the optimal weight vector must lie in this ball (see Menon), due to the strong duality theorem.\nLoss Function The Pegasos algorithm uses the hinge loss function\nL(w;x, y) = max{0, 1 \u2212 y\u3008x, y\u3009}\nAlgorithm 2 Pegasos Stochastic Subgradient Method\nfor t = 1 . . . T do Pick random At \u2282 T s.t. | At |= k M \u2190 {(x, y) \u2208 At : 1\u2212 y(w \u00b7 x) > 0} \u25bdt \u2190 \u03bbwt \u2212 1|M | \u2211 (x,y)\u2208M yx\nUpdate wt+ 1 2 \u2190 wt \u2212 1\u03bbt \u00b7 \u25bdt Let wt+1 \u2190 min (\n1, 1\u221a \u03bb||wt+1/2||\n)\nwt+ 1 2\nreturn wT+1\nRuntime The runtime is independent of the number of training examples, and the algorithm finds an \u01eb-accurate solution in O( d\n\u03bb\u01eb ) time, where d is the\nnumber of non-zero features in each training example."}, {"heading": "5 Code", "text": "We created an SVM framework in Python, as well as a custom DataParser. Our SVM class takes an Optimizer object as input, where the Optimizer calculates weights for the SVM according to the optimization function encoded in the object. We did not use any SVM packages, and only used the numpy package for evaluation purposes."}, {"heading": "5.1 Evaluation", "text": "For evaluation, we created a CrossValidationTester, so that we could train and test on the training dataset from kaggle.com. For each round, the CrossValidationTester randomly orders the data, then selects 10% to be left out for testing purposes, while the rest is used for training the model; we used a default of 10 rounds, then averaged the accuracy and timing results across rounds. The only model that we didn\u2019t use this for was the Newton approximation, because it took too long to run."}, {"heading": "6 Results", "text": ""}, {"heading": "6.1 Cross Validation Results", "text": "To find the best accuracy for each algorithm, we tested different parameter settings. For gradient descent, we iterated over the set of learning rates { .01, .02, .03, .04, .05, .1, .2, .3, .4, .5, 1, 2, 3, 4, 5 } to find the optimal learning rate, then used this value\n.\nto find the number of iterations needed for convergence. Similarly, for the Pegasos method, we iterated over the set of ball diameters { .001, .01, .1, 1, 10 } to find the optimal \u03bb, then used this to determine the best sample size and number of iterations. The optimal parameters for each algorithm under each data representation setting are available in the results file we\u2019re providing with our project.\nFor each parameter setting, we ran 10-fold cross validation to determine the average accuracy and runtime. Once we determined the \u201dbest\u201d parameters for each model, we ran 10-fold cross validation on the training data under various data representation settings. The modes we evaluated were binary (bin) versus multiclass (multi) SVMs, and the data settings were binary feature representation (bin) versus the frequency representation (freq). Table 3 shows the comparison between different settings for gradient descent (GD) and Pegasos stochastic subgradient (SSG) on the full data set, while table 4 shows the comparison between settings for all three algorithms, where Newton\u2019s method is abbreviated as NM.\nWe were unable to evaluate Newton\u2019s on training sets larger than 1000 instances, but wanted to compare its results with the other two methods, so we used a limited data set to produce the results in table 4.\nOn both datasets, the Pegasos method\u2019s results are nearly equal to those of gradient descent, but the runtime is much quicker. On the small dataset, Newton\u2019s method performs almost equally well with the multiclass SVM, but is outperformed by both gradient descent and Pegasos on the binary SVM. We can clearly see that this method took significantly longer,\nwithout yielding superior results. Overall, we consider the Pegasos algorithm to be the most successful on this dataset."}, {"heading": "6.2 Comparison to Proposal", "text": "We revised our proposal from focusing on solving the kaggle.com problem to using the dataset to explore different optimization techniques. Our achievements did still align somewhat with what we initially planned to do, which was to first create a binary SVM classifier, then extend it to a multi-class SVM. We created both successfully. However, instead of exploring various feature representations of our data, we explored different primal optimization techniques. Our goal was to implement three different algorithms, and we met this goal. We did not generate results to submit to kaggle.com, preferring to evaluate our methods using 10-fold cross validation."}], "references": [{"title": "Sgd-qn: Careful quasi-newton stochastic gradient descent", "author": ["Antoine Bordes", "L\u00e9on Bottou", "Patrick Gallinari"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Training a support vector machine in the primal", "author": ["Olivier Chapelle"], "venue": "Neural Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Online learning with kernels", "author": ["Jyrki Kivinen", "Alex J Smola", "Robert C Williamson"], "venue": "Ad-  vances in neural information processing systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Laplacian support vector machines trained in the primal", "author": ["Stefano Melacci", "Mikhail Belkin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "one against one or one against all: Which one is better for handwriting recognition with svms", "author": ["Jonathan Milgram", "Mohamed Cheriet", "Robert Sabourin"], "venue": "In Tenth International Workshop on Frontiers in Handwriting Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Stochastic Subgradient Approach for Solving Linear Support Vector Machines\u2013An Overview", "author": ["Jan Rupnik"], "venue": "SiKDD,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro"], "venue": "In Proceedings of the 24th International Conference on Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}], "referenceMentions": [{"referenceID": 2, "context": "Other algorithms that are related to these are the NORMA algorithm[3], which is another application of stochastic gradient descent, and SGD-QN [1], which combines stochastic gradient descent with a quasi-Newton method.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "Other algorithms that are related to these are the NORMA algorithm[3], which is another application of stochastic gradient descent, and SGD-QN [1], which combines stochastic gradient descent with a quasi-Newton method.", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": "Other previous work in Newton\u2019s method [2] has used the USPS dataset, which is significantly smaller than our data set.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "Since two-class problems are easier to solve, we will generate multiple pairwise SVMs for multi-class classification, using the \u201cone against one\u201d method [6].", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "Chapelle shows not only do the primal and dual optimization methods reach the same result, but that when an approximate solution is desired, primal optimization is superior[2].", "startOffset": 172, "endOffset": 175}, {"referenceID": 1, "context": "Since the hinge loss is nondifferentiable, Chapelle considered smooth loss functions instead of the hinge loss[2], while the Pegasos algorithm[8] uses sub-gradients.", "startOffset": 110, "endOffset": 113}, {"referenceID": 6, "context": "Since the hinge loss is nondifferentiable, Chapelle considered smooth loss functions instead of the hinge loss[2], while the Pegasos algorithm[8] uses sub-gradients.", "startOffset": 142, "endOffset": 145}], "year": 2014, "abstractText": "This paper examines the efficacy of different optimization techniques in a primal formulation of a support vector machine (SVM). Three main techniques are compared. The dataset used to compare all three techniques was the Sentiment Analysis on Movie Reviews dataset, from kaggle.com.", "creator": "LaTeX with hyperref package"}}}