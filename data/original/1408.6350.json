{"id": "1408.6350", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Aug-2014", "title": "Definition and properties to assess multi-agent environments as social intelligence tests", "abstract": "Social intelligence in natural and artificial systems is usually measured by the evaluation of associated traits or tasks that are deemed to represent some facets of social behaviour. The amalgamation of these traits is then used to configure the intuitive notion of social intelligence. Instead, in this paper we start from a parametrised definition of social intelligence as the expected performance in a set of environments with several agents, and we assess and derive tests from it. This definition makes several dependencies explicit: (1) the definition depends on the choice (and weight) of environments and agents, (2) the definition may include both competitive and cooperative behaviours depending on how agents and rewards are arranged into teams, (3) the definition mostly depends on the abilities of other agents, and (4) the actual difference between social intelligence and general intelligence (or other abilities) depends on these choices. As a result, we address the problem of converting this definition into a more precise one where some fundamental properties ensuring social behaviour (such as action and reward dependency and anticipation on competitive/cooperative behaviours) are met as well as some other more instrumental properties (such as secernment, boundedness, symmetry, validity, reliability, efficiency), which are convenient to convert the definition into a practical test. From the definition and the formalised properties, we take a look at several representative multi-agent environments, tests and games to see whether they meet these properties.", "histories": [["v1", "Wed, 27 Aug 2014 08:56:09 GMT  (2078kb,D)", "http://arxiv.org/abs/1408.6350v1", "53 pages + appendix"]], "COMMENTS": "53 pages + appendix", "reviews": [], "SUBJECTS": "cs.MA cs.AI", "authors": ["javier insa-cabrera", "jos\\'e hern\\'andez-orallo"], "accepted": false, "id": "1408.6350"}, "pdf": {"name": "1408.6350.pdf", "metadata": {"source": "CRF", "title": "Definition and properties to assess multi-agent environments as social intelligence tests", "authors": ["Javier Insa-Cabrera", "Jos\u00e9 Hern\u00e1ndez-Orallo"], "emails": ["jinsa@dsic.upv.es", "jorallo@dsic.upv.es"], "sections": [{"heading": null, "text": "Keywords: social intelligence, artificial intelligence, multi-agent systems, cooperation, competition, interaction, game theory, teams, rewards, intelligence testing, universal psychometrics.\nar X\niv :1\n40 8.\n63 50\nv1 [\ncs .M\nA ]\n2 7\nA ug\n2 01"}, {"heading": "Contents", "text": ""}, {"heading": "1 Introduction 3", "text": ""}, {"heading": "2 Background 3", "text": ""}, {"heading": "3 Defining Social Intelligence Universally 6", "text": "3.1 Multi-agent environments and team rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.2 Teams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3.3 A formal definition of social intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.4 Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11"}, {"heading": "4 Properties about social intelligence testbeds 12", "text": "4.1 Boundedness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4.2 Interactivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 4.3 Non-neutralism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4.4 Secernment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.5 Anticipation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.6 Symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4.7 Validity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 4.8 Reliability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 4.9 Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 4.10 Summary of properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26"}, {"heading": "5 Degree of compliance of several multi-agent and social scenarios 26", "text": "5.1 Graphical analysis for the properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 5.2 Matching pennies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 5.3 Prisoner\u2019s dilemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 5.4 Predator-prey (Pursuit game) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 5.5 Pac-Man . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 5.6 RoboCup Soccer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 5.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43"}, {"heading": "6 Conclusions and Future work 48", "text": ""}, {"heading": "A Matching Pennies properties 53", "text": "A.1 Action Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 A.2 Reward Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 A.3 Fine Discrimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 A.4 Strict Total Grading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 A.5 Partial Grading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 A.6 Slot Reward Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 A.7 Competitive Anticipation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84"}, {"heading": "B Prisoner\u2019s Dilemma properties 86", "text": "B.1 Action Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 B.2 Reward Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 B.3 Fine Discrimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 B.4 Strict Total Grading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 B.5 Partial Grading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 B.6 Slot Reward Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 B.7 Competitive Anticipation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106"}, {"heading": "C Predator-prey properties 108", "text": "C.1 Action Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 C.2 Reward Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 C.3 Fine Discrimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 C.4 Strict Total Grading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 C.5 Partial Grading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169 C.6 Slot Reward Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205 C.7 Competitive Anticipation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212 C.8 Cooperative Anticipation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216"}, {"heading": "1 Introduction", "text": "Evaluation tools are crucial in any discipline as a way to assess its progress and creations. Artificial intelligence, as a discipline, lacks general, well-grounded and universally accepted intelligence measurement tools. In fact, artificial intelligence is a paradigmatic case of how useful these tools would be and how impeding this lack is. There are, of course, some tools, benchmarks and contests, aimed at the measurement of humanoid intelligence or the performance in a particular set of tasks. However, the evolution and state of the art of artificial intelligence is now more focussed towards social abilities, and here the measuring tools are still rather incipient.\nIn the past two decades, the notion of agent and the area of multi-agent systems have shifted artificial intelligence to problems and solutions where \u2018social\u2019 intelligence is more relevant. This shift towards a more social-oriented AI is related to the modern view of human intelligence as highly social, actually one of the most distinctive features of human intelligence over other kinds of animal intelligence. Some significant questions that appear here are then whether we are able to properly evaluate social intelligence in general (not only in AI, but universally) and whether we can develop measurement tools that distinguish between social intelligence and general intelligence.\nIn this paper, we address these questions by attempting a formalisation of social intelligence and some of its associated properties, such as social dependency and anticipation, and the properties a good test should have, such as discrimination, grading, reliability, boundedness, symmetry, efficiency, validity, etc. These properties help to: identify the components of social intelligence and its varieties; make clear that the mere appearance of other agents does not make a context social; pave the way for the analysis of whether many social environments, games and tests found in the literature are useful for measuring social intelligence. In particular, we will analyse a representative selection of environments, some usual in game theory, such as matching pennies and the prisoner\u2019s dilemma, and some more sophisticated (and realistic) social scenarios, such as the predator-prey (a pursuit game), Pac-Man or RoboCup Soccer. We will see whether they meet the properties a social intelligence testbed should have.\nOne side question that we will try to analyse here is whether social intelligence can be fully separated from general intelligence or, conversely, whether general intelligence can be seen as a special case of social intelligence where the presence and intelligence of other agents is not so relevant.\nThe paper is organised as follows. Section 2 gives an overview of the approaches to social intelligence from several disciplines. Section 3 presents a formal, parametric definition of social intelligence and specifies how a test can be derived from the definition. Section 4 discusses several properties that are needed (or desirable) for a good test of social intelligence. Section 5 discusses current social environments and games in AI, and their suitability to evaluate social intelligence following the definition and the properties. Finally, section 6 closes the paper with some discussion and future work."}, {"heading": "2 Background", "text": "Social intelligence (and its true distinction from general intelligence) has been a matter of study for many years. Many definitions have been proposed such as the \u201cability to understand and manage men and women, boys and girls \u2013 to act wisely in human relations\u201d [56], the \u201cability to get along with others\u201d [41], the \u201cfacility in dealing with human beings\u201d [60], or more specific definitions including \u201c[the] ability to get along with people in general, social technique or ease in society, knowledge of social matters, susceptibility to stimuli from other members of a group, as well as insight into the temporary moods or the underlying personality traits of friends and of strangers\u201d [58]. Nonetheless, none of these definitions is sufficiently formal and operational to provide a clear measurement procedure. In fact, all these definitions require the definition of many other new concepts that appear in the definition.\nDespite the ambiguity of what social intelligence is, many tests have been proposed to measure social intelligence in humans (see [62] for a survey). Typically there are two kinds of tests: 1) some use paper and pencil tests to measure social knowledge, and 2) others use more real situation tests (such as viewing photographies) in order to find out how people react in social situations. Some examples of these tests propose storylines which must be ordered to make sense, find the best end to a given joke, or select a correct emotion to a given face. However, there is a third, but unusual way: to measure social intelligence in terms of the definitions above, by confronting a human against other humans and see whether the subject deals with them or get along well. Apart from practical reasons that make this approach more difficult for testing, there are\nimportant questions, such as the selection and role of the other humans in order to make the test objective and effective.\nBut social intelligence is not only present in humans. Other animal species have also demonstrated this kind of intelligence. The evaluation of animals is more difficult, as we cannot ask them to perform a test as we do with humans, so the third type of tests is more common in this setting. Also, tests include some food as rewards in order to motivate the animals to perform the tasks. This is the same configuration as in reinforcement learning (RL) [54], where rewards are provided in order to encourage agents to perform tasks. In social intelligence tests for animals, especially for those focussing on cooperation, animals must obtain some food or reward that cannot be obtained by one individual alone, but two or more animals interacting are needed to get their reward. Some of the capabilities evaluated with those tests are their predisposition to deal with others, and their selfishness or altruism.\nAlthough these tests measure some aspects of social intelligence, many have been devised to evaluate social intelligence for a particular species and for very specific tasks. In these tests, it is highly debatable whether the tasks are representative of a broader view of social intelligence. Also, it is usually very difficult to compare the results with those of other species. Fortunately, there have been some exceptions to this (species) specialisation, and they are proliferating in the past decade. For instance, some recent work has shown that social abilities can be compared in a systematic way between human children and apes [28, 29].\nEven in the cases when the tests are generalised, they are still composed of a set of tasks that have been associated to social cognition indirectly, by observation or correlation, according to decades of experiments in the comparative cognition literature (see, e.g., [59, 51]). As a result, they cannot directly relate to the common definitions of social intelligence. For instance, one typical task used in social intelligence is to establish eye contact or to recognise oneself in a mirror (see, e.g., [52, pp. 452-453]). These tasks do not seem to be derived from any definition of social intelligence.\nIn order to elude this gap, many studies in ethology, comparative cognition and psychology just focus on specific issues, such as competition, cooperation, symbiosis, communication, group/swarm abilities, etc. However, in these scenarios, the emphasis is usually put on detecting and observing some of these phenomena, rather than properly evaluating abilities. For instance, prey-predator interaction and behaviour have been studied from many different points of view (including game theory [43]), but it is not clear how the ability of each subject can be objectively evaluated, especially because the interaction depends on the cognitive abilities of both prey and predator. For instance, lions are well prepared to predict zebra\u2019s movements and chase them, but they may be less able for other kinds of animals.\nDespite these difficulties, comparative cognition [59, 51] is more and more concerned about performing tests that compare the abilities of many different species and also the abilities of individuals of different species. From this point of view, it should be more natural to provide a single test (with possibly many different customised interfaces and rewards) to assess every kind of species (or, in other words, a more general, or universal [27, 19, 10], test). To achieve this, such a test should be able to evaluate any level and spectrum of social intelligence, instead of focussing on the specific range and particular abilities of a single species.\nWhen thinking about social tests and making them more species-independent, we can take the most general perspective, which leads us to the consideration of machines as well. However, evaluating social intelligence in machines has been quite different to the assessment of human and animal social intelligence. Nonetheless, as occurs with animals, rewards or scores may be used as a measure of their performance and a way of giving feedback to make them show their abilities. Besides, environments must be presented in such a way that a machine can process the observations and perform a set of actions. This is done by providing them with sensors and actuators that interact with our physical world, or provide them with a logical or virtual environment.\nThe environments used in social tests for machines tend to represent tasks that the agents must perform by interacting with other agents, so the performance is calculated as their capability to successfully cooperate with and/or compete against them to achieve some goals (see [3, 38] for two testbeds in multi-agent environments). In this way, the evaluation is a simulation in a social context, which is more directly linked to the definition of social intelligence.\nIn the context of social sciences (stretching from economics to AI), game theory [43] has also studied the interaction of different agents in formalised structures (called games). For this purpose, game theory uses a formal approach to define a utility function, and the effort is made for finding the best strategy among all possible strategies, assuming that the rest of agents also try to obtain their best results following some kind of rational actions. Although game theory needs the interaction of several agents, the goal is not to evaluate social intelligence but rather to analyse how the agents (or just policies) behave in these games and whether they\nreach some kind of equilibrium. Several kinds of games try to represent or to analyse a variety of properties: cooperative or non-cooperative games, simultaneous or sequential games, normal-form or extensive-form games, zero-sum or general-sum games, and symmetric or asymmetric games [43, 42]. However, games that may have the most interesting properties or applications are not necessarily useful for testing. For instance, a game where equilibria are easy may be inappropriate if we want a discriminative test. Similarly, asymmetric games make it more difficult to assess agent performance, as they depend on the role each agent takes.\nOne important concept in game theory is the notion of zero-sum vs. non-zero-sum games. Zero-sum games are a particular set of games where a player\u2019s gains (or losses) are equally balanced by the other players\u2019 losses (or gains). These kinds of games are known as competitive games, since one\u2019s gains reduces the gains (or increases the losses) of the other player(s), making the players having opposed interests. When a zero-sum game only has two players it becomes a pure competitive game. But zero-sum games can also contain cooperation in games with three or more players. Two players can cooperate in order to compete against a third or more players. As the number of players increase, cooperation becomes more important. In contrast, general-sum games are those games where the payoffs sum more or less than zero, and games can be cooperative even for two players. Finally, another particular feature in game theory is that environments are generally simple (without objects or spaces) and it is just the continuous interaction between agents that matters.\nMulti-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44]. Agents are usually evaluated according to their performance in some tasks interacting with other agents. Environments are usually selected to represent some particular problems for which techniques are developed and evaluated. However, these evaluations lack some important features. They do not evaluate social intelligence in a general way, but they are typically designed to evaluate one kind of task. However, the most important problem is that they usually require very specific abilities, or when they require many, it is not clear how to disentangle them. For instance, if a MAS setting requires both competition and cooperation to solve a problem, it is not always easy to select or gauge the degree of relevance of each one in order to give more relevance to competition over cooperation, or vice versa. Nonetheless, the major issue is that many capabilities other than social intelligence also contaminate the results, which makes many MAS scenarios unsuitable if we want to measure social abilities only.\nAs an alternative1, can we start from a formal definition of an ability and derive tests from it? This approach has been investigated for machine intelligence evaluation. Formal approaches started in the late 1990s using notions from Kolmogorov complexity, Solomonoff prediction and the MML principle [8, 23, 16, 17]. Dobrev [7] suggested that machine intelligence should be measured by evaluating agent performance in a range of worlds, an idea that was independently developed in [39] under the name \u201cUniversal Intelligence\u201d. This definition treats intelligence as a general notion, calculating it as the performance of the agent in a wide range of environments. Following this definition, a framework to evaluate intelligence [27] and an environment following the framework [24] were proposed. In order to show their effectiveness, some experiments were performed [33, 32, 35], but their results suggested that the framework still has some limitations. One of the possible reasons is that these environments lacked the richness of interaction. From the formal definition, it is virtually impossible to randomly generate an environment that contains some kind of social behaviour. Therefore, in order to overcome some of the limitations of these tests, some other agents need to be included in the environment to generate social situations. This was the goal in [36], where other agents were directly included in the environment. Some simple experiments were performed to evaluate machine social behaviour in environments where the agents were forced to compete and/or cooperate with other agents. The results of these experiments showed the impact on agents\u2019 performance when other agents are directly introduced in a test of general intelligence. These experiments were performed using the framework in [27], which was originally designed to evaluate general intelligence, by simply including other agents in the environment. Nonetheless, a general environment such as this one does not seem enough to evaluate social intelligence, since some abilities other than social intelligence are also evaluated in these kinds of environments. In order to measure social intelligence in isolation, we need to provide an appropriate environment class where only social intelligence is needed (or at least, where the degree of social intelligence needed can be fine tuned).\nIn this context, the key issue is to determine what kind of agents we must include in the test to interact with and what their roles are. This boils down to choosing a distribution of agents. However, in order to provide an environment with some rich social situations, we need first to know the level of social intelligence of the agents provided by the distribution. This circular problem is turned into a recursive one in [18], where different levels of distributions are recursively provided by constructing a new distribution of agents from a prior distribution\n1Not only as an alternative to MAS scenarios, but also to the Turing Test, CAPTCHAs and IQ tests (see [9] for a discussion).\nby selecting (or increasing the probability of) those agents with higher performance. However, it is not easy to derive a definition of social intelligence from here or a procedure to create environments that would be the base for social intelligence tests.\nOverall, there are many different approaches for the study and evaluation of social intelligence, but we lack a comprehensive theory, well-grounded tools and wide comparisons to better understand the problem and find better measurement devices."}, {"heading": "3 Defining Social Intelligence Universally", "text": "One way of reaching a universal definition of social intelligence is to consider more specific definitions and generalising them for any kind of subject. Thorndike\u2019s definition of social intelligence refers to \u201cmen, women, boys and girs\u201d [56]. So this approach would generalise this view with the variety of species in animal cognition, but also including machines, robots and other artificial systems. This is in the spirit of universal psychometrics [19], where we must consider any kind of agent (natural or artificial). Any of these systems can, in principle, be evaluated and can also be subject of interaction with the evaluee.\nThis can take us to definitions such as \u201cperformance of an agent in a wide range of environments while interacting with other agents\u201d [36]. As a result, we see clearly that social intelligence is a relative property, where we need to specify these other agents (and the range of environments).\nWith this approach, we distinguish those traits that have positive consequences on the performance (rewards) from those that are associated to social intelligence but do not necessarily lead to better performance (such as being generous, open, extroverted, etc.). In other words, we understand that an agent is socially intelligent if it has the ability to perform better in a social environment, but not if it is very sociable but showing very poor performance. In the end, we want an operational definition such that its measurement can be directly linked to it, and not derived by some other traits that are usually associated to social intelligence in humans and animals.\nSo we must focus our attention on the specification of the set of environments used for measuring and, most especially, on the characteristics of other agents. Nonetheless, it is important to determine the role these agents take in the environment relative to the evaluated agent. For instance, the environment can be populated by very intelligent agents, but the possibilities of an evaluated agent to achieve its goals will depend on whether these agents are allies or enemies, or more generally if they are cooperative or competitive. The key issue is to establish whether the other agents goals and interests are compatible with one\u2019s goals. The concept is complex, as alliances can be created and broken even if no clear teams are established from the beginning (and this is an interesting property of social intelligence). Nonetheless we have to consider the notion of role from the beginning and make it visible at the top, jointly with the kind of environment and the kind of agents.\nThese roles or alliances determine two major social behaviours: cooperation and competition. These are in fact linked to the issue that some agents share some goals while some other agents compete or are against other agents\u2019 goals. If we think of rewards (or any other kind of utility function) as a general way of expressing goals, interests and even resources they share or compete for, we can distinguish two major kinds of social intelligence:\nDefinition 1. Competitive social intelligence is the capability to obtain the best performance in an environment where other agents compete for the same rewards.\nDefinition 2. Cooperative social intelligence is the capability to obtain the best performance in an environment where other agents share the same rewards.\nNote that both definitions are not exclusive, as there are environments where both competitive and cooperative behaviours are possible. This is similar to the several degrees of general-sum games in game theory. Nonetheless, it would be very useful to have some way to analyse competition and cooperation separately (as two main facets of social intelligence). How clear-cut this separation can be done is an open question, as both abilities are occasionally correlated. For instance, the creation of alliances in a purely competitive scenario leads to temporary or permanent cooperation, where the other agents are seen in an instrumental way.\nIn what follows, we will see how these informal definitions can be formalised and integrated."}, {"heading": "3.1 Multi-agent environments and team rewards", "text": "Before addressing a formal integration of definitions 1 and 2, we need to give a definition of (multi-agent) environment. An environment is a world where an agent can interact through actions, rewards and observations\nas seen in figure 1 (left). This general view of the interaction between an agent and an environment can be extended to multi-agent systems by letting various agents interact simultaneously with the environment as seen in figure 1 (right).\nA multi-agent environment is an interactive scenario with several agents. An environment accepting n agents defines n parameters (one for each agent) denoted as slots. We use i = 1, . . . , n to denote the agent slots. Ai is the action set for agent in slot i and a = (a1, . . . , an) \u2208 A1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7An is a joint action profile of the n agents in the available discrete set of actions. Oi is the observation set that the agent in slot i can perceive and Ri \u2286 Q represents the possible rewards obtained by the agent in slot i. Both joint observation and reward profiles are denoted as o = (o1, . . . , on) and r = (r1, . . . , rn) respectively, similarly as we did with actions. For each step, every agent must receive an observation oi \u2208 Oi and a reward ri \u2208 Ri, and perform an action ai \u2208 Ai. We will use ai,k, ri,k and oi,k to respectively denote the action, reward and observation at step k for the agent in slot i. We use ak, rk and ok respectively to denote the joint actions, rewards and observations of all the agents at step k (i.e., a1 = (a1,1, . . . , an,1) represents the joint actions at step k = 1). The order of events is always: observation, action and reward. For example, a sequence of two steps in a multi-agent environment is then a string such as o1a1r1o2a2r2 and the string o1,1a1,1r1,1o1,2a1,2r1,2 denotes the sequence of observations, actions and rewards for the agent in slot 1.\nBoth the environment and the agents are defined as probabilistic measures. For the agent in slot i, the term \u03c0(ai,k|oi,1ai,1ri,1 . . . oi,k) \u2192 [0, 1] denotes its probability to perform action ai,k after the sequence of events oi,1ai,1ri,1 . . . oi,k. The observations provided by the environment to the agent in slot i also have a probabilistic measure \u03c9(oi,k|o1a1r1 . . . ok\u22121ak\u22121rk\u22121) \u2192 [0, 1]. As with observations, rewards are provided to the agent in slot i depending on observations and actions on previous steps \u03c1(ri,k|o1a1r1 . . . okak) \u2192 [0, 1]. Note that the rewards obtained by each agent depend on the joint actions, observations and rewards of all the agents interacting in the environment, and not only on their own. A random agent (usually denoted by \u03c0r) in slot i is an agent that chooses its actions from Ai using a uniform distribution.\nWe use A\u0306Ki (\u03c0, \u00b5) to denote the distribution (a probability measure) of strings representing the sequences of actions that \u03c0 performs in slot i of \u00b5 during K steps. If K is omitted, we assume that K is infinite, i.e., infinite sequences of actions for an endless episode. If agents and environment are deterministic (not stochastic) then this boils down to a probability measure giving probability 1 to one single string, the sequence of actions performed by \u03c0 on \u00b5.\nSimilarly, we use R\u0306Ki (\u03c0, \u00b5) to denote the distribution of reward strings of \u03c0 in slot i of \u00b5 during K steps. If K is omitted, we assume that K is infinite. Again, if neither agents nor environment are stochastic, this is just a string. In the general case, we use RKi (\u03c0, \u00b5) to denote the expected average reward (or value) with a discount factor \u03b3. For instance, for K = \u221e this is Ri(\u03c0, \u00b5) , E( lim K\u2192\u221e \u2211K k=1 \u03b3 k\u22121ri,k\u2211K k=1 \u03b3 k\u22121 ). Unless stated otherwise, we assume \u03b3 = 1."}, {"heading": "3.2 Teams", "text": "We need to address a characterisation of slots, such that we can specify how agents participate in the environment. This actually means that we need to decide how the environment distributes rewards among the agents. An easy possibility will be to make each agent get its rewards without further constraints over other\nagents\u2019 rewards. With this configuration (e.g., general-sum games), both competition and cooperation may be completely useless for most environments, as the rewards are not limited or linked to the other agents. In contrast, if we set that the total set of rewards is limited in some way, we will foster competition, as happens in zero-sum games. But in any of these two cases cooperation will hardly take place. Alliances could arise sporadically between at least two agents in order to bother (or defend against) a third agent. However, with low levels of social intelligence this seems unlikely to happen. For this reason we need to find a way to make agents cooperate, or at least to make it more likely before any (sophisticated) alliance can emerge on its own. One possible answer is the use of teams, defined as follows:\nDefinition 3. Agent slots i and j are in the same team iff \u2200k : ri,k = rj,k\nwhich means that all agents in a team receive exactly the same rewards. This differs from alliances, where the agents could receive different rewards. Note that teams are not alliances as usually understood in game theory. In fact, teams are fixed and cannot be changed by the agents. Also, we do not use the term alliance as we do not use any sophisticated mechanism to award rewards, related to the contribution of each agent in the team, as it is done with the Shapley Value [48]. We just set rewards uniformly.\nAt this moment, we are ready to define an environment with parametrised agents by only specifying their slots and their team arrangement.\nDefinition 4. A multi-agent environment \u00b5 accepting N(\u00b5) agents (i.e., the number of slots in \u00b5) is a tuple \u27e8A,R,O, \u03c9, \u03c1, \u03c4\u27e9, where A, R, O represent the action sets, reward sets and observation sets respectively (i.e., A = A1\u00d7\u00b7 \u00b7 \u00b7\u00d7AN(\u00b5), R = R1\u00d7\u00b7 \u00b7 \u00b7\u00d7RN(\u00b5) and O = O1\u00d7\u00b7 \u00b7 \u00b7\u00d7ON(\u00b5)) and \u03c9 and \u03c1 are the observation function and reward function respectively as defined in section 3.1. \u03c4 is a partition on the set of slots {1, . . . , N(\u00b5)}, where each set in \u03c4 represents a team.\nNote that with this definition the agents are not included in the environment. For instance, noughts and crosses could be defined as an environment \u00b5nc with two agents, where the partition set \u03c4 is defined as {{1}, {2}}, which represents that this game allows two teams, and one agent in each. Another example is RoboCup Soccer [38], denoted by \u00b5rc, whose \u03c4 would be {{1, 2, 3, 4, 5}, {6, 7, 8, 9, 10}}, which represents that there are two teams, with slots {1, 2, 3, 4, 5} in the first team and slots {6, 7, 8, 9, 10} in the second team.\nOnce environments are defined, without including the agents, now we can define an instantiation for a particular agent setup. Formally, a team line-up l is a list of agents. For instance, if we have a set of agents \u03a0 = {\u03c01, \u03c02, \u03c03, \u03c04}, a line-up from this set could be l1 = (\u03c02, \u03c03). The use of the same agent twice is allowed, so l2 = (\u03c01, \u03c01) is also a line-up. We denote by \u00b5[l] the instantiation of an environment \u00b5 with a line-up l, provided that the length of l is greater than or equal to the number of agents allowed by \u00b5 (if l has more agents, the excess is ignored). The slots of the environment are then matched with the corresponding elements of l following their order. For instance, for the noughts and crosses, an instantiation would be \u00b5nc[l1]. Note that different instantiations over the same environment would normally lead to different results. A line-up pattern l\u0307 is a list of agents where one or more elements are not instantiated. We can use instantiation to create more specific patterns or even to convert a pattern into a line-up. The instantiation of an agent \u03c0 at position i on line-up pattern l\u0307 of length n is denoted by l\u0307 i\u2190 \u03c0, which is exactly l\u03071:(i\u22121) \u00b7 \u03c0 \u00b7 l\u0307(i+1):n, where l \u00b7 m denotes the concatenation of lists l and m and lj:k denotes the elements in l from position j to k. This notation can be extended to instantiate several agents simultaneously using l\u0307 i,...,j\u2190 \u03c01, . . . , \u03c0n to represent l\u0307 i\u2190 \u03c01 \u00b7 \u00b7 \u00b7 j\u2190 \u03c0n. Once a line-up pattern l\u0307 has all its elements instantiated becomes a line-up l. Note that environments can only be instantiated with line-ups, so first we need to instantiate all the elements from a line-up pattern to convert it to a line-up, and then use it to instantiate an environment. For instance, a line-up pattern for the set of agents \u03a0 could be l\u03073 = (\u03c03, \u2217), where \u2217 represents an element that is not instantiated, and l\u03073 2\u2190 \u03c04 instantiates position 2 with agent \u03c04, converting the line-up pattern into the line-up l3 = (\u03c03, \u03c04). We will use Ln(\u03a0) to specify the set of all the line-ups of length n with agents of \u03a0, and L\u0307n\u2212i,...,j(\u03a0) to denote the set of all the line-up patterns of length n with agents of \u03a0 where positions i, . . . , j are not instantiated. For instance, L\u0307n\u2212i(\u03a0) defines the set of all possible line-up patterns {l\u03071:i\u22121 \u00b7 \u2217 \u00b7 l\u0307i+1:n} from the set of agents \u03a0.\nWe will use line-up patterns with positions i, . . . , j not being instantiated to evaluate agents in these positions, while the rest of the line-up pattern will be instantiated with the agents they will have to interact with.\nwL denotes weights to line-ups formed with agents from a certain set \u03a0, giving weights to the agents in the line-up and their positions. Similarly wL\u0307 denotes weights for line-up patterns formed with agents from \u03a0. As line-ups and line-up patterns are clearly related, we will assume that wL\u0307 can be derived from wL.\nAssumption 1. The value of wL\u0307 for line-up patterns with only one element that is not instantiated is derived from wL as:\n\u2200i, n,\u03a0, l\u0307 \u2208 L\u0307n\u2212i(\u03a0) : wL\u0307(l\u0307) = \u2211 \u03c0\u2208\u03a0 wL(l\u0307 i\u2190 \u03c0)\nand wL\u0307 for line-up patterns with 2 or more non-instantiated elements is recursively derived as: \u2200i, . . . , j, n,\u03a0, l\u0307 \u2208 L\u0307n\u2212i,...,j(\u03a0) : wL\u0307(l\u0307) = \u2211 \u03c0\u2208\u03a0 wL\u0307(l\u0307 i\u2190 \u03c0) = \u00b7 \u00b7 \u00b7 = \u2211 \u03c0\u2208\u03a0 wL\u0307(l\u0307 j\u2190 \u03c0)\nFinally, note that we can calculate the expected average reward for any agent in line-up l in an environment \u00b5. We will use the notation Ri(\u00b5[l]), which gives us the expected average reward of the ith agent in line-up l for environment \u00b5 (also in slot i). We can extend this notation to distributions as well (i.e. A\u0306i(\u00b5[l]) for distribution of action sequences and R\u0306i(\u00b5[l]) for distribution of reward sequences)."}, {"heading": "3.3 A formal definition of social intelligence", "text": "Having these ideas in mind we can now attempt a first definition of social intelligence. We first fix the line-up and vary on the possible environments.\nDefinition 5. We define the ability of an agent \u03c0 interacting in line-up l which contains \u03c0 at position i, over a set of environments M accepting at least i agents and at most |l| agents (being | \u00b7 | the length of a list), weighting the environments by wM as:\n\u03a5i(l,M,wM ) , \u2211 \u00b5\u2208M wM (\u00b5)Ri(\u00b5[l1:N(\u00b5)]) (1)\nwhere |M | \u2265 1.\nAlternatively, we can think about a definition of the ability of an agent for a varying set of line-ups while fixing the environment.\nDefinition 6. We define the ability of an agent \u03c0 in slot i, interacting in an environment \u00b5 accepting at least i agents, with a set of line-up patterns defined over agent set \u03a0 and wL\u0307 as a weight for line-up patterns for the environment \u00b5:\n\u03a5i(\u03c0,\u03a0, wL\u0307, \u00b5) , \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u2212i (\u03a0)\nwL\u0307(l\u0307, \u00b5)Ri(\u00b5[l\u0307 i\u2190 \u03c0]) (2)\nwhere N(\u00b5) \u2265 1 and if N(\u00b5) > 1 then |\u03a0| \u2265 1 otherwise |\u03a0| \u2265 0.\nIn the definition we have used a weight wL\u0307 that depends on \u00b5 (i.e., wL\u0307(l\u0307, \u00b5)). Note that wL\u0307 applies to l\u0307, giving weights to the agents (and their positions) such that the evaluated agent \u03c0 interacts in the environment \u00b5 when it is located at position i. Note also that now l\u0307 has always N(\u00b5) elements when instantiated, so now no upper-restriction exists over the number of slots of \u00b5.\nIn fact, both wM and wL\u0307 could be integrated into a single weight for instantiated environments. However, we want to decouple agents and environments and use both of them as independent parameters. To make the agents and the environment independent we work with the next assumption.\nAssumption 2. If wL(l, \u00b5) and wL\u0307(l\u0307, \u00b5), where l \u2208 LN(\u00b5)(\u03a0) and l\u0307 \u2208 L\u0307 N(\u00b5) \u2212i (\u03a0), are independent of \u00b5 then:\n\u2200l, \u00b5 : wL(l, \u00b5) = wL(l) \u2200l\u0307, \u00b5 : wL\u0307(l\u0307, \u00b5) = wL\u0307(l\u0307)\nAssumption 2 gives the same weight to any line-up and line-up pattern whatever the environment. In what follows we will use wL(l) and wL\u0307(l\u0307) as a weight for line-up l and line-up pattern l\u0307 respectively.\nUnder the assumption 2 we can integrate both equations 1 and 2 as follows:\nDefinition 7. The social intelligence of an agent \u03c0 in slot i, interacting with the class of agents \u03a0 with a weight for line-up patterns wL\u0307, in a set of environments M with a weight for environments wM is defined as:\n\u03a5i(\u03c0,\u03a0, wL\u0307,M,wM ) , \u2211 \u00b5\u2208M wM (\u00b5) \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u2212i (\u03a0)\nwL\u0307(l\u0307)Ri(\u00b5[l\u0307 i\u2190 \u03c0]) (3)\nwhere |M | \u2265 1,\u2200\u00b5 \u2208 M then N(\u00b5) \u2265 1 and if \u2203\u00b5 \u2208 M |N(\u00b5) > 1 then |\u03a0| \u2265 1 otherwise |\u03a0| \u2265 0.\nAnd summing the performance over all possible slots of the environments of M , weighting the slots of each environment by wS , we have:\nDefinition 8. The social intelligence of \u03c0 interacting with the class of agents \u03a0 with a weight for line-up patterns wL\u0307, in a set of environments M with a weight for environments wM and a weight for slots wS is defined as:\n\u03a5(\u03c0,\u03a0, wL\u0307,M,wM , wS) , \u2211 \u00b5\u2208M wM (\u00b5) N(\u00b5)\u2211 i=1 wS(i, \u00b5) \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u2212i (\u03a0)\nwL\u0307(l\u0307)Ri(\u00b5[l\u0307 i\u2190 \u03c0]) (4)\nwhere |M | \u2265 1,\u2200\u00b5 \u2208 M then N(\u00b5) \u2265 1 and if \u2203\u00b5 \u2208 M |N(\u00b5) > 1 then |\u03a0| \u2265 1 otherwise |\u03a0| \u2265 0.\nThis equation now removes the lower-restriction of the number of slots on the environments. Finally the positions of the agents in the line-up patterns can be assumed independent:\nAssumption 3. If w\u03a0(\u03c0, i) defines the weight for the agent \u03c0 appearing at position i in a line-up or line-up pattern, we assume w\u03a0(\u03c0) to appear in all positions in a line-up or line-up pattern. Formally:\n\u2200\u03c0, i : w\u03a0(\u03c0, i) = w\u03a0(\u03c0)\nUnder the assumption 3, wL and wL\u0307 can be derived as a function of terms from w\u03a0. Finally, we assume that the probability of an agent in a line-up and line-up pattern is independent of its position.\nAssumption 4. We calculate wL as a product of agents weights w\u03a0 as: \u2200n,\u03a0, l \u2208 Ln(\u03a0) : wL(l) = \u220f\n1\u2264k\u2264n\nw\u03a0(lk:k)\nand wL\u0307 is calculated as a product of agent weights w\u03a0 as: \u2200i, n,\u03a0, l\u0307 \u2208 L\u0307n\u2212i(\u03a0) : wL\u0307(l\u0307) = \u220f\n1\u2264k<i\nw\u03a0(l\u0307k:k) \u220f\ni<k\u2264n\nw\u03a0(l\u0307k:k) (5)\nThis assumption is also extended to line-up patterns with 2 or more elements not instantiated as well.\nProposition 1. Under assumptions 3 and 4, social intelligence as per equation 4 is also defined as:\n\u03a5(\u03c0,\u03a0, wL\u0307,M,wM , wS) = \u03a5(\u03c0,\u03a0, w\u03a0,M,wM , wS) =\n= \u221e\u2211 j=1 j\u2211 i=1 \u2211 l\u0307\u2208L\u0307j\u2212i(\u03a0)  \u220f 1\u2264k<i w\u03a0(l\u0307k:k) \u220f i<k\u2264j w\u03a0(l\u0307k:k)  \u2211 \u00b5\u2208Mj wM (\u00b5)wS(i, \u00b5)Ri(\u00b5[l\u0307 i\u2190 \u03c0])\nwhere M j denotes all the environments in M with j agent slots, |M | \u2265 1, and if \u2203\u00b5 \u2208 M |N(\u00b5) > 1 then |\u03a0| \u2265 1 otherwise |\u03a0| \u2265 0.\nProof. Definition 8 ranges over environments, their slots and then over line-up patterns, but we could express an equivalent equation by ranging over line-up patterns first and environments and their slots next:\n\u03a5(\u03c0,\u03a0, wL\u0307,M,wM , wS) = \u2211 \u00b5\u2208M wM (\u00b5) N(\u00b5)\u2211 i=1 wS(i, \u00b5) \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u2212i (\u03a0)\nwL\u0307(l\u0307)Ri(\u00b5[l\u0307 i\u2190 \u03c0]) =\n= \u2211 \u00b5\u2208M wM (\u00b5) N(\u00b5)\u2211 i=1 wS(i, \u00b5) \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u2212i (\u03a0)\n \u220f 1\u2264k<i w\u03a0(l\u0307k:k) \u220f i<k\u2264N(\u00b5) w\u03a0(l\u0307k:k) Ri(\u00b5[l\u0307 i\u2190 \u03c0]) = =\n\u221e\u2211 j=1 \u2211 \u00b5\u2208Mj wM (\u00b5) j\u2211 i=1 wS(i, \u00b5) \u2211\nl\u0307\u2208L\u0307j\u2212i(\u03a0)\n \u220f 1\u2264k<i w\u03a0(l\u0307k:k) \u220f i<k\u2264j w\u03a0(l\u0307k:k) Ri(\u00b5[l\u0307 i\u2190 \u03c0]) = = \u03a5(\u03c0,\u03a0, w\u03a0,M,wM , wS) =\n\u221e\u2211 j=1 j\u2211 i=1 \u2211 l\u0307\u2208L\u0307j\u2212i(\u03a0)  \u220f 1\u2264k<i w\u03a0(l\u0307k:k) \u220f i<k\u2264j w\u03a0(l\u0307k:k)  \u2211 \u00b5\u2208Mj wM (\u00b5)wS(i, \u00b5)Ri(\u00b5[l\u0307 i\u2190 \u03c0])\nThis shows how we can parametrise the definition in terms of the weight of the other participants (w\u03a0) independently of their order in line-up patterns. For instance, the weight for each agent could depend on its (social) intelligence, provided we are able to estimate this value. The use of a product of weights makes sense if w\u03a0 is a unit measure.\nThe interpretation of the above definition is the expected performance of agent \u03c0 interacting with all possible line-up patterns generated using the set of agents \u03a0, and in a set of environments M with \u03c0 playing at all possible slots in each environment2.\nProposition 1 is not only useful for parametrising the definition in terms of the agents in isolation, but also because it decouples agents from environments. This makes sense in the context of (social) intelligence evaluation, as we want to consider other agents that are able to work in different environments, and not very specific agents that only work in one environment.\nDefinition 8 and its reformulation by proposition 1 integrates all kinds of social behaviour, as it does not distinguish between agents appearing in the same team or opponent teams. For instance, if we consider a set \u03a0 with very intelligent agents, some environments (and line-ups) will be easier if many of these agents appear in the same team, but will be harder if they appear in opponent teams. Also, the aggregation may consider many other environments where no social behaviour takes place. This means that the above equations are a skeleton for the definition, but we need to better analyse the pair (\u03a0, wL\u0307) or (\u03a0, w\u03a0) and the triplet (M , wM , wS)."}, {"heading": "3.4 Tests", "text": "A definition is not a test, most especially because many definitions range over infinite sets or an infinite number of steps. A test must be a finite procedure that can be feasibly applicable to an agent. For the moment, we will focus on non-adaptive tests, which are based on performing just a finite number of finite experiments or trials (episodes), which are independent of the previous ones.\nConsequently, a test is defined using the definition of \u03a5(\u03c0,\u03a0, wL\u0307,M,wM , wS), where \u03a0 is sampled with some distribution, M is sampled with some distribution and the number of steps for each experiment is limited in some way. Sampling is understood to be without replacement when there is determinism (it does not make sense to repeat the same episode if the result is already known) but is understood to be with replacement for non-deterministic agents or environments. We denote by S \u223cn [A]p a sample S of n elements from set A using probability distribution p for the powerset of A, i.e., for 2A. The use of a distribution over samples instead of a distribution over exercises gives more flexibility about the conditions that we could establish on the sampling procedure. For instance, we could define a sample probability such that high diversity is ensured (apart from high accumulated relevance of the exercises that are chosen) or such that a range of difficulties is covered. Keep\n2The above definitions could be simplified if for every environment \u00b51 and slot i in it, there is always an environment \u00b52 with exactly the same behaviour where slot i becomes 1. That means that we could easily get rid of the summation over slots and work just with slot 1 for agent \u03c0. In other words, this would be like considering that the evaluated agent always plays with slot 1. As we will discuss later on, if the environments are symmetric, this problem also vanishes. Another option is to consider a uniform distribution for slots.\nin mind that with this definition, the issues of with replacement or without replacement is re-understood as whether these samples allow repeated values or not. With this notation, we can give the following definition of test:\nDefinition 9. A test over \u03a5 (definition 8 in the previous subsection), denoted by \u03a5\u0302[p\u03a0, pM , pS , pK , nE ], is a sample of nE episodes (or exercises) from all those summed in the definition, using agent distribution p\u03a0, an environment distribution pM , a slot distribution pS , and a distribution on the number of steps pK .\n\u03a5\u0302[p\u03a0, pM , pS , pK , nE ](\u03c0,\u03a0, wL\u0307,M,wM , wS) , \u03b7E \u2211\n\u27e8\u00b5,i,l\u0307\u27e9\u2208E wM (\u00b5)wS(i, \u00b5)wL\u0307(l\u0307)R\nK i (\u00b5[l\u0307 i\u2190 \u03c0])\nwhere \u03b7E normalises the formula with \u03b7E = 1\u2211\n\u27e8\u00b5,i,l\u0307\u27e9\u2208E wM (\u00b5)wS(i,\u00b5)wL\u0307(l\u0307) , K is chosen using probability distribution\npK and the episodes (or exercises) E are sampled as: E \u223cnE  \u22c3 \u00b5\u2208M N(\u00b5)\u22c3 i=1 {\u2329 \u00b5, i, l\u0307 \u232a : l\u0307 \u2208 L\u0307N(\u00b5)\u2212i (\u03a0) } pE\nwith pE being a distribution on the set of triplets \u2329 \u00b5, i, l\u0307 \u232a based on pM , pS and p\u03a0.\nNote that we use p\u03a0 for the line-up patterns, which could be the line-up pattern probability derived as the product of the probabilities of each agent in the line-up pattern following assumption 3, as in equation 5. As we will see, if the environments are symmetric, we can get rid of pS and wS and just evaluate for slot 1.\nIt is important not to confuse the probabilities of sampling the line-up patterns, environments, slots and number of steps (p\u03a0, pM , pS , pK) with any weight defined on them, in particular, the weights wL\u0307, wM and wS defined on line-up patterns, environments and slots respectively. Weights represent the relevance of an environment, their slots and line-up pattern for the definition (so it determines the abilities, roles and agents that have higher weight in the formula), while the distributions are just a way of sampling the usually large or infinite set of environments, slots and line-ups of agents. Weights and distributions might be related (or may be equal in order to ensure fast convergence to the actual value), but some other considerations may suggest that a less relevant case (low weight) can be sampled with high probability, as it may be highly representative or more robust, for instance. Actually, we want that a diversity of cases is sampled, rather than similar cases that will provide redundant information. This is why we use a distribution on 2A and not on A because otherwise we would not be able to measure how good (e.g., informative) a set of trials is. We will discuss this issue again in the following section in the context of reliability and efficiency."}, {"heading": "4 Properties about social intelligence testbeds", "text": "In order to evaluate social intelligence and distinguish it from general intelligence, we need tests where social ability has to be used and, also, where we can perceive its consequences. This means that not every environment is useful for measuring social intelligence and not every subset of agents is also useful. We want tests such that the evaluated agent must use its social intelligence to understand and/or have influence over other agents\u2019 policies in such a way that this is useful to accomplish the evaluated agent\u2019s goals. We also need situations where common general intelligence is not enough. In a way, we want to subtract (from the summation of all environments and line-up patterns) those problems (as defined by classes of environments and agents) where general intelligence is enough (and social intelligence is useless) and those where intelligence (of any kind, social or non-social) is useless.\nWe will investigate some properties that are hence desirable (or necessary) for a testbed of environments3 and agents to actually measure social intelligence. Some other properties are more of a practical nature, such as the degree of discrimination and grading of the environment, the symmetry of slots, its reliability and efficiency.\nHereinafter, we will differentiate two kinds of set of agents: the set of agents we want to evaluate, denoted by \u03a0e, and the set of agents we want the environment to be populated with as opponents and team players,\n3In what follows, we will explore some properties that can be applied to sets of environments rather than single environments. A definition or test can be composed of just one environment if M has only one element (as for definition 8 having |M | = 1). Actually, we will give the definitions for one environment but they are easily extensible to a family or distribution of environments with a weight function used as a testbed.\ndenoted by \u03a0o 4. On many occasions both sets can be set equal, but it will be useful to keep them as separate sets. Figure 2 gives a summary of the properties we will introduce in this section. They have different purposes and will reach different levels of formalisation. Actually, many of the properties (the quantitative ones) will be of the form Prop(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS), i.e., given these two sets \u03a0e and \u03a0o, and the weights for them (in the form of agent weight w\u03a0e and line-up weight wL\u0307 respectively), they give a value for a given environment \u00b5 and slot weight wS .\nFinally, some of the properties below are presented for two slots but they could be extended to three or more slots as well. Similarly, all the following definitions are introduced for one environment, but they can be easily extended for a set of environments as well.\nNext we will analyse and formalise these properties."}, {"heading": "4.1 Boundedness", "text": "One property that we need to impose in order to make many of the previous definitions meaningful is that rewards must be bounded (otherwise, some summations will diverge). Any arbitrary choice of upper and lower bounds can be scaled to any other choice so, without loss of generality, we can assume that all of them are bounded between \u22121 and 1. Formally:\n\u2200i, k : \u22121 \u2264 ri,k \u2264 1\nNote that they are bounded for every step. As Ri(\u00b7) is an average, then it is bounded as well if the rewards are bounded.\nHowever, bounded rewards do not ensure that the measurement from definition 8 is bounded. In order to ensure a bounded result we also need to consider that weights are bounded, i.e., there are constants cM , cS , cL\u0307 and c\u03a0 such that:\n4Instead of using only one set for opponents and team players, it can also be extended by using two different sets; one for opponent players and another for team players.\n\u2200M : \u2211 \u00b5\u2208M wM (\u00b5) = cM (6)\n\u2200\u00b5 : N(\u00b5)\u2211 i=1 wS(i, \u00b5) = cS (7)\n\u2200i, n,\u03a0o : \u2211\nl\u0307\u2208L\u0307n\u2212i(\u03a0o)\nwL\u0307(l\u0307) = cL\u0307 (8)\n\u2200\u03a0 : \u2211 \u03c0\u2208\u03a0 w\u03a0(\u03c0) = c\u03a0 (9)\nEquation 8 can also be applied for two or more non-instantiated slots and equation 9 is used when assumption 4 is made.\nA convenient choice would be to have cM = cS = cL\u0307 = c\u03a0 = 1, and these weights would become unit measures (which should not be confused with the probabilities used in definition 9). The expression of wL\u0307 in terms of a product of w\u03a0 make more sense if w\u03a0 is a unit measure. With these conditions \u03a5 and \u03a5\u0302 are bounded 5\nAn optional property that might be interesting occasionally is to consider environments whose reward sum is constant. Without loss of generality, we can take this constant to be zero, which leads to the well-known notion of zero-sum games in game theory.\nDefinition 10. An environment \u00b5 is zero-sum if and only if:\n\u2200k : N(\u00b5)\u2211 i=1 ri,k = 0 (10)\nThe above definition may be too strict when we have environments with an episode goal at the end, but we want some positive or negative rewards to be given while agents approach the goal. A more convenient version follows:\nDefinition 11. An environment \u00b5 is zero-sum in the limit iff:\nlim K\u2192\u221e K\u2211 k=1 N(\u00b5)\u2211 i=1 ri,k = 0 (11)\nNote that if we have teams, the previous definition could be changed in such a way that:\nlim K\u2192\u221e K\u2211 k=1 \u2211 t\u2208\u03c4 \u2211 i\u2208t ri,k = 0 (12)\nSo the sum of the agents\u2019 rewards in a team (or team\u2019s reward) does not need to be zero but the sum of all teams\u2019 rewards does. For instance, if we have a team with agents {1, 2} and another team with agents {3, 4, 5}, then a reward (in the limit) of 1/4 for agents 1 and 2 will imply \u22121/6 for each of the agents in the other team.\nThe zero-sum properties are appropriate for competition between teams. In fact, if teams have only one agent then we have pure competitive environments. We can have both competition and cooperation by using teams in a zero-sum game, where agents in a team cooperate and agents in different teams compete. If we want to evaluate pure cooperation (with one or more teams) then zero-sum games will not be appropriate."}, {"heading": "4.2 Interactivity", "text": "By interactivity we mean the property that agents\u2019 actions have implications on the actions (and rewards) of the other agents. This is a key property as the existence of several agents in an environment does not ensure, per se, any social behaviour. In fact, it is important to realise that the use of several agents and their arrangement into\n5Note that we are talking about the measure. For instance, \u03a5 can be a measure that represents the, e.g., sigmoid function of an unbounded magnitude, easily recovered with a logit or probit function.\nteams does not ensure that some social behaviour can ever take place. Imagine a non-social environment, such as finding the way out of a maze with no other agents. Rewards depend on the agent finding the way out or not. While this is clearly non-social, we can use this environment as a building block and create an environment that takes two agents but makes them play separately on two equal mazes. We can generate rewards in at least four different ways: (1) we can give rewards separately without any modification on the outputs of the building blocks, (2) we can normalise them to a constant or a zero-sum (in the spirit of definition 1), (3) we can average both rewards and give them to both agents (in the spirit of definition 2) or (4) any other combination of the rewards, including a stochastic (non-deterministic) combination. Note that none of these four options would contain or foster any kind of social behaviour. In fact, no agent is aware of the other (apart from the effect on rewards). However, the rewards can get highly correlated (as in ways 2 or 3 above) and do so in a non-additive or functional way.\nThe explanation of why there is no social behaviour in this case is that there is no influence between the actions of both agents. As a result, the big issue about choosing social contexts is how to determine that an agent has influence on other\u2019s actions. In fact, this is at the roots of definitions of interaction [65, 11, 34, 12], and the distinction between several kinds of interaction [37]. Some other approaches have looked for some common information content between the peers. However, as pointed out by [12], \u201cthis may originate from a common source\u201d, so common or mutual information is not sufficient for interaction to have taken place.\nSo we need a measure of interaction that is not based on correlation or common information content. However, the degree of influence that other agents may have on the actions of the agent we are evaluating is difficult to grasp; As environments and agents can be non-deterministic, changes can appear just randomly."}, {"heading": "4.2.1 Action Dependency", "text": "We need to take a different approach. The key idea defines interaction in terms of sensitivity to other agents or, in other words, whether the inclusion of different agents in the environment has an effect on what the evaluated agent does. One formalisation of this idea goes as follows:\nDefinition 12. The action dependency degree for evaluated agent \u03c0 playing in slot i in environment \u00b5 with a class of opponents and team players \u03a0o with a weight of line-up patterns wL\u0307, is given by:\nADi(\u03c0,\u03a0o, wL\u0307, \u00b5) , \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u2212i (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u0306i(\u00b5[u\u0307 i\u2190 \u03c0]), A\u0306i(\u00b5[v\u0307 i\u2190 \u03c0])) (13)\nwhere \u03b7L\u03072 normalises the formula with \u03b7L\u03072 = 1\u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u2212i (\u03a0o)|u\u0307 \u0338=v\u0307 wL\u0307(u\u0307)wL\u0307(v\u0307)\n, \u2206S is a divergence function between\nstring distributions, |\u03a0o| \u2265 2, N(\u00b5) \u2265 2 and \u2203u\u0307, v\u0307 \u2208 L\u0307N(\u00b5)\u2212i (\u03a0o)|u\u0307 \u0338= v\u0307, wL\u0307(u\u0307) > 0 and wL\u0307(v\u0307) > 0.\nNote that wL\u0307 can be written in terms of w\u03a0o if we assume independence for the environment and agent positions (assumption 2 and 3), as we did in assumption 4. Note also that A\u0306 returns a distribution of action sequences if the environment or any of the agents is non-deterministic. If ADi is high, then the proportion of cases where two team line-up patterns lead to different sequences of actions for \u03c0 is high. This means that \u03c0 is highly sensitive in this environment about who else is in the environment. Conversely, if we have that for many pairs of line-up patterns the sequences of actions of \u03c0 are similar, this means that \u03c0\u2019s actions are not affected by other agents.\nThe previous definition is relative to a distribution of line-up patterns on a population of agents, but it is given for a particular evaluated agent \u03c0. We may have that one evaluated agent can be very insensitive to line-up pattern changes, but other evaluated agents can be more sensitive in the same environment. If we want to generalise this for a class of evaluated agents \u03a0e we have:\nDefinition 13. The action dependency degree for a set of evaluated agents \u03a0e with associated weight w\u03a0e playing in slot i in environment \u00b5 with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nADi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) , \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)ADi(\u03c0,\u03a0o, wL\u0307, \u00b5) (14)\nwhere |\u03a0e| \u2265 1.\nThis definition is given only for slot i. Finally, we need to aggregate the action dependency degree for all slots:\nDefinition 14. The action dependency degree for a set of evaluated agents \u03a0e with associated weight w\u03a0e in environment \u00b5 with weight of slots wS , a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307, is given by:\nAD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) , N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) (15)\nwhere N(\u00b5) \u2265 1.\nIt certainly remains to clarify what \u2206S can be. For deterministic cases an edit distance can be used. However, for non-deterministic cases we need to find alignments between the distributions or aggregate strings into some prototypes and compare them. One simple approach for both the deterministic and non-deterministic cases could be based on comparing action frequencies (independently of their order) or, alternatively, n-grams. We will see some specific examples in section 5. Note that different \u2206S functions may lead to different interpretations of action influence. For instance, there can be environments where a first few actions are interactive, but then no interaction takes places any more. In this case, the strings may be very different, but the degree, or more precisely, the timespan of interaction is small (like a butterfly effect)."}, {"heading": "4.3 Non-neutralism", "text": "The existence of interaction between agents does not ensure that these interactions are meaningful in terms of rewards. For instance, two agents can interact, but they may not affect each other\u2019s rewards. This, in ecological terms, is known as \u2018neutralism\u2019. In fact, in ecology, given two species, there are seven possible combinations of positive, negative or no effect between them, leading to six forms of symbiosis [40]: neutralism (0,0), amensalism (0,-), commensalism (+,0), competition (-,-), mutualism (+,+), and predation/parasitism (+,-). In our case, as we want to characterise environments that may contain individuals (possibly more than two), we can simplify this to neutralism, cooperation (including commensalism and mutualism) and competition (including the rest). In other words, we want to analyse whether interaction has no effect on rewards, has a positive relation or a negative one."}, {"heading": "4.3.1 Reward Dependency", "text": "So, the first thing that we need to determine is whether there is a dependency in rewards. This is very similar to the action dependency seen above:\nDefinition 15. The reward dependency degree for evaluated agent \u03c0 playing in slot i in environment \u00b5 with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307, is given by:\nRDi(\u03c0,\u03a0o, wL\u0307, \u00b5) , \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u2212i (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(Ri(\u00b5[u\u0307 i\u2190 \u03c0]), Ri(\u00b5[v\u0307 i\u2190 \u03c0])) (16)\nwhere \u03b7L\u03072 normalises the formula with \u03b7L\u03072 = 1\u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u2212i (\u03a0o)|u\u0338\u0307=v\u0307 wL\u0307(u\u0307)wL\u0307(v\u0307)\n, \u2206Q is a divergence function for\nrational numbers, |\u03a0o| \u2265 2, N(\u00b5) \u2265 2 and \u2203u\u0307, v\u0307 \u2208 L\u0307N(\u00b5)\u2212i (\u03a0o)|u\u0307 \u0338= v\u0307, wL\u0307(u\u0307) > 0 and wL\u0307(v\u0307) > 0.\nNote that we use expected average rewards instead of a history of rewards. So, here the divergence compares numbers. For instance, we can use \u2206Q(a, b) = 1 \u2212 \u03b4(a, b), where \u03b4 is the Kronecker delta function (\u03b4(a, b) = 1 if a = b and 0 otherwise). With this choice, the previous function would boil down to the probability that by taking two team line-up patterns (using a weight or distribution wL\u0307), after instantiating both with \u03c0 in slot i, the expected average rewards of \u03c0 are different. Another option could be relative absolute difference, i.e., \u2206Q(a, b) = |a\u2212b| |a|+|b| .\nNow, we can generalise this for a set of evaluated agents \u03a0e:\nDefinition 16. The reward dependency degree for a set of evaluated agents \u03a0e with associated weight w\u03a0e playing in slot i in environment \u00b5 with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nRDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) , \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)RDi(\u03c0,\u03a0o, wL\u0307, \u00b5) (17)\nwhere |\u03a0e| \u2265 1.\nSo now we measure how dependent the rewards are in general (for any evaluated agent in \u03a0e). The previous definition may slightly resemble the Shapley Value [48] in cooperative game theory, but here we are not concerned about how relevant each agent is in a team (whether its contribution is higher than the contribution of its teammates), but to see whether there is effect on the rewards.\nFinally, we aggregate for all slots:\nDefinition 17. The reward dependency degree for a set of evaluated agents \u03a0e with associated weight w\u03a0e in environment \u00b5 with weight of slots wS , a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) , N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) (18)\nwhere N(\u00b5) \u2265 1."}, {"heading": "4.3.2 Slot Reward Dependency", "text": "Both definitions 14 and 17 are necessary as we can have reward dependency without action dependency and action dependency without reward dependency.\nNow, we are interested in telling the sign of this dependency, i.e., how much cooperative or competitive this is.\nDefinition 18. The slot reward dependency for evaluated agent \u03c0 playing in slot i with slot j (with i \u0338= j) in environment \u00b5 with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nSRDi,j(\u03c0,\u03a0o, wL\u0307, \u00b5) , corrl\u0307\u2208L\u0307N(\u00b5)\u2212i (\u03a0o) [wL\u0307(l\u0307)](Ri(\u00b5[l\u0307\ni\u2190 \u03c0]), Rj(\u00b5[l\u0307 i\u2190 \u03c0])) (19)\nwhere corrx\u2208X [w](a, b) is a weighted (w) correlation function between a and b for the elements generated by X, |\u03a0o| \u2265 1 and N(\u00b5) \u2265 2.\nAny correlation function can be used here (Pearson, Spearman, etc.). Clearly, if two slots are in the same team, from definition 3, we have that SRD is 1. In the case of a zero-sum game with only two teams, any two slots of different teams have a SRD of \u22121 (provided there is at least one \u2018match\u2019 which is not a tie). Note that as usual with correlation measures, if we have that two slots are reward independent, then SRD is 0. However, having SRD = 0 does not necessarily imply independency. This means that we need to calculate the reward dependency degree and then ask whether this dependency is positive or negative for pairs of slots.\nNow, we can generalise this for a set of evaluated agents \u03a0e:\nDefinition 19. The slot reward dependency for a set of evaluated agents \u03a0e with associated weight w\u03a0e playing in slot i with slot j (with i \u0338= j) in environment \u00b5 with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nSRDi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) , \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)SRDi,j(\u03c0,\u03a0o, wL\u0307, \u00b5) (20)\nwhere |\u03a0e| \u2265 1.\nFinally we aggregate all combinations of pairs of slots:\nDefinition 20. The slot reward dependency for a set of evaluated agents \u03a0e with associated weight w\u03a0e in environment \u00b5 with weight of slots wS , with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nSRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) , \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)SRDi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)SRDi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  (21)\nwhere \u03b7S21 normalises the formula with \u03b7S21 = 1\u2211N(\u00b5) i=1 wS(i,\u00b5) (\u2211i\u22121 j=1 wS(j,\u00b5)+ \u2211N(\u00b5) j=i+1 wS(j,\u00b5) ) , N(\u00b5) \u2265 2 and \u2203i, j|1 \u2264 i \u2264 N(\u00b5), 1 \u2264 j \u2264 N(\u00b5), i \u0338= j, wS(i, \u00b5) > 0 and wS(j, \u00b5) > 0.\nIn practice, in order to evaluate social abilities, we require environments with high RD. Then, depending on the use of teams and normalisations, we can gauge whether we want to evaluate competition or cooperation, and have some positive SRD with some slots and some negative SRD with some other slots. This is easily obtained by using teams."}, {"heading": "4.4 Secernment", "text": "It is an important characteristic for a test to be able to give different values for different evaluated agents. Otherwise, if the results are the same (or very similar) for most evaluated agents, we get little information. In other words, we want tests (i.e., environment and set of agents populating it) to secern, to be discriminative. Although there are many approaches to the idea of discriminating power (see e.g., [26]), one simple notion that accounts for this concept quite well is the variance of results."}, {"heading": "4.4.1 Fine and Coarse Discrimination", "text": "In order to formalise this notion of variance (or number of different values) of the expected average rewards of the set of evaluated agents, we can just compare pairs of values as follows:\nDefinition 21. The fine discriminating power for evaluated agents \u03c01 and \u03c02 playing in slot i in environment \u00b5 with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nFDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) , \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u2212i (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(Ri(\u00b5[l\u0307 i\u2190 \u03c01]), Ri(\u00b5[l\u0307 i\u2190 \u03c02])) (22)\nwhere \u2206Q is a divergence function for rational numbers, N(\u00b5) \u2265 1 and if N(\u00b5) > 1 then |\u03a0o| \u2265 1 otherwise |\u03a0o| \u2265 0.\nThis measures the expected average reward divergence of two evaluated agents placed both in slot i in the same line-up patterns. If \u2206Q is some kind of numeric difference (e.g., the absolute difference or the squared difference), then this measure would be similar to some kind of dispersion of expected average rewards (like a variance). If \u2206Q equals 1 \u2212 \u03b4 (with \u03b4 being the Kronecker delta function) we have that this measures the number of times two different evaluated agents score differently.\nWe can generalise this for a set of evaluated agents \u03a0e:\nDefinition 22. The fine discriminating power for a set of evaluated agents \u03a0e with associated weight w\u03a0e playing in slot i in environment \u00b5 with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nFDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) , \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) (23)\nwhere \u03b7\u03a02 normalises the formula with \u03b7\u03a02 = 1\u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02 w\u03a0e (\u03c01)w\u03a0e (\u03c02) , |\u03a0e| \u2265 2 and \u2203\u03c01, \u03c02 \u2208 \u03a0e|\u03c01 \u0338= \u03c02, w\u03a0e(\u03c01) > 0 and w\u03a0e(\u03c02) > 0.\nAgain, we can sum over all slots:\nDefinition 23. The fine discriminating power for a set of evaluated agents \u03a0e with associated weight w\u03a0e in environment \u00b5 with weight of slots wS , with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nFD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) , N(\u00b5)\u2211 i=1 wS(i, \u00b5)FDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) (24)\nwhere N(\u00b5) \u2265 1.\nBeing able to discriminate in terms of pair of agents for each line-up pattern in an environment can be generalised with the overall result of a measure \u03a5 by considering the aggregated values on this measure, namely:\nDefinition 24. The coarse discriminating power for evaluated agents \u03c01 and \u03c02 playing in slot i in environment \u00b5 with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nCDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) , \u2206Q(\u03a5i(\u03c01,\u03a0o, wL\u0307, \u00b5),\u03a5i(\u03c02,\u03a0o, wL\u0307, \u00b5)) (25)\nwhere \u03a5i(\u03c0,\u03a0, wL\u0307, \u00b5) is defined in equation 2 and \u2206Q is a divergence function for rational numbers.\nWe generalise this for a set of evaluated agents \u03a0e:\nDefinition 25. The coarse discriminating power for a set of evaluated agents \u03a0e with associated weight w\u03a0e playing in slot i in environment \u00b5 with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nCDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) , \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)CDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) (26)\nwhere \u03b7\u03a02 normalises the formula with \u03b7\u03a02 = 1\u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02 w\u03a0e (\u03c01)w\u03a0e (\u03c02) , |\u03a0e| \u2265 2 and \u2203\u03c01, \u03c02 \u2208 \u03a0e|\u03c01 \u0338= \u03c02, w\u03a0e(\u03c01) > 0 and w\u03a0e(\u03c02) > 0.\nAnd summing over all slots:\nDefinition 26. The coarse discriminating power for a set of evaluated agents \u03a0e with associated weight w\u03a0e in environment \u00b5 with weight of slots wS , with a class of opponents and team players \u03a0o and a weight of line-ups pattern wL\u0307 is given by:\nCD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) , N(\u00b5)\u2211 i=1 wS(i, \u00b5)CDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) (27)\nwhere N(\u00b5) \u2265 1.\nIn both fine and coarse discrimination the goal is to check if two evaluated agents obtain similar results. The difference resides at the level we analyse their results. While the fine checks the similarities for each line-up pattern, the coarse is more oriented to seeing the overall similarity."}, {"heading": "4.4.2 Strict Total and Partial Grading", "text": "An environment and set of agents populating it being discriminative when comparing evaluated agents does not mean that there is a gradation or order between the results for a set of evaluated agents. For instance, if we have three agents \u03c01, \u03c02 and \u03c03 that we want to evaluate in a competitive environment with two agent slots and two teams, and we get that \u03c01 scores better when interacts with \u03c02, \u03c02 scores better when interacts with \u03c03 and \u03c03 scores better when interacts with \u03c01, then there is no way to establish a gradation for these three agents. Idealistically, we would like to have a strict total order, but this is unrealistic for many environments.\nSo the idea we will pursue is to evaluate how close an environment and set of agents populating it are to this ideal situation from the expected average rewards of the evaluated agents (without an aggregated rating system6):\n6A common approach is to create a rating when we have many experiments, as done with sport ratings, such as the ELO rating [13] in chess.\nDefinition 27. The strict total grading quality for evaluated agents \u03c01, \u03c02 and \u03c03 playing in slots i and j (with i \u0338= j) in environment \u00b5 with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nSTGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) , \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u2212i,j (\u03a0o)\nwL\u0307(l\u0307)STOi,j(\u03c01, \u03c02, \u03c03, l\u0307, \u00b5) (28)\nwhere N(\u00b5) \u2265 2, if N(\u00b5) > 2 then |\u03a0o| \u2265 1 otherwise |\u03a0o| \u2265 0 and STOi,j(\u03c01, \u03c02, \u03c03, l\u0307, \u00b5) (where l\u0307 has all its elements instantiated except positions i and j) is 1 if there is a permutation of the three evaluated agents such that there is a strict total order in their expected average rewards when placed by pairs in l\u0307 interacting in environment \u00b5 in slots i and j, and 0 otherwise. Formally, it is 1 iff there is a permutation (\u03c0\u20321, \u03c0 \u2032 2, \u03c0 \u2032 3) such that: Ri(\u00b5[l\u0307 i,j\u2190 \u03c0\u20321, \u03c0\u20322]) < Rj(\u00b5[l\u0307 i,j\u2190 \u03c0\u20321, \u03c0\u20322]), Ri(\u00b5[l\u0307 i,j\u2190 \u03c0\u20322, \u03c0\u20323]) < Rj(\u00b5[l\u0307 i,j\u2190 \u03c0\u20322, \u03c0\u20323]) and Ri(\u00b5[l\u0307 i,j\u2190 \u03c0\u20321, \u03c0\u20323]) < Rj(\u00b5[l\u0307\ni,j\u2190 \u03c0\u20321, \u03c0\u20323]). For instance, if we have three agents a, b and c in an environment \u00b5 and line-up pattern l\u0307 for slots i and j, and their expected average rewards when placed by pairs in l\u0307 shows us that b < a, a < c and b < c, then we have STOi,j(a, b, c, l\u0307, \u00b5) = 1 with the permutation (b, a, c). This property is related to reliability, which we will see later on.\nWe generalise this for a set of evaluated agents \u03a0e:\nDefinition 28. The strict total grading quality for a set of evaluated agents \u03a0e with associated weight w\u03a0e playing in slots i and j (with i \u0338= j) in environment \u00b5 with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nSTGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) , \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5)\n(29) where \u03b7\u03a03 normalises the formula with \u03b7\u03a03 = 1\u2211 \u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03 w\u03a0e (\u03c01)w\u03a0e (\u03c02)w\u03a0e (\u03c03) , |\u03a0e| \u2265 3 and \u2203\u03c01, \u03c02, \u03c03 \u2208 \u03a0e|\u03c01 \u0338= \u03c02 \u0338= \u03c03, w\u03a0e(\u03c01) > 0, w\u03a0e(\u03c02) > 0 and w\u03a0e(\u03c03) > 0.\nNow, if we aggregate all combinations of pairs of slots, we have:\nDefinition 29. The strict total grading quality for a set of evaluated agents \u03a0e with associated weight w\u03a0e in environment \u00b5 with weight of slots wS , with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nSTG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) , \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  (30)\nwhere \u03b7S21 normalises the formula with \u03b7S21 = 1\u2211N(\u00b5) i=1 wS(i,\u00b5) (\u2211i\u22121 j=1 wS(j,\u00b5)+ \u2211N(\u00b5) j=i+1 wS(j,\u00b5) ) , N(\u00b5) \u2265 2 and \u2203i, j|1 \u2264 i \u2264 N(\u00b5), 1 \u2264 j \u2264 N(\u00b5), i \u0338= j, wS(i, \u00b5) > 0 and wS(j, \u00b5) > 0.\nThe previous definition only considers strict total orders, and is useful to determine whether we can find a strict total order for the evaluated agents. However, this does not say much about the existence of grading \u2018inconsistencies\u2019, such as non-discriminative cases such as \u03c01 = \u03c02, \u03c02 = \u03c03 and \u03c01 = \u03c03 which, for the above definition, are considered in the same way as not ordering cases such as \u03c01 > \u03c02, \u03c02 > \u03c03 and \u03c01 < \u03c03. In order to distinguish these cases, we can give a new definition as follows:\nDefinition 30. The partial grading quality for a set of evaluated agents \u03a0e with associated weight w\u03a0e in environment \u00b5 with weight of slots wS , with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is defined as in definition 29 with the use of a partial order with \u2264 instead of a strict total order with <. It is denoted by PG.\nIf STG and PG are high, this means that a derivation of a rating is highly consistent to what we see when using evaluated agents from \u03a0e on slots i and j. A very similar property is known as monotonicity in [21, sec. 5][22], showing an agent set for the matching pennies game that is non-monotonic. Nonetheless, a partial order can still be constructed for the agent set of all finite state machines for this game [30].\nThe existence of a meaningful rating allows for subselections of \u03a0o according to this rating, which can be used to furbish the tests with high-rank agents that can lead to more sophisticated social environments (which can be detrimental or beneficial, so making it more or less difficult, respectively, depending on whether it is used for the same team or for opponents)."}, {"heading": "4.5 Anticipation", "text": "One crucial property that is related to social intelligence is anticipation, which means that in both competition and cooperation, evaluated agents can benefit from anticipating other agents\u2019 moves or, in more general terms, by having a theory of other\u2019s minds. While a formalisation of this concept is very elusive, we can at least introduce an approximation."}, {"heading": "4.5.1 Competitive Anticipation", "text": "The first thing we need to do is to distinguish between competition and cooperation. In competitive anticipation we usually expect that evaluated agents can perform better if their opponents can be well anticipated. An example of this is a predator-prey situation. This phenomenon is difficult to define in general, but we can introduce a simplified approach based on the idea that one evaluated agent anticipates competitively if its expected average reward competing against a (generally) non-random agent is higher than its expected average reward competing against a random agent. This can be generalised as follows:\nDefinition 31. The anticipation benefit for evaluated agent \u03c01 against agent \u03c02, playing in slots i and j respectively (with i and j in different teams) when competing in environment \u00b5 with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nACompi,j(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) , \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u2212i,j (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( Ri(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c02])\u2212Ri(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c0r]) ) (31)\nwhere \u03c0r is a random agent, 1 2 normalises the result of ACompi,j to be between \u22121 and 1, N(\u00b5) \u2265 2 and if N(\u00b5) > 2 then |\u03a0o| \u2265 1 otherwise |\u03a0o| \u2265 0.\nWe generalise this for a set of evaluated agents \u03a0e:\nDefinition 32. The anticipation benefit for a set of evaluated agents \u03a0e with associated weight w\u03a0e playing in slot i when competing against slot j (with i and j in different teams) in environment \u00b5 with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nACompi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) , \u2211\n\u03c01\u2208\u03a0e\nw\u03a0e(\u03c01) max \u03c02\u2208\u03a0o ACompi,j(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) (32)\nwhere |\u03a0e| \u2265 1 and |\u03a0o| \u2265 1.\nBy aggregating all combinations of pairs of slots, we have:\nDefinition 33. The anticipation benefit for a set of evaluated agents \u03a0e with associated weight w\u03a0e when competing in environment \u00b5 with weight of slots wS , with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nAComp(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) , \u03b7S22 \u2211 t1,t2\u2208\u03c4 |t1 \u0338=t2 \u2211 i\u2208t1 wS(i, \u00b5) \u2211 j\u2208t2 wS(j, \u00b5)ACompi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) (33)\nwhere \u03b7S22 normalises the formula with \u03b7S22 = 1\u2211\nt1,t2\u2208\u03c4|t1 \u0338=t2 \u2211 i\u2208t1 wS(i,\u00b5) \u2211 j\u2208t2 wS(j,\u00b5) , \u03c4 is the partition of slots\non teams for environment \u00b5, N(\u00b5) \u2265 2 and \u2203t1, t2 \u2208 \u03c4 |t1 \u0338= t2,\u2203i \u2208 t1, j \u2208 t2|wS(i, \u00b5) > 0 and wS(j, \u00b5) > 0.\nIf AComp is positive this means that evaluated agents behave better against non-random agents (in general) than against random agents.One good example of the above definition is when slot i is a predator and j is a prey. If, in general, evaluated agents in a class perform better with non-random preys than with random preys then the environment shows a benefit for this evaluated agent class (for these slots). Of course, this depends on \u03a0o, but if we include non-random opponents with some movement patterns and/or some degree of intelligence, the definition becomes more meaningful."}, {"heading": "4.5.2 Cooperative Anticipation", "text": "On the other hand, in cooperative anticipation (or coordination) good rewards can only be obtained with both agents performing some actions together. Again a general definition accounting for all possible coordination situations is difficult, but we can introduce a simplified approach based on the following intuitive definition: two agents \u03c01 and \u03c02 coordinate cooperatively if the sum of the expected average rewards of \u03c01 and \u03c02 are higher when they interact together than the sum of each one interacting with a random agent.\nDefinition 34. The anticipation benefit for evaluated agent \u03c01 and agent \u03c02 playing in slots i and j respectively (with i \u0338= j but in the same team) when cooperating in environment \u00b5 with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nACoopi,j(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) , \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u2212i,j (\u03a0o)\nwL\u0307(l\u0307) 1\n4 \u00d7\n\u00d7 ( Ri(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c02]) +Rj(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c02])\u2212Ri(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c0r])\u2212Rj(\u00b5[l\u0307 i,j\u2190 \u03c0r, \u03c02]) ) (34) where \u03c0r is a random agent, 1 4 normalises the result of ACoopi,j to be between \u22121 and 1, N(\u00b5) \u2265 2 and if N(\u00b5) > 2 then |\u03a0o| \u2265 1 otherwise |\u03a0o| \u2265 0.\nThe use of random agents for the above definition may not work in some cases if there are more than two elements in a team, as coordination may only take place when all of them coordinate and not only a pair (if a random agent is included, this can be very disruptive). In this case, the above definition could be extended to reach the number of elements in the team instead.\nWe can generalise this for a set of evaluated agents \u03a0e:\nDefinition 35. The anticipation benefit for a set of evaluated agents \u03a0e with associated weight w\u03a0e playing in slot i when cooperating with slot j (with i \u0338= j but in the same team) in environment \u00b5 with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nACoopi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) , \u2211\n\u03c01\u2208\u03a0e\nw\u03a0e(\u03c01) max \u03c02\u2208\u03a0o ACoopi,j(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) (35)\nwhere |\u03a0e| \u2265 1 and |\u03a0o| \u2265 1.\nFinally, if we aggregate all combinations of pairs of slots, we have:\nDefinition 36. The anticipation benefit for a set of evaluated agents \u03a0e with associated weight w\u03a0e when cooperating in environment \u00b5 with weight of slots wS , with a class of opponents and team players \u03a0o and a weight of line-up patterns wL\u0307 is given by:\nACoop(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) , \u03b7S23 \u2211 t\u2208\u03c4 \u2211 i,j\u2208t|i \u0338=j wS(i, \u00b5)wS(j, \u00b5)ACoopi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+ \u2211\nt1,t2,t3\u2208\u03c4 |t1 \u0338=t2 \u0338=t3 \u2211 i\u2208t1 wS(i, \u00b5) \u2211 j\u2208t2 wS(j, \u00b5)ACoopi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) (36)\nwhere \u03b7S23 normalises the formula with \u03b7S23 = 1\u2211 t\u2208\u03c4 \u2211 i,j\u2208t|i\u0338=j wS(i,\u00b5)wS(j,\u00b5)+ \u2211\nt1,t2,t3\u2208\u03c4|t1 \u0338=t2 \u0338=t3 \u2211 i\u2208t1 wS(i,\u00b5) \u2211 j\u2208t2 wS(j,\u00b5) ,\n\u03c4 is the partition of slots on teams for environment \u00b5, N(\u00b5) \u2265 2 and \u2203i, j \u2208 t \u2208 \u03c4 |i \u0338= j, wS(i, \u00b5) > 0 and wS(j, \u00b5) > 0 or \u2203t1, t2, t3 \u2208 \u03c4 |t1 \u0338= t2 \u0338= t3,\u2203i \u2208 t1, j \u2208 t2|wS(i, \u00b5) > 0 and wS(j, \u00b5) > 0.\nThe previous definition includes slots in two different teams when the environment contains three or more teams. This is because two agents from different teams can cooperate against another agent in a third team.\nWe have defined the anticipation with only two slots, but competition and cooperation can also appear with three or more agents."}, {"heading": "4.6 Symmetry", "text": "In game theory, a symmetric game is a game where the payoffs for playing a particular strategy depend only on the other strategies employed by the rest of agents, not on who is playing them. This property is very useful for evaluating purposes. Since we can change the positions of the agents and they will maintain their results, we only need to evaluate the agent playing in one position of the environment.\nFor our definition of multi-agent environments this definition of symmetry must be reconsidered. The previous definition means that for each pair of line-ups with the same agents but in different order, the agents obtain exactly the same results. But with the inclusion of teams this definition is not appropriate. For example, using an environment with the partition of slots on teams \u03c4 = {{1, 2}, {3, 4}} and line-up l = (\u03c01, \u03c02, \u03c03, \u03c04), we have that agents \u03c01 and \u03c02 must both obtain the same result, as \u03c03 and \u03c04 as well. Following the definition and switching the positions of \u03c02 and \u03c03 we obtain line-up l\n\u2032 = (\u03c01, \u03c03, \u03c02, \u03c04), which now means that agents \u03c01 and \u03c03 must have the same rewards (since they are now in the same team) while maintaining their previous results, as \u03c02 and \u03c04 as well. This situation can only occur when all slots (and therefore teams) obtain equal results.\nInstead, we extend this definition of symmetry to include teams. First, we denote by \u03c3(l) the set of line-ups permuting the agent positions in line-up l. This set will correspond with the one used in game theory to define symmetry. To adapt this set to our definition, we must select a subset of line-ups from \u03c3(l) respecting the teams defined in \u03c4 . We denote this subset with \u03c3(l, \u03c4), where we only select line-ups from \u03c3(l) if original teams are maintained. Following the example, line-up l\u2032 is not included in \u03c3(l, \u03c4) since \u03c01 and \u03c03 from l\n\u2032 were not in the same team in l (as \u03c02 and \u03c04 as well). However, l\n\u2032\u2032 = (\u03c03, \u03c04, \u03c02, \u03c01) is included in \u03c3(l, \u03c4), since both pair of agents (\u03c01, \u03c02) and (\u03c03, \u03c04) are still in the same team. From here, we define symmetry for a multi-agent environment as follows:\nDefinition 37. We say a multi-agent environment \u00b5 is symmetric if and only if every team in \u03c4 has the same number of elements and:\n\u2200i,K,\u03a0, l \u2208 LN(\u00b5)(\u03a0), l\u2032 \u2208 \u03c3(l, \u03c4) : R\u0306Ki (\u00b5[l]) = R\u0306Ki\u2032 (\u00b5[l\u2032]) (37)\nwhere i\u2032 represents the slot of agent li:i in l \u2032.\nNote that we impose that each set in \u03c4 must have the same number of elements. This is because we only consider environments to be symmetric if we can evaluate an agent in every slot and obtain the same result. Having teams with different number of elements will not allow us to do this.\nThis definition now fits our goal of symmetry. But too few environments will fit this definition of symmetry because it is too restrictive. However, we could divide this definition of symmetry of teams into two parts depending on the relation between the slots:\nFor the first part we look at the relation between the slots within each team:\nDefinition 38. We say an environment is Intra-Team Symmetric when the agents within every team can be swapped without affecting their results.\nThis kind of symmetry will allow us to evaluate a team in an environment without taking into account their positions within every set of \u03c4 .\nFor the second part we look at the relation between the slots in different teams. This can be also divided into two parts:\nDefinition 39. We say an environment is Partial Inter-Team Symmetric if every pair of teams having the same number of elements can be swapped without affecting their results.\nDefinition 40. We say an environment is Total Inter-Team Symmetric if every pair of teams can be swapped without affecting their results.\nThis kind of symmetry will allow us to evaluate a team in an environment without taking into account in which set of \u03c4 they are situated.\nDefinition 37 will correspond with an Intra-Team and Total Inter-Team Symmetry, where every team of agents can be located in every set of \u03c4 and in different order, maintaining their performance expectation.\nSymmetry is not a necessary condition for social behaviours, but it is a very practical one for measurement as the result does not depend on the slot we use to evaluate and all slots are useful for evaluating the same ability. This simplifies all previous calculations (for social intelligence and properties) as we can obtain the same results by calculating them for only one slot or pair of slots."}, {"heading": "4.7 Validity", "text": "Validity is the most important property of a cognitive test in psychometrics. In our context, the validity of a definition is that it accounts for the notion we expect it to grasp. For instance, if we say that a given definition of \u03a5 measures social intelligence but it actually measures arithmetic abilities then the definition is not valid. In our case, this depends on the choice of \u03a0o (\u03a0) and M as the core of definition 8.\nPoor validity may have two sources (or may appear in two different variants): a definition may be too specific (it does not account for all the abilities the notion is thought to consider) or it is too general (it includes some abilities that are not part of the notion to be measured). In other words, the measure should account for all, but not more, of the concept it tries to represent. We refer to these two issues of validity as the generality of the measure and the specificity of the measure.\nRegarding generality, we should be careful about the use of very restrictive choices for \u03a0o and M . It could be possible to find a single environment that meets all the properties seen in the previous subsections. However, using just one environment is prone to specialisation, as usual in many AI benchmarks. For instance, if we use a particular maze as an environment with a set of particular agents, then we can have good scores by using a very specialised agent for this situation, which may be unable to succeed in other social contexts. For instance, chess with current chess players is an example where a specialised system (e.g., Deep Blue) is able to score well, while it is clearly useless for other problems. A similar over-specialisation may happen if the agent class is too small. This is usual in biology, where some species specialise for predating (or establishing a symbiosis) with other species.\nConsequently, the environment class and the agent class must be general enough to avoid that some predefined or hardwired policies could be optimal for these classes. This is the key issue of a (social) intelligence test; it must be as general as possible. We need to choose a diverse environment class. One possibility is to consider all environments (as done by [39, 27]), and another is to find an environment class that is sufficiently representative (as attempted in [24]).\nSimilarly, as happens with the environment diversity and most especially while evaluating social intelligence, we need to consider a class of agents that leads to a diversity in line-up patterns. This class of agents should incorporate many different types of agents: random agents, agents with some predetermined policies, agents that are able to learn, human agents, agents with low social intelligence, agents with high social intelligence, etc. The set of all possible agents (either artificial or biological) is known as machine kingdom in [19, 20] and raises many questions about the feasibility of any test considering this astronomically large set. Also, there are doubts about what the weight for this universal set should be. Instead, some representative kinds of agents could be chosen. In this way, we could aim at social intelligence relative to a smaller (and well-defined) set of agents, possibly specialising the definition by limiting the resources, the program size [25] or the intelligence of the agents [18].\nRegarding specificity, it is equally important for a measurement to only include those environments and agents that really reflect what we want to measure. For instance, it is desirable that the evaluation of an ability is done in an environment where no other abilities are required, or in other words, we want that the environment evaluates the ability in isolation. Otherwise, it will not be clear which part of the result comes from the ability to be evaluated, and which part comes from other abilities. Although it is very difficult to avoid any contamination, the idea is to ensure that the role of these other abilities are minor, or are taken for granted for all agents.\nThe properties of dependency (interactivity, non-neutralism) and anticipation (both competitive and cooperative) seen in previous subsections have been included for the sake of specificity. We are certainly not interested in non-social environments as this would contaminate the measure with other abilities. In fact, one of the recurrent issues in defining and measuring social intelligence is to be specific enough to distinguish it from\ngeneral intelligence. Unlike all other properties in this section, validity is not a property for which a formal account can be given, as it precisely accounts for how well the definition reflects the natural or intuitive notion that is to be measured. The assurement about validity must come then from the use of a formal definition (as for definition 8) with a meaningful instantiation for the agent and environment classes, and also from the experimental results that can be obtained through the tests derived from the definition. Note that in psychometrics there is usually a lack of a proper definition of the cognitive abilities of interest (e.g., psychometrics has not presented an unambiguous definition of intelligence), so validity is applied to a test and not to the definition of a cognitive ability. In fact, the concept is frequently derived from the test, as has happened with the modern view of intelligence, as \u201cthe ability measured by IQ tests\u201d.\nAs discussed in the introduction, we are interested in a formal definition whose validity we can discuss and appraise, and from which tests are derived, and not the other way round."}, {"heading": "4.8 Reliability", "text": "Another key issue in psychometric tests is the notion of reliability, which means that the measurement is close to the actual value. Note that this is different to validity, which refers about the true identification or definition of the actual value. In other words, if we assume validity, i.e., that our definition is correct7, reliability refers to the quality of the measurement with respect to the actual value. More technically, if the actual value of \u03c0 for an ability \u03c6 is v then we want a test to give a value which is close to v. The cause of the divergence may be systematic (bias), non-systematic (variance) or both.\nFirst, we need to consider that reliability applies to tests, as introduced in section 3.4. Reliability is then defined by considering that a test can be repeated many times, so becoming a random variable that we can compare to the true value. Formally:\nDefinition 41. Given a definition of a cognitive ability \u03a5 and a test \u03a5\u0302, the test error is given by:\nTE(\u03a5\u0302) , Mean((\u03a5\u0302\u2212\u03a5)2) (38)\nwhere the mean is calculated over the repeated application of the test (to one subject or more subjects).\nThe reliability Rel(\u03a5\u0302) can be defined as a decreasing function over TE(\u03a5\u0302), such as Rel(\u03a5\u0302) = e\u2212TE(\u03a5\u0302). The reason for defining test error as the mean squared error (and not an absolute error) is a customary choice in many measures of error, as we can decompose it into the squared bias: (Mean(\u03a5\u0302)\u2212\u03a5)2 and the variance of the error V ar(\u03a5\u0302\u2212\u03a5). These values can be calculated for just one evaluated agent or for all evaluated agents.\nIf the bias is not zero this means that the mechanism to sample the exercises and/or the number of iterations is inappropriate, and the choices for p\u03a0o (p\u03a0), pM , pS and pK in definition 9 must be revised. If there is a high variance, this suggests that the number of episodes nE is too small, and we need more (i.e., exercises in a tests) to get a less volatile result.\nIn this sense, note that some of the properties studied in previous subsections can hold for \u03a5 but may be significantly different for unreliable tests (i.e., approximations) \u03a5\u0302.\nThe estimation of TE(\u03a5\u0302) or Rel(\u03a5\u0302) depends on knowing the true value of \u03a5. This is not possible in practice for most environments, so \u03a5 will need to be estimated for large samples and compared with an actual test (working with a small sample). Because of the difficulties of estimating this, in what follows we will just give a qualitative assessment."}, {"heading": "4.9 Efficiency", "text": "This property refers to how efficient a test is in terms of the (computational) time required to get a reliable score. It is easy to see that efficiency and reliability are opposed. If we were able to perform an infinitely number of infinite episodes, then we would have \u03a5\u0302 = \u03a5, with perfect reliability, as we would exhaust \u03a0o and M . However, as we try to make tests not only finite but more efficient, we lose reliability because of the sampling procedure. If done properly, it is usually the variance component of the reliability decomposition that is affected if it is possible to keep the bias close to 0 even with very low values of the number of episodes nE in definition 9.\n7The lack of a proper definition for many abilities makes reliability refer to the quality of the result of a single application of the test in comparison to the idealised average result if the test could be repeated indefinitely.\nThe definition of efficiency then needs to be defined as a ratio between the reliability and the time taken by the test (depending mostly on nE and pK , but also on p\u03a0o (p\u03a0), pM and pS).\nDefinition 42. Given a definition of a cognitive ability \u03a5 and a test \u03a5\u0302, the efficiency is given by:\nEff(\u03a5\u0302) , Rel(\u03a5\u0302)/T ime(\u03a5\u0302) (39)\nwhere Time is the average time taken by test \u03a5\u0302. Time can be measured as physical (real) time or as computational time (steps).\nWhile this is the way it should be measured, the big issue is how to choose environments and agents such that a high efficiency is attained. Clearly, if the selected environments are insensitive to agents actions or require too many actions to affect rewards, then this will negatively affect efficiency. As we are interested in social abilities, interactivity and non-neutralism must be high, as otherwise most steps will be useless to get information about the evaluated agent. This of course includes cases where the evaluated agent is stuck or bored because their opponents (or teammates) are too good or too bad, or the environment leads the evaluated agent to heaven or hell situations where actions are almost irrelevant,\nNaturally, a way of making tests more efficient is by the use of adaptive tests, as in computerised adaptive testing. We will not explore this possibility in this paper as our definition of test in section 3.4 is not adaptive (for adaptive versions of universal tests, the reader is referred to [27, 19]).\nSimilarly to reliability, we will just give a qualitative assessment of efficiency."}, {"heading": "4.10 Summary of properties", "text": "In tables 1 and 2 we can see a summary of all previous properties. Table 1 shows the quantitative properties, while table 2 shows the qualitative8 properties. This completes our picture jointly with figure 2.\nWith these properties we managed to represent how appropriate an environment \u00b5 and the set of agents \u03a0o we use are in order to evaluate the social intelligence of a given set of evaluated agents \u03a0e. The set of properties we propose provides key information about the testbed we are analysing. First, we can measure the influence that a set of agents \u03a0o produces in a set of evaluated agents \u03a0e. Second, we can analyse to what extent the anticipation abilities are useful for a set of evaluated agents \u03a0e interacting with a set of agents \u03a0o. Third, we can determine whether cooperation or competition is given more importance in the testbed. Fourth, we estimate the discriminative power that the testbed has for the evaluation of different agents. Fifth, the grading power of the testbed indicates how effective it is to rank agents. And sixth, we have some instrumental properties that are convenient to convert the definition into a practical test.\nWe think that these properties can be useful to characterise a testbed. For the quantitative properties we find two kinds of properties. The properties whose values range from 0 to 1 determine the percentage of fulfilment that the environment \u00b5 and the set of agents \u03a0o have about this property when evaluating a set of agents \u03a0e. Therefore, the lower the value the worse the system is in regard to this property, and the higher the value the better. On the contrary, the kind of properties whose values range from \u22121 to 1 must not be interpreted in the same way. Instead, these properties measure to which kind of type the system is more focussed on, and not a level of accomplishment or quality."}, {"heading": "5 Degree of compliance of several multi-agent and social scenarios", "text": "Many games and environments have been proposed as testbeds to evaluate performance in a multi-agent environment [45, 64, 53, 66]. Typically these games and environments are created or selected to represent a specific problem or family to analyse or solve. Since we are interested in developing social intelligence tests, it is first mandatory to evaluate whether these other previous testbeds could be valid as they are (or with minor modifications). If not, they can still be a good source of inspiration to figure out new environment classes by reusing some of their ideas or hybridising some of their features.\nWe would have liked to explore many games and environments, but we can just practically do a selection of some of the most common and representative in the area of multi-agent systems, game theory and (social) computer games. We will focus on some testbeds whose specification is complete, so we can analyse the level of\n8Some of them can in principle be quantified, but we only give a qualitative assessment in this paper.\ncompliance of these testbeds for the properties seen in the previous section. In particular, the testbeds that we are going to analyse in this section are: matching pennies, prisoner\u2019s dilemma, predator-prey (a pursuit game), Pac-Man and RoboCup Soccer."}, {"heading": "5.1 Graphical analysis for the properties", "text": "Before starting with the games and environments testbeds, we are going to introduce some indicators and a graphical representation that will be illustrated on a figurative environment. In table 3 we show a summary with the most important elements we are using in this section.\nIn order to assess compliance with interactivity, non-neutralism, anticipation and other properties for an environment \u00b5 we need to specify the evaluated agent class \u03a0e with associated weight w\u03a0e , the agent class \u03a0o which populates the environment, line-up pattern weights wL\u0307 and slot weights wS . One choice for \u03a0e, w\u03a0e and \u03a0o would be to consider any possible agent that is expressible using a given policy language. This, however, would make the calculation of most properties difficult (if not impossible). A better approach would be to use a (representative) sample of all agents or a sample of a meaningful class. Instead of that, and in order to give a more general picture of the environment itself, we will show the range of values that each property can have (independently of the agents), and how much this range can be restricted (for better or worse) depending on which \u03a0e, w\u03a0e and \u03a0o we select. In fact, when evaluating a set of agents \u03a0e in a certain setup, we should provide which set of agents \u03a0o will populate the environment, but we could also let this set unfixed in order to measure the environment itself. Finally, the use of different weights can lead to different ranges for the properties, but in what follows we will assume uniform weights for line-up patterns wL\u0307 and slots wS .\nWe divided the properties into three types. In the first type we have the properties which have a quantitative value that can range between 0 and 1. In the second type we have the properties which have a quantitative value that can range between \u22121 and 1. And in the third type are the properties for which we provide a qualitative value.\nFor the first two types of properties we calculate the range that each property can have in an environment. For this, we need to calculate the lowest and highest values that this range can have for each quantitative property Prop. To achieve this, we will select \u03a0e, w\u03a0e and \u03a0o (from the set of all possible \u03a0e, w\u03a0e and \u03a0o such that Prop is defined) that obtain the lowest and highest values respectively. We define General as follows:\nDefinition 43. We denoteGeneral to be the range of values fromGeneralmin(Prop, \u00b5) toGeneralmax(Prop, \u00b5), where:\nGeneralmin(Prop, \u00b5) , min \u03a0e,w\u03a0e ,\u03a0o Prop(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) (40)\nGeneralmax(Prop, \u00b5) , max \u03a0e,w\u03a0e ,\u03a0o Prop(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) (41)\nwhere the weight for slots wS and weight for line-up patterns wL\u0307 are uniform weights and the triplet \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 is selected (from the set of all possible \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 such that Prop is defined) to minimise/maximise the values of a quantitative property Prop for environment \u00b5.\nFor the first type of properties Prop we can select some set of agents \u03a0o to obtain a situation where General is restricted in such a way that Generalmax(Prop, \u00b5) decreases. In particular, we are interested in the setup with the \u201clowest maximum\u201d, i.e., we consider those \u03a0o that minimise this maximum. We define Left as follows:\nDefinition 44. We denote Left to be the most restricted range of values from Leftmin(Prop, \u00b5) to Leftmax(Prop, \u00b5) that we can obtain when \u03a0o is selected (from the set of all possible \u03a0o such that Prop is defined) to decrease the values of a quantitative property Prop which range is between 0 and 1 for environment \u00b5, where:\nLeftmin(Prop, \u00b5) , Generalmin(Prop, \u00b5) (42)\nLeftmax(Prop, \u00b5) , min \u03a0o max \u03a0e,w\u03a0e Prop(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) (43)\nIn the same way, for the first type of properties Prop we can select some set of agents \u03a0o to obtain a situation where General is restricted in such a way that Generalmin(Prop, \u00b5) increases. In particular, we are interested in the setup with the \u201chighest minimum\u201d, i.e., we consider those \u03a0o that maximise this minimum. We define Right as follows:\nDefinition 45. We denoteRight to be the most restricted range of values fromRightmin(Prop, \u00b5) toRightmax(Prop, \u00b5) that we can obtain when \u03a0o is selected (from the set of all possible \u03a0o such that Prop is defined) to increase the values of a quantitative property Prop which range is between 0 and 1 for environment \u00b5, where:\nRightmin(Prop, \u00b5) , max \u03a0o min \u03a0e,w\u03a0e Prop(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) (44)\nRightmax(Prop, \u00b5) , Generalmax(Prop, \u00b5) (45)\nFor the first type of properties, the General, Left and Right ranges become better as long as their minimum and maximum values become higher. If the Left range values are lower, this would mean that a bad selection of \u03a0o is disastrous for the quality of the testbed. If Right range values are higher would mean that there is a good selection of \u03a0o which improves the quality of the testbed. The comparison between Left and Right with General shows us the importance that a good selection for the set of opponents and team players \u03a0o has for a property Prop in an environment \u00b5. As these three ranges are more different, the selection of agents \u03a0o becomes more important in order to provide a better quality for the testbed.\nIn figure 3 we present the properties of a figurative environment divided in three sections. The top section represents five quantitative properties whose range can be between 0 and 19. The middle section represents the three quantitative properties whose range can be between \u22121 and 1. Finally, the bottom section represents the five qualitative properties.\nWe can see that each property of the first type has the early mentioned General, Left and Right ranges represented with three bands. The first property (Action Dependency) has a General range from 0 to 1,\n9Since FD and CD are similar properties, we decided to just calculate the FD property in order to simplify our analysis.\nrepresented with the first band. This is the broadest range that this kind of property can have. This means that this environment can have any value for this property depending on the set of agents \u03a0o, the set of evaluated agents \u03a0e and its weight distribution w\u03a0e . The second band represents its Left range, which is equal to [0, 0]. In this case, there exists a set of agents \u03a0o that restricts this range to the minimum possible range. The third band represents its Right range, which remains from 0 to 1. Now, no set of agents \u03a0o can be selected to restrict this range. In the next four properties we see some other examples for the three ranges that this type of property can have. As we can see in the last property of this type (PG), we use a lighter color to represent that (part of) a range is not formally calculated, but instead we provide an estimation.\nNext we arrive at the second type of properties. Here the values for the General range can be between \u22121 and 1. Unlike the previous case, we do not provide Left and Right ranges for this type of properties. This is because these properties represent for which kind of social intelligence the environment is more oriented, so there are not really good or bad ranges for this property.\nIn the first property of this second type (Slot Reward Dependency), we see that the General range for this property is equal to [\u22121,\u22121], indicating that this environment is purely competitive. The next property shows another example for this type of properties. Meanwhile, the last property has a label with the text \u201cNot Defined\u201d. This is because for this environment the property is not defined, so we cannot represent it.\nFinally, we arrived at the last type of properties. Here, we denote whether the environment meets the properties by using a tick (X) or cross (\u00d7) mark respectively.\nNow that we have explained how we will represent the properties, let us start analysing some true environments."}, {"heading": "5.2 Matching pennies", "text": "Matching pennies [61] can be considered the simplest game in game theory featuring competition. This game consists of two players (or agents) each flipping a coin. If both coins match player 1 wins, otherwise player 2\nwins. This game is played as a repeated game, which means that the game is played on a single iteration and the game is repeated for several iterations. Each player can see the actions performed by the other player. The game is usually repeated during K steps (i.e., it is the iterated matching pennies), so players can use past steps in order to predict the other player\u2019s strategy. Following definition 4, for agent slot i this environment only allows two actions Ai = {Head, Tail} and only provides two rewards Ri = {\u22121, 1}, which correspond to lose and win respectively. Clearly, in this game, \u03c4 = {{1}, {2}} represents the partition of slots in teams, i.e., has two teams and only one slot in each. For agent in slot i the environment provides an observation set Oi = Aj \u222a {null} (where agent slot j represents the slot of the other agent) and the observation function \u03c9 returns to each agent the action performed by the other agent in the previous iteration or null if it is the first iteration. Figure 4 shows the reward function \u03c1 as a reward matrix, which has the actions of both agents as input and their rewards as outputs.\nNext we discuss the level of compliance of the matching pennies environment with respect to the properties seen in section 4. We can see a summary of the social properties for the matching pennies in figure 5. In appendix A we prove how we obtained these values.\nIf we start with the properties, we see that this game is bounded, as rewards are always between \u22121 and 1. This means that if the weight functions wS and wL\u0307 are bounded, the value of \u03a5 (and many other properties)\nwill be bounded. Also, matching pennies is a well known zero-sum game, which means that the payoffs of the agents always sum zero (as we can see in figure 4) and, therefore, agents have totally opposed interests.\nWe next move to the symmetry property. This game has only two teams with one agent on each team. So, in order to prove that this environment is not symmetric, we only need to find a pair of line-ups l1 = (\u03c01, \u03c02) and l2 = (\u03c02, \u03c01) where the sequence of rewards for \u03c01 and/or \u03c02 differs in both line-ups. This becomes trivial by using the same agent \u03c0t (which always performs Tail) as both \u03c01 and \u03c02 (i.e., l1 = l2 = (\u03c0t, \u03c0t)) and check whether the agent obtains the same result in both slots. Since \u03c0t gets a result of 1 in slot 1 and a result of \u22121 in slot 2, we can conclude that this environment is not symmetric. This forces us to calculate some other properties for all the slots.\nIf we look to the General range for the action dependency (AD) property, we see that it goes from 0 to 1 (propositions 2 and 3). That means that the evaluated agents can either interact without noticing the other agent or can perform actions depending on which agent they encounter. But some particular selection of \u03a0o could make this environment to have a too restrictive Left range with respect to this property, making it equal to [0, 0] (proposition 4), so no evaluated agent could perform different actions depending on which agent it interacts with. In addition, we see that no particular selection of \u03a0o can restrict the Right range, which remains from 0 to 1 (proposition 5).\nWhen we look at the General range for the reward dependency (RD) property, we see that it goes from 0 to 1 (propositions 6 and 7). This means that the evaluated agents can either obtain the same expected average reward or can obtain different expected average rewards depending on which agent they encounter. But some particular selection of \u03a0o could make this environment have a too restrictive Left range with respect to this property, making it equal to [0, 0] (proposition 8). In addition, we see that no particular selection of \u03a0o can restrict the Right range, which remains from 0 to 1 (proposition 9).\nThe General range for the fine discrimination (FD) property goes from 0 to 1 (propositions 10 and 11). This means that two different evaluated agents can either obtain the same expected average reward or can obtain different expected average rewards. The Left range can be restricted to be equal to [0, 0] (proposition 12), and the Right range goes from 0 to 1 (proposition 13). This means that, with a bad selection of \u03a0o, no pair of evaluated agents can be differentiated in terms of performance, and it does not exist a \u03a0o to always differentiate any pair of evaluated agents.\nThe General range for the strict total grading (STG) and partial grading (PG) properties has, as in all previous properties, a minimum value of 0 (propositions 14 and 18) and a maximum value of 1 (propositions 15 and 19). This means that we cannot provide an ordering for some sets of evaluated agents, but we can provide it for some other sets of evaluated agents. In both STG and PG, Left cannot be restricted by any \u03a0o, remaining from 0 to 1 (propositions 16 and 20), and the same occurs with Right, which cannot be restricted by any \u03a0o, remaining from 0 to 1 (propositions 17 and 21).\nThis environment always has a General range for the slot reward dependency (SRD) property equal to [\u22121,\u22121] (proposition 22). This means that both agent slots have opposed interests, i.e., it is entirely competitive.\nThe General range for the competitive anticipation (AComp) property goes from \u2212 12 to 1 2 (propositions 23 and 24). These values tell us that an evaluated agent can improve its results by correctly anticipating the actions of the other agent, but an incorrect anticipation will worsen its results.\nOn the contrary, we cannot evaluate the cooperative anticipation (ACoop) property in this environment. This is because each of the two teams has only one slot (in addition, this is also a zero-sum game), so the formula cannot be applied.\nWe now discuss the validity property. Matching pennies is a very simple game. As a result, it seems clear that it is not general enough to be used (alone) as the basis of a social intelligent test. Nonetheless, there are different opinions about this, as it has been suggested that matching pennies could be an intelligence test on its own, under the name \u2018Adversarial Sequence Prediction\u2019 [30, 31]. In fact, a tournament was organised in 2011 where computer algorithms competed10 and very interesting emergence phenomena were observed. Only strategies that were able to see patterns in the other players scored well (better than random). There are of course some counter-intuitive things about this game. Actually, random agents score exactly the same with any opponent, even with very intelligent ones. This raises concerns about the validity and reliability of this environment for a test since, in an intelligence test, the average score of a random policy should not obtain a good result, since random agents are clearly not intelligent. Also, matching pennies results are non-monotonic for a set of agents. In [21] there is an example of an agent set for matching pennies that is non-monotonic (so PG < 1). Nonetheless, partial orders can still be constructed for the agent set of all finite state machines\n10See http://matchingpennies.com/tournament/.\n[31]. Another important problem about matching pennies is that it only evaluates pure competition (it is a zero-sum game), and no form of cooperation can be found (although some versions, or the ternary extension, rock-paper-scissors, could allow for cooperation). Finally, another strong argument against the validity of this game as a good environment (alone) for a social intelligence test is that some current systems may score better than humans, even though these systems are not (socially) intelligent at all and they are designed to play matching pennies only.\nFinally, efficiency is a property where matching pennies excels, since every action has immediate consequences. This means that in most cases, it can be enough to design a test where only a few dozen steps are performed in order to have a good approximation. Of course, there might be agents that may change their behaviour in step 10,000 so their assessment up to this point will be completely different to their assessment in the limit (if no discounting factor is used). For some agents, with our definition of reliability, we get that some pairs of agents are stable from step 2, so high degrees of reliability (low test error) can be achieved very soon. When a random agent is involved, the approximation is slower, but not much slower, depending on a binomial distribution.\nOverall, matching pennies is an interesting game, but it lacks the generality that a social intelligence test should have. Nonetheless, it is a very simple game that illustrates how the range values of several properties can be calculated and provide very useful information about how an environment or game behaves. In the end, it has been a useful exercise before analysing more sophisticated scenarios below."}, {"heading": "5.3 Prisoner\u2019s dilemma", "text": "In the prisoner\u2019s dilemma [46] two prisoners (or agents) are suspects of a crime, and are asked if the other prisoner is guilty of that crime. If both cooperate and do not blame the other, both spend a short time in prison. If one cooperates but not the other, the one who blamed reduces its time in prison to the minimum sentence, but the other prisoner obtains the maximum sentence. Finally, if both prisoners blame the other, both spend a long time in prison.\nAs happens with the matching pennies, this game is played as a repeated game, which means that the game is played on a single iteration and the game is repeated for several iterations. Each player can see the actions performed by the other player. The game is usually repeated during K steps (i.e., it is the iterated prisoner\u2019s dilemma), so players can use past steps in order to predict the other player\u2019s strategy. Following definition 4, for agent slot i this environment only allows two actions Ai = {Cooperate, Blame} and provides four rewards Ri = {1, 2, 3, 4}, which correspond to the time spent in prison. In this game, \u03c4 = {{1}, {2}} represents the partition of slots in teams, which has two teams and only one slot in each. For agent in slot i the environment provides an observation set Oi = Aj \u222a{null} (where agent slot j represents the slot of the other agent) and the observation function \u03c9 returns to each agent the action performed by the other agent in the previous iteration or null if it is the first iteration. Figure 6 shows the reward function \u03c1 as a reward matrix, which has the actions of both agents as input and their rewards as outputs.\nThe payoff matrix in figure 6 is not normalised. We can normalise this matrix to be between \u22121 and 1, giving the lowest reward to the highest period in prison and vice versa. Once rewards are normalised, for agent slot i they are set to Ri = {\u22121,\u22120.33, 0.33, 1}. In figure 7 we can see this payoff matrix normalised.\nPrisoner\u2019s dilemma differs from matching pennies by including some cooperation, since both agents can choose to cooperate to spend a relatively little time in prison.\nWe can see a summary of the social properties for the prisoner\u2019s dilemma in figure 8. In appendix B we prove how we obtained these values.\nWith the normalised payoff matrix we can see that this game is bounded, since 1K\u00b7n\n\u2211K\nk=1\n\u2211n\ni=1 ri,k = c where \u22120.33 \u2264 c \u2264 0.33. In this game some cooperation can appear, as for example, when both agents always decide to cooperate, so they obtain the maximum joint reward (0.33). But blaming can provide the best reward to one player if the other player still cooperates, so cooperating now provides the worst reward. Finally, if both decide to blame, both obtain the worst joint reward (\u22120.33). As in matching pennies, if the weight functions wS and wL\u0307 are bounded, the value of \u03a5 (and many properties) will be bounded.\nAnalysing the symmetry property, we can see that the payoff matrix is clearly symmetric for both players. This makes that the payoffs of any strategies made by the agents do not depend on which slots they are, since they will obtain the same rewards. From this observation we can conclude that this environment is symmetric, which allows us to calculate some other properties only for one slot and assume that it is maintained for the other.\nIf we look at the ranges for the action dependency (AD) property, we encounter exactly the same scenario than in the matching pennies. That means that evaluated agents can either interact without noticing the other agent, obtaining a value of 0 (proposition 25), or can always perform actions depending on the agent they encounter, obtaining a value of 1 (proposition 26). But some particular selections of \u03a0o can provide a too restricted Left range, forcing this value to be equal to [0, 0] (proposition 27), so no evaluated agent from \u03a0e can behave differently depending on the agents of \u03a0o. In addition, no particular selection of \u03a0o can restrict the Right range, remaining from 0 to 1 (proposition 28).\nWe start to find some differences with the matching pennies when we analyse the reward dependency (RD) property. As in matching pennies, the General range for this property goes from 0 to 1 (propositions 29 and\n30), so the expected average rewards of the evaluated agents can either depend or not on which agent they encounter. Also, some particular selection of \u03a0o could make this environment to have a too restrictive Left range with respect to this property, making it equal to [0, 0] (proposition 31), so no evaluated agent obtains different expected average rewards depending on which agent it interacts with. But, a good selection of \u03a0o can restrict the Right range making it equal to [1, 1] (proposition 32). This means that the evaluated agents will obtain different expected average rewards depending on which agent they interact with.\nWe now move to the fine discrimination (FD) property. The General range for this property goes from 0 to 1 (propositions 33 and 34). This means that two evaluated agents can either obtain the same expected average reward or different expected average rewards depending on the set of agents \u03a0o we select. It exists a particular \u03a0o which restricts the Left range to be equal to [0, 0] (conjecture 1), meaning that the environment will not be discriminating the evaluated agents, since every evaluated agent will obtain the same expected average reward. It is not possible to restrict the Right range in such a way that we can always discriminate every pair of evaluated agents, so it remains from 0 to 1 (proposition 35), so it is possible to find two different evaluated agents from some particular \u03a0e obtaining the same result.\nThe General range for the strict total grading (STG) and partial grading (PG) properties can, as in all previous properties, reach a minimum value of 0 (propositions 36 and 40) and a maximum value of 1 (propositions 37 and 41) for this environment. This means that we cannot provide an ordering for some sets of evaluated agents, but we can provide it for some other sets of evaluated agents. In both STG and PG, the Left range cannot be restricted by any \u03a0o, remaining from 0 to 1 (propositions 38 and 42), and the same occurs with the Right range, which cannot be restricted by any \u03a0o, remaining from 0 to 1 (propositions 39 and 43).\nThe General range for the slot reward dependency (SRD) property goes from \u22121 to 1 (propositions 44 and 45). This provides very different distributions of expected average rewards depending on the strategies used by the agents.\nThe General range for the competitive anticipation (AComp) property goes from \u2212 23 to 2 3 (propositions 46 and 47). Anticipating the strategy of the other agent can be really useful to obtain a good expected average reward, but it can also provide a really bad expected average reward if the strategy is not correctly anticipated.\nWe cannot evaluate the cooperative anticipation (ACoop) property in this environment. This is because, as in the matching pennies environment, each of the two teams has only one slot, so the formula cannot be applied. This seems counter-intuitive since one of the actions is named to cooperate, but this cooperation is not meant to improve the agent\u2019s own rewards, but to improve the other agent\u2019s rewards. Indeed, if we reframe the game by using only one team and calculating the team reward as the mean of the agents\u2019 rewards, then both agents can cooperate to obtain the best joint reward. This will lead us to a situation where a bad cooperative anticipation between the agents\u2019 actions can negatively affect the team reward, but also, a good cooperative anticipation will have good benefits for the team.\nWe now discuss the validity property. The prisoner\u2019s dilemma is similar in simplicity to the matching pennies game. But in this game, competition is not so strong, providing some cooperation between the two teams and making this game more general than the matching pennies. But in this game we cannot evaluate cooperation within a team, so it is not general enough to evaluate social intelligence. Actually, some simple strategies can clearly make the adversary\u2019s results get stuck, forcing it to obtain bad rewards independently of its strategy. This raises concerns about the validity of this environment for a test, since an intelligence test should not give bad results to intelligent agents.\nFinally, the efficiency property is almost as good as in the matching pennies environment. Every action has immediate consequences on the agents\u2019 results, having good approximations for their results in few steps. Therefore, reliable values for the ability measured will be reached in short time.\nThe prisoner\u2019s dilemma resembles the matching pennies in many aspects, but it is slightly a more complex environment. As many similarities exists between both environments many properties remain equal.\nLet us see a more complex environment."}, {"heading": "5.4 Predator-prey (Pursuit game)", "text": "One typical environment for cooperation that uses a 2D discrete space is a pursuit game called Predator-prey [3], where the evaluee acts as a predator and has to cooperate/coordinate with other two predators in order to chase a prey. If they succeed chasing the prey, the goal is achieved. Figure 9 shows an example of a predator-prey environment.\nMany variants have been proposed about this scenario, which provides a high diversity of environments. Some\nexamples include spaces with and without obstacles or boundaries, and many variants about the parameters have been considered: the distance of the scenario that the agents can perceive, the number of predators or preys, the speed of the agents, etc. Even the definition of how the prey is chased has been modified, e.g., the prey is surrounded by the predators, or one predator chases the prey by occupying the same position. Some of these variants add a variety of social complexity to the game, such as different levels of cooperation/competition by having to interact with different numbers of predators or preys, or having faster preys.\nThese and other pursuit games have been widely studied and used in multi-agent systems (i.e., [55, 6, 5, 14, 1]), but, in our opinion, no thorough study about their properties has been developed so far.\nSince we cannot analyse all the variants we just select one of them to analyse it, but different variants could have different properties\u2019 values. For this analysis we will use the environment shown in figure 9, which also shows its initial observation. The game is typically performed in episodes. We will make an episode to end after six iterations are performed.\nFollowing definition 4, for agent slot i this environment allows four actions Ai = {Up, Right, Down, Left}, which leads the agent to the cell facing this direction (when an agent performs an action leading to a block or boundary, the agent does not move). For agent slot i the environment provides three rewards Ri = {0,\u22126, 6}, which correspond to \u2018the episode is not finished\u2019, \u2018lose the chase\u2019 and \u2018win the chase\u2019 respectively. \u03c4 = {{1}, {2, 3, 4}} represents the partition of slots in teams. The first team {1} contains the prey (which is located in the upper left corner) and the second team {2, 3, 4} contains three predators (which are located in the upper right, bottom left and bottom right corners respectively). For agent in slot i the environment provides an observation set Oi which corresponds to the set of spaces with any possible location of the agents, and the observation function \u03c9 returns for every agent a description of the space as, for example, figure 9. For the analysis of this environment we will follow the same procedure as for previous environments. This is the reason why we allow the evaluated agent to play in every slot, even as the prey. But, as mentioned above, if we only let the agent play as a predator, the values of the properties will be different. Figure 10 shows the reward function \u03c1 as a payoff matrix which has the current iteration and the chasing situation as input and the agents\u2019 rewards as output. Note that after the six iterations, the average reward will be \u22121 or 1 depending on whether the agent wins the chase or not.\nWe can see a summary of the social properties for the predator-prey in figure 11. In appendix C we prove how we obtained these values.\nWhen we start with the properties we see that this game is bounded since its rewards are between \u22126 and 6, so average rewards are between \u22121 and 1. As with previous environments, if weight functions wS and wL\u0307 are bounded, the value of \u03a5 and many properties will be bounded.\nAnalysing the symmetry property, we clearly see that this game is not symmetric. First, prey and predator\nteams do not have the same number of agents. And second, changing the slots of the agents within the predator team does not provide the same sequences of rewards for the agents, owing to they start in different positions. This forces us to calculate all the other properties for all the slots.\nIf we look at the action dependency (AD) property, we cannot see any difference with previous environments, even having a so different environment. For the General range, evaluated agents can either interact without noticing the other agents, providing a minimum value of 0 (proposition 48), or can perform different actions depending on the agents they encounter, providing a maximum value of 1 (proposition 49). But some particular bad selections of \u03a0o could restrict Left range to be equal to [0, 0] (proposition 50). On the contrary, no particular selection of \u03a0o can restrict the Right range for this property, remaining from 0 to 1 (proposition 51).\nWhen we look at the ranges for the reward dependency (RD) property, we can see that the General range goes from 0 to 1 (proposition 52 and conjecture 2) so the rewards of the evaluated agents can either differ or not depending on the agents they encounter. A really bad selection of \u03a0o can make the Left range too restrictive, staying on [0, 0] (proposition 53) no matter which \u03a0e we are evaluating. But we can restrict the Right range by selecting a properly \u03a0o, obtaining a range from 13 28 to 1 (approximation 1). This would force that almost half of the results of the evaluated agents will be different depending on the line-up pattern they interact with. We now move to the ranges for the fine discrimination (FD) property. The General range goes from 0 to 1 (propositions 54 and 55), i.e., the result of one evaluated agent can be different or not from the result of another evaluated agent depending on which set of agents \u03a0o is selected. But a bad selection of \u03a0o can restrict too much the Left range, making this value equal to [0, 0] (proposition 56), so every evaluated agent will obtain the same result. However, the Right range cannot be restricted, remaining from 0 to 1 (proposition 57), since there are always two different evaluated agents that can obtain the same expected average reward, independently of the set of agents \u03a0o.\nStrict total grading (STG) and partial grading (PG) properties are clearly different from previous environments. The General range for the strict total grading property can only go from 0 to 12 (propositions 58 and\n59), so we cannot even have a strict total ordering for all the evaluated agents. In addition, its Left range can be restricted to be from 0 to 14 (approximation 2), so it would be possible to obtain some strict total ordering. At least, its Right range can be restricted to be from 14 to 1 2 (approximation 3), which somehow alleviates this situation. But instead, its partial grading has really good ranges. Its General range goes from 12 to 1 (propositions 60 and 61), its Left range can be restricted to be from 12 to 3 4 (approximation 4) and its Right range can be restricted to be from 34 to 1 (approximation 5). This makes this environment good for grading the evaluated agents, so even with a bad selection of \u03a0o we will still be able to obtain some partial orders between some of the evaluated agents, and provides a promising partial grading for the evaluated agents if \u03a0o is well selected.\nThe slot reward dependency (SRD) has a General range equal to [0, 0] (proposition 62). This particular value comes from the opposite results of preys and predators. As early mentioned, if we had only used the slots of the predators for evaluating the agents, we would have had another range.\nThe General range for the competitive anticipation (AComp) property goes from \u22121 to 12 (propositions 63 and 64). Anticipating the strategy of the other team can be useful to improve the expected average reward, but only when playing in the predator team, since playing as the prey does not have this benefit. But conversely, a bad anticipation does really penalise the expected average reward of an evaluated agent either playing in the prey or predator team.\nIn this environment we can find cooperation between the agents within the predator team. The General range for the cooperative anticipation (ACoop) goes from \u22121 to 1 (propositions 65 and 66). This makes a good coordination within the predator team to always chase the prey, but a bad coordination can let the prey to escape.\nWe now discuss the validity property. In the predator-prey environment we can encounter both competition between the prey and predators, and cooperation among the predators, which makes it a complex game to evaluate social intelligence. Both competition and cooperation seem important in this game, giving more importance to competition rather than cooperation as we can see from AComp and ACoop, where a bad result can come from incorrectly anticipating while an incorrect anticipation in cooperation does not necessarily provide bad rewards. But this game gives us a general situation to evaluate social intelligence in a broad way. Also, agents that are evaluated in this environment can have good orderings as properties STG and PG reflect, making it a good environment to classify the agents. However, the abilities that the agents need to accomplish their goals are not balanced. It is easier for the predator team to win the chase if they cooperate adequately, where the prey will not have a chance to survive. Also, the Left ranges of the properties are usually very restrictive when \u03a0o is not selected carefully. So it is really necessary to select a correct set of agents, but still, once a certain level of intelligence is reached we cannot evaluate higher levels of intelligence, since their result will remain equal. From here, we can say that the social intelligence that this environment is evaluating is clearly limited to a certain level, and once this level is reached the results will not vary as happens in the Left range for the RD property. Summarizing, this environment allows us to evaluate both competition and cooperation which makes it a good environment to evaluate social intelligence, but this can only be evaluated until a certain level of intelligence. Over this level, the results will not reflect the true intelligence, so the environment will not be evaluating its actual value. As a result, the game is not valid for interesting levels of social intelligence.\nFinally, this environment is not reliable for just one exercise. It has a high variability as only 6 iteration are allowed and the space is small, so small changes in the first movements may have very different consequences. With respect to its efficiency, we just need a few interactions to provide us a fast result for the evaluated agent, which makes this environment efficient. This means that reliability can be obtained in reasonable time by the repetition of many episodes.\nThe predator-prey gives us a more complex environment than previous environments. However, it can still provide us a good environment to evaluate social intelligence if \u03a0o is wisely chosen. But we must be careful, since we could obtain a really poor environment if they are not properly selected. The results we have obtained came from our choice to include the possibility to evaluate the agent also playing as the prey. A more classical approach could have given us a different picture for this environment."}, {"heading": "5.5 Pac-Man", "text": "Computer games are also used as mainstream environments to evaluate AI systems. One example of the use of games for evaluating AI is the ALE (Arcade Learning Environment) [2], a framework where a set of arcade computer games are used to evaluate the performance of current AI algorithms. Here, we will analyse Pac-Man,\na simple and well known game, but still complex enough to the state of the art in AI, which uses a 2D maze. The AI community has used this environment as a testbed in order to evaluate their algorithms (e.g., [57, 15]). This game resembles a pursuit game, but this time the player represents the prey role (most of the time), so it must avoid being caught by the enemies (represented by ghosts). In order to win, Pac-Man must also collect all the pills that are present in the environment, which also provide some points. On the other hand, ghosts are appearing in the environment one by one over time, and they win if at least one of them is able to chase Pac-Man. If Pac-Man is able to reach certain locations in the environment and eat specific pills, it becomes invulnerable for a short period of time, and receives additional points by chasing the ghosts. Figure 12 shows a Pac-Man game screenshot.\nFrom the huge diversity of possible situations that can occur in this environment, it is difficult to formally analyse some of the properties as we did with previous environments. As long as the systems are more complex, it becomes more difficult to determine their actual levels of cooperation and competition, and more effort is needed to formalise them and find some \u03a0e, w\u03a0e and \u03a0o to find the environment ranges. Instead, we will analyse this environment in an informal way. As in previous environments we will assume uniform weights for wL\u0307 and wS .\nIn figure 13 we show a summary with an estimation of the properties for Pac-Man. When we start with the properties we see that this game is not bounded, since Pac-Man can obtain more and more points (or rewards) as long as it continues surpassing levels. This makes that \u03a5 will not be bounded for this environment. Reframing the game by calculating an average of points by time as rewards in order to make it bounded will change the goal of the game significantly.\nAlso, the game is not symmetric. On one hand, both teams do not have the same number of slots, which makes the game not Total Inter-Team Symmetric. On the other hand, we could say that the environment is Intra-Team Symmetric, since every ghost has the same probability to chase Pac-Man, but this is not exact, since each ghost appears in different moments of the game, so swapping their behaviour could not provide exactly the same results, making the environment not Intra-Team Symmetric.\nThe action dependency (AD) property seems to be as in previous environments. All agents have the possibility to ignore the actions of the other agents or act according to what they did in previous interactions.\nWhen we look at the reward dependency (RD) property, it could obtain some different values. Indeed, it is too easy to chase Pac-Man if the four ghosts cooperate coherently, but a bad behaviour for the ghosts can facilitate the game for Pac-Man. In addition, small differences in the behaviour of the agents can provide very different results as, for example, a ghost passes near Pac-Man and decides to chase or to avoid it. This small difference in behaviour will provide high differences in their results. Also, when a ghost is far from Pac-Man, small differences in its behaviour will probably lead to similar results.\nThe fine discrimination (FD) property also has a huge range of values. As mentioned above, the behaviour\nof two different evaluated agents can both obtain the same or very different results, highly depending on the behaviour of the other agents.\nIt seems difficult to know whether we can establish a grading between the evaluated agents in this environment. But we venture that the grading properties could be similar to the ones provided in the predator-prey environment, since both environments have many similarities.\nIt is also difficult to provide a slot reward dependency, since rewards obtained by one team typically do not reflect on the other team. For example, every point obtained by Pac-Man does not directly have influence on the ghost team\u2019s rewards, and chasing Pac-Man only prevents it from obtaining more points. But just assuming that the rewards of each team are always different, we can obtain a value as we did in the SRD for the predator-prey environment (which has a similar configuration) to obtain an approximated value, meaning that the slot reward dependency is more focused on cooperation than competition.\nIf we look at the anticipation properties, it is possible that competitive anticipation does not have a huge reflect on rewards, but still anticipating competitors will provide some good rewards. In cooperative anticipation, it is possible that one ghost can do worse than a random agent, leading to really bad values in this case, but a good anticipation can make chasing Pac-Man easier.\nThis game is not very reliable as it depends on many small details. Also, the game is not efficient. We will need to run the game at least for dozens of iterations to get some stability in the agents\u2019 expected average rewards, since many of the first actions obtain the same rewards but the crucial part of the game comes when the pills become scattered.\nFinally, with this game we can both evaluate competition and cooperation. We can find competition, since each team can only gain rewards by making the other team lose rewards. Additionally, in the ghost team cooperation is also needed to properly chase Pac-Man. For this game, the selection of agents is crucial, a set of predators with high level of intelligence can make Pac-Man efforts useless, which will always obtain bad results. This makes the game somewhat valid, but only for lower levels of (social) intelligence if the selection of agents is wisely chosen, but not for agents with high levels of (social) intelligence where the game becomes not valid."}, {"heading": "5.6 RoboCup Soccer", "text": "As an example of a 3D space game we find the RoboCup Soccer competition [38]. Here, two artificial multi-agent systems (or teams) have to compete against each other in order to win a soccer match. The agents in each team must cooperate to make the ball reach the adversary\u2019s goal, while cooperate to avoid the adversary to score a goal. The game follows the rules of a typical soccer match. Figure 14 shows a RoboCup Soccer match.\nAs happens with the previous environment, this game has a huge diversity of possible situations that can occur (including physical and virtual versions), which makes difficult to formally analyse it. Again, in such complex scenario, it becomes more difficult to determine the levels of cooperation and competition. But also, due to the high level of complexity of this game, teams tend to some specialisation, with each player focussing on some specific aspects of the game instead of focusing on the problem in a general way. Again, we will analyse this environment in an informal way. As in previous environments we will assume uniform weights for wL\u0307 and wS .\nAs in the previous environment, in figure 15 we show a summary with an estimation of the social properties for the RoboCup Soccer.\nWhen we start with the properties we see that this game is bounded, since rewards (1 for win, 0 for a tie and \u22121 for lose) are bounded. This makes that \u03a5 will be bounded if weights are also bounded.\nWhen we look at the symmetry property, both teams have the same number of slots and, if we ignore which team starts with the possession of the ball on each half, it makes the environment Total Inter-Team Symmetric, so both teams can swap their slots and the result will remain the same. But, if we want to swap the slots of two players within the same team, they will not obtain the same results (as for example the goalkeeper has different rules), so the environment is not Intra-Team Symmetric. Since for the symmetry condition we need the environment to be both Total Inter-Team Symmetric and Intra-Team Symmetric, we can conclude that the RoboCup Soccer is not symmetric.\nThe action dependency (AD) property is similar to previous environments. All the evaluated agents have the possibility to act differently depending on the agents they encounter, but, at least, in this game an agent can affect the actions of the evaluated agents. For example, one agent can knock the evaluated agent down to the ground, so now it will only be able to stand up.\nWe now see the reward dependency (RD) property, which can have a huge range. A change in the line-up of course can change the result of the match. Conversely, only changing one agent in the line-up can completely change the match result to make a team lose. changing the agents\u2019 rewards. And we can reason in the same way for the fine discrimination (FD) property, since the behaviour of only one agent (the agent to be evaluated) can also make its team to either win/lose the game, or obtain the same result.\nIt is not easy to determine if there exists some grading between the evaluated agents in the RoboCup Soccer. Instead, we can take a look at some professional (human) Soccer leagues. It is not unusual to see situations where two teams repeatedly tie, and a third team beats one of them while loses against the other one repeatedly. This situation shows us that there is no strict grading between teams (and neither is between their players) for this game.\nThe slot reward dependency is straightforward for this environment. We have two teams with five players\non each team. Every agent within a team will obtain the same reward, while the other agents within the other team will obtain the opposite reward. Using a correlation function over these rewards and over all slots, we obtain a slot reward dependency equal to 0.\nIf we look at the anticipation properties, correctly anticipating both competitive and cooperative can provide a high advantage for each of the teams, so the team of the evaluated agent can score more goals and win the game more easily. A bad competitive anticipation can lead the opponent to win the game, while a good one can provide good results. In cooperative anticipation, the evaluated agent could play worse if it is not correctly anticipating its teammate, but a good anticipation can provide them really good results.\nLet us now consider the validity of this game. First, with this game we can both evaluate competition and cooperation. We can find competition, since both teams must compete to win the game. Additionally, the agents within each team can cooperate to mislead the other team and score more goals. Second, increasing the social intelligence of the agents will typically increase the difficulty of the match, since more skilled agents will score goals more easily, and also defend better, preventing the other team to score. This makes the game useful to match a high variety of skill levels. But conversely, this game also evaluates some other skills than social intelligence, such as for example, their ability to predict the movement that the ball will do when it is kicked. In principle, there are reasons to consider this game a valid scenario to evaluate social intelligence. However, since the agents will need more than their social intelligence in order to play the game, we think that an evaluation using this kind of environment will not only evaluate social intelligence, but also other abilities such as motion understanding. For this reason we consider this environment not valid to evaluate social intelligence.\nThis game is not reliable. In this game, it is widely known that two teams that are facing each other do not necessarily obtain the same result in every match. This depends highly on the decisions made by the agents, since a small decision change can lead to a very different result on the match. Also, the result of the game depends on luck (at least for the physical version), since the players cannot always predict the correct movement of the ball."}, {"heading": "5.7 Summary", "text": "So far, we have analysed some ranges of values that the properties presented in this paper can have for the five environments we have selected. Next, let us group the environment properties by the different ranges in order to make a comparison between the environments through their properties.\nIn figure 16 we see the General range of the quantitative properties for the five environments.\nAs we can see in the top part of figure 16, there are few differences with respect to this range for the environments. In the first three properties all the environments have the broadest possible range. We only see some difference for the grading properties in the last three environments. While the first two environments have the broadest possible range, the last three environments seems that as long as the strict total grading gets worse, the partial grading gets a better range. This is simply explained because agents in the same team obtain the same rewards, so it is not possible to obtain a strict total ordering for them while it is still possible for agents in different teams. Obviously, the agents in the same team always have a partial ordering, making this range higher.\nIn the bottom part of figure 16 we can see more differences. In the slot reward dependency property we can see the first big difference between the environments. The majority of the environments have a unique (and usually different) value for this property. While the matching pennies is completely competitive, Pac-Man is slightly more oriented to cooperation, and predator-prey and RoboCup Soccer are neutral (i.e., provides both competition and cooperation to the same extent). But, prisoner\u2019s dilemma is the only one that has the broadest possible range instead of having a predetermined configuration, so this environment allows the agents to dynamically cooperate and compete with agents in the other team depending on which actions they perform.\nFinally we have the two anticipation properties. In both of them, the agents obtain better rewards when\nthey correctly anticipate, but their rewards get worse when they incorrectly anticipate. In the competitive anticipation we see some small differences between the environments. While a correct competitive anticipation provides good rewards, some environments usually provide much worse rewards when incorrectly anticipating competitive agents, as we can see for the predator-prey and RoboCup Soccer. For the cooperative anticipation we see another different picture. In this case two of the environments do not have this property, since they do not provide teams where the agents will be able to cooperate. However, the last three environments provide teams where cooperative anticipation can be useful between cooperative agents, and also they (almost) provide the broadest range for this property.\nIt is much more interesting to see what happens with these environments if we make a bad selection of \u03a0o. In figure 17 we can see the Left ranges of the quantitative properties which range is between 0 and 1 for the five environments.\nWe can see that both the action and reward dependency properties have the worst possible range for all the environments. This means that a bad selection of \u03a0o will be disastrous with respect to these properties. In fact, this is not surprising, since \u03a0o could be populated only with agents having the same exact behaviour, so the evaluated agents will not be able to behave or obtain different rewards depending on which agents from \u03a0o they are interacting with.\nFor the fine discrimination property, we can see that (almost) all the environments have a very low range, so the evaluated agents can hardly be discriminated.\nThe strict total grading property clearly gives us an order of their compliance for the environments. RoboCup Soccer is clearly the worst environment, while predator-prey and Pac-Man follow it. Meanwhile, matching pennies and prisoner\u2019s dilemma cannot be restricted with any particular \u03a0o, obtaining the best range for this property.\nFinally, the partial grading has different ranges for the environments. Predator-prey and Pac-Man have the same range, and they also have a clearly better range than the RoboCup Soccer. Matching pennies and prisoner\u2019s dilemma have the broadest possible range. At this point, it is not clear which pair matching pennies and prisoner\u2019s dilemma, or predator-prey and Pac-Man has a better range. From one side, the range for matching pennies and prisoner\u2019s dilemma goes from 0 to 1, so the selection of \u03a0o does not necessarily worsens the property, but still it is possible to have a bad value. From the other side, the worst value for the predatorprey and Pac-Man cannot be as bad as in matching pennies and prisoner\u2019s dilemma. However, the selection of \u03a0o does worsen the values for this property.\nLet us next see the effect that a good selection of \u03a0o can have on some quantitative properties for the environments. This is possibly the most interesting picture, because it gives us the best we can do with a right\nchoice of \u03a0o. In figure 18 we can see the Right ranges of the quantitative properties which range is between 0 and 1 for the five environments.\nWe can see that the action dependency and fine discrimination properties cannot improve much by selecting an appropriate \u03a0o. At least RoboCup Soccer can slightly restrict its action dependency range, which means that even with the best possible choice of \u03a0o we cannot ensure that there will be a high action dependency.\nFor the reward dependency property the ranges vary. Matching pennies and RoboCup Soccer cannot restrict their respective ranges. Pac-Man can slightly restrict this range and predator-prey does not have a bad range, so depending on which line-up pattern the evaluated agents encounter, they will certainly obtain some differences in their expected average rewards. But prisoner\u2019s dilemma can ace this property, making the expected average rewards different for every evaluated agent depending on the line-up pattern they interact with.\nFor the strict total grading we find more differences. It is not clear which environment has a better range, but at least we can say that RoboCup Soccer has the worst range among the five environments. However, it is not as clear which of the other four environment has a better range. From one side, the range of matching pennies and prisoners\u2019 dilemma goes from 0 to 1, so the selection of \u03a0o does not necessarily improve their property, but still it is possible to have a good value. On the other hand, the best value for the predator-prey and Pac-Man cannot be as good as in matching pennies and prisoner\u2019s dilemma. However, the selection of \u03a0o does improve the values for this property.\nFinally, the partial grading gives us more information about the orderings. RoboCup Soccer has improved its range, but it is still worse than the predator-prey and Pac-Man, which now are clearly the best to obtain an ordering. However matching pennies and prisoner\u2019s dilemma cannot restrict this range, making them the worst of the five environments.\nOverall, the Right range is more informative. Note that the \u03a0o that we use to calculate the values for each environment\u2019s property is not necessarily the same. We just obtain the values locally for each property. That means that some points could not be achievable at the same time.\nLastly, in figure 19 we see a summary of the qualitative properties to obtain a practical test for the environments.\nAs we can see, almost all the environments have bounded rewards. This provides a bounded value for \u03a5. But only the prisoner\u2019s dilemma is symmetric, so in order to evaluate an agent in the other environments, we will need to evaluate them in all the slots. With respect to the validity property, no environment is correctly evaluating social intelligence. Some of them are not sufficiently general, as happens with the matching pennies or the prisoners\u2019 dilemma. We have the opposite situation with RoboCup Soccer, where more abilities are evaluated and it seems difficult to isolate social intelligence from these other abilities. Finally, other environments can only\nevaluate the social intelligence to a certain degree, as happens in the predator-prey or Pac-Man. With respect to the reliable property, the prisoners\u2019 dilemma can evaluate every level of performance with a good result, but we cannot say the same for the rest of the environments. In matching pennies every agent interacting against a random agent will obtain the same result independently of their intelligence, while the rest of environments will not be able to provide a correct value for this property over certain level of intelligence. Lastly, due to its simplicity, matching pennies, prisoners\u2019 dilemma and predator-prey are really efficient to provide a result for the agents, but the rest of environments will need to run the game during several iterations and episodes to converge to their results.\nFrom this analysis and comparison between the properties of the environments we made in this section, we can provide some insights. First, we give some findings about the five environments.\n\u2022 As we have seen in our analysis, these environments are typically covering anticipation well. Competitive anticipation is well covered in all the environments, while cooperative anticipation is not defined for the first two environments, but the last three are covering it very well. It also seems that the partial grading is generally well covered, so we can find some partial orderings between the evaluated agents.\n\u2022 We can find some other properties that the environments are not covering well. One example is the action dependency, where (almost) every environment analysed in this paper obtained the same poor ranges. This property is something which is not usually thought about when designing an environment, but the possibility of having influence on the actions that other players can do is an interesting thing to consider when designing a multi-agent environment and, in particular, if we want to evaluate social intelligence. Another property which is not usually well covered is the slot reward dependency, where these environments are typically only giving one value. We do not mean that the environments do not have good values, but instead, an environment having a broader range of values will provide us a more interesting scenario, where the relations of competition and cooperation between agents can change dynamically.\n\u2022 As we can see from their qualitative properties, four of the five environments we selected have some difficulties to be used as a practical test. None of them provides symmetry to simplify the evaluation. The range of abilities required to succeed in these environments are not appropriate to be a valid test to evaluate social intelligence accurately, as well as their reliability is compromised depending on the agents we use to populate the environment. At least some of them are efficient enough to obtain results in a short period of time, and they usually provide bounded rewards, so we can calculate a bounded value of the (social) intelligence of the agents.\n\u2022 With these properties, we obtained different ranges of General, Left and Right for each of the five environments. In addition, we could see that some little changes over the definition of an environment (as occurs with the matching pennies and prisoner\u2019s dilemma) are clearly reflected with these properties. In fact, every kind of environment will have particular ranges of values for these properties, with which we will be able to select the (social) environment(s) that best fits our goals (e.g., select an environment focused on anticipating other agents).\n\u2022 As we have seen, a good selection of \u03a0o is crucial in order to obtain appropriate social environments. But, how could we provide a \u03a0o which is appropriate to evaluate the social ability of an agent in a large number of environments? This is a difficult task. When starting the evaluation, since the intelligence of the evaluated agent is not known, it would be appropriate to use one \u03a0o which agents are not too smart during the first environments. As long as the evaluation goes ahead and the intelligence of the evaluee is\nbetter known, it would be better to use another \u03a0o whose agents are conforming to this level of intelligence. In order to solve this problem, we could provide a unique \u03a0o and use some kind of distribution which is continuously evolving, giving more probability to the agents which are obtaining better results on these environments (e.g., as in the spirit of the Darwin-Wallace distribution [18]).\nFrom the previous analysis we can now distinguish the features of the environments that could be reused for the design of better environments to measure social intelligence more effectively. The first environment we saw is matching pennies, but it does not seem to have any particular useful feature from the properties we analysed. Next we saw the prisoner\u2019s dilemma environment, which is similar to the matching pennies with some little modifications. This prisoner\u2019s dilemma offers some nice features to include in a social intelligence test. First, we notice its capability to dynamically change the relation between the slots, providing a competitive and cooperative environment at the same time depending on the agents\u2019 actions. Second, the evaluated agent can obtain drastically different results when it interacts with very different agents from \u03a0o. And third, the symmetry of the payoff matrix, its reliability and efficiency makes this environment a good candidate to provide a simple test. The third environment was the predator-prey. This is the first environment that we analysed providing several agents in (at least) one team. From this team of agents, it is possible to anticipate cooperative agents in such a way that really good performance can be achieved when it is done correctly, and an incorrect anticipation can provide really bad rewards. The same occurs while anticipating competitive agents, but in this case, both teams can anticipate the agents in the other team. Also, we can obtain good partial gradings for the evaluated agents and it is a really efficient game, providing a result in a short period of time. However, Pac-Man and RoboCup Soccer do not provide significant features beyond those provided by predator-prey. At least, in RoboCup Soccer it is possible to exert a slight influence on other agents\u2019 actions, but only to some extent.\nConversely, we also distinguish those features that we do not want to appear in environments for social intelligent tests. The first feature we distinguish is that none of these environments are valid to evaluate social intelligence since they are evaluating: 1) more abilities than necessary, as in RoboCup Soccer where the agents need their motion understanding to play the game, 2) not enough abilities, as in matching pennies and prisoner\u2019s dilemma where the agents cannot cooperate with agents in the same team, or 3) is only valid for lower levels of intelligence, as in predator-prey and Pac-Man where the predators and ghost can easily chase the prey and Pac-Man respectively once they reach a certain level of (social) intelligence. Also, the majority of the environments do not provide a reliable result due to many reasons. For example, the results of the agents can be easily restricted to always obtain the same results (as occurs in matching pennies by using a random agent). Also, little changes in the behaviour of the agents can create a butterfly effect, making the agents to obtain very different results (as occurs in the last three environments). Also, the environments are typically not symmetric, which will force to evaluate the agents in all slots, and complex environments usually need a lot of iterations to provide a \u201creliable\u201d result. In these five environments, it is weird (if not impossible) to find a situation where an agent can directly influence on which actions are available for one (or more) of the other agents. The capability to directly influence on the available actions of the rest of agents could provide us a richer social environment. Also, some environments (matching pennies and prisoner\u2019s dilemma) are not suitable to let the agents anticipate cooperation within a team, since they do not provide the agents a team of agents to cooperate with them. Finally, when we see in more detail some environments, we notice that predator-prey and Pac-Man provide a really difficult/hostile environment, where the predators and ghosts respectively have an enormous advantage to win the game.\nEven if it is not the goal of this paper (but a future work), we consider that a good environment measuring social intelligence would have (at least) these characteristics: 1) It provides two or more teams to interact with, and two or more agents on each team. By having this, the agents will be able to compete against the other team(s), cooperate with the agents within the same team and, in the case where more than two teams are presented in the environment, cooperate with other teams. This will provide anticipation to the environment, so the agents will be able to competitive and cooperative anticipate other agents. 2) The agents should influence in some way the rewards obtained by the other agents, providing reward dependency to the environment. 3) There should be limited rewards that the agents can obtain and the payoff of the agents will only depend on the actions they perform. This will provide us a bounded and symmetric environment, which will be ideal to create a practical test. 4) There should not be easy equilibria in the environment. If such circumstance occurs, most of the agents (the intelligent ones) will always perform the same actions, which will limit the results obtained by the agents. Avoiding easy equilibria will provide to the environment more discriminative power, reward dependency and grading for the agents. 5) The environment should provide different kind of spaces where the agents can move. This will avoid the agents to specialise to a particular space. This will make the environment\nmore valid to evaluate (social) intelligence in a more broader way. 6) The agents must be able to influence in some extent the actions that the other agents can perform, creating richer social situations and providing some action dependency to the environment.\nFinally, what can we say about the properties? Are they sufficient to characterise any environment? How should they be used? Are the plots useful? Is the Left and Right ranges more meaningful than the General ranges? Some insights below.\n\u2022 With these properties we are able to obtain different values for each environment. This gives us some idea about the strengths and weakness of each environment.\n\u2022 Here, we only evaluated one agent from a set of evaluated agents \u03a0e interacting with a set of opponents and team players \u03a0o. But more specialised properties (and even a definition of social collective intelligence) can be easily extended by, for example, dividing the set of agents \u03a0o into two sets (i.e., one for opponent players and one for team players) or, instead of evaluating the agents in isolation, evaluating together a group (or collective) of agents.\n\u2022 There are also other issues which may not be covered on these properties, as for example communication among teammates. They do not provide information about misleading opponents, or the possibility of the agents to influence the actions of other agents on its benefit. Also, the properties do not show us the contribution of the agents to their teams\u2019 rewards, as well as the impact that their inclusion in the line-up has in the results of the other teams.\n\u2022 We proposed some useful properties to measure the appropriateness of the environments to evaluate social intelligence, which are also useful to characterise these environments. These properties provide us some interesting information about the environments such as the fine and coarse discrimination, which give us a measure of their discriminative power. Other interesting properties are the action, reward and slot reward dependency, giving us an idea about the existing dependency between the actions/rewards of the agents, and which relation between the slots is given more importance in the environment (i.e., it is a more competitive- or cooperative-oriented environment), and if this relation is static or can change during the evaluation.\n\u2022 We used the Left and Right ranges in order to compare for each property how a particular good or bad selection of \u03a0o can affect that property in an environment. Conversely, a real test shall provide a unique \u03a0o to evaluate the agents, obtaining a unique range for each property. This selection of \u03a0o will (most probably) make the properties to barely look like the Left or Right ranges we calculated for this five environments, providing instead more varied ranges. Therefore, a comparison between some testbeds with their \u03a0o fixed will give us a more clear idea about their differences."}, {"heading": "6 Conclusions and Future work", "text": "Social intelligence has been an important area of study in psychology, comparative cognition and economics for more than a century, and more recently, in artificial intelligence. However, despite the fact that other tests have been created to evaluate other cognitive abilities, nowadays it is still difficult to find a proper test to evaluate social intelligence. Also, current tests tend to be focussed on evaluating the ability of a single species and it is even more complicated to find a test to evaluate social intelligence that is applicable to machines. In fact, tests designed to succeed on a task that requires social intelligence usually also require other abilities to succeed in the task, making them not appropriate for this purpose. This lack of general socially-oriented tests may be due to the absence of a precise (and formal) definition of social intelligence.\nIn this paper we formalise a definition of social intelligence and some useful social properties for multi-agent environments. In particular, the contributions are:\n\u2022 We analysed what social behaviour implies, reviewing what it means from several disciplines.\n\u2022 We have considered various options for a definition, and we finally proposed a parametrised definition that formalises the notion of performing well in an environment with other (social) agents. We also indicate how a test can be constructed using this definition.\n\u2022 We proposed some properties along with their formal definitions in order to better analyse the appropriateness of an environment to evaluate social intelligence.\n\u2022 With this definition of social intelligence and the properties proposed, we analysed and compared several tests and games from artificial intelligence and game theory where social intelligence has an important role to see which properties they meet and which can be improved in order to evaluate social intelligence.\nThis definition of social intelligence along with the properties proposed here are a first attempt in order to determine whether a testbed is useful for the assessment of social intelligence. As far as we know, this is the first approximation to provide a formal definition of social intelligence along with some useful properties to judge a certain testbed. More research will provide us more information about which properties can be improved and information about other properties to complement the ones presented here.\nThe characterisation of social intelligence proposed here along with the properties we propose to determine whether an environment is useful to evaluate social intelligence has some open features to be solved.\n\u2022 First, it is not clear which utility function the agents must have. Should they be regulated using a discount factor as usual in reinforcement learning? Should we give more importance to later rewards, when the agents are supposed to understand how to behave? Or is it better to use an average (as we did in this paper), giving the same importance to all the rewards?\n\u2022 Second, every test should provide which level of complexity it is evaluating. We postulated that the level of complexity should be determined by the agents included in the environment (and their intelligence), the agent setting that determines how teams are formed and the environment where the agent is evaluated. We determined how the first two parameters should influence the formula, but without indicating the formula itself. Should the level of intelligence of the agents weight more than the agent setting, or should it be otherwise? How can we consider the environment in this formula?\n\u2022 Third, we noticed that every space used to define an environment necessarily evaluates some spatial intelligence. In theory, we should calculate which part of the result comes from spatial intelligence and subtract it, but this seems very difficult. Alternatively, we could figure out an environment class where no other abilities needed to interact will be really useful. Or at least with a wide variety such that, on average, these other abilities do not bias the result.\n\u2022 Fourth, we could also relax the definition of social intelligence by letting the agents cooperate without placing them in a team. Life has taught us that alliances can arise from several agents, even when they do not share the same objectives, to improve the chances of success. It would be interesting to analyse whether a test evaluating only competition can indirectly evaluate this spontaneous cooperation.\n\u2022 Fifth, social intelligence is linked to communication and language. We have not included any property or feature in the definitions to account for the presence of communication and language, or to facilitate that. Clearly, communication is possible through actions. Even language can be transmitted by the agent actions with a proper coding. However, this could be rendered more easily to agents. Nonetheless, any particular communication protocol can make the test non-universal. Instead we think that some extra actions that could be observed immediately by other agents (or by a subset of them) could be basic enough as a signal.\nThis article goes beyond the simple properties of game theory in many ways, opening a number of possibilities for the evaluation of multi-agent environments. Of course, further research is needed to clarify all these questions. We have set some formal principles and made the difficulties arise. In fact, the evaluation of non-social intelligence itself has not been fully achieved and it is still being investigated. The evaluation of social intelligence is still more convoluted. This article provides the basis of how we can evaluate social intelligence in a formal way following these principles."}, {"heading": "Acknowledgements", "text": "This work was supported by the MEC projects EXPLORA-INGENIO TIN 2009-06078-E, CONSOLIDERINGENIO 26706 and TINs 2010-21062-C02-02, 2013-45732-C4-1-P and GVA projects PROMETEO/2008/051 and PROMETEO/2011/052. Javier Insa-Cabrera was sponsored by Spanish MEC-FPU grant AP2010-4389."}, {"heading": "Appendix", "text": "Before starting with each of the environments, we will prove a lemma that will be helpful for the Left and Right ranges.\nWe could calculate Left and Right using \u03a0e and \u03a0o with a high number of agents. However, the more agents we include the more difficult the calculation becomes. Instead of this, and in order to simplify calculations, we can just use the minimum necessary number of agents in \u03a0e and \u03a0o for that property to obtain the maximum/minimum value following the idea on lemma 1:\nLemma 1. In order to calculate Left/Right maximum/minimum value for a property Prop, the length of the set of evaluated agents |\u03a0e| and the length of the set of opponents and team players |\u03a0o| can be respectively equal to the minimum number of evaluated agents n and agents to populate the environment m needed to calculate Prop.\nProof. Let \u03a0e = {\u03c01, . . . , \u03c0n, . . . , \u03c0p} be the set of agents evaluated with weight w\u03a0e in an environment \u00b5 with weight of slots wS using a set of opponents and team players \u03a0o and wL\u0307 as a weight for line-up patterns.\nLet us suppose that we want to calculate the value for a property Prop which needs n evaluated agents to be defined, its definition calculates first the value for each evaluated agent \u03c0 and then these values are weighted using w\u03a0e(\u03c0) to provide the property value. Following this definition we obtain a list of values (v1, . . . , vn, . . . , vp) (one for each evaluated agent) that will be weighted with w\u03a0e to obtain the property value v. If we get rid of the evaluated agent which obtains the maximum value for Prop and we rearrange w\u03a0e to sum 1, then the new property value v\u2032 will always be lower than v. We can repeat this process until n agents remain in \u03a0e (i.e., |\u03a0e| = n) to obtain vmin for this set of evaluated agents. An analogous process applies to obtaining vmax by getting rid of the evaluated agent which obtains the minimum value for Prop.\nThe same reasoning applies to the properties that calculate the values for pairs of evaluated agents, but in this case we get rid of the agent whose sum of values (the values of the pairs where this agent is used) is highest/lowest. Also, the same reasoning applies for \u03a0o."}, {"heading": "A Matching Pennies properties", "text": "In this section we prove how we obtained the values for the properties for the matching pennies environment (section 5.2). To calculate some of the values for the properties, we make use of lemma 2.\nLemma 2. In the matching pennies environment and for any slot, introducing a random agent \u03c0r in a line-up will always provide an expected average reward equal to 0 for both agents.\nProof. A random agent \u03c0r has a probability of p j r,h = p j r,t = 1 2 to perform both Head and Tail at iteration j. Let us denote with \u03c0s the agent that \u03c0r is interacting with, and denote with p 1 s,h the probability of performing Head and p1s,t = 1\u2212 p1s,h the probability of performing Tail at the first iteration for \u03c0s. To calculate the expected reward of an agent, we sum the possible rewards that this agent can obtain multiplied by the probability that these rewards occurs. When we calculate the expected reward for \u03c0r for the first iteration in the matching pennies environment \u00b5 in any slot i, we obtain:\n\u2200i : R1i (\u03c0r, \u00b5) = p1r,h ( p1s,h \u00d7 r1h,h,i + p1s,t \u00d7 r1h,t,i ) + p1r,t ( p1s,h \u00d7 r1t,h,i + p1s,t \u00d7 r1t,t,i ) where rja1,a2,i is the reward that the agent in slot i obtains at iteration j when one agent performs a1 and the other agent performs a2.\nFrom the matching pennies\u2019 payoff matrix (figure 4), we can see that for every slot i, rh,h,i = rt,t,i and rh,t,i = rt,h,i, so we name them re,i and rd,i respectively. We can also see that the reward values are the inverse of each other, having rd,i = \u2212re,i. Renaming the rewards in the formula and rearranging it we obtain:\n\u2200i : R1i (\u03c0r, \u00b5) = p1r,h ( p1s,h \u00d7 re,i + p1s,t \u00d7 rd,i ) + p1r,t ( p1s,h \u00d7 rd,i + p1s,t \u00d7 re,i ) =\n= p1r,h ( p1s,h \u00d7 re,i + p1s,t \u00d7 (\u2212re,i) ) + p1r,t ( p1s,h \u00d7 (\u2212re,i) + p1s,t \u00d7 re,i ) =\n= p1r,h ( re,i \u00d7 ( p1s,h \u2212 p1s,t )) + p1r,t ( (\u2212re,i)\u00d7 ( p1s,h \u2212 p1s,t )) =\n= p1r,h ( re,i \u00d7 ( p1s,h \u2212 p1s,t )) \u2212 p1r,t ( re,i \u00d7 ( p1s,h \u2212 p1s,t )) =\n= ( p1r,h \u2212 p1r,t ) \u00d7 ( re,i \u00d7 ( p1s,h \u2212 p1s,t )) And since \u03c0r gives the same probability to both Head and Tail (p 1 r,h = p 1 r,t = 1 2 ) we obtain the following\nexpected reward:\n\u2200i : R1i (\u03c0r, \u00b5) = ( 1\n2 \u2212 1 2\n) \u00d7 ( re,i \u00d7 ( p1s,h \u2212 p1s,t )) = 0\nWe calculated the expected reward for the first iteration. At this point \u03c0s could change its behaviour depending on what happened on the previous iteration, using different probabilities p2s,h and p 2 s,t for iteration 2. But note that it does not matter which probabilities pns,h and p n s,t we use, the result will still be 0, and averaging over the iterations we obtain an expected average reward equal to 0. Obviously, since this is a zero-sum game, when \u03c0r obtains an expected average reward of 0, \u03c0s obtains the same expected average reward of 0."}, {"heading": "A.1 Action Dependency", "text": "We start with the action dependency (AD) property. As given in section 4.2.1, we want to know if the evaluated agents behave differently depending on which line-up they interact with. We use \u2206S(a, b) = 1 if distributions a and b are equal and 0 otherwise.\nProposition 2. Generalmin for the action dependency (AD) property is equal to 0 for the matching pennies environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0t} with w\u03a0e(\u03c0t) = 1 and \u03a0o = {\u03c0h1, \u03c0h2} (a \u03c0h agent always performs Head and a \u03c0t agent always performs Tail).\nFollowing definition 14, we obtain the AD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 13, we could calculate its AD value for each slot but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 12. We start with slot 1:\nAD1(\u03c0t,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03061(\u00b5[u\u0307 1\u2190 \u03c0t]), A\u03061(\u00b5[v\u0307 1\u2190 \u03c0t])) =\n= 2 2\n1\n1\n2\n1 2 \u2206S(A\u03061(\u00b5[\u03c0t, \u03c0h1]), A\u03061(\u00b5[\u03c0t, \u03c0h2]))\nNote that we avoided to calculate both \u2206S(a, b) and \u2206S(b, a) since they provide the same result, by calculating only \u2206S(a, b) and multiplying the result by 2.\nIn this case, we only need to calculate \u2206S(A\u03061(\u00b5[\u03c0t, \u03c0h1 ]), A\u03061(\u00b5[\u03c0t, \u03c0h2 ])), where the agent in both slots 1 (\u03c0t) will perform the same sequence of actions (always Tail) independently of the line-up. So:\nAD1(\u03c0t,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd for slot 2, the agent in both slots 2 (\u03c0t) will also perform the same sequence of actions (always Tail) independently of the line-up. So:\nAD2(\u03c0t,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03062(\u00b5[u\u0307 2\u2190 \u03c0t]), A\u03062(\u00b5[v\u0307 2\u2190 \u03c0t])) =\n= 2 2\n1\n1\n2\n1 2 \u2206S(A\u03062(\u00b5[\u03c0h1, \u03c0t]), A\u03062(\u00b5[\u03c0h2, \u03c0t])) =\n= 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, we weight over the slots:\nAD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03c0t,\u03a0o, wL\u0307, \u00b5) = = 1\n2 {AD1(\u03c0t,\u03a0o, wL\u0307, \u00b5) +AD2(\u03c0t,\u03a0o, wL\u0307, \u00b5)} =\n= 1\n2 {0 + 0} = 0\nSince 0 is the lowest possible value for the action dependency property, therefore matching pennies has Generalmin = 0 for this property.\nProposition 3. Generalmax for the action dependency (AD) property is equal to 1 for the matching pennies environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0m} with w\u03a0e(\u03c0m) = 1 and \u03a0o = {\u03c0h, \u03c0t} (a \u03c0m agent first acts randomly and then always mimics the other agent\u2019s last action, a \u03c0h agent always performs Head and a \u03c0t agent always performs Tail).\nFollowing definition 14, we obtain the AD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 13, we could calculate its AD value for each slot but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 12. We start with slot 1:\nAD1(\u03c0m,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03061(\u00b5[u\u0307 1\u2190 \u03c0m]), A\u03061(\u00b5[v\u0307 1\u2190 \u03c0m])) =\n= 2 2\n1\n1\n2\n1 2 \u2206S(A\u03061(\u00b5[\u03c0m, \u03c0h]), A\u03061(\u00b5[\u03c0m, \u03c0t]))\nNote that we avoided to calculate both \u2206S(a, b) and \u2206S(b, a) since they provide the same result, by calculating only \u2206S(a, b) and multiplying the result by 2.\nFrom iteration 2, \u03c0m will mimic the last action of the agent in slot 2, and since \u03c0h will always perform Head and \u03c0t will always perform Tail, the agent in both slots 1 (\u03c0m) will perform different sequences of actions on each line-up. So:\nAD1(\u03c0m,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 1 = 1\nAnd for slot 2, the agent in both slots 2 (\u03c0m) will also perform different sequences of actions on each line-up. So:\nAD2(\u03c0m,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03062(\u00b5[u\u0307 2\u2190 \u03c0m]), A\u03062(\u00b5[v\u0307 2\u2190 \u03c0m])) =\n= 2 2\n1\n1\n2\n1 2 \u2206S(A\u03062(\u00b5[\u03c0h, \u03c0m]), A\u03062(\u00b5[\u03c0t, \u03c0m])) =\n= 2 2\n1\n1\n2\n1 2 1 = 1\nAnd finally, we weight over the slots:\nAD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03c0m,\u03a0o, wL\u0307, \u00b5) = = 1\n2 {AD1(\u03c0m,\u03a0o, wL\u0307, \u00b5) +AD2(\u03c0m,\u03a0o, wL\u0307, \u00b5)} =\n= 1\n2 {1 + 1} = 1\nSince 1 is the highest possible value for the action dependency property, therefore matching pennies has Generalmax = 1 for this property.\nProposition 4. Leftmax for the action dependency (AD) property is equal to 0 for the matching pennies environment.\nProof. To find Leftmax (equation 43), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which maximises the property as much as possible while \u03a0o minimises it. Using \u03a0o = {\u03c0h1, \u03c0h2} (a \u03c0h agent always performs Head) we find this situation no matter which pair \u27e8\u03a0e, w\u03a0e\u27e9 we use.\nFollowing definition 14, we obtain the AD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0e and w\u03a0e are instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 13, we can calculate its AD value for each slot. We start with slot 1:\nAD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)AD1(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate AD1(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 12 to calculate this value for a figurative evaluated agent \u03c0 from \u03a0e:\nAD1(\u03c0,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03061(\u00b5[u\u0307 1\u2190 \u03c0]), A\u03061(\u00b5[v\u0307 1\u2190 \u03c0])) =\n= 2 2\n1\n1\n2\n1 2 \u2206S(A\u03061(\u00b5[\u03c0, \u03c0h1]), A\u03061(\u00b5[\u03c0, \u03c0h2]))\nNote that we avoided to calculate both \u2206S(a, b) and \u2206S(b, a) since they provide the same result, by calculating only \u2206S(a, b) and multiplying the result by 2.\nA \u03c0h agent will always perform Head, so we obtain a situation where the agent in both slots 1 (any \u03c0) will be able to differentiate with which agent is interacting, so it will not be able to change its distribution of action sequences depending on the opponent\u2019s behaviour. So:\nAD1(\u03c0,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nTherefore, no matter which agents are in \u03a0e and their weights w\u03a0e we obtain:\nAD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nAnd for slot 2:\nAD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)AD2(\u03c0,\u03a0o, wL\u0307, \u00b5)\nAgain, we do not know which \u03a0e we have, but we know that we will need to evaluate AD2(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 12 to calculate this value for a figurative evaluated agent \u03c0 from \u03a0e:\nAD2(\u03c0,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03062(\u00b5[u\u0307 2\u2190 \u03c0]), A\u03062(\u00b5[v\u0307 2\u2190 \u03c0])) =\n= 2 2\n1\n1\n2\n1 2 \u2206S(A\u03062(\u00b5[\u03c0h1, \u03c0]), A\u03062(\u00b5[\u03c0h2, \u03c0]))\nAgain, a \u03c0h agent will always perform Head, so we obtain a situation where the agent in both slots 2 (any \u03c0) will be able to differentiate with which agent is interacting, so it will not be able to change its distribution of action sequences depending on the opponent\u2019s behaviour. So:\nAD2(\u03c0,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nTherefore, no matter which agents are in \u03a0e and their weights w\u03a0e we obtain:\nAD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nAnd finally, we weight over the slots:\nAD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 1\n2 {AD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +AD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} =\n= 1\n2 {0 + 0} = 0\nSo, for every pair \u27e8\u03a0e, w\u03a0e\u27e9 we obtain the same result:\n\u2200\u03a0e, w\u03a0e : AD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 0\nTherefore, matching pennies has Leftmax = 0 for this property.\nProposition 5. Rightmin for the action dependency (AD) property is equal to 0 for the matching pennies environment.\nProof. To find Rightmin (equation 44), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which minimises the property as much as possible while \u03a0o maximises it. Using \u03a0e = {\u03c0h} with w\u03a0e(\u03c0h) = 1 (a \u03c0h agent always performs Head) we find this situation no matter which \u03a0o we use.\nFollowing definition 14, we obtain the AD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 13, we could calculate its AD value for each slot but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 12. We start with slot 1:\nAD1(\u03c0h,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03061(\u00b5[u\u0307 1\u2190 \u03c0h]), A\u03061(\u00b5[v\u0307 1\u2190 \u03c0h]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain two different line-up patterns\nu\u0307 and v\u0307 from L\u0307 N(\u00b5) \u22121 (\u03a0o) to calculate \u2206S(A\u03061(\u00b5[u\u0307 1\u2190 \u03c0h]), A\u03061(\u00b5[v\u0307 1\u2190 \u03c0h])). We calculate this value for two figurative line-up patterns u\u0307 = (\u2217, \u03c01) and v\u0307 = (\u2217, \u03c02) from L\u0307N(\u00b5)\u22121 (\u03a0o):\n\u2206S(A\u03061(\u00b5[u\u0307 1\u2190 \u03c0h]), A\u03061(\u00b5[v\u0307 1\u2190 \u03c0h])) = \u2206S(A\u03061(\u00b5[\u03c0h, \u03c01]), A\u03061(\u00b5[\u03c0h, \u03c02]))\nHere, the agent in both slots 1 (\u03c0h) will perform the same sequence of actions (always Head) independently of the line-up. So no matter which agents are in \u03a0o we obtain:\nAD1(\u03c0h,\u03a0o, wL\u0307, \u00b5) = 0\nAnd for slot 2: AD2(\u03c0h,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)|u\u0307 \u0338=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03062(\u00b5[u\u0307 2\u2190 \u03c0h]), A\u03062(\u00b5[v\u0307 2\u2190 \u03c0h]))\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain two different line-up\npatterns u\u0307 and v\u0307 from L\u0307 N(\u00b5) \u22122 (\u03a0o) to calculate \u2206S(A\u03062(\u00b5[u\u0307 2\u2190 \u03c0h]), A\u03062(\u00b5[v\u0307 2\u2190 \u03c0h])). We calculate this value for two figurative line-up patterns u\u0307 = (\u2217, \u03c01) and v\u0307 = (\u2217, \u03c02) from L\u0307N(\u00b5)\u22122 (\u03a0o):\n\u2206S(A\u03062(\u00b5[u\u0307 2\u2190 \u03c0h]), A\u03062(\u00b5[v\u0307 2\u2190 \u03c0h])) = \u2206S(A\u03062(\u00b5[\u03c01, \u03c0h]), A\u03062(\u00b5[\u03c02, \u03c0h]))\nAgain, the agent in both slots 2 (\u03c0h) will perform the same sequence of actions (always Head) independently of the line-up. So no matter which agents are in \u03a0o we obtain:\nAD2(\u03c0h,\u03a0o, wL\u0307, \u00b5) = 0\nAnd finally, we weight over the slots:\nAD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03c0h,\u03a0o, wL\u0307, \u00b5) = = 1\n2 {AD1(\u03c0h,\u03a0o, wL\u0307, \u00b5) +AD2(\u03c0h,\u03a0o, wL\u0307, \u00b5)} =\n= 1\n2 {0 + 0} = 0\nSo, for every \u03a0o we obtain the same result:\n\u2200\u03a0o : AD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 0\nTherefore, matching pennies has Rightmin = 0 for this property."}, {"heading": "A.2 Reward Dependency", "text": "We continue with the reward dependency (RD) property. As given in section 4.3.1, we want to know if the evaluated agents obtain different expected average rewards depending on which line-up they interact with. We use \u2206Q(a, b) = 1 if numbers a and b are equal and 0 otherwise.\nProposition 6. Generalmin for the reward dependency (RD) property is equal to 0 for the matching pennies environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0t} with w\u03a0e(\u03c0t) = 1 and \u03a0o = {\u03c0h1, \u03c0h2} (a \u03c0h agent always performs Head and a \u03c0t agent always performs Tail).\nFollowing definition 17, we obtain the RD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 16, we could calculate its RD value for each slot but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 15. We start with slot 1:\nRD1(\u03c0t,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0307 \u0338=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R1(\u00b5[u\u0307 1\u2190 \u03c0t]), R1(\u00b5[v\u0307 1\u2190 \u03c0t])) =\n= 2 2\n1\n1\n2\n1 2 \u2206Q(R1(\u00b5[\u03c0t, \u03c0h1]), R1(\u00b5[\u03c0t, \u03c0h2]))\nNote that we avoided to calculate both \u2206Q(a, b) and \u2206Q(b, a) since they provide the same result, by calculating only \u2206Q(a, b) and multiplying the result by 2.\nIn this case, we only need to calculate \u2206Q(R1(\u00b5[\u03c0t, \u03c0h1]), R1(\u00b5[\u03c0t, \u03c0h2])), where \u03c0t will always perform Tail and a \u03c0h agent will always perform Head, so the agent in both slots 1 (\u03c0t) will obtain the same expected average reward (\u22121) independently of the line-up. So:\nRD1(\u03c0t,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd for slot 2, the agent in both slots 2 (\u03c0t) will also obtain the same expected average reward (1) independently of the line-up. So:\nRD2(\u03c0t,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)|u\u0307 \u0338=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R2(\u00b5[u\u0307 2\u2190 \u03c0t]), R2(\u00b5[v\u0307 2\u2190 \u03c0t])) =\n= 2 2\n1\n1\n2\n1 2 \u2206Q(R2(\u00b5[\u03c0h1, \u03c0t]), R2(\u00b5[\u03c0h2, \u03c0t])) =\n= 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, we weight over the slots:\nRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03c0t,\u03a0o, wL\u0307, \u00b5) = = 1\n2 {RD1(\u03c0t,\u03a0o, wL\u0307, \u00b5) +RD2(\u03c0t,\u03a0o, wL\u0307, \u00b5)} =\n= 1\n2 {0 + 0} = 0\nSince 0 is the lowest possible value for the reward dependency property, therefore matching pennies has Generalmin = 0 for this property.\nProposition 7. Generalmax for the reward dependency (RD) property is equal to 1 for the matching pennies environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0t} with w\u03a0e(\u03c0t) = 1 and \u03a0o = {\u03c0h, \u03c0t} (a \u03c0h agent always performs Head and a \u03c0t agent always performs Tail).\nFollowing definition 17, we obtain the RD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 16, we could calculate its RD value for each slot but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 15. We start with slot 1:\nRD1(\u03c0t,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0307 \u0338=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R1(\u00b5[u\u0307 1\u2190 \u03c0t]), R1(\u00b5[v\u0307 1\u2190 \u03c0t])) =\n= 2 2\n1\n1\n2\n1 2 \u2206Q(R1(\u00b5[\u03c0t, \u03c0h]), R1(\u00b5[\u03c0t, \u03c0t]))\nNote that we avoided to calculate both \u2206Q(a, b) and \u2206Q(b, a) since they provide the same result, by calculating only \u2206Q(a, b) and multiplying the result by 2.\nIn line-up (\u03c0t, \u03c0h), where \u03c0t will always perform Tail and \u03c0h will always perform Head, the agent in slot 1 (\u03c0t) will obtain one expected average reward (\u22121), while in line-up (\u03c0t, \u03c0t), where both \u03c0t will always perform Tail, the agent in slot 1 (\u03c0t) will obtain a different expected average reward (1). So:\nRD1(\u03c0t,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 1 = 1\nAnd for slot 2, the agent in slot 2 (\u03c0t) will also obtain different expected average rewards depending on the line-up. So:\nRD2(\u03c0t,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)|u\u0307 \u0338=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R2(\u00b5[u\u0307 2\u2190 \u03c0t]), R2(\u00b5[v\u0307 2\u2190 \u03c0t])) =\n= 2 2\n1\n1\n2\n1 2 \u2206Q(R2(\u00b5[\u03c0h, \u03c0t]), R2(\u00b5[\u03c0t, \u03c0t]))\n= 2 2\n1\n1\n2\n1 2 1 = 1\nAnd finally, we weight over the slots:\nRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03c0t,\u03a0o, wL\u0307, \u00b5) = = 1\n2 {RD1(\u03c0t,\u03a0o, wL\u0307, \u00b5) +RD2(\u03c0t,\u03a0o, wL\u0307, \u00b5)} =\n= 1\n2 {1 + 1} = 1\nSince 1 is the highest possible value for the reward dependency property, therefore matching pennies has Generalmax = 1 for this property.\nProposition 8. Leftmax for the reward dependency (RD) property is equal to 0 for the matching pennies environment.\nProof. To find Leftmax (equation 43), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which maximises the property as much as possible while \u03a0o minimises it. Using \u03a0o = {\u03c0h1, \u03c0h2} (a \u03c0h agent always performs Head) we find this situation no matter which pair \u27e8\u03a0e, w\u03a0e\u27e9 we use.\nFollowing definition 17, we obtain the RD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0e and w\u03a0e are instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 16, we can calculate its RD value for each slot. We start with slot 1:\nRD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)RD1(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate RD1(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 15 to calculate this value for a figurative evaluated agent \u03c0 from \u03a0e:\nRD1(\u03c0,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R1(\u00b5[u\u0307 1\u2190 \u03c0]), R1(\u00b5[v\u0307 1\u2190 \u03c0])) =\n= 2 2\n1\n1\n2\n1 2 \u2206Q(R1(\u00b5[\u03c0, \u03c0h1]), R1(\u00b5[\u03c0, \u03c0h2]))\nNote that we avoided to calculate both \u2206Q(a, b) and \u2206Q(b, a) since they provide the same result, by calculating only \u2206Q(a, b) and multiplying the result by 2.\nA \u03c0h agent will always perform Head, so we obtain a situation where the agent in both slots 1 (any \u03c0) will be able to differentiate with which agent is interacting, so it will not be able to change its distribution of action\nsequences depending on the opponent\u2019s behaviour, obtaining agent in both slots 1 (any \u03c0) the same expected average reward. So:\nRD1(\u03c0,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nTherefore, no matter which agents are in \u03a0e and their weights w\u03a0e we obtain:\nRD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nAnd for slot 2:\nRD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)RD2(\u03c0,\u03a0o, wL\u0307, \u00b5)\nAgain, we do not know which \u03a0e we have, but we know that we will need to evaluate RD2(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 15 to calculate this value for a figurative evaluated agent \u03c0 from \u03a0e:\nRD2(\u03c0,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R2(\u00b5[u\u0307 2\u2190 \u03c0]), R2(\u00b5[v\u0307 2\u2190 \u03c0])) =\n= 2 2\n1\n1\n2\n1 2 \u2206Q(R2(\u00b5[\u03c0h1, \u03c0]), R2(\u00b5[\u03c0h2, \u03c0]))\nAgain, a \u03c0h agent will always perform Head, so we obtain a situation where the agent in both slots 2 (any \u03c0) will be able to differentiate with which agent is interacting, so it will not be able to change its distribution of action sequences depending on the opponent\u2019s behaviour, obtaining agent in both slots 2 (any \u03c0) the same expected average reward. So:\nRD2(\u03c0,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nTherefore, no matter which agents are in \u03a0e and their weights w\u03a0e we obtain:\nRD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nAnd finally, we weight over the slots:\nRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 1\n2 {RD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +RD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} =\n= 1\n2 {0 + 0} = 0\nSo, for every pair \u27e8\u03a0e, w\u03a0e\u27e9 we obtain the same result:\n\u2200\u03a0e, w\u03a0e : RD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 0\nTherefore, matching pennies has Leftmax = 0 for this property.\nProposition 9. Rightmin for the reward dependency (RD) property is equal to 0 for the matching pennies environment.\nProof. To find Rightmin (equation 44), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which minimises the property as much as possible while \u03a0o maximises it. Using \u03a0e = {\u03c0r} with w\u03a0e(\u03c0r) = 1 (a \u03c0r agent always acts randomly) we find this situation no matter which \u03a0o we use.\nFollowing definition 17, we obtain the RD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 16, we could calculate its RD value for each slot but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 15. We start with slot 1:\nRD1(\u03c0r,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0307 \u0338=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R1(\u00b5[u\u0307 1\u2190 \u03c0r]), R1(\u00b5[v\u0307 1\u2190 \u03c0r]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain two different line-up patterns u\u0307\nand v\u0307 from L\u0307 N(\u00b5) \u22121 (\u03a0o) to calculate \u2206Q(R1(\u00b5[u\u0307 1\u2190 \u03c0r]), R1(\u00b5[v\u0307 1\u2190 \u03c0r])). We calculate this value for two figurative line-up patterns u\u0307 = (\u2217, \u03c01) and v\u0307 = (\u2217, \u03c02) from L\u0307N(\u00b5)\u22121 (\u03a0o):\n\u2206Q(R1(\u00b5[u\u0307 1\u2190 \u03c0r]), R1(\u00b5[v\u0307 1\u2190 \u03c0r])) = \u2206Q(R1(\u00b5[\u03c0r, \u03c01]), R1(\u00b5[\u03c0r, \u03c02]))\nHere, the agent in both slots 1 (\u03c0r) makes its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So no matter which agents are in \u03a0o we obtain:\nRD1(\u03c0r,\u03a0o, wL\u0307, \u00b5) = 0\nAnd for slot 2: RD2(\u03c0r,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)|u\u0307 \u0338=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R2(\u00b5[u\u0307 2\u2190 \u03c0r]), R2(\u00b5[v\u0307 2\u2190 \u03c0r]))\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain two different line-up\npatterns u\u0307 and v\u0307 from L\u0307 N(\u00b5) \u22122 (\u03a0o) to calculate \u2206Q(R2(\u00b5[u\u0307 2\u2190 \u03c0r]), R2(\u00b5[v\u0307 2\u2190 \u03c0r])). We calculate this value for two figurative line-up patterns u\u0307 = (\u2217, \u03c01) and v\u0307 = (\u2217, \u03c02) from L\u0307N(\u00b5)\u22122 (\u03a0o):\n\u2206Q(R2(\u00b5[u\u0307 2\u2190 \u03c0r]), R2(\u00b5[v\u0307 2\u2190 \u03c0r])) = \u2206Q(R2(\u00b5[\u03c01, \u03c0r]), R2(\u00b5[\u03c02, \u03c0r]))\nAgain, the agent in both slots 2 (\u03c0r) makes its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So no matter which agents are in \u03a0o we obtain:\nRD2(\u03c0r,\u03a0o, wL\u0307, \u00b5) = 0\nAnd finally, we weight over the slots:\nRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03c0r,\u03a0o, wL\u0307, \u00b5) = = 1\n2 {RD1(\u03c0r,\u03a0o, wL\u0307, \u00b5) +RD2(\u03c0r,\u03a0o, wL\u0307, \u00b5)} =\n= 1\n2 {0 + 0} = 0\nSo, for every \u03a0o we obtain the same result:\n\u2200\u03a0o : RD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 0\nTherefore, matching pennies has Rightmin = 0 for this property."}, {"heading": "A.3 Fine Discrimination", "text": "Now we move to the fine discrimination (FD) property. As given in section 4.4.1, we want to know if different evaluated agents obtain different expected average rewards when interacting in the environment. We use \u2206Q(a, b) = 1 if numbers a and b are equal and 0 otherwise.\nProposition 10. Generalmin for the fine discrimination (FD) property is equal to 0 for the matching pennies environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0t1, \u03c0t2} with uniform weight for w\u03a0e and \u03a0o = {\u03c0h} (a \u03c0h agent always performs Head and a \u03c0t agent always performs Tail).\nFollowing definition 23, we obtain the FD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 22, we can calculate its FD value for each slot. We start with slot 1:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD1(\u03c0t1, \u03c0t2,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate both FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)} and FDi(\u03c02, \u03c01,\u03a0o, wL\u0307, \u00b5)} since they provide the same result, by calculating only FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)} and multiplying the result by 2.\nIn this case, we only need to calculate FD1(\u03c0t1, \u03c0t2,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value:\nFD1(\u03c0t1, \u03c0t2,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R1(\u00b5[l\u0307 1\u2190 \u03c0t1]), R1(\u00b5[l\u0307 1\u2190 \u03c0t2])) =\n= \u2206Q(R1(\u00b5[\u03c0t1, \u03c0h]), R1(\u00b5[\u03c0t2, \u03c0h]))\nHere, a \u03c0t agent will always perform Tail and \u03c0h will always perform Head, so both agents in slot 1 (\u03c0t1 and \u03c0t2) will obtain the same expected average reward (\u22121). So:\nFD1(\u03c0t1, \u03c0t2,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd for slot 2: FD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD2(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD2(\u03c0t1, \u03c0t2,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate FD2(\u03c0t1, \u03c0t2,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value:\nFD2(\u03c0t1, \u03c0t2,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R2(\u00b5[l\u0307 2\u2190 \u03c0t1]), R2(\u00b5[l\u0307 2\u2190 \u03c0t2])) =\n= \u2206Q(R2(\u00b5[\u03c0h, \u03c0t1]), R2(\u00b5[\u03c0h, \u03c0t2]))\nHere, \u03c0h will always perform Head and a \u03c0t agent will always perform Tail, so both agents in slot 2 (\u03c0t1 and \u03c0t2) will obtain the same expected average reward (1). So:\nFD2(\u03c0t1, \u03c0t2,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, we weight over the slots:\nFD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)FDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 1\n2 {FD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + FD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} =\n= 1\n2 {0 + 0} = 0\nSince 0 is the lowest possible value for the fine discriminative property, therefore matching pennies has Generalmin = 0 for this property.\nProposition 11. Generalmax for the fine discrimination (FD) property is equal to 1 for the matching pennies environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0h, \u03c0t} with uniform weight for w\u03a0e and \u03a0o = {\u03c0h} (a \u03c0h agent always performs Head and a \u03c0t agent always performs Tail).\nFollowing definition 23, we obtain the FD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 22, we can calculate its FD value for each slot. We start with slot 1:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD1(\u03c0h, \u03c0t,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate both FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)} and FDi(\u03c02, \u03c01,\u03a0o, wL\u0307, \u00b5)} since they provide the same result, by calculating only FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)} and multiplying the result by 2.\nIn this case, we only need to calculate FD1(\u03c0h, \u03c0t,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value: FD1(\u03c0h, \u03c0t,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R1(\u00b5[l\u0307 1\u2190 \u03c0h]), R1(\u00b5[l\u0307 1\u2190 \u03c0t])) =\n= \u2206Q(R1(\u00b5[\u03c0h, \u03c0h]), R1(\u00b5[\u03c0t, \u03c0h]))\nIn line-up (\u03c0h, \u03c0h), where both \u03c0h will always perform Head, the agent in slot 1 (\u03c0h) will obtain one expected average reward (1), while in line-up (\u03c0t, \u03c0h), where \u03c0t will always perform Tail and \u03c0h will always perform Head, the agent in slot 1 (\u03c0t) will obtain a different expected average reward (\u22121). So:\nFD1(\u03c0h, \u03c0t,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 1 = 1\nAnd for slot 2: FD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD2(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD2(\u03c0h, \u03c0t,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate FD2(\u03c0h, \u03c0t,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value: FD2(\u03c0h, \u03c0t,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R2(\u00b5[l\u0307 2\u2190 \u03c0h]), R2(\u00b5[l\u0307 2\u2190 \u03c0t])) =\n= \u2206Q(R2(\u00b5[\u03c0h, \u03c0h]), R2(\u00b5[\u03c0h, \u03c0t]))\nIn line-up (\u03c0h, \u03c0h), where both \u03c0h will always perform Head, the agent in slot 2 (\u03c0h) will obtain one expected average reward (\u22121), while in line-up (\u03c0h, \u03c0t), where \u03c0h will always perform Head and \u03c0t will always perform Head, the agent in slot 2 (\u03c0t) will obtain a different expected average reward (1). So:\nFD2(\u03c0h, \u03c0t,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nFD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 1 = 1\nAnd finally, we weight over the slots:\nFD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)FDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 1\n2 {FD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + FD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} =\n= 1\n2 {1 + 1} = 1\nSince 1 is the highest possible value for the fine discriminative property, therefore matching pennies has Generalmax = 1 for this property.\nProposition 12. Leftmax for the fine discrimination (FD) property is equal to 0 for the matching pennies environment.\nProof. To find Leftmax (equation 43), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which maximises the property as much as possible while \u03a0o minimises it. Using \u03a0o = {\u03c0r} (a \u03c0r agent always acts randomly) we find this situation no matter which pair \u27e8\u03a0e, w\u03a0e\u27e9 we use.\nFollowing definition 23, we obtain the FD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0e and w\u03a0e are instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 22, we can calculate its FD value for each slot. We start with slot 1:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate FD1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) for all pair of evaluated agents \u03c01, \u03c02 \u2208 \u03a0e|\u03c01 \u0338= \u03c02. We follow definition 21 to calculate this value for two figurative evaluated agents \u03c01 and \u03c02 from \u03a0e such that \u03c01 \u0338= \u03c02:\nFD1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R1(\u00b5[l\u0307 1\u2190 \u03c01]), R1(\u00b5[l\u0307 1\u2190 \u03c02])) =\n= \u2206Q(R1(\u00b5[\u03c01, \u03c0r]), R1(\u00b5[\u03c02, \u03c0r]))\nHere, the agent in both slots 2 (\u03c0r) makes its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So no matter which agents \u03c01 and \u03c02 are we obtain:\nFD1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nAnd for slot 2: FD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD2(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)\nAgain, we do not know which \u03a0e we have, but we know that we will need to evaluate FD2(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) for all pair of evaluated agents \u03c01, \u03c02 \u2208 \u03a0e|\u03c01 \u0338= \u03c02. We follow definition 21 to calculate this value for two figurative evaluated agents \u03c01 and \u03c02 from \u03a0e such that \u03c01 \u0338= \u03c02:\nFD2(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R2(\u00b5[l\u0307 2\u2190 \u03c01]), R1(\u00b5[l\u0307 2\u2190 \u03c02])) =\n= \u2206Q(R2(\u00b5[\u03c0r, \u03c01]), R2(\u00b5[\u03c0r, \u03c02]))\nAgain, the agent in both slots 1 (\u03c0r) makes its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So no matter which agents \u03c01 and \u03c02 are we obtain:\nFD2(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nAnd finally, we weight over the slots:\nFD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)FDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)\n= 1\n2 {FD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + FD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} =\n= 1\n2 {0 + 0} = 0\nSo, for every pair \u27e8\u03a0e, w\u03a0e\u27e9 we obtain the same result:\n\u2200\u03a0e, w\u03a0e : FD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 0\nTherefore, matching pennies has Leftmax = 0 for this property.\nProposition 13. Rightmin for the fine discrimination (FD) property is equal to 0 for the matching pennies environment.\nProof. To find Rightmin (equation 44), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which minimises the property as much as possible while \u03a0o maximises it. Using \u03a0e = {\u03c0t1, \u03c0t2} with uniform weight for w\u03a0e (a \u03c0t agent always performs Tail) we find this situation no matter which \u03a0o we use.\nFollowing definition 23, we obtain the FD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 22, we can calculate its FD value for each slot. We start with slot 1:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD1(\u03c0t1, \u03c0t2,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate both FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)} and FDi(\u03c02, \u03c01,\u03a0o, wL\u0307, \u00b5)} since they provide the same result, by calculating only FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)} and multiplying the result by 2.\nIn this case, we only need to calculate FD1(\u03c0t1, \u03c0t2,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value:\nFD1(\u03c0t1, \u03c0t2,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R1(\u00b5[l\u0307 1\u2190 \u03c0t1]), R1(\u00b5[l\u0307 1\u2190 \u03c0t2]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22121 (\u03a0o) to calculate \u2206Q(R1(\u00b5[l\u0307 1\u2190 \u03c0t1]), R1(\u00b5[l\u0307 1\u2190 \u03c0t2])). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c0) from L\u0307N(\u00b5)\u22121 (\u03a0o):\n\u2206Q(R1(\u00b5[l\u0307 1\u2190 \u03c0t1]), R1(\u00b5[l\u0307 1\u2190 \u03c0t2])) = \u2206Q(R1(\u00b5[\u03c0t1, \u03c0]), R1(\u00b5[\u03c0t2, \u03c0]))\nA \u03c0t agent will always perform Tail, so we obtain a situation where the agent in both slots 2 (any \u03c0) will be able to differentiate with which agent is interacting, so it will not be able to change its distribution of action sequences depending on the opponent\u2019s behaviour, obtaining both agents in slot 1 (\u03c0t1 and \u03c0t2) the same expected average reward. So:\nFD1(\u03c0t1, \u03c0t2,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd for slot 2: FD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD2(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD2(\u03c0t1, \u03c0t2,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate FD2(\u03c0t1, \u03c0t2,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value:\nFD2(\u03c0t1, \u03c0t2,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R2(\u00b5[l\u0307 2\u2190 \u03c0t1]), R2(\u00b5[l\u0307 2\u2190 \u03c0t2]))\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22122 (\u03a0o) to calculate \u2206Q(R2(\u00b5[l\u0307 2\u2190 \u03c0t1]), R2(\u00b5[l\u0307 2\u2190 \u03c0t2])). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c0, \u2217) from L\u0307N(\u00b5)\u22122 (\u03a0o):\n\u2206Q(R2(\u00b5[l\u0307 2\u2190 \u03c0t1]), R2(\u00b5[l\u0307 2\u2190 \u03c0t2])) = \u2206Q(R2(\u00b5[\u03c0, \u03c0t1]), R2(\u00b5[\u03c0, \u03c0t2]))\nAgain, a \u03c0t agent will always perform Tail, so we obtain a situation where the agent in both slots 1 (any \u03c0) will be able to differentiate with which agent is interacting, so it will not be able to change its distribution of action sequences depending on the opponent\u2019s behaviour, obtaining both agents in slot 2 (\u03c0t1 and \u03c0t2) the same expected average reward. So:\nFD2(\u03c0t1, \u03c0t2,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, we weight over the slots:\nFD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)FDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)\n= 1\n2 {FD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + FD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} =\n= 1\n2 {0 + 0} = 0\nSo, for every \u03a0o we obtain the same result:\n\u2200\u03a0o : FD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 0\nTherefore, matching pennies has Rightmin = 0 for this property."}, {"heading": "A.4 Strict Total Grading", "text": "We arrive to the strict total grading (STG) property. As given in section 4.4.2, we want to know if there exists a strict ordering between the evaluated agents when interacting in the environment.\nTo simplify the notation, we use the next table to represent the STO: Ri(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c02]) < Rj(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c02]), Ri(\u00b5[l\u0307 i,j\u2190 \u03c02, \u03c03]) < Rj(\u00b5[l\u0307 i,j\u2190 \u03c02, \u03c03]) and Ri(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c03]) < Rj(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c03]).\nSlot i Slot j \u03c01 < \u03c02 \u03c02 < \u03c03 \u03c01 < \u03c03\nProposition 14. Generalmin for the strict total grading (STG) property is equal to 0 for the matching pennies environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0r1, \u03c0r2, \u03c0r3} with uniform weight for w\u03a0e and \u03a0o = \u2205 (a \u03c0r agent always acts randomly).\nFollowing definition 29, we obtain the STG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28, we can calculate its STG value for each pair of slots. We start with slots 1 and 2:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for STGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,2(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)STO1,2(\u03c0r1, \u03c0r2, \u03c0r3, l\u0307, \u00b5) =\n= STO1,2(\u03c0r1, \u03c0r2, \u03c0r3, (\u2217, \u2217), \u00b5)\nThe following table shows us STO1,2 for all the permutations of \u03c0r1, \u03c0r2, \u03c0r3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0r1 < \u03c0r2 \u03c0r1 < \u03c0r3 \u03c0r2 < \u03c0r1 \u03c0r2 < \u03c0r3 \u03c0r3 < \u03c0r2 \u03c0r1 < \u03c0r3 \u03c0r1 < \u03c0r3 \u03c0r1 < \u03c0r2 \u03c0r2 < \u03c0r3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0r2 < \u03c0r3 \u03c0r3 < \u03c0r1 \u03c0r3 < \u03c0r2 \u03c0r3 < \u03c0r1 \u03c0r1 < \u03c0r2 \u03c0r2 < \u03c0r1 \u03c0r2 < \u03c0r1 \u03c0r3 < \u03c0r2 \u03c0r3 < \u03c0r1\nBut, it is not possible to find a STO, since for every permutation we have at least one random agent making its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So:\nSTG1,2(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd for slots 2 and 1:\nSTG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG2,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,1(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5)\nAgain, we only need to calculate STG2,1(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG2,1(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307)STO2,1(\u03c0r1, \u03c0r2, \u03c0r3, l\u0307, \u00b5) =\n= STO2,1(\u03c0r1, \u03c0r2, \u03c0r3, (\u2217, \u2217), \u00b5) The following table shows us STO2,1 for all the permutations of \u03c0r1, \u03c0r2, \u03c0r3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0r1 < \u03c0r2 \u03c0r1 < \u03c0r3 \u03c0r2 < \u03c0r1 \u03c0r2 < \u03c0r3 \u03c0r3 < \u03c0r2 \u03c0r1 < \u03c0r3 \u03c0r1 < \u03c0r3 \u03c0r1 < \u03c0r2 \u03c0r2 < \u03c0r3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0r2 < \u03c0r3 \u03c0r3 < \u03c0r1 \u03c0r3 < \u03c0r2 \u03c0r3 < \u03c0r1 \u03c0r1 < \u03c0r2 \u03c0r2 < \u03c0r1 \u03c0r2 < \u03c0r1 \u03c0r3 < \u03c0r2 \u03c0r3 < \u03c0r1\nAgain, it is not possible to find a STO, since for every permutation we have at least one random agent making its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So:\nSTG2,1(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, we weight over the slots:\nSTG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = 2\n1\n1\n2\n1 2 {STG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} =\n= 2\n1\n1\n2\n1 2 {0 + 0} = 0\nSince 0 is the lowest possible value for the strict total grading property, therefore matching pennies has Generalmin = 0 for this property.\nProposition 15. Generalmax for the strict total grading (STG) property is equal to 1 for the matching pennies environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0h, \u03c0h/t, \u03c0m/o} with uniform weight for w\u03a0e and \u03a0o = \u2205 (a \u03c0h agent always performs Head, a \u03c0h/t agent always performs Head when playing in slot 1 and always performs Tail when playing in slot 2, and a \u03c0m/o agent always mimics the last action of its opponent when playing in slot 1 and always performs the opposite of this action when playing in slot 2)11.\nFollowing definition 29, we obtain the STG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28, we can calculate its STG value for each pair of slots. We start with slots 1 and 2:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(\u03c0h, \u03c0h/t, \u03c0m/o,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for STGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(\u03c0h, \u03c0h/t, \u03c0m/o,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,2(\u03c0h, \u03c0h/t, \u03c0m/o,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)STO1,2(\u03c0h, \u03c0h/t, \u03c0m/o, l\u0307, \u00b5) =\n= STO1,2(\u03c0h, \u03c0h/t, \u03c0m/o, (\u2217, \u2217), \u00b5)\nThe following table shows us STO1,2 for all the permutations of \u03c0h, \u03c0h/t, \u03c0m/o.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0h < \u03c0h/t \u03c0h < \u03c0m/o \u03c0h/t < \u03c0h \u03c0h/t < \u03c0m/o \u03c0m/o < \u03c0h/t \u03c0h < \u03c0m/o \u03c0h < \u03c0m/o \u03c0h < \u03c0h/t \u03c0h/t < \u03c0m/o Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0h/t < \u03c0m/o \u03c0m/o < \u03c0h \u03c0m/o < \u03c0h/t \u03c0m/o < \u03c0h \u03c0h < \u03c0h/t \u03c0h/t < \u03c0h \u03c0h/t < \u03c0h \u03c0m/o < \u03c0h/t \u03c0m/o < \u03c0h\nIt is possible to find a STO for the first permutation. In \u03c0h < \u03c0h/t, \u03c0h will always perform Head and \u03c0h/t will always perform Tail, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0h/t < \u03c0m/o, \u03c0h/t will always perform Head and \u03c0m/o will always perform Tail, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0h < \u03c0m/o, \u03c0h will always perform Head and \u03c0m/o will always perform Tail, so they will obtain an expected average reward of \u22121 and 1 respectively. So:\nSTG1,2(\u03c0h, \u03c0h/t, \u03c0m/o,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd for slots 2 and 1:\n11\u03c0h/t and \u03c0m/o have to know in which slot they are playing. To infer this, they start with a random action at the first iteration and then look at the action of their opponent and the reward they obtain.\nSTG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG2,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,1(\u03c0h, \u03c0h/t, \u03c0m/o,\u03a0o, wL\u0307, \u00b5)\nAgain, we only need to calculate STG2,1(\u03c0h, \u03c0h/t, \u03c0m/o,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG2,1(\u03c0h, \u03c0h/t, \u03c0m/o,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307)STO2,1(\u03c0h, \u03c0h/t, \u03c0m/o, l\u0307, \u00b5) =\n= STO2,1(\u03c0h, \u03c0h/t, \u03c0m/o, (\u2217, \u2217), \u00b5)\nThe following table shows us STO2,1 for all the permutations of \u03c0h, \u03c0h/t, \u03c0m/o.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0h < \u03c0h/t \u03c0h < \u03c0m/o \u03c0h/t < \u03c0h \u03c0h/t < \u03c0m/o \u03c0m/o < \u03c0h/t \u03c0h < \u03c0m/o \u03c0h < \u03c0m/o \u03c0h < \u03c0h/t \u03c0h/t < \u03c0m/o Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0h/t < \u03c0m/o \u03c0m/o < \u03c0h \u03c0m/o < \u03c0h/t \u03c0m/o < \u03c0h \u03c0h < \u03c0h/t \u03c0h/t < \u03c0h \u03c0h/t < \u03c0h \u03c0m/o < \u03c0h/t \u03c0m/o < \u03c0h\nAgain, it is possible to find a STO for the first permutation. In \u03c0h < \u03c0h/t, \u03c0h will always perform Head and \u03c0h/t will always perform Head, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0h/t < \u03c0m/o, \u03c0h/t will always perform Tail and \u03c0m/o will always perform Tail, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0h < \u03c0m/o, \u03c0h will always perform Head and \u03c0m/o will always perform Head, so they will obtain an expected average reward of \u22121 and 1 respectively. So:\nSTG2,1(\u03c0h, \u03c0h/t, \u03c0m/o,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSTG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, we weight over the slots:\nSTG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = 2\n1\n1\n2\n1 2 {STG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} =\n= 2\n1\n1\n2\n1 2 {1 + 1} = 1\nSince 1 is the highest possible value for the strict total grading property, therefore matching pennies has Generalmax = 1 for this property.\nProposition 16. Leftmax for the strict total grading (STG) property is equal to 1 for the matching pennies environment.\nProof. To find Leftmax (equation 43), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which maximises the property as much as possible while \u03a0o minimises it. Using \u03a0e = {\u03c0h, \u03c0h/t, \u03c0m/o} with uniform weight for w\u03a0e (a \u03c0h agent always performs Head, a \u03c0h/t agent always performs Head when playing in slot 1 and always performs Tail when playing in slot 2, and a \u03c0m/o agent always mimics the last action of its opponent when playing in slot 1 and always performs the opposite of this action when playing in slot 2)12 we find this situation no matter which \u03a0o we use.\nFollowing definition 29, we obtain the STG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28, we can calculate its STG value for each pair of slots. We start with slots 1 and 2:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(\u03c0h, \u03c0h/t, \u03c0m/o,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for STGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(\u03c0h, \u03c0h/t, \u03c0m/o,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,2(\u03c0h, \u03c0h/t, \u03c0m/o,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)STO1,2(\u03c0h, \u03c0h/t, \u03c0m/o, l\u0307, \u00b5) =\n= STO1,2(\u03c0h, \u03c0h/t, \u03c0m/o, (\u2217, \u2217), \u00b5)\nNote that the choice of \u03a0o does not affect the result of STG1,2. The following table shows us STO1,2 for all the permutations of \u03c0h, \u03c0h/t, \u03c0m/o.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0h < \u03c0h/t \u03c0h < \u03c0m/o \u03c0h/t < \u03c0h \u03c0h/t < \u03c0m/o \u03c0m/o < \u03c0h/t \u03c0h < \u03c0m/o \u03c0h < \u03c0m/o \u03c0h < \u03c0h/t \u03c0h/t < \u03c0m/o Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0h/t < \u03c0m/o \u03c0m/o < \u03c0h \u03c0m/o < \u03c0h/t \u03c0m/o < \u03c0h \u03c0h < \u03c0h/t \u03c0h/t < \u03c0h \u03c0h/t < \u03c0h \u03c0m/o < \u03c0h/t \u03c0m/o < \u03c0h\nIt is possible to find a STO for the first permutation. In \u03c0h < \u03c0h/t, \u03c0h will always perform Head and \u03c0h/t will always perform Tail, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0h/t < \u03c0m/o, \u03c0h/t will always perform Head and \u03c0m/o will always perform Tail, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0h < \u03c0m/o, \u03c0h will always perform Head and \u03c0m/o will always perform Tail, so they will obtain an expected average reward of \u22121 and 1 respectively. So:\nSTG1,2(\u03c0h, \u03c0h/t, \u03c0m/o,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd for slots 2 and 1:\n12\u03c0h/t and \u03c0m/o have to know in which slot they are playing. To infer this, they start with a random action at the first iteration and then look at the action of their opponent and the reward they obtain.\nSTG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG2,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,1(\u03c0h, \u03c0h/t, \u03c0m/o,\u03a0o, wL\u0307, \u00b5)\nAgain, we only need to calculate STG2,1(\u03c0h, \u03c0h/t, \u03c0m/o,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG2,1(\u03c0h, \u03c0h/t, \u03c0m/o,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307)STO2,1(\u03c0h, \u03c0h/t, \u03c0m/o, l\u0307, \u00b5) =\n= STO2,1(\u03c0h, \u03c0h/t, \u03c0m/o, (\u2217, \u2217), \u00b5)\nNote again that the choice of \u03a0o does not affect the result of STG2,1. The following table shows us STO2,1 for all the permutations of \u03c0h, \u03c0h/t, \u03c0m/o.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0h < \u03c0h/t \u03c0h < \u03c0m/o \u03c0h/t < \u03c0h \u03c0h/t < \u03c0m/o \u03c0m/o < \u03c0h/t \u03c0h < \u03c0m/o \u03c0h < \u03c0m/o \u03c0h < \u03c0h/t \u03c0h/t < \u03c0m/o Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0h/t < \u03c0m/o \u03c0m/o < \u03c0h \u03c0m/o < \u03c0h/t \u03c0m/o < \u03c0h \u03c0h < \u03c0h/t \u03c0h/t < \u03c0h \u03c0h/t < \u03c0h \u03c0m/o < \u03c0h/t \u03c0m/o < \u03c0h\nAgain, it is possible to find a STO for the first permutation. In \u03c0h < \u03c0h/t, \u03c0h will always perform Head and \u03c0h/t will always perform Head, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0h/t < \u03c0m/o, \u03c0h/t will always perform Tail and \u03c0m/o will always perform Tail, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0h < \u03c0m/o, \u03c0h will always perform Head and \u03c0m/o will always perform Head, so they will obtain an expected average reward of \u22121 and 1 respectively. So:\nSTG2,1(\u03c0h, \u03c0h/t, \u03c0m/o,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSTG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, we weight over the slots:\nSTG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = 2\n1\n1\n2\n1 2 {STG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} =\n= 2\n1\n1\n2\n1 2 {1 + 1} = 1\nSince 1 is the highest possible value for the strict total grading property and no \u03a0o is able to influence this result, therefore matching pennies has Leftmax = 1 for this property.\nProposition 17. Rightmin for the strict total grading (STG) property is equal to 0 for the matching pennies environment.\nProof. To find Rightmin (equation 44), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which minimises the property as much as possible while \u03a0o maximises it. Using \u03a0e = {\u03c0r1, \u03c0r2, \u03c0r3} with uniform weight for w\u03a0e (a \u03c0r agent always acts randomly) we find this situation no matter which \u03a0o we use.\nFollowing definition 29, we obtain the STG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28, we can calculate its STG value for each pair of slots. We start with slots 1 and 2:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for STGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,2(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)STO1,2(\u03c0r1, \u03c0r2, \u03c0r3, l\u0307, \u00b5) =\n= STO1,2(\u03c0r1, \u03c0r2, \u03c0r3, (\u2217, \u2217), \u00b5)\nNote that the choice of \u03a0o does not affect the result of STG1,2. The following table shows us STO1,2 for all the permutations of \u03c0r1, \u03c0r2, \u03c0r3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0r1 < \u03c0r2 \u03c0r1 < \u03c0r3 \u03c0r2 < \u03c0r1 \u03c0r2 < \u03c0r3 \u03c0r3 < \u03c0r2 \u03c0r1 < \u03c0r3 \u03c0r1 < \u03c0r3 \u03c0r1 < \u03c0r2 \u03c0r2 < \u03c0r3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0r2 < \u03c0r3 \u03c0r3 < \u03c0r1 \u03c0r3 < \u03c0r2 \u03c0r3 < \u03c0r1 \u03c0r1 < \u03c0r2 \u03c0r2 < \u03c0r1 \u03c0r2 < \u03c0r1 \u03c0r3 < \u03c0r2 \u03c0r3 < \u03c0r1\nBut, it is not possible to find a STO, since for every permutation we have at least one random agent making its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So:\nSTG1,2(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd for slots 2 and 1:\nSTG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG2,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,1(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5)\nAgain, we only need to calculate STG2,1(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG2,1(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307)STO2,1(\u03c0r1, \u03c0r2, \u03c0r3, l\u0307, \u00b5) =\n= STO2,1(\u03c0r1, \u03c0r2, \u03c0r3, (\u2217, \u2217), \u00b5)\nNote again that the choice of \u03a0o does not affect the result of STG2,1. The following table shows us STO2,1 for all the permutations of \u03c0r1, \u03c0r2, \u03c0r3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0r1 < \u03c0r2 \u03c0r1 < \u03c0r3 \u03c0r2 < \u03c0r1 \u03c0r2 < \u03c0r3 \u03c0r3 < \u03c0r2 \u03c0r1 < \u03c0r3 \u03c0r1 < \u03c0r3 \u03c0r1 < \u03c0r2 \u03c0r2 < \u03c0r3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0r2 < \u03c0r3 \u03c0r3 < \u03c0r1 \u03c0r3 < \u03c0r2 \u03c0r3 < \u03c0r1 \u03c0r1 < \u03c0r2 \u03c0r2 < \u03c0r1 \u03c0r2 < \u03c0r1 \u03c0r3 < \u03c0r2 \u03c0r3 < \u03c0r1\nAgain, it is not possible to find a STO, since for every permutation we have at least one random agent making its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So:\nSTG2,1(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, we weight over the slots:\nSTG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = 2\n1\n1\n2\n1 2 {STG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} =\n= 2\n1\n1\n2\n1 2 {0 + 0} = 0\nSince 0 is the lowest possible value for the strict total grading property and no \u03a0o is able to influence this result, therefore matching pennies has Rightmin = 0 for this property."}, {"heading": "A.5 Partial Grading", "text": "Now we arrive to the partial grading (PG) property. As given in section 4.4.2, we want to know if there exists a partial ordering between the evaluated agents when interacting in the environment.\nTo simplify the notation, we use the next table to represent the PO: Ri(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c02]) \u2264 Rj(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c02]), Ri(\u00b5[l\u0307 i,j\u2190 \u03c02, \u03c03]) \u2264 Rj(\u00b5[l\u0307 i,j\u2190 \u03c02, \u03c03]) and Ri(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c03]) \u2264 Rj(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c03]).\nSlot i Slot j \u03c01 \u2264 \u03c02 \u03c02 \u2264 \u03c03 \u03c01 \u2264 \u03c03\nProposition 18. Generalmin for the partial grading (PG) property is equal to 0 for the matching pennies environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0h1, \u03c0h2, \u03c0t} with uniform weight for w\u03a0e and \u03a0o = \u2205 (a \u03c0h agent always performs Head and a \u03c0t agent always performs Tail).\nFollowing definition 30, we obtain the PG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28 (for PG), we can calculate its PG value for each pair of slots. We start with slots 1 and 2:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(\u03c0h1, \u03c0h2, \u03c0t,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for PGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(\u03c0h1, \u03c0h2, \u03c0t,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG1,2(\u03c0h1, \u03c0h2, \u03c0t,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)PO1,2(\u03c0h1, \u03c0h2, \u03c0t, l\u0307, \u00b5) =\n= PO1,2(\u03c0h1, \u03c0h2, \u03c0t, (\u2217, \u2217), \u00b5)\nThe following table shows us PO1,2 for all the permutations of \u03c0h1, \u03c0h2, \u03c0t.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0h1 \u2264 \u03c0h2 \u03c0h1 \u2264 \u03c0t \u03c0h2 \u2264 \u03c0h1 \u03c0h2 \u2264 \u03c0t \u03c0t \u2264 \u03c0h2 \u03c0h1 \u2264 \u03c0t \u03c0h1 \u2264 \u03c0t \u03c0h1 \u2264 \u03c0h2 \u03c0h2 \u2264 \u03c0t Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0h2 \u2264 \u03c0t \u03c0t \u2264 \u03c0h1 \u03c0t \u2264 \u03c0h2 \u03c0t \u2264 \u03c0h1 \u03c0h1 \u2264 \u03c0h2 \u03c0h2 \u2264 \u03c0h1 \u03c0h2 \u2264 \u03c0h1 \u03c0t \u2264 \u03c0h2 \u03c0t \u2264 \u03c0h1\nBut, it is not possible to find a PO, since for every permutation we have either \u03c0h1 \u2264 \u03c0h2 or \u03c0h2 \u2264 \u03c0h1. In both cases, a \u03c0h agent will always perform Head, so they will obtain an expected average reward of 1 and \u22121 respectively. So:\nPG1,2(\u03c0h1, \u03c0h2, \u03c0t,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd for slots 2 and 1:\nPG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG2,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,1(\u03c0h1, \u03c0h2, \u03c0t,\u03a0o, wL\u0307, \u00b5)\nAgain, we only need to calculate PG2,1(\u03c0h1, \u03c0h2, \u03c0t,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG2,1(\u03c0h1, \u03c0h2, \u03c0t,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307)PO2,1(\u03c0h1, \u03c0h2, \u03c0t, l\u0307, \u00b5) =\n= PO2,1(\u03c0h1, \u03c0h2, \u03c0t, (\u2217, \u2217), \u00b5) The following table shows us PO2,1 for all the permutations of \u03c0h1, \u03c0h2, \u03c0t.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0h1 \u2264 \u03c0h2 \u03c0h1 \u2264 \u03c0t \u03c0h2 \u2264 \u03c0h1 \u03c0h2 \u2264 \u03c0t \u03c0t \u2264 \u03c0h2 \u03c0h1 \u2264 \u03c0t \u03c0h1 \u2264 \u03c0t \u03c0h1 \u2264 \u03c0h2 \u03c0h2 \u2264 \u03c0t Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0h2 \u2264 \u03c0t \u03c0t \u2264 \u03c0h1 \u03c0t \u2264 \u03c0h2 \u03c0t \u2264 \u03c0h1 \u03c0h1 \u2264 \u03c0h2 \u03c0h2 \u2264 \u03c0h1 \u03c0h2 \u2264 \u03c0h1 \u03c0t \u2264 \u03c0h2 \u03c0t \u2264 \u03c0h1\nAgain, it is not possible to find a PO, since for every permutation we have either \u03c0h1 \u2264 \u03c0t or \u03c0t \u2264 \u03c0h1. In \u03c0h1 \u2264 \u03c0t, \u03c0h1 will always perform Head and \u03c0t will always perform Tail, so they will obtain an expected average reward of 1 and \u22121 respectively. In \u03c0t \u2264 \u03c0h1, \u03c0t will always perform Tail and \u03c0h1 will always perform Head, so they will obtain an expected average reward of 1 and \u22121 respectively. So:\nPG2,1(\u03c0h1, \u03c0h2, \u03c0t,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nPG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, we weight over the slots:\nPG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = 2\n1\n1\n2\n1 2 {PG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} =\n= 2\n1\n1\n2\n1 2 {0 + 0} = 0\nSince 0 is the lowest possible value for the partial grading property, therefore matching pennies hasGeneralmin = 0 for this property.\nProposition 19. Generalmax for the partial grading (PG) property is equal to 1 for the matching pennies environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0r1, \u03c0r2, \u03c0r3} with uniform weight for w\u03a0e and \u03a0o = \u2205 (a \u03c0r agent always acts randomly).\nFollowing definition 30, we obtain the PG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28 (for PG), we can calculate its PG value for each pair of slots. We start with slots 1 and 2:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for PGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG1,2(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)PO1,2(\u03c0h, \u03c0h/t, \u03c0m/o, l\u0307, \u00b5) =\n= PO1,2(\u03c0r1, \u03c0r2, \u03c0r3, (\u2217, \u2217), \u00b5)\nThe following table shows us PO1,2 for all the permutations of \u03c0r1, \u03c0r2, \u03c0r3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0r1 \u2264 \u03c0r2 \u03c0r1 \u2264 \u03c0r3 \u03c0r2 \u2264 \u03c0r1 \u03c0r2 \u2264 \u03c0r3 \u03c0r3 \u2264 \u03c0r2 \u03c0r1 \u2264 \u03c0r3 \u03c0r1 \u2264 \u03c0r3 \u03c0r1 \u2264 \u03c0r2 \u03c0r2 \u2264 \u03c0r3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0r2 \u2264 \u03c0r3 \u03c0r3 \u2264 \u03c0r1 \u03c0r3 \u2264 \u03c0r2 \u03c0r3 \u2264 \u03c0r1 \u03c0r1 \u2264 \u03c0r2 \u03c0r2 \u2264 \u03c0r1 \u03c0r2 \u2264 \u03c0r1 \u03c0r3 \u2264 \u03c0r2 \u03c0r3 \u2264 \u03c0r1\nIt is possible to find a PO for every permutation, since we have at least one random agent making its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So:\nPG1,2(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd for slots 2 and 1:\nPG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG2,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,1(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5)\nAgain, we only need to calculate PG2,1(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG2,1(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307)PO2,1(\u03c0r1, \u03c0r2, \u03c0r3, l\u0307, \u00b5) =\n= PO2,1(\u03c0r1, \u03c0r2, \u03c0r3, (\u2217, \u2217), \u00b5)\nThe following table shows us PO2,1 for all the permutations of \u03c0r1, \u03c0r2, \u03c0r3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0r1 \u2264 \u03c0r2 \u03c0r1 \u2264 \u03c0r3 \u03c0r2 \u2264 \u03c0r1 \u03c0r2 \u2264 \u03c0r3 \u03c0r3 \u2264 \u03c0r2 \u03c0r1 \u2264 \u03c0r3 \u03c0r1 \u2264 \u03c0r3 \u03c0r1 \u2264 \u03c0r2 \u03c0r2 \u2264 \u03c0r3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0r2 \u2264 \u03c0r3 \u03c0r3 \u2264 \u03c0r1 \u03c0r3 \u2264 \u03c0r2 \u03c0r3 \u2264 \u03c0r1 \u03c0r1 \u2264 \u03c0r2 \u03c0r2 \u2264 \u03c0r1 \u03c0r2 \u2264 \u03c0r1 \u03c0r3 \u2264 \u03c0r2 \u03c0r3 \u2264 \u03c0r1\nAgain, it is possible to find a PO for every permutation, since we have at least one random agent making its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So:\nPG2,1(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, we weight over the slots:\nPG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = 2\n1\n1\n2\n1 2 {PG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} =\n= 2\n1\n1\n2\n1 2 {1 + 1} = 1\nSince 1 is the highest possible value for the partial grading property, therefore matching pennies has Generalmax = 1 for this property.\nProposition 20. Leftmax for the partial grading (PG) property is equal to 1 for the matching pennies environment.\nProof. To find Leftmax (equation 43), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which maximises the property as much as possible while \u03a0o minimises it. Using \u03a0e = {\u03c0r1, \u03c0r2, \u03c0r3} with uniform weight for w\u03a0e (a \u03c0r agent always acts randomly) we find this situation no matter which \u03a0o we use.\nFollowing definition 30, we obtain the PG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28 (for PG), we can calculate its PG value for each pair of slots. We start with slots 1 and 2:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for PGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG1,2(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)PO1,2(\u03c0h, \u03c0h/t, \u03c0m/o, l\u0307, \u00b5) =\n= PO1,2(\u03c0r1, \u03c0r2, \u03c0r3, (\u2217, \u2217), \u00b5)\nNote that the choice of \u03a0o does not affect the result of PG1,2. The following table shows us PO1,2 for all the permutations of \u03c0r1, \u03c0r2, \u03c0r3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0r1 \u2264 \u03c0r2 \u03c0r1 \u2264 \u03c0r3 \u03c0r2 \u2264 \u03c0r1 \u03c0r2 \u2264 \u03c0r3 \u03c0r3 \u2264 \u03c0r2 \u03c0r1 \u2264 \u03c0r3 \u03c0r1 \u2264 \u03c0r3 \u03c0r1 \u2264 \u03c0r2 \u03c0r2 \u2264 \u03c0r3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0r2 \u2264 \u03c0r3 \u03c0r3 \u2264 \u03c0r1 \u03c0r3 \u2264 \u03c0r2 \u03c0r3 \u2264 \u03c0r1 \u03c0r1 \u2264 \u03c0r2 \u03c0r2 \u2264 \u03c0r1 \u03c0r2 \u2264 \u03c0r1 \u03c0r3 \u2264 \u03c0r2 \u03c0r3 \u2264 \u03c0r1\nIt is possible to find a PO for every permutation, since we have at least one random agent making its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So:\nPG1,2(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd for slots 2 and 1:\nPG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG2,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,1(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5)\nAgain, we only need to calculate PG2,1(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG2,1(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307)PO2,1(\u03c0r1, \u03c0r2, \u03c0r3, l\u0307, \u00b5) =\n= PO2,1(\u03c0r1, \u03c0r2, \u03c0r3, (\u2217, \u2217), \u00b5)\nNote again that the choice of \u03a0o does not affect the result of PG2,1. The following table shows us PO2,1 for all the permutations of \u03c0r1, \u03c0r2, \u03c0r3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0r1 \u2264 \u03c0r2 \u03c0r1 \u2264 \u03c0r3 \u03c0r2 \u2264 \u03c0r1 \u03c0r2 \u2264 \u03c0r3 \u03c0r3 \u2264 \u03c0r2 \u03c0r1 \u2264 \u03c0r3 \u03c0r1 \u2264 \u03c0r3 \u03c0r1 \u2264 \u03c0r2 \u03c0r2 \u2264 \u03c0r3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0r2 \u2264 \u03c0r3 \u03c0r3 \u2264 \u03c0r1 \u03c0r3 \u2264 \u03c0r2 \u03c0r3 \u2264 \u03c0r1 \u03c0r1 \u2264 \u03c0r2 \u03c0r2 \u2264 \u03c0r1 \u03c0r2 \u2264 \u03c0r1 \u03c0r3 \u2264 \u03c0r2 \u03c0r3 \u2264 \u03c0r1\nAgain, it is possible to find a PO for every permutation, since we have at least one random agent making its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So:\nPG2,1(\u03c0r1, \u03c0r2, \u03c0r3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, we weight over the slots:\nPG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = 2\n1\n1\n2\n1 2 {PG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} =\n= 2\n1\n1\n2\n1 2 {1 + 1} = 1\nSince 1 is the highest possible value for the partial grading property and no \u03a0o is able to influence this result, therefore matching pennies has Leftmax = 1 for this property.\nProposition 21. Rightmin for the partial grading (PG) property is equal to 0 for the matching pennies environment.\nProof. To find Rightmin (equation 44), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which minimises the property as much as possible while \u03a0o maximises it. Using \u03a0e = {\u03c0h1, \u03c0h2, \u03c0t} with uniform weight for w\u03a0e (a \u03c0h agent always perform Head and a \u03c0t agent always perform Tail) we find this situation no matter which \u03a0o we use.\nFollowing definition 30, we obtain the PG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28 (for PG), we can calculate its PG value for each pair of slots. We start with slots 1 and 2:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(\u03c0h1, \u03c0h2, \u03c0t,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for PGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(\u03c0h1, \u03c0h2, \u03c0t,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG1,2(\u03c0h1, \u03c0h2, \u03c0t,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)PO1,2(\u03c0h1, \u03c0h2, \u03c0t, l\u0307, \u00b5) =\n= PO1,2(\u03c0h1, \u03c0h2, \u03c0t, (\u2217, \u2217), \u00b5) Note that the choice of \u03a0o does not affect the result of PG1,2.\nThe following table shows us PO1,2 for all the permutations of \u03c0h1, \u03c0h2, \u03c0t.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0h1 \u2264 \u03c0h2 \u03c0h1 \u2264 \u03c0t \u03c0h2 \u2264 \u03c0h1 \u03c0h2 \u2264 \u03c0t \u03c0t \u2264 \u03c0h2 \u03c0h1 \u2264 \u03c0t \u03c0h1 \u2264 \u03c0t \u03c0h1 \u2264 \u03c0h2 \u03c0h2 \u2264 \u03c0t Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0h2 \u2264 \u03c0t \u03c0t \u2264 \u03c0h1 \u03c0t \u2264 \u03c0h2 \u03c0t \u2264 \u03c0h1 \u03c0h1 \u2264 \u03c0h2 \u03c0h2 \u2264 \u03c0h1 \u03c0h2 \u2264 \u03c0h1 \u03c0t \u2264 \u03c0h2 \u03c0t \u2264 \u03c0h1\nBut, it is not possible to find a PO, since for every permutation we have either \u03c0h1 \u2264 \u03c0h2 or \u03c0h2 \u2264 \u03c0h1. In both cases, \u03c0h will always perform Head, so they will obtain an expected average reward of 1 and \u22121 respectively. So:\nPG1,2(\u03c0h1, \u03c0h2, \u03c0t,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd for slots 2 and 1:\nPG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG2,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,1(\u03c0h1, \u03c0h2, \u03c0t,\u03a0o, wL\u0307, \u00b5)\nAgain, we only need to calculate PG2,1(\u03c0h1, \u03c0h2, \u03c0t,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG2,1(\u03c0h1, \u03c0h2, \u03c0t,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307)PO2,1(\u03c0h1, \u03c0h2, \u03c0t, l\u0307, \u00b5) =\n= PO2,1(\u03c0h1, \u03c0h2, \u03c0t, (\u2217, \u2217), \u00b5) Note again that the choice of \u03a0o does not affect the result of PG2,1.\nThe following table shows us PO2,1 for all the permutations of \u03c0h1, \u03c0h2, \u03c0t.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0h1 \u2264 \u03c0h2 \u03c0h1 \u2264 \u03c0t \u03c0h2 \u2264 \u03c0h1 \u03c0h2 \u2264 \u03c0t \u03c0t \u2264 \u03c0h2 \u03c0h1 \u2264 \u03c0t \u03c0h1 \u2264 \u03c0t \u03c0h1 \u2264 \u03c0h2 \u03c0h2 \u2264 \u03c0t Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0h2 \u2264 \u03c0t \u03c0t \u2264 \u03c0h1 \u03c0t \u2264 \u03c0h2 \u03c0t \u2264 \u03c0h1 \u03c0h1 \u2264 \u03c0h2 \u03c0h2 \u2264 \u03c0h1 \u03c0h2 \u2264 \u03c0h1 \u03c0t \u2264 \u03c0h2 \u03c0t \u2264 \u03c0h1\nAgain, it is not possible to find a PO, since for every permutation we have either \u03c0h1 \u2264 \u03c0t or \u03c0t \u2264 \u03c0h1. In \u03c0h1 \u2264 \u03c0t, \u03c0h1 will always perform Head and \u03c0t will always perform Tail, so they will obtain an expected average reward of 1 and \u22121 respectively. In \u03c0t \u2264 \u03c0h1, \u03c0t will always perform Tail and \u03c0h1 will always perform Head, so they will obtain an expected average reward of 1 and \u22121 respectively. So:\nPG2,1(\u03c0h1, \u03c0h2, \u03c0t,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nPG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, we weight over the slots:\nPG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = 2\n1\n1\n2\n1 2 {PG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} =\n= 2\n1\n1\n2\n1 2 {0 + 0} = 0\nSince 0 is the lowest possible value for the partial grading property and no \u03a0o is able to influence this result, therefore matching pennies has Rightmin = 0 for this property."}, {"heading": "A.6 Slot Reward Dependency", "text": "Next we see the slot reward dependency (SRD) property. As given in section 4.3.2, we want to know how much competitive or cooperative the environment is.\nProposition 22. General range for the slot reward dependency (SRD) property is equal to [\u22121,\u22121] for the matching pennies environment.\nProof. Following definition 20, we obtain the SRD value for any \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0e, w\u03a0e and \u03a0o are instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 19, we can calculate its SRD value for each pair of slots. We start with slots 1 and 2:\nSRD1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)SRD1,2(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate SRD1,2(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 18 to calculate this value for a figurative evaluated agent \u03c01 from \u03a0e:\nSRD1,2(\u03c01,\u03a0o, wL\u0307, \u00b5) = corrl\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o) [wL\u0307(l\u0307)](R1(\u00b5[l\u0307\n1\u2190 \u03c01]), R2(\u00b5[l\u0307 1\u2190 \u03c01]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22121 (\u03a0o) to calculate corr(R1(\u00b5[l\u0307 1\u2190 \u03c01]), R2(\u00b5[l\u0307 1\u2190 \u03c01])). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c02) from L\u0307N(\u00b5)\u22121 (\u03a0o):\ncorr(R1(\u00b5[l\u0307 1\u2190 \u03c01]), R2(\u00b5[l\u0307 1\u2190 \u03c01])) = corr(R1(\u00b5[\u03c01, \u03c02]), R2(\u00b5[\u03c01, \u03c02]))\nThis game is a zero-sum game with two agents. That means that, in every game, the sum of both agents\u2019 rewards will always be zero, or in other words, when the agent in slot 1 (any \u03c01) obtains a reward r the agent in slot 2 (any \u03c02) obtains \u2212r as reward, and this relation is propagated to expected average rewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 1 and 2 always obtain opposite expected average reward, then the correlation function will always obtain the same value13 of \u22121. So:\nSRD1,2(\u03c01,\u03a0o, wL\u0307, \u00b5) = \u22121\nTherefore:\nSRD1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u22121\nAnd for slots 2 and 1: SRD2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)SRD2,1(\u03c0,\u03a0o, wL\u0307, \u00b5)\nAgain, we do not know which \u03a0e we have, but we know that we will need to evaluate SRD2,1(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 18 to calculate this value for a figurative evaluated agent \u03c01 from \u03a0e:\nSRD2,1(\u03c01,\u03a0o, wL\u0307, \u00b5) = corrl\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o) [wL\u0307(l\u0307)](R2(\u00b5[l\u0307\n2\u2190 \u03c01]), R1(\u00b5[l\u0307 2\u2190 \u03c01]))\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307\nfrom L\u0307 N(\u00b5) \u22122 (\u03a0o) to calculate corr(R2(\u00b5[l\u0307 2\u2190 \u03c01]), R1(\u00b5[l\u0307 2\u2190 \u03c01])). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c02, \u2217) from L\u0307N(\u00b5)\u22122 (\u03a0o):\ncorr(R2(\u00b5[l\u0307 2\u2190 \u03c01]), R1(\u00b5[l\u0307 2\u2190 \u03c01])) = corr(R2(\u00b5[\u03c02, \u03c01]), R1(\u00b5[\u03c02, \u03c01])) 13Provided there is at least one game which is not a tie.\nAgain, this game is a zero-sum game with two agents. That means that, in every game, the sum of both agents\u2019 rewards will always be zero, or in other words, when the agent in slot 2 (any \u03c01) obtains a reward r the agent in slot 1 (any \u03c02) obtains \u2212r as reward, and this relation is propagated to expected average rewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 2 and 1 always obtain opposite expected average reward, then the correlation function will always obtain the same value of \u22121. So:\nSRD2,1(\u03c01,\u03a0o, wL\u0307, \u00b5) = \u22121\nTherefore:\nSRD2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u22121\nAnd finally, we weight over the slots:\nSRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)SRDi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)SRDi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = 2\n1\n1\n2\n1 2 {SRD1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + SRD2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} =\n= 2\n1\n1\n2\n1 2 {\u22121 + (\u22121)} = \u22121\nSo, for every trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 we obtain the same result:\n\u2200\u03a0e, w\u03a0e ,\u03a0o : SRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u22121\nTherefore, matching pennies has General = [\u22121,\u22121] for this property."}, {"heading": "A.7 Competitive Anticipation", "text": "Finally, we follow with the competitive anticipation (AComp) property. As given in section 4.5.1, we want to know how much benefit the evaluated agents obtain when they anticipate competing agents.\nProposition 23. Generalmin for the competitive anticipation (AComp) property is equal to \u2212 12 for the matching pennies environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0t/h} with w\u03a0e(\u03c0t/h) = 1 and \u03a0o = {\u03c0h} (a \u03c0h agent always performs Head and a \u03c0t/h agent always performs Tail when playing in slot 1 and always performs Head when playing in slot 2)14.\nFollowing definition 33, we obtain the AComp value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every pair of slots in different teams. Following definition 32, we could calculate its AComp value for each pair of slots but, since \u03a0e has only one agent, its weight is equal to 1 and \u03a0o also has only one agent, it is equivalent to use directly definition 31. We start with slots 1 and 2:\nAComp1,2(\u03c0t/h, \u03c0h,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( R1(\u00b5[l\u0307 1,2\u2190 \u03c0t/h, \u03c0h])\u2212R1(\u00b5[l\u0307 1,2\u2190 \u03c0t/h, \u03c0r]) ) =\n= 1\n2\n( R1(\u00b5[\u03c0t/h, \u03c0h])\u2212R1(\u00b5[\u03c0t/h, \u03c0r]) ) 14\u03c0t/h has to know in which slot it is playing. To infer this, it starts with a random action at the first iteration and then look\nat the action of its opponent and the reward it obtains.\nWe know from lemma 2 that R1(\u00b5[\u03c0t/h, \u03c0r]) = 0, so we only need to calculate R1(\u00b5[\u03c0t/h, \u03c0h]), where \u03c0t/h will always perform Tail and \u03c0h will always perform Head, so the agent in slot 1 (\u03c0t/h) will obtain an expected average reward of \u22121. So:\nAComp1,2(\u03c0t/h, \u03c0h,\u03a0o, wL\u0307, \u00b5) = 1 2 ((\u22121)\u2212 0) = \u22121\n2 Note that this is the minimum possible value for AComp1,2(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) since R1(\u00b5[\u03c01, \u03c0r]) will always be equal to 0 for this environment.\nAnd for slots 2 and 1:\nAComp2,1(\u03c0t/h, \u03c0h,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( R2(\u00b5[l\u0307 2,1\u2190 \u03c0t/h, \u03c0h])\u2212R2(\u00b5[l\u0307 2,1\u2190 \u03c0t/h, \u03c0r]) ) =\n= 1\n2\n( R2(\u00b5[\u03c0h, \u03c0t/h])\u2212R2(\u00b5[\u03c0r, \u03c0t/h]) ) Again, we know from lemma 2 that R2(\u00b5[\u03c0r, \u03c0t/h]) = 0, so we only need to calculate R2(\u00b5[\u03c0h, \u03c0t/h]), where \u03c0h and \u03c0t/h will always perform Head, so the agent in slot 2 (\u03c0t/h) will obtain an expected average reward of \u22121. So:\nAComp2,1(\u03c0t/h, \u03c0h,\u03a0o, wL\u0307, \u00b5) = 1 2 ((\u22121)\u2212 0) = \u22121\n2 Note again that this is the minimum possible value for AComp2,1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) since R2(\u00b5[\u03c0r, \u03c01]) will always be equal to 0 for this environment.\nAnd finally, we weight over the slots:\nAComp(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S22 \u2211 t1,t2\u2208\u03c4 |t1 \u0338=t2 \u2211 i\u2208t1 wS(i, \u00b5) \u2211 j\u2208t2 wS(j, \u00b5)ACompi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 2\n1\n1\n2\n1 2 {AComp1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +AComp2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} =\n= 2\n1\n1\n2\n1 2 {AComp1,2(\u03c0t/h, \u03c0h,\u03a0o, wL\u0307, \u00b5) +AComp2,1(\u03c0t/h, \u03c0h,\u03a0o, wL\u0307, \u00b5)} =\n= 2\n1\n1\n2\n1\n2 { \u22121 2 + ( \u22121 2 )} = \u22121 2\nSince \u2212 12 is the lowest possible value that we can obtain for the competitive anticipation property, therefore matching pennies has Generalmin = \u2212 12 for this property.\nProposition 24. Generalmax for the competitive anticipation (AComp) property is equal to 1 2 for the matching pennies environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0h/t} with w\u03a0e(\u03c0h/t) = 1 and \u03a0o = {\u03c0h} (a \u03c0h agent always performs Head and a \u03c0h/t agent always performs Head when playing in slot 1 and always performs Tail when playing in slot 2)15.\nFollowing definition 33, we obtain the AComp value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every pair of slots in different teams. Following definition 32, we could calculate its AComp value for each pair of slots but, since \u03a0e has only one agent, its weight is equal to 1 and \u03a0o also has only one agent, it is equivalent to use directly definition 31. We start with slots 1 and 2:\nAComp1,2(\u03c0h/t, \u03c0h,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( R1(\u00b5[l\u0307 1,2\u2190 \u03c0h/t, \u03c0h])\u2212R1(\u00b5[l\u0307 1,2\u2190 \u03c0h/t, \u03c0r]) ) =\n= 1\n2\n( R1(\u00b5[\u03c0h/t, \u03c0h])\u2212R1(\u00b5[\u03c0h/t, \u03c0r]) ) 15\u03c0h/t has to know in which slot it is playing. To infer this, it starts with a random action at the first iteration and then look\nat the action of its opponent and the reward it obtains.\nWe know from lemma 2 that R1(\u00b5[\u03c0h/t, \u03c0r]) = 0, so we only need to calculate R1(\u00b5[\u03c0h/t, \u03c0h]), where \u03c0h/t and \u03c0h will always perform Head, so the agent in slot 1 (\u03c0h/t) will obtain an expected average reward of 1. So:\nAComp1,2(\u03c0h/t, \u03c0h,\u03a0o, wL\u0307, \u00b5) = 1 2 (1\u2212 0) = 1 2\nNote that this is the maximum possible value for AComp1,2(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) since R1(\u00b5[\u03c01, \u03c0r]) will always be equal to 0 for this environment.\nAnd for slots 2 and 1:\nAComp2,1(\u03c0h/t, \u03c0h,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( R2(\u00b5[l\u0307 2,1\u2190 \u03c0h/t, \u03c0h])\u2212R2(\u00b5[l\u0307 2,1\u2190 \u03c0h/t, \u03c0r]) ) =\n= 1\n2\n( R2(\u00b5[\u03c0h, \u03c0h/t])\u2212R2(\u00b5[\u03c0r, \u03c0h/t]) ) Again, we know from lemma 2 that R2(\u00b5[\u03c0r, \u03c0h/t]) = 0, so we only need to calculate R2(\u00b5[\u03c0h, \u03c0h/t]), where \u03c0h will always perform Head and \u03c0h/t will always perform Tail, so the agent in slot 2 (\u03c0h/t) will obtain an expected average reward of 1. So:\nAComp2,1(\u03c0h/t, \u03c0h,\u03a0o, wL\u0307, \u00b5) = 1 2 (1\u2212 0) = 1 2\nNote again that this is the maximum possible value for AComp2,1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) since R2(\u00b5[\u03c0r, \u03c01]) will always be equal to 0 for this environment.\nAnd finally, we weight over the slots:\nAComp(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S22 \u2211 t1,t2\u2208\u03c4 |t1 \u0338=t2 \u2211 i\u2208t1 wS(i, \u00b5) \u2211 j\u2208t2 wS(j, \u00b5)ACompi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 2\n1\n1\n2\n1 2 {AComp1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +AComp2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} =\n= 2\n1\n1\n2\n1 2 {AComp1,2(\u03c0h/t, \u03c0h,\u03a0o, wL\u0307, \u00b5) +AComp2,1(\u03c0h/t, \u03c0h,\u03a0o, wL\u0307, \u00b5)} =\n= 2\n1\n1\n2\n1\n2\n{ 1\n2 +\n1\n2\n} = 1\n2\nSince 12 is the highest possible value that we can obtain for the competitive anticipation property, therefore matching pennies has Generalmax = 1 2 for this property."}, {"heading": "B Prisoner\u2019s Dilemma properties", "text": "In this section we prove how we obtained the values for the properties for the prisoner\u2019s dilemma environment (section 5.3)."}, {"heading": "B.1 Action Dependency", "text": "We start with the action dependency (AD) property. As given in section 4.2.1, we want to know if the evaluated agents behave differently depending on which line-up they interact with. We use \u2206S(a, b) = 1 if distributions a and b are equal and 0 otherwise.\nProposition 25. Generalmin for the action dependency (AD) property is equal to 0 for the prisoner\u2019s dilemma environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0b} with w\u03a0e(\u03c0b) = 1 and \u03a0o = {\u03c0c1, \u03c0c2} (a \u03c0c agent always performs Cooperate and a \u03c0b agent always performs Blame).\nFollowing definition 14, we obtain the AD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 13, we could calculate its AD value for slot 1 but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 12 for slot 1:\nAD1(\u03c0b,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03061(\u00b5[u\u0307 1\u2190 \u03c0b]), A\u03061(\u00b5[v\u0307 1\u2190 \u03c0b])) =\n= 2 2\n1\n1\n2\n1 2 \u2206S(A\u03061(\u00b5[\u03c0b, \u03c0c1]), A\u03061(\u00b5[\u03c0b, \u03c0c2]))\nNote that we avoided to calculate both \u2206S(a, b) and \u2206S(b, a) since they provide the same result, by calculating only \u2206S(a, b) and multiplying the result by 2.\nIn this case, we only need to calculate \u2206S(A\u03061(\u00b5[\u03c0b, \u03c0c1 ]), A\u03061(\u00b5[\u03c0b, \u03c0c2 ])), where the agent in both slots 1 (\u03c0b) will perform the same sequence of actions (always Blame) independently of the line-up. So:\nAD1(\u03c0b,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, generalising for all slots:\nAD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= AD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = = AD1(\u03c0b,\u03a0o, wL\u0307, \u00b5) = = 0\nSince 0 is the lowest possible value for the action dependency property, therefore prisoner\u2019s dilemma has Generalmin = 0 for this property.\nProposition 26. Generalmax for the action dependency (AD) property is equal to 1 for the prisoner\u2019s dilemma environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0m} with w\u03a0e(\u03c0m) = 1 and \u03a0o = {\u03c0c, \u03c0b} (a \u03c0m agent first acts randomly and then always mimics the other agent\u2019s last action, a \u03c0c agent always performs Cooperate and a \u03c0b agent always performs Blame).\nFollowing definition 14, we obtain the AD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 13, we could calculate its AD value for slot 1 but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 12 for slot 1:\nAD1(\u03c0m,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03061(\u00b5[u\u0307 1\u2190 \u03c0m]), A\u03061(\u00b5[v\u0307 1\u2190 \u03c0m])) =\n= 2 2\n1\n1\n2\n1 2 \u2206S(A\u03061(\u00b5[\u03c0m, \u03c0c]), A\u03061(\u00b5[\u03c0m, \u03c0b]))\nNote that we avoided to calculate both \u2206S(a, b) and \u2206S(b, a) since they provide the same result, by calculating only \u2206S(a, b) and multiplying the result by 2.\nFrom iteration 2, \u03c0m will mimic the last action of the agent in slot 2, and since \u03c0c will always perform Cooperate and \u03c0b will always perform Blame, the agent in both slots 1 (\u03c0m) will perform different sequences of actions on each line-up. So:\nAD1(\u03c0m,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 1 = 1\nAnd finally, generalising for all slots:\nAD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= AD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = = AD1(\u03c0m,\u03a0o, wL\u0307, \u00b5) = = 1\nSince 1 is the highest possible value for the action dependency property, therefore prisoner\u2019s dilemma has Generalmax = 1 for this property.\nProposition 27. Leftmax for the action dependency (AD) property is equal to 0 for the prisoner\u2019s dilemma environment.\nProof. To find Leftmax (equation 43), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which maximises the property as much as possible while \u03a0o minimises it. Using \u03a0o = {\u03c0c1, \u03c0c2} (a \u03c0c agent always performs Cooperate) we find this situation no matter which pair \u27e8\u03a0e, w\u03a0e\u27e9 we use.\nFollowing definition 14, we obtain the AD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0e and w\u03a0e are instantiated with any permitted values). Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 13, we can calculate its AD value for slot 1:\nAD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)AD1(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate AD1(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 12 to calculate this value for a figurative evaluated agent \u03c0 from \u03a0e:\nAD1(\u03c0,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03061(\u00b5[u\u0307 1\u2190 \u03c0]), A\u03061(\u00b5[v\u0307 1\u2190 \u03c0])) =\n= 2 2\n1\n1\n2\n1 2 \u2206S(A\u03061(\u00b5[\u03c0, \u03c0c1]), A\u03061(\u00b5[\u03c0, \u03c0c2]))\nNote that we avoided to calculate both \u2206S(a, b) and \u2206S(b, a) since they provide the same result, by calculating only \u2206S(a, b) and multiplying the result by 2.\nA \u03c0c agent will always perform Cooperate, so we obtain a situation where the agent in both slots 1 (any \u03c0) will be able to differentiate with which agent is interacting, so it will not be able to change its distribution of action sequences depending on the opponent\u2019s behaviour. So:\nAD1(\u03c0,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nTherefore, no matter which agents are in \u03a0e and their weights w\u03a0e we obtain:\nAD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nAnd finally, generalising for all slots:\nAD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= AD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = = 0\nSo, for every pair \u27e8\u03a0e, w\u03a0e\u27e9 we obtain the same result:\n\u2200\u03a0e, w\u03a0e : AD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 0\nTherefore, prisoner\u2019s dilemma has Leftmax = 0 for this property.\nProposition 28. Rightmin for the action dependency (AD) property is equal to 0 for the prisoner\u2019s dilemma environment.\nProof. To find Rightmin (equation 44), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which minimises the property as much as possible while \u03a0o maximises it. Using \u03a0e = {\u03c0c} with w\u03a0e(\u03c0c) = 1 (a \u03c0c agent always performs Cooperate) we find this situation no matter which \u03a0o we use.\nFollowing definition 14, we obtain the AD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted value). Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 13, we could calculate its AD value for slot 1 but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 12 for slot 1:\nAD1(\u03c0c,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03061(\u00b5[u\u0307 1\u2190 \u03c0c]), A\u03061(\u00b5[v\u0307 1\u2190 \u03c0c]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain two different line-up patterns u\u0307\nand v\u0307 from L\u0307 N(\u00b5) \u22121 (\u03a0o) to calculate \u2206S(A\u03061(\u00b5[u\u0307 1\u2190 \u03c0c]), A\u03061(\u00b5[v\u0307 1\u2190 \u03c0c])). We calculate this value for two figurative line-up patterns u\u0307 = (\u2217, \u03c01) and v\u0307 = (\u2217, \u03c02) from L\u0307N(\u00b5)\u22121 (\u03a0o):\n\u2206S(A\u03061(\u00b5[u\u0307 1\u2190 \u03c0c]), A\u03061(\u00b5[v\u0307 1\u2190 \u03c0c])) = \u2206S(A\u03061(\u00b5[\u03c0c, \u03c01]), A\u03061(\u00b5[\u03c0c, \u03c02]))\nHere, the agent in both slots 1 (\u03c0c) will perform the same sequence of actions (always Cooperate) independently of the line-up. So no matter which agents are in \u03a0o we obtain:\nAD1(\u03c0c,\u03a0o, wL\u0307, \u00b5) = 0\nAnd finally, generalising for all slots:\nAD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= AD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = = AD1(\u03c0c,\u03a0o, wL\u0307, \u00b5) = = 0\nSo, for every \u03a0o we obtain the same result:\n\u2200\u03a0o : AD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 0\nTherefore, prisoner\u2019s dilemma has Rightmin = 0 for this property."}, {"heading": "B.2 Reward Dependency", "text": "We continue with the reward dependency (RD) property. As given in section 4.3.1, we want to know if the evaluated agents obtain different expected average rewards depending on which line-up they interact with. We use \u2206Q(a, b) = 1 if numbers a and b are equal and 0 otherwise.\nProposition 29. Generalmin for the reward dependency (RD) property is equal to 0 for the prisoner\u2019s dilemma environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0b} with w\u03a0e(\u03c0b) = 1 and \u03a0o = {\u03c0c1, \u03c0c2} (a \u03c0c agent always performs Cooperate and a \u03c0b agent always performs Blame).\nFollowing definition 17, we obtain the RD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 16, we could calculate its RD value for slot 1 but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 15 for slot 1:\nRD1(\u03c0b,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0307 \u0338=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R1(\u00b5[u\u0307 1\u2190 \u03c0b]), R1(\u00b5[v\u0307 1\u2190 \u03c0b])) =\n= 2 2\n1\n1\n2\n1 2 \u2206Q(R1(\u00b5[\u03c0b, \u03c0c1]), R1(\u00b5[\u03c0b, \u03c0c2]))\nNote that we avoided to calculate both \u2206Q(a, b) and \u2206Q(b, a) since they provide the same result, by calculating only \u2206Q(a, b) and multiplying the result by 2.\nIn this case, we only need to calculate \u2206Q(R1(\u00b5[\u03c0b, \u03c0c1]), R1(\u00b5[\u03c0b, \u03c0c2])), where \u03c0b will always perform Blame and \u03c0c will always perform Cooperate, so the agent in both slots 1 (\u03c0b) will obtain the same expected average reward (1) independently of the line-up. So:\nRD1(\u03c0b,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, generalising for all slots:\nRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= RD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = = RD1(\u03c0b,\u03a0o, wL\u0307, \u00b5) = = 0\nSince 0 is the lowest possible value for the reward dependency property, therefore prisoner\u2019s dilemma has Generalmin = 0 for this property.\nProposition 30. Generalmax for the reward dependency (RD) property is equal to 1 for the prisoner\u2019s dilemma environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0b} with w\u03a0e(\u03c0b) = 1 and \u03a0o = {\u03c0c, \u03c0b} (a \u03c0c agent always performs Cooperate and a \u03c0b agent always performs Blame).\nFollowing definition 17, we obtain the RD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 16, we could calculate its RD value for slot 1 but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 15 for slot 1:\nRD1(\u03c0b,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R1(\u00b5[u\u0307 1\u2190 \u03c0b]), R1(\u00b5[v\u0307 1\u2190 \u03c0b])) =\n= 2 2\n1\n1\n2\n1 2 \u2206Q(R1(\u00b5[\u03c0b, \u03c0c]), R1(\u00b5[\u03c0b, \u03c0b]))\nNote that we avoided to calculate both \u2206Q(a, b) and \u2206Q(b, a) since they provide the same result, by calculating only \u2206Q(a, b) and multiplying the result by 2.\nIn line-up (\u03c0b, \u03c0c), where \u03c0b will always perform Blame and \u03c0c will always perform Cooperate, the agent in slot 1 (\u03c0b) will obtain one expected average reward (1), while in line-up (\u03c0b, \u03c0b), where both \u03c0b will always perform Blame, the agent in slot 1 (\u03c0b) will obtain a different expected average reward (\u2212 13 ). So:\nRD1(\u03c0b,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 1 = 1\nAnd finally, generalising for all slots:\nRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= RD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = = RD1(\u03c0b,\u03a0o, wL\u0307, \u00b5) = = 1\nSince 1 is the highest possible value for the reward dependency property, therefore prisoner\u2019s dilemma has Generalmax = 1 for this property.\nProposition 31. Leftmax for the reward dependency (RD) property is equal to 0 for the prisoner\u2019s dilemma environment.\nProof. To find Leftmax (equation 43), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which maximises the property as much as possible while \u03a0o minimises it. Using \u03a0o = {\u03c0c1, \u03c0c2} (a \u03c0c agent always performs Cooperate) we find this situation no matter which pair \u27e8\u03a0e, w\u03a0e\u27e9 we use.\nFollowing definition 17, we obtain the RD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0e and w\u03a0e are instantiated with any permitted values). Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 16, we can calculate its RD value for slot 1:\nRD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)RD1(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate RD1(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 15 to calculate this value for a figurative evaluated agent \u03c0 from \u03a0e:\nRD1(\u03c0,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R1(\u00b5[u\u0307 1\u2190 \u03c0]), R1(\u00b5[v\u0307 1\u2190 \u03c0])) =\n= 2 2\n1\n1\n2\n1 2 \u2206Q(R1(\u00b5[\u03c0, \u03c0c1]), R1(\u00b5[\u03c0, \u03c0c2]))\nNote that we avoided to calculate both \u2206Q(a, b) and \u2206Q(b, a) since they provide the same result, by calculating only \u2206Q(a, b) and multiplying the result by 2.\nA \u03c0c agent will always perform Cooperate, so we obtain a situation where the agent in both slots 1 (any \u03c0) will be able to differentiate with which agent is interacting, so it will not be able to change its distribution of action sequences depending on the opponent\u2019s behaviour, obtaining agent in both slots 1 (any \u03c0) the same expected average reward. So:\nRD1(\u03c0,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nTherefore, no matter which agents are in \u03a0e and their weights w\u03a0e we obtain:\nRD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nAnd finally, generalising for all slots:\nRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= RD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = = 0\nSo, for every pair \u27e8\u03a0e, w\u03a0e\u27e9 we obtain the same result:\n\u2200\u03a0e, w\u03a0e : RD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 0\nTherefore, prisoner\u2019s dilemma has Leftmax = 0 for this property.\nProposition 32. Rightmin for the reward dependency (RD) property is equal to 1 for the prisoner\u2019s dilemma environment.\nProof. To find Rightmin (equation 44), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which minimises the property as much as possible while \u03a0o maximises it. Using \u03a0o = {\u03c0c, \u03c0b} (a \u03c0c agent always performs Cooperate and a \u03c0b agent always performs Blame) we find this situation no matter which pair \u27e8\u03a0e, w\u03a0e\u27e9 we use.\nFollowing definition 17, we obtain the RD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0e and w\u03a0e are instantiated with any permitted values). Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 16, we can calculate its RD value for slot 1:\nRD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)RD1(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate RD1(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 15 to calculate this value for a figurative evaluated agent \u03c0 from \u03a0e:\nRD1(\u03c0,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R1(\u00b5[u\u0307 1\u2190 \u03c0]), R1(\u00b5[v\u0307 1\u2190 \u03c0])) =\n= 2 2\n1\n1\n2\n1 2 \u2206Q(R1(\u00b5[\u03c0, \u03c0c]), R1(\u00b5[\u03c0, \u03c0b]))\nNote that we avoided to calculate both \u2206Q(a, b) and \u2206Q(b, a) since they provide the same result, by calculating only \u2206Q(a, b) and multiplying the result by 2. From the matching pennies\u2019 payoff matrix (figure 4)\nIn line-up (\u03c0, \u03c0c), where \u03c0c will always perform Cooperate, the agent in slot 1 (\u03c0) will obtain an expected average reward between 13 and 1, while in line-up (\u03c0, \u03c0b), where \u03c0b will always perform Blame, the agent in slot 1 (\u03c0) will obtain another expected average reward between \u22121 and \u2212 13 . So:\nRD1(\u03c0,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 1 = 1\nTherefore, no matter which agents are in \u03a0e and their weights w\u03a0e we obtain:\nRD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 1\nAnd finally, generalising for all slots:\nRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= RD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = = 1\nSo, for every pair \u27e8\u03a0e, w\u03a0e\u27e9 we obtain the same result:\n\u2200\u03a0e, w\u03a0e : RD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 1\nTherefore, prisoner\u2019s dilemma has Rightmin = 1 for this property."}, {"heading": "B.3 Fine Discrimination", "text": "Now we move to the fine discrimination (FD) property. As given in section 4.4.1, we want to know if different evaluated agents obtain different expected average rewards when interacting in the environment. We use \u2206Q(a, b) = 1 if numbers a and b are equal and 0 otherwise.\nProposition 33. Generalmin for the fine discrimination (FD) property is equal to 0 for the prisoner\u2019s dilemma environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0b1, \u03c0b2} with uniform weight for w\u03a0e and \u03a0o = {\u03c0c} (a \u03c0c agent always performs Cooperate and a \u03c0b agent always performs Blame).\nFollowing definition 23, we obtain the FD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 22, we can calculate its FD value for slot 1:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD1(\u03c0b1, \u03c0b2,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate both FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)} and FDi(\u03c02, \u03c01,\u03a0o, wL\u0307, \u00b5)} since they provide the same result, by calculating only FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)} and multiplying the result by 2.\nIn this case, we only need to calculate FD1(\u03c0b1, \u03c0b2,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value:\nFD1(\u03c0b1, \u03c0b2,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R1(\u00b5[l\u0307 1\u2190 \u03c0b1]), R1(\u00b5[l\u0307 1\u2190 \u03c0b2])) =\n= \u2206Q(R1(\u00b5[\u03c0b1, \u03c0c]), R1(\u00b5[\u03c0b2, \u03c0c]))\nHere, a \u03c0b agent will always perform Blame and \u03c0c will always perform Cooperate, so both agents in slot 1 (\u03c0b1 and \u03c0b2) will obtain the same expected average reward (1). So:\nFD1(\u03c0b1, \u03c0b2,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, generalising for all slots:\nFD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)FDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= FD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = = 0\nSince 0 is the lowest possible value for the fine discriminative property, therefore prisoner\u2019s dilemma has Generalmin = 0 for this property.\nProposition 34. Generalmax for the fine discrimination (FD) property is equal to 1 for the prisoner\u2019s dilemma environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0c, \u03c0b} with uniform weight for w\u03a0e and \u03a0o = {\u03c0c} (a \u03c0c agent always performs Cooperate and a \u03c0b agent always performs Blame).\nFollowing definition 23, we obtain the FD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 22, we can calculate its FD value for slot 1:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD1(\u03c0c, \u03c0b,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate both FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)} and FDi(\u03c02, \u03c01,\u03a0o, wL\u0307, \u00b5)} since they provide the same result, by calculating only FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)} and multiplying the result by 2.\nIn this case, we only need to calculate FD1(\u03c0c, \u03c0b,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value:\nFD1(\u03c0c, \u03c0b,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R1(\u00b5[l\u0307 1\u2190 \u03c0c]), R1(\u00b5[l\u0307 1\u2190 \u03c0b])) =\n= \u2206Q(R1(\u00b5[\u03c0c, \u03c0c]), R1(\u00b5[\u03c0b, \u03c0c]))\nIn line-up (\u03c0c, \u03c0c), where both \u03c0c will always perform Cooperate, the agent in slot 1 (\u03c0c) will obtain one expected average reward ( 13 ), while in line-up (\u03c0b, \u03c0c), where \u03c0b will always perform Blame and \u03c0c will always perform Cooperate, the agent in slot 1 (\u03c0b) will obtain a different expected average reward (1). So:\nFD1(\u03c0c, \u03c0b,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 1 = 1\nAnd finally, generalising for all slots:\nFD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)FDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= FD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = = 1\nSince 1 is the highest possible value for the fine discriminative property, therefore prisoner\u2019s dilemma has Generalmax = 1 for this property.\nConjecture 1. Leftmax for the fine discrimination (FD) property is equal to 0 for the prisoner\u2019s dilemma environment.\nAn agent \u03c0 \u2208 \u03a0o can force every evaluated agent to obtain an expected average reward equal to 0 (in the limit). The procedure is simple. While the evaluated agent has an expected average reward lower/greater than 0, \u03c0 performs Cooperate/Blame forcing the evaluated agent to increase/decrease its expected average reward. If this procedure is repeated indefinitely, the expected average reward of any evaluated agent will tend to 0. So:\n\u2200\u03c01, \u03c02\u2203\u03a0o : FD(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5, wS) = 0 Therefore, prisoner\u2019s dilemma has Leftmax = 0 for this property.\nProposition 35. Rightmin for the fine discrimination (FD) property is equal to 0 for the prisoner\u2019s dilemma environment.\nProof. To find Rightmin (equation 44), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which minimises the property as much as possible while \u03a0o maximises it. Using \u03a0e = {\u03c0b1, \u03c0b2} with uniform weight for w\u03a0e (a \u03c0b agent always performs Blame) we find this situation no matter which \u03a0o we use.\nFollowing definition 23, we obtain the FD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted values). Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 22, we can calculate its FD value for slot 1:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD1(\u03c0b1, \u03c0b2,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate both FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)} and FDi(\u03c02, \u03c01,\u03a0o, wL\u0307, \u00b5)} since they provide the same result, by calculating only FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)} and multiplying the result by 2.\nIn this case, we only need to calculate FD1(\u03c0b1, \u03c0b2,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value:\nFD1(\u03c0b1, \u03c0b2,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R1(\u00b5[l\u0307 1\u2190 \u03c0b1]), R1(\u00b5[l\u0307 1\u2190 \u03c0b2]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22121 (\u03a0o) to calculate \u2206Q(R1(\u00b5[l\u0307 1\u2190 \u03c0b1]), R1(\u00b5[l\u0307 1\u2190 \u03c0b2])). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c0) from L\u0307N(\u00b5)\u22121 (\u03a0o):\n\u2206Q(R1(\u00b5[l\u0307 1\u2190 \u03c0b1]), R1(\u00b5[l\u0307 1\u2190 \u03c0b2])) = \u2206Q(R1(\u00b5[\u03c0b1, \u03c0]), R1(\u00b5[\u03c0b2, \u03c0]))\nA \u03c0b agent will always perform Blame, so we obtain a situation where the agent in both slots 2 (any \u03c0) will be able to differentiate with which agent is interacting, so it will not be able to change its distribution of action sequences depending on the opponent\u2019s behaviour, obtaining both agents in slot 1 (\u03c0b1 and \u03c0b2) the same expected average reward. So:\nFD1(\u03c0b1, \u03c0b2,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, generalising for all slots:\nFD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)FDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)\n= FD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nSo, for every \u03a0o we obtain the same result:\n\u2200\u03a0o : FD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 0\nTherefore, prisoner\u2019s dilemma has Rightmin = 0 for this property."}, {"heading": "B.4 Strict Total Grading", "text": "We arrive to the strict total grading (STG) property. As given in section 4.4.2, we want to know if there exists a strict ordering between the evaluated agents when interacting in the environment.\nTo simplify the notation, we use the next table to represent the STO: Ri(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c02]) < Rj(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c02]), Ri(\u00b5[l\u0307 i,j\u2190 \u03c02, \u03c03]) < Rj(\u00b5[l\u0307 i,j\u2190 \u03c02, \u03c03]) and Ri(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c03]) < Rj(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c03]).\nSlot i Slot j \u03c01 < \u03c02 \u03c02 < \u03c03 \u03c01 < \u03c03\nProposition 36. Generalmin for the strict total grading (STG) property is equal to 0 for the prisoner\u2019s dilemma environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0b1, \u03c0b2, \u03c0b3} with uniform weight for w\u03a0e and \u03a0o = \u2205 (a \u03c0b agent always performs Blame).\nFollowing definition 29, we obtain the STG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 28, we can calculate its STG value for slots 1 and 2:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(\u03c0b1, \u03c0b2, \u03c0b3,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for STGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(\u03c0b1, \u03c0b2, \u03c0b3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,2(\u03c0b1, \u03c0b2, \u03c0b3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)STO1,2(\u03c0b1, \u03c0b2, \u03c0b3, l\u0307, \u00b5) =\n= STO1,2(\u03c0b1, \u03c0b2, \u03c0b3, (\u2217, \u2217), \u00b5)\nThe following table shows us STO1,2 for all the permutations of \u03c0b1, \u03c0b2, \u03c0b3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0b1 < \u03c0b2 \u03c0b1 < \u03c0b3 \u03c0b2 < \u03c0b1 \u03c0b2 < \u03c0b3 \u03c0b3 < \u03c0b2 \u03c0b1 < \u03c0b3 \u03c0b1 < \u03c0b3 \u03c0b1 < \u03c0b2 \u03c0b2 < \u03c0b3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0b2 < \u03c0b3 \u03c0b3 < \u03c0b1 \u03c0b3 < \u03c0b2 \u03c0b3 < \u03c0b1 \u03c0b1 < \u03c0b2 \u03c0b2 < \u03c0b1 \u03c0b2 < \u03c0b1 \u03c0b3 < \u03c0b2 \u03c0b3 < \u03c0b1\nBut, it is not possible to find a STO, since for every permutation we always have \u03c0bi < \u03c0bj , where a \u03c0b agent will always perform Blame, so they will both obtain an expected average reward of \u2212 13 . So:\nSTG1,2(\u03c0b1, \u03c0b2, \u03c0b3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, generalising for all slots:\nSTG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = STG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 0\nSince 0 is the lowest possible value for the strict total grading property, therefore prisoner\u2019s dilemma has Generalmin = 0 for this property.\nProposition 37. Generalmax for the strict total grading (STG) property is equal to 1 for the prisoner\u2019s dilemma environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0c, \u03c0b, \u03c0r} with uniform weight for w\u03a0e and \u03a0o = \u2205 (a \u03c0c agent always performs Cooperate, a \u03c0b agent always performs Blame and a \u03c0r agent always acts randomly).\nFollowing definition 29, we obtain the STG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 28, we can calculate its STG value for slots 1 and 2:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(\u03c0c, \u03c0b, \u03c0r,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for STGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(\u03c0c, \u03c0b, \u03c0r,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,2(\u03c0c, \u03c0b, \u03c0r,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)STO1,2(\u03c0c, \u03c0b, \u03c0r, l\u0307, \u00b5) =\n= STO1,2(\u03c0c, \u03c0b, \u03c0r, (\u2217, \u2217), \u00b5)\nThe following table shows us STO1,2 for all the permutations of \u03c0c, \u03c0b, \u03c0r.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0c < \u03c0b \u03c0c < \u03c0r \u03c0b < \u03c0c \u03c0b < \u03c0r \u03c0r < \u03c0b \u03c0c < \u03c0r \u03c0c < \u03c0r \u03c0c < \u03c0b \u03c0b < \u03c0r Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0b < \u03c0r \u03c0r < \u03c0c \u03c0r < \u03c0b \u03c0r < \u03c0c \u03c0c < \u03c0b \u03c0b < \u03c0c \u03c0b < \u03c0c \u03c0r < \u03c0b \u03c0r < \u03c0c\nIt is possible to find a STO for the second permutation. In \u03c0c < \u03c0r, \u03c0c will always perform Cooperate and \u03c0r will always act randomly, so they will obtain an expected average reward of \u2212 13 and 2 3 respectively. In \u03c0r < \u03c0b, \u03c0r will always act randomly and \u03c0b will always perform Blame, so they will obtain an expected average reward of \u2212 23 and 1 3 respectively. In \u03c0c < \u03c0b, \u03c0c will always perform Cooperate and \u03c0b will always perform Blame, so they will obtain an expected average reward of \u22121 and 1 respectively. So:\nSTG1,2(\u03c0c, \u03c0b, \u03c0r,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, generalising for all slots:\nSTG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = STG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 1\nSince 1 is the highest possible value for the strict total grading property, therefore prisoner\u2019s dilemma has Generalmax = 1 for this property.\nProposition 38. Leftmax for the strict total grading (STG) property is equal to 1 for the prisoner\u2019s dilemma environment.\nProof. To find Leftmax (equation 43), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which maximises the property as much as possible while \u03a0o minimises it. Using \u03a0e = {\u03c0c, \u03c0b, \u03c0r} with uniform weight for w\u03a0e (a \u03c0c agent always performs Cooperate, a \u03c0b agent always performs Blame and a \u03c0r agent always acts randomly) we find this situation no matter which \u03a0o we use.\nFollowing definition 29, we obtain the STG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted values). Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 28, we can calculate its STG value for slots 1 and 2:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(\u03c0c, \u03c0b, \u03c0r,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for STGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(\u03c0c, \u03c0b, \u03c0r,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,2(\u03c0c, \u03c0b, \u03c0r,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)STO1,2(\u03c0c, \u03c0b, \u03c0r, l\u0307, \u00b5) =\n= STO1,2(\u03c0c, \u03c0b, \u03c0r, (\u2217, \u2217), \u00b5)\nNote that the choice of \u03a0o does not affect the result of STG1,2. The following table shows us STO1,2 for all the permutations of \u03c0c, \u03c0b, \u03c0r.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0c < \u03c0b \u03c0c < \u03c0r \u03c0b < \u03c0c \u03c0b < \u03c0r \u03c0r < \u03c0b \u03c0c < \u03c0r \u03c0c < \u03c0r \u03c0c < \u03c0b \u03c0b < \u03c0r Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0b < \u03c0r \u03c0r < \u03c0c \u03c0r < \u03c0b \u03c0r < \u03c0c \u03c0c < \u03c0b \u03c0b < \u03c0c \u03c0b < \u03c0c \u03c0r < \u03c0b \u03c0r < \u03c0c\nIt is possible to find a STO for the second permutation. In \u03c0c < \u03c0r, \u03c0c will always perform Cooperate and \u03c0r will always act randomly, so they will obtain an expected average reward of \u2212 13 and 2 3 respectively. In \u03c0r < \u03c0b, \u03c0r will always act randomly and \u03c0b will always perform Blame, so they will obtain an expected average reward of \u2212 23 and 1 3 respectively. In \u03c0c < \u03c0b, \u03c0c will always perform Cooperate and \u03c0b will always perform Blame, so they will obtain an expected average reward of \u22121 and 1 respectively. So:\nSTG1,2(\u03c0c, \u03c0b, \u03c0r,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, generalising for all slots:\nSTG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = STG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 1\nSo, for every \u03a0o we obtain the same result:\n\u2200\u03a0o : STG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 1\nTherefore, prisoner\u2019s dilemma has Leftmax = 1 for this property.\nProposition 39. Rightmin for the strict total grading (STG) property is equal to 0 for the prisoner\u2019s dilemma environment.\nProof. To find Rightmin (equation 44), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which minimises the property as much as possible while \u03a0o maximises it. Using \u03a0e = {\u03c0b1, \u03c0b2, \u03c0b3} with uniform weight for w\u03a0e (a \u03c0b agent always performs Blame) we find this situation no matter which \u03a0o we use.\nFollowing definition 29, we obtain the STG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted values). Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 28, we can calculate its STG value for slots 1 and 2:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(\u03c0b1, \u03c0b2, \u03c0b3,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for STGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(\u03c0b1, \u03c0b2, \u03c0b3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,2(\u03c0b1, \u03c0b2, \u03c0b3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)STO1,2(\u03c0b1, \u03c0b2, \u03c0b3, l\u0307, \u00b5) =\n= STO1,2(\u03c0b1, \u03c0b2, \u03c0b3, (\u2217, \u2217), \u00b5)\nNote that the choice of \u03a0o does not affect the result of STG1,2. The following table shows us STO1,2 for all the permutations of \u03c0b1, \u03c0b2, \u03c0b3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0b1 < \u03c0b2 \u03c0b1 < \u03c0b3 \u03c0b2 < \u03c0b1 \u03c0b2 < \u03c0b3 \u03c0b3 < \u03c0b2 \u03c0b1 < \u03c0b3 \u03c0b1 < \u03c0b3 \u03c0b1 < \u03c0b2 \u03c0b2 < \u03c0b3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0b2 < \u03c0b3 \u03c0b3 < \u03c0b1 \u03c0b3 < \u03c0b2 \u03c0b3 < \u03c0b1 \u03c0b1 < \u03c0b2 \u03c0b2 < \u03c0b1 \u03c0b2 < \u03c0b1 \u03c0b3 < \u03c0b2 \u03c0b3 < \u03c0b1\nBut, it is not possible to find a STO, since for every permutation we always have \u03c0bi < \u03c0bj , where a \u03c0b agent will always perform Blame, so they will both obtain an expected average reward of \u2212 13 . So:\nSTG1,2(\u03c0b1, \u03c0b2, \u03c0b3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, generalising for all slots:\nSTG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = STG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 0\nSo, for every \u03a0o we obtain the same result:\n\u2200\u03a0o : STG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 0\nTherefore, prisoner\u2019s dilemma has Rightmin = 0 for this property."}, {"heading": "B.5 Partial Grading", "text": "Now we arrive to the partial grading (PG) property. As given in section 4.4.2, we want to know if there exists a partial ordering between the evaluated agents when interacting in the environment.\nTo simplify the notation, we use the next table to represent the PO: Ri(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c02]) \u2264 Rj(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c02]), Ri(\u00b5[l\u0307 i,j\u2190 \u03c02, \u03c03]) \u2264 Rj(\u00b5[l\u0307 i,j\u2190 \u03c02, \u03c03]) and Ri(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c03]) \u2264 Rj(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c03]).\nSlot i Slot j \u03c01 \u2264 \u03c02 \u03c02 \u2264 \u03c03 \u03c01 \u2264 \u03c03\nProposition 40. Generalmin for the partial grading (PG) property is equal to 0 for the prisoner\u2019s dilemma environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0\u2212ccbb , \u03c0 \u2212bbc b , \u03c0m} with uniform weight for w\u03a0e and \u03a0o = \u2205 (a \u03c0m agent first acts randomly and then always mimics the other agent\u2019s last action, a \u03c0\u2212ccbb agent always performs Blame except for the last three actions where it performs Cooperate twice and finalises performing Blame, and a \u03c0\u2212bbcb agent always performs Blame except for the last three actions where it performs Blame twice and finalises performing Cooperate).\nFollowing definition 30, we obtain the PG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 28 (for PG), we can calculate its PG value for slots 1 and 2:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(\u03c0 \u2212ccb b , \u03c0 \u2212bbc b , \u03c0m,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for PGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(\u03c0 \u2212ccb b , \u03c0 \u2212bbc b , \u03c0m,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG)\nto calculate this value:\nPG1,2(\u03c0 \u2212ccb b , \u03c0 \u2212bbc b , \u03c0m,\u03a0o, wL\u0307, \u00b5) = \u2211 l\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o) wL\u0307(l\u0307)PO1,2(\u03c0 \u2212ccb b , \u03c0 \u2212bbc b , \u03c0m, l\u0307, \u00b5) =\n= PO1,2(\u03c0 \u2212ccb b , \u03c0 \u2212bbc b , \u03c0m, (\u2217, \u2217), \u00b5)\nThe following table shows us PO1,2 for all the permutations of \u03c0 \u2212ccb b , \u03c0 \u2212bbc b , \u03c0m.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0\u2212ccbb \u2264 \u03c0 \u2212bbc b \u03c0 \u2212ccb b \u2264 \u03c0m \u03c0 \u2212bbc b \u2264 \u03c0 \u2212ccb b \u03c0\u2212bbcb \u2264 \u03c0m \u03c0m \u2264 \u03c0 \u2212bbc b \u03c0 \u2212ccb b \u2264 \u03c0m \u03c0\u2212ccbb \u2264 \u03c0m \u03c0 \u2212ccb b \u2264 \u03c0 \u2212bbc b \u03c0 \u2212bbc b \u2264 \u03c0m Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0\u2212bbcb \u2264 \u03c0m \u03c0m \u2264 \u03c0 \u2212ccb b \u03c0m \u2264 \u03c0 \u2212bbc b\n\u03c0m \u2264 \u03c0\u2212ccbb \u03c0 \u2212ccb b \u2264 \u03c0 \u2212bbc b \u03c0 \u2212bbc b \u2264 \u03c0 \u2212ccb b\n\u03c0\u2212bbcb \u2264 \u03c0 \u2212ccb b \u03c0m \u2264 \u03c0 \u2212bbc b \u03c0m \u2264 \u03c0 \u2212ccb b\nBut, it is not possible to find a PO for any permutation, since \u03c0\u2212ccbb > \u03c0m, \u03c0 \u2212bbc b > \u03c0 \u2212ccb b and \u03c0m > \u03c0 \u2212bbc b .\nSo:\nPG1,2(\u03c0 \u2212ccb b , \u03c0 \u2212bbc b , \u03c0m,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, generalising for all slots:\nPG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = PG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 0\nSince 0 is the lowest possible value for the partial grading property, therefore prisoner\u2019s dilemma has Generalmin = 0 for this property.\nProposition 41. Generalmax for the partial grading (PG) property is equal to 1 for the prisoner\u2019s dilemma environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0b1, \u03c0b2, \u03c0b3} with uniform weight for w\u03a0e and \u03a0o = \u2205 (a \u03c0b agent always performs Blame).\nFollowing definition 30, we obtain the PG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 28 (for PG), we can calculate its PG value for slots 1 and 2:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(\u03c0b1, \u03c0b2, \u03c0b3,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for PGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(\u03c0b1, \u03c0b2, \u03c0b3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG1,2(\u03c0b1, \u03c0b2, \u03c0b3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)PO1,2(\u03c0b1, \u03c0b2, \u03c0b3, l\u0307, \u00b5) =\n= PO1,2(\u03c0b1, \u03c0b2, \u03c0b3, (\u2217, \u2217), \u00b5)\nThe following table shows us PO1,2 for all the permutations of \u03c0b1, \u03c0b2, \u03c0b3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0b1 \u2264 \u03c0b2 \u03c0b1 \u2264 \u03c0b3 \u03c0b2 \u2264 \u03c0b1 \u03c0b2 \u2264 \u03c0b3 \u03c0b3 \u2264 \u03c0b2 \u03c0b1 \u2264 \u03c0b3 \u03c0b1 \u2264 \u03c0b3 \u03c0b1 \u2264 \u03c0b2 \u03c0b2 \u2264 \u03c0b3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0b2 \u2264 \u03c0b3 \u03c0b3 \u2264 \u03c0b1 \u03c0b3 \u2264 \u03c0b2 \u03c0b3 \u2264 \u03c0b1 \u03c0b1 \u2264 \u03c0b2 \u03c0b2 \u2264 \u03c0b1 \u03c0b2 \u2264 \u03c0b1 \u03c0b3 \u2264 \u03c0b2 \u03c0b3 \u2264 \u03c0b1\nIt is possible to find a PO for every permutation, since we always have \u03c0bi \u2264 \u03c0bj , where a \u03c0b agent will always perform Blame, so they will both obtain an expected average reward of \u2212 13 . So:\nPG1,2(\u03c0b1, \u03c0b2, \u03c0b3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, generalising for all slots:\nPG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = PG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 1\nSince 1 is the highest possible value for the partial grading property, therefore prisoner\u2019s dilemma has Generalmax = 1 for this property.\nProposition 42. Leftmax for the partial grading (PG) property is equal to 1 for the prisoner\u2019s dilemma environment.\nProof. To find Leftmax (equation 43), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which maximises the property as much as possible while \u03a0o minimises it. Using \u03a0e = {\u03c0b1, \u03c0b2, \u03c0b3} with uniform weight for w\u03a0e (a \u03c0b agent always performs Blame) we find this situation no matter which \u03a0o we use.\nFollowing definition 30, we obtain the PG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted values). Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 28 (for PG), we can calculate its PG value for slots 1 and 2:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(\u03c0b1, \u03c0b2, \u03c0b3,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for PGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(\u03c0b1, \u03c0b2, \u03c0b3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG1,2(\u03c0b1, \u03c0b2, \u03c0b3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)PO1,2(\u03c0b1, \u03c0b2, \u03c0b3, l\u0307, \u00b5) =\n= PO1,2(\u03c0b1, \u03c0b2, \u03c0b3, (\u2217, \u2217), \u00b5) Note that the choice of \u03a0o does not affect the result of PG1,2.\nThe following table shows us PO1,2 for all the permutations of \u03c0b1, \u03c0b2, \u03c0b3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0b1 \u2264 \u03c0b2 \u03c0b1 \u2264 \u03c0b3 \u03c0b2 \u2264 \u03c0b1 \u03c0b2 \u2264 \u03c0b3 \u03c0b3 \u2264 \u03c0b2 \u03c0b1 \u2264 \u03c0b3 \u03c0b1 \u2264 \u03c0b3 \u03c0b1 \u2264 \u03c0b2 \u03c0b2 \u2264 \u03c0b3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0b2 \u2264 \u03c0b3 \u03c0b3 \u2264 \u03c0b1 \u03c0b3 \u2264 \u03c0b2 \u03c0b3 \u2264 \u03c0b1 \u03c0b1 \u2264 \u03c0b2 \u03c0b2 \u2264 \u03c0b1 \u03c0b2 \u2264 \u03c0b1 \u03c0b3 \u2264 \u03c0b2 \u03c0b3 \u2264 \u03c0b1\nIt is possible to find a PO for every permutation, since we always have \u03c0bi \u2264 \u03c0bj , where a \u03c0b agent will always perform Blame, so they will both obtain an expected average reward of \u2212 13 . So:\nPG1,2(\u03c0b1, \u03c0b2, \u03c0b3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, generalising for all slots:\nPG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = PG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 1\nSo, for every \u03a0o we obtain the same result:\n\u2200\u03a0o : PG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 1\nTherefore, prisoner\u2019s dilemma has Leftmax = 1 for this property.\nProposition 43. Rightmin for the partial grading (PG) property is equal to 0 for the prisoner\u2019s dilemma environment.\nProof. To find Rightmin (equation 44), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which minimises the property as much as possible while \u03a0o maximises it. Using \u03a0e = {\u03c0\u2212ccbb , \u03c0 \u2212bbc b , \u03c0m} with uniform weight for w\u03a0e (a \u03c0m agent first acts randomly and then always mimics the other agent\u2019s last action, a \u03c0\u2212ccbb agent always performs Blame except for the last three actions where it performs Cooperate twice and finalises performing Blame, and a \u03c0\u2212bbcb agent always performs Blame except for the last three actions where it performs Blame twice and finalises performing Cooperate) we find this situation no matter which \u03a0o we use.\nFollowing definition 30, we obtain the PG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted values). Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 28 (for PG), we can calculate its PG value for slots 1 and 2:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(\u03c0 \u2212ccb b , \u03c0 \u2212bbc b , \u03c0m,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for PGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(\u03c0 \u2212ccb b , \u03c0 \u2212bbc b , \u03c0m,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG)\nto calculate this value:\nPG1,2(\u03c0 \u2212ccb b , \u03c0 \u2212bbc b , \u03c0m,\u03a0o, wL\u0307, \u00b5) = \u2211 l\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o) wL\u0307(l\u0307)PO1,2(\u03c0 \u2212ccb b , \u03c0 \u2212bbc b , \u03c0m, l\u0307, \u00b5) =\n= PO1,2(\u03c0 \u2212ccb b , \u03c0 \u2212bbc b , \u03c0m, (\u2217, \u2217), \u00b5)\nNote that the choice of \u03a0o does not affect the result of PG1,2. The following table shows us PO1,2 for all the permutations of \u03c0 \u2212ccb b , \u03c0 \u2212bbc b , \u03c0m.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0\u2212ccbb \u2264 \u03c0 \u2212bbc b \u03c0 \u2212ccb b \u2264 \u03c0m \u03c0 \u2212bbc b \u2264 \u03c0 \u2212ccb b \u03c0\u2212bbcb \u2264 \u03c0m \u03c0m \u2264 \u03c0 \u2212bbc b \u03c0 \u2212ccb b \u2264 \u03c0m \u03c0\u2212ccbb \u2264 \u03c0m \u03c0 \u2212ccb b \u2264 \u03c0 \u2212bbc b \u03c0 \u2212bbc b \u2264 \u03c0m Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0\u2212bbcb \u2264 \u03c0m \u03c0m \u2264 \u03c0 \u2212ccb b \u03c0m \u2264 \u03c0 \u2212bbc b\n\u03c0m \u2264 \u03c0\u2212ccbb \u03c0 \u2212ccb b \u2264 \u03c0 \u2212bbc b \u03c0 \u2212bbc b \u2264 \u03c0 \u2212ccb b\n\u03c0\u2212bbcb \u2264 \u03c0 \u2212ccb b \u03c0m \u2264 \u03c0 \u2212bbc b \u03c0m \u2264 \u03c0 \u2212ccb b\nBut, it is not possible to find a PO for any permutation, since \u03c0\u2212ccbb < \u03c0 \u2212bbc b , \u03c0 \u2212bbc b < \u03c0m but \u03c0 \u2212ccb b > \u03c0m.\nSo:\nPG1,2(\u03c0 \u2212ccb b , \u03c0 \u2212bbc b , \u03c0m,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, generalising for all slots:\nPG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = PG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 0\nSo, for every \u03a0o we obtain the same result:\n\u2200\u03a0o : PG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 0\nTherefore, prisoner\u2019s dilemma has Rightmin = 0 for this property."}, {"heading": "B.6 Slot Reward Dependency", "text": "Next we see the slot reward dependency (SRD) property. As given in section 4.3.2, we want to know how much competitive or cooperative the environment is.\nProposition 44. Generalmin for the slot reward dependency (SRD) property is equal to \u22121 for the prisoner\u2019s dilemma environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0b} with w\u03a0e(\u03c0b) = 1 and \u03a0o = {\u03c0c} (a \u03c0c agent always performs Cooperate and a \u03c0b agent always performs Blame).\nFollowing definition 20, we obtain the SRD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 19, we could calculate its SRD value for slots 1 and 2 but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 18 for slots 1 and 2:\nSRD1,2(\u03c0b,\u03a0o, wL\u0307, \u00b5) = corrl\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o) [wL\u0307(l\u0307)](R1(\u00b5[l\u0307\n1\u2190 \u03c0b]), R2(\u00b5[l\u0307 1\u2190 \u03c0b])) =\n= corr(R1(\u00b5[\u03c0b, \u03c0c]), R2(\u00b5[\u03c0b, \u03c0c]))\nIn line-up (\u03c0b, \u03c0c), where \u03c0b will always perform Blame and \u03c0c will always perform Cooperate, they will obtain an expected average reward of 1 and \u22121 respectively. Since we use a correlation function between the expected average rewards, and the agents in slots 1 and 2 obtain an expected average reward of \u22121 and 1 respectively, then the correlation function will obtain the value of \u22121. So:\nSRD1,2(\u03c0b,\u03a0o, wL\u0307, \u00b5) = \u22121\nAnd finally:\nSRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)SRDi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)SRDi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = SRD1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= SRD1,2(\u03c0b,\u03a0o, wL\u0307, \u00b5) = = \u22121\nSince \u22121 is the lowest possible value for the slot reward dependency property, therefore prisoner\u2019s dilemma has Generalmin = \u22121 for this property.\nProposition 45. Generalmax for the slot reward dependency (SRD) property is equal to 1 for the prisoner\u2019s dilemma environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0b} with w\u03a0e(\u03c0b) = 1 and \u03a0o = {\u03c0b} (a \u03c0b agent always performs Blame).\nFollowing definition 20, we obtain the SRD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 19, we could calculate its SRD value for slots 1 and 2 but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 18 for slots 1 and 2:\nSRD1,2(\u03c0b,\u03a0o, wL\u0307, \u00b5) = corrl\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o) [wL\u0307(l\u0307)](R1(\u00b5[l\u0307\n1\u2190 \u03c0b]), R2(\u00b5[l\u0307 1\u2190 \u03c0b])) =\n= corr(R1(\u00b5[\u03c0b, \u03c0b]), R2(\u00b5[\u03c0b, \u03c0b]))\nIn line-up (\u03c0b, \u03c0b), where both \u03c0b will always perform Blame, they will both obtain an expected average reward of \u2212 13 . Since we use a correlation function between the expected average rewards, and the agents in slots 1 and 2 obtain the same expected average reward of \u2212 13 , then the correlation function will obtain the value of 1. So:\nSRD1,2(\u03c0b,\u03a0o, wL\u0307, \u00b5) = 1\nAnd finally:\nSRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)SRDi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)SRDi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = SRD1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= SRD1,2(\u03c0b,\u03a0o, wL\u0307, \u00b5) = = 1\nSince 1 is the highest possible value for the slot reward dependency property, therefore prisoner\u2019s dilemma has Generalmax = 1 for this property."}, {"heading": "B.7 Competitive Anticipation", "text": "Finally, we follow with the competitive anticipation (AComp) property. As given in section 4.5.1, we want to know how much benefit the evaluated agents obtain when they anticipate competing agents.\nProposition 46. Generalmin for the competitive anticipation (AComp) property is equal to \u2212 23 for the prisoner\u2019s dilemma environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0c/b} with w\u03a0e(\u03c0c/b) = 1 and \u03a0o = {\u03c0b} (a \u03c0c/b agent performs Cooperate until the other agent also performs Cooperate, then it starts to perform Blame, and a \u03c0b agent always performs Blame).\nFollowing definition 33, we obtain the AComp value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is symmetric, we just need to calculate this property for one pair of slots in different teams, and generalise its result to all pair of slots. Following definition 32, we could calculate its AComp value for slots 1 and 2 but, since \u03a0e has only one agent, its weight is equal to 1 and \u03a0o also has only one agent, it is equivalent to use directly definition 31 for slots 1 and 2:\nAComp1,2(\u03c0c/b, \u03c0b,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( R1(\u00b5[l\u0307 1,2\u2190 \u03c0c/b, \u03c0b])\u2212R1(\u00b5[l\u0307 1,2\u2190 \u03c0c/b, \u03c0r]) ) =\n= 1\n2\n( R1(\u00b5[\u03c0c/b, \u03c0b])\u2212R1(\u00b5[\u03c0c/b, \u03c0r]) ) In line-up (\u03c0c/b, \u03c0b), where \u03c0c/b will always perform Cooperate and \u03c0b will always perform Blame, the agent in slot 1 (\u03c0c/b) will obtain an expected average reward of \u22121. In line-up (\u03c0c/b, \u03c0r), where \u03c0c/b will start with Cooperate and then will continue with Blame once \u03c0r performs Cooperate, and \u03c0r will always act randomly, the agent in slot 1 (\u03c0c/b) will obtain an expected average reward of 1 3 (in the limit). So:\nAComp1,2(\u03c0c/b, \u03c0b,\u03a0o, wL\u0307, \u00b5) = 1\n2\n( (\u22121)\u2212 1\n3\n) = \u22122\n3\nNote that this is the minimum possible value for AComp1,2(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) since interacting with a random agent cannot provide a greater result than 13 for this environment.\nAnd finally, generalising for all slots:\nAComp(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S22 \u2211 t1,t2\u2208\u03c4 |t1 \u0338=t2 \u2211 i\u2208t1 wS(i, \u00b5) \u2211 j\u2208t2 wS(j, \u00b5)ACompi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= AComp1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = = AComp1,2(\u03c0c/b, \u03c0b,\u03a0o, wL\u0307, \u00b5) = = \u22122 3\nSince \u2212 23 is the lowest possible value that we can obtain for the competitive anticipation property, therefore prisoner\u2019s dilemma has Generalmin = \u2212 23 for this property.\nProposition 47. Generalmax for the competitive anticipation (AComp) property is equal to 2 3 for the prisoner\u2019s dilemma environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0b/c} with w\u03a0e(\u03c0b/c) = 1 and \u03a0o = {\u03c0c} (a \u03c0b/c agent performs Blame until the other agent also performs Blame, then it starts to perform Cooperate, and a \u03c0c agent always performs Cooperate).\nFollowing definition 33, we obtain the AComp value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is symmetric, we just need to calculate this property for one pair of slots in different teams, and generalise its result to all pair of slots. Following definition 32, we could calculate its AComp value for slots 1 and 2 but, since \u03a0e has only one agent, its weight is equal to 1 and \u03a0o also has only one agent, it is equivalent to use directly definition 31 for slots 1 and 2:\nAComp1,2(\u03c0b/c, \u03c0c,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( R1(\u00b5[l\u0307 1,2\u2190 \u03c0b/c, \u03c0c])\u2212R1(\u00b5[l\u0307 1,2\u2190 \u03c0b/c, \u03c0r]) ) =\n= 1\n2\n( R1(\u00b5[\u03c0b/c, \u03c0c])\u2212R1(\u00b5[\u03c0b/c, \u03c0r]) ) In line-up (\u03c0b/c, \u03c0c), where \u03c0b/c will always perform Blame and \u03c0c will always perform Cooperate, the agent in slot 1 (\u03c0b/c) will obtain an expected average reward of 1. In line-up (\u03c0b/c, \u03c0r), where \u03c0b/c will start with Blame and then will continue with Cooperate once \u03c0r performs Blame, and \u03c0r will always act randomly, the agent in slot 1 (\u03c0b/c) will obtain an expected average reward of \u2212 13 (in the limit). So:\nAComp1,2(\u03c0b/c, \u03c0c,\u03a0o, wL\u0307, \u00b5) = 1\n2\n( 1\u2212 ( \u22121 3 )) = 2 3\nNote that this is the maximum possible value for AComp1,2(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) since interacting with a random agent cannot provide a lower result than \u2212 13 for this environment.\nAnd finally, generalising for all slots:\nAComp(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S22 \u2211 t1,t2\u2208\u03c4 |t1 \u0338=t2 \u2211 i\u2208t1 wS(i, \u00b5) \u2211 j\u2208t2 wS(j, \u00b5)ACompi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= AComp1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = = AComp1,2(\u03c0b/c, \u03c0c,\u03a0o, wL\u0307, \u00b5) = = 2\n3\nSince 23 is the highest possible value that we can obtain for the competitive anticipation property, therefore prisoner\u2019s dilemma has Generalmax = 2 3 for this property."}, {"heading": "C Predator-prey properties", "text": "In this section we prove how we obtained the values for the properties for the predator-prey environment (section 5.4). To calculate some of the values for the properties, we make use of lemma 3.\nLemma 3. When three well coordinated predators are trying to chase the prey, it will always be chased in 5 iterations or less no matter the behaviour of the prey.\nSince there exists a lot of variants to chase the prey, we cannot show them all. Instead, here we show one of the largest sequences of actions to chase the prey in 5 iterations when the prey is trying to escape and the predators are well coordinated.\n\u2297\nOther behaviours of the prey will lead it closer to the boundaries, which would be easier for the predators to chase it."}, {"heading": "C.1 Action Dependency", "text": "We start with the action dependency (AD) property. As given in section 4.2.1, we want to know if the evaluated agents behave differently depending on which line-up they interact with. We use \u2206S(a, b) = 1 if distributions a and b are equal and 0 otherwise.\nProposition 48. Generalmin for the action dependency (AD) property is equal to 0 for the predator-prey environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0u} with w\u03a0e(\u03c0u) = 1 and \u03a0o = {\u03c0d1, \u03c0d2} (a \u03c0d agent always performs Down and a \u03c0u agent always performs Up).\nFollowing definition 14, we obtain the AD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 13, we could calculate its AD value for each slot but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 12. We start with slot 1:\nAD1(\u03c0u,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0307 \u0338=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03061(\u00b5[u\u0307 1\u2190 \u03c0u]), A\u03061(\u00b5[v\u0307 1\u2190 \u03c0u])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206S(A\u03061(\u00b5[\u03c0u, \u03c0d1, \u03c0d1, \u03c0d1]), A\u03061(\u00b5[\u03c0u, \u03c0d1, \u03c0d1, \u03c0d2]))+\n+\u2206S(A\u03061(\u00b5[\u03c0u, \u03c0d1, \u03c0d1, \u03c0d1]), A\u03061(\u00b5[\u03c0u, \u03c0d1, \u03c0d2, \u03c0d1]))+\n...\n+ \u2206S(A\u03061(\u00b5[\u03c0u, \u03c0d2, \u03c0d2, \u03c0d1]), A\u03061(\u00b5[\u03c0u, \u03c0d2, \u03c0d2, \u03c0d2]))}\nNote that we avoided to calculate both \u2206S(a, b) and \u2206S(b, a) since they provide the same result, by calculating only \u2206S(a, b) and multiplying the result by 2.\nIn this case, we have 28 possible pairs of line-ups where the agent in both slots 1 (\u03c0u) will perform the same sequence of actions (always Up) independently of the line-up. So:\nAD1(\u03c0u,\u03a0o, wL\u0307, \u00b5) = 2 8\n7\n1\n8\n1 8 {28\u00d7 0} = 0\nFor slot 2, the agent in both slots 2 (\u03c0u) will also perform the same sequence of actions (always Up) independently of the line-up. So:\nAD2(\u03c0u,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)|u\u0307 \u0338=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03062(\u00b5[u\u0307 2\u2190 \u03c0u]), A\u03062(\u00b5[v\u0307 2\u2190 \u03c0u])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206S(A\u03062(\u00b5[\u03c0d1, \u03c0u, \u03c0d1, \u03c0d1]), A\u03062(\u00b5[\u03c0d1, \u03c0u, \u03c0d1, \u03c0d2]))+\n+\u2206S(A\u03062(\u00b5[\u03c0d1, \u03c0u, \u03c0d1, \u03c0d1]), A\u03062(\u00b5[\u03c0d1, \u03c0u, \u03c0d2, \u03c0d1]))+\n...\n+ \u2206S(A\u03062(\u00b5[\u03c0d2, \u03c0u, \u03c0d2, \u03c0d1]), A\u03062(\u00b5[\u03c0d2, \u03c0u, \u03c0d2, \u03c0d2]))} = = 2 8\n7\n1\n8\n1 8 {28\u00d7 0} = 0\nFor slot 3, the agent in both slots 3 (\u03c0u) will also perform the same sequence of actions (always Up) independently of the line-up. So:\nAD3(\u03c0u,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22123 (\u03a0o)|u\u0307 \u0338=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03063(\u00b5[u\u0307 3\u2190 \u03c0u]), A\u03063(\u00b5[v\u0307 3\u2190 \u03c0u])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206S(A\u03063(\u00b5[\u03c0d1, \u03c0d1, \u03c0u, \u03c0d1]), A\u03063(\u00b5[\u03c0d1, \u03c0d1, \u03c0u, \u03c0d2]))+\n+\u2206S(A\u03063(\u00b5[\u03c0d1, \u03c0d1, \u03c0u, \u03c0d1]), A\u03063(\u00b5[\u03c0d1, \u03c0d2, \u03c0u, \u03c0d1]))+\n...\n+ \u2206S(A\u03063(\u00b5[\u03c0d2, \u03c0d2, \u03c0u, \u03c0d1]), A\u03063(\u00b5[\u03c0d2, \u03c0d2, \u03c0u, \u03c0d2]))} = = 2 8\n7\n1\n8\n1 8 {28\u00d7 0} = 0\nAnd for slot 4, the agent in both slots 4 (\u03c0u) will also perform the same sequence of actions (always Up) independently of the line-up. So:\nAD4(\u03c0u,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22124 (\u03a0o)|u\u0307 \u0338=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03064(\u00b5[u\u0307 4\u2190 \u03c0u]), A\u03064(\u00b5[v\u0307 4\u2190 \u03c0u])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206S(A\u03064(\u00b5[\u03c0d1, \u03c0d1, \u03c0d1, \u03c0u]), A\u03064(\u00b5[\u03c0d1, \u03c0d1, \u03c0d2, \u03c0u]))+\n+\u2206S(A\u03064(\u00b5[\u03c0d1, \u03c0d1, \u03c0d1, \u03c0u]), A\u03064(\u00b5[\u03c0d1, \u03c0d2, \u03c0d1, \u03c0u]))+\n...\n+ \u2206S(A\u03064(\u00b5[\u03c0d2, \u03c0d2, \u03c0d1, \u03c0u]), A\u03064(\u00b5[\u03c0d2, \u03c0d2, \u03c0d2, \u03c0u]))} = = 2 8\n7\n1\n8\n1 8 {28\u00d7 0} = 0\nAnd finally, we weight over the slots:\nAD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03c0u,\u03a0o, wL\u0307, \u00b5) = = 1\n4 {AD1(\u03c0u,\u03a0o, wL\u0307, \u00b5) +AD2(\u03c0u,\u03a0o, wL\u0307, \u00b5)+\n+AD3(\u03c0u,\u03a0o, wL\u0307, \u00b5) +AD4(\u03c0u,\u03a0o, wL\u0307, \u00b5)} = = 1\n4 {0 + 0 + 0 + 0} = 0\nSince 0 is the lowest possible value for the action dependency property, therefore predator-prey hasGeneralmin = 0 for this property.\nProposition 49. Generalmax for the action dependency (AD) property is equal to 1 for the predator-prey environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0m} with w\u03a0e(\u03c0m) = 1 and \u03a0o = {\u03c0u, \u03c0d} (a \u03c0m agent first acts randomly and then always mimics sequentially the other agents\u2019 last action, a \u03c0u agent always performs Up and a \u03c0d agent always performs Down).\nFollowing definition 14, we obtain the AD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 13, we could calculate its AD value for each slot but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 12. We start with slot 1:\nAD1(\u03c0m,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03061(\u00b5[u\u0307 1\u2190 \u03c0m]), A\u03061(\u00b5[v\u0307 1\u2190 \u03c0m])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206S(A\u03061(\u00b5[\u03c0m, \u03c0u, \u03c0u, \u03c0u]), A\u03061(\u00b5[\u03c0m, \u03c0u, \u03c0u, \u03c0d]))+\n+\u2206S(A\u03061(\u00b5[\u03c0m, \u03c0u, \u03c0u, \u03c0u]), A\u03061(\u00b5[\u03c0m, \u03c0u, \u03c0d, \u03c0u]))+\n...\n+ \u2206S(A\u03061(\u00b5[\u03c0m, \u03c0d, \u03c0d, \u03c0u]), A\u03061(\u00b5[\u03c0m, \u03c0d, \u03c0d, \u03c0d]))} Note that we avoided to calculate both \u2206S(a, b) and \u2206S(b, a) since they provide the same result, by calculating only \u2206S(a, b) and multiplying the result by 2.\nIn this case, we have 28 possible pairs of line-ups where the agent in both slots 1 (\u03c0m) will perform different sequences of actions depending on the line-up. So:\nAD1(\u03c0m,\u03a0o, wL\u0307, \u00b5) = 2 8\n7\n1\n8\n1 8 {28\u00d7 1} = 1\nFor slot 2, the agent in both slots 2 (\u03c0m) will also perform different sequences of actions depending on the line-up. So:\nAD2(\u03c0m,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03062(\u00b5[u\u0307 2\u2190 \u03c0m]), A\u03062(\u00b5[v\u0307 2\u2190 \u03c0m])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206S(A\u03062(\u00b5[\u03c0u, \u03c0m, \u03c0u, \u03c0u]), A\u03062(\u00b5[\u03c0u, \u03c0m, \u03c0u, \u03c0d]))+\n+\u2206S(A\u03062(\u00b5[\u03c0u, \u03c0m, \u03c0u, \u03c0u]), A\u03062(\u00b5[\u03c0u, \u03c0m, \u03c0d, \u03c0u]))+\n...\n+ \u2206S(A\u03062(\u00b5[\u03c0d, \u03c0m, \u03c0d, \u03c0u]), A\u03062(\u00b5[\u03c0d, \u03c0m, \u03c0d, \u03c0d]))} =\n= 2 8\n7\n1\n8\n1 8 {28\u00d7 1} = 1\nFor slot 3, the agent in both slots 3 (\u03c0m) will also perform different sequences of actions depending on the line-up. So:\nAD3(\u03c0m,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22123 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03063(\u00b5[u\u0307 3\u2190 \u03c0m]), A\u03063(\u00b5[v\u0307 3\u2190 \u03c0m])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206S(A\u03063(\u00b5[\u03c0u, \u03c0u, \u03c0m, \u03c0u]), A\u03063(\u00b5[\u03c0u, \u03c0u, \u03c0m, \u03c0d]))+\n+\u2206S(A\u03063(\u00b5[\u03c0u, \u03c0u, \u03c0m, \u03c0u]), A\u03063(\u00b5[\u03c0u, \u03c0d, \u03c0m, \u03c0u]))+\n...\n+ \u2206S(A\u03063(\u00b5[\u03c0d, \u03c0d, \u03c0m, \u03c0u]), A\u03063(\u00b5[\u03c0d, \u03c0d, \u03c0m, \u03c0d]))} =\n= 2 8\n7\n1\n8\n1 8 {28\u00d7 1} = 1\nAnd for slot 4, the agent in both slots 4 (\u03c0m) will also perform different sequences of actions depending on the line-up. So:\nAD4(\u03c0m,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22124 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03064(\u00b5[u\u0307 4\u2190 \u03c0m]), A\u03064(\u00b5[v\u0307 4\u2190 \u03c0m])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206S(A\u03064(\u00b5[\u03c0u, \u03c0u, \u03c0u, \u03c0m]), A\u03064(\u00b5[\u03c0u, \u03c0u, \u03c0d, \u03c0m]))+\n+\u2206S(A\u03064(\u00b5[\u03c0u, \u03c0u, \u03c0u, \u03c0m]), A\u03064(\u00b5[\u03c0u, \u03c0d, \u03c0u, \u03c0m]))+\n...\n+ \u2206S(A\u03064(\u00b5[\u03c0d, \u03c0d, \u03c0u, \u03c0m]), A\u03064(\u00b5[\u03c0d, \u03c0d, \u03c0d, \u03c0m]))} =\n= 2 8\n7\n1\n8\n1 8 {28\u00d7 1} = 1\nAnd finally, we weight over the slots:\nAD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03c0m,\u03a0o, wL\u0307, \u00b5) = = 1\n4 {AD1(\u03c0m,\u03a0o, wL\u0307, \u00b5) +AD2(\u03c0m,\u03a0o, wL\u0307, \u00b5)+\n+AD3(\u03c0m,\u03a0o, wL\u0307, \u00b5) +AD4(\u03c0m,\u03a0o, wL\u0307, \u00b5)} = = 1\n4 {1 + 1 + 1 + 1} = 1\nSince 1 is the highest possible value for the action dependency property, therefore predator-prey has Generalmax = 1 for this property.\nProposition 50. Leftmax for the action dependency (AD) property is equal to 0 for the predator-prey environment.\nProof. To find Leftmax (equation 43), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which maximises the property as much as possible while \u03a0o minimises it. Using \u03a0o = {\u03c0d1, \u03c0d2} (a \u03c0d agent always performs Down) we find this situation no matter which pair \u27e8\u03a0e, w\u03a0e\u27e9 we use.\nFollowing definition 14, we obtain the AD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0e and w\u03a0e are instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 13, we can calculate its AD value for each slot. We start with slot 1:\nAD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)AD1(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate AD1(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 12 to calculate this value for a figurative evaluated agent \u03c0 from \u03a0e:\nAD1(\u03c0,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03061(\u00b5[u\u0307 1\u2190 \u03c0]), A\u03061(\u00b5[v\u0307 1\u2190 \u03c0])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206S(A\u03061(\u00b5[\u03c0, \u03c0d1, \u03c0d1, \u03c0d1]), A\u03061(\u00b5[\u03c0, \u03c0d1, \u03c0d1, \u03c0d2]))+\n+\u2206S(A\u03061(\u00b5[\u03c0, \u03c0d1, \u03c0d1, \u03c0d1]), A\u03061(\u00b5[\u03c0, \u03c0d1, \u03c0d2, \u03c0d1]))+\n...\n+ \u2206S(A\u03061(\u00b5[\u03c0, \u03c0d2, \u03c0d2, \u03c0d1]), A\u03061(\u00b5[\u03c0, \u03c0d2, \u03c0d2, \u03c0d2]))}\nNote that we avoided to calculate both \u2206S(a, b) and \u2206S(b, a) since they provide the same result, by calculating only \u2206S(a, b) and multiplying the result by 2.\nA \u03c0d agent will always perform Down, so for the 28 possible pairs of line-ups we obtain a situation where the agent in both slots 1 (any \u03c0) will be able to differentiate with which agents is interacting, so it will not be able to change its distribution of action sequences depending on the line-up. So:\nAD1(\u03c0,\u03a0o, wL\u0307, \u00b5) = 2 8\n7\n1\n8\n1 8 {28\u00d7 0} = 0\nTherefore, no matter which agents are in \u03a0e and their weights w\u03a0e we obtain:\nAD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nFor slot 2:\nAD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)AD2(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate AD2(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 12 to calculate this value for a figurative evaluated agent \u03c0 from \u03a0e:\nAD2(\u03c0,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03062(\u00b5[u\u0307 2\u2190 \u03c0]), A\u03062(\u00b5[v\u0307 2\u2190 \u03c0])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206S(A\u03062(\u00b5[\u03c0d1, \u03c0, \u03c0d1, \u03c0d1]), A\u03062(\u00b5[\u03c0d1, \u03c0, \u03c0d1, \u03c0d2]))+\n+\u2206S(A\u03062(\u00b5[\u03c0d1, \u03c0, \u03c0d1, \u03c0d1]), A\u03062(\u00b5[\u03c0d1, \u03c0, \u03c0d2, \u03c0d1]))+\n...\n+ \u2206S(A\u03062(\u00b5[\u03c0d2, \u03c0, \u03c0d2, \u03c0d1]), A\u03062(\u00b5[\u03c0d2, \u03c0, \u03c0d2, \u03c0d2]))}\nA \u03c0d agent will always perform Down, so for the 28 possible pairs of line-ups we obtain a situation where the agent in both slots 2 (any \u03c0) will be able to differentiate with which agents is interacting, so it will not be able to change its distribution of action sequences depending on the line-up. So:\nAD2(\u03c0,\u03a0o, wL\u0307, \u00b5) = 2 8\n7\n1\n8\n1 8 {28\u00d7 0} = 0\nTherefore, no matter which agents are in \u03a0e and their weights w\u03a0e we obtain:\nAD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nFor slot 3:\nAD3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)AD3(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate AD3(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 12 to calculate this value for a figurative evaluated agent \u03c0 from \u03a0e:\nAD3(\u03c0,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22123 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03063(\u00b5[u\u0307 3\u2190 \u03c0]), A\u03063(\u00b5[v\u0307 3\u2190 \u03c0])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206S(A\u03063(\u00b5[\u03c0d1, \u03c0d1, \u03c0, \u03c0d1]), A\u03063(\u00b5[\u03c0d1, \u03c0d1, \u03c0, \u03c0d2]))+\n+\u2206S(A\u03063(\u00b5[\u03c0d1, \u03c0d1, \u03c0, \u03c0d1]), A\u03063(\u00b5[\u03c0d1, \u03c0d2, \u03c0, \u03c0d1]))+\n...\n+ \u2206S(A\u03063(\u00b5[\u03c0d2, \u03c0d2, \u03c0, \u03c0d1]), A\u03063(\u00b5[\u03c0d2, \u03c0d2, \u03c0, \u03c0d2]))}\nA \u03c0d agent will always perform Down, so for the 28 possible pairs of line-ups we obtain a situation where the agent in both slots 3 (any \u03c0) will be able to differentiate with which agents is interacting, so it will not be able to change its distribution of action sequences depending on the line-up. So:\nAD3(\u03c0,\u03a0o, wL\u0307, \u00b5) = 2 8\n7\n1\n8\n1 8 {28\u00d7 0} = 0\nTherefore, no matter which agents are in \u03a0e and their weights w\u03a0e we obtain:\nAD3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nAnd for slot 4:\nAD4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)AD4(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate AD4(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 12 to calculate this value for a figurative evaluated agent \u03c0 from \u03a0e:\nAD4(\u03c0,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22124 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03064(\u00b5[u\u0307 4\u2190 \u03c0]), A\u03064(\u00b5[v\u0307 4\u2190 \u03c0])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206S(A\u03064(\u00b5[\u03c0d1, \u03c0d1, \u03c0d1, \u03c0]), A\u03064(\u00b5[\u03c0d1, \u03c0d1, \u03c0d2, \u03c0]))+\n+\u2206S(A\u03064(\u00b5[\u03c0d1, \u03c0d1, \u03c0d1, \u03c0]), A\u03064(\u00b5[\u03c0d1, \u03c0d2, \u03c0d1, \u03c0]))+\n...\n+ \u2206S(A\u03064(\u00b5[\u03c0d2, \u03c0d2, \u03c0d1, \u03c0]), A\u03064(\u00b5[\u03c0d2, \u03c0d2, \u03c0d2, \u03c0]))}\nA \u03c0d agent will always perform Down, so for the 28 possible pairs of line-ups we obtain a situation where the agent in both slots 4 (any \u03c0) will be able to differentiate with which agents is interacting, so it will not be able to change its distribution of action sequences depending on the line-up. So:\nAD4(\u03c0,\u03a0o, wL\u0307, \u00b5) = 2 8\n7\n1\n8\n1 8 {28\u00d7 0} = 0\nTherefore, no matter which agents are in \u03a0e and their weights w\u03a0e we obtain:\nAD4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nAnd finally, we weight over the slots:\nAD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 1\n4 {AD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +AD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+AD3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +AD4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 1\n4 {0 + 0 + 0 + 0} = 0\nSo, for every pair \u27e8\u03a0e, w\u03a0e\u27e9 we obtain the same result:\n\u2200\u03a0e, w\u03a0e : AD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 0\nTherefore, predator-prey has Leftmax = 0 for this property.\nProposition 51. Rightmin for the action dependency (AD) property is equal to 0 for the predator-prey environment.\nProof. To find Rightmin (equation 44), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which minimises the property as much as possible while \u03a0o maximises it. Using \u03a0e = {\u03c0u} with w\u03a0e(\u03c0u) = 1 (a \u03c0u agent always performs Up) we find this situation no matter which \u03a0o we use.\nFollowing definition 14, we obtain the AD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 13, we could calculate its AD value for each slot but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 12. We start with slot 1:\nAD1(\u03c0u,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03061(\u00b5[u\u0307 1\u2190 \u03c0u]), A\u03061(\u00b5[v\u0307 1\u2190 \u03c0u]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain two different line-up patterns\nu\u0307 and v\u0307 from L\u0307 N(\u00b5) \u22121 (\u03a0o) to calculate \u2206S(A\u03061(\u00b5[u\u0307 1\u2190 \u03c0u]), A\u03061(\u00b5[v\u0307 1\u2190 \u03c0u])). We calculate this value for two figurative line-up patterns u\u0307 = (\u2217, \u03c01, \u03c02, \u03c03) and v\u0307 = (\u2217, \u03c04, \u03c05, \u03c06) from L\u0307N(\u00b5)\u22121 (\u03a0o):\n\u2206S(A\u03061(\u00b5[u\u0307 1\u2190 \u03c0u]), A\u03061(\u00b5[v\u0307 1\u2190 \u03c0u])) = \u2206S(A\u03061(\u00b5[\u03c0u, \u03c01, \u03c02, \u03c03]), A\u03061(\u00b5[\u03c0u, \u03c04, \u03c05, \u03c06]))\nHere, the agent in both slots 1 (\u03c0u) will perform the same sequence of actions (always Up) independently of the line-up. So no matter which agents are in \u03a0o we obtain:\nAD1(\u03c0u,\u03a0o, wL\u0307, \u00b5) = 0\nFor slot 2: AD2(\u03c0u,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03062(\u00b5[u\u0307 2\u2190 \u03c0u]), A\u03062(\u00b5[v\u0307 2\u2190 \u03c0u]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain two different line-up patterns\nu\u0307 and v\u0307 from L\u0307 N(\u00b5) \u22122 (\u03a0o) to calculate \u2206S(A\u03062(\u00b5[u\u0307 2\u2190 \u03c0u]), A\u03062(\u00b5[v\u0307 2\u2190 \u03c0u])). We calculate this value for two figurative line-up patterns u\u0307 = (\u03c01, \u2217, \u03c02, \u03c03) and v\u0307 = (\u03c04, \u2217, \u03c05, \u03c06) from L\u0307N(\u00b5)\u22122 (\u03a0o):\n\u2206S(A\u03062(\u00b5[u\u0307 2\u2190 \u03c0u]), A\u03062(\u00b5[v\u0307 2\u2190 \u03c0u])) = \u2206S(A\u03062(\u00b5[\u03c01, \u03c0u, \u03c02, \u03c03]), A\u03062(\u00b5[\u03c04, \u03c0u, \u03c05, \u03c06]))\nHere, the agent in both slots 2 (\u03c0u) will perform the same sequence of actions (always Up) independently of the line-up. So no matter which agents are in \u03a0o we obtain:\nAD2(\u03c0u,\u03a0o, wL\u0307, \u00b5) = 0\nFor slot 3: AD3(\u03c0u,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22123 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03063(\u00b5[u\u0307 3\u2190 \u03c0u]), A\u03063(\u00b5[v\u0307 3\u2190 \u03c0u]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain two different line-up patterns\nu\u0307 and v\u0307 from L\u0307 N(\u00b5) \u22123 (\u03a0o) to calculate \u2206S(A\u03063(\u00b5[u\u0307 3\u2190 \u03c0u]), A\u03063(\u00b5[v\u0307 3\u2190 \u03c0u])). We calculate this value for two figurative line-up patterns u\u0307 = (\u03c01, \u03c02, \u2217, \u03c03) and v\u0307 = (\u03c04, \u03c05, \u2217, \u03c06) from L\u0307N(\u00b5)\u22123 (\u03a0o):\n\u2206S(A\u03063(\u00b5[u\u0307 3\u2190 \u03c0u]), A\u03063(\u00b5[v\u0307 3\u2190 \u03c0u])) = \u2206S(A\u03063(\u00b5[\u03c01, \u03c02, \u03c0u, \u03c03]), A\u03063(\u00b5[\u03c04, \u03c05, \u03c0u, \u03c06]))\nHere, the agent in both slots 3 (\u03c0u) will perform the same sequence of actions (always Up) independently of the line-up. So no matter which agents are in \u03a0o we obtain:\nAD3(\u03c0u,\u03a0o, wL\u0307, \u00b5) = 0\nAnd for slot 4: AD4(\u03c0u,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22124 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206S(A\u03064(\u00b5[u\u0307 4\u2190 \u03c0u]), A\u03064(\u00b5[v\u0307 4\u2190 \u03c0u]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain two different line-up patterns\nu\u0307 and v\u0307 from L\u0307 N(\u00b5) \u22124 (\u03a0o) to calculate \u2206S(A\u03064(\u00b5[u\u0307 4\u2190 \u03c0u]), A\u03064(\u00b5[v\u0307 4\u2190 \u03c0u])). We calculate this value for two figurative line-up patterns u\u0307 = (\u03c01, \u03c02, \u03c03, \u2217) and v\u0307 = (\u03c04, \u03c05, \u03c06, \u2217) from L\u0307N(\u00b5)\u22124 (\u03a0o):\n\u2206S(A\u03064(\u00b5[u\u0307 4\u2190 \u03c0u]), A\u03064(\u00b5[v\u0307 4\u2190 \u03c0u])) = \u2206S(A\u03064(\u00b5[\u03c01, \u03c02, \u03c03, \u03c0u]), A\u03064(\u00b5[\u03c04, \u03c05, \u03c06, \u03c0u]))\nHere, the agent in both slots 4 (\u03c0u) will perform the same sequence of actions (always Up) independently of the line-up. So no matter which agents are in \u03a0o we obtain:\nAD4(\u03c0u,\u03a0o, wL\u0307, \u00b5) = 0\nAnd finally, we weight over the slots:\nAD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= N(\u00b5)\u2211 i=1 wS(i, \u00b5)ADi(\u03c0u,\u03a0o, wL\u0307, \u00b5) = = 1\n4 {AD1(\u03c0u,\u03a0o, wL\u0307, \u00b5) +AD2(\u03c0u,\u03a0o, wL\u0307, \u00b5)+\n+AD3(\u03c0u,\u03a0o, wL\u0307, \u00b5) +AD4(\u03c0u,\u03a0o, wL\u0307, \u00b5)} = = 1\n4 {0 + 0 + 0 + 0} = 0\nSo, for every \u03a0o we obtain the same result:\n\u2200\u03a0o : AD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 0\nTherefore, predator-prey has Rightmin = 0 for this property."}, {"heading": "C.2 Reward Dependency", "text": "We continue with the reward dependency (RD) property. As given in section 4.3.1, we want to know if the evaluated agents obtain different expected average rewards depending on which line-up they interact with. We use \u2206Q(a, b) = 1 if numbers a and b are equal and 0 otherwise.\nProposition 52. Generalmin for the reward dependency (RD) property is equal to 0 for the predator-prey environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0u} with w\u03a0e(\u03c0u) = 1 and \u03a0o = {\u03c0d1, \u03c0d2} (a \u03c0d agent always performs Down and a \u03c0u agent always performs Up).\nFollowing definition 17, we obtain the RD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 16, we could calculate its RD value for each slot but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 15. We start with slot 1:\nRD1(\u03c0u,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R1(\u00b5[u\u0307 1\u2190 \u03c0u]), R1(\u00b5[v\u0307 1\u2190 \u03c0u])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206Q(R1(\u00b5[\u03c0u, \u03c0d1, \u03c0d1, \u03c0d1]), R1(\u00b5[\u03c0u, \u03c0d1, \u03c0d1, \u03c0d2]))+\n+\u2206Q(R1(\u00b5[\u03c0u, \u03c0d1, \u03c0d1, \u03c0d1]), R1(\u00b5[\u03c0u, \u03c0d1, \u03c0d2, \u03c0d1]))+\n...\n+ \u2206Q(R1(\u00b5[\u03c0u, \u03c0d2, \u03c0d2, \u03c0d1]), R1(\u00b5[\u03c0u, \u03c0d2, \u03c0d2, \u03c0d2]))}\nNote that we avoided to calculate both \u2206Q(a, b) and \u2206Q(b, a) since they provide the same result, by calculating only \u2206Q(a, b) and multiplying the result by 2.\nIn this case, we have 28 possible pairs of line-ups, where \u03c0u will always perform Up and a \u03c0d agent will always perform Down, so the agent in both slots 1 (\u03c0u) will obtain the same expected average reward (1) independently of the line-up. So:\nRD1(\u03c0u,\u03a0o, wL\u0307, \u00b5) = 2 8\n7\n1\n8\n1 8 {28\u00d7 0} = 0\nFor slot 2, the agent in both slots 2 (\u03c0u) will also obtain the same expected average reward (1) independently of the line-up. So:\nRD2(\u03c0u,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R2(\u00b5[u\u0307 2\u2190 \u03c0u]), R2(\u00b5[v\u0307 2\u2190 \u03c0u])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206Q(R2(\u00b5[\u03c0d1, \u03c0u, \u03c0d1, \u03c0d1]), R2(\u00b5[\u03c0d1, \u03c0u, \u03c0d1, \u03c0d2]))+\n+\u2206Q(R2(\u00b5[\u03c0d1, \u03c0u, \u03c0d1, \u03c0d1]), R2(\u00b5[\u03c0d1, \u03c0u, \u03c0d2, \u03c0d1]))+\n...\n+ \u2206Q(R2(\u00b5[\u03c0d2, \u03c0u, \u03c0d2, \u03c0d1]), R2(\u00b5[\u03c0d2, \u03c0u, \u03c0d2, \u03c0d2]))} = = 2 8\n7\n1\n8\n1 8 {28\u00d7 0} = 0\nFor slot 3, the agent in both slots 3 (\u03c0u) will also obtain the same expected average reward (\u22121) independently of the line-up. So:\nRD3(\u03c0u,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22123 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R3(\u00b5[u\u0307 3\u2190 \u03c0u]), R3(\u00b5[v\u0307 3\u2190 \u03c0u])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206Q(R3(\u00b5[\u03c0d1, \u03c0d1, \u03c0u, \u03c0d1]), R3(\u00b5[\u03c0d1, \u03c0d1, \u03c0u, \u03c0d2]))+\n+\u2206Q(R3(\u00b5[\u03c0d1, \u03c0d1, \u03c0u, \u03c0d1]), R3(\u00b5[\u03c0d1, \u03c0d2, \u03c0u, \u03c0d1]))+\n...\n+ \u2206Q(R3(\u00b5[\u03c0d2, \u03c0d2, \u03c0u, \u03c0d1]), R3(\u00b5[\u03c0d2, \u03c0d2, \u03c0u, \u03c0d2]))} = = 2 8\n7\n1\n8\n1 8 {28\u00d7 0} = 0\nAnd for slot 4, the agent in both slots 4 (\u03c0u) will also obtain the same expected average reward (1) independently of the line-up. So:\nRD4(\u03c0u,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22124 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R4(\u00b5[u\u0307 4\u2190 \u03c0u]), R4(\u00b5[v\u0307 4\u2190 \u03c0u])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206Q(R4(\u00b5[\u03c0d1, \u03c0d1, \u03c0d1, \u03c0u]), R4(\u00b5[\u03c0d1, \u03c0d1, \u03c0d2, \u03c0u]))+\n+\u2206Q(R4(\u00b5[\u03c0d1, \u03c0d1, \u03c0d1, \u03c0u]), R4(\u00b5[\u03c0d1, \u03c0d2, \u03c0d1, \u03c0u]))+\n...\n+ \u2206Q(R4(\u00b5[\u03c0d2, \u03c0d2, \u03c0d1, \u03c0u]), R4(\u00b5[\u03c0d2, \u03c0d2, \u03c0d2, \u03c0u]))} = = 2 8\n7\n1\n8\n1 8 {28\u00d7 0} = 0\nAnd finally, we weight over the slots:\nRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03c0u,\u03a0o, wL\u0307, \u00b5) = = 1\n4 {RD1(\u03c0u,\u03a0o, wL\u0307, \u00b5) +RD2(\u03c0u,\u03a0o, wL\u0307, \u00b5)+\n+RD3(\u03c0u,\u03a0o, wL\u0307, \u00b5) +RD4(\u03c0u,\u03a0o, wL\u0307, \u00b5)} = = 1\n4 {0 + 0 + 0 + 0} = 0\nSince 0 is the lowest possible value for the reward dependency property, therefore predator-prey hasGeneralmin = 0 for this property.\nConjecture 2. Generalmax for the reward dependency (RD) property is equal to 1 for the predator-prey environment.\nTo find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0r} with w\u03a0e(\u03c0r) = 1 and \u03a0o = {\u03c0s, \u03c0r} (a \u03c0r agent always acts randomly and a \u03c0s agent always stays in the same cell\n16). Following definition 17, we obtain the RD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 16, we could calculate its RD value for each slot but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 15. We start with slot 1:\nRD1(\u03c0r,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0307 \u0338=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R1(\u00b5[u\u0307 1\u2190 \u03c0r]), R1(\u00b5[v\u0307 1\u2190 \u03c0r])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206Q(R1(\u00b5[\u03c0r, \u03c0s, \u03c0s, \u03c0s]), R1(\u00b5[\u03c0r, \u03c0s, \u03c0s, \u03c0r]))+\n+\u2206Q(R1(\u00b5[\u03c0r, \u03c0s, \u03c0s, \u03c0s]), R1(\u00b5[\u03c0r, \u03c0s, \u03c0r, \u03c0s]))+\n...\n+ \u2206Q(R1(\u00b5[\u03c0r, \u03c0r, \u03c0r, \u03c0s]), R1(\u00b5[\u03c0r, \u03c0r, \u03c0r, \u03c0r]))}\nNote that we avoided to calculate both \u2206Q(a, b) and \u2206Q(b, a) since they provide the same result, by calculating only \u2206Q(a, b) and multiplying the result by 2.\nThe expected average reward of these line-ups highly depends on the agents\u2019 positions, obtaining an expected average reward from \u22121 to 1 (exclusive, since there always exists some probability that the prey will either be chased or not) to the agent in slot 1 (\u03c0r). One reason is the stochastic behaviour of the \u03c0r agents, which makes that no pair of line-ups can obtain exactly the same result. Another reason is that the positions of the slots where the random agents play as a predator do not have a symmetric place in the space (blocks are not symmetrically located in the space) which, for each \u03c0r in a different slot, provides (most likely) different probabilities to chase the prey17. This makes every pair of line-up to have different expected average rewards, making its reward dependency equal to 1.\nRD1(\u03c0r,\u03a0o, wL\u0307, \u00b5) = 2 8\n7\n1\n8\n1 8 {28\u00d7 1} = 1\nFor slot 2, also the result of these line-ups highly depends on the agents\u2019 positions. So:\nRD2(\u03c0r,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)|u\u0307 \u0338=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R2(\u00b5[u\u0307 2\u2190 \u03c0r]), R2(\u00b5[v\u0307 2\u2190 \u03c0r])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206Q(R2(\u00b5[\u03c0s, \u03c0r, \u03c0s, \u03c0s]), R2(\u00b5[\u03c0s, \u03c0r, \u03c0s, \u03c0r]))+\n+\u2206Q(R2(\u00b5[\u03c0s, \u03c0r, \u03c0s, \u03c0s]), R2(\u00b5[\u03c0s, \u03c0r, \u03c0r, \u03c0s]))+\n...\n+ \u2206Q(R2(\u00b5[\u03c0r, \u03c0r, \u03c0r, \u03c0s]), R2(\u00b5[\u03c0r, \u03c0r, \u03c0r, \u03c0r]))} =\n= 2 8\n7\n1\n8\n1 8 {28\u00d7 1} = 1\nFor slot 3, also the result of these line-ups highly depends on the agents\u2019 positions. So:\n16Note that every cell has an action which is blocked by a block or a boundary, therefore an agent performing this action will stay at its current cell.\n17It is more likely that the prey will be chased by the lower left predator than the upper right predator, and the lower right predator will have the lowest chance to chase the prey.\nRD3(\u03c0r,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22123 (\u03a0o)|u\u0307 \u0338=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R3(\u00b5[u\u0307 3\u2190 \u03c0r]), R3(\u00b5[v\u0307 3\u2190 \u03c0r])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206Q(R3(\u00b5[\u03c0s, \u03c0s, \u03c0r, \u03c0s]), R3(\u00b5[\u03c0s, \u03c0s, \u03c0r, \u03c0r]))+\n+\u2206Q(R3(\u00b5[\u03c0s, \u03c0s, \u03c0r, \u03c0s]), R3(\u00b5[\u03c0s, \u03c0r, \u03c0r, \u03c0s]))+\n...\n+ \u2206Q(R3(\u00b5[\u03c0r, \u03c0r, \u03c0r, \u03c0s]), R3(\u00b5[\u03c0r, \u03c0r, \u03c0r, \u03c0r]))} =\n= 2 8\n7\n1\n8\n1 8 {28\u00d7 1} = 1\nAnd for slot 4, also the result of these line-ups highly depends on the agents\u2019 positions. So:\nRD4(\u03c0r,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22124 (\u03a0o)|u\u0307 \u0338=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R4(\u00b5[u\u0307 4\u2190 \u03c0r]), R4(\u00b5[v\u0307 4\u2190 \u03c0r])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206Q(R4(\u00b5[\u03c0s, \u03c0s, \u03c0s, \u03c0r]), R4(\u00b5[\u03c0s, \u03c0s, \u03c0r, \u03c0r]))+\n+\u2206Q(R4(\u00b5[\u03c0s, \u03c0s, \u03c0s, \u03c0r]), R4(\u00b5[\u03c0s, \u03c0r, \u03c0s, \u03c0r]))+\n...\n+ \u2206Q(R4(\u00b5[\u03c0r, \u03c0r, \u03c0s, \u03c0r]), R4(\u00b5[\u03c0r, \u03c0r, \u03c0r, \u03c0r]))} =\n= 2 8\n7\n1\n8\n1 8 {28\u00d7 1} = 1\nAnd finally, we weight over the slots:\nRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03c0r,\u03a0o, wL\u0307, \u00b5) = = 1\n4 {RD1(\u03c0r,\u03a0o, wL\u0307, \u00b5) +RD2(\u03c0r,\u03a0o, wL\u0307, \u00b5)+\n+RD3(\u03c0r,\u03a0o, wL\u0307, \u00b5) +RD4(\u03c0r,\u03a0o, wL\u0307, \u00b5)} = = 1\n4 {1 + 1 + 1 + 1} = 1\nSince 1 is the highest possible value for the reward dependency property, therefore predator-prey has Generalmax = 1 for this property.\nProposition 53. Leftmax for the reward dependency (RD) property is equal to 0 for the predator-prey environment.\nProof. To find Leftmax (equation 43), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which maximises the property as much as possible while \u03a0o minimises it. Using \u03a0o = {\u03c0d1, \u03c0d2} (a \u03c0d agent always performs Down) we find this situation no matter which pair \u27e8\u03a0e, w\u03a0e\u27e9 we use.\nFollowing definition 17, we obtain the RD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0e and w\u03a0e are instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 16, we can calculate its RD value for slot 1:\nRD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)RD1(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate RD1(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 15 to calculate this value for a figurative evaluated agent \u03c0 from \u03a0e:\nRD1(\u03c0,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R1(\u00b5[u\u0307 1\u2190 \u03c0]), R1(\u00b5[v\u0307 1\u2190 \u03c0])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206Q(R1(\u00b5[\u03c0, \u03c0d1, \u03c0d1, \u03c0d1]), R1(\u00b5[\u03c0, \u03c0d1, \u03c0d1, \u03c0d2]))+\n+\u2206Q(R1(\u00b5[\u03c0, \u03c0d1, \u03c0d1, \u03c0d1]), R1(\u00b5[\u03c0, \u03c0d1, \u03c0d2, \u03c0d1]))+\n...\n+ \u2206Q(R1(\u00b5[\u03c0, \u03c0d2, \u03c0d2, \u03c0d1]), R1(\u00b5[\u03c0, \u03c0d2, \u03c0d2, \u03c0d2]))}\nNote that we avoided to calculate both \u2206Q(a, b) and \u2206Q(b, a) since they provide the same result, by calculating only \u2206Q(a, b) and multiplying the result by 2.\nA \u03c0d agent will always perform Down, so for the 28 possible pairs of line-ups we obtain a situation where the agent in both slots 1 (any \u03c0) will be able to differentiate with which agents is interacting, so it will not be able to change its distribution of action sequences depending on the line-up, obtaining agent in both slots 1 (any \u03c0) the same expected average reward. So:\nRD1(\u03c0,\u03a0o, wL\u0307, \u00b5) = 2 8\n7\n1\n8\n1 8 {28\u00d7 0} = 0\nTherefore, no matter which agents are in \u03a0e and their weights w\u03a0e we obtain:\nRD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nFor slot 2:\nRD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)RD2(\u03c0,\u03a0o, wL\u0307, \u00b5)\nAgain, we do not know which \u03a0e we have, but we know that we will need to evaluate RD2(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 15 to calculate this value for a figurative evaluated agent \u03c0 from \u03a0e:\nRD2(\u03c0,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R2(\u00b5[u\u0307 2\u2190 \u03c0]), R2(\u00b5[v\u0307 2\u2190 \u03c0])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206Q(R2(\u00b5[\u03c0d1, \u03c0, \u03c0d1, \u03c0d1]), R2(\u00b5[\u03c0d1, \u03c0, \u03c0d1, \u03c0d2]))+\n+\u2206Q(R2(\u00b5[\u03c0d1, \u03c0, \u03c0d1, \u03c0d1]), R2(\u00b5[\u03c0d1, \u03c0, \u03c0d2, \u03c0d1]))+\n...\n+ \u2206Q(R2(\u00b5[\u03c0d2, \u03c0, \u03c0d2, \u03c0d1]), R2(\u00b5[\u03c0d2, \u03c0, \u03c0d2, \u03c0d2]))}\nAgain, a \u03c0d agent will always perform Down, so for the 28 possible pairs of line-ups we obtain a situation where the agent in both slots 2 (any \u03c0) will be able to differentiate with which agents is interacting, so it will not be able to change its distribution of action sequences depending on the line-up, obtaining agent in both slots 2 (any \u03c0) the same expected average reward. So:\nRD2(\u03c0,\u03a0o, wL\u0307, \u00b5) = 2 8\n7\n1\n8\n1 8 {28\u00d7 0} = 0\nTherefore, no matter which agents are in \u03a0e and their weights w\u03a0e we obtain:\nRD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nFor slot 3:\nRD3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)RD3(\u03c0,\u03a0o, wL\u0307, \u00b5)\nAgain, we do not know which \u03a0e we have, but we know that we will need to evaluate RD3(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 15 to calculate this value for a figurative evaluated agent \u03c0 from \u03a0e:\nRD3(\u03c0,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22123 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R3(\u00b5[u\u0307 3\u2190 \u03c0]), R3(\u00b5[v\u0307 3\u2190 \u03c0])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206Q(R3(\u00b5[\u03c0d1, \u03c0d1, \u03c0, \u03c0d1]), R3(\u00b5[\u03c0d1, \u03c0d1, \u03c0, \u03c0d2]))+\n+\u2206Q(R3(\u00b5[\u03c0d1, \u03c0d1, \u03c0, \u03c0d1]), R3(\u00b5[\u03c0d1, \u03c0d2, \u03c0, \u03c0d1]))+\n...\n+ \u2206Q(R3(\u00b5[\u03c0d2, \u03c0d2, \u03c0, \u03c0d1]), R3(\u00b5[\u03c0d2, \u03c0d2, \u03c0, \u03c0d2]))}\nAgain, a \u03c0d agent will always perform Down, so for the 28 possible pairs of line-ups we obtain a situation where the agent in both slots 3 (any \u03c0) will be able to differentiate with which agents is interacting, so it will not be able to change its distribution of action sequences depending on the line-up, obtaining agent in both slots 3 (any \u03c0) the same expected average reward. So:\nRD3(\u03c0,\u03a0o, wL\u0307, \u00b5) = 2 8\n7\n1\n8\n1 8 {28\u00d7 0} = 0\nTherefore, no matter which agents are in \u03a0e and their weights w\u03a0e we obtain:\nRD3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nAnd for slot 4:\nRD4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)RD4(\u03c0,\u03a0o, wL\u0307, \u00b5)\nAgain, we do not know which \u03a0e we have, but we know that we will need to evaluate RD4(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 15 to calculate this value for a figurative evaluated agent \u03c0 from \u03a0e:\nRD4(\u03c0,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22124 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R4(\u00b5[u\u0307 4\u2190 \u03c0]), R4(\u00b5[v\u0307 4\u2190 \u03c0])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206Q(R4(\u00b5[\u03c0d1, \u03c0d1, \u03c0d1, \u03c0]), R4(\u00b5[\u03c0d1, \u03c0d1, \u03c0d2, \u03c0]))+\n+\u2206Q(R4(\u00b5[\u03c0d1, \u03c0d1, \u03c0d1, \u03c0]), R4(\u00b5[\u03c0d1, \u03c0d2, \u03c0d1, \u03c0]))+\n...\n+ \u2206Q(R4(\u00b5[\u03c0d2, \u03c0d2, \u03c0d1, \u03c0]), R4(\u00b5[\u03c0d2, \u03c0d2, \u03c0d2, \u03c0]))}\nAgain, a \u03c0d agent will always perform Down, so for the 28 possible pairs of line-ups we obtain a situation where the agent in both slots 4 (any \u03c0) will be able to differentiate with which agents is interacting, so it will not be able to change its distribution of action sequences depending on the line-up, obtaining agent in both slots 4 (any \u03c0) the same expected average reward. So:\nRD4(\u03c0,\u03a0o, wL\u0307, \u00b5) = 2 8\n7\n1\n8\n1 8 {28\u00d7 0} = 0\nTherefore, no matter which agents are in \u03a0e and their weights w\u03a0e we obtain:\nRD4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nAnd finally, we weight over the slots:\nRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 1\n4 {RD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +RD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+RD3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +RD4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 1\n4 {0 + 0 + 0 + 0} = 0\nSo, for every pair \u27e8\u03a0e, w\u03a0e\u27e9 we obtain the same result:\n\u2200\u03a0e, w\u03a0e : RD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 0\nTherefore, predator-prey has Leftmax = 0 for this property.\nApproximation 1. Rightmin for the reward dependency (RD) property is equal to 13 28 (as a lower approximation) for the predator-prey environment.\nProof. To find Rightmin (equation 44), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which minimises the property as much as possible while \u03a0o maximises it. Using \u03a0e = {\u03c0chase} with w\u03a0e(\u03c0chase) = 1 and \u03a0o = {\u03c0lose, \u03c0win} (a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, a \u03c0lose agent always tries to be chased when playing as the prey and tries to do not chase when playing as the predator, and a \u03c0win agent always tries to not be chased when playing as the prey and tries to chase when playing as the predator) we find a lower approximation with this situation.\nFollowing definition 17, we obtain the RD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 16, we could calculate its RD value for each slot but, since \u03a0e has only one agent and its weight is equal to 1, it is equivalent to use directly definition 15. We start with slot 1:\nRD1(\u03c0chase,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)|u\u0307 \u0338=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R1(\u00b5[u\u0307 1\u2190 \u03c0chase]), R1(\u00b5[v\u0307 1\u2190 \u03c0chase])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206Q(R1(\u00b5[\u03c0chase, \u03c0lose, \u03c0lose, \u03c0lose]), R1(\u00b5[\u03c0chase, \u03c0lose, \u03c0lose, \u03c0win]))+\n+\u2206Q(R1(\u00b5[\u03c0chase, \u03c0lose, \u03c0lose, \u03c0lose]), R1(\u00b5[\u03c0chase, \u03c0lose, \u03c0win, \u03c0lose]))+\n...\n+ \u2206Q(R1(\u00b5[\u03c0chase, \u03c0win, \u03c0win, \u03c0lose]), R1(\u00b5[\u03c0chase, \u03c0win, \u03c0win, \u03c0win]))}\nNote that we avoided to calculate both \u2206Q(a, b) and \u2206Q(b, a) since they provide the same result, by calculating only \u2206Q(a, b) and multiplying the result by 2.\nFrom the 28 possible pairs of line-ups that we obtained, \u03c0chase tries to make equal the maximum number of pairs, while \u03c0win and \u03c0lose try to diverge the maximum number of pairs. In this case, the agents from \u03a0o can only assure that two line-up patterns obtain different results ((\u2217, \u03c0lose, \u03c0lose, \u03c0lose) and (\u2217, \u03c0win, \u03c0win, \u03c0win)), therefore the agent from \u03a0e (\u03c0chase) can make equal seven of the eight line-ups 18. So:\nRD1(\u03c0chase,\u03a0o, wL\u0307, \u00b5) = 2 8\n7\n1\n8\n1 8 {7\u00d7 1 + 21\u00d7 0} = 1 4\nFor slot 2:\n18Note that only one predator trying to win is enough to chase a prey who wants to be chased.\nRD2(\u03c0chase,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R2(\u00b5[u\u0307 2\u2190 \u03c0chase]), R2(\u00b5[v\u0307 2\u2190 \u03c0chase])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206Q(R2(\u00b5[\u03c0lose, \u03c0chase, \u03c0lose, \u03c0lose]), R2(\u00b5[\u03c0lose, \u03c0chase, \u03c0lose, \u03c0win]))+\n+\u2206Q(R2(\u00b5[\u03c0lose, \u03c0chase, \u03c0lose, \u03c0lose]), R2(\u00b5[\u03c0lose, \u03c0chase, \u03c0win, \u03c0lose]))+\n...\n+ \u2206Q(R2(\u00b5[\u03c0win, \u03c0chase, \u03c0win, \u03c0lose]), R2(\u00b5[\u03c0win, \u03c0chase, \u03c0win, \u03c0win]))}\nFrom the 28 possible pairs of line-ups that we obtained, \u03c0chase tries to make equal the maximum number of pairs, while \u03c0win and \u03c0lose try to diverge the maximum number of pairs. In this case, the agents from \u03a0o can assure that three line-up patterns obtain the same result ((\u03c0lose, \u2217, \u03c0lose, \u03c0win), (\u03c0lose, \u2217, \u03c0win, \u03c0lose) and (\u03c0lose, \u2217, \u03c0win, \u03c0win)) and other three line-up patterns obtain a different result ((\u03c0win, \u2217, \u03c0lose, \u03c0lose), (\u03c0win, \u2217, \u03c0lose, \u03c0win) and (\u03c0win, \u2217, \u03c0win, \u03c0lose)), therefore the agent from \u03a0e (\u03c0chase) can only make equal five of the eight line-ups. So:\nRD2(\u03c0chase,\u03a0o, wL\u0307, \u00b5) = 2 8\n7\n1\n8\n1 8 {15\u00d7 1 + 13\u00d7 0} = 15 28\nFor slot 3:\nRD3(\u03c0chase,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22123 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R3(\u00b5[u\u0307 3\u2190 \u03c0chase]), R3(\u00b5[v\u0307 3\u2190 \u03c0chase])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206Q(R3(\u00b5[\u03c0lose, \u03c0lose, \u03c0chase, \u03c0lose]), R3(\u00b5[\u03c0lose, \u03c0lose, \u03c0chase, \u03c0win]))+\n+\u2206Q(R3(\u00b5[\u03c0lose, \u03c0lose, \u03c0chase, \u03c0lose]), R3(\u00b5[\u03c0lose, \u03c0win, \u03c0chase, \u03c0lose]))+\n...\n+ \u2206Q(R3(\u00b5[\u03c0win, \u03c0win, \u03c0chase, \u03c0lose]), R3(\u00b5[\u03c0win, \u03c0win, \u03c0chase, \u03c0win]))}\nFrom the 28 possible pairs of line-ups that we obtained, \u03c0chase tries to make equal the maximum number of pairs, while \u03c0win and \u03c0lose try to diverge the maximum number of pairs. In this case, the agents from \u03a0o can assure that three line-up patterns obtain the same result ((\u03c0lose, \u03c0lose, \u2217, \u03c0win), (\u03c0lose, \u03c0win, \u2217, \u03c0lose) and (\u03c0lose, \u03c0win, \u2217, \u03c0win)) and other three line-up patterns obtain a different result ((\u03c0win, \u03c0lose, \u2217, \u03c0lose), (\u03c0win, \u03c0lose, \u2217, \u03c0win) and (\u03c0win, \u03c0win, \u2217, \u03c0lose)), therefore the agent from \u03a0e (\u03c0chase) can only make equal five of the eight line-ups. So:\nRD3(\u03c0chase,\u03a0o, wL\u0307, \u00b5) = 2 8\n7\n1\n8\n1 8 {15\u00d7 1 + 13\u00d7 0} = 15 28\nAnd for slot 4:\nRD4(\u03c0chase,\u03a0o, wL\u0307, \u00b5) = \u03b7L\u03072 \u2211\nu\u0307,v\u0307\u2208L\u0307N(\u00b5)\u22124 (\u03a0o)|u\u0338\u0307=v\u0307\nwL\u0307(u\u0307)wL\u0307(v\u0307)\u2206Q(R4(\u00b5[u\u0307 4\u2190 \u03c0chase]), R4(\u00b5[v\u0307 4\u2190 \u03c0chase])) =\n= 2 8\n7\n1\n8\n1 8 {\u2206Q(R4(\u00b5[\u03c0lose, \u03c0lose, \u03c0lose, \u03c0chase]), R4(\u00b5[\u03c0lose, \u03c0lose, \u03c0win, \u03c0chase]))+\n+\u2206Q(R4(\u00b5[\u03c0lose, \u03c0lose, \u03c0lose, \u03c0chase]), R4(\u00b5[\u03c0lose, \u03c0win, \u03c0lose, \u03c0chase]))+\n...\n+ \u2206Q(R4(\u00b5[\u03c0win, \u03c0win, \u03c0lose, \u03c0chase]), R4(\u00b5[\u03c0win, \u03c0win, \u03c0win, \u03c0chase]))}\nFrom the 28 possible pairs of line-ups that we obtained, \u03c0chase tries to make equal the maximum number of pairs, while \u03c0win and \u03c0lose try to diverge the maximum number of pairs. In this case, the agents from\n\u03a0o can assure that three line-up patterns obtain the same result ((\u03c0lose, \u03c0lose, \u03c0win, \u2217), (\u03c0lose, \u03c0win, \u03c0lose, \u2217) and (\u03c0lose, \u03c0win, \u03c0win, \u2217)) and other three line-up patterns obtain a different result ((\u03c0win, \u03c0lose, \u03c0lose, \u2217), (\u03c0win, \u03c0lose, \u03c0win, \u2217) and (\u03c0win, \u03c0win, \u03c0lose, \u2217)), therefore the agent from \u03a0e (\u03c0chase) can only make equal five of the eight line-ups. So:\nRD4(\u03c0chase,\u03a0o, wL\u0307, \u00b5) = 2 8\n7\n1\n8\n1 8 {15\u00d7 1 + 13\u00d7 0} = 15 28\nAnd finally, we weight over the slots:\nRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= N(\u00b5)\u2211 i=1 wS(i, \u00b5)RDi(\u03c0chase,\u03a0o, wL\u0307, \u00b5) = = 1\n4 {RD1(\u03c0chase,\u03a0o, wL\u0307, \u00b5) +RD2(\u03c0chase,\u03a0o, wL\u0307, \u00b5)+\n+RD3(\u03c0chase,\u03a0o, wL\u0307, \u00b5) +RD4(\u03c0chase,\u03a0o, wL\u0307, \u00b5)} = = 1\n4\n{ 1\n4 +\n15 28 + 15 28 + 15 28\n} = 13\n28\nTherefore, predator-prey has Rightmin = 13 28 (as a lower approximation) for this property."}, {"heading": "C.3 Fine Discrimination", "text": "Now we move to the fine discrimination (FD) property. As given in section 4.4.1, we want to know if different evaluated agents obtain different expected average rewards when interacting in the environment. We use \u2206Q(a, b) = 1 if numbers a and b are equal and 0 otherwise.\nProposition 54. Generalmin for the fine discrimination (FD) property is equal to 0 for the predator-prey environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0u1, \u03c0u2} with uniform weight for w\u03a0e and \u03a0o = {\u03c0d} (a \u03c0u agent always performs Up and a \u03c0d agent always performs Down).\nFollowing definition 23, we obtain the FD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 22, we can calculate its FD value for each slot. We start with slot 1:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD1(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate both FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)} and FDi(\u03c02, \u03c01,\u03a0o, wL\u0307, \u00b5)} since they provide the same result, by calculating only FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)} and multiplying the result by 2.\nIn this case, we only need to calculate FD1(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value:\nFD1(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R1(\u00b5[l\u0307 1\u2190 \u03c0u1]), R1(\u00b5[l\u0307 1\u2190 \u03c0u2])) =\n= \u2206Q(R1(\u00b5[\u03c0u1, \u03c0d, \u03c0d, \u03c0d]), R1(\u00b5[\u03c0u2, \u03c0d, \u03c0d, \u03c0d]))\nHere, a \u03c0u agent will always perform Up and \u03c0d will always perform Down, so both agents in slot 1 (\u03c0u1 and \u03c0u2) will obtain the same expected average reward (1). So:\nFD1(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nFor slot 2: FD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD2(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD2(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5)\nAgain, we only need to calculate FD2(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value: FD2(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R2(\u00b5[l\u0307 2\u2190 \u03c0u1]), R2(\u00b5[l\u0307 2\u2190 \u03c0u2])) =\n= \u2206Q(R2(\u00b5[\u03c0d, \u03c0u1, \u03c0d, \u03c0d]), R2(\u00b5[\u03c0d, \u03c0u2, \u03c0d, \u03c0d]))\nAgain, a \u03c0u agent will always perform Up and \u03c0d will always perform Down, so both agents in slot 2 (\u03c0u1 and \u03c0u2) will obtain the same expected average reward (1). So:\nFD2(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nFor slot 3: FD3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD3(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD3(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5)\nAgain, we only need to calculate FD3(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value: FD3(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R3(\u00b5[l\u0307 3\u2190 \u03c0u1]), R3(\u00b5[l\u0307 3\u2190 \u03c0u2])) =\n= \u2206Q(R3(\u00b5[\u03c0d, \u03c0d, \u03c0u1, \u03c0d]), R3(\u00b5[\u03c0d, \u03c0d, \u03c0u2, \u03c0d]))\nAgain, a \u03c0u agent will always perform Up and \u03c0d will always perform Down, so both agents in slot 3 (\u03c0u1 and \u03c0u2) will obtain the same expected average reward (\u22121). So:\nFD3(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd for slot 4: FD4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD4(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD4(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5)\nAgain, we only need to calculate FD4(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value:\nFD4(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R4(\u00b5[l\u0307 4\u2190 \u03c0u1]), R4(\u00b5[l\u0307 4\u2190 \u03c0u2])) =\n= \u2206Q(R4(\u00b5[\u03c0d, \u03c0d, \u03c0d, \u03c0u1]), R4(\u00b5[\u03c0d, \u03c0d, \u03c0d, \u03c0u2]))\nAgain, a \u03c0u agent will always perform Up and \u03c0d will always perform Down, so both agents in slot 4 (\u03c0u1 and \u03c0u2) will obtain the same expected average reward (1). So:\nFD4(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, we weight over the slots:\nFD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)FDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 1\n4 {FD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + FD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+ FD3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + FD4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 1\n4 {0 + 0 + 0 + 0} = 0\nSince 0 is the lowest possible value for the fine discriminative property, therefore predator-prey hasGeneralmin = 0 for this property.\nProposition 55. Generalmax for the fine discrimination (FD) property is equal to 1 for the predator-prey environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0s, \u03c0r} with uniform weight for w\u03a0e and \u03a0o = {\u03c0s} (a \u03c0r agent always acts randomly and a \u03c0s agent always stays in the same cell19).\nFollowing definition 23, we obtain the FD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 22, we can calculate its FD value for each slot. We start with slot 1:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD1(\u03c0s, \u03c0r,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate both FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)} and FDi(\u03c02, \u03c01,\u03a0o, wL\u0307, \u00b5)} since they provide the same result, by calculating only FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)} and multiplying the result by 2.\nIn this case, we only need to calculate FD1(\u03c0s, \u03c0r,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value: FD1(\u03c0s, \u03c0r,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R1(\u00b5[l\u0307 1\u2190 \u03c0s]), R1(\u00b5[l\u0307 1\u2190 \u03c0r])) =\n= \u2206Q(R1(\u00b5[\u03c0s, \u03c0s, \u03c0s, \u03c0s]), R1(\u00b5[\u03c0r, \u03c0s, \u03c0s, \u03c0s]))\nHere, a \u03c0s agent will always stay in the same cell and \u03c0r will always act randomly. In this case, \u03c0s will never been chased but \u03c0r will have a possibility to be chased, so they will obtain different expected average rewards. So:\nFD1(\u03c0s, \u03c0r,\u03a0o, wL\u0307, \u00b5) = 1\n19Note that every cell has an action which is blocked by a block or a boundary, therefore an agent performing this action will stay at its current cell.\nTherefore:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 1 = 1\nFor slot 2: FD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD2(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD2(\u03c0s, \u03c0r,\u03a0o, wL\u0307, \u00b5)\nAgain, we only need to calculate FD2(\u03c0s, \u03c0r,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value: FD2(\u03c0s, \u03c0r,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R2(\u00b5[l\u0307 2\u2190 \u03c0s]), R2(\u00b5[l\u0307 2\u2190 \u03c0r])) =\n= \u2206Q(R2(\u00b5[\u03c0s, \u03c0s, \u03c0s, \u03c0s]), R2(\u00b5[\u03c0s, \u03c0r, \u03c0s, \u03c0s]))\nAgain, a \u03c0s agent will always stay in the same cell and \u03c0r will always act randomly. In this case, \u03c0s will never chase the prey but \u03c0r will have a possibility to chase the prey, so they will obtain different expected average rewards. So:\nFD2(\u03c0s, \u03c0r,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nFD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 1 = 1\nFor slot 3: FD3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD3(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD3(\u03c0s, \u03c0r,\u03a0o, wL\u0307, \u00b5)\nAgain, we only need to calculate FD3(\u03c0s, \u03c0r,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value: FD3(\u03c0s, \u03c0r,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R3(\u00b5[l\u0307 3\u2190 \u03c0s]), R3(\u00b5[l\u0307 3\u2190 \u03c0r])) =\n= \u2206Q(R3(\u00b5[\u03c0s, \u03c0s, \u03c0s, \u03c0s]), R3(\u00b5[\u03c0s, \u03c0s, \u03c0r, \u03c0s]))\nAgain, a \u03c0s agent will always stay in the same cell and \u03c0r will always act randomly. In this case, \u03c0s will never chase the prey but \u03c0r will have a possibility to chase the prey, so they will obtain different expected average rewards. So:\nFD3(\u03c0s, \u03c0r,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nFD3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 1 = 1\nAnd for slot 4: FD4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD4(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD4(\u03c0s, \u03c0r,\u03a0o, wL\u0307, \u00b5)\nAgain, we only need to calculate FD4(\u03c0s, \u03c0r,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value:\nFD4(\u03c0s, \u03c0r,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R4(\u00b5[l\u0307 4\u2190 \u03c0s]), R4(\u00b5[l\u0307 4\u2190 \u03c0r])) =\n= \u2206Q(R4(\u00b5[\u03c0s, \u03c0s, \u03c0s, \u03c0s]), R4(\u00b5[\u03c0s, \u03c0s, \u03c0s, \u03c0r]))\nAgain, a \u03c0s agent will always stay in the same cell and \u03c0r will always act randomly. In this case, \u03c0s will never chase the prey but \u03c0r will have a possibility to chase the prey, so they will obtain different expected average rewards. So:\nFD4(\u03c0s, \u03c0r,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nFD4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 1 = 1\nAnd finally, we weight over the slots:\nFD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)FDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 1\n4 {FD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + FD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+ FD3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + FD4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 1\n4 {1 + 1 + 1 + 1} = 1\nSince 1 is the highest possible value for the fine discriminative property, therefore predator-prey hasGeneralmax = 1 for this property.\nProposition 56. Leftmax for the fine discrimination (FD) property is equal to 0 for the predator-prey environment.\nProof. To find Leftmax (equation 43), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which maximises the property as much as possible while \u03a0o minimises it. Using \u03a0o = {\u03c0chase} (a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator) we find this situation no matter which pair \u27e8\u03a0e, w\u03a0e\u27e9 we use.\nFollowing definition 23, we obtain the FD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0e and w\u03a0e are instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 22, we can calculate its FD value for each slot. We start with slot 1:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate FD1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) for all pair of evaluated agents \u03c01, \u03c02 \u2208 \u03a0e|\u03c01 \u0338= \u03c02. We follow definition 21 to calculate this value for two figurative evaluated agents \u03c01 and \u03c02 from \u03a0e such that \u03c01 \u0338= \u03c02:\nFD1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R1(\u00b5[l\u0307 1\u2190 \u03c01]), R1(\u00b5[l\u0307 1\u2190 \u03c02])) =\n= \u2206Q(R1(\u00b5[\u03c01, \u03c0chase, \u03c0chase, \u03c0chase]), R1(\u00b5[\u03c02, \u03c0chase, \u03c0chase, \u03c0chase]))\nHere, the predators will coordinate to always chase the prey as seen in lemma 3. So no matter which agents \u03c01 and \u03c02 are we obtain:\nFD1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nFor slot 2: FD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD2(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate FD2(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) for all pair of evaluated agents \u03c01, \u03c02 \u2208 \u03a0e|\u03c01 \u0338= \u03c02. We follow definition 21 to calculate this value for two figurative evaluated agents \u03c01 and \u03c02 from \u03a0e such that \u03c01 \u0338= \u03c02:\nFD2(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R2(\u00b5[l\u0307 2\u2190 \u03c01]), R2(\u00b5[l\u0307 2\u2190 \u03c02])) =\n= \u2206Q(R2(\u00b5[\u03c0chase, \u03c01, \u03c0chase, \u03c0chase]), R2(\u00b5[\u03c0chase, \u03c02, \u03c0chase, \u03c0chase]))\nHere, the prey will always try to be chased, and also at least one predator is trying to chase they prey, therefore the prey will always been chased. So no matter which agents \u03c01 and \u03c02 are we obtain:\nFD2(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nFor slot 3: FD3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD3(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate FD3(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) for all pair of evaluated agents \u03c01, \u03c02 \u2208 \u03a0e|\u03c01 \u0338= \u03c02. We follow definition 21 to calculate this value for two figurative evaluated agents \u03c01 and \u03c02 from \u03a0e such that \u03c01 \u0338= \u03c02:\nFD3(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R3(\u00b5[l\u0307 3\u2190 \u03c01]), R3(\u00b5[l\u0307 3\u2190 \u03c02])) =\n= \u2206Q(R3(\u00b5[\u03c0chase, \u03c0chase, \u03c01, \u03c0chase]), R3(\u00b5[\u03c0chase, \u03c0chase, \u03c02, \u03c0chase]))\nHere, the prey will always try to be chased, and also at least one predator is trying to chase they prey, therefore the prey will always been chased. So no matter which agents \u03c01 and \u03c02 are we obtain:\nFD3(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nAnd for slot 4: FD4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD4(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate FD4(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) for all pair of evaluated agents \u03c01, \u03c02 \u2208 \u03a0e|\u03c01 \u0338= \u03c02. We follow definition 21 to calculate this value for two figurative evaluated agents \u03c01 and \u03c02 from \u03a0e such that \u03c01 \u0338= \u03c02:\nFD4(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R4(\u00b5[l\u0307 4\u2190 \u03c01]), R4(\u00b5[l\u0307 4\u2190 \u03c02])) =\n= \u2206Q(R4(\u00b5[\u03c0chase, \u03c0chase, \u03c0chase, \u03c01]), R4(\u00b5[\u03c0chase, \u03c0chase, \u03c0chase, \u03c02]))\nHere, the prey will always try to be chased, and also at least one predator is trying to chase they prey, therefore the prey will always been chased. So no matter which agents \u03c01 and \u03c02 are we obtain:\nFD4(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 0\nAnd finally, we weight over the slots:\nFD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)FDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)\n= 1\n4 {FD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + FD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+ FD3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + FD4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 1\n4 {0 + 0 + 0 + 0} = 0\nSo, for every pair \u27e8\u03a0e, w\u03a0e\u27e9 we obtain the same result:\n\u2200\u03a0e, w\u03a0e : FD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 0\nTherefore, predator-prey has Leftmax = 0 for this property.\nProposition 57. Rightmin for the fine discrimination (FD) property is equal to 0 for the predator-prey environment.\nProof. To find Rightmin (equation 44), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which minimises the property as much as possible while \u03a0o maximises it. Using \u03a0e = {\u03c0u1, \u03c0u2} with uniform weight for w\u03a0e (a \u03c0u agent always performs Up) we find this situation no matter which \u03a0o we use.\nFollowing definition 23, we obtain the FD value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 22, we can calculate its FD value for each slot. We start with slot 1:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD1(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD1(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate both FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)} and FDi(\u03c02, \u03c01,\u03a0o, wL\u0307, \u00b5)} since they provide the same result, by calculating only FDi(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5)} and multiplying the result by 2.\nIn this case, we only need to calculate FD1(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value:\nFD1(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R1(\u00b5[l\u0307 1\u2190 \u03c0u1]), R1(\u00b5[l\u0307 1\u2190 \u03c0u2]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22121 (\u03a0o) to calculate \u2206Q(R1(\u00b5[l\u0307 1\u2190 \u03c0u1]), R1(\u00b5[l\u0307 1\u2190 \u03c0u2])). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c01, \u03c02, \u03c03) from L\u0307N(\u00b5)\u22121 (\u03a0o):\n\u2206Q(R1(\u00b5[l\u0307 1\u2190 \u03c0u1]), R1(\u00b5[l\u0307 1\u2190 \u03c0u2])) = \u2206Q(R1(\u00b5[\u03c0u1, \u03c01, \u03c02, \u03c03]), R1(\u00b5[\u03c0u2, \u03c01, \u03c02, \u03c03]))\nA \u03c0u agent will always perform Up, so we obtain a situation where the other agents (any \u03c01, \u03c02, \u03c03) will be able to differentiate with which agent they are interacting, so they will not be able to change their distribution of action sequences depending on the behaviour of the agent in slot 1, obtaining both agents in slot 1 (\u03c0u1 and \u03c0u2) the same expected average reward. So:\nFD1(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nFor slot 2: FD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD2(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD2(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate FD2(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value:\nFD2(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R2(\u00b5[l\u0307 2\u2190 \u03c0u1]), R2(\u00b5[l\u0307 2\u2190 \u03c0u2]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22122 (\u03a0o) to calculate \u2206Q(R2(\u00b5[l\u0307 2\u2190 \u03c0u1]), R2(\u00b5[l\u0307 2\u2190 \u03c0u2])). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u2217, \u03c02, \u03c03) from L\u0307N(\u00b5)\u22122 (\u03a0o):\n\u2206Q(R2(\u00b5[l\u0307 2\u2190 \u03c0u1]), R2(\u00b5[l\u0307 2\u2190 \u03c0u2])) = \u2206Q(R2(\u00b5[\u03c01, \u03c0u1, \u03c02, \u03c03]), R2(\u00b5[\u03c01, \u03c0u2, \u03c02, \u03c03]))\nA \u03c0u agent will always perform Up, so we obtain a situation where the other agents (any \u03c01, \u03c02, \u03c03) will be able to differentiate with which agent they are interacting, so they will not be able to change their distribution of action sequences depending on the behaviour of the agent in slot 2, obtaining both agents in slot 2 (\u03c0u1 and \u03c0u2) the same expected average reward. So:\nFD2(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nFor slot 3: FD3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD3(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD3(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate FD3(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value:\nFD3(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R3(\u00b5[l\u0307 3\u2190 \u03c0u1]), R3(\u00b5[l\u0307 3\u2190 \u03c0u2]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22123 (\u03a0o) to calculate \u2206Q(R3(\u00b5[l\u0307 3\u2190 \u03c0u1]), R3(\u00b5[l\u0307 3\u2190 \u03c0u2])). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u03c02, \u2217, \u03c03) from L\u0307N(\u00b5)\u22123 (\u03a0o):\n\u2206Q(R3(\u00b5[l\u0307 3\u2190 \u03c0u1]), R3(\u00b5[l\u0307 3\u2190 \u03c0u2])) = \u2206Q(R3(\u00b5[\u03c01, \u03c02, \u03c0u1, \u03c03]), R3(\u00b5[\u03c01, \u03c02, \u03c0u2, \u03c03]))\nA \u03c0u agent will always perform Up, so we obtain a situation where the other agents (any \u03c01, \u03c02, \u03c03) will be able to differentiate with which agent they are interacting, so they will not be able to change their distribution\nof action sequences depending on the behaviour of the agent in slot 3, obtaining both agents in slot 3 (\u03c0u1 and \u03c0u2) the same expected average reward. So:\nFD3(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd for slot 4: FD4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a02 \u2211\n\u03c01,\u03c02\u2208\u03a0e|\u03c01 \u0338=\u03c02\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)FD4(\u03c01, \u03c02,\u03a0o, wL\u0307, \u00b5) =\n= 2 2\n1\n1\n2\n1 2 FD4(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate FD4(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5). We follow definition 21 to calculate this value:\nFD4(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124 (\u03a0o)\nwL\u0307(l\u0307)\u2206Q(R4(\u00b5[l\u0307 4\u2190 \u03c0u1]), R4(\u00b5[l\u0307 4\u2190 \u03c0u2]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22124 (\u03a0o) to calculate \u2206Q(R4(\u00b5[l\u0307 4\u2190 \u03c0u1]), R4(\u00b5[l\u0307 4\u2190 \u03c0u2])). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u03c02, \u03c03, \u2217) from L\u0307N(\u00b5)\u22124 (\u03a0o):\n\u2206Q(R4(\u00b5[l\u0307 4\u2190 \u03c0u1]), R4(\u00b5[l\u0307 4\u2190 \u03c0u2])) = \u2206Q(R4(\u00b5[\u03c01, \u03c02, \u03c03, \u03c0u1]), R4(\u00b5[\u03c01, \u03c02, \u03c03, \u03c0u2]))\nA \u03c0u agent will always perform Up, so we obtain a situation where the other agents (any \u03c01, \u03c02, \u03c03) will be able to differentiate with which agent they are interacting, so they will not be able to change their distribution of action sequences depending on the behaviour of the agent in slot 4, obtaining both agents in slot 4 (\u03c0u1 and \u03c0u2) the same expected average reward. So:\nFD4(\u03c0u1, \u03c0u2,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nFD4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, we weight over the slots:\nFD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = N(\u00b5)\u2211 i=1 wS(i, \u00b5)FDi(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)\n= 1\n4 {FD1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + FD2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+ FD3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + FD4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 1\n4 {0 + 0 + 0 + 0} = 0\nSo, for every \u03a0o we obtain the same result:\n\u2200\u03a0o : FD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 0\nTherefore, predator-prey has Rightmin = 0 for this property."}, {"heading": "C.4 Strict Total Grading", "text": "We arrive to the strict total grading (STG) property. As given in section 4.4.2, we want to know if there exists a strict ordering between the evaluated agents when interacting in the environment.\nTo simplify the notation, we use the next table to represent the STO: Ri(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c02]) < Rj(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c02]), Ri(\u00b5[l\u0307 i,j\u2190 \u03c02, \u03c03]) < Rj(\u00b5[l\u0307 i,j\u2190 \u03c02, \u03c03]) and Ri(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c03]) < Rj(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c03]).\nSlot i Slot j \u03c01 < \u03c02 \u03c02 < \u03c03 \u03c01 < \u03c03\nProposition 58. Generalmin for the strict total grading (STG) property is equal to 0 for the predator-prey environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0s1, \u03c0s2, \u03c0s3} with uniform weight for w\u03a0e and \u03a0o = {\u03c0x} (a \u03c0s agent always stays in the same cell20, and a \u03c0x agent acts stochastically with a probability of 1/ \u221a 2 to do not reach the upper left corner and a probability of 1\u2212 1/ \u221a 2 to reach this corner).\nFollowing definition 29, we obtain the STG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28, we can calculate its STG value for each pair of slots. We start with slots 1 and 2:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for STGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)STO1,2(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= STO1,2(\u03c0s1, \u03c0s2, \u03c0s3, (\u2217, \u2217, \u03c0x, \u03c0x), \u00b5)\nThe following table shows us STO1,2 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0s1 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 (\u03c0x) and 4 (\u03c0x) have a probability of (1/ \u221a 2)\u00d7 (1/ \u221a 2) = 1/2 to do not chase the prey (any \u03c0s) and the same probability 1\u22121/2 = 1/2 to chase the prey, making for both agents in slots 1 (any \u03c0s) and 2 (any \u03c0s) to obtain the same expected average reward (0). So:\n20Note that every cell has an action which is blocked by a block or a boundary, therefore an agent performing this action will stay at its current cell.\nSTG1,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 1 and 3:\nSTG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG1,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,3 (\u03a0o)\nwL\u0307(l\u0307)STO1,3(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= STO1,3(\u03c0s1, \u03c0s2, \u03c0s3, (\u2217, \u03c0x, \u2217, \u03c0x), \u00b5) The following table shows us STO1,3 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 \u03c0s1 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 (\u03c0x) and 4 (\u03c0x) have a probability of (1/ \u221a 2)\u00d7 (1/ \u221a 2) = 1/2 to do not chase the prey (any \u03c0s) and the same probability 1\u22121/2 = 1/2 to chase the prey, making for both agents in slots 1 (any \u03c0s) and 3 (any \u03c0s) to obtain the same expected average reward (0). So:\nSTG1,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 1 and 4:\nSTG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG1,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,4 (\u03a0o)\nwL\u0307(l\u0307)STO1,4(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= STO1,4(\u03c0s1, \u03c0s2, \u03c0s3, (\u2217, \u03c0x, \u03c0x, \u2217), \u00b5) The following table shows us STO1,4 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 \u03c0s1 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s3 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 (\u03c0x) and 3 (\u03c0x) have a probability of (1/ \u221a 2)\u00d7 (1/ \u221a 2) = 1/2 to do not chase the prey (any \u03c0s) and the same probability 1\u22121/2 = 1/2 to chase the prey, making for both agents in slots 1 (any \u03c0s) and 4 (any \u03c0s) to obtain the same expected average reward (0). So:\nSTG1,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 1:\nSTG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG2,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG2,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG2,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307)STO2,1(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= STO2,1(\u03c0s1, \u03c0s2, \u03c0s3, (\u2217, \u2217, \u03c0x, \u03c0x), \u00b5)\nThe following table shows us STO2,1 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0s1 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 (\u03c0x) and 4 (\u03c0x) have a probability of (1/ \u221a 2)\u00d7 (1/ \u221a 2) = 1/2 to do not chase the prey (any \u03c0s) and the same probability 1\u22121/2 = 1/2 to chase the prey, making for both agents in slots 2 (any \u03c0s) and 1 (any \u03c0s) to obtain the same expected average reward (0). So:\nSTG2,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 3:\nSTG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG2,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG2,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG2,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,3 (\u03a0o)\nwL\u0307(l\u0307)STO2,3(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= STO2,3(\u03c0s1, \u03c0s2, \u03c0s3, (\u03c0x, \u2217, \u2217, \u03c0x), \u00b5) The following table shows us STO2,3 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 \u03c0s1 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 (any \u03c0s) and 3 (any \u03c0s) share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG2,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 4:\nSTG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG2,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG2,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG2,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,4 (\u03a0o)\nwL\u0307(l\u0307)STO2,4(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= STO2,4(\u03c0s1, \u03c0s2, \u03c0s3, (\u03c0x, \u2217, \u03c0x, \u2217), \u00b5) The following table shows us STO2,4 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 \u03c0s1 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s3 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 (any \u03c0s) and 4 (any \u03c0s) share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG2,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 1:\nSTG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG3,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG3,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG3,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,1 (\u03a0o)\nwL\u0307(l\u0307)STO3,1(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= STO3,1(\u03c0s1, \u03c0s2, \u03c0s3, (\u2217, \u03c0x, \u2217, \u03c0x), \u00b5)\nThe following table shows us STO3,1 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 \u03c0s1 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s3 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 (\u03c0x) and 4 (\u03c0x) have a probability of (1/ \u221a 2)\u00d7 (1/ \u221a 2) = 1/2 to do not chase the prey (any \u03c0s) and the same probability 1\u22121/2 = 1/2 to chase the prey, making for both agents in slots 3 (any \u03c0s) and 1 (any \u03c0s) to obtain the same expected average reward (0). So:\nSTG3,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 2:\nSTG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG3,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG3,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG3,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,2 (\u03a0o)\nwL\u0307(l\u0307)STO3,2(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= STO3,2(\u03c0s1, \u03c0s2, \u03c0s3, (\u03c0x, \u2217, \u2217, \u03c0x), \u00b5)\nThe following table shows us STO3,2 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 \u03c0s1 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s3 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 (any \u03c0s) and 2 (any \u03c0s) share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG3,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 4:\nSTG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG3,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG3,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG3,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,4 (\u03a0o)\nwL\u0307(l\u0307)STO3,4(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= STO3,4(\u03c0s1, \u03c0s2, \u03c0s3, (\u03c0x, \u03c0x, \u2217, \u2217), \u00b5)\nThe following table shows us STO3,4 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 \u03c0s1 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s3 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 (any \u03c0s) and 4 (any \u03c0s) share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG3,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 4 and 1:\nSTG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG4,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG4,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG4,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,1 (\u03a0o)\nwL\u0307(l\u0307)STO4,1(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= STO4,1(\u03c0s1, \u03c0s2, \u03c0s3, (\u2217, \u03c0x, \u03c0x, \u2217), \u00b5)\nThe following table shows us STO4,1 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 \u03c0s1 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s3 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 (\u03c0x) and 3 (\u03c0x) have a probability of (1/ \u221a 2)\u00d7 (1/ \u221a 2) = 1/2 to do not chase the prey (any \u03c0s) and the same probability 1\u22121/2 = 1/2 to chase the prey, making for both agents in slots 4 (any \u03c0s) and 1 (any \u03c0s) to obtain the same expected average reward (0). So:\nSTG4,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 4 and 2:\nSTG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG4,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG4,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG4,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,2 (\u03a0o)\nwL\u0307(l\u0307)STO4,2(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= STO4,2(\u03c0s1, \u03c0s2, \u03c0s3, (\u03c0x, \u2217, \u03c0x, \u2217), \u00b5)\nThe following table shows us STO4,2 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 \u03c0s1 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s3 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 4 (any \u03c0s) and 2 (any \u03c0s) share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG4,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd for slots 4 and 3:\nSTG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG4,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG4,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG4,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,3 (\u03a0o)\nwL\u0307(l\u0307)STO4,3(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= STO4,3(\u03c0s1, \u03c0s2, \u03c0s3, (\u03c0x, \u03c0x, \u2217, \u2217), \u00b5)\nThe following table shows us STO4,3 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 \u03c0s1 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s2 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s3 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 \u03c0s2 < \u03c0s3 \u03c0s3 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1 \u03c0s1 < \u03c0s2 \u03c0s2 < \u03c0s1 \u03c0s2 < \u03c0s1 \u03c0s3 < \u03c0s2 \u03c0s3 < \u03c0s1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 4 (any \u03c0s) and 3 (any \u03c0s) share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG4,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, we weight over the slots:\nSTG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = 4\n3\n1\n4\n1 4 {STG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+ STG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + STG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + STG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + STG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + STG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 4\n3\n1\n4\n1 4 {12\u00d7 0} = 0\nSince 0 is the lowest possible value for the strict total grading property, therefore predator-prey hasGeneralmin = 0 for this property.\nProposition 59. Generalmax for the strict total grading (STG) property is equal to 1 2 for the predator-prey environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0x, \u03c0y, \u03c0z} with uniform weight for w\u03a0e and \u03a0o = {\u03c0s} (a \u03c0s agent always stays in the same cell21).\n\u03c0x behaves as shown in figure 21 when playing on each of the 4 slots.\n\u03c0y behaves as shown in figure 22 when playing on each of the 4 slots.\n\u03c0z behaves as shown in figure 23 when playing on each of the 4 slots. Following definition 29, we obtain the STG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28, we can calculate its STG value for each pair of slots. We start with slots 1 and 2:\n21Note that every cell has an action which is blocked by a block or a boundary, therefore an agent performing this action will stay at its current cell.\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for STGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)STO1,2(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= STO1,2(\u03c0x, \u03c0y, \u03c0z, (\u2217, \u2217, \u03c0s, \u03c0s), \u00b5)\nThe following table shows us STO1,2 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0x < \u03c0y \u03c0x < \u03c0z \u03c0y < \u03c0x \u03c0y < \u03c0z \u03c0z < \u03c0y \u03c0x < \u03c0z \u03c0x < \u03c0z \u03c0x < \u03c0y \u03c0y < \u03c0z Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0y < \u03c0z \u03c0z < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x \u03c0x < \u03c0y \u03c0y < \u03c0x \u03c0y < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x\nIt is possible to find a STO for the first permutation. In \u03c0x < \u03c0y, \u03c0x will stay at the upper left corner and \u03c0y will chase it in that cell in the 3rd iteration, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0y < \u03c0z, \u03c0y will reach the 2nd row 2nd column cell and \u03c0z will chase it in that cell in the 3rd iteration, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0x < \u03c0z, \u03c0x will stay at the upper left corner and \u03c0z will chase it in that cell in the 5th iteration, so they will obtain an expected average reward of \u22121 and 1 respectively. So:\nSTG1,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 3:\nSTG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG1,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,3 (\u03a0o)\nwL\u0307(l\u0307)STO1,3(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= STO1,3(\u03c0x, \u03c0y, \u03c0z, (\u2217, \u03c0s, \u2217, \u03c0s), \u00b5)\nThe following table shows us STO1,3 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 \u03c0x < \u03c0y \u03c0x < \u03c0z \u03c0y < \u03c0x \u03c0y < \u03c0z \u03c0z < \u03c0y \u03c0x < \u03c0z \u03c0x < \u03c0z \u03c0x < \u03c0y \u03c0y < \u03c0z Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 \u03c0y < \u03c0z \u03c0z < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x \u03c0x < \u03c0y \u03c0y < \u03c0x \u03c0y < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x\nIt is possible to find a STO for the first permutation. In \u03c0x < \u03c0y, \u03c0x will stay at the upper left corner and \u03c0y will chase it in that cell in the 3rd iteration, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0y < \u03c0z, \u03c0y will reach the 2nd row 2nd column cell and \u03c0z will chase it in that cell in the 3rd iteration, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0x < \u03c0z, \u03c0x will stay at the upper left corner and \u03c0z will chase it in that cell in the 5th iteration, so they will obtain an expected average reward of \u22121 and 1 respectively. So:\nSTG1,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSTG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 4:\nSTG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG1,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,4 (\u03a0o)\nwL\u0307(l\u0307)STO1,4(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= STO1,4(\u03c0x, \u03c0y, \u03c0z, (\u2217, \u03c0s, \u03c0s, \u2217), \u00b5)\nThe following table shows us STO1,4 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 \u03c0x < \u03c0y \u03c0x < \u03c0z \u03c0y < \u03c0x \u03c0y < \u03c0z \u03c0z < \u03c0y \u03c0x < \u03c0z \u03c0x < \u03c0z \u03c0x < \u03c0y \u03c0y < \u03c0z Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 \u03c0y < \u03c0z \u03c0z < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x \u03c0x < \u03c0y \u03c0y < \u03c0x \u03c0y < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x\nIt is possible to find a STO for the first permutation. In \u03c0x < \u03c0y, \u03c0x will stay at the upper left corner and \u03c0y will chase it in that cell in the 6th iteration, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0y < \u03c0z, \u03c0y will reach the 2nd row 2nd column cell and \u03c0z will chase it in that cell in the 4th iteration, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0x < \u03c0z, \u03c0x will stay at the upper left corner and \u03c0z will chase it in that cell in the 6th iteration, so they will obtain an expected average reward of \u22121 and 1 respectively. So:\nSTG1,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSTG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 1:\nSTG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG2,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG2,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG2,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307)STO2,1(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= STO2,1(\u03c0x, \u03c0y, \u03c0z, (\u2217, \u2217, \u03c0s, \u03c0s), \u00b5) The following table shows us STO2,1 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0x < \u03c0y \u03c0x < \u03c0z \u03c0y < \u03c0x \u03c0y < \u03c0z \u03c0z < \u03c0y \u03c0x < \u03c0z \u03c0x < \u03c0z \u03c0x < \u03c0y \u03c0y < \u03c0z Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0y < \u03c0z \u03c0z < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x \u03c0x < \u03c0y \u03c0y < \u03c0x \u03c0y < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x\nIt is possible to find a STO for the first permutation. In \u03c0x < \u03c0y, \u03c0y will reach the 2nd row 2nd column cell and \u03c0x will never chase it in that cell, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0y < \u03c0z, \u03c0z will reach the 2nd row 2nd column cell and \u03c0y will never chase it in that cell, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0x < \u03c0z, \u03c0z will reach the 2nd row 2nd column cell and \u03c0x will never chase it in that cell, so they will obtain an expected average reward of \u22121 and 1 respectively. So:\nSTG2,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSTG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 3:\nSTG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG2,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG2,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG2,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,3 (\u03a0o)\nwL\u0307(l\u0307)STO2,3(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= STO2,3(\u03c0x, \u03c0y, \u03c0z, (\u03c0s, \u2217, \u2217, \u03c0s), \u00b5)\nThe following table shows us STO2,3 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 \u03c0x < \u03c0y \u03c0x < \u03c0z \u03c0y < \u03c0x \u03c0y < \u03c0z \u03c0z < \u03c0y \u03c0x < \u03c0z \u03c0x < \u03c0z \u03c0x < \u03c0y \u03c0y < \u03c0z Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 \u03c0y < \u03c0z \u03c0z < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x \u03c0x < \u03c0y \u03c0y < \u03c0x \u03c0y < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 and 3 share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG2,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 4:\nSTG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG2,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG2,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG2,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,4 (\u03a0o)\nwL\u0307(l\u0307)STO2,4(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= STO2,4(\u03c0x, \u03c0y, \u03c0z, (\u03c0s, \u2217, \u03c0s, \u2217), \u00b5)\nThe following table shows us STO2,4 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 \u03c0x < \u03c0y \u03c0x < \u03c0z \u03c0y < \u03c0x \u03c0y < \u03c0z \u03c0z < \u03c0y \u03c0x < \u03c0z \u03c0x < \u03c0z \u03c0x < \u03c0y \u03c0y < \u03c0z Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 \u03c0y < \u03c0z \u03c0z < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x \u03c0x < \u03c0y \u03c0y < \u03c0x \u03c0y < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 and 4 share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG2,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 1:\nSTG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG3,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG3,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG3,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,1 (\u03a0o)\nwL\u0307(l\u0307)STO3,1(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= STO3,1(\u03c0x, \u03c0y, \u03c0z, (\u2217, \u03c0s, \u2217, \u03c0s), \u00b5) The following table shows us STO3,1 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 \u03c0x < \u03c0y \u03c0x < \u03c0z \u03c0y < \u03c0x \u03c0y < \u03c0z \u03c0z < \u03c0y \u03c0x < \u03c0z \u03c0x < \u03c0z \u03c0x < \u03c0y \u03c0y < \u03c0z Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 \u03c0y < \u03c0z \u03c0z < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x \u03c0x < \u03c0y \u03c0y < \u03c0x \u03c0y < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x\nIt is possible to find a STO for the first permutation. In \u03c0x < \u03c0y, \u03c0y will reach the 2nd row 2nd column cell and \u03c0x will never chase it in that cell, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0y < \u03c0z, \u03c0z will reach the 2nd row 2nd column cell and \u03c0y will never chase it in that cell, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0x < \u03c0z, \u03c0z will reach the 2nd row 2nd column cell and \u03c0x will never chase it in that cell, so they will obtain an expected average reward of \u22121 and 1 respectively. So:\nSTG3,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSTG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 2:\nSTG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG3,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG3,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG3,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,2 (\u03a0o)\nwL\u0307(l\u0307)STO3,2(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= STO3,2(\u03c0x, \u03c0y, \u03c0z, (\u03c0s, \u2217, \u2217, \u03c0s), \u00b5) The following table shows us STO3,2 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 \u03c0x < \u03c0y \u03c0x < \u03c0z \u03c0y < \u03c0x \u03c0y < \u03c0z \u03c0z < \u03c0y \u03c0x < \u03c0z \u03c0x < \u03c0z \u03c0x < \u03c0y \u03c0y < \u03c0z Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 \u03c0y < \u03c0z \u03c0z < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x \u03c0x < \u03c0y \u03c0y < \u03c0x \u03c0y < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 and 2 share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG3,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 4:\nSTG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG3,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG3,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG3,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,4 (\u03a0o)\nwL\u0307(l\u0307)STO3,4(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= STO3,4(\u03c0x, \u03c0y, \u03c0z, (\u03c0s, \u03c0s, \u2217, \u2217), \u00b5) The following table shows us STO3,4 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 \u03c0x < \u03c0y \u03c0x < \u03c0z \u03c0y < \u03c0x \u03c0y < \u03c0z \u03c0z < \u03c0y \u03c0x < \u03c0z \u03c0x < \u03c0z \u03c0x < \u03c0y \u03c0y < \u03c0z Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 \u03c0y < \u03c0z \u03c0z < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x \u03c0x < \u03c0y \u03c0y < \u03c0x \u03c0y < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 and 4 share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG3,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 4 and 1:\nSTG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG4,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG4,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG4,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,1 (\u03a0o)\nwL\u0307(l\u0307)STO4,1(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= STO4,1(\u03c0x, \u03c0y, \u03c0z, (\u2217, \u03c0s, \u03c0s, \u2217), \u00b5)\nThe following table shows us STO4,1 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 \u03c0x < \u03c0y \u03c0x < \u03c0z \u03c0y < \u03c0x \u03c0y < \u03c0z \u03c0z < \u03c0y \u03c0x < \u03c0z \u03c0x < \u03c0z \u03c0x < \u03c0y \u03c0y < \u03c0z Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 \u03c0y < \u03c0z \u03c0z < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x \u03c0x < \u03c0y \u03c0y < \u03c0x \u03c0y < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x\nIt is possible to find a STO for the first permutation. In \u03c0x < \u03c0y, \u03c0y will reach the 2nd row 2nd column cell and \u03c0x will never chase it in that cell, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0y < \u03c0z, \u03c0z will reach the 2nd row 2nd column cell and \u03c0y will never chase it in that cell, so they will obtain an expected average reward of \u22121 and 1 respectively. In \u03c0x < \u03c0z, \u03c0z will reach the 2nd row 2nd column cell and \u03c0x will never chase it in that cell, so they will obtain an expected average reward of \u22121 and 1 respectively. So:\nSTG4,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSTG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 4 and 2:\nSTG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG4,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG4,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG4,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,2 (\u03a0o)\nwL\u0307(l\u0307)STO4,2(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= STO4,2(\u03c0x, \u03c0y, \u03c0z, (\u03c0s, \u2217, \u03c0s, \u2217), \u00b5)\nThe following table shows us STO4,2 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 \u03c0x < \u03c0y \u03c0x < \u03c0z \u03c0y < \u03c0x \u03c0y < \u03c0z \u03c0z < \u03c0y \u03c0x < \u03c0z \u03c0x < \u03c0z \u03c0x < \u03c0y \u03c0y < \u03c0z Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 \u03c0y < \u03c0z \u03c0z < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x \u03c0x < \u03c0y \u03c0y < \u03c0x \u03c0y < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x\nBut, it is not possible to find a STO, since for every permutation the agents in slots 4 and 2 share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG4,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd for slots 4 and 3:\nSTG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG4,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG4,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG4,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,3 (\u03a0o)\nwL\u0307(l\u0307)STO4,3(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= STO4,3(\u03c0x, \u03c0y, \u03c0z, (\u03c0s, \u03c0s, \u2217, \u2217), \u00b5)\nThe following table shows us STO4,3 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 \u03c0x < \u03c0y \u03c0x < \u03c0z \u03c0y < \u03c0x \u03c0y < \u03c0z \u03c0z < \u03c0y \u03c0x < \u03c0z \u03c0x < \u03c0z \u03c0x < \u03c0y \u03c0y < \u03c0z Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 \u03c0y < \u03c0z \u03c0z < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x \u03c0x < \u03c0y \u03c0y < \u03c0x \u03c0y < \u03c0x \u03c0z < \u03c0y \u03c0z < \u03c0x\nBut, it is not possible to find a STO, since for every permutation the agents in slots 4 and 3 share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG4,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, we weight over the slots:\nSTG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = 4\n3\n1\n4\n1 4 {STG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+ STG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + STG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + STG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + STG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + STG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 4\n3\n1\n4\n1 4 {6\u00d7 1 + 6\u00d7 0} = 1 2\nSince 12 is the highest possible value that we can obtain for the strict total grading property, therefore predator-prey has Generalmax = 1 2 for this property.\nApproximation 2. Leftmax for the strict total grading (STG) property is equal to 1 4 (as a lower approximation) for the predator-prey environment.\nProof. To find Leftmax (equation 43), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which maximises the property as much as possible while \u03a0o minimises it. Using \u03a0e = {\u03c0chase1, \u03c0chase2, \u03c0chase3} with uniform weight for w\u03a0e (a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator) we find a lower approximation of this situation no matter which \u03a0o we use.\nFollowing definition 29, we obtain the STG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28, we can calculate its STG value for each pair of slots. We start with slots 1 and 2:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for STGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)STO1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22121,2 (\u03a0o) to calculate STO1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u2217, \u03c01, \u03c02) from L\u0307N(\u00b5)\u22121,2 (\u03a0o):\nSTO1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u2217, \u03c01, \u03c02), \u00b5)\nThe following table shows us STO1,2 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nIt is possible to find a STO for every permutation, since we always have \u03c0chasei < \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 2 will obtain an expected average reward of \u22121 and 1 respectively. Note that the choice of \u03a0o does not affect the result of STO1,2, so no matter which agents are in \u03a0o we obtain:\nSTG1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 3:\nSTG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,3 (\u03a0o)\nwL\u0307(l\u0307)STO1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22121,3 (\u03a0o) to calculate STO1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c01, \u2217, \u03c02) from L\u0307N(\u00b5)\u22121,3 (\u03a0o):\nSTO1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u03c01, \u2217, \u03c02), \u00b5)\nThe following table shows us STO1,3 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nAgain, it is possible to find a STO for every permutation, since we always have \u03c0chasei < \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 3 will obtain an expected average reward of \u22121 and 1 respectively. Note that the choice of \u03a0o does not affect the result of STO1,3, so no matter which agents are in \u03a0o we obtain:\nSTG1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSTG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 4:\nSTG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,4 (\u03a0o)\nwL\u0307(l\u0307)STO1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22121,4 (\u03a0o) to calculate STO1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c01, \u03c02, \u2217) from L\u0307N(\u00b5)\u22121,4 (\u03a0o):\nSTO1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u03c01, \u03c02, \u2217), \u00b5)\nThe following table shows us STO1,4 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nAgain, it is possible to find a STO for every permutation, since we always have \u03c0chasei < \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 4 will obtain an expected average reward of \u22121 and 1 respectively. Note that the choice of \u03a0o does not affect the result of STO1,4, so no matter which agents are in \u03a0o we obtain:\nSTG1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSTG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 1:\nSTG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG2,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307)STO2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22122,1 (\u03a0o) to calculate STO2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u2217, \u03c01, \u03c02) from L\u0307N(\u00b5)\u22122,1 (\u03a0o):\nSTO2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u2217, \u03c01, \u03c02), \u00b5)\nThe following table shows us STO2,1 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nIt is not possible to find a STO for any permutation, since we always have \u03c0chasei < \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 2 and 1 will obtain an expected average reward of 1 and \u22121 respectively. Note that the choice of \u03a0o does not affect the result of STO2,1, so no matter which agents are in \u03a0o we obtain:\nSTG2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 3:\nSTG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG2,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,3 (\u03a0o)\nwL\u0307(l\u0307)STO2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22122,3 (\u03a0o) to calculate STO2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u2217, \u2217, \u03c02) from L\u0307N(\u00b5)\u22122,3 (\u03a0o):\nSTO2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u2217, \u2217, \u03c02), \u00b5)\nThe following table shows us STO2,3 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 and 3 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nSTG2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 4:\nSTG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG2,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,4 (\u03a0o)\nwL\u0307(l\u0307)STO2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22122,4 (\u03a0o) to calculate STO2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u2217, \u03c02, \u2217) from L\u0307N(\u00b5)\u22122,4 (\u03a0o):\nSTO2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u2217, \u03c02, \u2217), \u00b5)\nThe following table shows us STO2,4 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 and 4 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nSTG2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 1:\nSTG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG3,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,1 (\u03a0o)\nwL\u0307(l\u0307)STO3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22123,1 (\u03a0o) to calculate STO3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c01, \u2217, \u03c02) from L\u0307N(\u00b5)\u22123,1 (\u03a0o):\nSTO3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u03c01, \u2217, \u03c02), \u00b5)\nThe following table shows us STO3,1 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nAgain, it is not possible to find a STO for any permutation, since we always have \u03c0chasei < \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 3 and 1 will obtain an expected average reward of 1 and \u22121 respectively. Note that the choice of \u03a0o does not affect the result of STO3,1, so no matter which agents are in \u03a0o we obtain:\nSTG3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 2:\nSTG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG3,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,2 (\u03a0o)\nwL\u0307(l\u0307)STO3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22123,2 (\u03a0o) to calculate STO3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u2217, \u2217, \u03c02) from L\u0307N(\u00b5)\u22123,2 (\u03a0o):\nSTO3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u2217, \u2217, \u03c02), \u00b5)\nThe following table shows us STO3,2 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 and 2 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nSTG3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 4:\nSTG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG3,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,4 (\u03a0o)\nwL\u0307(l\u0307)STO3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22123,4 (\u03a0o) to calculate STO3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u03c02, \u2217, \u2217) from L\u0307N(\u00b5)\u22123,4 (\u03a0o):\nSTO3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u03c02, \u2217, \u2217), \u00b5)\nThe following table shows us STO3,4 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 and 4 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nSTG3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 4 and 1:\nSTG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG4,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,1 (\u03a0o)\nwL\u0307(l\u0307)STO4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22124,1 (\u03a0o) to calculate STO4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c01, \u03c02, \u2217) from L\u0307N(\u00b5)\u22124,1 (\u03a0o):\nSTO4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u03c01, \u03c02, \u2217), \u00b5)\nThe following table shows us STO4,1 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nAgain, it is not possible to find a STO for any permutation, since we always have \u03c0chasei < \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 4 and 1 will obtain an expected average reward of 1 and \u22121 respectively. Note that the choice of \u03a0o does not affect the result of STO4,1, so no matter which agents are in \u03a0o we obtain:\nSTG4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 4 and 2:\nSTG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG4,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,2 (\u03a0o)\nwL\u0307(l\u0307)STO4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22124,2 (\u03a0o) to calculate STO4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u2217, \u03c02, \u2217) from L\u0307N(\u00b5)\u22124,2 (\u03a0o):\nSTO4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u2217, \u03c02, \u2217), \u00b5)\nThe following table shows us STO4,2 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 4 and 2 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nSTG4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd for slots 4 and 3:\nSTG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG4,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,3 (\u03a0o)\nwL\u0307(l\u0307)STO4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22124,3 (\u03a0o) to calculate STO4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u03c02, \u2217, \u2217) from L\u0307N(\u00b5)\u22124,3 (\u03a0o):\nSTO4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u03c02, \u2217, \u2217), \u00b5)\nThe following table shows us STO4,3 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 4 and 3 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nSTG4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, we weight over the slots:\nSTG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = 4\n3\n1\n4\n1 4 {STG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+ STG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + STG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + STG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + STG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + STG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 4\n3\n1\n4\n1 4 {3\u00d7 1 + 9\u00d7 0} = 1 4\nSo, for every \u03a0o we obtain the same result:\n\u2200\u03a0o : STG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 1\n4\nTherefore, predator-prey has Leftmax = 1 4 (as a lower approximation) for this property.\nApproximation 3. Rightmin for the strict total grading (STG) property is equal to 1 4 (as a higher approximation) for the predator-prey environment.\nProof. To find Rightmin (equation 44), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which minimises the property as much as possible while \u03a0o maximises it. Using \u03a0e = {\u03c0chase1, \u03c0chase2, \u03c0chase3} with uniform weight for w\u03a0e (a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator) we find a higher approximation of this situation no matter which \u03a0o we use.\nFollowing definition 29, we obtain the STG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28, we can calculate its STG value for each pair of slots. We start with slots 1 and 2:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for STGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)STO1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22121,2 (\u03a0o) to calculate STO1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u2217, \u03c01, \u03c02) from L\u0307N(\u00b5)\u22121,2 (\u03a0o):\nSTO1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u2217, \u03c01, \u03c02), \u00b5)\nThe following table shows us STO1,2 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nIt is possible to find a STO for every permutation, since we always have \u03c0chasei < \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 2 will obtain an expected average reward of \u22121 and 1 respectively. Note that the choice of \u03a0o does not affect the result of STO1,2, so no matter which agents are in \u03a0o we obtain:\nSTG1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSTG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 3:\nSTG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,3 (\u03a0o)\nwL\u0307(l\u0307)STO1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22121,3 (\u03a0o) to calculate STO1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c01, \u2217, \u03c02) from L\u0307N(\u00b5)\u22121,3 (\u03a0o):\nSTO1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u03c01, \u2217, \u03c02), \u00b5)\nThe following table shows us STO1,3 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nAgain, it is possible to find a STO for every permutation, since we always have \u03c0chasei < \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 3 will obtain an expected average reward of \u22121 and 1 respectively. Note that the choice of \u03a0o does not affect the result of STO1,3, so no matter which agents are in \u03a0o we obtain:\nSTG1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSTG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 4:\nSTG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG1,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,4 (\u03a0o)\nwL\u0307(l\u0307)STO1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22121,4 (\u03a0o) to calculate STO1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c01, \u03c02, \u2217) from L\u0307N(\u00b5)\u22121,4 (\u03a0o):\nSTO1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u03c01, \u03c02, \u2217), \u00b5) The following table shows us STO1,4 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nAgain, it is possible to find a STO for every permutation, since we always have \u03c0chasei < \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 4 will obtain an expected average reward of \u22121 and 1 respectively. Note that the choice of \u03a0o does not affect the result of STO1,4, so no matter which agents are in \u03a0o we obtain:\nSTG1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSTG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 1:\nSTG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG2,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307)STO2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22122,1 (\u03a0o) to calculate STO2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u2217, \u03c01, \u03c02) from L\u0307N(\u00b5)\u22122,1 (\u03a0o):\nSTO2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u2217, \u03c01, \u03c02), \u00b5) The following table shows us STO2,1 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nIt is not possible to find a STO for any permutation, since we always have \u03c0chasei < \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 2 and 1 will obtain an expected average reward of 1 and \u22121 respectively. Note that the choice of \u03a0o does not affect the result of STO2,1, so no matter which agents are in \u03a0o we obtain:\nSTG2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 3:\nSTG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG2,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,3 (\u03a0o)\nwL\u0307(l\u0307)STO2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22122,3 (\u03a0o) to calculate STO2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u2217, \u2217, \u03c02) from L\u0307N(\u00b5)\u22122,3 (\u03a0o):\nSTO2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u2217, \u2217, \u03c02), \u00b5) The following table shows us STO2,3 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 and 3 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nSTG2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 4:\nSTG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG2,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,4 (\u03a0o)\nwL\u0307(l\u0307)STO2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22122,4 (\u03a0o) to calculate STO2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u2217, \u03c02, \u2217) from L\u0307N(\u00b5)\u22122,4 (\u03a0o):\nSTO2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u2217, \u03c02, \u2217), \u00b5)\nThe following table shows us STO2,4 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 and 4 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nSTG2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 1:\nSTG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG3,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,1 (\u03a0o)\nwL\u0307(l\u0307)STO3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22123,1 (\u03a0o) to calculate STO3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c01, \u2217, \u03c02) from L\u0307N(\u00b5)\u22123,1 (\u03a0o):\nSTO3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u03c01, \u2217, \u03c02), \u00b5)\nThe following table shows us STO3,1 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nAgain, it is not possible to find a STO for any permutation, since we always have \u03c0chasei < \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 3 and 1 will obtain an expected average reward of 1 and \u22121 respectively. Note that the choice of \u03a0o does not affect the result of STO3,1, so no matter which agents are in \u03a0o we obtain:\nSTG3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 2:\nSTG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG3,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,2 (\u03a0o)\nwL\u0307(l\u0307)STO3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22123,2 (\u03a0o) to calculate STO3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u2217, \u2217, \u03c02) from L\u0307N(\u00b5)\u22123,2 (\u03a0o):\nSTO3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u2217, \u2217, \u03c02), \u00b5)\nThe following table shows us STO3,2 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 and 2 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nSTG3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 4:\nSTG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG3,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,4 (\u03a0o)\nwL\u0307(l\u0307)STO3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22123,4 (\u03a0o) to calculate STO3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u03c02, \u2217, \u2217) from L\u0307N(\u00b5)\u22123,4 (\u03a0o):\nSTO3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u03c02, \u2217, \u2217), \u00b5)\nThe following table shows us STO3,4 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 and 4 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nSTG3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 4 and 1:\nSTG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG4,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,1 (\u03a0o)\nwL\u0307(l\u0307)STO4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22124,1 (\u03a0o) to calculate STO4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c01, \u03c02, \u2217) from L\u0307N(\u00b5)\u22124,1 (\u03a0o):\nSTO4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u03c01, \u03c02, \u2217), \u00b5)\nThe following table shows us STO4,1 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nAgain, it is not possible to find a STO for any permutation, since we always have \u03c0chasei < \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 4 and 1 will obtain an expected average reward of 1 and \u22121 respectively. Note that the choice of \u03a0o does not affect the result of STO4,1, so no matter which agents are in \u03a0o we obtain:\nSTG4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 4 and 2:\nSTG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG4,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,2 (\u03a0o)\nwL\u0307(l\u0307)STO4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22124,2 (\u03a0o) to calculate STO4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u2217, \u03c02, \u2217) from L\u0307N(\u00b5)\u22124,2 (\u03a0o):\nSTO4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u2217, \u03c02, \u2217), \u00b5)\nThe following table shows us STO4,2 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 4 and 2 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nSTG4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd for slots 4 and 3:\nSTG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)STG4,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate STG4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 to calculate this value:\nSTG4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,3 (\u03a0o)\nwL\u0307(l\u0307)STO4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22124,3 (\u03a0o) to calculate STO4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u03c02, \u2217, \u2217) from L\u0307N(\u00b5)\u22124,3 (\u03a0o):\nSTO4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = STO4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u03c02, \u2217, \u2217), \u00b5)\nThe following table shows us STO4,3 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 \u03c0chase1 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase2 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase3 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 \u03c0chase2 < \u03c0chase3 \u03c0chase3 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1 \u03c0chase1 < \u03c0chase2 \u03c0chase2 < \u03c0chase1 \u03c0chase2 < \u03c0chase1 \u03c0chase3 < \u03c0chase2 \u03c0chase3 < \u03c0chase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 4 and 3 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nSTG4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nSTG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, we weight over the slots:\nSTG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)STGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = 4\n3\n1\n4\n1 4 {STG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+ STG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + STG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + STG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + STG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + STG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + STG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 4\n3\n1\n4\n1 4 {3\u00d7 1 + 9\u00d7 0} = 1 4\nSo, for every \u03a0o we obtain the same result:\n\u2200\u03a0o : STG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 1\n4\nTherefore, predator-prey has Rightmin = 1 4 (as a higher approximation) for this property."}, {"heading": "C.5 Partial Grading", "text": "Now we arrive to the partial grading (PG) property. As given in section 4.4.2, we want to know if there exists a partial ordering between the evaluated agents when interacting in the environment.\nTo simplify the notation, we use the next table to represent the PO: Ri(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c02]) \u2264 Rj(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c02]), Ri(\u00b5[l\u0307 i,j\u2190 \u03c02, \u03c03]) \u2264 Rj(\u00b5[l\u0307 i,j\u2190 \u03c02, \u03c03]) and Ri(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c03]) \u2264 Rj(\u00b5[l\u0307 i,j\u2190 \u03c01, \u03c03]).\nSlot i Slot j \u03c01 \u2264 \u03c02 \u03c02 \u2264 \u03c03 \u03c01 \u2264 \u03c03\nProposition 60. Generalmin for the partial grading (PG) property is equal to 1 2 for the predator-prey environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0x, \u03c0y, \u03c0z} with uniform weight for w\u03a0e and \u03a0o = {\u03c0s} (a \u03c0s agent always stays in the same cell22).\n\u03c0x behaves as shown in figure 24 when playing on each of the 4 slots. \u03c0y behaves as shown in figure 25 when playing on each of the 4 slots. \u03c0z behaves as shown in figure 26 when playing on each of the 4 slots. Following definition 30, we obtain the PG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28 (for PG), we can calculate its PG value for each pair of slots. We start with slots 1 and 2:\n22Note that every cell has an action which is blocked by a block or a boundary, therefore an agent performing this action will stay at its current cell.\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for PGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG1,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)PO1,2(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= PO1,2(\u03c0x, \u03c0y, \u03c0z, (\u2217, \u2217, \u03c0s, \u03c0s), \u00b5)\nThe following table shows us PO1,2 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0x \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0z Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x\nBut, it is not possible to find a PO for any permutation, since \u03c0x > \u03c0z, \u03c0y > \u03c0x and \u03c0z > \u03c0y. So:\nPG1,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 1 and 3:\nPG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG1,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG1,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,3 (\u03a0o)\nwL\u0307(l\u0307)PO1,3(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= PO1,3(\u03c0x, \u03c0y, \u03c0z, (\u2217, \u03c0s, \u2217, \u03c0s), \u00b5)\nThe following table shows us PO1,3 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 \u03c0x \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0z Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x\nBut, it is not possible to find a PO for any permutation, since \u03c0x > \u03c0z, \u03c0y > \u03c0x and \u03c0z > \u03c0y. So:\nPG1,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nPG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 1 and 4:\nPG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG1,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG1,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,4 (\u03a0o)\nwL\u0307(l\u0307)PO1,4(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= PO1,4(\u03c0x, \u03c0y, \u03c0z, (\u2217, \u03c0s, \u03c0s, \u2217), \u00b5)\nThe following table shows us PO1,4 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 \u03c0x \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0z Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x\nBut, it is not possible to find a PO for any permutation, since \u03c0x > \u03c0z, \u03c0y > \u03c0x and \u03c0z > \u03c0y. So:\nPG1,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nPG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 1:\nPG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG2,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG2,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG2,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307)PO2,1(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= PO2,1(\u03c0x, \u03c0y, \u03c0z, (\u2217, \u2217, \u03c0s, \u03c0s), \u00b5)\nThe following table shows us PO2,1 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0x \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0z Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x\nBut, it is not possible to find a PO for any permutation, since \u03c0x > \u03c0z, \u03c0y > \u03c0x and \u03c0z > \u03c0y. So:\nPG2,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nPG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 3:\nPG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG2,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG2,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG2,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,3 (\u03a0o)\nwL\u0307(l\u0307)PO2,3(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= PO2,3(\u03c0x, \u03c0y, \u03c0z, (\u03c0s, \u2217, \u2217, \u03c0s), \u00b5)\nThe following table shows us PO2,3 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 \u03c0x \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0z Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x\nIt is possible to find a PO for every permutation, since the agents in slots 2 and 3 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG2,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 4:\nPG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG2,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG2,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG2,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,4 (\u03a0o)\nwL\u0307(l\u0307)PO2,4(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= PO2,4(\u03c0x, \u03c0y, \u03c0z, (\u03c0s, \u2217, \u03c0s, \u2217), \u00b5)\nThe following table shows us PO2,4 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 \u03c0x \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0z Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x\nIt is possible to find a PO for every permutation, since the agents in slots 2 and 4 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG2,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 1:\nPG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG3,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG3,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG3,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,1 (\u03a0o)\nwL\u0307(l\u0307)PO3,1(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= PO3,1(\u03c0x, \u03c0y, \u03c0z, (\u2217, \u03c0s, \u2217, \u03c0s), \u00b5)\nThe following table shows us PO3,1 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 \u03c0x \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0z Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x\nBut, it is not possible to find a PO for any permutation, since \u03c0x > \u03c0z, \u03c0y > \u03c0x and \u03c0z > \u03c0y. So:\nPG3,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nPG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 2:\nPG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG3,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG3,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG3,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,2 (\u03a0o)\nwL\u0307(l\u0307)PO3,2(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= PO3,2(\u03c0x, \u03c0y, \u03c0z, (\u03c0s, \u2217, \u2217, \u03c0s), \u00b5)\nThe following table shows us PO3,2 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 \u03c0x \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0z Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x\nIt is possible to find a PO for every permutation, since the agents in slots 3 and 2 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG3,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 4:\nPG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG3,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG3,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG3,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,4 (\u03a0o)\nwL\u0307(l\u0307)PO3,4(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= PO3,4(\u03c0x, \u03c0y, \u03c0z, (\u03c0s, \u03c0s, \u2217, \u2217), \u00b5) The following table shows us PO3,4 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 \u03c0x \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0z Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x\nIt is possible to find a PO for every permutation, since the agents in slots 3 and 4 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG3,4(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 4 and 1:\nPG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG4,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG4,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG4,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,1 (\u03a0o)\nwL\u0307(l\u0307)PO4,1(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= PO4,1(\u03c0x, \u03c0y, \u03c0z, (\u2217, \u03c0s, \u03c0s, \u2217), \u00b5)\nThe following table shows us PO4,1 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 \u03c0x \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0z Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x\nBut, it is not possible to find a PO for any permutation, since \u03c0x > \u03c0z, \u03c0y > \u03c0x and \u03c0z > \u03c0y. So:\nPG4,1(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nPG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 4 and 2:\nPG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG4,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG4,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG4,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,2 (\u03a0o)\nwL\u0307(l\u0307)PO4,2(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= PO4,2(\u03c0x, \u03c0y, \u03c0z, (\u03c0s, \u2217, \u03c0s, \u2217), \u00b5)\nThe following table shows us PO4,2 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 \u03c0x \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0z Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x\nIt is possible to find a PO for every permutation, since the agents in slots 4 and 2 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG4,2(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd for slots 4 and 3:\nPG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG4,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG4,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG4,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,3 (\u03a0o)\nwL\u0307(l\u0307)PO4,3(\u03c0x, \u03c0y, \u03c0z, l\u0307, \u00b5) =\n= PO4,3(\u03c0x, \u03c0y, \u03c0z, (\u03c0s, \u03c0s, \u2217, \u2217), \u00b5) The following table shows us PO4,3 for all the permutations of \u03c0x, \u03c0y, \u03c0z.\nSlot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 \u03c0x \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0y \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0z \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0z Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 \u03c0y \u2264 \u03c0z \u03c0z \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x \u03c0x \u2264 \u03c0y \u03c0y \u2264 \u03c0x \u03c0y \u2264 \u03c0x \u03c0z \u2264 \u03c0y \u03c0z \u2264 \u03c0x\nIt is possible to find a PO for every permutation, since the agents in slots 4 and 3 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG4,3(\u03c0x, \u03c0y, \u03c0z,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, we weight over the slots:\nPG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = 4\n3\n1\n4\n1 4 {PG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+ PG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + PG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + PG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + PG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + PG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 4\n3\n1\n4\n1 4 {6\u00d7 1 + 6\u00d7 0} = 1 2\nSince 12 is the lowest possible value that we can obtain for the partial grading property, therefore predatorprey has Generalmin = 1 2 for this property.\nProposition 61. Generalmax for the partial grading (PG) property is equal to 1 for the predator-prey environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0s1, \u03c0s2, \u03c0s3} with uniform weight for w\u03a0e and \u03a0o = {\u03c0x} (a \u03c0s agent always stays in the same cell23, and a \u03c0x agent acts stochastically with a probability of 1/ \u221a 2 to do not reach the upper left corner and a probability of 1\u2212 1/ \u221a 2 to reach this corner).\nFollowing definition 30, we obtain the PG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28 (for PG), we can calculate its PG value for each pair of slots. We start with slots 1 and 2:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for PGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG1,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)PO1,2(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= PO1,2(\u03c0s1, \u03c0s2, \u03c0s3, (\u2217, \u2217, \u03c0x, \u03c0x), \u00b5) The following table shows us PO1,2 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0s1 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1\nIt is possible to find a PO for every permutation, since the agents in slots 3 (\u03c0x) and 4 (\u03c0x) have a probability of (1/ \u221a 2)\u00d7 (1/ \u221a 2) = 1/2 to do not chase the prey (any \u03c0s) and the same probability 1\u2212 1/2 = 1/2 to chase the prey, making for both agents in slots 1 (any \u03c0s) and 2 (any \u03c0s) to obtain the same expected average reward (0). So:\nPG1,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 3:\nPG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\n23Note that every cell has an action which is blocked by a block or a boundary, therefore an agent performing this action will stay at its current cell.\nIn this case, we only need to calculate PG1,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG1,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,3 (\u03a0o)\nwL\u0307(l\u0307)PO1,3(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= PO1,3(\u03c0s1, \u03c0s2, \u03c0s3, (\u2217, \u03c0x, \u2217, \u03c0x), \u00b5)\nThe following table shows us PO1,3 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 \u03c0s1 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1\nIt is possible to find a PO for every permutation, since the agents in slots 2 (\u03c0x) and 4 (\u03c0x) have a probability of (1/ \u221a 2)\u00d7 (1/ \u221a 2) = 1/2 to do not chase the prey (any \u03c0s) and the same probability 1\u2212 1/2 = 1/2 to chase the prey, making for both agents in slots 1 (any \u03c0s) and 3 (any \u03c0s) to obtain the same expected average reward (0). So:\nPG1,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 4:\nPG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG1,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG1,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,4 (\u03a0o)\nwL\u0307(l\u0307)PO1,4(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= PO1,4(\u03c0s1, \u03c0s2, \u03c0s3, (\u2217, \u03c0x, \u03c0x, \u2217), \u00b5)\nThe following table shows us PO1,4 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 \u03c0s1 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s3 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1\nIt is possible to find a PO for every permutation, since the agents in slots 2 (\u03c0x) and 3 (\u03c0x) have a probability of (1/ \u221a 2)\u00d7 (1/ \u221a 2) = 1/2 to do not chase the prey (any \u03c0s) and the same probability 1\u2212 1/2 = 1/2 to chase the prey, making for both agents in slots 1 (any \u03c0s) and 4 (any \u03c0s) to obtain the same expected average reward (0). So:\nPG1,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 1:\nPG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG2,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG2,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG2,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307)PO2,1(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= PO2,1(\u03c0s1, \u03c0s2, \u03c0s3, (\u2217, \u2217, \u03c0x, \u03c0x), \u00b5)\nThe following table shows us PO2,1 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0s1 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1\nIt is possible to find a PO for every permutation, since the agents in slots 3 (\u03c0x) and 4 (\u03c0x) have a probability of (1/ \u221a 2)\u00d7 (1/ \u221a 2) = 1/2 to do not chase the prey (any \u03c0s) and the same probability 1\u2212 1/2 = 1/2 to chase the prey, making for both agents in slots 2 (any \u03c0s) and 1 (any \u03c0s) to obtain the same expected average reward (0). So:\nPG2,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 3:\nPG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG2,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG2,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG2,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,3 (\u03a0o)\nwL\u0307(l\u0307)PO2,3(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= PO2,3(\u03c0s1, \u03c0s2, \u03c0s3, (\u03c0x, \u2217, \u2217, \u03c0x), \u00b5)\nThe following table shows us PO2,3 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 \u03c0s1 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1\nIt is possible to find a PO for every permutation, since the agents in slots 2 and 3 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG2,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 4:\nPG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG2,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG2,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG2,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,4 (\u03a0o)\nwL\u0307(l\u0307)PO2,4(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= PO2,4(\u03c0s1, \u03c0s2, \u03c0s3, (\u03c0x, \u2217, \u03c0x, \u2217), \u00b5)\nThe following table shows us PO2,4 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 \u03c0s1 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s3 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1\nIt is possible to find a PO for every permutation, since the agents in slots 2 and 4 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG2,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 1:\nPG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG3,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG3,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG3,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,1 (\u03a0o)\nwL\u0307(l\u0307)PO3,1(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= PO3,1(\u03c0s1, \u03c0s2, \u03c0s3, (\u2217, \u03c0x, \u2217, \u03c0x), \u00b5)\nThe following table shows us PO3,1 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 \u03c0s1 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s3 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1\nIt is possible to find a PO for every permutation, since the agents in slots 2 (\u03c0x) and 4 (\u03c0x) have a probability of (1/ \u221a 2)\u00d7 (1/ \u221a 2) = 1/2 to do not chase the prey (any \u03c0s) and the same probability 1\u2212 1/2 = 1/2 to chase the prey, making for both agents in slots 3 (any \u03c0s) and 1 (any \u03c0s) to obtain the same expected average reward (0). So:\nPG3,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 2:\nPG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG3,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG3,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG3,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,2 (\u03a0o)\nwL\u0307(l\u0307)PO3,2(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= PO3,2(\u03c0s1, \u03c0s2, \u03c0s3, (\u03c0x, \u2217, \u2217, \u03c0x), \u00b5)\nThe following table shows us PO3,2 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 \u03c0s1 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s3 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1\nIt is possible to find a PO for every permutation, since the agents in slots 3 and 2 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG3,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 4:\nPG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG3,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG3,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG3,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,4 (\u03a0o)\nwL\u0307(l\u0307)PO3,4(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= PO3,4(\u03c0s1, \u03c0s2, \u03c0s3, (\u03c0x, \u03c0x, \u2217, \u2217), \u00b5) The following table shows us PO3,4 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 \u03c0s1 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s3 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1\nIt is possible to find a PO for every permutation, since the agents in slots 3 and 4 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG3,4(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 4 and 1:\nPG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG4,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG4,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG4,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,1 (\u03a0o)\nwL\u0307(l\u0307)PO4,1(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= PO4,1(\u03c0s1, \u03c0s2, \u03c0s3, (\u2217, \u03c0x, \u03c0x, \u2217), \u00b5)\nThe following table shows us PO4,1 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 \u03c0s1 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s3 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1\nIt is possible to find a PO for every permutation, since the agents in slots 2 (\u03c0x) and 3 (\u03c0x) have a probability of (1/ \u221a 2)\u00d7 (1/ \u221a 2) = 1/2 to do not chase the prey (any \u03c0s) and the same probability 1\u2212 1/2 = 1/2 to chase the prey, making for both agents in slots 4 (any \u03c0s) and 1 (any \u03c0s) to obtain the same expected average reward (0). So:\nPG4,1(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 4 and 2:\nPG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG4,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG4,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG4,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,2 (\u03a0o)\nwL\u0307(l\u0307)PO4,2(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= PO4,2(\u03c0s1, \u03c0s2, \u03c0s3, (\u03c0x, \u2217, \u03c0x, \u2217), \u00b5)\nThe following table shows us PO4,2 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 \u03c0s1 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s3 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1\nIt is possible to find a PO for every permutation, since the agents in slots 4 and 2 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG4,2(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd for slots 4 and 3:\nPG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG4,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG4,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG4,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,3 (\u03a0o)\nwL\u0307(l\u0307)PO4,3(\u03c0s1, \u03c0s2, \u03c0s3, l\u0307, \u00b5) =\n= PO4,3(\u03c0s1, \u03c0s2, \u03c0s3, (\u03c0x, \u03c0x, \u2217, \u2217), \u00b5)\nThe following table shows us PO4,3 for all the permutations of \u03c0s1, \u03c0s2, \u03c0s3.\nSlot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 \u03c0s1 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s2 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s3 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 \u03c0s2 \u2264 \u03c0s3 \u03c0s3 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1 \u03c0s1 \u2264 \u03c0s2 \u03c0s2 \u2264 \u03c0s1 \u03c0s2 \u2264 \u03c0s1 \u03c0s3 \u2264 \u03c0s2 \u03c0s3 \u2264 \u03c0s1\nIt is possible to find a PO for every permutation, since the agents in slots 4 and 3 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG4,3(\u03c0s1, \u03c0s2, \u03c0s3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, we weight over the slots:\nPG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = 4\n3\n1\n4\n1 4 {PG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+ PG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + PG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + PG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + PG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + PG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 4\n3\n1\n4\n1 4 {12\u00d7 1} = 1\nSince 1 is the highest possible value for the partial grading property, therefore predator-prey hasGeneralmax = 1 for this property.\nApproximation 4. Leftmax for the partial grading (PG) property is equal to 3 4 (as a lower approximation) for the predator-prey environment.\nProof. To find Leftmax (equation 43), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which maximises the property as much as possible while \u03a0o minimises it. Using \u03a0e = {\u03c0chase1, \u03c0chase2, \u03c0chase3} with uniform weight for w\u03a0e (a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator) we find a lower approximation of this situation no matter which \u03a0o we use.\nFollowing definition 30, we obtain the PG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28 (for PG), we can calculate its PG value for each pair of slots. We start with slots 1 and 2:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for PGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)PO1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22121,2 (\u03a0o) to calculate PO1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u2217, \u03c01, \u03c02) from L\u0307N(\u00b5)\u22121,2 (\u03a0o):\nPO1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u2217, \u03c01, \u03c02), \u00b5)\nThe following table shows us PO1,2 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nIt is possible to find a PO for every permutation, since we always have \u03c0chasei \u2264 \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 2 will obtain an expected average reward of \u22121 and 1 respectively. Note that the choice of \u03a0o does not affect the result of PO1,2, so no matter which agents are in \u03a0o we obtain:\nPG1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 3:\nPG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,3 (\u03a0o)\nwL\u0307(l\u0307)PO1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22121,3 (\u03a0o) to calculate PO1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c01, \u2217, \u03c02) from L\u0307N(\u00b5)\u22121,3 (\u03a0o):\nPO1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u03c01, \u2217, \u03c02), \u00b5)\nThe following table shows us PO1,3 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nAgain, it is possible to find a PO for every permutation, since we always have \u03c0chasei \u2264 \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 3 will obtain an expected average reward of \u22121 and 1 respectively. Note that the choice of \u03a0o does not affect the result of PO1,3, so no matter which agents are in \u03a0o we obtain:\nPG1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 4:\nPG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,4 (\u03a0o)\nwL\u0307(l\u0307)PO1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22121,4 (\u03a0o) to calculate PO1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c01, \u03c02, \u2217) from L\u0307N(\u00b5)\u22121,4 (\u03a0o):\nPO1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u03c01, \u03c02, \u2217), \u00b5)\nThe following table shows us PO1,4 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nAgain, it is possible to find a PO for every permutation, since we always have \u03c0chasei \u2264 \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 4 will obtain an expected average reward of \u22121 and 1 respectively. Note that the choice of \u03a0o does not affect the result of PO1,4, so no matter which agents are in \u03a0o we obtain:\nPG1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 1:\nPG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG2,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307)PO2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22122,1 (\u03a0o) to calculate PO2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u2217, \u03c01, \u03c02) from L\u0307N(\u00b5)\u22122,1 (\u03a0o):\nPO2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u2217, \u03c01, \u03c02), \u00b5)\nThe following table shows us PO2,1 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nIt is not possible to find a PO for any permutation, since we always have \u03c0chasei \u2264 \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 2 and 1 will obtain an expected average reward of 1 and \u22121 respectively. Note that the choice of \u03a0o does not affect the result of PO2,1, so no matter which agents are in \u03a0o we obtain:\nPG2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nPG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 3:\nPG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG2,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,3 (\u03a0o)\nwL\u0307(l\u0307)PO2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22122,3 (\u03a0o) to calculate PO2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u2217, \u2217, \u03c02) from L\u0307N(\u00b5)\u22122,3 (\u03a0o):\nPO2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u2217, \u2217, \u03c02), \u00b5)\nThe following table shows us PO2,3 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nIt is possible to find a PO for every permutation, since the agents in slots 2 and 3 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nPG2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 4:\nPG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG2,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,4 (\u03a0o)\nwL\u0307(l\u0307)PO2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22122,4 (\u03a0o) to calculate PO2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u2217, \u03c02, \u2217) from L\u0307N(\u00b5)\u22122,4 (\u03a0o):\nPO2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u2217, \u03c02, \u2217), \u00b5)\nThe following table shows us PO2,4 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nIt is possible to find a PO for every permutation, since the agents in slots 2 and 4 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nPG2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 1:\nPG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG3,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,1 (\u03a0o)\nwL\u0307(l\u0307)PO3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22123,1 (\u03a0o) to calculate PO3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c01, \u2217, \u03c02) from L\u0307N(\u00b5)\u22123,1 (\u03a0o):\nPO3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u03c01, \u2217, \u03c02), \u00b5)\nThe following table shows us PO3,1 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nAgain, it is not possible to find a PO for any permutation, since we always have \u03c0chasei \u2264 \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 3 and 1 will obtain an expected average reward of 1 and \u22121 respectively. Note that the choice of \u03a0o does not affect the result of PO3,1, so no matter which agents are in \u03a0o we obtain:\nPG3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nPG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 0\nFor slots 3 and 2:\nPG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG3,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,2 (\u03a0o)\nwL\u0307(l\u0307)PO3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22123,2 (\u03a0o) to calculate PO3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u2217, \u2217, \u03c02) from L\u0307N(\u00b5)\u22123,2 (\u03a0o):\nPO3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u2217, \u2217, \u03c02), \u00b5)\nThe following table shows us PO3,2 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nIt is possible to find a PO for every permutation, since the agents in slots 3 and 2 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nPG3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 4:\nPG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG3,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,4 (\u03a0o)\nwL\u0307(l\u0307)PO3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22123,4 (\u03a0o) to calculate PO3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u03c02, \u2217, \u2217) from L\u0307N(\u00b5)\u22123,4 (\u03a0o):\nPO3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u03c02, \u2217, \u2217), \u00b5)\nThe following table shows us PO3,4 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nIt is possible to find a PO for every permutation, since the agents in slots 3 and 4 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nPG3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 4 and 1:\nPG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG4,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,1 (\u03a0o)\nwL\u0307(l\u0307)PO4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22124,1 (\u03a0o) to calculate PO4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c01, \u03c02, \u2217) from L\u0307N(\u00b5)\u22124,1 (\u03a0o):\nPO4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u03c01, \u03c02, \u2217), \u00b5)\nThe following table shows us PO4,1 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nAgain, it is not possible to find a PO for any permutation, since we always have \u03c0chasei \u2264 \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 4 and 1 will obtain an expected average reward of 1 and \u22121 respectively. Note that the choice of \u03a0o does not affect the result of PO4,1, so no matter which agents are in \u03a0o we obtain:\nPG4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nPG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 0\nFor slots 4 and 2:\nPG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG4,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,2 (\u03a0o)\nwL\u0307(l\u0307)PO4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22124,2 (\u03a0o) to calculate PO4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u2217, \u03c02, \u2217) from L\u0307N(\u00b5)\u22124,2 (\u03a0o):\nPO4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u2217, \u03c02, \u2217), \u00b5)\nThe following table shows us PO4,2 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nIt is possible to find a PO for every permutation, since the agents in slots 4 and 2 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nPG4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd for slots 4 and 3:\nPG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG4,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,3 (\u03a0o)\nwL\u0307(l\u0307)PO4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22124,3 (\u03a0o) to calculate PO4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u03c02, \u2217, \u2217) from L\u0307N(\u00b5)\u22124,3 (\u03a0o):\nPO4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u03c02, \u2217, \u2217), \u00b5)\nThe following table shows us PO4,3 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nIt is possible to find a PO for every permutation, since the agents in slots 4 and 3 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nPG4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, we weight over the slots:\nPG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = 4\n3\n1\n4\n1 4 {PG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+ PG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + PG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + PG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + PG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + PG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 4\n3\n1\n4\n1 4 {9\u00d7 1 + 3\u00d7 0} = 3 4\nSo, for every \u03a0o we obtain the same result:\n\u2200\u03a0o : PG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 3\n4\nTherefore, predator-prey has Leftmax = 3 4 (as a lower approximation) for this property.\nApproximation 5. Rightmin for the partial grading (PG) property is equal to 3 4 (as a higher approximation) for the predator-prey environment.\nProof. To find Rightmin (equation 44), we need to find a pair \u27e8\u03a0e, w\u03a0e\u27e9 which minimises the property as much as possible while \u03a0o maximises it. Using \u03a0e = {\u03c0chase1, \u03c0chase2, \u03c0chase3} with uniform weight for w\u03a0e (a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator) we find a higher approximation of this situation no matter which \u03a0o we use.\nFollowing definition 30, we obtain the PG value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0o is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28 (for PG), we can calculate its PG value for each pair of slots. We start with slots 1 and 2:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nNote that we avoided to calculate all the permutations of \u03c01, \u03c02, \u03c03 for PGi,j(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307)PO1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22121,2 (\u03a0o) to calculate PO1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u2217, \u03c01, \u03c02) from L\u0307N(\u00b5)\u22121,2 (\u03a0o):\nPO1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u2217, \u03c01, \u03c02), \u00b5)\nThe following table shows us PO1,2 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nIt is possible to find a PO for every permutation, since we always have \u03c0chasei \u2264 \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 2 will obtain an expected average reward of \u22121 and 1 respectively. Note that the choice of \u03a0o does not affect the result of PO1,2, so no matter which agents are in \u03a0o we obtain:\nPG1,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 3:\nPG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,3 (\u03a0o)\nwL\u0307(l\u0307)PO1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22121,3 (\u03a0o) to calculate PO1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c01, \u2217, \u03c02) from L\u0307N(\u00b5)\u22121,3 (\u03a0o):\nPO1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u03c01, \u2217, \u03c02), \u00b5)\nThe following table shows us PO1,3 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nAgain, it is possible to find a PO for every permutation, since we always have \u03c0chasei \u2264 \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 3 will obtain an expected average reward of \u22121 and 1 respectively. Note that the choice of \u03a0o does not affect the result of PO1,3, so no matter which agents are in \u03a0o we obtain:\nPG1,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 4:\nPG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG1,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,4 (\u03a0o)\nwL\u0307(l\u0307)PO1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22121,4 (\u03a0o) to calculate PO1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c01, \u03c02, \u2217) from L\u0307N(\u00b5)\u22121,4 (\u03a0o):\nPO1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u03c01, \u03c02, \u2217), \u00b5) The following table shows us PO1,4 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nAgain, it is possible to find a PO for every permutation, since we always have \u03c0chasei \u2264 \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 4 will obtain an expected average reward of \u22121 and 1 respectively. Note that the choice of \u03a0o does not affect the result of PO1,4, so no matter which agents are in \u03a0o we obtain:\nPG1,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 1:\nPG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG2,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307)PO2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22122,1 (\u03a0o) to calculate PO2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u2217, \u03c01, \u03c02) from L\u0307N(\u00b5)\u22122,1 (\u03a0o):\nPO2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u2217, \u03c01, \u03c02), \u00b5) The following table shows us PO2,1 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nIt is not possible to find a PO for any permutation, since we always have \u03c0chasei \u2264 \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 2 and 1 will obtain an expected average reward of 1 and \u22121 respectively. Note that the choice of \u03a0o does not affect the result of PO2,1, so no matter which agents are in \u03a0o we obtain:\nPG2,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nPG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 3:\nPG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG2,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,3 (\u03a0o)\nwL\u0307(l\u0307)PO2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22122,3 (\u03a0o) to calculate PO2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u2217, \u2217, \u03c02) from L\u0307N(\u00b5)\u22122,3 (\u03a0o):\nPO2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u2217, \u2217, \u03c02), \u00b5) The following table shows us PO2,3 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nIt is possible to find a PO for every permutation, since the agents in slots 2 and 3 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nPG2,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 4:\nPG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG2,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,4 (\u03a0o)\nwL\u0307(l\u0307)PO2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22122,4 (\u03a0o) to calculate PO2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u2217, \u03c02, \u2217) from L\u0307N(\u00b5)\u22122,4 (\u03a0o):\nPO2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u2217, \u03c02, \u2217), \u00b5)\nThe following table shows us PO2,4 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nIt is possible to find a PO for every permutation, since the agents in slots 2 and 4 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nPG2,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 1:\nPG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG3,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,1 (\u03a0o)\nwL\u0307(l\u0307)PO3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22123,1 (\u03a0o) to calculate PO3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c01, \u2217, \u03c02) from L\u0307N(\u00b5)\u22123,1 (\u03a0o):\nPO3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u03c01, \u2217, \u03c02), \u00b5)\nThe following table shows us PO3,1 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nAgain, it is not possible to find a PO for any permutation, since we always have \u03c0chasei \u2264 \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 3 and 1 will obtain an expected average reward of 1 and \u22121 respectively. Note that the choice of \u03a0o does not affect the result of PO3,1, so no matter which agents are in \u03a0o we obtain:\nPG3,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nPG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 2:\nPG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG3,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,2 (\u03a0o)\nwL\u0307(l\u0307)PO3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22123,2 (\u03a0o) to calculate PO3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u2217, \u2217, \u03c02) from L\u0307N(\u00b5)\u22123,2 (\u03a0o):\nPO3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u2217, \u2217, \u03c02), \u00b5)\nThe following table shows us PO3,2 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nIt is possible to find a PO for every permutation, since the agents in slots 3 and 2 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nPG3,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 4:\nPG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG3,4(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,4 (\u03a0o)\nwL\u0307(l\u0307)PO3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22123,4 (\u03a0o) to calculate PO3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u03c02, \u2217, \u2217) from L\u0307N(\u00b5)\u22123,4 (\u03a0o):\nPO3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u03c02, \u2217, \u2217), \u00b5)\nThe following table shows us PO3,4 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nIt is possible to find a PO for every permutation, since the agents in slots 3 and 4 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nPG3,4(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 4 and 1:\nPG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG4,1(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,1 (\u03a0o)\nwL\u0307(l\u0307)PO4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22124,1 (\u03a0o) to calculate PO4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c01, \u03c02, \u2217) from L\u0307N(\u00b5)\u22124,1 (\u03a0o):\nPO4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u2217, \u03c01, \u03c02, \u2217), \u00b5)\nThe following table shows us PO4,1 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nAgain, it is not possible to find a PO for any permutation, since we always have \u03c0chasei \u2264 \u03c0chasej , where a \u03c0chase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 4 and 1 will obtain an expected average reward of 1 and \u22121 respectively. Note that the choice of \u03a0o does not affect the result of PO4,1, so no matter which agents are in \u03a0o we obtain:\nPG4,1(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 0\nTherefore:\nPG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 4 and 2:\nPG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG4,2(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,2 (\u03a0o)\nwL\u0307(l\u0307)PO4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22124,2 (\u03a0o) to calculate PO4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u2217, \u03c02, \u2217) from L\u0307N(\u00b5)\u22124,2 (\u03a0o):\nPO4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u2217, \u03c02, \u2217), \u00b5)\nThe following table shows us PO4,2 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nIt is possible to find a PO for every permutation, since the agents in slots 4 and 2 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nPG4,2(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd for slots 4 and 3:\nPG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u03b7\u03a03 \u2211\n\u03c01,\u03c02,\u03c03\u2208\u03a0e|\u03c01 \u0338=\u03c02 \u0338=\u03c03\nw\u03a0e(\u03c01)w\u03a0e(\u03c02)w\u03a0e(\u03c03)PG4,3(\u03c01, \u03c02, \u03c03,\u03a0o, wL\u0307, \u00b5) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5)\nIn this case, we only need to calculate PG4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5). We follow definition 27 (for PG) to calculate this value:\nPG4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,3 (\u03a0o)\nwL\u0307(l\u0307)PO4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5)\nAgain, we do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from L\u0307 N(\u00b5) \u22124,3 (\u03a0o) to calculate PO4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c01, \u03c02, \u2217, \u2217) from L\u0307N(\u00b5)\u22124,3 (\u03a0o):\nPO4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, l\u0307, \u00b5) = PO4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3, (\u03c01, \u03c02, \u2217, \u2217), \u00b5)\nThe following table shows us PO4,3 for all the permutations of \u03c0chase1, \u03c0chase2, \u03c0chase3.\nSlot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase3 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 \u03c0chase2 \u2264 \u03c0chase3 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1 \u03c0chase1 \u2264 \u03c0chase2 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase2 \u2264 \u03c0chase1 \u03c0chase3 \u2264 \u03c0chase2 \u03c0chase3 \u2264 \u03c0chase1\nIt is possible to find a PO for every permutation, since the agents in slots 4 and 3 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in \u03a0o we obtain:\nPG4,3(\u03c0chase1, \u03c0chase2, \u03c0chase3,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nPG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, we weight over the slots:\nPG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)PGi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = 4\n3\n1\n4\n1 4 {PG1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+ PG1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + PG2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + PG3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + PG3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + PG4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + PG4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 4\n3\n1\n4\n1 4 {9\u00d7 1 + 3\u00d7 0} = 3 4\nSo, for every \u03a0o we obtain the same result:\n\u2200\u03a0o : PG(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 3\n4\nTherefore, predator-prey has Rightmin = 3 4 (as a higher approximation) for this property."}, {"heading": "C.6 Slot Reward Dependency", "text": "Next we see the slot reward dependency (SRD) property. As given in section 4.3.2, we want to know how much competitive or cooperative the environment is.\nProposition 62. General range for the slot reward dependency (SRD) property is equal to [0, 0] for the predator-prey environment.\nProof. Following definition 20, we obtain the SRD value for any \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 (where \u03a0e, w\u03a0e and \u03a0o are instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 19, we can calculate its SRD value for each pair of slots. We start with slots 1 and 2:\nSRD1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)SRD1,2(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate SRD1,2(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 18 to calculate this value for a figurative evaluated agent \u03c01 from \u03a0e:\nSRD1,2(\u03c01,\u03a0o, wL\u0307, \u00b5) = corrl\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o) [wL\u0307(l\u0307)](R1(\u00b5[l\u0307\n1\u2190 \u03c01]), R2(\u00b5[l\u0307 1\u2190 \u03c01]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22121 (\u03a0o) to calculate corr(R1(\u00b5[l\u0307 1\u2190 \u03c01]), R2(\u00b5[l\u0307 1\u2190 \u03c01])). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c02, \u03c03, \u03c04) from L\u0307N(\u00b5)\u22121 (\u03a0o):\ncorr(R1(\u00b5[l\u0307 1\u2190 \u03c01]), R2(\u00b5[l\u0307 1\u2190 \u03c01])) = corr(R1(\u00b5[\u03c01, \u03c02, \u03c03, \u03c04]), R2(\u00b5[\u03c01, \u03c02, \u03c03, \u03c04]))\nFrom the predator-prey\u2019s payoff matrix (figure 10), we can see that when the agent in slot 1 (any \u03c01) obtains a reward r the agent in slot 2 (any \u03c02) obtains \u2212r as reward, and this relation is propagated to expected average\nrewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 1 and 2 always obtain opposite expected average reward, then the correlation function will always obtain the same value24 of \u22121. So:\nSRD1,2(\u03c01,\u03a0o, wL\u0307, \u00b5) = \u22121\nTherefore:\nSRD1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u22121\nFor slots 1 and 3: SRD1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)SRD1,3(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate SRD1,3(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 18 to calculate this value for a figurative evaluated agent \u03c01 from \u03a0e:\nSRD1,3(\u03c01,\u03a0o, wL\u0307, \u00b5) = corrl\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o) [wL\u0307(l\u0307)](R1(\u00b5[l\u0307\n1\u2190 \u03c01]), R3(\u00b5[l\u0307 1\u2190 \u03c01]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22121 (\u03a0o) to calculate corr(R1(\u00b5[l\u0307 1\u2190 \u03c01]), R3(\u00b5[l\u0307 1\u2190 \u03c01])). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c02, \u03c03, \u03c04) from L\u0307N(\u00b5)\u22121 (\u03a0o):\ncorr(R1(\u00b5[l\u0307 1\u2190 \u03c01]), R3(\u00b5[l\u0307 1\u2190 \u03c01])) = corr(R1(\u00b5[\u03c01, \u03c02, \u03c03, \u03c04]), R3(\u00b5[\u03c01, \u03c02, \u03c03, \u03c04]))\nFrom the predator-prey\u2019s payoff matrix (figure 10), we can see that when the agent in slot 1 (any \u03c01) obtains a reward r the agent in slot 3 (any \u03c03) obtains \u2212r as reward, and this relation is propagated to expected average rewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 1 and 3 always obtain opposite expected average reward, then the correlation function will always obtain the same value of \u22121. So:\nSRD1,3(\u03c01,\u03a0o, wL\u0307, \u00b5) = \u22121\nTherefore:\nSRD1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u22121\nFor slots 1 and 4: SRD1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)SRD1,4(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate SRD1,4(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 18 to calculate this value for a figurative evaluated agent \u03c01 from \u03a0e:\nSRD1,4(\u03c01,\u03a0o, wL\u0307, \u00b5) = corrl\u0307\u2208L\u0307N(\u00b5)\u22121 (\u03a0o) [wL\u0307(l\u0307)](R1(\u00b5[l\u0307\n1\u2190 \u03c01]), R4(\u00b5[l\u0307 1\u2190 \u03c01]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22121 (\u03a0o) to calculate corr(R1(\u00b5[l\u0307 1\u2190 \u03c01]), R4(\u00b5[l\u0307 1\u2190 \u03c01])). We calculate this value for a figurative line-up pattern l\u0307 = (\u2217, \u03c02, \u03c03, \u03c04) from L\u0307N(\u00b5)\u22121 (\u03a0o):\ncorr(R1(\u00b5[l\u0307 1\u2190 \u03c01]), R4(\u00b5[l\u0307 1\u2190 \u03c01])) = corr(R1(\u00b5[\u03c01, \u03c02, \u03c03, \u03c04]), R4(\u00b5[\u03c01, \u03c02, \u03c03, \u03c04]))\nFrom the predator-prey\u2019s payoff matrix (figure 10), we can see that when the agent in slot 1 (any \u03c01) obtains a reward r the agent in slot 4 (any \u03c04) obtains \u2212r as reward, and this relation is propagated to expected average\n24Provided there is at least one game which is not a tie.\nrewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 1 and 4 always obtain opposite expected average reward, then the correlation function will always obtain the same value of \u22121. So:\nSRD1,4(\u03c01,\u03a0o, wL\u0307, \u00b5) = \u22121\nTherefore:\nSRD1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u22121\nFor slots 2 and 1: SRD2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)SRD2,1(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate SRD2,1(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 18 to calculate this value for a figurative evaluated agent \u03c01 from \u03a0e:\nSRD2,1(\u03c01,\u03a0o, wL\u0307, \u00b5) = corrl\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o) [wL\u0307(l\u0307)](R2(\u00b5[l\u0307\n2\u2190 \u03c01]), R1(\u00b5[l\u0307 2\u2190 \u03c01]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22122 (\u03a0o) to calculate corr(R2(\u00b5[l\u0307 2\u2190 \u03c01]), R1(\u00b5[l\u0307 2\u2190 \u03c01])). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c02, \u2217, \u03c03, \u03c04) from L\u0307N(\u00b5)\u22122 (\u03a0o):\ncorr(R2(\u00b5[l\u0307 2\u2190 \u03c01]), R1(\u00b5[l\u0307 2\u2190 \u03c01])) = corr(R2(\u00b5[\u03c02, \u03c01, \u03c03, \u03c04]), R1(\u00b5[\u03c02, \u03c01, \u03c03, \u03c04]))\nFrom the predator-prey\u2019s payoff matrix (figure 10), we can see that when the agent in slot 2 (any \u03c01) obtains a reward r the agent in slot 1 (any \u03c02) obtains \u2212r as reward, and this relation is propagated to expected average rewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 2 and 1 always obtain opposite expected average reward, then the correlation function will always obtain the same value of \u22121. So:\nSRD2,1(\u03c01,\u03a0o, wL\u0307, \u00b5) = \u22121\nTherefore:\nSRD2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u22121\nFor slots 2 and 3: SRD2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)SRD2,3(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate SRD2,3(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 18 to calculate this value for a figurative evaluated agent \u03c01 from \u03a0e:\nSRD2,3(\u03c01,\u03a0o, wL\u0307, \u00b5) = corrl\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o) [wL\u0307(l\u0307)](R2(\u00b5[l\u0307\n2\u2190 \u03c01]), R3(\u00b5[l\u0307 2\u2190 \u03c01]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22122 (\u03a0o) to calculate corr(R2(\u00b5[l\u0307 2\u2190 \u03c01]), R3(\u00b5[l\u0307 2\u2190 \u03c01])). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c02, \u2217, \u03c03, \u03c04) from L\u0307N(\u00b5)\u22122 (\u03a0o):\ncorr(R2(\u00b5[l\u0307 2\u2190 \u03c01]), R3(\u00b5[l\u0307 2\u2190 \u03c01])) = corr(R2(\u00b5[\u03c02, \u03c01, \u03c03, \u03c04]), R3(\u00b5[\u03c02, \u03c01, \u03c03, \u03c04]))\nFrom the predator-prey\u2019s payoff matrix (figure 10), we can see that when the agent in slot 2 (any \u03c01) obtains a reward r the agent in slot 3 (any \u03c03) obtains the same value r as reward (since they are in the same team), and this relation is propagated to expected average rewards as well. Since we use a correlation function between\nthe expected average rewards, and the agents in slots 2 and 3 always obtain the same expected average reward, then the correlation function will always obtain the same value of 1. So:\nSRD2,3(\u03c01,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSRD2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 1\nFor slots 2 and 4: SRD2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)SRD2,4(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate SRD2,4(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 18 to calculate this value for a figurative evaluated agent \u03c01 from \u03a0e:\nSRD2,4(\u03c01,\u03a0o, wL\u0307, \u00b5) = corrl\u0307\u2208L\u0307N(\u00b5)\u22122 (\u03a0o) [wL\u0307(l\u0307)](R2(\u00b5[l\u0307\n2\u2190 \u03c01]), R4(\u00b5[l\u0307 2\u2190 \u03c01]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22122 (\u03a0o) to calculate corr(R2(\u00b5[l\u0307 2\u2190 \u03c01]), R4(\u00b5[l\u0307 2\u2190 \u03c01])). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c02, \u2217, \u03c03, \u03c04) from L\u0307N(\u00b5)\u22122 (\u03a0o):\ncorr(R2(\u00b5[l\u0307 2\u2190 \u03c01]), R4(\u00b5[l\u0307 2\u2190 \u03c01])) = corr(R2(\u00b5[\u03c02, \u03c01, \u03c03, \u03c04]), R4(\u00b5[\u03c02, \u03c01, \u03c03, \u03c04]))\nFrom the predator-prey\u2019s payoff matrix (figure 10), we can see that when the agent in slot 2 (any \u03c01) obtains a reward r the agent in slot 4 (any \u03c04) obtains the same value r as reward (since they are in the same team), and this relation is propagated to expected average rewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 2 and 4 always obtain the same expected average reward, then the correlation function will always obtain the same value of 1. So:\nSRD2,4(\u03c01,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSRD2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 1\nFor slots 3 and 1: SRD3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)SRD3,1(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate SRD3,1(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 18 to calculate this value for a figurative evaluated agent \u03c01 from \u03a0e:\nSRD3,1(\u03c01,\u03a0o, wL\u0307, \u00b5) = corrl\u0307\u2208L\u0307N(\u00b5)\u22123 (\u03a0o) [wL\u0307(l\u0307)](R3(\u00b5[l\u0307\n3\u2190 \u03c01]), R1(\u00b5[l\u0307 3\u2190 \u03c01]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22123 (\u03a0o) to calculate corr(R3(\u00b5[l\u0307 3\u2190 \u03c01]), R1(\u00b5[l\u0307 3\u2190 \u03c01])). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c02, \u03c03, \u2217, \u03c04) from L\u0307N(\u00b5)\u22123 (\u03a0o):\ncorr(R3(\u00b5[l\u0307 3\u2190 \u03c01]), R1(\u00b5[l\u0307 3\u2190 \u03c01])) = corr(R3(\u00b5[\u03c02, \u03c03, \u03c01, \u03c04]), R1(\u00b5[\u03c02, \u03c03, \u03c01, \u03c04]))\nFrom the predator-prey\u2019s payoff matrix (figure 10), we can see that when the agent in slot 3 (any \u03c01) obtains a reward r the agent in slot 1 (any \u03c02) obtains \u2212r as reward, and this relation is propagated to expected average rewards as well. Since we use a correlation function between the expected average rewards, and the agents in\nslots 3 and 1 always obtain opposite expected average reward, then the correlation function will always obtain the same value of \u22121. So:\nSRD3,1(\u03c01,\u03a0o, wL\u0307, \u00b5) = \u22121\nTherefore:\nSRD3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u22121\nFor slots 3 and 2: SRD3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)SRD3,2(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate SRD3,2(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 18 to calculate this value for a figurative evaluated agent \u03c01 from \u03a0e:\nSRD3,2(\u03c01,\u03a0o, wL\u0307, \u00b5) = corrl\u0307\u2208L\u0307N(\u00b5)\u22123 (\u03a0o) [wL\u0307(l\u0307)](R3(\u00b5[l\u0307\n3\u2190 \u03c01]), R2(\u00b5[l\u0307 3\u2190 \u03c01]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22123 (\u03a0o) to calculate corr(R3(\u00b5[l\u0307 3\u2190 \u03c01]), R2(\u00b5[l\u0307 3\u2190 \u03c01])). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c02, \u03c03, \u2217, \u03c04) from L\u0307N(\u00b5)\u22123 (\u03a0o):\ncorr(R3(\u00b5[l\u0307 3\u2190 \u03c01]), R2(\u00b5[l\u0307 3\u2190 \u03c01])) = corr(R3(\u00b5[\u03c02, \u03c03, \u03c01, \u03c04]), R2(\u00b5[\u03c02, \u03c03, \u03c01, \u03c04]))\nFrom the predator-prey\u2019s payoff matrix (figure 10), we can see that when the agent in slot 3 (any \u03c01) obtains a reward r the agent in slot 2 (any \u03c03) obtains the same value r as reward (since they are in the same team), and this relation is propagated to expected average rewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 3 and 2 always obtain the same expected average reward, then the correlation function will always obtain the same value of 1. So:\nSRD3,2(\u03c01,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSRD3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 1\nFor slots 3 and 4: SRD3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)SRD3,4(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate SRD3,4(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 18 to calculate this value for a figurative evaluated agent \u03c01 from \u03a0e:\nSRD3,4(\u03c01,\u03a0o, wL\u0307, \u00b5) = corrl\u0307\u2208L\u0307N(\u00b5)\u22123 (\u03a0o) [wL\u0307(l\u0307)](R3(\u00b5[l\u0307\n3\u2190 \u03c01]), R4(\u00b5[l\u0307 3\u2190 \u03c01]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22123 (\u03a0o) to calculate corr(R3(\u00b5[l\u0307 3\u2190 \u03c01]), R4(\u00b5[l\u0307 3\u2190 \u03c01])). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c02, \u03c03, \u2217, \u03c04) from L\u0307N(\u00b5)\u22123 (\u03a0o):\ncorr(R3(\u00b5[l\u0307 3\u2190 \u03c01]), R4(\u00b5[l\u0307 3\u2190 \u03c01])) = corr(R3(\u00b5[\u03c02, \u03c03, \u03c01, \u03c04]), R4(\u00b5[\u03c02, \u03c03, \u03c01, \u03c04]))\nFrom the predator-prey\u2019s payoff matrix (figure 10), we can see that when the agent in slot 3 (any \u03c01) obtains a reward r the agent in slot 4 (any \u03c04) obtains the same value r as reward (since they are in the same team), and this relation is propagated to expected average rewards as well. Since we use a correlation function between\nthe expected average rewards, and the agents in slots 3 and 4 always obtain the same expected average reward, then the correlation function will always obtain the same value of 1. So:\nSRD3,4(\u03c01,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSRD3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 1\nFor slots 4 and 1: SRD4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)SRD4,1(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate SRD4,1(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 18 to calculate this value for a figurative evaluated agent \u03c01 from \u03a0e:\nSRD4,1(\u03c01,\u03a0o, wL\u0307, \u00b5) = corrl\u0307\u2208L\u0307N(\u00b5)\u22124 (\u03a0o) [wL\u0307(l\u0307)](R4(\u00b5[l\u0307\n4\u2190 \u03c01]), R1(\u00b5[l\u0307 4\u2190 \u03c01]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22124 (\u03a0o) to calculate corr(R4(\u00b5[l\u0307 4\u2190 \u03c01]), R1(\u00b5[l\u0307 4\u2190 \u03c01])). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c02, \u03c03, \u03c04, \u2217) from L\u0307N(\u00b5)\u22124 (\u03a0o):\ncorr(R4(\u00b5[l\u0307 4\u2190 \u03c01]), R1(\u00b5[l\u0307 4\u2190 \u03c01])) = corr(R4(\u00b5[\u03c02, \u03c03, \u03c04, \u03c01]), R1(\u00b5[\u03c02, \u03c03, \u03c04, \u03c01]))\nFrom the predator-prey\u2019s payoff matrix (figure 10), we can see that when the agent in slot 4 (any \u03c01) obtains a reward r the agent in slot 1 (any \u03c02) obtains \u2212r as reward, and this relation is propagated to expected average rewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 4 and 1 always obtain opposite expected average reward, then the correlation function will always obtain the same value of \u22121. So:\nSRD4,1(\u03c01,\u03a0o, wL\u0307, \u00b5) = \u22121\nTherefore:\nSRD4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u22121\nFor slots 4 and 2: SRD4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)SRD4,2(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate SRD4,2(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 18 to calculate this value for a figurative evaluated agent \u03c01 from \u03a0e:\nSRD4,2(\u03c01,\u03a0o, wL\u0307, \u00b5) = corrl\u0307\u2208L\u0307N(\u00b5)\u22124 (\u03a0o) [wL\u0307(l\u0307)](R4(\u00b5[l\u0307\n4\u2190 \u03c01]), R2(\u00b5[l\u0307 4\u2190 \u03c01]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22124 (\u03a0o) to calculate corr(R4(\u00b5[l\u0307 4\u2190 \u03c01]), R2(\u00b5[l\u0307 4\u2190 \u03c01])). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c02, \u03c03, \u03c04, \u2217) from L\u0307N(\u00b5)\u22124 (\u03a0o):\ncorr(R4(\u00b5[l\u0307 4\u2190 \u03c01]), R2(\u00b5[l\u0307 4\u2190 \u03c01])) = corr(R4(\u00b5[\u03c02, \u03c03, \u03c04, \u03c01]), R2(\u00b5[\u03c02, \u03c03, \u03c04, \u03c01]))\nFrom the predator-prey\u2019s payoff matrix (figure 10), we can see that when the agent in slot 4 (any \u03c01) obtains a reward r the agent in slot 2 (any \u03c03) obtains the same value r as reward (since they are in the same team), and this relation is propagated to expected average rewards as well. Since we use a correlation function between\nthe expected average rewards, and the agents in slots 4 and 2 always obtain the same expected average reward, then the correlation function will always obtain the same value of 1. So:\nSRD4,2(\u03c01,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSRD4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 1\nAnd for slots 4 and 3: SRD4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = \u2211 \u03c0\u2208\u03a0e w\u03a0e(\u03c0)SRD4,3(\u03c0,\u03a0o, wL\u0307, \u00b5)\nWe do not know which \u03a0e we have, but we know that we will need to evaluate SRD4,3(\u03c0,\u03a0o, wL\u0307, \u00b5) for all evaluated agent \u03c0 \u2208 \u03a0e. We follow definition 18 to calculate this value for a figurative evaluated agent \u03c01 from \u03a0e:\nSRD4,3(\u03c01,\u03a0o, wL\u0307, \u00b5) = corrl\u0307\u2208L\u0307N(\u00b5)\u22124 (\u03a0o) [wL\u0307(l\u0307)](R4(\u00b5[l\u0307\n4\u2190 \u03c01]), R3(\u00b5[l\u0307 4\u2190 \u03c01]))\nWe do not know which \u03a0o we have, but we know that we will need to obtain a line-up pattern l\u0307 from\nL\u0307 N(\u00b5) \u22124 (\u03a0o) to calculate corr(R4(\u00b5[l\u0307 4\u2190 \u03c01]), R3(\u00b5[l\u0307 4\u2190 \u03c01])). We calculate this value for a figurative line-up pattern l\u0307 = (\u03c02, \u03c03, \u03c04, \u2217) from L\u0307N(\u00b5)\u22124 (\u03a0o):\ncorr(R4(\u00b5[l\u0307 4\u2190 \u03c01]), R3(\u00b5[l\u0307 4\u2190 \u03c01])) = corr(R4(\u00b5[\u03c02, \u03c03, \u03c04, \u03c01]), R3(\u00b5[\u03c02, \u03c03, \u03c04, \u03c01]))\nFrom the predator-prey\u2019s payoff matrix (figure 10), we can see that when the agent in slot 4 (any \u03c01) obtains a reward r the agent in slot 3 (any \u03c04) obtains the same value r as reward (since they are in the same team), and this relation is propagated to expected average rewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 4 and 3 always obtain the same expected average reward, then the correlation function will always obtain the same value of 1. So:\nSRD4,3(\u03c01,\u03a0o, wL\u0307, \u00b5) = 1\nTherefore:\nSRD4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) = 1\nAnd finally, we weight over the slots:\nSRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S21 N(\u00b5)\u2211 i=1 wS(i, \u00b5)\u00d7\n\u00d7 i\u22121\u2211 j=1 wS(j, \u00b5)SRDi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + N(\u00b5)\u2211 j=i+1 wS(j, \u00b5)SRDi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)  = = 8\n3\n1\n4\n1 4 {SRD1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + SRD1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+ SRD1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + SRD2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + SRD2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + SRD2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + SRD3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + SRD3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + SRD3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + SRD4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ + SRD4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) + SRD4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 4\n3\n1\n4\n1 4 {6\u00d7 (\u22121) + 6\u00d7 1} = 0\nSo, for every trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 we obtain the same result:\n\u2200\u03a0e, w\u03a0e ,\u03a0o : SRD(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = 0 Therefore, predator-prey has General = [0, 0] for this property."}, {"heading": "C.7 Competitive Anticipation", "text": "Then, we follow with the competitive anticipation (AComp) property. As given in section 4.5.1, we want to know how much benefit the evaluated agents obtain when they anticipate competing agents.\nProposition 63. Generalmin for the competitive anticipation (AComp) property is equal to \u22121 for the predator-prey environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0win/win\u2032} with w\u03a0e(\u03c0win/win\u2032) = 1 and \u03a0o = {\u03c0win} (a \u03c0win agent always tries to not be chased when playing as the prey and tries to chase when playing as the predator, and a \u03c0win/win\u2032 agent always tries to not be chased when playing as the prey and tries to chase when playing as the predator but from the fifth iteration stops chasing the prey).\nFollowing definition 33, we obtain the AComp value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every pair of slots in different teams. Following definition 32, we could calculate its AComp value for each pair of slots but, since \u03a0e has only one agent, its weight is equal to 1 and \u03a0o also has only one agent, it is equivalent to use directly definition 31. We start with slots 1 and 2: AComp1,2(\u03c0win/win\u2032 , \u03c0win,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( R1(\u00b5[l\u0307 1,2\u2190 \u03c0win/win\u2032 , \u03c0win])\u2212R1(\u00b5[l\u0307 1,2\u2190 \u03c0win/win\u2032 , \u03c0r]) ) =\n= 1\n2\n( R1(\u00b5[\u03c0win/win\u2032 , \u03c0win, \u03c0win, \u03c0win])\u2212R1(\u00b5[\u03c0win/win\u2032 , \u03c0r, \u03c0win, \u03c0win]) ) We know from lemma 3 that three predators correctly coordinating will always chase the prey. In line-up\n(\u03c0win/win\u2032 , \u03c0win, \u03c0win, \u03c0win) the agent in slot 1 (\u03c0win/win\u2032) will obtain an expected average reward of \u22121, while in line-up (\u03c0win/win\u2032 , \u03c0r, \u03c0win, \u03c0win) the agent in slot 1 (\u03c0win/win\u2032), where the prey will almost always be able to avoid the predators, will almost obtain an expected average reward of 1. So:\nAComp1,2(\u03c0win/win\u2032 , \u03c0win,\u03a0o, wL\u0307, \u00b5) = 1\n2 ((\u22121)\u2212 1) = \u22121\nFor slots 1 and 3:\nAComp1,3(\u03c0win/win\u2032 , \u03c0win,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,3 (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( R1(\u00b5[l\u0307 1,3\u2190 \u03c0win/win\u2032 , \u03c0win])\u2212R1(\u00b5[l\u0307 1,3\u2190 \u03c0win/win\u2032 , \u03c0r]) ) =\n= 1\n2\n( R1(\u00b5[\u03c0win/win\u2032 , \u03c0win, \u03c0win, \u03c0win])\u2212R1(\u00b5[\u03c0win/win\u2032 , \u03c0win, \u03c0r, \u03c0win]) ) We know from lemma 3 that three predators correctly coordinating will always chase the prey. In line-up\n(\u03c0win/win\u2032 , \u03c0win, \u03c0win, \u03c0win) the agent in slot 1 (\u03c0win/win\u2032) will obtain an expected average reward of \u22121, while in line-up (\u03c0win/win\u2032 , \u03c0win, \u03c0r, \u03c0win) the agent in slot 1 (\u03c0win/win\u2032), where the prey will almost always be able to avoid the predators, will almost obtain an expected average reward of 1. So:\nAComp1,3(\u03c0win/win\u2032 , \u03c0win,\u03a0o, wL\u0307, \u00b5) = 1\n2 ((\u22121)\u2212 1) = \u22121\nFor slots 1 and 4:\nAComp1,4(\u03c0win/win\u2032 , \u03c0win,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,4 (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( R1(\u00b5[l\u0307 1,4\u2190 \u03c0win/win\u2032 , \u03c0win])\u2212R1(\u00b5[l\u0307 1,4\u2190 \u03c0win/win\u2032 , \u03c0r]) ) =\n= 1\n2\n( R1(\u00b5[\u03c0win/win\u2032 , \u03c0win, \u03c0win, \u03c0win])\u2212R1(\u00b5[\u03c0win/win\u2032 , \u03c0win, \u03c0win, \u03c0r]) )\nWe know from lemma 3 that three predators correctly coordinating will always chase the prey. In line-up (\u03c0win/win\u2032 , \u03c0win, \u03c0win, \u03c0win) the agent in slot 1 (\u03c0win/win\u2032) will obtain an expected average reward of \u22121, while in line-up (\u03c0win/win\u2032 , \u03c0win, \u03c0win, \u03c0r) the agent in slot 1 (\u03c0win/win\u2032), where the prey will almost always be able to avoid the predators, will almost obtain an expected average reward of 1. So:\nAComp1,4(\u03c0win/win\u2032 , \u03c0win,\u03a0o, wL\u0307, \u00b5) = 1\n2 ((\u22121)\u2212 1) = \u22121\nFor slots 2 and 1:\nAComp2,1(\u03c0win/win\u2032 , \u03c0win,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( R2(\u00b5[l\u0307 2,1\u2190 \u03c0win/win\u2032 , \u03c0win])\u2212R2(\u00b5[l\u0307 2,1\u2190 \u03c0win/win\u2032 , \u03c0r]) ) =\n= 1\n2\n( R2(\u00b5[\u03c0win, \u03c0win/win\u2032 , \u03c0win, \u03c0win])\u2212R2(\u00b5[\u03c0r, \u03c0win/win\u2032 , \u03c0win, \u03c0win]) ) We know from lemma 3 that three predators correctly coordinating will always chase the prey. In line-up\n(\u03c0win, \u03c0win/win\u2032 , \u03c0win, \u03c0win) the agent in slot 2 (\u03c0win/win\u2032), where they prey will not be chased due to the misscoordination of \u03c0win/win\u2032 in the last iterations, will obtain an expected average reward of \u22121, while in line-up (\u03c0r, \u03c0win/win\u2032 , \u03c0win, \u03c0win) the agent in slot 2 (\u03c0win/win\u2032), where the prey will almost always be chased by the predators, will almost obtain an expected average reward of 1. So:\nAComp2,1(\u03c0win/win\u2032 , \u03c0win,\u03a0o, wL\u0307, \u00b5) = 1\n2 ((\u22121)\u2212 1) = \u22121\nFor slots 3 and 1:\nAComp3,1(\u03c0win/win\u2032 , \u03c0win,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,1 (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( R3(\u00b5[l\u0307 3,1\u2190 \u03c0win/win\u2032 , \u03c0win])\u2212R3(\u00b5[l\u0307 3,1\u2190 \u03c0win/win\u2032 , \u03c0r]) ) =\n= 1\n2\n( R3(\u00b5[\u03c0win, \u03c0win, \u03c0win/win\u2032 , \u03c0win])\u2212R3(\u00b5[\u03c0r, \u03c0win, \u03c0win/win\u2032 , \u03c0win]) ) We know from lemma 3 that three predators correctly coordinating will always chase the prey. In line-up\n(\u03c0win, \u03c0win, \u03c0win/win\u2032 , \u03c0win) the agent in slot 3 (\u03c0win/win\u2032), where they prey will not be chased due to the misscoordination of \u03c0win/win\u2032 in the last iterations, will obtain an expected average reward of \u22121, while in line-up (\u03c0r, \u03c0win, \u03c0win/win\u2032 , \u03c0win) the agent in slot 3 (\u03c0win/win\u2032), where the prey will almost always be chased by the predators, will almost obtain an expected average reward of 1. So:\nAComp3,1(\u03c0win/win\u2032 , \u03c0win,\u03a0o, wL\u0307, \u00b5) = 1\n2 ((\u22121)\u2212 1) = \u22121\nAnd for slots 4 and 1:\nAComp4,1(\u03c0win/win\u2032 , \u03c0win,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,1 (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( R4(\u00b5[l\u0307 4,1\u2190 \u03c0win/win\u2032 , \u03c0win])\u2212R4(\u00b5[l\u0307 4,1\u2190 \u03c0win/win\u2032 , \u03c0r]) ) =\n= 1\n2\n( R4(\u00b5[\u03c0win, \u03c0win, \u03c0win, \u03c0win/win\u2032 ])\u2212R4(\u00b5[\u03c0r, \u03c0win, \u03c0win, \u03c0win/win\u2032 ]) ) We know from lemma 3 that three predators correctly coordinating will always chase the prey. In line-up\n(\u03c0win, \u03c0win, \u03c0win, \u03c0win/win\u2032) the agent in slot 4 (\u03c0win/win\u2032), where they prey will not be chased due to the misscoordination of \u03c0win/win\u2032 in the last iterations, will obtain an expected average reward of \u22121, while in line-up (\u03c0r, \u03c0win, \u03c0win, \u03c0win/win\u2032) the agent in slot 4 (\u03c0win/win\u2032), where the prey will almost always be chased by the predators, will almost obtain an expected average reward of 1. So:\nAComp4,1(\u03c0win/win\u2032 , \u03c0win,\u03a0o, wL\u0307, \u00b5) = 1\n2 ((\u22121)\u2212 1) = \u22121\nAnd finally, we weight over the slots:\nAComp(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S22 \u2211 t1,t2\u2208\u03c4 |t1 \u0338=t2 \u2211 i\u2208t1 wS(i, \u00b5) \u2211 j\u2208t2 wS(j, \u00b5)ACompi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 8\n3\n1\n4\n1 4 {AComp1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +AComp1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+AComp1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +AComp2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ +AComp3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +AComp4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 8\n3\n1\n4\n1 4 {AComp1,2(\u03c0win/win\u2032 , \u03c0win,\u03a0o, wL\u0307, \u00b5) +AComp1,3(\u03c0win/win\u2032 , \u03c0win,\u03a0o, wL\u0307, \u00b5)+\n+AComp1,4(\u03c0win/win\u2032 , \u03c0win,\u03a0o, wL\u0307, \u00b5) +AComp2,1(\u03c0win/win\u2032 , \u03c0win,\u03a0o, wL\u0307, \u00b5)+ +AComp3,1(\u03c0win/win\u2032 , \u03c0win,\u03a0o, wL\u0307, \u00b5) +AComp4,1(\u03c0win/win\u2032 , \u03c0win,\u03a0o, wL\u0307, \u00b5)} = = 8\n3\n1\n4\n1 4 {6\u00d7 (\u22121)} = \u22121\nSince \u22121 is the lowest possible value for the competitive anticipation property, therefore predator-prey has Generalmin = \u22121 for this property.\nProposition 64. Generalmax for the competitive anticipation (AComp) property is equal to 1 2 for the predatorprey environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0tl/br} with w\u03a0e(\u03c0tl/br) = 1 and \u03a0o = {\u03c0br} (a \u03c0tl/br agent always stays in the top left corner when playing as the prey and always goes to the bottom right corner when playing as a predator, and a \u03c0br agent always goes to the bottom right corner).\nFollowing definition 33, we obtain the AComp value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every pair of slots in different teams. Following definition 32, we could calculate its AComp value for each pair of slots but, since \u03a0e has only one agent, its weight is equal to 1 and \u03a0o also has only one agent, it is equivalent to use directly definition 31. We start with slots 1 and 2:\nAComp1,2(\u03c0tl/br, \u03c0br,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,2 (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( R1(\u00b5[l\u0307 1,2\u2190 \u03c0tl/br, \u03c0br])\u2212R1(\u00b5[l\u0307 1,2\u2190 \u03c0tl/br, \u03c0r]) ) =\n= 1\n2\n( R1(\u00b5[\u03c0tl/br, \u03c0br, \u03c0br, \u03c0br])\u2212R1(\u00b5[\u03c0tl/br, \u03c0r, \u03c0br, \u03c0br]) ) In line-up (\u03c0tl/br, \u03c0br, \u03c0br, \u03c0br) the agent in slot 1 (\u03c0tl/br) will obtain an expected average reward of 1, while in line-up (\u03c0tl/br, \u03c0r, \u03c0br, \u03c0br) the agent in slot 1 (\u03c0tl/br), where the prey will almost always be able to avoid the predators, will almost obtain an expected average reward of 125. So:\nAComp1,2(\u03c0tl/br, \u03c0br,\u03a0o, wL\u0307, \u00b5) = 1\n2 (1\u2212 1) = 0\nFor slots 1 and 3:\nAComp1,3(\u03c0tl/br, \u03c0br,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,3 (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( R1(\u00b5[l\u0307 1,3\u2190 \u03c0tl/br, \u03c0br])\u2212R1(\u00b5[l\u0307 1,3\u2190 \u03c0tl/br, \u03c0r]) ) =\n= 1\n2\n( R1(\u00b5[\u03c0tl/br, \u03c0br, \u03c0br, \u03c0br])\u2212R1(\u00b5[\u03c0tl/br, \u03c0br, \u03c0r, \u03c0br]) ) In line-up (\u03c0tl/br, \u03c0br, \u03c0br, \u03c0br) the agent in slot 1 (\u03c0tl/br) will obtain an expected average reward of 1, while in line-up (\u03c0tl/br, \u03c0br, \u03c0r, \u03c0br) the agent in slot 1 (\u03c0tl/br), where the prey will almost always be able to avoid the predators, will almost obtain an expected average reward of 1. So:\n25It is arguably that some behaviours for the prey could try to be chased by the random agent, providing a greater value for this property, but this probability of been chased will still remain too low, which would not increase too much the value.\nAComp1,3(\u03c0tl/br, \u03c0br,\u03a0o, wL\u0307, \u00b5) = 1\n2 (1\u2212 1) = 0\nFor slots 1 and 4:\nAComp1,4(\u03c0tl/br, \u03c0br,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22121,4 (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( R1(\u00b5[l\u0307 1,4\u2190 \u03c0tl/br, \u03c0br])\u2212R1(\u00b5[l\u0307 1,4\u2190 \u03c0tl/br, \u03c0r]) ) =\n= 1\n2\n( R1(\u00b5[\u03c0tl/br, \u03c0br, \u03c0br, \u03c0br])\u2212R1(\u00b5[\u03c0tl/br, \u03c0br, \u03c0br, \u03c0r]) ) In line-up (\u03c0tl/br, \u03c0br, \u03c0br, \u03c0br) the agent in slot 1 (\u03c0tl/br) will obtain an expected average reward of 1, while in line-up (\u03c0tl/br, \u03c0br, \u03c0br, \u03c0r) the agent in slot 1 (\u03c0tl/br), where the prey will almost always be able to avoid the predators, will almost obtain an expected average reward of 1. So:\nAComp1,4(\u03c0tl/br, \u03c0br,\u03a0o, wL\u0307, \u00b5) = 1\n2 (1\u2212 1) = 0\nFor slots 2 and 1:\nAComp2,1(\u03c0tl/br, \u03c0br,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,1 (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( R2(\u00b5[l\u0307 2,1\u2190 \u03c0tl/br, \u03c0br])\u2212R2(\u00b5[l\u0307 2,1\u2190 \u03c0tl/br, \u03c0r]) ) =\n= 1\n2\n( R2(\u00b5[\u03c0br, \u03c0tl/br, \u03c0br, \u03c0br])\u2212R2(\u00b5[\u03c0r, \u03c0tl/br, \u03c0br, \u03c0br]) ) In line-up (\u03c0br, \u03c0tl/br, \u03c0br, \u03c0br), where \u03c0tl/br and \u03c0br will always go to the bottom right corner, the agent in slot 2 (\u03c0tl/br) will obtain an expected average reward of 1, since the prey will always be chased. In line-up (\u03c0r, \u03c0tl/br, \u03c0br, \u03c0br), where \u03c0tl/br and \u03c0br will always go to the bottom right corner and \u03c0r will act randomly, the agent in slot 2 (\u03c0tl/br) will obtain an expected average reward of \u22121, since the prey will rarely be chased. So:\nAComp2,1(\u03c0tl/br, \u03c0br,\u03a0o, wL\u0307, \u00b5) = 1\n2 (1\u2212 (\u22121)) = 1\nFor slots 3 and 1:\nAComp3,1(\u03c0tl/br, \u03c0br,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,1 (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( R3(\u00b5[l\u0307 3,1\u2190 \u03c0tl/br, \u03c0br])\u2212R3(\u00b5[l\u0307 3,1\u2190 \u03c0tl/br, \u03c0r]) ) =\n= 1\n2\n( R3(\u00b5[\u03c0br, \u03c0br, \u03c0tl/br, \u03c0br])\u2212R3(\u00b5[\u03c0r, \u03c0br, \u03c0tl/br, \u03c0br]) ) In line-up (\u03c0br, \u03c0br, \u03c0tl/br, \u03c0br), where \u03c0tl/br and \u03c0br will always go to the bottom right corner, the agent in slot 3 (\u03c0tl/br) will obtain an expected average reward of 1, since the prey will always be chased. In line-up (\u03c0r, \u03c0br, \u03c0tl/br, \u03c0br), where \u03c0tl/br and \u03c0br will always go to the bottom right corner and \u03c0r will act randomly, the agent in slot 3 (\u03c0tl/br) will obtain an expected average reward of \u22121, since the prey will rarely be chased. So:\nAComp3,1(\u03c0tl/br, \u03c0br,\u03a0o, wL\u0307, \u00b5) = 1\n2 (1\u2212 (\u22121)) = 1\nAnd for slots 4 and 1:\nAComp4,1(\u03c0tl/br, \u03c0br,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,1 (\u03a0o)\nwL\u0307(l\u0307) 1\n2\n( R4(\u00b5[l\u0307 4,1\u2190 \u03c0tl/br, \u03c0br])\u2212R4(\u00b5[l\u0307 4,1\u2190 \u03c0tl/br, \u03c0r]) ) =\n= 1\n2\n( R4(\u00b5[\u03c0br, \u03c0br, \u03c0br, \u03c0tl/br])\u2212R4(\u00b5[\u03c0r, \u03c0br, \u03c0br, \u03c0tl/br]) )\nIn line-up (\u03c0br, \u03c0br, \u03c0br, \u03c0tl/br), where \u03c0tl/br and \u03c0br will always go to the bottom right corner, the agent in slot 4 (\u03c0tl/br) will obtain an expected average reward of 1, since the prey will always be chased. In line-up (\u03c0r, \u03c0br, \u03c0br, \u03c0tl/br), where \u03c0tl/br and \u03c0br will always go to the bottom right corner and \u03c0r will act randomly, the agent in slot 4 (\u03c0tl/br) will obtain an expected average reward of \u22121, since the prey will rarely be chased. So:\nAComp4,1(\u03c0tl/br, \u03c0br,\u03a0o, wL\u0307, \u00b5) = 1\n2 (1\u2212 (\u22121)) = 1\nAnd finally, we weight over the slots:\nAComp(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S22 \u2211 t1,t2\u2208\u03c4 |t1 \u0338=t2 \u2211 i\u2208t1 wS(i, \u00b5) \u2211 j\u2208t2 wS(j, \u00b5)ACompi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 8\n3\n1\n4\n1 4 {AComp1,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +AComp1,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+AComp1,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +AComp2,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ +AComp3,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +AComp4,1(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 8\n3\n1\n4\n1 4 {AComp1,2(\u03c0tl/br, \u03c0br,\u03a0o, wL\u0307, \u00b5) +AComp1,3(\u03c0tl/br, \u03c0br,\u03a0o, wL\u0307, \u00b5)+\n+AComp1,4(\u03c0tl/br, \u03c0br,\u03a0o, wL\u0307, \u00b5) +AComp2,1(\u03c0tl/br, \u03c0br,\u03a0o, wL\u0307, \u00b5)+ +AComp3,1(\u03c0tl/br, \u03c0br,\u03a0o, wL\u0307, \u00b5) +AComp4,1(\u03c0tl/br, \u03c0br,\u03a0o, wL\u0307, \u00b5)} = = 8\n3\n1\n4\n1 4 {3\u00d7 0 + 3\u00d7 1} = 1 2\nSince 12 is the highest possible value that we can obtain for the competitive anticipation property, therefore predator-prey has Generalmax = 1 2 for this property."}, {"heading": "C.8 Cooperative Anticipation", "text": "Finally, we follow with the cooperative anticipation (ACoop) property. As given in section 4.5.2, we want to know how much benefit the evaluated agents obtain when they anticipate cooperating agents.\nProposition 65. Generalmin for the cooperative anticipation (ACoop) property is equal to \u22121 for the predatorprey environment.\nProof. To find Generalmin (equation 40), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which minimises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0br\u2032} with w\u03a0e(\u03c0br\u2032) = 1 and \u03a0o = {\u03c033/br\u2032} (a \u03c0br\u2032 agent always goes to the bottom right corner, but if it notices that not all the predators are going directly to this corner, then it will go to the cell in the 3rd row and 3rd column, and a \u03c033/br\u2032 agent always goes to the cell in the 3rd row and 3rd column when playing as the prey and will go directly to the bottom right corner when playing as a predator, but if it notices that not all the predators are going directly to this corner, then it will go to the cell in the 3rd row and 3rd column).\nFollowing definition 36, we obtain the ACoop value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every pair of different slots in the same team. Following definition 35, we could calculate its ACoop value for each pair of slots but, since \u03a0e has only one agent, its weight is equal to 1 and \u03a0o also has only one agent, it is equivalent to use directly definition 34. We start with slots 2 and 3: ACoop2,3(\u03c0br\u2032 , \u03c033/br\u2032 ,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,3 (\u03a0o)\nwL\u0307(l\u0307) 1\n4 \u00d7\n\u00d7 ( R2(\u00b5[l\u0307 2,3\u2190 \u03c0br\u2032 , \u03c033/br\u2032 ]) +R3(\u00b5[l\u0307 2,3\u2190 \u03c0br\u2032 , \u03c033/br\u2032 ])\u2212R2(\u00b5[l\u0307 2,3\u2190 \u03c0br\u2032 , \u03c0r])\u2212R3(\u00b5[l\u0307 2,3\u2190 \u03c0r, \u03c033/br\u2032 ]) ) =\n= 1\n4\n( R2(\u00b5[\u03c033/br\u2032 , \u03c0br\u2032 , \u03c033/br\u2032 , \u03c033/br\u2032 ]) +R3(\u00b5[\u03c033/br\u2032 , \u03c0br\u2032 , \u03c033/br\u2032 , \u03c033/br\u2032 ])\u2212\n\u2212R2(\u00b5[\u03c033/br\u2032 , \u03c0br\u2032 , \u03c0r, \u03c033/br\u2032 ])\u2212R3(\u00b5[\u03c033/br\u2032 , \u03c0r, \u03c033/br\u2032 , \u03c033/br\u2032 ]) )\nIn both line-ups (\u03c033/br\u2032 , \u03c0br\u2032 , \u03c033/br\u2032 , \u03c033/br\u2032), where the predators will go directly to the bottom right cell and the prey will go to the cell in the 3rd row and 3rd column, the agents in slot 2 (\u03c0br\u2032) and slot 3 (\u03c033/br\u2032) will both obtain an expected average reward of \u22121. In line-up (\u03c033/br\u2032 , \u03c0br\u2032 , \u03c0r, \u03c033/br\u2032), where \u03c0r will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 2 (\u03c0br\u2032) will almost obtain an expected average reward of 1. In line-up (\u03c033/br\u2032 , \u03c0r, \u03c033/br\u2032 , \u03c033/br\u2032), where \u03c0r will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 3 (\u03c033/br\u2032) will almost obtain an expected average reward of 1. So:\nACoop2,3(\u03c0br\u2032 , \u03c033/br\u2032 ,\u03a0o, wL\u0307, \u00b5) = 1\n4 ((\u22121) + (\u22121)\u2212 1\u2212 1) = \u22121\nFor slots 2 and 4:\nACoop2,4(\u03c0br\u2032 , \u03c033/br\u2032 ,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,4 (\u03a0o)\nwL\u0307(l\u0307) 1\n4 \u00d7\n\u00d7 ( R2(\u00b5[l\u0307 2,4\u2190 \u03c0br\u2032 , \u03c033/br\u2032 ]) +R4(\u00b5[l\u0307 2,4\u2190 \u03c0br\u2032 , \u03c033/br\u2032 ])\u2212R2(\u00b5[l\u0307 2,4\u2190 \u03c0br\u2032 , \u03c0r])\u2212R4(\u00b5[l\u0307 2,4\u2190 \u03c0r, \u03c033/br\u2032 ]) ) =\n= 1\n4\n( R2(\u00b5[\u03c033/br\u2032 , \u03c0br\u2032 , \u03c033/br\u2032 , \u03c033/br\u2032 ]) +R4(\u00b5[\u03c033/br\u2032 , \u03c0br\u2032 , \u03c033/br\u2032 , \u03c033/br\u2032 ])\u2212\n\u2212R2(\u00b5[\u03c033/br\u2032 , \u03c0br\u2032 , \u03c033/br\u2032 , \u03c0r])\u2212R4(\u00b5[\u03c033/br\u2032 , \u03c0r, \u03c033/br\u2032 , \u03c033/br\u2032 ]) )\nIn both line-ups (\u03c033/br\u2032 , \u03c0br\u2032 , \u03c033/br\u2032 , \u03c033/br\u2032), where the predators will go directly to the bottom right cell and the prey will go to the cell in the 3rd row and 3rd column, the agents in slot 2 (\u03c0br\u2032) and slot 4 (\u03c033/br\u2032) will both obtain an expected average reward of \u22121. In line-up (\u03c033/br\u2032 , \u03c0br\u2032 , \u03c033/br\u2032 , \u03c0r), where \u03c0r will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 2 (\u03c0br\u2032) will almost obtain an expected average reward of 1. In line-up (\u03c033/br\u2032 , \u03c0r, \u03c033/br\u2032 , \u03c033/br\u2032), where \u03c0r will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 4 (\u03c033/br\u2032) will almost obtain an expected average reward of 1. So:\nACoop2,4(\u03c0br\u2032 , \u03c033/br\u2032 ,\u03a0o, wL\u0307, \u00b5) = 1\n4 ((\u22121) + (\u22121)\u2212 1\u2212 1) = \u22121\nFor slots 3 and 2:\nACoop3,2(\u03c0br\u2032 , \u03c033/br\u2032 ,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,2 (\u03a0o)\nwL\u0307(l\u0307) 1\n4 \u00d7\n\u00d7 ( R3(\u00b5[l\u0307 3,2\u2190 \u03c0br\u2032 , \u03c033/br\u2032 ]) +R2(\u00b5[l\u0307 3,2\u2190 \u03c0br\u2032 , \u03c033/br\u2032 ])\u2212R3(\u00b5[l\u0307 3,2\u2190 \u03c0br\u2032 , \u03c0r])\u2212R2(\u00b5[l\u0307 3,2\u2190 \u03c0r, \u03c033/br\u2032 ]) ) =\n= 1\n4\n( R3(\u00b5[\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0br\u2032 , \u03c033/br\u2032 ]) +R2(\u00b5[\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0br\u2032 , \u03c033/br\u2032 ])\u2212\n\u2212R3(\u00b5[\u03c033/br\u2032 , \u03c0r, \u03c0br\u2032 , \u03c033/br\u2032 ])\u2212R2(\u00b5[\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0r, \u03c033/br\u2032 ]) )\nIn both line-ups (\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0br\u2032 , \u03c033/br\u2032), where the predators will go directly to the bottom right cell and the prey will go to the cell in the 3rd row and 3rd column, the agents in slot 3 (\u03c0br\u2032) and slot 2 (\u03c033/br\u2032) will both obtain an expected average reward of \u22121. In line-up (\u03c033/br\u2032 , \u03c0r, \u03c0br\u2032 , \u03c033/br\u2032), where \u03c0r will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 3 (\u03c0br\u2032) will almost obtain an expected average reward of 1. In line-up (\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0r, \u03c033/br\u2032), where \u03c0r will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 2 (\u03c033/br\u2032) will almost obtain an expected average reward of 1. So:\nACoop3,2(\u03c0br\u2032 , \u03c033/br\u2032 ,\u03a0o, wL\u0307, \u00b5) = 1\n4 ((\u22121) + (\u22121)\u2212 1\u2212 1) = \u22121\nFor slots 3 and 4:\nACoop3,4(\u03c0br\u2032 , \u03c033/br\u2032 ,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,4 (\u03a0o)\nwL\u0307(l\u0307) 1\n4 \u00d7\n\u00d7 ( R3(\u00b5[l\u0307 3,4\u2190 \u03c0br\u2032 , \u03c033/br\u2032 ]) +R4(\u00b5[l\u0307 3,4\u2190 \u03c0br\u2032 , \u03c033/br\u2032 ])\u2212R3(\u00b5[l\u0307 3,4\u2190 \u03c0br\u2032 , \u03c0r])\u2212R4(\u00b5[l\u0307 3,4\u2190 \u03c0r, \u03c033/br\u2032 ]) ) =\n= 1\n4\n( R3(\u00b5[\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0br\u2032 , \u03c033/br\u2032 ]) +R4(\u00b5[\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0br\u2032 , \u03c033/br\u2032 ])\u2212\n\u2212R3(\u00b5[\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0br\u2032 , \u03c0r])\u2212R4(\u00b5[\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0r, \u03c033/br\u2032 ]) )\nIn both line-ups (\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0br\u2032 , \u03c033/br\u2032), where the predators will go directly to the bottom right cell and the prey will go to the cell in the 3rd row and 3rd column, the agents in slot 3 (\u03c0br\u2032) and slot 4 (\u03c033/br\u2032) will both obtain an expected average reward of \u22121. In line-up (\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0br\u2032 , \u03c0r), where \u03c0r will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 3 (\u03c0br\u2032) will almost obtain an expected average reward of 1. In line-up (\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0r, \u03c033/br\u2032), where \u03c0r will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 4 (\u03c033/br\u2032) will almost obtain an expected average reward of 1. So:\nACoop3,4(\u03c0br\u2032 , \u03c033/br\u2032 ,\u03a0o, wL\u0307, \u00b5) = 1\n4 ((\u22121) + (\u22121)\u2212 1\u2212 1) = \u22121\nFor slots 4 and 2:\nACoop4,2(\u03c0br\u2032 , \u03c033/br\u2032 ,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,2 (\u03a0o)\nwL\u0307(l\u0307) 1\n4 \u00d7\n\u00d7 ( R4(\u00b5[l\u0307 4,2\u2190 \u03c0br\u2032 , \u03c033/br\u2032 ]) +R2(\u00b5[l\u0307 4,2\u2190 \u03c0br\u2032 , \u03c033/br\u2032 ])\u2212R4(\u00b5[l\u0307 4,2\u2190 \u03c0br\u2032 , \u03c0r])\u2212R2(\u00b5[l\u0307 4,2\u2190 \u03c0r, \u03c033/br\u2032 ]) ) =\n= 1\n4\n( R4(\u00b5[\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0br\u2032 ]) +R2(\u00b5[\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0br\u2032 ])\u2212\n\u2212R4(\u00b5[\u03c033/br\u2032 , \u03c0r, \u03c033/br\u2032 , \u03c0br\u2032 ])\u2212R2(\u00b5[\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0r]) )\nIn both line-ups (\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0br\u2032), where the predators will go directly to the bottom right cell and the prey will go to the cell in the 3rd row and 3rd column, the agents in slot 4 (\u03c0br\u2032) and slot 2 (\u03c033/br\u2032) will both obtain an expected average reward of \u22121. In line-up (\u03c033/br\u2032 , \u03c0r, \u03c033/br\u2032 , \u03c0br\u2032), where \u03c0r will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 4 (\u03c0br\u2032) will almost obtain an expected average reward of 1. In line-up (\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0r), where \u03c0r will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 2 (\u03c033/br\u2032) will almost obtain an expected average reward of 1. So:\nACoop4,2(\u03c0br\u2032 , \u03c033/br\u2032 ,\u03a0o, wL\u0307, \u00b5) = 1\n4 ((\u22121) + (\u22121)\u2212 1\u2212 1) = \u22121\nAnd for slots 4 and 3:\nACoop4,3(\u03c0br\u2032 , \u03c033/br\u2032 ,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,3 (\u03a0o)\nwL\u0307(l\u0307) 1\n4 \u00d7\n\u00d7 ( R4(\u00b5[l\u0307 4,3\u2190 \u03c0br\u2032 , \u03c033/br\u2032 ]) +R3(\u00b5[l\u0307 4,3\u2190 \u03c0br\u2032 , \u03c033/br\u2032 ])\u2212R4(\u00b5[l\u0307 4,3\u2190 \u03c0br\u2032 , \u03c0r])\u2212R3(\u00b5[l\u0307 4,3\u2190 \u03c0r, \u03c033/br\u2032 ]) ) =\n= 1\n4\n( R4(\u00b5[\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0br\u2032 ]) +R3(\u00b5[\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0br\u2032 ])\u2212\n\u2212R4(\u00b5[\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0r, \u03c0br\u2032 ])\u2212R3(\u00b5[\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0r]) )\nIn both line-ups (\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0br\u2032), where the predators will go directly to the bottom right cell and the prey will go to the cell in the 3rd row and 3rd column, the agents in slot 4 (\u03c0br\u2032) and slot 3 (\u03c033/br\u2032) will both obtain an expected average reward of \u22121. In line-up (\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0r, \u03c0br\u2032), where \u03c0r will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 4 (\u03c0br\u2032) will almost obtain an expected average reward of 1. In line-up (\u03c033/br\u2032 , \u03c033/br\u2032 , \u03c033/br\u2032 , \u03c0r), where \u03c0r will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 3 (\u03c033/br\u2032) will almost obtain an expected average reward of 1. So:\nACoop4,3(\u03c0br\u2032 , \u03c033/br\u2032 ,\u03a0o, wL\u0307, \u00b5) = 1\n4 ((\u22121) + (\u22121)\u2212 1\u2212 1) = \u22121\nAnd finally, we weight over the slots:\nACoop(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S23 \u2211 t\u2208\u03c4 \u2211 i,j\u2208t|i\u0338=j wS(i, \u00b5)wS(j, \u00b5)ACoopi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+ \u2211\nt1,t2,t3\u2208\u03c4 |t1 \u0338=t2 \u0338=t3 \u2211 i\u2208t1 wS(i, \u00b5) \u2211 j\u2208t2 wS(j, \u00b5)ACoopi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 8\n3\n1\n4\n1 4 {ACoop2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +ACoop2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+ACoop3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +ACoop3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ +ACoop4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +ACoop4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 8\n3\n1\n4\n1 4 {ACoop2,3(\u03c0br\u2032 , \u03c033/br\u2032 ,\u03a0o, wL\u0307, \u00b5) +ACoop2,4(\u03c0br\u2032 , \u03c033/br\u2032 ,\u03a0o, wL\u0307, \u00b5)+\n+ACoop3,2(\u03c0br\u2032 , \u03c033/br\u2032 ,\u03a0o, wL\u0307, \u00b5) +ACoop3,4(\u03c0br\u2032 , \u03c033/br\u2032 ,\u03a0o, wL\u0307, \u00b5)+ +ACoop4,2(\u03c0br\u2032 , \u03c033/br\u2032 ,\u03a0o, wL\u0307, \u00b5) +ACoop4,3(\u03c0br\u2032 , \u03c033/br\u2032 ,\u03a0o, wL\u0307, \u00b5)} = = 8\n3\n1\n4\n1 4 {6\u00d7 (\u22121)} = \u22121\nSince \u22121 is the lowest possible value for the cooperative anticipation property, therefore predator-prey has Generalmin = \u22121 for this property.\nProposition 66. Generalmax for the cooperative anticipation (ACoop) property is equal to 1 for the predatorprey environment.\nProof. To find Generalmax (equation 41), we need to find a trio \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9 which maximises the property as much as possible. We can have this situation by selecting \u03a0e = {\u03c0win} with w\u03a0e(\u03c0win) = 1 and \u03a0o = {\u03c0win} (a \u03c0win agent always tries to not be chased when playing as the prey and tries to chase when playing as the predator).\nFollowing definition 36, we obtain the ACoop value for this \u27e8\u03a0e, w\u03a0e ,\u03a0o\u27e9. Since the environment is not symmetric, we need to calculate this property for every pair of different slots in the same team. Following definition 35, we could calculate its ACoop value for each pair of slots but, since \u03a0e has only one agent, its weight is equal to 1 and \u03a0o also has only one agent, it is equivalent to use directly definition 34. We start with slots 2 and 3:\nACoop2,3(\u03c0win, \u03c0win,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,3 (\u03a0o)\nwL\u0307(l\u0307) 1\n4 \u00d7\n\u00d7 ( R2(\u00b5[l\u0307 2,3\u2190 \u03c0win, \u03c0win]) +R3(\u00b5[l\u0307 2,3\u2190 \u03c0win, \u03c0win])\u2212R2(\u00b5[l\u0307 2,3\u2190 \u03c0win, \u03c0r])\u2212R3(\u00b5[l\u0307 2,3\u2190 \u03c0r, \u03c0win]) ) =\n= 1\n4\n( R2(\u00b5[\u03c0win, \u03c0win, \u03c0win, \u03c0win]) +R3(\u00b5[\u03c0win, \u03c0win, \u03c0win, \u03c0win])\u2212\n\u2212R2(\u00b5[\u03c0win, \u03c0win, \u03c0r, \u03c0win])\u2212R3(\u00b5[\u03c0win, \u03c0r, \u03c0win, \u03c0win]) )\nIn both line-ups (\u03c0win, \u03c0win, \u03c0win, \u03c0win), where the predators will coordinate to always chase the prey as seen in lemma 3, the agents in slot 2 (\u03c0win) and slot 3 (\u03c0win) will both obtain an expected average reward of 1. In line-up (\u03c0win, \u03c0win, \u03c0r, \u03c0win), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 2 (\u03c0win) will almost obtain an expected average reward of \u22121. In line-up (\u03c0win, \u03c0r, \u03c0win, \u03c0win), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 3 (\u03c0win) will almost obtain an expected average reward of \u22121. So:\nACoop2,3(\u03c0win, \u03c0win,\u03a0o, wL\u0307, \u00b5) = 1\n4 (1 + 1\u2212 (\u22121)\u2212 (\u22121)) = 1\nFor slots 2 and 4:\nACoop2,4(\u03c0win, \u03c0win,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22122,4 (\u03a0o)\nwL\u0307(l\u0307) 1\n4 \u00d7\n\u00d7 ( R2(\u00b5[l\u0307 2,4\u2190 \u03c0win, \u03c0win]) +R4(\u00b5[l\u0307 2,4\u2190 \u03c0win, \u03c0win])\u2212R2(\u00b5[l\u0307 2,4\u2190 \u03c0win, \u03c0r])\u2212R4(\u00b5[l\u0307 2,4\u2190 \u03c0r, \u03c0win]) ) =\n= 1\n4\n( R2(\u00b5[\u03c0win, \u03c0win, \u03c0win, \u03c0win]) +R4(\u00b5[\u03c0win, \u03c0win, \u03c0win, \u03c0win])\u2212\n\u2212R2(\u00b5[\u03c0win, \u03c0win, \u03c0win, \u03c0r])\u2212R4(\u00b5[\u03c0win, \u03c0r, \u03c0win, \u03c0win]) )\nIn both line-ups (\u03c0win, \u03c0win, \u03c0win, \u03c0win), where the predators will coordinate to always chase the prey as seen in lemma 3, the agents in slot 2 (\u03c0win) and slot 4 (\u03c0win) will both obtain an expected average reward of 1. In line-up (\u03c0win, \u03c0win, \u03c0win, \u03c0r), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 2 (\u03c0win) will almost obtain an expected average reward of \u22121. In line-up (\u03c0win, \u03c0r, \u03c0win, \u03c0win), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 4 (\u03c0win) will almost obtain an expected average reward of \u22121. So:\nACoop2,4(\u03c0win, \u03c0win,\u03a0o, wL\u0307, \u00b5) = 1\n4 (1 + 1\u2212 (\u22121)\u2212 (\u22121)) = 1\nFor slots 3 and 2:\nACoop3,2(\u03c0win, \u03c0win,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,2 (\u03a0o)\nwL\u0307(l\u0307) 1\n4 \u00d7\n\u00d7 ( R3(\u00b5[l\u0307 3,2\u2190 \u03c0win, \u03c0win]) +R2(\u00b5[l\u0307 3,2\u2190 \u03c0win, \u03c0win])\u2212R3(\u00b5[l\u0307 3,2\u2190 \u03c0win, \u03c0r])\u2212R2(\u00b5[l\u0307 3,2\u2190 \u03c0r, \u03c0win]) ) =\n= 1\n4\n( R3(\u00b5[\u03c0win, \u03c0win, \u03c0win, \u03c0win]) +R2(\u00b5[\u03c0win, \u03c0win, \u03c0win, \u03c0win])\u2212\n\u2212R3(\u00b5[\u03c0win, \u03c0r, \u03c0win, \u03c0win])\u2212R2(\u00b5[\u03c0win, \u03c0win, \u03c0r, \u03c0win]) )\nIn both line-ups (\u03c0win, \u03c0win, \u03c0win, \u03c0win), where the predators will coordinate to always chase the prey as seen in lemma 3, the agents in slot 3 (\u03c0win) and slot 2 (\u03c0win) will both obtain an expected average reward of 1. In line-up (\u03c0win, \u03c0r, \u03c0win, \u03c0win), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 3 (\u03c0win) will almost obtain an expected average reward of \u22121. In line-up (\u03c0win, \u03c0win, \u03c0r, \u03c0win), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 2 (\u03c0win) will almost obtain an expected average reward of \u22121. So:\nACoop3,2(\u03c0win, \u03c0win,\u03a0o, wL\u0307, \u00b5) = 1\n4 (1 + 1\u2212 (\u22121)\u2212 (\u22121)) = 1\nFor slots 3 and 4:\nACoop3,4(\u03c0win, \u03c0win,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22123,4 (\u03a0o)\nwL\u0307(l\u0307) 1\n4 \u00d7\n\u00d7 ( R3(\u00b5[l\u0307 3,4\u2190 \u03c0win, \u03c0win]) +R4(\u00b5[l\u0307 3,4\u2190 \u03c0win, \u03c0win])\u2212R3(\u00b5[l\u0307 3,4\u2190 \u03c0win, \u03c0r])\u2212R4(\u00b5[l\u0307 3,4\u2190 \u03c0r, \u03c0win]) ) =\n= 1\n4\n( R3(\u00b5[\u03c0win, \u03c0win, \u03c0win, \u03c0win]) +R4(\u00b5[\u03c0win, \u03c0win, \u03c0win, \u03c0win])\u2212\n\u2212R3(\u00b5[\u03c0win, \u03c0win, \u03c0win, \u03c0r])\u2212R4(\u00b5[\u03c0win, \u03c0win, \u03c0r, \u03c0win]) )\nIn both line-ups (\u03c0win, \u03c0win, \u03c0win, \u03c0win), where the predators will coordinate to always chase the prey as seen in lemma 3, the agents in slot 3 (\u03c0win) and slot 4 (\u03c0win) will both obtain an expected average reward of 1. In line-up (\u03c0win, \u03c0win, \u03c0win, \u03c0r), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 3 (\u03c0win) will almost obtain an expected average reward of \u22121. In line-up (\u03c0win, \u03c0win, \u03c0r, \u03c0win), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 4 (\u03c0win) will almost obtain an expected average reward of \u22121. So:\nACoop3,4(\u03c0win, \u03c0win,\u03a0o, wL\u0307, \u00b5) = 1\n4 (1 + 1\u2212 (\u22121)\u2212 (\u22121)) = 1\nFor slots 4 and 2:\nACoop4,2(\u03c0win, \u03c0win,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,2 (\u03a0o)\nwL\u0307(l\u0307) 1\n4 \u00d7\n\u00d7 ( R4(\u00b5[l\u0307 4,2\u2190 \u03c0win, \u03c0win]) +R2(\u00b5[l\u0307 4,2\u2190 \u03c0win, \u03c0win])\u2212R4(\u00b5[l\u0307 4,2\u2190 \u03c0win, \u03c0r])\u2212R2(\u00b5[l\u0307 4,2\u2190 \u03c0r, \u03c0win]) ) =\n= 1\n4\n( R4(\u00b5[\u03c0win, \u03c0win, \u03c0win, \u03c0win]) +R2(\u00b5[\u03c0win, \u03c0win, \u03c0win, \u03c0win])\u2212\n\u2212R4(\u00b5[\u03c0win, \u03c0r, \u03c0win, \u03c0win])\u2212R2(\u00b5[\u03c0win, \u03c0win, \u03c0win, \u03c0r]) )\nIn both line-ups (\u03c0win, \u03c0win, \u03c0win, \u03c0win), where the predators will coordinate to always chase the prey as seen in lemma 3, the agents in slot 4 (\u03c0win) and slot 2 (\u03c0win) will both obtain an expected average reward of 1. In line-up (\u03c0win, \u03c0r, \u03c0win, \u03c0win), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 4 (\u03c0win) will almost obtain an expected average reward of \u22121. In line-up (\u03c0win, \u03c0win, \u03c0win, \u03c0r), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 2 (\u03c0win) will almost obtain an expected average reward of \u22121. So:\nACoop4,2(\u03c0win, \u03c0win,\u03a0o, wL\u0307, \u00b5) = 1\n4 (1 + 1\u2212 (\u22121)\u2212 (\u22121)) = 1\nAnd for slots 4 and 3:\nACoop4,3(\u03c0win, \u03c0win,\u03a0o, wL\u0307, \u00b5) = \u2211\nl\u0307\u2208L\u0307N(\u00b5)\u22124,3 (\u03a0o)\nwL\u0307(l\u0307) 1\n4 \u00d7\n\u00d7 ( R4(\u00b5[l\u0307 4,3\u2190 \u03c0win, \u03c0win]) +R3(\u00b5[l\u0307 4,3\u2190 \u03c0win, \u03c0win])\u2212R4(\u00b5[l\u0307 4,3\u2190 \u03c0win, \u03c0r])\u2212R3(\u00b5[l\u0307 4,3\u2190 \u03c0r, \u03c0win]) ) =\n= 1\n4\n( R4(\u00b5[\u03c0win, \u03c0win, \u03c0win, \u03c0win]) +R3(\u00b5[\u03c0win, \u03c0win, \u03c0win, \u03c0win])\u2212\n\u2212R4(\u00b5[\u03c0win, \u03c0win, \u03c0r, \u03c0win])\u2212R3(\u00b5[\u03c0win, \u03c0win, \u03c0win, \u03c0r]) )\nIn both line-ups (\u03c0win, \u03c0win, \u03c0win, \u03c0win), where the predators will coordinate to always chase the prey as seen in lemma 3, the agents in slot 4 (\u03c0win) and slot 3 (\u03c0win) will both obtain an expected average reward of 1.\nIn line-up (\u03c0win, \u03c0win, \u03c0win, \u03c0r), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 4 (\u03c0win) will almost obtain an expected average reward of \u22121. In line-up (\u03c0win, \u03c0win, \u03c0r, \u03c0win), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 3 (\u03c0win) will almost obtain an expected average reward of \u22121. So:\nACoop4,3(\u03c0win, \u03c0win,\u03a0o, wL\u0307, \u00b5) = 1\n4 (1 + 1\u2212 (\u22121)\u2212 (\u22121)) = 1\nAnd finally, we weight over the slots:\nACoop(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5, wS) = \u03b7S23 \u2211 t\u2208\u03c4 \u2211 i,j\u2208t|i \u0338=j wS(i, \u00b5)wS(j, \u00b5)ACoopi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+ \u2211\nt1,t2,t3\u2208\u03c4 |t1 \u0338=t2 \u0338=t3 \u2211 i\u2208t1 wS(i, \u00b5) \u2211 j\u2208t2 wS(j, \u00b5)ACoopi,j(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) =\n= 8\n3\n1\n4\n1 4 {ACoop2,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +ACoop2,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+\n+ACoop3,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +ACoop3,4(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)+ +ACoop4,2(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5) +ACoop4,3(\u03a0e, w\u03a0e ,\u03a0o, wL\u0307, \u00b5)} = = 8\n3\n1\n4\n1 4 {ACoop2,3(\u03c0win, \u03c0win,\u03a0o, wL\u0307, \u00b5) +ACoop2,4(\u03c0win, \u03c0win,\u03a0o, wL\u0307, \u00b5)+\n+ACoop3,2(\u03c0win, \u03c0win,\u03a0o, wL\u0307, \u00b5) +ACoop3,4(\u03c0win, \u03c0win,\u03a0o, wL\u0307, \u00b5)+ +ACoop4,2(\u03c0win, \u03c0win,\u03a0o, wL\u0307, \u00b5) +ACoop4,3(\u03c0win, \u03c0win,\u03a0o, wL\u0307, \u00b5)} = = 8\n3\n1\n4\n1 4 {6\u00d7 1} = 1\nSince 1 is the highest possible value for the cooperative anticipation property, therefore predator-prey has Generalmax = 1 for this property."}], "references": [{"title": "Expertness based cooperative q-learning. Systems, Man, and Cybernetics, Part B: Cybernetics", "author": ["Majid Nili Ahmadabadi", "Masoud Asadpour"], "venue": "IEEE Transactions on,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "The arcade learning environment", "author": ["MG Bellemare", "Y Naddaf", "J Veness", "M Bowling"], "venue": "J. Artificial Intelligence Res,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "On optimal cooperation of knowledge sources", "author": ["Miroslav Benda"], "venue": "Technical Report,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1985}, {"title": "The first international roshambo programming competition", "author": ["Darse Billings"], "venue": "International Computer Games Association Journal,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Evolutionary online learning of cooperative behavior with situationaction pairs", "author": ["Joerg Denzinger", "Michael Kordt"], "venue": "InMultiAgent Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Experiments in learning prototypical situations for variants of the pursuit game", "author": ["J\u00f6rg Denzinger", "Matthias Fuchs"], "venue": "In Proc. ICMAS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "AI - What is this? A definition of artificial intelligence", "author": ["D. Dobrev"], "venue": "PC Magazine Bulgaria (in Bulgarian, English version at http://www.dobrev.com/AI),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "A non-behavioural, computational extension to the Turing Test", "author": ["D.L. Dowe", "A.R. Hajek"], "venue": "In Intl. Conf. on Computational Intelligence & multimedia applications (ICCIMA\u201998), Gippsland,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "IQ tests are not for machines", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo"], "venue": "yet. Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "How universal can an intelligence test be", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo"], "venue": "Adaptive Behavior,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Compression and intelligence: social environments and communication", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo", "P.K. Das"], "venue": "Artificial General Intelligence 2011,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "On interaction complexity, (space-time) resolution and intelligence", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo"], "venue": "In ReteCog interaction Workshop,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "The rating of chessplayers", "author": ["A.E. Elo"], "venue": "past and present,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1978}, {"title": "On a pursuit game on cayley", "author": ["Peter Frankl"], "venue": "graphs. Combinatorica,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1987}, {"title": "Learning to play pac-man: An evolutionary, rule-based approach", "author": ["Marcus Gallagher", "Amanda Ryan"], "venue": "In Evolutionary Computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Beyond the Turing Test", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "J. Logic, Language & Information,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "On the computational measurement of intelligence factors", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "National Institute of Standards and Technology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "On more realistic environment distributions for defining, evaluating and developing intelligence", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe", "S. Espa\u00f1a-Cubillo", "M.V. Hern\u00e1ndez-Lloreda", "J. Insa-Cabrera"], "venue": "Artificial General Intelligence 2011,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Universal psychometrics: Measuring cognitive abilities in the machine kingdom", "author": ["J. Hern\u00e1ndez-Orallo", "David L. Dowe", "M.Victoria Hern\u00e1ndez-Lloreda"], "venue": "Cognitive Systems Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "On potential cognitive abilities in the machine kingdom", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe"], "venue": "Minds and Machines,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Turing tests with Turing machines", "author": ["J. Hern\u00e1ndez-Orallo", "J. Insa", "D.L. Dowe", "B. Hibbard"], "venue": "The Alan Turing Centenary Conference,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Turing machines and recursive Turing tests", "author": ["J. Hernandez-Orallo", "J. Insa-Cabrera", "D.L. Dowe", "B. Hibbard"], "venue": "AISB/IACAP 2012 Symposium \u201dRevisiting Turing and his Test\u201d,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "A formal definition of intelligence based on an intensional variant of Kolmogorov complexity", "author": ["J. Hern\u00e1ndez-Orallo", "N. Minaya-Collado"], "venue": "In Proc. Intl Symposium of Engineering of Intelligent Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "A (hopefully) non-biased universal environment class for measuring intelligence of biological and artificial systems", "author": ["Jos\u00e9 Hern\u00e1ndez-Orallo"], "venue": "In Artificial General Intelligence, 3rd Intl Conf,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Complexity distribution of agent policies", "author": ["Jos\u00e9 Hern\u00e1ndez-Orallo"], "venue": "CoRR, abs/1302.2056,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "On environment difficulty and discriminating power", "author": ["Jos\u00e9 Hern\u00e1ndez-Orallo"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Measuring universal intelligence: Towards an anytime intelligence test", "author": ["Jos\u00e9 Hern\u00e1ndez-Orallo", "David L. Dowe"], "venue": "Artificial Intelligence,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Humans have evolved specialized skills of social cognition: The cultural intelligence hypothesis", "author": ["E. Herrmann", "J. Call", "M.V. Hern\u00e1ndez-Lloreda", "B. Hare", "M. Tomasello"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "The structure of individual differences in the cognitive abilities of children and chimpanzees", "author": ["E. Herrmann", "M.V. Hern\u00e1ndez-Lloreda", "J. Call", "B. Hare", "M. Tomasello"], "venue": "Psychological Science,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Adversarial sequence prediction", "author": ["B. Hibbard"], "venue": "In Proceeding of the 2008 conference on Artificial General Intelligence 2008: Proceedings of the First AGI Conference,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "Measuring agent intelligence via hierarchies of environments", "author": ["B. Hibbard"], "venue": "Artificial General Intelligence,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Comparing humans and AI agents", "author": ["J. Insa-Cabrera", "D.L. Dowe", "S. Espa\u00f1a-Cubillo", "M.V. Hern\u00e1ndez-Lloreda", "J. Hern\u00e1ndez-Orallo"], "venue": "Artificial General Intelligence 2011,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Evaluating a reinforcement learning algorithm with a general intelligence test", "author": ["J. Insa-Cabrera", "D.L. Dowe", "J. Hernandez-Orallo"], "venue": "Advances in Artificial Intelligence - 14th Conference of the Spanish Association for Artificial Intelligence,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Interaction settings for measuring (social) intelligence in multiagent systems", "author": ["J. Insa-Cabrera", "J. Hern\u00e1ndez-Orallo"], "venue": "In ReteCog interaction Workshop,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Hernandez-Lloreda. The anynt project intelligence test : Lambda - one", "author": ["J. Insa-Cabrera", "J. Hernandez-Orallo", "D.L. Dowe", "S. Espaa", "M.V"], "venue": "AISB/IACAP 2012 Symposium \u201dRevisiting Turing and his Test\u201d,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "On measuring social intelligence: experiments on competition and cooperation", "author": ["Javier Insa-Cabrera", "Jos\u00e9-Luis Benacloch-Ayuso", "Jos\u00e9 Hern\u00e1ndez-Orallo"], "venue": "In Proceedings of the 5th international conference on Artificial General Intelligence,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Indirect interaction in environments for multi-agent systems", "author": ["David Keil", "Dina Q. Goldin"], "venue": "editors, E4MAS,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2005}, {"title": "Robocup: The robot world cup initiative", "author": ["Hiroaki Kitano", "Minoru Asada", "Yasuo Kuniyoshi", "Itsuki Noda", "Eiichi Osawa"], "venue": "In Proceedings of the first international conference on Autonomous agents,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1997}, {"title": "Universal intelligence: A definition of machine intelligence", "author": ["Shane Legg", "Marcus Hutter"], "venue": "Minds and Machines,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2007}, {"title": "Environmental science: systems and solutions", "author": ["Michael L Mac Kinney", "Robert Milton Schoch"], "venue": "Jones & Bartlett Learning,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2003}, {"title": "Are you socially intelligent", "author": ["F.A. Moss", "T. Hunt"], "venue": "Scientific American,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1927}, {"title": "Game theory: analysis of conflict", "author": ["Roger B Myerson"], "venue": "Harvard university press,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "An introduction to game theory", "author": ["Martin J Osborne"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2004}, {"title": "Emergent bucket brigading: a simple mechanisms for improving performance in multi-robot constrained-space foraging tasks", "author": ["Esben H Ostergaard", "Gaurav S Sukhatme", "Maja J Matari"], "venue": "In Proceedings of the fifth international conference on Autonomous agents,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2001}, {"title": "A strategic metagame player for general chess-like games", "author": ["Barney Pell"], "venue": "Computational Intelligence,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1996}, {"title": "Using genetic programming with multiple data types and automatic modularization to evolve decentralized and coordinated navigation in multiagent systems", "author": ["Alan Robinson", "Lee Spector"], "venue": "In Late Breaking Papers at the Genetic and Evolutionary Computation Conference", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2002}, {"title": "The Shapley value: essays in honor of Lloyd S", "author": ["Alvin E Roth"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1988}, {"title": "Behaviour of trading automata in a computerized double auction", "author": ["John Rust", "Richard Palmer", "John H Miller"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1992}, {"title": "Learning to coordinate without sharing information", "author": ["Sandip Sen", "Ip Sen", "Mahendra Sekaran", "John Hale"], "venue": "Proceedings of the Twelfth National Conference on Artificial Intelligence,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1994}, {"title": "Fundamentals of Comparative Cognition", "author": ["S.J. Shettleworth", "P. Bloom", "L. Nadel"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2013}, {"title": "Shettleworth. Cognition, evolution, and behavior", "author": ["J Sara"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2009}, {"title": "Multiagent systems: algorithmic, game-theoretic, and logical foundations", "author": ["Y. Shoham", "K. Leyton-Brown"], "venue": "Cambridge Univ Pr,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2008}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "The MIT press,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1998}, {"title": "Multi-agent reinforcement learning: Independent vs. cooperative agents", "author": ["Ming Tan"], "venue": "In Proceedings of the Tenth International Conference on Machine Learning,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 1993}, {"title": "A monte-carlo aixi approximation", "author": ["Joel Veness", "Kee Siong Ng", "Marcus Hutter", "William Uther", "David Silver"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2011}, {"title": "Some characteristics of the good judge of personality", "author": ["Philip E. Vernon"], "venue": "The Journal of Social Psychology,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 1933}, {"title": "Comparative cognition: Experimental explorations of animal intelligence", "author": ["Edward A Wasserman", "Thomas R Zentall"], "venue": null, "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2006}, {"title": "The measurement and appraisal of adult intelligence", "author": ["David Wechsler"], "venue": "Academic Medicine,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 1958}, {"title": "Evolutionary game theory", "author": ["J\u00f6rgen W Weibull"], "venue": "The MIT press,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1995}, {"title": "Theory and Measurement of Social Intelligence as a Cognitive Performance Construct", "author": ["Susanne Weis"], "venue": "PhD thesis, Otto-von-Guericke-Universita\u0308t Magdeburg, Universita\u0308tsbibliothek,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2008}, {"title": "Designing the market game for a trading agent competition", "author": ["Michael P. Wellman", "Peter R. Wurman", "Kevin O\u2019Malley", "Roshan Bangera", "S-d Lin", "Daniel Reeves", "William E. Walsh"], "venue": "Internet Computing,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2001}, {"title": "Environments for multi-agent systems, state-of-the-art and research challenges", "author": ["D. Weyns", "H.V.D. Parunak", "F. Michel", "T. Holvoet", "J. Ferber"], "venue": "In Environments for MAS,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2005}, {"title": "Generalized measures of information transfer", "author": ["P.L. Williams", "R.D. Beer"], "venue": "arXiv preprint arXiv:1102.1507,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2011}, {"title": "Learning mazes with aliasing states: An LCS algorithm with associative perception", "author": ["Z. Zatuchna", "A. Bagnall"], "venue": "Adaptive Behavior,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2009}, {"title": "co-evolutionary fitness switching: Learning complex collective behaviors using genetic programming", "author": ["Byoung-Tak Zhang", "Dong-Yeon Cho"], "venue": "Advances in genetic programming,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 1999}], "referenceMentions": [{"referenceID": 40, "context": "Many definitions have been proposed such as the \u201cability to understand and manage men and women, boys and girls \u2013 to act wisely in human relations\u201d [56], the \u201cability to get along with others\u201d [41], the \u201cfacility in dealing with human beings\u201d [60], or more specific definitions including \u201c[the] ability to get along with people in general, social technique or ease in society, knowledge of social matters, susceptibility to stimuli from other members of a group, as well as insight into the temporary moods or the underlying personality traits of friends and of strangers\u201d [58].", "startOffset": 193, "endOffset": 197}, {"referenceID": 57, "context": "Many definitions have been proposed such as the \u201cability to understand and manage men and women, boys and girls \u2013 to act wisely in human relations\u201d [56], the \u201cability to get along with others\u201d [41], the \u201cfacility in dealing with human beings\u201d [60], or more specific definitions including \u201c[the] ability to get along with people in general, social technique or ease in society, knowledge of social matters, susceptibility to stimuli from other members of a group, as well as insight into the temporary moods or the underlying personality traits of friends and of strangers\u201d [58].", "startOffset": 243, "endOffset": 247}, {"referenceID": 55, "context": "Many definitions have been proposed such as the \u201cability to understand and manage men and women, boys and girls \u2013 to act wisely in human relations\u201d [56], the \u201cability to get along with others\u201d [41], the \u201cfacility in dealing with human beings\u201d [60], or more specific definitions including \u201c[the] ability to get along with people in general, social technique or ease in society, knowledge of social matters, susceptibility to stimuli from other members of a group, as well as insight into the temporary moods or the underlying personality traits of friends and of strangers\u201d [58].", "startOffset": 573, "endOffset": 577}, {"referenceID": 59, "context": "Despite the ambiguity of what social intelligence is, many tests have been proposed to measure social intelligence in humans (see [62] for a survey).", "startOffset": 130, "endOffset": 134}, {"referenceID": 52, "context": "This is the same configuration as in reinforcement learning (RL) [54], where rewards are provided in order to encourage agents to perform tasks.", "startOffset": 65, "endOffset": 69}, {"referenceID": 27, "context": "For instance, some recent work has shown that social abilities can be compared in a systematic way between human children and apes [28, 29].", "startOffset": 131, "endOffset": 139}, {"referenceID": 28, "context": "For instance, some recent work has shown that social abilities can be compared in a systematic way between human children and apes [28, 29].", "startOffset": 131, "endOffset": 139}, {"referenceID": 56, "context": ", [59, 51]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 49, "context": ", [59, 51]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 42, "context": "For instance, prey-predator interaction and behaviour have been studied from many different points of view (including game theory [43]), but it is not clear how the ability of each subject can be objectively evaluated, especially because the interaction depends on the cognitive abilities of both prey and predator.", "startOffset": 130, "endOffset": 134}, {"referenceID": 56, "context": "Despite these difficulties, comparative cognition [59, 51] is more and more concerned about performing tests that compare the abilities of many different species and also the abilities of individuals of different species.", "startOffset": 50, "endOffset": 58}, {"referenceID": 49, "context": "Despite these difficulties, comparative cognition [59, 51] is more and more concerned about performing tests that compare the abilities of many different species and also the abilities of individuals of different species.", "startOffset": 50, "endOffset": 58}, {"referenceID": 26, "context": "From this point of view, it should be more natural to provide a single test (with possibly many different customised interfaces and rewards) to assess every kind of species (or, in other words, a more general, or universal [27, 19, 10], test).", "startOffset": 223, "endOffset": 235}, {"referenceID": 18, "context": "From this point of view, it should be more natural to provide a single test (with possibly many different customised interfaces and rewards) to assess every kind of species (or, in other words, a more general, or universal [27, 19, 10], test).", "startOffset": 223, "endOffset": 235}, {"referenceID": 9, "context": "From this point of view, it should be more natural to provide a single test (with possibly many different customised interfaces and rewards) to assess every kind of species (or, in other words, a more general, or universal [27, 19, 10], test).", "startOffset": 223, "endOffset": 235}, {"referenceID": 2, "context": "The environments used in social tests for machines tend to represent tasks that the agents must perform by interacting with other agents, so the performance is calculated as their capability to successfully cooperate with and/or compete against them to achieve some goals (see [3, 38] for two testbeds in multi-agent environments).", "startOffset": 277, "endOffset": 284}, {"referenceID": 37, "context": "The environments used in social tests for machines tend to represent tasks that the agents must perform by interacting with other agents, so the performance is calculated as their capability to successfully cooperate with and/or compete against them to achieve some goals (see [3, 38] for two testbeds in multi-agent environments).", "startOffset": 277, "endOffset": 284}, {"referenceID": 42, "context": "In the context of social sciences (stretching from economics to AI), game theory [43] has also studied the interaction of different agents in formalised structures (called games).", "startOffset": 81, "endOffset": 85}, {"referenceID": 42, "context": "Several kinds of games try to represent or to analyse a variety of properties: cooperative or non-cooperative games, simultaneous or sequential games, normal-form or extensive-form games, zero-sum or general-sum games, and symmetric or asymmetric games [43, 42].", "startOffset": 253, "endOffset": 261}, {"referenceID": 41, "context": "Several kinds of games try to represent or to analyse a variety of properties: cooperative or non-cooperative games, simultaneous or sequential games, normal-form or extensive-form games, zero-sum or general-sum games, and symmetric or asymmetric games [43, 42].", "startOffset": 253, "endOffset": 261}, {"referenceID": 47, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 111, "endOffset": 130}, {"referenceID": 3, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 111, "endOffset": 130}, {"referenceID": 37, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 111, "endOffset": 130}, {"referenceID": 60, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 111, "endOffset": 130}, {"referenceID": 2, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 147, "endOffset": 170}, {"referenceID": 53, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 147, "endOffset": 170}, {"referenceID": 48, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 147, "endOffset": 170}, {"referenceID": 45, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 147, "endOffset": 170}, {"referenceID": 64, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 147, "endOffset": 170}, {"referenceID": 43, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 147, "endOffset": 170}, {"referenceID": 7, "context": "Formal approaches started in the late 1990s using notions from Kolmogorov complexity, Solomonoff prediction and the MML principle [8, 23, 16, 17].", "startOffset": 130, "endOffset": 145}, {"referenceID": 22, "context": "Formal approaches started in the late 1990s using notions from Kolmogorov complexity, Solomonoff prediction and the MML principle [8, 23, 16, 17].", "startOffset": 130, "endOffset": 145}, {"referenceID": 15, "context": "Formal approaches started in the late 1990s using notions from Kolmogorov complexity, Solomonoff prediction and the MML principle [8, 23, 16, 17].", "startOffset": 130, "endOffset": 145}, {"referenceID": 16, "context": "Formal approaches started in the late 1990s using notions from Kolmogorov complexity, Solomonoff prediction and the MML principle [8, 23, 16, 17].", "startOffset": 130, "endOffset": 145}, {"referenceID": 6, "context": "Dobrev [7] suggested that machine intelligence should be measured by evaluating agent performance in a range of worlds, an idea that was independently developed in [39] under the name \u201cUniversal Intelligence\u201d.", "startOffset": 7, "endOffset": 10}, {"referenceID": 38, "context": "Dobrev [7] suggested that machine intelligence should be measured by evaluating agent performance in a range of worlds, an idea that was independently developed in [39] under the name \u201cUniversal Intelligence\u201d.", "startOffset": 164, "endOffset": 168}, {"referenceID": 26, "context": "Following this definition, a framework to evaluate intelligence [27] and an environment following the framework [24] were proposed.", "startOffset": 64, "endOffset": 68}, {"referenceID": 23, "context": "Following this definition, a framework to evaluate intelligence [27] and an environment following the framework [24] were proposed.", "startOffset": 112, "endOffset": 116}, {"referenceID": 32, "context": "In order to show their effectiveness, some experiments were performed [33, 32, 35], but their results suggested that the framework still has some limitations.", "startOffset": 70, "endOffset": 82}, {"referenceID": 31, "context": "In order to show their effectiveness, some experiments were performed [33, 32, 35], but their results suggested that the framework still has some limitations.", "startOffset": 70, "endOffset": 82}, {"referenceID": 34, "context": "In order to show their effectiveness, some experiments were performed [33, 32, 35], but their results suggested that the framework still has some limitations.", "startOffset": 70, "endOffset": 82}, {"referenceID": 35, "context": "This was the goal in [36], where other agents were directly included in the environment.", "startOffset": 21, "endOffset": 25}, {"referenceID": 26, "context": "These experiments were performed using the framework in [27], which was originally designed to evaluate general intelligence, by simply including other agents in the environment.", "startOffset": 56, "endOffset": 60}, {"referenceID": 17, "context": "This circular problem is turned into a recursive one in [18], where different levels of distributions are recursively provided by constructing a new distribution of agents from a prior distribution 1Not only as an alternative to MAS scenarios, but also to the Turing Test, CAPTCHAs and IQ tests (see [9] for a discussion).", "startOffset": 56, "endOffset": 60}, {"referenceID": 8, "context": "This circular problem is turned into a recursive one in [18], where different levels of distributions are recursively provided by constructing a new distribution of agents from a prior distribution 1Not only as an alternative to MAS scenarios, but also to the Turing Test, CAPTCHAs and IQ tests (see [9] for a discussion).", "startOffset": 300, "endOffset": 303}, {"referenceID": 18, "context": "This is in the spirit of universal psychometrics [19], where we must consider any kind of agent (natural or artificial).", "startOffset": 49, "endOffset": 53}, {"referenceID": 35, "context": "This can take us to definitions such as \u201cperformance of an agent in a wide range of environments while interacting with other agents\u201d [36].", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "oi,k) \u2192 [0, 1] denotes its probability to perform action ai,k after the sequence of events oi,1ai,1ri,1 .", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "ok\u22121ak\u22121rk\u22121) \u2192 [0, 1].", "startOffset": 16, "endOffset": 22}, {"referenceID": 0, "context": "okak) \u2192 [0, 1].", "startOffset": 8, "endOffset": 14}, {"referenceID": 46, "context": "Also, we do not use the term alliance as we do not use any sophisticated mechanism to award rewards, related to the contribution of each agent in the team, as it is done with the Shapley Value [48].", "startOffset": 193, "endOffset": 197}, {"referenceID": 37, "context": "Another example is RoboCup Soccer [38], denoted by \u03bcrc, whose \u03c4 would be {{1, 2, 3, 4, 5}, {6, 7, 8, 9, 10}}, which represents that there are two teams, with slots {1, 2, 3, 4, 5} in the first team and slots {6, 7, 8, 9, 10} in the second team.", "startOffset": 34, "endOffset": 38}, {"referenceID": 62, "context": "In fact, this is at the roots of definitions of interaction [65, 11, 34, 12], and the distinction between several kinds of interaction [37].", "startOffset": 60, "endOffset": 76}, {"referenceID": 10, "context": "In fact, this is at the roots of definitions of interaction [65, 11, 34, 12], and the distinction between several kinds of interaction [37].", "startOffset": 60, "endOffset": 76}, {"referenceID": 33, "context": "In fact, this is at the roots of definitions of interaction [65, 11, 34, 12], and the distinction between several kinds of interaction [37].", "startOffset": 60, "endOffset": 76}, {"referenceID": 11, "context": "In fact, this is at the roots of definitions of interaction [65, 11, 34, 12], and the distinction between several kinds of interaction [37].", "startOffset": 60, "endOffset": 76}, {"referenceID": 36, "context": "In fact, this is at the roots of definitions of interaction [65, 11, 34, 12], and the distinction between several kinds of interaction [37].", "startOffset": 135, "endOffset": 139}, {"referenceID": 11, "context": "However, as pointed out by [12], \u201cthis may originate from a common source\u201d, so common or mutual information is not sufficient for interaction to have taken place.", "startOffset": 27, "endOffset": 31}, {"referenceID": 39, "context": "In fact, in ecology, given two species, there are seven possible combinations of positive, negative or no effect between them, leading to six forms of symbiosis [40]: neutralism (0,0), amensalism (0,-), commensalism (+,0), competition (-,-), mutualism (+,+), and predation/parasitism (+,-).", "startOffset": 161, "endOffset": 165}, {"referenceID": 46, "context": "The previous definition may slightly resemble the Shapley Value [48] in cooperative game theory, but here we are not concerned about how relevant each agent is in a team (whether its contribution is higher than the contribution of its teammates), but to see whether there is effect on the rewards.", "startOffset": 64, "endOffset": 68}, {"referenceID": 25, "context": ", [26]), one simple notion that accounts for this concept quite well is the variance of results.", "startOffset": 2, "endOffset": 6}, {"referenceID": 12, "context": "So the idea we will pursue is to evaluate how close an environment and set of agents populating it are to this ideal situation from the expected average rewards of the evaluated agents (without an aggregated rating system): 6A common approach is to create a rating when we have many experiments, as done with sport ratings, such as the ELO rating [13] in chess.", "startOffset": 347, "endOffset": 351}, {"referenceID": 21, "context": "5][22], showing an agent set for the matching pennies game that is non-monotonic.", "startOffset": 2, "endOffset": 6}, {"referenceID": 29, "context": "Nonetheless, a partial order can still be constructed for the agent set of all finite state machines for this game [30].", "startOffset": 115, "endOffset": 119}, {"referenceID": 38, "context": "One possibility is to consider all environments (as done by [39, 27]), and another is to find an environment class that is sufficiently representative (as attempted in [24]).", "startOffset": 60, "endOffset": 68}, {"referenceID": 26, "context": "One possibility is to consider all environments (as done by [39, 27]), and another is to find an environment class that is sufficiently representative (as attempted in [24]).", "startOffset": 60, "endOffset": 68}, {"referenceID": 23, "context": "One possibility is to consider all environments (as done by [39, 27]), and another is to find an environment class that is sufficiently representative (as attempted in [24]).", "startOffset": 168, "endOffset": 172}, {"referenceID": 18, "context": "The set of all possible agents (either artificial or biological) is known as machine kingdom in [19, 20] and raises many questions about the feasibility of any test considering this astronomically large set.", "startOffset": 96, "endOffset": 104}, {"referenceID": 19, "context": "The set of all possible agents (either artificial or biological) is known as machine kingdom in [19, 20] and raises many questions about the feasibility of any test considering this astronomically large set.", "startOffset": 96, "endOffset": 104}, {"referenceID": 24, "context": "In this way, we could aim at social intelligence relative to a smaller (and well-defined) set of agents, possibly specialising the definition by limiting the resources, the program size [25] or the intelligence of the agents [18].", "startOffset": 186, "endOffset": 190}, {"referenceID": 17, "context": "In this way, we could aim at social intelligence relative to a smaller (and well-defined) set of agents, possibly specialising the definition by limiting the resources, the program size [25] or the intelligence of the agents [18].", "startOffset": 225, "endOffset": 229}, {"referenceID": 26, "context": "4 is not adaptive (for adaptive versions of universal tests, the reader is referred to [27, 19]).", "startOffset": 87, "endOffset": 95}, {"referenceID": 18, "context": "4 is not adaptive (for adaptive versions of universal tests, the reader is referred to [27, 19]).", "startOffset": 87, "endOffset": 95}, {"referenceID": 44, "context": "Many games and environments have been proposed as testbeds to evaluate performance in a multi-agent environment [45, 64, 53, 66].", "startOffset": 112, "endOffset": 128}, {"referenceID": 61, "context": "Many games and environments have been proposed as testbeds to evaluate performance in a multi-agent environment [45, 64, 53, 66].", "startOffset": 112, "endOffset": 128}, {"referenceID": 51, "context": "Many games and environments have been proposed as testbeds to evaluate performance in a multi-agent environment [45, 64, 53, 66].", "startOffset": 112, "endOffset": 128}, {"referenceID": 63, "context": "Many games and environments have been proposed as testbeds to evaluate performance in a multi-agent environment [45, 64, 53, 66].", "startOffset": 112, "endOffset": 128}, {"referenceID": 0, "context": "[0, 1] The evaluated agents do not take into account other agents\u2019 actions in their behaviour.", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[0, 1] Each evaluated agent obtains the same expected average reward independently of the line-up pattern.", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[0, 1] Every evaluated agent obtains the same expected average reward for each line-up pattern.", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[0, 1] Every evaluated agent obtains the same (social) intelligence value.", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[0, 1] There is no strict total order between the evaluated agents.", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[0, 1] There is no partial order between the evaluated agents.", "startOffset": 0, "endOffset": 6}, {"referenceID": 58, "context": "2 Matching pennies Matching pennies [61] can be considered the simplest game in game theory featuring competition.", "startOffset": 36, "endOffset": 40}, {"referenceID": 29, "context": "Nonetheless, there are different opinions about this, as it has been suggested that matching pennies could be an intelligence test on its own, under the name \u2018Adversarial Sequence Prediction\u2019 [30, 31].", "startOffset": 192, "endOffset": 200}, {"referenceID": 30, "context": "Nonetheless, there are different opinions about this, as it has been suggested that matching pennies could be an intelligence test on its own, under the name \u2018Adversarial Sequence Prediction\u2019 [30, 31].", "startOffset": 192, "endOffset": 200}, {"referenceID": 20, "context": "In [21] there is an example of an agent set for matching pennies that is non-monotonic (so PG < 1).", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "But, a good selection of \u03a0o can restrict the Right range making it equal to [1, 1] (proposition 32).", "startOffset": 76, "endOffset": 82}, {"referenceID": 0, "context": "But, a good selection of \u03a0o can restrict the Right range making it equal to [1, 1] (proposition 32).", "startOffset": 76, "endOffset": 82}, {"referenceID": 2, "context": "4 Predator-prey (Pursuit game) One typical environment for cooperation that uses a 2D discrete space is a pursuit game called Predator-prey [3], where the evaluee acts as a predator and has to cooperate/coordinate with other two predators in order to chase a prey.", "startOffset": 140, "endOffset": 143}, {"referenceID": 53, "context": ", [55, 6, 5, 14, 1]), but, in our opinion, no thorough study about their properties has been developed so far.", "startOffset": 2, "endOffset": 19}, {"referenceID": 5, "context": ", [55, 6, 5, 14, 1]), but, in our opinion, no thorough study about their properties has been developed so far.", "startOffset": 2, "endOffset": 19}, {"referenceID": 4, "context": ", [55, 6, 5, 14, 1]), but, in our opinion, no thorough study about their properties has been developed so far.", "startOffset": 2, "endOffset": 19}, {"referenceID": 13, "context": ", [55, 6, 5, 14, 1]), but, in our opinion, no thorough study about their properties has been developed so far.", "startOffset": 2, "endOffset": 19}, {"referenceID": 0, "context": ", [55, 6, 5, 14, 1]), but, in our opinion, no thorough study about their properties has been developed so far.", "startOffset": 2, "endOffset": 19}, {"referenceID": 1, "context": "One example of the use of games for evaluating AI is the ALE (Arcade Learning Environment) [2], a framework where a set of arcade computer games are used to evaluate the performance of current AI algorithms.", "startOffset": 91, "endOffset": 94}, {"referenceID": 54, "context": ", [57, 15]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": ", [57, 15]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 37, "context": "6 RoboCup Soccer As an example of a 3D space game we find the RoboCup Soccer competition [38].", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": ", as in the spirit of the Darwin-Wallace distribution [18]).", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "[1] Majid Nili Ahmadabadi and Masoud Asadpour.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] MG Bellemare, Y Naddaf, J Veness, and M Bowling.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Miroslav Benda.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Darse Billings.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Joerg Denzinger and Michael Kordt.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] J\u00f6rg Denzinger and Matthias Fuchs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Peter Frankl.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Marcus Gallagher and Amanda Ryan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Jos\u00e9 Hern\u00e1ndez-Orallo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Jos\u00e9 Hern\u00e1ndez-Orallo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Jos\u00e9 Hern\u00e1ndez-Orallo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] Jos\u00e9 Hern\u00e1ndez-Orallo and David L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] Javier Insa-Cabrera, Jos\u00e9-Luis Benacloch-Ayuso, and Jos\u00e9 Hern\u00e1ndez-Orallo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] David Keil and Dina Q.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38] Hiroaki Kitano, Minoru Asada, Yasuo Kuniyoshi, Itsuki Noda, and Eiichi Osawa.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[39] Shane Legg and Marcus Hutter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[40] Michael L Mac Kinney and Robert Milton Schoch.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[41] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[42] Roger B Myerson.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[43] Martin J Osborne.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[44] Esben H Ostergaard, Gaurav S Sukhatme, and Maja J Matari.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[45] Barney Pell.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[47] Alan Robinson and Lee Spector.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[48] Alvin E Roth.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[49] John Rust, Richard Palmer, and John H Miller.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[50] Sandip Sen, Ip Sen, Mahendra Sekaran, and John Hale.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[51] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[52] Sara J Shettleworth.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[53] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[54] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "[55] Ming Tan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "[57] Joel Veness, Kee Siong Ng, Marcus Hutter, William Uther, and David Silver.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "[58] Philip E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "[59] Edward A Wasserman and Thomas R Zentall.", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "[60] David Wechsler.", "startOffset": 0, "endOffset": 4}, {"referenceID": 58, "context": "[61] J\u00f6rgen W Weibull.", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "[62] Susanne Weis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "[63] Michael P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[64] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "[65] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "[66] Z.", "startOffset": 0, "endOffset": 4}, {"referenceID": 64, "context": "[67] Byoung-Tak Zhang and Dong-Yeon Cho.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "Social intelligence in natural and artificial systems is usually measured by the evaluation of associated traits or tasks that are deemed to represent some facets of social behaviour. The amalgamation of these traits is then used to configure the intuitive notion of social intelligence. Instead, in this paper we start from a parametrised definition of social intelligence as the expected performance in a set of environments with several agents, and we assess and derive tests from it. This definition makes several dependencies explicit: (1) the definition depends on the choice (and weight) of environments and agents, (2) the definition may include both competitive and cooperative behaviours depending on how agents and rewards are arranged into teams, (3) the definition mostly depends on the abilities of other agents, and (4) the actual difference between social intelligence and general intelligence (or other abilities) depends on these choices. As a result, we address the problem of converting this definition into a more precise one where some fundamental properties ensuring social behaviour (such as action and reward dependency and anticipation on competitive/cooperative behaviours) are met as well as some other more instrumental properties (such as secernment, boundedness, symmetry, validity, reliability, efficiency), which are convenient to convert the definition into a practical test. From the definition and the formalised properties, we take a look at several representative multi-agent environments, tests and games to see whether they meet these properties.", "creator": "LaTeX with hyperref package"}}}