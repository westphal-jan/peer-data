{"id": "1409.6052", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2014", "title": "Oblivious Bounds on the Probability of Boolean Functions", "abstract": "This paper develops upper and lower bounds for the probability of Boolean functions by treating multiple occurrences of variables as independent and assigning them new individual probabilities. We call this approach dissociation and give an exact characterization of optimal oblivious bounds, i.e. when the new probabilities are chosen independent of the probabilities of all other variables. Our motivation comes from the weighted model counting problem (or, equivalently, the problem of computing the probability of a Boolean function), which is #P-hard in general. By performing several dissociations, one can transform a Boolean formula whose probability is difficult to compute, into one whose probability is easy to compute, and which is guaranteed to provide an upper or lower bound on the probability of the original formula by choosing appropriate probabilities for the dissociated variables. Our new bounds shed light on the connection between previous relaxation-based and model-based approximations and unify them as concrete choices in a larger design space. We also show how our theory allows a standard relational database management system (DBMS) to both upper and lower bound hard probabilistic queries in guaranteed polynomial time.", "histories": [["v1", "Sun, 21 Sep 2014 23:32:34 GMT  (1663kb,D)", "http://arxiv.org/abs/1409.6052v1", "34 pages, 14 figures, supersedes:this http URL"]], "COMMENTS": "34 pages, 14 figures, supersedes:this http URL", "reviews": [], "SUBJECTS": "cs.AI cs.DB", "authors": ["wolfgang gatterbauer", "dan suciu"], "accepted": false, "id": "1409.6052"}, "pdf": {"name": "1409.6052.pdf", "metadata": {"source": "CRF", "title": "Oblivious Bounds on the Probability of Boolean Functions", "authors": ["WOLFGANG GATTERBAUER", "DAN SUCIU"], "emails": ["gatt@cmu.edu;"], "sections": [{"heading": null, "text": "1 Oblivious Bounds on the Probability of Boolean Functions\nWOLFGANG GATTERBAUER, Carnegie Mellon University DAN SUCIU, University of Washington\nThis paper develops upper and lower bounds for the probability of Boolean functions by treating multiple occurrences of variables as independent and assigning them new individual probabilities. We call this approach dissociation and give an exact characterization of optimal oblivious bounds, i.e. when the new probabilities are chosen independent of the probabilities of all other variables. Our motivation comes from the weighted model counting problem (or, equivalently, the problem of computing the probability of a Boolean function), which is #P-hard in general. By performing several dissociations, one can transform a Boolean formula whose probability is difficult to compute, into one whose probability is easy to compute, and which is guaranteed to provide an upper or lower bound on the probability of the original formula by choosing appropriate probabilities for the dissociated variables. Our new bounds shed light on the connection between previous relaxation-based and model-based approximations and unify them as concrete choices in a larger design space. We also show how our theory allows a standard relational database management system (DBMS) to both upper and lower bound hard probabilistic queries in guaranteed polynomial time.\nCategories and Subject Descriptors: G.3 [Probability and Statistics]; H.2.m [Database Management]: Miscellaneous; I.1.1 [Symbolic and algebraic manipulation]: Expressions and Their Representation\nGeneral Terms: Algorithms, Theory\nAdditional Key Words and Phrases: Probabilistic databases, Weighted model counting, Boolean expressions, Oblivious approximations, Relaxation"}, {"heading": "1. INTRODUCTION", "text": "Query evaluation on probabilistic databases is based on weighted model counting for positive Boolean expressions. Since model counting is #P-hard in general, today\u2019s probabilistic database systems evaluate queries using one of the following three approaches: (1) incomplete approaches identify tractable cases (e.g., read-once formulas) either at the query-level [Dalvi and Suciu 2007; Dalvi et al. 2010] or the datalevel [Olteanu and Huang 2008; Sen et al. 2010]; (2) exact approaches apply exact probabilistic inference, such as repeated application of Shannon expansion [Olteanu et al. 2009] or tree-width based decompositions [Jha et al. 2010]; and (3) approximate approaches either apply general purpose sampling methods [Jampani et al. 2008; Kennedy and Koch 2010; Re et al. 2007] or approximate the number of models of the Boolean lineage expression [Olteanu et al. 2010; Fink and Olteanu 2011].\nThis paper provides a new algebraic framework for approximating the probability of positive Boolean expressions. While our method was motivated by query evaluation on probabilistic databases, it is more general and applies to all problems that rely on weighted model counting, e.g., general probabilistic inference in graphical mod-\nAuthors\u2019 addresses: W. Gatterbauer (corresponding author), Tepper School of Business, Carnegie Mellon University; email: gatt@cmu.edu; D. Suciu, Computer Science and Engineering Department, University of Washington, Seattle. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org. c\u00a9 YYYY ACM 0362-5915/YYYY/01-ART1 $10.00 DOI 10.1145/0000000.0000000 http://doi.acm.org/10.1145/0000000.0000000\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nar X\niv :1\n40 9.\n60 52\nv1 [\ncs .A\nI] 2\n1 Se\np 20\n14\nels [Chavira and Darwiche 2008].1 An important aspect of our method is that it is not model-based in the traditional sense. Instead, it enlarges the original variable space by treating multiple occurrences of variables as independent and assigning them new individual probabilities. We call this approach dissociation2 and explain where existing relaxation-based and model-based approximations fit into this larger space of approximations. We characterize probability assignments that lead to guaranteed upper or lower bounds on the original expression and identify the best possible oblivious bounds, i.e. after looking at only a limited scope of the expression. We prove that for every model-based bound there is always a dissociation-based bound that is as good or better. And we illustrate how a standard relational DBMS can both upper and lower bound hard probabilistic conjunctive queries without self-joins with appropriate SQL queries that use dissociation in a query-centric way.\nWe briefly discuss our results: We want to compute the probability P[\u03d5] of a Boolean expression \u03d5 when each of its Boolean variables xi is set independently to true with some given probability pi = P[xi]. Computing P[\u03d5] is known to be #P-hard in general [Valiant 1979] and remains hard to even approximate [Roth 1996]. Our approach is to approximate P[\u03d5] with P[\u03d5\u2032] that is easier to compute. The new formula \u03d5\u2032 is derived from \u03d5 through a sequence of dissociation steps, where each step replaces d distinct occurrences of some variable x in \u03d5 with d fresh variables x\u20321, x\u20322, . . . x\u2032d. Thus,\n1Note that weighted model counting is essentially the same problem as computing the probability P[\u03d5] of a Boolean expression \u03d5. Each truth assignment of the Boolean variables corresponds to one model whose weight is the probability of this truth assignment. Weighted model counting then asks for the sum of the weights of all satisfying assignments. 2Dissociation is the breaking of an existing association between related, but not necessarily identical items.\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nafter applying dissociation repeatedly, we transform \u03d5 into another expression \u03d5\u2032 and approximate P[\u03d5] with P[\u03d5\u2032]. The question that we address in this paper is: how should we set the probabilities of the dissociated variables x\u2032i in order to ensure that P[\u03d5\u2032] is a good approximation of P[\u03d5]? In particular, we seek conditions under which \u03d5\u2032 is guaranteed to be either an upper bound P[\u03d5\u2032] \u2265 P[\u03d5] or a lower bound P[\u03d5\u2032] \u2264 P[\u03d5].\nOur main result can be summarized as follows: Suppose that x occurs positively in \u03d5. Dissociate x into two variables x\u20321 and x\u20322 such that the dissociated formula is \u03d5\u2032 = \u03d5\u20321 \u2227 \u03d5\u20322, where x\u20321 occurs only in \u03d5\u20321 and x\u20322 occurs only in \u03d5\u20322; in other words, \u03d5 \u2261 \u03d5\u20321[x\u20321/x] \u2227 \u03d5\u20322[x\u20322/x] (we will later define this as \u201cconjunctive dissociation\u201d). Let p = P[x], p\u20321 = P[x\u20321], p\u20322 = P[x\u20322] be their probabilities. Then (1) P[\u03d5\u2032] \u2265 P[\u03d5] iff p\u20321 \u00b7 p\u20322 \u2265 p, and (2) P[\u03d5\u2032] \u2264 P[\u03d5] iff p\u20321 \u2264 p and p\u20322 \u2264 p. In particular, the best upper bounds are obtained by choosing any p\u20321, p\u20322 that satisfy p\u20321 \u00b7 p\u20322 = p, and the best lower bound is obtained by setting p\u20321 = p\u20322 = p. The \u201conly if\u201d direction holds assuming \u03d5\u2032 satisfies certain mild conditions (e.g., it should not be redundant), and under the assumption that p\u20321, p\u20322 are chosen obliviously, i.e. they are functions only of p = P[x] and independent of the probabilities of all other variables. This restriction to oblivious probabilities guarantees the computation of the probabilities p\u20321, p\u20322 to be very simple.3 Our result extends immediately to the case when the variable x is dissociated into several variables x\u20321, x \u2032 2, . . . , x \u2032 d, and also extends (with appropriate changes) to the case when the expressions containing the dissociated variables are separated by \u2228 rather than \u2227 (Fig.1). Example 1.1 (2CNF Dissociation). For a simple illustration of our main result, consider a Positive-Partite-2CNF expression with |E| clauses \u03d5 = \u2227\n(i,j)\u2208E (xi \u2228 yj) (1)\nfor which calculating its probability is already #P-hard [Provan and Ball 1983]. If we dissociate all occurrences of all m variables xi, then the expression becomes:\n\u03d5\u2032 = \u2227\n(i,j)\u2208E (x\u2032i,j \u2228 yj) (2)\nwhich is equivalent to \u2227 j ( yj \u2228 \u2227 i,j x \u2032 i,j ) . This is a read-once expression whose probability can always be computed in PTIME [Gurvich 1977]. Our main result implies the following: Let pi = P[xi], i \u2208 [m] be the probabilities of the original variables and denote p\u2032i,j = P[x\u2032i,j ] the probabilities of the fresh variables. Then (1) if \u2200i \u2208 [m] : p\u2032i,j1 \u00b7 p\u2032i,j2 \u00b7 \u00b7 \u00b7 p\u2032i,jdi = pi, then \u03d5\n\u2032 is an upper bound (P[\u03d5\u2032] \u2265 P[\u03d5]); (2) if \u2200i \u2208 [m] : p\u2032i,j1 = p\u2032i,j2 = . . . = p\u2032i,jdi = pi, then \u03d5\n\u2032 is a lower bound (P[\u03d5\u2032] \u2264 P[\u03d5]). Furthermore, these are the best possible oblivious bounds, i.e. where p\u2032i,j depends only on pi = P[xi] and is chosen independently of other variables in \u03d5.\nWe now explain how dissociation generalizes two other approximation methods in the literature (Fig.1 gives a high-level summary and Sect.5 the formal details).\nRelaxation & Compensation. This is a framework by Choi and Darwiche [2009; 2010] for approximate probabilistic inference in graphical models. The approach performs exact inference in an approximate model that is obtained by relaxing equiv-\n3 Our usage of the term oblivious is inspired by the notion of oblivious routing algorithms [Valiant 1982] which use only local information and which can therefore be implemented very efficiently. Similarly, our oblivious framework forces p\u20321, p \u2032 2 to be computed only as a function of p, without access to the rest of \u03d5. One can always find values p\u20321, p \u2032 2 for which P[\u03d5] = P[\u03d5\u2032]. However, to find those value in general, one has to first compute q = P[\u03d5], then find appropriate values p\u20321, p\u20322 for which the equality P[\u03d5\u2032] = q holds. This is not practical, since our goal is to compute q in the first place.\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nalence constraints in the original model, i.e. by removing edges. The framework allows one to improve the resulting approximations by compensating for the relaxed constraints. In the particular case of a conjunctive Boolean formula \u03d5 = \u03d51 \u2227 \u03d52, relaxation refers to substituting any variable x that occurs in both \u03d51 and \u03d52 with two fresh variables x\u20321 in \u03d51 and x\u20322 in \u03d52. Compensation refers to then setting their probabilities p\u20321 = P[x\u20321] and p\u20322 = P[x\u20322] to p\u20321 = p and p\u20322 = P[x|\u03d51]. This new probability assignment is justified by the fact that, if x is the only variable shared by \u03d51 and \u03d52, then compensation ensures that P[\u03d5\u2032] = P[\u03d5] (we will show this claim in Prop. 5.1). In general, however, \u03d51, \u03d52 have more than one variable in common, and in this case we have P[\u03d5\u2032] 6= P[\u03d5] for the same compensation. Thus in general, compensation is applied as a heuristics. Furthermore, it is then not known whether compensation provides an upper or lower bound.\nIndeed, let p\u20321 = p, p\u20322 = P[x|\u03d51] be the probabilities set by the compensation method. Recall that our condition for P[\u03d5\u2032] to be an upper bound is p\u20321 \u00b7 p\u20322 \u2265 p, but we have p\u20321 \u00b7 p\u20322 = p \u00b7 P[x|\u03d51] \u2264 p. Thus, the compensation method does not satisfy our oblivious upper bound condition. Similarly, because of p\u20321 = p and p\u20322 \u2265 p, these values fail to satisfy our oblivious lower bound condition. Thus, relaxation is neither a guaranteed upper bound, nor a guaranteed lower bound. In fact, relaxation is not oblivious at all (since p\u20322 is computed from the probabilities of all variables, not just P[x]). This enables it to be an exact approximation in the special case of a single shared variable, but fails to guarantee any bounds, in general.\nModel-based approximations. Another technique for approximation described by Fink and Olteanu [2011] is to replace \u03d5 with another expression whose set of models is either a subset or superset of those of \u03d5. Equivalently, the upper bound is a formula \u03d5U such that \u03d5 \u21d2 \u03d5U , and the lower bound is \u03d5L such that \u03d5L \u21d2 \u03d5. We show in this paper, that if \u03d5 is a positive Boolean formula, then all upper and lower modelbased bounds can be obtained by repeated dissociation: the model-based upper bound is obtained by repeatedly setting probabilities of dissociated variables to 1, and the model-based lower bound by setting the probabilities to 0. While the thus generated model-based upper bounds for conjunctive expressions correspond to optimal oblivious dissociation bounds, the model-based lower bounds for conjunctive expressions are not optimal and can always be improved by dissociation.\nIndeed, consider first the upper bound for conjunctions: the implication \u03d5 \u21d2 \u03d5U holds iff there exists a formula \u03d51 such that \u03d5 \u2261 \u03d51\u2227\u03d5U .4 Pick a variable x, denote p = P[x] its probability, dissociate it into x\u20321 in \u03d51 and x\u20322 in \u03d5U , and set their probabilities as p\u20321 = 1 and p\u20322 = p. Thus, \u03d5U remains unchanged (except for the renaming of x to x\u20322), while in \u03d51 we have set x1 = 1. By repeating this process, we eventually transform \u03d51 into true (Recall that our formula is monotone). Thus, model-based upper bounds are obtained by repeated dissociation and setting p\u20321 = 1 and p\u20322 = p at each step. Our results show that this is only one of many oblivious upper bounds as any choices with p\u20321p \u2032 2 \u2265 p lead to an oblivious upper bound for conjunctive dissociations.\nConsider now the lower bound: the implication \u03d5L \u21d2 \u03d5 is equivalent to \u03d5L \u2261 \u03d5 \u2227 \u03d52. Then there is a sequence of finitely many conjunctive dissociation steps, which transforms \u03d5 into \u03d5 \u2227 \u03d52 and thus into \u03d5L. At each step, a variable x is dissociated into x\u20321 and x\u20322, and their probabilities are set to p\u20321 = p and p\u20322 = 0, respectively.5 According to our result, this choice is not optimal: instead one obtains a tighter bound by also setting p\u20322 = p, which no longer corresponds to a model-based lower bound.\n4Fink and Olteanu [2011] describe their approach for approximating DNF expressions only. However, the idea of model-based bounds applies equally well to arbitrary Boolean expressions, including those in CNF. 5The details here are more involved and are given in detail in Sect.5.2.\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nThus, model-based lower bounds for conjunctive expressions are not optimal and can always be improved by using dissociation.\nOur dual result states the following for the case when the two formulas are connected with disjunction \u2228 instead of conjunction \u2227: (1) the dissociation is an upper bound iff p\u20321 \u2265 p and p\u20322 \u2265 p, and (2) it is a lower bound iff (1 \u2212 p\u20321)(1 \u2212 p\u20322) \u2265 1 \u2212 p. We see that model-based approximation gives an optimal lower bound for disjunctions, because (1\u2212 p\u20321)(1\u2212 p\u20322) = 1 \u00b7 (1\u2212 p) = 1\u2212 p, however non-optimal upper bounds. Example 7.2 illustrates this asymmetry and the possible improvement through dissociation with a detailed simulation-based example.\nBounds for hard probabilistic queries. Query evaluation on probabilistic databases reduces to the problem of computing the probability of its lineage expression which is a a monotone, k-partite Boolean DNF where k is fixed by the number of joins in the query. Computing the probability of the lineage is known to be #P-hard for some queries [Dalvi and Suciu 2007], hence we are interested in approximating these probabilities by computing dissociated Boolean expressions for the lineage. We have previously shown in [Gatterbauer et al. 2010] that every query plan for a query corresponds to one possible dissociation for its lineage expression. The results in this paper show how to best set the probabilities for the dissociated expressions in order to obtain both upper bounds and lower bounds. We further show that all the computation can be pushed inside a standard relational database engine with the help of SQL queries that use User-Defined-Aggregates and views that replace the probabilities of input tuples with their optimal symmetric lower bounds. We illustrate this approach in Sect.6 and validate it on TPC-H data in Sect.7.5.\nMain contributions. (1) We introduce an algebraic framework for approximating the probability of Boolean functions by treating multiple occurrences of variables as independent and assigning them new individual probabilities. We call this approach dissociation; (2) we determine the optimal upper and lower bounds for conjunctive and disjunctive dissociations under the assumption of oblivious value assignments; (3) we show how existing relaxation-based and model-based approximations fit into the larger design space of dissociations, and show that for every model-based bound there is at least one dissociation-based bound which is as good or tighter; (4) we apply our general framework to both upper and lower bound hard probabilistic conjunctive queries without self-joins in guaranteed PTIME by translating the query into a sequence of standard SQL queries; and (5) we illustrate and evaluate with several detailed examples the application of this technique. Note that this paper does not address the algorithmic complexities in determining alternative dissociations, in general.\nOutline. Section 2 starts with some notational background, and Sect. 3 formally defines dissociation. Section 4 contains our main results on optimal oblivious bounds. Section 5 formalizes the connection between relaxation, model-based bounds and dissociation, and shows how both previous approaches can be unified under the framework of dissociation. Section 6 applies our framework to derive upper and lower bounds for hard probabilistic queries with standard relational database management systems. Section 7 gives detailed illustrations on the application of dissociation and oblivious bounds. Finally, Sect.8 relates to previous work before Sect.9 concludes."}, {"heading": "2. GENERAL NOTATIONS AND CONVENTIONS", "text": "We use [m] as short notation for {1, . . . ,m}, use the bar sign for the complement of an event or probability (e.g., x\u0304 = \u00acx, and p\u0304 = 1\u2212 p), and use a bold notation for sets (e.g., s \u2286 [m]) or vectors (e.g., x = \u3008x1, . . . , xm\u3009) alike. We assume a set x of independent Boolean random variables, and assign to each variable xi a primitive event which is true with probability pi = P[xi]. We do not formally distinguish between the variable xi and the event xi that it is true. By default, all primitive events are assumed to be\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nindependent (e.g., P[x1 \u2227 x2] = p1p2). We are interested in bounding the probability P[f ] of a Boolean function f , i.e. the probability that the function is true if each of the variables is independently true or false with given probabilities. When no confusion arises, we blur the distinction between a Boolean expression \u03d5 and the Boolean function f\u03d5 it represents (cf. [Crama and Hammer 2011, Sect. 1.2]) and write P[\u03d5] instead of P[f\u03d5]. We also use the words formula and expression interchangeably. We write f(x) to indicate that x is the set of primitive events appearing in the function f , and f [x1/x] to indicate that x1 is substituted for x in f . We often omit the operator \u2227 and denote conjunction by mere juxtaposition instead."}, {"heading": "3. DISSOCIATION OF BOOLEAN FUNCTIONS AND EXPRESSIONS", "text": "We define here dissociation formally. Let f(x,y) and f \u2032(x\u2032,y) be two Boolean functions, where x,x\u2032,y are three disjoint sets of variables. Denote |x| = m, |x\u2032| = m\u2032, and |y| = n. We restrict f and f \u2032 to be positive in x and x\u2032, respectively [Crama and Hammer 2011, Def. 1.24].\nDefinition 3.1 (Dissociation). We call a function f \u2032 a dissociation of f if there exists a substitution \u03b8 : x\u2032 \u2192 x s.t. f \u2032[\u03b8] = f .\nExample 3.2 (CNF Dissociation). Consider two functions f and f \u2032 given by CNF expressions\nf = (x1 \u2228 y1)(x1 \u2228 y2)(x2 \u2228 y1)(x2 \u2228 y3) f \u2032 = (x\u20321,1 \u2228 y1)(x\u20321,2 \u2228 y2)(x\u20322 \u2228 y1)(x\u20322 \u2228 y3)\nThen f \u2032 is a dissociation of f as f \u2032[\u03b8] = f for the substitution \u03b8 = {(x\u20321,1, x1), (x\u20321,2, x1), (x\u20322, x2)}. Figure 2 shows the CNF expressions\u2019 primal graphs.6\nIn practice, to find a dissociation for a function f(x,y), one proceeds like this: Choose any expression \u03d5(x,y) for f and thus f = f\u03d5. Replace the ki distinct occurrences of variables xi in \u03d5 with di fresh variables x\u2032i,1, x\u2032i,2, . . . , x\u2032i,di , with di \u2264 ki. The resulting expression \u03d5\u2032 represents a function f \u2032 that is a dissociation of f . Notice that we may obtain different dissociations by deciding for which occurrences of xi to use distinct fresh variables, and for which occurrences to use the same variable. We may further obtain more dissociations by starting with different, equivalent expressions \u03d5 for the function f . In fact, we may construct infinitely many dissociations this way. We also note that every dissociation of f can be obtained through the process outlined here. Indeed, let f \u2032(x\u2032,y) be a dissociation of f(x,y) according to Definition 3.1, and let \u03b8 be the substitution for which f \u2032[\u03b8] = f . Then, if \u03d5\u2032 is any expression representing f \u2032, the expression \u03d5 = \u03d5\u2032[\u03b8] represents f . We can thus apply the described dissociation process to a certain expression \u03d5 and obtain f \u2032.\n6The primal graph of a CNF (DNF) has one node for each variable and one edge for each pair of variables that co-occur in some clause (conjunct). This concept originates in constraint satisfaction and it is also varyingly called co-occurrence graph or variable interaction graph [Crama and Hammer 2011].\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nExample 3.3 (Alternative Dissociations). Consider the two expressions: \u03d5 = (x \u2228 y1)(x \u2228 y2)(x \u2228 y3)(y4 \u2228 y5) \u03c8 = xy4 \u2228 xy5 \u2228 y1y2y3y4 \u2228 y1y2y3y5\nBoth are equivalent (\u03d5 \u2261 \u03c8) and thus represent the same Boolean function (f\u03d5 = f\u03c8). Yet each leads to a quite different dissociation in the variable x:\n\u03d5\u2032 = (x\u20321 \u2228 y1)(x\u20322 \u2228 y2)(x\u20323 \u2228 y3)(y4 \u2228 y5) \u03c8\u2032 = x\u20321y4 \u2228 x\u20322y5 \u2228 y1y2y3y4 \u2228 y1y2y3y5\nHere, \u03d5\u2032 and \u03c8\u2032 represent different functions (f\u03d5\u2032 6= f\u03c8\u2032) and are both dissociations of f for the substitutions \u03b81 = {(x\u20321, x), (x\u20322, x), (x\u20323, x)} and \u03b82 = {(x\u20321, x), (x\u20322, x)}, respectively.\nExample 3.4 (More alternative Dissociations). Consider the AND-function f(x, y) = xy. It can be represented by the expressions xxy, or xxxy, etc., leading to the dissociations x\u20321x\u20322y, or x\u20321x\u20322x\u20323y, etc. For even more dissociations, represent f using the expression (x \u2228 x)y \u2228 xy, which can dissociate to (x\u20321 \u2228 x\u20322)y \u2228 x\u20323y, or (x\u20321 \u2228 x\u20322)y \u2228 x\u20321y, etc. Note that several occurrences of a variable can be replaced by the same new variables in the dissociated expression."}, {"heading": "4. OBLIVIOUS BOUNDS FOR DISSOCIATED EVENT EXPRESSIONS", "text": "Throughout this section, we fix two Boolean functions f(x,y) and f \u2032(x\u2032,y) such that f \u2032 is a dissociation of f . We are given the probabilities p = P[x] and q = P[y]. Our goal is to find probabilities p\u2032 = P[x\u2032] of the dissociated variables so that P[f \u2032] is an upper or lower bound for P[f ]. We first define oblivious bounds (Sect. 4.1), then characterize them, in general, through valuations (Sect.4.2) and, in particular, for conjunctive and disjunctive dissociations (Sect. 4.3), then derive optimal bounds (Sect. 4.4), and end with illustrated examples for CNF and DNF dissociations (Sect.4.5)."}, {"heading": "4.1. Definition of Oblivious Bounds", "text": "We use the subscript notation Pp,q[f ] and Pp\u2032,q[f \u2032] to emphasize that the probability space is defined by the probabilities p = \u3008p1, p2, . . .\u3009, q = \u3008q1, q2, . . .\u3009, and p\u2032 = \u3008p\u20321, p\u20322, . . .\u3009, respectively. Given p and q, our goal is thus to find p\u2032 such that Pp\u2032,q[f \u2032] \u2265 Pp,q[f ] or Pp\u2032,q[f \u2032] \u2264 Pp,q[f ].\nDefinition 4.1 (Oblivious Bounds). Let f \u2032 be a dissociation of f and p = P[x]. We call p\u2032 an oblivious upper bound for p and dissociation f \u2032 of f iff \u2200q : Pp\u2032,q[f \u2032] \u2265 Pp,q[f ]. Similarly, p\u2032 is an oblivious lower bound iff \u2200q : Pp\u2032,q[f \u2032] \u2264 Pp,q[f ].\nIn other words, p\u2032 is an oblivious upper bound if the probability of the dissociated function f \u2032 is bigger than that of f for every choice of q. Put differently, the probabilities of x\u2032 depend only on the probabilities of x and not on those of y.\nAn immediate upper bound is given by p\u2032 = 1, since f is monotone and f \u2032[1/x\u2032] = f [1/x]. Similarly, p = 0 is a na\u0131\u0308ve lower bound. This proves that the set of upper and lower bounds is never empty. Our next goal is to characterize all oblivious bounds and to then find optimal ones."}, {"heading": "4.2. Characterization of Oblivious Bounds through Valuations", "text": "We will give a necessary and sufficient characterization of oblivious bounds, but first we need to introduce some notations. If f(x,y) is a Boolean function, let \u03bd : y \u2192 {0, 1} be a truth assignment or valuation for y. We use \u03bd for the vector \u3008\u03bd(y1), . . . , \u03bd(yn)\u3009, and denote with f [\u03bd] the Boolean function obtained after applying the substitution \u03bd. Note that f [\u03bd] depends on variables x only. Furthermore, let g be n Boolean functions,\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nover variables z. We denote with g\u03bd the Boolean function g\u03bd = \u2227 j g \u03bd j , where g\u03bdj = g\u0304j if \u03bd(yj) = 0 and g\u03bdj = gj if \u03bd(yj) = 1.\nExample 4.2 (Valuation Notation). Assume g = \u3008g1, g2\u3009 with g1 = z1z2 and g2 = z1z3, and \u03bd = \u30080, 1\u3009. Then g\u03bd = \u00ac(z1z2)\u2227z1z3 = z1z\u03042z3. Figure 3 illustrates our notation for this simple example with the help of Karnaugh maps. We encourage the reader to take a moment and carefully study the correspondences between g, \u03bd, and g\u03bd .\nThen, any function f(x,y) admits the following expansion by the y-variables:\nf(x,y) = \u2228\n\u03bd\n( f [\u03bd] \u2227 y\u03bd ) (3)\nNote that any two expressions in the expansion above are logically contradictory, a property called determinism by Darwiche and Marquis [2002], and that it can be seen as the result of applying Shannon\u2019s expansion to all variables of y.\nExample 4.3 (Valuation Notation continued). Consider the function f = (x\u2228y1)(x\u2228 y2). For the example valuation \u03bd = \u30080, 1\u3009, we have f [\u03bd] = (x\u22280)(x\u22281) = x and y\u03bd = y\u03041y2. Equation 3 gives us an alternative way to write f as disjunction over all 22 valuations of y as f = x(y\u03041y\u03042) \u2228 x(y1y\u03042) \u2228 x(y\u03041y2) \u2228 y1y2.\nThe following proposition is a necessary and sufficient condition for oblivious upper and lower bounds, based on valuations.\nPROPOSITION 4.4 (OBLIVIOUS BOUNDS AND VALUATIONS). Fix two Boolean functions f(x,y), f \u2032(x\u2032,y) s.t. f \u2032 is a dissociation of f , and let p and p\u2032 denote the probabilities of the variables x and x\u2032, respectively. Then p\u2032 is an oblivious upper bound iff Pp\u2032 [f \u2032[\u03bd]] \u2265 Pp[f [\u03bd]] for every valuation \u03bd for y. The proposition holds similarly for oblivious lower bounds.\nPROOF. Remember that any two events in Eq. 3 are disjoint. The total probability theorem thus allows us to sum over the probabilities of all conjuncts:\nPp,q[f(x,y)] = \u2211\n\u03bd\n( Pp[f [\u03bd]] \u00b7 Pq[y\u03bd ] )\nPp\u2032,q[f \u2032(x,y)] = \u2211\n\u03bd\n( Pp\u2032 [f \u2032[\u03bd]] \u00b7 Pq[y\u03bd ] )\nThe \u201cif\u201d direction follows immediately. For the \u201conly if\u201d direction, assume that p\u2032 is an oblivious upper bound. By definition, Pp\u2032,q[f \u2032] \u2265 Pp,q[f ] for every q. Fix any valuation \u03bd : y \u2192 {0, 1}, and define the following probabilities q: qi = 0 when \u03bd(yi) = 0, and qi = 1 when \u03bd(yi) = 1. It is easy to see that Pp,q[f ] = Pp[f [\u03bd]], and similarly, Pp\u2032,q[f \u2032] = Pp\u2032 [f \u2032[\u03bd]], which proves Pp\u2032 [f \u2032[\u03bd]] \u2265 Pp[f [\u03bd]].\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nA consequence of choosing p\u2032 obliviously is that it remains a bound even if we allow the variables y to be arbitrarily correlated. More precisely:\nCOROLLARY 4.5 (OBLIVIOUS BOUNDS AND CORRELATIONS). Let f \u2032(x\u2032,y) be a dissociation of f(x,y), let p\u2032 be an oblivious upper bound for p, and let g = \u3008g1, . . . , g|y|\u3009 be Boolean functions in some variables z with probabilities r = P[z]. Then: Pp\u2032,r[f \u2032 ( x\u2032,g(z) ) ] \u2265 Pp,r[f ( x,g(z) ) ]. The result for oblivious lower bounds is similar.\nThe intuition is that, by substituting the variables y with functions g in f(x,y), we make y correlated. The corollary thus says that an oblivious upper bound remains an upper bound even if the variables y are correlated. This follows from folklore that any correlation between the variables y can be captured by general Boolean functions g. For completeness, we include the proof in Appendix B.\nPROOF OF COROLLARY 4.5. We derive the probabilities of f and f \u2032 from Eq.3:\nPp,r[f(x,g)] = \u2211\n\u03bd\n( Pp[f [\u03bd]] \u00b7 Pr[g\u03bd ] )\nPp\u2032,r[f \u2032(x,g)] = \u2211\n\u03bd\n( Pp\u2032 [f \u2032[\u03bd]] \u00b7 Pr[g\u03bd ] )\nThe proof follows now immediately from Prop. 4.4."}, {"heading": "4.3. Oblivious Bounds for Unary Conjunctive and Disjunctive Dissociations", "text": "A dissociation f \u2032(x\u2032,y) of f(x,y) is called unary if |x| = 1, in which case we write the function as f(x,y). We next focus on unary dissociations, and establish a necessary and sufficient condition for probabilities to be oblivious upper or lower bounds for the important classes of conjunctive and disjunctive dissociations. The criterion also extends as a sufficient condition to non-unary dissociations, since these can be obtained as a sequence of unary dissociations.7\nDefinition 4.6 (Conjunctive and Disjunctive Dissociation). Let f \u2032(x\u2032,y) be a Boolean function in variables x\u2032,y. We say that the variables x\u2032 are conjunctive in f \u2032 if f \u2032(x\u2032,y) = \u2227 j\u2208[d] fj(x \u2032 j ,y), d = |x\u2032|. We say that a dissociation f \u2032(x\u2032,y) of f(x,y) is conjunctive if x\u2032 are conjunctive in f \u2032. Similarly, we say that x\u2032 are disjunctive in f \u2032 if f \u2032(x\u2032,y) = \u2228 j\u2208[d] fj(x \u2032 j ,y), and a dissociation is disjunctive if x\u2032 is disjunctive in f \u2032.\nThus in a conjunctive dissociation, each dissociated variable x\u2032j occurs in exactly one Boolean function fj and these functions are combined by \u2227 to obtain f \u2032. In practice, we start with f written as a conjunction, then replace x with a fresh variable in each conjunct:\nf(x,y) = \u2227 fj(x,y)\nf \u2032(x\u2032,y) = \u2227 fj(x \u2032 j ,y)\nDisjunctive dissociations are similar. Note that if x\u2032 is conjunctive in f \u2032(x\u2032,y), then for any substitution \u03bd : y\u2192 {0, 1}, f \u2032[\u03bd]\nis either 0, 1, or a conjunction of variables in x\u2032: f \u2032[\u03bd] = \u2227 j\u2208s x \u2032 j , for some set s \u2286 [d],\nwhere d = |x\u2032|. Similarly, if x\u2032 is disjunctive, then f \u2032[\u03bd] is 0, 1, or \u2228j\u2208s x\u2032j .8\n7Necessity does not always extend to non-unary dissociations. The reason is that an oblivious dissociation for x may set the probability of a fresh variable by examining all variables x, while a in sequence of oblivious dissociations each new probability P[x\u2032i,j ] may depend only on the variable xi currently being dissociated. 8 Note that for s = \u2205: f \u2032[\u03bd] = \u2227j\u2208s x\u2032j = 1 and f \u2032[\u03bd] = \u2228 j\u2208s x \u2032 j = 0.\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nWe need one more definition before we state the main result in our paper.\nDefinition 4.7 (Cover). Let x\u2032 be conjunctive in f \u2032(x\u2032,y). We say that f \u2032 covers the set s \u2286 [d] if there exists a substitution \u03bd s.t. f \u2032[\u03bd] = \u2227j\u2208s x\u2032j . Similarly, if x\u2032 is disjunctive, then we say that f \u2032 covers s if there exists \u03bd s.t. f \u2032[\u03bd] = \u2228 j\u2208s x \u2032 j .\nTHEOREM 4.8 (OBLIVIOUS BOUNDS). Let f \u2032(x\u2032,y) be a conjunctive dissociation of f(x,y), and let p = P[x], p\u2032 = P[x\u2032] be probabilities of x and x\u2032, respectively. Then:\n(1) If p\u2032j \u2264 p for all j, then p\u2032 is an oblivious lower bound for p, i.e. \u2200q : Pp\u2032,q[f \u2032] \u2264 Pp,q[f ]. Conversely, if p\u2032 is an oblivious lower bound for p and f \u2032 covers all singleton sets {j} with j \u2208 [d], then p\u2032j \u2264 p for all j.\n(2) If \u220f j p \u2032 j \u2265 p, then p\u2032 is an oblivious upper bound for p, i.e. \u2200q : Pp\u2032,q[f \u2032] \u2265 Pp,q[f ].\nConversely, if p\u2032 is an oblivious upper bound for p and f \u2032 covers the set [d], then\u220f j p \u2032 j \u2265 p.\nSimilarly, the dual result holds for disjunctive dissociations f \u2032(x\u2032,y) of f(x,y): (3) If p\u2032j \u2265 p for all j, then p\u2032 is an oblivious upper bound for p. Conversely, if p\u2032 is\nan oblivious upper bound for p and f \u2032 covers all singleton sets {j}, j \u2208 [d], then p\u2032j \u2265 p.\n(4) If \u220f j(1\u2212 p\u2032j) \u2265 1\u2212 p, then p\u2032 is an oblivious lower bound for p. Conversely, if p\u2032 is\nan oblivious lower bound for p and f \u2032 covers the set [d], then \u220f j(1\u2212 p\u2032j) \u2265 1\u2212 p.\nPROOF. We make repeated use of Prop. 4.4. We give here the proof for conjunctive dissociations only; the proof for disjunctive dissociations is dual and similar.\n(1) We need to check Pp\u2032 [f \u2032[\u03bd]] \u2264 Pp[f [\u03bd]] for every \u03bd. Since the dissociation is unary, f [\u03bd] can be only 0, 1, or x, while f \u2032[\u03bd] is 0, 1, or \u2227 j\u2208s x \u2032 j for some set s \u2286 [d].\nCase 1: f [\u03bd] = 0. We will show that f \u2032[\u03bd] = 0, which implies Pp\u2032 [f \u2032[\u03bd]] = Pp[f [\u03bd]] = 0. Recall that, by definition, f \u2032(x\u2032,y) becomes f(x,y) if we substitute x for all variables x\u2032j . Therefore, f \u2032[\u03bd][x/x\u20321, . . . , x/x\u2032d] = 0, which implies f \u2032[\u03bd] = 0 because f \u2032 is monotone in the variables x\u2032. Case 2: f [\u03bd] = 1. Then Pp\u2032 [f \u2032[\u03bd]] \u2264 Pp[f [\u03bd]] holds trivially. Case 3: f [\u03bd] = x. Then Pp[f [\u03bd]] = p, while Pp\u2032 [f \u2032[\u03bd]] = \u220f j\u2208s p \u2032 j . We prove that\ns 6= \u2205: this implies our claim, because \u220fj\u2208s p\u2032j \u2264 p\u2032j \u2264 p, for any choice of j \u2208 s. Suppose otherwise, that s = \u2205, hence f \u2032[\u03bd] = 1. Substituting all variables x\u2032 with x transforms f \u2032 to f . This implies f [\u03bd] = 1, which contradicts f [\u03bd] = x.\nFor the converse, assume that p\u2032 is an oblivious lower bound. Since f \u2032 covers {j}, there exists a substitution \u03bd s.t. f \u2032[\u03bd] = x\u2032j , and therefore f [\u03bd] = x. By Prop. 4.4 we have p\u2032j = Pp\u2032 [f \u2032[\u03bd]] \u2264 Pp[f [\u03bd]] = p, proving the claim.\n(2) Here we need to check Pp\u2032 [f \u2032[\u03bd]] \u2265 Pp[f [\u03bd]] for every \u03bd. The cases when f [\u03bd] is either 0 or 1 are similar to the cases above, so we only consider the case when f [\u03bd] = x. Then f \u2032[\u03bd] = \u2227 j\u2208s x \u2032 j and Pp\u2032 [f \u2032[\u03bd]] = \u220f j\u2208s p \u2032 j \u2265 \u220f j pj \u2265 p = Pp[f [\u03bd]].\nFor the converse, assume p\u2032 is an oblivious upper bound, and let \u03bd be the substitution for which f \u2032[\u03bd] = \u2227 j x \u2032 j (which exists since f \u2032 is covers [d]). Then\nPp\u2032 [f \u2032[\u03bd]] \u2265 Pp[f [\u03bd]] implies p \u2264 \u220f j pj ."}, {"heading": "4.4. Optimal Oblivious Bounds for Unary Conjunctive and Disjunctive Dissociations", "text": "We are naturally interested in the \u201cbest possible\u201d oblivious bounds. Call a dissociation f \u2032 non-degenerate if it covers all singleton sets {j}, j \u2208 [d] and the complete set [d]. Theorem 4.8 then implies:\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nCOROLLARY 4.9 (OPTIMAL OBLIVIOUS BOUNDS). If f \u2032 is a conjunctive dissociation of f and f \u2032 is non-degenerate, then the optimal oblivious lower bound is p\u20321 = p\u20322 = . . . = p, while the optimal oblivious upper bounds are obtained whenever p\u20321p\u20322 \u00b7 \u00b7 \u00b7 = p. Similarly, if f \u2032 is a disjunctive dissociation of f and f \u2032 is non-degenerate, then the optimal oblivious upper bound is p\u20321 = p\u20322 = . . . = p, while the optimal oblivious lower bounds are obtained whenever (1\u2212 p\u20321) \u00b7 (1\u2212 p\u20322) \u00b7 \u00b7 \u00b7 = 1\u2212 p.\nNotice that while optimal lower bounds for conjunctive dissociations and optimal upper bounds for disjunctive dissociations are uniquely defined with p\u2032j = p, there are infinitely many optimal bounds for the other directions (see Fig.1). Let us call bounds symmetric if all dissociated variable have the same probability. Then optimal symmetric upper bounds for conjunctive dissociations are p\u2032j = d \u221a p, and optimal symmetric\nlower bounds for disjunctive dissociations p\u2032j = 1\u2212 d \u221a\n1\u2212 p. We give two examples of degenerate dissociations. First, the dissociation f \u2032 = (x\u20321y1 \u2228 y3) \u2227 (x\u20322y2 \u2228 y3) does not cover either {1} nor {2}: no matter how we substitute y1, y2, y3, we can never transform f \u2032 to x\u20321. For example, f \u2032[1/y1, 0/y2, 0/y3] = 0 and f \u2032[1/y1, 0/y2, 1/y3] = 1. But f \u2032 does cover the set {1, 2} because f \u2032[1/y1, 1/y2, 0/y3] = x1x2. Second, the dissociation f \u2032 = (x\u20321 y1\u2228y2)\u2227 (x\u20322 y2\u2228y1) covers both {1} and {2}, but does not cover the entire set {1, 2}. In these cases the oblivious upper or lower bounds in Theorem 4.8 still hold, but are not necessarily optimal.\nHowever, most cases of practical interest result in dissociations that are nondegenerate, in which case the bounds in Theorem 4.8 are tight. We explain this here. Consider the original function, pre-dissociation, written in a conjunctive form:\nf(x,y) =g0 \u2227 \u2227\nj\u2208[d] (x \u2228 gj) = g0 \u2227\n\u2227\nj\u2208[d] fj (4)\nwhere each gj is a Boolean function in the variables y, and where we denoted fj = x\u2228gj . For example, if f is a CNF expression, then each fj is a clause containing x, and g0 is the conjunction of all clauses that do not contain x. Alternatively, we may start with a CNF expression, and group the clauses containing x in equivalence classes, such that each fj represents one equivalence class. For example, starting with four clauses, we group into two functions f = [(x\u2228y1)(x\u2228y2)]\u2227 [(x\u2228y3)(x\u2228y4)] = (x\u2228y1y2)\u2227(x\u2228y3y4) = f1\u2227f2. Our only assumption about Eq.4 is that it is non-redundant, meaning that none of the expressions g0 or fj may be dropped. Then we prove:\nPROPOSITION 4.10 (NON-DEGENERATE DISSOCIATION). Suppose the function f in Eq. 4 is non-redundant. Define f \u2032 = g0 \u2227 \u2227 j(x \u2032 j \u2228 gj). Then f \u2032 covers every single-\nton set {j}. Moreover, if the implication g0 \u21d2 \u2228 j gj does not hold, then f\n\u2032 also covers the set [d]. Hence f \u2032 is non-degenerate. A similar result holds for disjunctive dissociations if the dual implication g0 \u21d0 \u2227 j gj does not hold.\nPROOF. We give here the proof for conjunctive dissociations only; the proof for disjunctive dissociations follows from duality. We first prove that f \u2032 covers any singleton set {j}, for j \u2208 [d]. We claim that the following logical implication does not hold:\ng0 \u2227 \u2227\ni 6=j gi \u21d2 gj (5)\nIndeed, suppose the implication holds for some j. Then the following implication also holds: g0 \u2227 \u2227 i 6=j(x \u2228 gi) \u21d2 (x \u2228 gj), since for x = 0 it is the implication above, while for x = 1 it is a tautology. Therefore, the function fj is redundant in Eq. 4, which contradicts our assumption. Hence, the implication Eq. 5 does not hold. Let \u03bd be any\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nassignment that causes Eq. 5 to fail: thus, for all j \u2208 {0, . . . , d}, j 6= i, gi[\u03bd] = 1 and gj [\u03bd] = 0. Therefore f \u2032[\u03bd] = xj , proving that it covers {j}.\nNext, assume that g0 \u21d2 \u2228 j gj does not hold. We prove that f\n\u2032 covers [d]. Let \u03bd be any substitution that causes the implication to fail: g0[\u03bd] = 1 and gj [\u03bd] = 0 for j \u2208 [d]. Then f \u2032[\u03bd] = \u2227 j\u2208[d] x \u2032 j ."}, {"heading": "4.5. Illustrated Examples for Optimal Oblivious Bounds", "text": "We next give two examples that illustrate optimal oblivious bounds for conjunctive and disjunctive dissociations in some detail.\nExample 4.11 (CNF Dissociation). Consider the function fc given by an CNF expression and its dissociation f \u2032c:\nfc = (x \u2228 y1)(x \u2228 y2)(x \u2228 y3)y4 f \u2032c = (x \u2032 1 \u2228 y1)(x\u20322 \u2228 y2)(x\u20323 \u2228 y3)y4\nThere are 24 = 16 valuations for y = \u3008y1, y2, y3, y4\u3009. Probabilities p\u2032 = \u3008p\u20321, p\u20322, p\u20323\u3009 are thus an oblivious upper bound exactly if they satisfy the 16 inequalities given under \u201cCNF dissociation\u201d in Fig. 4a. For valuations with \u03bd4 = 0 (and thus fc[\u03bd] = 0) or all\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\n\u03bdj = 1 (and thus f \u2032c[\u03bd] = 1) the inequalities trivially hold. For the remaining 7 nontrivial inequalities, p\u20321p\u20322p\u20323 \u2265 p implies all others. Figure 4b shows the partial order between the non-trivial valuations, with x\u20321x\u20322x\u20323 implying all others. Since fc and f \u2032c are positive in x and x\u2032, respectively, it follows that optimal oblivious upper bounds are given by p\u20321p\u20322p\u20323 = p, e.g., by setting p\u2032i = 3 \u221a p for symmetric bounds.\nOblivious lower bounds are given by the 16 inequalities after inverting the inequality sign. Here we see that the three inequalities p\u2032j \u2264 p together imply the others. Hence, oblivious lower bounds are those that satisfy all three inequalities. The only optimal oblivious upper bounds are then given by p\u2032j = p.\nExample 4.12 (DNF Dissociation). Consider the function fd given by an DNF expression and its dissociation f \u2032d:\nfd = xy1 \u2228 xy2 \u2228 xy3 \u2228 y4 f \u2032d = x \u2032 1y1 \u2228 x\u20322y2 \u2228 x\u20323y3 \u2228 y4\nAn oblivious upper bound p\u2032 = \u3008p\u20321, p\u20322, p\u20323\u3009 must thus satisfy the 16 inequalities9 given under \u201cDNF dissociation\u201d in Fig. 4a. For valuations with \u03bd4 = 1 (and thus f \u2032d[\u03bd] = 1) or \u03bdj = 0 (and thus fd[\u03bd] = 0) the inequalities trivially hold. For the remaining inequalities we see that the elements of set {x\u20321, x\u20322, x\u20323} together imply all others, and that x\u20321 \u2228 x\u20322 \u2228 x\u20323 is implied by all others (Fig. 4c shows the partial order between the non-trivial valuations). Thus, an oblivious upper bound must satisfy p\u2032j \u2265 p, and the optimal one is given by p\u2032j = p. Analogously, an oblivious lower bound must satisfy p\u0304\u20321p\u0304 \u2032 2p\u0304 \u2032 3 \u2265 p\u0304. Optimal ones are given for p\u0304\u20321p\u0304\u20322p\u0304\u20323 = p\u0304, e.g., by setting p\u2032j = 1\u2212 3 \u221a p\u0304."}, {"heading": "5. RELAXATION AND MODEL-BASED BOUNDS AS DISSOCIATION", "text": "This section formalizes the connection between relaxation, model-based bounds and dissociation that was outlined in the introduction. In other words, we show how both previous approaches can be unified under the framework of dissociation."}, {"heading": "5.1. Relaxation & Compensation", "text": "The following proposition shows relaxation & compensation as conjunctive dissociation and was brought to our attention by Choi and Darwiche [2011].\nPROPOSITION 5.1 (COMPENSATION AND CONJUNCTIVE DISSOCIATION). Let f1, f2 be two monotone Boolean functions which share only one single variable x. Let f be their conjunction, and f \u2032 be the dissociation of f on x, i.e.\nf = f1 \u2227 f2 f \u2032 = f1[x \u2032 1/x] \u2227 f2[x\u20322/x]\nThen P[f ] = P[f \u2032] for P[x\u20321] = P[x] and P[x\u20322] = P[x|f1].\nPROOF OF PROP. 5.1. First, note that P[f ] = P[f1]P[f2|f1]. On the other hand, P[f \u2032] = P[f \u20321]P[f \u20322] as f \u20321 and f \u20322 are independent after dissociating on the only shared variable x. We also have P[f1] = P[f \u20321] since P[x] = P[x\u20321]. It remains to be shown that\n9Remember that the probability of a disjunction of two independent events is P[x\u20321\u2228x\u20322] = 1\u2212 p\u0304\u20321p\u0304\u20322.\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nP[f \u20322] = P[f2|f1]. Indeed: P[f \u20322] = P[x\u20322 \u00b7 f \u20322[1/x\u20322] \u2228 x\u0304\u20322 \u00b7 f \u20322[0/x\u20322]]\n= P[x\u20322] \u00b7 P[f \u20322[1/x\u20322]] + P[x\u0304\u20322] \u00b7 P[f \u20322[0/x\u20322]] = P[x|f1] \u00b7 P[f2[1/x]] + P[x\u0304|f1] \u00b7 P[f2[0/x]] = P[x|f1] \u00b7 P[f2[1/x]|f1] + P[x\u0304|f1] \u00b7 P[f2[0/x]|f1] = P[f2|f1]\nwhich proves the claim.\nNote that compensation is not oblivious, since the probability p\u20322 depends on the other variables occurring in \u03d51. Further note that, in general, \u03d51, \u03d52 have more than one variable in common, and in this case we have P[\u03d5\u2032] 6= P[\u03d5] for the same compensation. Thus in general, compensation is applied as a heuristics, and it is then not known whether it provides an upper or lower bound.\nThe dual result for disjunctions holds by replacing f1 with its negation f\u03041 in P[x\u20322] = P[x|f\u03041]. This result is not immediately obvious from the previous one and has, to our best knowledge, not been stated or applied anywhere before.\nPROPOSITION 5.2 (\u201cDISJUNCTIVE COMPENSATION\u201d). Let f1, f2 be two monotone Boolean functions which share only one single variable x. Let f be their disjunction, and f \u2032 be the dissociation of f on x, i.e. f = f1 \u2228 f2, and f \u2032 = f1[x\u20321/x] \u2228 f2[x\u20322/x]. Then P[f ] = P[f \u2032] for P[x\u20321] = P[x] and P[x\u20322] = P[x|f\u03041].\nPROOF OF PROP. 5.2. Let g = f\u0304 , g1 = f\u03041, and g2 = f\u03042. Then f = f1 \u2228 f2 is equivalent to g = g1 \u2227 g2. From Prop. 5.1, we know that P[g] = P[g\u2032], and thus P[f ] = P[f \u2032], for P[x\u20321] = P[x] and P[x\u20322] = P[x|g1] = P[x|f\u03041]."}, {"heading": "5.2. Model-based Approximation", "text": "The following proposition shows that all model-based bounds can be derived by repeated dissociation. However, not all dissociation-bounds can be explained as models since dissociation is in its essence an algebraic and not a model-based technique (dissociation creates more variables and thus changes the probability space). Therefore, dissociation can improve any existing model-based approximation approach. Example 7.2 will illustrate this with a detailed simulation-based example.\nPROPOSITION 5.3 (MODEL-BASED BOUNDS AS DISSOCIATIONS). Let f , fU be two monotone Boolean functions over the same set of variables, and for which the logical implication f \u21d2 fU holds. Then: (a) there exists a sequence of optimal conjunctive dissociations that transform f to fU , and (b) there exists a sequence of non-optimal disjunctive dissociations that transform f to fU . The dual result holds for the logical implication fL \u21d2 f : (c) there exists a sequence of optimal disjunctive dissociations that transform f to fL, and (d) there exists a sequence of non-optimal conjunctive dissociations that transform f to fL.\nPROOF OF PROP. 5.3. We focus here on the implication f \u21d2 fU . The proposition for the results fL \u21d2 f then follows from duality.\n(a) The implication f \u21d2 fU holds iff there exists a positive function f2 such that f = fU \u2227 f2. Pick a set of variables x s.t. f2[1/x] = 1, and dissociate f on x into f \u2032 = fU [x\u20321/x] \u2227 f2[x\u20322/x]. By setting the probabilities of the dissociated variables to p\u20321 = p and p\u20322 = 1, the bounds become optimal (p\u20321p\u20322 = p). Further more, fU remains unchanged (except for the renaming of x to x\u20321), whereas f2 becomes true. Hence, we get f \u2032 = fU . Thus, all model-based upper bounds can be obtained by conjunctive dissociation and choosing optimal oblivious bounds at each dissociation step.\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\n(b) The implication f \u21d2 fU also holds iff there exists a positive function fd such that fU = f \u2228 fd. Let m be the positive minterm or elementary conjunction involving all variables of f . The function fd can be then written as DNF fd = c1 \u2228 c2 \u2228 . . . , with products ci \u2286 m. Since f is monotone, we know m\u21d2 f , and thus also mfd \u21d2 f . We can therefore write f = f \u2228mfd or as\nf = f \u2228mc1 \u2228mc2 \u2228 . . . Let xi be the set of all variables in m that do not occur in ci and denote with mi the conjunction of xi. Then then each mci can instead be written as mici and thus:\nf = f \u2228m1c1 \u2228m2c2 \u2228 . . . WLOG, we now separate one particular conjunct mici and dissociate on the set xi\nf \u2032 = f \u2228m1c1 \u2228m2c2 \u2228 . . .\ufe38 \ufe37\ufe37 \ufe38 f1 [x\u20321/xi] \u2228mici\ufe38\ufe37\ufe37\ufe38 f2 [x\u20322/xi]\nBy setting the probabilities of the dissociated variables to the non-optimal upper bounds p\u20321 = p and p\u20322 = 1, f1 remains unchanged (except for the renaming of xi to x\u20321), whereas f2 becomes ci. Hence, we get f \u2032 = f \u2228 m1c1 \u2228 m2c2 \u2228 \u00b7 \u00b7 \u00b7 \u2228 ci. We can now repeat the same process for all conjuncts mci and receive after a finite number of dissociation steps\nf \u2032\u2032 = f \u2228 (c1 \u2228 c2 \u2228 . . . ) = f \u2228 fd Hence f \u2032\u2032 = fU . Thus, all model-based upper bounds can be obtained by disjunctive dissociation and choosing non-optimal bounds at each dissociation step."}, {"heading": "6. QUERY-CENTRIC DISSOCIATION BOUNDS FOR PROBABILISTIC QUERIES", "text": "Our previous work [Gatterbauer et al. 2010] has shown how to upper bound the probability of conjunctive queries without self-joins by issuing a sequence of SQL statements over a standard relational DBMS. This section illustrates such dissociationbased upper bounds and also complements them with new lower bounds. We use the Boolean query Q :\u2212R(X), S(X,Y ), T (Y ), for which the probability computation problem is known to be #P-hard, over the following database instance D:\nR A x1 1 x2 2\nS A B z1 1 1 z2 2 1 z3 2 2\nT B y1 1 y2 2\nThus, relation S has three tuples (1, 1), (2, 1), (2, 2), and both R and T have two tuples (1) and (2). Each tuple is annotated with a Boolean variable x1, x2, z1, . . . , y2, which represents the independent event that the corresponding tuple is present in the database. The lineage expression \u03d5 is then the DNF that states which tuples need to be present in order for the Boolean query Q to be true:\n\u03d5 = x1z1y1 \u2228 x2z2y1 \u2228 x2z3y2 Calculating P[\u03d5] for a general database instance is #P-hard. However, if we treat each occurrence of a variable xi in \u03d5 as different (in other words, we dissociate \u03d5 eagerly on all tuples xi from table R), then we get a read-once expression\n\u03d5\u2032 = x1z1y1 \u2228 x\u20322,1z2y1 \u2228 x\u20322,2z3y2 = (x\u20321z1 \u2228 x\u20322,1z2)y1 \u2228 x\u20322,2z3y2\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nWriting pi, qi, ri for the probabilities of variables xi, yi, zi, respectively, we can calculate\nP[\u03d5\u2032] = (( (p\u20321 \u00b7 r1)\u2297 (p\u20322,1 \u00b7 r2) ) \u00b7 q1 ) \u2297 (p\u20322,2 \u00b7 r3 \u00b7 q2)\nwhere \u201c\u00b7\u201d stands for multiplication and \u201c\u2297\u201d for independent-or.10 We know from Theorem 4.8 (3) that P[\u03d5\u2032] is an upper bound to P[\u03d5] by assigning the original probabilities to the dissociated variables. Furthermore, we have shown in [Gatterbauer et al. 2010] that P[\u03d5] can be calculated with a probabilistic query plan\nPR = \u03c0 p \u2205 1 p Y [ \u03c0pY 1 p X [ R(X), S(X,Y ) ] , T (Y ) ]\nwhere the probabilistic join operator 1p [ . . . ]\n(in prefix notation) and the probabilistic project operator with duplicate elimination \u03c0p compute the probability assuming that their inputs are independent [Fuhr and Ro\u0308lleke 1997]. Thus, when the join operator joins two tuples with probabilities p1 and p2, respectively, the output has probability p1p2. When the independent project operator eliminates k duplicate records with probabilities p1, . . . , pk, respectively, the output has probability 1\u2212 p\u03041 \u00b7 \u00b7 \u00b7 p\u0304k. This connection between read-once formulas and query plans was first observed by Olteanu and Huang [2008]. We write here PR to emphasize that this plan dissociates tuples in table R.11 Figure 5a shows the corresponding SQL query assuming that each of the input tables has one additional attribute P for the probability of a tuple. The query deploys a userdefined aggregate (UDA) IOR that calculates the independent-or for the probabilities of the tuples grouped together, i.e. IOR(p1, p2, . . . , pn) = 1 \u2212 p\u03041p\u03042 \u00b7 \u00b7 \u00b7 p\u0304n. Appendix C states the UDA definition for PostgreSQL.\nWe also know from Theorem 4.8 (4) that P[\u03d5\u2032] is a lower bound to P[\u03d5] by assigning new probabilities 1 \u2212 2\u221a1\u2212 p2 to x\u20322,1 and x\u20322,2 (or more generally, any probabilities p\u20322,1 and p\u20322,2 with p\u0304\u20322,1 \u00b7 p\u0304\u20322,2 \u2265 p\u03042). Because of the connection between the read-once expression \u03d5\u2032 and the query plan PR, we can calculate the lower bound by using the same SQL query from Fig. 5a after exchanging the table R with a view VR (Fig. 5b); VR is basically a copy of R that replaces the probability pi of a tuple xi appearing in \u03d5 with 1 \u2212 di\u221a1\u2212 pi where di is the number of times that xi appears in the lineage of Q. The view joins tables R, S and T , groups the original input tuples xi from R, and assigns each xi the new probability 1\u2212 di \u221a 1\u2212 pi calculated as 1-power(1-T.P,1e0/count(*)).\n10The independent-or combines two probabilities as if calculating the disjunction between two independent events. It is defined as p1 \u2297 p2 := 1\u2212 p\u03041p\u03042. 11Note we use the notation PR for both the probabilistic query plan and the corresponding SQL query.\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nAlternatively to \u03d5\u2032, if we treat each occurrence of a variable yj in \u03d5 as different (in other words, we dissociate \u03d5 eagerly on all tuples yj from table T ), then we get another read-once expression\n\u03d5\u2032\u2032 = x1z1y \u2032 1,1 \u2228 x2z2y\u20321,2 \u2228 x2z3y2\n= x1z1y \u2032 1,1 \u2228 x2(z2y\u20321,2 \u2228 z3y2)\nP[\u03d5\u2032\u2032] is an upper bound to P[\u03d5] by assigning the original probabilities to the dissociated variables. In turn, P[\u03d5\u2032\u2032] can be calculated with another probabilistic query plan that dissociates all tuples from table T instead of R:\nPT = \u03c0 p \u2205 1 p X [ R(X), \u03c0pX 1 p Y [ S(X,Y ), T (Y ) ]]\nSimilarly to before, P[\u03d5\u2032\u2032] is a lower bound to P[\u03d5] by assigning new probabilities 1 \u2212 2 \u221a\n1\u2212 q1 to y\u20321,1 and y\u20321,2. And we can calculate this lower bound with the same query PT after exchanging T with a view VT that replaces the probability qj of a tuple yj with 1\u2212 dj \u221a 1\u2212 qj where dj is the number of times that yj appears in the lineage of Q.\nNote that both query plans will calculate upper and lower bounds to query Q over any database instance D. In fact, all possible query plans give upper bounds to the true query probability. And as we have illustrated here, by replacing the input tables with appropriate views, we can use the same query plans to derive lower bounds. We refer the reader to [Gatterbauer et al. 2010] where we develop the theory of the partial dissociation order among all possible query plans and give a sound and complete algorithm that returns a set of query plans which are guaranteed to give the tightest bounds possible in a query-centric way for any conjunctive query without self-joins. For our example hard query Q, plans PR and PT are the best possible plans. We further refer to [Gatterbauer and Suciu 2013] for more details and an extensive discussion on how to speed up the resulting multi-query evaluation.\nAlso note that these upper and lower bounds can be derived with the help of any standard relational database, even cloud-based databases which commonly do not allow users to define their own UDAs.12 To our best knowledge, this is the first technique that can upper and lower bound hard probabilistic queries without any modifications to the database engine nor performing any calculations outside the database."}, {"heading": "7. ILLUSTRATIONS OF OBLIVIOUS BOUNDS", "text": "In this section, we study the quality of oblivious bounds across varying scenarios: We study the bounds as function of correlation between non-dissociated variables (Sect. 7.1), compare dissociation-based with model-based approximations (Sect. 7.2), illustrate a fundamental asymmetry between optimal upper and lower bounds (Sect. 7.3), show that increasing the number of simultaneous dissociations does not necessarily worsen the bounds (Sect. 7.4), and apply our framework to approximate hard probabilistic queries over TPC-H data with a standard relational database management system (Sect.7.5)."}, {"heading": "7.1. Oblivious Bounds as Function of Correlation between Variables", "text": "Example 7.1 (Oblivious Bounds and Correlations). Here we dissociate the DNF \u03d5d = xA \u2228 xB and the analogous CNF \u03d5c = (x \u2228 A)(x \u2228 B) on x and study the er-\n12The UDA IOR can be expressed with standard SQL aggregates, e.g., \u201cIOR(Q3.P)\u201d can be evaluated with \u201c1-exp(sum(log(case Q3.P when 1 then 1E-307 else 1-Q3.P end)))\u201d on Microsoft SQL Azure.\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nror of the optimal oblivious bounds as function of the correlation between A and B.13 Clearly, the bounds also depend on the probabilities of the variables x, A, and B. Let p = P[x] and assume A and B have the same probability q = P[A] = P[B]. We set p\u2032 = P[x\u20321] = P[x\u20322] according to the optimal symmetric bounds from Corollary 4.9.\nIn a few steps, one can calculate the probabilities as\nP[\u03d5d] = 2pq \u2212 pP[AB] P[\u03d5\u2032d] = 2p\u2032q \u2212 p\u2032\n2P[AB] P[\u03d5c] = p+ (1\u2212 p)P[AB] P[\u03d5\u2032c] = 2p\u2032q + p\u2032 2 (1\u2212 2q) + (1\u2212 p\u20322)P[AB]\nResults: Figure 6 shows the probabilities of the expressions P[\u03d5] (full lines) and those of their dissociations P[\u03d5\u2032] (border of shaded areas) for various values of p, q and as function of the correlation \u03c1(A,B).14 For example, Fig.6d shows the case for DNF when P[x] is p = 0.8 and A,B have the same probability q of either 0.8, 0.5, or 0.2. When A,B\n13Note that this simplified example also illustrates the more general case \u03c8d = xA \u2228 xB \u2228 C when C is independent of A and B, and thus P[\u03c8d] = P[\u03d5d](1\u2212 P[C]) + P[C]. As a consequence, the graphs in Fig.6 for P[C] 6= 0 would be vertically compressed and the bounds tighter in absolute terms. 14The correlation \u03c1(A,B) between Boolean events A and B is defined as \u03c1(A,B) = cov(A,B)\u221a\nvar(A)var(B) with\ncovariance cov(A,B) = P[AB] \u2212 P[A]P[B] and variance var(A) = P[A] \u2212 (P[A])2 [Feller 1968]. Notice that\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nare not correlated at all (\u03c1 = 0), then the upper bound is a better approximation when q is small, and the lower bound is a better approximation when q is large. On the other hand, if A,B are not correlated, then there is no need to dissociate the two instances of x as one can then compute P[(x\u2228A)(x\u2228B)] simply as p+p\u0304P[A]P[B]. The more interesting case is when A,B are positively correlated (P[AB] \u2265 P[A]P[B], e.g., positive Boolean functions of other independent variables z, such as the provenance for probabilistic conjunctive queries). The right of the vertical dashed line of Fig. 6d shows that, in this case, dissociation offers very good upper and lower bounds, especially when the formula has a low probability. The graph also shows the effect of dissociation when A,B are negatively correlated (left of dashed line). Notice that the correlation cannot always be \u22121 (e.g., two events, each with probability > 0.5, can never be disjunct). The graphs also illustrate why these bounds are obliviously optimal, i.e. without knowledge of A,B: for every choice of p, there are some A,B for which the upper or lower bound becomes tight."}, {"heading": "7.2. Oblivious Bounds versus Model-based Approximations", "text": "Example 7.2 (Disjunctive Dissociation and Models). This example compares the approximation of our dissociation-based approach with the model-based approach by Fink and Olteanu [2011] and illustrates how dissociation-based bounds are tighter, in general, than model-based approximations. For this purpose, we consider again the hard Boolean query Q :\u2212R(X), Sd(X,Y ), T (Y ) over the database D from Sect. 6. We now only assume that the table S is deterministic, as indicated by the superscript d in Sd. The query-equivalent lineage formula is then\n\u03d5 = x1y1 \u2228 x2y1 \u2228 x2y2 for which Fig.7a shows the bipartite primal graph. We use this instance as its primal graph forms a P4, which is the simplest 2-partite lineage that is not read-once.15 In order to compare the approximation quality, we need to limit ourselves to an example which is tractable enough so we can generate the whole probability space. In practice, we allow each variable to have any of 11 discrete probabilities D = {0, 0.1, 0.2, . . . , 1} and consider all 114 = 14641 possible probability assignments \u03bd : \u3008p1, p2, q1, q2\u3009 \u2192 D4 with p = P[x] and q = P[y]. For each \u03bd, we calculate both the absolute error \u03b4\u2217 = P[\u03d5\u2217]\u2212 P[\u03d5] and the relative error \u03b5\u2217 = \u03b4 \u2217\nP[\u03d5] , where P[\u03d5 \u2217] stands for any of the approximations,\nand the exact probability P[\u03d5] is calculated by the Shannon expansion on y1 as \u03d5 \u2261 y1(x1 \u2228 x2) \u2228 \u00acy1(x2y2) and thus Pp,q[\u03d5] = (1\u2212 (1\u2212 p1)(1\u2212 p2))q1 + (1\u2212 q1)p2q2.\nModels: We use the model-based approach by Fink and Olteanu [2011] to approximate \u03d5 with lowest upper bound (LUB) formulas \u03d5Ui and greatest lower bound (GLB) formulas \u03d5Li, for which \u03d5Li \u21d2 \u03d5 and \u03d5 \u21d2 \u03d5Ui, and neither of the upper (lower) bounds implies another upper (lower) bound. Among all models considered, we focus on only read-once formulas. Given the lineage \u03d5, the 4 LUBs are \u03d5U1 = x1y1 \u2228 x2, \u03d5U2 = y1 \u2228 x2y2, \u03d5U3 = (x1 \u2228 x2)y1 \u2228 y2, and \u03d5U4 = x1 \u2228 x2(y1 \u2228 y2). The 3 GLBs are \u03d5L1 = (x1 \u2228 x2)y1, \u03d5L2 = x1(y1 \u2228 y2), and \u03d5L3 = x1y1 \u2228 x2y2. For each \u03bd, we choose mini(P[\u03d5Ui]) and maxi(P[\u03d5Li]) as the best upper and lower model-based bounds, respectively.\n\u03c1(A,B) = P[AB]\u2212q2 q\u2212q2 and, hence: P[AB] = \u03c1(A,B)\u00b7(q\u2212q\n2)+q2. Further, P[AB] = 0 (i.e. disjointness between A and B) is not possible for q > 0.5, and from P[A \u2228B] \u2264 1, one can derive P[AB] \u2265 2q \u2212 1. In turn, \u03c1 = \u22121 is not possible for q < 0.5, and it must hold P[AB] \u2265 0. From both together, one can derive the condition \u03c1min(q) = max(\u2212 q1\u2212q ,\u2212 1+q2\u22122q q\u2212q2 ) which gives the minimum possible value for \u03c1, and which marks the left starting point of the graphs in Fig.6 as function of q. 15 A path Pn is a graph with vertices {x1, . . . , xn} and edges {x1x2, x2x3, . . . , xn1xn}.\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nDissociation: Analogously, we consider the possible dissociations into read-once formulas. For our given \u03d5, those are \u03d5\u20321 = (x1\u2228x\u20322,1)y1\u2228x\u20322,2y2 and \u03d5\u20322 = x1y\u20321,1\u2228x2(y\u20321,2\u2228y2), with Fig. 7b and Fig. 7c illustrating the dissociated read-once primal graphs.16 From Corollary 4.9, we know that Pp\u2032,q[\u03d5\u20321] \u2265 Pp,q[\u03d5] for the only optimal oblivious upper bounds p\u20322,1 = p\u20322,2 = p2 and Pp\u2032,q[\u03d5\u20321] \u2264 Pp,q[\u03d5] for any p\u20322 with p\u0304\u20322,1p\u0304\u20322,2 = p\u03042. In particular, we choose 3 alternative optimal oblivious lower bounds p\u20322 \u2208 {\u3008p2, 0\u3009, \u30081 \u2212\u221a\n1\u2212 p2, 1 \u2212 \u221a\n1\u2212 p2\u3009, \u30080, p2\u3009} (see Fig. 7d). Analogously Pp,q\u2032 [\u03d5\u20322] \u2265 Pp,q[\u03d5] for q\u20321,1 = q\u20321,2 = q1 and Pp,q\u2032 [\u03d5\u20322] \u2264 Pp,q[\u03d5] for q\u20321 \u2208 {\u3008q1, 0\u3009, \u30081 \u2212 \u221a 1\u2212 q1, 1 \u2212 \u221a 1\u2212 q1\u3009, \u30080, q1\u3009}. For each \u03bd, we choose the minimum among the 2 upper bounds and the maximum among the 6 lower bounds as the best upper and lower dissociation-based bounds, respectively.\nResults: Figures 7e-g show that dissociation-based bounds are always better or equal to model-based bounds. The reason is that all model-based bounds are a special case of oblivious dissociation bounds. Furthermore, dissociation gives far better upper bounds, but only slighter better lower bounds. The reason is illustrated in Fig. 7d: the single dissociation-based upper bound p\u2032 = \u3008p, p\u3009 always dominates the two model-based upper bounds, whereas the two model-based lower bounds are special cases of infinitely many optimal oblivious lower dissociation-based bounds. As extreme case, it is therefore possible for a model-based lower bound to coincide with the best among all optimal\n16Note that we consider here dissociation on both x- and y-variables, thus do not treat them as distinct.\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\noblivious lower dissociation bounds. For our example, we evaluate three oblivious lower bounds, two of which coincide with models."}, {"heading": "7.3. Conjunctive versus Disjunctive Dissociation", "text": "Example 7.3 (Disjunctive and Conjunctive Dissociation). This example illustrates an interesting asymmetry: optimal upper bounds for disjunctive dissociations and optimal lower bounds for conjunctive dissociations are not only unique but also better, on average, than optimal upper bounds for conjunctive dissociations and optimal lower bounds for disjunctive dissociations, respectively (see Figure 1). We show this by comparing the approximation of a function by either dissociating a conjunctive or a disjunctive expression for the same function.\nWe re-use the setup from Example 7.2 where we had a function expressed by a disjunctive expression \u03d5. Our DNF \u03d5 can be written as CNF \u03c8 = (x1 \u2228x2)(y1 \u2228x2)(y1 \u2228 y2) with f\u03d5 = f\u03c817, and two conjunctive dissociations \u03c8\u20321 = (x1 \u2228 x\u20322,1)(y1 \u2228 x\u20322,2)(y1 \u2228 y2) and \u03c8\u20322 = (x1 \u2228 x2)(y\u20321,1 \u2228 x2)(y\u20321,2 \u2228 y2) (Figures 8a-c shows the primal graphs). Again from Corollary 4.9, we know that Pp\u2032,q[\u03d5\u20321] \u2264 Pp,q[\u03d5] for the only optimal oblivious lower bounds p\u20322,1 = p\u20322,2 = p2 and Pp\u2032,q[\u03d5\u20321] \u2265 Pp,q[\u03d5] for any p\u20322 with p\u20322,1p \u2032 2,2 = p2. In particular, we choose 3 alternative optimal oblivious lower bounds\np\u20322 \u2208 {\u3008p2, 1\u3009, \u3008 \u221a p2, \u221a p2\u3009, \u30081, p2\u3009} (see Fig. 7d). Analogously Pp,q\u2032 [\u03d5\u20322] \u2264 Pp,q[\u03d5] for\n17Notice that this transformation from DNF to CNF is hard, in general. We not do not focus on algorithmic aspects in this paper, but rather show the potential of this new approach.\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nq\u20321,1 = q \u2032 1,2 = q1 and Pp,q\u2032 [\u03d5\u20322] \u2265 Pp,q[\u03d5] for q\u20321 \u2208 {\u3008q1, 1\u3009, \u3008 \u221a q1, \u221a q1\u3009, \u30081, q1\u3009}. For each \u03bd, we choose the maximum among the 2 lower bounds and the minimum among the 6 upper bounds as the best upper and lower conjunctive dissociation-based bounds, respectively. We then compare with the approximations from the DNF \u03d5 in Example 7.2.\nResults: Figures 8e-g show that optimal disjunctive upper bounds are, in general but not consistently, better than optimal conjunctive upper bounds for the same function (\u2248 83.5% of those data points with different approximations are better for conjunctive dissociations). The dual result holds for lower bounds. This duality can be best seen in the correspondences of absolute errors between upper and lower bounds."}, {"heading": "7.4. Multiple Dissociations at Once", "text": "Here we investigate the influence of the primal graph and number of dissociations on the tightness of the bounds. Both examples correspond to the lineage of the standard unsafe query Q :\u2212R(X), S(X,Y ), T (Y ) over two different database instances.\nExample 7.4 (Path Pn as Primal Graph). This example considers a DNF expression whose primal graph forms a Pn, i.e. a path of length n (see Fig.9a). Note that this is a generalization of the path P4 from Example 7.2 and corresponds to the lineage of the same unsafe query over larger database instance with 2n\u2212 1 tuples:\n\u03d5n = x1y1 \u2228 x1y2 \u2228 x2y2 \u2228 . . . \u2228 xn\u22121yn \u2228 xnyn \u03d5\u2032n = x \u2032 1,1y1 \u2228 x\u20321,2y2 \u2228 x\u20322,1y2 \u2228 . . . \u2228 x\u2032n\u22121,2yn \u2228 x\u2032nyn\nExact: In the following, we assume the probabilities of all variables to be p and use the notation pn := P[\u03d5n] and p\u2217n := P[\u03d5\u2217n], where \u03d5\u2217n corresponds to the formula \u03d5n without the last conjunct xnyn. We can then express pn as function of p\u2217n, pn\u22121 and p\u2217n\u22121 by recursive application of Shannon\u2019s expansion to xn and yn:\npn = P[xn] ( P[yn] + P[y\u0304n]pn\u22121 ) + P[x\u0304n]p\u2217n p\u2217n = P[yn] ( P[xn\u22121] + P[x\u0304n\u22121]p\u2217n\u22121 ) + P[y\u0304n]pn\u22121\nWe thus get the linear recurrence system\npn = A1pn\u22121 +B1p \u2217 n\u22121 + C1 p\u2217n = A2pn\u22121 +B2p \u2217 n\u22121 + C2\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nwith A1 = p\u0304, B1 = pp\u03042, C1 = p2(2\u2212 p), A2 = p\u0304, B2 = pp\u0304, and C2 = p2. With a few manipulations, this recurrence system can be transformed into a linear non-homogenous recurrence relation of second order pn = Apn\u22121 +Bpn\u22122 +C where A = A1 +B2 = p\u0304(1+p), B = A2B1 \u2212 A1B2 = \u2212p2p\u03042, and C = B1C2 + C1(1 \u2212 B2) = p2(pp\u03042 + (2 \u2212 p)(1 \u2212 pp\u0304)). Thus we can recursively calculate P[\u03d5n] for any probability assignment p starting with initial values p1 = p2 and p2 = 3p2 \u2212 2p3.\nDissociation: Figure 9b shows the primal graph for the dissociation \u03d5\u2032n. Variables x1 to xn\u22121 are dissociated into two variables with same probability p\u2032, whereas xn into one with original probability p. In other words, with increasing n, there are more variables dissociated into two fresh ones each. The probability P[\u03d5\u2032n] is then equal to the probability that at least one variable xi is connected to one variable yj :\nP[\u03d5\u2032n] = 1\u2212 (1\u2212 pp\u2032) ( 1\u2212 p(1\u2212 p\u0304\u20322) )n\u22122( 1\u2212 p(1\u2212 p\u0304p\u0304\u2032) )\nWe set p\u2032 = p for upper bounds, and p\u2032 = 1\u2212\u221a1\u2212 p for lower bounds. Results: Figures 9c-d shows the interesting result that the disjunctive upper bounds become tight for increasing size of the primal graph, and thus increasing number of dissociations. This can be best seen in Fig.9d for which p is chosen as to keep P[\u03d5] = 0.5 constant for varying n and we have limn\u2192\u221e P[\u03d5\u2032n] = P[\u03d5n] = 0.5 for upper bounds. In contrast, the disjunctive lower bounds become weaker but still have a limit value limn\u2192\u221e P[\u03d5\u2032n] \u2248 0.2929 (derived numerically).\nExample 7.5 (Complete bipartite graph Kn,n as Primal Graph). This example considers a DNF whose primal graph forms a complete bipartite graph of size n, i.e. each variable xi is appearing in one clause with each variable yj (see Fig. 10a). Note that this example corresponds to lineage for the standard unsafe query over a database instance with O(n2) tuples:\n\u03d5n = \u2228\n(i,j)\u2208 [n]2 xiyj\n\u03d5\u2032n = \u2228\nj\u2208[n]\n( yj \u2228 i\u2208[n] x\u2032i,j )\nExact: We again assume that all variables have the same probability p = P[xi] = P[yi]. P[\u03d5n] is then equal to the probability that there is at least one tuple xi and at\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nleast one tuple yi:\nP[\u03d5n] = ( 1\u2212 (1\u2212 p)n )2 (6) Dissociation: Figure 10b shows the primal graph for the dissociation \u03d5\u2032n. Each variable xi is dissociated into n fresh variables with same probability p\u2032, i.e. there are n2 fresh variables in total. The probability P[\u03d5\u2032n] is then equal to the probability that at least one variable yi is connected to one variables xi,j :\nP[\u03d5\u2032n] = 1\u2212 ( 1\u2212 p ( 1\u2212 (1\u2212 p\u2032)n ))n\nWe will again choose p as to keep r := P[\u03d5n] constant with increasing n, and then calculate P[\u03d5\u2032n] as function of r. From Eq.6, we get p = 1\u2212 n \u221a 1\u2212\u221ar and then set p\u2032 = p\nfor upper bounds, and p\u2032 = 1\u2212 n\u221a1\u2212 p for lower bounds as each dissociated variable is replaced by n fresh variables. It can then be shown that P[\u03d5\u2032n] for the upper bound is monotonically increasing for n and bounded below 1 with the limit value:\nlim n\u2192\u221e\nP[\u03d5\u2032n] = 1\u2212 (1\u2212 \u221a r) \u221a r\nResults: Figure 10d keeps P[\u03d5] = 0.5 constant (by decreasing p for increasing n) and shows the interesting result that the optimal upper bound is itself upper bounded and reaches a limit value, although there are more variables dissociated, and each variable is dissociated into more fresh ones. This limit value is 0.5803 for r = 0.5. However, lower bounds are not useful in this case."}, {"heading": "7.5. Dissociation with a Standard Relational Database Management System", "text": "Example 7.6 (TPC-H). Here we apply the theory of dissociation to bound hard probabilistic queries with the help of PostgreSQL 9.2, an open-source relational DMBS.18 We use the TPC-H DBGEN data generator19 to generate a 1GB database. We then add a column P to each table, and assign to each tuple either the probability p = 0.1, or p = 0.5, or a random probability from the set {0.01, 0.02, . . . , 0.5} (\u201cp = rand 0.5\u201d). Choosing tuple probabilities p \u2264 0.5 helps us avoid floating-point errors for queries with very large lineages whose query answer probabilities would otherwise be too close to 1.20 Our experiments use the following parameterized query (Fig.11):\nQ(a) :\u2212S(s, a),PS (s, u), P (u, n), s \u2264 $1, n like $2 Relations S, PS and P represent tables Supplier, PartSupp and Part, respectively. Variable a stands for attribute nationkey (\u201canswer tuple\u201d), s for suppkey, u for partkey (\u201cunit\u201d),\n18http://www.postgresql.org/download/ 19http://www.tpc.org/tpch/ 20Compare Fig. 13b and Fig. 13e to observe the impact of the choice of input tuple probabilities p on the probabilities of query answers for the same query.\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nand n for name. The probabilistic version of this query asks which nations (as determined by the attribute nationkey) are most likely to have suppliers with suppkey \u2264 $1 that supply parts with a name like $2 when all tuples in Supplier, PartSupp, and Part are probabilistic. Parameters $1 and $2 allow us to reduce the number of tuples that can participate from tables Supplier and Part, respectively, and to thus study the effects of the lineage size on the predicted dissociation bounds and running times. By default, tables Supplier, Partsupp and Part have 10k, 800k and 200k tuples, respectively, and there are 25 different numeric attributes for nationkey. For parameter $1, we choose a value \u2208 {500, 1000, . . . , 10000} which selects the respective number of tuples from table Supplier. For parameter $2, we choose a value \u2208 {\u2019%\u2019, \u2019%red%\u2019, \u2019%red%green%\u2019 } which selects 200k, 11k or 251 tuples in table Part, respectively.\nTranslation into SQL: Note that the lineage for each individual query answer corresponds to the lineage for the Boolean query Q from Sect. 6, which we know is hard, in general. We thus bound the probabilities of the query answers by evaluating four different queries that correspond to the query-centric dissociation bounds from Sect.6: dissociating either table Supplier or table Part, and calculating either upper and lower bounds. To get the final upper (lower) bounds, we take the minimum (maximum) of the two upper (lower) bounds for each answer tuple. The two query plans are as follows:\nPS(a) = \u03c0 p a 1 p u [ \u03c0pa,u 1 p s [ S(s, a),PS (s, u), s \u2264 $1 ] , P (u, n), n like $2 ]\nPP (a) = \u03c0 p a 1 p s [ S(s, a), \u03c0ps 1 p u [ PS (s, u), s \u2264 $1, P (u, n), n like $2 ]]\nNotice one technical detail for determining the lower bounds with plan PP : Any tuple t from table Part may appear a different number of times in the lineages for different query answers.21 Thus, for every answer a that has t in its lineage, we need to create a distinct copy of t in the view VP , with a probability that depends only on the number of times that t appears in the lineage for a.22 Thus, the view definition for VR needs to include the attribute nationkey (Fig.12a) and PP needs to be adapted as follows (Fig.12b):\nP \u2217P (a) = \u03c0 p a 1 p s [ S(s, a), \u03c0ps,a 1 p u [ PS (s, u), s \u2264 $1,VP(u, a) ]]\n21Lower bounds by dissociating Supplier are easier since the table includes the query answer attribute nationkey. As consequence, any tuple from Supplier may appear in the lineage of one query answer only. 22We could actually use the total number of times the tuple appears in all lineages and still get lower bounds. However, the resulting lower bounds would be weaker and not obliviously optimal.\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nIn order to speed up the resulting multi-query evaluation, we apply a deterministic semi-join reduction on the input tables and then reuse intermediate query results across all four subsequent queries. Since query optimization is not the focus of this paper, we do not show these techniques in Fig. 12 and instead refer to [Gatterbauer and Suciu 2013] for a detailed discussion of techniques to speed up query dissociation. In addition, the exact SQL statements that allow the interested reader to repeat our experiments with PostgreSQL are available on the LaPushDB project page.23\n23http://LaPushDB.com/\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nGround truth: To compare our bounds against ground truth, we construct the lineage DNF for each query answer and use DeMorgan to write it as \u201cdual lineage CNF\u201d without exponential increase in size. For example, the DNF \u03d5 = x1x3 \u2228 x1x2 can be written as CNF \u03d5\u0304 = (x\u03041 \u2228 x\u03043) \u2227 (x\u03041 \u2228 x\u03042) with P[\u03d5\u0304] = 1 \u2212 P[\u03d5]. The problem of evaluating a probabilistic CNF can further be translated into the problem of computing the partition function of a propositional Markov random field, for which the AI community has developed sophisticated solvers. For our experiments, we use a tool called SampleSearch [Gogate and Domingos 2010; Gogate and Dechter 2011]24.\nQuality Results: Figure 13 shows the top 10 query answers, as predicted by the upper dissociation bounds for varying query parameters $1 and $2, as well as varying tuple probabilities p. The crosses show the ground truth probabilities as determined by SampleSearch. The red intervals shows the interval between upper and lower dissociation bounds. Recall that the final dissociation interval is the intersection between the interval from dissociation on Supplier (left of the red interval) and on Part (right of the red interval). The graphs suggest that the upper dissociation bounds are very close to the actual probabilities, which is reminiscent of Sect. 7.4 having shown that upper bounds for DNF dissociations are commonly closer to the true probabilities than lower bounds. For Fig.13f, we have no ground truth as the lineage for the top tuple (11) has size 32899 (i.e. the corresponding DNF has 32899 clauses), which is too big for exact probabilistic inference. However, extrapolating from Fig. 13d and Fig. 13e to Fig. 13f, we speculate that upper dissociation bounds give good approximations here as well.\nThe different interval sizes (i.e. quality of the bounds) arise from different numbers of dissociated tuples in the respective lineages. We illustrate with 4 cases (Fig.13g):\n(A) If there is a plan that does not dissociate any tuple, then both upper and lower bounds coincide. This scenario is also called data-safe [Jha et al. 2010]. For example, the lineage for answer (2) in Fig. 13c has 40 unique tuples from table Part, out of which 3 (7.5%) are dissociated into 2 fresh ones with PP . However, all of the 43 tuples from table Supplier that appear in the lineage appear only once. Therefore, PS dissociates no tuple when calculating the answer probability for (2), and as a result gives us the exact value. (B) The lineage for the top-ranked answer (6) in Fig.13a has 42 unique tuples from table Part, out of which 5 (\u2248 11.9%) are dissociated into 2 fresh ones with PP . In contrast, the lineage has 53 unique tuples from table Supplier, out of which only 4 (\u2248 7.5%) are dissociated into 2 fresh ones with PS . Intuitively, PS should give us therefore tighter bounds, which is confirmed by the results. (C) The lineage for the top-ranked answer (11) in Fig.13b and Fig.13e (both figures show results for the same query, but for different tuple probabilities p = 0.1 or p = rand 0.5) has 1830 unique tuples from table Part, out of which \u2248 5.8% are dissociated into 2 or 3 fresh ones with PP . In contrast, the lineage has only 434 unique tuples from table Supplier, out of which \u2248 95.6% are dissociated into into 2 to 11 fresh variables. Thus, PP gives far tighter bounds in this case. (D) The lineage for the same answer (11) in Fig. 13f has 32899 unique tuples from table Part, out of which \u2248 6.3% are dissociated into 2-4 fresh ones with PP . In contrast, the lineage has only 438 unique tuples from table Supplier, out of which all are dissociated into 80 fresh ones with PS (this is an artifact of the TPC-H random database generator). Thus, the bounds for PS are very poor.\nImportantly, relevance ranking of the answer tuples by upper dissociation bounds approximates the ranking by query reliability very well. For example, for the case of p = 0.1 and $2=\u2019%red%\u2019 (Fig. 13d and Fig. 13e), the ranking given by the minimum upper bounds was identical to the ranking given by the ground truth for all parameter\n24http://www.hlt.utdallas.edu/~vgogate/SampleSearch.html\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nchoices $1 \u2208 {500, 1000, . . . , 10000}. Figure 13c shows an example of an incorrect rankings (for parameters $1=10000, $2=\u2019%red%green%\u2019, p = 0.5): Here tuples 6 and 21 are flipped as compared to their actual probabilities 0.99775 and 0.99777, respectively.\nTiming Results: Figure 14 compares the times for evaluating the deterministic query (Fig.11a) with the times for calculating the dissociation bounds for changing parameters $1 and $2. As experimental platform, we use PostgreSQL 9.2 on a 2.5 Ghz Intel Core i5 with 16G of main memory. We run each query 5 times and take the average execution time. Figure 14d also shows the size of the total lineage of a query (which is the same as the number of query results for the deterministic query without projection) and the times needed by SampleSearch to evaluate the ground truth, if possible.25 Since table Supplier contains exactly 10k tuples with suppkey \u2208 {1, . . . , 10000}, any choice of $1\u226510000 has no effect on the query. We show separate graphs for the time needed to calculate the upper bounds only (which our theory and experiments suggest give better absolute approximations) and the time for both upper and lower bounds (lower bounds are more expensive due to the required manipulation of the input tuples). We also show the times for retrieving the lineage with a lineage query. Any probabilistic approach that evaluates the probabilities outside of the database engine needs to issue this query to retrieve the DNF for each answer. The time needed for the lineage query thus serves as minimum benchmark for any probabilistic approximation.\nOur timing results show that, for small lineages (< 10000), calculating upper and lower bounds can be achieved in a time that is only a small multiple (< 4) of the time needed for an equivalent deterministic query. For large lineages (> 10000), calculating the bounds scales linearly with the size of the lineage (Fig. 14c), whereas determin-\n25The reported times are for evaluating all answer DNFs without the overhead for the lineage query.\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nistic query evaluation can scale in sublinear time (recall that the cardinality of the answer set is maximal 25 across all queries since the database contains only 25 different values for the answer attribute nationkey). Importantly, scalability for dissociation is independent of the tractability of the data instance, e.g. maximum treewidth of the lineage for any query answer. In contrast, SampleSearch quickly takes too long for increasing lineage. For example, SampleSearch needs 108 sec for calculating the ground truth for Fig. 13e (The maximum lineage size among all 25 query answers is 1941 in this scenario). Upper dissociation bounds can be calculated in only 3 sec and give the same ranking (and almost the same probabilities).\nKey take-aways: Overall, our quality and timing experiments suggest that dissociation bounds (in particular, the upper bounds) can provide a good approximation of the actual probabilities and provide a ranking of the query answers that is often identical to the ranking for their actual probabilities. These bounds can be calculated with guaranteed polynomial scalability in the size of the data. In particular for queries with small lineage sizes (< 10000), calculating the bounds took only a small multiple (< 4) of the time needed to evaluate standard deterministic queries."}, {"heading": "8. RELATED WORK AND DISCUSSION", "text": "Dissociation is related to a number of recent approaches in the graphical models and constraint satisfaction literature which approximate an intractable problem with a tractable relaxed version after treating multiple occurrences of variables or nodes as independent or ignoring some equivalence constraints: Choi et al. [2007] approximate inference in Bayesian networks by \u201cnode splitting,\u201d i.e. removing some dependencies from the original model. Ram\u0131\u0301rez and Geffner [2007] treat the problem of obtaining a minimum cost satisfying assignment of a CNF formula by \u201cvariable renaming,\u201d i.e. replacing a variable that appears in many clauses by many fresh new variables that appear in few. Pipatsrisawat and Darwiche [2007] provide lower bounds for MaxSAT by \u201cvariable splitting,\u201d i.e. compiling a relaxation of the original CNF. Andersen et al. [2007] improve the relaxation for constraint satisfaction problems by \u201crefinement through node splitting,\u201d i.e. making explicit some interactions between variables. Choi and Darwiche [2009] relax the maximum a posteriori (MAP) problem in probabilistic graphical models by dropping equivalence constraints and partially compensating for the relaxation. Our work provides a general framework for approximating the probability of Boolean functions with both upper and lower bounds. These bounds shed light on the connection between previous relaxation-based and model-based approximations and unify them as concrete choices in a larger design space. We thus refer to all of the above approaches as instances of dissociation-based approximations.\nAnother line of work that is varyingly called \u201cdiscretization,\u201d \u201cbucketing,\u201d \u201cbinning,\u201d or \u201cquantization\u201d proposes relaxations by merging or partitioning states or nodes (instead of splitting them), and to then perform simplified calculations over those partitions: Dechter and Rish [2003] approximate a function with high arity by a collection of smaller-arity functions with a parameterized scheme called \u201cmini-buckets.\u201d As example, the sum \u2211 i f(xi) \u00b7 g(xi) for non-negative functions f and g can be upper bounded\nby the summation (maxi f(xi)) \u00b7 \u2211 i g(xi), i.e. all different values of f(xi) are replaced by the single maximum value maxi f(xi), which simplifies the calculations. Similarly, the sum can be lower bounded by (mini f(xi)) \u00b7 \u2211 i g(xi). St-Aubin et al. [2000] use Algebraic Decision Diagrams (ADDs) and reduce the sizes of the intermediate value functions generated by replacing the values at the terminals of the ADD with ranges of values. Bergman et al. [2011],[2013] construct relaxations of Multivalued Decision Diagrams (MDDs) by merging vertices when the size of the partially constructed MDD grows too large. Gogate and Domingos [2011] compress potentials computed during the execution of variable elimination by \u201cquantizing\u201d them, i.e. replacing a number\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\nof distinct values in range of the potential by a single value. Since all of the above approaches reduce the number of distinct values in the range of a function, we collectively refer to them as quantization-based approximations. A more detailed literature overview in this space is given by Gogate and Domingos [2013].\nNote that dissociation-based approaches (that split nodes) and quantization-based approaches (that merge nodes) are not inverse operations, but are rather two complementary approaches that may be combined to yield improved methods. The inverse of dissociation is what we refer to as assimilation:26 Consider the Boolean formula \u03d5 = x1y1 \u2228 x1y2 \u2228 x2y2 which is a dissociation of the much simpler formula \u03d5\u2217 = x(y1 \u2228 y2). Hence, we know from dissociation that P[\u03d5\u2217] \u2264 P[\u03d5] for p \u2264 min(p1, p2) and that P[\u03d5\u2217] \u2265 P[\u03d5] for p \u2265 1\u2212 p\u03041p\u03042. Note that for quantization, we can always choose max(p1, p2) or min(p1, p2) to get a guaranteed upper or lower bound. In contrast, for assimilation, we may have to choose a different value to get a guaranteed bound. Also, for the case p1 = p2, assimilation will generally still be an approximation, whereas quantization would be exact. Thus, these are two different approaches.\nExisting approaches for query processing over probabilistic databases that are both general and tractable are either: (1) simulation-based approaches that adapt general purpose sampling methods [Jampani et al. 2008; Kennedy and Koch 2010; Re et al. 2007]; or (2) model-based approaches that approximate the original number of models with guaranteed lower or upper bounds [Olteanu et al. 2010; Fink and Olteanu 2011]. We have show in this paper that, for every model-based bound, there exists a dissociation bound that is at least as good or better.\nOur work on dissociation originated while generalizing propagation-based ranking methods from graphs [Detwiler et al. 2009] to hypergraphs and conjunctive queries. In [Gatterbauer et al. 2010], we applied dissociation in a query-centric way to upper bound hard probabilistic queries, and showed the connection between propagation on graphs and dissociation on hypergraphs (see [Gatterbauer and Suciu 2013] for all details). In this paper, we provide the theoretical underpinnings of these results in a generalized framework with both upper and lower bounds. A previous version of this paper was made available as [Gatterbauer and Suciu 2011]."}, {"heading": "9. OUTLOOK", "text": "We introduced dissociation as a new algebraic technique for approximating the probability of Boolean functions. We applied this technique to derive obliviously optimal upper and lower bounds for conjunctive and disjunctive dissociations and proved that dissociation always gives equally good or better approximations than models. We did not address the algorithmic complexities of exploring the space of alternative dissociations, but rather see our technique as a basic building block for new algorithmic approaches. Such future approaches can apply dissociation at two conceptual levels: (1) at the query-level, i.e. at query time and before analyzing the data, or (2) at the data-level, i.e. while analyzing the data.\nThe advantage of query-centric approaches is that they can run in guaranteed polynomial time,27 yet at the cost of no general approximation guarantees.28 Here, we envision that the query-centric, first-order logic-based view of operating on data by the\n26We prefer the word \u201cassimilation\u201d as inverse of dissociation over of the more natural choice of \u201cassociation\u201d as it implies correctly that two items are actually merged and not merely associated. 27Recall from our experiments (Fig. 14c) that query-centric dissociation scales linearly in the size of the lineage, independent of intricacies in the data, such as the treewidth of the lineage. 28However, also recall from our experiments (Fig.13) that query-centric approaches may work well in practice. Notice here the similarity to loopy belief propagation [Frey and MacKay 1997], which is applied widely and successfully, despite lacking general performance guarantees.\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY.\ndatabase community can also influence neighboring communities, in particular those working on lifted inference (see e.g. [Van den Broeck et al. 2011]).\nThe advantage of data-centric approaches is that exact solutions can be arbitrarily approximated, yet at the cost of no guaranteed runtime [Roth 1996]. Here, we envision a range of new approaches (that may combine dissociation with quantization) to compile an existing intractable formula into a tractable target language, e.g., read-once formulas or formulas with bounded treewidth. For example, one can imagine an approximation scheme that adds repeated dissociation to Shannon expansion in order to avoid the intermediate state explosion."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Arthur Choi and Adnan Darwiche for helpful discussions on Relaxation & Compensation, and for bringing Prop. 5.1 to our attention [Choi and Darwiche 2011]. We would also like to thank Vibhav Gogate for helpful discussions on quantization and guidance for using his tool SampleSearch, Alexandra Meliou for suggesting the name \u201cdissociation,\u201d and the reviewers for their careful reading of this manuscript and their detailed feedback. This work was partially supported by NSF grants IIS-0915054 and IIS-1115188. More information about this research, including the PostgreSQL statements to repeat the experiments on TPC-H data, can be found on the project page: http://LaPushDB.com/."}, {"heading": "A. NOMENCLATURE", "text": "x, y, z independent Boolean random variables \u03d5,\u03c8 Boolean formulas, probabilistic event expressions f, g, f\u03d5 Boolean function, represented by an expression \u03d5 P[x],P[\u03d5] probability of an event or expression pi, qj , rk probabilities pi = P[xi], qj = P[yj ], rk = P[zk] x,g,p sets {x1, . . . , xk} or vectors \u3008x1, . . . , xk\u3009 of variables, functions or probabilities Pp,q[f ] probability of function f(x,y) for p = P[x], q = P[y] x\u0304, \u03d5\u0304, p\u0304 complements \u00acx,\u00ac\u03d5, 1\u2212 p f \u2032, \u03d5\u2032 dissociation of a function f or expression \u03d5 \u03b8 substitution \u03b8 : x\u2032 \u2192 x; defines a dissociation f \u2032 of f if f \u2032[\u03b8] = f f [x\u2032/x] substitution of x\u2032 for x in f m,m\u2032, n m = |x|, m\u2032 = |x\u2032|, n = |y| di number of new variables that xi is dissociated into \u03bd valuation or truth assignment \u03bd : y\u2192 {0, 1} with yi = \u03bdi f [\u03bd], \u03d5[\u03bd] function f or expression \u03d5 with valuation \u03bd substituted for y g\u03bd g\u03bd = \u2227 j g \u03bd j , where g\u03bdj = g\u0304j if \u03bdj =0 and g\u03bdj = gj if \u03bdj =1"}, {"heading": "B. REPRESENTING COMPLEX EVENTS (DISCUSSION OF COROLLARY 4.5)", "text": "It is known from Poole\u2019s independent choice logic [Poole 1993] that arbitrary correlations between events can be composed from disjoint-independent events only. A disjoint-independent event is represented by a non-Boolean independent random variable y which takes either of k values v = \u3008v1, . . . , vk\u3009 with respective probabilities q = \u3008q1, . . . , qk\u3009 and \u2211 i qi = 1. Poole writes such a \u201cdisjoint declaration\u201d as y([v1 :q1, . . . , vk :qk]). In turn, any k disjoint events can be represented starting from k \u2212 1 independent Boolean variables z = \u3008z1, . . . , zk\u22121\u3009 and probabilities P[z] = \u3008q1, q2q\u03041 , q3 q\u03041q\u03042\n, . . . , qk\u22121q\u03041...q\u0304k\u22122 \u3009, by assigning the disjoint-independent event variable y its value vi whenever event Ai is true, with Ai defined as:\n(y = v1) \u2261 A1 :\u2212 z1 (y = v2) \u2261 A2 :\u2212 z\u03041z2\n... (y = vk\u22121) \u2261 Ak1 :\u2212 z1 . . . z\u0304k\u22122zk\u22121\n(y = vk) \u2261 Ak :\u2212 z\u03041 . . . z\u0304k\u22122z\u0304k\u22121 . For example, a disjoint-independent event y(v1 : 15 , v2 : 1 2 , v3 : 1 5 , v4 : 1 10 ) can be represented with three independent Boolean variables z = (z1, z2, z3) and P[z] = (15 , 5 8 , 2 3 ).\nIt follows that arbitrary correlations between events can be modeled starting from independent Boolean random variables alone. For example, two complex events A and B with P[A] = P[B] = q and varying correlation (see Sect. 7.1) can be represented as composed events A :\u2212 z1z2 \u2228 z3 \u2228 z4 and B :\u2212 z\u03041z2 \u2228 z3 \u2228 z5 over the primitive events z with varying probabilities P[z]. Events A and B become identical for P[z] = (0, 0, q, 0, 0), independent for P[z] = (0, 0, 0, q, q), and disjoint for P[z] = (0.5, q, 0, 0, 0) with q \u2264 0.5.\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY."}, {"heading": "C. USER-DEFINED AGGREGATE IOR (SEC. 6 AND SEC. 7.5)", "text": "Here we show the User-defined Aggregate (UDA) IOR in PostgreSQL:\ncreate or replace function ior sfunc(float, float) returns float as \u2019select $1 * (1.0 - $2)\u2019 language SQL;\ncreate or replace function ior finalfunc(float) returns float as \u2019select 1.0 - $1\u2019 language SQL;\ncreate aggregate ior (float)( sfunc = ior sfunc, stype = float, finalfunc = ior finalfunc, initcond = \u20191.0\u2019);\nACM Transactions on Database Systems, Vol. V, No. N, Article 1, Publication date: January YYYY."}], "references": [{"title": "A constraint store based on multivalued decision diagrams", "author": ["H.R. ANDERSEN", "T. HADZIC", "J.N. HOOKER", "P. TIEDEMANN"], "venue": "Proceedings of the 13th International Conference on Principles and Practice of Constraint Programming (CP\u201907). 118\u2013132.", "citeRegEx": "ANDERSEN et al\\.,? 2007", "shortCiteRegEx": "ANDERSEN et al\\.", "year": 2007}, {"title": "Optimization bounds from binary decision diagrams", "author": ["D. BERGMAN", "A.A. CIRE", "VAN HOEVE", "W.-J.", "J.N. HOOKER"], "venue": "INFORMS Journal on Computing. (to appear).", "citeRegEx": "BERGMAN et al\\.,? 2013", "shortCiteRegEx": "BERGMAN et al\\.", "year": 2013}, {"title": "Manipulating MDD relaxations for combinatorial optimization", "author": ["D. BERGMAN", "W.J. VAN HOEVE", "J.N. HOOKER"], "venue": "8th International Conference on Integration of AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems (CPAIOR\u201911). 20\u201335.", "citeRegEx": "BERGMAN et al\\.,? 2011", "shortCiteRegEx": "BERGMAN et al\\.", "year": 2011}, {"title": "On probabilistic inference by weighted model counting", "author": ["M. CHAVIRA", "A. DARWICHE"], "venue": "Artif. Intell. 172, 6-7, 772\u2013799.", "citeRegEx": "CHAVIRA and DARWICHE,? 2008", "shortCiteRegEx": "CHAVIRA and DARWICHE", "year": 2008}, {"title": "Node splitting: A scheme for generating upper bounds in Bayesian networks", "author": ["A. CHOI", "M. CHAVIRA", "A. DARWICHE"], "venue": "Proceedings of the 23rd Conference in Uncertainty in Artificial Intelligence (UAI\u201907). 57\u201366.", "citeRegEx": "CHOI et al\\.,? 2007", "shortCiteRegEx": "CHOI et al\\.", "year": 2007}, {"title": "Relax then compensate: On max-product belief propagation and more", "author": ["A. CHOI", "A. DARWICHE"], "venue": "Proceedings of the 23rd Annual Conference on Neural Information Processing Systems (NIPS\u201909). 351\u2013359. (Alternative title: Approximating MAP by Compensating for Structural Relaxations).", "citeRegEx": "CHOI and DARWICHE,? 2009", "shortCiteRegEx": "CHOI and DARWICHE", "year": 2009}, {"title": "Relax, compensate and then recover", "author": ["A. CHOI", "A. DARWICHE"], "venue": "Proceedings of New Frontiers in Artificial Intelligence Workshops (JSAI-isAI\u201910). 167\u2013180.", "citeRegEx": "CHOI and DARWICHE,? 2010", "shortCiteRegEx": "CHOI and DARWICHE", "year": 2010}, {"title": "Boolean Functions: Theory, Algorithms, and Applications", "author": ["Y. CRAMA", "P.L. HAMMER"], "venue": "Cambridge University Press.", "citeRegEx": "CRAMA and HAMMER,? 2011", "shortCiteRegEx": "CRAMA and HAMMER", "year": 2011}, {"title": "Computing query probability with incidence algebras", "author": ["N.N. DALVI", "K. SCHNAITTER", "D. SUCIU"], "venue": "Proceedings of the 29th Symposium on Principles of Database Systems (PODS\u201910). 203\u2013214.", "citeRegEx": "DALVI et al\\.,? 2010", "shortCiteRegEx": "DALVI et al\\.", "year": 2010}, {"title": "Efficient query evaluation on probabilistic databases", "author": ["N.N. DALVI", "D. SUCIU"], "venue": "VLDB J. 16, 4, 523\u2013544.", "citeRegEx": "DALVI and SUCIU,? 2007", "shortCiteRegEx": "DALVI and SUCIU", "year": 2007}, {"title": "A knowledge compilation map", "author": ["A. DARWICHE", "P. MARQUIS"], "venue": "J. Artif. Int. Res. 17, 1, 229\u2013264.", "citeRegEx": "DARWICHE and MARQUIS,? 2002", "shortCiteRegEx": "DARWICHE and MARQUIS", "year": 2002}, {"title": "Mini-buckets: A general scheme for bounded inference", "author": ["R. DECHTER", "I. RISH"], "venue": "J. ACM 50, 2, 107\u2013153.", "citeRegEx": "DECHTER and RISH,? 2003", "shortCiteRegEx": "DECHTER and RISH", "year": 2003}, {"title": "Integrating and ranking uncertain scientific data", "author": ["L. DETWILER", "W. GATTERBAUER", "B. LOUIE", "D. SUCIU", "P. TARCZY-HORNOCH"], "venue": "Proceedings of the 25th International Conference on Data Engineering (ICDE\u201909). 1235\u20131238.", "citeRegEx": "DETWILER et al\\.,? 2009", "shortCiteRegEx": "DETWILER et al\\.", "year": 2009}, {"title": "An introduction to probability theory and its applications 3d ed Ed", "author": ["W. FELLER"], "venue": "Wiley, New York.", "citeRegEx": "FELLER,? 1968", "shortCiteRegEx": "FELLER", "year": 1968}, {"title": "On the optimal approximation of queries using tractable propositional languages", "author": ["R. FINK", "D. OLTEANU"], "venue": "Proceedings 14th International Conference on Database Theory (ICDT\u201911). 174\u2013185.", "citeRegEx": "FINK and OLTEANU,? 2011", "shortCiteRegEx": "FINK and OLTEANU", "year": 2011}, {"title": "A revolution: Belief propagation in graphs with cycles", "author": ["B.J. FREY", "D.J.C. MACKAY"], "venue": "NIPS.", "citeRegEx": "FREY and MACKAY,? 1997", "shortCiteRegEx": "FREY and MACKAY", "year": 1997}, {"title": "A probabilistic relational algebra for the integration of information retrieval and database systems", "author": ["N. FUHR", "T. R\u00d6LLEKE"], "venue": "ACM Trans. Inf. Syst. 15, 1, 32\u201366.", "citeRegEx": "FUHR and R\u00d6LLEKE,? 1997", "shortCiteRegEx": "FUHR and R\u00d6LLEKE", "year": 1997}, {"title": "Dissociation and propagation for efficient query evaluation over probabilistic databases", "author": ["W. GATTERBAUER", "A.K. JHA", "D. SUCIU"], "venue": "Proceedings of the 4th International VLDB workshop on Management of Uncertain Data (MUD\u201910). 83\u201397.", "citeRegEx": "GATTERBAUER et al\\.,? 2010", "shortCiteRegEx": "GATTERBAUER et al\\.", "year": 2010}, {"title": "Optimal upper and lower bounds for Boolean expressions by dissociation", "author": ["W. GATTERBAUER", "D. SUCIU"], "venue": "arXiv:1105.2813 [cs.AI].", "citeRegEx": "GATTERBAUER and SUCIU,? 2011", "shortCiteRegEx": "GATTERBAUER and SUCIU", "year": 2011}, {"title": "Dissociation and propagation for efficient query evaluation over probabilistic databases", "author": ["W. GATTERBAUER", "D. SUCIU"], "venue": "arXiv:1310.6257 [cs.DB].", "citeRegEx": "GATTERBAUER and SUCIU,? 2013", "shortCiteRegEx": "GATTERBAUER and SUCIU", "year": 2013}, {"title": "SampleSearch: Importance sampling in presence of determinism", "author": ["V. GOGATE", "R. DECHTER"], "venue": "Artificial Intelligence 175, 2, 694\u2013729.", "citeRegEx": "GOGATE and DECHTER,? 2011", "shortCiteRegEx": "GOGATE and DECHTER", "year": 2011}, {"title": "Formula-based probabilistic inference", "author": ["V. GOGATE", "P. DOMINGOS"], "venue": "Proceedings of the 26th Conference in Uncertainty in Artificial Intelligence (UAI\u201910). 210\u2013219.", "citeRegEx": "GOGATE and DOMINGOS,? 2010", "shortCiteRegEx": "GOGATE and DOMINGOS", "year": 2010}, {"title": "Approximation by quantization", "author": ["V. GOGATE", "P. DOMINGOS"], "venue": "Proceedings of the 27th Conference in Uncertainty in Artificial Intelligence (UAI\u201911). 247\u2013255.", "citeRegEx": "GOGATE and DOMINGOS,? 2011", "shortCiteRegEx": "GOGATE and DOMINGOS", "year": 2011}, {"title": "Approximation by quantization", "author": ["V. GOGATE", "P. DOMINGOS"], "venue": "Proceedings of the 29th Conference in Uncertainty in Artificial Intelligence (UAI\u201913). 252\u2013261.", "citeRegEx": "GOGATE and DOMINGOS,? 2013", "shortCiteRegEx": "GOGATE and DOMINGOS", "year": 2013}, {"title": "Repetition-free Boolean functions", "author": ["V. GURVICH"], "venue": "Uspekhi Mat. Nauk 32, 183\u2013184. (in Russian).", "citeRegEx": "GURVICH,? 1977", "shortCiteRegEx": "GURVICH", "year": 1977}, {"title": "MCDB: a Monte Carlo approach to managing uncertain data", "author": ["R. JAMPANI", "F. XU", "M. WU", "L.L. PEREZ", "C.M. JERMAINE", "P.J. HAAS"], "venue": "Proceedings International Conference on Management of Data (SIGMOD\u201908). 687\u2013700.", "citeRegEx": "JAMPANI et al\\.,? 2008", "shortCiteRegEx": "JAMPANI et al\\.", "year": 2008}, {"title": "Bridging the gap between intensional and extensional query evaluation in probabilistic databases", "author": ["A. JHA", "D. OLTEANU", "D. SUCIU"], "venue": "Proceedings of the 13th International Conference on Extending Database Technology (EDBT\u201910). 323\u2013334.", "citeRegEx": "JHA et al\\.,? 2010", "shortCiteRegEx": "JHA et al\\.", "year": 2010}, {"title": "Pip: A database system for great and small expectations", "author": ["O. KENNEDY", "C. KOCH"], "venue": "Proceedings of the 26th International Conference on Data Engineering (ICDE\u201910). 157\u2013168.", "citeRegEx": "KENNEDY and KOCH,? 2010", "shortCiteRegEx": "KENNEDY and KOCH", "year": 2010}, {"title": "Using OBDDs for efficient query evaluation on probabilistic databases", "author": ["D. OLTEANU", "J. HUANG"], "venue": "Proceedings of the 4th International Conference on Scalable Uncertainty Management (SUM\u201908). 326\u2013 340.", "citeRegEx": "OLTEANU and HUANG,? 2008", "shortCiteRegEx": "OLTEANU and HUANG", "year": 2008}, {"title": "Sprout: Lazy vs", "author": ["D. OLTEANU", "J. HUANG", "C. KOCH"], "venue": "eager query plans for tuple-independent probabilistic databases. In Proceedings of the 25th International Conference on Data Engineering (ICDE\u201909). 640\u2013651.", "citeRegEx": "OLTEANU et al\\.,? 2009", "shortCiteRegEx": "OLTEANU et al\\.", "year": 2009}, {"title": "Approximate confidence computation in probabilistic databases", "author": ["D. OLTEANU", "J. HUANG", "C. KOCH"], "venue": "Proceedings of the 26th International Conference on Data Engineering (ICDE\u201910). 145\u2013 156.", "citeRegEx": "OLTEANU et al\\.,? 2010", "shortCiteRegEx": "OLTEANU et al\\.", "year": 2010}, {"title": "Clone: Solving weighted max-sat in a reduced search space", "author": ["K. PIPATSRISAWAT", "A. DARWICHE"], "venue": "Proceedings of the 20th Australian Joint Conference on Artificial Intelligence (AUS-AI\u201907). 223\u2013233.", "citeRegEx": "PIPATSRISAWAT and DARWICHE,? 2007", "shortCiteRegEx": "PIPATSRISAWAT and DARWICHE", "year": 2007}, {"title": "Probabilistic horn abduction and Bayesian networks", "author": ["D. POOLE"], "venue": "Artif. Intell. 64, 1, 81\u2013129.", "citeRegEx": "POOLE,? 1993", "shortCiteRegEx": "POOLE", "year": 1993}, {"title": "The complexity of counting cuts and of computing the probability that a graph is connected", "author": ["J.S. PROVAN", "M.O. BALL"], "venue": "SIAM J. Comput. 12, 4, 777\u2013788.", "citeRegEx": "PROVAN and BALL,? 1983", "shortCiteRegEx": "PROVAN and BALL", "year": 1983}, {"title": "Structural relaxations by variable renaming and their compilation for solving mincostsat", "author": ["M. RAM\u00cdREZ", "H. GEFFNER"], "venue": "Proceedings of the 13th International Conference on Principles and Practice of Constraint Programming (CP\u201907). 605\u2013619.", "citeRegEx": "RAM\u00cdREZ and GEFFNER,? 2007", "shortCiteRegEx": "RAM\u00cdREZ and GEFFNER", "year": 2007}, {"title": "Efficient top-k query evaluation on probabilistic data", "author": ["C. RE", "N.N. DALVI", "D. SUCIU"], "venue": "Proceedings of the 23rd International Conference on Data Engineering (ICDE\u201907). 886\u2013895.", "citeRegEx": "RE et al\\.,? 2007", "shortCiteRegEx": "RE et al\\.", "year": 2007}, {"title": "On the hardness of approximate reasoning", "author": ["D. ROTH"], "venue": "Artif. Intell. 82, 1-2, 273\u2013302.", "citeRegEx": "ROTH,? 1996", "shortCiteRegEx": "ROTH", "year": 1996}, {"title": "Read-once functions and query evaluation in probabilistic databases", "author": ["P. SEN", "A. DESHPANDE", "L. GETOOR"], "venue": "Proceedings of the VLDB Endowment 3, 1, 1068\u20131079.", "citeRegEx": "SEN et al\\.,? 2010", "shortCiteRegEx": "SEN et al\\.", "year": 2010}, {"title": "APRICODD: Approximate policy construction using decision diagrams", "author": ["R. ST-AUBIN", "J. HOEY", "C. BOUTILIER"], "venue": "Advances in Neural Information Processing Systems 13 (NIPS\u201900). 1089\u20131095.", "citeRegEx": "ST.AUBIN et al\\.,? 2000", "shortCiteRegEx": "ST.AUBIN et al\\.", "year": 2000}, {"title": "A scheme for fast parallel communication", "author": ["L. VALIANT"], "venue": "SIAM Journal on Computing 11, 2, 350\u2013361.", "citeRegEx": "VALIANT,? 1982", "shortCiteRegEx": "VALIANT", "year": 1982}, {"title": "The complexity of computing the permanent", "author": ["L.G. VALIANT"], "venue": "Theor. Comput. Sci. 8, 189\u2013201.", "citeRegEx": "VALIANT,? 1979", "shortCiteRegEx": "VALIANT", "year": 1979}, {"title": "Lifted probabilistic inference by first-order knowledge compilation", "author": ["G. VAN DEN BROECK", "N. TAGHIPOUR", "W. MEERT", "J. DAVIS", "L.D. RAEDT"], "venue": "Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI\u201911). 2178\u20132185.", "citeRegEx": "BROECK et al\\.,? 2011", "shortCiteRegEx": "BROECK et al\\.", "year": 2011}, {"title": "COMPLEX EVENTS (DISCUSSION OF COROLLARY 4.5) It is known from Poole\u2019s independent choice logic [Poole 1993] that arbitrary correlations between events can be composed from disjoint-independent events only. A disjoint-independent event is represented by a non-Boolean independent random vari", "author": ["B. REPRESENTING"], "venue": null, "citeRegEx": "REPRESENTING,? \\Q1993\\E", "shortCiteRegEx": "REPRESENTING", "year": 1993}], "referenceMentions": [{"referenceID": 9, "context": ", read-once formulas) either at the query-level [Dalvi and Suciu 2007; Dalvi et al. 2010] or the datalevel [Olteanu and Huang 2008; Sen et al.", "startOffset": 48, "endOffset": 89}, {"referenceID": 8, "context": ", read-once formulas) either at the query-level [Dalvi and Suciu 2007; Dalvi et al. 2010] or the datalevel [Olteanu and Huang 2008; Sen et al.", "startOffset": 48, "endOffset": 89}, {"referenceID": 28, "context": "2010] or the datalevel [Olteanu and Huang 2008; Sen et al. 2010]; (2) exact approaches apply exact probabilistic inference, such as repeated application of Shannon expansion [Olteanu et al.", "startOffset": 23, "endOffset": 64}, {"referenceID": 37, "context": "2010] or the datalevel [Olteanu and Huang 2008; Sen et al. 2010]; (2) exact approaches apply exact probabilistic inference, such as repeated application of Shannon expansion [Olteanu et al.", "startOffset": 23, "endOffset": 64}, {"referenceID": 29, "context": "2010]; (2) exact approaches apply exact probabilistic inference, such as repeated application of Shannon expansion [Olteanu et al. 2009] or tree-width based decompositions [Jha et al.", "startOffset": 115, "endOffset": 136}, {"referenceID": 26, "context": "2009] or tree-width based decompositions [Jha et al. 2010]; and (3) approximate approaches either apply general purpose sampling methods [Jampani et al.", "startOffset": 41, "endOffset": 58}, {"referenceID": 25, "context": "2010]; and (3) approximate approaches either apply general purpose sampling methods [Jampani et al. 2008; Kennedy and Koch 2010; Re et al. 2007] or approximate the number of models of the Boolean lineage expression [Olteanu et al.", "startOffset": 84, "endOffset": 144}, {"referenceID": 27, "context": "2010]; and (3) approximate approaches either apply general purpose sampling methods [Jampani et al. 2008; Kennedy and Koch 2010; Re et al. 2007] or approximate the number of models of the Boolean lineage expression [Olteanu et al.", "startOffset": 84, "endOffset": 144}, {"referenceID": 35, "context": "2010]; and (3) approximate approaches either apply general purpose sampling methods [Jampani et al. 2008; Kennedy and Koch 2010; Re et al. 2007] or approximate the number of models of the Boolean lineage expression [Olteanu et al.", "startOffset": 84, "endOffset": 144}, {"referenceID": 30, "context": "2007] or approximate the number of models of the Boolean lineage expression [Olteanu et al. 2010; Fink and Olteanu 2011].", "startOffset": 76, "endOffset": 120}, {"referenceID": 14, "context": "2007] or approximate the number of models of the Boolean lineage expression [Olteanu et al. 2010; Fink and Olteanu 2011].", "startOffset": 76, "endOffset": 120}, {"referenceID": 3, "context": "els [Chavira and Darwiche 2008].", "startOffset": 4, "endOffset": 31}, {"referenceID": 40, "context": "Computing P[\u03c6] is known to be #P-hard in general [Valiant 1979] and remains hard to even approximate [Roth 1996].", "startOffset": 49, "endOffset": 63}, {"referenceID": 36, "context": "Computing P[\u03c6] is known to be #P-hard in general [Valiant 1979] and remains hard to even approximate [Roth 1996].", "startOffset": 101, "endOffset": 112}, {"referenceID": 33, "context": "for which calculating its probability is already #P-hard [Provan and Ball 1983].", "startOffset": 57, "endOffset": 79}, {"referenceID": 24, "context": "This is a read-once expression whose probability can always be computed in PTIME [Gurvich 1977].", "startOffset": 81, "endOffset": 95}, {"referenceID": 39, "context": "3 Our usage of the term oblivious is inspired by the notion of oblivious routing algorithms [Valiant 1982] which use only local information and which can therefore be implemented very efficiently.", "startOffset": 92, "endOffset": 106}, {"referenceID": 14, "context": "Another technique for approximation described by Fink and Olteanu [2011] is to replace \u03c6 with another expression whose set of models is either a subset or superset of those of \u03c6.", "startOffset": 49, "endOffset": 73}, {"referenceID": 14, "context": "4Fink and Olteanu [2011] describe their approach for approximating DNF expressions only.", "startOffset": 1, "endOffset": 25}, {"referenceID": 9, "context": "Computing the probability of the lineage is known to be #P-hard for some queries [Dalvi and Suciu 2007], hence we are interested in approximating these probabilities by computing dissociated Boolean expressions for the lineage.", "startOffset": 81, "endOffset": 103}, {"referenceID": 17, "context": "We have previously shown in [Gatterbauer et al. 2010] that every query plan for a query corresponds to one possible dissociation for its lineage expression.", "startOffset": 28, "endOffset": 53}, {"referenceID": 7, "context": "This concept originates in constraint satisfaction and it is also varyingly called co-occurrence graph or variable interaction graph [Crama and Hammer 2011].", "startOffset": 133, "endOffset": 156}, {"referenceID": 10, "context": "Note that any two expressions in the expansion above are logically contradictory, a property called determinism by Darwiche and Marquis [2002], and that it can be seen as the result of applying Shannon\u2019s expansion to all variables of y.", "startOffset": 115, "endOffset": 143}, {"referenceID": 5, "context": "Relaxation & Compensation The following proposition shows relaxation & compensation as conjunctive dissociation and was brought to our attention by Choi and Darwiche [2011].", "startOffset": 148, "endOffset": 173}, {"referenceID": 17, "context": "QUERY-CENTRIC DISSOCIATION BOUNDS FOR PROBABILISTIC QUERIES Our previous work [Gatterbauer et al. 2010] has shown how to upper bound the probability of conjunctive queries without self-joins by issuing a sequence of SQL statements over a standard relational DBMS.", "startOffset": 78, "endOffset": 103}, {"referenceID": 17, "context": "Furthermore, we have shown in [Gatterbauer et al. 2010] that P[\u03c6] can be calculated with a probabilistic query plan", "startOffset": 30, "endOffset": 55}, {"referenceID": 28, "context": "This connection between read-once formulas and query plans was first observed by Olteanu and Huang [2008]. We write here PR to emphasize that this plan dissociates tuples in table R.", "startOffset": 81, "endOffset": 106}, {"referenceID": 17, "context": "We refer the reader to [Gatterbauer et al. 2010] where we develop the theory of the partial dissociation order among all possible query plans and give a sound and complete algorithm that returns a set of query plans which are guaranteed to give the tightest bounds possible in a query-centric way for any conjunctive query without self-joins.", "startOffset": 23, "endOffset": 48}, {"referenceID": 19, "context": "We further refer to [Gatterbauer and Suciu 2013] for more details and an extensive discussion on how to speed up the resulting multi-query evaluation.", "startOffset": 20, "endOffset": 48}, {"referenceID": 13, "context": "covariance cov(A,B) = P[AB] \u2212 P[A]P[B] and variance var(A) = P[A] \u2212 (P[A])2 [Feller 1968].", "startOffset": 76, "endOffset": 89}, {"referenceID": 14, "context": "This example compares the approximation of our dissociation-based approach with the model-based approach by Fink and Olteanu [2011] and illustrates how dissociation-based bounds are tighter, in general, than model-based approximations.", "startOffset": 108, "endOffset": 132}, {"referenceID": 14, "context": "Models: We use the model-based approach by Fink and Olteanu [2011] to approximate \u03c6 with lowest upper bound (LUB) formulas \u03c6Ui and greatest lower bound (GLB) formulas \u03c6Li, for which \u03c6Li \u21d2 \u03c6 and \u03c6 \u21d2 \u03c6Ui, and neither of the upper (lower) bounds implies another upper (lower) bound.", "startOffset": 43, "endOffset": 67}, {"referenceID": 19, "context": "12 and instead refer to [Gatterbauer and Suciu 2013] for a detailed discussion of techniques to speed up query dissociation.", "startOffset": 24, "endOffset": 52}, {"referenceID": 21, "context": "For our experiments, we use a tool called SampleSearch [Gogate and Domingos 2010; Gogate and Dechter 2011]24.", "startOffset": 55, "endOffset": 106}, {"referenceID": 20, "context": "For our experiments, we use a tool called SampleSearch [Gogate and Domingos 2010; Gogate and Dechter 2011]24.", "startOffset": 55, "endOffset": 106}, {"referenceID": 26, "context": "This scenario is also called data-safe [Jha et al. 2010].", "startOffset": 39, "endOffset": 56}, {"referenceID": 1, "context": "RELATED WORK AND DISCUSSION Dissociation is related to a number of recent approaches in the graphical models and constraint satisfaction literature which approximate an intractable problem with a tractable relaxed version after treating multiple occurrences of variables or nodes as independent or ignoring some equivalence constraints: Choi et al. [2007] approximate inference in Bayesian networks by \u201cnode splitting,\u201d i.", "startOffset": 337, "endOffset": 356}, {"referenceID": 1, "context": "RELATED WORK AND DISCUSSION Dissociation is related to a number of recent approaches in the graphical models and constraint satisfaction literature which approximate an intractable problem with a tractable relaxed version after treating multiple occurrences of variables or nodes as independent or ignoring some equivalence constraints: Choi et al. [2007] approximate inference in Bayesian networks by \u201cnode splitting,\u201d i.e. removing some dependencies from the original model. Ram\u0131\u0301rez and Geffner [2007] treat the problem of obtaining a minimum cost satisfying assignment of a CNF formula by \u201cvariable renaming,\u201d i.", "startOffset": 337, "endOffset": 505}, {"referenceID": 1, "context": "RELATED WORK AND DISCUSSION Dissociation is related to a number of recent approaches in the graphical models and constraint satisfaction literature which approximate an intractable problem with a tractable relaxed version after treating multiple occurrences of variables or nodes as independent or ignoring some equivalence constraints: Choi et al. [2007] approximate inference in Bayesian networks by \u201cnode splitting,\u201d i.e. removing some dependencies from the original model. Ram\u0131\u0301rez and Geffner [2007] treat the problem of obtaining a minimum cost satisfying assignment of a CNF formula by \u201cvariable renaming,\u201d i.e. replacing a variable that appears in many clauses by many fresh new variables that appear in few. Pipatsrisawat and Darwiche [2007] provide lower bounds for MaxSAT by \u201cvariable splitting,\u201d i.", "startOffset": 337, "endOffset": 751}, {"referenceID": 0, "context": "Andersen et al. [2007] improve the relaxation for constraint satisfaction problems by \u201crefinement through node splitting,\u201d i.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Andersen et al. [2007] improve the relaxation for constraint satisfaction problems by \u201crefinement through node splitting,\u201d i.e. making explicit some interactions between variables. Choi and Darwiche [2009] relax the maximum a posteriori (MAP) problem in probabilistic graphical models by dropping equivalence constraints and partially compensating for the relaxation.", "startOffset": 0, "endOffset": 206}, {"referenceID": 0, "context": "Andersen et al. [2007] improve the relaxation for constraint satisfaction problems by \u201crefinement through node splitting,\u201d i.e. making explicit some interactions between variables. Choi and Darwiche [2009] relax the maximum a posteriori (MAP) problem in probabilistic graphical models by dropping equivalence constraints and partially compensating for the relaxation. Our work provides a general framework for approximating the probability of Boolean functions with both upper and lower bounds. These bounds shed light on the connection between previous relaxation-based and model-based approximations and unify them as concrete choices in a larger design space. We thus refer to all of the above approaches as instances of dissociation-based approximations. Another line of work that is varyingly called \u201cdiscretization,\u201d \u201cbucketing,\u201d \u201cbinning,\u201d or \u201cquantization\u201d proposes relaxations by merging or partitioning states or nodes (instead of splitting them), and to then perform simplified calculations over those partitions: Dechter and Rish [2003] approximate a function with high arity by a collection of smaller-arity functions with a parameterized scheme called \u201cmini-buckets.", "startOffset": 0, "endOffset": 1049}, {"referenceID": 0, "context": "Andersen et al. [2007] improve the relaxation for constraint satisfaction problems by \u201crefinement through node splitting,\u201d i.e. making explicit some interactions between variables. Choi and Darwiche [2009] relax the maximum a posteriori (MAP) problem in probabilistic graphical models by dropping equivalence constraints and partially compensating for the relaxation. Our work provides a general framework for approximating the probability of Boolean functions with both upper and lower bounds. These bounds shed light on the connection between previous relaxation-based and model-based approximations and unify them as concrete choices in a larger design space. We thus refer to all of the above approaches as instances of dissociation-based approximations. Another line of work that is varyingly called \u201cdiscretization,\u201d \u201cbucketing,\u201d \u201cbinning,\u201d or \u201cquantization\u201d proposes relaxations by merging or partitioning states or nodes (instead of splitting them), and to then perform simplified calculations over those partitions: Dechter and Rish [2003] approximate a function with high arity by a collection of smaller-arity functions with a parameterized scheme called \u201cmini-buckets.\u201d As example, the sum \u2211 i f(xi) \u00b7 g(xi) for non-negative functions f and g can be upper bounded by the summation (maxi f(xi)) \u00b7 \u2211 i g(xi), i.e. all different values of f(xi) are replaced by the single maximum value maxi f(xi), which simplifies the calculations. Similarly, the sum can be lower bounded by (mini f(xi)) \u00b7 \u2211 i g(xi). St-Aubin et al. [2000] use Algebraic Decision Diagrams (ADDs) and reduce the sizes of the intermediate value functions generated by replacing the values at the terminals of the ADD with ranges of values.", "startOffset": 0, "endOffset": 1534}, {"referenceID": 0, "context": "Andersen et al. [2007] improve the relaxation for constraint satisfaction problems by \u201crefinement through node splitting,\u201d i.e. making explicit some interactions between variables. Choi and Darwiche [2009] relax the maximum a posteriori (MAP) problem in probabilistic graphical models by dropping equivalence constraints and partially compensating for the relaxation. Our work provides a general framework for approximating the probability of Boolean functions with both upper and lower bounds. These bounds shed light on the connection between previous relaxation-based and model-based approximations and unify them as concrete choices in a larger design space. We thus refer to all of the above approaches as instances of dissociation-based approximations. Another line of work that is varyingly called \u201cdiscretization,\u201d \u201cbucketing,\u201d \u201cbinning,\u201d or \u201cquantization\u201d proposes relaxations by merging or partitioning states or nodes (instead of splitting them), and to then perform simplified calculations over those partitions: Dechter and Rish [2003] approximate a function with high arity by a collection of smaller-arity functions with a parameterized scheme called \u201cmini-buckets.\u201d As example, the sum \u2211 i f(xi) \u00b7 g(xi) for non-negative functions f and g can be upper bounded by the summation (maxi f(xi)) \u00b7 \u2211 i g(xi), i.e. all different values of f(xi) are replaced by the single maximum value maxi f(xi), which simplifies the calculations. Similarly, the sum can be lower bounded by (mini f(xi)) \u00b7 \u2211 i g(xi). St-Aubin et al. [2000] use Algebraic Decision Diagrams (ADDs) and reduce the sizes of the intermediate value functions generated by replacing the values at the terminals of the ADD with ranges of values. Bergman et al. [2011],[2013] construct relaxations of Multivalued Decision Diagrams (MDDs) by merging vertices when the size of the partially constructed MDD grows too large.", "startOffset": 0, "endOffset": 1737}, {"referenceID": 0, "context": "Andersen et al. [2007] improve the relaxation for constraint satisfaction problems by \u201crefinement through node splitting,\u201d i.e. making explicit some interactions between variables. Choi and Darwiche [2009] relax the maximum a posteriori (MAP) problem in probabilistic graphical models by dropping equivalence constraints and partially compensating for the relaxation. Our work provides a general framework for approximating the probability of Boolean functions with both upper and lower bounds. These bounds shed light on the connection between previous relaxation-based and model-based approximations and unify them as concrete choices in a larger design space. We thus refer to all of the above approaches as instances of dissociation-based approximations. Another line of work that is varyingly called \u201cdiscretization,\u201d \u201cbucketing,\u201d \u201cbinning,\u201d or \u201cquantization\u201d proposes relaxations by merging or partitioning states or nodes (instead of splitting them), and to then perform simplified calculations over those partitions: Dechter and Rish [2003] approximate a function with high arity by a collection of smaller-arity functions with a parameterized scheme called \u201cmini-buckets.\u201d As example, the sum \u2211 i f(xi) \u00b7 g(xi) for non-negative functions f and g can be upper bounded by the summation (maxi f(xi)) \u00b7 \u2211 i g(xi), i.e. all different values of f(xi) are replaced by the single maximum value maxi f(xi), which simplifies the calculations. Similarly, the sum can be lower bounded by (mini f(xi)) \u00b7 \u2211 i g(xi). St-Aubin et al. [2000] use Algebraic Decision Diagrams (ADDs) and reduce the sizes of the intermediate value functions generated by replacing the values at the terminals of the ADD with ranges of values. Bergman et al. [2011],[2013] construct relaxations of Multivalued Decision Diagrams (MDDs) by merging vertices when the size of the partially constructed MDD grows too large.", "startOffset": 0, "endOffset": 1744}, {"referenceID": 0, "context": "Andersen et al. [2007] improve the relaxation for constraint satisfaction problems by \u201crefinement through node splitting,\u201d i.e. making explicit some interactions between variables. Choi and Darwiche [2009] relax the maximum a posteriori (MAP) problem in probabilistic graphical models by dropping equivalence constraints and partially compensating for the relaxation. Our work provides a general framework for approximating the probability of Boolean functions with both upper and lower bounds. These bounds shed light on the connection between previous relaxation-based and model-based approximations and unify them as concrete choices in a larger design space. We thus refer to all of the above approaches as instances of dissociation-based approximations. Another line of work that is varyingly called \u201cdiscretization,\u201d \u201cbucketing,\u201d \u201cbinning,\u201d or \u201cquantization\u201d proposes relaxations by merging or partitioning states or nodes (instead of splitting them), and to then perform simplified calculations over those partitions: Dechter and Rish [2003] approximate a function with high arity by a collection of smaller-arity functions with a parameterized scheme called \u201cmini-buckets.\u201d As example, the sum \u2211 i f(xi) \u00b7 g(xi) for non-negative functions f and g can be upper bounded by the summation (maxi f(xi)) \u00b7 \u2211 i g(xi), i.e. all different values of f(xi) are replaced by the single maximum value maxi f(xi), which simplifies the calculations. Similarly, the sum can be lower bounded by (mini f(xi)) \u00b7 \u2211 i g(xi). St-Aubin et al. [2000] use Algebraic Decision Diagrams (ADDs) and reduce the sizes of the intermediate value functions generated by replacing the values at the terminals of the ADD with ranges of values. Bergman et al. [2011],[2013] construct relaxations of Multivalued Decision Diagrams (MDDs) by merging vertices when the size of the partially constructed MDD grows too large. Gogate and Domingos [2011] compress potentials computed during the execution of variable elimination by \u201cquantizing\u201d them, i.", "startOffset": 0, "endOffset": 1917}, {"referenceID": 25, "context": "Existing approaches for query processing over probabilistic databases that are both general and tractable are either: (1) simulation-based approaches that adapt general purpose sampling methods [Jampani et al. 2008; Kennedy and Koch 2010; Re et al. 2007]; or (2) model-based approaches that approximate the original number of models with guaranteed lower or upper bounds [Olteanu et al.", "startOffset": 194, "endOffset": 254}, {"referenceID": 27, "context": "Existing approaches for query processing over probabilistic databases that are both general and tractable are either: (1) simulation-based approaches that adapt general purpose sampling methods [Jampani et al. 2008; Kennedy and Koch 2010; Re et al. 2007]; or (2) model-based approaches that approximate the original number of models with guaranteed lower or upper bounds [Olteanu et al.", "startOffset": 194, "endOffset": 254}, {"referenceID": 35, "context": "Existing approaches for query processing over probabilistic databases that are both general and tractable are either: (1) simulation-based approaches that adapt general purpose sampling methods [Jampani et al. 2008; Kennedy and Koch 2010; Re et al. 2007]; or (2) model-based approaches that approximate the original number of models with guaranteed lower or upper bounds [Olteanu et al.", "startOffset": 194, "endOffset": 254}, {"referenceID": 30, "context": "2007]; or (2) model-based approaches that approximate the original number of models with guaranteed lower or upper bounds [Olteanu et al. 2010; Fink and Olteanu 2011].", "startOffset": 122, "endOffset": 166}, {"referenceID": 14, "context": "2007]; or (2) model-based approaches that approximate the original number of models with guaranteed lower or upper bounds [Olteanu et al. 2010; Fink and Olteanu 2011].", "startOffset": 122, "endOffset": 166}, {"referenceID": 12, "context": "Our work on dissociation originated while generalizing propagation-based ranking methods from graphs [Detwiler et al. 2009] to hypergraphs and conjunctive queries.", "startOffset": 101, "endOffset": 123}, {"referenceID": 17, "context": "In [Gatterbauer et al. 2010], we applied dissociation in a query-centric way to upper bound hard probabilistic queries, and showed the connection between propagation on graphs and dissociation on hypergraphs (see [Gatterbauer and Suciu 2013] for all details).", "startOffset": 3, "endOffset": 28}, {"referenceID": 19, "context": "2010], we applied dissociation in a query-centric way to upper bound hard probabilistic queries, and showed the connection between propagation on graphs and dissociation on hypergraphs (see [Gatterbauer and Suciu 2013] for all details).", "startOffset": 190, "endOffset": 218}, {"referenceID": 18, "context": "A previous version of this paper was made available as [Gatterbauer and Suciu 2011].", "startOffset": 55, "endOffset": 83}, {"referenceID": 16, "context": "A more detailed literature overview in this space is given by Gogate and Domingos [2013]. Note that dissociation-based approaches (that split nodes) and quantization-based approaches (that merge nodes) are not inverse operations, but are rather two complementary approaches that may be combined to yield improved methods.", "startOffset": 62, "endOffset": 89}, {"referenceID": 15, "context": "Notice here the similarity to loopy belief propagation [Frey and MacKay 1997], which is applied widely and successfully, despite lacking general performance guarantees.", "startOffset": 55, "endOffset": 77}, {"referenceID": 36, "context": "The advantage of data-centric approaches is that exact solutions can be arbitrarily approximated, yet at the cost of no guaranteed runtime [Roth 1996].", "startOffset": 139, "endOffset": 150}], "year": 2015, "abstractText": "This paper develops upper and lower bounds for the probability of Boolean functions by treating multiple occurrences of variables as independent and assigning them new individual probabilities. We call this approach dissociation and give an exact characterization of optimal oblivious bounds, i.e. when the new probabilities are chosen independent of the probabilities of all other variables. Our motivation comes from the weighted model counting problem (or, equivalently, the problem of computing the probability of a Boolean function), which is #P-hard in general. By performing several dissociations, one can transform a Boolean formula whose probability is difficult to compute, into one whose probability is easy to compute, and which is guaranteed to provide an upper or lower bound on the probability of the original formula by choosing appropriate probabilities for the dissociated variables. Our new bounds shed light on the connection between previous relaxation-based and model-based approximations and unify them as concrete choices in a larger design space. We also show how our theory allows a standard relational database management system (DBMS) to both upper and lower bound hard probabilistic queries in guaranteed polynomial time.", "creator": "LaTeX with hyperref package"}}}