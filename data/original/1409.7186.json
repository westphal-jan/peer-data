{"id": "1409.7186", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2014", "title": "Feature-based tuning of simulated annealing applied to the curriculum-based course timetabling problem", "abstract": "We consider the university course timetabling problem, which is one of the most studied problems in educational timetabling. In particular, we focus our attention on the formulation known as curriculum-based course timetabling problem (CB-CTT), which has been tackled by many researchers and has many available benchmarks.", "histories": [["v1", "Thu, 25 Sep 2014 08:53:04 GMT  (33kb,D)", "http://arxiv.org/abs/1409.7186v1", null], ["v2", "Wed, 8 Jul 2015 07:53:36 GMT  (32kb,D)", "http://arxiv.org/abs/1409.7186v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ruggero bellio", "sara ceschia", "luca di gaspero", "rea schaerf", "tommaso urli"], "accepted": false, "id": "1409.7186"}, "pdf": {"name": "1409.7186.pdf", "metadata": {"source": "CRF", "title": "Feature-based tuning of simulated annealing applied to the curriculum-based course timetabling problem", "authors": ["Ruggero Bellio", "Sara Ceschia", "Luca Di Gaspero", "Andrea Schaerf", "Tommaso Urli"], "emails": ["ruggero.bellio@uniud.it", "sara.ceschia@uniud.it", "luca.digaspero@uniud.it", "schaerf@uniud.it", "tommaso.urli@uniud.it"], "sections": [{"heading": null, "text": "We consider the university course timetabling problem, which is one of the most studied problems in educational timetabling. In particular, we focus our attention on the formulation known as curriculumbased course timetabling problem (CB-CTT), which has been tackled by many researchers and has many available benchmarks.\nThe contributions of this paper are twofold. On the one side, we propose an effective and robust single-stage simulated annealing search method for solving the problem. On the other side, we design and apply an extensive and statistically-principled analysis methodology for the algorithm parameter tuning procedure. The outcome of this analysis is a linear regression model between instance features and search method parameters, that allows us to set the parameters for unseen instances on the basis of a simple inspection of the instance itself. Using this method, our algorithm, despite its apparent simplicity, has been able to achieve very good experimental results on a set of popular benchmark testbeds.\nar X\niv :1\n40 9.\n71 86\nFinally, we propose many new, real-world instances, that could be used as ground for future comparisons.\nKeywords: Course Timetabling, Simulated Annealing, Metaheuristics, Feature Analysis, Parameter Tuning"}, {"heading": "1 Introduction", "text": "Designing the timetable for the courses is a typical problem that universities have to face every term or semester. There exists a large number of variants of this problem, depending on the specific requirements of the institution involved (see, e.g., Kingston, 2013; Lewis, 2008; Schaerf, 1999).\nThanks mainly to the international timetabling competitions ITC-2002 and ITC-2007 (McCollum et al., 2010), two formulations have, to some extent, lifted up to the status of \u201cstandard\u201d. These are the so-called PostEnrolment Course Timetabling (PE-CTT) (Lewis et al., 2007) and CurriculumBased Course Timetabling (CB-CTT) (Di Gaspero et al., 2007). These two formulations have received attention in the research community, so that many recent articles deal with either one of them.\nThe distinguishing difference between the two formulations is the origin of conflicts between courses, which is based on student enrolment, in one case, and on predefined curricula, in the other. This is however only one of the differences, which actually include also many other distinctive features and cost components. For example, in the PE-CTT, each course is a selfstanding event, whereas in CB-CTT a course consists of multiple lectures. Consequently, the soft constraints are different: in PE-CTT they are all related to events, penalizing late, consecutive, and isolated ones; in CBCTT they mainly involve curricula and courses, ensuring compactness in a curriculum, trying to evenly spread the lectures of a course in the weekdays, and possibly preserving the same room for a course.\nIn this work we focus on CB-CTT, and we build upon our previous work on this problem (Bellio et al., 2012). The key ingredients of our method are, on the one side, a fast single-stage Simulated Annealing (SA) method, and, on the other side, a comprehensive statistical analysis methodology and a principled parameter tuning phase. The result of the analysis is that we have been able to find a linear regression model between the most relevant parameters of the solver and the features of the problem instance under\nconsideration. Thanks to this linear regression model, we are able to predict the ideal parameter setting for each instance. The adequacy of this method is confirmed by the experimental results on unseen instances. Indeed, we show that our method compares favorably with all the state of the art methods on the available testbed instances. Finally, we propose many new real-world instances that will be added to the set of benchmarks to be used also by other researchers, and thus included in future comparison."}, {"heading": "2 Curriculum-Based Course Timetabling", "text": "The formulation of the problem that we use in this paper is the one proposed for the ITC-2007, which is by far the most popular one. This can be be found in (Di Gaspero et al., 2007) but, for the sake of self-completeness, we briefly report it also in the following. Alternative formulations are described in (Bonutti et al., 2012).\nThe problem consists of the following entities:\nDays, Timeslots, and Periods. We are given a number of teaching days in the week. Each day is split in a fixed number of timeslots. A period is a pair composed of a day and a timeslot.\nCourses and Teachers. Each course consists of a fixed number of lectures to be scheduled in distinct periods, it is attended by a number of students, and is taught by a teacher. For each course there is a minimum number of days that the lectures of the course should be spread across, moreover there are some periods in which the course cannot be scheduled.\nRooms. Each room has a capacity, i.e., a number of available seats.\nCurricula. A curriculum is a group of courses such that any pair of courses in the group have students in common. Therefore, courses belonging to the same curriculum are in conflict and cannot be scheduled at the same period.\nA solution of the problem is an assignment of a period (day and timeslot) and a room to all lectures of each course, so as to satisfy a set of hard constraints and to minimize the violations of soft constraints."}, {"heading": "2.1 Hard constraints", "text": "There are three types of hard constraints:\nRoomOccupancy: Two lectures cannot take place in the same room in the same period.\nConflicts: Lectures of courses in the same curriculum or taught by the same teacher must be scheduled in different periods.\nAvailabilities: A course may be not available in a certain period. If this is the case, then no lecture of the course can be scheduled at that period."}, {"heading": "2.2 Soft constraints", "text": "For the formulation ITC-2007 there are four types of soft constraints:\nRoomCapacity: For each lecture, the capacity of the room to which it is assigned must be greater or equal to the number of students attending the course. The penalty for the violation is multiplied by the number of students in excess.\nMinWorkingDays: The lectures of each course must be spread into the given minimum number of days.\nIsolatedLectures: Lectures belonging to a curriculum should be adjacent to each other (i.e., in consecutive periods). For a given curriculum we account for a violation every time there is one lecture not adjacent to any other lecture within the same day.\nRoomStability: All lectures of a course should be given in the same room.\nFor all the details, including input and output data formats and validation tools, we refer to (Di Gaspero et al., 2007)."}, {"heading": "3 Related Work", "text": "We briefly survey the literature on CB-CTT. This section is organized as follows: we firstly present the solution approaches based on metaheuristic techniques; secondly, we report the contributions on exact methods and on\nlower bounds; finally, we discuss papers that investigate additional aspects related to the CB-CTT problem, such as instance generation and multiobjective formulations."}, {"heading": "3.1 Metaheuristic approaches", "text": "Mu\u0308ller (2009) applied a constraint-based solver which incorporates several local search algorithms operating in three stages: a construction phase which uses an Iterative Forward Search algorithm to find a feasible solution, a first search phase delegated to a Hill Climbing algorithm, followed by a Great Deluge or Simulated Annealing strategy to escape from local minima. The algorithm won two out of three tracks of ITC-2007 and was among the finalists in the remaining track.\nAlso the Adaptive Tabu Search proposed by Lu\u0308 and Hao (2009) follows a three stage scheme: in the initialization phase a feasible timetable is built using a fast heuristic; then the intensification and diversification phases are alternatively applied through an adaptive tabu search, in order to reduce the violations of the soft constraints.\nA novel hybrid metaheuristic technique, obtained combining Electromagnetic -like Mechanisms and the Great Deluge algorithm, has been applied by Abdullah et al. (2012). They obtained good results on both CB-CTT and PE-CTT testbeds.\nFinally, Lu\u0308 et al. (2011) investigated the search performance of different neighborhood relations typically used by local search algorithms. The neighborhoods are compared using different evaluation criteria, and new combinations of neighborhoods are explored and analyzed."}, {"heading": "3.2 Exact methods and lower bounds", "text": "Several authors employed exact methods with the goal of finding both solutions and lower bounds.\nBurke et al. (2010b) engineered a hybrid method based on the decomposition of the whole problem into different sub-problems, each one solved using a mix of different IP formulations. Subsequently (Burke et al., 2010a), the authors presented a new MIP formulation based on the concept of \u201csupernode\u201d which is used to model graph coloring problems. This new encoding has been applied to CB-CTT benchmarks, and compared with the standard\ntwo-index MILP model implemented in CPLEX, showing that the supernodes formulation is able to considerably reduce computational time. Lastly, in (Burke et al., 2012) a Branch-and-Cut procedure is developed and lower bounds are obtained for various formulations.\nLach and Lu\u0308bbecke (2012) proposed an IP approach that decomposes the problem in two stages: the first one, whose goal is to assign courses to periods, focused mainly on satisfying the hard constraints; the second one takes care of soft constraints and assigns lectures to rooms by solving a matching problem.\nHao and Benlic (2011) developed a partition-based approach to compute new lower bounds: The original instance is divided into sub-instances through an Iterative Tabu Search procedure, and each subproblem is solved via an ILP solver using the model proposed by Lach and Lu\u0308bbecke (2012). The lower bound for the original problem is obtained summing up the lower bounds of the sub-instances.\nRecently, Cacchiani et al. (2013) computed new lower bounds using an approach somewhat similar to the one by Hao and Benlic (2011), however in this case the partition is based on soft constraints. Once the initial problem is partitioned, two separated problems are formulated as ILPs and then solved to optimality by a Column Generation technique.\nAs\u0301\u0131n Acha\u0301 and Nieuwenhuis (2012) presented an application of several satisfiability (SAT) solvers to the CB-CTT problem. The difference among these SAT-solvers is in the encoding used, that defines in each case which constraints are considered soft or hard. Using different encodings, they were able to compute new lower bounds and obtain new best solutions for the benchmarks.\nFinally, Banbara et al. (2013) translated the CB-CTT formulation into an Answer Set Programming (ASP) problem and solved it by the ASP solver clasp.\nA summary of the results of the cited contributions is given in Table 1 that reports the lower bounds for the instances of ITC-2007 testbed, called comp, where the tightest ones are highlighted in boldface. In the table, we also report the best results known at the time of writing. Best values marked with an asterisk are guaranteed optima (i.e., they match the lower bound)."}, {"heading": "3.3 Additional issues", "text": "We now discuss research activities on CB-CTT that consider other issues, besides the development of a search method for the problem.\nFirst, we mention the development of principled instance generators. The first one has been devised by Burke et al. (2010a) based on the structure of the comp instances. This contribution has been improved by Lopes and Smith-Miles (2010), who base their work on a deeper insight on the features of the instances. We use the generator developed in the latter work, which is publicly available, for our experimental analysis.\nA further issue is the study of the problem as a multi-objective one. To this regard, Geiger (2009) investigates the CB-CTT problem as a multicriteria decision problem by studying the influence of the weighted sum aggregation methodology in the performance of the solution technique.\nRelated to the multi-objective question, is the issue of fairness. In fact, in the standard formulation, some curricula can be heavily penalized in order to obtain the best overall result. Mu\u0308hlenthaler and Wanka (2013) consider various notions of fairness and compare their effects with the other objectives."}, {"heading": "4 Search Method", "text": "We propose a solution method based on Simulated Annealing (Kirkpatrick et al., 1983), which is one of the most widespread local search methods. As for any local search method, we need thus to define a search space, a neighborhood relation, and a cost function."}, {"heading": "4.1 Search space", "text": "We consider the search space composed of all the assignments of lectures to rooms and periods for which the hard constraint Availability is satisfied. On the contrary, hard constraints Conflicts and RoomOccupancy are considered as components of the cost function, however their violation is highly penalized."}, {"heading": "4.2 Neighborhood relations", "text": "We employ two different neighborhood relations, defined by the set of solutions that can be reached by applying either of the following moves to a solution:\nMoveLecture (ML): Move one lecture from its currently assigned period and room to another period and/or another room.\nSwapLectures (SL): Take two lectures of distinct courses in different periods and swap their periods and their rooms.\nWe consider the neighborhood obtained by the union of ML and SL; however, since our previous study (Bellio et al., 2012) revealed that the ML neighborhood is more effective if restricted to moves that place the lecture in an empty room in the new timeslot, we apply here the same restriction whenever possible; i.e., in instances in which the room occupancy is less than 100%, as there are no empty rooms in such case.\nMoreover, we use a swap rate parameter sr to control how often the second neighborhood is selected with respect to the first one. In detail, the move selection strategy proceeds in two stages: first the neighborhood is randomly selected with a non-uniform probability with bias sr, then a random move in the selected neighborhood is uniformly drawn."}, {"heading": "4.3 Cost function", "text": "The cost of a solution s is a weighted sum of the violations of the hard constraints Conflicts and RoomOccupancy and of the objectives (i.e., the violations of the soft constraints):\nCost(s) = Conflicts(s) \u00d7 whard + RoomOccupancy(s) \u00d7 whard + MinWorkingDays(s) \u00d7 wwd + RoomStability(s) \u00d7 wrs + RoomCapacity(s) \u00d7 wrc + IsolatedLectures(s) \u00d7 wil ,\nwhere the weights wwd = 5, wrs = wrc = 1 and wil = 2 are defined by the problem formulation, while whard is a parameter to the algorithm (see Section 4.5). In fact, this value should be high enough to give precedence to feasibility over the objectives, but it should not be too high so as to allow the SA metaheuristic (whose move acceptance criterion is based on the difference in the cost function) to select also moves that will increase the number of violations in early stages of the search."}, {"heading": "4.4 Metaheuristic", "text": "In (Bellio et al., 2012) the metaheuristic that guides the search is a combination (token-ring) of Tabu Search and a \u201cstandard\u201d version of SA. In this work, we show that a single-stage enhanced version of the SA, once properly tuned, can outperform such a combination. The main differences of the SA approach implemented in this algorithm with respect to the SA in (Bellio et al., 2012) are the following:\n1. a cutoff-based temperature cooling scheme (Johnson et al., 1989);\n2. a different stopping condition for the solver, based on the maximum number of allowed iterations;\nAnother major difference with respect to Bellio et al. (2012) consists in the statistical analysis and the tuning process. Indeed, in this work we perform a thorough tuning process of the SA method, that has been carried out and validated on a extensive set of instances (see Section 5 for details). The statistical analysis aims at distinguishing a fixed setting of the parameters that generalizes reasonably well over the whole set of instances, and an automatic procedure to predict the ideal parameter setup, on the basis of computable features of the instance at hand.\nIn the rest of this section we are going to describe the main algorithmic aspects of the SA method, and we defer the explanation of the statistical analysis and the tuning process to Section 6.\nCutoff-based cooling scheme In order to better exploit the time at its disposal, our algorithm employs a cutoff-based cooling scheme. In practice, instead of sampling a fixed number ns of solutions at each temperature level (as it is customary in SA implementations), the algorithm is allowed to decrease the temperature prematurely, i.e., by multiplying it for the cooling rate cr, if a portion na \u2264 ns of the sampled solutions has been accepted already. This allows to speed-up the search in the initial stages of the search, thus saving iterations that can be used in the final stages, where intensification sets in.\nStopping condition To allow a fair comparison with the existing work in literature, our SA variant stops based on an iteration budget (which will be roughly equivalent to a time budget, given that the cost of one iteration is\napproximately constant), rather than when a specific (minimum) temperature tmin is reached. This has the drawback that when the budget runs out, the temperature might still be too high. In order to overcome this problem, we fix an expected minimum temperature tmin to a reasonable value and we compute the number ns (see Equation 1) of solutions sampled at each temperature so that the minimum temperature is reached exactly when the maximum number of iterations is met.\nns = itermax\n/( \u2212 log (t0/tmin)\nlog cr\n) . (1)\nBecause of the cutoff-based cooling scheme, at the beginning of the search the temperature decreases before all ns solutions have been sampled, thus tmin is reached k iterations in advance, where k depends on the cost landscape and on the ratio na/ns. These k iterations are thus saved to be exploited at the end of the search, i.e., when tmin has been reached, to carry out further (intensification) moves.\nMoreover, given the dependence of the cutoff-based scheme on the ratio na/ns and the relation na \u2264 ns, in order to simplify the parameters of the algorithm, we decided to indirectly specify the value of the parameter na by employing a real-valued parameter \u03c1 \u2208]0, 1] that represents the ratio of the number of sampled solutions that will be accepted."}, {"heading": "4.5 Summary of parameters", "text": "According to the description of the search method provided, the algorithm involves many parameters. In order to refrain from making any \u201cpremature commitment\u201d (see Hoos, 2012), we consider all of them in the experimental analysis. They are summarized in Table 2, along with the ranges involved in the experimental analysis which have been fixed based on preliminary experiments.\nThe iterations budget has been fixed, for each instance, to a value that provides the algorithm with a running time which is equivalent to the one allowed by ITC-2007 computation rules (408 seconds on our test machines)."}, {"heading": "5 Problem Instances", "text": "We now discuss the instances considered for CB-CTT and how we used them to validate the results. In particular, according to the customary crossvalidation guidelines (e.g. Hastie et al., 2009), we have split the entire instance set in three groups: a set of training instances used to tune the algorithm, a set of validation instances used to evaluate and possibly correct the tuning, and finally a set of novel instances to verify the quality of the proposed method on previously unseen instances."}, {"heading": "5.1 Training instances", "text": "The first group is a large set of artificial instances, created using the generator by Lopes and Smith-Miles (2010), which has been specifically designed to reproduce the features of real-world instances. In order to avoid overtuning phenomena, only this set has been used for the tuning phase and the individual results on these instances will not be reported.\nThe generator is parametrized upon two features of the instances: the total number of lectures and the percentage occupation of the rooms.\nA set of 4200 generated instances has been made available by Lopes and Smith-Miles (2010) along with the generator, based on a range of values of the two control features. In order to have a wider range of values necessary for our objectives, however, we have created our own instances using the generator.\nSpecifically, we have created 5 instances for each pair of values of the two parameters. The number of lectures ranges in {i \u00b7 50 | i = 1, . . . , 24},\nso that they will be comprised between 50 and 1200. As the percentage of occupation we consider the four values {50%, 70%, 80%, 90%}. On overall, the full testbed consists of 480 instances (5\u00d7 24\u00d7 4).\nAfter screening them in detail, it appears that not all instances generated are useful for our parameter tuning purposes. In particular, we have identified the following classes:\nProvably infeasible: infeasibility is easily proven (e.g., some courses have more lectures than available periods);\nUnrealistic room endowment: only high cost solutions exist, due to the presence of courses with more students than the available rooms;\nToo hard: the solver has never been able to find a feasible solution (given a reasonable long timeout);\nToo easy: easily solved to cost 0 by all the solver variants.\nGiven that these kinds of behavior generally do not appear in real cases, and they can be identified rather quickly, we have decided to exclude all the above cases and replace them with instances that do not belong to these classes."}, {"heading": "5.2 Validation instances", "text": "The second group is the set of instances employed in the literature. It comprises the usual set of instances, the so-called comp ones, which is composed by 21 elements, mainly from the University of Udine, that have been used for ITC-2007.\nOver this set of instances we illustrate the solver emerging from the tuning on the first group of instances, and we compare it with the state of the art methods."}, {"heading": "5.3 Novel instances", "text": "The final set is composed by four families of new instances, which have been proposed recently so that no (or few) results are available from the literature. This set is a candidate to become a new benchmark for future comparisons.\nThe first family in this group, called Erlangen, is composed of four instances of CB-CTT problems arising at the University of Erlangen.1 These instances are larger and exhibit a very different structure than most of the other real-world instances.\nThe second and third families are a set of recent real-world instances from the University of Udine and a group of cases from other Italian universities, kindly provided by EasyStaff S.r.l.,2 a company specializing in timetabling solutions, and were collected by the commercial software EasyAcademy. We call these two instance families Udine and EasyAcademy, respectively.\nA further set of 7 instances, called DDS, and coming from other Italian universities, is available and it has been used occasionally in the literature."}, {"heading": "5.4 Summary of features", "text": "Table 3 summarizes the available families of instances, highlighting some aggregate indicators (i.e., minimum and maximum values) of the most relevant features of the instances belonging to each family. All instances employed in this work, with the exception of the training instances, are available from the CB-CTT website http://satt.diegm.uniud.it/ctt."}, {"heading": "6 Experimental Analysis", "text": "As a fundamental point of our contribution, we conduct a principled and extensive experimental analysis, whose purpose is twofold.\nOn the one hand, we want to investigate the possible relationships between the instance features, reported in Table 3, and the ideal setup of the solver parameters. The ultimate goal of this study is to find, for each parameter, either a fixed value that works well on a broad set of instances, or a formula to predict the best value based on measurable features of each instance. Ideally, the results of this study should carry over to unseen instances, thus making the approach more general than typical parameter tuning. This is, in fact, an attempt to alleviate the effect of the No Free Lunch Theorems for Optimization (Wolpert and Macready, 1997), which state that, for any algorithm (resp. any parameter setup), any elevated performance over one class of problems is exactly paid for in performance over another class.\n1Contributed by Moritz Mu\u0308hlenthaler 2See http://www.easystaff.it\nOn the other hand, as the outcome of these analysis, we aim at obtaining a robust tuning of the various parameters of our algorithm, so that its performances compare favorably with those obtained by other state of the art methods.\nIn the following, we first illustrate the experimental setting and the statistical methodology we employ in this study. Then we summarize our findings and present the experimental results on the validation and novel instances compared against the best known results in literature."}, {"heading": "6.1 Design of experiments and experimental setting", "text": "Our analysis is based on the 480 training instances described in Section 5.1. The compared parameter setups are sampled from the Hammersley point set (Hammersley et al., 1965), for the ranges whose bounds are reported in Table 2. This choice has been driven by two properties that make this point generation strategy particularly suitable for parameter tuning. First, the Hammersley point set is scalable, both with respect to the number of sampled parameter setups, and to the dimensions of the sampled space. Sec-\nond, the sampled points exhibit low discrepancy, i.e., they are space-filling, despite being random-like. For these reasons, by sampling the sequence, one can generate any number of representative combinations of any number of parameters. Note that the sequence is deterministic, and must be seeded with a list of prime numbers. Also, the sequence generates points p \u2208 [0, 1]n, which must then be re-scaled in their desired intervals.\nAll the experiments were generated and executed using json2run (Urli, 2013) on an Ubuntu Linux 13.04 machine with 16 Intel\u00ae Xeon\u00ae CPU E52660 (2.20 GHz) physical cores, hyper-threaded to 32 virtual cores. A single virtual core has been dedicated to each experiment."}, {"heading": "6.2 Exploratory experiments", "text": "Before proceeding with our study, we carried out two preparatory steps. The first involved running a F-Race(RSD) tuning (Birattari et al., 2010) over the training instances with a 95% confidence, in order to establish a baseline for further comparisons. The race ended with more than one surviving setups, mainly differing for the values of whard and cr, but giving a clear indication about the good values for the other parameters. This suggested that setting a specific value for whard and cr, at least within the investigated intervals, was essentially irrelevant to the performance, and allowed us to simplify the analysis, by fixing whard = 100 and cr = 0.99 (see Table 4). Observe that, removing a parameter from the analysis has the double benefit of removing some experimental noise, and to allow a finer-grained tuning of the other parameters, at the same computational cost. We thus repeated the race with fewer parameters, and found a single winning setup, fixing \u03c1 = 0.0364, t0 = 30, tmin = 0.16 and sr = 0.43.\nThe second step consisted in testing all the sampled parameter setups against the whole set of training instances. This allowed us to further refine our study in two ways. First, we realized that our initial estimates over the parameters intervals were too conservative, encompassing low-performance areas of the parameters space. A notable finding was that, on the whole set of training instances, a golden spot for sr was around 0.43, the same value found with F-Race; we thus fixed this parameter as well, along with whard and cr. Table 4 summarizes the whole parameter space after this preliminary phase (parameters in boldface have not been fixed in this phase, and are the subject of the following analysis). Second, by running a KruskalWallis test (see Hollander and Wolfe, 1999) with significance level 10% on the\ndependence of cost distribution on parameter values, we realized that some of the instances were irrelevant to our analysis, and we therefore decided to drop them, and to limit our study to the significant ones (314 instances).\nFinally, we sampled 20 parameter setups from the remaining 3-dimensional (t0, \u03c1, tmin) Hammersley point set, and performed 10 independent runs of each parameter setup on every instance. This is the data upon which the rest of our experimental analysis is based."}, {"heading": "6.3 Statistical methodology", "text": "In order to train a model to predict the ideal parameters values for each instance, we need two elements. The first is a set of instances with known or measurable features (the training set described in Section 5.1). The second is the known ideal parameter setup for each of these instances. To identify the latter, we use median regression, a special case of quantile regression, see (Koenker, 2005) for a detailed description."}, {"heading": "6.3.1 Parameter tuning for each instance", "text": "For each instance, the basic idea is to approximate the cost as a function of the parameters by a regression model, be encoding them as experimental factors assuming values in the [-1,1] range. As we cannot exclude interactions between the three parameters under study, we decided to consider all of them together. In particular, we compared three different models.\nLinear model (M1): A simple model approximates the cost as a linear function of the parameters. Namely, for i = 1, . . . , n, with n equal to the\nsample size for each instance (n = 20 \u00d7 10), the deterministic component of the model is given by\ng1(xi, \u03b2) = \u03b20 + 3\u2211\nj=1\nxij \u03b2j , (2)\nwhere xij is the i-th value for the j-th encoded parameter (j = 1, 2, 3) and xi = (xi1, xi2, xi3), whereas \u03b2 = (\u03b20, \u03b21, \u03b22, \u03b23) are coefficients that are estimated from the experimental data.\nQuadratic model (M2): This model extends the previous one by including quadratic terms for each encoded parameter and interaction term\ng2(xi, \u03b2) = \u03b20 + 3\u2211\nj=1\n\u03b2j xij + 3\u2211\nj=1\n\u03b2j+3 x 2 ij\n+ \u03b27 xi1 xi2 + \u03b28 xi1 xi3 + \u03b29 xi2 xi3 . (3)\nGroup-effect model (M3): These models simply assume a constant level of cost at each different experimental point, namely\ng3(xi, \u03b2) = \u03b2j , with j = m(i) , (4)\nwhere m(i) is the function that returns the experimental point associated to each observation, namely m(i) \u2208 {1, . . . , 20}. This can be seen as the model corresponding to one-way ANOVA analyses.\nBased on the idea of experimental response surfaces (see Bellio et al., 2012, Section 4.2), we first fitted both M1 and M2, and compared them with the M3 model. From the result of the preliminary Kruskal-Wallis test it was already known that the selected instances were those where M3 was a meaningful model, therefore the task performed at this step correspond to checking whether a lower-dimensional linear or quadratic function of the parameters could approximate significantly well the fit provided by the groupeffect model.\nAll the models were fitted by median regression, rather than ordinary least squares. Median regression assumes that the deterministic function gj(xi, \u03b2), j = 1, 2, 3 approximates the median cost, rather than the mean cost, corresponding to a certain combination of parameters. Denoting by yi the\ncost of the i-th observation, the estimated \u03b2 coefficients are the coefficients that minimize the objective function\nn\u2211 i=1 |yi \u2212 gj(xi, \u03b2)| , (5)\nand therefore the technique is also known as Least Absolute Deviation (LAD) regression. Being based on the median rather than the mean, median regression is much less influenced by outliers in the response then ordinary least squares.\nMedian regression is therefore suited for robust estimation of regression models from experimental data where outliers may arise in the response, such as in the algorithm under study here. Inferential usage of the method does not require the normality assumption of the response variable, hence it is not crucial to choose the right scale for the cost. This is a clear advantage when data from multiple instances are analyzed, as the normalizing transformations of cost typically vary across instances. Last but not least, median regression models can be easily trained by casting the optimization of (5) as a linear programming problem, as done by the R package quantreg (Koenker, 2013). All in all, we find such reasons quite\ncompelling to endorse the usage of median regression, or, more generally, quantile regression, for the analysis of the performances of stochastic algorithms.\nThe model among M1,M2,M3 that provided the best fit was selected by means of the Akaike Information Criterion (AIC) for model selection (Koenker, 2005, Section 4.9.1), which is known to have good performances for prediction. When the linear or quadratic regression models were selected, the parameter setup corresponding to the minimum predicted cost was then\nselected, and the corresponding parameters deemed as being optimal. When the group-effect model was selected by the AIC criterion, we simply selected the design point corresponding to the smallest sample median, as M3 model fitted by median regression corresponds to computing a different sample median at each design point. Figure 1 displays the box-plots of cost values on a single training instance corresponding to each design point, projected on t0."}, {"heading": "6.3.2 Regression of optimal setups based on measurable instance features", "text": "Once the ideal parameter setup has been identified for each instance, we include instance features in the process. We thus build a new median regression model for each parameter that we want to predict, and we use the instance features, together with the identified ideal setups, to train the model. Clearly, not all features are equally relevant for the prediction of our parameters, therefore we start from a model involving the complete set of features and, again by using the AIC criterion, we select the best fitting model, i.e., the one with smallest AIC.\nAs it turns out, only three of the considered features are significant for predicting the algorithm parameters, namely the total number of lectures (Le), the number of curricula (Cu), and the average number of daily lectures per curriculum (DL). Moreover, different parameters are predicted by different features. In particular, in order to predict the ideal t0 and \u03c1 we need to know the total number of lectures and the number of curricula, while to compute the ideal tmin we need the number of lectures and the average number of daily lectures per curriculum. Table 5 reports the coefficients of our fitted linear models, together with the intercept term (Int).\nBy looking at the predicted parameter values for the validation instances, with respect to their features (see Figure 2), some conclusions can be drawn. First, t0 and \u03c1 are highly correlated with the number of lectures, a fea-\nture which, in literature, is commonly considered as a measure of instance hardness. This was, in fact, an expected result of the analysis. Second, by increasing the number of lectures, the starting temperature increases as well, suggesting that, when instances become more difficult, it is beneficial to accept a lot of worsening solutions at the beginning of the search. This will foster the exploration of the search space in the hope of finding basins of attraction of lower cost. Moreover, the ideal number of neighbors that have to be accepted before decreasing the temperature gets lower when the problem gets harder. This indicates that the cutoffs mechanism, described in Section 4.4, plays a determinant role in achieving good performances on the larger instances. Finally, the ideal minimum temperature increases linearly with the increase of the average number of lectures per curriculum, which is another measure related to instance hardness. The consequence of this choice is that the temperature will decrease more quickly on larger instances, supporting the effect of cutoffs, and performing a lot of intensification towards the end of the search. Note that, in order for this strategy to perform a sufficient amount of diversification, the initial temperature must be higher (as suggested by the predictor)."}, {"heading": "6.4 Comparison with other approaches", "text": "In order to validate the quality of our approach, we compared its results against the best ones in literature using the ITC-2007 timeout and instances.\nFor the sake of fairness, the results that are obtained by allotting a higher runtime, for example those of As\u0301\u0131n Acha\u0301 and Nieuwenhuis (2012), who use 10\u2019000 to 100\u2019000 seconds instead of about 300-500 seconds as established by the competition rules, have not been included in the comparison.\nTable 6 shows the average of 31 runs of the algorithm, in which we have highlighted in boldface the lowest average costs. We report the results obtained with both feature-based tuning (FBT) and standard F-Race tuning (further discussed in Section 6.5). The figures show that our approach matches or outperforms the state of the art algorithms in more than half of the instances, also improving on our previous results (Bellio et al., 2012).\nThis outcome is particularly significant also because, differently from all the previous approaches, we have not involved the validation instances in the tuning process."}, {"heading": "6.5 Comparison with the F-Race baseline", "text": "Table 6 shows also the results of the standard F-Race. The figures reveal that the feature-based tuning outperforms the F-Race approach on most instances.\nHowever the difference in performance is not as large as we could have expected, since feature-based tuning, should exhibit a better generalization behavior over unseen instances. We looked for an explanation of this effect in the training phase of our regression models, and found out that, in fact, the per-instance tuning (ideally the best possible, see Section 6.3.1), has itself a quality comparable to standard F-Race. By looking at Figure 1, which is representative of a large portion of the training instances, it is possible to see why. The parameter space (in this case for t0) is split in two well-distinct parts. One part (the leftmost in Figure 1) yields poor results, while any choice of values inside the other part is reasonably safe. In our scenario, the portion of the parameter space leading to poor results is typically very narrow, while the portion leading to better results is broader. This suggests that the chosen algorithm is rather robust with respect to parameter choice, at least for this problem domain. As a consequence, it is possible, for F-Race, to find a parameter setup that works consistently well across a large set of instances."}, {"heading": "6.6 Results on the novel instances", "text": "In Table 7 we show the results obtained by 31 repetitions of our algorithm on the instances of the novel family, for future comparison with these instances. These experiments were carried out outside of the ITC-2007 competition setup, as they were allotted 3 \u00b7 108 maximum iterations. The results are reported both for the F-Race and the feature-based tuning. Regarding the latter, the lack of results on the Erlangen family can be attributed to the particular structure of such instances, caused by the peculiar way in which the original timetabling problem is mapped on the described formulation. In particular, the average number of lectures is very close to the average number of courses (see Table 3), suggesting that, in almost all cases, lectures actually correspond to courses. Similarly, the number of curricula, which is typically smaller than the number of courses, is instead one order of magnitude larger, possibly because each curriculum represents the choice of a single student. The different semantics of the involved features make it impossible to use our feature-based predictor. This kind of mapping is closer in structure to the one used in the PE-CTT formulation, which has been addressed in Ceschia et al. (2012)."}, {"heading": "7 Conclusions and Future Work", "text": "In this paper we have proposed a quite simple yet effective SA approach to the CB-CTT problem.\nMoreover, we performed a comprehensive statistical analysis trying to identify the correlation between instance features and search method parameters. The outcomes of this analysis make it possible to set the parameters for unseen instances on the basis of a simple inspection of the instance itself.\nThe results of the feature-based tuned algorithm on a testbed of validation instances allowed us to improve our previous results (Bellio et al., 2012) and outperform the (average) best results in the literature on 12 instances out of 21.\nFor the future, we plan to investigate new versions of SA that could be applied to this problem. For example, we plan to consider the version that includes reheating. In addition, we plan to investigate other metaheuristics techniques for the problem and let them go through the same statistical analysis. Finally, we aim at applying this search and tuning technique to\nother timetabling problems, including other variants of CB-CTT and PECTT."}], "references": [{"title": "A hybrid metaheuristic approach to the university course timetabling problem", "author": ["S. Abdullah", "H. Turabieh", "B. McCollum", "P. McMullan"], "venue": "Journal of Heuristics", "citeRegEx": "Abdullah et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abdullah et al\\.", "year": 2012}, {"title": "Curriculum-based course timetabling with SAT and MaxSAT", "author": ["R. A\u015b\u0131n Ach\u00e1", "R. Nieuwenhuis"], "venue": "Annals of Operations Research", "citeRegEx": "Ach\u00e1 and Nieuwenhuis,? \\Q2012\\E", "shortCiteRegEx": "Ach\u00e1 and Nieuwenhuis", "year": 2012}, {"title": "Answer set programming as a modeling language for course timetabling", "author": ["M. Banbara", "T. Soh", "N. Tamura", "K. Inoue", "T. Schaub"], "venue": "Theory and Practice of Logic Programming", "citeRegEx": "Banbara et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Banbara et al\\.", "year": 2013}, {"title": "Design and statistical analysis of a hybrid local search algorithm for course timetabling", "author": ["R. Bellio", "L. Di Gaspero", "A. Schaerf"], "venue": "Journal of Scheduling", "citeRegEx": "Bellio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bellio et al\\.", "year": 2012}, {"title": "F-Race and iterated F-race: An overview", "author": ["M. Birattari", "Z. Yuan", "P. Balaprakash", "T. St\u00fctzle"], "venue": null, "citeRegEx": "Birattari et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Birattari et al\\.", "year": 2010}, {"title": "Benchmarking curriculum-based course timetabling: formulations, data formats, instances, validation, visualization, and results", "author": ["A. Bonutti", "F. De Cesco", "L. Di Gaspero", "A. Schaerf"], "venue": "Annals of Operations Research", "citeRegEx": "Bonutti et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bonutti et al\\.", "year": 2012}, {"title": "A supernodal formulation of vertex colouring with applications in course timetabling", "author": ["E. Burke", "J. Mare\u010dek", "A. Parkes", "H. Rudov\u00e1"], "venue": "Annals of Operations Research", "citeRegEx": "Burke et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Burke et al\\.", "year": 2010}, {"title": "A branch-and-cut procedure fortheudine course timetabling problem", "author": ["E.K. Burke", "J. Mare\u010dek", "A.J. Parkes", "H. Rudov\u00e1"], "venue": "Annals of Operations Research", "citeRegEx": "Burke et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Burke et al\\.", "year": 2012}, {"title": "Decomposition, reformulation, and diving in university course timetabling", "author": ["E.K. Burke", "J. Mare\u010dek", "A.J. Parkes", "H. Rudov\u00e1"], "venue": "Computers and Operations Research", "citeRegEx": "Burke et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Burke et al\\.", "year": 2010}, {"title": "A new lower bound for curriculum-based course timetabling", "author": ["V. Cacchiani", "A. Caprara", "R. Roberti", "P. Toth", "Feb."], "venue": "Computers & Operations Research.", "citeRegEx": "Cacchiani et al\\.,? 2013", "shortCiteRegEx": "Cacchiani et al\\.", "year": 2013}, {"title": "Design, engineering, and experimental analysis of a simulated annealing approach to the postenrolment course timetabling problem", "author": ["S. Ceschia", "L. Di Gaspero", "A. Schaerf"], "venue": "Computers & Operations Research", "citeRegEx": "Ceschia et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ceschia et al\\.", "year": 2012}, {"title": "The second international timetabling competition (ITC-2007): Curriculum-based course timetabling (track 3)", "author": ["L. Di Gaspero", "B. McCollum", "A. Schaerf", "August"], "venue": "Tech. rep., Queen\u2019s University, Belfast (UK).", "citeRegEx": "Gaspero et al\\.,? 2007", "shortCiteRegEx": "Gaspero et al\\.", "year": 2007}, {"title": "Multi-criteria curriculum-based course timetabling \u2013 a comparison of a weighted sum and a reference point based approach. In: Evolutionary Multi-Criterion Optimization", "author": ["M.J. Geiger"], "venue": null, "citeRegEx": "Geiger,? \\Q2009\\E", "shortCiteRegEx": "Geiger", "year": 2009}, {"title": "Lower bounds for the ITC-2007 curriculumbased course timetabling problem", "author": ["Hao", "J.-K", "U. Benlic"], "venue": "European Journal of Operational Research", "citeRegEx": "Hao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hao et al\\.", "year": 2011}, {"title": "The Elements of Statistical Learning, 2nd Edition", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Nonparametric Statistical Methods, 2nd Edition", "author": ["M. Hollander", "D.A. Wolfe"], "venue": null, "citeRegEx": "Hollander and Wolfe,? \\Q1999\\E", "shortCiteRegEx": "Hollander and Wolfe", "year": 1999}, {"title": "Programming by optimization", "author": ["H.H. Hoos"], "venue": "Communications of the ACM", "citeRegEx": "Hoos,? \\Q2012\\E", "shortCiteRegEx": "Hoos", "year": 2012}, {"title": "Optimization by simulated annealing: an experimental evaluation; part I, graph partitioning", "author": ["D.S. Johnson", "C.R. Aragon", "L.A. McGeoch", "C. Schevon"], "venue": "Operations Research", "citeRegEx": "Johnson et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 1989}, {"title": "Educational timetabling", "author": ["J.H. Kingston"], "venue": null, "citeRegEx": "Kingston,? \\Q2013\\E", "shortCiteRegEx": "Kingston", "year": 2013}, {"title": "Optimization by simulated annealing", "author": ["S. Kirkpatrick", "Gelatt", "C.D. Jr.", "M.P. Vecchi"], "venue": "Science", "citeRegEx": "Kirkpatrick et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Kirkpatrick et al\\.", "year": 1983}, {"title": "quantreg: Quantile regression. R package version 5.05", "author": ["R. Koenker"], "venue": "URL http://CRAN.R-project.org/package=quantreg", "citeRegEx": "Koenker,? \\Q2013\\E", "shortCiteRegEx": "Koenker", "year": 2013}, {"title": "Curriculum based course timetabling: new solutions to Udine benchmark instances", "author": ["G. Lach", "M. L\u00fcbbecke"], "venue": "Annals of Operations Research", "citeRegEx": "Lach and L\u00fcbbecke,? \\Q2012\\E", "shortCiteRegEx": "Lach and L\u00fcbbecke", "year": 2012}, {"title": "A survey of metaheuristic-based techniques for university timetabling problems", "author": ["R. Lewis"], "venue": "OR Spectrum", "citeRegEx": "Lewis,? \\Q2008\\E", "shortCiteRegEx": "Lewis", "year": 2008}, {"title": "Post enrolment based course timetabling: A description of the problem model used for track two of the second international timetabling competition", "author": ["R. Lewis", "B. Paechter", "B. McCollum"], "venue": null, "citeRegEx": "Lewis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2007}, {"title": "Pitfalls in instance generation for Udine timetabling. In: Learning and Intelligent Optimization (LION4)", "author": ["L. Lopes", "K. Smith-Miles"], "venue": null, "citeRegEx": "Lopes and Smith.Miles,? \\Q2010\\E", "shortCiteRegEx": "Lopes and Smith.Miles", "year": 2010}, {"title": "Adaptive tabu search for course timetabling", "author": ["Z. L\u00fc", "Hao", "J.-K"], "venue": "European Journal of Operational Research", "citeRegEx": "L\u00fc et al\\.,? \\Q2009\\E", "shortCiteRegEx": "L\u00fc et al\\.", "year": 2009}, {"title": "Neighborhood analysis: a case study on curriculum-based course timetabling", "author": ["Z. L\u00fc", "Hao", "J.-K", "F. Glover"], "venue": "Journal of Heuristics", "citeRegEx": "L\u00fc et al\\.,? \\Q2011\\E", "shortCiteRegEx": "L\u00fc et al\\.", "year": 2011}, {"title": "Setting the research agenda in automated timetabling: The second international timetabling competition", "author": ["B. McCollum", "A. Schaerf", "B. Paechter", "P. McMullan", "R. Lewis", "A.J. Parkes", "L. Di Gaspero", "R. Qu", "E.K. Burke"], "venue": "INFORMS Journal on Computing", "citeRegEx": "McCollum et al\\.,? \\Q2010\\E", "shortCiteRegEx": "McCollum et al\\.", "year": 2010}, {"title": "Fairness in academic course timetabling", "author": ["M. M\u00fchlenthaler", "R. Wanka"], "venue": "CoRR abs/1303.2860", "citeRegEx": "M\u00fchlenthaler and Wanka,? \\Q2013\\E", "shortCiteRegEx": "M\u00fchlenthaler and Wanka", "year": 2013}, {"title": "ITC2007 solver description: a hybrid approach", "author": ["T. M\u00fcller"], "venue": "Annals of Operations Research", "citeRegEx": "M\u00fcller,? \\Q2009\\E", "shortCiteRegEx": "M\u00fcller", "year": 2009}, {"title": "A survey of automated timetabling", "author": ["A. Schaerf"], "venue": "Artificial Intelligence", "citeRegEx": "Schaerf,? \\Q1999\\E", "shortCiteRegEx": "Schaerf", "year": 1999}, {"title": "No free lunch theorems for opti", "author": ["D.H. abs/1305.1112. Wolpert", "W.G. Macready"], "venue": null, "citeRegEx": "Wolpert and Macready,? \\Q1997\\E", "shortCiteRegEx": "Wolpert and Macready", "year": 1997}], "referenceMentions": [{"referenceID": 22, "context": "There exists a large number of variants of this problem, depending on the specific requirements of the institution involved (see, e.g., Kingston, 2013; Lewis, 2008; Schaerf, 1999).", "startOffset": 124, "endOffset": 179}, {"referenceID": 30, "context": "There exists a large number of variants of this problem, depending on the specific requirements of the institution involved (see, e.g., Kingston, 2013; Lewis, 2008; Schaerf, 1999).", "startOffset": 124, "endOffset": 179}, {"referenceID": 27, "context": "Thanks mainly to the international timetabling competitions ITC-2002 and ITC-2007 (McCollum et al., 2010), two formulations have, to some extent, lifted up to the status of \u201cstandard\u201d.", "startOffset": 82, "endOffset": 105}, {"referenceID": 23, "context": "These are the so-called PostEnrolment Course Timetabling (PE-CTT) (Lewis et al., 2007) and CurriculumBased Course Timetabling (CB-CTT) (Di Gaspero et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 3, "context": "In this work we focus on CB-CTT, and we build upon our previous work on this problem (Bellio et al., 2012).", "startOffset": 85, "endOffset": 106}, {"referenceID": 5, "context": "Alternative formulations are described in (Bonutti et al., 2012).", "startOffset": 42, "endOffset": 64}, {"referenceID": 26, "context": "1 Metaheuristic approaches M\u00fcller (2009) applied a constraint-based solver which incorporates several local search algorithms operating in three stages: a construction phase which uses an Iterative Forward Search algorithm to find a feasible solution, a first search phase delegated to a Hill Climbing algorithm, followed by a Great Deluge or Simulated Annealing strategy to escape from local minima.", "startOffset": 27, "endOffset": 41}, {"referenceID": 26, "context": "1 Metaheuristic approaches M\u00fcller (2009) applied a constraint-based solver which incorporates several local search algorithms operating in three stages: a construction phase which uses an Iterative Forward Search algorithm to find a feasible solution, a first search phase delegated to a Hill Climbing algorithm, followed by a Great Deluge or Simulated Annealing strategy to escape from local minima. The algorithm won two out of three tracks of ITC-2007 and was among the finalists in the remaining track. Also the Adaptive Tabu Search proposed by L\u00fc and Hao (2009) follows a three stage scheme: in the initialization phase a feasible timetable is built using a fast heuristic; then the intensification and diversification phases are alternatively applied through an adaptive tabu search, in order to reduce the violations of the soft constraints.", "startOffset": 27, "endOffset": 567}, {"referenceID": 0, "context": "A novel hybrid metaheuristic technique, obtained combining Electromagnetic -like Mechanisms and the Great Deluge algorithm, has been applied by Abdullah et al. (2012). They obtained good results on both CB-CTT and PE-CTT testbeds.", "startOffset": 144, "endOffset": 167}, {"referenceID": 0, "context": "A novel hybrid metaheuristic technique, obtained combining Electromagnetic -like Mechanisms and the Great Deluge algorithm, has been applied by Abdullah et al. (2012). They obtained good results on both CB-CTT and PE-CTT testbeds. Finally, L\u00fc et al. (2011) investigated the search performance of different neighborhood relations typically used by local search algorithms.", "startOffset": 144, "endOffset": 257}, {"referenceID": 6, "context": "Burke et al. (2010b) engineered a hybrid method based on the decomposition of the whole problem into different sub-problems, each one solved using a mix of different IP formulations.", "startOffset": 0, "endOffset": 21}, {"referenceID": 7, "context": "Lastly, in (Burke et al., 2012) a Branch-and-Cut procedure is developed and lower bounds are obtained for various formulations.", "startOffset": 11, "endOffset": 31}, {"referenceID": 4, "context": "Lastly, in (Burke et al., 2012) a Branch-and-Cut procedure is developed and lower bounds are obtained for various formulations. Lach and L\u00fcbbecke (2012) proposed an IP approach that decomposes the problem in two stages: the first one, whose goal is to assign courses to periods, focused mainly on satisfying the hard constraints; the second one takes care of soft constraints and assigns lectures to rooms by solving a matching problem.", "startOffset": 12, "endOffset": 153}, {"referenceID": 4, "context": "Lastly, in (Burke et al., 2012) a Branch-and-Cut procedure is developed and lower bounds are obtained for various formulations. Lach and L\u00fcbbecke (2012) proposed an IP approach that decomposes the problem in two stages: the first one, whose goal is to assign courses to periods, focused mainly on satisfying the hard constraints; the second one takes care of soft constraints and assigns lectures to rooms by solving a matching problem. Hao and Benlic (2011) developed a partition-based approach to compute new lower bounds: The original instance is divided into sub-instances through an Iterative Tabu Search procedure, and each subproblem is solved via an ILP solver using the model proposed by Lach and L\u00fcbbecke (2012).", "startOffset": 12, "endOffset": 459}, {"referenceID": 4, "context": "Lastly, in (Burke et al., 2012) a Branch-and-Cut procedure is developed and lower bounds are obtained for various formulations. Lach and L\u00fcbbecke (2012) proposed an IP approach that decomposes the problem in two stages: the first one, whose goal is to assign courses to periods, focused mainly on satisfying the hard constraints; the second one takes care of soft constraints and assigns lectures to rooms by solving a matching problem. Hao and Benlic (2011) developed a partition-based approach to compute new lower bounds: The original instance is divided into sub-instances through an Iterative Tabu Search procedure, and each subproblem is solved via an ILP solver using the model proposed by Lach and L\u00fcbbecke (2012). The lower bound for the original problem is obtained summing up the lower bounds of the sub-instances.", "startOffset": 12, "endOffset": 722}, {"referenceID": 4, "context": "Lastly, in (Burke et al., 2012) a Branch-and-Cut procedure is developed and lower bounds are obtained for various formulations. Lach and L\u00fcbbecke (2012) proposed an IP approach that decomposes the problem in two stages: the first one, whose goal is to assign courses to periods, focused mainly on satisfying the hard constraints; the second one takes care of soft constraints and assigns lectures to rooms by solving a matching problem. Hao and Benlic (2011) developed a partition-based approach to compute new lower bounds: The original instance is divided into sub-instances through an Iterative Tabu Search procedure, and each subproblem is solved via an ILP solver using the model proposed by Lach and L\u00fcbbecke (2012). The lower bound for the original problem is obtained summing up the lower bounds of the sub-instances. Recently, Cacchiani et al. (2013) computed new lower bounds using an approach somewhat similar to the one by Hao and Benlic (2011), however in this case the partition is based on soft constraints.", "startOffset": 12, "endOffset": 860}, {"referenceID": 4, "context": "Lastly, in (Burke et al., 2012) a Branch-and-Cut procedure is developed and lower bounds are obtained for various formulations. Lach and L\u00fcbbecke (2012) proposed an IP approach that decomposes the problem in two stages: the first one, whose goal is to assign courses to periods, focused mainly on satisfying the hard constraints; the second one takes care of soft constraints and assigns lectures to rooms by solving a matching problem. Hao and Benlic (2011) developed a partition-based approach to compute new lower bounds: The original instance is divided into sub-instances through an Iterative Tabu Search procedure, and each subproblem is solved via an ILP solver using the model proposed by Lach and L\u00fcbbecke (2012). The lower bound for the original problem is obtained summing up the lower bounds of the sub-instances. Recently, Cacchiani et al. (2013) computed new lower bounds using an approach somewhat similar to the one by Hao and Benlic (2011), however in this case the partition is based on soft constraints.", "startOffset": 12, "endOffset": 957}, {"referenceID": 1, "context": "A\u015b\u0131n Ach\u00e1 and Nieuwenhuis (2012) presented an application of several satisfiability (SAT) solvers to the CB-CTT problem.", "startOffset": 5, "endOffset": 33}, {"referenceID": 1, "context": "A\u015b\u0131n Ach\u00e1 and Nieuwenhuis (2012) presented an application of several satisfiability (SAT) solvers to the CB-CTT problem. The difference among these SAT-solvers is in the encoding used, that defines in each case which constraints are considered soft or hard. Using different encodings, they were able to compute new lower bounds and obtain new best solutions for the benchmarks. Finally, Banbara et al. (2013) translated the CB-CTT formulation into an Answer Set Programming (ASP) problem and solved it by the ASP solver clasp.", "startOffset": 5, "endOffset": 409}, {"referenceID": 6, "context": "The first one has been devised by Burke et al. (2010a) based on the structure of the comp instances.", "startOffset": 34, "endOffset": 55}, {"referenceID": 6, "context": "The first one has been devised by Burke et al. (2010a) based on the structure of the comp instances. This contribution has been improved by Lopes and Smith-Miles (2010), who base their work on a deeper insight on the features of the instances.", "startOffset": 34, "endOffset": 169}, {"referenceID": 6, "context": "The first one has been devised by Burke et al. (2010a) based on the structure of the comp instances. This contribution has been improved by Lopes and Smith-Miles (2010), who base their work on a deeper insight on the features of the instances. We use the generator developed in the latter work, which is publicly available, for our experimental analysis. A further issue is the study of the problem as a multi-objective one. To this regard, Geiger (2009) investigates the CB-CTT problem as a multicriteria decision problem by studying the influence of the weighted sum aggregation methodology in the performance of the solution technique.", "startOffset": 34, "endOffset": 455}, {"referenceID": 6, "context": "The first one has been devised by Burke et al. (2010a) based on the structure of the comp instances. This contribution has been improved by Lopes and Smith-Miles (2010), who base their work on a deeper insight on the features of the instances. We use the generator developed in the latter work, which is publicly available, for our experimental analysis. A further issue is the study of the problem as a multi-objective one. To this regard, Geiger (2009) investigates the CB-CTT problem as a multicriteria decision problem by studying the influence of the weighted sum aggregation methodology in the performance of the solution technique. Related to the multi-objective question, is the issue of fairness. In fact, in the standard formulation, some curricula can be heavily penalized in order to obtain the best overall result. M\u00fchlenthaler and Wanka (2013) consider various notions of fairness and compare their effects with the other objectives.", "startOffset": 34, "endOffset": 858}, {"referenceID": 19, "context": "We propose a solution method based on Simulated Annealing (Kirkpatrick et al., 1983), which is one of the most widespread local search methods.", "startOffset": 58, "endOffset": 84}, {"referenceID": 3, "context": "We consider the neighborhood obtained by the union of ML and SL; however, since our previous study (Bellio et al., 2012) revealed that the ML neighborhood is more effective if restricted to moves that place the lecture in an empty room in the new timeslot, we apply here the same restriction whenever possible; i.", "startOffset": 99, "endOffset": 120}, {"referenceID": 3, "context": "4 Metaheuristic In (Bellio et al., 2012) the metaheuristic that guides the search is a combination (token-ring) of Tabu Search and a \u201cstandard\u201d version of SA.", "startOffset": 19, "endOffset": 40}, {"referenceID": 3, "context": "The main differences of the SA approach implemented in this algorithm with respect to the SA in (Bellio et al., 2012) are the following:", "startOffset": 96, "endOffset": 117}, {"referenceID": 17, "context": "a cutoff-based temperature cooling scheme (Johnson et al., 1989);", "startOffset": 42, "endOffset": 64}, {"referenceID": 3, "context": "Another major difference with respect to Bellio et al. (2012) consists in the statistical analysis and the tuning process.", "startOffset": 41, "endOffset": 62}, {"referenceID": 24, "context": "1 Training instances The first group is a large set of artificial instances, created using the generator by Lopes and Smith-Miles (2010), which has been specifically designed to reproduce the features of real-world instances.", "startOffset": 108, "endOffset": 137}, {"referenceID": 24, "context": "1 Training instances The first group is a large set of artificial instances, created using the generator by Lopes and Smith-Miles (2010), which has been specifically designed to reproduce the features of real-world instances. In order to avoid overtuning phenomena, only this set has been used for the tuning phase and the individual results on these instances will not be reported. The generator is parametrized upon two features of the instances: the total number of lectures and the percentage occupation of the rooms. A set of 4200 generated instances has been made available by Lopes and Smith-Miles (2010) along with the generator, based on a range of values of the two control features.", "startOffset": 108, "endOffset": 612}, {"referenceID": 31, "context": "This is, in fact, an attempt to alleviate the effect of the No Free Lunch Theorems for Optimization (Wolpert and Macready, 1997), which state that, for any algorithm (resp.", "startOffset": 100, "endOffset": 128}, {"referenceID": 4, "context": "The first involved running a F-Race(RSD) tuning (Birattari et al., 2010) over the training instances with a 95% confidence, in order to establish a baseline for further comparisons.", "startOffset": 48, "endOffset": 72}, {"referenceID": 20, "context": "Last but not least, median regression models can be easily trained by casting the optimization of (5) as a linear programming problem, as done by the R package quantreg (Koenker, 2013).", "startOffset": 169, "endOffset": 184}, {"referenceID": 3, "context": "The figures show that our approach matches or outperforms the state of the art algorithms in more than half of the instances, also improving on our previous results (Bellio et al., 2012).", "startOffset": 165, "endOffset": 186}, {"referenceID": 1, "context": "For the sake of fairness, the results that are obtained by allotting a higher runtime, for example those of A\u015b\u0131n Ach\u00e1 and Nieuwenhuis (2012), who use 10\u2019000 to 100\u2019000 seconds instead of about 300-500 seconds as established by the competition rules, have not been included in the comparison.", "startOffset": 113, "endOffset": 141}, {"referenceID": 10, "context": "This kind of mapping is closer in structure to the one used in the PE-CTT formulation, which has been addressed in Ceschia et al. (2012).", "startOffset": 115, "endOffset": 137}, {"referenceID": 3, "context": "The results of the feature-based tuned algorithm on a testbed of validation instances allowed us to improve our previous results (Bellio et al., 2012) and outperform the (average) best results in the literature on 12 instances out of 21.", "startOffset": 129, "endOffset": 150}], "year": 2017, "abstractText": "We consider the university course timetabling problem, which is one of the most studied problems in educational timetabling. In particular, we focus our attention on the formulation known as curriculumbased course timetabling problem (CB-CTT), which has been tackled by many researchers and has many available benchmarks. The contributions of this paper are twofold. On the one side, we propose an effective and robust single-stage simulated annealing search method for solving the problem. On the other side, we design and apply an extensive and statistically-principled analysis methodology for the algorithm parameter tuning procedure. The outcome of this analysis is a linear regression model between instance features and search method parameters, that allows us to set the parameters for unseen instances on the basis of a simple inspection of the instance itself. Using this method, our algorithm, despite its apparent simplicity, has been able to achieve very good experimental results on a set of popular benchmark testbeds. 1 ar X iv :1 40 9. 71 86 v1 [ cs .A I] 2 5 Se p 20 14 Finally, we propose many new, real-world instances, that could be used as ground for future comparisons.", "creator": "LaTeX with hyperref package"}}}