{"id": "1409.7938", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2014", "title": "Lazier Than Lazy Greedy", "abstract": "Is it possible to maximize a monotone submodular function faster than the widely used lazy greedy algorithm (also known as accelerated greedy), both in theory and practice? In this paper, we develop the first linear-time algorithm for maximizing a general monotone submodular function subject to a cardinality constraint. We show that our randomized algorithm, Rand-Greedy, can achieve a (1-1/e) approximation guarantee to the optimum solution in time linear in the size of the data and independent of the cardinality constraint. We empirically demonstrate the effectiveness of our algorithm on submodular functions arising in data summarization, including training large-scale kernel methods and exemplar-based clustering. We observe that Rand-Greedy practically achieves the same utility value as lazy greedy but runs much faster. More surprisingly, we observe that in many practical scenarios Rand-Greedy does not evaluate the whole fraction of data points even once and still achieves indistinguishable results compared to lazy greedy.", "histories": [["v1", "Sun, 28 Sep 2014 18:06:23 GMT  (44kb)", "https://arxiv.org/abs/1409.7938v1", "14 pages, 9 figures, sibmitted to AAAI 2014"], ["v2", "Wed, 26 Nov 2014 08:45:32 GMT  (855kb)", "http://arxiv.org/abs/1409.7938v2", "In Proc. Conference on Artificial Intelligence (AAAI), 2015"], ["v3", "Fri, 28 Nov 2014 13:06:54 GMT  (855kb)", "http://arxiv.org/abs/1409.7938v3", "In Proc. Conference on Artificial Intelligence (AAAI), 2015"]], "COMMENTS": "14 pages, 9 figures, sibmitted to AAAI 2014", "reviews": [], "SUBJECTS": "cs.LG cs.DS cs.IR", "authors": ["baharan mirzasoleiman", "ashwinkumar badanidiyuru", "amin karbasi", "jan vondr\u00e1k", "andreas krause 0001"], "accepted": true, "id": "1409.7938"}, "pdf": {"name": "1409.7938.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Baharan Mirzasoleiman", "Ashwinkumar Badanidiyuru", "Amin Karbasi", "Jan Vondr\u00e1k", "Andreas Krause"], "emails": ["baharanm@inf.ethz.ch", "ashwinkumarbv@google.com", "amin.karbasi@yale.edu", "jvondrak@us.ibm.com", "krausea@ethz.ch"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 9.\n79 38\nv3 [\ncs .L\nG ]\n2 8\nN ov\n2 01"}, {"heading": "Introduction", "text": "For the last several years, we have witnessed the emergence of datasets of an unprecedented scale across different scientific disciplines. The large volume of such datasets presents new computational challenges as the diverse, feature-rich, unstructured and usually high-resolution data does not allow for effective data-intensive inference. In this regard, data summarization is a compelling (and sometimes the only) approach that aims at both exploiting the richness of largescale data and being computationally tractable. Instead of operating on complex and large data directly, carefully constructed summaries not only enable the execution of various data analytics tasks but also improve their efficiency and scalability.\nIn order to effectively summarize the data, we need to define a measure for the amount of representativeness that lies within a selected set. If we think of representative elements as the ones that cover best, or are most informative w.r.t. the items in a dataset then naturally adding a new element to a set of representatives, say A, is more beneficial than adding it to its superset, say B \u2287 A, as the new element\nIn Proc. Conference on Artificial Intelligence (AAAI), 2015.\ncan potentially enclose more uncovered items when considered with elements in A rather than B. This intuitive diminishing returns property can be systematically formalized through submodularity (c.f., Nemhauser, Wolsey, and Fisher (1978)). More precisely, a submodular function f : 2V \u2192 R assigns a subset A \u2286 V a utility value f(A) \u2013measuring the representativeness of the set A\u2013 such that\nf(A \u222a {i})\u2212 f(A) \u2265 f(B \u222a {i})\u2212 f(B)\nfor any A \u2286 B \u2286 V and i \u2208 V \\ B. Note that \u2206(i|A) . = f(A \u222a {i}) \u2212 f(A) measures the marginal gain of adding a new element i to a summary A. Of course, the meaning of representativeness (or utility value) depends very much on the underlying application; for a collection of random variables, the utility of a subset can be measured in terms of entropy, and for a collection of vectors, the utility of a subset can be measured in terms of the dimension of a subspace spanned by them. In fact, summarization through submodular functions has gained a lot of interest in recent years with application ranging from exemplarbased clustering (Gomes and Krause 2010), to document (Lin and Bilmes 2011; Dasgupta, Kumar, and Ravi 2013) and corpus summarization (Sipos et al. 2012), to recommender systems (Leskovec et al. 2007; El-Arini et al. 2009; El-Arini and Guestrin 2011).\nSince we would like to choose a summary of a manageable size, a natural optimization problem is to find a summary A\u2217 of size at most k that maximizes the utility, i.e.,\nA\u2217 = argmaxA:|A|\u2264kf(A). (1)\nUnfortunately, this optimization problem is NPhard for many classes of submodular functions (Nemhauser and Wolsey 1978; Feige 1998). We say a submodular function is monotone if for any A \u2286 B \u2286 V we have f(A) \u2264 f(B). A celebrated result of Nemhauser, Wolsey, and Fisher (1978) \u2013with great importance in artificial intelligence and machine learning\u2013 states that for non-negative monotone submodular functions a simple greedy algorithm provides a solution with (1\u22121/e) approximation guarantee to the optimal (intractable) solution. This greedy algorithm starts with the empty set A0 and in iteration i, adds an element maximizing the marginal gain \u2206(e|Ai\u22121). For a ground set V of size n, this greedy algorithm needs O(n \u00b7 k) function evaluations in order to\nfind a summarization of size k. However, in many data intensive applications, evaluating f is expensive and running the standard greedy algorithm is infeasible. Fortunately, submodularity can be exploited to implement an accelerated version of the classical greedy algorithm, usually called LAZY-GREEDY (Minoux 1978). Instead of computing \u2206(e|Ai\u22121) for each element e \u2208 V , the LAZY-GREEDY algorithm keeps an upper bound \u03c1(e) (initially \u221e) on the marginal gain sorted in decreasing order. In each iteration i, the LAZY-GREEDY algorithm evaluates the element on top of the list, say e, and updates its upper bound, \u03c1(e) \u2190 \u2206(e|Ai\u22121). If after the update \u03c1(e) \u2265 \u03c1(e\u2032) for all e\u2032 6= e, submodularity guarantees that e is the element with the largest marginal gain. Even though the exact cost (i.e., number of function evaluations) of LAZY-GREEDY is unknown, this algorithm leads to orders of magnitude speedups in practice. As a result, it has been used as the state-of-the-art implementation in numerous applications including network monitoring (Leskovec et al. 2007), network inference (Rodriguez, Leskovec, and Krause 2012), document summarization (Lin and Bilmes 2011), and speech data subset selection (Wei et al. 2013), to name a few. However, as the size of the data increases, even for small values of k, running LAZY-GREEDY is infeasible. A natural question to ask is whether it is possible to further accelerate LAZY-GREEDY by a procedure with a weaker dependency on k. Or even better, is it possible to have an algorithm that does not depend on k at all and scales linearly with the data size n?\nIn this paper, we propose the first linear-time algorithm, STOCHASTIC-GREEDY, for maximizing a non-negative monotone submodular function subject to a cardinality constraint k. We show that STOCHASTIC-GREEDY achieves a (1 \u2212 1/e \u2212 \u01eb) approximation guarantee to the optimum solution with running time O(n log(1/\u01eb)) (measured in terms of function evaluations) that is independent of k. Our experimental results on exemplar-based clustering and active set selection in nonparametric learning also confirms that STOCHASTIC-GREEDY consistently outperforms LAZY-GREEDY by a large margin while achieving practically the same utility value. More surprisingly, in our experiments we observe that STOCHASTIC-GREEDY sometimes does not even evaluate all the items and shows a running time that is less than n while still providing solutions close to the ones returned by LAZY-GREEDY. Due to its independence of k, STOCHASTIC-GREEDY is the first algorithm that truly scales to voluminous datasets."}, {"heading": "Related Work", "text": "Submodularity is a property of set functions with deep theoretical and practical consequences. For instance, submodular maximization generalizes many well-known combinatorial problems including maximum weighted matching, max coverage, and facility location, to name a few. It has also found numerous applications in machine learning and artificial intelligence such as influence maximization (Kempe, Kleinberg, and Tardos 2003), information gathering (Krause and Guestrin 2011), document summarization (Lin and Bilmes 2011)\nand active learning (Guillory and Bilmes 2011; Golovin and Krause 2011). In most of these applications one needs to handle increasingly larger quantities of data. For this purpose, accelerated/lazy variants (Minoux 1978; Leskovec et al. 2007) of the celebrated greedy algorithm of Nemhauser, Wolsey, and Fisher (1978) have been extensively used.\nScaling Up: To solve the optimization problem (1) at scale, there have been very recent efforts to either make use of parallel computing methods or treat data in a streaming fashion. In particular, Chierichetti, Kumar, and Tomkins (2010) and Blelloch, Peng, and Tangwongsan (2011) addressed a particular instance of submodular functions, namely, maximum coverage and provided a distributed method with a constant factor approximation to the centralized algorithm. More generally, Kumar et al. (2013) provided a constant approximation guarantee for general submodular functions with bounded marginal gains. Contemporarily, Mirzasoleiman et al. (2013) developed a two-stage distributed algorithm that guarantees solutions close to the optimum if the dataset is massive and the submodular function is smooth.\nSimilarly, Gomes and Krause (2010) presented a heuristic streaming algorithm for submodular function maximization and showed that under strong assumptions about the way the data stream is generated their method is effective. Very recently, Badanidiyuru et al. (2014) provided the first onepass streaming algorithm with a constant factor approximation guarantee for general submodular functions without any assumption about the data stream.\nEven though the goal of this paper is quite different and complementary in nature to the aforementioned work, STOCHASTIC-GREEDY can be easily integrated into existing distributed methods. For instance, STOCHASTICGREEDY can replace LAZY-GREEDY for solving each subproblem in the approach of Mirzasoleiman et al. (2013). More generally, any distributed algorithm that uses LAZYGREEDY as a sub-routine, can directly benefit from our method and provide even more efficient large-scale algorithmic frameworks.\nOur Approach: In this paper, we develop the first centralized algorithm whose cost (i.e., number of function evaluations) is independent of the cardinality constraint, which in turn directly addresses the shortcoming of LAZY-GREEDY. Perhaps the closest, in spirit, to our efforts are approaches by Wei, Iyer, and Bilmes (2014) and Badanidiyuru and Vondra\u0301k (2014). Concretely, Wei, Iyer, and Bilmes (2014) proposed a multistage algorithm, MULTI-GREEDY, that tries to decrease the running time of LAZY-GREEDY by approximating the underlying submodular function with a set of (sub)modular functions that can be potentially evaluated less expensively. This approach is effective only for those submodular functions that can be easily decomposed and approximated. Note again that STOCHASTIC-GREEDY can be used for solving the subproblems in each stage of MULTI-GREEDY to develop a faster multistage method. Badanidiyuru and Vondra\u0301k (2014) proposed a different centralized algorithm that achieves a\n(1\u2212 1/e\u2212 \u01eb) approximation guarantee for general submodular functions using O(n/\u01eb log(n/\u01eb)) function evaluations. However, STOCHASTIC-GREEDY consistently outperforms their algorithm in practice in terms of cost, and returns higher utility value. In addition, STOCHASTIC-GREEDY uses only O(n log(1/\u01eb)) function evaluations in theory, and thus provides a stronger analytical guarantee."}, {"heading": "STOCHASTIC-GREEDY Algorithm", "text": "In this section, we present our randomized greedy algorithm STOCHASTIC-GREEDY and then show how to combine it with lazy evaluations. We will show that STOCHASTICGREEDY has provably linear running time independent of k, while simultaneously having the same approximation ratio guarantee (in expectation). In the following section we will further demonstrate through experiments that this is also reflected in practice, i.e., STOCHASTIC-GREEDY is substantially faster than LAZY-GREEDY, while being practically identical to it in terms of the utility.\nThe main idea behind STOCHASTIC-GREEDY is to produce an element which improves the value of the solution roughly the same as greedy, but in a fast manner. This is achieved by a sub-sampling step. At a very high level this is similar to how stochastic gradient descent improves the running time of gradient descent for convex optimization."}, {"heading": "Random Sampling", "text": "The key reason that the classic greedy algorithm works is that at each iteration i, an element is identified that reduces the gap to the optimal solution by a significant amount, i.e., by at least (f(A\u2217) \u2212 f(Ai\u22121))/k. This requires n oracle calls per step, the main bottleneck of the classic greedy algorithm. Our main observation here is that by submodularity, we can achieve the same improvement by adding a uniformly random element from A\u2217 to our current set A. To get this improvement, we will see that it is enough to randomly sample a set R of size (n/k) log(1/\u01eb), which in turn overlaps with A\u2217 with probability 1 \u2212 \u01eb. This is the main reason we are able to achieve a boost in performance.\nThe algorithm is formally presented in Algorithm 1. Similar to the greedy algorithm, our algorithm starts with an empty set and adds one element at each iteration. But in each step it first samples a set R of size (n/k) log(1/\u01eb) uniformly at random and then adds the element from R to A which increases its value the most.\nAlgorithm 1 STOCHASTIC-GREEDY\nInput: f : 2V \u2192 R+, k \u2208 {1, . . . , n}. Output: A set A \u2286 V satisfying |A| \u2264 k.\n1: A \u2190 \u2205. 2: for (i \u2190 1; i \u2264 k; i \u2190 i+ 1) do 3: R \u2190 a random subset obtained by sampling s random elements from V \\A. 4: ai \u2190 argmaxa\u2208R\u2206(a|A). 5: A \u2190 A \u222a {ai} 6: return A.\nOur main theoretical result is the following. It shows that STOCHASTIC-GREEDY achieves a near-optimal solution for general monotone submodular functions, with computational complexity independent of the cardinality constraint. Theorem 1. Let f be a non-negative monotone submoduar function. Let us also set s = n\nk log 1 \u01eb . Then STOCHASTIC-\nGREEDY achieves a (1\u2212 1/e\u2212 \u01eb) approximation guarantee in expectation to the optimum solution of problem (1) with only O(n log 1\n\u01eb ) function evaluations.\nSince there are k iterations in total and at each iteration we have (n/k) log(1/\u01eb) elements, the total number of function evaluations cannot be more than k \u00d7 (n/k) log(1/\u01eb) = n log(1/\u01eb). The proof of the approximation guarantee is given in the analysis section."}, {"heading": "Random Sampling with Lazy Evaluation", "text": "While our theoretical results show a provably linear time algorithm, we can combine the random sampling procedure with lazy evaluation to boost its performance. There are mainly two reasons why lazy evaluation helps. First, the randomly sampled sets can overlap and we can exploit the previously evaluated marginal gains. Second, as in LAZYGREEDY although the marginal values of the elements might change in each step of the greedy algorithm, often their ordering does not change (Minoux 1978). Hence in line 4 of Algorithm 1 we can apply directly lazy evaluation as follows. We maintain an upper bound \u03c1(e) (initially \u221e) on the marginal gain of all elements sorted in decreasing order. In each iteration i, STOCHASTIC-GREEDY samples a set R. From this set R it evaluates the element that comes on top of the list. Let\u2019s denote this element by e. It then updates the upper bound for e, i.e., \u03c1(e) \u2190 \u2206(e|Ai\u22121). If after the update \u03c1(e) \u2265 \u03c1(e\u2032) for all e\u2032 6= e where e, e\u2032 \u2208 R, submodularity guarantees that e is the element with the largest marginal gain in the set R. Hence, lazy evaluation helps us reduce function evaluation in each round."}, {"heading": "Experimental Results", "text": "In this section, we address the following questions: 1) how well does STOCHASTIC-GREEDY perform compared to previous art and in particular LAZY-GREEDY, and 2) How does STOCHASTIC-GREEDY help us get near optimal solutions on large datasets by reducing the computational complexity? To this end, we compare the performance of our STOCHASTIC-GREEDY method to the following benchmarks: RANDOM-SELECTION, where the output is k randomly selected data points from V ; LAZY-GREEDY, where the output is the k data points produced by the accelerated greedy method (Minoux 1978); SAMPLE-GREEDY, where the output is the k data points produced by applying LAZYGREEDY on a subset of data points parametrized by sampling probability p; and THRESHOLD-GREEDY, where the output is the k data points provided by the algorithm of Badanidiyuru and Vondra\u0301k (2014). In order to compare the computational cost of different methods independently of the concrete implementation and platform, in our experiments we measure the computational cost in terms of the number of function evaluations used. Moreover, to implement the SAMPLE-GREEDY method, random subsamples\nare generated geometrically using different values for probability p. Higher values of p result in subsamples of larger size from the original dataset. To maximize fairness, we implemented an accelerated version of THRESHOLD-GREEDY with lazy evaluations (not specified in the paper) and report the best results in terms of function evaluations. Among all benchmarks, RANDOM-SELECTION has the lowest computational cost (namely, one) as we need to only evaluate the selected set at the end of the sampling process. However, it provides the lowest utility. On the other side of the spectrum, LAZY-GREEDY makes k passes over the full ground set, providing typically the best solution in terms of utility. The lazy evaluation eliminates a large fraction of the function evaluations in each pass. Nonetheless, it is still computationally prohibitive for large values of k.\nIn our experimental setup, we focus on three important and classic machine learning applications: nonparametric learning, exemplar-based clustering, and sensor placement.\nNonparametric Learning. Our first application is data subset selection in nonparametric learning. We focus on the special case of Gaussian Processes (GPs) below, but similar problems arise in large-scale kernelized SVMs and other kernel machines. Let XV , be a set of random variables indexed by the ground set V . In a Gaussian Process (GP) we assume that every subset XS , for S = {e1, . . . , es}, is distributed according to a multivariate normal distribution, i.e., P (XS = xS) = N (xS ;\u00b5S ,\u03a3S,S), where \u00b5S = (\u00b5e1 , . . . , \u00b5es) and \u03a3S,S = [Kei,ej ](1 \u2264 i, j \u2264 k) are the prior mean vector and prior covariance matrix, respectively. The covariance matrix is given in terms of a positive definite kernel K, e.g., the squared exponential kernel Kei,ej = exp(\u2212|ei \u2212 ej | 2 2/h\n2) is a common choice in practice. In GP regression, each data point e \u2208 V is considered a random variable. Upon observations yA = xA+nA (where nA is a vector of independent Gaussian noise with variance \u03c32), the predictive distribution of a new data point e \u2208 V is a normal distribution P (Xe | yA) = N (\u00b5e|A,\u03a32e|A), where\n\u00b5e|A = \u00b5e +\u03a3e,A(\u03a3A,A + \u03c3 2I)\u22121(xA \u2212 \u00b5A),\n\u03c32e|A = \u03c3 2 e \u2212 \u03a3e,A(\u03a3A,A + \u03c3 2I)\u22121\u03a3A,e. (2)\nNote that evaluating (2) is computationally expensive as it requires a matrix inversion. Instead, most efficient approaches for making predictions in GPs rely on choosing a small \u2013 so called active \u2013 set of data points. For instance, in the Informative Vector Machine (IVM) we seek a summary A such that the information gain, f(A) = I(YA;XV ) = H(XV )\u2212H(XV |YA) = 1 2 log det(I+ \u03c3\n\u22122\u03a3A,A) is maximized. It can be shown that this choice of f is monotone submodular (Krause and Guestrin 2005). For small values of |A|, running LAZY-GREEDY is possible. However, we see that as the size of the active set or summary A increases, the only viable option in practice is STOCHASTIC-GREEDY.\nIn our experiment we chose a Gaussian kernel with h = 0.75 and \u03c3 = 1. We used the Parkinsons Telemonitoring dataset (Tsanas et al. 2010) consisting of 5,875 bio-medical voice measurements with 22 attributes from people with early-stage Parkinsons disease. We normalized the vectors\nto zero mean and unit norm. Fig. 1a and 1b compare the utility and computational cost of STOCHASTIC-GREEDY to the benchmarks for different values of k. For THRESHOLDGREEDY, different values of \u01eb have been chosen such that a performance close to that of LAZY-GREEDY is obtained. Moreover, different values of p have been chosen such that the cost of SAMPLE-GREEDY is almost equal to that of STOCHASTIC-GREEDY for different values of \u01eb. As we can see, STOCHASTIC-GREEDY provides the closest (practically identical) utility to that of LAZY-GREEDY with much lower computational cost. Decreasing the value of \u03b5 results in higher utility at the price of higher computational cost. Fig. 1c shows the utility versus cost of STOCHASTICGREEDY along with the other benchmarks for a fixed k = 200 and different values of \u01eb. STOCHASTIC-GREEDY provides very compelling tradeoffs between utility and cost compared to all benchmarks, including LAZY-GREEDY.\nExemplar-based clustering. A classic way to select a set of exemplars that best represent a massive dataset is to solve the k-medoid problem (Kaufman and Rousseeuw 2009) by minimizing the sum of pairwise dissimilarities between exemplarsA and elements of the dataset V as follows: L(A) = 1 V \u2211\ne\u2208V minv\u2208A d(e, v), where d : V \u00d7 V \u2192 R is a distance function, encoding the dissimilarity between elements. By introducing an appropriate auxiliary element e0 we can turn L into a monotone submodular function (Gomes and Krause 2010): f(A) = L({e0})\u2212L(A\u222a{e0}). Thus maximizing f is equivalent to minimizing L which provides a very good solution. But the problem becomes computationally challenging as the size of the summary A increases.\nIn our experiment we chose d(x, x\u2032) = ||x\u2212 x\u2032||2 for the dissimilarity measure. We used a set of 10,000 Tiny Images (Torralba, Fergus, and Freeman 2008) where each 32 \u00d7 32 RGB image was represented by a 3,072 dimensional vector. We subtracted from each vector the mean value, normalized it to unit norm, and used the origin as the auxiliary exemplar. Fig. 1d and 1e compare the utility and computational cost of STOCHASTIC-GREEDY to the benchmarks for different values of k. It can be seen that STOCHASTIC-GREEDY outperforms the benchmarks with significantly lower computational cost. Fig. 1f compares the utility versus cost of different methods for a fixed k = 200 and various p and \u01eb. Similar to the previous experiment, STOCHASTIC-GREEDY achieves near-maximal utility at substantially lower cost compared to the other benchmarks.\nLarge scale experiment. We also performed a similar experiment on a larger set of 50,000 Tiny Images. For this dataset, we were not able to run LAZY-GREEDY and THRESHOLD-GREEDY. Hence, we compared the utility and cost of STOCHASTIC-GREEDY with RANDOM-SELECTION using different values of p. As shown in Fig. 1j and Fig. 1k, STOCHASTIC-GREEDY outperforms SAMPLE-GREEDY in terms of both utility and cost for different values of k. Finally, as Fig. 1l shows that STOCHASTIC-GREEDY achieves the highest utility but performs much faster compare to SAMPLE-GREEDY which is the only practical solution for this larger dataset.\nSensor Placement. When monitoring spatial phenomena, we want to deploy a limited number of sensors in an area in order to quickly detect contaminants. Thus, the problem would be to select a subset of all possible locationsA \u2286 V to place sensors. Consider a set of intrusion scenarios I where each scenario i \u2208 I defines the introduction of a contaminant at a specified point in time. For each sensor s \u2208 S and scenario i, the detection time, T (s, i), is defined as the time it takes for s to detect i. If s never detects i, we set T (s, i) = \u221e. For a set of sensors A, detection time for scenario i could be defined as T (A, i) = mins\u2208A T (s, i). Depending on the time of detection, we incur penalty \u03c0i(t) for detecting scenario i at time t. Let \u03c0i(\u221e) be the maximum penalty incurred if the scenario i is not detected at all. Then, the penalty reduction for scenario i can be defined as R(A, i) = \u03c0i(\u221e) \u2212 \u03c0i(T (A, i)). Having a probability distribution over possible scenarios, we can calculate the expected penalty reduction for a sensor placement A as R(A) = \u2211\ni\u2208I P (i)R(A, i). This function is montone submodular (Krause et al. 2008) and for which the greedy algorithm gives us a good solution. For massive data however, we may need to resort to STOCHASTIC-GREEDY.\nIn our experiments we used the 12,527 node distribution network provided as part of the Battle of Water Sensor Networks (BWSN) challenge (Ostfeld et al. 2008). Fig. 1g and 1h compare the utility and computational cost of STOCHASTIC-GREEDY to the benchmarks for different values of k. It can be seen that STOCHASTIC-GREEDY outperforms the benchmarks with significantly lower computational cost. Fig. 1i compares the utility versus cost of different methods for a fixed k = 200 and various p and \u01eb. Again STOCHASTIC-GREEDY shows similar behavior to the previous experiments by achieving near-maximal utility at much lower cost compared to the other benchmarks."}, {"heading": "Conclusion", "text": "We have developed the first linear time algorithm STOCHASTIC-GREEDY with no dependence on k for cardinality constrained submodular maximization. STOCHASTIC-GREEDY provides a 1 \u2212 1/e \u2212 \u01eb approximation guarantee to the optimum solution with only n log 1\n\u01eb function evaluations. We have also demonstrated\nthe effectiveness of our algorithm through an extensive set of experiments. As these show, STOCHASTIC-GREEDY achieves a major fraction of the function utility with much less computational cost. This improvement is useful even in approaches that make use of parallel computing or decompose the submodular function into simpler functions for faster evaluation. The properties of STOCHASTIC-GREEDY make it very appealing and necessary for solving very large scale problems. Given the importance of submodular optimization to numerous AI and machine learning applications, we believe our results provide an important step towards addressing such problems at scale."}, {"heading": "Appendix, Analysis", "text": "The following lemma gives us the approximation guarantee.\nLemma 2. Given a current solution A, the expected gain of STOCHASTIC-GREEDY in one step is at least 1\u2212\u01eb k \u2211 a\u2208A\u2217\\A \u2206(a|A).\nProof. Let us estimate the probability that R\u2229(A\u2217\\A) 6= \u2205. The set R consists of s = n\nk log 1 \u01eb random samples from\nV \\A (w.l.o.g. with repetition), and hence\nPr[R \u2229 (A\u2217 \\A) = \u2205] =\n(\n1\u2212 |A\u2217 \\A|\n|V \\A|\n)s\n\u2264 e\u2212s |A\u2217\\A| |V \\A| \u2264 e\u2212 s n |A\u2217\\A|.\nTherefore, by using the concavity of 1\u2212 e\u2212 s n x as a function of x and the fact that x = |A\u2217 \\A| \u2208 [0, k], we have\nPr[R\u2229(A\u2217\\A) 6=\u2205]\u22651\u2212e\u2212 s n |A\u2217\\A|\u2265(1\u2212e\u2212 sk n )\n|A\u2217 \\A|\nk .\nRecall that we chose s = n k log 1 \u01eb , which gives\nPr[R \u2229 (A\u2217 \\A) 6= \u2205] \u2265 (1 \u2212 \u01eb) |A\u2217 \\A|\nk . (3)\nNow consider STOCHASTIC-GREEDY: it picks an element a \u2208 R maximizing the marginal value \u2206(a|A). This is clearly as much as the marginal value of an element randomly chosen from R \u2229 (A\u2217 \\ A) (if nonempty). Overall, R is equally likely to contain each element of A\u2217 \\ A, so a uniformly random element of R \u2229 (A\u2217 \\ A) is actually a uniformly random element of A\u2217 \\A. Thus, we obtain\nE[\u2206(a|A)]\u2265Pr[R\u2229(A\u2217\\A) 6= \u2205]\u00d7 1\n|A\u2217 \\A|\n\u2211\na\u2208A\u2217\\A\n\u2206(a|A).\nUsing (3), we conclude that E[\u2206(a|A)] \u2265 1\u2212\u01eb k \u2211 a\u2208A\u2217\\A \u2206(a|A).\nNow it is straightforward to finish the proof of Theorem 1. Let Ai = {a1, . . . , ai} denote the solution returned by STOCHASTIC-GREEDY after i steps. From Lemma 2,\nE[\u2206(ai+1|Ai) | Ai] \u2265 1\u2212 \u01eb\nk\n\u2211\na\u2208A\u2217\\Ai\n\u2206(a|Ai).\nBy submodularity, \u2211\na\u2208A\u2217\\Ai\n\u2206(a|Ai) \u2265 \u2206(A \u2217|Ai) \u2265 f(A \u2217)\u2212 f(Ai).\nTherefore, E[f(Ai+1)\u2212 f(Ai) | Ai] = E[\u2206(ai+1|Ai) | Ai]\n\u2265 1\u2212 \u01eb\nk (f(A\u2217)\u2212 f(Ai)).\nBy taking expectation over Ai,\nE[f(Ai+1)\u2212 f(Ai)] \u2265 1\u2212 \u01eb\nk E[f(A\u2217)\u2212 f(Ai)].\nBy induction, this implies that\nE[f(Ak)] \u2265\n(\n1\u2212\n(\n1\u2212 1\u2212 \u01eb\nk\n)k )\nf(A\u2217)\n\u2265 ( 1\u2212 e\u2212(1\u2212\u01eb) ) f(A\u2217) \u2265 (1\u2212 1/e\u2212 \u01eb)f(A\u2217).\nAcknowledgment. This research was supported by SNF 200021-137971, ERC StG 307036, a Microsoft Faculty Fellowship, and an ETH Fellowship."}], "references": [{"title": "and Vondr\u00e1k", "author": ["A. Badanidiyuru"], "venue": "J.", "citeRegEx": "Badanidiyuru and Vondr\u00e1k 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Streaming submodular optimization: Massive data summarization on the fly", "author": ["Badanidiyuru"], "venue": null, "citeRegEx": "Badanidiyuru,? \\Q2014\\E", "shortCiteRegEx": "Badanidiyuru", "year": 2014}, {"title": "G", "author": ["Blelloch"], "venue": "E.; Peng, R.; and Tangwongsan, K.", "citeRegEx": "Blelloch. Peng. and Tangwongsan 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Max-cover in mapreduce", "author": ["Kumar Chierichetti", "F. Tomkins 2010] Chierichetti", "R. Kumar", "A. Tomkins"], "venue": "In Proceedings of the 19th international conference on World wide web", "citeRegEx": "Chierichetti et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chierichetti et al\\.", "year": 2010}, {"title": "Summarization through submodularity and dispersion", "author": ["Kumar Dasgupta", "A. Ravi 2013] Dasgupta", "R. Kumar", "S. Ravi"], "venue": null, "citeRegEx": "Dasgupta et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2013}, {"title": "and Guestrin", "author": ["K. El-Arini"], "venue": "C.", "citeRegEx": "El.Arini and Guestrin 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Turning down the noise in the blogosphere", "author": ["El-Arini"], "venue": null, "citeRegEx": "El.Arini,? \\Q2009\\E", "shortCiteRegEx": "El.Arini", "year": 2009}, {"title": "and Krause", "author": ["D. Golovin"], "venue": "A.", "citeRegEx": "Golovin and Krause 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Krause", "author": ["R. Gomes"], "venue": "A.", "citeRegEx": "Gomes and Krause 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "and Bilmes", "author": ["A. Guillory"], "venue": "J.", "citeRegEx": "Guillory and Bilmes 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "P", "author": ["L. Kaufman", "Rousseeuw"], "venue": "J.", "citeRegEx": "Kaufman and Rousseeuw 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Maximizing the spread of influence through a social network", "author": ["Kleinberg Kempe", "D. Tardos 2003] Kempe", "J. Kleinberg", "E. Tardos"], "venue": "In Proceedings of the ninth ACM SIGKDD", "citeRegEx": "Kempe et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kempe et al\\.", "year": 2003}, {"title": "and Guestrin", "author": ["A. Krause"], "venue": "C.", "citeRegEx": "Krause and Guestrin 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "and Guestrin", "author": ["A. Krause"], "venue": "C.", "citeRegEx": "Krause and Guestrin 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient sensor placement optimization for securing large water distribution", "author": ["Krause"], "venue": null, "citeRegEx": "Krause,? \\Q2008\\E", "shortCiteRegEx": "Krause", "year": 2008}, {"title": "Fast greedy algorithms in mapreduce and streaming", "author": ["Kumar"], "venue": "In SPAA", "citeRegEx": "Kumar,? \\Q2013\\E", "shortCiteRegEx": "Kumar", "year": 2013}, {"title": "Cost-effective outbreak detection in networks", "author": ["Leskovec"], "venue": null, "citeRegEx": "Leskovec,? \\Q2007\\E", "shortCiteRegEx": "Leskovec", "year": 2007}, {"title": "and Bilmes", "author": ["H. Lin"], "venue": "J.", "citeRegEx": "Lin and Bilmes 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed submodular maximization: Identifying representative elements in massive data", "author": ["Mirzasoleiman"], "venue": "In Neural Information Processing Systems (NIPS)", "citeRegEx": "Mirzasoleiman,? \\Q2013\\E", "shortCiteRegEx": "Mirzasoleiman", "year": 2013}, {"title": "L", "author": ["G.L. Nemhauser", "Wolsey"], "venue": "A.", "citeRegEx": "Nemhauser and Wolsey 1978", "shortCiteRegEx": null, "year": 1978}, {"title": "M", "author": ["G.L. Nemhauser", "L.A. Wolsey", "Fisher"], "venue": "L.", "citeRegEx": "Nemhauser. Wolsey. and Fisher 1978", "shortCiteRegEx": null, "year": 1978}, {"title": "C", "author": ["A. Ostfeld", "J.G. Uber", "E. Salomons", "J.W. Berry", "W.E. Hart", "Phillips"], "venue": "A.; Watson, J.-P.; Dorini, G.; Jonkergouw, P.; Kapelan, Z.; et al.", "citeRegEx": "Ostfeld et al. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "M", "author": ["Rodriguez"], "venue": "G.; Leskovec, J.; and Krause, A.", "citeRegEx": "Rodriguez. Leskovec. and Krause 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Temporal corpus summarization using submodular word coverage", "author": ["Sipos"], "venue": "In CIKM", "citeRegEx": "Sipos,? \\Q2012\\E", "shortCiteRegEx": "Sipos", "year": 2012}, {"title": "W", "author": ["A. Torralba", "R. Fergus", "Freeman"], "venue": "T.", "citeRegEx": "Torralba. Fergus. and Freeman 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "L", "author": ["A. Tsanas", "M.A. Little", "P.E. McSharry", "Ramig"], "venue": "O.", "citeRegEx": "Tsanas et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Using document summarization techniques for speech data subset selection", "author": ["Wei"], "venue": null, "citeRegEx": "Wei,? \\Q2013\\E", "shortCiteRegEx": "Wei", "year": 2013}, {"title": "Fast multi-stage submodular maximization", "author": ["Iyer Wei", "K. Bilmes 2014] Wei", "R. Iyer", "J. Bilmes"], "venue": null, "citeRegEx": "Wei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2014}], "referenceMentions": [], "year": 2014, "abstractText": "Is it possible to maximize a monotone submodular function faster than the widely used lazy greedy algorithm (also known as accelerated greedy), both in theory and practice? In this paper, we develop the first linear-time algorithm for maximizing a general monotone submodular function subject to a cardinality constraint. We show that our randomized algorithm, STOCHASTIC-GREEDY, can achieve a (1 \u2212 1/e \u2212 \u03b5) approximation guarantee, in expectation, to the optimum solution in time linear in the size of the data and independent of the cardinality constraint. We empirically demonstrate the effectiveness of our algorithm on submodular functions arising in data summarization, including training large-scale kernel methods, exemplar-based clustering, and sensor placement. We observe that STOCHASTIC-GREEDY practically achieves the same utility value as lazy greedy but runs much faster. More surprisingly, we observe that in many practical scenarios STOCHASTIC-GREEDY does not evaluate the whole fraction of data points even once and still achieves indistinguishable results compared to lazy greedy.", "creator": "LaTeX with hyperref package"}}}