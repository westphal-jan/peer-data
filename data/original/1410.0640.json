{"id": "1410.0640", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Oct-2014", "title": "Term-Weighting Learning via Genetic Programming for Text Classification", "abstract": "This paper describes a novel approach to learning term-weighting schemes (TWSs) in the context of text classification. In text mining a TWS determines the way in which documents will be represented in a vector space model, before applying a classifier. Whereas acceptable performance has been obtained with standard TWSs (e.g., Boolean and term-frequency schemes), the definition of TWSs has been traditionally an art. Further, it is still a difficult task to determine what is the best TWS for a particular problem and it is not clear yet, whether better schemes, than those currently available, can be generated by combining known TWS. We propose in this article a genetic program that aims at learning effective TWSs that can improve the performance of current schemes in text classification. The genetic program learns how to combine a set of basic units to give rise to discriminative TWSs. We report an extensive experimental study comprising data sets from thematic and non-thematic text classification as well as from image classification. Our study shows the validity of the proposed method; in fact, we show that TWSs learned with the genetic program outperform traditional schemes and other TWSs proposed in recent works. Further, we show that TWSs learned from a specific domain can be effectively used for other tasks.", "histories": [["v1", "Thu, 2 Oct 2014 18:38:11 GMT  (161kb)", "https://arxiv.org/abs/1410.0640v1", null], ["v2", "Fri, 3 Oct 2014 19:47:03 GMT  (161kb)", "http://arxiv.org/abs/1410.0640v2", null], ["v3", "Mon, 6 Oct 2014 20:48:29 GMT  (161kb)", "http://arxiv.org/abs/1410.0640v3", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["hugo jair escalante", "mauricio a garc\\'ia-lim\\'on", "alicia morales-reyes", "mario graff", "manuel montes-y-g\\'omez", "eduardo f morales"], "accepted": false, "id": "1410.0640"}, "pdf": {"name": "1410.0640.pdf", "metadata": {"source": "CRF", "title": "Term-Weighting Learning via Genetic Programming for Text Classification", "authors": ["Hugo Jair Escalante", "Mauricio A. Gar\u0107\u0131a-Lim\u00f3n", "Alicia Morales-Reyes", "Mario Graff", "Manuel Montes-y-G\u00f3mez", "Eduardo F. Morales"], "emails": ["hugojair@inaoep.mx", "mauricio.garcia.cs@gmail.com", "a.morales@inaoep.mx", "mgraffg@gmail.com", "mmontesg@inaoep.mx", "emorales@inaoep.mx"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 0.\n06 40\nv3 [\ncs .N\nThis paper describes a novel approach to learning term-weighting schemes (TWSs) in the context of text classification. In text mining a TWS determines the way in which documents will be represented in a vector space model, before applying a classifier. Whereas acceptable performance has been obtained with standard TWSs (e.g., Boolean and term-frequency schemes), the definition of TWSs has been traditionally an art. Further, it is still a difficult task to determine what is the best TWS for a particular problem and it is not clear yet, whether better schemes, than those currently available, can be generated by combining known TWS. We propose in this article a genetic program that aims at learning effective TWSs that can improve the performance of current schemes in text classification. The genetic program learns how to combine a set of basic units to give rise to discriminative TWSs. We report an extensive experimental study comprising data sets from thematic and non-thematic text classification as well as from image classification. Our study shows the validity of the proposed method; in fact, we show that TWSs learned with the genetic program outperform traditional schemes and other\n\u2217Corresponding author. Email addresses: hugojair@inaoep.mx (Hugo Jair Escalante),\nmauricio.garcia.cs@gmail.com (Mauricio A. Garc\u0301\u0131a-Limo\u0301n), a.morales@inaoep.mx (Alicia Morales-Reyes), mgraffg@gmail.com (Mario Graff), mmontesg@inaoep.mx (Manuel Montes-y-Go\u0301mez), emorales@inaoep.mx (Eduardo F. Morales)\nPreprint submitted to Elsevier October 8, 2014\nTWSs proposed in recent works. Further, we show that TWSs learned from a specific domain can be effectively used for other tasks.\nKeywords: Term-weighting Learning, Genetic Programming, Text Mining, Representation Learning, Bag of words. 2000 MSC: code 68T10, 2000 MSC: 68T20"}, {"heading": "1. Introduction", "text": "Text classification (TC) is the task of associating documents with predefined categories that are related to their content. TC is an important and active research field because of the large number of digital documents available and the consequent need to organize them. The TC problem has been approached with pattern classification methods, where documents are represented as numerical vectors and standard classifiers (e.g., na\u0308\u0131ve Bayes and support vector machines) are applied (Sebastiani, 2008). This type of representation is known as the vector space model (VSM) (Salton and Buckley, 1988). Under the VSM one assumes a document is a point in aN -dimensional space and documents that are closer in that space are similar to each other (Turney and Pantel, 2010). Among the different instances of the VSM, perhaps the most used model is the bag-of-words (BOW) representation. In the BOW it is assumed that the content of a document can be determined by the (orderless) set of terms1 it contains. Documents are represented as points in the vocabulary space, that is, a document is represented by a numerical vector of length equal to the number of different terms in the vocabulary (the set of all different terms in the document collection). The elements of the vector specify how important the corresponding terms are for describing the semantics or the content of the document. BOW is the most used document representation in both TC and information retrieval. In fact, the BOW representation has been successfully adopted for processing other media besides text, including, images (Csurka et al., 2004), videos (Sivic and Zisserman, 2003), speech signals (S.Manchala et al., 2014), and time series (Wanga et al., 2013) among others.\nA crucial component of TC systems using the BOW representation is the\n1A term is any basic unit by which documents are formed, for instance, terms could be words, phrases, and sequences (n-grams) of words or characters.\nterm-weighting scheme (TWS), which is in charge of determining how relevant a term is for describing the content of a document (Feldman and Sanger, 2006; Altyncay and Erenel, 2010; Lan et al., 2009; Debole and Sebastiani, 2003). Traditional TWS are term-frequency (TF ), where the importance of a term in a document is given by its frequency of occurrence in the document; Boolean (B), where the importance of a term in document is either 1, when the term appear in the document or 0, when the term does not appear in the document; and term-frequency inverse-document-frequency (TF-IDF ), where the importance of a term for a document is determined by its occurrence frequency times the inverse frequency of the term across the corpus (i.e., frequent terms in the corpus, as prepositions and articles, receive a low weight). Although, TC is a widely studied topic with very important developments in the last two decades (Sebastiani, 2008; Feldman and Sanger, 2006), it is somewhat surprising that little attention has been paid to the development of new TWSs to better represent the content of documents for TC. In fact, it is quite common in TC systems that researchers use one or two common TWSs (e.g., B, TF or TF-IDF ) and put more effort in other processes, like feature selection (Forman, 2003; Yang and Pedersen, 1997), or the learning process itself (Agarwal and Mittal, 2014; Aggarwal, 2012; Escalante et al., 2009). Although all of the TC phases are equally important, we think that by putting more emphasis on defining or learning effective TWSs we can achieve substantial improvements in TC performance.\nThis paper introduces a novel approach to learning TWS for TC tasks. A genetic program is proposed in which a set of primitives and basic TWSs are combined through arithmetic operators in order to generate alternative schemes that can improve the performance of a classifier. Genetic programming is a type of evolutionary algorithm in which a population of programs is evolved (Langdon and Poli, 2001), where programs encode solutions to complex problems (mostly modeling problems), in this work programs encode TWSs. The underlying hypothesis of our proposed method is that an evolutionary algorithm can learn TWSs of comparable or even better performance than those proposed so far in the literature.\nTraditional TWSs combine term-importance and term-document-importance factors to generate TWSs. For instance in TF-IDF, TF and IDF are termdocument-importance and term-importance factors, respectively. Term-document weights are referred as local factors, because they account for the occurrence of a term in a document (locally). On the other hand, term-relevance weights are considered global factors, as they account for the importance of a term\nacross the corpus (globally). It is noteworthy that the actual factors that define a TWS and the combination strategy itself have been determined manually. Herein we explore the suitability of learning these TWSs automatically, by providing a genetic program with a pool of TWSs\u2019 building blocks with the goal of evolving a TWS that maximizes the classification performance for a TC classifier. We report experimental results in many TC collections that comprise both: thematic and non-thematic TC problems. Throughout extensive experimentation we show that the proposed approach is very competitive, learning very effective TWSs that outperform most of the schemes proposed so far. We evaluate the performance of the proposed approach under different settings and analyze the characteristics of the learned TWSs. Additionally, we evaluate the generalization capabilities of the learned TWSs and even show that a TWS learned from text can be used to effectively represent images under the BOW formulation.\nThe rest of this document is organized as follows. Next section formally introduces the TC task and describes common TWSs. Section 3 reviews related work on TWSs. Section 4 introduces the proposed method. Section 5 describes the experimental settings adopted in this work and reports results of experiments that aim at evaluating different aspects of the proposed approach. Section 6 presents the conclusions derived from this paper and outlines future research directions."}, {"heading": "2. Text classification with the Bag of Words", "text": "The most studied TC problem is the so called thematic TC (or simply text categorization) (Sebastiani, 2008), which means that classes are associated to different themes or topics (e.g., classifying news into \u201cSports\u201d vs. \u201cPolitics\u201d categories). In this problem, the sole occurrence of certain terms may be enough to determine the topic of a document; for example, the occurrence of words/terms \u201cBasketball\u201d, \u201cGoal\u201d, \u201cBall\u201d, and \u201cFootball\u201d in a document is strong evidence that the document is about \u201cSports\u201d. Of course, there are more complex scenarios for thematic TC, for example, distinguishing documents about sports news into the categories: \u201cSoccer\u201d vs. \u201cNFL\u201d. Non-thematic TC, on the other hand, deals with the problem of associating documents with labels that are not (completely) related to their topics. Nonthematic TC includes the problems of authorship attribution (Stamatatos, 2009), opinion mining and sentiment analysis (Pang et al., 2002), authorship verification (Koppel and Schler, 2004), author profiling (Koppel et al., 2002),\namong several others (Reyes and Rosso, 2014; Kiddon and Brun, 2011). In all of these problems, the thematic content is of no interest, nevertheless, it is common to adopt standard TWSs for representing documents in nonthematic TC as well (e.g., BOW using character n-grams or part-of-speech tags (Stamatatos, 2009)).\nIt is noteworthy that the BOW representation has even trespassed the boundaries of the text media. Nowadays, images (Csurka et al., 2004), videos (Sivic and Zisserman, 2003), audio (S.Manchala et al., 2014), and other types of data (Wanga et al., 2013) are represented throughout analogies to the BOW. In non-textual data, a codebook is first defined/learned and then the straight BOW formulation is adopted. In image classification, for example, visual descriptors extracted from images are clustered and the centers of the clusters are considered as visual words (Csurka et al., 2004; Zhang et al., 2007). Images are then represented by numerical vectors (i.e., a VSM) that indicate the relevance of visual words for representing the images. Interestingly, in other media than text (e.g., video, images) it is standard to use only the TF TWS, hence motivating the study on the effectiveness of alternative TWSs in non-textual tasks. Accordingly, in this work we also perform experiments on learning TWSs for a standard computer vision problem (Fei-Fei et al., 2004).\nTC is a problem that has been approached mostly as a supervised learning task, where the goal is to learn a model capable of associating documents to categories (Sebastiani, 2008; Feldman and Sanger, 2006; Agarwal and Mittal, 2014). Consider a data set of labeled documents D = (xi, yi){1,...,N} with N pairs of documents (xi) and their classes (yi) associated to a TC problem; where we assume xi \u2208 Rp (i.e., a VSM) and yi \u2208 C = {1, . . .K}, for a problem with K\u2212classes. The goal of TC is to learn a function f : Rp \u2192 C from D that can be used to make predictions for documents with unknown labels, the so called test set: T = {xT1 , . . . ,xTM}. Under the BOW formulation, the dimensionality of documents\u2019 representation, p, is defined as p = |V |, where V is the vocabulary (i.e., the set all the different terms/words that appear in a corpus). Hence, each document di is represented by a numerical vector xi = \u3008xi,1 . . . , xi,|V |\u3009, where each element xi,j , j = 1, . . . , |V |, of xi indicates how relevant word tj is for describing the content of di, and where the value of xi,j is determined by the TWS.\nMany TWSs have been proposed so far, including unsupervised (Sebastiani, 2008; Salton and Buckley, 1988; Feldman and Sanger, 2006) and supervised schemes (Debole and Sebastiani, 2003; Lan et al., 2009), see Section 3. Unsupervised TWSs are the most used ones, they were firstly proposed for\ninformation retrieval tasks and latter adopted for TC (Sebastiani, 2008; Salton and Buckley, 1988). Unsupervised schemes rely on term frequency statistics and measurements that do not take into account any label information. For instance, under the Boolean (B) scheme xi,j = 1 iff term tj appears in document i and 0 otherwise; while in the term-frequency (TF ) scheme, xi,j = #(di, tj), where #(di, tj) accounts for the times term tj appears in document di. On the other hand, supervised TWSs aim at incorporating discriminative information into the representation of documents (Debole and Sebastiani, 2003). For example in the TF-IG scheme, xi,j = #(di, tj) \u00d7 IG(tj), is the product of the TF TWS for term tj and document di (a local factor) with the information gain of term tj (IG(tj), global factor). In this way, the discrimination power of each term is taken into account for the document representation; in this case through the information gain value (Yang and Pedersen, 1997). It is important to emphasize that most TWSs combine information from both term-importance (global) and term-document-importance (local) factors (see Section 3), for instance, in the TF-IG scheme, IG is a term-importance factor, whereas TF is a termdocument-importance factor.\nAlthough acceptable performance has been reported with existing TWS, it is still an art determining the adequate TWS for a particular data set; as a result, mostly unsupervised TWSs (e.g., B, TF and TF-IDF ) have been adopted for TC systems (Feldman and Sanger, 2006; Aggarwal, 2012). A first hypothesis of this work is that different TWSs can achieve better performance on different TC tasks (e.g., thematic TC vs. non-thematic TC); in fact, we claim that within a same domain (e.g., news classification) different TWSs are required to obtain better classification performance on different data sets. On the other hand, we notice that TWSs have been defined as combinations of term-document weighting factors (which can be seen as other TWSs, e.g., TF ) and term-relevance measurements (e.g., IDF or IG), where the definition of TWSs has been done by relying on the expertise of users/researchers. Our second hypothesis is that the definition of new TWSs can be automated. With the aim of verifying both hypotheses, this paper introduces a genetic program that learns how to combine term-document-importance and termrelevance factors to generate effective TWSs for diverse TC tasks."}, {"heading": "3. Related work", "text": "As previously mentioned, in TC it is rather common to use unsupervised TWSs to represent documents, specifically B, TF and TF-IDF schemes are very popular (see Table 1). Their popularity derives from the fact that these schemes have proved to be very effective in information retrieval (Salton and Buckley, 1988; Baeza-Yates and Ribeiro-Neto, 1999; Turney and Pantel, 2010) and in many TC problems as well as (Sebastiani, 2008; Feldman and Sanger, 2006; Agarwal and Mittal, 2014; Aggarwal, 2012; Aggarwal and Zhai, 2012). Unsupervised TWSs mainly capture term-document occurrence (e.g., term occurrence frequency, TF ) and term-relevance (e.g., inverse document frequency, IDF ) information. While acceptable performance has been obtained with such TWSs in many applications, in TC one has available labeled documents, and hence, document-label information can also be exploited to obtain more discriminative TWSs. This observation was noticed by Debole & Sebastiani and other authors that have introduced supervised TWSs (Debole and Sebastiani, 2003; Lan et al., 2009).\nSupervised TWSs take advantage of labeled data by incorporating a discriminative term-weighting factor into the TWSs. In (Debole and Sebastiani, 2003) TWSs were defined by combining the unsupervised TF scheme with the following term-relevance criteria: information gain (TF-IG), which measures the reduction of entropy when using a term as classifier (Yang and Pedersen, 1997); \u03c72 (TF-CHI ), makes an independence test regarding a term and the classes (Sebastiani, 2008); and gain-ratio (TF-GR) measuring the gain-ratio when using the term as classifier (Debole and Sebastiani, 2003). The conclusions from (Debole and Sebastiani, 2003) were that small improvements can be obtained with supervised TWSs over unsupervised ones. Although somewhat disappointing, it is interesting that for some scenarios supervised TWSs were beneficial. More recently, Lan et al. proposed an alternative supervised TWS (Lan et al., 2009), the so called TF-RF scheme. TF-RF combines TF with a criterion that takes into account the true positives and true negative rates when using the occurrence of the term as classifier. In (Lan et al., 2009) the proposed TF-RF scheme obtained better performance than unsupervised TWSs and even outperformed the schemes proposed in (Debole and Sebastiani, 2003). In (Altyncay and Erenel, 2010) the RF term-relevance factor was compared with alternative weights, including mutual information, odds ratio and \u03c72; in that workRF outperformed the other term-importance criteria.\nTable 1 shows most of the TWSs proposed so far for TC. It can be observed that TWSs are formed by combining term-document (TDR) and term (TR) relevance weights. The selection of what TDR and TR weights to use rely on researchers choices (and hence on their biases). It is quite common to use TF as TDR, because undoubtedly the term-occurrence frequency carries on very important information: we need a way to know what terms a document is associated with. However, it is not that clear what TR weight to use, as there is a wide variety of TR factors that have been proposed. The goal of TRs is to determine the importance of a given term, with respect to the documents in a corpus (in the unsupervised case) or to the classes of the problem (in the supervised case). Unsupervised TRs include: global term-frequency, and inverse document frequency (IDF) TRs. These weights can capture word importance depending on its global usage across a corpus, however, for TC seems more appealing to use discriminative TRs as one can take advantage of training labeled data. In this aspect, there is a wide variety of supervised TRs that have been proposed, including: mutual information, information gain, odds ratio, etcetera (Aggarwal and Zhai, 2012).\nThe goal of a supervised TR weight is to determine the importance of a given term with respect to the classes. The simplest, TR would be to estimate the correlation of term frequencies and the classes, although any other\ncriterion that accounts for the association of terms and classes can be helpful as well. It is interesting that although many TRs are available out there, they have been mostly used for feature selection rather than for building TWSs for TC. Comprehensive and extensive comparative studies using supervised TRs for feature selection have been reported (Altyncay and Erenel, 2010; Forman, 2003; Yang and Pedersen, 1997; Mladenic and Grobelnik, 1999). Although not being conclusive, these studies serve to identify the most effective TRs weights, such weights are considered in this study.\nTo the best of our knowledge, the way we approach the problem of learning TWSs for TC is novel. Similar approaches based on genetic programming to learn TWSs have been proposed in (Cummins and O\u2019Riordan, 2006, 2007, 2005; Trotman, 2005; Oren, 2002; Fan et al., 2004a), however, these researchers have focused on the information retrieval problem, which differs significantly from TC. Early approaches using genetic programming to improve the TF-IDF scheme for information retrieval include those from (Trotman, 2005; Oren, 2002; Fan et al., 2004a,b). More recently, Cummins et al. proposed improved genetic programs to learn TWSs also for information retrieval (Cummins and O\u2019Riordan, 2006, 2007, 2005).\nAlthough the work by Cummins et al. is very related to ours, there are major differences (besides the problem being approached): Cummins et al. approached the information retrieval task and defined a TWS as a combination of three factors: local, global weighting schemes and a normalization factor2. The authors designed a genetic program that aimed at learning a TWS by evolving the local and global schemes separately. Only 11 terminals, including constants, were considered. Since information retrieval is an unsupervised task, the authors have to use a whole corpus with relevance judgements (i.e., a collection of documents with queries and the set of relevant documents to each query) to learn the TWS, which, once learned, could be used for other information retrieval tasks. Hence they require a whole collection of documents to learn a TWS. On the other hand, the authors learned a TWS separately, first a global TWS was evolved fixing a binary local scheme, then a local scheme was learned by fixing the learned global weight. Hence, they restrict the search space for the genetic program, which\n2Recall a local factor incorporates term information (locally) available in a document, whereas a global term factor takes into account term statistics estimated across the whole corpus. In information retrieval it is also common to normalize the vectors representing a document to reduce the impact of the length of a document.\nmay limit the TWSs that can be obtained. Also, it is worth noticing that the focus of the authors of (Cummins and O\u2019Riordan, 2006, 2007, 2005) was on learning a single, and generic TWS to be used for other information retrieval problems, hence the authors performed many experiments and reported the single best solution they found after extensive experimentation. Herein, we provide an extensive evaluation of the proposed approach, reporting average performance over many runs and many data sets. Finally, one should note that the approach from (Cummins and O\u2019Riordan, 2006, 2007, 2005) required of large populations and numbers of generations (1000 individuals and 500 generations were used), whereas in this work competitive performance is obtained with only 50 individuals and 50 generations."}, {"heading": "4. Learning term-weighting schemes via GP", "text": "As previously mentioned, the traditional approach for defining TWSs has been somewhat successful so far. Nevertheless, it is still unknown whether we can automatize the TWS definition process and obtain TWSs of better classification performance in TC tasks. In this context, we propose a genetic programming solution that aims at learning effective TWSs automatically. We provide the genetic program with a pool of TDR and TR weights as well as other TWSs and let a program search for the TWS that maximizes an estimate of classification performance. Thus, instead of defining TWSs based on our own experiences on text mining, we let a computer itself to build an effective TWS. The advantages of this approach are that it may allow to learn a specific TWS for each TC problem, or to learn TWSs from one data set (e.g., a small one) and implement it in a different collection (e.g., a huge one). Furthermore, the method reduces the dependency on users/dataanalysts and their degree of expertise and biases for defining TWSs. The rest of this section describes the proposed approach. We start by providing a brief overview of genetic programming, then we explain in detail the proposal, finally, we close this section with a discussion on the benefits and limitations of our approach."}, {"heading": "4.1. Genetic programming", "text": "Genetic programming (GP) (Langdon and Poli, 2001) is an evolutionary technique which follows the reproductive cycle of other evolutionary algorithms such as genetic algorithms (see Figure 1): an initial population is created (randomly or by a pre-defined criterion), after that, individuals are\nselected, recombined, mutated and then placed back into the solutions pool. The distinctive feature of GP, when compared to other evolutionary algorithms, is in that complex data structures are used to represent solutions (individuals), for example, trees or graphs. As a result, GP can be used for solving complex learning/modeling problems. In the following we describe the GP approach to learn TWSs for TC."}, {"heading": "4.2. TWS learning with genetic programming", "text": "We face the problem of learning TWSs as an optimization one, in which we want to find a TWSs that maximizes the classification performance of a classifier trained with the TWS. We define a valid TWS as the combination of: (1) other TWSs, (2) TR and (3) TDR factors, and restrict the way in which such components can be combined by a set of arithmetic operators. We use GP as optimization strategy, where each individual corresponds to a tree-encoded TWS. The proposed genetic program explores the search space of TWSs that can be generated by combining TWSs, TRs and TDRs with\na predefined set of operators. The rest of this section details the components of the proposed genetic program, namely, representation, terminals and function set, genetic operators and fitness function."}, {"heading": "4.2.1. Representation", "text": "Solutions to our problem are encoded as trees, where we define terminal nodes to be the building blocks of TWSs. On the other hand, we let internal nodes of trees to be instantiated by arithmetic operators that combine the building blocks to generate new TWSs. The representation is graphically described in Figure 2."}, {"heading": "4.2.2. Terminals and function set", "text": "As previously mentioned, traditional TWSs are usually formed by two factors: a term-document relevance (TDR) weight and a term-relevance (TR) factor. The most used TDR is term frequency (TF ), as allows one to relate documents with the vocabulary. We consider TF as TDR indicator, but also we consider standard TWSs (e.g., Boolean, TD, RF ) as TDR weights. The decision to include other TWSs as building blocks is in order to determine whether standard TWSs can be enhanced with GP. Regarding TR, there are many alternatives available. In this work we analyzed the most common and effective TR weights as reported in the literature (Sebastiani, 2008;\nAltyncay and Erenel, 2010; Lan et al., 2009; Debole and Sebastiani, 2003; Forman, 2003) and considered them as building blocks for generating TWSs. Finally we also considered some constants as building blocks. The full set of building blocks (terminals in the tree representation) considered is shown in Table 1, whereas the set of operators considered in the proposed method (i.e., the function set) is the following: F = {+,\u2212, \u2217, /, log2 x, \u221a x, x2}, where F includes operators of arities one and two.\nIn the proposed approach, a TWS is seen as a combination of building blocks by means of arithmetic operators. One should note, however, that three types of building blocks are considered: TDR, TR and constants. Hence we must define a way to combine matrices (TDR weights), vectors (TR scores) and scalars (the constants), in such a way that the combination leads to a TWS (i.e., a form of TDR). Accordingly, and for easiness of implementation, each building block shown in Table 1 is processed as a matrix of the same length as the TWS (i.e., N \u00d7 |V |) and operations are performed element-wise. In this way a tree can be directly evaluated, and the operators are applied between each element of the matrices, leading to a TWS.\nTDRs are already matrices of the same size as the TWSs: N \u00d7 |V |. In the case of TRs, we have a vector of length |V |, thus for each TR we generate a matrix of size N \u00d7 |V | where each of its rows is the TR; that is, we repeat N times the TR weight. In this way, for example, a TWS like TF-IDF can\nbe obtained as TF \u00d7 IDF , where the \u00d7 operator means that each element tfi,j of TF is multiplied by each element of the IDF matrix idfi,j and where idfi,j = log(\nN df(tj ) ) for i = 1, . . . , N , all TRs were treated similarly. In the case\nof constants we use a scalar-matrix operator, which means that the constant is operated with each element of the matrix under analysis.\nEstimating the matrices each time a tree is evaluated can be a time consuming process, therefore, at the beginning of the search process we compute the necessary matrices for every terminal from Table 1. Hence, when evaluating an individual we only have to use the values of the precomputed matrices and apply the operators specified by a tree."}, {"heading": "4.2.3. Genetic operators", "text": "As explained above, in GP a population of individuals is initialized and evolved according to some operators that aim at improving the quality of the population. For initialization we used the standard ramped-half-and-half strategy (Eiben and Smith, 2010), which generates half of the population with (balanced) trees of maximum depth, and the other half with trees of variable depth. As genetic operators we also used standard mechanisms: we considered the subtree crossover and point mutation. The role of crossover is to take two promising solutions and combine their information to give rise to two offspring, with the goal that the offspring have better performance than the parents. The subtree crossover works by selecting two parent solutions/trees (in our case, via tournament) and randomly select an internal node in each of the parent trees. Two offspring are created by interchanging the subtrees below the identified nodes in the parent solutions.\nThe function of the mutation operator is to produce random variations in the population, facilitating the exploration capabilities of GP. The considered mutation operator first selects an individual to be mutated. Next an internal node of the individual is identified, and if the internal node is an operator (i.e., a member of F) it is replaced by another operator of the same arity. If the chosen node is a terminal, it is replaced by another terminal. Where in both cases the replacing node is selected with uniform probability."}, {"heading": "4.2.4. Fitness function", "text": "As previously mentioned, the aim of the proposed GP approach is to generate a TWS that obtains competitive classification performance. In this direction, the goodness of an individual is assessed via the classification performance of a predictive model that uses the representation generated by the\nTWS. Specifically, given a solution to the problem, we first evaluate the tree to generate a TWS using the training set. Once training documents are represented by the corresponding TWS, we perform a k\u2212fold cross-validation procedure to assess the effectiveness of the solution. In k\u2212fold cross validation, the training set is split into k disjoint subsets, and k rounds of training and testing are performed; in each round k \u2212 1 subsets are used as training set and 1 subset is used for testing, the process is repeated k times using a different subset for testing each time. The average classification performance is directly used as fitness function. Specifically, we evaluate the performance of classification models with the f1 measure. Let TP , FP and FN to denote the true positives, false positives and false negative rates for a particular class, precision (Prec) is defined as TP\nTP+FP and recall (Rec) as TP TP+FN . f1-measure\nis simply the harmonic average between precision and recall: f1 = 2\u00d7Prec\u00d7Rec Prec+Rec\n. The average across classes is reported (also called, macro-average f1), this way of estimating the f1-measure is known to be particularly useful when tackling unbalanced data sets (Sebastiani, 2008).\nSince under the fitness function k models have to be trained and tested for the evaluation of a single TWS, we need to look for an efficient classification model that, additionally, can deal naturally with the high-dimensionality of data. Support vector machines (SVM) comprise a type of models that have proved to be very effective for TC (Sebastiani, 2008; Joachims, 2008). SVMs can deal naturally with the sparseness and high dimensionality of data, however, training and testing an SVM can be a time consuming process. Therefore, we opted for efficient implementations of SVMs that have been proposed recently (Zhang et al., 2012; Djuric et al., 2013). That methods are trained online and under the scheme of learning with a budget. We use the predictions of an SVM as the fitness function for learning TWSs. Among the methods available in (Djuric et al., 2013) we used the low-rank linearized SVM (LLSMV) (Zhang et al., 2012). LLSVM is a linearized version of non-linear SVMs, which can be trained efficiently with the so called block minimization framework (Chang and Roth, 2011). We selected LLSVM instead of alterative methods, because this method has outperformed several other efficient implementations of SVMs, see e.g., (Djuric et al., 2013; Zhang et al., 2012)."}, {"heading": "4.3. Summary", "text": "We have described the proposed approach to learn TWSs via GP. When facing a TC problem we start by estimating all of the terminals described in Table 1 for the training set. The terminals are feed into the genetic program,\ntogether with the function set. We used the GPLAB toolbox for implementing the genetic program with default parameters (Silva and Almeida, 2003). The genetic program searches for the tree that maximizes the k\u2212fold cross validation performance of an efficient SVM using training data only. After a fixed number of generations, the genetic program returns the best solution found so far, the best TWS. Training and test (which was not used during the search process) data sets are represented according to such TWS. One should note that all of the supervised term-weights in Table 1 are estimated from the training set only (e.g., the information gain for terms is estimated using only the labeled training data); for representing test data we use the precomputed term-weights. Next, the LLSVM is trained in training data and the trained model makes predictions for test samples. We evaluate the performance of the proposed method by comparing the predictions of the model and the actual labels for test samples. The next section reports results of experiments that aim at evaluating the validity of the proposed approach."}, {"heading": "5. Experiments and results", "text": "This section presents an empirical evaluation of the proposed TWL approach. The goal of the experimental study is to assess the effectiveness of the learned TWSs and compare their performance to existing schemes. Additionally, we evaluate the generalization performance of learned schemes, and their effectiveness under different settings."}, {"heading": "5.1. Experimental settings", "text": "For experimentation we considered a suite of benchmark data sets associated to three types of tasks: thematic TC, authorship attribution (AA, a non-thematic TC task) and image classification (IC). Table 3 shows the characteristics of the data sets. We considered three types of tasks because we wanted to assess the generality of the proposed approach.\nSeven thematic TC data sets were considered, in these data sets the goal is to learn a model for thematic categories (e.g., sports news vs. religion news). The considered data sets are the most used ones for the evaluation of TC systems (Sebastiani, 2008). For TC data sets, indexing terms are the words (unigrams). Likewise, seven data sets for AA were used, the goal in these data sets is to learn a model capable of associating documents with authors. Opposed to thematic collections, the goal in AA is to model the\nwriting style of authors, hence, it has been shown that different representations and attributes are necessary for facing this task (Stamatatos, 2009). Accordingly, indexing terms in AA data sets were 3-grams of characters, that is, sequences of 3-characters found in documents, these terms have proved to be the most effective ones in AA (Stamatatos, 2009; Escalante et al., 2011; Luyckx and Daelemans, 2010). Finally, two data sets for image classification, taken from the CALTECH-101 collection, were used. We considered the collection under the standard experimental settings (15 images per class for training and 15 images for testing), two subsets of the CALTECH-101 data set were used: a small one with only 5 categories and the whole data set with 102 classes (101 object categories plus background) (Fei-Fei et al., 2004). Images were represented under the Bag-of-Visual-Words formulation using dense sift descriptors (PHOW features): descriptors extracted from images were clustered using k\u2212means, the centers of the clusters are the visual words (indexing terms), images are then represented by accounting the occurrence of visual words, the VLFEAT toolbox was used for processing images (Vedaldi and Fulkerson, 2008).\nThe considered data sets have been partitioned into training and test subsets (the number of documents for each partition and each data set are shown in Table 3). For some data sets there were predefined categories, while\nfor others we randomly generated them using 70% of documents for training and the rest for testing. All of the preprocessed data sets in Matlab format are publicly available3.\nFor each experiment, the training partition was used to learn the TWS, as explained in Section 4. The learned TWS is then evaluated in the corresponding test subset. We report two performance measures: accuracy, which is the percentage of correctly classified instances, and f1 measure, which assesses the tradeoff between precision and recall across classes (macro-average f1), recall that f1 was used as fitness function (see Section 4).\nThe genetic program was run for 50 generations using populations of 50 individuals, we would like to point out that in each run of the proposed method we have used default parameters. It is expected that by optimizing parameters and running the genetic program for more generations and larger populations we could obtain even better results. The goal of our study, however, was to show the potential of our method even with default parameters."}, {"heading": "5.2. Evaluation of TWS Learning via Genetic Programming", "text": "This section reports experimental results on learning TWSs with the genetic program described in Section 4. The goal of this experiment is to assess how TWSs learned via GP compare with traditional TWSs. The GP method was run on each of the 16 data sets from Table 3, since the vocabulary size for some data sets is huge we decided to reduce the number of terms by using term-frequency as criterion. Thus, for each data set we considered the top 2000 more frequent terms during the search process. In this way, the search process is accelerated at no significant loss of accuracy. In Section 5.3 we analyze the robustness of our method when using the whole vocabulary size for some data sets.\nFor each data set we performed 5 runs with the GP-based approach, we evaluated the performance of each learned TWS and report the average and standard deviation of performance across the five runs. Tables 4, 5, and 6 show the performance obtained by TWSs learned for thematic TC, AA and IC data sets, respectively. In the mentioned tables we also show the result obtained by the best baseline in each collection. Best baseline is the best TWS we found (from the set of TWSs reviewed in related work and the TWSs in Table 1) for each data set (using the test-set performance). Please note that\n3http://ccc.inaoep.mx/ hugojair/TWL/\nunder these circumstances best baseline is in fact, a quite strong baseline for our GP method. Also, we would like to emphasize that no parameter of the GP has been optimized, we used the same default parameters for every execution of the genetic program.\nFrom Table 4 it can be seen that, regarding the best baseline, different TWSs obtained better performance for different data sets. Hence evidencing the fact that different TWSs are required for different problems. On the other hand, it can be seen that the average performance of TWSs learned with our GP outperformed significantly the best baseline in all but one result (accuracy for Reuters-10 data set). The differences in performance are large, mainly for the f1 measure, which is somewhat expected as this was the measure used as fitness function (recall f1 measure is appropriate to account for the class imbalance across classes); hence showing the competitiveness of our proposed approach for learning effective TWSs for thematic TC tasks.\nperforms similarly as the proposed approach. In terms of f1 measure, our method outperforms the best baseline in 5 out of 7 data sets, while in accuracy our method beats the best baseline in 4 out of 7 data sets. Therefore, our method still obtains comparable (slightly better) performance to the best baselines, which for AA tasks were much more competitive than in thematic TC problems. One should note that for PG we are reporting the average performance across 5 runs, among the 5 runs we found TWSs that consistently outperformed the best baseline.\nIt is quite interesting that, comparing the best baselines from Tables 4 and 5, for AA tasks supervised TWSs obtained the best results (in particular TF-CHI in 4 out of 7 data sets), whereas for thematic TC unsupervised TWSs performed better. Again, these results show that different TWSs are required for different data sets and different types of problems. In fact, our results confirm the fact that AA and thematic TC tasks are quite different, and, more importantly, our study provides evidence on the suitability of supervised TWSs for AA; to the best of our knowledge, supervised TWSs have not been used in AA problems.\nTable 6 shows the results obtained for the image categorization data sets. Again, the proposed method obtained TWSs that outperformed the best baselines. This result is quite interesting because we are showing that the TWS plays a key role in the classification of images under the BOVWs approach. In computer vision most of the efforts so far have been devoted to the development of novel/better low-level image-descriptors, using a BOW with predefined TWS. Therefore, our results pave the way for research on learning TWSs for image categorization and other tasks that rely in the BOW representation (e.g. speech recognition and video classification).\nFigure 3 and Table 7 complement the results presented so far. Figure 3 indicates the difference in performance between the (average of) learned TWSs and the best baseline for each of the considered data sets. We can clearly appreciate from this figure the magnitude of improvement offered by the learned TWSs, which in some cases is too large.\nTable 7, on the other hand, shows a more fair comparison between our method and the reference TWSs: it shows the average performance obtained by reference schemes and the average performance of our method for thematic TC, AA and IC data sets. It is clear from this table that in average our method performs consistently better than any of the reference methods in terms of both accuracy and f1 measure for the three types of tasks. Thus, from the results of this table and those from Tables 4, 5, and 6, it is evident that standard TWSs are competitive, but one can take advantage of them only when the right TWS is selected for each data set. Also, the performance of TWSs learned with our approach are a better option than standard TWSs, as in average we were able to obtain much better representations.\nSummarizing the results from this section, we can conclude that:\n\u2022 The proposed GP obtained TWSs that outperformed the best baselines in the three types of tasks: thematic TC, AA and IC. Evidencing the generality of our proposal across different data types and modalities. Larger improvements were observed for thematic TC and IC data sets. In average, learned TWSs outperformed standard ones in the three types of tasks."}, {"heading": "5.3. Varying vocabulary size", "text": "For the experiments from Section 5.2 each TWS was learned by using only the top 2000 most frequent terms during the search process. This reduction in the vocabulary allowed us to speed up the search process significantly, however, it is worth asking ourselves what the performance of the TWSs would be when using an increasing number of terms. We aim to answer such question in this section.\nFor this experiment we considered three data sets, one from each type of task: thematic TC, AA, and IC. The considered data sets were the Reuters8 (R8) for thematic TC, the CCA benchmark for AA, and Caltech-101 for IC. These data sets are the representative ones from each task: Reuters-8 is among the most used TC data sets, CCA has been widely used for AA as well, and Caltech-101 is the benchmark in image categorization For each\nof the considered data sets we use a specific TWS learned using the top2000 most frequent terms (see Section 5.2), and evaluate the performance of such TWSs when increasing the vocabulary size: terms were sorted in ascending order of their frequency. Figures 4, 5, and 6 show the results of this experiment in terms of f1 measure and accuracy (the selected TWS is shown in the caption of each image).\nW21 when increasing the number of considered terms. The left plot shows results in\nterms of f1 measure while the right plot shows accuracy performance.\nDifferent performance behavior can be observed in the different data sets. Regarding Figure 4, which shows the performance for a thematic TC data set, it can be seen that the TWS learned by our method outperformed all other TWSs for any data set size. Hence confirming the suitability of the proposed method for thematic TC.\nFigure 5, on the other hand, behaves differently: the proposed method outperforms all the other TWSs only for a single data set size (when 20%\n\u221a\n\u221a W17 \u2212 \u221a\u221a W22 when increasing the number of considered terms.\nof the terms were used). In general, our method consistently outperformed TF-CHI and TF-IG TWSs, and performs similarly as TF-IDF, but it was outperformed by the TF-RF TWS. This result can be due to the fact that for this AA data set, the genetic program learned a TWS that was suitable only for the vocabulary size that was used during the optimization. Although, interesting, this result is not that surprising: in fact, it is well known in AA that the number of terms considered in the vocabulary plays a key role on the performance of AA systems. AA studies suggest using a small amount of the most-frequent terms when approaching an AA problem (Stamatatos, 2009; Escalante et al., 2011; Luyckx and Daelemans, 2010). Results from Figure 5 corroborate the latter and seem to indicate that when approaching an AA problem, one should first determine an appropriate vocabulary size and then apply our method. One should note, however, that our method outperforms the other TWSs for the data set size that was used during the optimization, and this is, in fact, the highest performance that can be obtained with any other TWS and data set size combination.\nFinally, Figure 6 reports the performance of TWSs for the Caltech-101 data set under different data set sizes. In this case, the learned TWS outperforms all other TWSs when using more than 20% and 30% in terms of f1 measure and accuracy, respectively. The improvement is consistent and monotonically increases as more terms are considered. Hence showing the robustness of the learned TWS when increasing the vocabulary size for IC tasks. Among the other TWSs, TFIDF obtains competitive performance when using a small vocabulary, this could be due to the fact that when considering a small number of frequent terms the IDF component is important\nfor weighting the contribution of each of the terms. Summarizing the results from this section we can conclude the following:\n\u2022 TWSs learned with our method are robust to variations in the vocabulary size for thematic TC and IC tasks. This result suggests, we can learn TWSs using a small number of terms (making more efficient the search process) and evaluating the learned TWSs with larger vocabularies.\n\u2022 Learned TWSs outperform standard TWSs in thematic TC and IC tasks when varying the vocabulary size.\n\u2022 For AA, TWSs learned with our proposed approach seem to be more dependent on the number of terms used during training. Hence, when facing this type of problems it is a better option to fix the number of terms beforehand and then running our method."}, {"heading": "5.4. Generalization of the learned term-weights", "text": "In this section we evaluate the inter-data set generalization capabilities of the learned TWSs. Although results presented so far show the generality of our method across three types of tasks, we have reported results obtained with TWSs that were learned for each specific data set. It remains unclear whether the TWSs learned for a collection can perform similarly in other collections, we aim to answer to this question in this section.\nTo assess the inter-data set generalization of TWSs learned with our method we performed an experiment in which we considered for each data set a single TWS and evaluated its performance across all the 16 considered data sets. The considered TWSs are shown in Table 8, we named the variables with meaningful acronyms for clarity but also show the mathematical expression using variables as defined in Table 2.\nBefore presenting the results of the experiments it is worth analyzing the type of solutions (TWSs) learned with the proposed approach. First of all, it can be seen that the learned TWSs are not too complex: the depth of the trees is small and solutions have few terminals as components. This is a positive result because it allows us to better analyze the solutions and, more importantly, it is an indirect indicator of the absence of the over-fitting phenomenon. Secondly, as in other applications of genetic programming, it is unavoidable to have unnecessary terms in the solutions, for instance, the subtree: div(pow2(TF-RF),pow2(TF-RF))), (from TWS 2) is unnecessary\nbecause it reduces to a constant matrix; the same happens with the term pow2(sqrt(TFIDF)). Nevertheless, it is important to emphasize that this type of terms do not harm the performance of learned TWSs, and there are not too many of these type of subtrees. On the other hand, it is interesting that all of the learned TWSs incorporate supervised information. The most used TR weight is RF, likewise the most used TDR is TFIDF. Also it is interesting that simple operations over standard TWSs, TR and TDR weights results in significant performance improvements. For instance, compare the performance of TF-RF and the learned weight for Caltech-101 in Figure 6. By simply subtracting an odds-ratio from the TF-RF TWS and applying scaling operations, the resultant TWS outperforms significantly TF-RF.\nThe 16 TWSs shown in Table 8 were evaluated in the 16 data sets in order to determine the inter-data set generality of the learned TWSs. Figure 7 shows the results of this experiment. We show the results with boxplots,\nwhere each boxplot indicates the normalized4 performance of each TWSs across the 16 data sets, for completion we also show the performance of the reference TWSs on the 16 data sets.\nIt can be seen from Figure 7 that the generalization performance of learned TWSs is mixed. On the one hand, it is clear that TWSs learned for thematic TC (boxplots 7-13) achieve the highest generalization performance. Clearly, the generalization performance of these TWSs is higher than that of traditional TWSs (boxplots 1-6). It is interesting that TWSs learned for a particular data set/problem/modality perform well across different data sets/problems/modalities. In particular, TWSs learned for Reuters-10 and TDT-2 obtained the highest performance and the lowest variance among all\n4Before generating the boxplots we normalized the performance on a per-data set basis: for each data set, the performance of the 16 TWSs was normalized to the range [0, 1], in this way, the variation in f1-values across data sets is eliminated, i.e, all f1 values are in the same scale and are comparable to each other.\nof the TWSs. On the other hand, TWSs learned for AA and IC tasks obtained lower generalization performance: the worst in terms of variance is the TWS learned for the Poetry data set, while the worst average performance was obtained by the TWS learned for the Football data set. TWSs learned for IC are competitive (in generalization performance) with traditional TWSs. Because of the nature of the tasks, the generalization performance of TWSs learned from TC is better than that of TWSs learned for AA and IC. One should note that these results confirm our findings from previous sections: (i) the proposed approach is very effective mainly for thematic TC and IC tasks; and, (ii) AA data sets are difficult to model with TWSs.\nFinally, we evaluate the generality of learned TWSs across different classifiers. The goal of this experiment is to assess the extend to which the learned TWSs are tailored for the classifier they were learn for. For this experiment, we selected two TWSs corresponding to Caltech-tiny and Caltech-101 (15 and 16 in Table 8) and evaluated their performance of different classifiers across the 16 data sets. Figure 8 shows the results of this experiment.\nIt can be seen from Figure 8 that the considered TWSs behaved quite differently depending on the classifier. On the one hand, the classification performance when using na\u0308\u0131ve Bayes (Naive), kernel-logistic regression (KLogistic), and 1\u2212nearest neighbors (KNN ) classifiers degraded significantly. On the other hand, the performance SVM and the neural network (NN) was very similar. These results show that TWSs are somewhat robust across classifiers of similar nature as SVM and NN are very similar classifiers: both are linear models in the parameters. The other classifiers are quite different to the reference SVM and, therefore, the performance is poor5. It is interesting that in some cases the NN classifier outperformed the SVM, although in average the SVM performed better. This is a somewhat expected result as the performance of the SVM was used as fitness function.\nAccording to the experimental results from this section we can draw the following conclusions:\n\u2022 TWSs learned with the proposed approach are not too complex despite their effectiveness. Most of the learned TWSs included a supervised component, evidencing the importance of taking advantage of labeled\n5One should note that among the three worse classifiers, KNN, Naive and KLogistic, the latter obtained better performance than the former two, this is due to the fact that KLogistic is closer, in nature, to an SVM.\ndocuments.\n\u2022 TWSs offer acceptable inter-data set generalization performance, in particular, TWSs learned for TC generalize pretty well across data sets.\n\u2022 We showed evidence that TWSs learned for a modality (e.g., text / images) can be very competitive when evaluated on other modality.\n\u2022 TWSs are somewhat robust to the classifier choice. It is preferable\nto use the classifier used to estimate the fitness function, although classifiers of similar nature perform similarly."}, {"heading": "6. Conclusions", "text": "We have described a novel approach to term-weighting scheme (TWS) learning in text classification (TC). TWSs specify the way in which documents are represented under a vector space model. We proposed a genetic programming solution in which standard TWSs, term-document, and term relevance weights are combined to give rise to effective TWSs. We reported experimental results in 16 well-known data sets comprising thematic TC, authorship attribution and image classification tasks. The performance of the proposed method is evaluated under different scenarios. Experimental results show that the proposed approach learns very effective TWSs that outperform standard TWSs. The main findings of this work can be summarized as follows:\n\u2022 TWSs learned with the proposed approach outperformed significantly to standard TWSs and those proposed in related work.\n\u2022 Defining the appropriate TWS is crucial for image classification tasks, an ignored issue in the field of computer vision.\n\u2022 In authorship attribution, supervised TWSs are beneficial, in comparison with standard TWSs.\n\u2022 The performance of learned TWSs do not degrades when varying the vocabulary size for thematic TC and IC. For authorship attribution a near-optimal vocabulary size should be selected before applying our method.\n\u2022 TWSs learned for a particular data set or modality can be applied to other data sets or modalities without degrading the classification performance. This generalization capability is mainly present in TWSs learned for thematic TC and IC.\n\u2022 Learned TWSs are easy to analyze/interpret and do not seem to overfit the training data.\nFuture work directions include studying the suitability of the proposed approach to learn weighting schemes for cross domain TC. Also we would like to perform an in deep study on the usefulness of the proposed GP for computer vision tasks relying in the Bag-of-Visual-Words formulation."}], "references": [{"title": "Text classification using machine learning methods-a survey", "author": ["B. Agarwal", "N. Mittal"], "venue": "Proceedings of the Second International Conference on Soft Computing for Problem Solving (SocProS 2012). Vol. 236 of Advances in Intelligent Systems and Computing", "citeRegEx": "Agarwal and Mittal,? \\Q2014\\E", "shortCiteRegEx": "Agarwal and Mittal", "year": 2014}, {"title": "Analytical evaluation of term weighting schemes for text categorization", "author": ["H. Altyncay", "Z. Erenel"], "venue": "Pattern Recognition Letters 31,", "citeRegEx": "Altyncay and Erenel,? \\Q2010\\E", "shortCiteRegEx": "Altyncay and Erenel", "year": 2010}, {"title": "Selective block minimization for faster convergence of limited memory large-scale linear models", "author": ["K.W. Chang", "D. Roth"], "venue": "ACM SIGKDD Conference on Knowledge Discovery and Data Mining", "citeRegEx": "Chang and Roth,? \\Q2011\\E", "shortCiteRegEx": "Chang and Roth", "year": 2011}, {"title": "Visual categorization with bags of keypoints. In: International workshop on statistical learning in computer vision", "author": ["G. Csurka", "C.R. Dance", "L. Fan", "J. Willamowski", "C. Bra"], "venue": null, "citeRegEx": "Csurka et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Csurka et al\\.", "year": 2004}, {"title": "Evolving general term-weighting schemes for information retrieval: Tests on larger collections", "author": ["R. Cummins", "C. O\u2019Riordan"], "venue": "Artificial Intelligence Review", "citeRegEx": "Cummins and O.Riordan,? \\Q2005\\E", "shortCiteRegEx": "Cummins and O.Riordan", "year": 2005}, {"title": "Evolving local and global weighting schemes in information retrieval", "author": ["R. Cummins", "C. O\u2019Riordan"], "venue": "Information Retrieval", "citeRegEx": "Cummins and O.Riordan,? \\Q2006\\E", "shortCiteRegEx": "Cummins and O.Riordan", "year": 2006}, {"title": "Evolved term-weighting schemes in information retrieval: An analysis of the solution space", "author": ["R. Cummins", "C. O\u2019Riordan"], "venue": "Artificial Intelligence", "citeRegEx": "Cummins and O.Riordan,? \\Q2007\\E", "shortCiteRegEx": "Cummins and O.Riordan", "year": 2007}, {"title": "Supervised term weighting for automated text categorization", "author": ["F. Debole", "F. Sebastiani"], "venue": "Proceedings of the 2003 ACM Symposium on Applied Computing. SAC \u201903. ACM,", "citeRegEx": "Debole and Sebastiani,? \\Q2003\\E", "shortCiteRegEx": "Debole and Sebastiani", "year": 2003}, {"title": "Budgetedsvm: A toolbox for scalable svm approximations", "author": ["N. Djuric", "L. Lan", "S. Vucetic", "Z. Wang"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Djuric et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Djuric et al\\.", "year": 2013}, {"title": "Introduction to Evolutionary Computing", "author": ["A.E. Eiben", "J.E. Smith"], "venue": null, "citeRegEx": "Eiben and Smith,? \\Q2010\\E", "shortCiteRegEx": "Eiben and Smith", "year": 2010}, {"title": "Particle swarm model selection for authorship verification", "author": ["H.J. Escalante", "M. Montes", "L. Villasenor"], "venue": "Proceedings of the 14th Iberoamerican Congress on Pattern Recognition", "citeRegEx": "Escalante et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Escalante et al\\.", "year": 2009}, {"title": "Local histograms of character n-grams for authorship attribution", "author": ["H.J. Escalante", "T. Solorio", "M.M. y Gomez"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Escalante et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Escalante et al\\.", "year": 2011}, {"title": "The effects of fitness functions on genetic programming based ranking discovery for web search", "author": ["W. Fan", "E.A. Fox", "P. Pathak", "H. Wu"], "venue": "Journal of the american Society for Information Science and Technology", "citeRegEx": "Fan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2004}, {"title": "A generic ranking function discovery framework by genetic programming for information retrieval", "author": ["W. Fan", "M.D. Gordon", "P. Pathak"], "venue": "Information Processing and Management", "citeRegEx": "Fan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2004}, {"title": "Learning generative visual models from few training examples: an incremental bayesian approach tested on 101 object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "Workshop on GenerativeModel Based Vision", "citeRegEx": "Fei.Fei et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fei.Fei et al\\.", "year": 2004}, {"title": "The Text Mining Handbook Advanced Approaches in Analyzing Unstructured Data. ABS Ventures", "author": ["R. Feldman", "J. Sanger"], "venue": null, "citeRegEx": "Feldman and Sanger,? \\Q2006\\E", "shortCiteRegEx": "Feldman and Sanger", "year": 2006}, {"title": "An extensive empirical study of feature selection metrics for text classification", "author": ["G. Forman"], "venue": "J. of Mach. Learn. Res", "citeRegEx": "Forman,? \\Q2003\\E", "shortCiteRegEx": "Forman", "year": 2003}, {"title": "Text categorization with support vector machines", "author": ["T. Joachims"], "venue": null, "citeRegEx": "Joachims,? \\Q2008\\E", "shortCiteRegEx": "Joachims", "year": 2008}, {"title": "That\u2019s what she said: Double entendre", "author": ["C. Kiddon", "Y. Brun"], "venue": null, "citeRegEx": "Kiddon and Brun,? \\Q2011\\E", "shortCiteRegEx": "Kiddon and Brun", "year": 2011}, {"title": "Authorship verification as a one-class classifi", "author": ["M. 401\u2013412. Koppel", "J. Schler"], "venue": null, "citeRegEx": "Koppel and Schler,? \\Q2004\\E", "shortCiteRegEx": "Koppel and Schler", "year": 2004}, {"title": "Supervised and traditional term", "author": ["C.L. Tan", "J. Su", "Y. Lu"], "venue": "Machine Learning", "citeRegEx": "M. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "M. et al\\.", "year": 2009}, {"title": "The effect of author set size and data size", "author": ["K. Springer. Luyckx", "W. Daelemans"], "venue": null, "citeRegEx": "Luyckx and Daelemans,? \\Q2010\\E", "shortCiteRegEx": "Luyckx and Daelemans", "year": 2010}, {"title": "Feature selection for unbalanced class", "author": ["D. Mladenic", "M. Grobelnik"], "venue": "Literary and Linguistic Computing,", "citeRegEx": "Mladenic and Grobelnik,? \\Q1999\\E", "shortCiteRegEx": "Mladenic and Grobelnik", "year": 1999}, {"title": "On the difficulty of automatically detecting irony", "author": ["A. Reyes", "P. Rosso"], "venue": null, "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2014}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["G. Salton", "C. Buckley"], "venue": "Information Processing and Management,", "citeRegEx": "Salton and Buckley,? \\Q1988\\E", "shortCiteRegEx": "Salton and Buckley", "year": 1988}, {"title": "Machine learning in automated text categorization", "author": ["F. Sebastiani"], "venue": "ACM Computer Surveys", "citeRegEx": "Sebastiani,? \\Q2008\\E", "shortCiteRegEx": "Sebastiani", "year": 2008}, {"title": "Gplab-a genetic programming toolbox for matlab", "author": ["S. Silva", "J. Almeida"], "venue": "Proceedings of the Nordic MATLAB conference", "citeRegEx": "Silva and Almeida,? \\Q2003\\E", "shortCiteRegEx": "Silva and Almeida", "year": 2003}, {"title": "Video google: A text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "In: International Conference on Computer Vision", "citeRegEx": "Sivic and Zisserman,? \\Q2003\\E", "shortCiteRegEx": "Sivic and Zisserman", "year": 2003}, {"title": "GMM based language identification system using robust features", "author": ["S.Manchala", "V.K. Prasad", "V. Janaki"], "venue": "International Journal of Speech Technology", "citeRegEx": "S.Manchala et al\\.,? \\Q2014\\E", "shortCiteRegEx": "S.Manchala et al\\.", "year": 2014}, {"title": "A survey of modern authorship attribution methods", "author": ["E. Stamatatos"], "venue": "Journal of the American Society for Information Science and Technology", "citeRegEx": "Stamatatos,? \\Q2009\\E", "shortCiteRegEx": "Stamatatos", "year": 2009}, {"title": "Learning to rank", "author": ["A. Trotman"], "venue": "Information Retrieval", "citeRegEx": "Trotman,? \\Q2005\\E", "shortCiteRegEx": "Trotman", "year": 2005}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P. Turney", "P. Pantel"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "VLFeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": null, "citeRegEx": "Vedaldi and Fulkerson,? \\Q2008\\E", "shortCiteRegEx": "Vedaldi and Fulkerson", "year": 2008}, {"title": "Bag-ofwords representation for biomedical time series classification", "author": ["J. Wanga", "P. Liub", "M.F. Shea", "S. Nahavandia", "A. Kouzanid"], "venue": "Biomedical Signal Processing and Control", "citeRegEx": "Wanga et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wanga et al\\.", "year": 2013}, {"title": "A comparative study on feature selection in text categorization", "author": ["Y. Yang", "J.O. Pedersen"], "venue": "Proceedins of the 14th International Conference on Machine Learning", "citeRegEx": "Yang and Pedersen,? \\Q1997\\E", "shortCiteRegEx": "Yang and Pedersen", "year": 1997}, {"title": "Local features and kernels for classification of texture and object categories: A comprehensive study", "author": ["J. Zhang", "M. Marszablek", "S. Lazebnik", "C. Schmid"], "venue": "International Journal of Computer Vision", "citeRegEx": "Zhang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2007}, {"title": "Scaling up kernel svm on limited resources: A low-rank linearization approach", "author": ["K. Zhang", "L. Lan", "Z. Wang", "F. Moerchen"], "venue": "Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS)", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 25, "context": ", n\u00e4\u0131ve Bayes and support vector machines) are applied (Sebastiani, 2008).", "startOffset": 55, "endOffset": 73}, {"referenceID": 24, "context": "This type of representation is known as the vector space model (VSM) (Salton and Buckley, 1988).", "startOffset": 69, "endOffset": 95}, {"referenceID": 31, "context": "Under the VSM one assumes a document is a point in aN -dimensional space and documents that are closer in that space are similar to each other (Turney and Pantel, 2010).", "startOffset": 143, "endOffset": 168}, {"referenceID": 3, "context": "In fact, the BOW representation has been successfully adopted for processing other media besides text, including, images (Csurka et al., 2004), videos (Sivic and Zisserman, 2003), speech signals (S.", "startOffset": 121, "endOffset": 142}, {"referenceID": 27, "context": ", 2004), videos (Sivic and Zisserman, 2003), speech signals (S.", "startOffset": 16, "endOffset": 43}, {"referenceID": 28, "context": ", 2004), videos (Sivic and Zisserman, 2003), speech signals (S.Manchala et al., 2014), and time series (Wanga et al.", "startOffset": 60, "endOffset": 85}, {"referenceID": 33, "context": ", 2014), and time series (Wanga et al., 2013) among others.", "startOffset": 25, "endOffset": 45}, {"referenceID": 15, "context": "term-weighting scheme (TWS), which is in charge of determining how relevant a term is for describing the content of a document (Feldman and Sanger, 2006; Altyncay and Erenel, 2010; Lan et al., 2009; Debole and Sebastiani, 2003).", "startOffset": 127, "endOffset": 227}, {"referenceID": 1, "context": "term-weighting scheme (TWS), which is in charge of determining how relevant a term is for describing the content of a document (Feldman and Sanger, 2006; Altyncay and Erenel, 2010; Lan et al., 2009; Debole and Sebastiani, 2003).", "startOffset": 127, "endOffset": 227}, {"referenceID": 7, "context": "term-weighting scheme (TWS), which is in charge of determining how relevant a term is for describing the content of a document (Feldman and Sanger, 2006; Altyncay and Erenel, 2010; Lan et al., 2009; Debole and Sebastiani, 2003).", "startOffset": 127, "endOffset": 227}, {"referenceID": 25, "context": "Although, TC is a widely studied topic with very important developments in the last two decades (Sebastiani, 2008; Feldman and Sanger, 2006), it is somewhat surprising that little attention has been paid to the development of new TWSs to better represent the content of documents for TC.", "startOffset": 96, "endOffset": 140}, {"referenceID": 15, "context": "Although, TC is a widely studied topic with very important developments in the last two decades (Sebastiani, 2008; Feldman and Sanger, 2006), it is somewhat surprising that little attention has been paid to the development of new TWSs to better represent the content of documents for TC.", "startOffset": 96, "endOffset": 140}, {"referenceID": 16, "context": ", B, TF or TF-IDF ) and put more effort in other processes, like feature selection (Forman, 2003; Yang and Pedersen, 1997), or the learning process itself (Agarwal and Mittal, 2014; Aggarwal, 2012; Escalante et al.", "startOffset": 83, "endOffset": 122}, {"referenceID": 34, "context": ", B, TF or TF-IDF ) and put more effort in other processes, like feature selection (Forman, 2003; Yang and Pedersen, 1997), or the learning process itself (Agarwal and Mittal, 2014; Aggarwal, 2012; Escalante et al.", "startOffset": 83, "endOffset": 122}, {"referenceID": 0, "context": ", B, TF or TF-IDF ) and put more effort in other processes, like feature selection (Forman, 2003; Yang and Pedersen, 1997), or the learning process itself (Agarwal and Mittal, 2014; Aggarwal, 2012; Escalante et al., 2009).", "startOffset": 155, "endOffset": 221}, {"referenceID": 10, "context": ", B, TF or TF-IDF ) and put more effort in other processes, like feature selection (Forman, 2003; Yang and Pedersen, 1997), or the learning process itself (Agarwal and Mittal, 2014; Aggarwal, 2012; Escalante et al., 2009).", "startOffset": 155, "endOffset": 221}, {"referenceID": 25, "context": "The most studied TC problem is the so called thematic TC (or simply text categorization) (Sebastiani, 2008), which means that classes are associated to different themes or topics (e.", "startOffset": 89, "endOffset": 107}, {"referenceID": 29, "context": "Nonthematic TC includes the problems of authorship attribution (Stamatatos, 2009), opinion mining and sentiment analysis (Pang et al.", "startOffset": 63, "endOffset": 81}, {"referenceID": 19, "context": ", 2002), authorship verification (Koppel and Schler, 2004), author profiling (Koppel et al.", "startOffset": 33, "endOffset": 58}, {"referenceID": 18, "context": "among several others (Reyes and Rosso, 2014; Kiddon and Brun, 2011).", "startOffset": 21, "endOffset": 67}, {"referenceID": 29, "context": ", BOW using character n-grams or part-of-speech tags (Stamatatos, 2009)).", "startOffset": 53, "endOffset": 71}, {"referenceID": 3, "context": "Nowadays, images (Csurka et al., 2004), videos (Sivic and Zisserman, 2003), audio (S.", "startOffset": 17, "endOffset": 38}, {"referenceID": 27, "context": ", 2004), videos (Sivic and Zisserman, 2003), audio (S.", "startOffset": 16, "endOffset": 43}, {"referenceID": 28, "context": ", 2004), videos (Sivic and Zisserman, 2003), audio (S.Manchala et al., 2014), and other types of data (Wanga et al.", "startOffset": 51, "endOffset": 76}, {"referenceID": 33, "context": ", 2014), and other types of data (Wanga et al., 2013) are represented throughout analogies to the BOW.", "startOffset": 33, "endOffset": 53}, {"referenceID": 3, "context": "In image classification, for example, visual descriptors extracted from images are clustered and the centers of the clusters are considered as visual words (Csurka et al., 2004; Zhang et al., 2007).", "startOffset": 156, "endOffset": 197}, {"referenceID": 35, "context": "In image classification, for example, visual descriptors extracted from images are clustered and the centers of the clusters are considered as visual words (Csurka et al., 2004; Zhang et al., 2007).", "startOffset": 156, "endOffset": 197}, {"referenceID": 14, "context": "Accordingly, in this work we also perform experiments on learning TWSs for a standard computer vision problem (Fei-Fei et al., 2004).", "startOffset": 110, "endOffset": 132}, {"referenceID": 25, "context": "TC is a problem that has been approached mostly as a supervised learning task, where the goal is to learn a model capable of associating documents to categories (Sebastiani, 2008; Feldman and Sanger, 2006; Agarwal and Mittal, 2014).", "startOffset": 161, "endOffset": 231}, {"referenceID": 15, "context": "TC is a problem that has been approached mostly as a supervised learning task, where the goal is to learn a model capable of associating documents to categories (Sebastiani, 2008; Feldman and Sanger, 2006; Agarwal and Mittal, 2014).", "startOffset": 161, "endOffset": 231}, {"referenceID": 0, "context": "TC is a problem that has been approached mostly as a supervised learning task, where the goal is to learn a model capable of associating documents to categories (Sebastiani, 2008; Feldman and Sanger, 2006; Agarwal and Mittal, 2014).", "startOffset": 161, "endOffset": 231}, {"referenceID": 25, "context": "Many TWSs have been proposed so far, including unsupervised (Sebastiani, 2008; Salton and Buckley, 1988; Feldman and Sanger, 2006) and supervised schemes (Debole and Sebastiani, 2003; Lan et al.", "startOffset": 60, "endOffset": 130}, {"referenceID": 24, "context": "Many TWSs have been proposed so far, including unsupervised (Sebastiani, 2008; Salton and Buckley, 1988; Feldman and Sanger, 2006) and supervised schemes (Debole and Sebastiani, 2003; Lan et al.", "startOffset": 60, "endOffset": 130}, {"referenceID": 15, "context": "Many TWSs have been proposed so far, including unsupervised (Sebastiani, 2008; Salton and Buckley, 1988; Feldman and Sanger, 2006) and supervised schemes (Debole and Sebastiani, 2003; Lan et al.", "startOffset": 60, "endOffset": 130}, {"referenceID": 7, "context": "Many TWSs have been proposed so far, including unsupervised (Sebastiani, 2008; Salton and Buckley, 1988; Feldman and Sanger, 2006) and supervised schemes (Debole and Sebastiani, 2003; Lan et al., 2009), see Section 3.", "startOffset": 154, "endOffset": 201}, {"referenceID": 25, "context": "information retrieval tasks and latter adopted for TC (Sebastiani, 2008; Salton and Buckley, 1988).", "startOffset": 54, "endOffset": 98}, {"referenceID": 24, "context": "information retrieval tasks and latter adopted for TC (Sebastiani, 2008; Salton and Buckley, 1988).", "startOffset": 54, "endOffset": 98}, {"referenceID": 7, "context": "On the other hand, supervised TWSs aim at incorporating discriminative information into the representation of documents (Debole and Sebastiani, 2003).", "startOffset": 120, "endOffset": 149}, {"referenceID": 34, "context": "In this way, the discrimination power of each term is taken into account for the document representation; in this case through the information gain value (Yang and Pedersen, 1997).", "startOffset": 154, "endOffset": 179}, {"referenceID": 15, "context": ", B, TF and TF-IDF ) have been adopted for TC systems (Feldman and Sanger, 2006; Aggarwal, 2012).", "startOffset": 54, "endOffset": 96}, {"referenceID": 24, "context": "Their popularity derives from the fact that these schemes have proved to be very effective in information retrieval (Salton and Buckley, 1988; Baeza-Yates and Ribeiro-Neto, 1999; Turney and Pantel, 2010) and in many TC problems as well as (Sebastiani, 2008; Feldman and Sanger, 2006; Agarwal and Mittal, 2014; Aggarwal, 2012; Aggarwal and Zhai, 2012).", "startOffset": 116, "endOffset": 203}, {"referenceID": 31, "context": "Their popularity derives from the fact that these schemes have proved to be very effective in information retrieval (Salton and Buckley, 1988; Baeza-Yates and Ribeiro-Neto, 1999; Turney and Pantel, 2010) and in many TC problems as well as (Sebastiani, 2008; Feldman and Sanger, 2006; Agarwal and Mittal, 2014; Aggarwal, 2012; Aggarwal and Zhai, 2012).", "startOffset": 116, "endOffset": 203}, {"referenceID": 25, "context": "Their popularity derives from the fact that these schemes have proved to be very effective in information retrieval (Salton and Buckley, 1988; Baeza-Yates and Ribeiro-Neto, 1999; Turney and Pantel, 2010) and in many TC problems as well as (Sebastiani, 2008; Feldman and Sanger, 2006; Agarwal and Mittal, 2014; Aggarwal, 2012; Aggarwal and Zhai, 2012).", "startOffset": 239, "endOffset": 350}, {"referenceID": 15, "context": "Their popularity derives from the fact that these schemes have proved to be very effective in information retrieval (Salton and Buckley, 1988; Baeza-Yates and Ribeiro-Neto, 1999; Turney and Pantel, 2010) and in many TC problems as well as (Sebastiani, 2008; Feldman and Sanger, 2006; Agarwal and Mittal, 2014; Aggarwal, 2012; Aggarwal and Zhai, 2012).", "startOffset": 239, "endOffset": 350}, {"referenceID": 0, "context": "Their popularity derives from the fact that these schemes have proved to be very effective in information retrieval (Salton and Buckley, 1988; Baeza-Yates and Ribeiro-Neto, 1999; Turney and Pantel, 2010) and in many TC problems as well as (Sebastiani, 2008; Feldman and Sanger, 2006; Agarwal and Mittal, 2014; Aggarwal, 2012; Aggarwal and Zhai, 2012).", "startOffset": 239, "endOffset": 350}, {"referenceID": 7, "context": "This observation was noticed by Debole & Sebastiani and other authors that have introduced supervised TWSs (Debole and Sebastiani, 2003; Lan et al., 2009).", "startOffset": 107, "endOffset": 154}, {"referenceID": 7, "context": "In (Debole and Sebastiani, 2003) TWSs were defined by combining the unsupervised TF scheme with the following term-relevance criteria: information gain (TF-IG), which measures the reduction of entropy when using a term as classifier (Yang and Pedersen, 1997); \u03c7 (TF-CHI ), makes an independence test regarding a term and the classes (Sebastiani, 2008); and gain-ratio (TF-GR) measuring the gain-ratio when using the term as classifier (Debole and Sebastiani, 2003).", "startOffset": 3, "endOffset": 32}, {"referenceID": 34, "context": "In (Debole and Sebastiani, 2003) TWSs were defined by combining the unsupervised TF scheme with the following term-relevance criteria: information gain (TF-IG), which measures the reduction of entropy when using a term as classifier (Yang and Pedersen, 1997); \u03c7 (TF-CHI ), makes an independence test regarding a term and the classes (Sebastiani, 2008); and gain-ratio (TF-GR) measuring the gain-ratio when using the term as classifier (Debole and Sebastiani, 2003).", "startOffset": 233, "endOffset": 258}, {"referenceID": 25, "context": "In (Debole and Sebastiani, 2003) TWSs were defined by combining the unsupervised TF scheme with the following term-relevance criteria: information gain (TF-IG), which measures the reduction of entropy when using a term as classifier (Yang and Pedersen, 1997); \u03c7 (TF-CHI ), makes an independence test regarding a term and the classes (Sebastiani, 2008); and gain-ratio (TF-GR) measuring the gain-ratio when using the term as classifier (Debole and Sebastiani, 2003).", "startOffset": 333, "endOffset": 351}, {"referenceID": 7, "context": "In (Debole and Sebastiani, 2003) TWSs were defined by combining the unsupervised TF scheme with the following term-relevance criteria: information gain (TF-IG), which measures the reduction of entropy when using a term as classifier (Yang and Pedersen, 1997); \u03c7 (TF-CHI ), makes an independence test regarding a term and the classes (Sebastiani, 2008); and gain-ratio (TF-GR) measuring the gain-ratio when using the term as classifier (Debole and Sebastiani, 2003).", "startOffset": 435, "endOffset": 464}, {"referenceID": 7, "context": "The conclusions from (Debole and Sebastiani, 2003) were that small improvements can be obtained with supervised TWSs over unsupervised ones.", "startOffset": 21, "endOffset": 50}, {"referenceID": 7, "context": ", 2009) the proposed TF-RF scheme obtained better performance than unsupervised TWSs and even outperformed the schemes proposed in (Debole and Sebastiani, 2003).", "startOffset": 131, "endOffset": 160}, {"referenceID": 1, "context": "In (Altyncay and Erenel, 2010) the RF term-relevance factor was compared with alternative weights, including mutual information, odds ratio and \u03c7; in that workRF outperformed the other term-importance criteria.", "startOffset": 3, "endOffset": 30}, {"referenceID": 24, "context": "B Boolean xi,j = {#(ti,dj)>0} Indicates the prescense/abscense of terms (Salton and Buckley, 1988)", "startOffset": 72, "endOffset": 98}, {"referenceID": 24, "context": "TF TermFrequency xi,j = #(ti, dj) Accounts for the frequency of occurrence of terms (Salton and Buckley, 1988)", "startOffset": 84, "endOffset": 110}, {"referenceID": 24, "context": "IDF TF - Inverse Document Frequency xi,j = #(ti, dj) \u00d7 log( N df(tj ) ) An TF scheme that penalizes the frequency of terms across the collection (Salton and Buckley, 1988)", "startOffset": 145, "endOffset": 171}, {"referenceID": 7, "context": "(Debole and Sebastiani, 2003)", "startOffset": 0, "endOffset": 29}, {"referenceID": 7, "context": "CHI TF - Chisquare xi,j = #(ti, dj) \u00d7 CHI(tj) TF scheme that weights term occurrence by its \u03c7 statistic (Debole and Sebastiani, 2003)", "startOffset": 104, "endOffset": 133}, {"referenceID": 1, "context": "Comprehensive and extensive comparative studies using supervised TRs for feature selection have been reported (Altyncay and Erenel, 2010; Forman, 2003; Yang and Pedersen, 1997; Mladenic and Grobelnik, 1999).", "startOffset": 110, "endOffset": 206}, {"referenceID": 16, "context": "Comprehensive and extensive comparative studies using supervised TRs for feature selection have been reported (Altyncay and Erenel, 2010; Forman, 2003; Yang and Pedersen, 1997; Mladenic and Grobelnik, 1999).", "startOffset": 110, "endOffset": 206}, {"referenceID": 34, "context": "Comprehensive and extensive comparative studies using supervised TRs for feature selection have been reported (Altyncay and Erenel, 2010; Forman, 2003; Yang and Pedersen, 1997; Mladenic and Grobelnik, 1999).", "startOffset": 110, "endOffset": 206}, {"referenceID": 22, "context": "Comprehensive and extensive comparative studies using supervised TRs for feature selection have been reported (Altyncay and Erenel, 2010; Forman, 2003; Yang and Pedersen, 1997; Mladenic and Grobelnik, 1999).", "startOffset": 110, "endOffset": 206}, {"referenceID": 30, "context": "Similar approaches based on genetic programming to learn TWSs have been proposed in (Cummins and O\u2019Riordan, 2006, 2007, 2005; Trotman, 2005; Oren, 2002; Fan et al., 2004a), however, these researchers have focused on the information retrieval problem, which differs significantly from TC.", "startOffset": 84, "endOffset": 171}, {"referenceID": 9, "context": "For initialization we used the standard ramped-half-and-half strategy (Eiben and Smith, 2010), which generates half of the population with (balanced) trees of maximum depth, and the other half with trees of variable depth.", "startOffset": 70, "endOffset": 93}, {"referenceID": 25, "context": "The average across classes is reported (also called, macro-average f1), this way of estimating the f1-measure is known to be particularly useful when tackling unbalanced data sets (Sebastiani, 2008).", "startOffset": 180, "endOffset": 198}, {"referenceID": 25, "context": "Support vector machines (SVM) comprise a type of models that have proved to be very effective for TC (Sebastiani, 2008; Joachims, 2008).", "startOffset": 101, "endOffset": 135}, {"referenceID": 17, "context": "Support vector machines (SVM) comprise a type of models that have proved to be very effective for TC (Sebastiani, 2008; Joachims, 2008).", "startOffset": 101, "endOffset": 135}, {"referenceID": 36, "context": "Therefore, we opted for efficient implementations of SVMs that have been proposed recently (Zhang et al., 2012; Djuric et al., 2013).", "startOffset": 91, "endOffset": 132}, {"referenceID": 8, "context": "Therefore, we opted for efficient implementations of SVMs that have been proposed recently (Zhang et al., 2012; Djuric et al., 2013).", "startOffset": 91, "endOffset": 132}, {"referenceID": 8, "context": "Among the methods available in (Djuric et al., 2013) we used the low-rank linearized SVM (LLSMV) (Zhang et al.", "startOffset": 31, "endOffset": 52}, {"referenceID": 36, "context": ", 2013) we used the low-rank linearized SVM (LLSMV) (Zhang et al., 2012).", "startOffset": 52, "endOffset": 72}, {"referenceID": 2, "context": "LLSVM is a linearized version of non-linear SVMs, which can be trained efficiently with the so called block minimization framework (Chang and Roth, 2011).", "startOffset": 131, "endOffset": 153}, {"referenceID": 8, "context": ", (Djuric et al., 2013; Zhang et al., 2012).", "startOffset": 2, "endOffset": 43}, {"referenceID": 36, "context": ", (Djuric et al., 2013; Zhang et al., 2012).", "startOffset": 2, "endOffset": 43}, {"referenceID": 26, "context": "We used the GPLAB toolbox for implementing the genetic program with default parameters (Silva and Almeida, 2003).", "startOffset": 87, "endOffset": 112}, {"referenceID": 25, "context": "The considered data sets are the most used ones for the evaluation of TC systems (Sebastiani, 2008).", "startOffset": 81, "endOffset": 99}, {"referenceID": 29, "context": "writing style of authors, hence, it has been shown that different representations and attributes are necessary for facing this task (Stamatatos, 2009).", "startOffset": 132, "endOffset": 150}, {"referenceID": 29, "context": "Accordingly, indexing terms in AA data sets were 3-grams of characters, that is, sequences of 3-characters found in documents, these terms have proved to be the most effective ones in AA (Stamatatos, 2009; Escalante et al., 2011; Luyckx and Daelemans, 2010).", "startOffset": 187, "endOffset": 257}, {"referenceID": 11, "context": "Accordingly, indexing terms in AA data sets were 3-grams of characters, that is, sequences of 3-characters found in documents, these terms have proved to be the most effective ones in AA (Stamatatos, 2009; Escalante et al., 2011; Luyckx and Daelemans, 2010).", "startOffset": 187, "endOffset": 257}, {"referenceID": 21, "context": "Accordingly, indexing terms in AA data sets were 3-grams of characters, that is, sequences of 3-characters found in documents, these terms have proved to be the most effective ones in AA (Stamatatos, 2009; Escalante et al., 2011; Luyckx and Daelemans, 2010).", "startOffset": 187, "endOffset": 257}, {"referenceID": 14, "context": "We considered the collection under the standard experimental settings (15 images per class for training and 15 images for testing), two subsets of the CALTECH-101 data set were used: a small one with only 5 categories and the whole data set with 102 classes (101 object categories plus background) (Fei-Fei et al., 2004).", "startOffset": 298, "endOffset": 320}, {"referenceID": 32, "context": "Images were represented under the Bag-of-Visual-Words formulation using dense sift descriptors (PHOW features): descriptors extracted from images were clustered using k\u2212means, the centers of the clusters are the visual words (indexing terms), images are then represented by accounting the occurrence of visual words, the VLFEAT toolbox was used for processing images (Vedaldi and Fulkerson, 2008).", "startOffset": 367, "endOffset": 396}, {"referenceID": 29, "context": "AA studies suggest using a small amount of the most-frequent terms when approaching an AA problem (Stamatatos, 2009; Escalante et al., 2011; Luyckx and Daelemans, 2010).", "startOffset": 98, "endOffset": 168}, {"referenceID": 11, "context": "AA studies suggest using a small amount of the most-frequent terms when approaching an AA problem (Stamatatos, 2009; Escalante et al., 2011; Luyckx and Daelemans, 2010).", "startOffset": 98, "endOffset": 168}, {"referenceID": 21, "context": "AA studies suggest using a small amount of the most-frequent terms when approaching an AA problem (Stamatatos, 2009; Escalante et al., 2011; Luyckx and Daelemans, 2010).", "startOffset": 98, "endOffset": 168}], "year": 2014, "abstractText": "This paper describes a novel approach to learning term-weighting schemes (TWSs) in the context of text classification. In text mining a TWS determines the way in which documents will be represented in a vector space model, before applying a classifier. Whereas acceptable performance has been obtained with standard TWSs (e.g., Boolean and term-frequency schemes), the definition of TWSs has been traditionally an art. Further, it is still a difficult task to determine what is the best TWS for a particular problem and it is not clear yet, whether better schemes, than those currently available, can be generated by combining known TWS. We propose in this article a genetic program that aims at learning effective TWSs that can improve the performance of current schemes in text classification. The genetic program learns how to combine a set of basic units to give rise to discriminative TWSs. We report an extensive experimental study comprising data sets from thematic and non-thematic text classification as well as from image classification. Our study shows the validity of the proposed method; in fact, we show that TWSs learned with the genetic program outperform traditional schemes and other Corresponding author. Email addresses: hugojair@inaoep.mx (Hugo Jair Escalante), mauricio.garcia.cs@gmail.com (Mauricio A. Gar\u0107\u0131a-Lim\u00f3n), a.morales@inaoep.mx (Alicia Morales-Reyes), mgraffg@gmail.com (Mario Graff), mmontesg@inaoep.mx (Manuel Montes-y-G\u00f3mez), emorales@inaoep.mx (Eduardo F. Morales) Preprint submitted to Elsevier October 8, 2014 TWSs proposed in recent works. Further, we show that TWSs learned from a specific domain can be effectively used for other tasks.", "creator": "LaTeX with hyperref package"}}}