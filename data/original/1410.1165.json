{"id": "1410.1165", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2014", "title": "Understanding Locally Competitive Networks", "abstract": "Recently proposed neural network activation functions such as rectified linear, maxout, and local winner-take-all have allowed for faster and more effective training of deep neural architectures on large and complex datasets. The common trait among these functions is that they implement local competition between small groups of units within a layer, so that only part of the network is activated for any given input pattern. In this paper, we attempt to visualize and understand this self-modularization, and suggest a unified explanation for the beneficial properties of such networks. We also show how our insights can be directly useful for efficiently performing retrieval over large datasets using neural networks.", "histories": [["v1", "Sun, 5 Oct 2014 14:46:47 GMT  (5697kb,D)", "http://arxiv.org/abs/1410.1165v1", "11 pages, 8 figures"], ["v2", "Mon, 22 Dec 2014 20:07:17 GMT  (5796kb,D)", "http://arxiv.org/abs/1410.1165v2", "9 pages + 2 supplementary, Under review as a conference paper at ICLR 2015"], ["v3", "Thu, 9 Apr 2015 01:22:49 GMT  (5725kb,D)", "http://arxiv.org/abs/1410.1165v3", "9 pages + 2 supplementary, Accepted to ICLR 2015 Conference track"]], "COMMENTS": "11 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["rupesh kumar srivastava", "jonathan masci", "faustino gomez", "j\\\"urgen schmidhuber"], "accepted": true, "id": "1410.1165"}, "pdf": {"name": "1410.1165.pdf", "metadata": {"source": "CRF", "title": "Understanding Locally Competitive Networks", "authors": ["Rupesh Kumar Srivastava", "Jonathan Masci", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "emails": ["juergen}@idsia.ch"], "sections": [{"heading": null, "text": "A version of this paper was submitted to NIPS 2014 on 06-06-2014"}, {"heading": "1 Introduction", "text": "Recently proposed activation functions for neural networks such as rectified linear (ReL;[1]), maxout [2] and LWTA [3] are quite unlike sigmoidal activation functions. These functions depart from the conventional wisdom in that they are not continuously differentiable (and sometimes non-continuous) and are piecewise linear. Nevertheless, many researchers have found that such networks can be trained faster and better than sigmoidal networks, and they are increasingly in use for learning from large and complex datasets [4, 5]. Past research has shown observational evidence that such networks have beneficial properties such as not requiring unsupervised training for weight initialization [1], better gradient flow [2] and mitigation of catastrophic forgetting [3, 6]. Recently, the expressive power of deep networks with such functions has been theoretically analyzed [7]. However, we are far from a complete understanding of their behavior and advantages over sigmoidal networks, especially during learning. This paper sheds additional light on the properties of such networks by interpreting them as models of models. A common theme among the ReL, maxout and LWTA activation functions is that they are locally competitive. Maxout and LWTA utilize explicit competition between units in small groups within a layer, while in the case of the rectified linear function, the weighted input sum competes with a fixed value of 0. Related activation techniques have been studied in the past decades, including recurrent networks with locally competitive units [8]. Selfdelimiting recurrent networks with competitive units [3, 9] can in principle learn to decide their own run time and effective number of parameters, thus learning their own computable regularizers. In this paper, we restrict our analysis to networks trained with gradient-based algorithms which are often trained with the dropout regularization technique [10, 11] for improved generalization. We start from the observation that in locally competitive networks, a subnetwork of units has non-zero activations for each input pattern. Instead of treating a neural network as a complicated highly nonlinear function approximator, the expressive power of the network can be interpreted to be coming from its ability to activate different subsets of linear units\nar X\niv :1\n41 0.\n11 65\nv1 [\ncs .N\nE ]\n5 O\nct 2\nfor different patterns. We hypothesize that the network acts as a model that can switch between \u201csubmodels\u201d (subnetworks) such that similar submodels respond to similar patterns, and the relative ease of this switching behavior for locally competitive networks makes them easier to train. As evidence of this behavior, we analyze the activated subnetworks for a large subset of a dataset (which is not used for training) and show that the subnetworks activated for different input patterns exhibit a structure consistent with our hypothesis. These observations provide a unified explanation for improved credit assignment in locally competitive networks during training, which is believed to be the main reason for their success. Our new point of view suggests a link between these networks and competitive learning approaches of the past decades. We also show that a simple encoding of which units in a layer are activated for a given example (its subnetwork) can be used to represent the example for retrieval tasks. Experiments on MNIST, CIFAR-10, CIFAR-100 and the ImageNet dataset show that promising results are obtained for datasets of varying size and complexity."}, {"heading": "2 Locally Competitive Neural Networks", "text": "Neural networks with activation functions like rectified linear, maxout and LWTA are locally competitive. This means that local competition among units in the network decides which parts of it get activated or trained for a particular input example. For each unit, the total input or presynaptic activation z is first computed as z = wx + b, where x is the vector of inputs to the unit, w is a trainable weight vector, and b is a trainable bias. For the rectified linear function, the output or postsynaptic activation of each unit is simply max(z, 0), which can be interpreted as competition with a fixed value of 0. For LWTA, the units in a layer are considered to be divided into blocks of a fixed size. Then the output of each unit is Iz where I is an indicator which is 1 if the unit has the maximum z in its group and 0 otherwise. In maxout, the inputs from a few units compete using a max operation, and the block output is the maximum z among the units1. A maxout block can also be interpreted as an LWTA block with shared outgoing weights among the units. A comparison of the 3 activation functions is shown in Figure 1. In each of the three cases, there is a local gating mechanism which allows non-zero activations (and errors during training) to propagate only through part of the network, i.e. a subnetwork. Consider the activation of a neural network with ReLUs in a single hidden layer. For each\n1In our terminology, the terms unit and block correspond to the terms filter and units in [2].\ninput pattern, the subset of units with non-zero activations in the hidden layer form a subnetwork, and an examination of the subnetworks activated for several examples shows that a large number of different subnetworks are activated (Figure 2). The result of training the network can interpreted in the following way: when training a single network with a local gating mechanism, a large number of linear subnetworks with shared parameters are trained on the dataset such that different examples are gated to different subnetworks, each getting trained to produce the desired output. At test time, the system generalizes in the sense that the appropriate subnetwork for a given example is activated. Note that this model of models interpretation is different from the model averaging perspective of dropout [11] which posits that using dropout is equivalent to averaging the predictions of many subnetworks on the same data. Our interpretation is related to mixtures of local experts [12], where a gating network was trained to select one of many subnetworks or modules in a system. We come back to this relationship in Section 5."}, {"heading": "3 Subnetwork Analysis", "text": "This section investigates how the model of models that is implemented though local competition self-organizes due to training. In order to visualize the organization of subnetworks as a result of training, they are encoded as bit strings called submasks. For the input pattern i, the submask si \u2208 {0, 1}u, where u is the number of units in the full network, represents the corresponding subnetwork by having a 0 in position j, j = 1..u, if the corresponding unit has zero activation, and 1 otherwise. The submasks uniquely and compactly encode each subnetwork in a format that is amenable to analysis through clustering, and, as we show in Section 4.2, facilitates efficient data retrieval. In what follows, the subnetworks that emerge during training are first visualized using the t-SNE [13] algorithm. This dimensionality reduction technique enables a good visualization of the relationship between submasks for several examples in a dataset by preserving the local structure. Later in this section, we examine the evolution of subnetworks during training, and show that the submasks obtained from a trained network can directly be used for classification using a simple nearest neighbors approach. All experiments in this section are performed on the MNIST dataset [14]. This familiar dataset was chosen because it is relatively easy, and therefore provides a tractable setting in which to verify the repeatability of our results. Larger, more interesting datasets are used in section 4 to demonstrate the utility of techniques developed in this section for classification and retrieval."}, {"heading": "3.1 Visualization through Dimensionality Reduction", "text": "For visualizing the relationship between submasks for a large number of input patterns, we trained multiple networks with different activation functions on the MNIST training set, stopping when the error on a validation set did not improve. The submasks for the entire test set (10k examples) were then extracted and visualized using t-SNE. Since the competition between subnetworks is local and not global, subsets of units in deeper (closer to the output) layers are activated based on information extracted in the shallow layers. Therefore, like unit activations, submasks from deeper layers are expected to be better related to the task since deeper layers code for higher level abstractions. For this reason, we use only submasks extracted from the penultimate network layers in this paper, which considerably reduces the size of submasks to consider. Figure 3b shows a 2D visualization of the submasks from a 3 hidden layer ReL network. Each submask is a bitstring of length 1k (the size of the network\u2019s penultimate layer). Ten distinct clusters are present corresponding to the ten MNIST classes. It is remarkable that, irrespective of the actual activation values, the subnetworks which are active for the testing examples can be used to visually predict class memberships based on their similarity to each other. The visualization confirms that the subnetworks active for examples of the same class are much more similar to each other compared to the ones activated for the examples of different classes. Visualization of submasks from the same layer of a randomly initialized network does not show any structure (Figure 3a), but we observed some structure for the untrained first hidden layer (Appendix A). For trained networks, similar clustering is observed in the submasks from shallow layers in the network, though the clusters appear to be less separated and tight. The visualization also shows many instances where the network makes mistakes. The submasks for some examples lie in the cluster of submasks for the wrong class, indicating that the \u2018wrong\u2019 subnetwork was selected for these examples. The experiments in the next sections show that the organization of subnetworks is indicative of the classification performance of the full network. Other locally competitive activation functions such as LWTA and maxout result in similar clustering of submasks (visualizations included in Appendix A). For LWTA layers, the submasks can be directly constructed from the activations because there is no subsampling when going from presynaptic to postsynaptic activations, and it is reasonable to expect a subnetwork organization similar to that of ReL layers. Indeed, in a limited qualitative analysis, it has been shown previously [3] that in trained LWTA nets there are more units in common between subnetworks for examples of the same class than those for different class examples. For maxout layers, the situation is trickier at a first glance. The unit activations get pooled before being propagated to the next layer, so it is possible that the maximum activation value plays a much more important role than the identity of the winning units. However, using the same basic principle of credit assignment to subnetworks, we can construct submasks from maxout layers by binarizing the unit activations such that only the units producing the maximum activation are represented by a 1. Separation of subnetworks is necessary to gain the advantages of local competition during learning, and the visualization of the generated submasks produces results similar to those for ReLU and LWTA (included in Appendix A)."}, {"heading": "3.2 Behavior during Training", "text": "In order to measure how the subnetworks evolve over the course of training, the submasks of each sample in the training set were recorded at each epoch. Figure 4 characterizes the change in the subnets over time by counting the number of input patterns for which a unit flips from being on to being off, or vice-versa, from one epoch to the next. The curve in the figure shows the fraction of patterns for which an inter-epoch flip occurred, averaged across all units in the network. Higher values indicate that the assignment of subnets to patterns is not stable. The batch size for this experiment was 100, which means that each pass over the training set consists of 500 weight updates. For the run shown, the average fraction of flips starts at 0.2, but falls quickly below 0.05 and keeps falling as training continues, indicating that during training, the assignment of subnetworks to individual examples stabilizes quickly\nin this case. After a brief (\u223c3 epochs) transient period, a fine-tuning period follows where the selected subnetworks keep getting trained on their corresponding training patterns."}, {"heading": "3.3 Classification/Retrieval using Submasks", "text": "Since the visualization of submasks for the test set shows task-relevant structure, it is natural to ask: how well can the submask represent the data that produced it? If the submasks for similar examples are similar, perhaps they can be used as data descriptors for tasks such as similarity-based retrieval. Sparse binary codes enable efficient storage and retrieval for large and complex datasets due to which learning to produce them is an active research area [15, 16, 17]. This would make representative submasks very attractive since no explicit training for retrieval would be required to generate them. To evaluate if examples producing similar binary codes are indeed similar, we train locally competitive networks for classification and use a simple k nearest neighbors (kNN) algorithm for classifying data using the generated submasks. This approach is a simple way to examine the amount of information contained in the submasks (without utilizing the actual activation values). Further experiments using submasks in a more conventional retrieval setting are currently underway. We trained networks with fully connected layers on the MNIST training set, and selected the value of k with the lowest validation error to perform classification on the test set. Results are shown in Table 1. In each case, the kNN classification results are close to the classification result obtained using the network\u2019s softmax layer. For comparison, using the (non-pooled) unit activations from the maxout network instead of submasks for kNN classification results in 121 errors. Submasks can also be obtained from convolutional layers. Using a convolutional maxout network, we obtained 52 errors on the MNIST test set when we reproduced the model from [2]. Since the penultimate layer in this model is convolutional, the submasks were constructed using the presynaptic unit activations from this layer for all convolutional maps. Visualization of these submasks showed similar structure to that obtained from fully connected layers, kNN classification on the submasks resulted in 65 errors. As seen before, for a well-trained network the kNN performance is close to the performance of the network\u2019s softmax layer."}, {"heading": "3.4 Effect of Dropout", "text": "The dropout [10] regularization technique has proven to be very useful and efficient at improving generalization for large models, and is often used in combination with locally competitive activation functions [2, 4, 5]. We found that networks which were trained with dropout (and thus produced lower test set error) also yielded better submasks in terms of kNN classification performance. To observe the effect of dropout in more detail, we\ntrained a 3 hidden layer network with 800 ReLUs in each hidden layer without dropout on MNIST starting from 5 different initializations until the validation set error did not improve. The networks were then trained again from the same initialization with dropout until the validation error matched or fell below the lowest validation error from the non-dropout case. In both cases, minibatch gradient descent with momentum was used for training the networks. A comparison of kNN classification error for the dropout and non-dropout cases shows that when the validation errors are similar, the organization of subnetworks is not significantly better or worse. If dropout training is stopped at a point when validation error is similar to a no-dropout network, the submasks from both cases give similar results, but as dropout improves generalization (lowers validation set error), the organization of subnetworks also improves. This supports the interpretation of dropout as a regularization technique which prevents \u201cco-adaptation of feature detectors\u201d (units) [10], leading to better representation of data by the subnetworks. Dropout improves generalization by injecting noise in the organization of subnetworks, making them more robust. Due to this effect, the remaining results and visualizations in this paper are all derived from networks trained with dropout."}, {"heading": "4 Experimental Results", "text": "The following experiments apply the methods described in the previous section to more challenging benchmark problems: CIFAR-10, CIFAR-100, and ImageNet. For the CIFAR experiments, we used the models described in [2] since they use locally competitive activations (maxout), are trained with dropout, and good hyperparameter settings for them are available [18]. We report the classification error on the test set obtained using the softmax output layer, as well kNN classification on the penultimate layer unit activations and submasks. The best value of k is obtained using a validation set, though we found that k = 5 with distance weighting usually worked well."}, {"heading": "4.1 CIFAR-10 & CIFAR-100", "text": "CIFAR-10 is a dataset of 32\u00d732 color images of 10 classes split into a training set of size 50k and testing set of size 10k (6k images per class) [19]. CIFAR-100 is a similar dataset of color images but with 100 classes and 600 images per class, making it more challenging. The results obtained on these datasets are summarized in Table 2. The models from [2] for these dataset utilize preprocessing using global contrast normalization and ZCA whitening as well as data augmentation using translational and horizontal reflections. We find that when comparing nearest neighbor classification performance with submasks to unit activation values, we lose an accuracy of 1.4% on the CIFAR-10 dataset, and 1.75% on the CIFAR-100 dataset. This indicates a good extent of subnetwork organization according to different classes. Figure 5a shows the 2-D visualization of the test set submasks for CIFAR-10. Some classes can be seen to have highly representative submasks, while confusion between classes in the lower half is observed. The clusters of subnetworks are not as wellseparated as in the case of MNIST, reflecting the relatively worse classification performance. Submask visualization for CIFAR-100 (Figure 5b) reflects the high error rate in this dataset. Although any visualization with 100 classes can be hard to interpret, many small clusters of submasks can still be observed."}, {"heading": "4.2 ImageNet", "text": "The results of kNN classification and t-SNE visualization using submasks on small datasets of varying complexities show that the submasks contain substantial information about the data relevant to the task. In this section, the utility of the submasks obtained for a large\nconvolutional network trained on the ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC-2012) [20] dataset is evaluated. ILSVRC-2012 is a dataset of over a million natural images split into 1000 classes. An implementation of the network in [4], with some differences [21], is available publicly. For the experiments in this section, the penultimate-layer activations obtained using this model were downloaded from CloudCV [22]. The activations were obtained using the center-only option, meaning that only the activations for the central, 224\u00d7224 crop of each image were used. Each image in the training and validation set can thus be represented using a 4096 dimensional submask, which is much more efficient for storage and retrieval than a floatingpoint vector of activations. Using these submasks, the top-1 and top-5 classification errors obtained using kNN classification for the validation set are 56.7% and 29.2%. For each validation set example, 100 examples from the training set with the closest submasks were weighted by the inverse of the distance, then the classes with top-1 or top-5 weighted sums were returned as predictions. The results obtained using the network\u2019s softmax outputs2 are 42.9% and 19.2%. We only report errors on the validation set since the true labels for the test set are not public. Our ongoing experiments compare the utility of these submasks to other recently proposed algorithms which are designed to obtain binary data descriptors. The classification results show that the performance on this hard classification task is very good, considering the gain in efficiency obtained from binarization of the activations. For instance, submasks for the full ILSVRC-2012 training set can be stored in about 0.5 GB. Moreover, our experiments indicate that submasks obtained from a better trained network will result in even better performance, since quality of submasks improves as network training continues. In [4], it was first shown that the activations from the penultimate layer of the deep network can be used to retrieve similar images. Since retrieval using real-valued vectors is inefficient, it was suggested that the activations can be compressed to binary codes using auto-encoders. However, the submasks can be directly utilized for quick and efficient retrieval of data based on high level similarity. Sample retrieval results for examples from the ILSVRC-2012 dataset are shown in Figure 6, where the first image in each row is a query image from the validation set and the rest are images from the training set with the most similar submasks to the query image. The returned images are often very relevant to the query images, suggesting a new recipe for retrieval tasks in general: one can train a\n2The network\u2019s error is reported for classification using 5 crops for each image and their horizontal reflections, not with the center-only option which we used. This improves the network\u2019s error by about 1-2% [4].\nneural network with locally competitive units on a dataset for a high level task, extract the submasks from the penultimate layers, and directly utilize them for retrieval."}, {"heading": "5 Discussion", "text": "Training a system of many networks on a dataset such that they specialize to solve simpler tasks can be quite difficult without combining them into a single network with locally competitive units. Without such local competition, one needs to have a global gating mechanism as in [12]. The training algorithm and the objective function also need modifications such that competition between networks is encouraged. On the other hand, a locally competitive neural network can behave like a model composed of many subnetworks, and massive sharing of parameters between subnetworks enables better training. Stochastic gradient descent can be used to minimize the desired loss function, and the implementation is so simple that one does not even realize that a model of models is being trained. Figure 4 suggests that during optimization, the subnetworks get organized during an early transient phase such that subnetworks responding to similar examples have more parameters in common than those responding to dissimilar examples. This allows for better training of subnetworks due to reduced interference from dissimilar examples and shared parameters for similar examples. In the later fine-tuning phase, the parameters of subnetworks get adjusted to improve classification and much less re-assignment of subnetworks is needed. In this way, the gating mechanism induced by locally competitive activation functions accomplishes the purpose of global competition efficiently and no modifications to the error function are required. Due to above advantages, networks with such activation functions can usually be trained faster and better compared to networks with sigmoidal or similar activation functions for complex pattern recognition tasks. The nature of organization of subnetworks is reminiscent\nof the data manifold hypothesis for classification [23]. Just like data points of different classes are expected to concentrate along sub-manifolds, we expect that the organization of subnetworks that respond to the data points reflects the data manifold being modeled. An important take-away from these results is the unifying theme between locally competitive architectures, and its relation to past work on competitive learning. Insights from past literature on this topic can be utilized to develop improved learning algorithms and architectures for locally competitive learning. To the best of our knowledge, this paper is the first to show that simply training a deep network for classification results in binary descriptors that are useful for retrieval. These descriptors are not just results of a thresholding trick or unique to a particular activation function, but arise as a direct result of the way the network learns and processes information. Our experiments on datasets of increasing complexity show that when the network performance (softmax classification) improves, the performance gap to submask-based retrieval closes. This suggests that in the near future, as training techniques continue to advance and yield lower errors on larger datasets, submasks will perform as well as activation values for retrieval and transfer learning tasks. Importantly, these binary representation will always be far more efficient for storage and retrieval than continuous activation vectors."}, {"heading": "Acknowledgments", "text": "This research was funded by EU projects WAY (FP7-ICT-288551) and NASCENCE (FP7ICT-317662). We thank Jan Koutn\u00edk and Klaus Greff for their helpful comments."}, {"heading": "A Extra visualizations", "text": ""}], "references": [{"title": "Deep sparse rectifier networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Compete to compute", "author": ["Rupesh K. Srivastava", "Jonathan Masci", "Sohrob Kazerounian", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "On rectified linear units for speech processing", "author": ["Matthew D. Zeiler", "M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J. Dean"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "An empirical investigation of catastrophic forgetting in gradient-based neural networks", "author": ["Ian J. Goodfellow", "Mehdi Mirza", "Xiao Da", "Aaron Courville", "Yoshua Bengio"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "On the number of response regions of deep feed forward networks with piece-wise linear activations", "author": ["Razvan Pascanu", "Guido Montufar", "Yoshua Bengio"], "venue": "[cs],", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "A local learning algorithm for dynamic feedforward and recurrent networks", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Connection Science,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "Self-delimiting neural networks", "author": ["J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1210.0118,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov"], "venue": "[cs],", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "The dropout learning algorithm", "author": ["Pierre Baldi", "Peter Sadowski"], "venue": "Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Adaptive mixtures of local experts", "author": ["Robert A. Jacobs", "Michael I. Jordan", "Steven J. Nowlan", "Geoffrey E. Hinton"], "venue": "Neural computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1991}, {"title": "Visualizing data using t-SNE", "author": ["Laurens Van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Learning binary codes for high-dimensional data using bilinear projections", "author": ["Yunchao Gong", "Sanjiv Kumar", "Henry A. Rowley", "Svetlana Lazebnik"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Sparse similarity-preserving hashing", "author": ["Jonathan Masci", "Alex M. Bronstein", "Michael M. Bronstein", "Pablo Sprechmann", "Guillermo Sapiro"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Learning binary hash codes for large-scale image search. In Machine Learning for Computer Vision, page 49\u201387", "author": ["Kristen Grauman", "Rob Fergus"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Pylearn2: a machine learning research library", "author": ["Ian J. Goodfellow", "David Warde-Farley", "Pascal Lamblin", "Vincent Dumoulin", "Mehdi Mirza", "Razvan Pascanu", "James Bergstra", "Fr\u00e9d\u00e9ric Bastien", "Yoshua Bengio"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Computer Science Department,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "ImageNet large scale visual recognition competition", "author": ["Jia Deng", "Alex Berg", "Sanjeev Satheesh", "Su Hao", "Aditya Khosla", "Fei-Fei Li"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "DeCAF: a deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": "[cs],", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "CloudCV: Large-Scale distributed computer vision as a cloud service", "author": ["D. Batra", "H. Agrawal", "P. Banik", "N. Chavali", "A. Alfadda"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "The manifold tangent classifier", "author": ["Salah Rifai", "Yann Dauphin", "Pascal Vincent", "Yoshua Bengio", "Xavier Muller"], "venue": "Neural Information Processing Systems, page 2294\u20132302,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Recently proposed activation functions for neural networks such as rectified linear (ReL;[1]), maxout [2] and LWTA [3] are quite unlike sigmoidal activation functions.", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "Recently proposed activation functions for neural networks such as rectified linear (ReL;[1]), maxout [2] and LWTA [3] are quite unlike sigmoidal activation functions.", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "Nevertheless, many researchers have found that such networks can be trained faster and better than sigmoidal networks, and they are increasingly in use for learning from large and complex datasets [4, 5].", "startOffset": 197, "endOffset": 203}, {"referenceID": 3, "context": "Nevertheless, many researchers have found that such networks can be trained faster and better than sigmoidal networks, and they are increasingly in use for learning from large and complex datasets [4, 5].", "startOffset": 197, "endOffset": 203}, {"referenceID": 0, "context": "Past research has shown observational evidence that such networks have beneficial properties such as not requiring unsupervised training for weight initialization [1], better gradient flow [2] and mitigation of catastrophic forgetting [3, 6].", "startOffset": 163, "endOffset": 166}, {"referenceID": 1, "context": "Past research has shown observational evidence that such networks have beneficial properties such as not requiring unsupervised training for weight initialization [1], better gradient flow [2] and mitigation of catastrophic forgetting [3, 6].", "startOffset": 235, "endOffset": 241}, {"referenceID": 4, "context": "Past research has shown observational evidence that such networks have beneficial properties such as not requiring unsupervised training for weight initialization [1], better gradient flow [2] and mitigation of catastrophic forgetting [3, 6].", "startOffset": 235, "endOffset": 241}, {"referenceID": 5, "context": "Recently, the expressive power of deep networks with such functions has been theoretically analyzed [7].", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "Related activation techniques have been studied in the past decades, including recurrent networks with locally competitive units [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "Selfdelimiting recurrent networks with competitive units [3, 9] can in principle learn to decide their own run time and effective number of parameters, thus learning their own computable regularizers.", "startOffset": 57, "endOffset": 63}, {"referenceID": 7, "context": "Selfdelimiting recurrent networks with competitive units [3, 9] can in principle learn to decide their own run time and effective number of parameters, thus learning their own computable regularizers.", "startOffset": 57, "endOffset": 63}, {"referenceID": 8, "context": "In this paper, we restrict our analysis to networks trained with gradient-based algorithms which are often trained with the dropout regularization technique [10, 11] for improved generalization.", "startOffset": 157, "endOffset": 165}, {"referenceID": 9, "context": "In this paper, we restrict our analysis to networks trained with gradient-based algorithms which are often trained with the dropout regularization technique [10, 11] for improved generalization.", "startOffset": 157, "endOffset": 165}, {"referenceID": 9, "context": "Note that this model of models interpretation is different from the model averaging perspective of dropout [11] which posits that using dropout is equivalent to averaging the predictions of many subnetworks on the same data.", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "Our interpretation is related to mixtures of local experts [12], where a gating network was trained to select one of many subnetworks or modules in a system.", "startOffset": 59, "endOffset": 63}, {"referenceID": 11, "context": "In what follows, the subnetworks that emerge during training are first visualized using the t-SNE [13] algorithm.", "startOffset": 98, "endOffset": 102}, {"referenceID": 12, "context": "All experiments in this section are performed on the MNIST dataset [14].", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "Indeed, in a limited qualitative analysis, it has been shown previously [3] that in trained LWTA nets there are more units in common between subnetworks for examples of the same class than those for different class examples.", "startOffset": 72, "endOffset": 75}, {"referenceID": 13, "context": "Sparse binary codes enable efficient storage and retrieval for large and complex datasets due to which learning to produce them is an active research area [15, 16, 17].", "startOffset": 155, "endOffset": 167}, {"referenceID": 14, "context": "Sparse binary codes enable efficient storage and retrieval for large and complex datasets due to which learning to produce them is an active research area [15, 16, 17].", "startOffset": 155, "endOffset": 167}, {"referenceID": 15, "context": "Sparse binary codes enable efficient storage and retrieval for large and complex datasets due to which learning to produce them is an active research area [15, 16, 17].", "startOffset": 155, "endOffset": 167}, {"referenceID": 8, "context": "The dropout [10] regularization technique has proven to be very useful and efficient at improving generalization for large models, and is often used in combination with locally competitive activation functions [2, 4, 5].", "startOffset": 12, "endOffset": 16}, {"referenceID": 2, "context": "The dropout [10] regularization technique has proven to be very useful and efficient at improving generalization for large models, and is often used in combination with locally competitive activation functions [2, 4, 5].", "startOffset": 210, "endOffset": 219}, {"referenceID": 3, "context": "The dropout [10] regularization technique has proven to be very useful and efficient at improving generalization for large models, and is often used in combination with locally competitive activation functions [2, 4, 5].", "startOffset": 210, "endOffset": 219}, {"referenceID": 8, "context": "This supports the interpretation of dropout as a regularization technique which prevents \u201cco-adaptation of feature detectors\u201d (units) [10], leading to better representation of data by the subnetworks.", "startOffset": 134, "endOffset": 138}, {"referenceID": 16, "context": "For the CIFAR experiments, we used the models described in [2] since they use locally competitive activations (maxout), are trained with dropout, and good hyperparameter settings for them are available [18].", "startOffset": 202, "endOffset": 206}, {"referenceID": 17, "context": "CIFAR-10 is a dataset of 32\u00d732 color images of 10 classes split into a training set of size 50k and testing set of size 10k (6k images per class) [19].", "startOffset": 146, "endOffset": 150}, {"referenceID": 18, "context": "convolutional network trained on the ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC-2012) [20] dataset is evaluated.", "startOffset": 106, "endOffset": 110}, {"referenceID": 2, "context": "An implementation of the network in [4], with some differences [21], is available publicly.", "startOffset": 36, "endOffset": 39}, {"referenceID": 19, "context": "An implementation of the network in [4], with some differences [21], is available publicly.", "startOffset": 63, "endOffset": 67}, {"referenceID": 20, "context": "For the experiments in this section, the penultimate-layer activations obtained using this model were downloaded from CloudCV [22].", "startOffset": 126, "endOffset": 130}, {"referenceID": 2, "context": "In [4], it was first shown that the activations from the penultimate layer of the deep network can be used to retrieve similar images.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "This improves the network\u2019s error by about 1-2% [4].", "startOffset": 48, "endOffset": 51}, {"referenceID": 10, "context": "Without such local competition, one needs to have a global gating mechanism as in [12].", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "of the data manifold hypothesis for classification [23].", "startOffset": 51, "endOffset": 55}], "year": 2017, "abstractText": "Recently proposed neural network activation functions such as rectified linear, maxout, and local winner-take-all have allowed for faster and more effective training of deep neural architectures on large and complex datasets. The common trait among these functions is that they implement local competition between small groups of units within a layer, so that only part of the network is activated for any given input pattern. In this paper, we attempt to visualize and understand this self-modularization, and suggest a unified explanation for the beneficial properties of such networks. We also show how our insights can be directly useful for efficiently performing retrieval over large datasets using neural networks. A version of this paper was submitted to NIPS 2014 on 06-06-2014", "creator": "LaTeX with hyperref package"}}}