{"id": "1410.2082", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Oct-2014", "title": "Contrastive Unsupervised Word Alignment with Non-Local Features", "abstract": "Word alignment is an important natural language processing task that indicates the correspondence between natural languages. Recently, unsupervised learning of log-linear models for word alignment has received considerable attention as it combines the merits of generative and discriminative approaches. However, a major challenge still remains: it is intractable to calculate the expectations of non-local features that are critical for capturing the divergence between natural languages. We propose a contrastive approach that aims to differentiate observed training examples from noises. It not only introduces prior knowledge to guide unsupervised learning but also cancels out partition functions. Based on the observation that the probability mass of log-linear models for word alignment is usually highly concentrated, we propose to use top-n alignments to approximate the expectations with respect to posterior distributions. This allows for efficient and accurate calculation of expectations of non-local features. Experiments show that our approach achieves significant improvements over state-of-the-art unsupervised word alignment methods.", "histories": [["v1", "Wed, 8 Oct 2014 12:24:38 GMT  (70kb,D)", "https://arxiv.org/abs/1410.2082v1", "15 pages, 4 figures"], ["v2", "Fri, 10 Oct 2014 00:25:46 GMT  (70kb,D)", "http://arxiv.org/abs/1410.2082v2", "Added the missing Table 1; corrected a typo in Related Work"]], "COMMENTS": "15 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yang liu 0005", "maosong sun"], "accepted": true, "id": "1410.2082"}, "pdf": {"name": "1410.2082.pdf", "metadata": {"source": "CRF", "title": "Contrastive Unsupervised Word Alignment with Non-Local Features", "authors": ["Yang Liu"], "emails": ["liuyang2011@tsinghua.edu.cn", "sms@tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Word alignment is a natural language processing (NLP) task that aims to identify the correspondence between words in natural languages (Brown et al., 1993). Wordaligned parallel corpora are an indispensable resource for many NLP tasks such as machine translation and cross-lingual IR.\nCurrent word alignment approaches can be roughly divided into two categories: generative and discriminative. Generative approaches are often based on generative models (Brown et al., 1993; Vogel, Ney, and Tillmann, 1996; Liang, Taskar, and Klein, 2006), the parameters of which are learned by maximizing the likelihood of unlabeled\nar X\niv :1\n41 0.\n20 82\nv2 [\ncs .C\nL ]\ndata. One major drawback of these approaches is that they are hard to extend due to the strong dependencies between sub-models. On the other hand, discriminative approaches overcome this problem by leveraging log-linear models (Liu, Liu, and Lin, 2005; Blunsom and Cohn, 2006) and linear models (Taskar, Lacoste-Julien, and Klein, 2005; Moore, Yih, and Bode, 2006; Liu, Liu, and Lin, 2010) to include arbitrary features. However, labeled data is expensive to build and hence is unavailable for most language pairs and domains.\nAs generative and discriminative approaches seem to be complementary, a number of authors have tried to combine the advantages of both in recent years (BergKirkpatrick et al., 2010; Dyer et al., 2011; Dyer, Chahuneau, and Smith, 2013). They propose to train log-linear models for word alignment on unlabeled data, which involves calculating two expectations of features: one ranging over all possible alignments given observed sentence pairs and another over all possible sentence pairs and alignments. Due to the complexity and diversity of natural languages, it is intractable to calculate the two expectations. As a result, existing approaches have to either restrict log-linear models to be locally normalized (Berg-Kirkpatrick et al., 2010) or only use local features to admit efficient dynamic programming algorithms on compact representations (Dyer et al., 2011). Although it is possible to use MCMC methods to draw samples from alignment distributions (DeNero, Bouchard-Cot\u0302e\u0301, and Klein, 2008) to calculate expectations of non-local features, it is computationally expensive to reach the equilibrium distribution. Therefore, including non-local features, which are critical for capturing the divergence between natural languages, still remains a major challenge in unsupervised learning of log-linear models for word alignment.\nIn the paper, we present a contrastive learning approach to training log-linear models for word alignment on unlabeled data. Instead of maximizing the likelihood of log-linear models on the observed data, our approach follows contrastive estimation methods (Smith and Eisner, 2005; Gutmann and Hyva\u0308rinen, 2012) to guide the model to assign higher probabilities to observed data than to noisy data. To calculate the expectations of non-local features, we propose an approximation method called top-n sampling based on the observation that the probability mass of log-linear models for word alignment is highly concentrated. Hence, our approach has the following advantages over previous work:\n1. Partition functions canceled out. As learning only involves observed and noisy training examples, our training objective cancels out partition functions that comprise exponentially many sentence pairs and alignments.\n2. Efficient sampling. We use a dynamic programming algorithm to extract top-n alignments, which serve as samples to compute the approximate expectations.\n3. Arbitrary features. The expectations of both local and non-local features can be calculated using top-n approximation accurately and efficiently.\nExperiments on multilingual datasets show that our approach achieves significant improvements over state-of-the-art unsupervised alignment systems."}, {"heading": "2 Latent-Variable Log-Linear Models for Unsupervised Word Alignment", "text": "Figure 1(a) shows a (romanized) Chinese sentence, an English sentence, and the word alignment between them. The links indicate the correspondence between Chinese and English words. Word alignment is a challenging task because both the lexical choices and word orders in two languages are significantly different. For example, while the English word \u201cat\u201d corresponds to a discontinuous Chinese phrase \u201czai ... shang\u201d, the English function word \u201cthe\u201d has no counterparts in Chinese. In addition, a verb phrase (e.g., \u201cmade a speech\u201d) is usually followed by a prepositional phrase (e.g., \u201cat the meeting\u201d) in English but the order is reversed in Chinese. Therefore, it is important to design features to capture various characteristics of word alignment.\nTo allow for unsupervised word alignment with arbitrary features, latent-variable log-linear models have been studied in recent years (Berg-Kirkpatrick et al., 2010; Dyer et al., 2011; Dyer, Chahuneau, and Smith, 2013). Let x be a pair of source and target sentences and y be the word alignment. A latent-variable log-linear model parametrized by a real-valued vector \u03b8 \u2208 RK\u00d71 is given by\nP (x;\u03b8) = \u2211\ny\u2208Y(x)\nP (x,y;\u03b8) (1)\n=\n\u2211 y\u2208Y(x) exp(\u03b8 \u00b7 \u03c6(x,y))\nZ(\u03b8) (2)\nwhere \u03c6(\u00b7) \u2208 RK\u00d71 is a feature vector and Z(\u03b8) is a partition function for normalization:\nZ(\u03b8) = \u2211 x\u2208X \u2211 y\u2208Y(x) exp(\u03b8 \u00b7 \u03c6(x,y)) (3)\nWe use X to denote all possible pairs of source and target strings and Y(x) to denote the set of all possible alignments for a sentence pair x. Let l and m be the lengths\nof the source and target sentences in x, respectively. Then, the number of possible alignments for x is |Y(x)| = 2l\u00d7m. In this work, we use 5 local features (translation probability product, relative position absolute difference, link count, monotone and swapping neighbor counts) and 11 non-local features (cross count, source and target linked word counts, source and target sibling distances, source and target maximal fertilities, multiple link types) that prove to be effective in modeling regularities in word alignment (Taskar, Lacoste-Julien, and Klein, 2005; Moore, Yih, and Bode, 2006; Liu, Liu, and Lin, 2010).\nGiven a set of training examples {x(i)}Ii=1, the standard training objective is to find the parameter that maximizes the log-likelihood of the training set:\n\u03b8\u2217 = argmax \u03b8\n{ L(\u03b8) } (4)\n= argmax \u03b8\n{ log\nI\u220f i=1 P (x(i);\u03b8)\n} (5)\n= argmax \u03b8\n{ I\u2211\ni=1\nlog \u2211\ny\u2208Y(x(i))\nexp(\u03b8 \u00b7 \u03c6(x(i),y))\n\u2212 logZ(\u03b8) } (6)\nStandard numerical optimization methods such as L-BFGS and Stochastic Gradient Descent (SGD) require to calculate the partial derivative of the log-likelihood L(\u03b8) with respect to the k-th feature weight \u03b8k\n\u2202L(\u03b8)\n\u2202\u03b8k\n= I\u2211 i=1 \u2211 y\u2208Y(x(i)) P (y|x(i);\u03b8)\u03c6k(x(i),y)\n\u2212 \u2211 x\u2208X \u2211 y\u2208Y(x) P (x,y;\u03b8)\u03c6k(x,y) (7)\n= I\u2211 i=1 Ey|x(i);\u03b8[\u03c6k(x(i),y)]\u2212 Ex,y;\u03b8[\u03c6k(x,y)] (8)\nAs there are exponentially many sentences and alignments, the two expectations in Eq. (8) are intractable to calculate for non-local features that are critical for measuring the fertility and non-monotonicity of alignment (Liu, Liu, and Lin, 2010). Consequently, existing approaches have to use only local features to allow dynamic programming algorithms to calculate expectations efficiently on lattices (Dyer et al., 2011). Therefore, how to calculate the expectations of non-local features accurately and efficiently remains a major challenge for unsupervised word alignment.\n3 Contrastive Learning with Top-n Sampling Instead of maximizing the log-likelihood of the observed training data, we propose a contrastive approach to unsupervised learning of log-linear models. For example, given an observed training example as shown in Figure 1(a), it is possible to generate a noisy example as shown in Figure 1(b) by randomly shuffling and substituting words on both sides. Intuitively, we expect that the probability of the observed example is higher than that of the noisy example. This is called contrastive learning, which has been advocated by a number of authors (see Related Work).\nMore formally, let x\u0303 be a noisy training example derived from an observed example x. Our training data is composed of pairs of observed and noisy examples: D = {\u3008x(i), x\u0303(i)\u3009}Ii=1. The training objective is to maximize the difference of probabilities between observed and noisy training examples:\n\u03b8\u2217\n= argmax \u03b8\n{ J(\u03b8) } (9)\n= argmax \u03b8\n{ log\nI\u220f i=1 P (x(i)) P (x\u0303(i))\n} (10)\n= argmax \u03b8\n{ I\u2211\ni=1\nlog \u2211\ny\u2208Y(x(i))\nexp(\u03b8 \u00b7 \u03c6(x(i),y))\n\u2212 log \u2211\ny\u2208Y(x\u0303(i))\nexp(\u03b8 \u00b7 \u03c6(x\u0303(i),y)) } (11)\nAccordingly, the partial derivative of J(\u03b8) with respect to the k-th feature weight \u03b8k is given by\n\u2202J(\u03b8)\n\u2202\u03b8k = I\u2211\ni=1 \u2211 y\u2208Y(x(i)) P (y|x(i);\u03b8)\u03c6k(x(i),y)\n\u2212 \u2211\ny\u2208Y(x\u0303(i))\nP (y|x\u0303(i);\u03b8)\u03c6k(x\u0303(i),y) (12)\n= I\u2211 i=1 Ey|x(i);\u03b8[\u03c6k(x(i),y)]\u2212 Ey|x\u0303(i);\u03b8[\u03c6k(x\u0303(i),y)]\n(13)\nThe key difference is that our approach cancels out the partition function Z(\u03b8), which poses the major computational challenge in unsupervised learning of log-linear\nmodels. However, it is still intractable to calculate the expectation with respect to the posterior distribution Ey|x;\u03b8[\u03c6(x,y)] for non-local features due to the exponential search space (i.e., |Y(x)| = 2l\u00d7m). One possible solution is to use Gibbs sampling to draw samples from the posterior distribution P (y|x;\u03b8) (DeNero, Bouchard-Cot\u0302e\u0301, and Klein, 2008). But the Gibbs sampler usually runs for a long time to converge to the equilibrium distribution.\nFortunately, by definition, only alignments with highest probabilities play a central role in calculating expectations. If the probability mass of the log-linear model for word alignment is concentrated on a small number of alignments, it will be efficient and accurate to only use most likely alignments to approximate the expectation.\nFigure 2 plots the distributions of log-linear models parametrized by 1,000 random feature weight vectors. We used all the 16 features. The true distributions were calculated by enumerating all possible alignments for short Chinese and English sentences (\u2264 4 words). We find that top-5 alignments usually account for over 99% of the probability mass.\nMore importantly, we also tried various sentence lengths, language pairs, and feature groups and found this concentration property to hold consistently. One possible reason is that the exponential function enlarges the differences between variables dramatically (i.e., a > b\u21d2 exp(a) exp(b)).\nTherefore, we propose to approximate the expectation using most likely alignments:\nEy|x;\u03b8[\u03c6k(x,y)] = \u2211\ny\u2208Y(x)\nP (y|x;\u03b8)\u03c6k(x,y) (14)\n= \u2211 y\u2208Y(x) exp(\u03b8 \u00b7 \u03c6(x,y))\u03c6k(x,y)\u2211\ny\u2032\u2208Y(x) exp(\u03b8 \u00b7 \u03c6(x,y\u2032)) (15) \u2248 \u2211\ny\u2208N (x;\u03b8) exp(\u03b8 \u00b7 \u03c6(x,y))\u03c6k(x,y)\u2211 y\u2032\u2208N (x;\u03b8) exp(\u03b8 \u00b7 \u03c6(x,y\u2032))\n(16)\nwhere N (x;\u03b8) \u2286 Y(x) contains the most likely alignments depending on \u03b8:\n\u2200y1 \u2208 N (x;\u03b8),\u2200y2 \u2208 Y(x)\\N (x;\u03b8) : \u03b8 \u00b7 \u03c6(x,y1) > \u03b8 \u00b7 \u03c6(x,y2) (17)\nLet the cardinality ofN (x;\u03b8) be n. We refer to Eq. (16) as top-n sampling because the approximate posterior distribution is normalized over top-n alignments:\nPN (y|x;\u03b8) = exp(\u03b8 \u00b7 \u03c6(x,y))\u2211\ny\u2032\u2208N (x) exp(\u03b8 \u00b7 \u03c6(x,y\u2032)) (18)\nIn this paper, we use the beam search algorithm proposed by Liu, Liu, and Lin (2010) to retrieve top-n alignments from the full search space. Starting with an empty alignment, the algorithm keeps adding links until the alignment score will not increase. During the process, local and non-local feature values can be calculated in an incremental way efficiently. The algorithm generally runs in O(bl2m2) time, where b is the beam size. As it is intractable to calculate the objective function in Eq. (11), we use the stochastic gradient descent algorithm (SGD) for parameter optimization, which requires to calculate partial derivatives with respect to feature weights on single training examples."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Approximation Evaluation", "text": "To measure how well top-n sampling approximates the true expectations, we define the approximation error E(D,\u03b8) as\n1\nI \u00d7K I\u2211 i=1 ||\u03b4Y(x(i), x\u0303(i),\u03b8)\u2212 \u03b4N (x(i), x\u0303(i),\u03b8)||1 (19)\nwhere \u03b4Y(\u00b7) returns the true difference between the expectations of observed and noisy examples:\n\u03b4Y(x, x\u0303,\u03b8) = Ey|x;\u03b8[\u03c6(x,y)]\u2212 Ey|x\u0303;\u03b8[\u03c6(x\u0303,y)] (20)\nSimilarly, \u03b4N (\u00b7) returns the approximate difference. || \u00b7 ||1 is the L1 norm. In addition, we define average approximation error on a set of random feature weight vectors {\u03b8(t)}Tt=1:\n1\nT T\u2211 t=1 E(D,\u03b8(t)) (21)\nFigure 3 shows the average approximation errors of our top-n sampling method on short sentences (up to 4 words) with 1,000 random feature weight vectors. To calculate the true expectations of both local and non-local features, we need to enumerate all alignments in an exponential space. We randomly selected 1, 000 short ChineseEnglish sentence pairs. One noisy example was generated for each observed example by randomly shuffling, replacing, inserting, and deleting words. We used the beam search algorithm (Liu, Liu, and Lin, 2010) to retrieve n-best lists. We plotted the approximation errors for n up to 15. We find that the average approximation errors drop dramatically when n ranges from 1 to 5 and approach zero for large values of n, suggesting that a small value of n might suffice to approximate the expectations.\nFigure 4 shows the average approximation errors of top-n sampling on long sentences (up to 100 words) with 1,000 random feature weight vectors. To calculate the true expectations, we follow Dyer et al. (2011) to use a dynamic programming algorithm on lattices that compactly represent exponentially many asymmetric alignments. The average errors decrease much less dramatically than in Figure 3 but still maintain at a very low level (below 0.17). This finding implies that the probability mass of log-linear models is still highly concentrated for long sentences.\nTable 1 compares our approach with Gibbs sampling. We treat each link l as a binary variable and the alignment probability is a joint distribution over m\u00d7n variables, which can be sampled successively from the conditional distribution P (l|y\\{l}). Start-\ning with random alignments, the Gibbs sampler achieves an average approximation error of 0.5180 with 500 samples and takes a very long time to converge. In contrast, our approach achieves much lower errors than Gibbs even only using one sample. Therefore, using more likely alignments in sampling improves not only the accuracy but also efficiency."}, {"heading": "4.2 Alignment Evaluation", "text": "We evaluated our approach on French-English and Chinese-English alignment tasks. For French-English, we used the dataset from the HLT/NAACL 2003 alignment shared task (Mihalcea and Pedersen, 2003). The training set consists of 1.1M sentence pairs with 23.61M French words and 20.01M English words, the validation set consists of 37 sentence pairs, and the test set consists of 447 sentence pairs. Both the validation and test sets are annotated with gold-standard alignments. For Chinese-English, we used the dataset from Liu, Liu, and Lin (2005). The training set consists of 1.5M sentence pairs with 42.1M Chinese words and 48.3M English words, the validation set consists of 435 sentence pairs, and the test set consists of 500 sentence pairs. The evaluation\nmetric is alignment error rate (AER) (Och and Ney, 2003). The baseline systems we compared in our experiments include\n1. GIZA++ (Och and Ney, 2003): unsupervised training of IBM models 1-5 (Brown et al., 1993) and HMM (Vogel, Ney, and Tillmann, 1996) using EM,\n2. Berkeley (Liang, Taskar, and Klein, 2006): unsupervised training of joint HMMs using EM,\n3. fast align (Dyer, Chahuneau, and Smith, 2013): unsupervised training of loglinear models based on IBM model 2 using EM,\n4. Vigne (Liu, Liu, and Lin, 2010): supervised training of log-linear models using minimum error rate training (Och, 2003).\nAs both GIZA++ and fast align produce asymmetric alignments, we use the growdiag-final-and heuristic (Koehn et al., 2007) to generate symmetric alignments for evaluation. While the baseline systems used all the training sets, we randomly selected 500 sentences and generated noises by randomly shuffling, replacing, deleting, and inserting words. 1\n1As the translation probability product feature derived from GIZA++ is a very strong dense feature, using small training corpora (e.g., 50 sentence pairs) proves to yield very good results consistently (Liu, Liu, and Lin, 2010). However, if we model translation equivalence using millions of sparse features (Dyer et al., 2011), the unsupervised learning algorithm must make full use of all parallel corpora available like GIZA++. We leave this for future work.\nWe first used the validation sets to find the optimal setting of our approach: noisy generation, the value of n, feature group, and training corpus size.\nTable 2 shows the results of different noise generation strategies: randomly shuffling, inserting, replacing, and deleting words. We find shuffling source and target words randomly consistently yields the best results. One possible reason is that the translation probability product feature (Liu, Liu, and Lin, 2010) derived from GIZA++ suffices to evaluate lexical choices accurately. It is more important to guide the aligner to model the structural divergence by changing word orders randomly.\nTable 3 gives the results of different values of sample size n on the validation sets. We find that increasing n does not lead to significant improvements. This might result from the high concentration property of log-linear models. Therefore, we simply set n = 1 in the following experiments.\nTable 4 shows the effect of adding non-local features. As most structural divergence between natural languages are non-local, including non-local features leads to significant improvements for both French-English and Chinese-English. As a result, we used all 16 features in the following experiments.\nTable 5 gives our final result on the test sets. Our approach outperforms all unsupervised aligners significantly statistically (p < 0.01) except for the Berkeley aligner on the French-English data. The margins on Chinese-English are generally much larger than French-English because Chinese and English are distantly related and exhibit more non-local structural divergence. Vigne used the same features as our system but was trained in a supervised way. Its results can be treated as the upper bounds that our method can potentially approach.\nWe also compared our approach with baseline systems on French-English and Chinese-English translation tasks but only obtained modest improvements. As alignment and translation are only loosely related (i.e., lower AERs do not necessarily lead to higher BLEU scores), imposing appropriate structural constraints (e.g., the grow, diag, final operators in symmetrizing alignments) seems to be more important for improving translation translation quality than developing unsupervised training algorithms (Koehn et al., 2007)."}, {"heading": "5 Related Work", "text": "Our work is inspired by three lines of research: unsupervised learning of log-linear models, contrastive learning, and sampling for structured prediction."}, {"heading": "5.1 Unsupervised Learning of Log-Linear Models", "text": "Unsupervised learning of log-linear models has been widely used in natural language processing, including word segmentation (Berg-Kirkpatrick et al., 2010), morphological segmentation (Poon, Cherry, and Toutanova, 2009), POS tagging (Smith and Eisner, 2005), grammar induction (Smith and Eisner, 2005), and word alignment (Dyer et al., 2011; Dyer, Chahuneau, and Smith, 2013). The contrastive estimation (CE) approach proposed by Smith and Eisner (2005) is in spirit most close to our work. CE redefines the partition function as the set of each observed example and its noisy \u201cneighbors\u201d. However, it is still intractable to compute the expectations of non-local features. In contrast, our approach cancels out the partition function and introduces top-n sampling to approximate the expectations of non-local features."}, {"heading": "5.2 Contrastive Learning", "text": "Contrastive learning has received increasing attention in a variety of fields. Hinton (2002) proposes contrastive divergence (CD) that compares the data distribution with reconstructions of the data vector generated by a limited number of full Gibbs sampling steps. It is possible to apply CD to unsupervised learning of latent-variable log-linear models and use top-n sampling to approximate the expectation on posterior distributions within each full Gibbs sampling step. The noise-contrastive estimation (NCE) method (Gutmann and Hyva\u0308rinen, 2012) casts density estimation, which is a typical unsupervised learning problem, as supervised classification by introducing noisy data. However, a key limitation of NCE is that it cannot be used for models with latent variables that cannot be integrated out analytically. There are also many other efforts in developing contrastive objectives to avoid computing partition functions (LeCun and Huang, 2005; Liang and Jordan, 2008; Vickrey, Lin, and Koller, 2010). Their focus is on choosing assignments to be compared with the observed data and developing subobjectives that allow for dynamic programming for tractable sub-structures. In this work, we simply remove the partition functions by comparing pairs of observed and noisy examples. Using noisy examples to guide unsupervised learning has also been pursued in deep learning (Collobert and Weston, 2008; Tamura, Watanabe, and Sumita, 2014)."}, {"heading": "5.3 Sampling for Structured Prediction", "text": "Widely used in NLP for inference (Teh, 2006; Johnson, Griffiths, and Goldwater, 2007) and calculating expectations (DeNero, Bouchard-Cot\u0302e\u0301, and Klein, 2008), Gibbs sampling has not been used for unsupervised training of log-linear models for word alignment. Tamura, Watanabe, and Sumita (2014) propose a similar idea to use beam search to calculate expectations. However, they do not offer in-depth analyses and the accuracy of their unsupervised approach is far worse than the supervised counterpart in terms of F1 score (0.55 vs. 0.89)."}, {"heading": "6 Conclusion", "text": "We have presented a contrastive approach to unsupervised learning of log-linear models for word alignment. By introducing noisy examples, our approach cancels out partition functions that makes training computationally expensive. Our major contribution is to introduce top-n sampling to calculate expectations of non-local features since the probability mass of log-linear models for word alignment is usually concentrated on top-n alignments. Our unsupervised aligner outperforms state-of-the-art unsupervised systems on both closely-related (French-English) and distantly-related (Chinese-English) language pairs.\nAs log-linear models have been widely used in NLP, we plan to validate the effectiveness of our approach on more structured prediction tasks with exponential search spaces such as word segmentation, part-of-speech tagging, dependency parsing, and machine translation. It is important to verify whether the concentration property of log-linear models still holds. Since our contrastive approach compares between observed and noisy training examples, another promising direction is to develop large margin learning algorithms to improve generalization ability of our approach. Finally, it is interesting to include millions of sparse features (Dyer et al., 2011) to directly model the translation equivalence between words rather than relying on GIZA++."}, {"heading": "Acknowledgements", "text": "This research is supported by the 973 Program (No. 2014CB340501), the National Natural Science Foundation of China (No. 61331013), The National Key Technology R & D Program (No. 2014BAK10B03), Google Focused Research Award, the Singapore National Research Foundation under its International Research Center @ Singapore Funding Initiative and administered by the IDM Programme."}], "references": [{"title": "Painless unsupervised learning with features", "author": ["T. Berg-Kirkpatrick", "A. Bouchard-Cot\u0302\u00e9", "J. DeNero", "D. Klein"], "venue": "In Proceedings of NAACL", "citeRegEx": "Berg.Kirkpatrick et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2010}, {"title": "Discriminative word alignment with conditional random fields", "author": ["P. Blunsom", "T. Cohn"], "venue": "Proceedings of COLING-ACL 2006.", "citeRegEx": "Blunsom and Cohn,? 2006", "shortCiteRegEx": "Blunsom and Cohn", "year": 2006}, {"title": "The mathematics of statistical machine translation: parameter estimation", "author": ["P.F. Brown", "V.J.D. Pietra", "S.A.D. Pietra", "R.L. Mercer"], "venue": "Computational Linguistics.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of ICML 2008.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Sampling alignment structure under a bayesian translation model", "author": ["J. DeNero", "A. Bouchard-Cot\u0302\u00e9", "D. Klein"], "venue": "In Proceedings of EMNLP", "citeRegEx": "DeNero et al\\.,? \\Q2008\\E", "shortCiteRegEx": "DeNero et al\\.", "year": 2008}, {"title": "Unsupervised word alignment with arbitrary features", "author": ["C. Dyer", "J.H. Clark", "A. Lavie", "N.A. Smith"], "venue": "Proceedings of ACL 2011.", "citeRegEx": "Dyer et al\\.,? 2011", "shortCiteRegEx": "Dyer et al\\.", "year": 2011}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["C. Dyer", "V. Chahuneau", "N.A. Smith"], "venue": "Proceedings of NAACL 2013.", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["M.U. Gutmann", "Hyv\u00e4rinen."], "venue": "Journal of Machine Learning Research.", "citeRegEx": "Gutmann and Hyv\u00e4rinen.,? 2012", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen.", "year": 2012}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G. Hinton"], "venue": "Neural Computation.", "citeRegEx": "Hinton,? 2002", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "Bayesian inference for pcfgs via markov chain monte carlo", "author": ["M. Johnson", "T. Griffiths", "S. Goldwater"], "venue": "Proceedings of ACL 2007.", "citeRegEx": "Johnson et al\\.,? 2007", "shortCiteRegEx": "Johnson et al\\.", "year": 2007}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proceedings of ACL 2007 (Demo and Poster).", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Loss functions for discriminative training of energybased models", "author": ["Y. LeCun", "F.J. Huang"], "venue": "Proceedings of AISTATS 2005.", "citeRegEx": "LeCun and Huang,? 2005", "shortCiteRegEx": "LeCun and Huang", "year": 2005}, {"title": "An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators", "author": ["P. Liang", "M.I. Jordan"], "venue": "Proceedings of ICML 2008.", "citeRegEx": "Liang and Jordan,? 2008", "shortCiteRegEx": "Liang and Jordan", "year": 2008}, {"title": "Alignment by agreement", "author": ["P. Liang", "B. Taskar", "D. Klein"], "venue": "Proceedings of HLT-NAACL 2006.", "citeRegEx": "Liang et al\\.,? 2006", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Log-linear models for word alignment", "author": ["Y. Liu", "Q. Liu", "S. Lin"], "venue": "Proceedings of ACL 2005.", "citeRegEx": "Liu et al\\.,? 2005", "shortCiteRegEx": "Liu et al\\.", "year": 2005}, {"title": "Discriminative word alignment by linear modeling", "author": ["Y. Liu", "Q. Liu", "S. Lin"], "venue": "Computational Linguistics.", "citeRegEx": "Liu et al\\.,? 2010", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "An evaluation excercise for word alignment", "author": ["R. Mihalcea", "T. Pedersen"], "venue": "Proceedings of HLT-NAACL 2003 Workshop on Building and Using Parallel Texts.", "citeRegEx": "Mihalcea and Pedersen,? 2003", "shortCiteRegEx": "Mihalcea and Pedersen", "year": 2003}, {"title": "Improved discriminative bilingual word alignment", "author": ["R.C. Moore", "W.-t. Yih", "A. Bode"], "venue": "Proceedings of COLING-ACL 2006.", "citeRegEx": "Moore et al\\.,? 2006", "shortCiteRegEx": "Moore et al\\.", "year": 2006}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F. Och", "H. Ney"], "venue": "Computational Linguistics.", "citeRegEx": "Och and Ney,? 2003", "shortCiteRegEx": "Och and Ney", "year": 2003}, {"title": "Minimum error rate training in statistical machine translation", "author": ["F. Och"], "venue": "Proceedings of ACL 2003.", "citeRegEx": "Och,? 2003", "shortCiteRegEx": "Och", "year": 2003}, {"title": "Unsupervised morphological segmentation with log-linear models", "author": ["H. Poon", "C. Cherry", "K. Toutanova"], "venue": "Proceedings of NAACL 2009.", "citeRegEx": "Poon et al\\.,? 2009", "shortCiteRegEx": "Poon et al\\.", "year": 2009}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["N. Smith", "J. Eisner"], "venue": "Proceedings of ACL 2005.", "citeRegEx": "Smith and Eisner,? 2005", "shortCiteRegEx": "Smith and Eisner", "year": 2005}, {"title": "Recurrent neural networks for word alignment model", "author": ["A. Tamura", "T. Watanabe", "E. Sumita"], "venue": "Proceedings of EMNLP 2014.", "citeRegEx": "Tamura et al\\.,? 2014", "shortCiteRegEx": "Tamura et al\\.", "year": 2014}, {"title": "A discriminative matching approach to word alignment", "author": ["B. Taskar", "S. Lacoste-Julien", "D. Klein"], "venue": "Proceedings of EMNLP 2005.", "citeRegEx": "Taskar et al\\.,? 2005", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "A hierarchical bayesian language model based on pitman-yor processes", "author": ["Y.W. Teh"], "venue": "Proceedings of COLING/ACL 2006.", "citeRegEx": "Teh,? 2006", "shortCiteRegEx": "Teh", "year": 2006}, {"title": "Non-local contrastive objectives", "author": ["D. Vickrey", "C.C.-Y. Lin", "D. Koller"], "venue": "Proceedings of ICML 2010.", "citeRegEx": "Vickrey et al\\.,? 2010", "shortCiteRegEx": "Vickrey et al\\.", "year": 2010}, {"title": "Hmm-based word alignment in statistical translation", "author": ["S. Vogel", "H. Ney", "C. Tillmann"], "venue": "Proceedings of COLING 1996.", "citeRegEx": "Vogel et al\\.,? 1996", "shortCiteRegEx": "Vogel et al\\.", "year": 1996}], "referenceMentions": [{"referenceID": 2, "context": "Word alignment is a natural language processing (NLP) task that aims to identify the correspondence between words in natural languages (Brown et al., 1993).", "startOffset": 135, "endOffset": 155}, {"referenceID": 2, "context": "Generative approaches are often based on generative models (Brown et al., 1993; Vogel, Ney, and Tillmann, 1996; Liang, Taskar, and Klein, 2006), the parameters of which are learned by maximizing the likelihood of unlabeled", "startOffset": 59, "endOffset": 143}, {"referenceID": 1, "context": "On the other hand, discriminative approaches overcome this problem by leveraging log-linear models (Liu, Liu, and Lin, 2005; Blunsom and Cohn, 2006) and linear models (Taskar, Lacoste-Julien, and Klein, 2005; Moore, Yih, and Bode, 2006; Liu, Liu, and Lin, 2010) to include arbitrary features.", "startOffset": 99, "endOffset": 148}, {"referenceID": 5, "context": "As generative and discriminative approaches seem to be complementary, a number of authors have tried to combine the advantages of both in recent years (BergKirkpatrick et al., 2010; Dyer et al., 2011; Dyer, Chahuneau, and Smith, 2013).", "startOffset": 151, "endOffset": 234}, {"referenceID": 0, "context": "As a result, existing approaches have to either restrict log-linear models to be locally normalized (Berg-Kirkpatrick et al., 2010) or only use local features to admit efficient dynamic programming algorithms on compact representations (Dyer et al.", "startOffset": 100, "endOffset": 131}, {"referenceID": 5, "context": ", 2010) or only use local features to admit efficient dynamic programming algorithms on compact representations (Dyer et al., 2011).", "startOffset": 112, "endOffset": 131}, {"referenceID": 21, "context": "Instead of maximizing the likelihood of log-linear models on the observed data, our approach follows contrastive estimation methods (Smith and Eisner, 2005; Gutmann and Hyv\u00e4rinen, 2012) to guide the model to assign higher probabilities to observed data than to noisy data.", "startOffset": 132, "endOffset": 185}, {"referenceID": 7, "context": "Instead of maximizing the likelihood of log-linear models on the observed data, our approach follows contrastive estimation methods (Smith and Eisner, 2005; Gutmann and Hyv\u00e4rinen, 2012) to guide the model to assign higher probabilities to observed data than to noisy data.", "startOffset": 132, "endOffset": 185}, {"referenceID": 0, "context": "To allow for unsupervised word alignment with arbitrary features, latent-variable log-linear models have been studied in recent years (Berg-Kirkpatrick et al., 2010; Dyer et al., 2011; Dyer, Chahuneau, and Smith, 2013).", "startOffset": 134, "endOffset": 218}, {"referenceID": 5, "context": "To allow for unsupervised word alignment with arbitrary features, latent-variable log-linear models have been studied in recent years (Berg-Kirkpatrick et al., 2010; Dyer et al., 2011; Dyer, Chahuneau, and Smith, 2013).", "startOffset": 134, "endOffset": 218}, {"referenceID": 5, "context": "Consequently, existing approaches have to use only local features to allow dynamic programming algorithms to calculate expectations efficiently on lattices (Dyer et al., 2011).", "startOffset": 156, "endOffset": 175}, {"referenceID": 5, "context": "To calculate the true expectations, we follow Dyer et al. (2011) to use a dynamic programming algorithm on lattices that compactly represent exponentially many asymmetric alignments.", "startOffset": 46, "endOffset": 65}, {"referenceID": 16, "context": "For French-English, we used the dataset from the HLT/NAACL 2003 alignment shared task (Mihalcea and Pedersen, 2003).", "startOffset": 86, "endOffset": 115}, {"referenceID": 16, "context": "For French-English, we used the dataset from the HLT/NAACL 2003 alignment shared task (Mihalcea and Pedersen, 2003). The training set consists of 1.1M sentence pairs with 23.61M French words and 20.01M English words, the validation set consists of 37 sentence pairs, and the test set consists of 447 sentence pairs. Both the validation and test sets are annotated with gold-standard alignments. For Chinese-English, we used the dataset from Liu, Liu, and Lin (2005). The training set consists of 1.", "startOffset": 87, "endOffset": 466}, {"referenceID": 18, "context": "metric is alignment error rate (AER) (Och and Ney, 2003).", "startOffset": 37, "endOffset": 56}, {"referenceID": 18, "context": "GIZA++ (Och and Ney, 2003): unsupervised training of IBM models 1-5 (Brown et al.", "startOffset": 7, "endOffset": 26}, {"referenceID": 2, "context": "GIZA++ (Och and Ney, 2003): unsupervised training of IBM models 1-5 (Brown et al., 1993) and HMM (Vogel, Ney, and Tillmann, 1996) using EM, 2.", "startOffset": 68, "endOffset": 88}, {"referenceID": 19, "context": "Vigne (Liu, Liu, and Lin, 2010): supervised training of log-linear models using minimum error rate training (Och, 2003).", "startOffset": 108, "endOffset": 119}, {"referenceID": 10, "context": "As both GIZA++ and fast align produce asymmetric alignments, we use the growdiag-final-and heuristic (Koehn et al., 2007) to generate symmetric alignments for evaluation.", "startOffset": 101, "endOffset": 121}, {"referenceID": 5, "context": "However, if we model translation equivalence using millions of sparse features (Dyer et al., 2011), the unsupervised learning algorithm must make full use of all parallel corpora available like GIZA++.", "startOffset": 79, "endOffset": 98}, {"referenceID": 10, "context": ", the grow, diag, final operators in symmetrizing alignments) seems to be more important for improving translation translation quality than developing unsupervised training algorithms (Koehn et al., 2007).", "startOffset": 184, "endOffset": 204}, {"referenceID": 0, "context": "1 Unsupervised Learning of Log-Linear Models Unsupervised learning of log-linear models has been widely used in natural language processing, including word segmentation (Berg-Kirkpatrick et al., 2010), morphological segmentation (Poon, Cherry, and Toutanova, 2009), POS tagging (Smith and Eisner, 2005), grammar induction (Smith and Eisner, 2005), and word alignment (Dyer et al.", "startOffset": 169, "endOffset": 200}, {"referenceID": 21, "context": ", 2010), morphological segmentation (Poon, Cherry, and Toutanova, 2009), POS tagging (Smith and Eisner, 2005), grammar induction (Smith and Eisner, 2005), and word alignment (Dyer et al.", "startOffset": 85, "endOffset": 109}, {"referenceID": 21, "context": ", 2010), morphological segmentation (Poon, Cherry, and Toutanova, 2009), POS tagging (Smith and Eisner, 2005), grammar induction (Smith and Eisner, 2005), and word alignment (Dyer et al.", "startOffset": 129, "endOffset": 153}, {"referenceID": 5, "context": ", 2010), morphological segmentation (Poon, Cherry, and Toutanova, 2009), POS tagging (Smith and Eisner, 2005), grammar induction (Smith and Eisner, 2005), and word alignment (Dyer et al., 2011; Dyer, Chahuneau, and Smith, 2013).", "startOffset": 174, "endOffset": 227}, {"referenceID": 0, "context": "1 Unsupervised Learning of Log-Linear Models Unsupervised learning of log-linear models has been widely used in natural language processing, including word segmentation (Berg-Kirkpatrick et al., 2010), morphological segmentation (Poon, Cherry, and Toutanova, 2009), POS tagging (Smith and Eisner, 2005), grammar induction (Smith and Eisner, 2005), and word alignment (Dyer et al., 2011; Dyer, Chahuneau, and Smith, 2013). The contrastive estimation (CE) approach proposed by Smith and Eisner (2005) is in spirit most close to our work.", "startOffset": 170, "endOffset": 499}, {"referenceID": 7, "context": "The noise-contrastive estimation (NCE) method (Gutmann and Hyv\u00e4rinen, 2012) casts density estimation, which is a typical unsupervised learning problem, as supervised classification by introducing noisy data.", "startOffset": 46, "endOffset": 75}, {"referenceID": 11, "context": "There are also many other efforts in developing contrastive objectives to avoid computing partition functions (LeCun and Huang, 2005; Liang and Jordan, 2008; Vickrey, Lin, and Koller, 2010).", "startOffset": 110, "endOffset": 189}, {"referenceID": 12, "context": "There are also many other efforts in developing contrastive objectives to avoid computing partition functions (LeCun and Huang, 2005; Liang and Jordan, 2008; Vickrey, Lin, and Koller, 2010).", "startOffset": 110, "endOffset": 189}, {"referenceID": 3, "context": "Using noisy examples to guide unsupervised learning has also been pursued in deep learning (Collobert and Weston, 2008; Tamura, Watanabe, and Sumita, 2014).", "startOffset": 91, "endOffset": 155}, {"referenceID": 6, "context": "Hinton (2002) proposes contrastive divergence (CD) that compares the data distribution with reconstructions of the data vector generated by a limited number of full Gibbs sampling steps.", "startOffset": 0, "endOffset": 14}, {"referenceID": 24, "context": "3 Sampling for Structured Prediction Widely used in NLP for inference (Teh, 2006; Johnson, Griffiths, and Goldwater, 2007) and calculating expectations (DeNero, Bouchard-Cot\u0302\u00e9, and Klein, 2008), Gibbs sampling has not been used for unsupervised training of log-linear models for word alignment.", "startOffset": 70, "endOffset": 122}, {"referenceID": 24, "context": "3 Sampling for Structured Prediction Widely used in NLP for inference (Teh, 2006; Johnson, Griffiths, and Goldwater, 2007) and calculating expectations (DeNero, Bouchard-Cot\u0302\u00e9, and Klein, 2008), Gibbs sampling has not been used for unsupervised training of log-linear models for word alignment. Tamura, Watanabe, and Sumita (2014) propose a similar idea to use beam search to calculate expectations.", "startOffset": 71, "endOffset": 331}, {"referenceID": 5, "context": "Finally, it is interesting to include millions of sparse features (Dyer et al., 2011) to directly model the translation equivalence between words rather than relying on GIZA++.", "startOffset": 66, "endOffset": 85}], "year": 2014, "abstractText": "Word alignment is an important natural language processing task that indicates the correspondence between natural languages. Recently, unsupervised learning of log-linear models for word alignment has received considerable attention as it combines the merits of generative and discriminative approaches. However, a major challenge still remains: it is intractable to calculate the expectations of non-local features that are critical for capturing the divergence between natural languages. We propose a contrastive approach that aims to differentiate observed training examples from noises. It not only introduces prior knowledge to guide unsupervised learning but also cancels out partition functions. Based on the observation that the probability mass of log-linear models for word alignment is usually highly concentrated, we propose to use top-n alignments to approximate the expectations with respect to posterior distributions. This allows for efficient and accurate calculation of expectations of non-local features. Experiments show that our approach achieves significant improvements over stateof-the-art unsupervised word alignment methods.", "creator": "LaTeX with hyperref package"}}}