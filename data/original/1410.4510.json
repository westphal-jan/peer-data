{"id": "1410.4510", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Oct-2014", "title": "Graph-Sparse LDA: A Topic Model with Structured Sparsity", "abstract": "Originally designed to model text, topic modeling has become a powerful tool for uncovering latent structure in domains including medicine, finance, and vision. The goals for the model vary depending on the application: in some cases, the discovered topics may be used for prediction or some other downstream task. In other cases, the content of the topic itself may be of intrinsic scientific interest.", "histories": [["v1", "Thu, 16 Oct 2014 17:35:31 GMT  (62kb)", "https://arxiv.org/abs/1410.4510v1", null], ["v2", "Fri, 21 Nov 2014 16:38:59 GMT  (62kb)", "http://arxiv.org/abs/1410.4510v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.CL cs.LG", "authors": ["finale doshi-velez", "byron c wallace", "ryan adams"], "accepted": true, "id": "1410.4510"}, "pdf": {"name": "1410.4510.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ryan P. Adams"], "emails": ["finale@seas.harvard.edu", "byron.wallace@utexas.edu", "rpa@seas.harvard.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 0.\n45 10\nv2 [\nst at\n.M L\n] 2\n1 N\nUnfortunately, even using modern sparse techniques, the discovered topics are often difficult to interpret due to the high dimensionality of the underlying space. To improve topic interpretability, we introduce Graph-Sparse LDA, a hierarchical topic model that leverages knowledge of relationships between words (e.g., as encoded by an ontology). In our model, topics are summarized by a few latent concept-words from the underlying graph that explain the observed words. Graph-Sparse LDA recovers sparse, interpretable summaries on two real-world biomedical datasets while matching state-of-the-art prediction performance."}, {"heading": "1 Introduction", "text": "Probabilistic topic models [1, 2, 3] were originally developed to discover latent structure in unorganized text corpora, but these models have been generalized to provide a powerful and flexible framework for uncovering structure in a variety of domains including medicine, finance, and vision. In the popular Latent Dirichlet Allocation (LDA) [1] model, topics are distributions over the words in the vocabulary, and documents can then be summarized by the mixture of topics they contain. Here, a \u201cword\u201d is anything that can be counted and a \u201cdocument\u201d is an observation. LDA has been applied to diverse applications such as finding scientific topics in articles [4], classifying images [5], and recognizing human actions [6]. The modeling objective varies depending on the application. In some cases, topic models are used to provide compact summaries of documents which can then be used for downstream tasks such as prediction, classification, or recognition. In other situations, the content of the topics themselves may be of independent interest. For example, a clinician may want to understand why a certain topic within their patient\u2019s data is correlated with mortality (e.g., [7]). A geneticist, meanwhile, may wish to use topics discovered from publicly available datasets to formulate the next hypothesis to be tested in an expensive laboratory study.\nThese kinds of applications present unique challenges and opportunities for topic modeling. In the standard LDA formulation, topics are distributions over all of the words in a (usually very large) vocabulary. This vocabulary is typically assumed to be unstructured, i.e., words are not assumed to have any a priori relationship. Sparse topic models [8, 9, 10] offer a partial solution to this problem by enforcing the constraint that many of the word probabilities for a given topic should be zero. Unfortunately, when the vocabularies are large, there may still be hundreds of words with non-zero probabilities. Enforcing sparsity alone is therefore not sufficient to induce interpretable topics.\nIn this work we propose a new strategy for achieving interpretability: exploiting structured vocabularies, which exist in many specialized domains. These \u201ccontrolled structured vocabularies\u201d encode known relationships between the tokens comprising the vocabulary. For example, diseases are organized into billing hierarchies, and clinical concepts are related by directed acyclic graphs (DAGs) [11]. There are other examples as well. Keywords for biomedical\npublications are organized in a hierarchy known as MeSH [12]; searching with MeSH terms is standard practice for biomedical literature retrieval tasks. Genes are organized into pathways and interaction networks. Such structures often summarize large bodies of scientific research and human thought; a great deal of effort has gone into their construction. While these structured vocabularies are necessarily imperfect, they have the important property that they \u2014 by definition \u2014 represent how domain experts codify knowledge, and thus provide a window into how one might create models that such experts can meaningfully use and interpret. Because they were designed to be understood by humans, these structured relationships provide a form of information unique from any learned ontology.\nUnfortunately, existing topic modeling machinery is not equipped to capitalize on controlled structured vocabularies. We therefore propose a new model, Graph-Sparse LDA, that exploits DAG-structured vocabularies to induce interpretable topics that still summarize the data well. This approach is appropriate when documents come annotated with structured vocabulary terms, e.g., biomedical articles with MeSH headers, genes with known interactions, and species with known taxonomies. Graph-Sparse LDA introduces an additional layer of hierarchy into the standard LDA model: instead of topics being distributions over observed words, topics are distributions over concept-words, which then generate observed words using a noise process that is informed by the structure of the vocabulary (see example in figure 1). Using the structure of the vocabulary to guide the induced sparsity, we recover topics that are more interpretable to domain experts.\nWe demonstrate Graph-Sparse LDA on two real-world applications. The first is a collection of diagnoses for patients with autism spectrum disorder. For this we use a diagnosis hierarchy [11] to recover clinically relevant subtypes described by a small set of concepts. The second is a corpus of biomedical abstracts annotated with hierarchicallystructured Medical Subject Headings (MeSH) [12]. Here, Graph-Sparse LDA identifies meaningful, concise groupings (topics) of MeSH terms for use in biomedical literature retrieval tasks. In both cases, the topic models found by Graph-Sparse LDA have the same or better predictive performance as a state-of-the-art sparse topic model (Latent IBP compound Dirichlet Allocation [8]) while providing much sparser topic descriptions. To efficiently sample from this model, we introduce a novel inference procedure that prefers moves along manifolds of constant likelihood to identify sparse solutions."}, {"heading": "2 Graph-Sparse LDA", "text": "In this paper, our data are documents that are modeled using the \u201cbag of words\u201d representation that is common for topic models. Let the data X consist of the counts of each of the V words in the vocabulary for each of the N documents. The standard LDA model [1] posits the following generative process for the words win comprising each document (data instance) in X :\nBn \u223c Dirichlet(\u03b1B1K) Ak \u223c Dirichlet(\u03b1A1V ) (1)\nzin |Bn \u223c Discrete(Bn) win | zin, {Ak} \u223c Discrete(Azin) (2)\nwhere K is the number of topics. The rows of the N\u00d7K matrix B are the document-specific distributions over topics, and the K\u00d7V matrix A represents each topic\u2019s distribution over words. The notation Ak refers to the kth row of A and Bn is the nth row of B. The zin encode the topic to which the ith word in document n was assigned, and win\u2208 1, . . . , V is the ith word in document n. Since the words are assigned independently and identically, an N \u00d7 V matrix of how often each word occurs in each document is a sufficient statistic for the words win.\nOur Bayesian nonparametric model, Graph-Sparse LDA, builds upon a recent nonparametric extension of LDA, Latent IBP compound Dirichlet Allocation (LIDA) [8]. In addition to allowing an unbounded number of topics, LIDA introduces sparsity over both the document-topic matrixB and the topic-word matrixA using a three-parameter Indian Buffet Process. The prior expresses a preference for describing each document with few topics and each topic with few words. We extend LIDA by assuming that words in our document belong to a structured vocabulary with known relationships that form a tree or DAG, and that nearby groups of terms\u2014as defined with respect to the graph structure\u2014are associated with specific phenomena. For example, in a biomedical ontology, nodes on one sub-tree may correspond to a particular virus (e.g., HIV) and a different sub-tree may describe a specific drug or treatment (e.g., anti-retrovirals) used to treat HIV. Papers investigating anti-retrovirals for treatment of HIV would then tend to have terms drawn from both sub-trees. Intuitively, we would like to uncover these sub-trees as the concepts underpinning a topic.\nUsing concept-words to summarize the words in a topic is natural in many scenarios because structured vocabularies are often both very specific and inconsistently annotated. For example, a trial may be annotated with the term antiviral agents or its child anti-retroviral agents. Thus, from a generative modeling perspective, nearby words in the vocabulary can be thought of as having been produced from the same core concept. Our model posits that a topic is made up of a sparse set of concept-words that can explain words that are its ancestors or descendants (see Figure 1). Formally, we replace the previous LDA generative process with the following process that introduces w\u0303in as the concept word behind observed word win:\n\u03c0k \u223c IBP-Stick(\u03b3B) \u03c1v \u223c Beta(\u03b3A/V, 1) (3)\nB\u0304n |\u03c0k \u223c Bernoulli(\u03c0k) A\u0304k | \u03c1v \u223c Bernoulli(\u03c1v) (4)\nBn | B\u0304n \u223c Dirichlet(B\u0304n \u2299 \u03b1B1K) Ak | A\u0304k \u223c Dirichlet(A\u0304k \u2299 \u03b1A1V ) (5)\nzin |Bn \u223c Discrete(Bn) w\u0303in | zin, {Ak} \u223c Discrete(Azin) (6)\nPv \u223c Dirichlet(Ov \u2299 \u03b1P 1V ) win | w\u0303in, P \u223c Discrete(Pw\u0303in) (7)\nwhere \u2299 is the element-wise Hadamard product and IBP is the Indian Buffet Process [13]. As in the standard LDA model, the document-topic matrix B represents the distribution of topics in each document. However, Bn is now masked according to a document-specific vector B\u0304n, which is the nth row of a matrix B\u0304 that is itself drawn from an IBP with concentration parameter \u03b3B . Thus B\u0304nk is 1 if topic k has nonzero probability in documentn and 0 otherwise. Similarly, the topic-concept matrix A and the binary topic-concept mask matrix A\u0304 represent the topic matrix and its sparsity pattern, except that now A and A\u0304 represent the relationship between topics and concept-words. The priors over the document-topic and topic-concept matrices B and A (and their respective masks B\u0304 and A\u0304) follow those in LIDA [8].\nThe concept-word matrix P describes distributions over words for each concept. The form of the ontology O determines the sparsity pattern of P : we use the notation Ow to refer to a binary vector of length V that is 1 if the concept-word w\u0303 is a descendant or ancestor of observed word w and 0 otherwise. We illustrate these sparsity constraints in Figure 2, where the dark-shaded concept nodes 1, 2, and 3 can each only explain themselves, and words that are are ancestors or descendants. The brown and green nodes are ancestor observed words that are shared by more than one concept word.\nIntuitively, the concept-word matrix P can be viewed as allowing for variation in the process of assigning terms to documents (citations, diagnoses, etc.) on behalf of domain experts. For example, if a document is about antiretroviral agents, an annotator may describe the document with a key-word nearby in the vocabulary, such as antiviral agents, rather than the more specific term. Similarly, a primary care physician using the hierarchy in Figure 1 may note that a patient has epilepsy since he is not an expert in neurological disorders, while a specialist might bill for the more specific term Convulsive Epilepsy, Intractable. More generally, the concept-word matrix P can be thought of as describing a neighborhood of words that could be covered by the same concept. Introducing this additional layer of\nhierarchy allows us to find very sparse topic-concept matrices A that still explain a large number of observed words. (Note that setting P = IV recovers LIDA from Graph-Sparse LDA; Graph-Sparse LDA is therefore a generalization of LIDA that allows for much more structure.)\nFinally, let the data X be an N \u00d7 V matrix of counts, where Xnw is the number of times word w appears in document n. The log-likelihood of the data given B, A, and P is given by\nlog p(X |A,B, P ) = \u2211\nn,w\u0303\nXnw\u0303 log(BnAPw\u0303) (8)"}, {"heading": "3 Inference", "text": "We describe a blocked-Gibbs procedure for sampling B, B\u0304, A, A\u0304, and P as well as an additional Metropolis-Hastings (MH) procedure that helps the sampler to move toward sparser topic-concept word matrices A. Specifically, our MH proposal distribution is designed to prefer proposals of new A\u2032 and P \u2032 such that the overall likelihood does not significantly change. To our knowledge, MCMC that uses moves that result in near-constant likelihood to encourage large changes in the prior is a novel approach. We first describe how to resample instantiated parameters of the Graph-Sparse LDA model and then describe how we sample new topics."}, {"heading": "3.1 Blocked Gibbs Sampling", "text": "Our blocked Gibbs sampling procedure relies on first sampling two intermediate assignment tensors. The first, CNKV counts how often the word v is assigned to topic k in document n. The second,CKV V counts how often each observed word is assigned to each concept word in topic k. These two tensors are sampled as follows:\nCount Tensors CNKV and CKV V : The probability that an observed word w belongs to topic k is given by pnkw = Bnk \u2211V w\u0303=1Akw\u0303Pw\u0303w where the sum over w\u0303 marginalizes out potential concept-words. Thus we can use a multinomial distribution to allocate the counts Xnw across the K topics via [CNKV ]n:w \u223c Mult(pn:w, Xnw), where we use \u201c:\u201d to indicate a tensor slice. For updating CKV V , the probability that w\u0303 was the generating concept word, given the observed wordw and the topic k, is given by pkw\u0303w \u221d Akw\u0303Pw\u0303w. Thus we can sample the count tensorCKV V using the multinomial\n[CKV V ]k:w \u223c Mult(pk:w, \u2211 n[CKNV ]nkw)\nNote that given the topic assignment for an observed word, we do not need to know from which document it came to determine the distribution over concept-word assignments. Thus, we never need to consider a four-way {document, topic, concept-word, word} count tensor during inference.\nDocument-Topic Assignments B and B\u0304: Given the count tensor CNKV , we can sample the sparsity mask B\u0304 by marginalizing out B and the \u03c0k using the formula derived in [8]. First, we note that if \u2211V w=1[CNKV ]nkw > 0, then B\u0304nk must be 1 because there exists at least one word assigned to topic k in document n. Let B\u0304(nk=0) denote the matrix B\u0304, but with entry B\u0304nk = 0. If \u2211V w=1[CNKV ]nkw = 0, then the probability \u03c6nk that B\u0304nk = 1 is given by\n\u03c8nk =\n[\n1 + B(\u03b1BB\u0304\n(nk=0) n , \u03b1B)(N \u2212 B\u0304 (nk=0) k )\nB(\u03b1BB\u0304 (nk=0) n + \u2211V w=1Xnw, \u03b1B)B\u0304 (nk=0) k\n]\u22121\nwhere B(\u00b7, \u00b7) is the Beta function, B\u0304(nk=0)n \u2261 \u2211 k\u2032 B\u0304 (nk=0) nk\u2032 , and B\u0304 (nk=0) k \u2261 \u2211 n\u2032 B\u0304 (nk=0) n\u2032k . Once we have resampled B\u0304, we resample B using\nBn \u223c Dirichlet(B\u0304n \u2299 (\u03b1B + \u2211V w=1[CNKV ]n:w)).\nTopic-Concept Word Assignments A and A\u0304: As with B\u0304, we marginalize out A and \u03c1v when sampling A\u0304. If \u2211V\nw=1[CKV V ]kw\u0303w > 0, then at least one observed word was assigned to topic k and concept-word w\u0303, and therefore A\u0304kw\u0303 = 1. Let A\u0304(kw\u0303=0) be the same as matrix A\u0304, but with entry A\u0304kw\u0303 = 0. If \u2211V w=1[CKV V ]kw\u0303w = 0, then the probability \u03c6kw\u0303 that A\u0304kw\u0303 = 1 is given by\n\u03c6kw\u0303 =\n[\n1 + B(\u03b1AA\u0304\n(kw\u0303=0) k , \u03b1A)(K \u2212 A\u0304 (kw\u0303=0) w\u0303 ))\n(B(\u03b1AA\u0304 (kw\u0303=0) k + \u2211 w\u0303 \u2211 w[CKV V ]kw\u0303w, \u03b1A)A\u0304 (kw\u0303=0) w\u0303\n]\u22121\nwhereB is the Beta function,K is the number of instantiated topics, A\u0304(kw\u0303=0)k \u2261 \u2211 w\u0303\u2032 A\u0304 (kw\u0303=0) kw\u0303\u2032 , and A\u0304 (kw\u0303=0) w\u0303 \u2261 \u2211 k\u2032 A\u0304 (kw\u0303=0) k\u2032w\u0303 . Once we have resampled A\u0304, we resample A via\nAk \u223c Dirichlet(A\u0304k \u2299 (\u03b1A + \u2211 w[CKV V ]k:w))\nConcept Word-Word Distributions P : Finally, the concept word to observed word distributions can be resampled via\nPw\u0303 \u223c Dirichlet((Ow\u0303 > 0)\u2299 (\u03b1P + \u2211 k[CKV V ]kw\u0303:))."}, {"heading": "3.2 MH Moves for Improved Sparsity", "text": "Recall that one of our modeling objectives is to identify a small, interpretable set of concept-words in each topic. To this end, we have placed a sparsity-inducing prior onA. While the Gibbs sampling procedure above is computationally straightforward, it often does not give us the desired sparsity in A fast enough. Mixing is slow because the only time we set A\u0304kw\u0303 = 0 is when no counts of w\u0303 are assigned to topic k across any of the documents. When there are many documents, reaching zero counts is unlikely, and thus the sampler is slow to sparsify the topic-concept word matrix A.1\nWe introduce an MH procedure to encourage moves of the topic concept-word matrix A in directions of greater sparsity through joint moves on both A and P . Given a proposal distribution Q(A\u2032, P \u2032 |A,P ), the acceptance ratio for an MH procedure is given by\naMH = 1 \u2227 p(X |B,A\u2032, P \u2032) p(A\u2032) p(P \u2032)Q(A,P |A\u2032, P \u2032)\np(X |B,A, P ) p(A) p(P )Q(A\u2032, P \u2032 |A,P )\nThe sparsity-inducing prior on A will prefer topic-concept word matrices A\u2032 that have more zeros. However, as with all Bayesian models (and seen in our case in Equation 8), when the data get large, the likelihood term p(X |B,A, P ) will dominate the prior terms p(A) and p(P ).\nTo allow for moves toward greater sparsity, our MH proposal uses two core ideas. First, we use the form of the ontology to propose intelligent split-merge moves forA\u2032. Second, we attempt to make a move that keeps the likelihood as constant as possible by proposing a P \u2032 such that AP = A\u2032P \u2032. Thus, the prior terms p(A) and p(P ) will have a larger influence on the move. The form of Q(A\u2032, P \u2032 |A,P ) is as follows:\n\u2022 Q(A\u2032 |A,P ): We choose a random topic k and concept word w\u0303. LetDw\u0303 denote the set of concept words that are descendants of w\u0303 (including w\u0303). With probability psplit, we sample a random vector r from Dirichlet(1|Dw\u0303|) and create a newA\u2032k withA \u2032 kw\u0303 = 0 andA \u2032 kw\u0303\u2032 = Akw\u0303\u2032 + rAkw\u0303 , \u2200w\u0303\n\u2032 \u2208 Dw\u0303. Otherwise, we perform the mergeA\u2032kw\u0303\u2032 = 0, \u2200w\u0303\u2032 \u2208 Dw\u0303, and A\u2032kw\u0303 = \u2211 w\u0303\u2032\u2208Dw\u0303 Akw\u0303\u2032 . This split-merge move corresponds to adjusting probabilities in a subgraph of the ontology, with the merge move corresponding to moving all the mass to a single node.\n\u2022 Q(P \u2032 |A\u2032, A, P ): Let P \u22c6 be the solution to the optimization problem min P\u0302 ||AP \u2212A\u2032P\u0302 ||2F , where F denotes\nthe Frobenius norm, with the constraints that each row of P \u22c6 must lie on the simplex and respect the ontology O. This optimization can be solved as a quadratic program with linear constraints. We then sample each row of the proposal P \u2032 according to P \u2032v \u223c Dirichlet(\u03b2MHP \u22c6 v ). We find in practice that \u03b2MH generally needs to be large in order to propose appropriately conservative moves.\n1We focus on A in this section because we found that B\u0304 is faster to mix; each document may not have many words. However, a similar approach could be used to sparsify B as well.\nWhile this procedure can still propose moves over the entire parameter space (thus guaranteeing Harris recurrence on the appropriate stationary distribution corresponding to the prior), it guarantees visits to sparse, high-likelihood solutions with high probability."}, {"heading": "3.3 Adding and Deleting Topics", "text": "Finally, we describe how the number of topics in the data set is automatically learned. First, we remove any topics that are unused (that is, \u2211\nn B\u0304nk = 0). To propose new topics, we first choose a random document n. We propose a new A\u2032k\u2032 from the prior and propose that B\u0304 \u2032 nk\u2032 = 1. Finally, we propose a newB\u2032n \u223c Dirichlet(\u03b1B(B\u0304\u2032n \u2299 \u2211\nw[CNKV ]n:w)). The acceptance probability for adding the new topic Ak\u2032 is given by\n\u03b1add = 1 \u2227 p(Xn |B\n\u2032 n, A \u2032, P ) p(B\u2032n) \u03b3A N\np(Xn |Bn, A, P ) p(Bn)\nwhere the \u03b3A N term comes from the probability of adding exactly one new topic in the IBP prior."}, {"heading": "4 Results", "text": "We demonstrate the ability of our Graph-Sparse LDA model to find interpretable, predictive topics on one toy example and two real-world examples from biomedical domains. In each case we compare our model with the state-of-theart Bayesian nonparametric topic modeling approach LIDA [8]. We focus on LIDA because it subsumes two other popular sparse topic models, the focused topic model [9] and sparse topic model [14], and because the proposed model is a generalization of LIDA.\nAll samplers were run for 250 iterations. The topic matrix product AP was initialized using an LDA tensor decomposition [15] and then factored into A and P using an alternating minimization to find a sparse A that enforced the simplex and ontology constraints. These initialization procedures reduced the burn-in time. Finally, a random 1% of each data-set was held out to compute predictive log-likelihoods.\nDemonstration on a Toy Problem We first considered a toy problem with a 31-word vocabulary arranged in a binary tree (see Figure 2). There were three underlying topics, each with only a single concept (the three darker nodes in Figure 2, labeled 1, 2, and 3). Each row in the matrix Pw\u0303 uniformly distributed 10% of its probability mass to the ancestors of each concept word and 90% of its probability mass to the concept word\u2019s descendants (including itself). Each initialization of the problem had a randomly generated document-topic matrix comprising 1000 documents.\nFigures 3a and 3d show the difference in the held-out test likelihoods for the final 50 samples over 20 independent instantiations of the toy problem. The difference in held-out test likelihoods is skewed positive, implying that GraphSparse LDA makes somewhat better predictions than LIDA. More importantly, Graph-Sparse LDA also recovers a much sparser matrixA, as can be seen in figure 3d. We note, of course, that Graph-Sparse LDA has an additional layer of structure that allows for a very sparse topic concept-word matrix A; LIDA does not have access to the ontology information O. The important point is that by incorporating this available controlled structured vocabulary into our model, we find a solution with similar or better predictive performance than state-of-the-art models with the additional benefit of a much more interpretable structure.\nPatterns of Co-Occurring Diagnoses in Autism Spectrum Disorder Autism Spectrum Disorder (ASD) is a complex, heterogenous disease that is often accompanied by many co-occurring conditions such as epilepsy and intellectual disability. We consider a set of 3804 patients with 3626 different diagnoses where the datum Xnw corresponds to the number of times patient n received diagnosis w during the first 15 years of life.2 Diagnoses are organized in a tree-structured hierarchy known as ICD-9CM [11]. Diagnoses higher up in the hierarchy are less specific (such as \u201cDiseases of the Central Nervous System\u201d or \u201cEpilepsy with Recurrent Seizures,\u201d as opposed to \u201cEpilepsy, Unspecified, without mention of intractable epilepsy\u201d). Clinicians may encode a diagnosis at any level of the hierarchy, including less specific ones.\n2The Internal Review Board of the Harvard Medical School approved this study.\nFigure 3b shows the difference in test log-likelihood between Graph-Sparse LDA and LIDA over 5 independent runs, divided by the overall mean test-likelihood value. While less pronounced than in the toy example, Graph-Sparse LDA still has slightly better predictive performance\u2014certainly on par with current state-of-the-art topic modeling. However, the use of the ontology again allows for much sparser topics, as seen in Figure 3e. In this application, the topics correspond to possible subtypes in ASD. Being able to concisely summarize them is the first step toward using the output of this model for future clinical research.\nFinally, Table 1 shows an example of one topic recovered by Graph-Sparse LDA and its corresponding topic discovered by LIDA. While the corresponding topic in LIDA has very similar diagnoses, using the hierarchy allows for Graph-Sparse LDA to summarize most of the probability mass in this topic in 6 concept words rather than 119 words. This topic\u2014which shows a connection between the more severe form of ASD, intellectual disability, and epilepsy\u2014as well as the other topics, matched recently published clinical results on ASD subtypes [16].\nMedical Subject Headings for Biomedical Literature The National Library of Medicine maintains a controlled structured vocabulary of Medical Subject Headings (MeSH) [12]. These terms are hierarchical: terms near the root are more general than those further down the tree. For example, cardiovascular diseases subsumes heart diseases, which is in turn a parent of Heart Aneurysm.\nThese MeSH terms are useful for searching the biomedical literature. For example, when conducting a systematic review (SR) [17], one looks to summarize the totality of the published evidence pertaining to a precise clinical question. Identifying this evidence in the literature is a time-consuming, expensive and tedious endeavor; computational methods\nfor reducing the labor involved in this process have therefore been investigated [18, 19]. MeSH terms are helpful annotations for facilitating literature screening for systematic reviews, as they can help researchers undertaking a review quickly decide if articles are relevant to their query or not.\nHowever, MeSH terms are manually assigned to articles by a small group of annotators. Thus, there is inherent variability in the specificity of the terms assigned to articles. This variability can make leveraging the terms difficult. Graph-Sparse LDA provides a means of identifying latent concepts that define distributions over terms nearby in the MeSH structure. These interpretable, sparse topics can provide concise summaries of biomedical documents, thus easing the evidence retrieval process for overburdened physicians.\nWe consider a dataset of 1218 documents annotated with 5347 unique MeSH terms (23 average terms per document) that were screened for a systematic review of the effects of calcium-channel blocker (CCB) drugs [18]. In figure 3c, we see that the test log-likelihood for Graph-Sparse LDA on these data is on par with LIDA, while producing a much sparser summary of concept-words (figure 3f). Here, the concepts found by Graph-Sparse LDA correspond to sets of MeSH terms that might help researchers rapidly identify studies reporting results for trials investigating the use of CCB\u2019s\u2014without having to make sense of a topic comprising hundreds of unique MeSH terms.\nTable 2 shows the top concept-words in a sample topic discovered by Graph-Sparse LDA compared to a similar topic discovered by LIDA. Graph-Sparse LDA gives most of topic mass to double-blind trials and CCBs; knowing the relative prevalence in an article of this topic would clearly help a researcher looking to find reports of randomized controlled trials of CCBs. In contrast, words related to concept CCBs are divided among terms in LIDA. Some of the LIDA terms, such as Drug Therapy, Combination and Mibefradil are also present in Graph-Sparse LDA, but with\nmuch lower probability \u2013 the concept CCB summarizes most of the instances. We note that a professional systematic reviewer at [Anonymous] confirmed that the more concise topics found by Graph-Sparse LDA would be more useful in facilitating evidence retrieval tasks than those found by LIDA."}, {"heading": "5 Discussion and Related Work", "text": "Topic models [1, 2] have gained wide popularity as a flexible framework for uncovering latent structure in corpora. Existing topic models have typically assumed that observed words are unstructured. By contrast, here we have considered scenarios in which these words are drawn from a known underlying structure (such as an ontology).\nPrior work in interpretable topic models has focused on various notions of coherence. [20] introduced the idea of \u201cintrusion detection\u201d where they hypothesized that a more coherent, or interpretable, topic would be one where a human annotator would be able to identify an inserted \u201cintruder\u201d word among the top 5 words in a topic; [21] automated this process. Contrary to expecation, they found that interpretability (as rated by human annotators) was negatively correlated with test likelihood. [22] and [23] developed measures of topic coherence that strongly correlated with human annotations of topic quality.\nHowever, the evaluations in all of these works still focus only on the top n words in a topic (which powerfully indicates how linked sparsity is to interpretability; humans have trouble working with long lists). In contrast, our approach does not sacrifice on predictive quality and, by using the ontological structure, provides a compact summary that describes most of the words, not just the top n. This quality is particularly valuable in the kinds of scenarios that we described, where annotation disagreement or diagnostic \u201cslosh\u201d can result in a large number of words with non-trivial probabilities.\nThis use of a human-provided structure to induce interpretability also distinguishes Graph-Sparse LDA from other hierarchical and tree structured topic models where the structure is typically learned. For example, [24] use a nested Chinese Restaurant Process to learn hierarchies of topics where subtopics are more specific than their parents. [25] expand on this idea with a nonparametric Markov model that allows a subtopic to have multiple parents. [26] develop inference techniques for sparse versions of these tree-structured topic models. Learned hierarchies have also been used to capture correlations between topics, such as [27]. In all of these models, the learned hiearchical structure allows for various kinds of statistical sharing between the topics. However, each topic is still a distribution over a large vocabulary, and the interpretation task is only complicated by requiring a human to now both inspect the hierarchy and the topics for structure.\nAmong the fully unsupervised approaches, the closest to our work is the super-word concept modeling of [28], which uses a nested Beta process to describe a document with a sparse set of super-words, or concepts, each of which are associated with a sparse set of words. Known auxiliary information about the words, encoded in a feature vector,\ncan be used to encourage or discourage words from being part of the same concept. A key difference in our approach is that we use the graph structure to guide the formation of concepts, which maintains interpretability while removing the need for each concept to have a sparse set of words. Our graph-structured relationships also result in a much simpler inference procedure.\nWhile not applied to increase interpretability, expert-defined hierarchies have been used in topic models in other contexts. Early work by [29] used hierarchies for word-sense disambiguation in n-gram tuples. This idea was later incorporated into a topic modeling context by [30]. Other work has used hierarchical structure as partial supervision to improve topic-modeling output in scenarios in which some words come from controlled vocabularies (or have known relationships) and others do not. [31] consider representing the content of website summaries via a hierarchical model. Their approach exploits ontological structure by jointly modeling word and ontology term generation. They showed that their model (which leverages the hierarchical document labels) improved on existing approaches with respect to perplexity. [32] use Dirichlet forest priors to enforce expert-provided \u201cmust be in same topic\u201d and \u201ccannot be in same topic\u201d constraints between words. Finally, [33] propose a hierarchically supervised LDA model where there is a hierarchy on the document labels (rather than on the vocabulary). Specifically, they treat categories as \u2018labels\u2019 and model the assignment of these to documents via (probit) regression models. Their model stipulates that when a node is assigned to a category so too are all of its parents, thus capturing the hierarchical structure. In contrast to all of these works, which focus on prediction tasks, Graph-Sparse LDA uses the ontology in a probabilistic \u2014 rather than enforced \u2014 manner to obtain sparse topics from extant controlled vocabularies.\nWe note that our word generation model is much more general than other approaches. Here we have considered scenarios in which the ontological structure allows a concept-word to generate words that are its descendants and ancestors. However, we can imagine that a concept-word can generate any nearby observed word, where the definition of \u201cnearby\u201d is entirely up to the model-designer (concretely, \u201cnearby\u201d corresponds to the sparsity pattern in the matrix P ). This difference allows for much more flexibility in modeling: the underlying structure can be a tree, a DAG, or just some collection of neighborhoods. At the same time, our formulation results in a Gibbs sampling procedure that is simpler than many other hierarchical models."}, {"heading": "6 Conclusions", "text": "Topic models have revolutionized prediction and classification models in many domains, and many scientists are now attempting to use them to uncover structure from their data. For these applications, however, prediction is not enough: scientists wish to be able to understand the structure in order to posit new theories. At the same time, structured knowledge-bases often exist for scientific domains; these are information dense resources that capture a wealth of expertise.\nIn this paper we have proposed a model that exploits such resources to achieve the stated aim of identifying interpretable topics. More specifically, we have described a novel Bayesian nonparametric model, Graph-Sparse LDA, that leverages existing controlled vocabulary structures to induce interpretable topics. The Bayesian nonparametric aspect of the model allows us to discover the number of topics in our dataset. Leveraging ontological knowledge allows us to uncover sparse sets of concept words that provide succinct, interpretable topic summaries that maintain the ability to explain a large number of observed words. The combination of this representational power and an efficient inference procedure allowed us to realize topic interpretability while still matching (and often exceeding) state-of-the-art predictive performance.\nWhile we have focused on controlled vocabularies in the biomedical domain, this approach could be more generally applied to text corpora using standard hierarchies such as WordNet [34]. In these more general domains, using hierarchies could eliminate the need for basic pre-processing such as stemming. This model is relatively straightforward to implement, and we expect it to be useful for a variety of topic or factor-discovery applications where the observed dimensions have some human-understandable relationships."}, {"heading": "Acknowledgments", "text": "We are grateful to Isaac Kohane and the i2b2 team at Boston Children\u2019s Hospital for providing us the autism data and their feedback on the GS-LDA model as a data-mining tool."}], "references": [{"title": "Latent Dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Probabilistic topic models", "author": ["M. Steyvers", "T. Griffiths"], "venue": "Handbook of latent semantic analysis, vol. 427, no. 7, pp. 424\u2013440, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Probabilistic topic models", "author": ["D.M. Blei"], "venue": "Communications of the ACM, vol. 55, no. 4, pp. 77\u201384, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, vol. 101, pp. 5228\u20135235, 2004.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "A Bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "CVPR, vol. 2, pp. 524\u2013531, 2005.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Human action recognition by semilatent topic models", "author": ["Y. Wang", "G. Mori"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 10, pp. 1762\u20131774, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Topic models for mortality modeling in intensive care units", "author": ["M. Ghassemi", "T. Naumann", "R. Joshi", "A. Rumshisky"], "venue": "ICML 2012 Machine Learning for Clinical Data Analysis Workshop, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Latent IBP compound Dirichlet allocation", "author": ["C. Archambeau", "B. Lakshminarayanan", "G. Bouchard"], "venue": "NIPS Bayesian Nonparametrics Workshop, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "The IBP compound Dirichlet process and its application to focused topic modeling", "author": ["S. Williamson", "C. Wang", "K.A. Heller", "D.M. Blei"], "venue": "ICML, pp. 1151\u20131158, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse additive generative models of text", "author": ["J. Eisenstein", "A. Ahmed", "E.P. Xing"], "venue": "ICML, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "The unified medical language system (UMLS): integrating biomedical terminology", "author": ["O. Bodenreider"], "venue": "Nucleic acids research, vol. 32, pp. D267\u2013D270, 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Medical subject headings (MeSH)", "author": ["C.E. Lipscomb"], "venue": "Bull Med Libr Assoc., 2000. 88(3): 265266.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "The Indian buffet process: An introduction and review", "author": ["T. Griffiths", "Z. Ghahramani"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 1185\u20131224, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Decoupling Sparsity and Smoothness in the Discrete Hierarchical Dirichlet Process", "author": ["C. Wang", "D. Blei"], "venue": "Advances in Neural Information Processing Systems 22 (Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, eds.), pp. 1982\u20131989, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1982}, {"title": "Comorbidity clusters in autism spectrum disorders: An electronic health record time-series analysis", "author": ["F. Doshi-Velez", "Y. Ge", "I. Kohane"], "venue": "Pediatrics, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Effect of clinical guidelines on medical practice: a systematic review of rigorous evaluations", "author": ["J.M. Grimshaw", "I.T. Russell"], "venue": "The Lancet, vol. 342, no. 8883, pp. 1317\u20131322, 1993. 11", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1993}, {"title": "Reducing workload in systematic review preparation using automated citation classification", "author": ["A.M. Cohen", "W.R. Hersh", "K. Peterson", "P.-Y. Yen"], "venue": "Journal of the American Medical Informatics Association, vol. 13, no. 2, pp. 206\u2013219, 2006.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Active learning for biomedical citation screening", "author": ["B.C. Wallace", "K. Small", "C.E. Brodley", "T.A. Trikalinos"], "venue": "KDD, pp. 173\u2013182, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Reading tea leaves: How humans interpret topic models", "author": ["J. Chang", "J.L. Boyd-Graber", "S. Gerrish", "C. Wang", "D.M. Blei"], "venue": "NIPS, pp. 288\u2013296, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality", "author": ["J.H. Lau", "D. Newman", "T. Baldwin"], "venue": "14th Conference of the European Chapter of the Association for Computational Linguistics (EACL), 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic evaluation of topic coherence", "author": ["D. Newman", "J.H. Lau", "K. Grieser", "T. Baldwin"], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT \u201910, pp. 100\u2013108, Association for Computational Linguistics, 2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimizing semantic coherence in topic models", "author": ["D. Mimno", "H. Wallach", "E. Talley", "M. Leenders", "A. McCallum"], "venue": "EMNLP, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "The nested Chinese restaurant process and Bayesian nonparametric inference of topic hierarchies", "author": ["D.M. Blei", "T.L. Griffiths", "M.I. Jordan"], "venue": "Journal of the ACM, vol. 57, no. 2, pp. 7:1\u20137:30, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Topic modeling with nonparametric Markov tree", "author": ["H. Chen", "D.B. Dunson", "L. Carin"], "venue": "ICML, pp. 377\u2013384, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient tree-based topic modeling", "author": ["Y. Hu", "J. Boyd-Graber"], "venue": "ACL, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Pachinko allocation: DAG-structured mixture models of topic correlations", "author": ["W. Li", "A. McCallum"], "venue": "ICML, pp. 577\u2013584, 2006.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Concept modeling with superwords.", "author": ["K. El-Arini", "E.B. Fox", "C. Guestrin"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Hiding a semantic hierarchy in a Markov model", "author": ["S. Abney", "M. Light"], "venue": "Workshop on Unsupervised Learning in Natural Language Processing, pp. 1\u20138, 1999.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "A topic model for word sense disambiguation", "author": ["J.L. Boyd-Graber", "D.M. Blei", "X. Zhu"], "venue": "EMNLP-CoNLL, pp. 1024\u20131033, 2007.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Tree labeled LDA: A hierarchical model for web summaries", "author": ["A. Slutsky", "X. Hu", "Y. An"], "venue": "IEEE International Conference on Big Data, pp. 134\u2013140, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Incorporating domain knowledge into topic modeling via Dirichlet forest priors", "author": ["D. Andrzejewski", "X. Zhu", "M. Craven"], "venue": "ICML, pp. 25\u201332, 2009.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Hierarchically supervised latent Dirichlet allocation", "author": ["A.J. Perotte", "F. Wood", "N. Elhadad", "N. Bartlett"], "venue": "NIPS, pp. 2609\u20132617, 2011.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Wordnet: A lexical database for English", "author": ["G.A. Miller"], "venue": "Communications of the ACM, vol. 38, pp. 39\u201341, 1995. 12", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": "Probabilistic topic models [1, 2, 3] were originally developed to discover latent structure in unorganized text corpora, but these models have been generalized to provide a powerful and flexible framework for uncovering structure in a variety of domains including medicine, finance, and vision.", "startOffset": 27, "endOffset": 36}, {"referenceID": 1, "context": "Probabilistic topic models [1, 2, 3] were originally developed to discover latent structure in unorganized text corpora, but these models have been generalized to provide a powerful and flexible framework for uncovering structure in a variety of domains including medicine, finance, and vision.", "startOffset": 27, "endOffset": 36}, {"referenceID": 2, "context": "Probabilistic topic models [1, 2, 3] were originally developed to discover latent structure in unorganized text corpora, but these models have been generalized to provide a powerful and flexible framework for uncovering structure in a variety of domains including medicine, finance, and vision.", "startOffset": 27, "endOffset": 36}, {"referenceID": 0, "context": "In the popular Latent Dirichlet Allocation (LDA) [1] model, topics are distributions over the words in the vocabulary, and documents can then be summarized by the mixture of topics they contain.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "LDA has been applied to diverse applications such as finding scientific topics in articles [4], classifying images [5], and recognizing human actions [6].", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "LDA has been applied to diverse applications such as finding scientific topics in articles [4], classifying images [5], and recognizing human actions [6].", "startOffset": 115, "endOffset": 118}, {"referenceID": 5, "context": "LDA has been applied to diverse applications such as finding scientific topics in articles [4], classifying images [5], and recognizing human actions [6].", "startOffset": 150, "endOffset": 153}, {"referenceID": 6, "context": ", [7]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": "Sparse topic models [8, 9, 10] offer a partial solution to this problem by enforcing the constraint that many of the word probabilities for a given topic should be zero.", "startOffset": 20, "endOffset": 30}, {"referenceID": 8, "context": "Sparse topic models [8, 9, 10] offer a partial solution to this problem by enforcing the constraint that many of the word probabilities for a given topic should be zero.", "startOffset": 20, "endOffset": 30}, {"referenceID": 9, "context": "Sparse topic models [8, 9, 10] offer a partial solution to this problem by enforcing the constraint that many of the word probabilities for a given topic should be zero.", "startOffset": 20, "endOffset": 30}, {"referenceID": 10, "context": "For example, diseases are organized into billing hierarchies, and clinical concepts are related by directed acyclic graphs (DAGs) [11].", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "publications are organized in a hierarchy known as MeSH [12]; searching with MeSH terms is standard practice for biomedical literature retrieval tasks.", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "For this we use a diagnosis hierarchy [11] to recover clinically relevant subtypes described by a small set of concepts.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "The second is a corpus of biomedical abstracts annotated with hierarchicallystructured Medical Subject Headings (MeSH) [12].", "startOffset": 119, "endOffset": 123}, {"referenceID": 7, "context": "In both cases, the topic models found by Graph-Sparse LDA have the same or better predictive performance as a state-of-the-art sparse topic model (Latent IBP compound Dirichlet Allocation [8]) while providing much sparser topic descriptions.", "startOffset": 188, "endOffset": 191}, {"referenceID": 0, "context": "The standard LDA model [1] posits the following generative process for the words win comprising each document (data instance) in X :", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "Our Bayesian nonparametric model, Graph-Sparse LDA, builds upon a recent nonparametric extension of LDA, Latent IBP compound Dirichlet Allocation (LIDA) [8].", "startOffset": 153, "endOffset": 156}, {"referenceID": 12, "context": "where \u2299 is the element-wise Hadamard product and IBP is the Indian Buffet Process [13].", "startOffset": 82, "endOffset": 86}, {"referenceID": 7, "context": "The priors over the document-topic and topic-concept matrices B and A (and their respective masks B\u0304 and \u0100) follow those in LIDA [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 7, "context": "Document-Topic Assignments B and B\u0304: Given the count tensor CNKV , we can sample the sparsity mask B\u0304 by marginalizing out B and the \u03c0k using the formula derived in [8].", "startOffset": 165, "endOffset": 168}, {"referenceID": 7, "context": "In each case we compare our model with the state-of-theart Bayesian nonparametric topic modeling approach LIDA [8].", "startOffset": 111, "endOffset": 114}, {"referenceID": 8, "context": "We focus on LIDA because it subsumes two other popular sparse topic models, the focused topic model [9] and sparse topic model [14], and because the proposed model is a generalization of LIDA.", "startOffset": 100, "endOffset": 103}, {"referenceID": 13, "context": "We focus on LIDA because it subsumes two other popular sparse topic models, the focused topic model [9] and sparse topic model [14], and because the proposed model is a generalization of LIDA.", "startOffset": 127, "endOffset": 131}, {"referenceID": 10, "context": "2 Diagnoses are organized in a tree-structured hierarchy known as ICD-9CM [11].", "startOffset": 74, "endOffset": 78}, {"referenceID": 14, "context": "This topic\u2014which shows a connection between the more severe form of ASD, intellectual disability, and epilepsy\u2014as well as the other topics, matched recently published clinical results on ASD subtypes [16].", "startOffset": 200, "endOffset": 204}, {"referenceID": 11, "context": "Medical Subject Headings for Biomedical Literature The National Library of Medicine maintains a controlled structured vocabulary of Medical Subject Headings (MeSH) [12].", "startOffset": 164, "endOffset": 168}, {"referenceID": 15, "context": "For example, when conducting a systematic review (SR) [17], one looks to summarize the totality of the published evidence pertaining to a precise clinical question.", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "for reducing the labor involved in this process have therefore been investigated [18, 19].", "startOffset": 81, "endOffset": 89}, {"referenceID": 17, "context": "for reducing the labor involved in this process have therefore been investigated [18, 19].", "startOffset": 81, "endOffset": 89}, {"referenceID": 16, "context": "We consider a dataset of 1218 documents annotated with 5347 unique MeSH terms (23 average terms per document) that were screened for a systematic review of the effects of calcium-channel blocker (CCB) drugs [18].", "startOffset": 207, "endOffset": 211}, {"referenceID": 0, "context": "Topic models [1, 2] have gained wide popularity as a flexible framework for uncovering latent structure in corpora.", "startOffset": 13, "endOffset": 19}, {"referenceID": 1, "context": "Topic models [1, 2] have gained wide popularity as a flexible framework for uncovering latent structure in corpora.", "startOffset": 13, "endOffset": 19}, {"referenceID": 18, "context": "[20] introduced the idea of \u201cintrusion detection\u201d where they hypothesized that a more coherent, or interpretable, topic would be one where a human annotator would be able to identify an inserted \u201cintruder\u201d word among the top 5 words in a topic; [21] automated this process.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] introduced the idea of \u201cintrusion detection\u201d where they hypothesized that a more coherent, or interpretable, topic would be one where a human annotator would be able to identify an inserted \u201cintruder\u201d word among the top 5 words in a topic; [21] automated this process.", "startOffset": 245, "endOffset": 249}, {"referenceID": 20, "context": "[22] and [23] developed measures of topic coherence that strongly correlated with human annotations of topic quality.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] and [23] developed measures of topic coherence that strongly correlated with human annotations of topic quality.", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "For example, [24] use a nested Chinese Restaurant Process to learn hierarchies of topics where subtopics are more specific than their parents.", "startOffset": 13, "endOffset": 17}, {"referenceID": 23, "context": "[25] expand on this idea with a nonparametric Markov model that allows a subtopic to have multiple parents.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] develop inference techniques for sparse versions of these tree-structured topic models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Learned hierarchies have also been used to capture correlations between topics, such as [27].", "startOffset": 88, "endOffset": 92}, {"referenceID": 26, "context": "Among the fully unsupervised approaches, the closest to our work is the super-word concept modeling of [28], which uses a nested Beta process to describe a document with a sparse set of super-words, or concepts, each of which are associated with a sparse set of words.", "startOffset": 103, "endOffset": 107}, {"referenceID": 27, "context": "Early work by [29] used hierarchies for word-sense disambiguation in n-gram tuples.", "startOffset": 14, "endOffset": 18}, {"referenceID": 28, "context": "This idea was later incorporated into a topic modeling context by [30].", "startOffset": 66, "endOffset": 70}, {"referenceID": 29, "context": "[31] consider representing the content of website summaries via a hierarchical model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32] use Dirichlet forest priors to enforce expert-provided \u201cmust be in same topic\u201d and \u201ccannot be in same topic\u201d constraints between words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "Finally, [33] propose a hierarchically supervised LDA model where there is a hierarchy on the document labels (rather than on the vocabulary).", "startOffset": 9, "endOffset": 13}, {"referenceID": 32, "context": "While we have focused on controlled vocabularies in the biomedical domain, this approach could be more generally applied to text corpora using standard hierarchies such as WordNet [34].", "startOffset": 180, "endOffset": 184}], "year": 2014, "abstractText": "Originally designed to model text, topic modeling has become a powerful tool for uncovering latent structure in domains including medicine, finance, and vision. The goals for the model vary depending on the application: in some cases, the discovered topics may be used for prediction or some other downstream task. In other cases, the content of the topic itself may be of intrinsic scientific interest. Unfortunately, even using modern sparse techniques, the discovered topics are often difficult to interpret due to the high dimensionality of the underlying space. To improve topic interpretability, we introduce Graph-Sparse LDA, a hierarchical topic model that leverages knowledge of relationships between words (e.g., as encoded by an ontology). In our model, topics are summarized by a few latent concept-words from the underlying graph that explain the observed words. Graph-Sparse LDA recovers sparse, interpretable summaries on two real-world biomedical datasets while matching state-of-the-art prediction performance.", "creator": "LaTeX with hyperref package"}}}