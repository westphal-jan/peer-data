{"id": "1410.5518", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Oct-2014", "title": "On Symmetric and Asymmetric LSHs for Inner Product Search", "abstract": "In a recent manuscript (\"Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS)\", available on arXiv and to be presented in the upcoming NIPS), Shrivastava and Li argue that there is no symmetric LSH for the problem of Maximum Inner Product Search and propose an asymmetric LSH based on different mappings for query and database points. We show a simple LSH for the problem, using a simple symmetric mapping, with better performance, both theoretically and empirically.", "histories": [["v1", "Tue, 21 Oct 2014 02:00:34 GMT  (129kb)", "http://arxiv.org/abs/1410.5518v1", "9 pages"], ["v2", "Fri, 24 Oct 2014 21:31:06 GMT  (161kb)", "http://arxiv.org/abs/1410.5518v2", "14 pages"], ["v3", "Mon, 8 Jun 2015 19:30:35 GMT  (169kb)", "http://arxiv.org/abs/1410.5518v3", "11 pages, 3 figures, In Proceedings of The 32nd International Conference on Machine Learning (ICML)"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "stat.ML cs.DS cs.IR cs.LG", "authors": ["behnam neyshabur", "nathan srebro"], "accepted": true, "id": "1410.5518"}, "pdf": {"name": "1410.5518.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Behnam Neyshabur"], "emails": ["bneyshabur@ttic.edu", "nati@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 0.\n55 18\nv1 [\nst at\n.M L\n] 2\n1 O\nct 2\n01 4"}, {"heading": "1 Introduction", "text": "Locality Sensitive Hashing [6] is a popular tool for approximate nearest neighbor search and is also widely used in different settings [1, 3, 4]. LSH is based on a random mapping from objects to a small, possibly binary, alphabet, which can in turn be used to generate short hash words such that hamming distances between hash words correspond to similarity between objects. Recent studies have also explored the power of asymmetry in LSH and binary hashing, where two different mappings are used to approximate similarity [7, 8].\nShrivastava and Li [9] consider the problem of Maximum Inner Product Search (MIPS) where similarity between vectors is given by the unnormalized inner product between them. They show that there is no symmetric LSH for this similarity measure, per the standard definition of an LSH, and propose two distinct mappings, one of database objects and the other for queries, which yields an asymmetric LSH for the problem.\nAlthough as mentioned above, an asymmetric LSH can be useful in many cases, here we show that MIPS is not one of them. We present a simple parameter-free symmetric hash for MIPS. We argue that this hash constitutes an LSH when the query vectors are normalized, as in Shrivastava and Li, and that there is no need for an asymmetric hash. Our proposed LSH also allows for a better hashing parameter \u03c1, and by replicating the experiments of Shrivastava and Li, we show that it also attains better empirical performance."}, {"heading": "2 Problem Formulation and Background", "text": "In Maximum Inner Product Search (MIPS), we are given a \u201cdatabase\u201d of vectors S \u2282 Rd and a query point q \u2208 Rd and the goal is to find a point p \u2208 S that maximizes the inner product q\u22a4p:\np = argmax x\u2208S\nq\u22a4x\nAs in Shrivastava and Li [9], we assume the following without loss of generality:\n\u2013 The query q is normalized: Since given a vector q, the norm \u2016q\u20162 does not affect the argmax, we can assume \u2016q\u20162 = 1 always. \u2013 The database vectors are bounded inside the unit sphere: We assume \u2016x\u20162 \u2264 1 for all x \u2208 S. Otherwise we can rescale all vectors without changing the argmax.\nWe would like to find a good \u201clocality sensitive hash\u201d for MIPS. Formally, given an alphabet \u0393 , a hash H of objects X is a random mapping from X to \u0393 , i.e. a distribution over functions h : X \u2192 \u0393 . The hash H is sometimes thought of as a \u201cfamily\u201d of functions, where the uniform distribution over the family is implicit.\nGiven a similarity function sim : X \u00d7 X \u2192 R, such as inner product similarity sim(x, y) = x\u22a4y, Shrivastava and Li define an LSH as follows1:\nDefinition 1 (Locality Sensitive Hashing (LSH)). A hash H is said to be a (S, cS, p1, p2)-LSH for a similarity function sim : X \u00d7 X \u2192 R over X if for any x, y \u2208 X :\n\u2013 if sim(x, y) \u2265 S then Ph\u2208H[h(x) = h(y)] \u2265 p1, \u2013 if sim(x, y) \u2264 cS then Ph\u2208H[h(x) = h(y)] \u2264 p2.\nHere S > 0 is a threshold of interest, and for efficient approximate nearest neighbor search, we need p1 > p2 and c < 1. In particular, given a (S, cS, p1, p2)-sensitive hash H, a data structure for finding Ssimilar objects when cS-similar objects exist in the database can be constructed in time O(n\u03c1 log n) and space O(n1+\u03c1) where \u03c1 = log p1log p2 . This quantity \u03c1 is therefor of particular interest, as we are interested in an LSH with minimum possible \u03c1, and we refer to it as the hashing ratio.\nAs discussed above, in the MIPS problem, we make different assumptions about the database vectors and query. In particular, we assume the query is normalized to unit norm, but only assume boundedness, not normalization, for the database vectors in S. It therefor makes sense to modify the definition of an LSH accordingly:\nDefinition 2 (Locality Sensitive Hashing (LSH)\u2013Asymmetric Spaces). A hash H is said to be a (S, cS, p1, p2)-LSH for a similarity function sim : X \u00d7Y \u2192 R over a pair of spaces X and Y if for any x \u2208 X and y \u2208 Y:\n\u2013 if sim(x, y) \u2265 S then Ph\u2208H[h(x) = h(y)] \u2265 p1, \u2013 if sim(x, y) \u2264 cS then Ph\u2208H[h(x) = h(y)] \u2264 p2.\nThe hash H should map objects in both X and Y, i.e. be defined over X \u222a Y (or a superset).\nIn Definition 2, the hash itself is still symmetric, i.e. the same function h is applied to both x and y. The only asymmetry is in the problem definition, since we want the property to hold for x and y in different spaces. This should be contrasted with a truly asymmetric hash, where two different functions are used, one for each space. Formally, an asymmetric hash for a pair of spaces X and Y is a joint distribution over pairs of mappings (f, g), f : X \u2192 \u0393 , g : Y \u2192 \u0393 . The asymmetric hashes we consider are specified by a pair of deterministic mappings P : X \u2192 Z and Q : Y \u2192 Z and a single random mapping (i.e. distribution over functions) h : Z \u2192 \u0393 , where f(x) = h(P (x)) and g(y) = h(Q(y)). Given a similarity function sim : X\u00d7Y \u2192 R we define2:\nDefinition 3 (Asymmetric Locality Sensitive Hashing (ALSH)). An asymmetric hash is said to be an (S, cS, p1, p2)-ALSH for a similarity function sim : X \u00d7 Y \u2192 R over X ,Y if for any x \u2208 X and y \u2208 Y:\n\u2013 if sim(x, y) \u2265 S then P(f,g)[f(x) = g(y)] \u2265 p1, \u2013 if sim(x, y) \u2264 cS then P(f,g)[f(x) = g(y)] \u2264 p2.\nShrivastava and Li considered the problem of finding an LSH for MIPS, i.e. for the inner product similarity sim(x, y) = x\u22a4y. They first observe that for any S > 0, 0 < c < 1, p1 > p2, there is no symmetric hash that is an (S, cS, p1, p2)-LSH for sim(x, y) = x\n\u22a4y over X = RD by Definition 1. To address this they suggest an asymmetric hash based on the following pair of mappings:\nP (x) = [x; \u2016x\u201622 ; \u2016x\u2016 4 2 ; . . . ; \u2016x\u2016\n2m 2 ]\nQ(y) = [y; 1/2; 1/2; . . . ; 1/2], (1)\n1 This is a formalization of the definition given by Shrivastava and Li, which in turn is a modification of the definition of LSH for distance functions [6]. Note that even though inner product similarity could be negative, this definition is only concerned with positive similarities. 2 This is a formalization of the definition given by Shrivastava and Li, where we have made the distinction between the spaces X and Y, and the quantifiers on x, y, explicit. We also distinguish between an asymmetric hash and an asymmetric notion of an LSH.\nwhere m \u2265 1 is an integer parameter, combined with the standard L2 hash function\nhL2a,b(x) =\n\u230a a\u22a4x+ b\nr\n\u230b\n(2)\nwhere a \u223c N (0, I) is a spherical multi-Gaussian random vector, b \u223c U(0, r) is a uniformly distributed random variable on [0, r], and r \u2208 R is a parameter. We know that for any x, y \u2208 R, the collision probability of the hash hL2a,b can be written as [3]:\nP [ hL2a,b(x) = h L2 a,b(y) ]\n= 1\u2212 2\u03a6(\u2212r/\u03b4)\u2212 1\u2212 e \u2212(r/\u03b4)2/2\n\u221a 2\u03c0(r/\u03b4) = Fr(\u03b4) (3)\nwhere \u03b4 = \u2016x\u2212 y\u20162 and \u03a6(x) = \u222b x \u2212\u221e 1\u221a 2\u03c0 e\u2212x 2/2dx is the cumulative density function of standard normal distribution.\nThe alphabet \u0393 used is the integers, and for x, q \u2208 Rd the intermediate space is Z = Rd+m. To solve the MIPS problem, queries are first normalized such that \u2016q\u20162 = 1, and database vectors are scaled such that \u2016x\u20162 \u2264 U where U < 1 is a parameter. The asymmetric hash is then given by (f(x), g(q)) = (hL2a,b(P (x)), h L2 a,b(Q(q))).\nShrivastava and Li establish that for any 0 < c < 1 and 0 < S < 1, there exists 0 < U < 1, r > 0, m \u2265 1, and p1 > p2 such that (f(x), g(q)) is an (S0, cS0, p1, p2)-ALSH over X = {x|\u2016x\u20162 \u2264 U} and Y = {q|\u2016q\u20162 = 1} with S0 = SU . Note that the threshold S0 must also be scaled by U when we scale x such that \u2016x\u20162 \u2264 U . The more relevant measure is therefor the scale-invariant threshold S = S0/U . Another way to think about this is to assume without loss of generality that \u2016x\u20162 \u2264 1 and we are interested in the threshold S, and then allow scaling by U as part of the mapping. I.e. we consider the mapping\nP\u0303 (x) = [x/U ; \u2016x/U\u201622 ; \u2016x/U\u2016 4 2 ; . . . ; \u2016x/U\u2016\n2m 2 ] (4)\nand the corresponding asymmetric hash\n(f\u0303(x), g(q)) = (hL2a,b(P\u0303 (x)), h L2 a,b(Q(q))), (5)\nwith Q(q) as in (1). We refer to this hash as mips-alsh.\nIn terms of mips-alsh, we have that for any 0 < c < 1 and 0 < S < 1, there exists 0 < U < 1, r > 0, m \u2265 1, and p1 > p2 such that mips-alsh specified in (5) is an (S, cS, p1, p2)-ALSH over X = {x|\u2016x\u20162 \u2264 1} and Y = {q|\u2016q\u20162 = 1}. Stated this way, the problem is fixed (\u2016q\u20162 = 1, \u2016x\u20162 \u2264 1) and the mapping is parameterized bym,U and r. Shrivastava and Li furthermore calculate the hashing parameter \u03c1 as a function of m,U and r, and numerically find the optimal \u03c1 over a grid of possible values for m,U and r."}, {"heading": "3 Symmetric LSH for MIPS", "text": "The claim that there is no hash that fulfills Definition 1 for a symmetric LSH for inner product similarity over X = Rd is indeed true. However, this is because of the insistence that the hashing property holds for all x, q \u2208 Rd. But actually, since the queries are normalized and the database objects are bounded and could be scaled without loss of generality, we are only interested in how the hashing behaves for x \u2208 X = {x|\u2016x\u20162 \u2264 1} and q \u2208 Y = {q|\u2016q\u20162 = 1}. Indeed, the main result of Shrivastava and Li establishing the existence of an ALSH only applies to this pair of database and query spaces\u2014mips-alsh is not an ALSH over the entire space Rd (i.e. when queries are un-normalized and the database unbounded). But as we argue here, there do exist symmetric hashes that fulfill Definition 2 for inner product similarity over X = {x|\u2016x\u20162 \u2264 1} and Y = {q|\u2016q\u20162 = 1}. That is, we do need to consider the hashing property asymmetrically, with the database and query constrained in different ways, as in Definitions 2 and 3 and the analysis of Shrivastava and Li. But the same hash function can be used for both the database and the queries and there is no need for two different hash functions or two different mappings P (\u00b7) and Q(\u00b7)."}, {"heading": "3.1 The Standard LSH", "text": "We first point out that after scaling the database by a small enough U , no further asymmetric mapping is needed, and the standard L2 hash given by (2) is already an LSH for inner product similarity:\nTheorem 1. For any 0 < S < 1, 0 < c < 1 and U \u2264 2S(1 \u2212 c), there exists p1 > p2 such that the hash hL2a,b given in (2) is a (S0, cS0, p1, p2)-LSH for inner product similarity over X = {x|\u2016x\u20162 \u2264 U} and Y = {q|\u2016q\u20162 = 1} (under Definitions 2) where S0 = US.\nThe hash suggested here is exactly mips-alsh, but with m = 0, i.e. without the asymmetric mappings P (\u00b7), Q(\u00b7) of (1). Phrased in terms of mips-alsh, Theorem 1 states that a choice of m = 0 is always sufficient, as long as U is small enough (though, as we see later, this choice might lead to inferior hashing ratios and thus inferior hashes).\nProof. First, let\u2019s consider the case when q\u22a4x \u2265 S0 = SU . Since Fr is a monotonically decreasing function, we have:\nP [ hL2a,b(q) = h L2 a,b(x) ] = Fr (\n\u221a\n1\u2212 2q\u22a4x+ \u2016x\u201622 )\n\u2265 Fr (\n\u221a\n1\u2212 2SU + U2 )\nIf q\u22a4x < cS0 = cSU , we define \u03b1 = q\u22a4x cSU \u2264 1. So we have that\nP [ hL2a,b(q) = h L2 a,b(x) ] = Fr (\n\u221a\n1\u2212 2q\u22a4x+ \u2016x\u201622 )\n\u2264 Fr ( \u221a 1\u2212 2\u03b1cSU + (\u03b1cSU)2 )\nagain because Fr is a decreasing function, it is enough to prove that: \u221a\n1\u2212 2SU + U2 < \u221a 1\u2212 2\u03b1cSU + (\u03b1cSU)2\nwhich in turn is equivalent to: U2 \u2212 2SU < (\u03b1cSU)2 \u2212 2\u03b1cSU.\nRearranging, we have the following condition on U :\nU < 2S(1\u2212 \u03b1c) 1\u2212 (\u03b1cS)2\nwhich holds when U \u2264 2S(1\u2212 c) as postulated."}, {"heading": "3.2 SIMPLE-LSH", "text": "Although we argue in the previous Section that even the standard Euclidean LSH is an LSH for inner product similarity, its hashing parameter \u03c1 might be quite large for many thresholds S (see Section 4). Furthermore, ensuring the standard Euclidean LHS is an LSH requires scaling by a small enough U , and the resulting hashing depends on two parameters, namely U and r. Instead, we propose here a simpler, parameter-free, fully symmetric LSH, that does not require special pre-scaling, and which we call simple-lsh.\nFor x \u2208 Rd, \u2016x\u20162 \u2264 1, define P (x) \u2208 Rd+1 as follows:\nP (x) = [ x; \u221a\n1\u2212 \u2016x\u201622 ]\n(6)\nClearly, with this definition for any x with \u2016x\u20162 \u2264 1 we have \u2016P (x)\u20162 = 1. Moreover, for any query q, since \u2016q\u20162 = 1, we have:\nP (q)\u22a4P (x) = [ q; 0 ]\u22a4[ x; \u221a\n1\u2212 \u2016x\u201622 ] = q\u22a4x (7)\nNow, to define the hash simple-lsh, take a spherical random vector a \u223c N (0, I) (with each component an i.i.d. standard normal), and consider the following random mapping into the binary alphabet \u0393 = {\u00b11}:\nha(x) = sign(a \u22a4P (x)). (8)\nTheorem 2. For any 0 < S < 1 and 0 < c < 1, there exists p1 > p2 such that simple-lsh given in (8) is a (S, cS, p1, p2)-LSH for inner product similarity over X = {x|\u2016x\u20162 \u2264 1} and Y = {q|\u2016q\u20162 = 1} (under Definitions 2). Furthermore,\n\u03c1 = log p1 log p2 =\nlog\n(\n1\u2212 cos\u22121(S)\n\u03c0\n)\nlog\n(\n1\u2212 cos \u22121(cS) \u03c0\n) .\nProof. For any x \u2208 X and q \u2208 Y we have [5]:\nP[ha(P (q)) = ha(P (x))] = 1\u2212 \u03b8(P (q), P (x))\n\u03c0\n= 1\u2212 cos \u22121(q\u22a4x)\n\u03c0\nTherefore:\n\u2013 if q\u22a4x \u2265 S, then P [ ha(P (q)) = ha(P (x)) ] \u2265 1\u2212 cos \u22121(S)\n\u03c0\n\u2013 if q\u22a4x \u2264 cS, then P [ ha(P (q)) = ha(P (x)) ] \u2264 1\u2212 cos \u22121(cS)\n\u03c0\nSince for any 0 \u2264 x \u2264 1, 1\u2212 cos \u22121(x) \u03c0 is a monotonically increasing function, this gives us an LSH."}, {"heading": "4 Theoretical Comparison", "text": "Earlier we discussed that an LSH with the smallest possible hashing ratio \u03c1 is desirable. For mips-alsh, for each desired threshold S and ratio c, one can optimize over the three parameters m,U and r to find the hash with the best \u03c1. This optimal \u03c1 guarantee is given by [9]3:\n\u03c1\u2217mips-alsh(S, c) = min U,r,m\nlogFr ( \u221a 1 +m/4\u2212 2SU + U2m+1 )\nlogFr ( \u221a 1 +m/4\u2212 2cSU + (cSU)2m+1 )\n(9)\ns.t. U2 m \u2264 2S(1\u2212 c)\nm \u2265 1, 0 < r, 0 < U. This is a non-convex optimization problem, and Shrivastava and Li [9] solve it through a grid search over U, r and m and report \u03c1\u2217mips-alsh for several choices of S and c.\nFor L2-lsh the parameters U and r need to be optimized to obtain the minimal \u03c1 \u2217 L2-LSH\nfor each choice of S and c. We solve the following optimization by a grid search over U and r:\n\u03c1\u2217L2-lsh(S, c) = min0<r 0<U\nlogFr (\u221a 1\u2212 2SU + U2 )\nlogFr ( 1\u2212 cSU ) (10)\ns.t. U \u2264 2S(1\u2212 c)\nFor simple-lsh however, no parameters need to be tuned, and for each S, c the hashing parameter \u03c1\u2217 SIMPLE-ALSH\nis given by Theorem 2. In Figure 1 we compare the optimal hashing parameter \u03c1\u2217 for the three methods, for different values of S and c. It is clear that the simple-lsh dominates the other methods. For thresholds S far away from 1 (the typically interesting regime), even though the standard L2-lsh is a valid LSH, mips-alsh indeed yields significantly better hash ratios (though simple-lsh dominates both).\n3 This is a slight tightening of the analysis of Shrivastava and Li\u2014a careful analysis shows that an additional term\nof (cSU)2 m+1 can be added to the denominator expression, compared to the expression of Shrivastava and Li."}, {"heading": "5 Empirical Evaluation", "text": "We followed the exact same protocol as Shrivastava and Li [9] comparing mips-alsh to simple-lsh and L2-lsh on two collaborative filtering datasets, Netflix and Movielens 10M.\nFor a given user-item matrix Z, we followed the pureSVD procedure suggested in [2]: we first subtracted the overall average rating from each individual rating and created the matrix Z with these average-subtracted ratings for observed entries and zeros for unobserved entries. We then take a rank-f approximation (top f singular components, f = 150 for Movielens and f = 300 for Netflix) Z \u2248 W\u03a3R\u22a4 = Y and define L = W\u03a3 so that Y = LR\u22a4. We can think of each row of L as the vector presentation of a user and each row of R as the presentation for an item.\nThe database S consists of all rows Rj of R (corresponding to movies) and we use each row Li of L (corresponding to a user i) as a query. That is, for each user i we would like to find the top T movies, i.e. the T movies with highest \u3008Li, Rj\u3009, for different values of T .\nTo do so, for each hash family, we generate hash codes of length K for all movies and a random selection of 2000 users (queries). For each user, we sort movies in ascending order of hamming distance between the user hash and movie hash, breaking up ties randomly. For each of several values of T we calculate precision-recall curves for recalling the top T movies, averaging the precision-recall values over the 2000 randomly selected users.\nIn Figures 2, 3, we plot precision-recall curves of retrieving top T items by hash code of length K for Netflix and Movielens datasets where T \u2208 {1, 5, 10} and K \u2208 {64, 128, 256, 512}. For mips-alsh we used m = 3, U = 0.83, r = 2.5, suggested by the authors and used in their empirical evaluation. simple-lsh does not require any parameters. For L2-lsh we use U = 0.2, r = 2, which worked fairly well in all experiments."}, {"heading": "6 Conclusion", "text": "Contrary to what is suggested by [9], we showed that an inherently symmetric LSH can be used for MIPS. Our suggested simple-lsh is arguably simpler, symmetric, parameter-free, uses only a binary alphabet, and yields better theoretical and empirical performance compared to mips-alsh."}], "references": [{"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Performance of recommender algorithms on top-n recommendation tasks", "author": ["P. Cremonesi", "Y. Koren", "R. Turrin"], "venue": "In Proceedings of the fourth ACM conference on Recommender systems, ACM p", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["M. Datar", "N. Immorlica", "P. Indyk", "S.V. Mirrokni"], "venue": "In Proc. 20th SoCG pp", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "VLDB 99,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["M.X. Goemans", "D.P. Williamson"], "venue": "Journal of the ACM (JACM) 42.6,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "STOC pp", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Clustering, hamming embedding, generalized lsh and the max norm", "author": ["B. Neyshabur", "Y. Makarychev", "N. Srebro"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "The power of asymmetry in binary hashing", "author": ["B. Neyshabur", "P. Yadollahpour", "Y. Makarychev", "R. Salakhutdinov", "N. Srebro"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips)", "author": ["A. Shrivastava", "P. Li"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "1 Introduction Locality Sensitive Hashing [6] is a popular tool for approximate nearest neighbor search and is also widely used in different settings [1, 3, 4].", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "1 Introduction Locality Sensitive Hashing [6] is a popular tool for approximate nearest neighbor search and is also widely used in different settings [1, 3, 4].", "startOffset": 150, "endOffset": 159}, {"referenceID": 2, "context": "1 Introduction Locality Sensitive Hashing [6] is a popular tool for approximate nearest neighbor search and is also widely used in different settings [1, 3, 4].", "startOffset": 150, "endOffset": 159}, {"referenceID": 3, "context": "1 Introduction Locality Sensitive Hashing [6] is a popular tool for approximate nearest neighbor search and is also widely used in different settings [1, 3, 4].", "startOffset": 150, "endOffset": 159}, {"referenceID": 6, "context": "Recent studies have also explored the power of asymmetry in LSH and binary hashing, where two different mappings are used to approximate similarity [7, 8].", "startOffset": 148, "endOffset": 154}, {"referenceID": 7, "context": "Recent studies have also explored the power of asymmetry in LSH and binary hashing, where two different mappings are used to approximate similarity [7, 8].", "startOffset": 148, "endOffset": 154}, {"referenceID": 8, "context": "Shrivastava and Li [9] consider the problem of Maximum Inner Product Search (MIPS) where similarity between vectors is given by the unnormalized inner product between them.", "startOffset": 19, "endOffset": 22}, {"referenceID": 8, "context": "2 Problem Formulation and Background In Maximum Inner Product Search (MIPS), we are given a \u201cdatabase\u201d of vectors S \u2282 R and a query point q \u2208 R and the goal is to find a point p \u2208 S that maximizes the inner product q\u22a4p: p = argmax x\u2208S q\u22a4x As in Shrivastava and Li [9], we assume the following without loss of generality: \u2013 The query q is normalized: Since given a vector q, the norm \u2016q\u20162 does not affect the argmax, we can assume \u2016q\u20162 = 1 always.", "startOffset": 264, "endOffset": 267}, {"referenceID": 5, "context": "; 1/2], (1) 1 This is a formalization of the definition given by Shrivastava and Li, which in turn is a modification of the definition of LSH for distance functions [6].", "startOffset": 165, "endOffset": 168}, {"referenceID": 2, "context": "We know that for any x, y \u2208 R, the collision probability of the hash h2 a,b can be written as [3]:", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "For any x \u2208 X and q \u2208 Y we have [5]: P[ha(P (q)) = ha(P (x))] = 1\u2212 \u03b8(P (q), P (x)) \u03c0 = 1\u2212 cos \u22121(q\u22a4x) \u03c0 Therefore: \u2013 if q\u22a4x \u2265 S, then P [", "startOffset": 32, "endOffset": 35}, {"referenceID": 8, "context": "This optimal \u03c1 guarantee is given by [9]: \u03c1mips-alsh(S, c) = min U,r,m logFr ( \u221a 1 +m/4\u2212 2SU + U2m+1 )", "startOffset": 37, "endOffset": 40}, {"referenceID": 8, "context": "This is a non-convex optimization problem, and Shrivastava and Li [9] solve it through a grid search over U, r and m and report \u03c1mips-alsh for several choices of S and c.", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "We followed the exact same protocol as Shrivastava and Li [9] comparing mips-alsh to simple-lsh and L2-lsh on two collaborative filtering datasets, Netflix and Movielens 10M.", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "For a given user-item matrix Z, we followed the pureSVD procedure suggested in [2]: we first subtracted the overall average rating from each individual rating and created the matrix Z with these average-subtracted ratings for observed entries and zeros for unobserved entries.", "startOffset": 79, "endOffset": 82}, {"referenceID": 8, "context": "5 as suggested by [9] and for L2-lsh, we fix U = 0.", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": "6 Conclusion Contrary to what is suggested by [9], we showed that an inherently symmetric LSH can be used for MIPS.", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "5 as suggested by [9] and for L2-lsh, we fix U = 0.", "startOffset": 18, "endOffset": 21}], "year": 2017, "abstractText": "In a recent manuscript (\u201cAsymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS)\u201d, available on arXiv and to be presented in the upcoming NIPS), Shrivastava and Li argue that there is no symmetric LSH for the problem of Maximum Inner Product Search and propose an asymmetric LSH based on different mappings for query and database points. We show a simple LSH for the problem, using a simple symmetric mapping, with better performance, both theoretically and empirically.", "creator": "LaTeX with hyperref package"}}}