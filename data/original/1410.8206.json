{"id": "1410.8206", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2014", "title": "Addressing the Rare Word Problem in Neural Machine Translation", "abstract": "Neural Machine Translation (NMT) has recently attracted a lot of attention due to the very high performance achieved by deep neural networks in other domains. An inherent weakness in existing NMT systems is their inability to correctly translate rare words: end-to-end NMTs tend to have relatively small vocabularies with a single \"unknown-word\" symbol representing every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement a simple technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to output, for each OOV word in the target sentence, its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT'14 English to French translation task show that this simple method provides a substantial improvement over an equivalent NMT system that does not use this technique. The performance of our system achieves a BLEU score of 36.9, which improves the previous best end-to-end NMT by 2.1 points. Our model matches the performance of the state-of-the-art system while using three times less data.", "histories": [["v1", "Thu, 30 Oct 2014 00:20:31 GMT  (26kb)", "http://arxiv.org/abs/1410.8206v1", null], ["v2", "Fri, 31 Oct 2014 19:44:50 GMT  (26kb)", "http://arxiv.org/abs/1410.8206v2", null], ["v3", "Tue, 9 Dec 2014 23:11:46 GMT  (27kb)", "http://arxiv.org/abs/1410.8206v3", "Results updated. The best system now achieves 37.5 BLEU"], ["v4", "Sat, 30 May 2015 19:57:28 GMT  (31kb)", "http://arxiv.org/abs/1410.8206v4", "ACL 2015 camera-ready version"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["thang luong", "ilya sutskever", "quoc v le", "oriol vinyals", "wojciech zaremba"], "accepted": true, "id": "1410.8206"}, "pdf": {"name": "1410.8206.pdf", "metadata": {"source": "CRF", "title": "Addressing the Rare Word Problem in Neural Machine Translation", "authors": ["Thang Luong"], "emails": ["lmthang@stanford.edu", "ilyasu@google.com", "qvl@google.com", "vinyals@google.com", "woj.zaremba@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 0.\n82 06\nv1 [\ncs .C\nNeural Machine Translation (NMT) has recently attracted a lot of attention due to the very high performance achieved by deep neural networks in other domains. An inherent weakness in existing NMT systems is their inability to correctly translate rare words: end-to-end NMTs tend to have relatively small vocabularies with a single \u201cunknown-word\u201d symbol representing every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement a simple technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to output, for each OOV word in the target sentence, its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT\u201914 English to French translation task show that this simple method provides a substantial improvement over an equivalent NMT system that does not use this technique. The performance of our system achieves a BLEU score of 36.9, which improves the previous best end-to-end NMT by 2.1 points. Our model matches the performance of the state-of-the-art system while using three times less data."}, {"heading": "1 Introduction", "text": "Deep Neural Networks (DNNs) have achieved excellent results on speech recognition [11], visual object recognition [15], and other challenging tasks, so there has been much interest in applying them to natural language processing (NLP) problems as well. Among the important NLP tasks, machine translation (MT) is one where DNNs are likely to achieve strong results because of the availability of large parallel corpora.\nMachine translation is challenging because the space of possible translations for a given source sentence is vast. For more than a decade, the standard MT approach [14] has been subject to intensive\n\u2217Work done while the author was in Google. \u2020 indicates equal contribution.\nresearch and resulted in better systems over time [13, 4, 3, 8]. However, this comes at the cost of having a complex pipeline with many subcomponents that need to be tuned jointly, making it difficult to improve upon existing systems. In recent years, MT researchers have been seeking to incorporate neural network models into the standard pipeline as an additional subcomponent [20, 21, 23, 6]. At heart, these systems use phrase tables and thus rely primarily on small contexts during the translation process.\nLately, there have been a number of attempts to develop a purely neural machine translation system (NMT) [12, 5, 2, 22]. NTM systems should eventually outperform the best standard systems because neural networks scale well with larger models and generalize to word sequences that do not appear in the training set. In addition, NMT systems are easy to train with backpropagation and their decoder is easy to implement, unlike the highly intricate decoders used by the phrase-based systems [13]. NMT systems also use minimal domain knowledge, which makes them applicable to any other problem that can be formulated as mapping a sequence to another sequence [22].\nA major limitation of existing NMTs is their use of a fixed modest-sized vocabulary. NMT systems are completely incapable of translating rare words, as they use a single <unk> symbol to represent all out-of-vocabulary (OOV) words, as illustrated in Figure 1. Empirically, both Sutskever et al. [22] and Bahdanau et al. [2] have observed that sentences with many rare words tend to be translated much more poorly than sentences containing mainly frequent words. Standard phrase-based systems, on the other hand, suffer less from the rare word problem because they can afford a much larger vocabulary, and because of their use of explicit alignments and phrase counts allows them to memorize the translations of even extremely rare words.\nMotivated by the strengths of the standard phrase-based system, we propose and implement a simple approach to address the rare word problem of NMTs. Our approach augments the training data with alignment information that allows the NMT system to emit, for each OOV word, a \u201cpointer\u201d to its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates the OOV words using a dictionary or with the identity translation (if no translation is found).\nOur experiments confirm that this approach is effective. On the English to French WMT\u201914 translation task, this approach provides an improvement of more than 2 BLEU points over an equivalent NMT system that does not use this technique. Moreover, our system achieves 36.9 BLUE points, matching the performance of the state-of-the-art system [7] while using three times less data. Our system improves upon the previous best NMT system by 2.1 BLEU points.\nen: The ecotax portico in Pont-de-Buis , . . . [truncated] . . . , was taken down on Thursday morning\nfr: Le portique e\u0301cotaxe de Pont-de-Buis , . . . [truncated] . . . , a e\u0301te\u0301 de\u0301monte\u0301 jeudi matin\nnn: Le <unk> de <unk> a\u0300 <unk> , . . . [truncated] . . . , a e\u0301te\u0301 pris le jeudi matin\n\u271f\u271f \u271f\u271f \u274d\u274d \u274d\u274d \u2746 \u2746\n\u2745 \u2745 \u2702 \u2702\n\u2711 \u2711\u2711 \u271f\u271f \u271f\u271f\nFigure 1: Example of the rare word problem \u2013 An English source sentence (en), a human translation to French (fr), and a translation produced by one of our neural network systems (nn) before handling OOV words. We highlight words that are unknown to our model. The token <unk> indicates an OOV word. We also show a few important alignments between the pair of sentences."}, {"heading": "2 Neural Machine Translation", "text": "A neural machine translation system is any neural network that maps a source sentence, s1, . . . , sn, to a target sentence, t1, . . . , tm, where all sentences are assumed to terminate with a special \u201cend-ofsentence\u201d token <eos>. More concretely, an NMT system uses a neural network to parameterize\nthe conditional distributions p(tj |t<j , s\u2264n) (1)\nfor 1 \u2264 j \u2264 m. By doing so, it becomes possible to compute and therefore maximize the log probability of the target sentence given the source sentence\nlog p(t|s) =\nm\u2211\nj=1\nlog p (tj |t<j , s\u2264n) (2)\nThere are many ways to parameterize these conditional distributions. For example, Kalchbrenner et al. [12] used a combination of a convolutional neural network and a recurrent neural network, Sutskever et al. [22] used a large and deep Long Short-Term Memory (LSTM) model, Cho et al. [5] used an architecture similar to the LSTM, and Bahdanau et al. [2] used a more elaborate neural network architecture that uses an attentional mechanism over the input sequence, similarly to Graves [9] and Graves et al. [10].\nIn this work, we use the exact model of Sutskever et al. [22], which has a large deep LSTM to encodue the input sequence and a separate deep LSTM to produce a translation from the input sequence. The encoder reads the source sentence, one word at a time, and produces a large hidden state that represents the entire source sentence. The decoder is initialized from that final hidden state and generates a target translation, one word at a time, until the end-of-sentence symbol <eos> is emitted.\nDespite the relatively large amount of work done on pure neural machine translation systems, there has been no work addressing the OOV problem in NMT systems."}, {"heading": "3 Rare Word Models", "text": "To address the rare word problem discussed in Section 1, we train our neural machine translation system to track the source of the unknown words in the target sentences. If we knew the source word that is responsible for each unknown target word, we could introduce a post-processing step that would replace each <unk> in the system\u2019s output with a translation of its source word, using either a dictionary or the identity translation. For example, in Figure 1, if the model knows that the second unknown token in the NMT (line nn) originates from the source word ecotax, it can perform a word dictionary lookup to replace that unknown token by e\u0301cotaxe. Similarly, an identity translation of the source word Pont-de-Buis can be applied to the third unknown token.\nWe present three annotation strategies that can easily be applied to any NMT system. We treat the NMT system [12, 22, 5] as a black box and train it on a dataset annotated with alignment information specified by one of the models below. Such alignment data can be obtained from a parallel corpus using an unsupervised aligner. From the alignment links, we construct a word dictionary that will be used for the word translations in the post-processing step. If a word does not appear in our dictionary, then we apply the identity translation.\nThe first part of the sentence pair in Figure 1 (lines en and fr) is used to illustrate our models."}, {"heading": "3.1 Copyable Model", "text": "In this approach, we introduce multiple tokens to represent the unknown words in the source and in the target language, instead of just one token <unk> token. We annotate the OOV words in the source sentence with unk1, unk2, unk3, . . ., in that order, where repeating unknown words are given identical tokens. The annotation of the unknown words in the target language is slightly more elaborate: (a) each unknown target word that is aligned to an unknown source word is assigned the same unknown token (hence, the \u201ccopy\u201d model) and (b) an unknown target word that has no alignment or that is aligned with a known word uses the special null token unkn. See Figure 2 for an example. This annotation enables us to translate every non-null token."}, {"heading": "3.2 Positional All Model (PosAll)", "text": "The copyable model is limited by its inability to translate unknown target words that are aligned to known words in the source sentence, such as the pair of words portique \u2013 portico in our running example. This happens because source vocabularies tend to be much larger than target vocabularies due to the cost of the softmax (although much faster alternatives to the softmax exist which could potentially alleviate this problem). This limitation motivated us to develop a model that predicts the complete alignments between the source and the target sentence, which is straightforward since the complete alignments are available during training time.\nSpecifically, we return to using only a single universal <unk> token. However, on the target side, we insert a positional token posd after every word. Here, d indicates a relative position (d = \u22127, . . . ,\u22121, 0, 1, . . . , 7) to denote that a target word at position j is aligned to a source word at position i = j \u2212 d. Aligned words that are too far apart are not annotated. In addition, we have a null token posn to mark unaligned words. Our annotation is illustrated in Figure 4."}, {"heading": "3.3 Positional Unknown Model (PosUnk)", "text": "A major weakness of the PosAll model is that it doubles the length of the target sentence, which makes learning more difficult and nearly 2 times slower per parameter update. However, our postprocessing step is concerned only with the alignments of the unknown words, so it is more sensible to annotate only the alignments of the unknown words. This motivates our positional unknown model which uses the unkposd tokens (for d in \u22127, . . . , 7 or n) to simultaneously denote (a) the fact that a word is unknown and (b) its relative position d with respect to its aligned source word, similarly to the positional all model (where d is set to the null symbol n whenever the word does not have an alignment). We use the universal <unk> for all other unknown tokens in the source language.\nIt is possible that despite its slower speed, the PosAll model will learn better alignments as it is trained on many more examples of words and their alignments. We answer this question in the experimental section."}, {"heading": "4 Experiments", "text": "We evaluate the effectiveness of our OOV models on the WMT\u201914 English-to-French translation task.1 Translation quality is measured with the BLEU metric [17] on the newstest2014 (which has 3003 sentences)."}, {"heading": "4.1 Training Data", "text": "To be comparable with the results reported by previous work on neural machine translation systems [22, 5, 2], we train our models on the same training data of 12M parallel sentences (348M French and 304M English words). The 12M subset was selected from the full WMT\u201914 parallel corpora using the method proposed in [1].2\nDue to the computationally intensive nature of the naive softmax in the target language, we limit the French vocabulary to the 40K most frequent French words (note that [22] used a vocabulary of 80k French words). On the source side, however, we can afford a much larger vocabulary, so we use the 200K most frequent English words. The model treats all other words as unknowns. When the French (target) vocabulary has 40K words, there are on average 1.33 unknown words per sentence on the target side of the test set.\nWe annotate our training data using the three schemes described in the previous section. The alignment is computed with the Berkeley aligner [16] using its default settings. We discard sentence pairs in which either the source or the target sentence exceed 100 tokens."}, {"heading": "4.2 Training Details", "text": "Our training procedure and hyperparameter choices are similar to those used by Sutskever et al. [22]. In more details, we train multi-layer deep LSTMs, each of which has 1000 cells, with 1000 dimensional embeddings. Like Sutskever et al. [22], we reverse the words in the source sentences which has been shown to improve LSTM memory utilization and results in better translations of long sentences. Our hyperparameters can be summarized as follows: (a) the parameters are initialized uniformly in [-0.08, 0.08], (b) SGD has a fixed learning rate of 0.7, (c) we train for 8 epochs (after 5 epochs, we begin to halve the learning rate every 0.5 epoch), (d) the size of the mini-batch is 128, and (e) we rescale the normalized gradient to ensure that its norm does not exceed 5 [18].\nWe also follow the GPU parallelization scheme proposed in [22], allowing us to reach a training speed of 9.0K words per second ([22] achieved 6.3K words per second with a larger vocabulary of 80K; our target vocabulary has 40K words). Training takes about 7-10 days on an 8-GPU machine."}, {"heading": "4.3 A note on BLEU scores", "text": "The website http://matrix.statmt.org/matrix states that the state-of-the-art (SOTA) system [7] achieves a BLEU score of 35.8 on the English to French language pair on WMT\u201914. This numerical score is based on detokenized translations. However, all other systems that we compared against have been evaluated on the tokenized translations using the multi-bleu.pl script, which is consistent with previous work [5, 2, 19, 22]. Thus, to make it possible to compare our system against the system of Durrani et al. [7], we evaluated its tokenized predictions (which can be downloaded from statmt.org [7]) on the test set (newstest2014) and arrived at the BLEU score of 37.0 points [22].\n1http://www.statmt.org/wmt14/translation-task.html 2http://www-lium.univ-lemans.fr/\u02dcschwenk/cslm_joint_paper/."}, {"heading": "4.4 Main Results", "text": "We compare our system to other systems trained on the same training data of 12M sentence pairs, which include several recent end-to-end neural systems, as well as phrase-based baselines with neural components. We also compare to the performance of the state-of-the-art MT system [7] from the WMT\u201914 competition, which is trained on 36M sentence pairs.\nThe results shown in Table 1 demonstrate that our unknown word translation technique (in particular, the PosUnk model) significantly improves the translation quality for both the individual (non-ensemble) LSTM models (+2.3 BLEU) and the ensemble model (+2.8 BLEU). For the nonensemble models, we report the performance of models with 4 and 6 layers (see Section 5.2 for more analysis). For the ensemble setting, we use a combination of 5 depth-4 models and 3 depth-6 models. Our best result (36.9 BLEU) outperforms all other NMT systems by a large margin, and in particular, it outperforms the current best NMT system [22] by 2.1 BLEU points (we even outperform Sutskever et al. [22] when they rerank the n-best list of a phrase-based baseline [22]). We compare the other rare word translation schemes in the next section.\nIt is notable that the more accurate NMT systems obtain greater improvements from our postprocessing step. It is the case because the usefulness of the PosUnk model depends directly on the NMT\u2019s ability to correctly locate, for a given OOV target word, the word in the source sentence that is responsible for it. An ensemble of large models identifies these source words with greater accuracy, so it is not surprising that the PosUnk model provides the greatest improvement in performance for the best models."}, {"heading": "5 Analysis", "text": "We analyze and quantify the improvement obtained by our rare word translation approach and provide a detailed comparison of the different rare word techniques proposed in Section 3. We examine\nthe effect of depth on the LSTM architectures and demonstrate a strong correlation between perplexities and BLEU scores. We also highlight a few translation examples where our models succeed in correctly translating OOV words as well several failures."}, {"heading": "5.1 Rare Word Analysis", "text": "To analyze the effect of rare words on translation quality, we follow Sutskever et al. [22] and sort the sentences in newstest2014 by the average frequency rank of their words. We split the test sentences into groups where the sentences within each group have a comparable number of rare words and evaluate each group independently. We evaluate our systems before and after translating the OOV words and compare with the standard MT systems \u2013 we use the state-of-the-art (SOTA) system from WMT\u201914 [7], and neural MT systems \u2013 we use the ensemble system described in [22] (See Section 4).\nRare word translation is challenging for neural machine translation systems as shown in Figure 5. The translation quality of our model before applying the unknown word translations is shown by the green star line, and the current best NMT system [22] is the purple diamond line. While [22] produces excellent translations of sentences with frequent words (the left part of the graph), they are worse than SOTA system (red triangle line) on sentences with many rare words (the right side of the graph). When applying our unknown word translation technique (blue square line), we significantly improve the translation quality of our NMT: in for the last group of 500 sentences which have the greatest proportion of OOV words in the test set, we increase the BLEU score of our system by 6.5 BLEU points. Overall, our rare word translation model interpolates between the SOTA system and the system of Sutskever et al. [22], which allows us to outperform SOTA on sentences that consist predominantly of frequent words and approach its performance on sentences with many OOV words."}, {"heading": "5.2 Other Effects", "text": "In this section, all models are trained on the unreversed sentences, and we use the following hyperparameters: we initialize the parameters uniformly in [-0.1, 0.1], the learning rate is 1, the maximal gradient norm is 1, with a source vocabulary of 90k words, and a target vocabulary of 40k (see Section 4.2 for more details). While these LSTMs do not achieve the best possible performance, it is still useful to analyze them.\nRare Word Models \u2013 We examine the effect of the different rare word models presented in Section 3, namely: (a) Copyable \u2013 which aligns the unknown words on both the input and the target\nside by learning to copy indices, (b) the Positional All (PosAll) \u2013 which predicts the aligned source positions for every target word, and (c) the Positional Unknown (PosUnk) \u2013 which predicts the aligned source positions for only the unknown target words. It is also interesting to measure the improvement obtained when no alignment information is used during training. As such, we include a baseline model with no alignment knowledge (NoAlign) in which we simply assume that the ith unknown word on the target sentence is aligned to the ith unknown word in the source sentence.\nFrom the results in Figure 6, a simple monotone alignment assumption for the NoAlign model yields a modest gain of 0.8 BLEU points. If we train the model to predict the alignment, then the Copyable model offers slightly better gain of 1.0 BLEU. Note, however, that English and French have similar word order structure, so it would be interesting to experiment with other language pairs, such as English and Chinese, in which the word order is not as monotonic. These harder language pairs potentially imply a smaller gain for the NoAlign model and a larger gain for the Copyable model. We leave it for future work.\nThe positional models (PosAll and PosUnk) improve translation performance by more than 2 BLEU points. This proves that the limitation of the copyable model, which forces it to align each unknown output word with an unknown input word, is considerable. In contrast, the positional models can align the unknown target words with any source word, and as a result, post-processing has a much stronger effect. The PosUnk model achieves better translation results than the PosAll model which suggests that it is easier to train the LSTM on shorter sequences.\nDeep LSTM architecture \u2013 We compare a number of PosUnk models trained with different number of layers (3, 4, and 6). We observe that the gain obtained by the PosUnk model increases in tandem with the overall accuracy of the model, which is consistent with the idea that larger models can point to the appropriate source word more accurately. Additionally, we observe that on average, each extra LSTM layer provides roughly 1.0 BLEU point improvement as demonstrated in Figure 7.\nPerplexity and BLEU \u2013 Lastly, we find it interesting to observe a strong correlation between perplexity (the objective we are optimizing for) and the translation quality as measured by the BLEU score. Figure 8 shows the performance of a 4-layer LSTM, in which we compute both perplexity and BLEU scores at different points during training. We find that on average, a reduction of 0.5 perplexity gives us roughly 1.0 BLEU point improvement."}, {"heading": "5.3 Sample Translations", "text": "We present three sample translations of our best system (with 36.9 BLEU) in Table 2. In our first example, the model translates all the unknown words correctly: 2600, orthope\u0301diques, and cataracte. It is interesting to observe that the model can accurately predict an alignment of distances of 5 and 6 words. The second example highlights the fact that our model can translate long sentences reasonably well and that it was able to correctly translate the unknown word for JPMorgan at the very far end of the source sentence. Lastly, our examples also reveal several penalties incurred by our model: (a) incorrect entries in the word dictionary, as with ne\u0301gociateur vs. trader in the second example, and (b) incorrect alignment prediction, such as the unkpos3 word is incorrectly aligned with the source word was and not with abandoning, which resulted in an incorrect translation in the third sentence."}, {"heading": "6 Conclusion", "text": "We have shown that a simple alignment-based technique can mitigate and even overcome one of the main weaknesses of current NMT systems, which is their inability to translate words that are not in their vocabulary. A key advantage of our technique is the fact that it is applicable to any NMT system and not only to the deep LSTM model of Sutskever et al. [22]. A technique like ours is likely necessary if an NMT system is to achieve state-of-the-art performance on machine translation.\nWe have demonstrated empirically that on the WMT\u201914 English-French translation task, our technique yields a consistent and substantial improvement of 2-3 BLEU points over various NTM sys-\ntems of different architectures. Our system outperforms the current best end-to-end neural machine translation system by the large margin of 2.1 BLEU points. Most importantly, our models match the performance of the state-of-the-art system, which uses three times more data than our model."}], "references": [{"title": "Domain adaptation via pseudo in-domain data selection", "author": ["Amittai Axelrod", "Xiaodong He", "Jianfeng Gao"], "venue": "In EMNLP,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Phrasal: A statistical machine translation toolkit for exploring new model features", "author": ["D. Cer", "M. Galley", "D. Jurafsky", "C.D. Manning"], "venue": "In ACL, Demonstration Session,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang"], "venue": "Computational Linguistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In EMNLP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul"], "venue": "In ACL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Edinburgh\u2019s phrase-based machine translation systems for WMT-14", "author": ["Nadir Durrani", "Barry Haddow", "Philipp Koehn", "Kenneth Heafield"], "venue": "In WMT,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models", "author": ["Chris Dyer", "Jonathan Weese", "Hendra Setiawan", "Adam Lopez", "Ferhan Ture", "Vladimir Eidelman", "Juri Ganitkevitch", "Phil Blunsom", "Philip Resnik"], "venue": "In ACL, Demonstration Session,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "In Arxiv preprint arXiv:1308.0850,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "In EMNLP,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": "In ACL, Demonstration Session,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In NAACL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Alignment by agreement", "author": ["P. Liang", "B. Taskar", "D. Klein"], "venue": "In NAACL,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei jing Zhu"], "venue": "In ACL,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "arXiv preprint arXiv:1211.5063,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "University le mans", "author": ["H. Schwenk"], "venue": "http://www-lium.univ-lemans.fr/ \u0303schwenk/cslm_joint_paper/,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Continuous space translation models for phrase-based statistical machine translation", "author": ["Holger Schwenk"], "venue": "In COLING,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Continuous space translation models with neural networks", "author": ["Le Hai Son", "Alexandre Allauzen", "Fran\u00e7ois Yvon"], "venue": "In NAACL-HLT,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang"], "venue": "In EMNLP,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}], "referenceMentions": [{"referenceID": 10, "context": "Deep Neural Networks (DNNs) have achieved excellent results on speech recognition [11], visual object recognition [15], and other challenging tasks, so there has been much interest in applying them to natural language processing (NLP) problems as well.", "startOffset": 82, "endOffset": 86}, {"referenceID": 14, "context": "Deep Neural Networks (DNNs) have achieved excellent results on speech recognition [11], visual object recognition [15], and other challenging tasks, so there has been much interest in applying them to natural language processing (NLP) problems as well.", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "For more than a decade, the standard MT approach [14] has been subject to intensive \u2217Work done while the author was in Google.", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "research and resulted in better systems over time [13, 4, 3, 8].", "startOffset": 50, "endOffset": 63}, {"referenceID": 3, "context": "research and resulted in better systems over time [13, 4, 3, 8].", "startOffset": 50, "endOffset": 63}, {"referenceID": 2, "context": "research and resulted in better systems over time [13, 4, 3, 8].", "startOffset": 50, "endOffset": 63}, {"referenceID": 7, "context": "research and resulted in better systems over time [13, 4, 3, 8].", "startOffset": 50, "endOffset": 63}, {"referenceID": 19, "context": "In recent years, MT researchers have been seeking to incorporate neural network models into the standard pipeline as an additional subcomponent [20, 21, 23, 6].", "startOffset": 144, "endOffset": 159}, {"referenceID": 20, "context": "In recent years, MT researchers have been seeking to incorporate neural network models into the standard pipeline as an additional subcomponent [20, 21, 23, 6].", "startOffset": 144, "endOffset": 159}, {"referenceID": 22, "context": "In recent years, MT researchers have been seeking to incorporate neural network models into the standard pipeline as an additional subcomponent [20, 21, 23, 6].", "startOffset": 144, "endOffset": 159}, {"referenceID": 5, "context": "In recent years, MT researchers have been seeking to incorporate neural network models into the standard pipeline as an additional subcomponent [20, 21, 23, 6].", "startOffset": 144, "endOffset": 159}, {"referenceID": 11, "context": "Lately, there have been a number of attempts to develop a purely neural machine translation system (NMT) [12, 5, 2, 22].", "startOffset": 105, "endOffset": 119}, {"referenceID": 4, "context": "Lately, there have been a number of attempts to develop a purely neural machine translation system (NMT) [12, 5, 2, 22].", "startOffset": 105, "endOffset": 119}, {"referenceID": 1, "context": "Lately, there have been a number of attempts to develop a purely neural machine translation system (NMT) [12, 5, 2, 22].", "startOffset": 105, "endOffset": 119}, {"referenceID": 21, "context": "Lately, there have been a number of attempts to develop a purely neural machine translation system (NMT) [12, 5, 2, 22].", "startOffset": 105, "endOffset": 119}, {"referenceID": 12, "context": "In addition, NMT systems are easy to train with backpropagation and their decoder is easy to implement, unlike the highly intricate decoders used by the phrase-based systems [13].", "startOffset": 174, "endOffset": 178}, {"referenceID": 21, "context": "NMT systems also use minimal domain knowledge, which makes them applicable to any other problem that can be formulated as mapping a sequence to another sequence [22].", "startOffset": 161, "endOffset": 165}, {"referenceID": 21, "context": "[22] and Bahdanau et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] have observed that sentences with many rare words tend to be translated much more poorly than sentences containing mainly frequent words.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "9 BLUE points, matching the performance of the state-of-the-art system [7] while using three times less data.", "startOffset": 71, "endOffset": 74}, {"referenceID": 11, "context": "[12] used a combination of a convolutional neural network and a recurrent neural network, Sutskever et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] used a large and deep Long Short-Term Memory (LSTM) model, Cho et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] used an architecture similar to the LSTM, and Bahdanau et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] used a more elaborate neural network architecture that uses an attentional mechanism over the input sequence, similarly to Graves [9] and Graves et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[2] used a more elaborate neural network architecture that uses an attentional mechanism over the input sequence, similarly to Graves [9] and Graves et al.", "startOffset": 134, "endOffset": 137}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22], which has a large deep LSTM to encodue the input sequence and a separate deep LSTM to produce a translation from the input sequence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "We treat the NMT system [12, 22, 5] as a black box and train it on a dataset annotated with alignment information specified by one of the models below.", "startOffset": 24, "endOffset": 35}, {"referenceID": 21, "context": "We treat the NMT system [12, 22, 5] as a black box and train it on a dataset annotated with alignment information specified by one of the models below.", "startOffset": 24, "endOffset": 35}, {"referenceID": 4, "context": "We treat the NMT system [12, 22, 5] as a black box and train it on a dataset annotated with alignment information specified by one of the models below.", "startOffset": 24, "endOffset": 35}, {"referenceID": 16, "context": "1 Translation quality is measured with the BLEU metric [17] on the newstest2014 (which has 3003 sentences).", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "To be comparable with the results reported by previous work on neural machine translation systems [22, 5, 2], we train our models on the same training data of 12M parallel sentences (348M French and 304M English words).", "startOffset": 98, "endOffset": 108}, {"referenceID": 4, "context": "To be comparable with the results reported by previous work on neural machine translation systems [22, 5, 2], we train our models on the same training data of 12M parallel sentences (348M French and 304M English words).", "startOffset": 98, "endOffset": 108}, {"referenceID": 1, "context": "To be comparable with the results reported by previous work on neural machine translation systems [22, 5, 2], we train our models on the same training data of 12M parallel sentences (348M French and 304M English words).", "startOffset": 98, "endOffset": 108}, {"referenceID": 0, "context": "The 12M subset was selected from the full WMT\u201914 parallel corpora using the method proposed in [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 21, "context": "2 Due to the computationally intensive nature of the naive softmax in the target language, we limit the French vocabulary to the 40K most frequent French words (note that [22] used a vocabulary of 80k French words).", "startOffset": 171, "endOffset": 175}, {"referenceID": 15, "context": "The alignment is computed with the Berkeley aligner [16] using its default settings.", "startOffset": 52, "endOffset": 56}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22], we reverse the words in the source sentences which has been shown to improve LSTM memory utilization and results in better translations of long sentences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "5 epoch), (d) the size of the mini-batch is 128, and (e) we rescale the normalized gradient to ensure that its norm does not exceed 5 [18].", "startOffset": 134, "endOffset": 138}, {"referenceID": 21, "context": "We also follow the GPU parallelization scheme proposed in [22], allowing us to reach a training speed of 9.", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "0K words per second ([22] achieved 6.", "startOffset": 21, "endOffset": 25}, {"referenceID": 6, "context": "org/matrix states that the state-of-the-art (SOTA) system [7] achieves a BLEU score of 35.", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "pl script, which is consistent with previous work [5, 2, 19, 22].", "startOffset": 50, "endOffset": 64}, {"referenceID": 1, "context": "pl script, which is consistent with previous work [5, 2, 19, 22].", "startOffset": 50, "endOffset": 64}, {"referenceID": 18, "context": "pl script, which is consistent with previous work [5, 2, 19, 22].", "startOffset": 50, "endOffset": 64}, {"referenceID": 21, "context": "pl script, which is consistent with previous work [5, 2, 19, 22].", "startOffset": 50, "endOffset": 64}, {"referenceID": 6, "context": "[7], we evaluated its tokenized predictions (which can be downloaded from statmt.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "org [7]) on the test set (newstest2014) and arrived at the BLEU score of 37.", "startOffset": 4, "endOffset": 7}, {"referenceID": 21, "context": "0 points [22].", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "We also compare to the performance of the state-of-the-art MT system [7] from the WMT\u201914 competition, which is trained on 36M sentence pairs.", "startOffset": 69, "endOffset": 72}, {"referenceID": 21, "context": "9 BLEU) outperforms all other NMT systems by a large margin, and in particular, it outperforms the current best NMT system [22] by 2.", "startOffset": 123, "endOffset": 127}, {"referenceID": 21, "context": "[22] when they rerank the n-best list of a phrase-based baseline [22]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] when they rerank the n-best list of a phrase-based baseline [22]).", "startOffset": 65, "endOffset": 69}, {"referenceID": 6, "context": "System BLEU State of the art [7] 37.", "startOffset": 29, "endOffset": 32}, {"referenceID": 18, "context": "0 Standard MT + neural components LIUM [19] \u2013 neural language model 33.", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "[5] \u2013 phrase table neural features 34.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[22] \u2013 ensemble 5 LSTMs, reranking 36.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] \u2013 bi-directional gated single RNN 28.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[22] \u2013 single LSTM 30.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] \u2013 ensemble of 5 LSTMs 34.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] and sort the sentences in newstest2014 by the average frequency rank of their words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "We evaluate our systems before and after translating the OOV words and compare with the standard MT systems \u2013 we use the state-of-the-art (SOTA) system from WMT\u201914 [7], and neural MT systems \u2013 we use the ensemble system described in [22] (See Section 4).", "startOffset": 164, "endOffset": 167}, {"referenceID": 21, "context": "We evaluate our systems before and after translating the OOV words and compare with the standard MT systems \u2013 we use the state-of-the-art (SOTA) system from WMT\u201914 [7], and neural MT systems \u2013 we use the ensemble system described in [22] (See Section 4).", "startOffset": 233, "endOffset": 237}, {"referenceID": 21, "context": "The translation quality of our model before applying the unknown word translations is shown by the green star line, and the current best NMT system [22] is the purple diamond line.", "startOffset": 148, "endOffset": 152}, {"referenceID": 21, "context": "While [22] produces excellent translations of sentences with frequent words (the left part of the graph), they are worse than SOTA system (red triangle line) on sentences with many rare words (the right side of the graph).", "startOffset": 6, "endOffset": 10}, {"referenceID": 21, "context": "[22], which allows us to outperform SOTA on sentences that consist predominantly of frequent words and approach its performance on sentences with many OOV words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "Neural Machine Translation (NMT) has recently attracted a lot of attention due to the very high performance achieved by deep neural networks in other domains. An inherent weakness in existing NMT systems is their inability to correctly translate rare words: end-to-end NMTs tend to have relatively small vocabularies with a single \u201cunknown-word\u201d symbol representing every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement a simple technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to output, for each OOV word in the target sentence, its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT\u201914 English to French translation task show that this simple method provides a substantial improvement over an equivalent NMT system that does not use this technique. The performance of our system achieves a BLEU score of 36.9, which improves the previous best end-to-end NMT by 2.1 points. Our model matches the performance of the state-of-the-art system while using three times less data.", "creator": "LaTeX with hyperref package"}}}