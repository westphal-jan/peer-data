{"id": "1411.1076", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2014", "title": "A statistical model for tensor PCA", "abstract": "We consider the Principal Component Analysis problem for large tensors of arbitrary order $k$ under a single-spike (or rank-one plus noise) model. On the one hand, we use information theory, and recent results in probability theory, to establish necessary and sufficient conditions under which the principal component can be estimated using unbounded computational resources. It turns out that this is possible as soon as the signal-to-noise ratio $\\beta$ becomes larger than $C\\sqrt{k\\log k}$ (and in particular $\\beta$ can remain bounded as the problem dimensions increase).", "histories": [["v1", "Tue, 4 Nov 2014 21:01:56 GMT  (102kb,D)", "http://arxiv.org/abs/1411.1076v1", "Neural Information Processing Systems (NIPS) 2014 (slightly expanded: 30 pages, 6 figures)"]], "COMMENTS": "Neural Information Processing Systems (NIPS) 2014 (slightly expanded: 30 pages, 6 figures)", "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT stat.ML", "authors": ["emile richard", "andrea montanari"], "accepted": true, "id": "1411.1076"}, "pdf": {"name": "1411.1076.pdf", "metadata": {"source": "CRF", "title": "A statistical model for tensor PCA", "authors": ["Andrea Montanari"], "emails": [], "sections": [{"heading": null, "text": "tensor unfolding, power iteration and message passing ideas from graphical models. We show that, unless the signal-to-noise ratio diverges in the system dimensions, none of these approaches succeeds. This is possibly related to a fundamental limitation of computationally tractable estimators for this problem.\nWe discuss various initializations for tensor power iteration, and show that a tractable initialization based on the spectrum of the matricized tensor outperforms significantly baseline methods, statistically and computationally. Finally, we consider the case in which additional side information is available about the unknown signal. We characterize the amount of side information that allows the iterative algorithms to converge to a good estimate."}, {"heading": "1 Introduction", "text": "Given a data matrix X, Principal Component Analysis (PCA) can be regarded as a \u2018denoising\u2019 technique that replaces X by its closest rank-one approximation. This optimization problem can be solved efficiently, and its statistical properties are well-understood. The generalization of PCA to tensors is motivated by problems in which it is important to exploit higher order moments, or data elements are naturally given more than two indices. Examples include topic modeling [AGH+12], video processing, collaborative filtering in presence of temporal/context information, community detection [AGHK13], spectral hypergraph theory and hyper-graph matching [DBKP09]. Further, finding a rank-one approximation to a tensor is a bottleneck for tensor-valued optimization algorithms using conditional gradient type of schemes. While tensor factorization is NP-hard [HL13], this does not necessarily imply intractability for natural statistical models. Over the last ten years, it was repeatedly observed that either convex optimization or greedy methods yield optimal solutions to statistical problems that are intractable from a worst case perspective (wellknown examples include sparse regression [DE03, Tro04, CT07] and low-rank matrix completion [CR09, KMO10]).\n\u2217Department of Electrical Engineering and Department of Statistics, Stanford University \u2020Department of Electrical Engineering, Stanford University\nar X\niv :1\n41 1.\n10 76\nv1 [\ncs .L\nG ]\n4 N\nov 2\n01 4\nIn order to investigate the fundamental tradeoffs between computational resources and statistical power in tensor PCA, we consider the simplest possible model where this arises, whereby an unknown unit vector v0 is to be inferred from noisy multilinear measurements. Namely, for each unordered k-uple {i1, i2, . . . , ik} \u2286 [n], we measure\nXi1,i2,...,ik = \u03b2(v0)i1(v0)i2 \u00b7 \u00b7 \u00b7 (v0)ik + Zi1,i2,...,ik , (1)\nwith Z Gaussian noise (see below for a precise definition) and wish to reconstruct v0. In tensor notation, the observation model reads (see the end of this section for notations)\nX = \u03b2 v0 \u2297k + Z . Spiked Tensor Model\nThis is analogous to the so called \u2018spiked covariance model\u2019 used to study matrix PCA in high dimensions [JL09].\nIt is immediate to see that maximum-likelihood estimator vML is given by a solution of the following problem\nmaximize \u3008X,v\u2297k\u3009, Tensor PCA subject to \u2016v\u20162 = 1 .\nSolving it exactly is \u2013in general\u2013 NP hard [HL13]. We next summarize our results. Note that, given a completely observed rank-one symmetric tensor v0 \u2297k (i.e. for \u03b2 =\u221e), it is easy to recover the vector v0 \u2208 Rn. It is therefore natural to ask the question for which signal-to-noise ratios one can one still reliably estimate v0? The answer appears to depend dramatically on the computational resources1.\nIdeal estimation. Assuming unbounded computational resources, we can solve the Tensor PCA optimization problem and hence implement the maximum likelihood estimator v\u0302ML. We use recent results in probability theory to show that this approach is successful for \u03b2 \u2265 \u00b5k (here \u00b5k is a constant given explicitly below, with \u00b5k = \u221a k log k(1 + ok(1))). In particular, above\nthis threshold2 we have, with high probability,\n\u2016v\u0302ML \u2212 v0\u201622 \u2264 2.01\u00b5k \u03b2 . (2)\nWe use an information-theoretic argument to show that no approach can do significantly better, namely no procedure can estimate v0 accurately for \u03b2 \u2264 c \u221a k (for c a universal constant).\nTractable estimators: Unfolding. We consider two approaches to estimate v0 that can be implemented in polynomial time. The first approach is based on tensor unfolding: starting from the tensor X \u2208 \u2297k Rn, we produce a matrix Mat(X) of dimensions nq \u00d7 nk\u2212q. We then perform matrix PCA on Mat(X). We show that this method is successful for \u03b2 & n(dk/2e\u22121)/2\n(provided we choose q = dk/2e). 1Here we write F (n) . G(n) if there exists a constant c independent of n (but possibly dependent on k), such that F (n) \u2264 cG(n) 2Note that, for k even, v0 can only be recovered modulo sign. For the sake of simplicity, we assume here that this ambiguity is correctly resolved.\nA heuristics argument suggests that the necessary and sufficient condition for tensor unfolding to succeed is indeed \u03b2 & n(k\u22122)/4 (which is below the rigorous bound by a factor n1/4 for k odd). We can indeed confirm this conjecture for k even and under an asymmetric noise model. Numerical simulations confirm the conjecture for k = 3.\nTractable estimators: Power iteration. We then consider a simple tensor power iteration method, that proceeds by repeatedly applying the tensor to a vector. We prove that, initializing this iteration uniformly at random, it converges very rapidly to an accurate estimate provided \u03b2 & n(k\u22121)/2. A heuristic argument suggests that the correct necessary and sufficient threshold is given by \u03b2 & n(k\u22122)/2. In other words, power iteration is substantially less powerful than unfolding.\nTractable estimators: Warm-start power iteration. Motivated by the last observation, we consider a \u2018warm-start\u2019 power iteration algorithm, in which we initialize power iteration with the output of tensor unfolding. This approach appears to have the same threshold signal-tonoise ratio as simple unfolding, but significantly better accuracy above that threshold.\nWe also study a number of variations on this, with improved unfolding methods.\nTractable estimators: Approximate Message Passing. Finally we consider an approximate message passing (AMP) algorithm [DMM09, BM11]. Such algorithms proved effective in compressed sensing and several other estimation problems. We show that the behavior of AMP is qualitatively similar to the one of naive power iteration. In particular, AMP fails for any \u03b2 bounded as n\u2192\u221e.\nSide information. Given the above computational complexity barrier, it is natural to study weaker version of the original problem. Here we assume that extra information about v0 is available. This can be provided by additional measurements or by approximately solving a related problem, for instance a matrix PCA problem as in [AGH+12]. We model this additional information as y = \u03b3v0+g (with g an independent Gaussian noise vector), and incorporate it in the initial condition of AMP algorithm. We characterize exactly the threshold value \u03b3\u2217 = \u03b3\u2217(\u03b2) above which AMP converges to an accurate estimator.\nThe thresholds for various classes of algorithms are summarized below.\nMethod Required \u03b2 (rigorous) Required \u03b2 (heuristic)\nTensor Unfolding O(n(dk/2e\u22121)/2) n(k\u22122)/4\nTensor Power Iteration (with random init.) O(n(k\u22121)/2) n(k\u22122)/2\nMaximum Likelihood 1 \u2013 Information-theory lower bound 1 \u2013\nWe will conclude the paper with some insights that we believe provide useful guidance for tensor factorization heuristics. We illustrate these insights through simulations.\nThroughout the paper, proofs will be deferred to the Appendices."}, {"heading": "1.1 Notations", "text": "We will use lower-case boldface for vectors (e.g. u, v, and so on) and upper-case boldface for matrices and tensors (e.g. X,Z, and so on). The ordinary scalar product and `p norm over vectors\nare denoted by \u3008u,v\u3009 = \u2211n\ni=1 uivi, and \u2016v\u2016p. We write Sn\u22121 for the unit sphere in n dimensions\nSn\u22121 \u2261 { x \u2208 Rn : \u2016x\u20162 = 1 } . (3)\nGiven X \u2208 \u2297k Rn a real k-th order tensor, we let {Xi1,...,ik}i1,...,ik denote its coordinates and\ndefine a map X : Rn \u2192 Rn, by letting, for v \u2208 Rn,\nX{v}i = \u2211\nj1,\u00b7\u00b7\u00b7 ,jk\u22121\u2208[n]\nXi,j1,\u00b7\u00b7\u00b7 ,jk\u22121 vj1 \u00b7 \u00b7 \u00b7vjk\u22121 . (4)\nThe outer product of two tensors is X\u2297Y, and, for v \u2208 Rn, we define v\u2297k = v\u2297 \u00b7 \u00b7 \u00b7 \u2297 v \u2208 \u2297k Rn\nas the k-th outer power of v. We define the inner product of two tensors X,Y \u2208 \u2297k Rn as\n\u3008X,Y\u3009 = \u2211\ni1,\u00b7\u00b7\u00b7 ,ik\u2208[n]\nXi1,\u00b7\u00b7\u00b7 ,ikYi1,\u00b7\u00b7\u00b7 ,ik . (5)\nWe define the Frobenius (Euclidean) norm of a tensor X by \u2016X\u2016F = \u221a \u3008X,X\u3009, and its operator norm by\n\u2016X\u2016op \u2261 max{\u3008X,u1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 uk\u3009 : \u2200i \u2208 [k] , \u2016ui\u20162 \u2264 1}. (6)\nIt is easy to check that this is indeed a norm. For the special case k = 2, it reduces to the ordinary `2 matrix operator norm (equivalently, to the largest singular value of X).\nFor a permutation \u03c0 \u2208 Sk, we will denote by X\u03c0 the tensor with permuted indices X\u03c0i1,\u00b7\u00b7\u00b7 ,ik = X\u03c0(i1),\u00b7\u00b7\u00b7 ,\u03c0(ik). We call the tensor X symmetric if, for any permutation \u03c0 \u2208 Sk, X\n\u03c0 = X. It is proved [Wat90] that, for symmetric tensors, the value of problem Tensor PCA coincides with \u2016X\u2016op up to a sign. More precisely, for symmetric tensors we have the equivalent representation\n\u2016X\u2016op \u2261 max{|\u3008X,u\u2297k\u3009| : \u2016u\u20162 \u2264 1}. (7)\nWe denote by G \u2208 \u2297k Rn a tensor with independent and identically distributed entries Gi1,\u00b7\u00b7\u00b7 ,ik \u223c N(0, 1) (note that this tensor is not symmetric). We define the symmetric standard normal noise tensor Z \u2208\n\u2297k Rn by Z = 1\nk!\n\u221a k\nn \u2211 \u03c0\u2208Sk G\u03c0 . (8)\nNote that the subset of entries with unequal indices form an i.i.d. collection {Zi1,i2,...,ik}i1<\u00b7\u00b7\u00b7<ik \u223c N(0, 1/(n(k \u2212 1)!)). The normalization adopted here is convenient because it yields, for any fixed vector v \u2208 Rn,\nX{v} = \u03b2\u3008v0,v\u3009k\u22121 v0 + 1\u221a n \u2016v\u2016k\u221212 g + o(1) . (9)\nwhere g \u223c N(0, In), and o(1) is a vector with \u2016o(1)\u20162 \u2192 0 in probability as n \u2192 \u221e. We further have,\nE { \u3008Z,v\u2297k\u30092 } = k n E { \u3008G,v\u2297k\u30092 } = k n \u2016v\u20162k2 , (10)\nand\n\u3008X{v},v\u3009 = \u03b2\u3008v0,v\u3009k + \u221a k\nn g , (11)\nwith g \u223c N(0, 1). Finally notice that, for k even, in Spiked Tensor Model, the vector v0 can always be recovered up to a sign flip. This suggest the use of the loss function\nLoss(v\u0302,v0) \u2261 min ( \u2016v\u0302 \u2212 v0\u201622, \u2016v\u0302 + v0\u201622 ) = 2\u2212 2|\u3008v\u0302,v0\u3009| . (12)"}, {"heading": "2 Ideal estimation", "text": "In this section we consider the problem of estimating v0 under the Spiked Tensor Model, when no constraint is imposed on the complexity of the estimator. Our first result is a lower bound on the loss of any estimator.\nTheorem 1. For any estimator v\u0302 = v\u0302(X) of v0 from data X, such that \u2016v\u0302(X)\u20162 = 1 (i.e. v\u0302 : \u2297kRn \u2192 Sn\u22121), we have, for all n \u2265 4,\n\u03b2 \u2264 \u221a k\n10 \u21d2 E Loss(v\u0302,v0) \u2265\n1\n32 . (13)\nIn order to establish a matching upper bound on the loss, we consider the maximum likelihood estimator v\u0302ML, obtained by solving the Tensor PCA problem. As in the case of matrix denoising, we expect the properties of this estimator to depend on signal to noise ratio \u03b2, and on the \u2018norm\u2019 of the noise \u2016Z\u2016op (i.e. on the value of the optimization problem Tensor PCA in the case \u03b2 = 0). For the matrix case k = 2, this coincides with the largest eigenvalue of Z. Classical random matrix theory shows that \u2013in this case\u2013 \u2016Z\u2016op concentrates tightly around 2 [Gem80, DS01a, BS10].\nIt turns out that tight results for k \u2265 3 follow immediately from a technically sophisticated analysis of the stationary points of random Morse functions by Auffinger, Ben Arous and Cerny [ABAC13]. (See Appendix B.1 for further background.)\nLemma 2.1. There exists a sequence of real numbers {\u00b5k}k\u22652, such that\nlim sup n\u2192\u221e\n\u2016Z\u2016op \u2264 \u00b5k (k odd), (14)\nlim n\u2192\u221e\n\u2016Z\u2016op = \u00b5k (k even). (15)\nFurther \u2016Z\u2016op concentrates tightly around its expectation. Namely, for any n, k\nP (\u2223\u2223\u2016Z\u2016op \u2212 E\u2016Z\u2016op\u2223\u2223 \u2265 s) \u2264 2 e\u2212ns2/(2k) . (16)\nFinally \u00b5k = \u221a k log k(1 + ok(1)) for large k.\nAn explicit expression for the quantity \u00b5k is given in Appendix B (which also contains a proof, that uses [ABAC13]). Evaluating this expression for small values of k, we get the following explicit values, that we also compare with the large-k asymptotics \u221a k log k. (It is not hard to increase the number of digits in these evaluations, using the expressions in Appendix.)\nk \u00b5k \u221a k log k 3 2.8700 1.8154 4 3.5882 2.3548 5 4.2217 2.8368 10 6.7527 4.7985 100 27.311 21.460\nFor instance, this table indicates that a large order-3 Gaussian tensor should have \u2016Z\u2016op \u2248 2.87, while a large order 10 tensor has \u2016Z\u2016op \u2248 6.75. As a simple consequence of Lemma 2.1, we establish an upper bound on the error incurred by the maximum likelihood estimator, see Section B.2 for a proof.\nTheorem 2. Let \u00b5k be the sequence of real numbers introduced above. Letting v\u0302 ML denote the maximum likelihood estimator (i.e. the solution of Tensor PCA), we have for n large enough, and all s > 0\n\u03b2 \u2265 \u00b5k \u21d2 Loss(v\u0302ML,v0) \u2264 2\n\u03b2 (\u00b5k + s) , (17)\nwith probability at least 1\u2212 2e\u2212ns2/(16k).\nThe following upper bound on the value of the problem Tensor PCA is proved using SudakovFernique inequality. While it is looser than Lemma 2.1 (corresponding to the case \u03b2 = 0), we expect it to become sharp for \u03b2 \u2265 \u03b2k a suitably large constant. We refer to Appendix B.3 for its proof.\nLemma 2.2. Under Spiked Tensor Model model, we have\nlim sup n\u2192\u221e E\u2016X\u2016op \u2264 max \u03c4\u22650\n{ \u03b2 ( \u03c4\u221a\n1 + \u03c42\n)k +\nk\u221a 1 + \u03c42\n} . (18)\nFurther, for any s \u2265 0,\nP (\u2223\u2223\u2016X\u2016op \u2212 E\u2016X\u2016op\u2223\u2223 \u2265 s) \u2264 2 e\u2212ns2/(2k) . (19)"}, {"heading": "2.1 Historical Background", "text": "At this point it is useful to pause, in order to provide some further background. The random cost function v 7\u2192 HZ(v) \u2261 \u3008Z,v\u2297k\u3009 (defined on the unit sphere v \u2208 Sn\u22121) was studied in the context of statistical physics under the name of \u2018spherical p-spin model.\u2019 In particular, Crisanti and So\u0308mmers [CS92] used the non-rigorous replica method from spin glass theory to compute the asymptotic value \u00b5k. Their results were confirmed rigorously by Talagrand [Tal06].\nThe most striking prediction from statistical physics is that the function HZ(v) has an exponential number of local maxima on the unit sphere [CS95]. Furthermore, there exists \u03b7k < \u00b5k such that, for each x \u2208 [\u03b7k, \u00b5k) the number of local maxima with value HZ(v) \u2248 x is exp{\u0398(n)}. In [ABAC13] rigorous evidence is developed to this support picture.\nIn the Spiked Tensor Model these local maxima translate into undesired local maxima of \u3008X,v\u2297k\u3009. It is natural to guess that these local maxima affect local iterative algorithms, and that these do not converge to a good estimate of v0 unless they are initialized within thee \u2018basin of attraction\u2019 of v0. The analysis in the next sections confirms this intuition."}, {"heading": "3 Tensor Unfolding", "text": "A simple and popular heuristics to obtain tractable estimators of v0 consists in constructing a suitable matrix with the entries of X, and performing principal component analysis on this matrix. Since the number of distinct entries of X is of order nk, the resulting matrix Matq(X) has dimension \u0398(nq)\u00d7\u0398(nk\u2212q). This operation is variously referred as matricization, unfolding, flattening. While the details of this construction can vary, we do not expect them to affect qualitatively our results, that we summarize for the sake of convenience:\n1. The best way to unfold X amounts to form a matrix as square as possible.\n2. Setting b = (dk/2e \u2212 1)/2 (in particular b = 1/2 for k \u2208 {3, 4}), the unfolding approach succeeds when \u03b2 is larger than nb. This is to be compared with \u03b2 = \u0398(1) that is sufficient for the maximum likelihood estimator (see previous section).\nBased on heuristic arguments, we believe that the tight threshold is \u03b2 & n(k\u22122)/4 (i.e. that \u03b2 & n(k\u22122)/4 is both necessary and sufficient \u2013modulo constants).\n3. A sharper analysis is possible when the symmetric noise tensor Z in our Spiked Tensor Model is replaced by non-symmetric Gaussian noise, and k is even. In particular, we can confirm the above conjecture in this case. (As mentioned, we expect similar results to hold more generally.)\nIn this case, if \u03b2 \u2264 (1\u2212\u03b5)nb, then the estimator from unfolding is essentially orthogonal to the signal v0. On the other hand, if \u03b2 \u2265 (1 + \u03b5)nb, we construct an estimator with |\u3008v\u0302,v0\u3009| \u2192 1.\n4. We achieves the remarkable behavior at the last point by a recursive unfolding method. In a nutshell we perform principal component analysis on Matq(X), construct a matrix out of the principal vector, and then perform again principal component analysis."}, {"heading": "3.1 Symmetric noise", "text": "For an integer 0 \u2264 q \u2264 k, we introduce the unfolding (also referred to as matricization or reshape) operator Matq : \u2297kRn \u2192 Rn\nq\u00d7nk\u2212q as follows. For any indices i1, i2, \u00b7 \u00b7 \u00b7 , ik \u2208 [n], we let a = 1 + \u2211q j=1(ij \u2212 1)nj\u22121 and b = 1 + \u2211k j=q+1(ij \u2212 1)nj\u2212q\u22121, and define\n[Matq(X)]a,b = Xi1,i2,\u00b7\u00b7\u00b7 ,ik . (20)\nStandard convex relaxations of low-rank tensor estimation problem compute factorizations of Matq(X)[TSHK11, LMWY13, MHG13, RPP13]. Not all unfoldings (choices of q) are equivalent. It is natural to expect that this approach will be successful only if the signal-to-noise ratio exceeds the operator norm of the unfolded noise \u2016Matq(Z)\u2016op. The next lemma suggests that the latter is minimal when Matq(Z) is \u2018as square as possible\u2019 . A similar phenomenon was observed in a different context in [MHG13].\nLemma 3.1. For any integer 0 \u2264 q \u2264 k we have, for some universal constant Ck,\n1\u221a (k \u2212 1)!\nnmax(q\u22121,k\u2212q\u22121)/2 (\n1\u2212 Ck nmax(q,k\u2212q)\n) \u2264 E\u2016Matq(Z)\u2016op \u2264 \u221a k ( n(q\u22121)/2 + n(k\u2212q\u22121)/2 ) .\n(21)\nFor all n large enough, both bounds are minimized for q = dk/2e. Further\nP {\u2223\u2223\u2016Matq(Z)\u2016op \u2212 E\u2016Matq(Z)\u2016op\u2223\u2223 \u2265 t} \u2264 2 e\u2212nt2/(2k) . (22)\nProof. The concentration bound (22) follows because, for u \u2208 Rnq ,v \u2208 Rnk\u2212q of norm 1, the function\n\u3008u,Matq(Z)v\u3009 = 1\nk!\n\u221a k\nn \u2211 \u03c0\u2208Sk \u3008u,Matq(G\u03c0)v\u3009 (23)\nis a Lipschitz function of the Gaussian vector G with modulus at most \u221a k/n. Hence the same holds for \u2016Matq(Z)\u2016op = maxu,v\u3008u,Matq(Z)v\u3009, and the claim follows from Gaussian concentration of measure.\nFor the upper bound in Eq. (21), note that Matq(G \u03c0) has i.i.d. standard normal entries. The proof follows from the definition (8) together with triangular inequality and standard bounds on the norm of the random Gaussian matrices Matq(G \u03c0) \u2208 Rnq\u00d7nk\u2212q [DS01a]:\nE\u2016Matq(Z)\u2016op = 1\nk!\n\u221a k\nn E\u2016Matq( \u2211 \u03c0 G\u03c0)\u2016op\n\u2264 1 k!\n\u221a k\nn \u2211 \u03c0 E\u2016Matq(G\u03c0)\u2016op\n\u2264 \u2211 \u03c0 1 k!\n\u221a k\nn\n( nq/2 + n(k\u2212q)/2 ) = \u221a k ( n(q\u22121)/2 + n(k\u2212q\u22121)/2 ) .\nFor the upper bound in Eq. (21), note that\nmin(nq, nk\u2212q)E{\u2016Matq(Z)\u20162op} \u2265 E\u2016Matq(Z)\u20162F = 1\nk!\nk\nn \u2211 \u03c0\u2208Sk E{\u3008G,G\u03c0\u3009} \u2265 k n nk k! , (24)\nwhere the last inequality is proved by considering \u03c0 = id the identity permutation (all the terms in the sum are positive). The desired lower bound follows since the concentration inequality (22) implies E{\u2016Matq(Z)\u20162op} \u2212 E{\u2016Matq(Z)\u2016op}2 \u2264 (2k/n), and we therefore have\nE{\u2016Matq(Z)\u2016op} \u2265 E{\u2016Matq(Z)\u20162op}1/2 (\n1\u2212 k nE{\u2016Matq(Z)\u20162op}\n) . (25)\nThe last lemma suggests the choice q = dk/2e, which we shall adopt in the following, unless stated otherwise. We will drop the subscript from Mat.\nLet us recall the following standard result derived directly from Wedin perturbation Theorem [Wed72], and stated in the context of the spiked model.\nTheorem 3 (Wedin perturbation). Let M = \u03b2u0w0 T + \u039e \u2208 Rm\u00d7p be a matrix with \u2016u0\u20162 = \u2016w0\u20162 = 1. Let w\u0302 denote the right singular vector of M. If \u03b2 > 2\u2016\u039e\u2016op, then\nLoss(w\u0302,w0) \u2264 8\u2016\u039e\u20162op \u03b22 . (26)\nProof of Theorem 3. Note \u03b2 > 0 is the only singular value of \u03b2u0w0 T, while the second singular value of (\u03b2u0w0 T + \u039e) is at most \u2016\u039e\u2016op. Wedin Theorem states that, for all \u03b2 > \u2016\u039e\u2016op, we have\n| sin(w\u0302,w0)| \u2264 \u2016\u039e\u2016op\n\u03b2 \u2212 \u2016\u039e\u2016op . (27)\nIn particular | sin(w\u0302,w0)| \u2264 2\u2016\u039e\u2016op/\u03b2 for \u03b2 \u2265 2\u2016\u039e\u2016op. Hence the claim (26) follows from\n|\u3008w\u0302,w0\u3009| = | cos(w\u0302,w0)| \u2265 \u221a 1\u2212\n4\u2016\u039e\u20162op \u03b22 \u2265 1\u2212 4\u2016\u039e\u20162op \u03b22 . (28)\nTheorem 4. Letting w = w(X) denote the top right singular vector of Mat(X), we have the following, for some universal constant C = Ck > 0, and b \u2261 (1/2)(dk/2e \u2212 1).\nIf \u03b2 \u2265 5 k1/2 nb then, with probability at least 1\u2212 n\u22122, we have\nLoss ( w, vec ( v0 \u2297bk/2c)) \u2264 C kn2b\n\u03b22 . (29)\nProof of Theorem 4. By definition we have\nMat(X) = \u03b2u0w0 T + Mat(Z) , (30)\nwhere u0 = vec(v0 \u2297bk/2c), w0 = vec(v0 \u2297dk/2e). We know by Lemma 3.1 that \u2016Mat(Z)\u2016op \u2264 (5/2) \u221a k nb with the claimed probability. The loss upper bound (29) follows immediately from this upper bound and Wedin\u2019s theorem Eq. (26)."}, {"heading": "3.2 Asymmetric noise and recursive unfolding", "text": "A technical complication in analyzing the random matrix Matq(X) lies in the fact that its entries are not independent, because the noise tensor Z is assumed to be symmetric. In the next theorem we consider the case of non-symmetric noise and even k. This allows us to leverage upon known results in random matrix theory [Pau07, FP09, BGN12] to obtain: (i) Asymptotically sharp estimates on the critical signal-to-noise ratio; (ii) A lower bound on the loss below the critical signal-to-noise ratio. Namely, we consider observations\nX\u0303 = \u03b2v0 \u2297k + 1\u221a n G . (31)\nwhere G \u2208 \u2297kRn is a standard Gaussian tensor (i.e. a tensor with i.i.d. standard normal entries).\nLet w = w(X\u0303) \u2208 Rnk/2 denote the top right singular vector of Mat(X). For k \u2265 4 even, and define b \u2261 (k \u2212 2)/4, as above. By [Pau07, Theorem 4], or [BGN12, Theorem 2.3], we have the following almost sure limits\n\u03b2 \u2264 (1\u2212 \u03b5)nb \u21d2 lim n\u2192\u221e \u3008w(X\u0303), vec(v0\u2297(k/2))\u3009 = 0 , (32)\n\u03b2 \u2265 (1 + \u03b5)nb \u21d2 lim inf n\u2192\u221e \u2223\u2223\u3008w(X\u0303), vec(v0\u2297(k/2))\u3009\u2223\u2223 \u2265\u221a \u03b5 1 + \u03b5 . (33)\nIn other words w(X\u0303) is a good estimate of v0 \u2297(k/2) if and only if \u03b2 is larger than nb.\nWe can use w(X\u0303) \u2208 R2b+1 to estimate v0 as follows. Construct the matricization Mat1(w) \u2208 Rn\u00d7n2b (slight abuse of notation) of w by letting, for i \u2208 [n], and j \u2208 [n2b],\nMat1(w)i,j = wi+(j\u22121)n , (34)\nwe then let v\u0302 to be the left principal vector of Mat1(X). We refer to this algorithm 3 as to recursive unfolding.\nTheorem 5. Let X\u0303 be distributed according to the non-symmetric model (31) with k \u2265 4 even, define b \u2261 (k \u2212 2)/4. and let v\u0302 be the estimate obtained by two-steps recursive unfolding.\nIf \u03b2 \u2265 (1 + \u03b5)nb then, almost surely\nlim n\u2192\u221e Loss(v\u0302,v0) = 0 . (35)\nProof of Theorem 5. For the sake of simplicity, we assume \u03b2/nb \u2192 \u03b5. The limit along other sequences follows from a standard subsequence argument.\nIt follows from the invariance of the noise distribution in Eq. (31) that\nw(X\u0303) = \u03c1n vec(v0 \u2297(k/2)) + \u03c1n nk/4 g , (36)\nwhere g \u223c N(0, Ink/2). It follows from Eq. (33), together with the almost sure limits limn\u2192\u221e \u2016g\u20162/nk/4 = 1 and limn\u2192\u221e\u3008g, vec(v0\u2297(k/2))\u3009/nk/4 = 1 that (almost surely)\nlim n\u2192\u221e \u03c1n =\n\u221a \u03b5\n1 + \u03b5 , (37)\nlim n\u2192\u221e \u03c1n =\n\u221a 1\n1 + \u03b5 . (38)\nUsing the definition (34), we then have (recall that b = (k \u2212 2)/4)\nMat1(w) = \u03c1nv0 u T + \u03c1n n(2b+1)/2 G\u2032 , (39)\nwhere u = vec(v0 2b) and G\u2032 \u2208 Rn\u00d7n2b is a matrix with i.i.d standard normal entries. Using [DS01b], we have, with probability 1\u2212 e\u2212\u0398(n), \u03c1n\nn(2b+1)/2 \u2016G\u2032\u2016op \u2264\n1 n(2b+1)/2 ( n1/2 + nb ) \u2264 2\u221a n . (40)\nSince \u03c1n is bounded away from zero as n\u2192\u221e, Wedin\u2019s theorem implies limn\u2192\u221e |\u3008v\u0302,v0\u3009| = 1, and therefore the claim (35).\n3In practice int might be more effective to use a balanced matricization at the second step. For instance if k is a power of two one could construct a square matricization and repeat the same process. For analysis purposes, we prefer the version described here.\nWe conjecture that the weaker condition n & n(k\u22122)/4 is indeed sufficient also for our original symmetric noise model model, both for k even and for k odd."}, {"heading": "4 Power Iteration", "text": "Iterating over (multi-) linear maps induced by a (tensor) matrix is a standard method for finding leading eigenpairs, see [KM11] and references therein for tensor-related results. In this section we will consider a simple power iteration, and then its possible uses in conjunction with tensor unfolding. Finally, we will compare our analysis with results available in the literature.\nApproximate Message Passing (AMP) provides a different iterative strategy and will be discussed in Section 5. While the qualitative behavior is the same as for naive power iteration, a sharper asymptotic analysis is possible for AMP."}, {"heading": "4.1 Naive power iteration", "text": "The simplest iterative approach is defined by the following recursion\nv0 = y\n\u2016y\u20162 , and vt+1 = X{vt} \u2016X{vt}\u20162 . Power Iteration\nThe following result establishes convergence criteria for this iteration, first for generic noise Z and then for standard normal noise (using Lemma 2.1).\nTheorem 6. Assume\n\u03b2 \u2265 2 e(k \u2212 1) \u2016Z\u2016op , (41)\n\u3008y,v0\u3009 \u2016y\u20162\n\u2265 [\n(k \u2212 1)\u2016Z\u2016op \u03b2\n]1/(k\u22121) , (42)\nThen for all t \u2265 t0(k), the power iteration estimator satisfies\nLoss(vt,v0) \u2264 2e\u2016Z\u2016op\n\u03b2 . (43)\nIf Z is a standard normal noise tensor, then conditions (41), (41) are satisfied with high probability provided\n\u03b2 \u2265 2ek \u00b5k = 6 \u221a k3 log k ( 1 + ok(1) ) , (44)\n\u3008y,v0\u3009 \u2016y\u20162 \u2265 [ k\u00b5k \u03b2 ]1/(k\u22121) = \u03b2\u22121/(k\u22121) ( 1 + ok(1) ) . (45)\nWe next discuss two aspects of this result: (i) The requirement of a positive correlation between initialization and ground truth ; (ii) Possible scenarios under which the assumptions of Theorem 6 are satisfied.\nNotice that we require a positive correlation of the initialization y with the ground truth v0. The underlying reason is that, if \u3008v0,v0\u3009 is small, then \u3008vt,v0\u3009 remains small at all subsequent\niterations. In order to clarify this point, it is instructive to compute the distribution of v1 for standard Gaussian noise Z. We let\n\u03c4\u03030 \u2261 \u3008v0,y\u3009 \u2016y\u20162 . (46)\nUsing Eq. (9) and the fact that v0 is independent of Z, we get\nv1 = \u03b2\u03c4k\u221210 v0 + n \u22121/2g\u221a \u03b22\u03c4\n2(k\u22121) 0 + 1\n+ o(1) (47)\nwhere g \u223c N(0, In), and o(1) is a vector with \u2016o(1)\u20162 \u2192 0 in probability as n\u2192\u221e. In particular\n\u03c4\u03031 = \u3008v1,v0\u3009 = \u03b2\u03c4\u0303k\u221210\u221a\n\u03b22\u03c4\u0303 2(k\u22121) 0 + 1\n+ o(1) . (48)\nIn particular \u03c41 . \u03c40 only if \u03b2\u03c4k\u221220 & 1, or, equivalently, \u3008y,v0\u3009/\u2016y\u20162 & \u03b2\u22121/(k\u22122). This suggest that the condition in Eq. (45) is not too far from being tight (in the sense that the exponent \u22121/(k \u2212 1) can at best replaced by \u22121/(k \u2212 2)).\nIn general we cannot assume that an initialization satisfying the conditions of Theorem 6. Hence, unlike for ordinary matrix factorization, power iteration is not a practical solution to the tensor principal component problem. There are however circumstances under which a sufficiently good initialization exists.\nExtremely low noise. If y is a uniformly random vector on the unit sphere, then \u3008v0,y\u3009 is approximately normal with mean zero and variance 1/n. For instance |\u3008v0,y\u3009| \u2265 1/ \u221a n with\nprobability roughly 0.32.\nComparing this with condition (42), we obtain that a random initialization succeed with positive probability if\n\u03b2 \u2265 (2n)(k\u22121)/2\u2016Z\u2016op . (49)\nFor standard Gaussian noise, this amounts to requiring \u03b2 \u2265 (2n)(k\u22121)/2\u00b5k. The above heuristic analysis suggests that the correct condition should be \u03b2 & n(k\u22122)/2.\nAdditional side information. Additional information might be available about the vector v0. This information can be used for initiating the power iteration. In the next section we consider the special case in which tensor unfolding is used for initializing power iteration."}, {"heading": "4.2 Comparison with Tensor Unfolding", "text": "It is instructive to compare the result of the previous section with the ones for tensor unfolding, cf. Section 3. Summarizing, for standard Gaussian noise\n\u2022 Tensor unfolding is guaranteed to succeed provided \u03b2 & nb, with b = (dk/2e \u2212 1)/2. We conjecture that a necessary and sufficient condition is in fact \u03b2 & n(k\u22122)/4 (e.g. \u03b2 & n1/4 for order 3 tensors).\n\u2022 Power iteration, with random initialization requires \u03b2 & n(k\u22121)/2. Our heuristic calculation suggests that a necessary and sufficient condition is in fact \u03b2 & n(k\u22122)/2 (e.g. \u03b2 & n1/2 for order 3 tensors)..\nIn other words, tensor unfolding is successful under a signal-to-noise ratio that is order of magnitudes smaller than power iteration. This suggests the following warm start procedure: (i) Compute a first estimate v\u0302Unfold of v0 using tensor unfolding; (ii) Use this as initialization for the power iteration, hence setting v0 = v\u0302Unfold. We will explore this approach numerically in Section 6."}, {"heading": "4.3 Related work", "text": "As mentioned above, power iteration is a natural approach to tensor factorization and was studied in several earlier papers. Most recently, interest within machine learning was spurred by [AGH+12].\nOur Theorem 6 is analogous to the main result of [AGH+12] although incomparable: \u2022 In [AGH+12] the \u2018signal\u2019 part of the tensor X is assumed to have an orthogonal decomposition\u2211n i=1 \u03bbiv \u2297k i with mini(\u03bbi) bounded away from zero. Here, the signal part has rank one\n(equivalently, all the \u03bbi\u2019s but one vanish).\n\u2022 In [AGH+12] only the case of third order tensors (k = 3) is considered. We characterize power iteration for general k.\n\u2022 We establish convergence in a number of iterations t that is independent of the dimensions n. In [AGH+12] the number of iterations is bounded by a polynomial in n.\n\u2022 We evaluate our bounds in the case of Gaussian noise. This allows a comparison with other methods, such as tensor unfolding."}, {"heading": "5 Asymptotics via Approximate Message Passing", "text": "Approximate message passing (AMP) algorithms [DMM09, BM11] proved successful in several highdimensional estimation problems including compressed sensing, low rank matrix reconstruction, and phase retrieval [FRVB11, KRFU12, SC11, SR12]. An appealing feature of this class of algorithms is that their high-dimensional limit can be characterized exactly through a technique known as \u2018state evolution.\u2019 Here we develop an AMP algorithm for tensor data, and its state evolution analysis focusing on the fixed \u03b2, n\u2192\u221e limit. Proofs follows the approach of [BM11] and will be presented in a journal publication.\nIn a nutshell, our AMP for Tensor PCA can be viewed as a sophisticated version of the power iteration method of the last section. With the notation f(x) = x/\u2016x\u20162, we define the AMP iteration over vectors vt \u2208 Rn by v0 = y, f(v\u22121) = 0, and{\nvt+1 = X{f(vt)} \u2212 bt f(vt\u22121) , bt = (k \u2212 1) ( \u3008f(vt), f(vt\u22121)\u3009 )k\u22122 .\nAMP\n(Note that, unlike in power iteration, we normalize vt \u2018before\u2019 multiplying it by X. This choice is equivalent but yields slightly simpler expression.)\nOur main conclusion is that the behavior of AMP is qualitatively similar to the one of power iteration. However, we can establish stronger results in two respects:\n1. We can prove that, unless side information is provided about the signal v0, the AMP estimates remains essentially orthogonal to v0, for any fixed number of iterations. This corresponds to a converse to Theorem 6.\n2. Since state evolution is asymptotically exact, we can prove sharp phase transition results with explicit characterization of their locations.\nWe assume that the additional information takes the form of a noisy observation y = \u03b3 v0 + z, where z \u223c N(0, In/n). Our next results summarizes the state evolution analysis. Its proof is deferred to a journal publication.\nProposition 5.1. Let k \u2265 2 be a fixed integer. Let {v0(n)}n\u22651 be a sequence of unit norm vectors v0(n) \u2208 Sn\u22121. Let also {X(n)}n\u22651 denote a sequence of tensors X(n) \u2208 \u2297kRn generated following Spiked Tensor Model. Finally, let vt denote the t-th iterate produced by AMP, and consider its orthogonal decomposition\nvt = vt\u2016 + v t \u22a5 , (50)\nwhere vt\u2016 is proportional to v0, and v t \u22a5 is perpendicular. Then v t \u22a5 is uniformly random, conditional on its norm. Further, almost surely\nlim n\u2192\u221e \u3008vt,v0\u3009 = lim n\u2192\u221e \u3008vt\u2016,v0\u3009 = \u03c4t , (51)\nlim n\u2192\u221e\n\u2016vt\u22a5\u20162 = 1 , (52)\nwhere \u03c4t is given recursively by letting \u03c40 = \u03b3 and, for t \u2265 0 (we refer to this as to state evolution):\n\u03c42t+1 = \u03b2 2\n( \u03c42t\n1 + \u03c42t\n)k\u22121 . (53)\nNote that state evolution coincides with the equation that we derived for the first iteration of power iteration, cf. Eq. (48) (apart from the different scaling). It is important to notice that for subsequent iterations t \u2265 1, state evolution (53) does not correctly describe naive power iteration. The reason is that vt depends on X, and hence the same argument does not apply. The AMP iteration differ from naive power iteration because of the \u2018memory term\u2019, \u2212bt f(vt\u22121). As shown in [BM11], this memory term approximately cancels dependencies. As a consequence, the resulting algorithm obeys state evolution.\nThe following result characterizes the minimum required additional information \u03b3 to allow AMP to escape from those undesired local optima. We will say that {vt}t converges almost surely to a desired local optimum if, almost surely,\nlim t\u2192\u221e lim n\u2192\u221e\nLoss(vt/\u2016vt\u20162,v0) \u2264 6\n\u03b22 .\nTheorem 7. Consider the Tensor PCA problem with k \u2265 3 and \u03b2 > \u03c9k \u2261 \u221a (k \u2212 1)k\u22121/(k \u2212 2)k\u22122 \u223c \u221a ek .\nThen AMP converges almost surely to a desired local optimum if and only if \u03b3 > \u221a\n1/ k(\u03b2)\u2212 1 where k(\u03b2) is the largest solution of (1\u2212 )(k\u22122) = \u03b2\u22122,\nIn the special case k = 3, and \u03b2 > 2, assuming \u03b3 > \u03b2(1/2 \u2212 \u221a\n1/4\u2212 1/\u03b22), AMP tends to a desired local optimum. Numerically \u03b2 > 2.69 is enough for AMP to achieve \u3008v0, v\u0302\u3009 \u2265 0.9 if \u03b3 > 0.45.\nAs a final remark, we note that the methods of [MR14] can be used to show that, under the assumptions of Theorem 7, for \u03b2 > \u03b2k a sufficiently large constant, AMP asymptotically solves the optimization problem Tensor PCA. Formally, we have, almost surely,\nlim t\u2192\u221e lim n\u2192\u221e \u2223\u2223\u2223\u3008X, (vt)\u2297k\u3009 \u2212 \u2016X\u2016op\u2223\u2223\u2223 = 0. (54)"}, {"heading": "6 Numerical experiments", "text": "Let us emphasize two practical suggestions that arise from our work:\n\u2022 Tensor unfolding is superior to tensor power iteration under our spiked model. For instance, for k = 3, we expect tensor power iteration to require \u03b2 & n1/4 and unfolding to require \u03b2 & n1/2.\n\u2022 For smaller values of \u03b2, iterative methods (tensor power iteration or approximate message passing) only produce a good estimate if the initialization has a scalar product with the ground truth v0 that is bounded away from zero.\n\u2022 As a consequence of the above, side information about the unknown vector v0 can greatly improve performances.\nA special case, we will study the behavior of warm start algorithms that first perform a singular value decomposition of Mat(X), and then apply an iterative method (tensor power iteration or approximate message passing).\nIn this section we will illustrate these suggestions through numerical simulations. Section 6.1 describes a refinement of tensor unfolding that provides a tighter relaxation. Section 6.2 compares different algorithms. Finally, Section 6.3 provides additional illustration of how side information can dramatically simplify the estimation problem."}, {"heading": "6.1 PSD-constrained principal component", "text": "Note that, for v \u2208 Rn, the outer product v\u2297v (regarded as an n\u00d7n matrix) is positive semi-definite (PSD). Considering the case k = 3, we have\nMat(X) = \u03b2vec(v0 \u2297 v0) v0T + Mat(Z) . (55)\nThis remark suggest to perform a cone-constrained principal component analysis of Mat(X), where the left singular vector (viewed as a matrix) belongs to the PDS cone. In order to write this formally, it is convenient to introduce the operator reshapen\u00d7n : Rn\n2 \u2192 Rn\u00d7n that matricizes vectors as reshapen\u00d7n(w)i,j = wn(i\u22121)+j . The PSD-cone-constrained principal component of Mat(X), is defined by\n(w\u0302, v\u0302) \u2261 arg max { \u3008w,Mat(X)v\u3009 : reshapen\u00d7n(w) 0 , \u2016w\u20162 \u2264 1 , \u2016v\u20162 \u2264 1 } . (56)\nThis optimization problem is NP hard, since it includes copositive programming as a special case. However [DMR14] provides rigorous and empirical evidence that problems of this type can be solved efficiently by a projected power iteration, under statistical model dor X.\nDenoting P : Rn 2 \u2192 Rn2 the orthogonal projector onto the PSD cone, we iterate the following\nfor t \u2265 0, using random initialization of u0 \u2208 Rn,{ wt = P (Mat(X)v t),\nvt+1 = Mat(X)Twt/\u2016Mat(X)Twt\u20162 . (57)"}, {"heading": "6.2 Comparison of different algorithms", "text": "In Fig. 1 we compare different algorithms on data generated following Spiked Tensor Model with k = 3, and n \u2208 {25, 50, 100, 200, 400, 800} and for a range of values of \u03b2 \u2208 [2, 10]. The plots represent measured values of the absolute correlation |\u3008v\u0302,v0\u3009| versus \u03b2, averaged over 50 samples (except for n = 800, where we used 8 samples).\nThe main findings are consistent with the theory developed above:\n\u2022 Tensor power iteration (with random initialization) performs poorly with respect to other approaches that use some form of tensor unfolding. The gap widens as the dimension n increases.\n\u2022 PSD-constrained principal component analysis (described in the last section) is slightly superior to plain unfolding.\n\u2022 All algorithms based on initial unfolding have essentially the same threshold. Above that threshold, those that process the singular component (either by recursive unfolding or by tensor power iteration) have superior performances over simpler one-step algorithms.\nIn addition, we noted that the two iterative algorithms (Power Iteration and AMP) show very close behaviors in our experiments.\nIn Figure 2 we compare the scaling with n of the threshold signal-to-noise ratio for different type of algorithms. Our heuristic arguments suggest that tensor power iteration with random initialization will work for \u03b2 & n1/2, while unfolding only requires \u03b2 & n1/4 (our theorems guarantee this for, respectively, \u03b2 & n and \u03b2 & n1/2). We plot the average correlation |\u3008v\u0302,v0\u3009| versus (respectively) \u03b2/n1/2 and \u03b2/n1/4. The curve superposition confirms that our prediction captures the correct behavior already for n of the order of 50."}, {"heading": "6.3 The value of side information", "text": "Our next experiment concerns a simultaneous matrix and tensor PCA task: we are given a tensor X \u2208 \u22973Rn of Spiked Tensor Model with k = 3 and the signal to noise ratio \u03b2 = 3 is fixed. In addition, we observe M = \u03bbv0v0\nT + N where N \u2208 Rn\u00d7n is a symmetric noise matrix with upper diagonal elements i < j iid Ni,j \u223c N(0, 1/n) and the value of \u03bb \u2208 [0, 2] varies. This experiment mimics a rank-1 version of topic modeling method presented in [AGH+12] where M is a matrix representing pairwise co-occurences and X triples.\nThe analysis in previous sections suggest to use the leading eigenvector of M as the initial point of AMP algorithm for tensor PCA on X. We performed the experiments on 100 randomly\ngenerated instances with n = 50, 200, 500 and report in Figure 3 the mean values of |\u3008v0, v\u0302(X)\u3009| with confidence intervals.\nRandom matrix theory predicts limn\u2192\u221e\u3008v\u03021(M),v0\u3009 = \u221a\n1\u2212 \u03bb\u22122 [FP09]. Thus we can set \u03b3 = \u221a 1\u2212 \u03bb\u22122 and apply the theory of the previous section. In particular, Proposition 5.1 implies\nlim n\u2192\u221e\n\u3008v\u0302(X),v0\u3009 = \u03b2 ( 1/2 + \u221a 1/4\u2212 1/\u03b22 ) if \u03b3 > \u03b2 ( 1/2\u2212 \u221a 1/4\u2212 1/\u03b22 )\nand limn\u2192\u221e\u3008v\u0302(X),v0\u3009 = 0 otherwise Simultaneous PCA appears vastly superior to simple PCA. Our theory captures this difference quantitatively already for n = 500."}, {"heading": "Acknowledgements", "text": "This work was partially supported by the NSF grant CCF-1319979 and the grants AFOSR/DARPA FA9550-12-1-0411 and FA9550-13-1-0036.\nA Information theoretic bound: Proof of Theorem 1\nIntroduce the operator\nU :\u2297k Rn \u2192 R( n k)\nX 7\u2192 U(X) ,\nwhere for indices i1 < i2 < \u00b7 \u00b7 \u00b7 < ik, we have U(X)a(i1,\u00b7\u00b7\u00b7 ,ik) = Xi1,\u00b7\u00b7\u00b7 ,ik with a(i1, \u00b7 \u00b7 \u00b7 , ik) = 1 + \u2211k j=1 n\nj\u22121(ij \u2212 1). Let D(\u00b7\u2016\u00b7) denote the Kullback-Leiber divergence where Pw is the law of U(X) conditional on v0 = w. Lemma A.1. For any pairs of vectors w,w\u2032 \u2208 Sn\u22121 we have\nD(Pw\u2016Pw\u2032) \u2264 2 n\nk \u03b22.\nProof. First note that for any w \u2208 Sn\u22121, Pw is a Gaussian probability distribution\nPw = N\n( \u03b2U(w\u2297k),\n1\n(k \u2212 1)!n I(nk)\n) .\nOn the other hand for any symmetric tensor W \u2208 \u2297kRn we have k!\u2016U(W)\u201622 \u2264 \u2016W\u20162F . Therefore we have\nD(Pw\u2016Pw\u2032) = n(k \u2212 1)!\u03b22\u2016U(w\u2297k)\u2212 U(w\u2032\u2297k)\u201622 \u2264 n k \u03b22\u2016w\u2297k \u2212w\u2032\u2297k\u20162F\n= 2n\nk \u03b22(1\u2212 \u3008w,w\u2032\u3009k)\n\u2264 2n k \u03b22.\nWe are now in position to prove Theorem 1. Let V denote the class of estimators v\u0302 with unit norm:\nV =\n{ v\u0302 :\u2297k Rn \u2192 Sn\u22121\nX 7\u2192 v\u0302(X)\n} . (58)\nProof of Theorem 1. Recall that the packing number Nn(\u03b5) of Sn\u22121 is the maximum cardinality of any set N \u2286 Sn\u22121 such that (\u2016x \u2212 x\u2032\u20162 \u2227 \u2016x + x\u2032\u20162) \u2265 \u03b5 for any x,x\u2032 \u2208 N . By a standard argument, letting Mn(\u03b5) the corresponding covering number\n4, we have, for x \u2208 Sn\u22121 a point on the unit sphere,\nNn(\u03b5) \u2265Mn(\u03b5) \u2265 Voln\u22121(Sn\u22121) 2Voln\u22121(Sn\u22121 \u2229B(x, \u03b5)) \u2265 ( 1 \u03b5 )n\u22121 . (59)\n4That is, the minimum cardinality of any set N \u2217 such that minx\u2208N\u2217(\u2016u\u2212 x\u20162 \u2227 \u2016u + x\u20162) \u2264 \u03b5 for any u \u2208 Sn\u22121.\n(Here Voln\u22121( \u00b7 ) denotes the (n\u2212 1)-dimensional volume, and B(x, \u03b5) the ball of radius \u03b5 centered at x.)\nLet N denote an \u03b5-packing with cardinality |N | \u2265 Nn(\u03b5). Let v0 be uniformly distributed in the set N . For an estimator v\u0302 \u2208 V, we define G(v\u0302(X)) = arg minw\u2208N \u2016v\u0302(X) \u2212 w\u20162. Consider the error event {G(v\u0302(X)) 6= v0}. By definition of G(v\u0302(X)), the event G(v\u0302(X)) 6= v0 implies (\u2016v\u0302(X)\u2212 v0\u20162 \u2227 \u2016v\u0302(X) + v0\u20162) \u2265 \u03b5/2. By Markov inequality we have:\nP{G(v\u0302(X)) 6= v0} \u2264 P { (\u2016v\u0302(X)\u2212 v0\u20162 \u2227 \u2016v\u0302(X)\u2212 v0\u20162) \u2265 \u03b5/2}\n\u2264 4E{Loss(v\u0302,v0)} \u03b52 \u2264 4 \u03b52 inf v\u0302\u2208V\nsup v0\u2208Sn\u22121\nE{Loss(v\u0302,v0)} . (60)\nBy Fano\u2019s inequality [CT12] we have that:\nP{G(v\u0302(X)) 6= v0} \u2265 1\u2212 I(v0; X) + log 2\nlog |N | (61)\n\u2265 1\u2212 \u2206 + log 2 log |N | , (62)\nwhere \u2206 = maxw 6=w\u2032\u2208N D(Pw\u2016Pw\u2032), and in the second inequality we used [HV94]\nI(v0; X) \u2264 1 |N |2 \u2211\nw 6=w\u2032\u2208N D(Pw\u2016Pw\u2032) . (63)\nUsing Eq. (59) and Lemma A.1, in Eq. (61), we get\nP{G(v\u0302(X)) 6= v0} \u2265 1\u2212 2n\u03b22/k + log 2\n(n\u2212 1) log(1/\u03b5) . (64)\nChoosing \u03b5 = 1/2, we get, P{G(v\u0302(X)) 6= v0} \u2265 1\u2212 (5\u03b22/k) for n \u2265 4 and \u03b2 \u2264 \u221a k/3. In particular\nP{G(v\u0302(X)) 6= v0} \u2265 1/2 provided \u03b2 < \u221a k/10. By Eq. (60) this implies\ninf v\u0302\u2208V sup v0\u2208Sn\u22121\nE{Loss(v\u0302,v0)} \u2265 1\n32 . (65)"}, {"heading": "B Maximum likelihood: Proof Theorem 2", "text": "B.1 Operator norm of the noise tensor: Proof of Lemma 2.1\nLet Zn \u2208 \u2297kRn be a symmetric standard normal tensor, and consider the associated objective function\nHZ : Sn\u22121 \u2192 R , (66) v 7\u2192 HZ(v) \u2261 \u3008Z,v\u2297k\u3009 . (67)\nWhile the function HZ( \u00b7 ) is obviously non-convex, it turns out that \u2013for random data Z\u2013 it is dramatically so. Namely, it has an exponential number of local maximum, whose value is \u2013typically\u2013 only a fraction of the value of the global maximum.\nIn order to quantify this phenomenon, for x \u2208 R, let Ck(Zn, x) denote the number of local maxima of HZ( \u00b7 ) over Sn\u22121, that have value larger or equal than x. The next Lemma from [ABAC13] characterizes the growth rate of the number of local minima.\nTheorem 8 (Theorem 2.5 and Lemma 6.3 in [ABAC13]). For any k \u2265 3, we have\nlim n\u2192\u221e\n1 n log ECk(Zn, x) = gk(x) , (68)\nwhere, for x \u2265 \u03b7k \u2261 2 \u221a k \u2212 1\ngk(x) = 1\n2 { 2\u2212 k k \u2212 log ( kz2 2 ) + k \u2212 1 2 z2 \u2212 2 k2z2 } , z =\n1\n(k \u2212 1) \u221a 2k\n( x\u2212 \u221a x2 \u2212 4(k \u2212 1) ) .\n(69)\nFurther, for x < \u03b7k, gk(x) = gk(\u03b7k).\nThe function gk(x) is monotone decreasing for x \u2265 \u03b7k, and non-negative if and only if x \u2208 [\u03b7k, \u00b5k] for some \u00b5k > 0 (strictly positive for x \u2208 [\u03b7k, \u00b5k)). In Figure 4, we plot gk(x) for k \u2208 {3, 4, 5}. Informally, this means that the function HZ(v) has exponentially many local maxima with value HZ(v) \u2248 x for any x \u2208 [\u03b7k, \u00b5k). To leading exponential order, the number of such maxima is given by exp{n gk(x)}.\nThe value \u00b5k can be determined as the unique solution to the equation g(x) = 0. It is immediately to do this numerically, obtaining the values in Section 2.\nThe last result implies that the global maximum of HZ(v) is (asymptotically) at least \u00b5k. Indeed the global maximum is necessarily a local maximum as well. The next result implies that indeed the global maximum converges to \u00b5k.\nTheorem 9 (Theorem 2.12 in [ABAC13]). Let \u00b5k denote the unique non-negative root of the equation gk(x) = 0, for x \u2265 \u03b7k \u2261 2 \u221a k \u2212 1. Then\nlim n\u2192\u221e\nE\u2016Z\u2016op = \u00b5k . (70)\nIn order to derive the large-k asymptotics of \u00b5k, we rewrite Eq. (69) in terms of the variable y \u2261 k2z2/2. We get gk(x) = fk(y(x))/2, where\nfk(y) = 2\u2212 k k + log(k)\u2212 log(y) + k \u2212 1 k y \u2212 1 y , x = (k \u2212 1)\n\u221a y\nk +\n\u221a k\ny . (71)\nFurther y \u2208 (0, k/(k \u2212 1)]. The claimed asymptotics follows by showing that the only solution of fk(y) = 0 in this interval obeys yk = (log k)\n\u22121(1 + ok(1)). This in turns can be showed by using the bounds\nlog(ke\u22121+(2/k))\u2212 log y \u2212 1 y \u2264 fk(y) \u2264 log(ke2/k)\u2212 log(y)\u2212 1 y , (72)\nand showing that the solution of y\u22121 + log(y) = log(a) for large a is y\u22121 = a+ \u0398(log(a)). Finally, the norm \u2016Z\u2016op concentrates tightly around its expectation.\nLemma B.1. For any s \u2265 0, we have\nP (\u2223\u2223\u2016Z\u2016op \u2212 E\u2016Z\u2016op\u2223\u2223 \u2265 s) \u2264 2 e\u2212ns2/(2k) . (73)\nProof. Note that\n\u3008Z,v\u2297k\u3009 = \u221a k\nn \u3008G,v\u2297k\u3009 (74)\nis a Lipschitz function with Lipschitz modulus \u221a k/n (with respect to Euclidean norm) of the Gaussian vector (tensor) G. Hence \u2016Z\u2016op is Lipchitz continuous with the same modulus. The claim follows from Gaussian isoperimetry [Led01].\nRemark B.2. Note that to make the connection with the notations used in [ABAC13], one has to use the proper scaling Hn,k(v) =\nn\u221a k LZ(v/\n\u221a n) (Hn,k(v)is the objective function considered in\n[ABAC13]).\nRemark B.3. The upper bound on the tensor operator norm obtained from Sudakov-Fernique inequality is not tight. In fact taking \u03b2 = 0 in Lemma 2.2 gives the loose upper bound \u2016Z\u2016op \u2264 k. Except in the case of random matrices (k = 2), this is loose roughly by a factor \u221a k.\nB.2 Proof of Theorem 2\nBy optimality of v\u0302, we have\n\u03b2\u3008v0, v\u0302\u3009k + \u3008Z, v\u0302\u2297k\u3009 \u2265 \u03b2 + \u3008Z,v0\u2297k\u3009 , (75)\nwhence\n\u3008v0, v\u0302\u3009k \u2265 1\u2212 1 \u03b2 \u3008Z, v\u0302\u2297k \u2212 v0\u2297k\u3009 (76)\n\u2265 1\u2212 1 \u03b2\n( \u2016Z\u2016op \u2212 \u3008Z,v0\u2297k\u3009 ) . (77)\nNote that \u2016Z\u2016op \u2212 \u3008z,v0\u2297k\u3009 is Lipchitz continuous in the Gaussian random variables G, with modulus bounder by 2 \u221a k/n. Hence, by Gaussian isoperimetry, with probability at least 1 \u2212 2e\u2212ns 2/(8k), we have (since Z and v0 are independent, E\u3008Z,v0\u2297k\u3009 = 0)\n\u3008v0, v\u0302\u3009k \u2265 1\u2212 1\n\u03b2\n( E\u2016Z\u2016op + s ) (78)\nUsing (1\u2212\u03b1)1/k \u2265 (1\u2212\u03b1) which holds for \u03b1 \u2208 [0, 1], and rescaling s, we get |\u3008v0, v\u0302\u3009| \u2265 1\u2212(\u00b5k+s)/\u03b2 with probability at least 1\u2212 2e\u2212ns2/(16k) for all n large enough.\nB.3 Proof of Lemma 2.2\nLemma B.4. For each n \u2208 N, let g \u223c N(0, In/n) and v0(n) \u2208 Rn be a vector with \u2016v0(n)\u20162 = 1. Then there exists a sequence \u03b4n independent from x, such that limn\u2192\u221e \u03b4n = 0 and the following happens. With probability one, there exists (a random) n0 such that, for all n \u2265 n0,\nsup \u03c4\u2208[0,\u03c4max] \u2223\u2223\u2223 \u2016g + \u03c4v0\u20162 \u2212\u221a1 + \u03c42\u2223\u2223\u2223 \u2264 \u03b4n . (79) Proof. Since x 7\u2192 \u221a x is uniformly continuous on bounded intervals [0,M ], it is sufficient to prove\nsup \u03c4\u2208[0,\u03c4max] \u2223\u2223\u2223 \u2016g + \u03c4v0\u201622 \u2212 (1 + \u03c42)\u2223\u2223\u2223 \u2264 \u03b4n . (80) for all n \u2265 n0, and an eventually different sequence \u03b4n. By triangular inequality and using \u2016v0\u20162 = 1,\nsup \u03c4\u2208[0,\u03c4max] \u2223\u2223\u2223 \u2016g + \u03c4v0\u201622 \u2212 (1 + \u03c42)\u2223\u2223\u2223 \u2264 \u2223\u2223\u2016g\u20162 \u2212 1\u2223\u2223+ 2\u03c4max\u2223\u2223\u3008v0,g\u3009\u2223\u2223 (81) Next we have limn\u2192\u221e \u2016g\u20162 = 1 almost surely by the strong law of large numbers, and \u3008v0,g\u3009 \u223c N(0, 1/n) whence limn\u2192\u221e\u3008v0,g\u3009 = 0 by Borel-Cantelli.\nProof of Lemma 2.2. For \u03ba \u2208 [0, 1], we define\nW\u03ba \u2261 { v \u2208 Sn\u22121 : \u3008v,v0\u3009 = \u03ba } , (82)\nMX(\u03ba) \u2261 max { \u3008X,v\u2297k\u3009 : v \u2208 W\u03ba } , (83)\nM(\u03ba) \u2261 EMX(\u03ba) = Emax { \u3008X,v\u2297k\u3009 : v \u2208 W\u03ba } . (84)\nNote that\n\u03bb1(X) = max \u03ba\u2208[0,1] MX(\u03ba). (85)\nThe function X 7\u2192 MX(\u03ba) is a Lipschitz continuous function with Lipschitz constant \u221a k/n of the standard Gaussian tensor G (namely |MX(\u03ba) \u2212MX\u2032(\u03ba)| \u2264 (k/n)1/2\u2016G \u2212G\u2032\u2016F ). Hence, by Gaussian isoperimetry, we have\nP {\u2223\u2223MX(\u03ba)\u2212M(\u03ba)\u2223\u2223 \u2265 t} \u2264 2 e\u2212nt2/(2k) . (86)\nFurther we claim that \u03ba 7\u2192MX(\u03ba) is uniformly continuous for \u03ba \u2208 [0, 1]. In order to prove this, let v(\u03ba) = \u03bav0 + \u221a 1\u2212 \u03ba2v\u22a5 = argmax { \u3008X,v\u2297k\u3009 : v \u2208 W\u03ba } , (87)\nwhere \u3008v\u22a5,v0\u3009 = 0. We have, for \u03ba1, \u03ba2 \u2208 [0, 1], and by letting v\u22a5 and w\u22a5 denote the perpendicular components of v(\u03ba1) and v(\u03ba2), we have for some constant c > 0\nMX(\u03ba1) =\u3008X,v(\u03ba1)\u2297k\u3009 = \u3008X, ( \u03ba1v0 + \u221a 1\u2212 \u03ba21v \u22a5 ) \u2297k\u3009\n\u2265 \u3008X, ( \u03ba1v0 + \u221a 1\u2212 \u03ba21w \u22a5 ) \u2297k\u3009 by optimality\n= \u3008X, { \u03ba2v0 + \u221a 1\u2212 \u03ba22w \u22a5 + (\u03ba1 \u2212 \u03ba2)v0 + (\u221a 1\u2212 \u03ba21 \u2212 \u221a 1\u2212 \u03ba22 ) w\u22a5 } \u2297k\u3009 = MX(\u03ba2) + k\u2211 q=1 ( k q ) \u3008X,v(\u03ba2)\u2297q \u2297 { (\u03ba1 \u2212 \u03ba2)v0 + ( \u221a 1\u2212 \u03ba21 \u2212 \u221a 1\u2212 \u03ba22)w \u22a5 }\u2297(k\u2212q) \u3009\n(88)\n\u2265MX(\u03ba2)\u2212 c\u2016X\u2016op { (\u03ba1 \u2212 \u03ba2)2 + (\u221a 1\u2212 \u03ba21 \u2212 \u221a 1\u2212 \u03ba22 )2}1/2 . (89)\nwhere Eq. (88) was obtained by exploiting the symmetry of the tensor X and Eq. (89) was derived using the norm of the vector { (\u03ba1 \u2212 \u03ba2)v0 + ( \u221a 1\u2212 \u03ba21 \u2212 \u221a 1\u2212 \u03ba22)w\u22a5 } . Using Eq. (86) over a grid \u03ba \u2208 {0, 1/n, 2/n, . . . , 1\u2212 1/n, 1}, and the fact5 that \u2016X\u2016op \u2264 C for some constant C > 0 with probability 1\u2212 e\u2212\u0398(n), we have for all t > 0 and some constant c > 0\nP {\nmax \u03ba\u2208[0,1] \u2223\u2223MX(\u03ba)\u2212M(\u03ba)\u2223\u2223 \u2265 t} \u2264 2ne\u2212nct2/2 + 2 e\u2212cn . (90) In particular, by Borel-Cantelli we have, almost surely,\nlim n\u2192\u221e max \u03ba\u2208[0,1] \u2223\u2223MX(\u03ba)\u2212M(\u03ba)\u2223\u2223 = 0 . (91) In order to upper bound M(\u03ba), we apply Sudakov-Fernique inequality for non-centered Gaussian processes [Vit00, Theorem 1] to the two processes {Xv}, {Yv} indexed by v \u2208 W\u03ba defined as follows:\nXv \u2261 \u3008X,v\u2297v\u3009 = \u03b2\u3008v0,v\u3009k + \u3008Z,v\u2297k\u3009 , (92) Yv \u2261 \u03b2\u3008v0,v\u3009k + k\u3008g,v\u3009 , (93)\n5This follows from Lemma 2.1 and triangular inequality, or from a standard \u03b5-net argument.\nfor random a vector g \u223c N(0, In/n). It is easy to see that EXv = EYv and\nE {[ Xv \u2212Xw ]2} = {EXv \u2212 EXw}2 + 2 k\nn\n( 1\u2212 \u3008v,w\u3009k ) , (94)\nE {[ Yv \u2212 Yw ]2} = {EYv \u2212 EYw}2 + 2 k2\nn\n( 1\u2212 \u3008v,w\u3009 ) . (95)\nHence E {[ Xv \u2212 Xw ]2} \u2264 E{[Yv \u2212 Yw]2} (this follows from 1\u2212 ak \u2264 k(1\u2212 a) for a \u2208 [\u22121, 1]). We conclude that\nM(\u03ba) \u2264 Emax { \u03b2\u03bak +\nk\u221a n \u3008g,v\u3009 : v \u2208 W\u03ba\n} (96)\n\u2264 max \u03c4\u22650 {( \u03c4\u221a 1 + \u03c42 )k + k\u221a 1 + \u03c42 } + \u03b4n (97)\nwhere \u03c4 = \u03ba/ \u221a\n1\u2212 \u03ba2 and \u03b4n satisfies limn\u2192\u221e \u03b4n = 0 uniformly over \u03ba \u2208 [0, 1], by Lemma B.4. We finally conclude that\nlim sup n\u2192\u221e E\u2016X\u2016op \u2264 max \u03c4\u22650\n( \u03c4\u221a\n1 + \u03c42\n)k +\nk\u221a 1 + \u03c42 . (98)\nConcentration around the expectation follows by Gaussian isoperimetry as in the proof of Lemma 2.1."}, {"heading": "C Power Iteration: Proof of Theorem 6", "text": "Let \u03c4t \u2261 \u3008v0,vt\u3009 and \u03be \u2261 \u2016Z\u2016op/\u03b2. Let \u03c4min, \u03c4\u2217 \u2208 [0, 1] be the two solutions of\n\u03c4k\u22121(1\u2212 \u03c4) = \u03be . (99)\nWe will show below that our assumptions imply \u03c40 > \u03c4min. Further \u03c4 \u2265 \u03c4min implies \u03c4k\u22121 \u2212 \u03be \u2265 0. By definition of X, we have\nX{vt} = \u03b2\u03c4k\u22121t v0 + Z{vt} , (100)\nwhich implies, by triangular inequality,\n\u3008v0,X{vt}\u3009 \u2265 \u03b2\u03c4k\u22121t \u2212 \u03b2\u03be , (101) \u2016X{vt}\u20162 \u2264 \u03b2\u03c4k\u22121t + \u03b2\u03be . (102)\nWe will prove the first inequality \u03c4t \u2265 \u03c4min by induction. It is true at t = 0 by assumption. Assume it is true at t. Then \u03c4t+1 \u2265 0 using Eq. (101).\nHence we can divide the two inequalities above obtaining \u03c4t+1 \u2265 (\u03c4k\u22121t \u2212 \u03be)/(\u03c4 k\u22121 t + \u03be) which\nimplies\n\u03c4t+1 \u2265 1\u2212 \u03be\n\u03c4k\u22121t . (103)\nIn particular \u03c4t \u2265 \u03c4\u03030t where the latter sequence is defined by \u03c4\u0303t+1 = f(\u03c4\u0303t), \u03c4\u03030 = \u03c40, for f(x) = 1\u2212 \u03be x\u2212k+1. The function f( \u00b7 ) is concave and monotone increasing, and maps [\u03c4min, \u03c4\u2217] into itself, with f \u2032(\u03c4min) > 1, f\n\u2032(\u03c4\u2217) < 1. By standard calculus argument \u03c4\u0303t \u2192 \u03c4\u2217 exponentially fast, which implies\n\u3008vt,v0\u3009 \u2265 \u03c4\u2217 \u2212 c0 e\u2212t/c0 . (104)\nTo conclude the proof of Eq. (43), we notice that, for \u03be \u2264 1/(2e(k \u2212 1))\n\u03c4\u2217 > 1\u2212 e \u03be , (105) \u03c4min < [(k \u2212 1)\u03be]1/(k\u22121) , (106)\nwhere we recall that \u03c4min, \u03c4\u2217 are the two solutions of gk(x) \u2261 xk\u22121(1 \u2212 x) = \u03be in the interval [0, 1]. For the first inequality, note that, in the interval [e\u22121/(k\u22121), 1], gk(x) is decreasing with gk(x) \u2265 e\u22121(1\u2212 x). This implies\ne\u22121(1\u2212 \u03c4\u2217) \u2265 \u03be (107)\ni.e. \u03c4\u2217 \u2265 1\u2212 e \u03be as long as 1\u2212 e \u03be \u2265 e\u22121/(k\u22121), which is implied by \u03be \u2264 1/(2e(k \u2212 1)). For the second inequality, note that, in the interval [0, 1\u2212 (k \u2212 1)\u22121], we have gk(x) increasing with gk(x) \u2265 xk\u22121/(k \u2212 1). This implies\n\u03c4k\u22121min k \u2212 1 \u2265 \u03be , (108)\nas long as [(k \u2212 1)\u03be]1/k\u22121 \u2264 1\u2212 (k \u2212 1)\u22121, which follows, again, by our assumptions. Finally, conditions (44), (45) follow directly by applying Lemma 2.1."}, {"heading": "D Approximate Message Passing: Proof of Theorem 7", "text": "Let us recall the state evolution recursion (53)\n\u03c42t+1 = f(\u03c4 2 t ;\u03b2) , (109) f(z;\u03b2) \u2261 \u03b22 ( z\n1 + z\n)k\u22121 . (110)\nNotice that f( \u00b7 ;\u03b2) is strictly positive and monotone increasing on R>0. The theorem follows by proving that the following claims hold for \u03b2 > \u03c9k\n1. The fixed point equation \u03c42 = f(\u03c42;\u03b2) has two strictly positive solutions \u03c421 (\u03b2) < \u03c4 2 2 (\u03b2). 2. The smallest fixed point is given by \u03c41(\u03b2) = \u221a 1/ k(\u03b2)\u2212 1 as in the statement.\n3. The largest fixed point satisfies \u03c42(\u03b2) > 1\u2212 (2/\u03b22).\nThe behavior of the function f(\u03c42;\u03b2) is illustrated in Fig. 5.\nIn order to prove the above statements, it is convenient to use the monotone parametrization x \u2261 \u03c42/(1 + \u03c42) that maps R\u22650 onto the interval [0, 1). After some algebra (and discarding the solution at x = 0), fixed point equation then reads\n1\n\u03b22 = xk\u22122(1\u2212 x) \u2261 hk(x) . (111)\nNow, the function x 7\u2192 hk(x) is continuously differentiable and strictly positive in the interval (0, 1), with hk(0) = hk(1) = 0. Further, simple calculus shows it has a unique stationary point (a maximum) at x\u2217 = (k \u2212 2)/(k \u2212 1) with hk(x\u2217) = 1/\u03c92k. This implies that, for \u03b2 > \u03c9k, Eq. (111) has two fixed points 0 < x2(\u03b2) < x\u2217 < x1(\u03b2) < 1 thus implying points 1 and 2 above (the latter immediately follows from inverting the re-parametrization).\nTo prove point 3, note that\nx2 = 1\u2212 1\nxk\u221222 \u03b2 2\n(112)\n\u2265 1\u2212 1 xk\u22122\u2217 \u03b22\n(113)\n\u2265 1\u2212 3 \u03b22\n(114)\nwhere the second inequality follows since x2 > x\u2217. By state evolution (Proposition 5.1), together with the fact that \u03c4t \u2192 \u03c42, we have\nlim t\u2192\u221e lim n\u2192\u221e\nLoss(vt/\u2016vt\u20162,v0) = 2 ( 1\u2212 \u221a \u03c422\n1 + \u03c422\n) (115)\n= 2(1\u2212 \u221a x2) \u2264 2(1\u2212 x2) \u2264 6\n\u03b22 . (116)"}], "references": [{"title": "Random matrices and complexity of spin glasses", "author": ["A. Auffinger", "G. Ben Arous", "J. Cerny"], "venue": "Communications on Pure and Applied Mathematics", "citeRegEx": "Auffinger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auffinger et al\\.", "year": 2013}, {"title": "A tensor spectral approach to learning mixed membership community", "author": ["Anima Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M Kakade"], "venue": null, "citeRegEx": "Anandkumar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2013}, {"title": "The singular values and vectors of low rank perturbations of large rectangular random matrices", "author": ["Florent Benaych-Georges", "Raj Rao Nadakuditi"], "venue": "Journal of Multivariate Analysis", "citeRegEx": "Benaych.Georges and Nadakuditi,? \\Q2012\\E", "shortCiteRegEx": "Benaych.Georges and Nadakuditi", "year": 2012}, {"title": "The dynamics of message passing on dense graphs, with applications to compressed sensing", "author": ["M. Bayati", "A. Montanari"], "venue": "IEEE Trans. on Inform. Theory", "citeRegEx": "Bayati and Montanari,? \\Q2011\\E", "shortCiteRegEx": "Bayati and Montanari", "year": 2011}, {"title": "Spectral Analysis of Large Dimensional Random Matrices (2nd edition)", "author": ["Z. Bai", "J. Silverstein"], "venue": null, "citeRegEx": "Bai and Silverstein,? \\Q2010\\E", "shortCiteRegEx": "Bai and Silverstein", "year": 2010}, {"title": "Exact matrix completion via convex optimization, Foundations of Computational mathematics", "author": ["E.J. Cand\u00e8s", "B. Recht"], "venue": null, "citeRegEx": "Cand\u00e8s and Recht,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht", "year": 2009}, {"title": "The spherical p-spin interaction spin glass model: the statics", "author": ["Andrea Crisanti", "H-J Sommers"], "venue": "Zeitschrift fu\u0308r Physik B Condensed Matter", "citeRegEx": "Crisanti and Sommers,? \\Q1992\\E", "shortCiteRegEx": "Crisanti and Sommers", "year": 1992}, {"title": "Thouless-Anderson-Palmer approach to the spherical p-spin spin glass model", "author": ["A Crisanti", "H-J Sommers"], "venue": "Journal de Physique I", "citeRegEx": "Crisanti and Sommers,? \\Q1995\\E", "shortCiteRegEx": "Crisanti and Sommers", "year": 1995}, {"title": "The Dantzig selector: Statistical estimation when p is much larger than n, The Annals of Statistics", "author": ["E. Candes", "T. Tao"], "venue": null, "citeRegEx": "Candes and Tao,? \\Q2007\\E", "shortCiteRegEx": "Candes and Tao", "year": 2007}, {"title": "Elements of information", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "Cover and Thomas,? \\Q2012\\E", "shortCiteRegEx": "Cover and Thomas", "year": 2012}, {"title": "A tensor-based algorithm for high-order graph matching", "author": ["O. Duchenne", "F. Bach", "I. Kweon", "J. Ponce"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Duchenne et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Duchenne et al\\.", "year": 2009}, {"title": "Optimally sparse representation in general (nonorthogonal) dictionaries via `1 minimization", "author": ["D.L. Donoho", "M. Elad"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "Donoho and Elad,? \\Q2003\\E", "shortCiteRegEx": "Donoho and Elad", "year": 2003}, {"title": "Message Passing Algorithms for Compressed Sensing", "author": ["D.L. Donoho", "A. Maleki", "A. Montanari"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "Donoho et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Donoho et al\\.", "year": 2009}, {"title": "Cone-constrained principal component analysis", "author": ["Y. Deshpande", "A. Montanari", "E. Richard"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Deshpande et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Deshpande et al\\.", "year": 2014}, {"title": "Local operator theory, random matrices and Banach spaces", "author": ["K.R. Davidson", "S.J. Szarek"], "venue": "Handbook on the Geometry of Banach spaces,", "citeRegEx": "Davidson and Szarek,? \\Q2001\\E", "shortCiteRegEx": "Davidson and Szarek", "year": 2001}, {"title": "Local operator theory, random matrices and banach spaces, Handbook of the geometry of Banach spaces", "author": ["Kenneth R Davidson", "Stanislaw J Szarek"], "venue": null, "citeRegEx": "Davidson and Szarek,? \\Q2001\\E", "shortCiteRegEx": "Davidson and Szarek", "year": 2001}, {"title": "P\u00e9ch\u00e9, The largest eigenvalues of sample covariance matrices for a spiked population: diagonal case", "author": ["S.D. F\u00e9ral"], "venue": "Journal of Mathematical Physics", "citeRegEx": "F\u00e9ral,? \\Q2009\\E", "shortCiteRegEx": "F\u00e9ral", "year": 2009}, {"title": "Neural reconstruction with approximate message passing (neuramp)", "author": ["A.K. Fletcher", "S. Rangan", "L.R. Varshney", "A. Bhargava"], "venue": null, "citeRegEx": "Fletcher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fletcher et al\\.", "year": 2011}, {"title": "A limit theorem for the norm of random matrices", "author": ["S. Geman"], "venue": "Annals of Probability", "citeRegEx": "Geman,? \\Q1980\\E", "shortCiteRegEx": "Geman", "year": 1980}, {"title": "Most tensor problems are np-hard", "author": ["Christopher J Hillar", "Lek-Heng Lim"], "venue": "Journal of the ACM (JACM)", "citeRegEx": "Hillar and Lim,? \\Q2013\\E", "shortCiteRegEx": "Hillar and Lim", "year": 2013}, {"title": "Generalizing the fano inequality", "author": ["Te Han", "Sergio Verdu"], "venue": "Information Theory, IEEE Transactions on", "citeRegEx": "Han and Verdu,? \\Q1994\\E", "shortCiteRegEx": "Han and Verdu", "year": 1994}, {"title": "On consistency and sparsity for principal components analysis in high dimensions", "author": ["I. M Johnstone", "A.Y. Lu"], "venue": "Journal of the American Statistical Association", "citeRegEx": "Johnstone and Lu,? \\Q2009\\E", "shortCiteRegEx": "Johnstone and Lu", "year": 2009}, {"title": "Shifted power method for computing tensor eigenpairs", "author": ["Tamara G Kolda", "Jackson R Mayo"], "venue": "SIAM Journal on Matrix Analysis and Applications", "citeRegEx": "Kolda and Mayo,? \\Q2011\\E", "shortCiteRegEx": "Kolda and Mayo", "year": 2011}, {"title": "Approximate message passing with consistent parameter estimation and applications to sparse learning", "author": ["U. Kamilov", "S. Rangan", "A.K. Fletcher", "M. Unser"], "venue": null, "citeRegEx": "Kamilov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kamilov et al\\.", "year": 2012}, {"title": "The concentration of measure phenomenon", "author": ["M. Ledoux"], "venue": "Mathematical Surveys and Monographs,", "citeRegEx": "Ledoux,? \\Q2001\\E", "shortCiteRegEx": "Ledoux", "year": 2001}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Square deal: Lower bounds and improved relaxations for tensor recovery", "author": ["C. Mu", "J. Huang", "B. Wright", "D. Goldfarb"], "venue": "International Conference in Machine Learning (ICML),", "citeRegEx": "Mu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mu et al\\.", "year": 2013}, {"title": "Non-negative principal component analysis: Message passing algorithms and sharp asymptotics", "author": ["Andrea Montanari", "Emile Richard"], "venue": null, "citeRegEx": "Montanari and Richard,? \\Q2014\\E", "shortCiteRegEx": "Montanari and Richard", "year": 2014}, {"title": "Asymptotics of sample eigenstructure for a large dimensional spiked covariance model", "author": ["Debashis Paul"], "venue": "Statistica Sinica", "citeRegEx": "Paul,? \\Q2007\\E", "shortCiteRegEx": "Paul", "year": 2007}, {"title": "A new convex relaxation for tensor completion", "author": ["B. Romera-Paredes", "M. Pontil"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Romera.Paredes and Pontil,? \\Q2013\\E", "shortCiteRegEx": "Romera.Paredes and Pontil", "year": 2013}, {"title": "Approximate message passing for bilinear models", "author": ["P. Schniter", "V. Cevher"], "venue": "Proc. Workshop Signal Process. Adaptive Sparse Struct. Repr.(SPARS),", "citeRegEx": "Schniter and Cevher,? \\Q2011\\E", "shortCiteRegEx": "Schniter and Cevher", "year": 2011}, {"title": "Compressive phase retrieval via generalized approximate message passing, Communication, Control, and Computing (Allerton), 2012", "author": ["P. Schniter", "S. Rangan"], "venue": "50th Annual Allerton Conference on,", "citeRegEx": "Schniter and Rangan,? \\Q2012\\E", "shortCiteRegEx": "Schniter and Rangan", "year": 2012}, {"title": "Free energy of the spherical mean field model, Probability theory and related fields", "author": ["Michel Talagrand"], "venue": null, "citeRegEx": "Talagrand,? \\Q2006\\E", "shortCiteRegEx": "Talagrand", "year": 2006}, {"title": "Greed is good: Algorithmic results for sparse approximation, Information Theory", "author": ["J. A Tropp"], "venue": "IEEE Transactions on", "citeRegEx": "Tropp,? \\Q2004\\E", "shortCiteRegEx": "Tropp", "year": 2004}, {"title": "Statistical performance of convex tensor decomposition", "author": ["R. Tomioka", "T. Suzuki", "K. Hayashi", "H. Kashima"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Tomioka et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tomioka et al\\.", "year": 2011}, {"title": "Some comparisons for gaussian processes", "author": ["R.A. Vitale"], "venue": "Proceedings of the American Mathematical Society", "citeRegEx": "Vitale,? \\Q2000\\E", "shortCiteRegEx": "Vitale", "year": 2000}, {"title": "The absolute-value estimate for symmetric multilinear forms, Linear Algebra and its Applications", "author": ["W.C. Waterhouse"], "venue": null, "citeRegEx": "Waterhouse,? \\Q1990\\E", "shortCiteRegEx": "Waterhouse", "year": 1990}, {"title": "Perturbation bounds in connection with singular value decomposition, BIT Numerical Mathematics", "author": ["P.A. Wedin"], "venue": null, "citeRegEx": "Wedin,? \\Q1972\\E", "shortCiteRegEx": "Wedin", "year": 1972}], "referenceMentions": [], "year": 2014, "abstractText": "We consider the Principal Component Analysis problem for large tensors of arbitrary order k under a single-spike (or rank-one plus noise) model. On the one hand, we use information theory, and recent results in probability theory, to establish necessary and sufficient conditions under which the principal component can be estimated using unbounded computational resources. It turns out that this is possible as soon as the signal-to-noise ratio \u03b2 becomes larger than C \u221a k log k (and in particular \u03b2 can remain bounded as the problem dimensions increase). On the other hand, we analyze several polynomial-time estimation algorithms, based on tensor unfolding, power iteration and message passing ideas from graphical models. We show that, unless the signal-to-noise ratio diverges in the system dimensions, none of these approaches succeeds. This is possibly related to a fundamental limitation of computationally tractable estimators for this problem. We discuss various initializations for tensor power iteration, and show that a tractable initialization based on the spectrum of the matricized tensor outperforms significantly baseline methods, statistically and computationally. Finally, we consider the case in which additional side information is available about the unknown signal. We characterize the amount of side information that allows the iterative algorithms to converge to a good estimate.", "creator": "LaTeX with hyperref package"}}}