{"id": "1411.1112", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2014", "title": "Learning of Agent Capability Models with Applications in Multi-agent Planning", "abstract": "One important challenge for a set of agents to achieve more efficient collaboration is for these agents to maintain proper models of each other. An important aspect of these models of other agents is that they are often partial and incomplete. Thus far, there are two common representations of agent models: MDP based and action based, which are both based on action modeling. In many applications, agent models may not have been given, and hence must be learnt. While it may seem convenient to use either MDP based or action based models for learning, in this paper, we introduce a new representation based on capability models, which has several unique advantages. First, we show that learning capability models can be performed efficiently online via Bayesian learning, and the learning process is robust to high degrees of incompleteness in plan execution traces (e.g., with only start and end states). While high degrees of incompleteness in plan execution traces presents learning challenges for MDP based and action based models, capability models can still learn to {\\em abstract} useful information out of these traces. As a result, capability models are useful in applications in which such incompleteness is common, e.g., robot learning human model from observations and interactions. Furthermore, when used in multi-agent planning (with each agent modeled separately), capability models provide flexible abstraction of actions. The limitation, however, is that the synthesized plan is incomplete and abstract.", "histories": [["v1", "Tue, 4 Nov 2014 23:54:07 GMT  (408kb,D)", "http://arxiv.org/abs/1411.1112v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.MA", "authors": ["yu zhang", "subbarao kambhampati"], "accepted": false, "id": "1411.1112"}, "pdf": {"name": "1411.1112.pdf", "metadata": {"source": "CRF", "title": "Learning of Agent Capability Models with Applications in Multi-agent Planning", "authors": ["Yu Zhang", "Subbarao Kambhampati"], "emails": ["yzhan442@asu.edu", "rao@asu.edu"], "sections": [{"heading": "Introduction", "text": "One important challenge for a set of agents to achieve more efficient collaboration is for these agents to maintain proper models of others. These models can be used to reduce communication and collaboration efforts. In many applications, agent models may not have been given, and hence must be learnt. Thus far, there are two common representations of agent models: MDP based (Puterman 1994) and action based (Fox and Long 2003), which are both based on action modeling. In this paper, we introduce a new representation based on capability models. We represent a capability as the ability to achieve a partial state given another partial state. Contrasting to action modeling in MDP based and action based models, a capability implicitly corresponds to a set of action sequences (or plans) that satisfy a specification,\ndescribed as the transition between two partial states. Each such action sequence is called an operation. The capability model represents a model that captures the probabilities of the existence of an operation for these specifications.\nCompared to MDP based and action based models, which do not naturally represent incomplete and abstract models, capability model has its unique benefits and limitations. In this aspect, capability models should not be considered as competitors to these more complete models. Instead, they are more useful when only incomplete and abstract information can be obtained for the models, e.g., human models.\nThe representation of the capability model is a generalization of a two time slice dynamic Bayesian network (2- TBN). Each node at the first level (also called a fact node) represents a variable that is used in the specification of a world state. The underlying structure, i.e., the (potentially partial) causal relationships between the fact nodes, is assumed to be provided by the domain expert. Furthermore, for each fact node, there is also a corresponding node that represents the final value of this fact as the result of an operation. In this paper, these corresponding nodes are called eventual nodes or e-nodes. The links between the e-nodes, as well as the links from the fact nodes to the e-nodes in the capability model, follow the same causal relationships (i.e., links) between the fact nodes. Furthermore, each fact node is also connected to its corresponding e-node (from the fact node to e-node). Figure 1 presents a capability model. One remark is that learning causal relationships to dynamically update the structures of capability models is possible (White and Frederiksen 1990), which is to be considered in future work. We argue that this information is easier to obtain for certain models, e.g., human models. Instead of modeling individual human actions, it is easier to associate causes with effects. For example, given a cup of water, the effect could be a cup of milk or coffee, even though complex sequences of actions may be required for these effects.\nLearning the parameters of capability models can be performed efficiently online via Bayesian learning. One of the unique advantages of capability model is that its learning process is robust to high degrees of incompleteness in plan execution traces. This incompleteness occurs commonly. For example, the execution observer may not always stay close enough to the executing agent, and the executing agent may not report the full traces (Zhuo and Kambhampati\nar X\niv :1\n41 1.\n11 12\nv1 [\ncs .A\nI] 4\nN ov\n2 01\n4\n2013). Capability models can even learn when only the initial and final states are given for plan executions. While high degrees of incompleteness in plan execution traces presents learning challenges for MDP based and action based models, capability models can still learn to abstract useful information out of these traces, e.g., the probability of the existence of a plan (operation) to achieve a final (partial) state given an initial (partial) state. When more information about the traces is given, it can be used to refine the capability models as with the other models. Hence, capability model is useful for applications in which the agent models must be learnt from incomplete traces, e.g., robot learning human model from observations and interactions.\nFurthermore, compared to MDP based and action based representations, capability models provide more flexible abstraction of actions. As a result, when using capability models in multi-agent planning, the planning performance can be potentially improved. Note that \u201cplanning\u201d with a single agent as a capability model is essentially Bayesian inference. The limitation with using capability models in planning, however, is that the synthesized plan is incomplete and abstract (see details later). Nevertheless, capability models can be used to create approximate multi-agent plans. In our settings, we assume a centralized planner for now. Also, part of the agents are each modeled by a capability model as in Figure 1, and other agents are modeled with more complete models (e.g., action based models). Agents with more complete models can be used to refine the plans when necessary. This situation can naturally occur in human-robot teaming scenarios, in which human models need to be learnt and the robot models are given. At the end of this paper, we present two problem formulations to demonstrate this."}, {"heading": "Motivating Example", "text": "We start with a motivating example to demonstrate the motivations for the capability model. In this example, we have\ntemporarily recruited human delivery agent, and the tasks involve delivering different parcels to their specified destinations. To deliver a parcel, an agent needs to first drive to a location that is close to the delivery location and then move the parcel to the location. However, there are a few complexities here. An agent may not be able to carry parcel, since the agent may not be strong enough. While the agent can use a trolley to load parcel, the agent may or may not remember to bring a trolley. The agent can visit a local rental office to rent trolley, but there is a chance that the agent may have forgotten to bring money for the rental. The trolley increases the probability of the agent being able to move the parcel to the delivery location. The rented trolley needs to be returned by the agent after use at the end of the day. Figure 1 presents the capability model for an individual human delivery agent.\nA capability model is a generalized 2-TBN. Meanwhile, one important difference between capability model and 2- TBN is, while the nodes in the same time slice in a 2- TBN are assumed to be synchronous, fact nodes (or enodes) in capability models are not necessarily synchronous within the (partial) initial and final states of an operation. This can also be seen from Figure 1, in which the edges between fact nodes (or e-nodes) can be labeled with actions. For example, given a capability specification, suppose that has money(AG) is true in the initial state, and delivered(parcel) is true in the final state. If a trolley is used in an operation for this capability, in which has trolley(AG) becomes true only after applying the action rent trolley, has trolley(AG) and has money(AG) are clearly not synchronous in the initial state of this operation. Capability models allow us to encode the probabilities of transitions between two partial states.\nTo learn the agent models from experiences, the agents must be given a set of delivery tasks. However, the only information that the manager has access to may be the number of parcels that have been delivered. While this provides significant learning challenges for the parameters of MDP based and action based models, the useful information for the manager is already encoded: the probability that the agent can deliver a parcel. Capability model can learn this information from the incomplete traces. When more information is provided, it can be used to refine the capability models as with the other models. One of our goals is to provide a model that can learn useful information, subject to various levels of incompleteness in plan execution traces.\nNow, suppose that the manager (i.e., a centralized planner) also has a set of robots (with action based models) to use that can bring goods into the delivery van. Observing that the presence of trolley increases the probability of delivery success, the manager can make a multi-agent plan in which the robot always ensures that there is a trolley in the van, before the delivery agents start working. This illustrates how capability models can be combined with other more complete models to refine the plans."}, {"heading": "Related Work", "text": "In MDP based representation, each node represents a state, and each edge represents a transition between the two connected states after applying an action. In action based rep-\nresentation, each action has a set of preconditions that must be satisfied in the current state, and a set of effects that specify changes to the current state when the action is applied. Assuming that the model structures are given (as with capability models) by the domain expert, the learning task involves only learning the model parameters. The learning is performed given a set of the training examples, which are plan execution traces of the agent whose model is being learnt.\nFor MDP based representation, the agent is often modeled as a Partially Observable MDP (Kaelbling, Littman, and Moore 1996)) or POMDP, with an underlying Hidden Markov Model (HMM). The learning process needs to learn the transition probabilities between the states, as well as the probabilities of observations given the hidden states. Although researchers have investigated efficient learning methods (Strehl and Littman 2007), in general, learning in such models is still an intense computational task. Online learning methods for POMDP models have also been explored (Shani, Brafman, and Shimony 2005). One note is that capability model can be considered as a special factored-MDP, which is often represented as a dynamic Bayesian network (Sanner 2011).\nIn terms of learning for action based models, there are many previous works that discuss about learning the structures of actions, e.g., (Zhuo and Kambhampati 2013). As discussed previously, learning in our context is more about learning the parameters. In this aspect, there are not many works about parameter learning in action based models.\nMeanwhile, both MDP based and action based models have been used in multi-agent planning. While Dec-POMDP (Seuken and Zilberstein 2007) and other related models (e.g., (Gmytrasiewicz and Doshi 2004)) have been popular, multi-agent planning for action based models has so far been concentrated on planning with deterministic models (Brafman and Domshlak 2008). Planning with multiple agents using action based models in stochastic domains still remains to be investigated.\nCapability model has connections to HTNs (Erol, Hendler, and Nau 1994), since both provide flexible abstractions. Finally, although there are other works that discuss about capability models, e.g., (Buehler and Pagnucco 2014), they are still based on action modeling."}, {"heading": "Capability Model", "text": "For simplicity, we assume in this paper that the state of the world is specified as a set of boolean variables X , which includes the agent state. More specifically, for all Xi \u2208 X , Xi has domain D(Xi) = {true, false}. First, we formally define capability for agent \u03c6, in which we denote the set of variables that are relevant to \u03c6 as X\u03c6 \u2286 X . Definition 1 (Capability). Given an agent \u03c6, and four subsets of variables of X\u03c6, A, B, C and D, a capability, specified as C \u2227 \u00acD \u2192 A \u2227 \u00acB, is an assertion about the existence of plans with initial state I and goal state G that satisfy the following: \u2200Xi \u2208 C, I[Xi] = true, \u2200Xi \u2208 D, I[Xi] = false,\n\u2200Xi \u2208 A,G[Xi] = true, \u2200Xi \u2208 B,G[Xi] = false.\nin which S[Xi] represents the value of Xi in state S. We call a plan that satisfies the above specification of a capability as an operation. Recall that the capability model captures the probabilities of the existence of an operation based on the specifications of capabilities, given in the form of C \u2227\u00acD \u2192 A\u2227\u00acB. For brevity, we use A to denote that A = {true} and \u00acA to denote that A = {false}. We construct the capability model of an agent as a Bayesian network from (potentially partial) causal relationships, which are assumed to be provided by the domain writer. Due to this partiality, certain causal relationships among variables may not have been captured. This also means that certain variables may not have been included inX\u03c6 even when it should have. For example, whether an agent can drive a car to a new location is dependent on whether the agent can drive a manual car, even through the agent has a driver license. However, the ability to drive a manual car may have been ignored by the domain expert when creating the capability model.\nWe model the capability model of each agent as an augmented Bayesian network (Neapolitan 2004). We use augmented Bayesian network since it allows various types of prior beliefs to be specified. Furthermore, we make the causal embedded faithfulness assumption (Neapolitan 2004). This assumption assumes that the probability distribution of the observed variables is encoded accurately (i.e., embedded faithfully (Neapolitan 2004)) in a causal DAG that contains these variables and the hidden variables (i.e, variables that are not modeled). While there are exceptions in which this assumption does not hold, their discussion is beyond the scope of this paper (see (Neapolitan 2004) for details). First, we introduce the definition of augmented Bayesian network.\nDefinition 2. An augmented Bayesian network (ABN) (G,F, \u03c1) is a Bayesian network with the following specifications:\n\u2022 A DAGG = (V,E), where V is a set of random variables, V = {V1, V2, ..., Vn}.\n\u2022 \u2200Vi \u2208 V , an auxiliary parent variable Fi \u2208 F of Vi, and a density function \u03c1i associated with Fi. Each Fi is a root and it is only connected to Vi.\n\u2022 \u2200Vi \u2208 V , for all values pai of the parents PAi \u2286 V of Vi, and for all values fi of Fi, a probability distribution P (Vi|pai, fi).\nA capability model of an agent then is defined as follows:\nDefinition 3 (Capability Model). A capability model of an agent \u03c6, as a binomial ABN (G,F, \u03c1), has the following specifications:\n\u2022 V\u03c6 = X\u03c6 \u222a X\u0307\u03c6. \u2022 \u2200Xi \u2208 V , the domain of Xi is D(Xi) = {true, false}. \u2022 \u2200Xi \u2208 V , Fi = {Fi1, Fi2, ...}, and each Fij is a root and\nhas a density function \u03c1ij(fij) (0 \u2264 fij \u2264 1). \u2022 \u2200Xi \u2208 V , P (Xi = true|paij , fi1, ...fij , ...) = fij .\nin which j in paij indexes into the values of PAi, while j in Fi indexes into the jth variable. X\u0307\u03c6 represents the set of eventual nodes or e-nodes that correspond (i.e., one-to-one mapping) to the nodes in X\u03c6. Recall that e-nodes represent the final value of a variable after an operation.\nAs discussed previously, the edges in the capability model of an agent is constructed based on (potentially partial) causal relationships. We assume that there are no causal feedback loops. Otherwise, loops can be broken randomly. Denote the set of edges that represent the given causal relationships between variables in X\u03c6 as Y\u03c6. For each edge Xi \u2192 Xj \u2208 Y\u03c6, we also add an edge Xi \u2192 X\u0307j . Furthermore, for each Xi \u2208 X\u03c6, we also add an edge Xi \u2192 X\u0307i. Denote the set of edges added as Y\u0307\u03c6. We then have G\u03c6 = (V\u03c6, E\u03c6) in the capability model satisfy E\u03c6 = Y\u03c6\u222a Y\u0307\u03c6. Figure 1 provides a simple example of a capability model."}, {"heading": "Model Learning", "text": "The learning of the capability model (for agent \u03c6) is performed online through Bayesian learning. The parameters (i.e., fij for F ) in the capability models of agents can be learnt from the previous plan execution traces. Plan execution traces can be collected each time that an operation succeeds or fails. When the values of this set of parameters are assumed, the conditional distributions in Def. 3 are also known. Definition 4 (Plan Execution Trace). A plan execution trace is a sequence of discontinuous state observations, T = {S1, Si, ..., Sj , SK}.\nIn the worse case, we only assume that S1 and SK are given. Note that since the observations are discontinuous, the transition between contiguous state observations is not necessarily the result of a single action. When more than the initial and final states are given in the trace, it can be considered as a set of traces, broken in the form of {{S1, Si}, ..., {Sj , SK}}. Furthermore, when observations of the state are incomplete, the simplest solution is to generate all possible compatible plan traces with complete state specifications. We assume that the new plan execution traces are processed as above, and a training set D is generated.\nA common way is to model Fij using a beta distribution (i.e., as its density function \u03c1). Refer to (Neapolitan 2004) for arguments for this distribution. Denote the parameters for the beta distribution of Fij as aij and bij .\nSuppose that the initial values or the current values for aij and bij are given. The only remaining task is to update aij and bij from the traces. Given the training set D, we can now follow Bayesian inference to update the parameters of Fij as follows:\nInitially or currently,\n\u03c1(fij) = beta(fij ; aij , bij) (1)\nAfter observing new training examples D, we have:\n\u03c1(fij |D) = beta(fij ; aij + sij , bij + tij) (2) in which sij is the number in whichXi \u2208 V\u03c6 is equal to true with PAi taking the value of paij , and tij is the number in which it equals false with PAi taking the value of paij ."}, {"heading": "More about Capability Models", "text": "As discussed, the capability model is also a two level dynamic Bayesian network, except that what connects the two levels are operations rather than actions. Theorem 1. Planning with a single agent using capability model is NP-hard, but is polynomial when the capability model is a Polytree.\nThe proof is straightforward since planning with a single agent using capability model is equivalent to Bayesian inference. However, as we discussed previously, the synthesized plan is incomplete and abstract given the inherent incomplete and abstract nature of such models. In the single agent case, the output is only a probability measure with a single operation (i.e., the most abstract plan). For multi-agent planning, this incompleteness and abstraction introduce a few complexities when using the model .\nSuppose that we plan to use an operation with the capability specification of C \u2227 \u00acD \u2192 A \u2227 \u00acB. Through Bayesian inference, the capability model would return the corresponding probability of P (A\u0307 \u2227 \u00acB\u0307|C \u2227 \u00acD).1 We use the corresponding variables (i.e., e-nodes) here for A and B, since they represent the same variables after the operation. This measure represents the probability of the existence of an operation for the specification of C\u2227\u00acD \u2192 A\u2227\u00acB. It means that this operation, unfortunately, may fail with certain probability as well.\nFurthermore, note that here, nothing about the values of other variables are specified in the (partial) initial and final states in the specification of this operation. This means that the agent may be relying on other variables (that are not in C or D) in the complete initial state to execute this operation (which may cause a failure for this operation), and that the agent can change the values of other variables (that are not in A or B) in the complete final state after this operation. To reduce the possible outcomes of an operation, we assume that the agent is rational, such that the agent would only change variables that are related to the achievement of A and \u00acB, i.e.,AC\u03c6(A\u222aB), which represents the union set of ancestors of A and B in X\u03c6 for agent \u03c6.\nIn the following, we note a few properties that the capability model satisfies:\nMonotonicity:\nP (A\u0307,\u00acB\u0307|C,\u00acD) \u2265 P (A\u0307\u2032,\u00acB\u0307|C,\u00acD)(A \u2286 A\u2032) (3)\nP (A\u0307,\u00acB\u0307|C,\u00acD) \u2265 P (A\u0307,\u00acB\u0307\u2032|C,\u00acD)(B \u2286 B\u2032) (4) These imply that it is always more difficult to achieve the specified values for more variables. Another aspect of monotonicity follows:\nP (A\u0307,\u00acB\u0307|C,\u00acD) \u2265 P (A\u0307,\u00acB\u0307|C \u2032,\u00acD)(C \u2287 C) (5)\nP (A\u0307,\u00acB\u0307|C,\u00acD) \u2265 P (A\u0307,\u00acB\u0307|C,\u00acD\u2032)(D \u2286 D\u2032) (6) 1Note that A\u0307, representing \u2227a\u2208Aa\u0307 = true, is not equivalent to \u2200a \u2208 A, a = true after an operation. Nevertheless, we assume that, e.g., P (A\u0307|C), is an approximation of the probability that an operation exists, which satisfies the capability specification of C \u2192 A (i.e., all variables in A become true after this operation).\nThese imply that it is always easier to achieve the desired state with more true-value variables. This property holds with the assumption that no negated propositions appear in the preconditions of actions. Note that this assumption does not reduce the generality of the domain, since negated propositions can be compiled away."}, {"heading": "MAP-MM", "text": "In this section and next, we concentrate on how capability models can be used along with other more complete models to refine the synthesized plans. The settings are similar to our motivating example. In the first problem, which we call multi-agent planning with mixed models (MAP-MM). This formulation is useful especially for applications that involve both human and robot agents. While robot agents are programmed and hence have given models (we assume action based models here), the models of human agents must be learnt. Hence, we use the agent capability model to model human agents and use the STRIPS action model to model robot agents. We assume that the models for the human agents are already learnt in the following discussions.\nFor robot agents, we assume the STRIPS action model \u3008R,O\u3009, in which R is a set of predicates with typed variables, O is a set of STRIPS operators. Each operator o \u2208 O is associated with a set of preconditions Pre(o) \u2286 R, add effects Add(o) \u2286 R and delete effects Del(o) \u2286 R. Definition 5. Given a set of robots R = {r}, a set of human agents \u03a6 = {\u03c6}, and a set of typed objects O, a multi-agent planning problem with mixed models is given by a tuple \u03a0 = \u3008X,\u03a6, R, I, U,G\u3009, where: \u2022 X is a finite set of propositions (i.e., corresponding to the\nset of variables specifying the world state) instantiated from predicates R and objects O; I \u2286 X encodes the set of propositions that are true in the initial state; U \u2286 X encodes the set of propositions that are unknown; other propositions (i.e., X \\ I \\ U ) are assumed to be false; G \u2286 X encodes the set of goal propositions.\n\u2022 A(r) is the set of actions that are instantiated fromO and O, which r \u2208 R can perform.\n\u2022 B\u03c6 is the capability model (see Figure 1 for an example) of \u03c6 \u2208 \u03a6, and V\u03c6 \u2286 X .\nThe solution for a MAP-MM problem is a plan that maximizes the probability of success.\nTheorem 2. The MAP-MM problem is at least PSPACEcomplete.\nProof. We only need to prove the result for one of the extreme cases: when there are only robot agents. The problem then essentially becomes a multi-agent planning problem, which is more general than the classical planning problem, which is known to be PSPACE-complete."}, {"heading": "Planning for MAP-MM", "text": "We discuss in this section how to use an A\u2217 search process to perform planning for a MAP-MM problem . The current planning state S is composed of three sets of propositions,\ndenoted as T (S),N(S), and U(S). T (S) is the set of propositions known to be true in the current state, N(S) is the set of propositions known to be false, and U(S) is the set of propositions that are unknown. Given the current state S, the (centralized) planner can expand it in the next step using the following options:\n\u2022 Choose an action a \u2208 A(r) such that Pre(a) \u2286 T (S). \u2022 Choose a human operation on human agent \u03c6 with the\nspecification C \u2227 \u00acD \u2192 A \u2227 \u00acB, such that C \u2286 T (S) and D \u2286 N(S). For an action a from the robot agent, the planning state after applying a becomes S\u2032, such that T (S\u2032) = T (S) \u222a Add(a) \\Del(a), N(S\u2032) = N(S) \u222aDel(a) \\ Add(a), and U(S\u2032) = U(S) \\Add(a) \\Del(a). Note that the number of unknown variables is reduced after applying this action.\nFor a human operation on agent \u03c6, the planning state after applying the operation becomes T (S\u2032) = T (S) \u222a A \\ B \\ AC\u03c6(A\u222aB), N(S\u2032) = N(S)\u222aB \\A \\AC\u03c6(A\u222aB), and U(S\u2032) = U(S) \u222a AC\u03c6(A \u222a B) \\ A \\ B. The number of unknown variables increases.\nTo compute the f (i.e., cost) values for the A\u2217 search, we take the negative logarithms of the associated probabilities of reachable planning states. Each planning state that is reachable from the initial state is associated with a probability of success, which is the product of the probabilities of success of the operations used to reach the current state from the initial state, given the partial plan constructed. Hence, the g value can be computed straightforwardly. To compute an admissible h value to each planning state, we denote the set of goal propositions that are currently false or unknown, and that can only be added by a human agent as G\u03a6. Denote the set of human agents that can add p \u2208 G\u03a6 by \u03a6p We first compute the human agent with the minimum cost to achieve p \u2208 G\u03a6 based on the following formula:\n\u03c6p = argmax \u03c6\u2208\u03a6p\n\u2212 logP (p|X\u03c6 \\ p) (7)\nWe then compute the h value as:\nh(S) = max p\u2208G\u03a6 \u2212 logP (p|X\u03c6p \\ p) (8)\nLemma 1. The heuristic given in Eq. (8) is admissible.\nProof. We need to prove that h(S) is not an over-estimate of the cost for any state S, given G. If G\u03a6 is empty, h(S) = 0, and it must not be an over-estimate given non-negative cost measures. Otherwise, the propositions in G\u03a6 must be made true by a human agent. The minimum cost that has to be incurred is no less than the maximum cost for making an individual proposition true, given that all the other propositions are true. This result is based on the monotonicity property. Hence, the conclusion holds.\nLemma 2. The heuristic given in Eq. (8) is consistent.\nProof. We need to prove that h(S) \u2212 h(S\u2032) \u2264 dist(S, S\u2032) (interpreted as from S to S\u2032). Note the asymmetry of the distance measure here. From Eq. (8), suppose that h(S) = \u2212 logP (p|X\u03c6p \\ p) and that h(S\u2032) =\n\u2212 logP (p\u2032|X\u03c6\u2032p\u2032 \\ p \u2032). If p = p\u2032, clearly, we have h(S) = h(S\u2032), and given non-negative distance measures, the conclusion holds. Otherwise, first, we know that S does not contain p and S\u2032 does not contain p\u2032. Furthermore, we cannot have that both S and S\u2032 contain neither p or p\u2032, given Eq. (7), since otherwise we would have h(S) = h(S\u2032), which implies that p = p\u2032. Without loss of generality, assume that h(S) \u2265 h(S\u2032). As a result, we must have that S\u2032 contains p, since otherwise we would have chosen p in h(S\u2032). Hence, dist(S, S\u2032) must be at least the cost for making p true in S. Hence, based on the monotonicity property, the conclusion holds.\nLem. 1 and 2 ensure that the A\u2217 search is optimally efficient, meaning that the search expands only the necessary nodes (i.e., planning state) in order to find the optimal solution. Nevertheless, given the complexity, approximation solutions must be provided to scale to large problem instances."}, {"heading": "MAP-MMI", "text": "There are many extensions to the MAP-MM problem. One immediate extension is to consider interaction cost between the human and robot agents, which we call MAP-MMI (for interaction-aware MAP-MM). While MAP-MM is useful for a plan that is most likely to succeed, MAP-MM formulation is useful when agents must coordinate to execute a conditional plan during the execution phase, and communication is limited. In particular, the human agents need to be informed when to execute an operation.\nIn such scenarios, the plan made is similar to a conditional plan. More specifically, the plan should specify the planning branches depending on whether an operation succeeds or fails. During plan execution, an actual request to perform the operation would be made. A failure would be reported when that operation fails to be executed in the current situation.\nIn MAP-MMI, for each request of a human operation (which is planned during the planning phase but incurred in the execution phase), there is a cost associated. For simplicity, we assume that the cost is constant per request. Since the communication is limited, only a fixed number of requests may be made. Definition 6. A MAP-MMI problem is a MAP-MM problem with a communication threshold C. The solution is a conditional plan with the maximum probability of success while keeping the number of requests (i.e., interaction cost) below or equal to C. Corollary 1. MAP-MMI is at least PSPACE-complete.\nProof. Since MAP-MMI can be considered as a special case of MAP-MM with a constraint on the interaction cost, the conclusion is straightforward. More specifically, we can reduce a MAP-MM problem to a MAP-MMI problem. Suppose that we have an efficient solution for MAP-MMI. We can derive an efficient solution for any MAP-MM problem by simply ending operation failures in undefined states, while assigning a large value to C. It is not difficult to see that the solution to this MAP-MMI problem is also the solution for the original MAP-MM problem.\nPlanning for MAP-MMI can be performed in a similar way as in MAP-MM. The major differences lie in how a planning state S is represented and how it can be expanded. In particular, a state S in MAP-MMI contains more than one substate to be expanded, since we are creating a conditional plan. Denote s \u2208 S as one substate (which is similar to a planning state in MAP-MM), and each such substate is associated with a probability of success. We can perform the similar expansions to each s in S as in MAP-MM, except that the resulting states are different, and that the number of requests from each state S is upper-bounded by C.\nWhen an action is chosen from a \u2208 A(r), s is expanded similarly as in MAP-MM. When a human operation on agent \u03c6 is used in the form of C \u2227 \u00acD \u2192 A \u2227 \u00acB, s is expanded into two substates. One substate represents that the operation succeeds, and the other one representing a failure. The probability of success with s is also split onto these two substates based on the probability of success of the chosen operation computed from B\u03c6. While the substate for a successful operation is updated according to our previous discussion for MAP-MM, the substate when the operation fails is updated in a different manner, since A may not have been achieved. In particular, the new substate s\u2032 is updated such that T (s\u2032) = T (s) \\ AC\u03c6(A \u222a B) \\ A \\ B, N(s\u2032) = N(s) \\ AC\u03c6(A \u222a B) \\ A \\ B, and U(s\u2032) = U(s) \u222aA \u222aB \u222aAC\u03c6(A \u222aB).\nThe heuristic cost for h can be used for each substate as in Eq. (8). To combine the heuristic costs for different substates, we need to first convert the costs to a probability measures, and then sum these converted measures to obtain the probability of success for S. This probability can then be converted to a cost measure to obtain the h value for S. The admissibility and consistency properties then follow immediately from Lem. 1 and 2."}, {"heading": "Conclusions", "text": "In this paper, we introduce a new representation to model agents based on capability models, which has several unique advantages. The underlying structures of these models are generalized 2-TBNs, and we assume that the (potentially partial) causal relationships between the nodes in the networks are provided by the domain expert. First, we show that learning capability models can be performed efficiently online via Bayesian learning, and the learning process is robust to high degrees of incompleteness in plan execution traces (e.g., with only start and end states). Furthermore, when used in multi-agent planning (with each agent modeled by a capability model), our models provide flexible abstraction of actions, which can potentially improve planning performance. The limitation, however, is that the synthesized plan is incomplete and abstract. At each step, instead of specifying which action to execute as with other models, the plan only specifies a partial state to be achieved. We provide details on how these models are constructed and how the parameters are learnt and updated online. Moreover, we introduce two multi-agent planning problems that use these models, and provide heuristic planning methods for them. These problems also illustrate how capability models can be combined with other more complete models to refine the plans."}], "references": [{"title": "From One to Many: Planning for Loosely Coupled Multi-Agent Systems", "author": ["R.I. Brafman", "C. Domshlak"], "venue": "ICAPS, 28\u201335. AAAI Press.", "citeRegEx": "Brafman and Domshlak,? 2008", "shortCiteRegEx": "Brafman and Domshlak", "year": 2008}, {"title": "A framework for task planning in heterogeneous multi robot systems based on robot capabilities", "author": ["J. Buehler", "M. Pagnucco"], "venue": "AAAI Conference on Artificial Intelligence.", "citeRegEx": "Buehler and Pagnucco,? 2014", "shortCiteRegEx": "Buehler and Pagnucco", "year": 2014}, {"title": "Htn planning: Complexity and expressivity", "author": ["K. Erol", "J. Hendler", "D.S. Nau"], "venue": "In Proceedings of the Twelfth National Conference on Artificial Intelligence, 1123\u20131128. AAAI Press.", "citeRegEx": "Erol et al\\.,? 1994", "shortCiteRegEx": "Erol et al\\.", "year": 1994}, {"title": "PDDL2.1: An extension to pddl for expressing temporal planning domains", "author": ["M. Fox", "D. Long"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "Fox and Long,? \\Q2003\\E", "shortCiteRegEx": "Fox and Long", "year": 2003}, {"title": "Interactive pomdps: Properties and preliminary results", "author": ["P.J. Gmytrasiewicz", "P. Doshi"], "venue": "Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 3, AAMAS \u201904, 1374\u20131375. Washington, DC, USA: IEEE Computer Soci-", "citeRegEx": "Gmytrasiewicz and Doshi,? 2004", "shortCiteRegEx": "Gmytrasiewicz and Doshi", "year": 2004}, {"title": "Reinforcement learning: A survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "J. Artif. Int. Res. 4(1):237\u2013285.", "citeRegEx": "Kaelbling et al\\.,? 1996", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "Learning Bayesian networks", "author": ["R.E. Neapolitan"], "venue": "Prentice Hall.", "citeRegEx": "Neapolitan,? 2004", "shortCiteRegEx": "Neapolitan", "year": 2004}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M.L. Puterman"], "venue": "New York, NY, USA: John Wiley & Sons, Inc., 1st edition.", "citeRegEx": "Puterman,? 1994", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Relational dynamic influence diagram language (rddl): Language description", "author": ["S. Sanner"], "venue": null, "citeRegEx": "Sanner,? \\Q2011\\E", "shortCiteRegEx": "Sanner", "year": 2011}, {"title": "Memory-bounded dynamic programming for dec-pomdps", "author": ["S. Seuken", "S. Zilberstein"], "venue": "Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI\u201907, 2009\u20132015. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.", "citeRegEx": "Seuken and Zilberstein,? 2007", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2007}, {"title": "Modelbased online learning of pomdps", "author": ["G. Shani", "R.I. Brafman", "S.E. Shimony"], "venue": "In ECML.", "citeRegEx": "Shani et al\\.,? 2005", "shortCiteRegEx": "Shani et al\\.", "year": 2005}, {"title": "Online linear regression and its application to model-based reinforcement learning", "author": ["E.L. Strehl", "M.L. Littman"], "venue": "In Advances in Neural Information Processing Systems 20 (NIPS-07, 737\u2013744.", "citeRegEx": "Strehl and Littman,? 2007", "shortCiteRegEx": "Strehl and Littman", "year": 2007}, {"title": "Causal model progressions as a foundation for intelligent learning environments", "author": ["B.Y. White", "J.R. Frederiksen"], "venue": "Artificial Intelligence 42(1):99 \u2013 157.", "citeRegEx": "White and Frederiksen,? 1990", "shortCiteRegEx": "White and Frederiksen", "year": 1990}, {"title": "Action-model acquisition from noisy plan traces", "author": ["H.H. Zhuo", "S. Kambhampati"], "venue": "Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI\u201913, 2444\u20132450. AAAI Press.", "citeRegEx": "Zhuo and Kambhampati,? 2013", "shortCiteRegEx": "Zhuo and Kambhampati", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "Thus far, there are two common representations of agent models: MDP based (Puterman 1994) and action based (Fox and Long 2003), which are both based on action modeling.", "startOffset": 74, "endOffset": 89}, {"referenceID": 3, "context": "Thus far, there are two common representations of agent models: MDP based (Puterman 1994) and action based (Fox and Long 2003), which are both based on action modeling.", "startOffset": 107, "endOffset": 126}, {"referenceID": 12, "context": "One remark is that learning causal relationships to dynamically update the structures of capability models is possible (White and Frederiksen 1990), which is to be considered in future work.", "startOffset": 119, "endOffset": 147}, {"referenceID": 11, "context": "Although researchers have investigated efficient learning methods (Strehl and Littman 2007), in general, learning in such models is still an intense computational task.", "startOffset": 66, "endOffset": 91}, {"referenceID": 8, "context": "One note is that capability model can be considered as a special factored-MDP, which is often represented as a dynamic Bayesian network (Sanner 2011).", "startOffset": 136, "endOffset": 149}, {"referenceID": 13, "context": ", (Zhuo and Kambhampati 2013).", "startOffset": 2, "endOffset": 29}, {"referenceID": 9, "context": "While Dec-POMDP (Seuken and Zilberstein 2007) and other related models (e.", "startOffset": 16, "endOffset": 45}, {"referenceID": 4, "context": ", (Gmytrasiewicz and Doshi 2004)) have been popular, multi-agent planning for action based models has so far been concentrated on planning with deterministic models (Brafman and Domshlak 2008).", "startOffset": 2, "endOffset": 32}, {"referenceID": 0, "context": ", (Gmytrasiewicz and Doshi 2004)) have been popular, multi-agent planning for action based models has so far been concentrated on planning with deterministic models (Brafman and Domshlak 2008).", "startOffset": 165, "endOffset": 192}, {"referenceID": 1, "context": ", (Buehler and Pagnucco 2014), they are still based on action modeling.", "startOffset": 2, "endOffset": 29}, {"referenceID": 6, "context": "We model the capability model of each agent as an augmented Bayesian network (Neapolitan 2004).", "startOffset": 77, "endOffset": 94}, {"referenceID": 6, "context": "Furthermore, we make the causal embedded faithfulness assumption (Neapolitan 2004).", "startOffset": 65, "endOffset": 82}, {"referenceID": 6, "context": ", embedded faithfully (Neapolitan 2004)) in a causal DAG that contains these variables and the hidden variables (i.", "startOffset": 22, "endOffset": 39}, {"referenceID": 6, "context": "While there are exceptions in which this assumption does not hold, their discussion is beyond the scope of this paper (see (Neapolitan 2004) for details).", "startOffset": 123, "endOffset": 140}, {"referenceID": 6, "context": "Refer to (Neapolitan 2004) for arguments for this distribution.", "startOffset": 9, "endOffset": 26}], "year": 2014, "abstractText": "One important challenge for a set of agents to achieve more efficient collaboration is for these agents to maintain proper models of each other. An important aspect of these models of other agents is that they are often partial and incomplete. Thus far, there are two common representations of agent models: MDP based and action based, which are both based on action modeling. In many applications, agent models may not have been given, and hence must be learnt. While it may seem convenient to use either MDP based or action based models for learning, in this paper, we introduce a new representation based on capability models, which has several unique advantages. First, we show that learning capability models can be performed efficiently online via Bayesian learning, and the learning process is robust to high degrees of incompleteness in plan execution traces (e.g., with only start and end states). While high degrees of incompleteness in plan execution traces presents learning challenges for MDP based and action based models, capability models can still learn to abstract useful information out of these traces. As a result, capability models are useful in applications in which such incompleteness is common, e.g., robot learning human model from observations and interactions. Furthermore, when used in multi-agent planning (with each agent modeled separately), capability models provide flexible abstraction of actions. The limitation, however, is that the synthesized plan is incomplete and abstract.", "creator": "TeX"}}}