{"id": "1411.2919", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2014", "title": "Bounded Regret for Finite-Armed Structured Bandits", "abstract": "We study a new type of K-armed bandit problem where the expected return of one arm may depend on the returns of other arms. We present a new algorithm for this general class of problems and show that under certain circumstances it is possible to achieve finite expected cumulative regret. We also give problem-dependent lower bounds on the cumulative regret showing that at least in special cases the new algorithm is nearly optimal.", "histories": [["v1", "Tue, 11 Nov 2014 18:55:35 GMT  (27kb)", "http://arxiv.org/abs/1411.2919v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tor lattimore", "r\u00e9mi munos"], "accepted": true, "id": "1411.2919"}, "pdf": {"name": "1411.2919.pdf", "metadata": {"source": "CRF", "title": "Bounded Regret for Finite-Armed Structured Bandits", "authors": ["Tor Lattimore"], "emails": ["tlattimo@ualberta.ca", "remi.munos@inria.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 1.\n29 19\nv1 [\ncs .L\nG ]\n1 1\nN ov"}, {"heading": "1 Introduction", "text": "The multi-armed bandit problem is a reinforcement learning problem with K actions. At each timestep a learner must choose an action i after which it receives a reward distributed with mean \u00b5i. The goal is to maximise the cumulative reward. This is perhaps the simplest setting in which the wellknown exploration/exploitation dilemma becomes apparent, with a learner being forced to choose between exploring arms about which she has little information, and exploiting by choosing the arm that currently appears optimal. (a)\n\u00b5\n\u22121 0 1\n\u22121\n0\n1 (b)\n\u22121 0 1\n(c)\nWe consider a general class of Karmed bandit problems where the expected return of each arm may be dependent on other arms. This model has already been considered when the dependencies are linear [18] and also in the general setting studied here [12, 1]. Let \u0398 \u220b \u03b8\u2217 be an arbitrary\nparameter space and define the expected return of arm i by \u00b5i(\u03b8\u2217) \u2208 R. The learner is permitted to know the functions \u00b51 \u00b7 \u00b7 \u00b7\u00b5K , but not the true parameter \u03b8\u2217. The unknown parameter \u03b8\u2217 determines the mean reward for each arm. The performance of a learner is measured by the (expected) cumulative regret, which is the difference between the expected return of the optimal policy and the (expected) return of the learner\u2019s policy. Rn := nmaxi\u22081\u00b7\u00b7\u00b7K \u00b5i(\u03b8\u2217) \u2212 \u2211n t=1 \u00b5It(\u03b8\n\u2217) where It is the arm chosen at time-step t.\nA motivating example is as follows. Suppose a long-running company must decide each week whether or not to purchase some new form of advertising with unknown expected returns. The problem may be formulated using the new setting by letting K = 2 and \u0398 = [\u2212\u221e,\u221e]. We assume the base-line performance without purchasing the advertising is known and so define \u00b51(\u03b8) = 0 for all \u03b8. The expected return of choosing to advertise is \u00b52(\u03b8) = \u03b8 (see Figure (b) above).\nOur main contribution is a new algorithm based on UCB [6] for the structured bandit problem with strong problem-dependent guarantees on the regret. The key improvement over UCB is that the new algorithm enjoys finite regret in many cases while UCB suffers logarithmic regret unless all arms have the same return. For example, in (a) and (c) above we show that finite regret is possible for all\n1Current affiliation: Google DeepMind.\n\u03b8\u2217, while in the advertising problem finite regret is attainable if \u03b8\u2217 \u2265 0. The improved algorithm exploits the known structure and so avoids the famous negative results by Lai and Robbins [17]. One insight from this work is that knowing the return of the optimal arm and a bound on the minimum gap is not the only information that leads to the possibility of finite regret. In the examples given above neither quantity is known, but the assumed structure is nevertheless sufficient for finite regret.\nDespite the enormous literature on bandits, as far as we are aware this is the first time this setting has been considered with the aim of achieving finite regret. There has been substantial work on exploiting various kinds of structure to reduce an otherwise impossible problem to one where sublinear (or even logarithmic) regret is possible [19, 4, 10, and references therein], but the focus is usually on efficiently dealing with large action spaces rather than sub-logarithmic/finite regret. The most comparable previous work studies the case where both the return of the best arm and a bound on the minimum gap between the best arm and some sub-optimal arm is known [11, 9], which extended the permutation bandits studied by Lai and Robbins [16] and more general results by the same authors [15]. Also relevant is the paper by Agrawal et. al. [1], which studied a similar setting, but where \u0398 was finite. Graves and Lai [12] extended the aforementioned contribution to continuous parameter spaces (and also to MDPs). Their work differs from ours in a number of ways. Most notably, their objective is to compute exactly the asymptotically optimal regret in the case where finite regret is not possible. In the case where finite regret is possible they prove only that the optimal regret is sub-logarithmic, and do not present any explicit bounds on the actual regret. Aside from this the results depend on the parameter space being a metric space and they assume that the optimal policy is locally constant about the true parameter."}, {"heading": "2 Notation", "text": "General. Most of our notation is common with [8]. The indicator function is denoted by 1{expr} and is 1 if expr is true and 0 otherwise. We use log for the natural logarithm. Logical and/or are denoted by \u2227 and \u2228 respectively. Define function \u03c9(x) = min {y \u2208 N : z \u2265 x log z, \u2200z \u2265 y}, which satisfies log\u03c9(x) \u2208 O(log x). In fact, limx\u2192\u221e log(\u03c9(x))/ log(x) = 1.\nBandits. Let \u0398 be a set. A K-armed structured bandit is characterised by a set of functions \u00b5k : \u0398 \u2192 R where \u00b5k(\u03b8) is the expected return of arm k \u2208 A := {1, \u00b7 \u00b7 \u00b7 ,K} given unknown parameter \u03b8. We define the mean of the optimal arm by the function \u00b5\u2217 : \u0398 \u2192 R with \u00b5\u2217(\u03b8) := maxi \u00b5i(\u03b8). The true unknown parameter that determines the means is \u03b8\u2217 \u2208 \u0398. The best arm is i\u2217 := argmaxi \u00b5i(\u03b8\n\u2217). The arm chosen at time-step t is denoted by It while Xi,s is the sth reward obtained when sampling from arm i. We denote the number of times arm i has been chosen at time-step t by Ti(t). The empiric estimate of the mean of arm i based on the first s samples is \u00b5\u0302i,s. We define the gap between the means of the best arm and arm i by \u2206i := \u00b5\u2217(\u03b8\u2217) \u2212 \u00b5i(\u03b8\u2217). The set of sub-optimal arms is A\u2032 := {i \u2208 A : \u2206i > 0}. The minimum gap is \u2206min := mini\u2208A\u2032 \u2206i while the maximum gap is \u2206max := maxi\u2208A \u2206i. The cumulative regret is defined\nRn := n \u2211\nt=1\n\u00b5\u2217(\u03b8\u2217)\u2212 n \u2211\nt=1\n\u00b5It = n \u2211\nt=1\n\u2206It\nNote quantities like \u2206i and i\u2217 depend on \u03b8\u2217, which is omitted from the notation. As is rather common we assume that the returns are sub-gaussian, which means that if X is the return sampled from some arm, then lnE exp(\u03bb(X \u2212 EX)) \u2264 \u03bb2\u03c32/2. As usual we assume that \u03c32 is known and does not depend on the arm. If X1 \u00b7 \u00b7 \u00b7Xn are sampled independently from some arm with mean \u00b5 and Sn = \u2211n t=1 Xt, then the following maximal concentration inequality is well-known.\nP\n{\nmax 1\u2264t\u2264n |St \u2212 t\u00b5| \u2265 \u03b5\n}\n\u2264 2 exp\n(\n\u2212 \u03b52\n2n\u03c32\n)\n.\nA straight-forward corollary is that P {|\u00b5\u0302i,n \u2212 \u00b5i| \u2265 \u03b5} \u2264 2 exp\n(\n\u2212 \u03b52n\n2\u03c32\n)\n.\nIt is an important point that \u0398 is completely arbitrary. The classic multi-armed bandit can be obtained by setting \u0398 = RK and \u00b5k(\u03b8) = \u03b8k, which removes all dependencies between the arms. The setting where the optimal expected return is known to be zero and a bound on \u2206i \u2265 \u03b5 is known can be regained by choosing \u0398 = (\u2212\u221e,\u2212\u03b5]K\u00d7{1, \u00b7 \u00b7 \u00b7 ,K} and \u00b5k(\u03b81, \u00b7 \u00b7 \u00b7 , \u03b8K , i) = \u03b8k1{k 6= i}. We do not demand that \u00b5k : \u0398 \u2192 R be continuous, or even that \u0398 be endowed with a topology."}, {"heading": "3 Structured UCB", "text": "We propose a new algorithm called UCB-S that is a straight-forward modification of UCB [6], but where the known structure of the problem is exploited. At each time-step it constructs a confidence interval about the mean of each arm. From this a subspace \u0398\u0303t \u2286 \u0398 is constructed, which contains the true parameter \u03b8 with high probability. The algorithm takes the optimistic action over all \u03b8 \u2208 \u0398\u0303t.\nAlgorithm 1 UCB-S\n1: Input: functions \u00b51, \u00b7 \u00b7 \u00b7 , \u00b5k : \u0398 \u2192 [0, 1] 2: for t \u2208 1, . . . ,\u221e do\n3: Define confidence set \u0398\u0303t \u2190\n{\n\u03b8\u0303 : \u2200i, \u2223 \u2223\n\u2223 \u00b5i(\u03b8\u0303)\u2212 \u00b5\u0302i,Ti(t\u22121)\n\u2223 \u2223 \u2223 <\n\u221a\n\u03b1\u03c32 log t\nTi(t\u2212 1)\n}\n4: if \u0398\u0303t = \u2205 then 5: Choose arm arbitrarily 6: else 7: Optimistic arm is i \u2190 argmaxi sup\u03b8\u0303\u2208\u0398\u0303t \u00b5i(\u03b8\u0303) 8: Choose arm i\nRemark 1. The choice of arm when \u0398\u0303t = \u2205 does not affect the regret bounds in this paper. In practice, it is possible to simply increase t without taking an action, but this complicates the analysis. In many cases the true parameter \u03b8\u2217 is never identified in the sense that we do not expect that \u0398\u0303t \u2192 {\u03b8\u2217}. The computational complexity of UCB-S depends on the difficulty of computing \u0398\u0303t and computing the optimistic arm within this set. This is efficient in simple cases, like when \u00b5k is piecewise linear, but may be intractable for complex functions."}, {"heading": "4 Theorems", "text": "We present two main theorems bounding the regret of the UCB-S algorithm. The first is for arbitrary \u03b8\u2217, which leads to a logarithmic bound on the regret comparable to that obtained for UCB by [6]. The analysis is slightly different because UCB-S maintains upper and lower confidence bounds and selects its actions optimistically from the model class, rather than by maximising the upper confidence bound as UCB does.\nTheorem 2. If \u03b1 > 2 and \u03b8 \u2208 \u0398, then the algorithm UCB-S suffers an expected regret of at most\nERn \u2264 2\u2206maxK(\u03b1\u2212 1) \u03b1\u2212 2 + \u2211\ni\u2208A\u2032\n8\u03b1\u03c32 logn\n\u2206i + \u2211\ni\n\u2206i\nIf the samples from the optimal arm are sufficient to learn the optimal action, then finite regret is possible. In Section 6 we give something of a converse by showing that if knowing the mean of the optimal arm is insufficient to act optimally, then logarithmic regret is unavoidable.\nTheorem 3. Let \u03b1 = 4 and assume there exists an \u03b5 > 0 such that\n(\u2200\u03b8 \u2208 \u0398) |\u00b5i\u2217(\u03b8 \u2217)\u2212 \u00b5i\u2217(\u03b8)| < \u03b5 =\u21d2 \u2200i 6= i \u2217, \u00b5i\u2217(\u03b8) > \u00b5i(\u03b8). (1)\nThen ERn \u2264 \u2211\ni\u2208A\u2032\n(\n32\u03c32 log\u03c9\u2217\n\u2206i +\u2206i\n)\n+ 3\u2206maxK + \u2206maxK\n3\n\u03c9\u2217 ,\nwith \u03c9\u2217 := max\n{\n\u03c9\n(\n8\u03c32\u03b1K\n\u03b52\n)\n, \u03c9\n(\n8\u03c32\u03b1K\n\u22062min\n)}\n.\nRemark 4. For small \u03b5 and large n the expected regret looks like ERn \u2208 O\n(\nK \u2211\ni=1\nlog ( 1 \u03b5 )\n\u2206i\n)\n(for small n the regret is, of course, even smaller).\nThe explanation of the bound is as follows. If at some time-step t it holds that all confidence intervals contain the truth and the width of the confidence interval about i\u2217 drops below \u03b5, then by the condition in Equation (1) it holds that i\u2217 is the optimistic arm within \u0398\u0303t. In this case UCB-S\nsuffers no regret at this time-step. Since the number of samples of each sub-optimal arm grows at most logarithmically by the proof of Theorem 2, the number of samples of the best arm must grow linearly. Therefore the number of time-steps before best arm has been pulled O(\u03b5\u22122) times is also O(\u03b5\u22122). After this point the algorithm suffers only a constant cumulative penalty for the possibility that the confidence intervals do not contain the truth, which is finite for suitably chosen values of \u03b1. Note that Agrawal et. al. [1] had essentially the same condition to achieve finite regret as (1), but specified to the case where \u0398 is finite.\nAn interesting question is raised by comparing the bound in Theorem 3 to those given by Bubeck et. al. [11] where if the expected return of the best arm is known and \u03b5 is a known bound on the minimum gap, then a regret bound of\nO\n(\n\u2211\ni\u2208A\u2032\n(\nlog ( 2\u2206i \u03b5 )\n\u2206i\n(\n1 + log log 1\n\u03b5\n)\n))\n(2)\nis achieved. If \u03b5 is close to \u2206i, then this bound is an improvement over the bound given by Theorem 3, although our theorem is more general. The improved UCB algorithm [7] enjoys a bound on the expected regret of O( \u2211\ni\u2208A\u2032 1 \u2206i logn\u22062i ). If we follow the same reasoning as above we obtain a bound comparable to (2). Unfortunately though, the extension of the improved UCB algorithm to the structured setting is rather challenging with the main obstruction being the extreme growth of the phases used by improved UCB. Refining the phases leads to super-logarithmic regret, a problem we ultimately failed to resolve. Nevertheless we feel that there is some hope of obtaining a bound like (2) in this setting.\nBefore the proofs of Theorems 2 and 3 we give some example structured bandits and indicate the regions where the conditions for Theorem 3 are (not) met. Areas where Theorem 3 can be applied to obtain finite regret are unshaded while those with logarithmic regret are shaded.\n(a) The conditions for Theorem 3 are met for all \u03b8 6= 0, but for \u03b8 = 0 the regret strictly vanishes for all policies, which means that the regret is bounded by ERn \u2208 O(1{\u03b8\u2217 6= 0} 1|\u03b8\u2217| log 1 |\u03b8\u2217| ).\n(b) Action 2 is uninformative and not globally optimal so Theorem 3 does not apply for \u03b8 < 1/2 where this action is optimal. For \u03b8 > 0 the optimal action is 1, when the conditions are met and finite regret is again achieved.\nERn \u2208 O\n(\n1{\u03b8\u2217 < 0} logn\n|\u03b8\u2217| + 1{\u03b8\u2217 > 0}\nlog 1\u03b8\u2217\n\u03b8\u2217\n)\n.\n(c) The conditions for Theorem 3 are again met for all non-zero \u03b8\u2217, which leads as in (a) to a regret of ERn \u2208 O(1{\u03b8\u2217 6= 0} 1|\u03b8\u2217| log 1 |\u03b8\u2217|).\nExamples (d) and (e) illustrate the potential complexity of the regions in which finite regret is possible. Note especially that in (e) the regret for \u03b8\u2217 = 12 is logarithmic in the horizon, but finite for \u03b8 \u2217 arbitrarily close. Example (f) is a permutation bandit with 3 arms where it can be clearly seen that the conditions of Theorem 3 are satisfied."}, {"heading": "5 Proof of Theorems 2 and 3", "text": "We start by bounding the probability that some mean does not lie inside the confidence set.\nLemma 5. P {Ft = 1} \u2264 2Kt exp(\u2212\u03b1 log(t)) where\nFt = 1\n{\n\u2203i : |\u00b5\u0302i,Ti(t\u22121) \u2212 \u00b5i| \u2265\n\u221a\n2\u03b1\u03c32 log t\nTi(t\u2212 1)\n}\n.\nProof. We use the concentration guarantees:\nP {Ft = 1} (a) = P\n{\n\u2203i : \u2223 \u2223\u00b5i(\u03b8 \u2217)\u2212 \u00b5\u0302i,Ti(t\u22121) \u2223 \u2223 \u2265\n\u221a\n2\u03b1\u03c32 log t\nTi(t\u2212 1)\n}\n(b) \u2264 K \u2211\ni=1\nP\n{\n\u2223 \u2223\u00b5i(\u03b8 \u2217)\u2212 \u00b5\u0302i,Ti(t\u22121) \u2223 \u2223 \u2265\n\u221a\n2\u03b1\u03c32 log t\nTi(t\u2212 1)\n}\n(c) \u2264 K \u2211\ni=1\nt \u2211\ns=1\nP\n{\n|\u00b5i(\u03b8 \u2217)\u2212 \u00b5\u0302i,s| \u2265\n\u221a\n2\u03b1\u03c32 log t\ns\n}\n(d) \u2264 K \u2211\ni=1\nt \u2211\ns=1\n2 exp(\u2212\u03b1 log t) (e) = 2Kt1\u2212\u03b1\nwhere (a) follows from the definition of Ft. (b) by the union bound. (c) also follows from the union bound and is the standard trick to deal with the random variable Ti(t \u2212 1). (d) follows from the concentration inequalities for sub-gaussian random variables. (e) is trivial.\nProof of Theorem 2. Let i be an arm with \u2206i > 0 and suppose that It = i. Then either Ft is true or\nTi(t\u2212 1) <\n\u2308\n8\u03c32\u03b1 logn\n\u22062i\n\u2309\n=: ui(n) (3)\nNote that if Ft does not hold then the true parameter lies within the confidence set, \u03b8\u2217 \u2208 \u0398\u0303t. Suppose on the contrary that Ft and (3) are both false.\nmax \u03b8\u0303\u2208\u0398\u0303t\n\u00b5i\u2217(\u03b8\u0303) (a) \u2265 \u00b5\u2217(\u03b8\u2217) (b) = \u00b5i(\u03b8 \u2217) + \u2206i (c) > \u2206i + \u00b5\u0302i,Ti(t\u22121) \u2212\n\u221a\n2\u03c32\u03b1 log t\nTi(t\u2212 1)\n(d) \u2265 \u00b5\u0302i,Ti(t\u22121) +\n\u221a\n2\u03b1\u03c32 log t\nTi(t\u2212 1)\n(e) \u2265 max \u03b8\u0303\u2208\u0398\u0303t \u00b5i(\u03b8\u0303),\nwhere (a) follows since \u03b8\u2217 \u2208 \u0398\u0303t. (b) is the definition of the gap. (c) since Ft is false. (d) is true because (3) is false. Therefore arm i is not taken. We now bound the expected number of times that arm i is played within the first n time-steps by\nETi(n) (a) = E\nn \u2211\nt=1\n1{It = i} (b) \u2264 ui(n) + E n \u2211\nt=ui+1\n1{It = i \u2227 (3) is false}\n(c) \u2264 ui(n) + E n \u2211\nt=ui+1\n1{Ft = 1 \u2227 It = i}\nwhere (a) follows from the linearity of expectation and definition of Ti(n). (b) by Equation (3) and the definition of ui(n) and expectation. (c) is true by recalling that playing arm i at time-step t implies that either Ft or (3) must be true. Therefore\nERn \u2264 \u2211\ni\u2208A\u2032\n\u2206i\n(\nui(n) + E\nn \u2211\nt=ui+1\n1{Ft = 1 \u2227 It = i}\n)\n\u2264 \u2211\ni\u2208A\u2032\n\u2206iui(n) + \u2206maxE\nn \u2211\nt=1\n1{Ft = 1}\n(4)\nBounding the second summation\nE\nn \u2211\nt=1\n1{Ft = 1} (a) =\nn \u2211\nt=1\nP {Ft = 1} (b) \u2264 n \u2211\nt=1\n2Kt1\u2212\u03b1 (c) \u2264 2K(\u03b1\u2212 1)\n\u03b1\u2212 2\nwhere (a) follows by exchanging the expectation and sum and because the expectation of an indicator function can be written as the probability of the event. (b) by Lemma 5 and (c) is trivial. Substituting into (4) leads to\nERn \u2264 2\u2206maxK(\u03b1\u2212 1) \u03b1\u2212 2 + \u2211\ni\u2208A\u2032\n8\u03b1\u03c32 logn\n\u2206i + \u2211\ni\n\u2206i.\nBefore the proof of Theorem 3 we need a high-probability bound on the number of times arm i is pulled, which is proven along the lines of similar results by [5].\nLemma 6. Let i \u2208 A\u2032 be some sub-optimal arm. If z > ui(n), then P {Ti(n) > z} \u2264 2Kz2\u2212\u03b1\n\u03b1\u2212 2 .\nProof. As in the proof of Theorem 2, if t \u2264 n and Ft is false and Ti(t \u2212 1) > ui(n) \u2265 ui(t), then arm i is not chosen. Therefore\nP {Ti(n) > z} \u2264 n \u2211\nt=z+1\nP {Ft = 1} (a) \u2264 n \u2211\nt=z+1\n2Kt1\u2212\u03b1 (b) \u2264 2K\n\u222b n\nz\nt1\u2212\u03b1dt (c) \u2264 2Kz2\u2212\u03b1\n\u03b1\u2212 2\nwhere (a) follows from Lemma 5 and (b) and (c) are trivial.\nLemma 7. Assume the conditions of Theorem 3 and additionally that Ti\u2217(t\u2212 1) \u2265 \u2308 8\u03b1\u03c32 log t \u03b52 \u2309 and"}, {"heading": "Ft is false. Then It = i\u2217.", "text": "Proof. Since Ft is false, for \u03b8\u0303 \u2208 \u0398\u0303t we have:\n|\u00b5i\u2217(\u03b8\u0303)\u2212 \u00b5i\u2217(\u03b8 \u2217)|\n(a) \u2264 |\u00b5i\u2217(\u03b8\u0303)\u2212 \u00b5\u0302i\u2217,Ti(t\u22121)|+ |\u00b5\u0302i\u2217,Ti(t\u22121) \u2212 \u00b5i\u2217(\u03b8 \u2217)| (b) < 2\n\u221a\n2\u03c32\u03b1 log t\nTi\u2217(t\u2212 1)\n(c) \u2264 \u03b5\nwhere (a) is the triangle inequality. (b) follows by the definition of the confidence interval and because Ft is false. (c) by the assumed lower bound on Ti\u2217(t\u2212 1). Therefore by (1), for all \u03b8\u0303 \u2208 \u0398\u0303t it holds that the best arm is i\u2217. Finally, since Ft is false, \u03b8\u2217 \u2208 \u0398\u0303t, which means that \u0398\u0303t 6= \u2205. Therefore It = i\u2217 as required.\nProof of Theorem 3. Let \u03c9\u2217 be some constant to be chosen later. Then the regret may be written as\nERn \u2264 E \u03c9\u2217 \u2211\nt=1\nK \u2211\ni=1\n\u2206i1{It = i}+\u2206maxE n \u2211\nt=\u03c9\u2217+1\n1{It 6= i \u2217} . (5)\nThe first summation is bounded as in the proof of Theorem 2 by\nE\n\u03c9\u2217 \u2211\nt=1\n\u2211\ni\u2208A\n\u2206i1{It = i} \u2264 \u2211\ni\u2208A\u2032\n(\n\u2206i + 8\u03b1\u03c32 log\u03c9\u2217\n\u2206i\n)\n+\n\u03c9\u2217 \u2211\nt=1\nP {Ft = 1} . (6)\nWe now bound the second sum in (5) and choose \u03c9\u2217. By Lemma 6, if nK > ui(n), then\nP\n{ Ti(n) > n\nK\n} \u2264 2K\n\u03b1\u2212 2\n(\nK\nn\n)\u03b1\u22122\n. (7)\nSuppose t \u2265 \u03c9\u2217 := max { \u03c9 ( 8\u03c32\u03b1K \u03b52 ) , \u03c9 ( 8\u03c32\u03b1K \u22062\nmin\n)}\n. Then tK > ui(t) for all i 6= i \u2217 and tK \u2265\n8\u03c32\u03b1 log t \u03b52 . By the union bound\nP\n{\nTi\u2217(t) < 8\u03c32\u03b1 log t\n\u03b52\n}\n(a) \u2264 P\n{\nTi\u2217(t) < t\nK\n}\n(b) \u2264 P\n{\n\u2203i : Ti(t) > t\nK\n}\n(c) <\n2K2\n\u03b1\u2212 2\n(\nK\nt\n)\u03b1\u22122\n(8)\nwhere (a) is true since tK \u2265 8\u03c32\u03b1 log t \u03b52 . (b) since \u2211K i=1 Ti(t) = t. (c) by the union bound and (7). Now if Ti(t) \u2265 8\u03c32\u03b1 log t \u03b52 and Ft is false, then the chosen arm is i \u2217. Therefore\nE\nn \u2211\nt=\u03c9\u2217+1\n1{It 6= i \u2217} \u2264\nn \u2211\nt=\u03c9\u2217+1\nP {Ft = 1}+ n \u2211\nt=\u03c9\u2217+1\nP\n{\nTi(t\u2212 1) < 8\u03c32\u03b1 log t\n\u03b52\n}\n(a) \u2264 n \u2211\nt=\u03c9\u2217+1\nP {Ft = 1}+ 2K2\n\u03b1\u2212 2\nn \u2211\nt=\u03c9\u2217+1\n(\nK\nt\n)\u03b1\u22122\n(b) \u2264 n \u2211\nt=\u03c9\u2217+1\nP {Ft = 1}+ 2K2\n(\u03b1\u2212 2)(\u03b1\u2212 3)\n(\nK\n\u03c9\u2217\n)\u03b1\u22123\n(9)\nwhere (a) follows from (8) and (b) by straight-forward calculus. Therefore by combining (5), (6) and (9) we obtain\nERn \u2264 \u2211\ni:\u2206i>0\n\u2206i\n\u2308\n8\u03c32\u03b1 log\u03c9\u2217\n\u22062i\n\u2309\n+ 2\u2206maxK\n2\n(\u03b1\u2212 2)(\u03b1\u2212 3)\n(\nK\n\u03c9\u2217\n)\u03b1\u22123\n+\u2206max\nn \u2211\nt=1\nP {Ft = 1}\n\u2264 \u2211\ni:\u2206i>0\n\u2206i\n\u2308\n8\u03c32\u03b1 log\u03c9\u2217\n\u22062i\n\u2309\n+ 2\u2206maxK\n2\n(\u03b1\u2212 2)(\u03b1\u2212 3)\n(\nK\n\u03c9\u2217\n)\u03b1\u22123\n+ 2\u2206maxK(\u03b1\u2212 1)\n\u03b1\u2212 2\nSetting \u03b1 = 4 leads to ERn \u2264 K \u2211\ni=1\n(\n32\u03c32 log\u03c9\u2217\n\u2206i +\u2206i\n)\n+ 3\u2206maxK + \u2206maxK\n3\n\u03c9\u2217 ."}, {"heading": "6 Lower Bounds and Ambiguous Examples", "text": "We prove lower bounds for two illustrative examples of structured bandits. Some previous work is also relevant. The famous paper by Lai and Robbins [17] shows that the bound of Theorem 2 cannot in general be greatly improved. Many of the techniques here are borrowed from Bubeck et. al. [11]. Given a fixed algorithm and varying \u03b8 we denote the regret and expectation by Rn(\u03b8) and E\u03b8 respectively. Returns are assumed to be sampled from a normal distribution with unit variance, so that \u03c32 = 1.\nTheorem 8. Given the structured bandit depicted in Figure 3.(a) or Figure 2.(c), then for all \u03b8 > 0 and all algorithms the regret satisfies max {E\u2212\u03b8Rn(\u2212\u03b8), E\u03b8Rn(\u03b8)} \u2265 18\u03b8 for sufficiently large n.\nProof. The proof uses the same technique as the proof of Theorem 5 in the paper by [11]. Fix an algorithm and let P\u03b8,t be the probability measure on the space of outcomes up to time-step t under the bandit determined by parameter \u03b8.\nE\u2212\u03b8Rn(\u2212\u03b8) + E\u03b8Rn(\u03b8) (a) = 2\u03b8\n(\nE\u2212\u03b8\nn \u2211\nt=1\n1{It = 1}+ E\u03b8\nn \u2211\nt=1\n1{It = 2}\n)\n(b) = 2\u03b8\nn \u2211\nt=1\n(P\u2212\u03b8,t {It = 1}+ P\u03b8,t {It = 2}) (c) \u2265 \u03b8 n \u2211\nt=1\nexp (\u2212KL(P\u2212\u03b8,t,P\u03b8,t))\n(d) = \u03b8\nn \u2211\nt=1\nexp ( \u22124t\u03b82 )\n(e) \u2265 1\n8\u03b8\nwhere (a) follows since 2|\u03b8| is the gap between the expected returns of the two arms given parameter \u03b8 and by the definition of the regret. (b) by replacing the expectations with probabilities. (c) follows from Lemma 4 by [11] where KL(P\u2212\u03b8,t,P\u03b8,t) is the relative entropy between measures P\u2212\u03b8,t and P\u03b8,t. (d) is true by computing the relative entropy between two normals with unit variance and means separated by 2\u03b8, which is 4\u03b82. (e) holds for sufficiently large n.\nTheorem 9. Let \u0398, {\u00b51, \u00b52} be a structured bandit where returns are sampled from a normal distribution with unit variance. Assume there exists a pair \u03b81, \u03b82 \u2208 \u0398 and constant \u2206 > 0 such that \u00b51(\u03b81) = \u00b51(\u03b82) and \u00b51(\u03b81) \u2265 \u00b52(\u03b81) + \u2206 and \u00b52(\u03b82) \u2265 \u00b51(\u03b82) + \u2206. Then the following hold:\n(1) E\u03b81Rn(\u03b81) \u2265 1+log 2n\u22062 8\u2206 \u2212 1 2E\u03b82Rn(\u03b82)\n(2) E\u03b82Rn(\u03b82) \u2265 n\u2206 2 exp (\u22124E\u03b81Rn(\u03b81)\u2206)\u2212 E\u03b81Rn(\u03b81)\nA natural example where the conditions are satisfied is depicted in Figure 3.(b) and by choosing \u03b81 = \u22121, \u03b82 = 1. We know from Theorem 3 that UCB-S enjoys finite regret of E\u03b82Rn(\u03b82) \u2208 O( 1 \u2206 log 1 \u2206 ) and logarithmic regret E\u03b81Rn(\u03b81) \u2208 O( 1 \u2206 logn). Part 1 of Theorem 9 shows that if we demand finite regret E\u03b82Rn(\u03b82) \u2208 O(1), then the regret E\u03b81Rn(\u03b81) is necessarily logarithmic. On the other hand, part 2 shows that if we demand E\u03b81Rn(\u03b81) \u2208 o(log(n)), then the regret E\u03b82Rn(\u03b82) \u2208 \u2126(n). Therefore the trade-off made by UCB-S essentially cannot be improved.\nProof of Theorem 9. Again, we make use of the techniques of [11].\nE\u03b81Rn(\u03b81) + E\u03b82Rn(\u03b82) (a) \u2265 \u2206(E\u03b81T2(n) + E\u03b82T1(n)) (b) \u2265 \u2206 n \u2211\nt=1\n(P\u03b81 {It = 2}+ P\u03b82 {It = 1})\n(c) \u2265 \u2206\n2\nn \u2211\nt=1\nexp (\u2212KL(P\u03b81,t,P\u03b82,t)) (d) \u2265 n\u2206\n2 exp (\u2212KL(P\u03b81,n,P\u03b82,n))\n(e) \u2265 n\u2206\n2 exp\n( \u22124\u22062E\u03b81T2(n) )\n(f) \u2265 n\u2206\n2 exp (\u22124\u2206E\u03b81Rn(\u03b81)) (10)\nwhere (a) follows from the definition of the regret and the bandits used. (b) by the definition of Tk(n). (c) by Lemma 4 of [11]. (d) since the relative entropy KL(P\u03b81,t,P\u03b82,t) is increasing with t. (e) By checking that KL(P\u03b81,n,P\u03b82,n) = 4\u2206 2 E\u03b81T2(n). (f) by substituting the definition of the regret. Now part 2 is completed by rearranging (10). For part 1 we also rearrange (10) to obtain\nE\u03b81Rn(\u03b81) \u2265 n\u2206\n2 exp (\u22124\u2206E\u03b81Rn(\u03b81))\u2212 E\u03b82Rn(\u03b82)\nLetting x = E\u03b81Rn(\u03b81) and using the constraint above we obtain:\nx \u2265 x\n2 +\n1\n2\n(\nn\u2206\n2 exp (\u22124\u2206x)\u2212 E\u03b82Rn(\u03b82)\n)\n.\nBut by simple calculus the function on the right hand side is minimised for x = 14\u2206 log(2n\u2206 2), which leads to\nE\u03b81Rn(\u03b81) \u2265 log(2n\u22062)\n8\u2206 +\n1\n8\u2206 \u2212\n1 2 E\u03b82Rn(\u03b82).\nDiscussion of Figure 3.(c/d). In both examples there is an ambiguous region for which the lower bound (Theorem 9) does not show that logarithmic regret is unavoidable, but where Theorem 3 cannot be applied to show that UCB-S achieves finite regret. We managed to show that finite regret is possible in both cases by using a different algorithm. For (c) we could construct a carefully tuned algorithm for which the regret was at most O(1) if \u03b8 \u2264 0 and O(1\u03b8 log log 1 \u03b8 ) otherwise. This result contradicts a claim by Bubeck et. al. [11, Thm. 8]. Additional discussion of the ambiguous case in general, as well as this specific example, may be found in the supplementary material. One observation is that unbridled optimism is the cause of the failure of UCB-S in these cases. This is illustrated by Figure 3.(d) with \u03b8 \u2264 0. No matter how narrow the confidence interval about \u00b51, if the second action has not been taken sufficiently often, then there will still be some belief that \u03b8 > 0 is possible where the second action is optimistic, which leads to logarithmic regret. Adapting the algorithm to be slightly risk averse solves this problem."}, {"heading": "7 Experiments", "text": "We tested Algorithm 1 on a selection of structured bandits depicted in Figure 2 and compared to UCB [6, 8]. Rewards were sampled from normal distributions with unit variances. For UCB we chose \u03b1 = 2, while we used the theoretically justified \u03b1 = 4 for Algorithm 1. All code is available in the supplementary material. Each data-point is the average of 500 independent samples with the blue crosses and red squares indicating the regret of UCB-S and UCB respectively.\n\u22120.2 \u22120.1 0 0.1 0.2 0\n100\n200\n\u03b8\nE\u0302 \u03b8 R\nn (\u03b8\n)\nK = 2, \u00b51(\u03b8) = \u03b8, \u00b52(\u03b8) = \u2212\u03b8, n = 50 000 (see Figure 2.(a))\n0 5e4 1e5 0\n100\n200\nn\nE\u0302 \u03b8 R\nn (\u03b8\n)\nK = 2, \u00b51(\u03b8) = \u03b8, \u00b52(\u03b8) = \u2212\u03b8, \u03b8 = 0.04 (see Figure 2.(a))\n\u22121 0 1 0\n200\n400\n\u03b8\nE\u0302 \u03b8 R\nn (\u03b8\n)\nK = 2, \u00b51(\u03b8) = 0, \u00b52(\u03b8) = \u03b8, n = 50 000 (see Figure 2.(b))\nThe results show that Algorithm 1 typically out-performs regular UCB. The exception is the top right experiment where UCB performs slightly better for \u03b8 < 0. This is not surprising, since in this case the structured version of UCB cannot exploit the additional structure and suffers due to worse constant factors. On the other hand, if \u03b8 > 0, then UCB endures logarithmic regret and performs significantly worse than its structured counterpart. The superiority of Algorithm 1 would be accentuated in the top left and bottom right experiments by increasing the horizon.\n\u22121 0 1 0\n50\n100\n150\n\u03b8\nE\u0302 \u03b8 R\nn (\u03b8\n)\nK = 2, \u00b51(\u03b8) = \u03b81{\u03b8 > 0}, \u00b52(\u03b8) = \u2212\u03b81{\u03b8 < 0}, n = 50 000 (see Figure 2.(c))"}, {"heading": "8 Conclusion", "text": "The limitation of the new approach is that the proof techniques and algorithm are most suited to the case where the number of actions is relatively small. Generalising the techniques to large action spaces is therefore an important open problem. There is still a small gap between the upper and lower bounds, and the lower bounds have only been proven for special examples. Proving a general problem-dependent lower bound is an interesting question, but probably extremely challenging given the flexibility of the setting. We are also curious to know if there exist problems for which the optimal regret is somewhere between finite and logarithmic. Another question is that of how to define Thompson sampling for structured bandits. Thompson sampling has recently attracted a great deal of attention [13, 2, 14, 3, 9], but so far we are unable even to define an algorithm resembling Thompson sampling for the general structured bandit problem. Not only because we have not endowed \u0398 with a topology, but also because choosing a reasonable prior seems rather problem-dependent. An advantage of our approach is that we do not rely on knowing the distribution of the rewards while with one notable exception [9] this is required for Thompson sampling.\nAcknowledgements. Tor Lattimore was supported by the Google Australia Fellowship for Machine Learning and the Alberta Innovates Technology Futures, NSERC. The majority of this work was completed while Re\u0301mi Munos was visiting Microsoft Research, New England. This research was partially supported by the European Community\u2019s Seventh Framework Programme under grant agreements no. 270327 (project CompLACS)."}, {"heading": "A Ambiguous Case", "text": "We assume for convenience that K = 2 and \u00b51(\u03b8) 6= \u00b52(\u03b8) for all \u03b8 \u2208 \u0398. The second assumption is non-restrictive, since an algorithm cannot perform badly on the \u03b8 for which \u00b51(\u03b8) = \u00b52(\u03b8), so we can simply remove these points from the parameter space. Now \u0398 can be partitioned into three sets according to whether or not finite regret is expected by Theorem 3, or impossible by Theorem 9.\n\u0398easy := { \u03b8 \u2208 \u0398 : \u2203\u03b5 > 0 such that \u2223 \u2223\u00b5i\u2217(\u03b8)(\u03b8 \u2032)\u2212 \u00b5i\u2217(\u03b8)(\u03b8 \u2032) \u2223 \u2223 < \u03b5 =\u21d2 i\u2217(\u03b8\u2032) = i\u2217(\u03b8) } \u0398hard := { \u03b8 \u2208 \u0398 : \u2203\u03b8\u2032 \u2208 \u0398 such that \u00b5i\u2217(\u03b8)(\u03b8) = \u00b5i\u2217(\u03b8)(\u03b8 \u2032) and i\u2217(\u03b8\u2032) 6= i\u2217(\u03b8) }\n\u0398amb := \u0398\u2212\u0398easy \u2212\u0398hard\nThe topic of this section is to study whether or not finite regret is possible on \u0398amb, and what sacrifices need to be made in order to achieve this. Some examples are given in Figure 4. Note that (a) was considered by Bubeck et. al. [11, Thm. 8] and will receive special attention here.\nThe main theorem is this section shows that finite regret is indeed possible for many \u03b8 \u2208 \u0398amb without incurring significant additional regret for \u03b8 \u2208 \u0398easy and retaining logarithmic regret for \u03b8 \u2208 \u0398hard. The following algorithm is similar to Algorithm 1, but favours actions which may be optimal for some plausible ambiguous \u03b8. Theorems will be given subsequently, but proofs are omitted. Algorithm 2\n1: Input: functions \u00b51, \u00b7 \u00b7 \u00b7 , \u00b5k : \u0398 \u2192 [0, 1], {\u03b2t} \u221e t=1 2: \u03ba1 = 0 3: for t \u2208 1, . . . ,\u221e do\n4: Define confidence set: \u0398\u0303t \u2190\n{\n\u03b8\u0303 : \u2200i, \u2223 \u2223\n\u2223 \u00b5i(\u03b8\u0303)\u2212 \u00b5\u0302i,Ti(t\u22121)\n\u2223 \u2223 \u2223 <\n\u221a\n\u03b1\u03c32 log t\nTi(t\u2212 1)\n}\n5: if \u03bat = 0 \u2227 \u2203\u0398\u0303t \u2229\u0398amb 6= \u2205 then 6: Choose \u03b8 \u2208 \u0398\u0303t \u2229\u0398amb arbitrarily and set \u03bat = i\u2217(\u03b8)\n7: Choose It = argmax k sup \u03b8\u2208\u0398\u0303t \u00b5k(\u03b8) + 1{k = \u03bat}\n\u221a\n\u03b2t log t\nTk(t\u2212 1)\n8: \u03bat+1 \u2190 1{It = \u03bat}\nTheorem 10. Suppose K = 2, \u03b8 \u2208 \u0398 and i\u2217 = 1 and \u03b2t = log log t and \u2206 := \u00b51(\u03b8) \u2212 \u00b52(\u03b8). Then Algorithm 2 satisfies:\n1. lim supn\u2192\u221e E\u03b8Rn(\u03b8)/ logn < \u221e.\n2. If \u03b8 \u2208 \u0398easy, then E\u03b8Rn(\u03b8) \u2208 O( 1\u2206 (log 1 \u2206)(log log 1 \u2206 )).\n3. If \u03b8 is such that\nlim \u03b4\u21920 sup \u03b8\u2032:|\u00b51(\u03b8)\u2212\u00b51(\u03b8\u2032)|<\u03b4\n\u00b52(\u03b8 \u2032)\u2212 \u00b51(\u03b8\u2032) |\u00b51(\u03b8)\u2212 \u00b51(\u03b8\u2032)| < \u221e, (11)\nthen limn\u2192\u221e E\u03b8Rn(\u03b8) < \u221e.\nRemark 11. The condition (11) not satisfied for \u03b8 \u2208 \u0398hard, since in this case there exists some \u03b8\u2032 with \u00b51(\u03b8) = \u00b51(\u03b8\u2032) but where \u00b52(\u03b8\u2032) \u2212 \u00b51(\u03b8\u2032) > 0. The condition may not be satisfied even for \u03b8 \u2208 \u0398amb. See, for example, Figure 4.(c). The condition is satisfied for all other ambiguous \u03b8 for the problems shown in Figure 4.(a,b,d) where the risk \u00b52(\u03b8\u2032) \u2212 \u00b51(\u03b8\u2032) decreases linearly as \u00b51(\u03b8) \u2212 \u00b51(\u03b8\u2032) converges to zero.\nThe following theorem shows that you cannot get finite regret for the ambiguous case where (11) is not satisfied without making sacrifices in the easy case. Theorem 12. Suppose \u03b8 \u2208 \u0398amb with i\u2217(\u03b8) = 1 andE\u03b8Rn(\u03b8) \u2208 O(1). Then there exists a constant c > 0 such that for each \u03b8\u2032 with i\u2217(\u03b8\u2032) = 2 we have\nE\u03b8\u2032Rn(\u03b8 \u2032) \u2265 c\n\u00b52(\u03b8 \u2032)\u2212 \u00b51(\u03b8\u2032)\n(\u00b51(\u03b8) \u2212 \u00b51(\u03b8\u2032))2 .\nTherefore if the condition (3) in the statement of Theorem 10 is not satisfied for some ambiguous \u03b8, then we can construct a sequence {\u03b8\u2032}\u221ei=1 such that limi\u2192\u221e \u00b51(\u03b8 \u2032 i) = \u00b51(\u03b8) and where\nlim i\u2192\u221e\n(\u00b52(\u03b8 \u2032 i)\u2212 \u00b51(\u03b8 \u2032))E\u03b8\u2032 i Rn(\u03b8 \u2032 i) = \u221e,\nwhich means that the regret must grow faster than the inverse of the gap. The situation becomes worse the faster the quantity below diverges to infinity.\nsup \u03b8\u2032:|\u00b51(\u03b8)\u2212\u00b51(\u03b8\u2032)|<\u03b4\n\u00b52(\u03b8 \u2032)\u2212 \u00b51(\u03b8)\n|\u00b51(\u03b8)\u2212 \u00b51(\u03b8\u2032)| .\nIn summary, finite regret is often possible in the ambiguous case, but may lead to worse regret guarantees in the easy case. Ultimately we are not sure how to optimise these trade-offs and there are still many interesting unanswered questions.\nAnalysis of Figure 4.(a)\nWe now consider a case of special interest that was previously studied by Bubeck et. al. [11] and is depicted in Figure 4.(a). The structured bandit falls into the ambiguous case when \u03b8 \u2264 0, since no interval about \u00b51(\u03b8) = 0 is sufficient to rule out the possibility that the second action is in fact optimal. Nevertheless, using a carefully crafted algorithm we show that the optimal regret is smaller than one might expect. The new algorithm operates in phases, choosing each action a certain number of times. If all evidence points to the first action being best, then this is taken until its optimality is proven to be implausible, while otherwise the second action is taken. The algorithm is heavily biased towards choosing the first action where estimation is more challenging, and where the cost of an error tends to be smaller. Algorithm 3\n1: \u03b1 \u2190 5 2: for \u2113 \u2208 2, . . . ,\u221e do // Iterate over phases 3: n1,\u2113 = 2 \u2113 and n2,\u2113 = \u21132 4: Choose each arm k \u2208 {1, 2} exactly nk,\u2113 times and let \u00b5\u0302k,\u2113,nk,\u2113 be the average return 5: s \u2190 0 6: if \u00b5\u03021,\u2113,n1,\u2113 \u2265 \u2212 \u221a \u03b1 n1,\u2113 log logn1,\u2113 and \u00b5\u03022,n2,\u2113 < \u22121/2 then 7: while \u00b5\u03021,\u2113,n1,\u2113+s \u2265 \u2212 \u221a \u03b1 log log(n1,\u2113+s) n1,\u2113+s\ndo 8: Choose action 1 and s \u2190 s+ 1 and \u00b5\u03021,\u2113,n1,\u2113+s is average return of arm 1 this phase\n9: else 10: while \u00b5\u03022,\u2113,n2,\u2113+s \u2265 \u2212 1 2 do 11: Choose action 2 and s \u2190 s+ 1 and \u00b5\u03022,\u2113,n2,\u2113+s is average return of arm 2 this phase\nTheorem 13. Let \u0398 = [\u22121, 1] and \u00b51(0) = \u2212\u03b81{\u03b8 > 0} and \u00b52(\u03b8) = \u22121{\u03b8 \u2264 0}. Assume returns are normally distributed with unit variance. Then Algorithm 3 suffers regret bounded by\nE\u03b8Rn(\u03b8) \u2208\n{ O ( 1 \u03b8 log log 1 \u03b8 )\nif \u03b8 > 0 O(1) otherwise.\nRemark 14. Theorem 13 contradicts a result by Bubeck et. al. [11, Thm. 8], which states that for any algorithm\nmax\n{\nE0Rn(0), sup \u03b8>0 \u03b8 \u00b7 E\u03b8Rn(\u03b8)\n}\n\u2208 \u2126 (logn) .\nBut by Theorem 13 there exists an algorithm for which\nmax\n{\nE0Rn(0), sup \u03b8>0 \u03b8 \u00b7 E\u03b8Rn(\u03b8)\n}\n\u2208 O\n(\nsup \u03b8>0 min\n{\n\u03b82n, log log 1\n\u03b8\n})\n= O(log log n).\nWe are currently unsure whether or not the dependence on log log 1\u03b8 can be dropped from the bound given in Theorem 13. Note that Theorem 3 cannot be applied when \u03b8 = 0, so Algorithm 1 suffers logarithmic regret in this case. Algorithm 3 is carefully tuned and exploits the asymmetry in the problem. It is possible that the result of Bubeck et. al. can be saved in spirit by using the symmetric structured bandit depicted in Figure 4.(b). This would still only give a worst-case bound and does not imply that finite problem-dependent regret is impossible.\nProof of Theorem 13. It is enough to consider only \u03b8 \u2208 [0, 1], since the returns on the arms is constant for \u03b8 \u2208 [\u22121, 0]. We let L be the number phases (times that the outer loop is executed) and T\u2113 be the number of times the sub-optimal action is taken in the \u2113th phase. Recall that \u00b5\u0302k,\u2113,t denotes the empirical estimate of \u00b5 based on t samples taken in the \u2113th phase.\nStep 1: Decomposing the regret\nThe regret is decomposed:\n(\u03b8 = 0) : E0Rn(0) = E0\nL \u2211\n\u2113=0\nT\u2113 =\n\u221e \u2211\n\u2113=0\nP0 {L \u2265 \u2113}E0[T\u2113|L \u2265 \u2113]\n(\u03b8 > 0) : E\u03b8Rn(\u03b8) = \u03b8E\u03b8\nL \u2211\n\u2113=0\nT\u2113 = \u03b8\n\u221e \u2211\n\u2113=0\nP\u03b8 {L \u2265 \u2113}E\u03b8[T\u2113|L \u2265 \u2113]\nStep 2: Bounding E\u03b8[T\u2113|L \u2265 \u2113]\nWe need to consider the cases when \u03b8 = 0 and \u03b8 > 0 separately. If s \u2265 1, then\nP0 {T\u2113 \u2265 n2,\u2113 + s|L \u2265 \u2113} (a) \u2264 P0\n{\n\u00b5\u03022,\u2113,n2,\u2113+s\u22121 \u2265 \u2212 1\n2\n}\n(b) = P0\n{\n\u00b5\u03022,\u2113,n2,\u2113+s\u22121 \u2212 \u00b52(0) \u2265 1\n2\n}\n(c) \u2264 exp\n(\n\u2212 1\n2 (n2,\u2113 + s\u2212 1)\n)\n(d) \u2264 exp ( \u2212 s\n2\n)\n,\nwhere (a) follows since if the second action is chosen more than n2,\u2113 times in the \u2113th phase, then that phase ends when \u00b5\u03022,\u2113,t < \u2212 12 , (b) by noting that \u00b52(0) = \u22121, (c) follows from the standard concentration inequality and the fact that unit variance is assumed, (d) since n2,\u2113 \u2265 1. Therefore by Lemma 17 we have that E0[T\u2113|L \u2265 \u2113] \u2264 n2,\u2113 + 2e1/2. Now assume \u03b8 > 0 and define\n\u03c92(x) = min {z : y \u2265 x log log y, \u2200y \u2265 z} ,\nwhich satisfies \u03c92(x) \u2208 O(x log log x). If n1,\u2113 + s\u2212 1 \u2265 \u03c92 ( 4\u03b1 \u03b82 ) , then\nP\u03b8 {T\u2113 \u2265 n1,\u2113 + s|L \u2265 \u2113} (a) \u2264 P\u03b8\n{\n\u00b5\u03021,\u2113,n1,\u2113+s\u22121 \u2265 \u2212\n\u221a\n\u03b1\nn\u2113,1 + s\u2212 1 log log(n1,\u2113 + s\u2212 1)\n}\n(b) = P\u03b8\n{\n\u00b5\u03021,\u2113,n1,\u2113+s\u22121 \u2212 \u00b51(\u03b8) \u2265 \u03b8 \u2212\n\u221a\n\u03b1\nn\u2113,1 + s\u2212 1 log log(n1,\u2113 + s\u2212 1)\n}\n(c) \u2264 P\u03b8 { \u00b5\u03021,\u2113,n1,\u2113+s\u22121 \u2212 \u00b51(\u03b8) \u2265 \u03b8/2 } (d) \u2264 exp\n(\n\u2212 \u03b82\n8 (n1,\u2113 + s\u2212 1)\n)\n(e) \u2264 exp\n(\n\u2212 \u03b82\n8 s\n)\n,\nwhere (a) follows since if the first arm (which is now sub-optimal) is chosen more than n1,\u2113 times, then the phase ends if \u00b5\u03021,\u2113,t drops below the confidence interval. (b) since \u00b51(\u03b8) = \u2212\u03b8. (c) since n1,\u2113 + s\u2212 1 \u2265 \u03c92 ( 4\u03b1 \u03b82 )\n. (d) by the usual concentration inequality and (e) since n1,\u2113 \u2265 1. Another application of Lemma 17 yields\nE\u03b8[T\u2113|L \u2265 \u2113] \u2264 max\n{\nn1,\u2113, \u03c92\n(\n4\u03b1\n\u03b82\n)}\n+ 8e\u03b8\n2/8\n\u03b82 ,\nwhere the max appears because we demanded that n1,\u2113 + s\u2212 1 \u2265 \u03c92 ( 4\u03b1 \u03b82 )\nand since at the start of each phase the first action is taken at least n1,\u2113 times before the phase can end.\nBounding the number of phases\nAgain we consider the cases when \u03b8 = 0 and \u03b8 \u2265 0 separately.\nP0 {L > \u2113} (a) \u2264 P0\n{\n\u00b5\u03022,\u2113,n2,\u2113 \u2265 \u2212 1\n2 \u2228 \u2203s : \u00b5\u03021,\u2113,n1,\u2113+s \u2264 \u2212\n\u221a\n\u03b1\nn1,\u2113 + s log log(n1,\u2113 + s)\n}\n(b) \u2264 P0\n{\n\u00b5\u03022,\u2113,n2,\u2113 \u2265 \u2212 1\n2\n}\n+ P0\n{\n\u2203s : \u00b5\u03021,\u2113,n1,\u2113+s \u2264 \u2212\n\u221a\n\u03b1\nn1,\u2113 + s log log(n1,\u2113 + s)\n}\n(c) \u2264 exp ( \u2212 n2,\u2113 8 ) + P0\n{\n\u2203s : \u00b5\u03021,\u2113,n1,\u2113+s \u2264 \u2212\n\u221a\n\u03b1\nn1,\u2113 + s log log(n1,\u2113 + s)\n}\n, (12)\nwhere (a) is true since the \u2113th phase will not end if \u00b5\u03022,\u2113,n2,\u2113 < \u22121/2 and if \u00b5\u03021,\u2113,t never drops below the confidence interval. (b) follows from the union bound and (c) by the concentration inequality. The second term is bounded using the maximal inequality and the peeling technique.\nP0\n{\n\u2203s : \u00b5\u03021,\u2113,n1,\u2113+s \u2264 \u2212\n\u221a\n\u03b1\nn1,\u2113 + s log log(n1,\u2113 + s)\n}\n(a) \u2264 \u221e \u2211\nk=0\nP0\n{\n\u2203t : 2kn1,\u2113 \u2264 t \u2264 2 k+1n1,\u2113 \u2227 \u00b5\u03021,\u2113,t \u2264 \u2212\n\u221a\n\u03b1 t log log t\n}\n(b) \u2264 \u221e \u2211\nk=0\nP0\n{\n\u2203t \u2264 2k+1n1,\u2113 : \u00b5\u03021,\u2113,t \u2264 \u2212\n\u221a\n\u03b1\nn1,\u21132k log log 2kn1,\u2113\n}\n(c) \u2264 \u221e \u2211\nk=0\nexp ( \u2212\u03b1 log log ( 2kn1,\u2113 )) (d) = \u221e \u2211\nk=0\n(\n1\nlog 2k + logn1,\u2113\n)\u03b1 (e)\n\u2264 2\nlog 2\n(\n1\n\u2113 log 2\n)\u03b1\u22121\nwhere (a) follows by the union bound, (b) by bounding t in the interval 2kn1,\u2113 \u2264 t \u2264 2k+1n1,\u2113. (c) follows from the maximal inequality. (d) is trivial while (e) follows by approximating the sum by an integral. By combining with (12) we obtain\nP0 {L > \u2113} \u2264 exp ( \u2212 n2,\u2113 8 ) + 2 log 2\n(\n1\n\u2113 log 2\n)\u03b1\u22121\n= exp\n(\n\u2212 \u2113\n8\n)\n+ 2\nlog 2\n(\n1\n\u2113 log 2\n)\u03b1\u22121\n.\nMore straight-forwardly, if \u03b8 > 0, then\nP\u03b8 {L > \u2113} \u2264 P\u03b8\n{\n\u2203s : \u00b5\u03022,n2,\u2113+s < \u2212 1\n2\n}\n\u2264 5 exp ( \u2212 n2,\u2113 16 ) ,\nwhere in the last inequality we used Lemma 16 and naive bounding.\nPutting it together\nWe now combine the results of the previous components to obtain the required bound on the regret. Recall that \u03b1 = 5.\n(\u03b8 = 0) : E0Rn(0) = E\u03b8\n\u221e \u2211\n\u2113=2\nT\u2113 =\n\u221e \u2211\n\u2113=2\nP0 {L \u2265 \u2113}E0[T\u2113|L \u2265 \u2113]\n\u2264 \u221e \u2211\n\u2113=2\n(\nexp ( \u2212 n2,\u2113 8 ) + 2 log 2\n(\n1\n\u2113 log 2\n)\u03b1\u22121 )\n( n2,\u2113 + 2e 1/2 )\n= \u221e \u2211\n\u2113=2\n(\nexp\n(\n\u2212 \u2113\n8\n)\n+ 2\nlog 2\n(\n1\n\u2113 log 2\n)\u03b1\u22121 )\n( \u21132 + 2e1/2 ) \u2208 O(1)\n(\u03b8 > 0) : E\u03b8Rn(\u03b8) = \u03b8E\u03b8\n\u221e \u2211\n\u2113=2\nT\u2113 = \u03b8\n\u221e \u2211\n\u2113=1\nP {L \u2265 \u2113}E[T\u2113|L \u2265 \u2113]\n\u2264 5\u03b8 \u221e \u2211\n\u2113=2\nexp ( \u2212 n2,\u2113 16 )\n(\nmax\n{\nn1,\u2113, \u03c92\n(\n4\u03b1\n\u03b82\n)}\n+ 8e\u03b8\n2/8\n\u03b82\n)\n\u2208 O ( \u03b8 \u00b7 \u03c92 ( \u03b1\n\u03b82\n))\n= O\n(\n1 \u03b8 log log 1 \u03b8\n)\n."}, {"heading": "B Technical Lemmas", "text": "Lemma 15. Define functions \u03c9 and \u03c92 by\n\u03c9(x) := min {z > 1 : y \u2265 x log y, \u2200y \u2265 z}\n\u03c92(x) := min {z > e : y \u2265 x log log y, \u2200y \u2265 z} .\nThen \u03c9(x) \u2208 O (x log x) and \u03c92(x) \u2208 O (x log log x).\nLemma 16. Let {Xi} \u221e i=1 be sampled from some sub-gaussian distributed arm with mean \u00b5 and unit sub-gaussian constant. Define \u00b5\u0302t = 1t \u2211t s=1 Xs. Then for s \u2265 6/\u2206 2 we have\nP {\u2203t \u2265 s : \u00b5\u0302t \u2212 \u00b5 \u2265 \u2206} \u2264 p+ 1\nlog 2 log\n1\n1\u2212 p\nwhere p = exp ( \u2212 s\u2206 2\n4\n)\n.\nProof. We assume without loss of generality that \u00b5 = 0 and use a peeling argument combined with Azuma\u2019s maximal inequality\nP {\u2203t > s : \u00b5\u0302t \u2265 \u2206} (a) = P { \u2203k \u2208 N, 2ks \u2264 t < 2k+1s : \u00b5\u0302t \u2265 \u2206 }\n(b) = P { \u2203k \u2208 N, 2ks \u2264 t < 2k+1s : t\u00b5\u0302t \u2265 t\u2206 }\n(c) \u2264 P { \u2203k \u2208 N, 2ks \u2264 t < 2k+1s : t\u00b5\u0302t \u2265 2 ks\u2206 }\n(d) \u2264 \u221e \u2211\nk=0\nP { \u22032ks \u2264 t < 2k+1s : t\u00b5\u0302t \u2265 2 ks\u2206/2 }\n(e) \u2264 \u221e \u2211\nk=0\nP { \u2203t < 2k+1s : t\u00b5\u0302t \u2265 2 ks\u2206/2 }\n(f) \u2264 \u221e \u2211\nk=0\nexp\n(\n\u2212 1\n2\n( 2ks\u2206 )2\n2k+1s\n)\n(g) =\n\u221e \u2211\nk=0\nexp\n(\n\u2212 2ks\u22062\n4\n)\n(h) =\n\u221e \u2211\nk=0\nexp\n(\n\u2212 s\u22062\n4\n)2k (i)\n\u2264 p+ 1\nlog 2 log\n1\n1\u2212 p\nwhere (a) follows by splitting the sum over an exponential grid. (b) by comparing cumulative differences rather than the means. (c) since t > 2ks. (d) by the union bound over all k. (e) follows by increasing the range. (f) by Azuma\u2019s maximal inequality. (g) and (h) are true by straight-forward arithmetic while (i) follows from Lemma 18.\nLemma 17. Suppose z is a positive random variable and for some \u03b1 > 0 it holds for all natural numbers k that P {z \u2265 k} \u2264 exp(\u2212k\u03b1). Then Ez \u2264 e \u03b1\n\u03b1\nProof. Let \u03b4 \u2208 (0, 1). Then\nP\n{\nz \u2265 log 1\n\u03b4\n}\n\u2264 P\n{\nz \u2265\n\u230a\nlog 1\n\u03b4\n\u230b}\n\u2264 exp\n(\n\u2212\u03b1\n\u230a\nlog 1\n\u03b4\n\u230b)\n\u2264 e\u03b1 \u00b7 exp\n(\n\u2212\u03b1 log\n(\n1\n\u03b4\n))\n= e\u03b1 \u00b7 \u03b4\u03b1.\nTo complete the proof we use a standard identity to bound the expectation\nEz \u2264\n\u222b 1\n0\n1 \u03b4 P\n{\nz \u2265 log 1\n\u03b4\n} d\u03b4 \u2264 e\u03b1 \u222b 1\n0\n\u03b4\u03b1\u22121d\u03b4 = e\u03b1\n\u03b1 .\nLemma 18. Let p \u2208 (0, 7/10). Then \u221e \u2211\nk=0\np2 k \u2264 p+ 1\nlog 2 log\n1\n1\u2212 p .\nProof. Splitting the sum and comparing to an integral yields: \u221e \u2211\nk=0\np2 k (a) = p+\n\u221e \u2211\nk=1\np2 k (b) \u2264 p+\n\u222b \u221e\n0\np2 k dk (c) = p+\n1\nlog 2\n\u222b \u221e\n1\npu/udu\n(d) \u2264 p+ 1\nlog 2\n\u221e \u2211\nu=1\npu/u (e) = p+\n1\nlog 2 log\n1\n1\u2212 p\nwhere (a) follows by splitting the sum. (b) by noting that p2 k\nis monotone decreasing and comparing to an integral. (c) by substituting u = 2k. (d) by reverting back to a sum. (e) follows from a standard formula."}, {"heading": "C Table of Notation", "text": "K number of arms \u0398 parameter space \u03b8\u2217 unknown parameter \u03b8\u2217 \u2208 \u0398 It arm played at time-step t Ti(n) number times arm i has been played after time-step n Xi,s sth reward obtained when playing arm i \u2206i gap between the means of the best arm and the ith arm \u2206min minimum gap, \u2206min := mini:\u2206i>0 \u2206i \u2206max maximum gap, \u2206max := maxi \u2206i A set of arms A := {1, 2, \u00b7 \u00b7 \u00b7 ,K} A\u2032 set of suboptimal arms A := {i : \u2206i > 0} Rn regret at time-step n given unknown true parameter \u03b8\u2217 Rn(\u03b8) regret at time-step n given parameter \u03b8 \u00b5i(\u03b8) mean of arm i given \u03b8 \u00b5\u0302i,s empiric estimate of the mean of arm i after s plays \u00b5\u2217(\u03b8) maximum return at \u03b8. \u00b5\u2217(\u03b8) := maxi \u00b5i(\u03b8) i\u2217 optimal arm given \u03b8\u2217 i\u2217(\u03b8) optimal arm given \u03b8 \u03c9(x) minimum value y such that z \u2265 x log z for all z \u2265 y \u03c92(x) minimum value y such that z \u2265 x log log z for all z \u2265 y Ft event that the true value of some mean is outside the confidence interval about the empiric estimate at time-step t \u03b1 parameter controlling how exploring the algorithm UCB-S is \u03c32 known parameter controlling the tails of the distributions governing the return\nof the arms\nui(n) critical number of samples for arm i. ui(n) := \u2308 8\u03c32\u03b1 logn \u22062\ni\n\u2309"}], "references": [{"title": "Asymptotically efficient adaptive allocation schemes for controlled markov chains: Finite parameter space", "author": ["Rajeev Agrawal", "Demosthenis Teneketzis", "Venkatachalam Anantharam"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "Analysis of Thompson sampling for the multi-armed bandit problem", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "Proceedings of the 25th Annual Conference on Learning Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Further optimal regret bounds for thompson sampling", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "Proceedings of the 16th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Bandits, query learning, and the haystack dimension", "author": ["Kareem Amin", "Michael Kearns", "Umar Syed"], "venue": "Journal of Machine Learning Research-Proceedings Track,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Variance estimates and exploration function in multi-armed bandit", "author": ["Jean-Yves Audibert", "R\u00e9mi Munos", "Csaba Szepesv\u00e1ri"], "venue": "Technical report, research report 07-31, Certis-Ecole des Ponts,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f3 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem", "author": ["Peter Auer", "Ronald Ortner"], "venue": "Periodica Mathematica Hungarica,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems. Foundations and Trends in Machine Learning", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi"], "venue": "Now Publishers Incorporated,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Prior-free and prior-dependent regret bounds for thompson sampling", "author": ["S\u00e9bastien Bubeck", "Che-Yu Liu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Online optimization in X-armed bandits", "author": ["S\u00e9bastien Bubeck", "R\u00e9mi Munos", "Gilles Stoltz", "Csaba Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Bounded regret in stochastic multiarmed bandits", "author": ["S\u00e9bastien Bubeck", "Vianney Perchet", "Philippe Rigollet"], "venue": "Proceedings of the 26th Annual Conference on Learning Theory,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Asymptotically efficient adaptive choice of control laws in controlled Markov chains", "author": ["Todd L Graves", "Tze Leung Lai"], "venue": "SIAM journal on control and optimization,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Thompson sampling: An asymptotically optimal finite-time analysis", "author": ["Emilie Kaufmann", "Nathaniel Korda", "R\u00e9mi Munos"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Thompson sampling for 1-dimensional exponential family bandits", "author": ["Nathaniel Korda", "Emilie Kaufmann", "R\u00e9mi Munos"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Asymptotically optimal allocation of treatments in sequential experiments", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Design of Experiments: Ranking and Selection,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1984}, {"title": "Optimal sequential sampling from two populations", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1984}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1985}, {"title": "A structured multiarmed bandit problem and the greedy policy", "author": ["Adam J Mersereau", "Paat Rusmevichientong", "John N Tsitsiklis"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}], "referenceMentions": [{"referenceID": 17, "context": "This model has already been considered when the dependencies are linear [18] and also in the general setting studied here [12, 1].", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "This model has already been considered when the dependencies are linear [18] and also in the general setting studied here [12, 1].", "startOffset": 122, "endOffset": 129}, {"referenceID": 0, "context": "This model has already been considered when the dependencies are linear [18] and also in the general setting studied here [12, 1].", "startOffset": 122, "endOffset": 129}, {"referenceID": 5, "context": "Our main contribution is a new algorithm based on UCB [6] for the structured bandit problem with strong problem-dependent guarantees on the regret.", "startOffset": 54, "endOffset": 57}, {"referenceID": 16, "context": "The improved algorithm exploits the known structure and so avoids the famous negative results by Lai and Robbins [17].", "startOffset": 113, "endOffset": 117}, {"referenceID": 10, "context": "The most comparable previous work studies the case where both the return of the best arm and a bound on the minimum gap between the best arm and some sub-optimal arm is known [11, 9], which extended the permutation bandits studied by Lai and Robbins [16] and more general results by the same authors [15].", "startOffset": 175, "endOffset": 182}, {"referenceID": 8, "context": "The most comparable previous work studies the case where both the return of the best arm and a bound on the minimum gap between the best arm and some sub-optimal arm is known [11, 9], which extended the permutation bandits studied by Lai and Robbins [16] and more general results by the same authors [15].", "startOffset": 175, "endOffset": 182}, {"referenceID": 15, "context": "The most comparable previous work studies the case where both the return of the best arm and a bound on the minimum gap between the best arm and some sub-optimal arm is known [11, 9], which extended the permutation bandits studied by Lai and Robbins [16] and more general results by the same authors [15].", "startOffset": 250, "endOffset": 254}, {"referenceID": 14, "context": "The most comparable previous work studies the case where both the return of the best arm and a bound on the minimum gap between the best arm and some sub-optimal arm is known [11, 9], which extended the permutation bandits studied by Lai and Robbins [16] and more general results by the same authors [15].", "startOffset": 300, "endOffset": 304}, {"referenceID": 0, "context": "[1], which studied a similar setting, but where \u0398 was finite.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Graves and Lai [12] extended the aforementioned contribution to continuous parameter spaces (and also to MDPs).", "startOffset": 15, "endOffset": 19}, {"referenceID": 7, "context": "Most of our notation is common with [8].", "startOffset": 36, "endOffset": 39}, {"referenceID": 5, "context": "3 Structured UCB We propose a new algorithm called UCB-S that is a straight-forward modification of UCB [6], but where the known structure of the problem is exploited.", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "Algorithm 1 UCB-S 1: Input: functions \u03bc1, \u00b7 \u00b7 \u00b7 , \u03bck : \u0398 \u2192 [0, 1] 2: for t \u2208 1, .", "startOffset": 59, "endOffset": 65}, {"referenceID": 5, "context": "The first is for arbitrary \u03b8, which leads to a logarithmic bound on the regret comparable to that obtained for UCB by [6].", "startOffset": 118, "endOffset": 121}, {"referenceID": 0, "context": "[1] had essentially the same condition to achieve finite regret as (1), but specified to the case where \u0398 is finite.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11] where if the expected return of the best arm is known and \u03b5 is a known bound on the minimum gap, then a regret bound of O (", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "The improved UCB algorithm [7] enjoys a bound on the expected regret of O( \u2211", "startOffset": 27, "endOffset": 30}, {"referenceID": 4, "context": "Before the proof of Theorem 3 we need a high-probability bound on the number of times arm i is pulled, which is proven along the lines of similar results by [5].", "startOffset": 157, "endOffset": 160}, {"referenceID": 16, "context": "The famous paper by Lai and Robbins [17] shows that the bound of Theorem 2 cannot in general be greatly improved.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The proof uses the same technique as the proof of Theorem 5 in the paper by [11].", "startOffset": 76, "endOffset": 80}, {"referenceID": 10, "context": "(c) follows from Lemma 4 by [11] where KL(P\u2212\u03b8,t,P\u03b8,t) is the relative entropy between measures P\u2212\u03b8,t and P\u03b8,t.", "startOffset": 28, "endOffset": 32}, {"referenceID": 10, "context": "Again, we make use of the techniques of [11].", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "(c) by Lemma 4 of [11].", "startOffset": 18, "endOffset": 22}, {"referenceID": 5, "context": "7 Experiments We tested Algorithm 1 on a selection of structured bandits depicted in Figure 2 and compared to UCB [6, 8].", "startOffset": 114, "endOffset": 120}, {"referenceID": 7, "context": "7 Experiments We tested Algorithm 1 on a selection of structured bandits depicted in Figure 2 and compared to UCB [6, 8].", "startOffset": 114, "endOffset": 120}], "year": 2014, "abstractText": "We study a new type of K-armed bandit problem where the expected return of one arm may depend on the returns of other arms. We present a new algorithm for this general class of problems and show that under certain circumstances it is possible to achieve finite expected cumulative regret. We also give problemdependent lower bounds on the cumulative regret showing that at least in special cases the new algorithm is nearly optimal.", "creator": "Creator"}}}