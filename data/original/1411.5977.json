{"id": "1411.5977", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2014", "title": "On the Impossibility of Convex Inference in Human Computation", "abstract": "Human computation or crowdsourcing involves joint inference of the ground-truth-answers and the worker-abilities by optimizing an objective function, for instance, by maximizing the data likelihood based on an assumed underlying model. A variety of methods have been proposed in the literature to address this inference problem. As far as we know, none of the objective functions in existing methods is convex. In machine learning and applied statistics, a convex function such as the objective function of support vector machines (SVMs) is generally preferred, since it can leverage the high-performance algorithms and rigorous guarantees established in the extensive literature on convex optimization. One may thus wonder if there exists a meaningful convex objective function for the inference problem in human computation. In this paper, we investigate this convexity issue for human computation. We take an axiomatic approach by formulating a set of axioms that impose two mild and natural assumptions on the objective function for the inference. Under these axioms, we show that it is unfortunately impossible to ensure convexity of the inference problem. On the other hand, we show that interestingly, in the absence of a requirement to model \"spammers\", one can construct reasonable objective functions for crowdsourcing that guarantee convex inference.", "histories": [["v1", "Fri, 21 Nov 2014 18:51:10 GMT  (194kb,D)", "http://arxiv.org/abs/1411.5977v1", "AAAI 2015"]], "COMMENTS": "AAAI 2015", "reviews": [], "SUBJECTS": "stat.ML cs.HC cs.LG", "authors": ["nihar b shah", "dengyong zhou"], "accepted": true, "id": "1411.5977"}, "pdf": {"name": "1411.5977.pdf", "metadata": {"source": "CRF", "title": "On the Impossibility of Convex Inference in Human Computation", "authors": ["Nihar B. Shah", "Dengyong Zhou"], "emails": ["nihar@eecs.berkeley.edu", "dengyong.zhou@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Human computation (or crowdsourcing) involves humans performing tasks which are generally difficult for computers to perform. Since humans may not have perfect abilities and sometimes may not even have good intentions, machine learning and statistical inference algorithms are typically employed to post-process the data obtained from the human workers in order to infer the true answers (Dawid and Skene 1979; Whitehill et al. 2009; Welinder et al. 2010; Raykar et al. 2010; Karger, Oh, and Shah 2011; Wauthier and Jordan 2011; McCreadie, Macdonald, and Ounis 2011; Luon, Aperjis, and Huberman 2012; Zhou et al. 2012; Liu, Peng, and Ihler 2012; Shah et al. 2013; Bachrach et al. 2012; Kamar, Hacker, and Horvitz 2012; Vempaty, Varshney, and Varshney 2013; Salek, Bachrach, and Key 2013;\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nDoes the picture show a Bengal tiger ?\nShah et al. 2014; Matsui et al. 2013; Piech et al. 2013; Chen et al. 2013). These algorithms infer the true solutions to the tasks and the worker abilities by minimizing some carefully designed function that captures these parameters and their dependence on the observed responses. We shall call this function the \u201cobjective function\u201d. For example, it is typical to assume a generative model and take a maximum likelihood estimation. In such a case, the objective function is the negative-likelihood or the negativelog-likelihood of the data obtained from the workers, and Expectation-Maximization (EM) style optimization procedures are often employed to minimize this function (Dawid and Skene 1979; Whitehill et al. 2009; Welinder et al. 2010; Raykar et al. 2010; Liu, Peng, and Ihler 2012; Zhou et al. 2012; Chen et al. 2013).\nTo the best our knowledge, none of the objective functions in the literature for the inference problem in human computation is convex, and there are no generic guarantees available regarding obtaining a global optimum. Inspired by the extremely successful machine learning algorithms which are developed with convex objective functions (such as support vector machines (SVMs) (Vapnik 1998)), we investigate if there exists a meaningful convex objective function for the inference problem in human computation. Convex functions have appealing properties, and convex problems have been studied extensively in the literature, with high performance algorithms and rigorous guarantees available for very generic settings. It is thus of interest to investigate \u2018reasonable\u2019 models for human computation that ensure convexity in inference, thereby providing the ability to take advantage of this vast body of literature.\nIn this paper, we investigate the problem of convexity in\nar X\niv :1\n41 1.\n59 77\nv1 [\nst at\n.M L\n] 2\n1 N\nov 2\n01 4\nhuman computation via an axiomatic approach. Our results show that unfortunately, under two mild and natural assumptions in crowdsourcing, no model can guarantee convexity in the inference procedure. We show subsequently that all known models for crowdsourcing satisfy the two proposed mild axioms. The takeaway from this result is that it is futile to construct human-computation models, for present setups, that attempt to gain tractability by ensuring convexity. Finally, we show that interestingly, if one can forgo the explicit modelling of \u201cspammers\u201d, it is indeed possible to construct \u2018reasonable\u2019 models for human computation that guarantee convexity in the objective for inference."}, {"heading": "2 Problem Setting", "text": "Consider a binary-choice setting, where the worker must select from two given options for every question (e.g., Figure 1).1 Suppose there are k workers, each of whom is assumed to have some latent \u2018ability\u2019. Denote the latent ability of worker i \u2208 [k] as wi \u2208 [Wmin,Wmax] for some Wmin < Wmax \u2208 R, with a higher value of wi representing a more able worker.2 We will use the value \u201c0\u201d to represent the ability of a spammer and assume that 0 \u2208 [Wmin,Wmax). A spammer is a worker who answers randomly with no regard to the question being asked. Spammers are known to exist in plentiful numbers among the worker pools on crowdsourcing platforms (Bohannon 2011; Kazai et al. 2011; Vuurens, de Vries, and Eickhoff 2011; Wais et al. 2010). Throughout the paper, we will restrict our attention to the (convex) subset [0,Wmax] of the parameter space of the worker ability, since a function that is not convex on this set [0,Wmax] will not be convex on the superset [Wmin,Wmax] either. For convenience of notation, we define w := [w1 \u00b7 \u00b7 \u00b7wk]T \u2208 [0,Wmax]k.\nThere are d questions, and each question has two choices, say, \u201c0\u201d and \u201c1\u201d. This paper looks at procedures to infer, via convex optimization, the true answers x\u2217 \u2208 {0, 1}d to these d questions and the worker abilities w \u2208 [0,Wmax]k from the responses received from the workers. However, an optimization problem over a discrete set is non-convex by definition. A convex optimization based procedure will thus relax the discrete domain of x\u2217 to a continuous set. To this end, we associate every question j \u2208 [d] to a parameter xj \u2208 [0, 1]. The inference procedure operates on x := [x1 \u00b7 \u00b7 \u00b7xd]T \u2208 [0, 1]d, and the inferred values may be subsequently quantized to obtain a solution in the discrete set {0, 1}d.\nEvery question is asked to one or more of the k workers, and every worker is asked one or more of the d questions. We shall represent the workers\u2019 responses as a {0, 1,\u221e}- valued matrix Y of size (k \u00d7 d). For every i \u2208 [k] and\n1Results on \u2018impossibility of convexity\u2019 for the binary-choice setting extend to more general settings (such as multiple-choice with more than two choices) by restricting attention to only two choices.\n2We adopt the standard notation of representing the set {1, 2, . . . , \u03b1} as [\u03b1] for any positive integer \u03b1, and representing the interval of the real line between (and including) \u03b21 and \u03b22 as [\u03b21, \u03b22] for any two real numbers \u03b21 < \u03b22.\nj \u2208 [d], we let Yi,j denote the (i, j)th element of the matrix Y . The value of Yi,j is set as \u221e if worker i was not asked question j, and is set as worker i\u2019s response to question j otherwise. The answers to the questions and the abilities of the workers are now inferred by minimizing some reasonable objective function L of x and w given the workerresponses Y :\n(x\u0302, w\u0302) = argmin x\u2208[0,1]d,w\u2208[0,Wmax]k L (x,w;Y ) .\nIn the sequel, we will discuss the possible convexity of this optimization program under certain proposed axioms, and show that this optimization cannot be convex even after relaxing the domain of the answers from {0, 1}d to [0, 1]d.\nUpon completion of the inference procedure, an additional rounding step is often executed to convert x\u0302 \u2208 [0, 1]d to a discrete space x\u0302\u2217 \u2208 {0, 1}d. This rounding may be performed via a deterministic approach, for instance, by quantizing xj to 1 iff x\u0302j > 0.5, or via a probabilistic approach, for instance, by quantizing x\u0302j to 1 with a probability x\u0302j , for every question j \u2208 [d]. Many a times, however, the inferred vector x\u0302 is often left in the continuous set [0, 1]d as a \u201csoft\u201d output. In either case, the value |x\u0302j \u2212 0.5| is interpreted as the \u201cconfidence\u201d associated with the inference of the answer to the jth question, with a higher value of |x\u0302j \u2212 0.5| implying a higher confidence in the answer inferred for question j. A larger value of x\u0302j indicates a greater confidence that x\u2217j is 1. A higher value of w\u0302i for any i \u2208 [k] is interpreted as a greater belief in the ability of worker i."}, {"heading": "3 Axiomatization of Objective Function", "text": "We take an axiomatic approach towards the design of the objective function L, and formulate two weak and natural axioms that the objective function L must satisfy. Recall that the inference procedure minimizes the objective function L, and hence the objective function should have a lower value when its arguments form a better fit to the observed data. While the axioms are stated in a general manner below, for an intuitive understanding one may think of a maximum likelihood approach with L(x,w;Y ) as the negative log likelihood of the observed data Y conditioned on (x,w).\nFirst consider the case when there is only d = 1 question and k = 1 worker. The vectors x and w can be represented by scalars x \u2208 [0, 1] and w \u2208 [0,Wmax] respectively, and the matrix Y can be represented as a scalar in {0, 1}. Axiom 1 (Distinguishing different worker abilities). There exists > 0 such that L(x,w; 1) > L(x, 0; 1) \u2200(x,w) \u2208 (0, ) \u00d7 (0, ) and L(x,w; 1) < L(x, 0; 1) \u2200(x,w) \u2208 (1 \u2212 , 1)\u00d7 (0, ).\nInformally, Axiom 1 says that if the worker reports \u20181\u2019 when x is also close to 1, then the worker is likely to be more able. On the other hand, if the worker reports the opposite answer, then the worker is likely to be less able.\nAxiom 2 (Modeling spammers). The objective function L(x,w; 1) is independent of x when w = 0.\nA spammer is a worker who answers randomly, without any regard to the question being posed. Spammers are\nhighly abundant in today\u2019s crowdsourcing systems, and pose a major challenge to the data collection as well as the inference procedures. Axiom 2 necessitates an explicit incorporation of a spammer into the parameter space.\nOne may also define analogous axioms for the function L(\u00b7, \u00b7; 0), but for the purposes of this paper, it will suffice to work with solely the function L(\u00b7, \u00b7; 1). Observe that we have not made any other assumptions on the function L such as continuity, differentiability, or Lipchitzness.\nIn the next section, we will show that no objective function L for inference in human computation that satisfies the two simple requirements identified above can be convex. We will also show that all existing inference techniques for crowdsourcing (that we are aware of) fall into this class. We will also subsequently demonstrate, by means of a constructive example, the interesting fact that in the absence of the requirement of modelling a spammer, the objective function can indeed be convex.\nThe three axioms listed above for the setting of d = k = 1 are translated to the general setting of d \u2265 1 questions and k \u2265 1 workers in the following manner. Consider the convex subset of the domain of parameters x and w that is given by x = x1 and w = w1, where x \u2208 [0, 1], w \u2208 [0,Wmax]. Furthermore, suppose the observed data is Yi,j = 1 for every (i, j) for which worker i is asked question j. In this case the objective function reduces to being a function of only the scalars x and w, as L(x,w; 1), and we can now call upon the two axioms listed above. Now, if an optimization problem is non-convex over a convex subset of the parameter space, then it is non-convex over the entire parameter space as well. Thus, it suffices to show non-convexity for the case of d = k = 1 and the result for d \u2265 1, k \u2265 1 follows."}, {"heading": "4 Impossibility of Convexity", "text": "Theorem 1 below proves the impossibility of models guaranteeing convex inference in crowdsourcing.\nTheorem 1. No function L satisfying Axiom 1 and Axiom 2 can be convex.\nNon-convex problems are often converted to convex problems via transformations of the variables involved. Our result however says that for any such transformation of the variables at hand, as long as the semantic meanings of the variables are retained, it is reasonable to expect the two aforementioned axioms to be satisfied, rendering the result of Theorem 1 applicable."}, {"heading": "5 Some Examples of Existing Models", "text": "To the best of our knowledge, all existing models for inference in crowdsourcing tasks satisfy our two axioms. We illustrate the same with a few examples in this section.\nWhile the two axioms of Section 3 were constructed to identify the precise cause of non-convexity, in this section, we propose three general properties that we would like any objective function for crowdsourcing to satisfy. As we will see later, satisfying these three properties automatically implies adherence to the two axioms. We will also show that\nall existing models satisfy these three properties. As before, while the properties are defined for generic humancomputation models, for an easier understanding one may think of the objective function L as the negative log likelihood function. P1 (Monotonicity in accuracy of the answer). L(x,w; 1) is non-increasing in x.\nProperty P1 says that the likelihood does not decrease if x is brought closer to the observed response \u201c1 \u201d (recall that we are considering only the interval [0,Wmax] for w, where the latent ability of the worker is no worse than randomly answering). P2 (Monotonicity in worker ability). There exists some nondecreasing function g : [0,Wmax) \u2192 [0.5, 1) such that L(x,w; 1) increases with w when x < g(w) and L(x,w; 1) decreases with an increase in w when x > g(w).\nRecall that for an inferred answer x\u0302, the value |x\u0302 \u2212 0.5| represents the confidence associated to the inference. A higher confidence in the inference directly relates to a greater belief in the ability of a worker. Property P2 formalizes this relation, with function g(w) capturing the confidence associated to the work of a worker with ability w. The likelihood is thus higher when the confidence associated to the work of the worker is closer to the corresponding confidence in the inferred answer. P3 (Modeling spammers). The objective functionL(x,w; 1) is independent of x when w = 0.\nProperty P3 is identical to Axiom 2. Proposition 2. Any objective function L that satisfies Property P2 also satisfies Axiom 1.\nWe now present examples of existing models for crowdsourcing and show that these models indeed satisfy the three properties listed above (and therefore the two axioms defined in Section 3). Throughout this section, we will let n denote the number of responses received from the workers, i.e, the number of {0, 1}-valued entries in the response matrix Y . Example 1 (Dawid-Skene model). Model: The Dawid-Skene model is one of the most popular models for crowdsourcing (Dawid and Skene 1979; Ipeirotis, Provost, and Wang 2010; Gao and Zhou 2013; Zhang et al. 2014; Karger, Oh, and Shah 2011; Dalvi et al. 2013; Ghosh, Kale, and McAfee 2011). The model assumes that the ability of a worker represents the probability of her correctly answering any individual question, i.e., if worker i \u2208 [k] is asked question j \u2208 [d], then she will give the correct answer with a probability pi and an incorrect answer with probability (1 \u2212 pi), for some parameter pi \u2208 [0, 1] whose value is unknown. The response of any worker to any question is independent of all else. In order to ensure that the model is identifiable, it is typically also assumed that pi \u2208 [0.5, 1] \u2200i \u2208 [k], or 1k \u2211k i=1 pi \u2265 0.5, or simply p1 \u2265 0.5. We shall restrict our attention to pi \u2208 [0.5, 1] \u2200 i \u2208 [k]. Further, we will work with a shifted and scaled version of pi\u2019s by defining wi = 2pi\u2212 1 \u2200 i \u2208 [k]. Under this transformation, we have Wmin = 0 and Wmax = 1.\nInference: Consider inferring x and w via maximum likelihood estimation. Observe that the likelihood of observation Y is \u220f\n(i,j):Yi,j 6=\u221e\n( 1 + wi\n2\n)Yi,jxj+(1\u2212Yi,j)(1\u2212xj)\n\u00d7 ( 1\u2212 wi\n2\n)Yi,j(1\u2212xj)+(1\u2212Yi,j)xj .\nThe negative log-likelihood is thus given by L(x,w;Y ):=\u2212 \u2211 (i,j): Yi,j 6=\u221e ( (Yi,jxj+(1\u2212Yi,j)(1\u2212xj))log ( 1+wi 2 )\n+(Yi,j(1\u2212xj)+(1\u2212Yi,j)xj)log ( 1\u2212wi 2 )) ,\nand this function is minimized over (x,w)\u2208[0,1]d+k. Consider the following subspace of the arguments: x=x1 and w=w1 for some x\u2208[0,1] and w\u2208[0,1]. Further, suppose Yi,j=1 for every (i,j) for which worker i was asked question j. Under this restriction, the objective function reduces to\nL(x,w;1)=\u2212n ( xlog ( 1+w\n2\n) +(1\u2212x)log ( 1\u2212w 2 )) (1)\nProperties: Let us understand what the three properties listed earlier mean in this context. Observe that\nL(x, 0; 1) = \u2212n(x log 0.5 + (1\u2212 x) log 0.5) = \u2212n log 0.5,\nand hence L(x,w; 1) is independent of x when w = 0. Thus w = 0 models a spammer, and the functionL obeys Property P3. For Property P1, observe that when w \u2208 [0, 1], \u2202\n\u2202x L(x,w; 1) = \u2212n\n( log ( 1 + w\n2\n) \u2212 log ( 1\u2212 w 2 )) \u2264 0.\nThus Property P1 is satisfied. Finally,\n\u2202\n\u2202w L(x,w; 1) = \u2212n\n( x\n1 + w \u2212 1\u2212 x 1\u2212 w\n) ,\nwhich is positive when 2x\u2212 1 < w and negative when 2x\u2212 1 > w. Property P2 is thus satisfied with g(w) = 1+w2 .\nThe objective function (1) is plotted in Figure 2. Example 2 (Two-coin Dawid-Skene model). The two-coin model associates the ability of every worker i \u2208 [k] to two latent variables pi,0 \u2208 [0.5, 1] and pi,1 \u2208 [0.5, 1]. Under the two-coin model, the probability with which a worker answers a question correctly depends on the true answer to that question: if the true answer to a question is x\u2217 \u2208 {0, 1} then the worker (correctly) provides x\u2217 as the answer with probability pi,x\u2217 and (incorrectly) provides (1\u2212x\u2217) as the answer otherwise, independent of all else. In order to connect to our theory, we simply restrict our attention to a (convex) subset of the parameters {pi,0, pi,1}ki=1 obtained by setting pi,0 = pi,1 \u2200 i \u2208 [k]. The resulting model is identical to the one-coin model discussed earlier.\nExample 3 (Additive Noise). Model: The additive noise model assumes that when worker i \u2208 [k] is asked question j \u2208 [d], the response Yi,j of the worker is given by\nYi,j = 1{wi(xj \u2212 0.5) + i,j > 0},\nwhere { i,j}i\u2208[d],j\u2208[k] is a set of i.i.d. random variables with some (known) c.d.f. F . The function F is non-constant in the domain of interest [\u22120.5Wmax, 0.5Wmax]. The response is assumed to be independent of all other questions and all other workers. A common choice for F is the c.d.f. of the Gaussian distribution (Piech et al. 2013; Thurstone 1927; Welinder et al. 2010).\nInference: The inference is usually performed by minimizing the negative log likelihood of the observed data Y :\nargmin x,w\n\u2212 \u2211\n(i,j):Yi,j 6=\u221e\n(Yi,j log(1\u2212 F (\u2212(xj \u2212 0.5)wi))\n+(1\u2212 Yi,j) log(F (\u2212(xj \u2212 0.5)wi)) .\nProperties: Let us now relate this model to the three properties enumerated earlier. Let us restrict our attention to the (convex) subset of the parameters where x = x1 and w = w1 for some x \u2208 [0, 1] and w \u2208 [0,Wmax]. Suppose Yi,j = 1 for every (i, j) for which worker i was asked question j. The objective function then reduces to\nL(x,w; 1) = \u2212n log(1\u2212 F (\u2212(x\u2212 0.5)w)).\nOne can verify that this function is non-increasing in x (whenever w \u2265 0), thereby satisfying Property P1. Furthermore, setting g(w) = 0.5 satisfies Property P2. A spammer is modeled by the parameter value w = 0, in which case, the function L ceases to be dependent on x. Example 4 (Minimax Entropy Model). Model: The minimax entropy model (Zhou et al. 2012) hypothesizes that when worker i \u2208 [k] answers question j \u2208 [d], she provides \u201c1\u201d as the answer with a probability \u03c0i,j and \u201c0\u201d otherwise, independent of all else, for some unknown value \u03c0i,j \u2208 [0, 1]. Under the \u2018minimax entropy principle\u2019 proposed therein, the set {\u03c0i,j}i\u2208[k],j\u2208[d] has the maximum entropy under the constraints imposed by the set\nof true answers and the observed data, and the true answers minimize this value of maximum entropy:\nmin x max {\u03c0i,j}i\u2208[k],j\u2208[d] \u2212 k\u2211 i=1 d\u2211 j=1 \u03c0i,j ln\u03c0i,j\ns.t. k\u2211 i=1 \u03c0i,j = k\u2211 i=1 Yi,j \u2200j \u2208 [k]\nd\u2211 j=1 xj\u03c0i,j = d\u2211 j=1 xjYi,j \u2200i \u2208 [k] d\u2211 j=1 (1\u2212 xj)\u03c0i,j = d\u2211 j=1 (1\u2212 xj)Yi,j \u2200i \u2208 [k]\n0 \u2264 \u03c0i,j , xj \u2264 1 \u2200i \u2208 [k], j \u2208 [d].\nInference: The authors show that the values \u03c0i,j must necessarily be of the form\n\u03c0i,j= exp((1\u2212xj)(\u03c4j,1+\u03c3i,1,0)+xj(\u03c4j,1+\u03c3i,1,1))\u22111 `=0 exp((1\u2212xj)(\u03c4j,`+\u03c3i,`,0)+xj(\u03c4j,`+\u03c3i,`,1))\nfor some parameters {\u03c3i,`1,`2}i\u2208[k],`1\u2208{0,1},`2\u2208{0,1} and {\u03c4j,`}j\u2208[d],`\u2208{0,1}. The authors then propose minimization (with respect to variables {xi, \u03c3i,`1,`2 , \u03c4j,`}) of the dual of the aforementioned program, which they derive to be of the form \u2212 \u2211 (i,j): Yi,j 6=\u221e exp ( (1\u2212xj)(\u03c4j,Yi,j+\u03c3i,Yi,j ,0)+xj(\u03c4j,Yi,j+\u03c3i,Yi,j ,1) )\u22111 `=0exp((1\u2212xj)(\u03c4j,`+\u03c3i,`,0)+xj(\u03c4j,`+\u03c3i,`,1))\nProperties: Consider the following (convex) subset of the parameter space: \u03c4j,` = 0 \u2200j \u2208 [d], ` \u2208 {0, 1}, \u03c3i,0,0 = \u03c3i,1,1 = 1\u2212\u03c3i,1,0 = 1\u2212\u03c3i,0,1 := 0.5+wi \u2208 [0, 1] \u2200i \u2208 [k]. The minimization program can now be rewritten as\nargmin x,w\u2208[0,1]d+k\n\u2211 L(xi, wj ;Yi,j)\nwhere\nL(x,w; 1) :=\ne(1\u2212x)(0.5\u2212w)+x(0.5+w)\ne(1\u2212x)(0.5\u2212w)+x(0.5+w) + ex(0.5\u2212w)+(1\u2212x)(0.5+w) ,\nand L(x,w; 0) = L(1 \u2212 x,w; 1). With some algebraic manipulations, one can verify that this function L(\u00b7, \u00b7; 1) satisfies Property P1, Property P2 (with g(w) = 0.5) and Property P3 (with w = 0 representing a spammer). Example 5 (GLAD model). Model and inference: The GLAD model was introduced in (Whitehill et al. 2009). We will restrict attention to the subspace of the parameter set which has, in the notation of (Whitehill et al. 2009), \u03b2j = 0 \u2200 j. Using the notation of the present paper for the rest of the parameters, the objective function (the negative log likelihood) is\nL(x,w; 1) = nx log ( 1 + e\u2212w ) + n(1\u2212 x) log (1 + ew)\nunder the convex subset x = x1 and w = w1 of the parameters, wth x \u2208 [0, 1], w \u2208 [0, 1].\nProperties: The derivative of L(x,w; 1) with respect to x is non-positive (for w \u2265 0), thus satisfying Property P1. One can also verify that Property P2 is satisfied with g(w) = 11+e\u2212w . Finally, we have L(x, 0; 1) = n log 2, thereby satisfying Property P3."}, {"heading": "6 If Not Modelling Spammers", "text": "We now discuss the role of modeling a spammer, i.e., Axiom 2. We show that ignoring this axiom indeed allows for convex objective functions for crowdsourcing, not only satisfying Axiom 1, but also satisfying the stronger properties P1 and P2. Theorem 3. Let Wmin = 0 and Wmax = 1. The function L : [0, 1]\u00d7 [0, 1]\u2192 R defined as\nL(x,w; 1) = { \u2212w \u2212 x\u2212 1 if w \u2264 2x\u2212 1 w \u2212 5x+ 1 if w \u2265 2x\u2212 1 (2)\nsatisfies properties P1 and P2 (and hence Axiom 1) and is (jointly) convex in its two arguments.\nFor the general setting of multiple workers and multiple questions, letting\nL(x,w; 0) = L(1\u2212 x,w; 1), and\nL(x,w;Y ) = \u2211\n(i,j):Yi,j 6=\u221e\nL(xi, wj ;Yij)\nensures that the function L(\u00b7, \u00b7;Y ) satisfies Axiom 1 and is convex in its arguments (x,w). The objective function L(\u00b7, \u00b7; 1) constructed in Theorem 3 is plotted in Figure 3.\nNote that we do not intend to claim the proposed function (2) as a \u201cgood\u201d objective function to use for crowdsourcing. Instead, the takeaway from this section is that if one forgoes the inclusion of spammers in the objective, then one may indeed be able to design a crowdsourcing model that is reasonable and permits convex inference."}, {"heading": "7 Discussion", "text": "It is important to be aware of the limitations of the framework of this paper. Throughout the paper we assumed no prior knowledge or complexity controls on the parameter space. One may alternatively consider the inference problem in a Bayesian setting with non-uniform priors, or impose some convex regularization. In fact, if the regularizer is strictly convex, then giving it a sufficiently large weight can make the objective function convex, albeit perhaps at the expense of the model not capturing certain essential features of the problem. However, as long as the objective function continues to satisfy the two axioms presented here, the conclusions drawn in this paper continue to apply.\nWhile convexity is certainly desirable, absence of convexity certainly does not mean the complete absence of guarantees; indeed, there is a line of recent works (Loh and Wainwright 2013; Netrapalli, Jain, and Sanghavi 2013; Zhang et al. 2014) which provide guarantees for non-convex problems as well. In particular, although existing models for human computation are not convex, there exist theoretical guarantees on inference under the popular Dawid-Skene model to a certain extent (Ghosh, Kale, and McAfee 2011; Karger, Oh, and Shah 2011; Dalvi et al. 2013; Gao and Zhou 2013; Zhang et al. 2014). For instance, (Zhang et al. 2014) show that the EM algorithm for the Dawid-Skene model can achieve a minimax rate up to a logarithmic factor when it is appropriately initialized by spectral methods. However, these results need certain conditions which may not hold in real scenarios. Moreover, algorithms that are minimax optimal may not always work well in practice, for instance, see the experimental results in (Liu, Peng, and Ihler 2012). Most importantly, guarantees for non-convex problems are constructed to-date on a case-by-case basis (for example, all known theoretical guarantees for crowdsourcing are for the Dawid-Skene model alone). These guarantees do not allow for a convenient application of the theory to any new model. On the other hand, although the theory of convex optimization is highly generic and extensive, the results of this paper imply that unfortunately one cannot readily exploit this theory in the context of human computation.\nThis paper also shows that a willingness to forgo the explicit incorporation of spammers into the crowdsourcing model indeed allows for reasonable objective functions guaranteeing convex inference. Successful deterrence of spammers in crowdsourcing systems, for instance, by designing suitable reward mechanisms, may thus expand the scope of model-design for human computation. In conclusion, we would like to enumerate, partially in jest, the problems resulting from spammers in crowdsourcing systems: (a) low-quality work, (b) depletion of the monetary budget, and now, (c) prevention of models guaranteeing convex inference."}, {"heading": "Appendix: Proofs", "text": "Proof of Theorem 1: The proof employs a contradictionbased argument. Suppose there exists some function L(\u00b7, \u00b7; 1) that satisfies the two axioms and is convex in its two arguments in the set [0, 1] \u00d7 [0,Wmax]. Without loss\nof generality assume \u2208 (0,min{0.5,Wmax}). Axiom 1 mandates\nL( \u2212 2/2, 2/2; 1) > L( \u2212 2/2, 0; 1) and (3) L(1\u2212 /2, /2; 1) < L(1\u2212 /2, 0; 1). (4)\nThe assumed convexity of function L implies\n(1\u2212 )L(0,0;1)+ L(1\u2212 /2, /2;1)\u2265L( \u2212 2/2, 2/2;1).\nSubstituting (3) and (4) in this inequality gives\n(1\u2212 )L(0, 0; 1) + L(1\u2212 /2, 0; 1) > L( \u2212 2/2, 0; 1).\nNow, calling upon Axiom 2 gives\n(1\u2212 )L(0, 0; 1) + L(0, 0; 1) > L(0, 0; 1),\nyielding the desired contradiction.\nProof of Proposition 2: Pick a value \u03b4 arbitrarily in (0,min{1,Wmax}/2). Set\n= min{\u03b4, 1\u2212 g(\u03b4)}.\nObserve that due to Property P2, \u2200 x < 0.5 (which includes all x < ), L(x,w; 1) increases with an increase in w, and hence L(x,w; 1) > L(x, 0; 1) when w > 0. Also, for our chosen , Property P2 implies that for any x \u2208 (1\u2212 , 1) and w < , the function L(x,w; 1) decreases with an increase in w. Thus we have L(x,w; 1) < L(x, 0; 1) \u2200 (x,w) \u2208 (1\u2212 , 1)\u00d7 (0, ). Axiom 1 is thus satisfied.\nProof of Theorem 3: Let us first verify that the proposed function L(\u00b7, \u00b7; 1) satisfies the two properties. First, observe from the definition of L in (2) that L is always (strictly) decreasing in its first argument, and hence satisfies Property P1. Towards Property P2, set\ng(w) = (1 + w)/2.\nBy definition, L(x,w; 1) (strictly) decreases with an increase in w when x > g(w), and (strictly) increases with an increase in w when x < g(w). Proposition 2 now guarantees that the function also satisfies Axiom 1.\nLet us now investigate the convexity of this function. Consider two hyperplanes H0 and H1 defined as\nH0(x,w) = \u2212w \u2212 x\u2212 1 H1(x,w) = w \u2212 5x+ 1.\nObserve that for any (x,w),\nH0(x,w)\u2212H1(x,w) = 2(\u2212w + 2x\u2212 1).\nThus, H0(x,w) \u2265 H1(x,w) if x \u2265 (1 + w)/2 and H0(x,w) \u2264 H1(x,w) if x \u2264 (1 + w)/2. It follows that\nL(x,w; 1) = max{H0(x,w), H1(x,w)}\nmeaning that L is the maximum of two linear functions. Hence L is convex."}], "references": [{"title": "How to grade a test without knowing the answers\u2014a Bayesian graphical model for adaptive crowdsourcing and aptitude testing", "author": ["Bachrach"], "venue": "In ICML", "citeRegEx": "Bachrach,? \\Q2012\\E", "shortCiteRegEx": "Bachrach", "year": 2012}, {"title": "P", "author": ["Chen, X.", "Bennett"], "venue": "N.; Collins-Thompson, K.; and Horvitz, E.", "citeRegEx": "Chen et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Aggregating crowdsourced binary ratings", "author": ["Dalvi"], "venue": "In International conference on World Wide Web,", "citeRegEx": "Dalvi,? \\Q2013\\E", "shortCiteRegEx": "Dalvi", "year": 2013}, {"title": "A", "author": ["A.P. Dawid", "Skene"], "venue": "M.", "citeRegEx": "Dawid and Skene 1979", "shortCiteRegEx": null, "year": 1979}, {"title": "and Zhou", "author": ["C. Gao"], "venue": "D.", "citeRegEx": "Gao and Zhou 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Who moderates the moderators?: Crowdsourcing abuse detection in user-generated content", "author": ["Kale Ghosh", "A. McAfee 2011] Ghosh", "S. Kale", "P. McAfee"], "venue": "In ACM conference on Electronic commerce,", "citeRegEx": "Ghosh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ghosh et al\\.", "year": 2011}, {"title": "Combining human and machine intelligence in large-scale crowdsourcing", "author": ["Hacker Kamar", "E. Horvitz 2012] Kamar", "S. Hacker", "E. Horvitz"], "venue": "In International Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Kamar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kamar et al\\.", "year": 2012}, {"title": "D", "author": ["Karger"], "venue": "R.; Oh, S.; and Shah, D.", "citeRegEx": "Karger. Oh. and Shah 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Crowdsourcing for book search evaluation: impact of HIT design on comparative system ranking", "author": ["Kazai"], "venue": "In ACM SIGIR conference on Research and development in Information Retrieval,", "citeRegEx": "Kazai,? \\Q2011\\E", "shortCiteRegEx": "Kazai", "year": 2011}, {"title": "Variational inference for crowdsourcing", "author": ["Peng Liu", "Q. Ihler 2012] Liu", "J. Peng", "A. Ihler"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "M", "author": ["Loh", "P.-L.", "Wainwright"], "venue": "J.", "citeRegEx": "Loh and Wainwright 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "B", "author": ["Y. Luon", "C. Aperjis", "Huberman"], "venue": "A.", "citeRegEx": "Luon. Aperjis. and Huberman 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Crowdsourcing quality control for item ordering tasks. In AAAI HCOMP", "author": ["Matsui"], "venue": null, "citeRegEx": "Matsui,? \\Q2013\\E", "shortCiteRegEx": "Matsui", "year": 2013}, {"title": "Crowdsourcing blog track top news judgments at TREC. In Workshop on crowdsourcing for search and data mining at the ACM international conference on web search and data mining (WSDM), 23\u201326", "author": ["Macdonald McCreadie", "R. Ounis 2011] McCreadie", "C. Macdonald", "I. Ounis"], "venue": null, "citeRegEx": "McCreadie et al\\.,? \\Q2011\\E", "shortCiteRegEx": "McCreadie et al\\.", "year": 2011}, {"title": "Phase retrieval using alternating minimization", "author": ["Jain Netrapalli", "P. Sanghavi 2013] Netrapalli", "P. Jain", "S. Sanghavi"], "venue": null, "citeRegEx": "Netrapalli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Netrapalli et al\\.", "year": 2013}, {"title": "Tuned models of peer assessment", "author": ["Piech"], "venue": null, "citeRegEx": "Piech,? \\Q2013\\E", "shortCiteRegEx": "Piech", "year": 2013}, {"title": "G", "author": ["V.C. Raykar", "S. Yu", "L.H. Zhao", "Valadez"], "venue": "H.; Florin, C.; Bogoni, L.; and Moy, L.", "citeRegEx": "Raykar et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Hotspotting\u2013a probabilistic graphical model for image object localization through crowdsourcing", "author": ["Bachrach Salek", "M. Key 2013] Salek", "Y. Bachrach", "P. Key"], "venue": "In TwentySeventh AAAI Conference on Artificial Intelligence", "citeRegEx": "Salek et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Salek et al\\.", "year": 2013}, {"title": "J", "author": ["Shah, N.B.", "Bradley"], "venue": "K.; Parekh, A.; Wainwright, M.; and Ramchandran, K.", "citeRegEx": "Shah et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "N", "author": ["Shah"], "venue": "B.; Balakrishnan, S.; Bradley, J.; Parekh, A.; Ramchandran, K.; and Wainwright, M.", "citeRegEx": "Shah et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "V", "author": ["Vapnik"], "venue": "N.", "citeRegEx": "Vapnik 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "P", "author": ["A. Vempaty", "L.R. Varshney", "Varshney"], "venue": "K.", "citeRegEx": "Vempaty. Varshney. and Varshney 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "How much spam can you take? An analysis of crowdsourcing results to increase accuracy", "author": ["de Vries Vuurens", "J. Eickhoff 2011] Vuurens", "A.P. de Vries", "C. Eickhoff"], "venue": "In ACM SIGIR Workshop on Crowdsourcing for Information Retrieval,", "citeRegEx": "Vuurens et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vuurens et al\\.", "year": 2011}, {"title": "Towards building a high-quality workforce with Mechanical Turk. NIPS workshop on computational social science and the wisdom of crowds", "author": ["Wais"], "venue": null, "citeRegEx": "Wais,? \\Q2010\\E", "shortCiteRegEx": "Wais", "year": 2010}, {"title": "and Jordan", "author": ["F.L. Wauthier"], "venue": "M.", "citeRegEx": "Wauthier and Jordan 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "S", "author": ["P. Welinder", "S. Branson", "P. Perona", "Belongie"], "venue": "J.", "citeRegEx": "Welinder et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Whose vote should count more: Optimal integration of labels from labelers of unknown expertise", "author": ["Whitehill"], "venue": null, "citeRegEx": "Whitehill,? \\Q2009\\E", "shortCiteRegEx": "Whitehill", "year": 2009}, {"title": "M", "author": ["Y. Zhang", "X. Chen", "D. Zhou", "Jordan"], "venue": "I.", "citeRegEx": "Zhang et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning from the wisdom of crowds by minimax entropy", "author": ["Zhou"], "venue": null, "citeRegEx": "Zhou,? \\Q2012\\E", "shortCiteRegEx": "Zhou", "year": 2012}], "referenceMentions": [], "year": 2014, "abstractText": "Human computation or crowdsourcing involves joint inference of the ground-truth-answers and the workerabilities by optimizing an objective function, for instance, by maximizing the data likelihood based on an assumed underlying model. A variety of methods have been proposed in the literature to address this inference problem. As far as we know, none of the objective functions in existing methods is convex. In machine learning and applied statistics, a convex function such as the objective function of support vector machines (SVMs) is generally preferred, since it can leverage the highperformance algorithms and rigorous guarantees established in the extensive literature on convex optimization. One may thus wonder if there exists a meaningful convex objective function for the inference problem in human computation. In this paper, we investigate this convexity issue for human computation. We take an axiomatic approach by formulating a set of axioms that impose two mild and natural assumptions on the objective function for the inference. Under these axioms, we show that it is unfortunately impossible to ensure convexity of the inference problem. On the other hand, we show that interestingly, in the absence of a requirement to model \u201cspammers\u201d, one can construct reasonable objective functions for crowdsourcing that guarantee convex inference.", "creator": "LaTeX with hyperref package"}}}