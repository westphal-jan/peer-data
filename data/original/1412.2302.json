{"id": "1412.2302", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2014", "title": "Theano-based Large-Scale Visual Recognition with Multiple GPUs", "abstract": "In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012) implementation and its naive data parallelism on multiple GPUs. Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU. To the best of our knowledge, this is the first open-source Python-based AlexNet implementation to-date.", "histories": [["v1", "Sun, 7 Dec 2014 01:12:10 GMT  (82kb,D)", "http://arxiv.org/abs/1412.2302v1", null], ["v2", "Thu, 1 Jan 2015 07:37:05 GMT  (82kb,D)", "http://arxiv.org/abs/1412.2302v2", null], ["v3", "Fri, 27 Feb 2015 04:53:46 GMT  (83kb,D)", "http://arxiv.org/abs/1412.2302v3", null], ["v4", "Mon, 6 Apr 2015 20:46:11 GMT  (83kb,D)", "http://arxiv.org/abs/1412.2302v4", "ICLR 2015 workshop camera-ready version"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weiguang ding", "ruoyan wang", "fei mao", "graham taylor"], "accepted": true, "id": "1412.2302"}, "pdf": {"name": "1412.2302.pdf", "metadata": {"source": "CRF", "title": "TION WITH MULTIPLE GPUS", "authors": ["Weiguang Ding", "Ruoyan Wang", "Fei Mao"], "emails": ["ruoyanry}@uoguelph.ca", "feimao@sharcnet.ca", "gwtaylor@uoguelph.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep neural networks have greatly impacted many application areas. In particular, AlexNet (Krizhevsky et al., 2012), a type of convolutional neural network (LeCun et al., 1998) (ConvNet), has significantly improved the performance of image classification by winning the 2012 ImageNet Large Scale Visual Recognition Challenge (Russakovsky et al., 2014) (ILSVRC 2012). With the increasing popularity of deep learning, many open-source frameworks have emerged with the capability to train deep ConvNets on datasets with over 1M examples. These include Caffe (Jia et al., 2014), Torch7 (Collobert et al., 2011) and cuda-convnet (Krizhevsky et al., 2012). However, the convenience of using them are limited to building \u201cstandard\u201d architectures. To experiment with brand new architectures, researchers have to derive and implement the corresponding gradient functions in order to do backpropagation or other types of gradient descent optimizations.\nTheano (Bergstra et al., 2010; Bastien et al., 2012), on the other hand, provides the automatic differentiation feature, which saves researchers from tedious derivations and can help in avoiding errors in such calculations. The other advantage of Theano is that it has a huge existing user and developer base which leverages the comprehensive scientific Python stack (102 contributors at the time of writing). However, there is no previously reported work of using Theano to do large scale experiments, such as the above mentioned ILSVRC 2012.\nHere, we report a Theano-based AlexNet trained on ImageNet data 1. We also introduce a naive data parallelism implementation on multiple GPUs, to further accelerate training."}, {"heading": "2 METHODS", "text": "\u201cAlexNet\u201d is a now a standard architecture known in the deep learning community and often used for benchmarking. It contains 5 convolutional layers, 3 of which are followed by max pooling\n1The code is open sourced at https://github.com/uoguelph-mlrg/theano_alexnet. In addition, a toy example is provided at https://github.com/uoguelph-mlrg/theano_multi_gpu.\nar X\niv :1\n41 2.\n23 02\nv1 [\ncs .L\nG ]\n7 D\nlayers, 2 fully connected layers, and 1 softmax layer (Krizhevsky et al., 2012). In our AlexNet implementation, we use convolution and max pooling operators from the Pylearn2 (Goodfellow et al., 2013) wrapper of cuda-convnet, the original implemenation of AlexNet. We also use functions in the PyCUDA library (Klo\u0308ckner et al., 2012) to transfer Theano shared variables between different python processes for two tasks: 1. loading image mini-batches into GPUs during training; and 2. exchanging weights between models trained on multiple-GPUs."}, {"heading": "2.1 PARALLEL DATA LOADING", "text": "Figure 1 illustrates the process of parallelized training and data loading. Two processes run at the same time, one is for training, and the other one is for loading image mini-batches. While the training process is working on the current minibatch, the loading process is copying the next minibatch from disk to host memory, preprocessing2 it and copying it from host memory to GPU memory. After training on the current minibatch finishes, the data batch will be moved \u201cinstantly\u201d from the loading process to the training process, as they access the same GPU."}, {"heading": "2.2 DATA PARALLELISM", "text": "In this implementation, 2 AlexNets are trained on 2 GPUs. They are initialized identically. At each step, they are updated on different minibatches respectively, and then their parameters (weights, biases) as well as momentum are exchanged and averaged.\n2Preprocessing includes subtracting the mean image, randomly cropping and flipping images (Krizhevsky et al., 2012).\nFigure 2 illustrates the steps involved in training on one minibatch. For each weight3 matrix in the model, there are 2 shared variables allocated: one for updating, and one for storing weights copied from the other GPU. The shared variables for updating on 2 GPUs start the same. In the 1st step, they are updated separately on different data batches. In the 2nd step, weights are exchanged between GPUs. In the 3rd step, these weights (no longer the same) are averaged on both GPUs. At this point, 2 AlexNets sharing the same parameters are ready for training on the next mini-batch."}, {"heading": "3 RESULTS", "text": "Our experimental system contains 2 Intel Xeon E5-2620 CPUs (6-core each and 2.10GHz), and 3 Nvidia Titan Black GPUs. 2 of the GPUs are under the same PCI-E switch and are used for the 2-GPU implementation. We do not use the third GPU.\nFor the experiments on a single GPU, we use batch size 256. Equivalently, we use batch size 128 for experiments on 2 GPUs. We recorded the time to train 20 batches (5,120 images) under different settings and compared them with Caffe4 in Table 1.\nWe can see that both parallel loading and data parallelism on 2 GPUs bring significant speed ups. The 2-GPU & parallel loading implementation is slightly faster than Caffe itself and slower than Caffe with cuDNN (Chetlur et al., 2014)."}, {"heading": "4 DISCUSSION", "text": ""}, {"heading": "4.1 ALTERNATIVE IMPLEMENTATION OF ALEXNET", "text": "A potential alternative to the cuda-convnet pylearn2 wrapper is to use cuDNN-accelerated Theano inbuilt convolutional operations. However, using this to replicate AlexNet exactly is difficult at this point, because Theano currently does not provide the padding mode \u201csame\u201d (where the input shape before and output shape after convolution are the same), which is used in two of its convolutional layers. Another reason is that, in AlexNet, max pooling strides might be different from max pooling filter sizes, which is not currently supported by inbuilt Theano operations 5. However, this limitation only applies for exactly replicating AlexNet. It does not prevent one from using Theano\u2019s inbuilt functions to construct and deploy other large architectures."}, {"heading": "4.2 NATIVE THEANO MULTI-GPU SUPPORT", "text": "Native Theano multi-GPU support is under development6. Our present implementation is a temporary work-around before its release, and might also provide helpful communication components on top of it."}, {"heading": "4.3 RELATED WORK", "text": "Many multi-GPU frameworks has been proposed and implemented (Yadan et al., 2013; Zou et al., 2014; Paine et al., 2013; Krizhevsky, 2014), usually adopting a mixed data and model parallelism.\n3The same operation is performed for biases and momentum. 4Performance of Caffe is according to http://caffe.berkeleyvision.org/performance_ hardware.html, where timing information for CaffeNet is provided. As CaffeNet has similar structures, we consider this as a rough reference.\n5Although pylearn2 provides this functionality, it is considerably slower. 6https://groups.google.com/d/msg/theano-users/vtR_L0QltpE/Kp5hK1nFLtsJ\nThis report only implements the data parallelism framework, but it could potentially, with a nontrivial amount of effort, be extended to incorporate model parallelism."}, {"heading": "4.4 LIMITATIONS", "text": "To use the fast peer-to-peer GPU memory copy, the GPUs have to be under the same PCI-E switch. Otherwise, the communication has to go through the host memory which results in longer latency. Situations involved with more GPUs are discussed in Krizhevsky (2014).\nDue to our current hardware limitation, we have only proposed and experimented with a 2-GPU implementation. This report and the code will be updated once experiments on more GPUs are performed."}, {"heading": "ACKNOWLEDGMENTS", "text": "We acknowledge Lev Givon for giving helpful suggestions on how to use the PyCUDA library."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1211.5590,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In Proceedings of the Python for scientific computing conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["Chetlur", "Sharan", "Woolley", "Cliff", "Vandermersch", "Philippe", "Cohen", "Jonathan", "Tran", "John", "Catanzaro", "Bryan", "Shelhamer", "Evan"], "venue": "arXiv preprint arXiv:1410.0759,", "citeRegEx": "Chetlur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Cl\u00e9ment"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Pylearn2: a machine learning research", "author": ["Goodfellow", "Ian J", "Warde-Farley", "David", "Lamblin", "Pascal", "Dumoulin", "Vincent", "Mirza", "Mehdi", "Pascanu", "Razvan", "Bergstra", "James", "Bastien", "Fr\u00e9d\u00e9ric", "Bengio", "Yoshua"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Pycuda and pyopencl: A scripting-based approach to gpu run-time code generation", "author": ["Kl\u00f6ckner", "Andreas", "Pinto", "Nicolas", "Lee", "Yunsup", "Catanzaro", "Bryan", "Ivanov", "Paul", "Fasih", "Ahmed"], "venue": "Parallel Computing,", "citeRegEx": "Kl\u00f6ckner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kl\u00f6ckner et al\\.", "year": 2012}, {"title": "One weird trick for parallelizing convolutional neural networks", "author": ["Krizhevsky", "Alex"], "venue": "arXiv preprint arXiv:1404.5997,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2014\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Gpu asynchronous stochastic gradient descent to speed up neural network training", "author": ["Paine", "Thomas", "Jin", "Hailin", "Yang", "Jianchao", "Lin", "Zhe", "Huang"], "venue": "arXiv preprint arXiv:1312.6186,", "citeRegEx": "Paine et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Paine et al\\.", "year": 2013}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Russakovsky", "Olga", "Deng", "Jia", "Su", "Hao", "Krause", "Jonathan", "Satheesh", "Sanjeev", "Ma", "Sean", "Huang", "Zhiheng", "Karpathy", "Andrej", "Khosla", "Aditya", "Bernstein", "Michael"], "venue": "arXiv preprint arXiv:1409.0575,", "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Multi-gpu training of convnets", "author": ["Yadan", "Omry", "Adams", "Keith", "Taigman", "Yaniv", "Ranzato", "MarcAurelio"], "venue": "arXiv preprint arXiv:1312.5853,", "citeRegEx": "Yadan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yadan et al\\.", "year": 2013}, {"title": "Mariana: Tencent deep learning platform and its applications", "author": ["Zou", "Yongqiang", "Jin", "Xing", "Li", "Yi", "Guo", "Zhimao", "Wang", "Eryu", "Xiao", "Bin"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "Zou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "ABSTRACT In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012) implementation and its naive data parallelism on multiple GPUs.", "startOffset": 60, "endOffset": 85}, {"referenceID": 5, "context": "Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU.", "startOffset": 76, "endOffset": 94}, {"referenceID": 8, "context": "In particular, AlexNet (Krizhevsky et al., 2012), a type of convolutional neural network (LeCun et al.", "startOffset": 23, "endOffset": 48}, {"referenceID": 9, "context": ", 2012), a type of convolutional neural network (LeCun et al., 1998) (ConvNet), has significantly improved the performance of image classification by winning the 2012 ImageNet Large Scale Visual Recognition Challenge (Russakovsky et al.", "startOffset": 48, "endOffset": 68}, {"referenceID": 11, "context": ", 1998) (ConvNet), has significantly improved the performance of image classification by winning the 2012 ImageNet Large Scale Visual Recognition Challenge (Russakovsky et al., 2014) (ILSVRC 2012).", "startOffset": 156, "endOffset": 182}, {"referenceID": 5, "context": "These include Caffe (Jia et al., 2014), Torch7 (Collobert et al.", "startOffset": 20, "endOffset": 38}, {"referenceID": 3, "context": ", 2014), Torch7 (Collobert et al., 2011) and cuda-convnet (Krizhevsky et al.", "startOffset": 16, "endOffset": 40}, {"referenceID": 8, "context": ", 2011) and cuda-convnet (Krizhevsky et al., 2012).", "startOffset": 25, "endOffset": 50}, {"referenceID": 1, "context": "Theano (Bergstra et al., 2010; Bastien et al., 2012), on the other hand, provides the automatic differentiation feature, which saves researchers from tedious derivations and can help in avoiding errors in such calculations.", "startOffset": 7, "endOffset": 52}, {"referenceID": 0, "context": "Theano (Bergstra et al., 2010; Bastien et al., 2012), on the other hand, provides the automatic differentiation feature, which saves researchers from tedious derivations and can help in avoiding errors in such calculations.", "startOffset": 7, "endOffset": 52}, {"referenceID": 8, "context": "layers, 2 fully connected layers, and 1 softmax layer (Krizhevsky et al., 2012).", "startOffset": 54, "endOffset": 79}, {"referenceID": 4, "context": "In our AlexNet implementation, we use convolution and max pooling operators from the Pylearn2 (Goodfellow et al., 2013) wrapper of cuda-convnet, the original implemenation of AlexNet.", "startOffset": 94, "endOffset": 119}, {"referenceID": 6, "context": "We also use functions in the PyCUDA library (Kl\u00f6ckner et al., 2012) to transfer Theano shared variables between different python processes for two tasks: 1.", "startOffset": 44, "endOffset": 67}, {"referenceID": 8, "context": "Preprocessing includes subtracting the mean image, randomly cropping and flipping images (Krizhevsky et al., 2012).", "startOffset": 89, "endOffset": 114}, {"referenceID": 2, "context": "The 2-GPU & parallel loading implementation is slightly faster than Caffe itself and slower than Caffe with cuDNN (Chetlur et al., 2014).", "startOffset": 114, "endOffset": 136}, {"referenceID": 12, "context": "3 RELATED WORK Many multi-GPU frameworks has been proposed and implemented (Yadan et al., 2013; Zou et al., 2014; Paine et al., 2013; Krizhevsky, 2014), usually adopting a mixed data and model parallelism.", "startOffset": 75, "endOffset": 151}, {"referenceID": 13, "context": "3 RELATED WORK Many multi-GPU frameworks has been proposed and implemented (Yadan et al., 2013; Zou et al., 2014; Paine et al., 2013; Krizhevsky, 2014), usually adopting a mixed data and model parallelism.", "startOffset": 75, "endOffset": 151}, {"referenceID": 10, "context": "3 RELATED WORK Many multi-GPU frameworks has been proposed and implemented (Yadan et al., 2013; Zou et al., 2014; Paine et al., 2013; Krizhevsky, 2014), usually adopting a mixed data and model parallelism.", "startOffset": 75, "endOffset": 151}], "year": 2017, "abstractText": "In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012) implementation and its naive data parallelism on multiple GPUs. Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU. To the best of our knowledge, this is the first open-source Python-based AlexNet implementation to-date.", "creator": "LaTeX with hyperref package"}}}