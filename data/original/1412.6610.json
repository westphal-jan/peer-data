{"id": "1412.6610", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Scoring and Classifying with Gated Auto-encoders", "abstract": "Auto-encoders are perhaps the best-known non-probabilistic methods for representation learning. They are conceptually simple and easy to train. Recent theoretical work has shed light on their ability to capture manifold structure, and drawn connections to density modelling. This has motivated researchers to seek ways of auto-encoder scoring, which has furthered their use in classification. Gated auto-encoders (GAEs) are an interesting and flexible extension of auto-encoders which can learn transformations among different images or pixel covariances within images. However, they have been much less studied, theoretically or empirically. In this work, we apply a dynamical systems view to GAEs, deriving a scoring function, and drawing connections to RBMs. On a set of deep learning benchmarks, we also demonstrate their effectiveness for single and multi-label classification.", "histories": [["v1", "Sat, 20 Dec 2014 05:46:05 GMT  (843kb,D)", "https://arxiv.org/abs/1412.6610v1", "Eight pages plus one page reference and four pages of appendix. Submit to the ICLR2015 conference track for review"], ["v2", "Thu, 26 Feb 2015 18:05:21 GMT  (843kb,D)", "http://arxiv.org/abs/1412.6610v2", "Eight pages plus one page reference and four pages of appendix. Submit to the ICLR2015 conference track for review"], ["v3", "Mon, 2 Mar 2015 16:35:39 GMT  (843kb,D)", "http://arxiv.org/abs/1412.6610v3", "Eight pages plus one page reference and four pages of appendix. Submit to the ICLR2015 conference track for review"], ["v4", "Thu, 2 Apr 2015 18:15:16 GMT  (849kb,D)", "http://arxiv.org/abs/1412.6610v4", null], ["v5", "Mon, 15 Jun 2015 00:25:47 GMT  (849kb,D)", "http://arxiv.org/abs/1412.6610v5", null]], "COMMENTS": "Eight pages plus one page reference and four pages of appendix. Submit to the ICLR2015 conference track for review", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["daniel jiwoong im", "graham w taylor"], "accepted": false, "id": "1412.6610"}, "pdf": {"name": "1412.6610.pdf", "metadata": {"source": "CRF", "title": "Scoring and Classifying with Gated Auto-encoders", "authors": ["Daniel Jiwoong Im", "Graham W. Taylor"], "emails": ["imj@uoguelph.ca", "gwtaylor@uoguelph.ca"], "sections": [{"heading": "1 Introduction", "text": "Representation learning algorithms are machine learning algorithms which involve the learning of features or explanatory factors. Deep learning techniques, which employ several layers of representation learning, have achieved much recent success in machine learning benchmarks and competitions, however, most of these successes have been achieved with purely supervised learning methods and have relied on large amounts of labeled data [10,22]. Though progress has been slower, it is likely that unsupervised learning will be important to future advances in deep learning [1].\nThe most successful and well-known example of non-probabilistic unsupervised learning is the auto-encoder. Conceptually simple and easy to train via backpropagation, various regularized variants of the model have recently been proposed [20,25,21] as well as theoretical insights into their operation [6,24].\nIn practice, the latent representation learned by auto-encoders has typically been used to solve a secondary problem, often classification. The most common setup is to train a single auto-encoder on data from all classes and then a classifier is tasked to discriminate among classes. However, this contrasts with the way probabilistic models have typically been used in the past: in that literature, it is more common to train one model per class and use Bayes\u2019 rule for classification.\nar X\niv :1\n41 2.\n66 10\nv5 [\ncs .L\nG ]\n1 5\nJu n\n2 There are two challenges to classifying using per-class auto-encoders. First, up until very recently, it was not known how to obtain the score of data under an auto-encoder, meaning how much the model \u201clikes\u201d an input. Second, autoencoders are non-probabilistic, so even if they can be scored, the scores do not integrate to 1 and therefore the per-class models need to be calibrated.\nKamyshanska and Memisevic have recently shown how scores can be computed from an auto-encoder by interpreting it as a dynamical system [7]. Although the scores do not integrate to 1, they show how one can combine the unnormalized scores into a generative classifier by learning class-specific normalizing constants from labeled data.\nIn this paper we turn our interest towards a variant of auto-encoders which are capable of learning higher-order features from data [15]. The main idea is to learn relations between pixel intensities rather than the pixel intensities themselves by structuring the model as a tri-partite graph which connects hidden units to pairs of images. If the images are different, the hidden units learn how the images transform. If the images are the same, the hidden units encode withinimage pixel covariances. Learning such higher-order features can yield improved results on recognition and generative tasks.\nWe adopt a dynamical systems view of gated auto-encoders, demonstrating that they can be scored similarly to the classical auto-encoder. We adopt the framework of [7] both conceptually and formally in developing a theory which yields insights into the operation of gated auto-encoders. In addition to the theory, we show in our experiments that a classification model based on gated auto-encoder scoring can outperform a number of other representation learning architectures, including classical auto-encoder scoring. We also demonstrate that scoring can be useful for the structured output task of multi-label classification."}, {"heading": "2 Gated Auto-encoders", "text": "In this section, we review the gated auto-encoder (GAE). Due to space constraints, we will not review the classical auto-encoder. Instead, we direct the reader to the reviews in [15,8] with which we share notation. Similar to the classical auto-encoder, the GAE consists of an encoder h(\u00b7) and decoder r(\u00b7). While the standard auto-encoder processes a datapoint x, the GAE processes inputoutput pairs (x,y). The GAE is usually trained to reconstruct y given x, though it can also be trained symmetrically, that is, to reconstruct both y from x and x from y. Intuitively, the GAE learns relations between the inputs, rather than representations of the inputs themselves1. If x 6= y, for example, they represent sequential frames of a video, intuitively, the mapping units h learn transformations. In the case that x = y (i.e. the input is copied), the mapping units learn pixel covariances.\nIn the simplest form of the GAE, the M hidden (mapping) units are given by a basis expansion of x and y. However, this leads to a parameterization\n1 Relational features can be mixed with standard features by simply adding connections that are not gated.\n3 that it is at least quadratic in the number of inputs and thus, prohibitively large. Therefore, in practice, x, y, and h are projected onto matrices or (\u201clatent factors\u201d), WX , WY , and WH , respectively. The number of factors, F , must be the same for X, Y , and H. Thus, the model is completely parameterized by \u03b8 = {WX ,WY ,WH} such that WX and WY are F \u00d7 D matrices (assuming both x and y are D-dimensional) and WH is an M \u00d7 F matrix. The encoder function is defined by\nh(x,y) = \u03c3(WH((WXx) (WY y))) (1)\nwhere is element-wise multiplication and \u03c3(\u00b7) is an activation function. The decoder function is defined by\nr(y|x, h) = (WY )T ((WXx) (WH)Th(x,y)). (2) r(x|y, h) = (WX)T ((WY y) (WH)Th(x,y)), (3)\nNote that the parameters are usually shared between the encoder and decoder. The choice of whether to apply a nonlinearity to the output, and the specific form of objective function will depend on the nature of the inputs, for example, binary, categorical, or real-valued. Here, we have assumed real-valued inputs for simplicity of presentation, therefore, Eqs. 2 and 3 are bi-linear functions of h and we use a squared-error objective:\nJ = 1\n2 \u2016r(y|x)\u2212 y\u20162. (4)\nWe can also constrain the GAE to be a symmetric model by training it to reconstruct both x given y and y given x [15]:\nJ = 1 2 \u2016r(y|x)\u2212 y\u20162 + 1 2 \u2016r(x|y)\u2212 x\u20162. (5)\nThe symmetric objective can be thought of as the non-probabilistic analogue of modeling a joint distribution over x and y as opposed to a conditional [15]."}, {"heading": "3 Gated Auto-Encoder Scoring", "text": "In [7], the authors showed that data could be scored under an auto-encoder by interpreting the model as a dynamical system. In contrast to the probabilistic views based on score matching [21,24,6] and regularization, the dynamical systems approach permits scoring under models with either linear (real-valued data) or sigmoid (binary data) outputs, as well as arbitrary hidden unit activation functions. The method is also agnostic to the learning procedure used to train the model, meaning that it is suitable for the various types of regularized auto-encoders which have been proposed recently. In this section, we demonstrate how the dynamical systems view can be extended to the GAE.\n4"}, {"heading": "3.1 Vector field representation", "text": "Similar to [7], we will view the GAE as a dynamical system with the vector field defined by\nF (y|x) = r(y|x)\u2212 y.\nThe vector field represents the local transformation that y|x undergoes as a result of applying the reconstruction function r(y|x). Repeatedly applying the reconstruction function to an input y|x \u2192 r(y|x) \u2192 r(r(y|x)|x) \u2192 \u00b7 \u00b7 \u00b7 \u2192 r(r \u00b7 \u00b7 \u00b7 r(y|x)|x) yields a trajectory whose dynamics, from a physics perspective, can be viewed as a force field. At any point, the potential force acting on a point is the gradient of some potential energy (negative goodness) at that point. In this light, the GAE reconstruction may be viewed as pushing pairs of inputs x,y in the direction of lower energy.\nOur goal is to derive the energy function, which we call a scoring function, and which measures how much a GAE \u201clikes\u201d a given pair of inputs (x,y) up to normalizing constant. In order to find an expression for the potential energy, the vector field must be able to be written as the derivative of a scalar field [7]. To check this, we can submit to Poincare\u0301\u2019s integrability criterion: For some open, simple connected set U , a continuously differentiable function F : U \u2192 <m defines a gradient field if and only if\n\u2202Fi(y) \u2202yj = \u2202Fj(y) \u2202yi , \u2200i, j = 1 \u00b7 \u00b7 \u00b7n.\nThe vector field defined by the GAE indeed satisfies Poincare\u0301\u2019s integrability criterion; therefore it can be written as the derivative of a scalar field. A derivation is given in the Appendix A.2 This also applies to the GAE with a symmetric objective function (Eq. 5) by setting the input as \u03be|\u03b3 such that \u03be = [y; x] and \u03b3 = [x; y] and following the exact same procedure."}, {"heading": "3.2 Scoring the GAE", "text": "As mentioned in Section 3.1, our goal is to find an energy surface, so that we can express the energy for a specific pair (x,y). From the previous section, we showed that Poincare\u0301\u2019s criterion is satisfied and this implies that we can write the vector field as the derivative of a scalar field. Moreover, it illustrates that this vector field is a conservative field and this means that the vector field is a gradient of some scalar function, which in this case is the energy function of a GAE:\nr(y|x)\u2212 y = \u2207E.\nHence, by integrating out the trajectory of the GAE (x,y), we can measure the energy along a path. Moreover, the line integral of a conservative vector field\n5 is path independent, which allows us to take the anti-derivative of the scalar function:\nE(y|x) = \u222b (r(y|x)\u2212 y)dy = \u222b WY (( WXx) WHh(u )) dy \u2212 \u222b ydy\n=WY (( WXx ) WH \u222b h (u) dy ) \u2212 \u222b ydy, (6)\nwhere u is an auxiliary variable such that u = WH((WY y) (WXx)) and du dy = W\nH(WY (WXx\u2297 1D)), and \u2297 is the Kronecker product. Moreover, the decoder can be re-formulated as\nr(y|x) = (WY )T (WXx (WH)Th(y,x)) = ( (WY )T (WXx\u2297 1D) ) (WH)Th(y,x).\nRe-writing Eq. 6 in terms of the auxiliary variable u, we get E(y|x) = ( (WY )T (WY x\u2297 1D) ) (WH)T (7)\u222b\nh(u) ( WH ( WY (WXx\u2297 1D) ))\u22121 du\u2212 \u222b ydy\n= \u222b h(u)du\u2212 1\n2 y2 + const. (8)\nA more detailed derivation from Eq. 6 to Eq. 8 is provided in the Appendix A.2. Identical to [7], if h(u) is an element-wise activation function and we know its anti-derivative, then it is very simple to compute E(x,y)."}, {"heading": "4 Relationship to Restricted Boltzmann Machines", "text": "In this section, we relate GAEs through the scoring function to other types of Restricted Boltzmann Machines, such as the Factored Gated Conditional RBM [23] and the Mean-covariance RBM [19]."}, {"heading": "4.1 Gated Auto-encoder and Factored Gated Conditional Restricted Boltzmann Machines", "text": "Kamyshanska and Memisevic showed that several hidden activation functions defined gradient fields, including sigmoid, softmax, tanh, linear, rectified linear function (ReLU), modulus, and squaring. These activation functions are applicable to GAEs as well.\nIn the case of the sigmoid activation function, \u03c3 = h(u) = 11+exp (\u2212u) , our\nenergy function becomes\nE\u03c3 =2\n\u222b (1 + exp\u2212(u))\u22121du\u2212 1\n2 (x2 + y2) + const,\n=2 \u2211 k log (1 + exp (WHk\u00b7 (W Xx WXy)))\u2212 1 2 (x2 + y2) + const.\n6 Note that if we consider the conditional GAE we reconstruct x given y only, this yields\nE\u03c3(y|x) = \u2211 k log (1 + exp (WH(WYk\u00b7y WXk\u00b7 x)))\u2212 y2 2 + const. (9)\nThis expression is identical, up to a constant, to the free energy in a Factored Gated Conditional Restricted Boltzmann Machine (FCRBM) with Gaussian visible units and Bernoulli hidden units. We have ignored biases for simplicity. A derivation including biases is provided in the Appendix B.1."}, {"heading": "4.2 Mean-Covariance Auto-encoder and Mean-covariance Restricted Boltzmann Machines", "text": "The Covariance auto-encoder (cAE) was introduced in [15]. It is a specific form of symmetrically trained auto-encoder with identical inputs: x = y, and tied input weights: WX = WY . It maintains a set of relational mapping units to model covariance between pixels. One can introduce a separate set of mapping units connected pairwise to only one of the inputs which model the mean intensity. In this case, the model becomes a Mean-covariance auto-encoder (mcAE).\nTheorem 1. Consider a cAE with encoder and decoder:\nh(x) = h(WH((WXx)2) + b)\nr(x|h) = (WX)T (WXx (WH)Th(x)) + a,\nwhere \u03b8 = {WX ,WH ,a,b} are the parameters of the model, and h(z) = 11+exp (\u2212z) is a sigmoid. Moreover, consider a Covariance RBM [19] with Gaussian-distributed visibles and Bernoulli-distributed hiddens, with an energy function defined by\nEc(x,h) = (a\u2212 x)2 \u03c32 \u2212 \u2211 f Ph(Cx)2 \u2212 bh.\nThen the energy function of the cAE with dynamics r(x|y)\u2212 x is equivalent to the free energy of Covariance RBM up to a constant:\nE(x,x) = \u2211 k log ( 1 + exp ( WH(WXx)2 + b )) \u2212 x 2 2 + const. (10)\nThe proof is given in the Appendix B.2. We can extend this analysis to the mcAE by using the above theorem and the results from [7].\nCorollary 1. The energy function of a mcAE and the free energy of a Meancovariance RBM (mcRBM) with Gaussian-distributed visibles and Bernoullidistributed hiddens are equivalent up to a constant. The energy of the mcAE is:\nE = \u2211 k log ( 1 + exp ( \u2212WH(WXx)2 \u2212 b )) + \u2211 k log (1 + exp(Wx + c))\u2212x2+const\n(11)\n7 where \u03b8m = {W, c} parameterizes the mean mapping units and \u03b8c = {WX ,WH , a,b} parameterizes the covariance mapping units.\nProof. The proof is very simple. Let Emc = Em + Ec, where Em is the energy of the mean auto-encoder, Ec is the energy of the covariance auto-encoder, and Emc is the energy of the mcAE. We know from Theorem 1 that Ec is equivalent to the free energy of a covariance RBM, and the results from [7] show that that Em is equivalent to the free energy of mean (classical) RBM. As shown in [19], the free energy of a mcRBM is equal to summing the free energies of a mean RBM and a covariance RBM."}, {"heading": "5 Classification with Gated Auto-encoders", "text": "Kamyshanska and Memisevic demonstrated that one application of the ability to assign energy or scores to auto-encoders was in constructing a classifier from class-specific auto-encoders. In this section, we explore two different paradigms for classification. Similar to that work, we consider the usual multi-class problem by first training class-specific auto-encoders, and using their energy functions as confidence scores. We also consider the more challenging structured output problem, specifically, the case of multi-label prediction where a data point may have more than one associated label, and there may be correlations among the labels."}, {"heading": "5.1 Classification using class-specific gated auto-encoders", "text": "One approach to classification is to take several class-specific models and assemble them into a classifier. The best-known example of this approach is to fit several directed graphical models and use Bayes\u2019 rule to combine them. The process is simple because the models are normalized, or calibrated. While it is possible to apply a similar technique to undirected or non-normalized models such as auto-encoders, one must take care to calibrate them.\nThe approach proposed in [7] is to train K class-specific auto-encoders, each of which assigns a non-normalized energy to the data Ei (x) , i = 1 . . . ,K, and then define the conditional distribution over classes zi as\nP (zi|x) = exp (Ei (x) +Bi)\u2211 j exp (Ej (x) +Bj) , (12)\nwhere Bi is a learned bias for class i. The bias terms take the role of calibrating the unnormalized energies. Note that we can similarly combine the energies from a symmetric gated auto-encoder where x = y (i.e. a covariance auto-encoder) and apply Eq. 12. If, for each class, we train both a covariance auto-encoder and a classical auto-encoder (i.e. a \u201cmean\u201d auto-encoder) then we can combine both sets of unnormalized energies as follows\nPmcAE(zi|x) = exp(EMi (x) + E C i (x) +Bi)\u2211\nj exp(E M j (x) + E C j (x) +Bj)\n, (13)\n8 where EMi (x) is the energy which comes from the \u201cmean\u201d (standard) autoencoder trained on class i and ECi (x) the energy which comes from the \u201ccovariance\u201d (gated) auto-encoder trained on class i. We call the classifiers in Eq. 12 and Eq. 13 \u201cCovariance Auto-encoder Scoring\u201d (cAES) and \u201cMean-Covariance Auto-encoder Scoring\u201d (mcAES), respectively.\nThe training procedure is summarized as follows:\n1. Train a (mean)-covariance auto-encoder individually for each class. Both the mean and covariance auto-encoder have tied weights in the encoder and decoder. The covariance auto-encoder is a gated auto-encoder with tied inputs. 2. Learn the Bi calibration terms using maximum likelihood, and backpropagate to the GAE parameters.\nExperimental results We followed the same experimental setup as [16] where we used a standard set of \u201cDeep Learning Benchmarks\u201d [11]. We used mini-batch stochastic gradient descent to optimize parameters during training. The hyperparameters: number of hiddens, number of factors, corruption level, learning rate, weight-decay, momentum rate, and batch sizes were chosen based on a held-out validation set. Corruption levels and weight-decay were selected from {0, 0.1, 0.2, 0.3, 0.4, 0.5}, and number of hidden and factors were selected from 100,300,500. We selected the learning rate and weight-decay from the range (0.001, 0.0001).\nClassification error results are shown in Table 1. First, the error rates of auto-encoder scoring variant methods illustrate that across all datasets AES outperforms cAES and mcAES outperforms both AES and cAES. AE models pixel means and cAE models pixel covariance, while mcAE models both mean and covariance, making it naturally more expressive. We observe that cAES and mcAES achieve lower error rates by a large margin on rotated MNIST with backgrounds (final row). On the other hand, both cAES and mcAES perform poorly on MNIST with random white noise background (second row from bottom). We believe this phenomenon is due to the inability to model covariance in this dataset. In MNIST with random white noise the pixels are typically uncorrelated, where in rotated MNIST with backgrounds the correlations are present and consistent."}, {"heading": "5.2 Multi-label classification via optimization in label space", "text": "The dominant application of deep learning approaches to vision has been the assignment of images to discrete classes (e.g. object recognition). Many applications, however, involve \u201cstructured outputs\u201d where the output variable is highdimensional and has a complex, multi-modal joint distribution. Structured output prediction may include tasks such as multi-label classification where there are regularities to be learned in the output, and segmentation, where the output is as high-dimensional as the input. A key challenge to such approaches lies in developing models that are able to capture complex, high level structure like shape, while still remaining tractable.\nThough our proposed work is based on a deterministic model, we have shown that the energy, or scoring function of the GAE is equivalent, up to a constant, to that of a conditional RBM, a model that has already seen some use in structured prediction problems [18,12].\nGAE scoring can be applied to structured output problems as a type of \u201cpost-classification\u201d [17]. The idea is to let a naiv\u0308e, non-structured classifier make an initial prediction of the outputs in a fast, feed-forward manner, and then allow a second model (in our case, a GAE) clean up the outputs of the first model. Since GAEs can model the relationship between input x and structured output y, we can initialize the output with the output of the naiv\u0308e model, and then optimize its energy function with respect to the outputs. Input x is held constant throughout the optimization.\nLi et al recently proposed Compositional High Order Pattern Potentials, a hybrid of Conditional Random Fields (CRF) and Restricted Boltzmann Machines. The RBM provides a global shape information prior to the locally-connected CRF. Adopting the idea of learning structured relationships between outputs, we propose an alternate approach which the inputs of the GAE are not (x,y) but (y,y). In other words, the post-classification model is a covariance autoencoder. The intuition behind the first approach is to use a GAE to learn the relationship between the input x and the output y, whereas the second method aims to learn the correlations between the outputs y.\nWe denote our two proposed methods GAEXY and GAEY 2 . GAEXY corresponds to a GAE, trained conditionally, whose mapping units directly model the relationship between input and output and GAEY 2 corresponds to a GAE which models correlations between output dimensions. GAEXY defines E (y|x), while GAEY 2 defines E (y|y) = E(y). They differ only in terms of the data vectors that they consume. The training and test procedures are detailed in Algorithm 1.\nExperimental results We consider multi-label classification, where the problem is to classify instances which can take on more than one label at a time. We\n10\nAlgorithm 1 Structured Output Prediction with GAE scoring\n1: procedure Multi-label Classification(D = {(xi,yi) \u2208 Xtrain \u00d7 Ytrain} ) 2: Train a Multi-layer Perceptron (MLP) to learn an input-output mapping f(\u00b7):\nargmin \u03b81 l(x,y; \u03b81) = \u2211 i loss1 ((f (xi; \u03b81)\u2212 yi) (14)\nwhere loss1 is an appropriate loss function for the MLP. 2\n3: Train a Gated Auto-encoder with inputs (xi,yi); For the case of GAEY 2 , set xi = yi.\nargmin \u03b82 l(x,y; \u03b82) = \u2211 i loss2 (r(yi|xi, \u03b82)\u2212 yi) (15)\nwhere loss2 is an appropriate reconstructive loss for the auto-encoder. 4: for each test data point xi \u2208 Xtest do 5: Initialize the output using the MLP.\ny0 = f (xtest) (16)\n6: while \u2016E(yt+1|x)\u2212 E(yt|x)\u2016 > or \u2264 max. iter. do 7: Compute OytE 8: Update yt+1 = yt \u2212 \u03bbOytE 9: where is the tolerance rate with respect to the convergence of the\noptimization.\nfollowed the same experimental set up as [18]. Four multi-labeled datasets were considered: Yeast [5] consists of biological attributes, Scene [2] is image-based, and MTurk [13] and MajMin [14] are targeted towards tagging music. Yeast consists of 103 biological attributes and has 14 possible labels, Scene consists of 294 image pixels with 6 possible labels, and MTurk and MajMin each consist of 389 audio features extracted from music and have 92 and 96 possible tags, respectively. Figure 1 visualizes the covariance matrix for the label dimensions in each dataset. We can see from this that there are correlations present in the labels which suggests that a structured approach may improve on a non-structured predictor.\n2 In our experiments, we used the cross-entropy loss function for loss1 and loss2.\n11\nWe compared our proposed approaches to logistic regression, a standard MLP, and the two structured CRBM training algorithms presented in [18]. To permit a fair comparison, we followed the same procedure for training and reporting errors as in that paper, where we cross validated over 10 folds and training, validation, test examples are randomly separated into 80%, 10%, and 10% in each fold. The error rate was measured by averaging the errors on each label dimension.\nThe performance on four multi-label datasets is shown in Table 2. We observed that adding a small amount of Gaussian noise to the input y improved the performance for GAEXY . However, adding noise to the input x did not have as much of an effect. We suspect that adding noise makes the GAE more robust to the input provided by the MLP. Interestingly, we found that the performance of GAEY 2 was negatively affected by adding noise. Both of our proposed methods, GAESXY and GAESY 2 generally outperformed the other methods except for GAESY 2 on the MajMin dataset. At least for these datasets, there is no clear winner between the two. GAESXY achieved lower error than GAESY 2 for Yeast and MajMin, and the same error rate on the MTurk dataset. However, GAESY 2 outperforms GAESXY on the Scene dataset. Overall, the results show that GAE scoring may be a promising means of post-classification in structured output prediction."}, {"heading": "6 Conclusion", "text": "There have been many theoretical and empirical studies on auto-encoders [25,20,21,24,6,7], however, the theoretical study of gated auto-encoders is limited apart from [15,4]. The GAE has several intriguing properties that a classical auto-encoder does not, based on its ability to model relations among pixel intensities rather than just the intensities themselves. This opens up a broader set of applications. In this paper, we derive some theoretical results for the GAE that enable us to gain more insight and understanding of its operation.\nWe cast the GAE as a dynamical system driven by a vector field in order to analyze the model. In the first part of the paper, by following the same procedure as [7], we showed that the GAE could be scored according to an energy function.\n12\nFrom this perspective, we demonstrated the equivalency of the GAE energy to the free energy of a FCRBM with Gaussian visible units, Bernoulli hidden units, and sigmoid hidden activations. In the same manner, we also showed that the covariance auto-encoder can be formulated in a way such that its energy function is the same as the free energy of a covariance RBM, and this naturally led to a connection between the mean-covariance auto-encoder and mean-covariance RBM. One interesting observation is that Gaussian-Bernoulli RBMs have been reported to be difficult to train [9,3], and the success of training RBMs is highly dependent on the training setup [26]. Auto-encoders are an attractive alternative, even when an energy function is required.\nStructured output prediction is a natural next step for representation learning. The main advantage of our approach compared to other popular approaches such as Markov Random Fields, is that inference is extremely fast, using a gradient-based optimization of the auto-encoder scoring function. In the future, we plan on tackling more challenging structured output prediction problems."}, {"heading": "A Gated Auto-encoder Scoring", "text": "A.1 Vector field representation\nTo check that the vector field can be written as the derivative of a scalar field, we can submit to Poincare\u0301\u2019s integrability criterion: For some open, simple connected set U , a continuously differentiable function F : U \u2192 <m defines a gradient field if and only if\n\u2202Fi(y) \u2202yj = \u2202Fj(y) \u2202yi , \u2200i, j = 1 \u00b7 \u00b7 \u00b7n.\nConsidering the GAE, note that ith component of the decoder ri(y|x) can be rewritten as\nri(y|x) = (WY\u00b7i )T (WXx (WH)Th(y,x)) = (WY\u00b7i WXx)T (WH)Th(y,x).\n14\nThe derivatives of ri(y|x)\u2212 yi with respect to yj are\n\u2202ri(y|x) \u2202yj =(WY\u00b7i WXx)T (WH)T \u2202h(x,y) \u2202yj = \u2202rj(y|x) \u2202yi\n\u2202h(y,x) \u2202yj = \u2202h(u) \u2202u WH(WY\u00b7j WXx) (17)\nwhere u = WH((WY y) (WXx)). By substituting Equation 17 into \u2202Fi\u2202yj , \u2202Fj \u2202yi , we have\n\u2202Fi \u2202yj = \u2202ri(y|x) \u2202yj \u2212\u03b4ij= \u2202rj(y|x) \u2202yi \u2212\u03b4ij= \u2202Fj \u2202yi\nwhere \u03b4ij = 1 for i = j and 0 for i 6= j. Similarly, the derivatives of ri(y|x)\u2212 yi with respect to xj are\n\u2202ri(y|x) \u2202xj =(WY\u00b7i WX\u00b7j )T (WH)Th(x,y) + (WY\u00b7i WXx)(WH)T \u2202h \u2202xj = \u2202rj(y|x) \u2202xi ,\n\u2202h(y,x) \u2202xj = \u2202h(u) \u2202u WH(WY\u00b7j WXx). (18)\nBy substituting Equation 18 into \u2202Fi\u2202xj , \u2202Fj \u2202xi , this yields\n\u2202Fi \u2202xj = \u2202ri(x|y) \u2202xj = \u2202rj(x|y) \u2202xi = \u2202Fj \u2202xi .\nA.2 Deriving an Energy Function\nIntegrating out the GAE\u2019s trajectory, we have E(y|x) = \u222b C (r(y|x)\u2212 y)dy\n= \u222b WY (( WXx) WHh(u )) dy \u2212 \u222b ydy\n=WY (( WXx ) WH \u222b h (u) du ) \u2212 \u222b ydy, (19)\nwhere u is an auxiliary variable such that u = WH((WY y) (WXx)) and du dy = W\nH(WY (WXx \u2297 1D)), where \u2297 is the Kronecker product. Consider the symmetric objective function, which is defined in Equation 5. Then we have to also consider the vector field system where both symmetric cases x|y and y|x are valid. As mentioned in Section 3.1, let \u03be = [x; y] and \u03b3 = [y; x]. As well, let W \u03be = diag(WX ,WY ) and W \u03b3 = diag(WY ,WX) where they are block diagonal matrices. Consequently, the vector field becomes\nF (\u03be|\u03b3) = r(\u03be|\u03b3)\u2212 \u03be, (20)\n15\nand the energy function becomes E(\u03be|\u03b3) = \u222b (r(\u03be|\u03b3)\u2212 \u03be)d\u03be\n= \u222b (W \u03be)T ((W \u03b3\u03b3) (WH)Th(u))d\u03be \u2212 \u222b \u03bed\u03be\n=(W \u03be)T ((W \u03b3\u03b3) (WH)T \u222b h(u)du)\u2212 \u222b \u03bed\u03be\nwhere u is an auxiliary variable such that u = WH ( (W \u03be\u03be) (W \u03b3\u03b3) ) . Then\ndu d\u03be = WH\n( W \u03be (W \u03b3\u03b3 \u2297 1D) ) .\nMoreover, note that the decoder can be re-formulated as\nr(\u03be|\u03b3) = (W \u03be)T (W \u03b3\u03b3 (WH)Th(\u03be,\u03b3)) = ( (W \u03be)T (W \u03b3\u03b3 \u2297 1D) ) (WH)Th(\u03be,\u03b3).\nRe-writing the first term of Equation 19 in terms of the auxiliary variable u, the energy reduces to E(\u03be|\u03b3) = ( (W \u03be)T (W \u03b3\u03b3 \u2297 1D) ) (WH)T \u222b h(u) ( WH(W \u03be (W \u03b3\u03b3 \u2297 1D)) )\u22121 du\u2212 \u222b \u03bed\u03be\n= ( (W \u03be)T (W \u03b3\u03b3 \u2297 1D) ) (WH)T ( (W \u03be (W \u03b3\u03b3 \u2297 1D))WH )\u2212T \u222b h(u)du\u2212 \u222b \u03bed\u03be\n= \u222b h(u)du\u2212 \u222b \u03bed\u03be\n= \u222b h(u)du\u2212 1\n2 \u03be2 + const."}, {"heading": "B Relation to other types of Restricted Boltzmann Machines", "text": "B.1 Gated Auto-encoder and Factored Gated Conditional Restricted Boltzmann Machines\nSuppose that the hidden activation function is a sigmoid. Moreover, we define our Gated Auto-encoder to consists of an encoder h(\u00b7) and decoder r(\u00b7) such that\nh(x,y) = h(WH((WXx) (WY y)) + b) r(x|y, h) = (WX)T ((WY y) (WH)Th(x,y)) + a,\nwhere \u03b8 = {WH ,WX ,WY ,b} is the parameters of the model. Note that the weights are not tied in this case. The energy function for the Gated Auto-encoder\n16\nwill be:\nE\u03c3(x|y) = \u222b (1 + exp (\u2212WH(WXx) (WY y)\u2212 b))\u22121du\u2212 x 2\n2 + ax + const\n= \u2211 k log (1 + exp (\u2212WHk\u00b7 (WXx) (WY y)\u2212 bk))\u2212 x2 2 + ax + const.\nNow consider the free energy of a Factored Gated Conditional Restricted Boltzmann Machine (FCRBM).\nThe energy function of a FCRBM with Gaussian visible units and Bernoulli hidden units is defined by\nE(x,h|y) = (a\u2212 x) 2\n2\u03c32 \u2212 bh\u2212 \u2211 f WXf \u00b7 x WYf \u00b7y WHf \u00b7h.\nGiven y, the conditional probability density assigned by the FCRBM to data point x is\np(x|y) = \u2211\nh exp\u2212(E(x,h|y)) Z(y) = exp (\u2212F (x|y)) Z(y)\n\u2212F (x|y) = log (\u2211 h exp (\u2212E(x,h|y)) )\nwhere Z(y) = \u2211\nx,h exp (E(x,h|y)) is the partition function and F (x|y) is the free energy function. Expanding the free energy function, we get \u2212F (x|y) = log \u2211 h exp (\u2212E(x,h|y))\n= log \u2211 h exp \u2212(a\u2212 x)2 2\u03c32 + bh + \u2211 f WXf \u00b7 x WYf \u00b7y WHf \u00b7h  =\u2212 (a\u2212 x) 2\n2\u03c32 + log \u2211 h exp bh +\u2211 f WXf \u00b7 x WYf \u00b7y WHf \u00b7h  =\u2212 (a\u2212 x) 2\n2\u03c32 + log \u2211 h \u220f k exp bkhk +\u2211 f (WXf \u00b7 x WYf \u00b7y) WHfkhk  =\u2212 (a\u2212 x) 2\n2\u03c32 + \u2211 k log\n1 + exp bk +\u2211\nf\n( (WHfk) T (WXx WY y) ) .\n17\nNote that we can center the data by subtracting mean of x and dividing by its standard deviation, and therefore assume that \u03c32 = 1. Substituting, we have\n\u2212F (x|y) =\u2212 (a\u2212 x) 2 2 + \u2211 k log\n1 + exp \u2212bk \u2212\u2211\nf\n(WHfk) T (WXx WY y)  = \u2211 k log 1 + exp bk +\u2211 f (WHfk) T (WXx WY y) \u2212 a2 + ax\u2212 x2 2 = \u2211 k log 1 + exp bk +\u2211 f (WHfk) T (WXx WY y) + ax\u2212 x2 2 + const\nLetting WH = (WH)T , we get\n= \u2211 k log\n1 + exp bk +\u2211\nf\nWHkf (W Xx WY y) + ax\u2212 x2 2 + const\nHence, the Conditional Gated Auto-encoder and the FCRBM are equal up to a constant.\nB.2 Gated Auto-encoder and mean-covariance Restricted Boltzmann Machines\nTheorem 2. Consider a covariance auto-encoder with an encoder and decoder,\nh(x,x) = h(WH((WFx)2) + b)\nr(x|y = x, h) = (WF )T (WFy (WH)Th(x,y)) + a,\nwhere \u03b8 = {WF ,WH ,a,b} are the parameters of the model. Moreover, consider a covariance Restricted Boltzmann Machine with Gaussian distribution over the visibles and Bernoulli distribution over the hiddens, such that its energy function is defined by\nEc(x,h) = (a\u2212 x)2 \u03c32 \u2212 \u2211 f Ph(Cx)2 \u2212 bh,\nwhere \u03b8 = {P,C,a,b} are its parameters. Then the energy function for a covariance Auto-encoder with dynamics r(x|y)\u2212x is equivalent to the free energy of a covariance Restricted Boltzmann Machine. The energy function of the covariance Auto-encoder is\nE(x,x) = \u2211 k log(1 + exp(WH(WFx)2 + b))\u2212 x2 + const (21)\n18\nProof. Note that the covariance auto-encoder is the same as a regular Gated Auto-encoder, but setting y = x and making the factor loading matrices the same, i.e. WF = WY = WX . Then applying the general energy equation for GAE, Equation 8, to the covariance auto-encoder, we get\nE(x,x) = \u222b h(u)du\u2212 1\n2 x2 + const = \u2211 k log(1 + exp(WH(WFx)2 + b))\u2212 x2 + ax + const, (22)\nwhere u = WH(WFx)2 + b. Now consider the free energy of the mean-covariance Restricted Boltzmann Machine (mcRBM) with Gaussian distribution over the visible units and Bernoulli distribution over the hidden units:\n\u2212F (x|y) = log \u2211 h exp (\u2212E(x,h|y))\n= log \u2211 h exp ( \u2212 (a\u2212 x) 2 \u03c32 + (Ph)(Cx)2 + bh ) = log \u2211 h \u220f k exp \u2212 (a\u2212 x)2 \u03c32 + \u2211 f (Pfkhk)(Cx) 2 + bkhk\n = \u2211 k log 1 + exp \u2211 f (Pfkhk)(Cx) 2 \u2212 (a\u2212 x)2 \u03c32 .\nAs before, we can center the data by subtracting mean of x and dividing by its standard deviation, and therefore assume that \u03c32 = 1. Substituting, we have\n= \u2211 k log\n1 + exp \u2211\nf\n(Pfkhk)(Cx) 2 \u2212 (a\u2212 x)2. (23) Letting WH = PT and WF = C, we get\n= \u2211 k log\n1 + exp \u2211\nf\n(Pfkhk)(Cx) 2 \u2212 x2 + ax + const. (24) Therefore, the two equations are equivalent."}], "references": [{"title": "Deep generative stochastic networks trainable by backprop", "author": ["Y. Bengio", "\u00c9. Thibodeau-Laufer"], "venue": "arXiv preprint arXiv:1306.1091", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning multi-label scene classification", "author": ["M.R. Boutell", "J. Luob", "X. Shen", "C.M. Brown"], "venue": "Pattern Recognition 37, 1757\u20131771", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Improved learning of Gaussian-Bernoulli restricted Boltzmann machines", "author": ["K. Cho", "A. Ilin", "T. Raiko"], "venue": "ICANN. pp. 10\u201317", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Gated autoencoders with tied input weights", "author": ["A. Droniou", "O. Sigaud"], "venue": "ICML", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "A kernel method for multi-labelled classification", "author": ["A. Elisseeff", "J. Weston"], "venue": "NIPS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "What regularized auto-encoders learn from the data generating distribution", "author": ["A. Guillaume", "Y. Bengio"], "venue": "ICLR", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "On autoencoder scoring", "author": ["H. Kamyshanska", "R. Memisevic"], "venue": "ICML. pp. 720\u2013728", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "The potential energy of an auto-encoder", "author": ["H. Kamyshanska", "R. Memisevic"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 37(6), 1261\u20131273", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Tech. rep., Department of Computer Science, University of Toronto", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "ICML", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Exploring compositional high order pattern potentials for structured output learning", "author": ["Y. Li", "D. Tarlow", "R. Zemel"], "venue": "CVPR", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning tags that vary within a song", "author": ["M.I. Mandel", "D. Eck", "Y. Bengio"], "venue": "ISMIR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "A web-based game for collecting music metadata", "author": ["M.I. Mandel", "D.P.W. Ellis"], "venue": "Journal New of Music Research 37, 151\u2013165", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Gradient-based learning of higher-order image features", "author": ["R. Memisevic"], "venue": "ICCV", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Gated softmax classification", "author": ["R. Memisevic", "C. Zach", "G. Hinton", "M. Pollefeys"], "venue": "NIPS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning to detect roads in high-resolution aerial images", "author": ["V. Mnih", "G. Hinton"], "venue": "Proceedings of the 11th European Conference on Computer Vision (ECCV)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Conditional restricted Boltzmann machines for structured output prediction", "author": ["V. Mnih", "H. Larochelle", "G.E. Hinton"], "venue": "UAI", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Modeling pixel means and covariances using factorized third-order Boltzmann machines", "author": ["M. Ranzato", "G.E. Hinton"], "venue": "CVPR", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai"], "venue": "ICML", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "On autoencoders and score matching for energy based models", "author": ["K. Swersky", "M. Ranzato", "D. Buchman", "N.D. Freitas", "B.M. Marlin"], "venue": "ICML. pp. 1201\u20131208", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Factored conditional restricted Boltzmann machines for modeling motion style", "author": ["G.W. Taylor", "G.E. Hinton"], "venue": "ICML. pp. 1025\u20131032", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "A connection between score matching and denoising auto-encoders", "author": ["P. Vincent"], "venue": "Neural Computation 23(7), 1661\u20131674", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P. Manzagol"], "venue": "ICML", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Gaussian-binary restricted Boltzmann machines on modeling natural image statistics", "author": ["N. Wang", "J. Melchior", "L. Wiskott"], "venue": "Tech. rep., Institut fur Neuroinformatik Ruhr-Universitat Bochum, Bochum, 44780, Germany", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "Deep learning techniques, which employ several layers of representation learning, have achieved much recent success in machine learning benchmarks and competitions, however, most of these successes have been achieved with purely supervised learning methods and have relied on large amounts of labeled data [10,22].", "startOffset": 306, "endOffset": 313}, {"referenceID": 21, "context": "Deep learning techniques, which employ several layers of representation learning, have achieved much recent success in machine learning benchmarks and competitions, however, most of these successes have been achieved with purely supervised learning methods and have relied on large amounts of labeled data [10,22].", "startOffset": 306, "endOffset": 313}, {"referenceID": 0, "context": "Though progress has been slower, it is likely that unsupervised learning will be important to future advances in deep learning [1].", "startOffset": 127, "endOffset": 130}, {"referenceID": 19, "context": "Conceptually simple and easy to train via backpropagation, various regularized variants of the model have recently been proposed [20,25,21] as well as theoretical insights into their operation [6,24].", "startOffset": 129, "endOffset": 139}, {"referenceID": 24, "context": "Conceptually simple and easy to train via backpropagation, various regularized variants of the model have recently been proposed [20,25,21] as well as theoretical insights into their operation [6,24].", "startOffset": 129, "endOffset": 139}, {"referenceID": 20, "context": "Conceptually simple and easy to train via backpropagation, various regularized variants of the model have recently been proposed [20,25,21] as well as theoretical insights into their operation [6,24].", "startOffset": 129, "endOffset": 139}, {"referenceID": 5, "context": "Conceptually simple and easy to train via backpropagation, various regularized variants of the model have recently been proposed [20,25,21] as well as theoretical insights into their operation [6,24].", "startOffset": 193, "endOffset": 199}, {"referenceID": 23, "context": "Conceptually simple and easy to train via backpropagation, various regularized variants of the model have recently been proposed [20,25,21] as well as theoretical insights into their operation [6,24].", "startOffset": 193, "endOffset": 199}, {"referenceID": 6, "context": "Kamyshanska and Memisevic have recently shown how scores can be computed from an auto-encoder by interpreting it as a dynamical system [7].", "startOffset": 135, "endOffset": 138}, {"referenceID": 14, "context": "In this paper we turn our interest towards a variant of auto-encoders which are capable of learning higher-order features from data [15].", "startOffset": 132, "endOffset": 136}, {"referenceID": 6, "context": "We adopt the framework of [7] both conceptually and formally in developing a theory which yields insights into the operation of gated auto-encoders.", "startOffset": 26, "endOffset": 29}, {"referenceID": 14, "context": "Instead, we direct the reader to the reviews in [15,8] with which we share notation.", "startOffset": 48, "endOffset": 54}, {"referenceID": 7, "context": "Instead, we direct the reader to the reviews in [15,8] with which we share notation.", "startOffset": 48, "endOffset": 54}, {"referenceID": 14, "context": "We can also constrain the GAE to be a symmetric model by training it to reconstruct both x given y and y given x [15]:", "startOffset": 113, "endOffset": 117}, {"referenceID": 14, "context": "The symmetric objective can be thought of as the non-probabilistic analogue of modeling a joint distribution over x and y as opposed to a conditional [15].", "startOffset": 150, "endOffset": 154}, {"referenceID": 6, "context": "In [7], the authors showed that data could be scored under an auto-encoder by interpreting the model as a dynamical system.", "startOffset": 3, "endOffset": 6}, {"referenceID": 20, "context": "In contrast to the probabilistic views based on score matching [21,24,6] and regularization, the dynamical systems approach permits scoring under models with either linear (real-valued data) or sigmoid (binary data) outputs, as well as arbitrary hidden unit activation functions.", "startOffset": 63, "endOffset": 72}, {"referenceID": 23, "context": "In contrast to the probabilistic views based on score matching [21,24,6] and regularization, the dynamical systems approach permits scoring under models with either linear (real-valued data) or sigmoid (binary data) outputs, as well as arbitrary hidden unit activation functions.", "startOffset": 63, "endOffset": 72}, {"referenceID": 5, "context": "In contrast to the probabilistic views based on score matching [21,24,6] and regularization, the dynamical systems approach permits scoring under models with either linear (real-valued data) or sigmoid (binary data) outputs, as well as arbitrary hidden unit activation functions.", "startOffset": 63, "endOffset": 72}, {"referenceID": 6, "context": "Similar to [7], we will view the GAE as a dynamical system with the vector field defined by F (y|x) = r(y|x)\u2212 y.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "In order to find an expression for the potential energy, the vector field must be able to be written as the derivative of a scalar field [7].", "startOffset": 137, "endOffset": 140}, {"referenceID": 6, "context": "Identical to [7], if h(u) is an element-wise activation function and we know its anti-derivative, then it is very simple to compute E(x,y).", "startOffset": 13, "endOffset": 16}, {"referenceID": 22, "context": "In this section, we relate GAEs through the scoring function to other types of Restricted Boltzmann Machines, such as the Factored Gated Conditional RBM [23] and the Mean-covariance RBM [19].", "startOffset": 153, "endOffset": 157}, {"referenceID": 18, "context": "In this section, we relate GAEs through the scoring function to other types of Restricted Boltzmann Machines, such as the Factored Gated Conditional RBM [23] and the Mean-covariance RBM [19].", "startOffset": 186, "endOffset": 190}, {"referenceID": 14, "context": "2 Mean-Covariance Auto-encoder and Mean-covariance Restricted Boltzmann Machines The Covariance auto-encoder (cAE) was introduced in [15].", "startOffset": 133, "endOffset": 137}, {"referenceID": 18, "context": "Moreover, consider a Covariance RBM [19] with Gaussian-distributed visibles and Bernoulli-distributed hiddens, with an energy function defined by", "startOffset": 36, "endOffset": 40}, {"referenceID": 6, "context": "We can extend this analysis to the mcAE by using the above theorem and the results from [7].", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": "We know from Theorem 1 that Ec is equivalent to the free energy of a covariance RBM, and the results from [7] show that that Em is equivalent to the free energy of mean (classical) RBM.", "startOffset": 106, "endOffset": 109}, {"referenceID": 18, "context": "As shown in [19], the free energy of a mcRBM is equal to summing the free energies of a mean RBM and a covariance RBM.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "The approach proposed in [7] is to train K class-specific auto-encoders, each of which assigns a non-normalized energy to the data Ei (x) , i = 1 .", "startOffset": 25, "endOffset": 28}, {"referenceID": 15, "context": "Experimental results We followed the same experimental setup as [16] where we used a standard set of \u201cDeep Learning Benchmarks\u201d [11].", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "Experimental results We followed the same experimental setup as [16] where we used a standard set of \u201cDeep Learning Benchmarks\u201d [11].", "startOffset": 128, "endOffset": 132}, {"referenceID": 23, "context": "SVM and RBM results are from [24], DEEP and GSM are results from [15], and AES is from [7].", "startOffset": 29, "endOffset": 33}, {"referenceID": 14, "context": "SVM and RBM results are from [24], DEEP and GSM are results from [15], and AES is from [7].", "startOffset": 65, "endOffset": 69}, {"referenceID": 6, "context": "SVM and RBM results are from [24], DEEP and GSM are results from [15], and AES is from [7].", "startOffset": 87, "endOffset": 90}, {"referenceID": 17, "context": "Though our proposed work is based on a deterministic model, we have shown that the energy, or scoring function of the GAE is equivalent, up to a constant, to that of a conditional RBM, a model that has already seen some use in structured prediction problems [18,12].", "startOffset": 258, "endOffset": 265}, {"referenceID": 11, "context": "Though our proposed work is based on a deterministic model, we have shown that the energy, or scoring function of the GAE is equivalent, up to a constant, to that of a conditional RBM, a model that has already seen some use in structured prediction problems [18,12].", "startOffset": 258, "endOffset": 265}, {"referenceID": 16, "context": "GAE scoring can be applied to structured output problems as a type of \u201cpost-classification\u201d [17].", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "followed the same experimental set up as [18].", "startOffset": 41, "endOffset": 45}, {"referenceID": 4, "context": "Four multi-labeled datasets were considered: Yeast [5] consists of biological attributes, Scene [2] is image-based, and MTurk [13] and MajMin [14] are targeted towards tagging music.", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "Four multi-labeled datasets were considered: Yeast [5] consists of biological attributes, Scene [2] is image-based, and MTurk [13] and MajMin [14] are targeted towards tagging music.", "startOffset": 96, "endOffset": 99}, {"referenceID": 12, "context": "Four multi-labeled datasets were considered: Yeast [5] consists of biological attributes, Scene [2] is image-based, and MTurk [13] and MajMin [14] are targeted towards tagging music.", "startOffset": 126, "endOffset": 130}, {"referenceID": 13, "context": "Four multi-labeled datasets were considered: Yeast [5] consists of biological attributes, Scene [2] is image-based, and MTurk [13] and MajMin [14] are targeted towards tagging music.", "startOffset": 142, "endOffset": 146}, {"referenceID": 17, "context": "We compared our proposed approaches to logistic regression, a standard MLP, and the two structured CRBM training algorithms presented in [18].", "startOffset": 137, "endOffset": 141}, {"referenceID": 24, "context": "There have been many theoretical and empirical studies on auto-encoders [25,20,21,24,6,7], however, the theoretical study of gated auto-encoders is limited apart from [15,4].", "startOffset": 72, "endOffset": 89}, {"referenceID": 19, "context": "There have been many theoretical and empirical studies on auto-encoders [25,20,21,24,6,7], however, the theoretical study of gated auto-encoders is limited apart from [15,4].", "startOffset": 72, "endOffset": 89}, {"referenceID": 20, "context": "There have been many theoretical and empirical studies on auto-encoders [25,20,21,24,6,7], however, the theoretical study of gated auto-encoders is limited apart from [15,4].", "startOffset": 72, "endOffset": 89}, {"referenceID": 23, "context": "There have been many theoretical and empirical studies on auto-encoders [25,20,21,24,6,7], however, the theoretical study of gated auto-encoders is limited apart from [15,4].", "startOffset": 72, "endOffset": 89}, {"referenceID": 5, "context": "There have been many theoretical and empirical studies on auto-encoders [25,20,21,24,6,7], however, the theoretical study of gated auto-encoders is limited apart from [15,4].", "startOffset": 72, "endOffset": 89}, {"referenceID": 6, "context": "There have been many theoretical and empirical studies on auto-encoders [25,20,21,24,6,7], however, the theoretical study of gated auto-encoders is limited apart from [15,4].", "startOffset": 72, "endOffset": 89}, {"referenceID": 14, "context": "There have been many theoretical and empirical studies on auto-encoders [25,20,21,24,6,7], however, the theoretical study of gated auto-encoders is limited apart from [15,4].", "startOffset": 167, "endOffset": 173}, {"referenceID": 3, "context": "There have been many theoretical and empirical studies on auto-encoders [25,20,21,24,6,7], however, the theoretical study of gated auto-encoders is limited apart from [15,4].", "startOffset": 167, "endOffset": 173}, {"referenceID": 6, "context": "In the first part of the paper, by following the same procedure as [7], we showed that the GAE could be scored according to an energy function.", "startOffset": 67, "endOffset": 70}, {"referenceID": 8, "context": "One interesting observation is that Gaussian-Bernoulli RBMs have been reported to be difficult to train [9,3], and the success of training RBMs is highly dependent on the training setup [26].", "startOffset": 104, "endOffset": 109}, {"referenceID": 2, "context": "One interesting observation is that Gaussian-Bernoulli RBMs have been reported to be difficult to train [9,3], and the success of training RBMs is highly dependent on the training setup [26].", "startOffset": 104, "endOffset": 109}, {"referenceID": 25, "context": "One interesting observation is that Gaussian-Bernoulli RBMs have been reported to be difficult to train [9,3], and the success of training RBMs is highly dependent on the training setup [26].", "startOffset": 186, "endOffset": 190}], "year": 2015, "abstractText": "Auto-encoders are perhaps the best-known non-probabilistic methods for representation learning. They are conceptually simple and easy to train. Recent theoretical work has shed light on their ability to capture manifold structure, and drawn connections to density modeling. This has motivated researchers to seek ways of auto-encoder scoring, which has furthered their use in classification. Gated auto-encoders (GAEs) are an interesting and flexible extension of auto-encoders which can learn transformations among different images or pixel covariances within images. However, they have been much less studied, theoretically or empirically. In this work, we apply a dynamical systems view to GAEs, deriving a scoring function, and drawing connections to Restricted Boltzmann Machines. On a set of deep learning benchmarks, we also demonstrate their effectiveness for single and multi-label classification.", "creator": "LaTeX with hyperref package"}}}