{"id": "1412.6650", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Incremental Adaptation Strategies for Neural Network Language Models", "abstract": "It is today acknowledged that neural network language models outperform back- off language models in applications like speech recognition or statistical machine translation. However, training these models on large amounts of data can take several days. We present efficient techniques to adapt a neural network language model to new data. Instead of training a completely new model or rely on mixture approaches, we propose two new methods: continued training on resampled data or insertion of adaptation layers. We present experimental results in an CAT en- vironment where the post-edits of professional translators are used to improve an SMT system. Both methods are very fast and achieve significant improvements without over-fitting the small adaptation data.", "histories": [["v1", "Sat, 20 Dec 2014 13:06:05 GMT  (280kb,D)", "https://arxiv.org/abs/1412.6650v1", null], ["v2", "Tue, 23 Dec 2014 13:43:19 GMT  (280kb,D)", "http://arxiv.org/abs/1412.6650v2", "submitted as conference paper to ICLR 2015"], ["v3", "Tue, 23 Jun 2015 11:36:36 GMT  (39kb,D)", "http://arxiv.org/abs/1412.6650v3", "accepted as workshop paper at ACL-IJCNLP 2015"], ["v4", "Tue, 7 Jul 2015 14:54:51 GMT  (36kb,D)", "http://arxiv.org/abs/1412.6650v4", "accepted as workshop paper at ACL-IJCNLP 2015"]], "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.LG", "authors": ["aram ter-sarkisov", "holger schwenk", "loic barrault", "fethi bougares"], "accepted": false, "id": "1412.6650"}, "pdf": {"name": "1412.6650.pdf", "metadata": {"source": "CRF", "title": "Incremental Adaptation Strategies for Neural Network Language Models", "authors": ["Aram Ter-Sarkisov", "Holger Schwenk", "Fethi Bougares"], "emails": ["tersarkisov@lium.univ-lemans.fr", "holger.schwenk@lium.univ-lemans.fr", "loic.barrault@lium.univ-lemans.fr", "fethi.bougares@lium.univ-lemans.fr"], "sections": [{"heading": "1 Introduction", "text": "A language model (LM) plays an important role in many natural language processing applications, namely speech recognition and statistical machine translation (SMT). For a very long time, back-off n-gram models were considered to be the state-ofthe-art, in particular when large amounts of training data are available.\nAn alternative approach is based on the use of high-dimensional embeddings of the words and the idea to perform the probability estimation in this space. By these means, meaningful interpolations can be expected. The projection and probability estimation can be jointly learned by a neural network (Bengio et al., 2003). These models, also called continuous space language models (CSLM), have seen a surge in popularity, and it was confirmed in many studies that they systematically outperform back-off n-gram models by a\nsignificant margin in SMT and speech recognition. Many variants of the basic approach were proposed during the last years, e.g. the use of recurrent architectures (Mikolov et al., 2010) or LSTM (Sundermeyer et al., 2012). More recently, neural networks were also used for the translation model in an SMT system (Le et al., 2012; Schwenk, 2012; Cho et al., 2014), and first translations systems entirely based on neural networks were proposed (Sutskever et al., 2014; Bahdanau et al., 2014).\nHowever, to the best of our knowledge, all these systems are static, i.e. they are trained once on a large representative corpus and are not changed or adapted to new data or conditions. The ability to adapt to changing conditions is a very important property of an operational SMT system. The need for adaptation occurs for instance in a system to translate daily news articles in order to account for the changing environment. Another typical application is the integration of an SMT system in an CAT1 tool: we want to improve the SMT systems with help of user corrections. Finally, one may also want to adapt a generic SMT to a particular genre or topic for which we lack large amounts of specific data. Various adaptation schemes were proposed for classical SMT systems, but to the best of our knowledge, there is only very limited works involving neural network models.\nWe are interested in a setting where an LM needs to be adapted to a small amount of data which is representative of a domain change, so that the overall system will perform better on this domain in the future. Our task, which corresponds to concrete needs in real-world applications, is the translation of a document by an human over several days. The human translator is assisted by an SMT system which proposes translation hypothesis to speed up his work (post editing). After one day of work, we adapt the CSLM to the transla-\n1Computer Assisted Translation\nar X\niv :1\n41 2.\n66 50\nv4 [\ncs .N\nE ]\n7 J\nul 2\n01 5\ntions already performed by the human translator, and show that the SMT system performs better on the remaining part of the document.\nIn this paper, we use the open-source MateCat tool2 and a closely integrated SMT system3 which is already adapted to the task (translation of legal documents). For each source sentence, the system proposes an eventual match in the translation memory and a translation by the SMT system. The human translator can decide to either post-edit them, or to perform a new translation from scratch. After one day of work, we want to use all the postedited sentences to adapt the SMT systems, so that the translation quality is improved for the next day. This means that the SMT system will be adapted to the specific translation project. One important particularity of the task is that we have a very small amount of adaptation data, usually around three thousand words per day.\nThis paper is organized as follows. In the next two sections, we summarize basic notions of statistical machine translation and continuous space language models. We then present our tasks and results. The paper concludes with a discussion and directions of future research."}, {"heading": "2 Related work", "text": "Popular approaches to adapt the LM in an SMT system are mixture models, e.g. (Foster and Kuhn, 2007; Koehn and Schroeder, 2007) and data selection. In the former case, separate LMs are trained on the available corpora and are then merged into one, the interpolation coefficients being estimated to minimize perplexity on an in-domain development corpus. This is known as linear mixture models. We can also integrate the various corpusspecific LMs as separate feature functions in the usual log-linear model of an SMT system.\nData selection aims at extracting the most relevant subset of all the available LM training data. The approach proposed in (Moore and Lewis, 2010) has turned out to be the most effective one in many settings. Adaptation of the LM of an SMT models in an CAT environment was also investigated in several studies, e.g. (Bach et al., 2009; Bertoldi et al., 2012; Cettolo et al., 2014).\nAdaptation to new data was also investigated in the neural network community, usually by some type of incremental training on a (subset) of the\n2https://www.matecat.com/ 3http://www.statmt.org/moses/\ndata. Curriculum learning (Bengio et al., 2009), which aims in presenting the training data in a particular order to improve generalization, could be also used to perform adaptation on some new data. There are a couple of papers which investigate adaptation in the context of a particular application, namely image processing and speech recognition. One could for instance mention a recent work which investigated how to transfer features in convolutional networks (Yosinski et al., 2014), or research to perform speaker adaptation of a phoneme classifier based on TRAPS (Trmal et al., 2010).\nThere are also a few publications which investigate adaptation of neural network language models, most of them very recent. The insertion of an additional adaption layer to perform speaker adaptation was proposed by Park et al. (Park et al., 2010). Earlier this idea was explored in (Yao et al., 2012) for speech recognition through an affine transform of the output layer. Adaptation through data selection was studied in (Jalalvand, 2013) (selection of sentences in out-of-domain corpora based on similarity between sentences) and (Duh et al., 2013) (training of three models: n-gram, RNN and interpolated LM on two SMT systems: in-domain data only and all-domain). Several variants of curriculum learning are explored by Shi et al. to adapt a recurrent LM to a sub-domain, again in the area of speech recognition (Shia et al., 2014). Finally, one of the early applications of RNN was in (Kombrink et al., 2011): it was used to rescore the n-best list, speed-up the rescoring process, adapt an LM and estimate the influence of history."}, {"heading": "3 Statistical Machine Translation", "text": "In the statistical approach to machine translation, all models are automatically estimated from examples. Let us assume that we want to translate a sentence in the source language s to a sentence in the target language t. Then, the fundamental equation of SMT is, applying Bayes rule:\nt\u2217 = argmax t P (t|s) = argmax t P (s|t)P (t)\n(1) The translation model P (s|t) is estimated from bitexts, bilingual sentence aligned data, and the language model P (t) from monolingual data in the target language. A popular approach are phrasebased models which translate short sequences of words together (Koehn et al., 2003; Och and\nNey, 2003). The translation probabilities of these phrase pairs are usually estimated by simple relative frequency. The LM is normally a 4-gram back-off model. The log-linear approach is commonly used to consider more models (Och, 2003), instead of just a translation and language model:\nt\u2217 = argmax t M\u2211 m=1 \u03bbmhm(s, t), (2)\nwhere hm(s, t) are so-called feature functions. The weights \u03bbm are optimized during the tuning stage. In the Moses system, fourteen feature functions are usually used.\nAutomatic evaluation of an SMT system remains an open question and many metrics have been proposed. In this study we use the BLEU score which measures the n-gram precision between the translation and a human reference translation (Papineni et al., 2002). Higher values mean better translation quality."}, {"heading": "4 Continuous Space Language Model", "text": "The basic architecture of an CSLM is shown in Figure 1. The words are first projected onto a continuous representation, the remaining part of the network estimates the probabilities. Usually one tanh hidden and a softmax output layer are used, but recent studies have shown that deeper architecture perform better (Schwenk et al., 2014). We will use three tanh hidden and a softmax output layer as depicted in Figure 1. This type of architecture is now well known and the reader is referred to the literature for further details, e.g. (Schwenk, 2007).\nAll our experiments were performed with the open-source CSLM toolkit4 (Schwenk, 2013), which was extended for our purposes. A major challenge for neural network LMs is how to handle the words at the output layer since a the softmax normalization would be very costly for large vocabularies. Various solutions have been proposed: short-lists (Schwenk, 2007), a class decomposition (Mikolov et al., 2011) or an hierarchical decomposition (Le et al., 2011). In this work, we use short-lists, but our adaptation scheme could be equally applied to the other solutions."}, {"heading": "4.1 Adaptation schemes", "text": "As mentioned above, the most popular and most successful adaptation schemes for standard backoff LMs are data selection and mixture models. Both could be also applied to CSLMs. In practice, this would mean that we train a completely new CSLM on data selected by the adaptation process, or that we train several CSLMs, e.g. a generic and task-specific one, and combine them in linear or log-linear way. However, full training of an CSLM usually takes a substantial amount of time, often several hours or even days in function of the size of the available training data. Building several CSLMs and combining them would also increase the translation time.\nTherefore, we propose and compare CSLM adaptation schemes which are very efficient: they can be performed in a couple of minutes. The underlying idea of both techniques is not to train new models, but to slightly change the existing CSLM in order to account for the new training data. In the first method, we perform continued training of the CSLM with a mixture of the new adaptation data and the original training data. In the second method, adaptation layers are inserted in the neural network as outlined in red in Figure 1. This additional layer is initialized with the identity matrix and only the weights of this layer are updated. This idea was previously proposed in framework of a speech recognition system (Park et al., 2010). We build on this work and explore different variants of this technique. An interesting alternative is to keep the original architecture of the NN and to only modify one layer, e.g. the weights between two tanh layers in Figure 1. This variant will be explored in future work.\n4The CSLM toolkit is available at http://www-lium.univlemans.fr/\u223ccslm/"}, {"heading": "5 Task and baselines", "text": "Our task is to improve an SMT system which is closely integrated into an open-source CAT tool with the post-edits provided by professional human translators. This tool and algorithms to update standard phrase-based SMT systems, including back-off language models, were developed in the framework of the European project MateCat (Cettolo et al., 2014). We consider the translation of legal texts from English into German and French. The available resources for each language pair are summarized in Table 1.\nEach SMT system is based on the Moses toolkit (Koehn et al., 2007) and built according to the following procedure: first we perform data selection on the parallel and monolingual corpora in order to extract the data which is the most representative to our development set. In our case, we are interested in the translation of legal documents. Data selection is now a well established method in the SMT community. It is performed for the language and translation model using the methods described in (Moore and Lewis, 2010) and (Axelrod et al., 2011) respectively.\nWe train a 4-gram back-off LM and a phrasebased system using the standard Moses parameters. The coefficients of the 14 feature functions are optimized by MERT to maximize the BLEU score on the development data. This system is then used to create up to 1000 distinct hypotheses for each source sentence. We then add a 15th feature function corresponding to the log probability generated by CSLM for each hypothesis and the coefficients are again optimized. This is usually referred to as n-best list rescoring. We call this final system domain-adapted since it is optimized to translate legal documents. This system is then used to assist human translators to translate a large document in the legal domain.\nTypically, we will process day by day: after one day of work, all the human translations (created from scratch or by post-editing the hypotheses from the SMT system) are injected into the system and we hope that SMT will perform better on the rest of the document to be translated, e.g. on the second day of work. This procedure can be repeated over several days when the document is rather large (see section 5.2). Usually humans are able to translate approximately 3 000 words per day. We call this procedure project-adaptation."}, {"heading": "5.1 Results for the English/German system", "text": "The 4-gram back-off LM built on the selected data has a perplexity of 151.1 on the domain-specific development data. Given the fact that an CSLM can be very efficiently trained on long context windows, we used a 28-gram in all experiments. By these means we hope to capture the long range dependencies of German. The projection layer of the CSLM was of dimension 320, followed by three tanh hidden layers of size 1024 and a softmax output layer of 32k neurons (short-list). This shortlist accounts for around 92 % of the tokens used in the corpus. The initial learning rate was set to 0.06 and exponentially decreased over the iterations. The network converged after 7 epochs with a perplexity of 96.6, i.e. a 36% relative reduction. The total training time is less than 7 hours on a Nvidia K20x GPU. Table 2 (upper part) gives the BLEU score of these baseline domain-adapted systems.\nTo analyze our project adaptation techniques we have split another legal document into two part, \u201cDay 1\u201d and \u201cDay 2\u201d. The first part, \u201cDay 1\u201d, containing around 3.2K words, is used to adapt the SMT system and the CSLM, aiming to improve the translation performance on the second part, named \u201cDay 2\u201d. Note that the performance on \u201cDay 1\u201d itself, after adaptation, is of limited interest since we could quite easily overtrain the model on this data. On the other hand, it is informative to monitor the performance on the domaingeneric development set. Ideally, we will improve the performance on \u201cDay 2\u201d, i.e. future text of the same project than the adaptation data, with only a slightly loss on the generic development data.\nVarious adaptation schemes are compared in Table 4. The network is adapted on the data from Day 1 and we want to improve performance on Day 2. At the same time, we do not want to\noverfit the data and keep good performance on the domain-specific Dev set. To achieve this, we continued training of the networks with a mixture of old and new data. All the adaptation data was always used (Day 1, 3.2k words) and small fractions of the domain-selected data were randomly sampled at each epoch, so that the adaptation data accounts for 14, 25, 45, 62 and 77 % respectively. Since the networks are trained on very small amounts of data (4 - 23k words), the overall adaptation process takes only a few minutes. The statistics of the data used at each epoch is detailed in Table 3. We will show below that it is important to perform the adaptation of the CSLM with a mixture of generic and adaptation data to prevent overfitting.\nWe experiment along the following lines:\n1. different resampling coefficients of adaptation and generic data according to Table 3.\n2. network topologies:\na) continue training of the original network updating all the weights.\nb) insert one or two hidden layers with 1024 neurons using linear or hyperbolic tangent activation functions respectively. These additional layers are initialized with the identity matrix and only these layers are updated using backpropagation function.\nWe record the perplexity of the adapted CSLM on Day 2 (\u223c 11K words), which is then used as a guideline for selecting the best networks to integrate into an SMT system (marked with an asterisk in the Table 4). Lowest perplexity was obtained by keeping the baseline network topology (upper part of Table 4) when Day 1 data constituted 14 % of the incremental training data set: the perplexity on Day 2 decreases from 126.1 to 94.6, with a minor increase on the Dev set (96.6\u219298.7). Using larger\nfractions of Day 1 leads to over-fitting of the network: the perplexity on Day 2 and the generic Dev set increases.\nThe lower part of Table 4 summarizes the results when inserting one adaptation layer, with a linear or tanh activation function, at three different slots respectively. For each configuration, we explored five different proportions of the baseline corpora and Day 1 (cf. Table 3), but for clarity, we only report the most interesting results. The overall tendency was that using more than 25% of Day 1 systematically leads to over-fitting of the network. Several conclusions can be made: a) an tanh adaptation layer outperforms a linear one; b) it is better to insert the adaptation layer at the end of the network; c) updating the weights of the inserted layer only overfits less than incremental training the whole network (comparing the last block in Table 4 with the second block): the perplexity on Day 1 decreases substantially (126.1\u2192101.5) and we observe a slight improve-\nment on the Dev set (96.6\u219295.1). Finally, Table 2 lower part gives the BLEU scores of the project-adapted systems. When no CSLM is used, the BLEU score on Day 2 increases from 19.31 to 20.14 (+0.83). This is achieved by adapting the translation and back-off LM (details of the algorithms can be found in (Cettolo et al., 2014)). Both CSLM adaptation schemes obtained quite similar BLEU scores: 21.12 and 21.26 respectively, the insertion of one additional tanh layer having a slight advantage. Overall, the adapted CSLM yields an improvement of 1.12 BLEU (20.14 to 21.26) while it was about 1 point BLEU for the domain-adapted system (19.31 to 20.28). This nicely shows the effectiveness of our adaptation scheme, which can be applied in a couple of minutes."}, {"heading": "5.2 Results for the English/French system", "text": "A second set of experiments was performed to confirm the effectiveness of our adaptation proce-\ndure on a different language pair: English/French. In the MT community it is well known that the translation into German is a very hard task which is reflected in the low BLEU scores around 20 (see Table 2). On the other hand, our baseline SMT system for the English/French language pair has a BLEU score well above 40. One may argue that it is more complicated to further improve such a system.\nIn addition, we investigate adaptation of the SMT system and the CSLM over five consecutive days: the human translator works for one day and corrects the SMT hypothesis, these corrections are used to adapt the system for the second day. Human corrections are again inserted into the system and a new system for the third day is built, and so on. With this adaptation scheme we want to verify whether our methods are robust or quickly overfit the adaptation data. The number of words for each day are about three thousand. A 16-gram CSLM for the French target language with a shortlist of 12k was used. Training was performed for 15 epochs.\nFor this task, we only used the incremental learning method (see Table 4) as it yielded the lowest perplexity in the English/German experiment. The data from the five consecutive days is coming from one large document which is assumed to be from one domain only. Therefore, we decided to always use all the available data from the preceding days to adapt our models. For instance, after the third day, the data from Day 1, 2 and 3 is used to build a new system for the fourth day. The proportions of each day in the corpus used to continue the training of the CSLM are given in Table 6 (note that every day\u2019s proportion decreases, but their combined share increases from 39% to 68%). The perplexities of the various CSLMs are\ngiven in Table 7.\nOne first observation is the rather high perplexity of the models on each day. This shows the importance of project adaptation even when domain related data is available. Adaptation allows to decrease the perplexity by more than 10% relative for each day. While the perplexities vary between the project days, they are reduced in every case, which demonstrates the effectiveness of the adaptation method.\nIn order to evaluate the impact of the CSLM adaptation on the SMT system, we performed various translation experiments. The results are provided in Table 5. The BLEU scores of the various systems using the baseline and the adapted CSLMs are presented. We run tests with three different human translators - for the sake of clarity, we provide detailed results for one translator only. The observed tendencies are similar for the two other translators. First of all, one can see that the CSLM improves the BLEU score of the baseline systems between 2.3 to 3.4 BLEU points, e.g. for Day 2 from 44.07 to 46.61. Adapting the whole SMT system to the new data improves significantly the translation quality, e.g. from 46.61 to 52.01 for Day 2, without changing the CSLM. The proposed adaptation scheme of the CSLM achieves additional important improvements, in average 2.6 BLEU points. This gain is relatively constant for all days.\nFor comparison, we also give the BLEU scores when using four reference translations: the one of the three human translators and one independent translation which was provided by the European Commission.\nWe still observe some small gains although three out of four translations were not used in the adaptation process. This shows that our adaptation scheme not only learns the particular style of one\ntranslator, but also achieves more generic improvements. This also shows that the adaptation process is beneficial for improving state-of-the-art systems which already perform very well on certain tasks."}, {"heading": "6 Conclusions", "text": "In this paper, we presented a thorough study of different techniques to adapt a continuous space language model to small amounts of new data. In our case, we want to integrate user corrections so that a statistical machine translation system performs better on similar texts. Our task, which corresponds to concrete needs in real-world applications, is the translation of a document by an human over several days. The human translator is assisted by an SMT system which proposes translation hypothesis to speed up his work (post editing). After one day of work, we adapt the CSLM to the translations already performed by the human translator, and show that the SMT system performs better on the remaining part of the document.\nWe explored two adaptation strategies: continued training of an existing neural network LM, and insertion of an adaptation layer with the weight updates being limited to that layer only. In both cases, the network is trained on a combination of adaptation data (3\u201315k words) and a portion of similar size, randomly sampled in the original training data. By these means, we avoid overfitting of the neural network to the adaptation data. Overall, the adaptation data is very small \u2013 less than 50k words \u2013 which leads to very fast training of the neural network language model: a couple of minutes on a standard GPU.\nWe provided experimental evidence of the effectiveness of our approach on two large SMT tasks: the translation of legal documents from English into German and French respectively. In both cases, significantly improvement of the translation quality was observed."}], "references": [{"title": "Domain adaptation via pseudo in-domain data selection", "author": ["Xiaodong He", "Jianfeng Gao"], "venue": "In EMNLP,", "citeRegEx": "Axelrod et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Axelrod et al\\.", "year": 2011}, {"title": "Incremental Adaptation of Speech-toSpeech Translation", "author": ["Bach et al.2009] Nguyen Bach", "Roger Hsiao", "Matthias Eck", "Paisarn Charoenpornsawat", "Stephan Vogel", "Tanja Schultz", "Ian Lane", "Alex Waibel", "Alan W. Black"], "venue": "In NAACL,", "citeRegEx": "Bach et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2009}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau et al.2014] D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In NIPS workshop on Modern Machine Learning and Natural Language Processing", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "Rejean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Evaluating the Learning Curve of Domain Adaptive Statistical MachineTranslation Systems", "author": ["Mauro Cettolo", "Marcello Federico", "Christian Buck"], "venue": "In Workshop on SMT,", "citeRegEx": "Bertoldi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bertoldi et al\\.", "year": 2012}, {"title": "Translation project adaptation for MT-enhanced computer assisted translation", "author": ["Nicola Bertoldi", "Marcello Federico", "Holger Schwenk", "Lo\u0131\u0308c Barrault", "Christophe Servan"], "venue": "Machine Translation,", "citeRegEx": "Cettolo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Adaptation data selection using neural language models: Experiments in machine translation", "author": ["Duh et al.2013] Kevin Duh", "Graham Neubig", "Katsuhito Sudoh", "Hajime Tsukada"], "venue": null, "citeRegEx": "Duh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duh et al\\.", "year": 2013}, {"title": "Mixture-model adaptation for SMT", "author": ["Foster", "Kuhn2007] George Foster", "Roland Kuhn"], "venue": "In EMNLP,", "citeRegEx": "Foster et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2007}, {"title": "Improving language model adaptation using automatic data selection and neural network", "author": ["Shahab Jalalvand"], "venue": "In RANLP,", "citeRegEx": "Jalalvand.,? \\Q2013\\E", "shortCiteRegEx": "Jalalvand.", "year": 2013}, {"title": "Experiments in domain adaptation for statistical machine translation", "author": ["Koehn", "Schroeder2007] Philipp Koehn", "Josh Schroeder"], "venue": "In Second Workshop on SMT,", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Statistical phrase-based machine translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In HLT/NACL,", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Herbst."], "venue": "ACL, demonstration session.", "citeRegEx": "Herbst.,? 2007", "shortCiteRegEx": "Herbst.", "year": 2007}, {"title": "Recurrent neural network based language modeling in meeting recognition", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget"], "venue": "In INTERSPEECH,", "citeRegEx": "Kombrink et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kombrink et al\\.", "year": 2011}, {"title": "Structured output layer neural network language model", "author": ["Le et al.2011] Hai-Son Le", "I. Oparin", "A. Allauzen", "JL. Gauvain", "F. Yvon"], "venue": "In ICASSP,", "citeRegEx": "Le et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Le et al\\.", "year": 2011}, {"title": "Continuous space translation models with neural networks", "author": ["Le et al.2012] Hai-Son Le", "Alexandre Allauzen", "Fran\u00e7ois Yvon"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Le et al\\.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Intelligent selection of language model training data", "author": ["Moore", "Lewis2010] Robert C. Moore", "William Lewis"], "venue": "In ACL,", "citeRegEx": "Moore et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Moore et al\\.", "year": 2010}, {"title": "A systematic comparison of various statistical alignement models", "author": ["Och", "Ney2003] Franz Josef Och", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In ACL,", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Papineni et al.2002] K. Papineni", "S. Roukos", "T. Ward", "W.J. Zhu"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Improved neural network based language modelling and adaptation", "author": ["Park et al.2010] Junho Park", "Xunying Liu", "Mark J.F. Gales", "Phil C. Woodland"], "venue": "In Interspeech,", "citeRegEx": "Park et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Park et al\\.", "year": 2010}, {"title": "Efficient training strategies for deep neural network language models", "author": ["Fethi Bougares", "Lo\u0131\u0308c Barrault"], "venue": "In NIPS workshop on Deep Learning and Representation Learning", "citeRegEx": "Schwenk et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Schwenk et al\\.", "year": 2014}, {"title": "Continuous space language models", "author": ["Holger Schwenk"], "venue": "Computer Speech and Language,", "citeRegEx": "Schwenk.,? \\Q2007\\E", "shortCiteRegEx": "Schwenk.", "year": 2007}, {"title": "Continuous space translation models for phrase-based statistical machine translation", "author": ["Holger Schwenk"], "venue": "In Coling,", "citeRegEx": "Schwenk.,? \\Q2012\\E", "shortCiteRegEx": "Schwenk.", "year": 2012}, {"title": "CSLM a modular open-source continuous space language modeling toolkit", "author": ["Holger Schwenk"], "venue": "In Interspeech,", "citeRegEx": "Schwenk.,? \\Q2013\\E", "shortCiteRegEx": "Schwenk.", "year": 2013}, {"title": "Recurrent neural network language model adaptation with curriculum learning", "author": ["Shia et al.2014] Yangyang Shia", "Martha Larsona", "Catholijn M. Jonkera"], "venue": "Computer Speech & Language,", "citeRegEx": "Shia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shia et al\\.", "year": 2014}, {"title": "LSTM neural networks for language modeling", "author": ["Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In Interspeech", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["O. Vinyals", "Q. Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Adaptation of a feedforward articifical neural network using a linear transform", "author": ["Trmal et al.2010] Jan Trmal", "Jan Zelinka", "Ludek M\u00fcller"], "venue": "In Text, Speech and Dialogue,", "citeRegEx": "Trmal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Trmal et al\\.", "year": 2010}, {"title": "Adaptation of context-dependent deep neural networks for automatic speech recognition", "author": ["Yao et al.2012] Kaisheng Yao", "Dong Yu", "Frank Seide", "Hang Su", "Li Deng", "Yifan Gong"], "venue": "In Spoken Language Technology Workshop (SLT), 2012 IEEE,", "citeRegEx": "Yao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2012}, {"title": "How transferable are features in deep neural networks", "author": ["Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": "In NIPS,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "The projection and probability estimation can be jointly learned by a neural network (Bengio et al., 2003).", "startOffset": 85, "endOffset": 106}, {"referenceID": 16, "context": "the use of recurrent architectures (Mikolov et al., 2010) or LSTM (Sundermeyer et al.", "startOffset": 35, "endOffset": 57}, {"referenceID": 27, "context": ", 2010) or LSTM (Sundermeyer et al., 2012).", "startOffset": 16, "endOffset": 42}, {"referenceID": 15, "context": "tion model in an SMT system (Le et al., 2012; Schwenk, 2012; Cho et al., 2014), and first translations systems entirely based on neural networks were proposed (Sutskever et al.", "startOffset": 28, "endOffset": 78}, {"referenceID": 24, "context": "tion model in an SMT system (Le et al., 2012; Schwenk, 2012; Cho et al., 2014), and first translations systems entirely based on neural networks were proposed (Sutskever et al.", "startOffset": 28, "endOffset": 78}, {"referenceID": 6, "context": "tion model in an SMT system (Le et al., 2012; Schwenk, 2012; Cho et al., 2014), and first translations systems entirely based on neural networks were proposed (Sutskever et al.", "startOffset": 28, "endOffset": 78}, {"referenceID": 28, "context": ", 2014), and first translations systems entirely based on neural networks were proposed (Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 88, "endOffset": 135}, {"referenceID": 2, "context": ", 2014), and first translations systems entirely based on neural networks were proposed (Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 88, "endOffset": 135}, {"referenceID": 1, "context": "(Bach et al., 2009; Bertoldi et al., 2012; Cettolo et al., 2014).", "startOffset": 0, "endOffset": 64}, {"referenceID": 4, "context": "(Bach et al., 2009; Bertoldi et al., 2012; Cettolo et al., 2014).", "startOffset": 0, "endOffset": 64}, {"referenceID": 5, "context": "(Bach et al., 2009; Bertoldi et al., 2012; Cettolo et al., 2014).", "startOffset": 0, "endOffset": 64}, {"referenceID": 31, "context": "One could for instance mention a recent work which investigated how to transfer features in convolutional networks (Yosinski et al., 2014), or research to perform speaker adaptation of a phoneme classifier based on TRAPS (Trmal et al.", "startOffset": 115, "endOffset": 138}, {"referenceID": 29, "context": ", 2014), or research to perform speaker adaptation of a phoneme classifier based on TRAPS (Trmal et al., 2010).", "startOffset": 90, "endOffset": 110}, {"referenceID": 21, "context": "(Park et al., 2010).", "startOffset": 0, "endOffset": 19}, {"referenceID": 30, "context": "Earlier this idea was explored in (Yao et al., 2012) for speech recognition through an affine transform of the output layer.", "startOffset": 34, "endOffset": 52}, {"referenceID": 9, "context": "Adaptation through data selection was studied in (Jalalvand, 2013)", "startOffset": 49, "endOffset": 66}, {"referenceID": 7, "context": "(selection of sentences in out-of-domain corpora based on similarity between sentences) and (Duh et al., 2013) (training of three models: n-gram, RNN and interpolated LM on two SMT systems: in-domain data only and all-domain).", "startOffset": 92, "endOffset": 110}, {"referenceID": 26, "context": "to adapt a recurrent LM to a sub-domain, again in the area of speech recognition (Shia et al., 2014).", "startOffset": 81, "endOffset": 100}, {"referenceID": 13, "context": "Finally, one of the early applications of RNN was in (Kombrink et al., 2011): it was used", "startOffset": 53, "endOffset": 76}, {"referenceID": 19, "context": "monly used to consider more models (Och, 2003), instead of just a translation and language model:", "startOffset": 35, "endOffset": 46}, {"referenceID": 20, "context": "In this study we use the BLEU score which measures the n-gram precision between the translation and a human reference translation (Papineni et al., 2002).", "startOffset": 130, "endOffset": 153}, {"referenceID": 22, "context": "Usually one tanh hidden and a softmax output layer are used, but recent studies have shown that deeper architecture perform better (Schwenk et al., 2014).", "startOffset": 131, "endOffset": 153}, {"referenceID": 23, "context": "(Schwenk, 2007).", "startOffset": 0, "endOffset": 15}, {"referenceID": 25, "context": "All our experiments were performed with the open-source CSLM toolkit4 (Schwenk, 2013), which was extended for our purposes.", "startOffset": 70, "endOffset": 85}, {"referenceID": 23, "context": "Various solutions have been proposed: short-lists (Schwenk, 2007), a class decomposition (Mikolov et al.", "startOffset": 50, "endOffset": 65}, {"referenceID": 14, "context": ", 2011) or an hierarchical decomposition (Le et al., 2011).", "startOffset": 41, "endOffset": 58}, {"referenceID": 21, "context": "This idea was previously proposed in framework of a speech recognition system (Park et al., 2010).", "startOffset": 78, "endOffset": 97}, {"referenceID": 5, "context": "ing back-off language models, were developed in the framework of the European project MateCat (Cettolo et al., 2014).", "startOffset": 94, "endOffset": 116}, {"referenceID": 10, "context": "Each SMT system is based on the Moses toolkit (Koehn et al., 2007) and built according to the fol-", "startOffset": 46, "endOffset": 66}, {"referenceID": 0, "context": "It is performed for the language and translation model using the methods described in (Moore and Lewis, 2010) and (Axelrod et al., 2011) respectively.", "startOffset": 114, "endOffset": 136}, {"referenceID": 5, "context": "This is achieved by adapting the translation and back-off LM (details of the algorithms can be found in (Cettolo et al., 2014)).", "startOffset": 104, "endOffset": 126}], "year": 2015, "abstractText": "It is today acknowledged that neural network language models outperform backoff language models in applications like speech recognition or statistical machine translation. However, training these models on large amounts of data can take several days. We present efficient techniques to adapt a neural network language model to new data. Instead of training a completely new model or relying on mixture approaches, we propose two new methods: continued training on resampled data or insertion of adaptation layers. We present experimental results in an CAT environment where the post-edits of professional translators are used to improve an SMT system. Both methods are very fast and achieve significant improvements without over-fitting the small adaptation data.", "creator": "LaTeX with hyperref package"}}}