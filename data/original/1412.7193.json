{"id": "1412.7193", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Audio Source Separation Using a Deep Autoencoder", "abstract": "This paper proposes a novel framework for unsupervised audio source separation using a deep autoencoder. The characteristics of unknown source signals mixed in the mixed input is automatically by properly configured autoencoders implemented by a network with many layers, and separated by clustering the coefficient vectors in the code layer. By investigating the weight vectors to the final target, representation layer, the primitive components of the audio signals in the frequency domain are observed. By clustering the activation coefficients in the code layer, the previously unknown source signals are segregated. The original source sounds are then separated and reconstructed by using code vectors which belong to different clusters. The restored sounds are not perfect but yield promising results for the possibility in the success of many practical applications.", "histories": [["v1", "Mon, 22 Dec 2014 22:38:06 GMT  (425kb)", "http://arxiv.org/abs/1412.7193v1", "3 pages, 4 figures, ICLR 2015"]], "COMMENTS": "3 pages, 4 figures, ICLR 2015", "reviews": [], "SUBJECTS": "cs.SD cs.LG cs.NE", "authors": ["giljin jang", "han-gyu kim", "yung-hwan oh"], "accepted": false, "id": "1412.7193"}, "pdf": {"name": "1412.7193.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["gjang@ee.knu.ac.kr", "hgkim@cs.kaist.ac.kr", "yhoh@cs.kaist.ac.kr"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 2.\n71 93\nv1 [\ncs .S\nD ]\n2 2\nThis paper proposes a novel framework for unsupervised audio source separation using a deep autoencoder. The characteristics of unknown source signals mixed in the mixed input is automatically by properly configured autoencoders implemented by a network with many layers, and separated by clustering the coefficient vectors in the code layer. By investigating the weight vectors to the final target, representation layer, the primitive components of the audio signals in the frequency domain are observed. By clustering the activation coefficients in the code layer, the previously unknown source signals are segregated. The original source sounds are then separated and reconstructed by using code vectors which belong to different clusters. The restored sounds are not perfect but yield promising results for the possibility in the success of many practical applications."}, {"heading": "1 INTRODUCTION", "text": "In audio analysis applications such as speech recognition and audio-text alignment, the clean target sound helps improve the quality of analysis while noisy input may disturb the analysis process. In a situation that we only have recordings from a single sensor, the problem is very difficult and hence no general solution has been found. Masking in the spectro-temporal region was proposed in Hu & Wang (2004) where the separation mask was constructed by estimated speech pitch. Such method worked well in extracting speech from noisy environment, but the performance is not guaranteed if the target source is not a speech signal. Several separation methods based on non-negative matrix factorization (NMF) was proposed to solve the monaural source separation problem (Raj et al., 2010). NMF utilized the redundancy of the sound spectrogram for source separation. NMF-based methods works for various types of sources. However, the mixing is assumed to be a non-negative linear mixing, and it cannot handle complex sound sources.\nIn this paper, we propose applying a deep autoencoder (Hinton & Salakhutdinov, 2006) to the singlechannel, source separation problem. The autoencoders constructed by networks with many hidden layers have been successfully adopted in many applications, such as image and audio denoising, speech recognition, data compression, and so on. Unlike other applications, our proposed method tries to apply the autoencoder to solve the problem of unsupervised audio source separation. The characteristics of sound sources in the input mixture are learned and stored in a deep autoencoder, and an appropriate source separation algorithm based on unsupervised learning is proposed. Experimental results on mixtures of 5 types of music and 2 types of speech signals suggest that the coefficients in the encoding layer is useful in distinguishing and separating the unknown target sources. The detailed descriptions are in the following sections."}, {"heading": "2 SOURCE SEPARATION USING AUTOENCODER", "text": "In order to learn an autoencoder for audio source mixtures, we first apply short-time Fourier analysis to the time-domain audio signal, resulting magnitude spectrum matrix denoted by Xc,m, where c is the frequency channel index and m the temporal frame index. For the input to the autoencoder, we construct a rectangular window with consecutive frequency channel and time index, such that\nWi,j = {Xc,m|i \u2264 c < i+ h, j \u2264 m < j + l}, (1)\nwhere Wi,j is the window starting from frequency channel i and frame j of the magnitude spectra, where h and l are the height and length of the windows, respectively. The rectangular windows are then unrolled into supervectors for the autoencoder training. The data set is generated by shifting the window one frame in time axis or one frame in frequency axis from all the input spectrum matrix.\nWe designed an autoencoder with 5 hidden layers, which have 50, 18, 6, 18, and 50 units, respectively, as shown in Figure 1.\nBecause there is no labeled training data for the unknown sources, a k-means clustering is performed on the 6-dimensional coefficient vectors of the middle layer. The whole process of the clustering is shown in Figure 2. In this method, we consider the feature vectors of the windows extracted using the autoencoder from different clusters for different sound sources. According to the clustering result of the feature vectors, the windows are also classified into the clusters, and original audio sources are reconstructed using the vectors in the given cluster only."}, {"heading": "3 EXPERIMENTAL RESULTS", "text": "To evaluate the performance of the proposed method, source separation experiments were carried out on a mixtures of 5 different music sounds and 2 different speech sources. The speech sources are\nselected from TIMIT speech corpus, and music sounds are jazz, drum, acoustic guitar, electric guitar and piano. These speech and music sounds were mixed together, generating 2\u00d7 5 = 10 mixtures in total. The audio sounds are sampled at 8 kHz, and only 8 seconds are used in training the respective autoencoders. The spectrogram matrix of the mixtures are obtained by short-time Fourier analysis with frame length 40 ms and shift size 10 ms. For window generation in Equation 1, h = 30 and l = 5 were used, which were decided empirically. Figure 3 shows 10 selected examples from the total 50 weight vectors connecting all the nodes in the last hidden layer to one of the nodes in the final output layer. The input is male speech and jazz music mixture. Four of them represents the change of the spectral peaks over time: third, fourth, fifth, and seventh, which reflect the temporal change of harmonics of speech signals. The remaining six vectors are composed of straight lines over time, modeling the stationary frequency components of music signals. Figure 4 represents the mask constructed by clustering all the code vectors of dimension 6 in the middle layer. The first and the second clusters are jazz music, with slowly changing spectral peaks, and the third and the fourth clusters are mostly speech sounds. These results show that the validity of the proposed method."}, {"heading": "4 CONCLUSIONS", "text": "An application of autoencoders to audio source separation is proposed. The spectrum matrix obtained by short-time Fourier analysis is used for the initial representation of the mixed source signal, and a deep autoencoder is used to represent the spatio-temporal local parts of the spectrum matrix. The code coefficients in the middle layer of the autoencoder is used as a feature for distinguishing audio sources. The main contribution of the proposed method is that the characteristics of unknown sources are extracted from the mixed signals. Although the experimental results are not complete yet, ongoing efforts are being made to improve the proposed method."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science and Technology (no. NRF-2010-0025642)."}], "references": [{"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Monaural speech segregation based on pitch tracking and amplitude modulation", "author": ["Hu", "Guoning", "Wang", "DeLiang"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Non-negative matrix factorization based compensation of music for automatic speech recognition", "author": ["Raj", "Bhiksha", "Virtanen", "Tuomas", "Chaudhuri", "Sourish", "Singh", "Rita"], "venue": "In Proc. INTERSPEECH,", "citeRegEx": "Raj et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Raj et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "Several separation methods based on non-negative matrix factorization (NMF) was proposed to solve the monaural source separation problem (Raj et al., 2010).", "startOffset": 137, "endOffset": 155}], "year": 2014, "abstractText": "This paper proposes a novel framework for unsupervised audio source separation using a deep autoencoder. The characteristics of unknown source signals mixed in the mixed input is automatically by properly configured autoencoders implemented by a network with many layers, and separated by clustering the coefficient vectors in the code layer. By investigating the weight vectors to the final target, representation layer, the primitive components of the audio signals in the frequency domain are observed. By clustering the activation coefficients in the code layer, the previously unknown source signals are segregated. The original source sounds are then separated and reconstructed by using code vectors which belong to different clusters. The restored sounds are not perfect but yield promising results for the possibility in the success of many practical applications.", "creator": "LaTeX with hyperref package"}}}