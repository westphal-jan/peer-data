{"id": "1501.03796", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2015", "title": "The Fast Convergence of Incremental PCA", "abstract": "We consider a situation in which we see samples in $\\mathbb{R}^d$ drawn i.i.d. from some distribution with mean zero and unknown covariance A. We wish to compute the top eigenvector of A in an incremental fashion - with an algorithm that maintains an estimate of the top eigenvector in O(d) space, and incrementally adjusts the estimate with each new data point that arrives. Two classical such schemes are due to Krasulina (1969) and Oja (1983). We give finite-sample convergence rates for both.", "histories": [["v1", "Thu, 15 Jan 2015 20:08:49 GMT  (40kb)", "http://arxiv.org/abs/1501.03796v1", "NIPS 2013"]], "COMMENTS": "NIPS 2013", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["akshay balsubramani", "sanjoy dasgupta", "yoav freund"], "accepted": true, "id": "1501.03796"}, "pdf": {"name": "1501.03796.pdf", "metadata": {"source": "CRF", "title": "The Fast Convergence of Incremental PCA", "authors": ["Akshay Balsubramani"], "emails": ["abalsubr@cs.ucsd.edu", "dasgupta@cs.ucsd.edu", "yfreund@cs.ucsd.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 1.\n03 79\n6v 1\n[ cs\n.L G\n] 1\n5 Ja\nn 20"}, {"heading": "1 Introduction", "text": "Principal component analysis (PCA) is a popular form of dimensionality reduction that projects a data set on the top eigenvector(s) of its covariance matrix. The default method for computing these eigenvectors uses O(d2) space for data in Rd, which can be prohibitive in practice. It is therefore of interest to study incremental schemes that take one data point at a time, updating their estimates of the desired eigenvectors with each new point. For computing one eigenvector, such methods use O(d) space.\nFor the case of the top eigenvector, this problem has long been studied, and two elegant solutions were obtained by Krasulina [7] and Oja [9]. Their methods are closely related. At time n\u2212 1, they have some estimate Vn\u22121 \u2208 Rd of the top eigenvector. Upon seeing the next data point, Xn, they update this estimate as follows:\nVn = Vn\u22121 + \u03b3n ( XnX T n \u2212 V Tn\u22121XnX T n Vn\u22121\n\u2016Vn\u22121\u20162 Id\n) Vn\u22121 (Krasulina)\nVn = Vn\u22121 + \u03b3nXnX\nT n Vn\u22121\n\u2016Vn\u22121 + \u03b3nXnXTn Vn\u22121\u2016 (Oja)\nHere \u03b3n is a \u201clearning rate\u201d that is typically proportional to 1/n.\nSuppose the points X1, X2, . . . are drawn i.i.d. from a distribution on Rd with mean zero and covariance matrix A. The original papers proved that these estimators converge almost surely to the top eigenvector of A (call it v\u2217) under mild conditions:\n\u2022 \u2211n \u03b3n = \u221e while \u2211 n \u03b3 2 n < \u221e. \u2022 If \u03bb1, \u03bb2 denote the top two eigenvalues of A, then \u03bb1 > \u03bb2. \u2022 E\u2016Xn\u2016k < \u221e for some suitable k (for instance, k = 8 works).\nThere are also other incremental estimators for which convergence has not been established; see, for instance, [12] and [16].\nIn this paper, we analyze the rate of convergence of the Krasulina and Oja estimators. They can be treated in a common framework, as stochastic approximation algorithms for maximizing the\nRayleigh quotient\nG(v) = vTAv\nvT v .\nThe maximum value of this function is \u03bb1, and is achieved at v\u2217 (or any nonzero multiple thereof). The gradient is\n\u2207G(v) = 2\u2016v\u20162 ( A\u2212 v TAv vT v Id ) v.\nSince EXnXTn = A, we see that Krasulina\u2019s method is stochastic gradient descent. The Oja procedure is closely related: as pointed out in [10], the two are identical to within second-order terms.\nRecently, there has been a lot of work on rates of convergence for stochastic gradient descent (for instance, [11]), but this has typically been limited to convex cost functions. These results do not apply to the non-convex Rayleigh quotient, except at the very end, when the system is near convergence. Most of our analysis focuses on the buildup to this finale.\nWe measure the quality of the solution Vn at time n using the potential function\n\u03a8n = 1\u2212 (Vn \u00b7 v\u2217)2 \u2016Vn\u20162 ,\nwhere v\u2217 is taken to have unit norm. This quantity lies in the range [0, 1], and we are interested in the rate at which it approaches zero. The result, in brief, is that E[\u03a8n] = O(1/n), under conditions that are similar to those above, but stronger. In particular, we require that \u03b3n be proportional to 1/n and that \u2016Xn\u2016 be bounded."}, {"heading": "1.1 The algorithm", "text": "We analyze the following procedure.\n1. Set starting time. Set the clock to time no. 2. Initialization. Initialize Vno uniformly at random from the unit sphere in R d. 3. For time n = no + 1, no + 2, . . .:\n(a) Receive the next data point, Xn. (b) Update step. Perform either the Krasulina or Oja update, with \u03b3n = c/n.\nThe first step is similar to using a learning rate of the form \u03b3n = c/(n + no), as is often done in stochastic gradient descent implementations [1]. We have adopted it because the initial sequence of updates is highly noisy: during this phase Vn moves around wildly, and cannot be shown to make progress. It becomes better behaved when the step size \u03b3n becomes smaller, that is to say when n gets larger than some suitable no. By setting the start time to no, we can simply fast-forward the analysis to this moment."}, {"heading": "1.2 Initialization", "text": "One possible initialization is to set Vno to the first data point that arrives, or to the average of a few data points. This seems sensible enough, but can fail dramatically in some situations.\nHere is an example. Suppose X can take on just 2d possible values: \u00b1e1,\u00b1\u03c3e2, . . . ,\u00b1\u03c3ed, where the ei are coordinate directions and 0 < \u03c3 < 1 is a small constant. Suppose further that the distribution of X is specified by a single positive number p < 1:\nPr(X = e1) = Pr(X = \u2212e1) = p\n2\nPr(X = \u03c3ei) = Pr(X = \u2212\u03c3ei) = 1\u2212 p\n2(d\u2212 1) for i > 1\nThen X has mean zero and covariance diag(p, \u03c32(1\u2212 p)/(d\u2212 1), . . . , \u03c32(1\u2212 p)/(d\u2212 1)). We will assume that p and \u03c3 are chosen so that p > \u03c32(1 \u2212 p)/(d\u2212 1); in our notation, the top eigenvalues are then \u03bb1 = p and \u03bb2 = \u03c32(1 \u2212 p)/(d\u2212 1), and the target vector is v\u2217 = e1.\nIf Vn is ever orthogonal to some ei, it will remain so forever. This is because both the Krasulina and Oja updates have the following properties:\nVn\u22121 \u00b7Xn = 0 =\u21d2 Vn = Vn\u22121 Vn\u22121 \u00b7Xn 6= 0 =\u21d2 Vn \u2208 span(Vn\u22121, Xn).\nIf Vno is initialized to a random data point, then with probability 1 \u2212 p, it will be assigned to some ei with i > 1, and will converge to a multiple of that same ei rather than to e1. Likewise, if it is initialized to the average of \u2264 1/p data points, then with constant probability it will be orthogonal to e1 and remain so always.\nSetting Vno to a random unit vector avoids this problem. However, there are doubtless cases, for instance when the data has intrinsic dimension \u226a d, in which a better initializer is possible."}, {"heading": "1.3 The setting of the learning rate", "text": "In order to get a sense of what rates of convergence we might expect, let\u2019s return to the example of a random vector X with 2d possible values. In the Oja update Vn = Vn\u22121 + \u03b3nXnXTn Vn\u22121, we can ignore normalization if we are merely interested in the progress of the potential function \u03a8n. Since the Xn correspond to coordinate directions, each update changes just one coordinate of V :\nXn = \u00b1e1 =\u21d2 Vn,1 = Vn\u22121,1(1 + \u03b3n) Xn = \u00b1\u03c3ei =\u21d2 Vn,i = Vn\u22121,i(1 + \u03c32\u03b3n)\nRecall that we initialize Vno to a random vector from the unit sphere. For simplicity, let\u2019s just suppose that no = 0 and that this initial value is the all-ones vector (again, we don\u2019t have to worry about normalization). On each iteration the first coordinate is updated with probability exactly p = \u03bb1, and thus\nE[Vn,1] = (1 + \u03bb1\u03b31)(1 + \u03bb1\u03b32) \u00b7 \u00b7 \u00b7 (1 + \u03bb1\u03b3n) \u223c exp(\u03bb1(\u03b31 + \u00b7 \u00b7 \u00b7+ \u03b3n)) \u223c nc\u03bb1\nsince \u03b3n = c/n. Likewise, for i > 1,\nE[Vn,i] = (1 + \u03bb2\u03b31)(1 + \u03bb2\u03b32) \u00b7 \u00b7 \u00b7 (1 + \u03bb2\u03b3n) \u223c nc\u03bb2 . If all goes according to expectation, then at time n,\n\u03a8n = 1\u2212 V 2n,1 \u2016Vn\u20162 \u223c 1\u2212 n 2c\u03bb1 n2c\u03bb1 + (d\u2212 1)n2c\u03bb2 \u223c d\u2212 1 n2c(\u03bb1\u2212\u03bb2) .\n(This is all very rough, but can be made precise by obtaining concentration bounds for lnVn,i.) From this, we can see that it is not possible to achieve a O(1/n) rate unless c \u2265 1/(2(\u03bb1 \u2212 \u03bb2)). Therefore, we will assume this when stating our final results, although most of our analysis is in terms of general \u03b3n. An interesting practical question, to which we do not have an answer, is how one would empirically set c without prior knowledge of the eigenvalue gap."}, {"heading": "1.4 Nested sample spaces", "text": "For n \u2265 no, let Fn denote the sigma-field of all outcomes up to and including time n: Fn = \u03c3(Vno , Xno+1, . . . , Xn). We start by showing that\nE[\u03a8n|Fn\u22121] \u2264 \u03a8n\u22121(1\u2212 2\u03b3n(\u03bb1 \u2212 \u03bb2)(1\u2212\u03a8n\u22121)) +O(\u03b32n). Initially \u03a8n is likely to be close to 1. For instance, if the initial Vno is picked uniformly at random from the surface of the unit sphere in Rd, then we\u2019d expect \u03a8no \u2248 1 \u2212 1/d. This means that the initial rate of decrease is very small, because of the (1 \u2212\u03a8n\u22121) term. To deal with this, we divide the analysis into epochs: the first takes \u03a8n from 1\u2212 1/d to 1\u2212 2/d, the second from 1\u22122/d to 1\u22124/d, and so on until \u03a8n finally drops below 1/2. We use martingale large deviation bounds to bound the length of each epoch, and also to argue that \u03a8n does not regress. In particular, we establish a sequence of times nj such that (with high probability)\nsup n\u2265nj\n\u03a8n \u2264 1\u2212 2j\nd . (1)\nThe analysis of each epoch uses martingale arguments, but at the same time, assumes that \u03a8n remains bounded above. Combining the two requires a careful specification of the sample space at each step. Let \u2126 denote the sample space of all realizations (vno , xno+1, xno+2, . . .), and P the probability distribution on these sequences. For any \u03b4 > 0, we define a nested sequence of spaces \u2126 \u2283 \u2126\u2032no \u2283 \u2126\u2032no+1 \u2283 \u00b7 \u00b7 \u00b7 such that each \u2126\u2032n is Fn\u22121-measurable, has probability P (\u2126\u2032n) \u2265 1 \u2212 \u03b4, and moreover consists exclusively of realizations \u03c9 \u2208 \u2126 that satisfy the constraints (1) up to and including time n \u2212 1. We can then build martingale arguments by restricting attention to \u2126\u2032n when computing the conditional expectations of quantities at time n."}, {"heading": "1.5 Main result", "text": "We make the following assumptions:\n(A1) The Xn \u2208 Rd are i.i.d. with mean zero and covariance A. (A2) There is a constant B such that \u2016Xn\u20162 \u2264 B. (A3) The eigenvalues \u03bb1 \u2265 \u03bb2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd of A satisfy \u03bb1 > \u03bb2. (A4) The step sizes are of the form \u03b3n = c/n.\nUnder these conditions, we get the following rate of convergence for the Krasulina update.\nTheorem 1.1. There are absolute constants Ao, A1 > 0 and 1 < a < 4 for which the following holds. Pick any 0 < \u03b4 < 1, and any co > 2. Set the step sizes to \u03b3n = c/n, where c = co/(2(\u03bb1 \u2212 \u03bb2)), and set the starting time to no \u2265 (AoB2c2d2/\u03b44) ln(1/\u03b4). Then there is a nested sequence of subsets of the sample space \u2126 \u2283 \u2126\u2032no \u2283 \u2126\u2032no+1 \u2283 \u00b7 \u00b7 \u00b7 such that for any n \u2265 no, we have:\nP (\u2126\u2032n) \u2265 1\u2212 \u03b4 and\nEn [ (Vn \u00b7 v\u2217)2 \u2016Vn\u20162 ] \u2265 1\u2212 ( c2B2eco/no 2(co \u2212 2) ) 1 n+ 1 \u2212A1 ( d \u03b42 )a ( no + 1 n+ 1 )co/2 ,\nwhere En denotes expectation restricted to \u2126\u2032n.\nSince co > 2, this bound is of the form En[\u03a8n] = O(1/n).\nThe result above also holds for the Oja update up to absolute constants.\nWe also remark that a small modification to the final step in the proof of the above yields a rate of En [\u03a8n] = O(n\n\u2212co/2) for co < 2, with an identical definition of En [\u03a8n]. The details are in the proof, in Appendix D.2."}, {"heading": "1.6 Related work", "text": "There is an extensive line of work analyzing PCA from the statistical perspective, in which the convergence of various estimators is characterized under certain conditions, including generative models of the data [5] and various assumptions on the covariance matrix spectrum [14, 4] and eigenvalue spacing [17]. Such works do provide finite-sample guarantees, but they apply only to the batch case and/or are computationally intensive, rather than considering an efficient incremental algorithm.\nAmong incremental algorithms, the work of Warmuth and Kuzmin [15] describes and analyzes worst-case online PCA, using an experts-setting algorithm with a super-quadratic per-iteration cost. More efficient general-purpose incremental PCA algorithms have lacked finite-sample analyses [2]. There have been recent attempts to remedy this situation by relaxing the nonconvexity inherent in the problem [3] or making generative assumptions [8]. The present paper directly analyzes the oldest known incremental PCA algorithms under relatively mild assumptions."}, {"heading": "2 Outline of proof", "text": "We now sketch the proof of Theorem 1.1; almost all the details are relegated to the appendix.\nRecall that for n \u2265 no, we take Fn to be the sigma-field of all outcomes up to and including time n, that is, Fn = \u03c3(Vno , Xno+1, . . . , Xn).\nAn additional piece of notation: we will use u\u0302 to denote u/\u2016u\u2016, the unit vector in the direction of u \u2208 Rd. Thus, for instance, the Rayleigh quotient can be written G(v) = v\u0302TAv\u0302."}, {"heading": "2.1 Expected per-step change in potential", "text": "We first bound the expected improvement in \u03a8n in each step of the Krasulina or Oja algorithms. Theorem 2.1. For any n > no, we can write \u03a8n \u2264 \u03a8n\u22121 + \u03b2n \u2212 Zn, where\n\u03b2n =\n{ \u03b32nB\n2/4 (Krasulina) 5\u03b32nB 2 + 2\u03b33nB 3 (Oja)\nand where Zn is a Fn-measurable random variable with the following properties:\n\u2022 E[Zn|Fn\u22121] = 2\u03b3n(V\u0302n\u22121 \u00b7 v\u2217)2(\u03bb1 \u2212G(Vn\u22121)) \u2265 2\u03b3n(\u03bb1 \u2212 \u03bb2)\u03a8n\u22121(1\u2212\u03a8n\u22121) \u2265 0.\n\u2022 |Zn| \u2264 4\u03b3nB.\nThe theorem follows from Lemmas A.4 and A.5 in the appendix. Its characterization of the two estimators is almost identical, and for simplicity we will henceforth deal only with Krasulina\u2019s estimator. All the subsequent results hold also for Oja\u2019s method, up to constants."}, {"heading": "2.2 A large deviation bound for \u03a8n", "text": "We know from Theorem 2.1 that \u03a8n \u2264 \u03a8n\u22121 + \u03b2n \u2212 Zn, where \u03b2n is non-stochastic and Zn is a quantity of positive expected value. Thus, in expectation, and modulo a small additive term, \u03a8n decreases monotonically. However, the amount of decrease at the nth time step can be arbitrarily small when \u03a8n is close to 1. Thus, we need to show that \u03a8n is eventually bounded away from 1, i.e. there exists some \u01ebo > 0 and some time no such that for any n \u2265 no, we have \u03a8n \u2264 1\u2212 \u01ebo. Recall from the algorithm specification that we advance the clock so as to skip the pre-no phase. Given this, what can we expect \u01ebo to be? If the initial estimate Vno is a random unit vector, then E[\u03a8no ] = 1\u2212 1/d and, roughly speaking, Pr(\u03a8no > 1\u2212 \u01eb/d) = O( \u221a \u01eb). If no is sufficiently large, then \u03a8n may subsequently increase a little bit, but not by very much. In this section, we establish the following bound.\nTheorem 2.2. Suppose the initial estimate Vno is chosen uniformly at random from the surface of the unit sphere in Rd. Assume also that the step sizes are of the form \u03b3n = c/n, for some constant c > 0. Then for any 0 < \u01eb < 1, if no \u2265 2B2c2d2/\u01eb2, we have\nPr ( sup n\u2265no \u03a8n \u2265 1\u2212 \u01eb d ) \u2264 \u221a 2e\u01eb.\nTo prove this, we start with a simple recurrence for the moment-generating function of \u03a8n. Lemma 2.3. Consider a filtration (Fn) and random variables Yn, Zn \u2208 Fn such that there are two sequences of nonnegative constants, (\u03b2n) and (\u03b6n), for which:\n\u2022 Yn \u2264 Yn\u22121 + \u03b2n \u2212 Zn.\n\u2022 Each Zn takes values in an interval of length \u03b6n.\nThen for any t > 0, we have E[etYn |Fn\u22121] \u2264 exp(t(Yn\u22121 \u2212 E[Zn|Fn\u22121] + \u03b2n + t\u03b62n/8)).\nThis relation shows how to define a supermartingale based on etYn , from which we can derive a large deviation bound on Yn. Lemma 2.4. Assume the conditions of Lemma 2.3, and also that E[Zn|Fn\u22121] \u2265 0. Then, for any integer m and any \u2206, t > 0,\nPr ( sup n\u2265m Yn \u2265 \u2206 ) \u2264 E[etYm ] exp ( \u2212 t ( \u2206\u2212 \u2211\n\u2113>m\n(\u03b2\u2113 + t\u03b6 2 \u2113 /8)\n)) .\nIn order to apply this to the sequence (\u03a8n), we need to first calculate the moment-generating function of its starting value \u03a8no . Lemma 2.5. Suppose a vector V is picked uniformly at random from the surface of the unit sphere in Rd, where d \u2265 3. Define Y = 1\u2212 (V 21 )/\u2016V \u20162. Then, for any t > 0,\nEetY \u2264 et \u221a\nd\u2212 1 2t .\nPutting these pieces together yields Theorem 2.2."}, {"heading": "2.3 Intermediate epochs of improvement", "text": "We have seen that, for suitable \u01eb and no, it is likely that \u03a8n \u2264 1 \u2212 \u01eb/d for all n \u2265 no. We now define a series of epochs in which 1\u2212\u03a8n successively doubles, until \u03a8n finally drops below 1/2. To do this, we specify intermediate goals (no, \u01ebo), (n1, \u01eb1), (n2, \u01eb2), . . . , (nJ , \u01ebJ), where no < n1 < \u00b7 \u00b7 \u00b7 < nJ and \u01ebo < \u01eb1 < \u00b7 \u00b7 \u00b7 < \u01ebJ = 1/2, with the intention that:\nFor all 0 \u2264 j \u2264 J , we have sup n\u2265nj \u03a8n \u2264 1\u2212 \u01ebj . (2)\nOf course, this can only hold with a certain probability.\nLet \u2126 denote the sample space of all realizations (vno , xno+1, xno+2, . . .), and P the probability distribution on these sequences. We will show that, for a certain choice of {(nj , \u01ebj)}, all J + 1 constraints (2) can be met by excluding just a small portion of \u2126.\nWe consider a specific realization \u03c9 \u2208 \u2126 to be good if it satisfies (2). Call this set \u2126\u2032: \u2126\u2032 = {\u03c9 \u2208 \u2126 : sup\nn\u2265nj\n\u03a8n(\u03c9) \u2264 1\u2212 \u01ebj for all 0 \u2264 j \u2264 J}.\nFor technical reasons, we also need to look at realizations that are good up to time n\u22121. Specifically, for each n, define\n\u2126\u2032n = {\u03c9 \u2208 \u2126 : sup nj\u2264\u2113<n \u03a8\u2113(\u03c9) \u2264 1\u2212 \u01ebj for all 0 \u2264 j \u2264 J}.\nCrucially, this is Fn\u22121-measurable. Also note that \u2126\u2032 = \u22c2\nn>no \u2126\u2032n.\nWe can talk about expectations under the distribution P restricted to subsets of \u2126. In particular, let Pn be the restriction of P to \u2126\u2032n; that is, for any A \u2282 \u2126, we have Pn(A) = P (A\u2229\u2126\u2032n)/P (\u2126\u2032n). As for expectations with respect to Pn, for any function f : \u2126 \u2192 R, we define\nEnf = 1\nP (\u2126\u2032n)\n\u222b\n\u2126\u2032n\nf(\u03c9)P (d\u03c9).\nHere is the main result of this section. Theorem 2.6. Assume that \u03b3n = c/n, where c = co/(2(\u03bb1 \u2212 \u03bb2)) and co > 0. Pick any 0 < \u03b4 < 1 and select a schedule (no, \u01ebo), . . . , (nJ , \u01ebJ) that satisfies the conditions\n\u01ebo = \u03b42 8ed , and 3 2\u01ebj \u2264 \u01ebj+1 \u2264 2\u01ebj for 0 \u2264 j < J , and \u01ebJ\u22121 \u2264 14 (nj+1 + 1) \u2265 e5/co(nj + 1) for 0 \u2264 j < J (3)\nas well as no \u2265 (20c2B2/\u01eb2o) ln(4/\u03b4). Then Pr(\u2126\u2032) \u2265 1\u2212 \u03b4.\nThe first step towards proving this theorem is bounding the moment-generating function of \u03a8n in terms of that of \u03a8n\u22121. Lemma 2.7. Suppose n > nj . Suppose also that \u03b3n = c/n, where c = co/(2(\u03bb1 \u2212 \u03bb2)). Then for any t > 0,\nEn[e t\u03a8n ] \u2264 En [ exp ( t\u03a8n\u22121 ( 1\u2212 co\u01ebj\nn\n))] exp\n( c2B2t(1 + 32t)\n4n2\n) .\nWe would like to use this result to bound En[\u03a8n] in terms of Em[\u03a8m] for m < n. The shift in sample spaces is easily handled using the following observation. Lemma 2.8. If g : R \u2192 R is nondecreasing, then En[g(\u03a8n\u22121)] \u2264 En\u22121[g(\u03a8n\u22121)] for any n > no.\nA repeated application of Lemmas 2.7 and 2.8 yields the following. Lemma 2.9. Suppose that conditions (3) hold. Then for 0 \u2264 j < J and any t > 0,\nEnj+1 [e t\u03a8nj+1 ] \u2264 exp ( t(1\u2212 \u01ebj+1)\u2212 t\u01ebj + tc2B2(1 + 32t)\n4\n( 1\nnj \u2212 1 nj+1\n)) .\nNow that we have bounds on the moment-generating functions of intermediate \u03a8n, we can apply martingale deviation bounds, as in Lemma 2.4, to obtain the following, from which Theorem 2.6 ensues. Lemma 2.10. Assume conditions (3) hold. Pick any 0 < \u03b4 < 1, and set no \u2265 (20c2B2/\u01eb2o) ln(4/\u03b4). Then\nJ\u2211\nj=1\nPnj ( sup n\u2265nj \u03a8n > 1\u2212 \u01ebj ) \u2264 \u03b4 2 ."}, {"heading": "2.4 The final epoch", "text": "Recall the definition of the intermediate goals (nj , \u01ebj) in (2), (3). The final epoch is the period n \u2265 nJ , at which point \u03a8n \u2264 1/2. The following consequence of Lemmas A.4 and 2.8 captures the rate at which \u03a8 decreases during this phase.\nLemma 2.11. For all n > nJ ,\nEn[\u03a8n] \u2264 (1\u2212 \u03b1n)En\u22121[\u03a8n\u22121] + \u03b2n, where \u03b1n = (\u03bb1 \u2212 \u03bb2)\u03b3n and \u03b2n = (B2/4)\u03b32n.\nBy solving this recurrence relation, and piecing together the various epochs, we get the overall convergence result of Theorem 1.1.\nNote that Lemma 2.11 closely resembles the recurrence relation followed by the squaredL2 distance from the optimum of stochastic gradient descent (SGD) on a strongly convex function [11]. As \u03a8n \u2192 0, the incremental PCA algorithms we study have convergence rates of the same form as SGD in this scenario."}, {"heading": "3 Experiments", "text": "When performing PCA in practice with massive d and a large/growing dataset, an incremental method like that of Krasulina or Oja remains practically viable, even as quadratic-time and -memory algorithms become increasingly impractical. Arora et al. [2] have a more complete discussion of the empirical necessity of incremental PCA algorithms, including a version of Oja\u2019s method which is shown to be extremely competitive in practice.\nSince the efficiency benefits of these types of algorithms are well understood, we now instead focus on the effect of the learning rate on the performance of Oja\u2019s algorithm (results for Krasulina\u2019s are extremely similar). We use the CMU PIE faces [13], consisting of 11554 images of size 32 \u00d7 32, as a prototypical example of a dataset with most of its variance captured by a few PCs, as shown in\nFig. 1. We set n0 = 0.\nWe expect from Theorem 1.1 and the discussion in the introduction that varying c (the constant in the learning rate) will influence the overall rate of convergence. In particular, if c is low, then halving it can be expected to halve the exponent of n, and the slope of the log-log convergence graph (ref. the remark after Thm. 1.1). This is exactly what occurs in practice, as illustrated in Fig. 2. The dotted line in that figure is a convergence rate of 1/n, drawn as a guide.\n0 5 10 15 20 25 30 0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\n5000\nComponent Number\nE ig\nen va\nlu e\nPIE Dataset Covariance Spectrum\n10 0\n10 1\n10 2\n10 3\n10 4\n10 5\n10 \u22126\n10 \u22125\n10 \u22124\n10 \u22123\n10 \u22122\n10 \u22121\n10 0\nIteration Number\nR ec\non st\nru ct\nio n\nE rr\nor\nOja Subspace Rule Dependence on c\nc=6 c=3 c=1.5 c=1 c=0.666 c=0.444 c=0.296\nFigures 1 and 2."}, {"heading": "4 Open problems", "text": "Several fundamental questions remain unanswered. First, the convergence rates of the two incremental schemes depend on the multiplier c in the learning rate \u03b3n. If it is too low, convergence will be slower than O(1/n). If it is too high, the constant in the rate of convergence will be large. Is there a simple and practical scheme for setting c?\nSecond, what can be said about incrementally estimating the top p eigenvectors, for p > 1? Both methods we consider extend easily to this case [10]; the estimate at time n is a d \u00d7 p matrix Vn whose columns correspond to the eigenvectors, with the invariant V Tn Vn = Ip always maintained. In Oja\u2019s algorithm, for instance, when a new data point Xn \u2208 Rd arrives, the following update is performed:\nWn = Vn\u22121 + \u03b3nXnX T n Vn\u22121\nVn = orth(Wn)\nwhere the second step orthonormalizes the columns, for instance by Gram-Schmidt. It would be interesting to characterize the rate of convergence of this scheme.\nFinally, our analysis applies to a modified procedure in which the starting time no is artificially set to a large constant. This seems unnecessary in practice, and it would be useful to extend the analysis to the case where no = 0."}, {"heading": "Acknowledgments", "text": "The authors are grateful to the National Science Foundation for support under grant IIS-1162581."}, {"heading": "A Expected per-step change in potential", "text": ""}, {"heading": "A.1 The change in potential of Krasulina\u2019s update", "text": "Write Krasulina\u2019s update equation as\nVn = Vn\u22121 + \u03b3n\u03ben \u03ben = ( XnX T n \u2212 V\u0302 Tn\u22121XnXTn V\u0302n\u22121Id ) Vn\u22121\nWe start with some basic observations.\nLemma A.1. For all n > no,\n(a) \u03ben is orthogonal to Vn\u22121.\n(b) \u2016\u03ben\u20162 \u2264 B2\u2016Vn\u22121\u20162/4.\n(c) E[\u03ben|Fn\u22121] = AVn\u22121 \u2212G(Vn\u22121)Vn\u22121.\n(d) \u2016Vn\u2016 \u2265 \u2016Vn\u22121\u2016.\nProof. For (a), let X\u22a5n denote the component of Xn orthogonal to Vn\u22121. Then\n\u03ben = (Vn\u22121\u00b7Xn)Xn\u2212(V\u0302n\u22121\u00b7Xn)2Vn\u22121 = (Vn\u22121\u00b7Xn)(Xn\u2212(V\u0302n\u22121\u00b7Xn)V\u0302n\u22121) = (Vn\u22121\u00b7Xn)X\u22a5n .\nFor (b), note from the previous formulation that \u2016\u03ben\u20162 = (Vn\u22121\u00b7Xn)2\u2016X\u22a5n \u20162 \u2264 \u2016Vn\u22121\u20162\u2016Xn\u20164/4. Part (c) follows directly from E[XnXTn |Fn\u22121] = A. For (d), we use \u2016Vn\u20162 = \u2016Vn\u22121 + \u03b3n\u03ben\u20162 = \u2016Vn\u22121\u20162 + \u03b32n\u2016\u03ben\u20162 \u2265 \u2016Vn\u22121\u20162.\nWe now check that (Vn \u00b7 v\u2217)2 grows in expectation with each iteration. Lemma A.2. For any n > no, we have\n(a) (Vn \u00b7 v\u2217)2 \u2265 (Vn\u22121 \u00b7 v\u2217)2 + 2\u03b3n(Vn\u22121 \u00b7 v\u2217)(\u03ben \u00b7 v\u2217).\n(b) E[\u03ben \u00b7 v\u2217|Fn\u22121] = (Vn\u22121 \u00b7 v\u2217)(\u03bb1 \u2212G(Vn\u22121)).\nProof. Part (a) follows directly from the update rule:\n(Vn \u00b7 v\u2217)2 = ((Vn\u22121 \u00b7 v\u2217) + \u03b3n(\u03ben \u00b7 v\u2217))2 \u2265 (Vn\u22121 \u00b7 v\u2217)2 + 2\u03b3n(Vn\u22121 \u00b7 v\u2217)(\u03ben \u00b7 v\u2217). Part (b) follows by substituting the expression for E[\u03ben|Fn\u22121] from Lemma A.1(c): E[\u03ben \u00b7 v\u2217|Fn\u22121] = (V Tn\u22121Av\u2217)\u2212G(Vn\u22121)(Vn\u22121 \u00b7 v\u2217) = \u03bb1(Vn\u22121 \u00b7 v\u2217)\u2212G(Vn\u22121)(Vn\u22121 \u00b7 v\u2217).\nIn order to use Lemma A.2 to bound the change in potential \u03a8n, we need to relate \u03a8n to the quantity \u03bb1 \u2212G(Vn). Lemma A.3. For any n \u2265 no, we have \u03bb1 \u2212G(Vn) \u2265 (\u03bb1 \u2212 \u03bb2)\u03a8n.\nProof. It is easiest to think of Vn in the eigenbasis of A: the component of Vn in direction v\u2217 is Vn \u00b7 v\u2217, and the orthogonal component is V \u22a5n = Vn \u2212 (Vn \u00b7 v\u2217)v\u2217. Then\nG(Vn) = V Tn AVn \u2016Vn\u20162 = (Vn \u00b7 v\u2217)2 \u2016Vn\u20162 \u03bb1 + (V \u22a5n ) TAV \u22a5n \u2016Vn\u20162 \u2264 \u03bb1(Vn \u00b7 v \u2217)2 + \u03bb2\u2016V \u22a5n \u20162 \u2016Vn\u20162 .\nTherefore,\n\u03bb1\u2212G(Vn) \u2265 \u03bb1\u2212 \u03bb1(Vn \u00b7 v\u2217)2 + \u03bb2(\u2016Vn\u20162 \u2212 (Vn \u00b7 v\u2217)2)\n\u2016Vn\u20162 = (\u03bb1\u2212\u03bb2)\n( 1\u2212 (Vn \u00b7 v \u2217)2 \u2016Vn\u20162 ) = (\u03bb1\u2212\u03bb2)\u03a8n.\nWe can now explicitly bound the expected change in \u03a8n in each iteration. Lemma A.4. For any n > no, we can write \u03a8n \u2264 \u03a8n\u22121 + \u03b2n \u2212 Zn, where \u03b2n = \u03b32nB2/4 and where Zn = 2\u03b3n(Vn\u22121 \u00b7 v\u2217)(\u03ben \u00b7 v\u2217)/\u2016Vn\u22121\u20162 is a Fn-measurable random variable with the following properties:\n\u2022 E[Zn|Fn\u22121] = 2\u03b3n(V\u0302n\u22121 \u00b7 v\u2217)2(\u03bb1 \u2212G(Vn\u22121)) \u2265 2\u03b3n(\u03bb1 \u2212 \u03bb2)\u03a8n\u22121(1\u2212\u03a8n\u22121) \u2265 0.\n\u2022 |Zn| \u2264 4\u03b3nB.\nProof. Using Lemmas A.1 and A.2(a),\n\u03a8n = \u2016Vn\u20162 \u2212 (Vn \u00b7 v\u2217)2 \u2016Vn\u20162 \u2264 \u2016Vn\u22121\u2016 2 + \u03b32n\u2016\u03ben\u20162 \u2212 (Vn \u00b7 v\u2217)2 \u2016Vn\u22121\u20162\n\u2264 1 + 1 4 \u03b32nB 2 \u2212 (Vn \u00b7 v \u2217)2 \u2016Vn\u22121\u20162\n\u2264 1 + 1 4 \u03b32nB 2 \u2212 (Vn\u22121 \u00b7 v \u2217)2 + 2\u03b3n(Vn\u22121 \u00b7 v\u2217)(\u03ben \u00b7 v\u2217) \u2016Vn\u22121\u20162 = \u03a8n\u22121 + 1\n4 \u03b32nB\n2 \u2212 2\u03b3n (Vn\u22121 \u00b7 v\u2217)(\u03ben \u00b7 v\u2217)\n\u2016Vn\u22121\u20162 ,\nwhich is \u03a8n\u22121+\u03b2n\u2212Zn. The conditional expectation ofZn can be determined from Lemma A.2(b):\nE[Zn|Fn\u22121] = 2\u03b3n(Vn\u22121 \u00b7 v\u2217)\n\u2016Vn\u22121\u20162 E[\u03ben \u00b7 v\u2217|Fn\u22121] = 2\u03b3n(V\u0302n\u22121 \u00b7 v\u2217)2(\u03bb1 \u2212G(Vn\u22121))\nand this can be lower-bounded using Lemma A.3.\nFinally, we need to determine the range of possible values of Zn. By expanding \u03ben, we get\nZn = 2\u03b3n(V\u0302n\u22121 \u00b7 v\u2217) ( (Xn \u00b7 v\u2217)(Xn \u00b7 V\u0302n\u22121)\u2212 (V\u0302n\u22121 \u00b7 v\u2217)(Xn \u00b7 V\u0302n\u22121)2 ) .\nSince \u2016Xn\u20162 \u2264 B, we see that Zn must lie in the range \u00b14\u03b3nB."}, {"heading": "A.2 The change in potential of the Oja update", "text": "Recall the Oja update:\nVn = Vn\u22121 + \u03b3nXnX\nT n Vn\u22121\n\u2016Vn\u22121 + \u03b3nXnXTn Vn\u22121\u2016 .\nSince our bounds are on the potential function \u03a8n, which is insensitive to the length of Vn, we can skip the normalization, and instead just consider the update rule\nVn = Vn\u22121 + \u03b3nXnX T n Vn\u22121.\nThe final bounds, as well as many of the intermediate results, are almost exactly the same as for Krasulina\u2019s estimator. Here is the analogue of Lemma A.4. Lemma A.5. For any n > no, we can write \u03a8n \u2264 \u03a8n\u22121 \u2212 Zn + \u03b2n, where Zn is the same as in Lemma A.4 and \u03b2n = 5\u03b32nB 2 + 2\u03b33nB 3.\nProof. This is a series of calculations. First,\n(Vn \u00b7 v\u2217)2 = ((Vn\u22121 \u00b7 v\u2217) + \u03b3n(V Tn\u22121XnXTn v\u2217))2\n\u2265 (Vn\u22121 \u00b7 v\u2217)2 + 2\u03b3n(Vn\u22121 \u00b7 v\u2217)(V Tn\u22121XnXTn v\u2217). Similarly,\n\u2016Vn\u20162 = \u2016Vn\u22121 + \u03b3nXnXTn Vn\u22121\u20162\n= \u2016Vn\u22121\u20162 + \u03b32n\u2016XnXTn Vn\u22121\u20162 + 2\u03b3n(Vn\u22121 \u00b7Xn)2 \u2264 \u2016Vn\u22121\u20162(1 + \u03b32nB2 + 2\u03b3n(V\u0302n\u22121 \u00b7Xn)2)\nwhere we have used \u2016Xn\u20162 \u2264 B. Combining these, (Vn \u00b7 v\u2217)2 \u2016Vn\u20162 \u2265 (Vn\u22121 \u00b7 v \u2217)2 + 2\u03b3n(Vn\u22121 \u00b7 v\u2217)(V Tn\u22121XnXTn v\u2217)\n\u2016Vn\u22121\u20162(1 + \u03b32nB2 + 2\u03b3n(V\u0302n\u22121 \u00b7Xn)2)\n= (V\u0302n\u22121 \u00b7 v\u2217)2 + 2\u03b3n(V\u0302n\u22121 \u00b7 v\u2217)(V\u0302 Tn\u22121XnXTn v\u2217)\n1 + \u03b32nB 2 + 2\u03b3n(V\u0302n\u22121 \u00b7Xn)2\n\u2265 ( (V\u0302n\u22121 \u00b7 v\u2217)2 + 2\u03b3n(V\u0302n\u22121 \u00b7 v\u2217)(V\u0302 Tn\u22121XnXTn v\u2217) )( 1\u2212 \u03b32nB2 \u2212 2\u03b3n(V\u0302n\u22121 \u00b7Xn)2 ) \u2265 (V\u0302n\u22121 \u00b7 v\u2217)2 + 2\u03b3n(V\u0302n\u22121 \u00b7 v\u2217) ( V\u0302 Tn\u22121XnX T n v \u2217 \u2212 (V\u0302n\u22121 \u00b7 v\u2217)(V\u0302n\u22121 \u00b7Xn)2 ) \u2212 5\u03b32nB2 \u2212 2\u03b33nB3\nwhere the final step involves some extra algebra that we have omitted. The lemma now follows by invoking \u03a8n = 1\u2212 (V\u0302n \u00b7 v\u2217)2.\nB A large deviation bound for \u03a8 n"}, {"heading": "B.1 Proof of Lemma 2.3", "text": "For any t > 0,\nE [ etYn |Fn\u22121 ] \u2264 E [ et(Yn\u22121+\u03b2n\u2212Zn)|Fn\u22121 ]\n= et(Yn\u22121+\u03b2n)E [ e\u2212tZn |Fn\u22121 ] = et(Yn\u22121+\u03b2n) E [ e\u2212tE[Zn|Fn\u22121]e\u2212t(Zn\u2212E[Zn|Fn\u22121])|Fn\u22121 ] \u2264 et(Yn\u22121+\u03b2n\u2212E[Zn|Fn\u22121]) E [ e\u2212t(Zn\u2212E[Zn|Fn\u22121])|Fn\u22121 ] .\nWe bound the last expected value using Hoeffding\u2019s lemma: E[etW ] \u2264 et2(b\u2212a)2/8 for any random variable W of mean zero and range [a, b]."}, {"heading": "B.2 Proof of Lemma 2.4", "text": "By Lemma 2.3,\nE [ etYn |Fn\u22121 ] \u2264 exp ( tYn\u22121 + t\u03b2n +\nt2\u03b62n 8\n) .\nNow let\u2019s define an appropriate martingale. Let \u03c4n = \u2211 \u2113>n(\u03b2\u2113+t\u03b6 2 \u2113 /8), and let Mn = exp(t(Yn+ \u03c4n)). Thus Mn \u2208 Fn, and\nE[Mn|Fn\u22121] = E[etYn |Fn\u22121] exp(t\u03c4n) \u2264 exp ( tYn\u22121 + t\u03b2n +\nt2\u03b62n 8 + t\u03c4n\n) = Mn\u22121.\nThus (Mn) is a positive-valued supermartingale adapted to (Fn). A version of Doob\u2019s martingale inequality\u2014see, for instance, page 274 of [6]\u2014then says that for any m, we have Pr(supn\u2265m Mn \u2265 \u03b4) \u2264 (EMm)/\u03b4. Using this, we see that for any \u2206 > 0,\nPr ( sup n\u2265m Yn \u2265 \u2206 ) \u2264 Pr ( sup n\u2265m Yn + \u03c4n \u2265 \u2206 ) = Pr ( sup n\u2265m Mn \u2265 et\u2206 )\n\u2264 EMm et\u2206 = exp(\u2212t(\u2206\u2212 \u03c4m))EetYm"}, {"heading": "B.3 Proof of Lemma 2.5", "text": "It is well known that V can be chosen by picking d values Z = (Z1, . . . , Zd) independently from the standard normal distribution and then setting V = Z/\u2016Z\u2016. Therefore,\nY = Z22 + \u00b7 \u00b7 \u00b7+ Z2d\nZ21 + (Z 2 2 + \u00b7 \u00b7 \u00b7+ Z2d)\n= W1\nW1 +W2 ,\nwhere W1 is drawn from a chi-squared distribution with d\u2212 1 degrees of freedom and W2 is drawn independently from a chi-squared distribution with one degree of freedom. This characterization implies that Y follows the Beta((d\u2212 1)/2, 1/2) distribution: specifically, for any 0 < y < 1,\nPr(Y = y) = \u0393(d2 )\n\u0393(d\u221212 )\u0393( 1 2 )\ny(d\u22123)/2(1\u2212 y)\u22121/2.\nThe moment-generating function of this distribution is\nEetY = \u0393(d2 )\n\u0393(d\u221212 )\u0393( 1 2 )\n\u222b 1\n0\netyy(d\u22123)/2(1\u2212 y)\u22121/2dy.\nThere isn\u2019t a closed form for this, but an upper bound on the integral can be obtained. Assuming d \u2265 3,\n\u222b 1\n0\netyy(d\u22123)/2(1 \u2212 y)\u22121/2dy \u2264 \u222b 1\n0\nety(1\u2212 y)\u22121/2dy\n= et\u221a t\n\u222b t\n0\ne\u2212zz\u22121/2dz\n\u2264 e t\n\u221a t\n\u222b \u221e\n0\ne\u2212zz\u22121/2dz = et\u221a t \u0393(1/2),\nwhere the second step uses a change of variable z = t(1 \u2212 y), and the fourth uses the definition of the gamma function. To finish up, we use the inequality \u0393(z+1/2) \u2264 \u221az \u0393(z) (Lemma B.1) to get\nEetY \u2264 \u0393( d 2 )\n\u0393(d\u221212 )\net\u221a t\n\u2264 et \u221a\nd\u2212 1 2t .\nThe following inequality is doubtless standard; we give a short proof here because we are unable to find a reference.\nLemma B.1. For any z > 0,\n\u0393 ( z + 1\n2\n) \u2264 \u221a z \u0393(z).\nProof. Suppose a random variable T > 0 is drawn according to the density Pr(T = t) \u221d tz\u22121e\u2212t. Let\u2019s compute ET and E \u221a T :\nET =\n\u222b\u221e 0\ntze\u2212tdt\u222b\u221e 0 tz\u22121e\u2212tdt = \u0393(z + 1) \u0393(z) = z\nE \u221a T =\n\u222b\u221e 0 t\nz\u22121/2e\u2212tdt\u222b\u221e 0 t z\u22121e\u2212tdt = \u0393(z + 1/2) \u0393(z) ,\nwhere we have used the standard fact \u0393(z + 1) = z\u0393(z). By concavity of the square root function, we know that E \u221a T \u2264 \u221a ET . This yields the lemma."}, {"heading": "B.4 Proof of Theorem 2.2", "text": "From Lemma A.4(a), we have \u03a8n \u2264 \u03a8n\u22121+\u03b2n\u2212Zn, where \u03b2n = \u03b32nB2/4, and E[Zn|Fn\u22121] \u2265 0, and Zn lies in an interval of length \u03b6n = 8\u03b3nB. We can thus directly apply the first deviation bound of Lemma 2.4.\nSince \u2211\n\u2113>n\n\u03b32n = c 2 \u2211\n\u2113>n\n1 \u21132 \u2264 c2\n\u222b \u221e\nn\ndx x2 =\nc2 n ,\nwe see that for any t > 0,\n\u2211\n\u2113>no\n( \u03b2\u2113 +\nt\u03b62\u2113 8\n) = \u2211\n\u2113>no\n( B2\n4 \u03b32\u2113 + 8B 2t\u03b32\u2113\n) \u2264 B 2c2\n4no (1 + 32t).\nTo make this \u2264 \u01eb/d, it suffices to take no \u2265 B2c2d(1 + 32t)/(4\u01eb), whereupon Lemma 2.4 yields\nPr ( sup n\u2265no \u03a8n \u2265 1\u2212 \u01eb d ) \u2264 E[exp(t\u03a8no)]e\u2212t(1\u2212(\u01eb/d)\u2212(\u01eb/d))\n\u2264 et \u221a d\n2t e\u2212t(1\u2212(2\u01eb/d)) = e2\u01ebt/d\n\u221a d\n2t .\nwhere the last step uses Lemma 2.5. The result follows by taking t = d/(4\u01eb).\nC Intermediate epochs of improvement"}, {"heading": "C.1 Proof of Lemma 2.7", "text": "Lemma A.4 establishes an inequality \u03a8n \u2264 \u03a8n\u22121 \u2212 Zn + \u03b2n as well as a lower bound on E[Zn|Fn\u22121], where Zn is a random variable that lies in an interval of length \u03b6n = 8\u03b3nB. From Lemma 2.3, we then have\nE[et\u03a8n |Fn\u22121] \u2264 exp ( t(\u03a8n\u22121 \u2212 E[Zn|Fn\u22121] + \u03b2n + t\u03b62n/8) )\n\u2264 exp ( t(\u03a8n\u22121 \u2212 2\u03b3n(\u03bb1 \u2212 \u03bb2)\u03a8n\u22121(1\u2212\u03a8n\u22121) + \u03b32nB2(1 + 32t)/4) ) = exp ( t(\u03a8n\u22121 \u2212 co\u03a8n\u22121(1 \u2212\u03a8n\u22121)/n+ c2B2(1 + 32t)/4n2) )\nFor any \u03c9 \u2208 \u2126\u2032n, we have \u03a8n\u22121(\u03c9) \u2264 1\u2212 \u01ebj . Taking expectations over \u2126\u2032n, we get the lemma."}, {"heading": "C.2 Proof of Lemma 2.8", "text": "Let j be the largest index such that nj < n. Then\n\u03a8n\u22121(\u03c9) has value { \u2264 1\u2212 \u01ebj for \u03c9 \u2208 \u2126\u2032n > 1\u2212 \u01ebj for \u03c9 \u2208 \u2126\u2032n\u22121 \\\u2126\u2032n\nThus the expected value of g(\u03a8n\u22121) over \u2126\u2032n is at most the expected value over \u2126 \u2032 n\u22121."}, {"heading": "C.3 Proof of Lemma 2.9", "text": "We begin with the following Lemma.\nLemma C.1. For any n > nj and any t > 0,\nEn[e t\u03a8n ] \u2264 exp ( t(1 \u2212 \u01ebj) ( nj + 1\nn+ 1\n)co\u01ebj + tc2B2(1 + 32t)\n4\n( 1\nnj \u2212 1 n\n)) .\nProof. Define \u03b1n = 1 \u2212 (co\u01ebj/n) and \u03ben(t) = c2B2t(1 + 32t)/4n2. By Lemmas 2.7 and 2.8, for n > nj ,\nEn[e t\u03a8n ] \u2264 En[et\u03b1n\u03a8n\u22121 ] exp(\u03ben(t)) \u2264 En\u22121[e(t\u03b1n)\u03a8n\u22121 ] exp(\u03ben(t)).\nBy applying these inequalities repeatedly, for n shrinking to nj +1 (and t shrinking as well), we get\nEn[e t\u03a8n ] \u2264 Enj+1\n[ exp ( t\u03a8nj\u03b1n\u03b1n\u22121 \u00b7 \u00b7 \u00b7\u03b1nj+1 )] exp(\u03ben(t)) exp(\u03ben\u22121(t\u03b1n)) \u00b7 \u00b7 \u00b7 exp(\u03benj+1(t\u03b1n \u00b7 \u00b7 \u00b7\u03b1nj+2))\n\u2264 Enj+1 [ exp ( t\u03a8nj\u03b1n\u03b1n\u22121 \u00b7 \u00b7 \u00b7\u03b1nj+1 )] exp(\u03ben(t)) exp(\u03ben\u22121(t)) \u00b7 \u00b7 \u00b7 exp(\u03benj+1(t))\n= Enj+1\n[ exp ( t\u03a8nj ( 1\u2212 co\u01ebj\nn\n)( 1\u2212 co\u01ebj\nn\u2212 1\n) \u00b7 \u00b7 \u00b7 ( 1\u2212 co\u01ebj\nnj + 1\n))] \u00d7\nexp\n( c2B2t(1 + 32t)\n4\n( 1\nn2 +\n1 (n\u2212 1)2 + \u00b7 \u00b7 \u00b7+ 1 (nj + 1)2\n))\n\u2264 exp ( t(1\u2212 \u01ebj) exp ( \u2212co\u01ebj ( 1\nnj + 1 + \u00b7 \u00b7 \u00b7+ 1 n\n))) \u00d7\nexp\n( c2B2t(1 + 32t)\n4\n( 1\nn2 +\n1 (n\u2212 1)2 + \u00b7 \u00b7 \u00b7+ 1 (nj + 1)2\n))\nsince \u03a8nj (\u03c9) \u2264 1\u2212 \u01ebj for all \u03c9 \u2208 \u2126\u2032nj+1. We then use the summations\n1 nj + 1 + \u00b7 \u00b7 \u00b7+ 1 n \u2265\n\u222b n+1\nnj+1\ndx\nx = ln\nn+ 1\nnj + 1\n1 (nj + 1)2 + \u00b7 \u00b7 \u00b7+ 1 n2 \u2264\n\u222b n\nnj\ndx x2 = 1 nj \u2212 1 n\nto get the lemma.\nTo prove Lemma 2.9, we note that under conditions (3),\n(1\u2212 \u01ebj) ( nj + 1\nnj+1 + 1\n)co\u01ebj \u2264 e\u2212\u01ebj (e\u22125/co)co\u01ebj = e\u22126\u01ebj \u2264 1\u2212 3\u01ebj \u2264 1\u2212 \u01ebj+1 \u2212 \u01ebj .\nWe have used the fact that e\u22122x \u2264 1\u2212x for 0 \u2264 x \u2264 3/4. The rest follows by applying Lemma C.1 with n = nj+1."}, {"heading": "C.4 Proof of Lemma 2.10", "text": "Pick any 0 < j \u2264 J . We will mimic the reasoning of Theorem 2.2, being careful to define martingales only on the restricted space \u2126\u2032nj and with starting time nj . Then\nPnj ( sup n\u2265nj \u03a8n > 1\u2212 \u01ebj ) \u2264 Enj [et\u03a8nj ] exp ( \u2212t(1\u2212 \u01ebj) + tc2B2(1 + 32t) 4nj )\n\u2264 exp ( \u2212t\u01ebj\u22121 + tc2B2(1 + 32t)\n4nj\u22121\n) ,\nwhere the second step invokes Lemma 2.9.\nTo finish, we pick t = (2/\u01ebo) ln(4/\u03b4). The lower bound on no is also a lower bound on nj\u22121, and implies that tc2B2(1 + 32t)/4nj\u22121 \u2264 t\u01ebo/2, whereupon\nPnj ( sup n\u2265nj \u03a8n > 1\u2212 \u01ebj ) \u2264 exp ( \u2212 t\u01ebj\u22121 2 ) = ( \u03b4 4 )\u01ebj\u22121/\u01ebo \u2264 \u03b4 2j+1 .\nSumming over j then yields the lemma."}, {"heading": "D The final epoch", "text": ""}, {"heading": "D.1 Proof of Lemma 2.11", "text": "By Lemma A.4,\nE[\u03a8n|Fn\u22121] \u2264 \u03a8n\u22121(1\u2212 2\u03b3n(1\u2212\u03a8n\u22121)(\u03bb1 \u2212 \u03bb2)) + \u03b2n.\nFor realizations \u03c9 \u2208 \u2126\u2032n, we have \u03a8n\u22121(\u03c9) \u2264 1/2 and thus the right-hand side of the above expression is at most (1 \u2212 \u03b1n)\u03a8n\u22121 + \u03b2n. Using the fact that \u2126\u2032n is Fn\u22121-measurable, and taking expectations over \u2126\u2032n,\nEn[\u03a8n] \u2264 (1\u2212 \u03b1n)En[\u03a8n\u22121] + \u03b2n \u2264 (1\u2212 \u03b1n)En\u22121[\u03a8n\u22121] + \u03b2n,\nas claimed. The last step uses Lemma 2.8."}, {"heading": "D.2 Proof of Theorem 1.1", "text": "Define epochs (nj , \u01ebj) that satisfy the conditions of Theorem 2.6, with \u01ebJ = 1/2, and with \u01ebj+1 = 2\u01ebj whenever possible. Then J = log2 1/(2\u01ebo) and\nnJ + 1 = (no + 1) exp\n( 5J\nco\n) = (no + 1) ( 1\n2\u01ebo\n)5/(co ln 2) = (no + 1) ( 4ed\n\u03b42\n)5/(co ln 2) .\nBy Theorem 2.6, with probability > 1 \u2212 \u03b4, we have \u03a8n \u2264 1/2 for all n \u2265 nJ . More precisely, P (\u2126\u2032n) \u2265 1\u2212 \u03b4 for all n > no. By Lemma 2.11, for n > nJ ,\nEn[\u03a8n] \u2264 ( 1\u2212 a\nn\n) En\u22121[\u03a8n\u22121] + b\nn2 ,\nfor a = co/2 and b = c2B2/4. By the a > 1 case of Lemma D.1,\nEn[\u03a8n] \u2264 ( nJ + 1\nn+ 1\n)a EnJ [\u03a8nJ ] + b\na\u2212 1\n( 1 +\n1\nnJ + 1\n)a+1 1\nn+ 1\n\u2264 1 2\n( no + 1\nn+ 1\n)a ( 4ed\n\u03b42\n)5/(2 ln 2) + b a\u2212 1 exp ( a+ 1\nnJ + 1\n) 1\nn+ 1 .\nwhich upon further simplification yields the bound of Theorem 1.1 for a > 1.\n(Note that the a < 1 case of Lemma D.1 yields a rate of En [\u03a8n] = O(n\u2212a).)\nLemma D.1. Consider a nonnegative sequence (ut : t \u2265 to), such that for some constants a, b > 0 and for all t > to \u2265 0,\nut \u2264 ( 1\u2212 a\nt\n) ut\u22121 + b\nt2 .\nThen, writing the zeta function \u03b6(s) = \u2211\u221e\ni=1 i \u2212s,\nut \u2264    ( to+1 t+1 )a uto + b a\u22121 ( 1 + 1to+1 )a+1 1 t+1 , a > 1\n( to+1 t+1 )a uto + 4b\u03b6(a\u2212 2) 1(t+1)a , a < 1\nProof. Recursively applying the given recurrence for ut yields\nut \u2264 ( t\u220f\ni=to+1\n( 1\u2212 a\ni\n)) uto + t\u2211\ni=to+1\nb i2\n  t\u220f\nj=i+1\n( 1\u2212 a\nj\n)  .\nTo bound the product term, we use\nt\u220f\ni=to+1\n( 1\u2212 a\ni\n) \u2264 exp ( \u2212a \u2211\ni=to\n1\ni\n) \u2264 exp ( \u2212a \u222b t+1\nto+1\ndx\nx\n) = ( to + 1\nt+ 1\n)a .\nTherefore,\nut \u2264 ( to + 1\nt+ 1\n)a uto + t\u2211\ni=to+1\nb i2\n( i+ 1\nt+ 1\n)a\n\u2264 ( to + 1\nt+ 1\n)a uto +\nb\n(t+ 1)a\n( to + 2\nto + 1\n)2 t\u2211\ni=to+1\n(i+ 1)a\u22122.\nWe finish by bounding the summation of (i+ 1)a\u22122 by a definite integral, to get:\nt\u2211\ni=to+1\n(i+ 1)a\u22122 \u2264    1 a\u22121 (t+ 2) a\u22121 , a > 1\n\u03b6(a\u2212 2) , a < 1 ."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>We consider a situation in which we see samples Xn \u2208 R drawn i.i.d. from some<lb>distribution with mean zero and unknown covariance A. We wish to compute the<lb>top eigenvector of A in an incremental fashion with an algorithm that maintains<lb>an estimate of the top eigenvector in O(d) space, and incrementally adjusts the<lb>estimate with each new data point that arrives. Two classical such schemes are<lb>due to Krasulina (1969) and Oja (1983). We give finite-sample convergence rates<lb>for both.", "creator": "LaTeX with hyperref package"}}}