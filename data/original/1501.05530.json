{"id": "1501.05530", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jan-2015", "title": "Belief Hidden Markov Model for speech recognition", "abstract": "Speech Recognition searches to predict the spoken words automatically. These systems are known to be very expensive because of using several pre-recorded hours of speech. Hence, building a model that minimizes the cost of the recognizer will be very interesting. In this paper, we present a new approach for recognizing speech based on belief HMMs instead of proba-bilistic HMMs. Experiments shows that our belief recognizer is insensitive to the lack of the data and it can be trained using only one exemplary of each acoustic unit and it gives a good recognition rates. Consequently, using the belief HMM recognizer can greatly minimize the cost of these systems.", "histories": [["v1", "Thu, 22 Jan 2015 15:20:28 GMT  (51kb,D)", "http://arxiv.org/abs/1501.05530v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["siwar jendoubi", "boutheina ben yaghlane", "arnaud martin"], "accepted": false, "id": "1501.05530"}, "pdf": {"name": "1501.05530.pdf", "metadata": {"source": "CRF", "title": "Belief Hidden Markov Model for Speech Recognition", "authors": ["Siwar Jendoubi", "Boutheina Ben Yaghlane", "Arnaud Martin"], "emails": ["jendoubi.siouar@laposte.net", "boutheina.yaghlane@ihec.rnu.tn", "arnaud.martin@univ-rennes1.fr"], "sections": [{"heading": null, "text": "Index Terms\u2014Speech recognition, HMM, Belief functions, Belief HMM.\nI. INTRODUCTION\nThe automatic speech recognition is a domain of science that attracts the attention of the public. Indeed, who never dreamed of talking with a machine or at least control an apparatus or a computer by voice. The speech processing includes two major disciplines which are the speech recognition and the speech synthesis. The automatic speech recognition allows the machine to understand and process oral information provided by a human. It uses matching techniques to compare a sound wave to a set of samples, compounds generally of words or sub-words. On the other hand, the automatic speech synthesis allows the machine to reproduce the speech sounds of a given text. Nowadays, most speech recognition systems are based on the modelling of speech units known as acoustic unit. Indeed, speech is composed of a sequence of elementary sounds. These sounds put together make up words. Then, from these units we seeks to derive a model (one model per unit), which will be used to recognize continuous speech signal. Hidden Markov Models (HMM) are very often used to recognize these units. HMM based recognizer is a widely used technique that allows as to recognize about 80% of a given speech signal, but this recognition rate still not yet satisfying. Also, this method needs many hours of speech for training which makes the automatic speech recognition task very expensive.\nRecently, [7], [6] extend the Hidden Markov Model to the theory of belief functions. The belief HMM will avoid disadvantages of probabilistic HMM which are, generally, due to the use of probability theory. Belief functions are used in\nseveral domains of research where incertitude and imprecision dominate. They provide many tools for managing and processing the existent pieces of evidence in order to extract knowledge and make better decision. They allow experts to have a more clear vision about their problems, which is helpful for finding better solutions. What\u2019s more, belief functions theories present a more flexible ways to model uncertainty and imprecise data than probability functions. Finally, it offers many tools with a higher ability to combine a great number of pieces of evidence.\nBelief HMM gives a better classification rate than the ordinary HMM when they are applied in a classification problem. Consequently, we propose to use the belief HMM in the speech recognition process. Finally, we note that this is the first time where belief functions are used in speech processing.\nIn the next section we talk about the probabilistic hidden Markov model and we define its three famous problems. In Section three we present the probabilistic HMM recognizer, the acoustic model and the recognition process. The transferable belief model is introduced in section four. In section five we will talk about the belief HMM. In section six, we present our belief HMM recognizer, the belief acoustic model and the belief recognition process. Finally, experiments are presented in section seven."}, {"heading": "II. PROBABILISTIC HMM", "text": "A Hidden Markov Model is a combination of two stochastic processes; the first one is a Markov chain that is characterized by a finite set1 \u2126t of non observable N states (hidden) and the transition probabilities, aij = P ( st+1j | sti ) , 1 \u2264 i, j \u2264 N , between them. The second stochastic process produces the sequence of T observations which depends on the probability density function of the observation model defined as bj (Ot) = P ( Ot | stj ) , 1 \u2264 j \u2264 N, 1 \u2264 t \u2264 T [4], in this paper we use a mixture of Gaussian densities. The initial state distribution is defined as \u03c0i = P ( s1i ) , 1 \u2264 i \u2264 N . Hence, an HMM \u03bb (A,B,\u03a0) is characterized by the transition matrix A = {aij}, the observation model B = {bj (Ot)} and the initial state distribution \u03a0 = {\u03c0i}.\n1t notes the current instant, it is put in exponent of states for simplicity.\nar X\niv :1\n50 1.\n05 53\n0v 1\n[ cs\n.A I]\n2 2\nJa n\n20 15\n2 There exist three basic problems of HMMs that must be solved in order to be able to use these models in real world applications. The first problem is named the evaluation problem, it searches to compute the probability P (O/\u03bb) that the observation sequence O was generated by the model \u03bb. This probability can be obtained using the forward propagation [4]. Recursively, it estimates the forward variable:\n\u03b1t(i) = P (O1O2. . .Ot, qt = si | \u03bb) (1)\n\u03b1t(i) = ( N\u2211 i=1 \u03b1t\u22121 (i) aij ) bj (Ot) (2)\nfor all states and at all time instant. Then, P (O/\u03bb) =\u2211N i=1 \u03b1T (i) is obtained by summing the terminal forward variables. Also, the backward propagation can be used to resolve this problem. Unlike forward, the backward propagation goes backward. At each instant, it calculates the backward variable:\n\u03b2t(i) = P (Ot+1Ot+1. . .OT | qt = si, \u03bb) (3)\n\u03b2t(i) = N\u2211 j=1 aijbj (Ot+1)\u03b2t+1 (i) (4)\nfinally, P (O | \u03bb) = \u2211N i=1 \u03b1t(i)\u03b2t(i) is obtained by combining the forward and backward variable. The second problem is named the decoding problem. It searches to predict the state sequence S that generated O. The Viterbi [4] algorithm solves this problem. It starts from the first instant, t = 1, for each moment t, it calculates \u03b4t(i) for every state i, then it keeps the state which have the maximum \u03b4t = maxq1,q2,...,qt\u22121 P (q1, q2, . . . qt\u22121, qt = i, O1O2 . . . Ot\u22121 | \u03bb) = max1\u2264i\u2264N (\u03b4t\u22121 (i) aij) bj (Ot). When, the algorithm reaches the last instance t = T , it keeps the state which maximize \u03b4T . Finally, Viterbi algorithm back-track the sequence of states as the pointer in each moment t indicates. The last problem is the learning problem, it seeks to adjust the model parameters in order to maximize P (O | \u03bb). Baum-Welch [4] method is widely used. This algorithm uses the forward and backward variables to re-estimate the model parameters."}, {"heading": "III. PREBABILISTIC HMM BASED RECOGNIZER", "text": ""}, {"heading": "A. Acoustic model", "text": "The acoustic model attempts to mimic the human auditory system, it is the model used by the HMM-based speech recognizer in order to transform the speech signal into a sequence of acoustic units, this last will be transformed into phoneme sequence and finally the desired text is generated by converting the phoneme sequence into text. Acoustic models are used by speech segmentation and speech recognition systems.\nThe acoustic model is composed of a set of HMMs [4], each HMM corresponds to an acoustic unit. To have a good acoustic model some choices have to be done:\na) The acoustic unit: the choice of the acoustic unit is very important, in fact, the number of them will influence the complexity of the model (more large the number, more complex the model). If we choose a small unit like the phone we will have an HMM for every possible phone in\nthe language, the problem with this choice is that the phone do not model its context. Such a model is called context independent model. These models are generally used for speech segmentation systems. Other units that take the context into account can be used as acoustic unit as the diphone which model the transition between two phones, the triphone which model the transition between three phones, subwords, words. These models are called context dependent models. According to [5], when the context is greater, the recognition performance improve.\nb) The model: for each acoustic unit we associate an HMM, then types of HMM model and the probability density function of the observation must be chosen. Generally, leftright models are used for speech recognition and speech synthesis systems [4]. In fact, Speech signal has the property that it changes over time, then the choice of the left-right model is justified by the fact that there is no back transitions and all transitions goes forward. The number of states is fixed in advance or chosen experimentally. [2], [3] fixed the number of state to three. This choice is justified by the fact that most phoneme acoustic realization is characterized by three sub-segments, hence we have a state for each sub-segment. [1], [12] used an HMM of six states. Finally, we choose the probability density function of the observation. They are represented by a mixture of Gaussian pdf, the number of mixtures is generally chosen experimentally.\nThe next step, consists on training parameters of each HMM using a speech corpus that contains many exemplary of each acoustic unit. Speech segments are transformed into sequence of acoustic vectors by the mean of a feature extraction method like MFCC, these acoustic vectors are our sequence of observations.\nThen, HMMs are concatenated to each other and we obtain the model that will be used to recognize the new speech signal. The recognizer contains three levels; the first one is the syntactic level. It represents all possible word sequences that can be recognized by our model. The second level is the lexical level. It represents the phonetic transcription (the phoneme sequence) of each word. Finally, the third level is the acoustic level. It models the realization of each acoustic unit (in this case the phone)."}, {"heading": "B. Speech recognition process", "text": "The model described above is used for the speech recognition process. Let S be our speech signal to be recognized. Recognizing S consists on finding the most likely path in the syntactic network. The first step, is to transform S into a sequence of acoustic vectors using the same feature extraction method used for training, then we obtain our sequence of observation O. The most likely path is the path that maximizes the probability of observing O such the model P (O|\u03bb). This probability can be done either by using the forward algorithm, or the Viterbi algorithm."}, {"heading": "IV. TRANSFERABLE BELIEF MODEL", "text": "The Transferable Belief Model (TBM) [11], [10] is a well used variant of belief functions theories. It is a more general system than the Bayesian model.\n3 Let \u2126t = {\u03c91, \u03c92, ..., \u03c9n} be our frame of discernment, The agent belief on \u2126t is represented by the basic belief assignment (BBA) m\u2126t defined from 2\u2126 to [0, 1]. m\u2126t (A) is the mass value assigned to the proposition A \u2286 \u2126t and it must respect:\u2211 A\u2286\u2126t m\n\u2126t (A) = 1. Also, we can define conditional BBA. Then we can have m\u2126t [ St\u22121 ] (A) which is a BBA defined conditionally to St\u22121 \u2286 \u2126t\u22121. If we have m\u2126t (\u2205) > 0, our BBA can be normalized by dividing the other masses by 1\u2212 m\u2126t (\u2205) then the conflict mass id redistributed and m\u2126t (\u2205) = 0.\nBasic belief assignment can be converted into other functions. They represent the same information under other forms. What\u2019s more, they are in one to one correspondence and they are defined from 2\u2126 to [0, 1]. We will use belief bel, plausibility pl and commonality q functions:\nbel\u2126 (A) = \u2211 \u22056=B\u2286A m\u2126 (B) , \u2200A \u2286 \u2205, A 6= \u2205 (5)\nm\u2126 (A) = \u2211 B\u2286A (\u22121)|A|\u2212|B| bel\u2126 (B) , \u2200A \u2286 \u2126 (6)\npl\u2126 (A) = \u2211\nB\u2229A=\u2205\nm\u2126 (B) , \u2200A \u2286 \u2126 (7)\nm\u2126 (A) = \u2211 B\u2286A (\u22121)|A|\u2212|B|\u22121 pl\u2126 ( B\u0304 ) , \u2200A \u2286 \u2126 (8)\nq\u2126 (A) = \u2211 B\u2287A m\u2126 (B) , \u2200A \u2286 \u2126 (9)\nm\u2126 (A) = \u2211 A\u2286B (\u22121)|B|\u2212|A| q\u2126 (B) , \u2200A \u2286 \u2126 (10)\nConsider two distinct BBA m\u21261 and m \u2126 2 defined on \u2126, we can obtain m\u21261\u22292 through the TBM conjunctive rule (also called conjunctive rule of combination CRC) [9] as:\nm\u21261\u22292 (A) = \u2211\nB\u2229C=A m\u21261 (B)m \u2126 2 (C) , \u2200A \u2286 \u2126 (11)\nEquivalently, we can calculate the CRC via a more simple expression defined with the commonality function:\nq\u21261\u22292 (A) = q \u2126 1 (A) q \u2126 2 (A) , \u2200A \u2286 \u2126 (12)"}, {"heading": "V. BELIEF HMM", "text": "Belief HMM is an extension of the probabilistic HMM to belief functions [7], [6], [8]. Like probabilistic HMM, the belief HMM is a combination of two stochastic processes. Hence, a belief HMM is characterized by: \u2022 The credal transition matrix A = { m\u2126ta [ St\u22121i ] ( Stj )}\na set of BBA functions defined conditionally to all possible subsets of states St\u22121i ,\n\u2022 The observation model B = { m\u2126tb [Ot] ( Stj )}\na set of BBA functions defined conditionally to the set of possible observation Ot,\n\u2022 The initial state distribution \u03a0 = { m\u21261\u03c0 ( S\u21261i )} .\nThe three basic problem of HMM and their solutions are extended to belief functions. As we know the forward algorithm resolves the evaluation problem in the probabilistic case. [7] introduced the credal forward algorithm in order to\nresolve this problem in the evidential case. It needs as inputs m\u2126ta [ St\u22121i ] ( Stj ) and m\u2126tb [Ot] ( Stj )\nto calculate the forward commonality:\nq\u2126t+1\u03b1 ( St+1j ) =  \u2211 Sti\u2286\u2126t m\u2126t\u03b1 ( Sti ) .q\u2126t+1a [S t i ] ( St+1j ) \u2229q\u2126t+1b [Ot] ( Stj + 1 ) (13)\nThis last is calculated recursively from t = 1 to T . [6] exploits the conflict of the forward BBA (obtained by using formula 10) to define an evaluation metric that can be used for classification to choose the model that best fits the observation sequence or it can also be used to evaluate the model. Then, given a model \u03bb and an observation sequence of length T , the conflict metric is defined by:\nLc (\u03bb) = 1\nT T\u2211 t=1 log ( 1\u2212m\u2126t+1\u03b1 [\u03bb] (\u2205) ) (14)\n\u03bb\u2217 = arg max \u03bb Lc (\u03bb) (15)\nA credal backward algorithm is also defined, recursively, it calculates the backward commonality from T to t = 1. More details can be found in [7], [6]. For the decoding problem, many solutions are proposed to extend the Viterbi algorithm to the TBM [7], [6], [8]. All of them search to maximize the state sequence plausibility. According to the definition given in [8], the plausibility of a sequence of singleton states S ={ s1, s2, . . . , sT } , st \u2208 \u2126t is given by:\npl\u03b4 (S) = pl\u03c0 ( s1 ) . T\u220f t=2 pl\u2126ta [ st\u22121 ] ( st ) . T\u220f t=1 plb ( st ) (16)\nHence, we can choose the best state sequence by maximizing this plausibility. For the learning problem, [6], [8] have proposed some solutions to estimate model parameters, we will talk about the method used in this paper. The first step consists on estimating the mixture of Gaussian models (GMM) parameters using Expectation-Maximization (EM) algorithm. For each state we estimate one GMM. These models are used to calculate m\u2126tb [Ot] ( Stj ) . [6] proposes to estimate the credal transition matrix independently from the transitions themselves. He uses the observation BBAs as:\nm \u2126t\u00d7\u2126t+1 a \u221d\n1\nT \u2212 1 (17)\n\u2217 T\u2211 t=1 ( m\u2126tb [Ot] \u2191\u2126t\u00d7\u2126t+1 \u2229m\u2126t+1b [Ot+1] \u2191\u2126t\u00d7\u2126t+1 ) where m\u2126tb [Ot] \u2191\u2126t\u00d7\u2126t+1 and m\u2126t+1b [Ot+1] \u2191\u2126t\u00d7\u2126t+1 are computed using the vacuous extension operator [9] of the BBA m\u2126tb [Ot] ( Stj ) on the cartesian product space as:\nm \u2126t\u2191\u2126t\u00d7\u2126t+1 b (A) = { m\u2126tb (B) ifA = B \u00d7 \u2126t+1 0 otherwise (18)\nThis estimation formula is used by [8] as an initialization for ITS (Iterative Transition Specialization) algorithm. ITS is an iterative algorithm that uses the credal forward algorithm to improve the estimation results of the credal transition matrix. It stops when the conflict metric (formula 14) converged.\n4"}, {"heading": "VI. BELIEF HMM BASED RECOGNIZER", "text": "Our goal is to create a speech recognizer using the belief HMM instead of the probabilistic HMM. HMM recognizer uses an acoustic model to recognize the content of the speech signal. Then, we seek to mimic this model in order to create a belief HMM based one. We should note that existent parameter estimation methods presented for the belief HMM cannot be used to estimate model parameters using multiple observation sequences. This fact should be taken into account when we design our belief acoustic model."}, {"heading": "A. Belief acoustic model", "text": "In the probabilistic case, we use an HMM for each acoustic unit, its parameters are trained using multiple speech realization of the unit [5], [1], [2], [12], [3]. In the credal case, a similar model cannot be used. Hence, we present an alternate method that takes this fact into account.\nLet K be the number of the speech realization of a given acoustic unit. These speech realization are transformed into MFCC feature vectors. Hence, we obtain K observation sequences. Our training set will be:O = [ O1, O2, . . . , OK\n] where Ok = ( Ok1 , O k 2 , . . . , O k Tk ) is the kth observation sequence of length Tk. These observations are supposed to be independent to each other. So instead of training one model for all observation set O, we propose to create a belief model for each observation sequence Ok. These K models will be used to represent the given acoustic unit in the recognition process.\nLike the acoustic model based on the probabilistic HMM, we have to make some choices in order to have a good belief acoustic model. In the first place, we choose the acoustic unit. The same choices of the probabilistic case can be adopted for the belief case. In the second place, we choose the model. We should note that we cannot choose the topology of the belief HMM, this is due to the estimation process of the credal transition matrix. In other words, the resultant credal observation model is used to estimate the credal transition matrix which does not give as the hand to choose the topology of our resultant model. Consequently, choosing the model in the credal case consists on choosing the number of states and the number of Gaussian mixtures. In our case we fix the number of states to three and we choose the number of Gaussian mixtures experimentally."}, {"heading": "B. Speech recognition process", "text": "The belief acoustic model is used in the speech recognition process. Now, we explain how the resultant model will be used for recognizing speech signal.\nLet S be our speech signal to be recognized. Recognizing S consists on finding the most likely set of models. The first step, is to transform S into a sequence of acoustic vectors using the same feature extraction method used for training, then we obtain our sequence of observation O. This last is used as input for all models. The credal forward algorithm is then applied, each model gives us an output which is the value of the conflict metric. An acoustic unit is presented by a set\nof models, every model gives a value for the conflict metric. Then we calculate the arithmetic mean of the resultant values. Finally, we choose the set of models that optimizes the average of the conflict metric instead of optimizing the conflict metric, as proposed by [6], using formula 15."}, {"heading": "VII. EXPERIMENTS", "text": "In this section we present experiments in order to validate our approach. We compare our belief HMM recognizer to a similar one implemented using the probabilistic HMM.\nWe use MFCC (Mel Frequency Cepstral Coefficient) as feature vectors. Also, we use a three state HMM and two Gaussian mixtures. Finally, to evaluate our models we calculate the percent of correctly recognized acoustic units (number of correctly recognized acoustic unit / total number of acoustic units). We use a speech corpus that contains speech realization of seven different acoustic units and we have fifteen exemplary of each one. Results are shown in figure 1.\nThe lack of data for training the probabilistic HMM leads to a very poor learning and the resultant acoustic model cannot be efficient. Then using a training set that contains only one exemplary of each acoustic unit leads to have a bad probabilistic recognizer. In this case our belief HMM based recognizer gives a recognition rate equal to 85.71% against 13.79% for the probabilistic HMM which is trained using HTK [13]. This results shows that the belief HMM recognizer is insensitive to the lack of data and we can obtain a good belief acoustic model using only one observation for each unit. In fact, the belief HMM models knowledge by taking into account doubt, imprecision and conflict which leads to a discriminative model in the case of the lack of data.\nHTK is a toolkit for HMMs and it is optimized for the HMM speech recognition process. It is known to be powerful under the condition of having many exemplary of each acoustic unit. Hence, it needs to use several hours of speech for training. Having a good speech corpus is very expensive which influence the cost of the recognition system. Then, the speech recognition systems are very expensive. Consequently, using the belief HMM recognizer can greatly minimize the cost of these systems."}, {"heading": "VIII. CONCLUSION", "text": "In this paper, we proposed the Belief HMM recognizer. We showed that incorporating belief functions theory in the\n5 speech recognition process is very beneficial, in fact, it reduces considerably the cost of the speech recognition system. Future works will be focuced on the case of the noisy speech signal. Indeed, existent speech recognizer still not yet good if we have a noisy signal to be decoded."}], "references": [{"title": "Automatic segmentation and labeling of speech based on hiddenmarkov models", "author": ["F. Brugnara", "D. Falavigna", "M. Omologo"], "venue": "Speech Communication,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1993}, {"title": "Concatenative speech synthesis for european portuguese", "author": ["P. Carvalho", "L.C. Oliveira", "I.M. Trancoso", "M.C. Viana"], "venue": "In 3rd ESCA/COCOSDA Worshop on Speech Synthesis,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Techniques for accurate automatic annotation of speech waveforms", "author": ["S. Cox", "R. Brady", "P. Jackson"], "venue": "In Proc. ICASSP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L. Rabiner"], "venue": "Proceedings of IEEE,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1989}, {"title": "Fundamentals of speech recognition", "author": ["L. Rabiner", "B.H. Juang"], "venue": "Prentice-Hall, Inc. Upper Saddle River, NJ,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "Contribution of belief functions to HMM with an application to fault diagnosis", "author": ["E. Ramasso"], "venue": "IEEE International Workshop on Machine Learning and Signal Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Forward-backward-viterbi procedures in thetransferable belief model for state sequence analysis using belief functions", "author": ["E. Ramasso", "M. Rombaut", "D. Pellerin"], "venue": "ECSQARU, Hammamet: Tunisie,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Time-sliced temporal evidential networks: the case of evidential hmm with application to dynamical system analysis", "author": ["L. Serir", "E. Ramasso", "N. Zerhouni"], "venue": "IEEE International Conference on Prognostics and Health Management", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Beliefs functions: The disjunctive rule of combination and the generalized bayesian theorem", "author": ["P. Smets"], "venue": "IJAR, 9:1\u201335,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1993}, {"title": "Belief functions and the transferable belief model", "author": ["P. Smets"], "venue": "Available on www. sipta.org/documentation/belief/belief.ps,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "The transferable belief model. artificial intelligence", "author": ["P. Smets", "R. Kennes"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1994}, {"title": "Automatic phonetic segmentation", "author": ["D.T. Toledano", "L.A.H. Gomez", "L.V. Grande"], "venue": "IEEE Trans. Speech, Audio Processing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "The htk book for htk version 3.4", "author": ["S. Young", "G. Evermann", "D. Kershaw", "G. Moore", "J. Odell", "D. Ollason", "V. Valtchev", "P. Woodland"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}], "referenceMentions": [{"referenceID": 6, "context": "Recently, [7], [6] extend the Hidden Markov Model to the theory of belief functions.", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "Recently, [7], [6] extend the Hidden Markov Model to the theory of belief functions.", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": "The second stochastic process produces the sequence of T observations which depends on the probability density function of the observation model defined as bj (Ot) = P ( Ot | sj ) , 1 \u2264 j \u2264 N, 1 \u2264 t \u2264 T [4], in this paper we use a mixture of Gaussian densities.", "startOffset": 203, "endOffset": 206}, {"referenceID": 3, "context": "This probability can be obtained using the forward propagation [4].", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "The Viterbi [4] algorithm solves this problem.", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "Baum-Welch [4] method is widely used.", "startOffset": 11, "endOffset": 14}, {"referenceID": 3, "context": "The acoustic model is composed of a set of HMMs [4], each HMM corresponds to an acoustic unit.", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "According to [5], when the context is greater, the recognition performance improve.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "Generally, leftright models are used for speech recognition and speech synthesis systems [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "[2], [3] fixed the number of state to three.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[2], [3] fixed the number of state to three.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "[1], [12] used an HMM of six states.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[1], [12] used an HMM of six states.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "The Transferable Belief Model (TBM) [11], [10] is a well used variant of belief functions theories.", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "The Transferable Belief Model (TBM) [11], [10] is a well used variant of belief functions theories.", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": ", \u03c9n} be our frame of discernment, The agent belief on \u03a9t is represented by the basic belief assignment (BBA) mt defined from 2 to [0, 1].", "startOffset": 131, "endOffset": 137}, {"referenceID": 0, "context": "What\u2019s more, they are in one to one correspondence and they are defined from 2 to [0, 1].", "startOffset": 82, "endOffset": 88}, {"referenceID": 8, "context": "Consider two distinct BBA m1 and m \u03a9 2 defined on \u03a9, we can obtain m1\u22292 through the TBM conjunctive rule (also called conjunctive rule of combination CRC) [9] as:", "startOffset": 155, "endOffset": 158}, {"referenceID": 6, "context": "Belief HMM is an extension of the probabilistic HMM to belief functions [7], [6], [8].", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "Belief HMM is an extension of the probabilistic HMM to belief functions [7], [6], [8].", "startOffset": 77, "endOffset": 80}, {"referenceID": 7, "context": "Belief HMM is an extension of the probabilistic HMM to belief functions [7], [6], [8].", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "[7] introduced the credal forward algorithm in order to resolve this problem in the evidential case.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] exploits the conflict of the forward BBA (obtained by using formula 10) to define an evaluation metric that can be used for classification to choose the model that best fits the observation sequence or it can also be used to evaluate the model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "More details can be found in [7], [6].", "startOffset": 29, "endOffset": 32}, {"referenceID": 5, "context": "More details can be found in [7], [6].", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "For the decoding problem, many solutions are proposed to extend the Viterbi algorithm to the TBM [7], [6], [8].", "startOffset": 97, "endOffset": 100}, {"referenceID": 5, "context": "For the decoding problem, many solutions are proposed to extend the Viterbi algorithm to the TBM [7], [6], [8].", "startOffset": 102, "endOffset": 105}, {"referenceID": 7, "context": "For the decoding problem, many solutions are proposed to extend the Viterbi algorithm to the TBM [7], [6], [8].", "startOffset": 107, "endOffset": 110}, {"referenceID": 7, "context": "According to the definition given in [8], the plausibility of a sequence of singleton states S = { s, s, .", "startOffset": 37, "endOffset": 40}, {"referenceID": 5, "context": "For the learning problem, [6], [8] have proposed some solutions to estimate model parameters, we will talk about the method used in this paper.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "For the learning problem, [6], [8] have proposed some solutions to estimate model parameters, we will talk about the method used in this paper.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "[6] proposes to estimate the", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "where mt b [Ot] \u2191\u03a9t\u00d7\u03a9t+1 and mt+1 b [Ot+1] \u2191\u03a9t\u00d7\u03a9t+1 are computed using the vacuous extension operator [9] of the BBA mt b [Ot] ( S j ) on the cartesian product space as:", "startOffset": 102, "endOffset": 105}, {"referenceID": 7, "context": "This estimation formula is used by [8] as an initialization for ITS (Iterative Transition Specialization) algorithm.", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "In the probabilistic case, we use an HMM for each acoustic unit, its parameters are trained using multiple speech realization of the unit [5], [1], [2], [12], [3].", "startOffset": 138, "endOffset": 141}, {"referenceID": 0, "context": "In the probabilistic case, we use an HMM for each acoustic unit, its parameters are trained using multiple speech realization of the unit [5], [1], [2], [12], [3].", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": "In the probabilistic case, we use an HMM for each acoustic unit, its parameters are trained using multiple speech realization of the unit [5], [1], [2], [12], [3].", "startOffset": 148, "endOffset": 151}, {"referenceID": 11, "context": "In the probabilistic case, we use an HMM for each acoustic unit, its parameters are trained using multiple speech realization of the unit [5], [1], [2], [12], [3].", "startOffset": 153, "endOffset": 157}, {"referenceID": 2, "context": "In the probabilistic case, we use an HMM for each acoustic unit, its parameters are trained using multiple speech realization of the unit [5], [1], [2], [12], [3].", "startOffset": 159, "endOffset": 162}, {"referenceID": 5, "context": "Finally, we choose the set of models that optimizes the average of the conflict metric instead of optimizing the conflict metric, as proposed by [6], using formula 15.", "startOffset": 145, "endOffset": 148}, {"referenceID": 12, "context": "79% for the probabilistic HMM which is trained using HTK [13].", "startOffset": 57, "endOffset": 61}], "year": 2015, "abstractText": "Speech Recognition searches to predict the spoken words automatically. These systems are known to be very expensive because of using several pre-recorded hours of speech. Hence, building a model that minimizes the cost of the recognizer will be very interesting. In this paper, we present a new approach for recognizing speech based on belief HMMs instead of probabilistic HMMs. Experiments shows that our belief recognizer is insensitive to the lack of the data and it can be trained using only one exemplary of each acoustic unit and it gives a good recognition rates. Consequently, using the belief HMM recognizer can greatly minimize the cost of these systems.", "creator": "LaTeX with hyperref package"}}}