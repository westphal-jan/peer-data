{"id": "1501.05614", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jan-2015", "title": "Int{\\'e}gration d'une mesure d'ind{\\'e}pendance pour la fusion d'informations", "abstract": "Many information sources are considered into data fusion in order to improve the decision in terms of uncertainty and imprecision. For each technique used for data fusion, the asumption on independance is usually made. We propose in this article an approach to take into acount an independance measure befor to make the combination of information in the context of the theory of belief functions.", "histories": [["v1", "Thu, 22 Jan 2015 19:57:59 GMT  (160kb,D)", "http://arxiv.org/abs/1501.05614v1", "in French, appears in Atelier Fouille de donn{\\'e}es complexes, Extraction et Gestion des Connaissances (EGC), Jan 2013, Toulouse, France. arXiv admin note: substantial text overlap witharXiv:1501.04786"]], "COMMENTS": "in French, appears in Atelier Fouille de donn{\\'e}es complexes, Extraction et Gestion des Connaissances (EGC), Jan 2013, Toulouse, France. arXiv admin note: substantial text overlap witharXiv:1501.04786", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mouloud kharoune", "arnaud martin"], "accepted": false, "id": "1501.05614"}, "pdf": {"name": "1501.05614.pdf", "metadata": {"source": "CRF", "title": "Inte\u0301gration d\u2019une mesure d\u2019inde\u0301pendance pour la fusion d\u2019informations", "authors": ["Mouloud Kharoune", "Arnaud Martin"], "emails": ["Mouloud.Kharoune@univ-rennes1.fr,", "Arnaud.Martin@univ-rennes1.fr"], "sections": [{"heading": null, "text": "Int\u00e9gration d\u2019une mesure d\u2019ind\u00e9pendance pour la fusion d\u2019informations\nMouloud Kharoune\u2217, Arnaud Martin\u2217\n\u2217UMR 6074 IRISA, Universit\u00e9 de Rennes1 / IUT de Lannion, Rue Edouard Branly BP 3021, 22302 Lannion cedex\nMouloud.Kharoune@univ-rennes1.fr, Arnaud.Martin@univ-rennes1.fr\nR\u00e9sum\u00e9. La fusion d\u2019informations fait intervenir plusieurs sources d\u2019informations afin d\u2019am\u00e9liorer la d\u00e9cision en terme de certitude et de pr\u00e9cision. Quelle que soit l\u2019approche retenue pour r\u00e9aliser la fusion d\u2019informations, l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance est g\u00e9n\u00e9ralement une hypoth\u00e8se forte et incontournable. Nous proposons dans cet article une approche permettant d\u2019int\u00e9grer une mesure d\u2019ind\u00e9pendance avant de r\u00e9aliser la combinaison des informations dans le cadre de la th\u00e9orie des fonctions de croyance."}, {"heading": "1 Introduction", "text": "Tel que repris par Martin (2005), la fusion d\u2019informations consiste \u00e0 combiner des informations issues de plusieurs sources afin d\u2019aider \u00e0 la prise de d\u00e9cision. Les approches de fusion d\u2019informations cherchent donc \u00e0 tenir compte des redondances des informations issues des diff\u00e9rentes sources. Les approches de fusion n\u2019ont bien s\u00fbr d\u2019int\u00e9r\u00eat que si les sources sont imparfaites et fournissent des informations peu s\u00fbres et pr\u00e9cises qui se compl\u00e8tent. Ainsi il faut donc chercher \u00e0 mod\u00e9liser aux mieux les imperfections des sources et des donn\u00e9es. Pour ce faire, diff\u00e9rentes th\u00e9ories de l\u2019incertain ont \u00e9t\u00e9 sollicit\u00e9es. Parmi elles, citons la th\u00e9orie des probabilit\u00e9s, des sous-ensembles flous et des possibilit\u00e9s ou encore la th\u00e9orie des fonctions de croyance. Quel que soit le cadre th\u00e9orique retenu, lors de l\u2019\u00e9tape de combinaison de l\u2019information, l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance des sources est g\u00e9n\u00e9ralement faite. Cette hypoth\u00e8se est plus ou moins forte. Par exemple, l\u2019ind\u00e9pendance statistique est g\u00e9n\u00e9ralement retenue pour appliquer plus ais\u00e9ment la combinaison bay\u00e9sienne du cadre probabiliste. En effet, les estimations peuvent s\u2019av\u00e9rer tr\u00e8s vite compliqu\u00e9es sans cette hypoth\u00e8se. Dans le cas de la th\u00e9orie des fonctions de croyance, il est question d\u2019ind\u00e9pendance cognitive d\u00e9finie par Shafer (1976). Elle correspond \u00e0 une absence de communication entre les sources sans que celles-ci soient pour autant ind\u00e9pendantes statistiquement. Malheureusement cette hypoth\u00e8se d\u2019ind\u00e9pendance est rarement v\u00e9rifi\u00e9e ou justifi\u00e9e.\nDans ce travail nous nous int\u00e9ressons plus particuli\u00e8rement \u00e0 la th\u00e9orie des fonctions de croyance car elle offre un outil riche de mod\u00e9lisation et de gestion de l\u2019information. En particulier, Chebbah et al. (2012, 2013) ont r\u00e9cemment propos\u00e9 des approches pour mesurer l\u2019ind\u00e9pendance des sources en distinguant de plus la d\u00e9pendance positive et n\u00e9gative.\nNous poursuivons ainsi cet article en pr\u00e9sentant les principes de base de la th\u00e9orie des fonctions de croyance et en particulier la notion d\u2019ind\u00e9pendance et d\u2019affaiblissement. Nous ex-\nar X\niv :1\n50 1.\n05 61\n4v 1\n[ cs\n.A I]\n2 2\nJa n\n20 15\nposons ensuite l\u2019approche propos\u00e9e permettant de tenir compte d\u2019une mesure d\u2019ind\u00e9pendance avant la combinaison des fonctions de croyance. Nous illustrons ce principe \u00e0 partir d\u2019exemples g\u00e9n\u00e9r\u00e9s."}, {"heading": "2 Th\u00e9orie des fonctions de croyance", "text": "La th\u00e9orie des fonctions de croyance issue des travaux de Dempster (1967), repris par Shafer (1976) est depuis quelques ann\u00e9es employ\u00e9e dans des applications de fusion d\u2019informations. Nous pr\u00e9sentons ci-dessous les principes de cette th\u00e9orie."}, {"heading": "2.1 Principes de base", "text": "Consid\u00e9rons le cadre de discernement \u2126 = {\u03c91, \u03c92, . . . , \u03c9n} correspondant \u00e0 l\u2019ensemble de toutes les hypoth\u00e8ses possibles de d\u00e9cision d\u2019un probl\u00e8me donn\u00e9. Les \u00e9l\u00e9ments \u03c9i repr\u00e9sentent ainsi toutes les hypoth\u00e8ses exclusives et exhaustives.\nL\u2019ensemble 2\u2126 = {A/A \u2286 \u2126} = {\u2205, \u03c91, \u03c92, . . . , \u03c9n, \u03c91 \u222a \u03c92, . . . ,\u2126}, est compos\u00e9 de toutes les disjonctions de \u2126. L\u2019espace puissance 2\u2126 comporte 2|\u2126| = 2n \u00e9l\u00e9ments.\nUne fonction de masse est une fonction de 2\u2126 vers l\u2019intervalle [0, 1] qui affecte \u00e0 chaque sous-ensemble de 2\u2126 une valeur de l\u2019intervalle [0, 1] repr\u00e9sentant sa masse de croyance \u00e9l\u00e9mentaire. Elle s\u2019\u00e9crit :\nm\u2126 : 2\u2126 7\u2192 [0, 1] (1)\ntelle que : \u2211 A\u2286\u2126 m\u2126(A) = 1 (2)\nUn sous-ensemble de 2\u2126 de masse de croyance non-nulle est un \u00e9l\u00e9ment focal. La masse affect\u00e9e \u00e0 un \u00e9l\u00e9ment focal A repr\u00e9sente le degr\u00e9 de croyance \u00e9l\u00e9mentaire de la source \u00e0 ce que la solution du probl\u00e8me soit A. Une fonction de masse permet ainsi de repr\u00e9senter des connaissances incertaines et impr\u00e9cises d\u2019une source d\u2019informations. En g\u00e9n\u00e9ral, nous manipulons des fonctions de masse non dogmatiques (i.e. dont l\u2019ignorance \u2126 est \u00e9l\u00e9ment focal), d\u2019une part car elles permettent de syst\u00e9matiquement mod\u00e9liser la part d\u2019ignorance intrins\u00e8que \u00e0 toute source, mais \u00e9galement car toute fonction de masse non dogmatique est d\u00e9composable en fonctions de masse \u00e0 support simple (i.e. qui ne comporte que deux \u00e9l\u00e9ments focaux dont \u2126). Les fonctions de masse \u00e0 support simple sont not\u00e9es Aw telles que m(A) = 1 \u2212 w \u2200A 6= \u2126 et m(\u2126) = w. Ainsi une fonction de masse non dogmatique peut s\u2019\u00e9crire :\nm\u2126 = \u2229\u00a9A\u2282\u2126A w(A) (3)\no\u00f9 \u2229\u00a9 est donn\u00e9es par l\u2019\u00e9quation (5) ci-dessous. La cr\u00e9dibilit\u00e9 bel et la plausibilit\u00e9 pl sont des fonctions duales d\u00e9finies \u00e0 partir de la fonctions de masse et repr\u00e9sentent respectivement une fonction de croyance minimale et maximale. Ainsi la fonction de plausibilit\u00e9 est donn\u00e9e par :\npl(X) = \u2211\nY\u2282\u2126,Y \u2229X 6=\u2205\nm(Y ) = bel(\u2126)\u2212 bel(Xc) = 1\u2212m(\u2205)\u2212 bel(Xc), (4)\no\u00f9 Xc est le compl\u00e9mentaire de X . Une fois les fonctions de masse m\u2126j d\u00e9termin\u00e9es pour chaque source d\u2019informations Sj , plusieurs op\u00e9rateurs de combinaison sont envisageables en fonction des hypoth\u00e8ses initiales. Les op\u00e9rateurs de type conjonctif peuvent \u00eatre employ\u00e9s lorsque les sources sont fiables et ind\u00e9pendantes cognitivement. La combinaison conjonctive s\u2019\u00e9crit pour deux fonctions de masse m\u21261 et m \u2126 2 et pour tout X \u2208 2\u2126 par :\nm\u2126Conj(X) = m1 \u2229\u00a9m2 = \u2211\nY1\u2229Y2=X m\u21261 (Y1)m \u2126 2 (Y2). (5)\nNotons que l\u2019\u00e9l\u00e9ment neutre pour cette r\u00e8gle est la masse : m\u2126\u2126(X) = 1 si X = \u2126 et 0 sinon. Lorsque cette hypoth\u00e8se de fiabilit\u00e9 est trop forte et que l\u2019on ne peut supposer que seule une des sources est fiable, la combinaison disjonctive peut alors \u00eatre employ\u00e9e toujours sous l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance cognitive :\nm\u2126Dis(X) = \u2211\nY1\u222aY2=X m\u21261 (Y1)m \u2126 2 (Y2). (6)\nNotons que l\u2019\u00e9l\u00e9ment neutre pour cette r\u00e8gle est la masse : m\u2126\u2205 (X) = 1 si X = \u2205 et 0 sinon. La plupart des r\u00e8gles de combinaison issues des r\u00e8gles conjonctives et disjonctives, en particulier pour r\u00e9partir le conflit, supposent que les sources sont ind\u00e9pendantes cognitivement. Martin (2010) en rappelle quelques unes.\nDen\u0153ux (2008) propose une famille de r\u00e8gles qui ne n\u00e9cessitent pas l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance cognitive. Ainsi selon le comportement conjonctif ou disjonctif deux r\u00e8gles principales sont d\u00e9finies, la r\u00e8gle prudente et hardie. La r\u00e8gle prudente s\u2019\u00e9crit pour les fonctions de masse non dogmatiques :\nm\u21261 \u2227\u00a9m \u2126 2 = \u2229\u00a9A\u2282\u2126A w1(A)\u2227w2(A) (7)\no\u00f9 \u2227 est le maximum. La r\u00e8gle hardie s\u2019\u00e9crit de m\u00eame en consid\u00e9rant le minimum au lieu du maximum. Si ces r\u00e8gles sont efficaces lorsque les sources sont d\u00e9pendantes, cette notion de d\u00e9pendance ou d\u2019ind\u00e9pendance n\u2019est par clairement d\u00e9finie par Den\u0153ux (2008).\nLorsqu\u2019une connaissance suppl\u00e9mentaire garantie un sous-ensembleA \u2282 \u2126, nous pouvons d\u00e9finir une fonction de masse conditionnelle par :\nm\u2126[A](X) = (m\u2126 \u2229\u00a9m\u2126A)(X) (8)\no\u00f9 m\u2126A(A) = 1 est la fonction de masse garantissant la r\u00e9alisation de A."}, {"heading": "2.2 Notion d\u2019ind\u00e9pendance", "text": "L\u2019ind\u00e9pendance statistique est d\u00e9finie pour deux variables A et B par P (A|B) = P (A) ou de fa\u00e7on \u00e9quivalente par P (A\u2229B) = P (A)P (B). Cette ind\u00e9pendance est \u00e9tendue par Shafer (1976) dans le cadre de la th\u00e9orie des fonctions de croyance et est donn\u00e9e par : pl(A \u2229 B) = pl(A)pl(B). Ben Yaghlane et al. (2002a,b) d\u00e9finissent une ind\u00e9pendance doxatique entre des variables d\u00e9finies sur des cadres de discernement diff\u00e9rents \u00e9ventuellement.\nCes d\u00e9finitions de l\u2019ind\u00e9pendance ne correspondent pas \u00e0 la notion d\u2019ind\u00e9pendance cognitive entre les sources d\u2019informations. Cette derni\u00e8re se r\u00e9v\u00e8le tr\u00e8s difficile \u00e0 mesurer. Chebbah\net al. (2012, 2013) proposent une d\u00e9finition d\u2019une mesure d\u2019ind\u00e9pendance entre deux sources d\u2019informations \u00e9tendues \u00e0 une mesure de d\u00e9pendance positive et n\u00e9gative. La mesure d\u2019ind\u00e9pendance entre deux sources est d\u00e9finie comme une sorte de corr\u00e9lation entre deux sources issues d\u2019un clustering (classification non-supervis\u00e9e) sur les fonctions de masse de chacune des sources en associant ensuite les clusters. Si |\u2126| = n, le clustering des fonctions de masse issues de la source S1 fourni n clusters, de m\u00eame pour S2. Les clusters des deux sources (Clk1 , Clk2 ) sont associ\u00e9s de fa\u00e7on non sym\u00e9trique en maximisant :\n\u03b1ik1,k2 = |Clk1 \u2229 Clk2 | |Clki | , i = 1, 2 (9)\nIl est ensuite possible de d\u00e9finir une fonction de masse sur \u2126I = {I, I\u0304} repr\u00e9sentant les deux possibilit\u00e9s : ind\u00e9pendant et d\u00e9pendant (I\u0304) de la source S1 par rapport \u00e0 la source S2 : m\u2126Ik1k2(I) = \u03b2 (1\u2212 \u03b1 1 k1k2 ) m\u2126Ik1k2(I\u0304) = \u03b2 \u03b1 1 k1k2\nm\u2126Ik1k2(I \u222a I\u0304) = 1\u2212 \u03b2 (10)\no\u00f9 \u03b2 est un facteur d\u2019affaiblissement permettant de tenir compte du nombre d\u2019observations dans chaque cluster. Ainsi la croyance \u00e9lementaire que la source S1 est ind\u00e9pendante de S2 est donn\u00e9e par la masse :\nm\u2126I (X) = 1\nn\n( n\u2211\nk1=1\nm\u2126Ik1k2\n) (X) (11)\no\u00f9 k2 est le cluster de la source S2 associ\u00e9 au cluster k1 de la source S1. La moyenne est ici employ\u00e9e du fait de la d\u00e9pendance des fonctions de masse.\nChebbah et al. (2012, 2013) proposent un prolongement pour diff\u00e9rencier la d\u00e9pendance positive (la source S1 suit les avis de la source S2) et la d\u00e9pendance n\u00e9gative (la source S1 dit le contraire des avis de la source S2). Ainsi, une fonction de masse conditionnelle est construite sur le cadre de discernement \u2126P = {P, P\u0304} : m\u2126Pk1k2 [I\u0304](P ) = 1\u2212Dist(Clk1 , Clk2) m\u2126Pk1k2 [I\u0304](P\u0304 ) = Dist(Clk1 , Clk2)\nm\u2126Pk1k2 [I\u0304](P \u222a P\u0304 ) = 0 (12)\no\u00f9Dist(Clk1 , Clk2) est la distance entre les deux clusters d\u00e9pendantsClk1 etClk2 li\u00e9s comme \u00e9tant la moyenne des distances entre les fonctions de masse des objets en commun :\nDist(Clk1 , Clk2) = 1\n|Clk1 \u2229 Clk2 | |Clk1\u2229Clk2 |\u2211 j=1 d(m\u21261,j ,m \u2126 2,j) (13)\no\u00f9 d est la distance propos\u00e9e par Jousselme et al. (2001) entre les fonctions de masse de la source S1 et S2 respectivement.\nSi nous consid\u00e9rons que I\u0304 = P \u222a P\u0304 , nous pouvons r\u00e9\u00e9crire les deux fonctions de masse pr\u00e9c\u00e9dentes dans le cadre de discernement I = {I, P, P\u0304}. Nous d\u00e9finissons ainsi la fonction\nde masse entre deux clusters de S1 et S2 : mIk1k2(I) = m \u2126I k1k2 (I) = \u03b2 \u03b11k1k2 mIk1k2(P ) = m \u2126I k1k2 (I\u0304)m\u2126Pk1k2 [I\u0304](P ) = \u03b2 (1\u2212 \u03b1 1 k1k2 )(1\u2212Dist(Clk1 , Clk2)) mIk1k2(P\u0304 ) = m \u2126I k1k2 (I\u0304)m\u2126Pk1k2 [I\u0304](P\u0304 ) = \u03b2 (1\u2212 \u03b1 1 k1k2 )Dist(Clk1 , Clk2) mIk1k2(P \u222a P\u0304 ) = m \u2126I k1k2\n(I\u0304)m\u2126Pk1k2 [I\u0304](P \u222a P\u0304 ) = 0 mIk1k2(I \u222a P \u222a P\u0304 ) = m \u2126I k1k2 (I \u222a I\u0304) = 1\u2212 \u03b2\n(14)\nLa fonction de masse sur la d\u00e9pendance de la source S1 par rapport \u00e0 S2 est donn\u00e9e par :\nmI(X) = 1\nn\n( n\u2211\nk1=1\nmIk1k2\n) (X) (15)\no\u00f9 k2 est le cluster de la source S2 associ\u00e9 au cluster k1 de la source S1. Cette fonction de masse repr\u00e9sente ainsi l\u2019ensemble des croyances \u00e9l\u00e9mentaires sur l\u2019ind\u00e9pendance et d\u00e9pendance positive et n\u00e9gative de la source S1 face \u00e0 la source S2."}, {"heading": "2.3 Notion d\u2019affaiblissement", "text": "Shafer (1976) a propos\u00e9 la proc\u00e9dure d\u2019affaiblissement suivante :\n\u03b1m\u2126(X) = \u03b1m\u2126(X) \u2200X \u2208 2\u2126 \\ \u2126 (16) \u03b1m\u2126(\u2126) = 1\u2212 \u03b1(1\u2212m\u2126(\u2126)) (17)\no\u00f9 \u03b1 est un facteur d\u2019affaiblissement de [0, 1]. Cette proc\u00e9dure est g\u00e9n\u00e9ralement employ\u00e9e pour affaiblir les fonctions de masse par la fiabilit\u00e9 \u03b1 des sources d\u2019informations. Cette proc\u00e9dure a pour effet d\u2019augmenter la masse sur l\u2019ignorance \u2126. Smets (1993) a justifi\u00e9 cette proc\u00e9dure en consid\u00e9rant que :\nm\u2126[F ](X) = m\u2126(X) (18) m\u2126[F\u0304 ](X) = m\u2126\u2126(X) (19)\no\u00f9 m\u2126\u2126(X) = 1 si X = \u2126 et 0 sinon, F et F\u0304 repr\u00e9sentent la fiabilit\u00e9 et la non fiabilit\u00e9 et m\u2126[F ] est une fonction de masse conditionnelement \u00e0 la fiabilit\u00e9 F . Soit F = {F, F\u0304} le cadre de discernement correspondant, et la fonction de masse repr\u00e9sentant la connaissance sur la fiabilit\u00e9 de la source : {\nmF (F ) = \u03b1 mF (F) = 1\u2212 \u03b1. (20)\nAfin de combiner les deux sources d\u2019informations fournissant les deux fonctions de masse m\u2126[F ] et mF , il faut pouvoir les repr\u00e9senter dans le m\u00eame espace \u2126\u00d7F . Ainsi, nous devons effectuer une extention \u00e0 vide sur la fonction de masse mF , op\u00e9ration que l\u2019on note mF\u2191\u2126\u00d7F :\nmF\u2191\u2126\u00d7F (Y ) = { mF (X) si Y = \u2126\u00d7X, X \u2286 F 0 sinon (21)\nDans le cas de la fonction de masse m\u2126[F ], il faut d\u00e9conditionner :\nm\u2126 [F ] \u21d1\u2126\u00d7F ( (A\u00d7 F ) \u222a (\u2126\u00d7 F ) ) = m\u2126 [F ] (A) , A \u2286 \u2126 (22)\nIl est ainsi possible d\u2019effectuer la combinaison :\nm\u2126\u00d7FConj (Y ) = m F\u2191\u2126\u00d7F \u2229\u00a9m\u2126 [F ]\u21d1\u2126\u00d7F (Y ), \u2200Y \u2282 \u2126\u00d7F (23)\nEnsuite il faut marginaliser la fonction de masse obtenue pour revenir dans l\u2019espace \u2126 :\nm\u2126\u00d7F\u2193\u2126 (X) = \u2211\n{Y\u2286\u2126\u00d7F|Proj(Y \u2193\u2126)=X}\nm\u2126\u00d7FConj (Y ) (24)\no\u00f9 Proj (Y \u2193 \u2126) est la projection de Y sur \u2126. Nous retrouvons ainsi :\n\u03b1m\u2126(X) = m\u2126\u00d7F\u2193\u2126 (X) (25)\nMercier (2006) a propos\u00e9 une extension de cet affaiblissement en contextualisant le coefficient d\u2019affaiblissement \u03b1 en fonction de sous-ensembles de \u2126."}, {"heading": "3 Int\u00e9gration de l\u2019ind\u00e9pendance dans une fonction de masse", "text": "Nous avons vu que la notion de l\u2019ind\u00e9pendance est g\u00e9n\u00e9ralement une information suppl\u00e9mentaire n\u00e9cessaire \u00e0 la fusion d\u2019informations, mais non prise en compte dans le formalisme choisi. La section 2.2 propose une mod\u00e9lisation et estimation d\u2019une mesure d\u2019ind\u00e9pendance dans le cadre de la th\u00e9orie des fonctions de croyance. Nous allons ici nous appuyer sur le principe de l\u2019affaiblissement pr\u00e9sent\u00e9 dans la section 2.3 afin de tenir compte de l\u2019ind\u00e9pendance dans les fonctions de masse en vue de la combinaison.\nEn effet, lors de la combinaison conjonctive par exemple l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance cognitive des sources d\u2019informations est n\u00e9cessaire. Si les sources ne sont pas ind\u00e9pendantes on peut penser qu\u2019elles ne devraient pas \u00eatre combin\u00e9es par ce biais. Cependant, comme le montre la section 2.2 les sources peuvent avoir des degr\u00e9s de d\u00e9pendance et d\u2019ind\u00e9pendance. L\u2019information fournie sur l\u2019ind\u00e9pendance n\u2019est pas cat\u00e9gorique. Ainsi, combiner deux sources ind\u00e9pendantes fortement devraient tendre vers le r\u00e9sultat de la combinaison de deux sources ind\u00e9pendantes. Si une source est d\u00e9pendante d\u2019une autre source, nous pouvons consid\u00e9rer que cette premi\u00e8re source ne doit pas influer la combinaison avec une seconde source. Ainsi cette source doit repr\u00e9senter l\u2019\u00e9l\u00e9ment neutre de la combinaison.\nDans ce cas, il suffit d\u2019appliquer la proc\u00e9dure d\u2019affiblissement de la section 2.3 sur la fonction de masse m\u2126 de la source S1 en consid\u00e9rant l\u2019ind\u00e9pendance donn\u00e9e par la fonction de masse de l\u2019\u00e9quation (11) au lieu de celle de l\u2019\u00e9quation (20) dans le cas de la fiabilit\u00e9.\n\u00c0 pr\u00e9sent, nous distinguons la d\u00e9pendance positive de la d\u00e9pendance n\u00e9gative. Si une source est d\u00e9pendante positivement d\u2019une autre source, il ne faut pas en tenir compte et donc tendre vers un r\u00e9sultat de combinaison qui prendrait cette premi\u00e8re source comme un \u00e9l\u00e9ment neutre. Enfin si une source est d\u00e9pendante n\u00e9gativement d\u2019une autre source, il peut \u00eatre int\u00e9ressant de marquer cette d\u00e9pendance conflictuelle en augmentant la masse sur l\u2019ensemble vide.\nPour r\u00e9aliser ce sch\u00e9ma, nous proposons d\u2019affaiblir les fonctions de masse d\u2019une source S1 en fonction de sa mesure d\u2019ind\u00e9pendance \u00e0 une autre source S2, donn\u00e9e par la fonction de massemI de l\u2019\u00e9quation (15). Nous r\u00e9\u00e9crivons ici cette fonctions de masse de fa\u00e7on \u00e0 simplifier\nles notations :  mI(I) = \u03b1\u03b2 mI(P ) = \u03b1(1\u2212 \u03b2)\u03b3 mI(P\u0304 ) = \u03b1(1\u2212 \u03b2)(1\u2212 \u03b3) mI(I \u222a P \u222a P\u0304 ) = 1\u2212 \u03b1\n(26)\nAinsi, le param\u00e8tre \u03b1 repr\u00e9sente la fiabilit\u00e9 de la source S1, \u03b2 l\u2019ind\u00e9dendance de S1 face \u00e0 S2 et \u03b3 la d\u00e9pendance positive de S1 face \u00e0 S2. Ces trois param\u00e8tres, \u03b1, \u03b2 et \u03b3 sont compris entre 0 et 1.\nNous consid\u00e9rons ici une fonction de masse d\u2019une source m\u2126 en fonction de son ind\u00e9pendance ou d\u00e9pendance \u00e0 une autre source. Ainsi nous d\u00e9finissons : m\n\u2126[I](X) = m\u2126(X) m\u2126[P\u0304 ](X) = m\u2126\u2205 (X) m\u2126[P ](X) = m\u2126\u2126(X)\n(27)\no\u00f9 m\u2126\u2126(X) = 1 si X = \u2126 et 0 sinon et m \u2126 \u2205 (X) = 1 si X = \u2205 et 0 sinon. Suivant la proc\u00e9dure d\u2019affaiblissement, nous effectuons une extension \u00e0 vide sur la fonction de masse mI :\nmI\u2191\u2126\u00d7I (Y ) = { mI (X) si Y = \u2126\u00d7X, X \u2286 I 0 sinon (28)\nLe d\u00e9conditionnement des fonctions de masse m\u2126[I], m\u2126[P ] et m\u2126[P\u0304 ] est donn\u00e9 par :\nm\u2126 [I] \u21d1\u2126\u00d7I ( (A\u00d7 I) \u222a (\u2126\u00d7 I) ) = m\u2126 [I] (A) , A \u2286 \u2126 (29)\no\u00f9 I\u0304 = P \u222a P\u0304 .\nm\u2126 [ P\u0304 ]\u21d1\u2126\u00d7I ( (A\u00d7 P\u0304 ) \u222a (\u2126\u00d7 {I \u222a P}) ) = m\u2126 [ P\u0304 ] (A) , A \u2286 \u2126 (30)\nm\u2126 [P ] \u21d1\u2126\u00d7I ( (A\u00d7 P ) \u222a (\u2126\u00d7 {I \u222a P\u0304}) ) = m\u2126 [P ] (A) , A \u2286 \u2126 (31)\nCe dernier d\u00e9conditionnement m\u00e8ne en fait \u00e0 la masse de l\u2019ignorance et est l\u2019\u00e9l\u00e9ment neutre de la combinaison conjonctive.\nNous r\u00e9alisons ensuite la combinaison conjonctive :\nm\u2126\u00d7IConj(Y ) = m I\u2191\u2126\u00d7I \u2229\u00a9m\u2126 [I]\u21d1\u2126\u00d7I \u2229\u00a9m\u2126\n[ P\u0304 ]\u21d1\u2126\u00d7I (Y ), \u2200Y \u2282 \u2126\u00d7 I (32)\nLa marginalisation de la fonction de masse permet ensuite de revenir dans l\u2019espace \u2126 :\nm\u2126\u00d7I\u2193\u2126 (X) = \u2211\n{Y\u2286\u2126\u00d7I|Proj(Y \u2193\u2126)=X}\nm\u2126\u00d7IConj (Y ) (33)\nCette proc\u00e9dure r\u00e9alis\u00e9e pour la source S1 en rapport \u00e0 la source S2 peut \u00eatre r\u00e9alis\u00e9e pour la source S2 au regard de la source S1. Ainsi les deux fonctions de masse obtenue peuvent \u00eatre combin\u00e9es par la r\u00e8gle de combinaison conjonctive qui suppose l\u2019ind\u00e9pendance."}, {"heading": "4 Illustration", "text": ""}, {"heading": "4.1 Fonctionnement de l\u2019affaiblissement par la mesure d\u2019ind\u00e9pendance", "text": "Nous allons dans un premier temps illustrer le fonctionnement de l\u2019affaiblissement par la mesure d\u2019ind\u00e9pendance. Nous consid\u00e9rons ici un cadre de discernement \u2126 = {\u03c91, \u03c92, \u03c93}. Supposons que nous ayons deux sources S1 et S2 donnant deux fonctions de masse :\nm\u21261 (\u03c91) = 0.2, m \u2126 1 (\u03c91 \u222a \u03c92) = 0.5, m\u21261 (\u2126) = 0.3, (34)\nm\u21262 (\u03c92) = 0.1, m \u2126 2 (\u03c91 \u222a \u03c92) = 0.6, m\u21262 (\u2126) = 0.3 (35)\nLa combinaison conjonctive donne :\nm\u21261\u22292(\u2205) = 0.02, m\u21261\u22292(\u03c91) = 0.18, m\u21261\u22292(\u03c92) = 0.08, m\u21261\u22292(\u03c91 \u222a \u03c92) = 0.63, m\u21261\u22292(\u2126) = 0.09\nCette combinaison conjonctive est effectu\u00e9e avec l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance cognitive des deux sources. Si une connaissance externe permet de mesurer la d\u00e9pendance positive et n\u00e9gative de la source S1 par rapport \u00e0 la source S2 telle que fournie par l\u2019\u00e9quation (36), nous devons en tenir compte avant la combinaison conjonctive. Supposons ainsi que \u03b1 = 0.95, \u03b2 = 0.05 et \u03b3 = 0.95 dans l\u2019\u00e9quation (36). Cette fonction de masse traduit donc une forte d\u00e9pendance positive de S1 par rapport \u00e0 S2. Nous avons ainsi la fonction de masse : mI(I) = 0.0475 mI(P ) = 0.8574 mI(P\u0304 ) = 0.0451 mI(I \u222a P \u222a P\u0304 ) = 0.05 (36)\nLe tableau 1 pr\u00e9sente les diff\u00e9rentes \u00e9tapes d\u2019extension \u00e0 vide, de d\u00e9conditionnement et de combinaison dans l\u2019espace \u2126 \u00d7 I. L\u2019extension \u00e0 vide et le d\u00e9conditionnement transf\u00e8rent les masse sur les \u00e9l\u00e9ments focaux correspondant de l\u2019espace \u2126\u00d7 I. La combinaison des trois fonctions de masse dans cet espace fait appara\u00eetre la masse sur l\u2019ensemble vide qui correspond \u00e0 la part de d\u00e9pendance n\u00e9gative.\nLe tableau 2 pr\u00e9sente ensuite la marginalisation et le r\u00e9sultat de combinaison avec la fonction de masse m2 non modifi\u00e9e (i.e. que l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance totale de S2 par rapport \u00e0 S1 est faite). Nous constatons que la masse transf\u00e9r\u00e9e sur l\u2019ignorance devient plus importante que lors de la combinaison conjonctive sans hypoth\u00e8se sur la d\u00e9pendance positive.\nAfin de bien illustrer le transfert de masse sur l\u2019ensemble vide et sur l\u2019ignorance, les figures 1 et 2 repr\u00e9sentent les masses en fonction des variations de \u03b1, \u03b2 et \u03b3 pour une fonction de masse dogmatique quelconque. Ainsi sur la figure 1 repr\u00e9sentant les variations de masse sur l\u2019ensemble vide, \u03b1 est fix\u00e9 \u00e0 1, \u03b2 et \u03b3 variant, alors que sur la figure 2 repr\u00e9sentant les variations de masse sur l\u2019ignorance, \u03b3 est fix\u00e9 \u00e0 1, \u03b1 et \u03b2 variant.\nLa figure 1 montre ainsi que plus \u03b2 et \u03b3 sont petits plus on obtient une masse importante sur l\u2019ensemble vide et donc une d\u00e9pendance n\u00e9gative. La quantit\u00e9 \u03b2 repr\u00e9sente la part d\u2019ind\u00e9pendance et la quantit\u00e9 \u03b3 repr\u00e9sente la part de d\u00e9pendance positive.\nLa figure 2 pr\u00e9sente quand \u00e0 elle, la variation de la masse sur \u2126, l\u2019ignorance. Cette masse est donn\u00e9e directement par \u03b1\u03b2 qui contient donc la part d\u2019ind\u00e9pendance \u03b2 et la fiabilit\u00e9 \u03b1 de la source.\nfocal mI\u2191\u2126\u00d7I m\u2126[I]\u21d1\u2126\u00d7I m\u2126[P\u0304 ]\u21d1\u2126\u00d7I m\u2126\u00d7IConj \u2205 0.0451\n\u03c91 \u00d7 I 0.0095 (\u03c91 \u222a \u03c92)\u00d7 I 0.0237\n\u2126\u00d7 I 0.0475 0.0142 \u2126\u00d7 P 0.8574 0.8574\n(\u03c91 \u00d7 I) \u222a (\u2126\u00d7 P ) 0.01 ((\u03c91 \u222a \u03c92)\u00d7 I) \u222a (\u2126\u00d7 P ) 0.025\n\u2126\u00d7 P\u0304 0.0451 \u2126\u00d7 (P \u222a P\u0304 ) 1\n(\u03c91 \u00d7 I) \u222a (\u2126\u00d7 (P \u222a P\u0304 )) 0.2 ((\u03c91 \u222a \u03c92)\u00d7 I) \u222a (\u2126\u00d7 (P \u222a P\u0304 )) 0.5\n\u2126\u00d7 I 0.05 0.3\nTAB. 1 \u2013 D\u00e9tails de l\u2019affaiblissement de la mesure d\u2019ind\u00e9pendance : fonctions de masse dans \u2126\u00d7 I.\nfocal m\u2126\u00d7I\u2193\u21261 m \u2126 2 m \u2126\u00d7I\u2193\u2126 1 \u2229\u00a9m \u2126 2\n\u2205 0.0451 0.0461 \u03c91 0.0095 0.0085 \u03c92 0.1 0.0945 \u03c91 \u222a \u03c92 0.0237 0.6 0.5743 \u2126 0.9216 0.3 0.2765\nTAB. 2 \u2013 D\u00e9tails de l\u2019affaiblissement de la mesure d\u2019ind\u00e9pendance : marginalisation et combinaison\nNous illustrons ainsi le r\u00e9sultat escompt\u00e9 de l\u2019affaiblissement par la mesure d\u2019ind\u00e9pendance, c\u2019est-\u00e0-dire que nous retrouvons sur la masse de l\u2019ensemble vide la quantit\u00e9 de d\u00e9pendance n\u00e9gative et sur l\u2019ignorance la quantit\u00e9 de fiabilit\u00e9 et de d\u2019ind\u00e9pendance."}, {"heading": "4.2 Influence sur le r\u00e9sultat de combinaison", "text": "Afin d\u2019illustrer l\u2019influence de la prise en compte de la mesure d\u2019ind\u00e9pendance sur les fonctions de masse, nous allons consid\u00e9rer ici les deux sources pr\u00e9c\u00e9dentes S1 et S2 qui fournissent les fonctions de masse donn\u00e9es par les \u00e9quations (34) et (35). Nous allons consid\u00e9rer trois cas pour chaque source avec un cas o\u00f9 la source S1 est plut\u00f4t ind\u00e9pendante de S2 (\u03b1 = 0.95, \u03b2 = 0.95, \u03b3 = 0.05), un cas o\u00f9 elle est plut\u00f4t d\u00e9pendante positivement (\u03b1 = 0.95, \u03b2 = 0.05, \u03b3 = 0.95) et un cas o\u00f9 elle est plut\u00f4t d\u00e9pendante n\u00e9gativement (\u03b1 = 0.95, \u03b2 = 0.05,\n\u03b3 = 0.05). Pour la source S2 nous consid\u00e9rons trois cas moins cat\u00e9gorique en fixant la fiabilit\u00e9 \u03b1 = 0.9 : le cas plut\u00f4t ind\u00e9pendant (\u03b2 = 0.9, \u03b3 = 0.1), le cas plut\u00f4t d\u00e9pendant positivement (\u03b2 = 0.1, \u03b3 = 0.9) et le cas plut\u00f4t d\u00e9pendant n\u00e9gativement (\u03b2 = 0.1, \u03b3 = 0.1).\nAinsi, le tableau 3 pr\u00e9sente les r\u00e9sultats de la combinaison des deux sources en fonction des hypoth\u00e8ses d\u2019ind\u00e9pendance et de d\u00e9pendance, positive ou n\u00e9gative des deux sources S1 et S2. Nous constatons que lorsque les deux sources sont plut\u00f4t ind\u00e9pendantes l\u2019une de l\u2019autre, les r\u00e9sultats obtenus sont proches de ceux obtenus par la combinaison conjonctive directe sous l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance. Lorsqu\u2019une des deux sources est d\u00e9pendante n\u00e9gativement de l\u2019autre, la masse transf\u00e9r\u00e9e sur l\u2019ensemble vide est importante. Lorsque l\u2019une des deux sources est d\u00e9pendante positivement la masse transf\u00e9r\u00e9e sur l\u2019ignorance mais de fa\u00e7on moins importante que pour la d\u00e9pendance n\u00e9gative. En effet, l\u2019ensemble vide est un \u00e9l\u00e9ment absorbant\nS2 : \u03b1 = 0.9 cas \u00e9l\u00e9ment \u03b2 = 0.9, \u03b3 = 0.1 \u03b2 = 0.1, \u03b3 = 0.9 \u03b2 = 0.1, \u03b3 = 0.1\nfocal m\u2126\u00d7I\u2193\u21261 m \u2126\u00d7I\u2193\u2126 2 m \u2126\u00d7I\u2193\u2126 1\u22292 m \u2126\u00d7I\u2193\u2126 2 m \u2126\u00d7I\u2193\u2126 1\u22292 m \u2126\u00d7I\u2193\u2126 2 m \u2126\u00d7I\u2193\u2126 1\u22292\nS1 \u2205 0.0451 0.081 0.1371 0.081 0.1240 0.729 0.7428 \u03b1 = 0.95 \u03c91 0.1805 0.1513 0.1643 0.0473 \u03b2 = 0.95 \u03c92 0.081 0.0627 0.009 0.007 0.009 0.007 \u03b3 = 0.05 \u03c91 \u222a \u03c92 0.4513 0.486 0.5352 0.054 0.4281 0.054 0.1357\n\u2126 0.3231 0.352 0.1137 0.856 0.2766 0.208 0.0672 S1 \u2205 0.0451 0.081 0.1232 0.081 0.1226 0.729 0.7413 \u03b1 = 0.95 \u03c91 0.0095 0.008 0.0086 0.0025 \u03b2 = 0.05 \u03c92 0.081 0.0766 0.009 0.0085 0.009 0.0085 \u03b3 = 0.95 \u03c91 \u222a \u03c92 0.0238 0.486 0.4678 0.0054 0.0714 0.054 0.056\n\u2126 0.9216 0.352 0.3244 0.856 0.7889 0.208 0.1917 S1 \u2205 0.8574 0.081 0.8697 0.081 0.869 0.729 0.9614 \u03b1 = 0.95 \u03c91 0.0095 0.008 0.0087 0.0025 \u03b2 = 0.05 \u03c92 0.081 0.0108 0.009 0.0012 0.009 0.0012 \u03b3 = 0.05 \u03c91 \u222a \u03c92 0.0237 0.486 0.073 0.054 0.0275 0.054 0.0121\n\u2126 0.1094 0.352 0.0385 0.856 0.0936 0.208 0.0228\nTAB. 3 \u2013 R\u00e9sultats de combinaison selon les hjypoth\u00e8ses de d\u00e9pendance et d\u2019ind\u00e9pendance des deux sources S1 et S2.\npour la combinaison conjonctive. Cette masse sur l\u2019ensemble vide \u00e0 l\u2019issue de la combinaison conjonctive peut ainsi jouer un r\u00f4le d\u2019alerte sur la d\u00e9pendance n\u00e9gative. Une autre alternative serait d\u2019envisager une autre r\u00e8gle de combinaison lorsque la masse issue de la d\u00e9pendance n\u00e9gative est trop importante."}, {"heading": "5 Conclusion", "text": "Cet article souligne l\u2019importance de mesurer la r\u00e9elle ind\u00e9pendance des sources et d\u2019en tenir compte en vue de la combinaison des informations issues de celles-ci. Nous nous restreignons ici \u00e0 la th\u00e9orie des fonctions de croyance qui repr\u00e9sente un cadre assez g\u00e9n\u00e9ral pour la fusion d\u2019informations. De ce contexte th\u00e9orique, nous avons montr\u00e9 une approche originale pour int\u00e9grer une mesure d\u2019ind\u00e9pendance exprim\u00e9e sous la forme d\u2019une fonction de masse. Nous avons ensuite explicit\u00e9 les fonctions de masse conditionnellement \u00e0 leur ind\u00e9pendance mutuelle par un proc\u00e9d\u00e9 d\u2019affaiblissement des masses initiales. Ce principe doit \u00eatre r\u00e9alis\u00e9 en vue d\u2019une combinaison de ces fonctions de masse qui n\u00e9cessite l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance.\nUne autre approche envisageable serait d\u2019int\u00e9grer la mesure d\u2019ind\u00e9pendance dans la combinaison des fonctions de masse.\nR\u00e9f\u00e9rences Ben Yaghlane, B., P. Smets, et K. Mellouli (2002a). Belief function independence : I. the\nmarginal case. International Journal of Approximate Reasoning 29(1), 47\u201370. Ben Yaghlane, B., P. Smets, et K. Mellouli (2002b). Belief function independence : II. the\nconditional case. International Journal of Approximate Reasoning 29(1), 47\u201370. Chebbah, M., A. Martin, et B. Ben Yaghlane (2012). Positive and negative dependence for\nevidential database enrichment. In IPMU, Italy, pp. 575\u2013584. Chebbah, M., A. Martin, et B. Ben Yaghlane (2013). D\u00e9pendance et ind\u00e9pendance des sources\nimparfaites. In EGC, Toulouse, France. Dempster, A. P. (1967). Upper and Lower probabilities induced by a multivalued mapping.\nAnnals of Mathematical Statistics 38, 325\u2013339. Den\u0153ux, T. (2008). Conjunctive and disjunctive combination of belief functions induced by\nnondistinct bodies of evidence. Artificial Intelligence 172, 234\u2013264. Jousselme, A.-L., D. Grenier, et E. Boss\u00e9 (2001). A new distance between two bodies of\nevidence. Information Fusion 2, 91\u2013101. Martin, A. (2005). Fusion de classifieurs pour la classification d\u2019images sonar. RNTI Extraction\ndes connaissances : Etat et perspectives E-5, 259\u2013268. Martin, A. (2010). Le conflit dans la th\u00e9orie des fonctions de croyance. In EGC, Hammamet,\nTunisie. Mercier, D. (2006). Fusion d\u2019informations pour la reconnaissance automatique d\u2019adresses\npostales dans le cadre de la th\u00e9orie des fonctions de croyance. Ph. D. thesis, Universit\u00e9 de Technologie de Compi\u00e8gne.\nShafer, G. (1976). A mathematical theory of evidence. Princeton University Press. Smets, P. (1993). Belief Functions : the Disjunctive Rule of Combination and the Generalized\nBayesian Theorem. International Journal of Approximate Reasoning 9, 1\u201335."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "R\u00e9sum\u00e9. La fusion d\u2019informations fait intervenir plusieurs sources d\u2019informations afin d\u2019am\u00e9liorer la d\u00e9cision en terme de certitude et de pr\u00e9cision. Quelle que soit l\u2019approche retenue pour r\u00e9aliser la fusion d\u2019informations, l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance est g\u00e9n\u00e9ralement une hypoth\u00e8se forte et incontournable. Nous proposons dans cet article une approche permettant d\u2019int\u00e9grer une mesure d\u2019ind\u00e9pendance avant de r\u00e9aliser la combinaison des informations dans le cadre de la th\u00e9orie des fonctions de croyance.", "creator": "LaTeX with hyperref package"}}}