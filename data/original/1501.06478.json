{"id": "1501.06478", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2015", "title": "Compressed Support Vector Machines", "abstract": "Support vector machines (SVM) can classify data sets along highly non-linear decision boundaries because of the kernel-trick. This expressiveness comes at a price: During test-time, the SVM classifier needs to compute the kernel inner-product between a test sample and all support vectors. With large training data sets, the time required for this computation can be substantial. In this paper, we introduce a post-processing algorithm, which compresses the learned SVM model by reducing and optimizing support vectors. We evaluate our algorithm on several medium-scaled real-world data sets, demonstrating that it maintains high test accuracy while reducing the test-time evaluation cost by several orders of magnitude---in some cases from hours to seconds. It is fair to say that most of the work in this paper was previously been invented by Burges and Sch\\\"olkopf almost 20 years ago. For most of the time during which we conducted this research, we were unaware of this prior work. However, in the past two decades, computing power has increased drastically, and we can therefore provide empirical insights that were not possible in their original paper.", "histories": [["v1", "Mon, 26 Jan 2015 16:51:34 GMT  (1724kb,D)", "https://arxiv.org/abs/1501.06478v1", null], ["v2", "Mon, 2 Feb 2015 16:24:38 GMT  (1724kb,D)", "http://arxiv.org/abs/1501.06478v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhixiang xu", "jacob r gardner", "stephen tyree", "kilian q weinberger"], "accepted": false, "id": "1501.06478"}, "pdf": {"name": "1501.06478.pdf", "metadata": {"source": "CRF", "title": "Compressed Support Vector Machines", "authors": ["Zhixiang (Eddie", "Jacob R. Gardner", "Stephen Tyree", "Kilian Q. Weinberger"], "emails": ["xuzx@cse.wustl.edu", "gardner.jake@gmail.com", "swtyree@wustl.edu", "kilian@wustl.edu"], "sections": [{"heading": "1 Introductions", "text": "Support Vector Machines (SVM) are arguably one of the great success stories of machine learning and have been used in many real world applications, including email spam classification [7], face recognition [12] and gene selection [11]. In real world applications, the evaluation cost (in terms of memory and CPU) during test-time is of crucial importance. This is particularly prominent in settings with strong resource constraints (e.g. embedded devices, cell phones or tablets) or frequently repeated tasks (e.g. webmail spam classification, web-search ranking, face detection in uploaded images), which can be performed billions of times per day. Reducing the resource requirements to classify an input can reduce hardware costs, enable product improvements, and help curb power consumption.\nTest-time cost is determined mainly by two components: classifier evaluation and feature extraction cost. Reducing feature extraction cost has recently obtained a significant amount of attention [3; 5; 15; 17; 19; 23; 25; 26]. These approaches reduce the test-time cost in scenarios where features are heterogeneous, extracted on-demand, and are significantly more expensive to compute than the classifier evaluation.\nIn this paper, we focus on the other common scenario where the classifier evaluation cost dominates the overall test-time cost. Specifically, we focus on kernel support vector machine (SVM) [20].\nar X\niv :1\n50 1.\n06 47\n8v 2\n[ cs\n.L G\n] 2\nKernel computation can be expensive because it is linear in the number of support vectors and, in addition, often requires expensive exponentiation (e.g. for the radial basis or \u03c72 kernels). Previous work has reduced the classifier complexity by selecting few support vectors through budgeted training [6; 24] or with heuristic selection prior to learning [13].\nWe describe an approach that does not select support vectors from the training set, but instead learns them to match a pre-defined SVM decision boundary. Given an existing SVM model withm support vectors, it learns r m \u201cartificial support vectors\u201d, which are not originally part of the training set. The resulting model is a standard SVM classifier (thus can be saved, for example, in a LibSVM [4] compatible file). Relative to the original model, it has comparable accuracy, but it is up to several orders of magnitudes smaller and faster to evaluate. We refer to our algorithm as Compressed Vector Machine (CVM) and demonstrate on eight real-world data sets of various size and complexity that it achieves unmatched accuracy vs. test-time cost trade offs."}, {"heading": "2 Related Work", "text": "Burges and Scho\u0308lkopf [2] invented Compressed Vector Machines long before us. While we conducted our research, we were not aware of their work until very late during the final stages of paper writing. We still consider our perspective and additional experiments valuable and decided to post our results as a techreport. However we do want to emphasize that all academic credit should go to them who were clearly ahead of us.\nReducing test-time cost has recently attracted much attention. Much work [3; 5; 10; 15; 17; 19; 23; 25] focuses on scenarios where features are extracted on-demand and the extraction cost dominates the overall test-time cost. Their objective is to minimize the feature extraction cost.\nModel compression was pioneered by [1]. Our work was inspired by their vision, however it differs substantially, as we do not focus on ensembles of classifiers and instead learn a model compressor explicitly for SVMs. More recently, [26] introduces an algorithm to reduce the test-time cost specifically for the SVM classifier. However, similar to the approaches mentioned above, they focus on learning a new representations consisting of cheap non-linear features for linear SVMs.\n[6] propose an algorithm to limit the memory usage for kernel based online classification. Different from our approach, their algorithm is not a post-process procedure, and instead they modify the kernel function directly to limit the amount of memory the algorithm uses. Similar to [6], [24] also focusses on online kernel SVM, and attacks primarily the training time complexity.\nOf particular relevance is [13], which, specifically reduces the SVM evaluation cost by reducing the number of support vectors. Heuristics are used to select a small subset of support vectors, up to a given budget, during training time, thus solving an approximate SVM optimization. In contrast, our method is a post-processing compression to the regular SVM. We begin from an exact SVM solution and compress the set of support vectors by choosing and optimizing over a small set of support vectors to approximate the optimal decision boundary. This post-processing optimization framework renders unmatched accuracy and cost performance. Similar approaches have successfully learned pseudo-inputs for compressed nearest neighbor classification sets [14] and sparse Gaussian process regression models [22]."}, {"heading": "3 Background", "text": "Let the data consist of input vectors {x1, . . . ,xn} \u2208 Rd and corresponding labels {y1, . . . , yn} \u2208 {\u22121,+1}. For simplicity we assume binary classification in the following section, but our algorithm is easily extended to multi-class settings using one-vs-one [21], one-vs-all [18], or DAG [16] approaches, and results are included for several multi-class datasets.\nKernel support vector machines. SVMs are popular for their large margin enforcement, which leads to good generalization to unseen test data, and their formulation as a convex quadratic optimization problem, guaranteeing a globally optimal solution. Most importantly, the kernel-trick [20] may be employed to learn highly non-linear decision boundaries for data sets that are not linearly separable. Specifically, the kernel-trick maps the original feature space xi into a higher (possibly infinite) dimensional space \u03c6(xi).\nSVMs learn a hyperplane in this higher dimensional space by maximizing the margin 1\u2016w\u2016 and penalizing training instances on the wrong side of the hyperplane,\nmin w,b \u2016w\u2016+ C n\u2211 i ( max ( 1\u2212 yiw>\u03c6(xi) + b, 0 ))2 , (1)\nwhere b is the bias, and C trades-off regularization/margin and training accuracy. Note that we use the quadratic hinge loss penalty and thus (1) is differentiable. The power of the kernel trick is that the higher dimensional space \u03c6(xi) never needs to be expressed explicitly, because (1) can be formulated in terms of inner products between input vectors. Let a matrix K denote these inner products, where Kij = \u03c6(xi)>\u03c6(xj), and K is the training kernel matrix. The optimization in (1) can be then expressed in terms of kernel matrix K in the dual form:\nmax \u03b11,...,\u03b1n n\u2211 i=1 \u03b1i \u2212 1 2 n\u2211 i,j=1 \u03b1i\u03b1jyiyjKij , s.t. n\u2211 i=1 \u03b1iyi = 0 and \u03b1i \u2265 0, (2)\nwhere \u03b1i are the Lagrange multipliers.\nthe classification rule f(\u00b7) for a test input xt can also be expressed by testing kernel K\u0303 that consists of inner products between test inputs E = {xt} and support vectors S = {xi|\u03b1i 6= 0}, K\u0303it = \u03c6(xi) >\u03c6(xt), where\nf(\u03c6(xt)) = n\u2211 i=1 \u03b1iyiK\u0303it + b. (3)\nNote that once testing kernel K\u0303 is computed, generating the prediction is merely a linear combination, and thus the dominating cost is computing the testing kernel itself.\nLeast angle regression. LARS [8] is a widely used forward selection algorithm because of its simplicity and efficiency. Given input vectors x, target labels y, and the quadratic loss `(\u03b2) = (x\u03b2\u2212y)2, LARS learns to approximate targets by building up the coefficient vector \u03b2 in successive steps, starting from an all-zero vector. To minimize the loss function `, LARS initially descends on a coordinate direction that has the largest gradient,\n\u03b2t = argmax \u03b2t\n\u2202`\n\u2202\u03b2t . (4)\nThe algorithm then incorporates this coordinate into its active set. After identifying the gradient direction, LARS selects the step size very carefully. Instead of too greedy or too tiny, LARS computes a step size that a new direction outside of the active set has the same maximum gradient as directions in the active set. LARS then include this new direction into the active set.\nIn the following iterations, LARS gradient descends on a direction that maintains the same gradient for all directions in the active set. In other words, LARS descends following an equiangular direction of all directions in the active set. The algorithm then repeats computing step-size, including new directions into the active set, and descending on an equiangular directions. This process makes LARS very efficient, as after T iterations, LARS solution has exactly T directions in the active set, or equivalently, only T non-zero coefficients in \u03b2."}, {"heading": "4 Method", "text": "In this section, we detail the CVM approach to reduce the test-time SVM evaluation cost. We regard CVM as a post-processing compression to the original SVM solution. After solving an SVM, we obtain a set of support vectors S = {xi|\u03b1i 6= 0}, and the corresponding Lagrange multipliers \u03b1i. Given the original SVM solution, we can model the test-time evaluation cost explicitly.\nKernel SVM evaluation cost. Based on the prediction function (3) we can formulate the exact SVM classifier evaluation cost. Let e denote the cost of computing a test kernel entry K\u0303it (i.e. kernel function of a test input xt and a support vector xi). We assume the computation cost is identical across all test inputs and all support vectors. As shown in (3), generating a prediction for a\ntesting input requires computing the kernel entry between the test input and all support vectors. The total evaluation cost is a function of the number of support vectors nsv. After obtaining the kernel entries for a test point xt, prediction is simply linear combination of the kernel row K\u0303t weighted by \u03b1. The cost of computing this linear combination is very low compared to the kernel computation, and therefore the total evaluation cost ce = nsve. We aim to reduce the size of the support vector set nsv without greatly affecting prediction accuracy.\nRemoving non-support vectors. Since the test-time evaluation cost is a function of the number of support vectors, the goal is to cherry-pick and optimize a subset of the optimal support vectors bounded in size by a user-specified compression ratio. We first note that all non-support vectors can be removed during this process without affecting the full SVM solution. If we define a design matrix K\u0302 \u2208 Rn\u00d7n, where K\u0302ij = yiKij . The squared penalty SVM objective function in (1) can be expressed with Lagrange parameter \u03b1 and the kernel matrix K:\nmin \u03b1,b\n( max(1\u2212 K\u0302\u03b1\u2212 yb, 0) )2 + \u03b1>K\u03b1. (5)\nSince (5) is a strongly convex function, and all non-support vectors have the corresponding Lagrange multiplier \u03b1i = 0, we can remove all non-support vectors from the optimization problem and the full SVM optimal solution stays the same.\nTo find an optimal subset of support vectors given the compression ratio, we re-train the SVM with only support vectors and a constraint on the number of support vectors. Note that \u03b1 are effectively the coefficients of support vectors, and we can efficiently control the number of support vectors by adding an l0 norm on \u03b1. The optimization problem becomes\nmin \u03b1,b\n( 1\u2212 K\u0302\u03b1\u2212 yb )2 + \u03b1>K\u03b1 (6)\ns.t.\u2016\u03b1\u20160 \u2264 1\ne Be,\nwhere Be evaluation cost budget, and consequently, 1eBe is the desired number of support vectors based on the budget. Note that after removing non-support vectors, we obtain a condensed matrix K\u0302 \u2208 Rnsv\u00d7nsv . Forming ordinary least squares problem. The current form of equation (6) can be made more amenable to optimization by rewriting the objective function as an ordinary least square problem. Expanding the squared term, simplifying, and fixing the bias term b (as it does not affect the solution dramatically), we re-format the objective function (6) into\nmin \u03b1\n(1\u2212 yb)>(1\u2212 yb)\u2212 2\u03b1>K\u0302>(1\u2212 yb) + \u03b1>(K\u0302>K\u0302 + K)\u03b1. (7)\nWe introduce two auxiliary variables \u2126 and \u03b2, where \u2126>\u2126 = K\u0302>K\u0302+K and \u2126>\u03b2 = \u2212K\u0302>(1\u2212yb). Because K\u0302>K\u0302 + K is a symmetric matrix, we can compute its eigen-decomposition\nK\u0302>K\u0302 + K = SDS>, (8)\nwhere D is the diagonal matrix of eigenvalues and S is the orthonormal matrix of eigenvectors. Moreover, because the matrix K\u0302>K\u0302+K is positive semi-definite, we can further decompose SDS> into an inner product of two real matrices by taking the square root of D. Let \u2126 = \u221a DS>, and we obtain a matrix \u2126 that satisfies \u2126>\u2126 = K\u0302>K\u0302 + K. After computing \u2126, we can readily compute \u03b2 = \u2212(\u2126>)\u22121K\u0302>(1\u2212 yb), where (\u2126>)\u22121 = 1\u221a\nD S>.\nWith the help of the two auxiliary variables, we convert (7), plus a constant term1, into least squares format. Together with relaxation of the non-continuous l0- norm constraint to an l1-norm constraint, we obtain\nmin \u03b1\n(\u2126\u03b1 + \u03b2)2, s.t. \u2016\u03b1\u20161 \u2264 1\ne Be. (9)\n1(1\u2212 yb)> ( K\u0302(K\u0302>K\u0302+K)\u22121K\u0302> \u2212 I ) (1\u2212 yb)\nCompressing the support vector set. The squared loss and l1 constraint in (9) lead naturally to the LARS algorithm. Given a budget Be, we can determine the maximum size m of the compressed support vector set (m = Bee ). Using LARS, we start from an empty support vector set and add m support vectors incrementally. Since adding a support vector is equivalent to activating a coefficient in \u03b1 to a non-zero value, we can obtain m optimal support vectors by running LARS optimization in (9) exactly m steps, where each step activates one coefficient. The resulting solution gives the optimal set of m support vectors. We refer this intermediate step as LARS-SVM. Note that this step is crucial for the problem, as this LARS-SVM solution serves as a very good initialization for the next step, which is a non-convex optimization problem.\nGradient support vectors. If we interpret \u03b1 as coordinates and the corresponding columns in the kernel matrix K as basis vectors, then these basis vectors span anRnsv space in which lie predictions of the original SVM model. In this compression algorithm, our goal is to find a lower dimensional subspace that supports good approximations of the original predictions. After running LARS for m iterations, we obtain m support vectors and their coefficients \u03b1, forming an Rm subspace of the space spanned by the full kernel matrix.\nWe illustrate this lower dimensional approximation in Figure 1. Vectors P1 and P2 are predictions of two training points made in the full SVM solution space (R3 and spanned by three support vectors). We want to compress the model to two support vectors by looking for a subspace V \u2208 R2 that supports the best approximations of these two predictions. Using existing support vectors as a basis, we can find subspaces V1 and V2, each spanned by a pair of support vectors. The projections of P1 and P2 on plane V1 (PV11 and P V1 2 ) are closest to the original predictions P1 and P2, and thus V1 is the better approximation. However, in this case, neither V1 nor V2 is a particularly good approximation. Suppose we remove the restriction of selecting a subspace spanned by existing basis vectors in the kernel matrix, instead optimizing the basis vectors to yield a more suitable subspace. In Figure 1, this is illustrated by the optimal subspace V \u2217 which produces a better approximation to the target predictions.\nNote that the basis vectors (columns of the kernel matrix) are parameterized by support vectors. By optimizing these underlying support vectors, we can search for a better low-dimensional subspace. If we denote Km as the training kernel matrix with only m columns corresponding to the support vectors chosen by LARS, and \u03b1m as the coefficients of these support vectors, we can formulate the search for artificial support vector as an optimization problem. Specifically, we minimize a squared loss between approximate and full SVM predictions over all support vectors, and the parameters are support vectors.\nmin (x1,...,xm)\nL = ( Km\u03b1m \u2212K\u03b1 )2 , (10)\nwhere Kij is the kernel entry, and for simplicity, we use radial basis function (RBF) kernel function (Kij = e \u2016xi\u2212xj\u20162 2\u03c32 ). However, other kernel functions are equally suitable. The unconstrained op-\ntimization problem (10) can be solved by conjugate gradient descent with respect to the chosen m support vectors. Since \u03b1\u2019s are the coordinates with respect to the basis, we optimize \u03b1 jointly with support vectors, which is faster than optimizing basis and solving coordinates alternatively. The gradients can be computed very efficiently using matrix operations. Since gradient descent on support vectors is equivalent to moving these support vectors in a continuous space, thereby generating m new support vectors, we refer to these newly generated support vectors as gradient support vectors. We denote this combined method of LARS-SVM and gradient support vectors as Compressed Vector Machine (CVM). Because the optimization problem in (10) is non-convex with respect to xi, we initialize our algorithm with the basis Km and coordinates \u03b1m returned in the LARS-SVM solution.\nIn practice, it may be desirable to optimize both the SVM cost parameter C and any kernel parameters (e.g. \u03c32 in the RBF kernel) for the final CVM model. Additionally, it may be preferable to optimize CVM constrained by the validation accuracy of the compressed model rather than the size of the support vector budget. Constrained Bayesian optimization [9] supports efficient constrained joint hyperparameter optimizations of this type. Additionally, the L1-penalized support vector selection in the LARS-SVM step may benefit from recent work on highly parallel Elastic Net solvers [27]."}, {"heading": "5 Results", "text": "In this section, we first demonstrate Compressed Vector Machine (CVM) on a synthetic data set to graphically illustrate each step in the algorithm. We then evaluate CVM on several medium-scale real-world data sets.\nSynthetic data set. The data set contains 600 sample inputs from two classes (red and blue), where each input contains two features. The blue inputs are sampled from a Gaussian distribution with mean at the origin and variance 1, and red inputs are sampled from a noisy circle surrounding the blue inputs. As shown in Figure 2(a), by design the data set is not linearly separable. For simplicity,\nwe treat all inputs as training inputs. To evaluate CVM, we first learn an SVM with the RBF kernel from the full training set. We plot the resulting optimal decision boundary in Figure 2(b) with a black curve. In total, the full model has 80 support vectors.\nTo compress the model, we first select a subset of support vectors by solving LARS-SVM optimization (9). Specifically, we compress the model to 10% of its original size, 8 support vectors, by running LARS for 8 iterations. The 8 LARS-SVM support vectors are shown in Figure 2(b) as solid gray points, and the approximate LARS-SVM decision boundary is shown by the gray curve.\nSince the subspace formed by 8 support vectors is heavily restricted by the discrete training input space, the approximation is poor. To overcome this problem, we search for a better subspace or basis in a continuous space, and perform gradient descent on support vectors by optimizing (10). In Figure 2(c-h), we illustrate the optimization with updated support vector locations and optimized decision boundaries as we gradually increase the number of iterations. The resulting gradient support vectors are shown as gray points and the new optimized decision boundaries formed from these new gradient support vectors are shown by green curves. After 2560 iterations, as shown in Figure 2(h), we can observe that the optimized decision boundary (green) is very close to the boundary captured in the full model (black). These optimized decision boundaries demonstrate that moving a small subset of support vectors in a continuous space can efficiently approximate the optimal decision boundary formed by full SVM solution, supporting effective SVM model compression.\nLarge real-world data sets. To evaluate the performance of CVM on real-world applications, we evaluate our algorithm on six data sets of varying size, dimensionality and complexity. Table 1 details the statistics of all six data sets. We use LibSVM [4] to train a regular RBF kernel SVM using regularization parameter C and RBF kernel width \u03c3 selected on a 20% validation split. For multi-class data sets, we use the one-vs-one multi-class scheme. The classification accuracy of test predictions from this SVM model serves as a baseline in Figure 3(full SVM).\nGiven the full SVM solution, we run CVM in two steps. First, we use LARS solve the optimization problem in (9) using all support vectors from the original SVM model. An initial compressed support vector set is selected with a target compressed size (e.g. 10 out of 500 support vectors). The selected support vectors serve as the second baseline in Figure 3(LARS-SVM). Second, we shift these support vectors in a continuous space by optimizing (10) w.r.t. the input support vectors and the corresponding Lagrange multipliers \u03b1, generating gradient support vectors. This final set of gradient support vectors constitutes the CVM model. To show the trend of accuracy/cost performance, we plot the classification accuracy for CVM after adding every 10 support vectors. Figure 3 shows the performance of CVM and the baselines on all six data sets.\nComparison with prior work. Figure 3 also shows a comparison of CVM with Reduced-SVM [13]. This algorithm takes an iterative two phase approach. First a set of support vectors is heuristically selected from random samples of the training set and added to the existing set of support vectors (initially empty). Then, the model weights are optimized by an SVM with the quadratic hinge loss. The algorithm alternates these two steps until the target number of support vectors is reached.\nAs shown in the Figure 3, CVM significantly improves over all baselines. Compared to the current state-of-the-art, Reduced-SVM, CVM has the capability of moving support vectors, generating a new basis, and learning a highly approximated basis to match the decision boundaries formed by the full SVM solution. It is this ability that distinguishes CVM from other algorithms when the evaluation budget is low. Across all data sets, CVM maintains close to the same accuracy as the full SVM with merely 10% of the support vectors."}, {"heading": "6 Acknowledgments", "text": "Most of this work was previously invented by Burges and Scho\u0308lkopf [2] whose research was truly visionary at the time."}], "references": [{"title": "Improving the accuracy and speed of support vector machines", "author": ["Chris C. Burges", "Bernhard Sch\u00f6lkopf"], "venue": "Advances in neural information processing systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Fast classification using sparse decision dags", "author": ["R. Busa-Fekete", "D. Benbouzid", "B. K\u00e9gl"], "venue": "In ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Libsvm: a library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Classifier cascade for minimizing feature evaluation cost", "author": ["M. Chen", "Z. Xu", "K.Q. Weinberger", "O. Chapelle"], "venue": "In AISTATS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "The forgetron: A kernel-based perceptron on a budget", "author": ["Ofer Dekel", "Shai Shalev-Shwartz", "Yoram Singer"], "venue": "SIAM Journal on Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Support vector machines for spam categorization", "author": ["Harris Drucker", "Donghui Wu", "Vladimir N Vapnik"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Least angle regression", "author": ["Bradley Efron", "Trevor Hastie", "Iain Johnstone", "Robert Tibshirani"], "venue": "The Annals of statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Bayesian optimization with inequality constraints", "author": ["Jacob Gardner", "Matt Kusner", "Kilian Q. Weinberger", "John Cunningham", "Zhixiang Xu"], "venue": "Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Speedboost: Anytime prediction with uniform near-optimality", "author": ["A. Grubb", "J.A. Bagnell"], "venue": "In AISTATS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["Isabelle Guyon", "Jason Weston", "Stephen Barnhill", "Vladimir Vapnik"], "venue": "Machine learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Face recognition with support vector machines: Global versus component-based approach", "author": ["Bernd Heisele", "Purdy Ho", "Tomaso Poggio"], "venue": "In Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Building support vector machines with reduced classifier complexity", "author": ["S Sathiya Keerthi", "Olivier Chapelle", "Dennis DeCoste"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Stochastic neighbor compression", "author": ["Matt Kusner", "Stephen Tyree", "Kilian Q. Weinberger", "Kunal Agrawal"], "venue": "Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Joint cascade optimization using a product of boosted classifiers", "author": ["L. Lefakis", "F. Fleuret"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Large margin dags for multiclass classification", "author": ["John C Platt", "Nello Cristianini", "John Shawe-taylor"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Using classifier cascades for scalable e-mail classification", "author": ["J. Pujara", "H. Daum\u00e9 III", "L. Getoor"], "venue": "In CEAS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "In defense of one-vs-all classification", "author": ["Ryan Rifkin", "Aldebaro Klautau"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Boosting classifier cascades", "author": ["M. Saberian", "N. Vasconcelos"], "venue": "In NIPS, pages 2047\u20132055", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Learning with kernels: Support vector machines, regularization, optimization, and beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "MIT press,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "The support vector machine", "author": ["Cristianini Shawe-Taylor", "Smola Sch\u00f6lkopf"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "Sparse gaussian processes using pseudo-inputs", "author": ["Edward Snelson", "Zoubin Ghahramani"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Local supervised learning through space partitioning", "author": ["J. Wang", "V. Saligrama"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Breaking the curse of kernelization: Budgeted stochastic gradient descent for large-scale svm training", "author": ["Zhuang Wang", "Koby Crammer", "Slobodan Vucetic"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Cost-sensitive tree of classifiers", "author": ["Zhixiang Xu", "Matt Kusner", "Minmin Chen", "Kilian Q. Weinberger"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Anytime representation learning", "author": ["Zhixiang (Eddie) Xu", "Matt J. Kusner", "Gao Huang", "Kilian Q. Weinberger"], "venue": "In Sanjoy Dasgupta and David McAllester, editors,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "A reduction of the elastic net to support vector machines with an application to gpu computing", "author": ["Quan Zhou", "Wenlin Chen", "Shiji Song", "Jacob R. Gardner", "Kilian Q. Weinberger", "Yixin Chen"], "venue": "In Association for the Advancement of Artificial Intelligence", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Support Vector Machines (SVM) are arguably one of the great success stories of machine learning and have been used in many real world applications, including email spam classification [7], face recognition [12] and gene selection [11].", "startOffset": 184, "endOffset": 187}, {"referenceID": 10, "context": "Support Vector Machines (SVM) are arguably one of the great success stories of machine learning and have been used in many real world applications, including email spam classification [7], face recognition [12] and gene selection [11].", "startOffset": 206, "endOffset": 210}, {"referenceID": 9, "context": "Support Vector Machines (SVM) are arguably one of the great success stories of machine learning and have been used in many real world applications, including email spam classification [7], face recognition [12] and gene selection [11].", "startOffset": 230, "endOffset": 234}, {"referenceID": 18, "context": "Specifically, we focus on kernel support vector machine (SVM) [20].", "startOffset": 62, "endOffset": 66}, {"referenceID": 11, "context": "Previous work has reduced the classifier complexity by selecting few support vectors through budgeted training [6; 24] or with heuristic selection prior to learning [13].", "startOffset": 165, "endOffset": 169}, {"referenceID": 2, "context": "The resulting model is a standard SVM classifier (thus can be saved, for example, in a LibSVM [4] compatible file).", "startOffset": 94, "endOffset": 97}, {"referenceID": 0, "context": "Burges and Sch\u00f6lkopf [2] invented Compressed Vector Machines long before us.", "startOffset": 21, "endOffset": 24}, {"referenceID": 24, "context": "More recently, [26] introduces an algorithm to reduce the test-time cost specifically for the SVM classifier.", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "[6] propose an algorithm to limit the memory usage for kernel based online classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Similar to [6], [24] also focusses on online kernel SVM, and attacks primarily the training time complexity.", "startOffset": 11, "endOffset": 14}, {"referenceID": 22, "context": "Similar to [6], [24] also focusses on online kernel SVM, and attacks primarily the training time complexity.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "Of particular relevance is [13], which, specifically reduces the SVM evaluation cost by reducing the number of support vectors.", "startOffset": 27, "endOffset": 31}, {"referenceID": 12, "context": "Similar approaches have successfully learned pseudo-inputs for compressed nearest neighbor classification sets [14] and sparse Gaussian process regression models [22].", "startOffset": 111, "endOffset": 115}, {"referenceID": 20, "context": "Similar approaches have successfully learned pseudo-inputs for compressed nearest neighbor classification sets [14] and sparse Gaussian process regression models [22].", "startOffset": 162, "endOffset": 166}, {"referenceID": 19, "context": "For simplicity we assume binary classification in the following section, but our algorithm is easily extended to multi-class settings using one-vs-one [21], one-vs-all [18], or DAG [16] approaches, and results are included for several multi-class datasets.", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "For simplicity we assume binary classification in the following section, but our algorithm is easily extended to multi-class settings using one-vs-one [21], one-vs-all [18], or DAG [16] approaches, and results are included for several multi-class datasets.", "startOffset": 168, "endOffset": 172}, {"referenceID": 14, "context": "For simplicity we assume binary classification in the following section, but our algorithm is easily extended to multi-class settings using one-vs-one [21], one-vs-all [18], or DAG [16] approaches, and results are included for several multi-class datasets.", "startOffset": 181, "endOffset": 185}, {"referenceID": 18, "context": "Most importantly, the kernel-trick [20] may be employed to learn highly non-linear decision boundaries for data sets that are not linearly separable.", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "LARS [8] is a widely used forward selection algorithm because of its simplicity and efficiency.", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "Constrained Bayesian optimization [9] supports efficient constrained joint hyperparameter optimizations of this type.", "startOffset": 34, "endOffset": 37}, {"referenceID": 25, "context": "Additionally, the L1-penalized support vector selection in the LARS-SVM step may benefit from recent work on highly parallel Elastic Net solvers [27].", "startOffset": 145, "endOffset": 149}, {"referenceID": 2, "context": "We use LibSVM [4] to train a regular RBF kernel SVM using regularization parameter C and RBF kernel width \u03c3 selected on a 20% validation split.", "startOffset": 14, "endOffset": 17}, {"referenceID": 11, "context": "Figure 3 also shows a comparison of CVM with Reduced-SVM [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "Most of this work was previously invented by Burges and Sch\u00f6lkopf [2] whose research was truly visionary at the time.", "startOffset": 66, "endOffset": 69}], "year": 2015, "abstractText": "Support vector machines (SVM) can classify data sets along highly non-linear decision boundaries because of the kernel-trick. This expressiveness comes at a price: During test-time, the SVM classifier needs to compute the kernel innerproduct between a test sample and all support vectors. With large training data sets, the time required for this computation can be substantial. In this paper, we introduce a post-processing algorithm, which compresses the learned SVM model by reducing and optimizing support vectors. We evaluate our algorithm on several medium-scaled real-world data sets, demonstrating that it maintains high test accuracy while reducing the test-time evaluation cost by several orders of magnitude\u2014in some cases from hours to seconds. It is fair to say that most of the work in this paper was previously been invented by Burges and Sch\u00f6lkopf almost 20 years ago. For most of the time during which we conducted this research, we were unaware of this prior work. However, in the past two decades, computing power has increased drastically, and we can therefore provide empirical insights that were not possible in their original paper.", "creator": "LaTeX with hyperref package"}}}