{"id": "1502.02535", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2015", "title": "On First-Order Model-Based Reasoning", "abstract": "Reasoning semantically in first-order logic is notoriously a challenge. This paper surveys a selection of {\\em semantically-guided} or {\\em model-based} methods that aim at meeting aspects of this challenge. For first-order logic we touch upon {\\em resolution-based} methods, {\\em tableaux-based} methods, {\\em DPLL-inspired} methods, and we give a preview of a new method called SGGS, for {\\em Semantically-Guided Goal-Sensitive} reasoning. For first-order theories we highlight {\\em hierarchical} and {\\em locality-based} methods, concluding with the recent {\\em Model-Constructing satisfiability calculus}.", "histories": [["v1", "Mon, 9 Feb 2015 16:14:40 GMT  (24kb)", "https://arxiv.org/abs/1502.02535v1", null], ["v2", "Tue, 9 Jun 2015 15:22:17 GMT  (30kb)", "http://arxiv.org/abs/1502.02535v2", "To appear in Narciso Marti-Oliet, Peter Olveczky, and Carolyn Talcott (Eds.), \"Logic, Rewriting, and Concurrency: Essays in Honor of Jose Meseguer\" Springer, Lecture Notes in Computer Science, September 2015, 24 pages"], ["v3", "Fri, 31 Jul 2015 21:16:13 GMT  (41kb)", "http://arxiv.org/abs/1502.02535v3", "In Narciso Marti-Oliet, Peter Olveczky, and Carolyn Talcott (Eds.), \"Logic, Rewriting, and Concurrency: Essays in Honor of Jose Meseguer\" Springer, Lecture Notes in Computer Science 9200, September 2015, 24 pages"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["maria paola bonacina", "ulrich furbach", "viorica sofronie-stokkermans"], "accepted": false, "id": "1502.02535"}, "pdf": {"name": "1502.02535.pdf", "metadata": {"source": "CRF", "title": "On First-Order Model-Based Reasoning", "authors": ["Maria Paola Bonacina", "Ulrich Furbach", "Viorica Sofronie-Stokkermans"], "emails": ["mariapaola.bonacina@univr.it", "furbach@uni-koblenz.de,", "sofronie@uni-koblenz.de"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 2.\n02 53\n5v 3\n[ cs\n.A I]"}, {"heading": "1 Introduction", "text": "Traditionally, automated reasoning has centered on proofs rather than models. However, models are useful for applications, intuitive for users, and the notion that semantic guidance would help proof search is almost as old as theorem proving itself. In recent years there has been a surge of model-based first-order reasoning methods, inspired in part by the success of model-based solvers for propositional satisfiability (SAT) and satisfiability modulo theories (SMT).\nThe core procedure of these solvers is the conflict-driven clause learning (CDCL) version [52,62,88,60] of the Davis-Putnam-Logemann-Loveland (DPLL) procedure for propositional logic [32]. The original Davis-Putnam (DP) procedure [33] was proposed for first-order logic, and featured propositional, or ground, resolution. The DPLL procedure replaced propositional resolution with splitting, initially viewed as breaking disjunctions apart by case analysis, to avoid the growth of clauses and the non-determinism of resolution. Later, splitting was understood as guessing, or deciding, the truth value of a propositional variable, in order to search for a model of the given set of clauses. This led to read DPLL as a model-based procedure, where all operations are centered around a candidate partial model, called context, represented by a sequence, or trail, of literals.\nDPLL-CDCL brought back propositional resolution as a mechanism to generate lemmas, and achieve a better balance between guessing and reasoning. The model-based character of the procedure became even more pronounced: when the\ncurrent candidate model falsifies a clause, this conflict is explained by a heuristically controlled series of resolution steps, a resolvent is added as lemma, and the candidate partial model is repaired in such a way to remove the conflict, satisfy the lemma, and backjump as far away as possible from the conflict. SMT-solvers integrate in DPLL-CDCL a decision procedure for satisfiability in a theory or combination of theories T : the T -satisfiability procedure raises a T -conflict when the candidate partial model is not consistent with T , and generates T -lemmas to add theory reasoning to the inference component [7,34].\nWhile SAT and SMT-solvers offer fast decision procedures, they typically apply to sets of propositional or ground clauses, without quantifiers. Indeed, decidability of the problem and termination of the procedure descend from the fact that the underlying language is the finite set of the input atoms.\nATP (Automated Theorem Proving) systems offer theorem-proving strategies that are designed for the far more expressive language of first-order logic, but are only semi-decision procedures for validity, as the underlying language, and search space, are infinite. This trade-off between expressivity and decidability is ubiquitous in logic and artificial intelligence. First-order satisfiability is not even semi-decidable, which means that first-order model-building cannot be mechanized in general. Nevertheless, there exist first-order reasoning methods that are semantically-guided by a fixed interpretation, and even model-based, in the sense that the state of a derivation contains a representation of a candidate partial model that evolves with the derivation.\nIn this survey, we illustrate a necessarily incomplete selection of such methods for first-order logic (Section 2) or first-order theories (Section 3). In each section the treatment approximately goes from syntactic or axiomatic approaches towards more semantic ones, also showing connections with Jose\u0301 Meseguer\u2019s work. All methods are described in expository style, and the interested reader may find the technical details in the references. Background material is available in previous surveys, such as [67,68,18,59,69] for theorem-proving strategies, [19] for decision procedures based on theorem-proving strategies or their integration with SMT-solvers, and books such as [70,17,76]."}, {"heading": "2 Model-based Reasoning in First-Order Logic", "text": "In this section we cover semantic resolution, which represents the early attempts at injecting semantics in resolution; hypertableaux, which illustrates model-based reasoning in tableaux, with applications to fault diagnosis and description logics; the model-evolution calculus, which lifts DPLL to first-order logic, and a new method called SGGS, for Semantically-Guided Goal-Sensitive reasoning, which realizes a first-order CDCL mechanism."}, {"heading": "2.1 Semantic Resolution", "text": "Soon after the seminal article by Alan Robinson introducing the resolution principle [75], JamesR. Slagle presented semantic resolution in [79]. Let S be the\nfinite set of first-order clauses to be refuted. Slagle\u2019s core idea was to use a given Herbrand interpretation I to avoid generating resolvents that are true in I, since expanding a consistent set should not lead to a refutation. The following example from [31] illustrates the concept in propositional logic:\nExample 1. Given S = {\u00acA1\u2228\u00acA2\u2228A3, A1\u2228A3, A2\u2228A3}, let I be all-negative, that is, I = {\u00acA1,\u00acA2,\u00acA3}. Resolution between \u00acA1 \u2228\u00acA2 \u2228A3 and A1 \u2228A3 generates \u00acA2\u2228A3, after merging identical literals. Similarly, resolution between \u00acA1 \u2228\u00acA2 \u2228A3 and A2 \u2228A3 generates \u00acA1 \u2228A3. However, these two resolvents are true in I. Semantic resolution prevents generating such resolvents, and uses all three clauses to generate only A3, which is false in I.\nFormally, say that we have a clause N , called nucleus, and clauses E1, . . . , Eq, with q \u2265 1, called electrons, such that the electrons are false in I. Then, if there is a series of clauses R1, R2, . . . , Rq, Rq+1, where R1 is N , Ri+1 is a resolvent of Ri and Ei, for i = 1, . . . , q, and Rq+1 is false in I, semantic resolution generates only Rq+1. The intuition is that electrons are used to resolve away literals in the nucleus until a clause false in I is generated.\nExample 2. In the above example, \u00acA1\u2228\u00acA2\u2228A3 is the nucleus N , and A1\u2228A3 and A2\u2228A3 are the electrons E1 and E2, respectively. Resolving N and E1 gives \u00acA2 \u2228A3, and resolving the latter with E2 yields A3: only A3 is retained, while the intermediate resolvent \u00acA2 \u2228 A3 is not.\nSemantic resolution can be further restricted by assuming a precedence > on predicate symbols, and stipulating that in each electron the predicate symbol of the literal resolved upon must be maximal in the precedence. The following example also from [31] is in first-order logic:\nExample 3. For S = {Q(x)\u2228Q(a)\u2228\u00acR(y)\u2228\u00acR(b)\u2228S(c), \u00acQ(z)\u2228\u00acQ(a), R(b)\u2228 S(c)}, let I be {Q(a), Q(b), Q(c),\u00acR(a),\u00acR(b),\u00acR(c),\u00acS(a),\u00acS(b),\u00acS(c)}, so that I 6|= \u00acQ(z)\u2228\u00acQ(a) and I 6|= R(b)\u2228S(c). Assume the precedenceQ > R > S. Thus, Q(x)\u2228Q(a)\u2228\u00acR(y)\u2228\u00acR(b)\u2228S(c) is the nucleus N , and \u00acQ(z)\u2228\u00acQ(a) and R(b) \u2228 S(c) are the electrons E1 and E2, respectively. Resolution between N and E1 on the Q-literals produces \u00acR(y) \u2228 \u00acR(b) \u2228 S(c), which is not false in I, and therefore it is not kept. Note that this resolution step is a binary resolution step between a factor of N and a factor of E1. Resolution between \u00acR(y)\u2228\u00acR(b)\u2228S(c) and E2 on the R-literals yields S(c). This second resolution step is a binary resolution between a factor of \u00acR(y) \u2228 \u00acR(b) \u2228 S(c) and E2. Resolvent S(c) is false in I and it is kept.\nIn these examples I is given by a finite set of literals: Example 1 is propositional, and in Example 3 the Herbrand base is finite, because there are no function symbols. The examples in [79] include a theorem from algebra, where the interpretation is given by a multiplication table and hence is really of semantic nature. The crux of semantic resolution is the representation of I. In theory, a Herbrand interpretation is given by a subset of the Herbrand base of S. In practice, one needs a finite representation of I, which is a non-trivial issue,\nwhenever the Herbrand base is not finite, or a mechanism to test the truth of a literal in I. Two instances of semantic resolution that aimed at addressing this issue are hyperresolution [74] and the set-of-support strategy [86].\nHyperresolution assumes that I contains either all negative literals or all positive literals. In the first case, it is called positive hyperresolution, because electrons and all resolvents are positive clauses: positive electrons are used to resolve away all negative literals in the nucleus to get a positive hyperresolvent. In the second case, it is called negative hyperresolution, because electrons and all resolvents are negative clauses: negative electrons are used to resolve away all positive literals in the nucleus to get a negative hyperresolvent. Example 1 is an instance of positive hyperresolution.\nThe set-of-support strategy assumes that S = T \u228e SOS, where SOS (for Set of Support) contains initially the clauses coming from the negation of the conjecture, and T = S \\ SOS is consistent, for some I such that I |= T and I 6|= SOS. A resolution of two clauses is permitted, if at least one is from SOS, in order to avoid expanding the consistent set T . All resolvents are added to SOS. Thus, all inferences involve clauses descending from the negation of the conjecture: a method with this property is deemed goal-sensitive.\nIn terms of implementation, positive hyperresolution is often implemented in contemporary theorem provers by resolution with selection of negative literals. Indeed, resolution can be restricted by a selection function that selects negative literals [4]. A clause can have all, some, or none of its negative literals selected, depending on the selection function. In resolution with negative selection, the negative literal resolved upon must be selected, and the other parent must not contain selected literals. If some negative literal is selected for each clause containing one, one parent in each resolution inference will be a positive clause, that is, an electron for positive hyperresolution. Thus, a selection function that selects some negative literal in each clause containing one induces resolution to simulate hyperresolution as a macro inference involving several steps of resolution.\nThe set-of-support strategy is available in all theorem provers that feature the given-clause loop [61], which is a de facto standard for resolution-based provers. This algorithm maintains two lists of clauses, named to-be-selected and alreadyselected, and at each iteration it extracts a given clause from to-be-selected. In its simplest version, with only resolution as inference rule, it performs all resolutions between the given clause and the clauses in already-selected; adds all resolvents to to-be-selected; and adds the given clause to already-selected. If one initializes these lists by putting the clauses in T in already-selected, and the clauses in SOS in to-be-selected, this algorithm implements the set-of-support strategy. Indeed, in the original version of the given-clause algorithm, to-be-selected was called SOS, and already-selected was called Usable.\nState-of-the-art resolution-based theorem provers implement more sophisticated versions of the given clause algorithm, which also accomodate contraction rules, that delete (e.g., subsumption, tautology deletion) or simplify clauses (e.g., clausal simplification, equational simplification). The compatibility of contraction rules with semantic strategies is not obvious, as shown by the following:\nExample 4. Let T = {\u00acP, P \u2228 Q} and SOS = {\u00acQ}. Clausal simplification, which is a combination of resolution and subsumption, applies \u00acQ to simplify P \u2228Q to P . If the result is T = {\u00acP, P} and SOS = {\u00acQ}, the consistent set T becomes inconsistent, and the refutational completeness of resolution with set-ofsupport collapses, since the set-of-support strategy does not allow us to resolve P and \u00acP , being both in T . The correct application of clausal simplification yields T = {\u00acP} and SOS = {\u00acQ, P}, so that the refutation can be found.\nIn other words, if a clause in SOS simplifies a clause, whether in T or in SOS, the resulting clause must be added to SOS. The integration of contraction rules and other enhancements, such as lemmaizing, in semantic strategies was investigated in general in [22].\nSemantic resolution, hyperresolution, and the set-of-support strategy exhibit semantic guidance. We deem a method semantically guided, if it employs a fixed interpretation to drive the inferences. We deem a method model-based, if it builds and transforms a candidate model, and uses it to drive the inferences.\nA beginning of the evolution from being semantically guided to being modelbased can be traced back to the SCOTT system [80], which combined the finite model finder FINDER, that searches for small models, and the resolution-based theorem prover OTTER [61]. As the authors write \u201cSCOTT brings semantic information gleaned from the proof attempt into the service of the syntax-based theorem prover.\u201d In SCOTT, FINDER provides OTTER with a guide model, which is used for an extended set-of-support strategy: in each resolution step at least one of the parent clauses must be false in the guide model. During the proof search FINDER updates periodically its model to make more clauses true. Thus, inferences are controlled as in the set-of-support strategy, but the guide model is not fixed, which is why SCOTT can be seen as a forerunner of model-based methods. Research on the cooperation between theorem prover and finite model finder continued with successors of OTTER, such as Prover9, and successors of FINDER, such as MACE4 [87]. This line of research has been especially fruitful in applications to mathematics (e.g., [3,38])."}, {"heading": "2.2 Hypertableaux", "text": "Tableau calculi offer an alternative to resolution and they have been discussed abundantly in the literature (e.g., Chapter 3 in [76]). Their advantages include no need for a clause normal form, a single proof object, and an easy extendability to other logics. The disadvantage, even in the case of clause normal form tableaux, is that variables are rigid, which means that substitutions have to be applied to all occurrences of a variable within the entire tableau. The hypertableau calculus [10] offers a more liberal treatment of variables, and borrows the concept of hyperinference from positive hyperresolution.\nIn this section, we adopt a Prolog-like notation for clauses: A1 \u2228 . . . \u2228 Am \u2228 \u00acB1 \u2228 . . . \u2228 \u00acBn is written A1, . . . , Am \u21d0 B1, . . . , Bn, where A1, . . . , Am form the head of the clause and are called head literals, and B1, . . . , Bn form the body. There are two rules for constructing a hypertableau (cf. [10]): the initialization\nrule gives a tableau consisting of a single node labeled with \u22a4; this one-element branch is open. The hyperextension rule selects an open branch and a clause A1, . . . , Am \u21d0 B1, . . . , Bn, where m,n \u2265 0, from the given set S, such that there exists a most general unifier \u03c3 which makes all the Bi\u03c3\u2019s follow logically from the model given by the branch. If there is a variable in the clause that has an occurrence in more than one head literal Ai, a purifying substitution \u03c0 is used to ground this variable. Then the branch is extended by new nodes labeled with Ai\u03c3\u03c0, . . . , Am\u03c3\u03c0. A branch is closed if it can be extended by a clause without head literals. S is unsatisfiable if and only if there is a hypertableau for S whose branches are all closed.\nTwo major advantages of hyperextension are that it avoids unnecessary branching, and only variables in the clauses are universally quantified and get instantiated, while variables in the branches are treated as free variables (except those occurring in different head literals). The latter feature allows a superposition-like handling of equality [11], while the former is relevant for hypertableaux for description logic [78], which we shall return to in the next section. Hypertableaux were implemented in the Hyper theorem prover for first-order logic, followed by E-Hyper implementing also the handling of equality.\nExample 5. An example refutation is given in Figure 1. The initial tableau is set up with the only positive clause. Extension at R(a) with the second clause uses \u03c3 = {x \u2190 a}: since y appears only once in the resulting head, \u03c0 = \u03b5 and y remains as a free variable. In the right subtree R(f(z)) is extended with the second clause and \u03c3 = {x \u2190 f(z)}. In the head P (f(z)), Q(f(z), y) of the resulting clause z is repeated: an instance generation mechanism produces \u03c0 = {z \u2190 b}, or the instance P (f(b)), Q(f(b), y) \u21d0 R(f(b)), to find a refutation. Note how the tableau contains by construction only positive literals, and the interpretation given by a branch is used to control the extension steps very much like in hyperresolution."}, {"heading": "2.3 Model-based Transformation of Clause Sets", "text": "Hypertableaux use partial models, that is, models for parts of a clause set, to control the search space. An open branch that cannot be expanded further represents a model for the entire clause set. In this section we present a transformation method, borrowed from model-based diagnosis and presented in [8], which is based on a given model and therefore can be installed on top of hypertableaux. In applications to diagnosis, one has a set of clauses S which corresponds to a description of a system, such as an electrical circuit. Very often there is a model I of a correctly functioning system available; in case of an electrical circuit it may be provided by the design tool itself. If the actual circuit is fed with an input and does not show the expected output, the task is to find a diagnosis, or those parts of the circuit which may be broken. Instead of doing reasoning with the system description S and its input and output in order to find the erroneous parts, the idea is to compute only deviations from the initially given model I.\nAssume that S is a set of propositional clauses and I a set of propositional atoms; as a very simple example take\nS = {B \u21d0, C \u21d0 A,B} and I = {A}.\nEach clause in S is transformed by replacing a positive literal L by \u00acneg L and a negative literal \u00acL by neg L, if L is contained in I. In other words, a literal which is contained in the initial model moves to the other side of the arrow and is renamed with the prefix neg as in\nS\u2032 = {B \u21d0, C, neg A \u21d0 B}.\nThis transformation is model-preserving, as every model of S is a model of S\u2032. For this it suffices to assign true to neg L if and only if L is false, for every L \u2208 I, and keep truth values unchanged for atoms outside of I. This property is independent of I, and it holds even if I is not a model of S. In our example, after initialization, first hyperextension with B \u21d0, and then hyperextension with C, neg A \u21d0 B, yield the open branches {B,C} and {B, neg A}. Hyperextension with C, neg A \u21d0 B can be applied because only B occurs in the body. Since A is assumed to be true in I, it can be added: adding A to {B,C} yields model {A,B,C}; adding A to {B, neg A} yields model {B}. If deriving A in S is very expensive, it pays off to save this derivation by moving A as neg A to the body of the clause. In this example a Horn clause becomes non-Horn, introducing the case where A is false, and neg A holds, although A is in I. Symmetrically, a non-Horn clause may become Horn. This transformation technique enabled a hypertableau prover to compute benchmarks from electrical engineering [8], and was also applied to the view update problem in databases [2].\nAlthough this transformation mechanism only works in the propositional case, it can be extended to description logic [39]. Indeed, most description logic reasoners are based on tableau calculi, and a hypertableau calculus was used in [78] as a basis for an efficient reasoner for the description logic SHIQ. For this purpose, the authors define DL-clauses as clauses without occurrences of\nfunction symbols, and such that the head is allowed to include disjunctions of atoms, which may contain existential role restrictions as in\n\u2203repairs .Car (x) \u21d0 Mechanic(x ).\nIn other words, a given SHIQ-Tbox is translated to a large extent into firstorder logic; only existential role restrictions are kept as positive \u201cliterals.\u201d Given a Tbox in the form of a set of DL-clauses, if we have in addition an Abox, or a set of ground assertions, we can use the interpretation given by the ABox as initial model for the model-based transformation [39]. On this basis, the already mentioned E-Hyper reasoner was modified to become E-KRHyper, which was shown to be a decision procedure for SHIQ in [16]."}, {"heading": "2.4 The Model Evolution Calculus", "text": "The practical success of DPLL-based SAT solvers suggested the goal of lifting features of DPLL to the first-order level. Research focused on splitting first-order clauses, seen as a way to improve the capability to handle non-Horn clauses. Breaking first-order clauses apart is not as simple as in propositional logic, because a clause stands for all its ground instances, and literals share variables that are implicitly universally quantified. Decomposing disjunction is a native feature in tableaux, whose downside is represented by rigid variables, as already discussed in Section 2.2, where we saw how hypertableaux offer a possible answer.\nThe quest for ways to split efficiently clauses such as A(x) \u2228 B(x) led to the model evolution calculus [13]. In this method splitting A(x) \u2228 B(x) yields a branch with A(x), meaning \u2200xA(x), and one with \u00acA(c), the Skolemized form of \u00ac\u2200xA(x) \u2261 \u2203x\u00acA(x). Splitting in this way has the disadvantage that the signature changes, and Skolem constants, being new, do not unify with other non-variable terms. Thus, the model evolution calculus employs parameters, in place of Skolem constants, to replace existentially quantified variables. These parameters are similar to the free variables of hypertableaux.\nThe similarity between the model evolution calculus and DPLL goes beyond splitting, as the model evolution calculus aims at being a faithful lifting of DPLL to first-order logic. Indeed, a central feature of the model evolution calculus is that it maintains a context \u039b, which is a finite set of literals, representing a Herbrand interpretation I\u039b, seen as a candidate partial model of the input set of clauses S. Thus, the model evolution calculus is a model-based first-order method. Literals in \u039b may contain variables, implicitly universally quantified as in clauses, and parameters. Clauses are written in the form \u039b \u22a2 C, so that each clause carries the context with itself.\nIn order to determine whether I\u039b |= L, for L an atom in the Herbrand base of S, one looks at the most specific literal in \u039b that subsumes L; in case of a tie, L is picked with positive sign. If I\u039b is not a model of S, the inference system unifies input clauses against \u039b to find instances that are not true in I\u039b: these instances are subject to splitting, to modify \u039b and repair I\u039b. Otherwise, the system recognizes that \u039b cannot be fixed and declares S unsatisfiable. As\nDPLL uses depth-first search with backtracking, the model evolution calculus uses depth-first search with backtracking and iterative deepening on term depth, which however may skew the search towards big proofs with small term depth. The model evolution calculus was implemented in the Darwin prover [9], and extended to handle equality on its own [12] and with superposition [14]."}, {"heading": "2.5 SGGS: Semantically-Guided Goal-Sensitive Reasoning", "text": "SGGS, for Semantically-Guided Goal-Sensitive reasoning, is a new theoremproving method for first-order logic [29,26,28,27], which inherits features from several of the strategies that we surveyed in the previous sections. SGGS is semantically guided by a fixed initial interpretation I like semantic resolution; and it is goal-sensitive like the set-of-support strategy. With hyperresolution and hypertableaux, it shares the concept of hyperinference, although the hyperinference in SGGS, as we shall see, is an instance generation inference, and therefore its closest ancestor is hyperlinking [58,71], an inference rule that uses the most general unifier of a hyperresolution step to generate instances of the parents, rather than a hyperresolvent.\nMost importantly, SGGS is model-based at the first-order level, in the sense of working by representing and transforming a candidate partial model of the given set S of first-order clauses. This fundamental characteristic is in common with the model evolution calculus, but while the latter lifts DPLL, SGGS lifts DPLLCDCL to first-order logic, and it combines the model-based character with the semantic guidance and the goal sensitivity. Indeed, SGGS was motivated by the quest for a method that is simultaneously first-order, model-based, semanticallyguided, and goal-sensitive. Furthermore, SGGS is proof confluent, which means it does not need backtracking, and it does not necessarily reduce to either DPLL or DPLL-CDCL, if given a propositional problem.\nIn DPLL-CDCL, if a literal L appears in the trail that represents the candidate partial model, all occurrences of \u00acL in the set of clauses are false. If all literals of a clause C are false, C is in conflict; if all literals of C except one, say Q, are false, Q is an implied literal with C as justification. The status of C depends on the decision levels where the complements of its literals were either guessed (decision) or implied (Boolean propagation). SGGS generalizes these concepts to first-order logic. Since variables in first-order literals are implicitly universally quantified, if L is true, \u00acL is false, but if L is false, we only know that a ground instance of \u00acL is true. SGGS restores the symmetry by introducing the notion of uniform falsity: L is uniformly false, if all its ground instances are false, or, equivalently, if \u00acL is true. A first ro\u0302le of the given interpretation I is to provide a reference model where to evaluate the truth value of literals: a literal is I-true, if it is true in I, and I-false, if it is uniformly false in I.\nAn SGGS clause sequence \u0393 is a sequence of clauses, where every literal is either I-true or I-false, so that it tells the truth value in I of all its ground instances. In every clause C in \u0393 a literal is selected: if C = L1 \u2228 . . . \u2228 Ln and Ln is selected, we write the clause as L1 \u2228 . . .\u2228 [Ln], or, more compactly, C[Ln], with a slight abuse of the notation. SGGS tries to modify I into a model of S\n(if I is a model of S the problem is solved). Thus, I-false literals are preferred for selection, and an I-true literal is selected only in a clause whose literals are all I-true, called I-all-true clause. A second ro\u0302le of the given interpretation I is to provide a starting point for the search of a model for S.\nAn SGGS clause sequence \u0393 represents a partial interpretation Ip(\u0393 ): if \u0393 is the empty sequence, denoted by \u03b5, Ip(\u0393 ) is empty; if \u0393 is C1[L1], . . . , Ci[Li], and Ip(\u0393 |i\u22121) is the partial interpretation represented by C1[L1], . . . , Ci\u22121[Li\u22121], then Ip(\u0393 ) is Ip(\u0393 |i\u22121) plus the ground instances Li\u03c3 of Li, such that Ci\u03c3 is ground, Ci\u03c3 is not satisfied by I\np(\u0393 |i\u22121), and \u00acLi\u03c3 is not in Ip(\u0393 |i\u22121), so that Li\u03c3 can be added to satisfy Ci\u03c3. In other words, each clause adds the ground instances of its selected literal that satisfy ground instances of the clause not satisfied thus far.\nAn interpretation I[\u0393 ] is obtained by consulting first Ip(\u0393 ), and then I: for a ground literal L, if its atom appears in Ip(\u0393 ), its truth value in I[\u0393 ] is that in Ip(\u0393 ); otherwise, it is that in I. Thus, I[\u0393 ] is I modified to satisfy the clauses in \u0393 by satisfying the selected literals, and since I-true selected literals are already true in I, the I-false selected literals are those that matter. For example, if \u0393 is [P (x)], \u00acP (f(y)) \u2228 [Q(y)], \u00acP (f(z)) \u2228 \u00acQ(g(z)) \u2228 [R(f(z), g(z))], and I is all negative like in positive hyperresolution, I[\u0393 ] satisfies all ground instances of P (x), Q(y), and R(f(z), g(z)), and no other positive literal.\nSGGS generalizes Boolean, or clausal, propagation to first-order logic. Consider an I-false (I-true) literal M selected in clause Cj in \u0393 , and an I-true (I-false) literal L in Ci, i > j: if all ground instances of L appear negated among the ground instances of M added to Ip(\u0393 ), L is uniformly false in I[\u0393 ] because of M , and depends on M , like \u00acL depends on L in propositional Boolean propagation, when L is in the trail. If this happens for all its literals, clause C[L] is in conflict with I[\u0393 ]; if this happens for all its literals except L, L is an implied literal with C[L] as justification. SGGS employs assignment functions to keep track of the dependencies of I-true literals on selected I-false literals, realizing a sort of first-order propagation modulo semantic guidance by I. SGGS ensures that I-all-true clauses in \u0393 are either conflict clauses or justifications.\nThe main inference rule of SGGS, called SGGS-extension, uses the current clause sequence \u0393 and a clause C in S to generate an instance E of C and add it to \u0393 to obtain the next clause sequence \u0393 \u2032. SGGS-extension is a hyperinference, because it unifies literals L1, . . . , Ln of C with I-false selected literalsM1, . . . ,Mn of opposite sign in \u0393 . The hyperinference is guided by I[\u0393 ], because I-false selected literals contribute to I[\u0393 ] as explained above. Another ingredient of the instance generation mechanism ensures that every literal in E is either I-true or I-false. SGGS-extension is also responsible for selecting a literal in E.\nThe lifting theorem for SGGS-extension shows that if I[\u0393 ] 6|= C\u2032 for some ground instance C\u2032 of a clause C \u2208 S, SGGS-extension builds an instance E of C such that C\u2032 is an instance of E. There are three kinds of SGGS-extension: (1) add a clause E which is in conflict with I[\u0393 ] and is I-all-true; (2) add a clause E which is in conflict with I[\u0393 ] but is not I-all-true; and (3) add a clause E which is not in conflict with I[\u0393 ]. In cases (1) and (2), it is necessary to solve\nthe conflict: it is here that SGGS lifts the conflict-driven clause learning (CDCL) mechanism of DPLL-CDCL to the first-order level.\nIn DPLL-CDCL a conflict is explained by resolving a conflict clause C with the justification D of a literal whose complement is in C, generating a new conflict clause. Typically resolution continues until we get either the empty clause \u22a5 or an asserting clause, namely a clause where only one literal Q is falsified in the current decision level. DPLL-CDCL learns the asserting clause and backjumps to the shallowest level where Q is undefined and all other literals in the asserting clause are false, so that Q enters the trail with the asserting clause as justification. SGGS explains a conflict by resolving the conflict clause E with an I-all-true clause D[M ] in \u0393 which is the justification of the literal M that makes an I-false literal L in E uniformly false in I[\u0393 ]. Resolution continues until we get either \u22a5 or a conflict clause E[L] which is I-all-true. If \u22a5 arises, S is unsatisfiable. Otherwise, SGGS moves the I-all-true clause E[L] to the left of the clause B[M ], whose I-false selected literal M makes L uniformly false in I[\u0393 ]. The effect is to flip at once the truth value of all ground instances of L in I[\u0393 ], so that the conflict is solved, L is implied, and E[L] satisfied.\nIn order to simplify the presentation, up to here we omitted that clauses in SGGS may have constraints. For example, x 6\u2261 y \u2704 P (x, y) \u2228 Q(y, x) is a constrained clause, which represents its ground instances that satisfy the constraints: P (a, b)\u2228Q(b, a) is an instance, while P (a, a)\u2228Q(a, a) is not. The reason for constraints is that selected literals of clauses in \u0393 may intersect, in the sense of having ground instances with the same atoms. Since selected literals determine Ip(\u0393 ), whence I[\u0393 ], non-empty intersections represent duplications, if the literals have the same sign, and contradictions, otherwise. SGGS removes duplications by deletion of clauses, and contradictions by resolution. However, before doing either, it needs to isolate the shared ground instances in the selected literal of one clause. For this purpose, SGGS features inference rules that replace a clause by a partition, that is, a set of clauses that represent the same ground instances and have disjoint selected literals. This requires constraints. For example, a partition of [P (x, y)]\u2228Q(x, y) is {true\u2704 [P (f(z), y)]\u2228Q(f(z), y), top(x) 6= f \u2704 [P (x, y)] \u2228 Q(x, y)}, where the constraint top(x) 6= f means that variable x cannot be instantiated with a term whose topmost symbol is f . If L and M in C[L] and D[M ] of \u0393 intersect, SGGS partitions C[L] by D[M ]: it partitions C[L] into A1 \u2704 C1[L1], . . . , An \u2704 Cn[Ln] so that only Lj , for some j, 1 \u2264 j \u2264 n, intersects with M , and Aj \u2704 Cj [Lj] is either deleted or resolved with D[M ].\nThe following example shows an SGGS-refutation:\nExample 6. Given S = {\u00acP (f(x)) \u2228 \u00acQ(g(x)) \u2228R(x), P (x), Q(y), \u00acR(c)}, let I be all negative. An SGGS-derivation starts with the empty sequence. Then, four SGGS-extension steps apply:\n\u03930 : \u03b5 \u03931 : [P (x)] \u03932 : [P (x)], [Q(y)] \u03933 : [P (x)], [Q(y)], \u00acP (f(x)) \u2228 \u00acQ(g(x)) \u2228 [R(x)] \u03934 : [P (x)], [Q(y)], \u00acP (f(x)) \u2228 \u00acQ(g(x)) \u2228 [R(x)], [\u00acR(c)]\nAt this stage, the selected literals R(x) and \u00acR(c) intersect, and therefore SGGS partitions \u00acP (f(x)) \u2228 \u00acQ(g(x)) \u2228 [R(x)] by [\u00acR(c)]:\n\u03935 : [P (x)], [Q(y)], x 6\u2261 c\u2704 \u00acP (f(x)) \u2228 \u00acQ(g(x)) \u2228 [R(x)], \u00acP (f(c)) \u2228 \u00acQ(g(c)) \u2228 [R(c)], [\u00acR(c)]\nNow the I-all-true clause \u00acR(c) is in conflict with I[\u03935]. Thus, SGGS moves it left of the clause \u00acP (f(c))\u2228\u00acQ(g(c))\u2228[R(c)] that makes \u00acR(c) false in I[\u03935], in order to amend the induced interpretation. Then, it resolves these two clauses, and replaces the parent that is not I-all-true, namely \u00acP (f(c)) \u2228 \u00acQ(g(c)) \u2228 [R(c)], by the resolvent \u00acP (f(c)) \u2228 \u00acQ(g(c)):\n\u03936 : [P (x)], [Q(y)], x 6\u2261 c\u2704 \u00acP (f(x)) \u2228 \u00acQ(g(x)) \u2228 [R(x)], [\u00acR(c)], \u00acP (f(c)) \u2228 \u00acQ(g(c)) \u2228 [R(c)] \u03937 : [P (x)], [Q(y)], x 6\u2261 c\u2704 \u00acP (f(x)) \u2228 \u00acQ(g(x)) \u2228 [R(x)], [\u00acR(c)], \u00acP (f(c)) \u2228 [\u00acQ(g(c))]\nAssuming that in the resolvent the literal \u00acQ(g(c)) gets selected, there is now an intersection between selected literals \u00acQ(g(c)) and Q(y), so that SGGS partitions Q(y) by \u00acP (f(c)) \u2228 \u00acQ(g(c)):\n\u03938 : [P (x)], top(y) 6= g \u2704 [Q(y)], z 6\u2261 c\u2704 [Q(g(z))], [Q(g(c))], x 6\u2261 c\u2704 \u00acP (f(x)) \u2228 \u00acQ(g(x)) \u2228 [R(x)], [\u00acR(c)], \u00acP (f(c)) \u2228 [\u00acQ(g(c))]\nAt this point, the I-all-true clause \u00acP (f(c)) \u2228 [\u00acQ(g(c))] is in conflict with I[\u03938]. As before, SGGS moves it left of the clause that makes its selected literal \u00acQ(g(c)) false, namely [Q(g(c))], in order to fix the candidate model, and then resolves \u00acP (f(c))\u2228[\u00acQ(g(c))] and [Q(g(c))], replacing the latter by the resolvent \u00acP (f(c)):\n\u03939 : [P (x)], top(y) 6= g \u2704 [Q(y)], z 6\u2261 c\u2704 [Q(g(z))], \u00acP (f(c)) \u2228 [\u00acQ(g(c))], [Q(g(c))], x 6\u2261 c\u2704 \u00acP (f(x)) \u2228 \u00acQ(g(x)) \u2228 [R(x)], [\u00acR(c)] \u039310 : [P (x)], top(y) 6= g \u2704 [Q(y)], z 6\u2261 c\u2704 [Q(g(z))], \u00acP (f(c)) \u2228 [\u00acQ(g(c))], [\u00acP (f(c))], x 6\u2261 c\u2704 \u00acP (f(x)) \u2228 \u00acQ(g(x)) \u2228 [R(x)], [\u00acR(c)]\nThe resolvent has only one literal which gets selected; since [\u00acP (f(c))] intersects with [P (x)], the next inference partitions [P (x)] by [\u00acP (f(c))]:\n\u039311 : top(x) 6= f \u2704 [P (x)], y 6\u2261 c\u2704 [P (f(y))], [P (f(c))], top(y) 6= g \u2704 [Q(y)], z 6\u2261 c\u2704 [Q(g(z))], \u00acP (f(c)) \u2228 [\u00acQ(g(c))], [\u00acP (f(c))], x 6\u2261 c\u2704 \u00acP (f(x)) \u2228 \u00acQ(g(x)) \u2228 [R(x)], [\u00acR(c)]\nThe next step moves the I-all-true clause [\u00acP (f(c))], which is in conflict with I[\u039311], to the left of the clause [P (f(c))] that makes [\u00acP (f(c))] false in I[\u039311], and then resolves these two clauses to generate the empty clause:\n\u039312 : top(x) 6= f \u2704 [P (x)], y 6\u2261 c\u2704 [P (f(y))], [\u00acP (f(c))], [P (f(c))], top(y) 6= g \u2704 [Q(y)], z 6\u2261 c\u2704 [Q(g(z))], \u00acP (f(c)) \u2228 [\u00acQ(g(c))], x 6\u2261 c\u2704 \u00acP (f(x)) \u2228 \u00acQ(g(x)) \u2228 [R(x)], [\u00acR(c)] \u039313 : top(x) 6= f \u2704 [P (x)], y 6\u2261 c\u2704 [P (f(y))], [\u00acP (f(c))], \u22a5, top(y) 6= g \u2704 [Q(y)], z 6\u2261 c\u2704 [Q(g(z))], \u00acP (f(c)) \u2228 [\u00acQ(g(c))], x 6\u2261 c\u2704 \u00acP (f(x)) \u2228 \u00acQ(g(x)) \u2228 [R(x)], [\u00acR(c)]\nThis example only illustrates the basic mechanisms of SGGS. This method is so new that it has not yet been implemented: the hope is that its conflictdriven model-repair mechanism will have on first-order theorem proving an effect similar to that of the transition from DPLL to DPLL-CDCL for SAT-solvers. If this were true, even in part, the benefit could be momentous, considering that CDCL played a key ro\u0302le in the success of SAT technology. Another expectation is that non-trivial semantic guidance (i.e., not based on sign like in hyperesolution) pays off in case of many axioms or large knowledge bases."}, {"heading": "3 Model-based Reasoning in First-Order Theories", "text": "There are basically two ways one can think about a theory presented by a set of axioms: as the set of all theorems that are logical consequences of the axioms, or as the set of all interpretations that are models of the axioms. The two are obviously connected, but may lead to different styles of reasoning, that we portray by the selection of methods in this section. We cover approaches that build axioms into resolution, hierarchical and locality-based theory reasoning, and a recent method called Model-Constructing satisfiability calculus or MCsat."}, {"heading": "3.1 Building Theory Axioms into Resolution and Superposition", "text": "The early approaches to theory reasoning emphasized the axioms, by building them into the inference systems. The first analyzed theory was equality: since submitting the equality axioms to resolution, or other inference systems for first-order logic, leads to an explosion of the search space, paramodulation, superposition, and rewriting were developed to build equality into resolution (e.g., [73,48,77,4,21] and Chapters 7 and 9 in [76]).\nOnce equality was conquered, research flourished on building-in theories (e.g., [72,66,54,49,36,40,53,30]). Equational theories, that are axiomatized by sets of equalities, and among them permutative theories, where the two sides of each axiom are permutations of the same symbols, as in associativity and commutativity, received the most attention. A main ingredient is to replace syntactic unification by unification modulo a set E of equational axioms, a concept generalized by Jose\u0301 Meseguer to order-sorted E-unification (e.g., [43,37,46]). This kind of approach was pursued further, by building into superposition axioms for monoids [42], groups [85], rings and modules [84], or by generalizing superposition to embed transitive relations other than equality [5]. The complexities and limitations of these techniques led to investigate the methods for hierarchical theory reasoning that follow."}, {"heading": "3.2 Hierarchical Reasoning by Superposition", "text": "Since Jose\u0301 Meseguer\u2019s work with Joe Goguen (e.g., [44]), it became clear that a major issue at the cross-roads of reasoning, specifying, and programming, is that theories, or specifications, are built by extension to form hierarchies. A\nbase theory T0 is defined by a set of sorts S0, a signature \u03a30, possibly a set of axioms N0, and the class C0 of its models (e.g., term-generated \u03a30-algebras). An extended or enriched theory T adds new sorts (S0 \u2286 S), new function symbols (\u03a30 \u2286 \u03a3), called extension functions, and new axioms (N0 \u2286 N), specifying properties of the new symbols. For the base theory the class of models is given, while the extension is defined axiomatically. A pair (T0, T ) as above forms a hierarchy with enrichment axioms N .\nThe crux of extending specifications was popularized by Joe Goguen and Jose\u0301 Meseguer as no junk and no confusion: an interpretation of S and \u03a3, which is a model of N , is a model of T only if it extends a model in C0, without collapsing its sorts, or making distinct elements equal (no confusion), or introducing new elements of base sort (no junk). A sufficient condition for the latter is sufficient completeness, a property studied also in inductive theorem proving, which basically says that every ground non-base term t\u2032 of base sort is equal to a ground base term t. Sufficient completeness is a strong restriction, violated by merely adding a constant symbol: if \u03a30 = {a, b}, N = N0 = {a 6\u2243 b}, and \u03a3 = {a, b, c}, where a, b, and c are constants of the same sort, the extension is not sufficiently complete, because c is junk, or a model with three distinct elements is not isomorphic to one with two. Although sufficient completeness is undecidable in general (e.g., [57]), sufficient completeness analyzers exist (e.g., [56,45,47]), with key contributions by Jose\u0301 Meseguer.\nHierarchic superposition was introduced in [6] and developed in [41] to reason about a hierarchy (T0, T ) with enrichment axioms N , where N is a set of clauses. We assume to have a decision procedure to detect that a finite set of \u03a30-clauses is T0-unsatisfiable. Given a set S of \u03a3-clauses, the problem is to determine whether S is false in all models of the hierarchic specification, or, equivalently, whether N \u222aS has no model whose reduct to \u03a30 is a model of T0. The problem is solved by using the T0-reasoner as a black-box to take care of the base part, while superposition-based inferences apply only to non-base literals.3 First, for every clause C, whenever a subterm t whose top symbol is a base operator occurs immediately below a non-base operator symbol (or vice versa), t is replaced by a new variable x and the equation x \u2243 t is added to the antecedent of C. This transformation is called abstraction. Then, the inference rules are modified to require that all substitutions are simple, meaning that they map variables of base sort to base terms. A meta-rule named constraint refutation detects that a finite set of \u03a30-clauses is inconsistent in T0 by invoking the T0-reasoner. Hierarchic superposition was proved refutationally complete in [6], provided T0 is compact, which is a basic preliminary to make constraint refutation mechanizable, and N \u222aS is sufficiently complete with respect to simple instances, which means that for every model I of all simple ground instances of the clauses in N \u222a S, and every ground non-base term t\u2032, there exists a ground base term t (which may depend on I) such that I |= t\u2032 \u2243 t.\n3 Other approaches to subdivide work between superposition and an SMT-solver appeared in [20,25].\nThere are situations where the enrichment adds partial functions: \u03a30 contains only total function symbols, while \u03a3 \\ \u03a30 may contain partial functions and total functions having as codomain a new sort. Hierarchic superposition was generalized to handle both total and partial function symbols, yielding a partial hierarchic superposition calculus [41]. To have an idea of the difficulties posed by partial functions, consider that replacement of equals by equals may be unsound in their presence. For example, s 6\u2243 s may hold in a partial algebra (i.e., a structure where some function symbols are interpreted as partial), if s is undefined. Thus, the equality resolution rule (e.g., resolution between C \u2228 s 6\u2243 s and x \u2243 x) is restricted to apply only if s is guaranteed to be defined. Other restrictions impose that terms replaced by inferences may contain a partial function symbol only at the top; substitutions cannot introduce partial function symbols; and every ground term made only of total symbols is smaller than any ground term containing a partial function symbol in the ordering used by the inference system. The following example portrays the partial function case:\nExample 7. Let T0 be the base theory defined by S0 = {data}, \u03a30 = {b : \u2192 data, f : data \u2192 data}, and N0 = {\u2200x f(f(x)) \u2243 f(x)}. We consider the extension with a new sort list, total functions {cons : data, list \u2192 list, nil : \u2192 list, d :\u2192 list}, partial functions {car : list \u2192 data, cdr : list \u2192 list}, and the following clauses, where N = {(1), (2), (3)} and S = {(4), (5)}:\n(1) car(cons(x, l))\u2243x (2) cdr(cons(x, l))\u2243l (3) cons(car(l), cdr(l))\u2243l (4) f(b)\u2243b (5) f(f(b))\u2243car(cdr(cons(f(b), cons(b, d))))\nThe partial hierarchic superposition calculus deduces:\n(6) x 6\u2243 f(f(b)) \u2228 y 6\u2243 f(b) \u2228 z 6\u2243 b \u2228 x 6\u2243 car(cdr(cons(y, cons(z, d)))) Abstr. (5) (7) x 6\u2243 f(f(b)) \u2228 y 6\u2243 f(b) \u2228 z 6\u2243 b \u2228 x 6\u2243 car(cons(z, d)) Superp. (2),(6) (8) x 6\u2243 f(f(b)) \u2228 y 6\u2243 f(b) \u2228 z 6\u2243 b \u2228 x 6\u2243 z Superp. (1),(7) (9) \u22a5 Constraint refutation (4),(8)\nUnder the assumption that T0 is a universal first-order theory, which ensures compactness, the partial hierarchic superposition calculus was proved sound and complete in [41]: if a contradiction cannot be derived from N\u222aS using this calculus, then N\u222aS has a model which is a partial algebra. Thus, if the unsatisfiability of N \u222a S does not depend on the totality of the extension functions, the partial hierarchic superposition calculus can detect its inconsistency. In certain problem classes where partial algebras can always be made total, the calculus is complete also for total functions. Research on hierarchic superposition continued in [1], where an implementation for extensions of linear arithmetic was presented, and in [15], where the calculus was made \u201cmore complete\u201d in practice."}, {"heading": "3.3 Hierarchical Reasoning in Local Theory Extensions", "text": "A series of papers starting with [81] identified a class of theory extensions (T0, T ), called local, which admit a complete hierarchical method for checking satisfiabil-\nity of ground clauses, without requiring either sufficient completeness or that T0 is a universal first-order theory. The enrichment axioms in N do not have to be clauses: if they are, we have an extension with clauses ; if N consists of formul\u00e6 of the form \u2200x\u0304 (\u03a6(x\u0304)\u2228D(x\u0304)), where \u03a6(x\u0304) is an arbitrary \u03a30-formula and D(x\u0304) is a \u03a3-clause, with at least one occurrence of an extension function, we have an extension with augmented clauses. The basic assumption that T0, or a fragment thereof, admits a decision procedure for satisfiability clearly remains.\nAs we saw throughout this survey, instantiating universally quantified variables is crucial in first-order reasoning. Informally, a theory extension is local, if it is sufficient to consider only a finite set of instances. Let G be a set of ground clauses to be refuted in T , and let N [G] denote the set of instances of the clauses in N where every term whose top symbol is an extension function is a ground term occurring in N or G. Theory T is a local extension of T0, if N [G] suffices to prove the T -unsatisfiability of G [81]. Subsequent papers studied variants of locality, including those for extensions with augmented clauses, and for combinations of local theories, and proved that locality can be recognized by showing that certain partial algebras embed into total ones [81,82,50,51].\nIf T is a local extension, it is possible to check the T -satisfiability of G by hierarchical reasoning [81,82,50,51], allowing the introduction of new constants by abstraction as in [64]. By locality, G is T -unsatisfiable if and only if there is no model of N [G] \u222aG whose restriction to \u03a30 is a model of T0. By abstracting away non-base terms, N [G] \u222a G is transformed into an equisatisfiable set N0 \u222a G0 \u222aD, where N0 and G0 are sets of \u03a30-clauses, and D contains the definitions introduced by abstraction, namely equalities of the form f(g1, . . . , gn)\u2243c, where f is an extension function, g1, . . . , gn are ground terms, and c is a new constant. The problem is reduced to that of testing the T0-satisfiability of N0\u222aG0 \u222aCon0, where Con0 contains the instances of the congruence axioms for the terms in D:\nCon0 = { n\u2227\ni=1\nci \u2243 di \u21d2 c \u2243 d | f(c1, . . . , cn) \u2243 c, f(d1, . . . , dn) \u2243 d \u2208 D},\nwhich can be solved by a decision procedure for T0 or a fragment thereof. In the following example T0 is the theory of linear arithmetic over the real numbers, and T is its extension with a monotone unary function f , which is known to be a local extension [81]:\nExample 8. Let G be (a \u2264 b\u2227 f(a) = f(b) + 1). The enrichment N = {x \u2264 y \u21d2 f(x) \u2264 f(y)} consists of the monotonicity axiom. In order to check whether G is T -satisfiable, we compute N [G], omitting the redundant clauses c \u2264 c \u21d2 f(c) \u2264 f(c) for c \u2208 {a, b}:\nN [G] = {a \u2264 b \u21d2 f(a) \u2264 f(b), b \u2264 a \u21d2 f(b) \u2264 f(a)}.\nThe application of abstraction to N [G] \u222aG yields N0 \u222aG0 \u222aD, where:\nN0 = {a \u2264 b \u21d2 a1 \u2264 b1, b \u2264 a \u21d2 b1 \u2264 a1}, G0 = {a \u2264 b, a1 \u2243 b1 + 1},\nD = {a1 \u2243 f(a), b1 \u2243 f(b)}, and a1 and b1 are new constants. Thus, Con0 is {a \u2243 b \u21d2 a1 \u2243 b1}. A decision procedure for linear arithmetic applied to N0 \u222aG0 \u222a Con0 detects unsatisfiability."}, {"heading": "3.4 Beyond SMT: Satisfiability Modulo Assignment and MCsat", "text": "Like SGGS generalizes conflict-driven clause learning (CDCL) to first-order logic and Herbrand interpretations, the Model-Constructing satisfiability calculus, or MCsat for short, generalizes CDCL to decidable fragments of first-order theories and their models [35,55].\nRecall that in DPLL-CDCL the trail that represents the candidate partial model contains only propositional literals; the inference mechanism that explains conflicts is propositional resolution; and learnt clauses are made of input atoms. These three characteristics are true also of the DPLL(T ) paradigm for SMTsolvers [7], where an abstraction function maps finitely many input first-order ground atoms to finitely many propositional atoms. In this way, the method bridges the gap between the first-order language of the theory T and the propositional language of the DPLL-CDCL core solver. In DPLL(T ), also T -lemmas are made of input atoms, and the guarantee that no new atoms are generated is a key ingredient of the proof of termination of the method in [65].\nAlso when T is a union of theories T = \u22c3n\ni=1 Ti, the language of atoms remains finite. The standard method to combine satisfiability procedures for theories T1, . . . , Tn to get a satisfiability procedure for their union is equality sharing [64], better known as Nelson-Oppen scheme, even if equality sharing was the original name given by Greg Nelson, as reconstructed in [63]. Indeed, a key feature of equality sharing is that the combined procedures only need to share equalities between constant symbols. These equalities are mapped by the abstraction function to proxy variables, that is, propositional variables that stand for the equalities. As there are finitely many constant symbols, there are also finitely many proxy variables.\nMCsat generalizes both model representation and inference mechanism beyond satisfiability modulo theories (SMT), because it is designed to decide a more general problem called satisfiability modulo assignment (SMA). An SMA problem consists of determining the satisfiability of a formula S in a theory T , given an initial assignment I to some of the variables occuring in S, including both propositional variables and free first-order variables. SMT can be seen as a special case of SMA where I is empty. Also, since an SMT-solver builds partial assignments during the search for a satisfying one, an intermediate state of an SMT search can be viewed as an instance of SMA. A first major generalization of MCsat with respect to DPLL-CDCL and DPLL(T ) is to allow the trail to contain also assignments to free first-order variables (e.g., x \u2190 3). Such assignments can be semantic decisions or semantic propagations, thus called to distinguish them from the Boolean decisions and Boolean propagations that yield the standard Boolean assignments (e.g., L \u2190 true).\nThe answer to an SMA problem is either a model of S including the initial assignment I, or \u201cunsatisfiable\u201d with an explanation, that is, a formula\nS\u2032 that follows from S and is inconsistent with I. This notion of explanation is a generalization of the explanation of conflicts by propositional resolution in DPLL-CDCL. Indeed, a second major generalization of MCsat with respect to DPLL-CDCL and DPLL(T ) is to allow the inference mechanism that explains conflicts to generate new atoms, as shown in the following example in the quantifier-free fragment of the theory of equality:\nExample 9. Assume that S is a conjunction of literals including {v \u2243 f(a), w \u2243 f(b)}, where a and b are constant symbols, f is a function symbol, and v and w are free variables. If the trail contains the assignments a \u2190 \u03b1, b \u2190 \u03b1, w \u2190 \u03b21, v \u2190 \u03b22, where \u03b1, \u03b21, and \u03b22 denote distinct values of the appropriate sorts, there is a conflict. The explanation is the formula a \u2243 b \u21d2 f(a) \u2243 f(b), which is an instance of the substitutivity axiom, or congruence axiom, for function f . Note how the atoms a \u2243 b and f(a) \u2243 f(b) need not appear in S, and therefore such a lemma could not be generated in DPLL(T ).\nIn order to apply MCsat to a theory T , one needs to give clausal inference rules to explain conflicts in T . These inference rules generate clauses that may contain new (i.e., non-input) ground atoms in the signature of the theory. New atoms come from a basis, defined as the closure of the set of input atoms with respect to the inference rules. The proof of termination of the MCsat transition rules in [35] requires that the basis be finite. The following example illustrates the importance of this finiteness requirement:\nExample 10. Given S = {x \u2265 2, \u00ac(x \u2265 1) \u2228 y \u2265 1, x2 + y2 \u2264 1 \u2228 xy > 1}, and starting with an empty trail M = \u2205, a Boolean propagation puts x \u2265 2 in the trail. Theory propagation adds x \u2265 1, because x \u2265 2 implies x \u2265 1 in the theory, and x \u2265 1 appears in S. A Boolean propagation over clause \u00ac(x \u2265 1) \u2228 y \u2265 1 adds y \u2265 1, so that we have M = x \u2265 2, x \u2265 1, y \u2265 1. If a Boolean decision guesses next x2 + y2 \u2264 1 and then a semantic decision adds x \u2190 2, we have M = x \u2265 2, x \u2265 1, y \u2265 1, x2 + y2 \u2264 1, x \u2190 2 and a conflict, as there is no value for y such that 4 + y2 \u2264 1. Learning \u00ac(x = 2) as an explanation of the conflict does not work, because the procedure can then try x \u2190 3, and hit another conflict. Clearly, we do not want to learn the infinite sequence \u00ac(x = 2), \u00ac(x = 3), \u00ac(x = 4) . . ..\nSimilarly, also a systematic application of the inference rules to enumerate all atoms in a finite basis would be too inefficient. The key point is that the inference rules are applied only to explain conflicts and amend the current partial model, so that the generation of new atoms is conflict-driven. This concept is connected with that of interpolation (e.g., [83] for interpolation and locality, [23] for a survey on interpolation of ground proofs, and [24] for an approach to interpolation of non-ground proofs): given two inconsistent formul\u00e6 A and B, a formula that follows from A and is inconsistent with B is an interpolant of A and B, if it is made only of symbols that appear in both A and B. In a theory T , the notions of being inconsistent and being logical consequence are relative to T , and the interpolant is allowed to contain theory symbols even if they are\nnot common to A and B. Since an explanation is a formula S\u2032 that follows from S and is inconsistent with I, an interpolant of S and I (written as a formula) is an explanation. We illustrate these ideas continuing Example 10:\nExample 11. The solution is to observe that x2+y2 \u2264 1 implies \u22121 \u2264 x\u2227x \u2264 1, which is inconsistent with x = 2. Note that \u22121 \u2264 x \u2227 x \u2264 1 is an interpolant of x2 + y2 \u2264 1 and x = 2, as x appears in both. Thus, a desirable explanation is (x2+y2 \u2264 1) \u21d2 x \u2264 1, or \u00ac(x2+y2 \u2264 1)\u2228x \u2264 1 in clausal form, which brings the procedure to update the trail to M = x \u2265 2, x \u2265 1, y \u2265 1, x2 + y2 \u2264 1, x \u2264 1. At this point, x \u2265 2 and x \u2264 1 cause another theory conflict, which leads the procedure to learn the lemma \u00ac(x \u2265 2) \u2228 \u00ac(x \u2264 1). A first step of explanation by resolution between \u00ac(x2 + y2 \u2264 1) \u2228 x \u2264 1 and \u00ac(x \u2265 2) \u2228 \u00ac(x \u2264 1) yields \u00ac(x2 + y2 \u2264 1) \u2228 \u00ac(x \u2265 2). A second step of explanation by resolution between \u00ac(x2 + y2 \u2264 1) \u2228 \u00ac(x \u2265 2) and x \u2265 2 yields \u00ac(x2 + y2 \u2264 1), so that the trail is amended to M = x \u2265 2, x \u2265 1, y \u2265 1, \u00ac(x2 + y2 \u2264 1), finally repairing the decision (asserting x2 + y2 \u2264 1) that caused the conflict.4\nIn summary, MCsat is a fully model-based procedure, which lifts CDCL to SMT and SMA. Assignments to first-order variables and new literals are involved in decisions, propagations, conflict detections, and explanations, on a par with Boolean assignments and input literals. The theories covered in [35,55] are the quantifier-free fragments of the theories of equality, linear arithmetic, and boolean values, and their combinations. MCsat is also the name of the implementation of the method as described in [55]."}, {"heading": "4 Discussion", "text": "We surveyed model-based reasoning methods, where inferences build or amend partial models, which guide in turn further inferences, balancing search with inference, and search for a model with search for a proof. We exemplified these concepts for first-order clausal reasoning, and then we lifted them, sort of speak, to theory reasoning. Automated reasoning has made giant strides, and state of the art systems are very sophisticated in working with mostly syntactic information. The challenge of model-based methods is to go towards a semantically-oriented style of reasoning, that may pay off for hard problems or new domains.\nAcknowledgments The first author thanks David Plaisted, for starting the research on SGGS and inviting her to join in August 2008; and Leonardo de Moura, for the discussions on MCsat at Microsoft Research in Redmond in June 2013. The third author\u2019s work was partially supported by DFG TCRC SFB/TR 14 AVACS (www.avacs.org).\n4 The problem in Examples 10 and 11 appeared in the slides of a talk entitled \u201cArithmetic and Optimization @ MCsat\u201d presenting joint work by Leonardo de Moura, Dejan Jovanovic\u0301, and Grant Olney Passmore, and given by Leonardo de Moura at a Schloss Dagsthul Seminar on \u201cDeduction and Arithmetic\u201d in October 2013."}], "references": [{"title": "Superposition modulo linear arithmetic SUP(LA)", "author": ["Ernst Althaus", "Evgeny Kruglov", "Christoph Weidenbach"], "venue": "Proceedings of FroCoS-7,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Theorem proving techniques for view deletion in databases", "author": ["Chandrabose Aravindan", "Peter Baumgartner"], "venue": "Journal of Symbolic Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Dual) Hoops have unique halving", "author": ["Rob Arthan", "Paulo Oliva"], "venue": "Automated Reasoning and Mathematics: Essays in Memory of William W. McCune, volume 7788 of Lecture Notes in Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Rewrite-based equational theorem proving with selection and simplification", "author": ["Leo Bachmair", "Harald Ganzinger"], "venue": "Journal of Logic and Computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Ordered chaining calculi for first-order theories of transitive relations", "author": ["Leo Bachmair", "Harald Ganzinger"], "venue": "Journal of the ACM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Refutational theorem proving for hierarchic first-order theories", "author": ["Leo Bachmair", "Harald Ganzinger", "Uwe Waldmann"], "venue": "Applicable Algebra in Engineering Communication and Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Satisfiability modulo theories", "author": ["Clark Barrett", "Roberto Sebastiani", "Sanjit A. Seshia", "Cesare Tinelli"], "venue": "Handbook of Satisfiability,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Semantically guided theorem proving for diagnosis applications", "author": ["Peter Baumgartner", "Peter Fr\u00f6hlich", "Ulrich Furbach", "Wolfgang Nejdl"], "venue": "In Proceedings of IJCAI16,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Implementing the model evolution calculus", "author": ["Peter Baumgartner", "Alexander Fuchs", "Cesare Tinelli"], "venue": "International Journal on Artificial Intelligence Tools,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Hyper tableaux", "author": ["Peter Baumgartner", "Ulrich Furbach", "Ilkka Niemel\u00e4"], "venue": "Proceedings of JELIA-5,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1996}, {"title": "The hyper tableaux calculus with equality and an application to finite model computation", "author": ["Peter Baumgartner", "Ulrich Furbach", "Bj\u00f6rn Pelzer"], "venue": "Journal of Logic and Computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Model evolution calculus with equality \u2013 revised and implemented", "author": ["Peter Baumgartner", "Bj\u00f6rn Pelzer", "Cesare Tinelli"], "venue": "Journal of Symbolic Computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "The model evolution calculus as a firstorder DPLL method", "author": ["Peter Baumgartner", "Cesare Tinelli"], "venue": "Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Superposition and model evolution combined", "author": ["Peter Baumgartner", "Uwe Waldmann"], "venue": "Proceedings of CADE-22,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Hierarchic superposition with weak abstraction", "author": ["Peter Baumgartner", "Uwe Waldmann"], "venue": "Proceedings of CADE-24,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "E-KRHyper 1.4: Extensions for unique names and description logic", "author": ["Markus Bender", "Bj\u00f6rn Pelzer", "Claudia Schon"], "venue": "Proceedings 20  of CADE-24,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Automated Deduction - A Basis for Applications (in 2 volumes)", "author": ["Wolfgang Bibel", "Peter H. Schmitt", "editors"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "A taxonomy of theorem-proving strategies", "author": ["Maria Paola Bonacina"], "venue": "Artificial Intelligence Today \u2013 Recent Trends and Developments,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "On theorem proving for program checking \u2013 Historical perspective and recent developments", "author": ["Maria Paola Bonacina"], "venue": "Proceedings of PPDP-12,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Theory decision by decomposition", "author": ["Maria Paola Bonacina", "Mnacho Echenim"], "venue": "Journal of Symbolic Computation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Towards a foundation of completion procedures as semidecision procedures", "author": ["Maria Paola Bonacina", "Jieh Hsiang"], "venue": "Theoretical Computer Science,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1995}, {"title": "On semantic resolution with lemmaizing and contraction and a formal treatment of caching", "author": ["Maria Paola Bonacina", "Jieh Hsiang"], "venue": "New Generation Computing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Interpolation of ground proofs in automated deduction: a survey", "author": ["Maria Paola Bonacina", "Moa Johansson"], "venue": "Journal of Automated Reasoning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "On interpolation in automated theorem proving", "author": ["Maria Paola Bonacina", "Moa Johansson"], "venue": "Journal of Automated Reasoning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "On deciding satisfiability by theorem proving with speculative inferences", "author": ["Maria Paola Bonacina", "Christopher A. Lynch", "Leonardo de Moura"], "venue": "Journal of Automated Reasoning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Constraint manipulation in SGGS", "author": ["Maria Paola Bonacina", "David A. Plaisted"], "venue": "Proceedings of UNIF-28, RISC Technical Reports,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Semantically guided goal-sensitive reasoning: inference system and completeness", "author": ["Maria Paola Bonacina", "David A. Plaisted"], "venue": "In preparation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Semantically-guided goal-sensitive reasoning: model representation", "author": ["Maria Paola Bonacina", "David A. Plaisted"], "venue": "Journal of Automated Reasoning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "SGGS theorem proving: an exposition", "author": ["Maria Paola Bonacina", "David A. Plaisted"], "venue": "Proceedings of PAAR-4 (2014),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Permutative rewriting and unification", "author": ["Thierry Boy de la Tour", "Mnacho Echenim"], "venue": "Information and Computation,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "Symbolic Logic and Mechanical Theorem Proving", "author": ["Chin-Liang Chang", "Richard Char-Tung Lee"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1973}, {"title": "A machine program for theorem-proving", "author": ["Martin Davis", "George Logemann", "Donald Loveland"], "venue": "Communications of the ACM,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1962}, {"title": "A computing procedure for quantification theory", "author": ["Martin Davis", "Hilary Putnam"], "venue": "Journal of the ACM,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1960}, {"title": "Satisfiability modulo theories: introduction and applications", "author": ["Leonardo de Moura", "Nikolaj Bj\u00f8rner"], "venue": "Communications of the ACM,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "A model-constructing satisfiability calculus", "author": ["Leonardo de Moura", "Dejan Jovanovi\u0107"], "venue": "Proceedings of VMCAI-14,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Variant narrowing and equational unification", "author": ["Santiago Escobar", "Jos\u00e9 Meseguer", "Ralf Sasse"], "venue": "Electronic Notes in Theoretical Computer Science,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Gibbard\u2019s collapse theorem for the indicative conditional: an axiomatic approach", "author": ["Branden Fitelson"], "venue": "Automated Reasoning and Mathematics: Essays in Memory of William W. McCune, volume 7788 of Lecture Notes in Artificial Intelligence,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Semantically guided evolution of SHI ABoxes", "author": ["Ulrich Furbach", "Claudia Schon"], "venue": "Proceedings of TABLEAUX-22,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Designing unification procedures using transformations: a survey", "author": ["Jean H. Gallier", "Wayne Snyder"], "venue": "EATCS Bulletin,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1990}, {"title": "Modular proof systems for partial functions with Evans equality", "author": ["Harald Ganzinger", "Viorica Sofronie-Stokkermans", "Uwe Waldmann"], "venue": "Information and Computation,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2006}, {"title": "Theorem proving in cancellative abelian monoids", "author": ["Harald Ganzinger", "Uwe Waldmann"], "venue": "Proceedings of CADE-13,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1996}, {"title": "Order-sorted unification", "author": ["Joe Goguen", "Jos\u00e9 Meseguer"], "venue": "Journal of Symbolic Computation,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1989}, {"title": "Order-sorted algebra I: equational deduction for multiple inheritance, overloading, exceptions and partial operations", "author": ["Joe Goguen", "Jos\u00e9 Meseguer"], "venue": "Theoretical Computer Science,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1992}, {"title": "A sufficient completeness reasoning tool for partial specifications", "author": ["Joe Hendrix", "Manuel Clavel", "Jos\u00e9 Meseguer"], "venue": "Proceedings of RTA-16,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2005}, {"title": "Order-sorted equational unification revisited", "author": ["Joe Hendrix", "Jos\u00e9 Meseguer"], "venue": "Electronic Notes in Theoretical Computer Science,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2012}, {"title": "A sufficient completeness checker for linear order-sorted specifications modulo axioms", "author": ["Joe Hendrix", "Jos\u00e9 Meseguer", "Hitoshi Ohsaki"], "venue": "Proceedings of IJCAR-3,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2006}, {"title": "Proving refutational completeness of theorem proving strategies: the transfinite semantic tree method", "author": ["Jieh Hsiang", "Micha\u00ebl Rusinowitch"], "venue": "Journal of the ACM,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1991}, {"title": "Complete inference rules for the cancellation laws", "author": ["Jieh Hsiang", "Micha\u00ebl Rusinowitch", "Ko Sakai"], "venue": "In Proceedings of IJCAI-10,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1987}, {"title": "On local reasoning in verification", "author": ["Carsten Ihlemann", "Swen Jacobs", "Viorica Sofronie-Stokkermans"], "venue": "Proceedings of TACAS-14,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2008}, {"title": "On hierarchical reasoning in combinations of theories", "author": ["Carsten Ihlemann", "Viorica Sofronie-Stokkermans"], "venue": "Proceedings of IJCAR-5,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}, {"title": "GRASP: A new search algorithm for satisfiability", "author": ["Jo\u00e3o P. Marques-Silva", "Karem A. Sakallah"], "venue": "In Proceedings of ICCAD", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1996}, {"title": "Solving equations in abstract algebras: a rule-based survey of unification", "author": ["Jean-Pierre Jouannaud", "Claude Kirchner"], "venue": "Computational Logic \u2013 Essays in Honor of Alan Robinson,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1991}, {"title": "Completion of a set of rules modulo a set of equations", "author": ["Jean-Pierre Jouannaud", "H\u00e9l\u00e8ne Kirchner"], "venue": "SIAM Journal of Computing,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1986}, {"title": "The design and implementation of the model-constructing satisfiability calculus", "author": ["Dejan Jovanovi\u0107", "Clark Barrett", "Leonardo de Moura"], "venue": "In Barbara Jobstman and Sandip Ray, editors, Proceedings of FMCAD-13. ACM and IEEE,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2013}, {"title": "An automated tool for analyzing completeness of equational specifications", "author": ["Deepak Kapur"], "venue": "In Proceedings of ISSTA-94,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1994}, {"title": "Sufficient-completeness, ground-reducibility and their complexity", "author": ["Deepak Kapur", "Paliath Narendran", "Daniel J. Rosenkrantz", "Hantao Zhang"], "venue": "Acta Informatica,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1991}, {"title": "Eliminating duplication with the hyperlinking strategy", "author": ["Shie-Jue Lee", "David A. Plaisted"], "venue": "Journal of Automated Reasoning,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 1992}, {"title": "Knowledge representation and classical logic", "author": ["Vladimir Lifschitz", "Leora Morgenstern", "David A. Plaisted"], "venue": "Handbook of Knowledge Representation,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2008}, {"title": "Boolean satisfiability: from theoretical hardness to practical success", "author": ["Sharad Malik", "Lintao Zhang"], "venue": "Communications of the ACM,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2009}, {"title": "OTTER 3.3 reference manual", "author": ["William W. McCune"], "venue": "Technical Report ANL/MCSTM-263,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2003}, {"title": "Chaff: Engineering an efficient SAT solver", "author": ["Matthew W. Moskewicz", "Conor F. Madigan", "Ying Zhao", "Lintao Zhang", "Sharad Malik"], "venue": "Proceedings of DAC-39,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2001}, {"title": "Combining satisfiability procedures by equality sharing", "author": ["Greg Nelson"], "venue": "American Mathematical Society,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 1983}, {"title": "Simplification by cooperating decision procedures", "author": ["Greg Nelson", "Derek C. Oppen"], "venue": "ACM Transactions on Programming Languages,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 1979}, {"title": "Solving SAT and SAT modulo theories: from an abstract Davis-Putnam-Logemann-Loveland procedure to DPLL(T)", "author": ["Robert Nieuwenhuis", "Albert Oliveras", "Cesare Tinelli"], "venue": "Journal of the ACM,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2006}, {"title": "Complete sets of reductions for some equational theories", "author": ["Gerald E. Peterson", "Mark E. Stickel"], "venue": "Journal of the ACM,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 1981}, {"title": "Mechanical theorem proving", "author": ["David A. Plaisted"], "venue": "Formal Techniques in Artificial Intelligence,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 1990}, {"title": "Equational reasoning and term rewriting systems", "author": ["David A. Plaisted"], "venue": "Handbook of Logic in Artificial Intelligence and Logic Programming,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 1993}, {"title": "Automated theorem proving", "author": ["David A. Plaisted"], "venue": "Wiley Interdisciplinary Reviews: Cognitive Science,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2014}, {"title": "The Efficiency of Theorem Proving Strategies", "author": ["David A. Plaisted", "Yunshan Zhu"], "venue": null, "citeRegEx": "70", "shortCiteRegEx": "70", "year": 1997}, {"title": "Ordered semantic hyper linking", "author": ["David A. Plaisted", "Yunshan Zhu"], "venue": "Journal of Automated Reasoning,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2000}, {"title": "Building in equational theories", "author": ["Gordon Plotkin"], "venue": "Machine Intelligence,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 1972}, {"title": "Paramodulation and theorem proving in first order theories with equality", "author": ["George A. Robinson", "Lawrence Wos"], "venue": "Machine Intelligence,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 1969}, {"title": "Automatic deduction with hyper-resolution", "author": ["John Alan Robinson"], "venue": "International Journal of Computer Mathematics,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 1965}, {"title": "A machine oriented logic based on the resolution principle", "author": ["John Alan Robinson"], "venue": "Journal of the ACM,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 1965}, {"title": "Handbook of Automated Reasoning (in 2 volumes)", "author": ["John Alan Robinson", "Andrei Voronkov", "editors"], "venue": null, "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2001}, {"title": "Theorem-proving with resolution and superposition", "author": ["Micha\u00ebl Rusinowitch"], "venue": "Journal of Symbolic Computation,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 1991}, {"title": "HermiT: A highly efficient OWL reasoner", "author": ["Rob Shearer", "Boris Motik", "Ian Horrocks"], "venue": "Proceedings of OWLED-5,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2008}, {"title": "Automatic theorem proving with renamable and semantic resolution", "author": ["James R. Slagle"], "venue": "Journal of the ACM,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 1967}, {"title": "SCOTT: Semantically constrained Otter", "author": ["John Slaney", "Ewing Lusk", "andWilliam McCune"], "venue": "Proceedings of CADE-12,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 1994}, {"title": "Hierarchic reasoning in local theory extensions", "author": ["Viorica Sofronie-Stokkermans"], "venue": "Proceedings of CADE-20,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2005}, {"title": "Hierarchical and modular reasoning in complex theories: The case of local theory extensions", "author": ["Viorica Sofronie-Stokkermans"], "venue": "Proceedings of FroCoS-6,", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2007}, {"title": "Interpolation in local theory extensions", "author": ["Viorica Sofronie-Stokkermans"], "venue": "Logical Methods in Computer Science,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2008}, {"title": "Superposition theorem proving for abelian groups represented as integer modules", "author": ["J\u00fcrgen Stuber"], "venue": "Theoretical Computer Science,", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 1998}, {"title": "Superposition for divisible torsion-free abelian groups", "author": ["Uwe Waldmann"], "venue": "Proceedings of CADE-15,", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 1998}, {"title": "Efficiency and completeness of the set of support strategy in theorem proving", "author": ["Larry Wos", "D. Carson", "G. Robinson"], "venue": "Journal of the ACM,", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 1965}, {"title": "MACE4 and SEM: A comparison of finite model generators", "author": ["Hantao Zhang", "Jian Zhang"], "venue": "Automated Reasoning and Mathematics: Essays in Memory of William W. McCune,", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2013}, {"title": "The quest for efficient boolean satisfiability solvers", "author": ["Lintao Zhang", "Sharad Malik"], "venue": "Proceedings of CADE-18,", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2002}], "referenceMentions": [{"referenceID": 50, "context": "The core procedure of these solvers is the conflict-driven clause learning (CDCL) version [52,62,88,60] of the Davis-Putnam-Logemann-Loveland (DPLL) procedure for propositional logic [32].", "startOffset": 90, "endOffset": 103}, {"referenceID": 60, "context": "The core procedure of these solvers is the conflict-driven clause learning (CDCL) version [52,62,88,60] of the Davis-Putnam-Logemann-Loveland (DPLL) procedure for propositional logic [32].", "startOffset": 90, "endOffset": 103}, {"referenceID": 86, "context": "The core procedure of these solvers is the conflict-driven clause learning (CDCL) version [52,62,88,60] of the Davis-Putnam-Logemann-Loveland (DPLL) procedure for propositional logic [32].", "startOffset": 90, "endOffset": 103}, {"referenceID": 58, "context": "The core procedure of these solvers is the conflict-driven clause learning (CDCL) version [52,62,88,60] of the Davis-Putnam-Logemann-Loveland (DPLL) procedure for propositional logic [32].", "startOffset": 90, "endOffset": 103}, {"referenceID": 31, "context": "The core procedure of these solvers is the conflict-driven clause learning (CDCL) version [52,62,88,60] of the Davis-Putnam-Logemann-Loveland (DPLL) procedure for propositional logic [32].", "startOffset": 183, "endOffset": 187}, {"referenceID": 32, "context": "The original Davis-Putnam (DP) procedure [33] was proposed for first-order logic, and featured propositional, or ground, resolution.", "startOffset": 41, "endOffset": 45}, {"referenceID": 6, "context": "SMT-solvers integrate in DPLL-CDCL a decision procedure for satisfiability in a theory or combination of theories T : the T -satisfiability procedure raises a T -conflict when the candidate partial model is not consistent with T , and generates T -lemmas to add theory reasoning to the inference component [7,34].", "startOffset": 306, "endOffset": 312}, {"referenceID": 33, "context": "SMT-solvers integrate in DPLL-CDCL a decision procedure for satisfiability in a theory or combination of theories T : the T -satisfiability procedure raises a T -conflict when the candidate partial model is not consistent with T , and generates T -lemmas to add theory reasoning to the inference component [7,34].", "startOffset": 306, "endOffset": 312}, {"referenceID": 65, "context": "Background material is available in previous surveys, such as [67,68,18,59,69] for theorem-proving strategies, [19] for decision procedures based on theorem-proving strategies or their integration with SMT-solvers, and books such as [70,17,76].", "startOffset": 62, "endOffset": 78}, {"referenceID": 66, "context": "Background material is available in previous surveys, such as [67,68,18,59,69] for theorem-proving strategies, [19] for decision procedures based on theorem-proving strategies or their integration with SMT-solvers, and books such as [70,17,76].", "startOffset": 62, "endOffset": 78}, {"referenceID": 17, "context": "Background material is available in previous surveys, such as [67,68,18,59,69] for theorem-proving strategies, [19] for decision procedures based on theorem-proving strategies or their integration with SMT-solvers, and books such as [70,17,76].", "startOffset": 62, "endOffset": 78}, {"referenceID": 57, "context": "Background material is available in previous surveys, such as [67,68,18,59,69] for theorem-proving strategies, [19] for decision procedures based on theorem-proving strategies or their integration with SMT-solvers, and books such as [70,17,76].", "startOffset": 62, "endOffset": 78}, {"referenceID": 67, "context": "Background material is available in previous surveys, such as [67,68,18,59,69] for theorem-proving strategies, [19] for decision procedures based on theorem-proving strategies or their integration with SMT-solvers, and books such as [70,17,76].", "startOffset": 62, "endOffset": 78}, {"referenceID": 18, "context": "Background material is available in previous surveys, such as [67,68,18,59,69] for theorem-proving strategies, [19] for decision procedures based on theorem-proving strategies or their integration with SMT-solvers, and books such as [70,17,76].", "startOffset": 111, "endOffset": 115}, {"referenceID": 68, "context": "Background material is available in previous surveys, such as [67,68,18,59,69] for theorem-proving strategies, [19] for decision procedures based on theorem-proving strategies or their integration with SMT-solvers, and books such as [70,17,76].", "startOffset": 233, "endOffset": 243}, {"referenceID": 16, "context": "Background material is available in previous surveys, such as [67,68,18,59,69] for theorem-proving strategies, [19] for decision procedures based on theorem-proving strategies or their integration with SMT-solvers, and books such as [70,17,76].", "startOffset": 233, "endOffset": 243}, {"referenceID": 74, "context": "Background material is available in previous surveys, such as [67,68,18,59,69] for theorem-proving strategies, [19] for decision procedures based on theorem-proving strategies or their integration with SMT-solvers, and books such as [70,17,76].", "startOffset": 233, "endOffset": 243}, {"referenceID": 73, "context": "Soon after the seminal article by Alan Robinson introducing the resolution principle [75], JamesR.", "startOffset": 85, "endOffset": 89}, {"referenceID": 77, "context": "Slagle presented semantic resolution in [79].", "startOffset": 40, "endOffset": 44}, {"referenceID": 30, "context": "The following example from [31] illustrates the concept in propositional logic:", "startOffset": 27, "endOffset": 31}, {"referenceID": 30, "context": "The following example also from [31] is in first-order logic:", "startOffset": 32, "endOffset": 36}, {"referenceID": 77, "context": "The examples in [79] include a theorem from algebra, where the interpretation is given by a multiplication table and hence is really of semantic nature.", "startOffset": 16, "endOffset": 20}, {"referenceID": 72, "context": "Two instances of semantic resolution that aimed at addressing this issue are hyperresolution [74] and the set-of-support strategy [86].", "startOffset": 93, "endOffset": 97}, {"referenceID": 84, "context": "Two instances of semantic resolution that aimed at addressing this issue are hyperresolution [74] and the set-of-support strategy [86].", "startOffset": 130, "endOffset": 134}, {"referenceID": 3, "context": "Indeed, resolution can be restricted by a selection function that selects negative literals [4].", "startOffset": 92, "endOffset": 95}, {"referenceID": 59, "context": "The set-of-support strategy is available in all theorem provers that feature the given-clause loop [61], which is a de facto standard for resolution-based provers.", "startOffset": 99, "endOffset": 103}, {"referenceID": 21, "context": "The integration of contraction rules and other enhancements, such as lemmaizing, in semantic strategies was investigated in general in [22].", "startOffset": 135, "endOffset": 139}, {"referenceID": 78, "context": "A beginning of the evolution from being semantically guided to being modelbased can be traced back to the SCOTT system [80], which combined the finite model finder FINDER, that searches for small models, and the resolution-based theorem prover OTTER [61].", "startOffset": 119, "endOffset": 123}, {"referenceID": 59, "context": "A beginning of the evolution from being semantically guided to being modelbased can be traced back to the SCOTT system [80], which combined the finite model finder FINDER, that searches for small models, and the resolution-based theorem prover OTTER [61].", "startOffset": 250, "endOffset": 254}, {"referenceID": 85, "context": "Research on the cooperation between theorem prover and finite model finder continued with successors of OTTER, such as Prover9, and successors of FINDER, such as MACE4 [87].", "startOffset": 168, "endOffset": 172}, {"referenceID": 2, "context": ", [3,38]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 36, "context": ", [3,38]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 74, "context": ", Chapter 3 in [76]).", "startOffset": 15, "endOffset": 19}, {"referenceID": 9, "context": "The hypertableau calculus [10] offers a more liberal treatment of variables, and borrows the concept of hyperinference from positive hyperresolution.", "startOffset": 26, "endOffset": 30}, {"referenceID": 9, "context": "[10]): the initialization", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The latter feature allows a superposition-like handling of equality [11], while the former is relevant for hypertableaux for description logic [78], which we shall return to in the next section.", "startOffset": 68, "endOffset": 72}, {"referenceID": 76, "context": "The latter feature allows a superposition-like handling of equality [11], while the former is relevant for hypertableaux for description logic [78], which we shall return to in the next section.", "startOffset": 143, "endOffset": 147}, {"referenceID": 7, "context": "In this section we present a transformation method, borrowed from model-based diagnosis and presented in [8], which is based on a given model and therefore can be installed on top of hypertableaux.", "startOffset": 105, "endOffset": 108}, {"referenceID": 7, "context": "This transformation technique enabled a hypertableau prover to compute benchmarks from electrical engineering [8], and was also applied to the view update problem in databases [2].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "This transformation technique enabled a hypertableau prover to compute benchmarks from electrical engineering [8], and was also applied to the view update problem in databases [2].", "startOffset": 176, "endOffset": 179}, {"referenceID": 37, "context": "Although this transformation mechanism only works in the propositional case, it can be extended to description logic [39].", "startOffset": 117, "endOffset": 121}, {"referenceID": 76, "context": "Indeed, most description logic reasoners are based on tableau calculi, and a hypertableau calculus was used in [78] as a basis for an efficient reasoner for the description logic SHIQ.", "startOffset": 111, "endOffset": 115}, {"referenceID": 37, "context": "\u201d Given a Tbox in the form of a set of DL-clauses, if we have in addition an Abox, or a set of ground assertions, we can use the interpretation given by the ABox as initial model for the model-based transformation [39].", "startOffset": 214, "endOffset": 218}, {"referenceID": 15, "context": "On this basis, the already mentioned E-Hyper reasoner was modified to become E-KRHyper, which was shown to be a decision procedure for SHIQ in [16].", "startOffset": 143, "endOffset": 147}, {"referenceID": 12, "context": "The quest for ways to split efficiently clauses such as A(x) \u2228 B(x) led to the model evolution calculus [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 8, "context": "The model evolution calculus was implemented in the Darwin prover [9], and extended to handle equality on its own [12] and with superposition [14].", "startOffset": 66, "endOffset": 69}, {"referenceID": 11, "context": "The model evolution calculus was implemented in the Darwin prover [9], and extended to handle equality on its own [12] and with superposition [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "The model evolution calculus was implemented in the Darwin prover [9], and extended to handle equality on its own [12] and with superposition [14].", "startOffset": 142, "endOffset": 146}, {"referenceID": 28, "context": "SGGS, for Semantically-Guided Goal-Sensitive reasoning, is a new theoremproving method for first-order logic [29,26,28,27], which inherits features from several of the strategies that we surveyed in the previous sections.", "startOffset": 109, "endOffset": 122}, {"referenceID": 25, "context": "SGGS, for Semantically-Guided Goal-Sensitive reasoning, is a new theoremproving method for first-order logic [29,26,28,27], which inherits features from several of the strategies that we surveyed in the previous sections.", "startOffset": 109, "endOffset": 122}, {"referenceID": 27, "context": "SGGS, for Semantically-Guided Goal-Sensitive reasoning, is a new theoremproving method for first-order logic [29,26,28,27], which inherits features from several of the strategies that we surveyed in the previous sections.", "startOffset": 109, "endOffset": 122}, {"referenceID": 26, "context": "SGGS, for Semantically-Guided Goal-Sensitive reasoning, is a new theoremproving method for first-order logic [29,26,28,27], which inherits features from several of the strategies that we surveyed in the previous sections.", "startOffset": 109, "endOffset": 122}, {"referenceID": 56, "context": "With hyperresolution and hypertableaux, it shares the concept of hyperinference, although the hyperinference in SGGS, as we shall see, is an instance generation inference, and therefore its closest ancestor is hyperlinking [58,71], an inference rule that uses the most general unifier of a hyperresolution step to generate instances of the parents, rather than a hyperresolvent.", "startOffset": 223, "endOffset": 230}, {"referenceID": 69, "context": "With hyperresolution and hypertableaux, it shares the concept of hyperinference, although the hyperinference in SGGS, as we shall see, is an instance generation inference, and therefore its closest ancestor is hyperlinking [58,71], an inference rule that uses the most general unifier of a hyperresolution step to generate instances of the parents, rather than a hyperresolvent.", "startOffset": 223, "endOffset": 230}, {"referenceID": 71, "context": ", [73,48,77,4,21] and Chapters 7 and 9 in [76]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 46, "context": ", [73,48,77,4,21] and Chapters 7 and 9 in [76]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 75, "context": ", [73,48,77,4,21] and Chapters 7 and 9 in [76]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 3, "context": ", [73,48,77,4,21] and Chapters 7 and 9 in [76]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 20, "context": ", [73,48,77,4,21] and Chapters 7 and 9 in [76]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 74, "context": ", [73,48,77,4,21] and Chapters 7 and 9 in [76]).", "startOffset": 42, "endOffset": 46}, {"referenceID": 70, "context": ", [72,66,54,49,36,40,53,30]).", "startOffset": 2, "endOffset": 27}, {"referenceID": 64, "context": ", [72,66,54,49,36,40,53,30]).", "startOffset": 2, "endOffset": 27}, {"referenceID": 52, "context": ", [72,66,54,49,36,40,53,30]).", "startOffset": 2, "endOffset": 27}, {"referenceID": 47, "context": ", [72,66,54,49,36,40,53,30]).", "startOffset": 2, "endOffset": 27}, {"referenceID": 38, "context": ", [72,66,54,49,36,40,53,30]).", "startOffset": 2, "endOffset": 27}, {"referenceID": 51, "context": ", [72,66,54,49,36,40,53,30]).", "startOffset": 2, "endOffset": 27}, {"referenceID": 29, "context": ", [72,66,54,49,36,40,53,30]).", "startOffset": 2, "endOffset": 27}, {"referenceID": 41, "context": ", [43,37,46]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 35, "context": ", [43,37,46]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 44, "context": ", [43,37,46]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 40, "context": "This kind of approach was pursued further, by building into superposition axioms for monoids [42], groups [85], rings and modules [84], or by generalizing superposition to embed transitive relations other than equality [5].", "startOffset": 93, "endOffset": 97}, {"referenceID": 83, "context": "This kind of approach was pursued further, by building into superposition axioms for monoids [42], groups [85], rings and modules [84], or by generalizing superposition to embed transitive relations other than equality [5].", "startOffset": 106, "endOffset": 110}, {"referenceID": 82, "context": "This kind of approach was pursued further, by building into superposition axioms for monoids [42], groups [85], rings and modules [84], or by generalizing superposition to embed transitive relations other than equality [5].", "startOffset": 130, "endOffset": 134}, {"referenceID": 4, "context": "This kind of approach was pursued further, by building into superposition axioms for monoids [42], groups [85], rings and modules [84], or by generalizing superposition to embed transitive relations other than equality [5].", "startOffset": 219, "endOffset": 222}, {"referenceID": 42, "context": ", [44]), it became clear that a major issue at the cross-roads of reasoning, specifying, and programming, is that theories, or specifications, are built by extension to form hierarchies.", "startOffset": 2, "endOffset": 6}, {"referenceID": 55, "context": ", [57]), sufficient completeness analyzers exist (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 54, "context": ", [56,45,47]), with key contributions by Jos\u00e9 Meseguer.", "startOffset": 2, "endOffset": 12}, {"referenceID": 43, "context": ", [56,45,47]), with key contributions by Jos\u00e9 Meseguer.", "startOffset": 2, "endOffset": 12}, {"referenceID": 45, "context": ", [56,45,47]), with key contributions by Jos\u00e9 Meseguer.", "startOffset": 2, "endOffset": 12}, {"referenceID": 5, "context": "Hierarchic superposition was introduced in [6] and developed in [41] to reason about a hierarchy (T0, T ) with enrichment axioms N , where N is a set of clauses.", "startOffset": 43, "endOffset": 46}, {"referenceID": 39, "context": "Hierarchic superposition was introduced in [6] and developed in [41] to reason about a hierarchy (T0, T ) with enrichment axioms N , where N is a set of clauses.", "startOffset": 64, "endOffset": 68}, {"referenceID": 5, "context": "Hierarchic superposition was proved refutationally complete in [6], provided T0 is compact, which is a basic preliminary to make constraint refutation mechanizable, and N \u222aS is sufficiently complete with respect to simple instances, which means that for every model I of all simple ground instances of the clauses in N \u222a S, and every ground non-base term t, there exists a ground base term t (which may depend on I) such that I |= t \u2243 t.", "startOffset": 63, "endOffset": 66}, {"referenceID": 19, "context": "3 Other approaches to subdivide work between superposition and an SMT-solver appeared in [20,25].", "startOffset": 89, "endOffset": 96}, {"referenceID": 24, "context": "3 Other approaches to subdivide work between superposition and an SMT-solver appeared in [20,25].", "startOffset": 89, "endOffset": 96}, {"referenceID": 39, "context": "Hierarchic superposition was generalized to handle both total and partial function symbols, yielding a partial hierarchic superposition calculus [41].", "startOffset": 145, "endOffset": 149}, {"referenceID": 39, "context": "Under the assumption that T0 is a universal first-order theory, which ensures compactness, the partial hierarchic superposition calculus was proved sound and complete in [41]: if a contradiction cannot be derived from N\u222aS using this calculus, then N\u222aS has a model which is a partial algebra.", "startOffset": 170, "endOffset": 174}, {"referenceID": 0, "context": "Research on hierarchic superposition continued in [1], where an implementation for extensions of linear arithmetic was presented, and in [15], where the calculus was made \u201cmore complete\u201d in practice.", "startOffset": 50, "endOffset": 53}, {"referenceID": 14, "context": "Research on hierarchic superposition continued in [1], where an implementation for extensions of linear arithmetic was presented, and in [15], where the calculus was made \u201cmore complete\u201d in practice.", "startOffset": 137, "endOffset": 141}, {"referenceID": 79, "context": "A series of papers starting with [81] identified a class of theory extensions (T0, T ), called local, which admit a complete hierarchical method for checking satisfiabil-", "startOffset": 33, "endOffset": 37}, {"referenceID": 79, "context": "Theory T is a local extension of T0, if N [G] suffices to prove the T -unsatisfiability of G [81].", "startOffset": 93, "endOffset": 97}, {"referenceID": 79, "context": "Subsequent papers studied variants of locality, including those for extensions with augmented clauses, and for combinations of local theories, and proved that locality can be recognized by showing that certain partial algebras embed into total ones [81,82,50,51].", "startOffset": 249, "endOffset": 262}, {"referenceID": 80, "context": "Subsequent papers studied variants of locality, including those for extensions with augmented clauses, and for combinations of local theories, and proved that locality can be recognized by showing that certain partial algebras embed into total ones [81,82,50,51].", "startOffset": 249, "endOffset": 262}, {"referenceID": 48, "context": "Subsequent papers studied variants of locality, including those for extensions with augmented clauses, and for combinations of local theories, and proved that locality can be recognized by showing that certain partial algebras embed into total ones [81,82,50,51].", "startOffset": 249, "endOffset": 262}, {"referenceID": 49, "context": "Subsequent papers studied variants of locality, including those for extensions with augmented clauses, and for combinations of local theories, and proved that locality can be recognized by showing that certain partial algebras embed into total ones [81,82,50,51].", "startOffset": 249, "endOffset": 262}, {"referenceID": 79, "context": "If T is a local extension, it is possible to check the T -satisfiability of G by hierarchical reasoning [81,82,50,51], allowing the introduction of new constants by abstraction as in [64].", "startOffset": 104, "endOffset": 117}, {"referenceID": 80, "context": "If T is a local extension, it is possible to check the T -satisfiability of G by hierarchical reasoning [81,82,50,51], allowing the introduction of new constants by abstraction as in [64].", "startOffset": 104, "endOffset": 117}, {"referenceID": 48, "context": "If T is a local extension, it is possible to check the T -satisfiability of G by hierarchical reasoning [81,82,50,51], allowing the introduction of new constants by abstraction as in [64].", "startOffset": 104, "endOffset": 117}, {"referenceID": 49, "context": "If T is a local extension, it is possible to check the T -satisfiability of G by hierarchical reasoning [81,82,50,51], allowing the introduction of new constants by abstraction as in [64].", "startOffset": 104, "endOffset": 117}, {"referenceID": 62, "context": "If T is a local extension, it is possible to check the T -satisfiability of G by hierarchical reasoning [81,82,50,51], allowing the introduction of new constants by abstraction as in [64].", "startOffset": 183, "endOffset": 187}, {"referenceID": 79, "context": "In the following example T0 is the theory of linear arithmetic over the real numbers, and T is its extension with a monotone unary function f , which is known to be a local extension [81]:", "startOffset": 183, "endOffset": 187}, {"referenceID": 34, "context": "Like SGGS generalizes conflict-driven clause learning (CDCL) to first-order logic and Herbrand interpretations, the Model-Constructing satisfiability calculus, or MCsat for short, generalizes CDCL to decidable fragments of first-order theories and their models [35,55].", "startOffset": 261, "endOffset": 268}, {"referenceID": 53, "context": "Like SGGS generalizes conflict-driven clause learning (CDCL) to first-order logic and Herbrand interpretations, the Model-Constructing satisfiability calculus, or MCsat for short, generalizes CDCL to decidable fragments of first-order theories and their models [35,55].", "startOffset": 261, "endOffset": 268}, {"referenceID": 6, "context": "These three characteristics are true also of the DPLL(T ) paradigm for SMTsolvers [7], where an abstraction function maps finitely many input first-order ground atoms to finitely many propositional atoms.", "startOffset": 82, "endOffset": 85}, {"referenceID": 63, "context": "In DPLL(T ), also T -lemmas are made of input atoms, and the guarantee that no new atoms are generated is a key ingredient of the proof of termination of the method in [65].", "startOffset": 168, "endOffset": 172}, {"referenceID": 62, "context": ", Tn to get a satisfiability procedure for their union is equality sharing [64], better known as Nelson-Oppen scheme, even if equality sharing was the original name given by Greg Nelson, as reconstructed in [63].", "startOffset": 75, "endOffset": 79}, {"referenceID": 61, "context": ", Tn to get a satisfiability procedure for their union is equality sharing [64], better known as Nelson-Oppen scheme, even if equality sharing was the original name given by Greg Nelson, as reconstructed in [63].", "startOffset": 207, "endOffset": 211}, {"referenceID": 34, "context": "The proof of termination of the MCsat transition rules in [35] requires that the basis be finite.", "startOffset": 58, "endOffset": 62}, {"referenceID": 81, "context": ", [83] for interpolation and locality, [23] for a survey on interpolation of ground proofs, and [24] for an approach to interpolation of non-ground proofs): given two inconsistent formul\u00e6 A and B, a formula that follows from A and is inconsistent with B is an interpolant of A and B, if it is made only of symbols that appear in both A and B.", "startOffset": 2, "endOffset": 6}, {"referenceID": 22, "context": ", [83] for interpolation and locality, [23] for a survey on interpolation of ground proofs, and [24] for an approach to interpolation of non-ground proofs): given two inconsistent formul\u00e6 A and B, a formula that follows from A and is inconsistent with B is an interpolant of A and B, if it is made only of symbols that appear in both A and B.", "startOffset": 39, "endOffset": 43}, {"referenceID": 23, "context": ", [83] for interpolation and locality, [23] for a survey on interpolation of ground proofs, and [24] for an approach to interpolation of non-ground proofs): given two inconsistent formul\u00e6 A and B, a formula that follows from A and is inconsistent with B is an interpolant of A and B, if it is made only of symbols that appear in both A and B.", "startOffset": 96, "endOffset": 100}, {"referenceID": 34, "context": "The theories covered in [35,55] are the quantifier-free fragments of the theories of equality, linear arithmetic, and boolean values, and their combinations.", "startOffset": 24, "endOffset": 31}, {"referenceID": 53, "context": "The theories covered in [35,55] are the quantifier-free fragments of the theories of equality, linear arithmetic, and boolean values, and their combinations.", "startOffset": 24, "endOffset": 31}, {"referenceID": 53, "context": "MCsat is also the name of the implementation of the method as described in [55].", "startOffset": 75, "endOffset": 79}], "year": 2015, "abstractText": "Reasoning semantically in first-order logic is notoriously a challenge. This paper surveys a selection of semantically-guided or modelbased methods that aim at meeting aspects of this challenge. For firstorder logic we touch upon resolution-based methods, tableaux-based methods, DPLL-inspired methods, and we give a preview of a new method called SGGS, for Semantically-Guided Goal-Sensitive reasoning. For firstorder theories we highlight hierarchical and locality-based methods, concluding with the recent Model-Constructing satisfiability calculus.", "creator": "LaTeX with hyperref package"}}}