{"id": "1502.02710", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2015", "title": "Scalable Multilabel Prediction via Randomized Methods", "abstract": "We propose an efficient technique for multilabel classification based on calibration, a term we use to mean learning a link function that maps independent predictions to joint predictions. Though a naive implementation of our proposal would require training individual classifiers for each label, we show that for natural datasets and linear classifiers we can sidestep this by leveraging techniques from randomized linear algebra. Moreover, our algorithm applies equally well to multiclass classification. The end result is an algorithm that scales to very large multilabel and multiclass problems, and offers state of the art accuracy on many datasets.", "histories": [["v1", "Mon, 9 Feb 2015 22:18:26 GMT  (16kb)", "http://arxiv.org/abs/1502.02710v1", null], ["v2", "Mon, 20 Apr 2015 21:08:19 GMT  (44kb)", "http://arxiv.org/abs/1502.02710v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nikos karampatziakis", "paul mineiro"], "accepted": false, "id": "1502.02710"}, "pdf": {"name": "1502.02710.pdf", "metadata": {"source": "CRF", "title": "Multilabel Prediction via Calibration", "authors": ["Nikos Karampatziakis"], "emails": ["nikosk@microsoft.com", "pmineiro@microsoft.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 2.\n02 71\n0v 1\n[ cs\n.L G"}, {"heading": "1 Introduction", "text": "In multilabel classification the learner is given an example and is asked to output a subset of the labels that are relevant for that example. Multilabel problems come up often in web applications where data of interest may be tagged with zero or more tags representing, for example, the objects in an image, the topics of a news story, or the relevant hashtags of a tweet. A simple approach to multilabel prediction is to learn one classifier per label and predict each label independently. Why should we expect that we can improve upon this procedure? One answer is because in many real world multilabel problems the labels are correlated. A blog post about cooking has a higher chance of also being related to nutrition than to finance.\nIn this paper we propose a simple and efficient technique for multilabel classification that we call calibration. Given a random variable y, a prediction p is calibrated with respect to y if E[y|p] = p. For binary classification, several calibration algorithms have been proposed Platt et al. (1999); Zadrozny and Elkan (2001); Kakade et al. (2011) and they all work by learning a link function that transforms the outputs of generic classifiers to calibrated probability estimates. Unlike binary classification where calibration has been well studied Niculescu-Mizil and Caruana (2005),\ncalibration procedures for multilabel and multiclass problems, where the link function must map vectors to vectors, have received little attention. As calibration in the sense of E[y|p] = p is generally computationally hard in high dimensions Hazan and Kakade (2012), for the rest of the paper we use the term calibration to only mean that we are learning a link function that jointly maps vectors to vectors. This simple idea is a surprisingly effective way to deal with multilabel (and multiclass) problems. Going back to the example of a blog post about cooking and nutrition, we want to have a second stage of learning where the prediction of the cooking classifier and the nutrition classifier can mutually reinforce one another.\nConcretely, we will motivate our approach by introducing a flexible link function and a straightforward algorithm for obtaining a multilabel classifier. We then present two crucial ideas: performing principal component analysis (PCA) on the outputs of the classifiers is well suited to multilabel data, and a creative way to perform randomized PCA (RPCA) on those outputs. Using these ideas we arrive at our final algorithm which avoids fitting individual classifiers and scales to very large output spaces, yet it is equivalent to the straightforward algorithm when the outputs of the individual classifiers are replaced by their projections onto their principal components.\nOur techniques naturally extend to multiclass classification which is just a special case of multilabel prediction where exactly one of the labels needs to be output. To illustrate this our experiments include both multilabel and multiclass data. As we expect to improve upon simple approaches for multilabel prediction because labels are correlated, in the multiclass setting we expect our approach to be fruitful when the posterior probabilities of many classes are correlated. This tends to be true for many large scale multiclass problems: in practice it is difficult to be interested in thousands of completely different concepts.\nIn the rest of the paper we overview a randomized algorithm for PCA (Section 2), we introduce our technique as fitting a flexible link function (Section 3), and we argue that statistical (Section 4) and computational (Section 5) improvements are possible if we use RPCA on the the inputs to the link function. We present related work on Section 6 and experiments on Section 7."}, {"heading": "2 Background and Notation", "text": "Here we will introduce notation and review RPCA, a technique we leverage and adapt in the sequel. (Halko et al., 2011) provides a thorough and formal exposition."}, {"heading": "2.1 Notation", "text": "Vectors are denoted by lowercase letters x, y etc. and matrices by uppercase letters W , Z etc. We are given n examples as matrices X \u2208 Rn\u00d7d and Y \u2208 Rn\u00d7c. We assume that the labels Y are sparse: each example is typically associated with only a handful of labels. We use ||X ||F for the Frobenius norm, E[\u00b7] for the expectation operator.\nAlgorithm 1 Randomized PCA 1: Input: X, k, \u2113, q 2: Q \u2190 randn(d, k + \u2113) 3: for i \u2208 {1, . . . , q} do 4: \u03a8 \u2190 X\u22a4XQ 5: Q \u2190 orthogonalize(\u03a8) 6: end for 7: F \u2190 (XQ)\u22a4(XQ) 8: (V\u0302 ,\u03a32) \u2190 eig(F, k) 9: V \u2190 QV\u0302 10: Return: (V,\u03a3)"}, {"heading": "2.2 Randomized PCA", "text": "Algorithm 1 shows a recipe for extracting the top k principal components of a matrix X \u2208 Rn\u00d7d. The algorithm is insensitive to the parameters \u2113 and q as long as they are large enough (in our experiments we use \u2113 = 20 and q = 1). We start with a set of k + \u2113 random vectors and use them to probe the range of X\u22a4X . Since principal eigenvectors can be thought as \u201cfrequent directions\u201d (Liberty, 2013), the range of \u03a8 will tend to align fairly with the space spanned by the top eigenvectors of X\u22a4X . We compute an orthogonal basis for the range of \u03a8 and refine it by repeating q times. This can also be thought as orthogonal (aka subspace) iteration for finding eigenvectors with early stopping (i.e., q is small). Afterwards we use our approximation of the principal subspace of X\u22a4X to optimize fully over that subspace (lines 7 and 8) and back out the solution (line 9). These last few steps are cheap because we are only working with a (k + \u2113) \u00d7 (k + \u2113) matrix and the largest bottleneck is either the computation of \u03a8 or the orthogonalization step. To run the algorithm observe that X or X\u22a4X need not be available explicitly; all we need is to compute the action of X\u22a4X on a given Q."}, {"heading": "3 From Link Functions to Calibration", "text": "We begin with the premise that in many multilabel problems certain label combinations frequently co-occur. We would like to take advantage of such co-occurrence patterns, but we cannot jump directly into modeling the output space because the classifier can only reach that space through the features it uses. Therefore, we will try to model correlations among the predictions. The simplest way to do this is to introduce a link function as an operator that maps a vector of activations p to a vector of predictions y\u0302 = Cp. A very popular operator in the setting of multiclass classification is the softmax: g(p) = \u2207 log \u2211 i exp(pi). This is an oblivious operator whose effect is to boost the largest activation while ensuring the predictions form a probability distribution. In multilabel classification, we would like to introduce a non-oblivious operator, because we do not know a priori which labels frequently occur together.\nA first possibility is a linear operator i.e. C is a c \u00d7 c matrix. A natural choice in\nthis case is\nC = 1\nn\nn\u2211\ni=1\nyiy \u22a4 i\nand with g0(p) = Cp we see that the link function is taking a weighted combination of training labels weighted by the inner product between the activation and each label. Though intuitive, this link function does not do much: the predictions are a fixed linear transformation of the activations with no adjustable parameters. This can be easily fixed by introducing a non-negative scalar \u03b1i for each label:\ng1(p) = \u2211 \u03b1iyiy \u22a4 i p, (1)\nso that labels with a large \u03b1i influence the prediction more towards themselves. This link function is now evidently a linear kernel machine with \u03b1 playing the role of dual variables. Our next step is to generalize the link function by replacing the inner product in input space with a kernel function, an inner product in a reproducing kernel Hilbert space:\ng2(p) = \u2211 \u03b1iyiK(yi, p) (2)\nOur final observation to motivate the connection between link functions and calibration is to consider the effect of approximating the kernel function with a finite sum of basis elements. There are several techniques for doing this, including the Nystro\u0308m method Williams and Seeger (2001), Incomplete Cholesky Factorization Fine and Scheinberg (2002), and Random Fourier Features (RFFs) Rahimi and Recht (2007). Here we will use RFFs which are especially easy to work with shift invariant kernels (i.e. K(y, p) = \u03ba(y\u2212 p)) and have the advantage that they can be generated in an oblivious way, without knowing how the activations p look like. This will prove important for the efficiency of the final algorithm.\nAlthough it is not yet obvious, for our final algorithm we believe (and present some empirical justification) that shift-invariant kernels are broadly applicable in this context, as we are not using kernels in the feature space. Rather, we are applying kernels to the activations (and, later, to projected activations), which are low-dimensional dense vectors for which a smoothness prior is appropriate.\nRecall that for shift invariant kernels\nK(y, p) \u221d E(r,b)\u223cq\u00d7U(0,2\u03c0) cos(r \u22a4y + b) cos(r\u22a4p+ b)\nwhere the distribution q depends on the kernel and U(a, b) is the uniform distribution in (a, b). Therefore we can approximate g2 by\ng3(p) =\nn\u2211\ni=1\n\u03b1iyi 1\ns\ns\u2211\nj=1\ncos(r\u22a4j yi + bj) cos(r \u22a4 j p+ bj)\nWe now replace 1 s \u2211n i=1 \u03b1iyi cos(r \u22a4 j yi+ bj) by optimization variables vj and arrive at\ng(p) def =\ns\u2211\nj=1\nvj cos(r \u22a4 j p+ bj)\nAlgorithm 2 Naive Calibration 1: Input: x, y 2: Fit independent binary classifiers: pij = fj(xi) 3: Draw random vectors rt and biases bt t = 1, . . . , s 4: Featurize activations \u03c6t(xi) = cos(r\u22a4t f(xi) + bt) 5: Learn weight matrix V\nV = argmin V \u2208Rs\u00d7c loss(Y,\u03a6(X)V )\nWe call the procedure of estimating the parameters vj calibration because it learns a link function similar to calibration techniques for binary classification including Platt scaling Platt et al. (1999) and isotonic regression Zadrozny and Elkan (2001). We reiterate that, unlike isotonic regression for binary classification, we cannot guarantee that g(p) is calibrated in the sense of E[y|g(p)] = g(p) because of hardness results Hazan and Kakade (2012) when y is high dimensional. Under some hard to verify conditions, specialized algorithms Agarwal et al. (2013) can yield a truly calibrated g(p) and these algorithms essentially perform many rounds of our procedure. In this work, however, we use a single stage of calibration which substantially simplifies the algorithm of Agarwal et al. (2013) and, as we will show later, already provides empirically superior performance over many strong baselines.\nAt this point we have the two-stage procedure outlined in Algorithm 2. The loss function in the final fit can be either independent logistic regressions or least squares. The former usually requires a smaller s to attain the same result, while the latter admits a fast fitting procedure Vincent (2014) that is independent of the size of the output as long as the output is sparse. As we argue in the next sections, Algorithm 2 is actually quite wasteful both statistically and computationally. In the following sections we will heal these problems using randomized PCA techniques."}, {"heading": "4 Dealing with Statistical Issues", "text": "Though we started with assuming that certain combinations of labels (and activations) are more correlated than others we have not yet clarified what is the exact property we will be exploiting. Correlations between activations are captured in their empirical second moment and our assumption is that this matrix is low rank and therefore can be described by k \u226a c eigenvectors:\nE\u0302[pp\u22a4] def =\n1\nn\nn\u2211\ni=1\npip \u22a4 i =\nk\u2211\ni=1\n\u03bbiuiu \u22a4 i .\nEmpirically we have found this assumption to hold for many multilabel problems. Furthermore, similar ideas, such as assuming that the second moment of the labels is low rank, have been employed in other techniques for multilabel problems Tai and Lin (2012). The key benefit of using activations instead of labels is that our proposed\nmethod applies equally well to multiclass problems whereas the method of Tai and Lin (2012) would yield trivial results in the multiclass setting.\nHow can the procedure outlined in Algorithm 2 be improved by taking advantage of the fact that activations are low rank? When activations are nominally in Rc but really only span a smaller subspace of dimension k then the kernel approximation employed in lines 3 and 4 or Algorithm 2 is very wasteful. To illustrate this point we will, for simplicity, focus on kernels that are not only shift invariant, but also rotationally invariant: K(p, p\u2032) = \u03ba(||p \u2212 p\u2032||2). If p, p\u2032 only span a space of dimension k then there exists a k \u00d7 c matrix U\u22a4 such that ||p\u2212 p\u2032||2 = ||U\u22a4p\u2212 U\u22a4p\u2032||2. Moreover the matrix U is given by the top k eigenvectors of E\u0302[pp\u22a4]. We can therefore reduce the dimensionality of the activations p with PCA before applying our kernel approximation. This reduces the variance, or, alternatively, requires drawing fewer random vectors to achieve the same level of approximation.\nSince each feature function is now computing cos(r\u22a4t U \u22a4p+ b) the random vectors that we use to project the activations are now Urt with rt \u2208 Rk. Furthermore, their covariance is equal to U\u03a3U\u22a4 with \u03a3 \u2208 Rk\u00d7k being the covariance of the sampling distribution for rt (typically a multiple the identity). On the other hand, Bochner\u2019s theorem tells us that there\u2019s a one to one mapping from sampling distributions to positive definite shift invariant kernels. Therefore, projecting the activations onto the principal components is implicitly tuning the kernel to the observed data."}, {"heading": "5 Dealing with Computational Issues", "text": "There are three computational issues with the algorithm as proposed thus far. First we need to fit individual classifiers for each problem which can be very time consuming if the number of labels c is large. Second, when c is large forming the empirical second moment of the activations and computing the top eigenvectors might be infeasible. Third, the final optimization over the matrix V still requires the solution of a large number of problems. Here we address all of them.\nWe start with the issue of fitting the s \u00d7 c matrix V . One possibility is to treat each of the columns of V in parallel as each of them can be learned independently: by this stage we have finished modeling dependencies among labels. For square loss we can alternatively use the recent technique of Vincent (2014) that shows how to perform stochastic gradient updates for least squares problems when the output is large but sparse. This method only requires O(s2) computation instead of O(sc) where s is the number of calibration functions (i.e., cosines) we use.\nWe tackle the other two issues together. Our key observation is that in order to run the algorithm we only need to have the projections of the activations onto their principal components. Surprisingly, for the case of linear classifiers (and kernel machines), it is possible to obtain these without fitting all individual classifiers and forming the second moment of the activations. Our procedure implicitly performs randomized PCA Halko et al. (2011) on the predictions without ever fitting individual classifiers.\nRecall that to perform RPCA on a matrix A, all that is required is to be able to compute A\u2126 for any given \u2126. We focus on executing RPCA on the activations when\nthey are the result of a least squares fit with a linear classifier1. In this case we have that the matrix of activations satisfies:\nP = X(X\u22a4X)\u22121X\u22a4Y\nand the empirical second moment is\n1 n P\u22a4P = 1 n Y \u22a4X(X\u22a4X)\u22121X\u22a4Y.\nTherefore to computeP\u22a4P\u2126 for a given\u2126 we just need to computeZ\u2217 = (X\u22a4X)\u22121X\u22a4Y \u2126 and then form Y \u22a4XZ\u2217. Now, notice that we can express Z\u2217 as\nZ\u2217 = argmin Z ||Y \u2126\u2212XZ||2F .\nThus we only need to solve k + 20 least squares problems to simulate the execution of (one iteration of) RPCA on the activations of c independent classifiers fitted with least squares.2 This can be substantially smaller than c the dimensionality of Y . Moreover, Y is sparse so the cost of forming Y \u2126 does not depend on c either.\nOnce we obtain U , the top k eigenvectors of P\u22a4P from RPCA we then find W \u2217 = argminW ||Y U \u2212XW || 2 F which allows us to compute the projection of the activations on the principal components since"}, {"heading": "XW = X(X\u22a4X)\u22121X\u22a4Y U = PU.", "text": "Algorithm 3 puts the above ingredients together. A few remarks are in order: For simplicity, we have specified k as a parameter but RPCA can also incorporate a strategy for increasing k if initial estimates of the captured variance are too low. To improve generalization performance, we use a regularized least squares fit for the RPCA solves. Regularization is less crucial (and sometimes detrimental) for learning the final V matrix, but this is not surprising given the correspondence between label embeddings and low-rank regularization Mineiro and Karampatziakis (2014).\nAnother (implicit) parameter of the algorithm is the sampling distribution of the vectors rt. This distribution defines the choice of shift invariant kernel we will be using to measure similarities between activations p and labels y. Fortunately, we can offer some guidance here using the spectral properties of various shift-invariant kernels (see also Le et al. (2013) for details). We recommend using Gaussian and Cauchy respectively for low and high dimensional y. These are special cases of the multivariate Student distribution with \u03bd = \u221e and \u03bd = 1 degrees of freedom. Intermediate values such as \u03bd = 3 and \u03bd = 5 can offer better results for medium dimensional y. The corresponding kernels are from the Mate\u0301rn family. Some empirical support for their superiority on medium to high dimensional vectors is offered in Le et al. (2013).\n1These arguments can be extended to the case of kernel machines. 2 In our experiments we run the loop in RPCA for one iteration and in general 1 \u2264 q \u2264 3 is recom-\nmended.\nAlgorithm 3 Calibration via Randomized PCA 1: Input: X,Y, k 2: (\u2113, q) \u2190 (20, 1) 3: Q \u2190 randn(c, k + \u2113) 4: for i \u2208 {1, . . . , q} do 5: Z \u2190 argminZ\u2208Rd\u00d7(k+\u2113) \u2016Y Q\u2212XZ\u2016 2 F\n6: Q \u2190 orthogonalize(Y \u22a4XZ) 7: end for 8: F \u2190 (Y \u22a4XQ)\u22a4(Y \u22a4XQ) 9: (U\u0303 ,\u03a32) \u2190 eig(F, k)\n10: U \u2190 QU\u0303 11: W = argminW\u2208Rd\u00d7k ||Y U \u2212XW || 2 F 12: Draw random vectors rt and biases bt t = 1, . . . , s 13: Featurize projected predictions\n\u03c6t(xi) = cos(r \u22a4 t Wxi + bt)\n14: Learn weight matrix V = argmin\nV \u2208Rs\u00d7c loss(Y,\u03a6(X)V ) (3)"}, {"heading": "6 Related Work", "text": "Many calibration procedures Platt et al. (1999); Zadrozny and Elkan (2001); Kakade et al. (2011) have focused on binary classification and they are now widely used in applications together with diagnostic tools such as calibration plots. Calibration for multiclass and multilabel classification has received little empirical attention. A notable exception for multiclass is Zadrozny and Elkan (2002) which first produces calibrated probability estimates for induced binary problems and then combines these estimates to a final estimate of the posterior probability of each class. However, it is not clear how well calibrated the final multiclass estimates are. In high dimensions, given hardness results Hazan and Kakade (2012) and a lack of diagnostic tools, our approach follows a more pragmatic route: select a family of flexible link functions via a kernel machine parameterization, then learn an efficient approximation to a good link function in that family using random features.\nExploitation of the low-rank structure of predictions via dimensionality reduction is a popular technique in the multilabel literature. Hsu et al. (2009), motivated by advances in compressed sensing, utilized a random embedding of the labels along with sparse decoding strategy. Bengio et al. (2010) combined a tree based decomposition with a low-dimensional label embedding. For the multilabel case, the principal label space transform (PLST) Tai and Lin (2012) constructs a low-dimensional embedding using principal components on the empirical label covariance, which is then utilized along with a greedy sparse decoding strategy.\nThe conditional principal label space transformation (CPLST), another dimension-\nality reduction approaches to multilabel classification, has strong connections to our technique. In particular (Chen and Lin, 2012) initially SVD the same matrix as in algorithm 3, denoted here as Y \u22a4XZ\u2217, albeit without leveraging randomized techniques and without exploiting the specialization of randomized PCA to this context. The similarities are intriguing given that CPLST is motivated by a bound on Hamming loss. However, CPLST solves an optimization problem which is designed to make an independent decode strategy effective, and apply kernelization to the feature space; whereas we learn a decoder and apply kernelization to the decoding problem, i.e., predictions. Our experimental finding of broad applicability of shift-invariant kernels is plausibly attributable to the shared statistical structure of multilabel problems encountered in practice, as opposed to the diverse statistical structure of features, e.g., sparse highcardinality text vs. dense low-cardinality images. In other words, the choice of kernel in our procedure is greatly simplified.\nTree-based approaches are the other major category of multilabel learning algorithms. Due to the richness of the literature, we refer the reader to a survey paper Cerri et al. (2014). Here we discuss FastXML Prabhu and Varma (2014), an multilabel tree ensemble method for which we have direct experimental comparisons. FastXML makes several design choices to mitigate the computational expense of applying decision trees to high label cardinality (aka extreme) multilabel problems. In particular, FastXML partitions the feature space, in contrast to some approaches that partition the label space. Furthermore, FastXML also avoids solving an expensive label assignment problem at each node by using the union of the labels encountered in the training set (ordered by empirical frequency). This yields state of the art performance on multiple datasets when using a precision-at-k metric."}, {"heading": "7 Experiments", "text": "We demonstrate the effectiveness of our approach on a variety of multilabel problems as well as a multiclass dataset with 1000 classes, which shows the versatility of our approach. We use four small but standard benchmark multilabel datasets (bibtex, delicious, mediamill and corel5k), as well as RCV1 industries which has high dimensional inputs. Our multiclass dataset is ALOI Geusebroek et al. (2005). Table 1 lists the details of these datasets."}, {"heading": "7.1 Multilabel benchmarks", "text": "Our first four datasets are standard multilabel benchmarks. We compare our approach with PLST. For the datasets that have also been used in Chen and Lin (2012) we compare with CPLST as well. We set our parameters as follows: we use a k large enough to capture at least 99% of the variance of the predictions. We sample s = 4000 random vectors and biases so as to approximate a Gaussian kernel with bandwidth equal to the median distance between projected activations of sample of 300 examples. We use square loss for the final fit (in our experience logistic loss works at least as well). Table 2 shows the average Hamming loss on the test set for each dataset. The numbers in the first two columns are from Tai and Lin (2012) and Chen and Lin (2012). We also list the training times for these datasets in Table 3 on a standard PC. We see that calibration not only offers a consistent improvement over the baselines, but it is also a computationally attractive procedure."}, {"heading": "7.2 Is it Just About Flexibility?", "text": "Our approach seems to be working well in practice but a reasonable question at this point is where is the better performance stemming from? Since we have a two stage procedure with non-linear features at the second step, could it be the case that we could have just obtained the same results by a much simpler method? Here we illustrate that simple methods such as blindly using a kernel (approximation) directly to the inputs and predicting the labels independently produce very different results than our judicious use of flexibility to model inter-label dependencies.\nWe focus on the RCV1 industries dataset that has a small training set, compared to its fairly larger number of features. Based on this, we should expect that naive application of flexible modeling can lead to decreased generalization performance. Indeed, we performed three experiments: learn individual logistic regressions to predict each\nof the 354 categories, learn individual kernel logistic regressions with a Gaussian kernel approximation (the bandwidth was selected as in the previous experiments), and our calibration procedure with a final least squares fit on Gaussian kernel approximation. The best Hamming loss of 0.00159 was attained by the calibration procedure. Second was the independent logistic regression with a loss of 0.00163. Finally learning independent kernel logistic regressions had a loss of 0.00193. Moreover, our two stage procedure was much faster than learning independent kernel logistic regressions, because it only relied on least squares."}, {"heading": "7.3 Comparison with Tree Approaches", "text": "Here we compare against a state of the art tree based approach, namely FastXML Prabhu and Varma (2014). FastXML uses tree ensembles so the cost of inference can be substantial. Furthermore FastXML is a multilabel ranking algorithm not a multilabel classification algorithm, so the authors only report precision-at-k metrics. In table 4 we list precisionat-1 on the test set for the subset of datasets that were used in this work as well as in Prabhu and Varma (2014). For comparison with previous state of the art we also list another method, the MultiLabel Random Forest Agrawal et al. (2013). On bibtex and delicious, calibration beats MLRF and statistically ties with FastXML. On Mediamill, calibration is not competitive with these methods but it does outperform the other baselines listed in Prabhu and Varma (2014).\nFor these experiments we used logistic loss for the final fit as it optimizes a tighter bound on Hamming loss than squared loss. If precision is the metric of interest it is possible to optimize for precision in a more direct way than using logistic loss. This can be done, for example, by giving positive examples (for each label) a larger importance weight."}, {"heading": "7.4 Multiclass Prediction", "text": "In this section we present results on a multiclass dataset to demonstrate the applicability of algorithm 3 to this setting.\nALOI is a color image collection of one-thousand small objects, recorded for scientific purposes (Geusebroek et al., 2005). For these experiments we will consider test classification accuracy utilizing the same train-test split and features from Choromanska and Langford (2014). Specifically there is a fixed train-test split of 90:10 for all experiments and the representation is linear in 128 raw pixel values. We use logistic loss for equation (3) of algorithm 3, and a dimensionality k of 50.\nThe results, from top to bottom, indicate an improvement from using logistic regression over one-against-all; beneficial regularization effects from composing the dimensionality reduction with logistic regression; and a substantial improvement from modeling the prediction correlations with a flexible kernel.\nUsing a Matlab implementation and a commodity laptop, training time of the Mate\u0301rn result on ALOI is 2600 seconds, chiefly due to the need to do 15 passes over the data set to fully optimize logistic loss with (preconditioned) SGD. Using the same implementation, inference proceeds at 2650 examples per second."}, {"heading": "8 Conclusions", "text": "In this paper we have proposed a procedure for learning a flexible link function for multilabel (as well as multiclass) problems. We call it calibration because it is inspired by calibration procedures for binary classification, though our procedure does not guarantee that the final predictions are indeed calibrated, or even that the learned mapping is a monotone operator. Nevertheless our procedure empirically works better than many strong baselines, it is fast, and scales to very large output spaces. In the future we plan to investigate the applicability and effectiveness of calibration procedures in more complex output spaces where the dependency structure of the output variables is specified by a graphical model."}], "references": [{"title": "Least squares revisited: Scalable approaches for multi-class prediction", "author": ["A. Agarwal", "S.M. Kakade", "N. Karampatziakis", "L. Song", "G. Valiant"], "venue": "arXiv preprint arXiv:1310.1949.", "citeRegEx": "Agarwal et al\\.,? 2013", "shortCiteRegEx": "Agarwal et al\\.", "year": 2013}, {"title": "Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages", "author": ["R. Agrawal", "A. Gupta", "Y. Prabhu", "M. Varma"], "venue": "Proceedings of the 22nd international conference on World Wide Web, pages 13\u201324. International World Wide Web Conferences Steering Committee.", "citeRegEx": "Agrawal et al\\.,? 2013", "shortCiteRegEx": "Agrawal et al\\.", "year": 2013}, {"title": "Label embedding trees for large multiclass tasks", "author": ["S. Bengio", "J. Weston", "D. Grangier"], "venue": "Advances in Neural Information Processing Systems, pages 163\u2013171.", "citeRegEx": "Bengio et al\\.,? 2010", "shortCiteRegEx": "Bengio et al\\.", "year": 2010}, {"title": "An extensive evaluation of decision tree\u2013based hierarchical multilabel classification methods and performance measures", "author": ["R. Cerri", "G.L. Pappa", "A.C.P. Carvalho", "A.A. Freitas"], "venue": "Computational Intelligence.", "citeRegEx": "Cerri et al\\.,? 2014", "shortCiteRegEx": "Cerri et al\\.", "year": 2014}, {"title": "Feature-aware label space dimension reduction for multi-label classification", "author": ["Chen", "Y.-N.", "Lin", "H.-T."], "venue": "Pereira, F., Burges, C., Bottou, L., and Weinberger, K., editors, Advances in Neural Information Processing Systems 25, pages 1529\u20131537. Curran Associates, Inc.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Logarithmic time online multiclass prediction", "author": ["A. Choromanska", "J. Langford"], "venue": "arXiv preprint arXiv:1406.1822.", "citeRegEx": "Choromanska and Langford,? 2014", "shortCiteRegEx": "Choromanska and Langford", "year": 2014}, {"title": "Efficient svm training using low-rank kernel representations", "author": ["S. Fine", "K. Scheinberg"], "venue": "The Journal of Machine Learning Research, 2:243\u2013264.", "citeRegEx": "Fine and Scheinberg,? 2002", "shortCiteRegEx": "Fine and Scheinberg", "year": 2002}, {"title": "The amsterdam library of object images", "author": ["Geusebroek", "J.-M.", "G.J. Burghouts", "A.W. Smeulders"], "venue": "International Journal of Computer Vision, 61(1):103\u2013112.", "citeRegEx": "Geusebroek et al\\.,? 2005", "shortCiteRegEx": "Geusebroek et al\\.", "year": 2005}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "Martinsson", "P.-G.", "J.A. Tropp"], "venue": "SIAM review, 53(2):217\u2013288.", "citeRegEx": "Halko et al\\.,? 2011", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "weak) calibration is computationally hard", "author": ["E. Hazan", "S. Kakade"], "venue": "arXiv preprint arXiv:1202.4478.", "citeRegEx": "Hazan and Kakade,? 2012", "shortCiteRegEx": "Hazan and Kakade", "year": 2012}, {"title": "Multi-label prediction via compressed sensing", "author": ["D. Hsu", "S. Kakade", "J. Langford", "T. Zhang"], "venue": "NIPS, volume 22, pages 772\u2013780.", "citeRegEx": "Hsu et al\\.,? 2009", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Efficient learning of generalized linear and single index models with isotonic regression", "author": ["S.M. Kakade", "V. Kanade", "O. Shamir", "A. Kalai"], "venue": "Advances in Neural Information Processing Systems, pages 927\u2013935.", "citeRegEx": "Kakade et al\\.,? 2011", "shortCiteRegEx": "Kakade et al\\.", "year": 2011}, {"title": "Fastfood\u2013approximating kernel expansions in loglinear time", "author": ["Q. Le", "T. Sarl\u00f3s", "A. Smola"], "venue": "Proceedings of the international conference on machine learning.", "citeRegEx": "Le et al\\.,? 2013", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Simple and deterministic matrix sketching", "author": ["E. Liberty"], "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 581\u2013588. ACM.", "citeRegEx": "Liberty,? 2013", "shortCiteRegEx": "Liberty", "year": 2013}, {"title": "Fast label embeddings for extremely large output spaces", "author": ["P. Mineiro", "N. Karampatziakis"], "venue": "CoRR, abs/1412.6547.", "citeRegEx": "Mineiro and Karampatziakis,? 2014", "shortCiteRegEx": "Mineiro and Karampatziakis", "year": 2014}, {"title": "Predicting good probabilities with supervised learning", "author": ["A. Niculescu-Mizil", "R. Caruana"], "venue": "Proceedings of the 22nd international conference on Machine learning, pages 625\u2013632. ACM.", "citeRegEx": "Niculescu.Mizil and Caruana,? 2005", "shortCiteRegEx": "Niculescu.Mizil and Caruana", "year": 2005}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["J Platt"], "venue": "Advances in large margin classifiers, 10(3):61\u201374.", "citeRegEx": "Platt,? 1999", "shortCiteRegEx": "Platt", "year": 1999}, {"title": "Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning", "author": ["Y. Prabhu", "M. Varma"], "venue": "Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining.", "citeRegEx": "Prabhu and Varma,? 2014", "shortCiteRegEx": "Prabhu and Varma", "year": 2014}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "Advances in neural information processing systems, pages 1177\u20131184.", "citeRegEx": "Rahimi and Recht,? 2007", "shortCiteRegEx": "Rahimi and Recht", "year": 2007}, {"title": "Multilabel classification with principal label space transformation", "author": ["F. Tai", "Lin", "H.-T."], "venue": "Neural Computation, 24(9):2508\u20132542.", "citeRegEx": "Tai et al\\.,? 2012", "shortCiteRegEx": "Tai et al\\.", "year": 2012}, {"title": "Efficient exact gradient update for training deep networks with very large sparse targets", "author": ["P. Vincent"], "venue": "CoRR, abs/1412.7091.", "citeRegEx": "Vincent,? 2014", "shortCiteRegEx": "Vincent", "year": 2014}, {"title": "Using the nystr\u00f6m method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "Proceedings of the 14th Annual Conference on Neural Information Processing Systems, number EPFL-CONF-161322, pages 682\u2013688.", "citeRegEx": "Williams and Seeger,? 2001", "shortCiteRegEx": "Williams and Seeger", "year": 2001}, {"title": "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers", "author": ["B. Zadrozny", "C. Elkan"], "venue": "ICML, volume 1, pages 609\u2013616. Citeseer.", "citeRegEx": "Zadrozny and Elkan,? 2001", "shortCiteRegEx": "Zadrozny and Elkan", "year": 2001}, {"title": "Transforming classifier scores into accurate multiclass probability estimates", "author": ["B. Zadrozny", "C. Elkan"], "venue": "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 694\u2013699. ACM.", "citeRegEx": "Zadrozny and Elkan,? 2002", "shortCiteRegEx": "Zadrozny and Elkan", "year": 2002}], "referenceMentions": [{"referenceID": 14, "context": "For binary classification, several calibration algorithms have been proposed Platt et al. (1999); Zadrozny and Elkan (2001); Kakade et al.", "startOffset": 77, "endOffset": 97}, {"referenceID": 14, "context": "For binary classification, several calibration algorithms have been proposed Platt et al. (1999); Zadrozny and Elkan (2001); Kakade et al.", "startOffset": 77, "endOffset": 124}, {"referenceID": 11, "context": "(1999); Zadrozny and Elkan (2001); Kakade et al. (2011) and they all work by learning a link function that transforms the outputs of generic classifiers to calibrated probability estimates.", "startOffset": 35, "endOffset": 56}, {"referenceID": 11, "context": "(1999); Zadrozny and Elkan (2001); Kakade et al. (2011) and they all work by learning a link function that transforms the outputs of generic classifiers to calibrated probability estimates. Unlike binary classification where calibration has been well studied Niculescu-Mizil and Caruana (2005),", "startOffset": 35, "endOffset": 294}, {"referenceID": 9, "context": "As calibration in the sense of E[y|p] = p is generally computationally hard in high dimensions Hazan and Kakade (2012), for the rest of the paper we use the term calibration to only mean that we are learning a link function that jointly maps vectors to vectors.", "startOffset": 95, "endOffset": 119}, {"referenceID": 8, "context": "(Halko et al., 2011) provides a thorough and formal exposition.", "startOffset": 0, "endOffset": 20}, {"referenceID": 13, "context": "Since principal eigenvectors can be thought as \u201cfrequent directions\u201d (Liberty, 2013), the range of \u03a8 will tend to align fairly with the space spanned by the top eigenvectors of XX .", "startOffset": 69, "endOffset": 84}, {"referenceID": 19, "context": "There are several techniques for doing this, including the Nystr\u00f6m method Williams and Seeger (2001), Incomplete Cholesky Factorization Fine and Scheinberg (2002), and Random Fourier Features (RFFs) Rahimi and Recht (2007).", "startOffset": 74, "endOffset": 101}, {"referenceID": 6, "context": "There are several techniques for doing this, including the Nystr\u00f6m method Williams and Seeger (2001), Incomplete Cholesky Factorization Fine and Scheinberg (2002), and Random Fourier Features (RFFs) Rahimi and Recht (2007).", "startOffset": 136, "endOffset": 163}, {"referenceID": 6, "context": "There are several techniques for doing this, including the Nystr\u00f6m method Williams and Seeger (2001), Incomplete Cholesky Factorization Fine and Scheinberg (2002), and Random Fourier Features (RFFs) Rahimi and Recht (2007). Here we will use RFFs which are especially easy to work with shift invariant kernels (i.", "startOffset": 136, "endOffset": 223}, {"referenceID": 14, "context": "We call the procedure of estimating the parameters vj calibration because it learns a link function similar to calibration techniques for binary classification including Platt scaling Platt et al. (1999) and isotonic regression Zadrozny and Elkan (2001).", "startOffset": 170, "endOffset": 204}, {"referenceID": 14, "context": "We call the procedure of estimating the parameters vj calibration because it learns a link function similar to calibration techniques for binary classification including Platt scaling Platt et al. (1999) and isotonic regression Zadrozny and Elkan (2001). We reiterate that, unlike isotonic regression for binary classification, we cannot guarantee that g(p) is calibrated in the sense of E[y|g(p)] = g(p) because of hardness results Hazan and Kakade (2012) when y is high dimensional.", "startOffset": 170, "endOffset": 254}, {"referenceID": 8, "context": "We reiterate that, unlike isotonic regression for binary classification, we cannot guarantee that g(p) is calibrated in the sense of E[y|g(p)] = g(p) because of hardness results Hazan and Kakade (2012) when y is high dimensional.", "startOffset": 178, "endOffset": 202}, {"referenceID": 0, "context": "Under some hard to verify conditions, specialized algorithms Agarwal et al. (2013) can yield a truly calibrated g(p) and these algorithms essentially perform many rounds of our procedure.", "startOffset": 61, "endOffset": 83}, {"referenceID": 0, "context": "Under some hard to verify conditions, specialized algorithms Agarwal et al. (2013) can yield a truly calibrated g(p) and these algorithms essentially perform many rounds of our procedure. In this work, however, we use a single stage of calibration which substantially simplifies the algorithm of Agarwal et al. (2013) and, as we will show later, already provides empirically superior performance over many strong baselines.", "startOffset": 61, "endOffset": 318}, {"referenceID": 0, "context": "Under some hard to verify conditions, specialized algorithms Agarwal et al. (2013) can yield a truly calibrated g(p) and these algorithms essentially perform many rounds of our procedure. In this work, however, we use a single stage of calibration which substantially simplifies the algorithm of Agarwal et al. (2013) and, as we will show later, already provides empirically superior performance over many strong baselines. At this point we have the two-stage procedure outlined in Algorithm 2. The loss function in the final fit can be either independent logistic regressions or least squares. The former usually requires a smaller s to attain the same result, while the latter admits a fast fitting procedure Vincent (2014) that is independent of the size of the output as long as the output is sparse.", "startOffset": 61, "endOffset": 726}, {"referenceID": 19, "context": "For square loss we can alternatively use the recent technique of Vincent (2014) that shows how to perform stochastic gradient updates for least squares problems when the output is large but sparse.", "startOffset": 65, "endOffset": 80}, {"referenceID": 8, "context": "Our procedure implicitly performs randomized PCA Halko et al. (2011) on the predictions without ever fitting individual classifiers.", "startOffset": 49, "endOffset": 69}, {"referenceID": 13, "context": "Regularization is less crucial (and sometimes detrimental) for learning the final V matrix, but this is not surprising given the correspondence between label embeddings and low-rank regularization Mineiro and Karampatziakis (2014). Another (implicit) parameter of the algorithm is the sampling distribution of the vectors rt.", "startOffset": 197, "endOffset": 231}, {"referenceID": 12, "context": "Fortunately, we can offer some guidance here using the spectral properties of various shift-invariant kernels (see also Le et al. (2013) for details).", "startOffset": 120, "endOffset": 137}, {"referenceID": 12, "context": "Fortunately, we can offer some guidance here using the spectral properties of various shift-invariant kernels (see also Le et al. (2013) for details). We recommend using Gaussian and Cauchy respectively for low and high dimensional y. These are special cases of the multivariate Student distribution with \u03bd = \u221e and \u03bd = 1 degrees of freedom. Intermediate values such as \u03bd = 3 and \u03bd = 5 can offer better results for medium dimensional y. The corresponding kernels are from the Mat\u00e9rn family. Some empirical support for their superiority on medium to high dimensional vectors is offered in Le et al. (2013).", "startOffset": 120, "endOffset": 604}, {"referenceID": 12, "context": "Many calibration procedures Platt et al. (1999); Zadrozny and Elkan (2001); Kakade et al.", "startOffset": 28, "endOffset": 48}, {"referenceID": 12, "context": "Many calibration procedures Platt et al. (1999); Zadrozny and Elkan (2001); Kakade et al.", "startOffset": 28, "endOffset": 75}, {"referenceID": 8, "context": "(1999); Zadrozny and Elkan (2001); Kakade et al. (2011) have focused on binary classification and they are now widely used in applications together with diagnostic tools such as calibration plots.", "startOffset": 35, "endOffset": 56}, {"referenceID": 8, "context": "(1999); Zadrozny and Elkan (2001); Kakade et al. (2011) have focused on binary classification and they are now widely used in applications together with diagnostic tools such as calibration plots. Calibration for multiclass and multilabel classification has received little empirical attention. A notable exception for multiclass is Zadrozny and Elkan (2002) which first produces calibrated probability estimates for induced binary problems and then combines these estimates to a final estimate of the posterior probability of each class.", "startOffset": 35, "endOffset": 359}, {"referenceID": 8, "context": "In high dimensions, given hardness results Hazan and Kakade (2012) and a lack of diagnostic tools, our approach follows a more pragmatic route: select a family of flexible link functions via a kernel machine parameterization, then learn an efficient approximation to a good link function in that family using random features.", "startOffset": 43, "endOffset": 67}, {"referenceID": 8, "context": "In high dimensions, given hardness results Hazan and Kakade (2012) and a lack of diagnostic tools, our approach follows a more pragmatic route: select a family of flexible link functions via a kernel machine parameterization, then learn an efficient approximation to a good link function in that family using random features. Exploitation of the low-rank structure of predictions via dimensionality reduction is a popular technique in the multilabel literature. Hsu et al. (2009), motivated by advances in compressed sensing, utilized a random embedding of the labels along with sparse decoding strategy.", "startOffset": 43, "endOffset": 480}, {"referenceID": 2, "context": "Bengio et al. (2010) combined a tree based decomposition with a low-dimensional label embedding.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Bengio et al. (2010) combined a tree based decomposition with a low-dimensional label embedding. For the multilabel case, the principal label space transform (PLST) Tai and Lin (2012) constructs a low-dimensional embedding using principal components on the empirical label covariance, which is then utilized along with a greedy sparse decoding strategy.", "startOffset": 0, "endOffset": 184}, {"referenceID": 3, "context": "Due to the richness of the literature, we refer the reader to a survey paper Cerri et al. (2014). Here we discuss FastXML Prabhu and Varma (2014), an multilabel tree ensemble method for which we have direct experimental comparisons.", "startOffset": 77, "endOffset": 97}, {"referenceID": 3, "context": "Due to the richness of the literature, we refer the reader to a survey paper Cerri et al. (2014). Here we discuss FastXML Prabhu and Varma (2014), an multilabel tree ensemble method for which we have direct experimental comparisons.", "startOffset": 77, "endOffset": 146}, {"referenceID": 7, "context": "Our multiclass dataset is ALOI Geusebroek et al. (2005). Table 1 lists the details of these datasets.", "startOffset": 31, "endOffset": 56}, {"referenceID": 16, "context": "Here we compare against a state of the art tree based approach, namely FastXML Prabhu and Varma (2014). FastXML uses tree ensembles so the cost of inference can be substantial.", "startOffset": 79, "endOffset": 103}, {"referenceID": 16, "context": "Here we compare against a state of the art tree based approach, namely FastXML Prabhu and Varma (2014). FastXML uses tree ensembles so the cost of inference can be substantial. Furthermore FastXML is a multilabel ranking algorithm not a multilabel classification algorithm, so the authors only report precision-at-k metrics. In table 4 we list precisionat-1 on the test set for the subset of datasets that were used in this work as well as in Prabhu and Varma (2014). For comparison with previous state of the art we also list another method, the MultiLabel Random Forest Agrawal et al.", "startOffset": 79, "endOffset": 467}, {"referenceID": 1, "context": "For comparison with previous state of the art we also list another method, the MultiLabel Random Forest Agrawal et al. (2013). On bibtex and delicious, calibration beats MLRF and statistically ties with FastXML.", "startOffset": 104, "endOffset": 126}, {"referenceID": 1, "context": "For comparison with previous state of the art we also list another method, the MultiLabel Random Forest Agrawal et al. (2013). On bibtex and delicious, calibration beats MLRF and statistically ties with FastXML. On Mediamill, calibration is not competitive with these methods but it does outperform the other baselines listed in Prabhu and Varma (2014). For these experiments we used logistic loss for the final fit as it optimizes a tighter bound on Hamming loss than squared loss.", "startOffset": 104, "endOffset": 353}, {"referenceID": 7, "context": "ALOI is a color image collection of one-thousand small objects, recorded for scientific purposes (Geusebroek et al., 2005).", "startOffset": 97, "endOffset": 122}, {"referenceID": 5, "context": "For these experiments we will consider test classification accuracy utilizing the same train-test split and features from Choromanska and Langford (2014). Specifically there is a fixed train-test split of 90:10 for all experiments and the representation is linear in 128 raw pixel values.", "startOffset": 122, "endOffset": 154}], "year": 2017, "abstractText": "We propose an efficient technique for multilabel classification based on calibration, a term we use to mean learning a link function that maps independent predictions to joint predictions. Though a naive implementation of our proposal would require training individual classifiers for each label, we show that for natural datasets and linear classifiers we can sidestep this by leveraging techniques from randomized linear algebra. Moreover, our algorithm applies equally well to multiclass classification. The end result is an algorithm that scales to very large multilabel and multiclass problems, and offers state of the art accuracy on many datasets.", "creator": "LaTeX with hyperref package"}}}