{"id": "1502.03322", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2015", "title": "Boost Phrase-level Polarity Labelling with Review-level Sentiment Classification", "abstract": "Sentiment analysis on user reviews helps to keep track of user reactions towards products, and make advices to users about what to buy. State-of-the-art review-level sentiment classification techniques could give pretty good precisions of above 90%. However, current phrase-level sentiment analysis approaches might only give sentiment polarity labelling precisions of around 70%~80%, which is far from satisfaction and restricts its application in many practical tasks. In this paper, we focus on the problem of phrase-level sentiment polarity labelling and attempt to bridge the gap between phrase-level and review-level sentiment analysis. We investigate the inconsistency between the numerical star ratings and the sentiment orientation of textual user reviews. Although they have long been treated as identical, which serves as a basic assumption in previous work, we find that this assumption is not necessarily true. We further propose to leverage the results of review-level sentiment classification to boost the performance of phrase-level polarity labelling using a novel constrained convex optimization framework. Besides, the framework is capable of integrating various kinds of information sources and heuristics, while giving the global optimal solution due to its convexity. Experimental results on both English and Chinese reviews show that our framework achieves high labelling precisions of up to 89%, which is a significant improvement from current approaches.", "histories": [["v1", "Wed, 11 Feb 2015 14:45:41 GMT  (136kb,D)", "http://arxiv.org/abs/1502.03322v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["yongfeng zhang", "min zhang", "yiqun liu", "shaoping ma"], "accepted": false, "id": "1502.03322"}, "pdf": {"name": "1502.03322.pdf", "metadata": {"source": "CRF", "title": "Boost Phrase-level Polarity Labelling with Review-level Sentiment Classification\u2217\u2217\u2217", "authors": ["Yongfeng Zhang", "Min Zhang", "Yiqun Liu", "Shaoping Ma"], "emails": ["yongfeng14@mails.tsinghua.edu.cn", "z-m@tsinghua.edu.cn", "yiqunliu@tsinghua.edu.cn", "msp@tsinghua.edu.cn"], "sections": [{"heading": null, "text": "Boost Phrase-level Polarity Labelling with Review-level Sentiment Classification\u2217\u2217\u2217\nYongfeng Zhang Department of Computer Science Tsinghua University yongfeng14@mails.tsinghua.edu.cn\nMin Zhang Department of Computer Science Tsinghua University z-m@tsinghua.edu.cn\nYiqun Liu Department of Computer Science Tsinghua University yiqunliu@tsinghua.edu.cn\nShaoping Ma Department of Computer Science Tsinghua University msp@tsinghua.edu.cn\nSentiment analysis on user reviews helps to keep track of user reactions towards products, and make advices to users about what to buy. State-of-the-art review-level sentiment classification techniques could give pretty good precisions of above 90%. However, current phrase-level sentiment analysis approaches might only give sentiment polarity labelling precisions of around 70% \u223c 80%, which is far from satisfaction and restricts its application in many practical tasks.\nIn this paper, we focus on the problem of phrase-level sentiment polarity labelling and attempt to bridge the gap between phrase-level and review-level sentiment analysis. We investigate the inconsistency between the numerical star ratings and the sentiment orientation of textual user reviews. Although they have long been treated as identical, which serves as a basic assumption in previous work, we find that this assumption is not necessarily true.\nWe further propose to leverage the results of review-level sentiment classification to boost the performance of phrase-level polarity labelling using a novel constrained convex optimization framework. Besides, the framework is capable of integrating various kinds of information sources and heuristics, while giving the global optimal solution due to its convexity. Experimental results on both English and Chinese reviews show that our framework achieves high labelling precisions of up to 89%, which is a significant improvement from current approaches."}, {"heading": "1. Introduction", "text": "Sentiment analysis techniques could be classified into three levels according to the different granularities on which the analysis is conducted, i.e., document-level, sentencelevel and phrase-level (Liu and Zhang 2012). When analyzing user reviews, documentlevel sentiment analysis is also referred to as review-level sentiment analysis. Figure 1 shows a user review on an Apple iPhone product, which is extracted from Amazon.com.\n\u2217 Part of this work was supported by Natural Science Foundation (60903107, 61073071) and National High Technology Research and Development (863) Program (2011AA01A205) of China, and the third author is sponsored by the National Science Foundation (IIS-0713111). The opinions, findings, suggestions or conclusions expressed in this paper are the authors\u2019, and do not necessarily reflect those of the sponsors.\n\u2217\u2217 This paper is an extended version of the work Do users rate or review? boost phrase-level sentiment labeling with review-level sentiment classification in SIGIR\u201914.\n\u00a9 2006 Association for Computational Linguistics\nar X\niv :1\n50 2.\n03 32\n2v 1\n[ cs\n.C L\n] 1\n1 Fe\nb 20\n15\nReview-level and sentence-level sentiment analysis attempt to determine the overall sentiment orientation of a review or sentence. In phrase-level sentiment analysis, however, we are particularly interested in those phrases that describe some features or aspects of products, which, in Figure 1 for example, are the phrases service and phone quality. The user used excellent to modify the product feature service, and perfect for phone quality. Both pairs express user\u2019s positive sentiment on the corresponding feature. In this work, we use Feature word (F) to represent the words or phrases that describe specific product features, use Opinion word (O) for the words or phrases expressing users\u2019 sentiments towards feature words, and use Sentiment polarity (S) for the sentiment orientation of a Feature-Opinion (FO) pair.\nThe construction of a sentiment lexicon is of key importance in phrase-level sentiment analysis (Taboada et al. 2011; Liu, Hu, and Cheng 2005; Ding, Liu, and Yu 2008; Lu et al. 2011). Each entry in the lexicon is an FO pair together with the corresponding sentiment polarity, represented by (F,O,S) (Popescu and Etzioni 2005; Lu et al. 2011; Taboada et al. 2011). For example, the entries (service, excellent, positive) and (phone quality, perfect, positive) could be extracted in the review of Figure 1. The underlying reason for such approaches is the observation that the sentiment polarity of opinion words could be contextual (Wilson, Wiebe, and Hoffmann 2005; Lu et al. 2011), which means that the same opinion word could lead to different sentiment orientations when used to modify different feature words. For example, the opinion word high has a positive sentiment when modifying the feature word quality, yet has a negative sentiment when accompanied by noise.\nHowever, current phrase-level sentiment lexicon construction approaches may only give sentiment polarity labelling (determining the S for an FO pair) precisions of around 70% \u223c 80% (Liu and Zhang 2012; Wilson, Wiebe, and Hoffmann 2005; Liu, Hu, and Cheng 2005; Ding, Liu, and Yu 2008). Although the literature has shown that the overall sentiment classification precisions of a whole sentence or review could be reasonably high even the sentiment lexicon is not that accurate (Cui, Mittal, and Datar 2006; Turney 2002; Liu, Hu, and Cheng 2005; Ding, Liu, and Yu 2008), we argue that the problem of constructing an accurate sentiment lexicon itself is important, because the use of a sentiment lexicon might not be limited to aggregating the overall sentiment of sentences or reviews. In fact, it can be used in many promising tasks, such as word of mouth tracking of brands, tools for product design and optimization, and feature-level product search or recommendation.\nPrevious work on phrase-level sentiment analysis and lexicon construction (Lu et al. 2011; Lu, Zhai, and Sundaresan 2009; Dave, Lawrence, and Pennock 2003) assumes that the accompanied user rating indicates the overall sentiment orientation of a review text. However, we would like to point out according to our experiments on user rating analysis that, the star ratings might not be a kind of reliable signal in this task, and a\nsubstantial amount of users tend to make similar or even the same ratings continuously, regardless of the review text that they comment on a specific product.\nIn this paper, however, we propose to boost the process of phrase-level sentiment polarity labelling in a reverse way, which is to use review-level sentiment classification results as a heuristic for phrase-level polarity labelling. State-of-the-art review-level sentiment classification techniques, even the unsupervised approaches, can give pretty good precisions of above 90% (Liu and Zhang 2012; Lin, He, and Everson 2010; Zagibalov and Carroll 2008a; Qiu et al. 2009; Zagibalov and Carroll 2008b), which could be reliable to help boost the performance of phrase-level sentiment polarity labelling. Here, we mainly focus our attention on unsupervised review-level sentiment classification techniques because of the fact that they need no manually annotated training data, which makes them domain-independent. Besides, they can achieve comparable or even better performance compared with supervised approaches, especially in some scenarios such as online product reviews (Liu and Zhang 2012; Lin, He, and Everson 2010).\nWe design a two-stage process for phrase-level polarity labelling. In the first stage, the overall sentiment orientations of the product reviews in the corpus are labeled using a review-level sentiment classifier. In the second stage, we extract feature-opinion pairs from the corpus, then use the overall sentiment orientations of reviews as constraints to learn the sentiment polarities of these pairs automatically using a novel optimization framework. Experimental results on both English and Chinese review datasets show that our framework improves the precision of phrase-level sentiment polarity labelling significantly, which means that it might be promising to leverage sentence- or reviewlevel sentiment analysis techniques to boost the performance of phrase-level sentiment analysis tasks. The main contributions of this paper are as follows:\n\u2022 We investigate the phenomenon of inconsistency between the numerical star ratings and the sentiment polarities of textual user reviews though extensive experimental studies. \u2022 We propose to leverage review-level sentiment analysis techniques to boost the performance of phrase-level sentiment polarity labelling in sentiment lexicon construction tasks. \u2022 We formally define the problem of phrase-level sentiment polarity labelling as a constrained convex optimization problem and design iterative optimization algorithms for model learning, where the global optimal solution is guaranteed. \u2022 Through a comprehensive experimental study on both English and Chinese datasets, the effectiveness of the proposed framework is verified.\nThe remainder of this paper will be structured as follows. Section 2 reviews some related work, and Section 3 formally defines the problem that we investigate. In Section 4, we propose our phrase-level sentiment polarity labelling framework, followed by the experimental results in Section 5. We conclude the work in Section 6."}, {"heading": "2. Related work", "text": "With the rapid growth of e-commerce, social networks and online discussion forums, the web has been rich in user-generated free-text data, where users express various attitudes towards products or events, which have been attracting researchers into Sentiment Analysis (Liu and Zhang 2012; Pang and Lee 2008). Sentiment analysis plays an important role in many applications, including opinion retrieval (Orimaye, Alhashmi,\nand Siew 2013), word-of-mouth tracking (Jansen et al. 2009), and opinion oriented document summarization (Liu, Seneff, and Zue 2010; Hu and Liu 2004), etc.\nOne of the core tasks in sentiment analysis is to determine the sentiment orientations that users express in reviews, sentences or on specific product features, corresponding to review(document)-level (Pang, Lee, and Vaithyanathan 2002), sentencelevel (Wiebe, Wilson, and Cardie 2005; Nakagawa, Inui, and Kurohashi 2010) and phrase-level (Wilson, Wiebe, and Hoffmann 2005; Lu et al. 2011; Ding, Liu, and Yu 2008) sentiment analysis.\nReview- and sentence-level sentiment analysis attempt to label a review or sentence as one of some predefined sentiment polarities, which are, typically, positive, negative and sometimes neutral (Liu and Zhang 2012). This task is referred to as Sentiment Classification, which has drawn much attention from the research community, and both supervised (Pang, Lee, and Vaithyanathan 2002; Mullen and Collier 2004; Yessenalina, Yue, and Cardie 2010; Cui, Mittal, and Datar 2006; Bickerstaffe and Zukerman 2010; Maas et al. 2011), unsupervised (Turney 2002; Hu and Liu 2004; Lin, He, and Everson 2010; Zagibalov and Carroll 2008a; Qiu et al. 2009; Zagibalov and Carroll 2008b) or semisupervised (Dasgupta and Ng 2009; Zhou, Chen, and Wang 2010; Li et al. 2011; Goldberg and Zhu 2006) methods have been investigated.\nPhrase-level sentiment analysis aims to analyze the sentiment expressed by users in a finer-grained granularity. It considers the sentiment expressed on specific product features or aspects (Hu and Liu 2004). Perhaps one of the most important tasks in phrase-level sentiment analysis is the construction of Sentiment Lexicon (Taboada et al. 2011; Liu, Hu, and Cheng 2005; Ding, Liu, and Yu 2008; Lu et al. 2011; Zhang et al. 2014b), which is to extract feature-opinion word pairs and their corresponding sentiment polarities from these opinion rich user-generated free-texts. The construction of a high-quality sentiment lexicon would benefit various tasks, for example, personalized recommendation (Zhang et al. 2014a, 2015; Zhang 2015) and automatic review summarization (Hu and Liu 2004; Taboada et al. 2011).\nAlthough some opinion words like \"good\" or \"bad\" usually express consistent sentiments in different cases, many others might have different sentiment polarities when accompanied with different feature words, which means that the sentiment lexicon is contextual (Wilson, Wiebe, and Hoffmann 2005).\nVarious information and heuristics could be used in the process of polarity labelling of the feature-opinion pairs. For example, it is often assumed that the overall sentiment orientation of a review is aggregated from all the feature-opinion pairs in it (Ding, Liu, and Yu 2008; Lu et al. 2011). Besides, some seed opinion words that express \"fixed\" sentiments are usually provided, which are used to propagate the sentiment polarities of the other words (Hu and Liu 2004; Lu et al. 2011). Some work takes advantage of linguistic heuristics (Liu 2010; Hu et al. 2013; Lu et al. 2011). For example, two featureopinion pairs concatenated with the conjunctive \"and\" might have the same sentiment, while they might have opposite sentiments if connected by \"but\". The assumption of linguistic heuristic is further extended by sentential sentiment consistency in (Kanayama and Nasukawa 2006).\nIn this paper, we consider two main disadvantages of previous work. First, seldom of them combine various heuristics in a unified framework (Taboada et al. 2011; Ding, Liu, and Yu 2008; Liu, Hu, and Cheng 2005; Hu and Liu 2004; Wilson, Wiebe, and Hoffmann 2005). Second, they simply use the numerical star rating as the overall sentiment polarity of the review text to supervise the process of phrase-level polarity labelling (Lu et al. 2011; Ding, Liu, and Yu 2008; Lu, Zhai, and Sundaresan 2009; Dave, Lawrence, and Pennock 2003). In this work, we propose to boost phrase-level polarity labelling\nwith review-level sentiment classification, while incorporating many of the commonly used heuristics in a unified framework."}, {"heading": "3. Problem Formalization", "text": "In this section, we formalize the problems to be investigated, as well as the notations to be used in this paper.\nDefinition (Sentiment Vector) Suppose we are considering r sentiment polarity labels S1, S2 \u00b7 \u00b7 \u00b7Sr, for example, positive and negative when r = 2. A sentiment vector x = [x1, x2, ..., xr]T (xi \u2265 0) represents the sentiment orientation of a review, sentence or feature-opinion pair. The i-th element xi in x indicates the extent of sentiment on the i-th polarity label Si. A function s : Rr \u2192 R is defined on sentiment vector x such that s(x) represents the overall sentiment of the review, sentence or feature-opinion pair.\nFor example, if S1 = positive and S2 = negative when r = 2, a sentiment vector x = [1, 0]T of a review is used to indicate that the review has a sentiment orientation on positive and no sentiment orientation on negative. If the function s(x) = x1 \u2212 x2 is defined, then the overall sentiment orientation of the review is 1. Some previous work (Lu et al. 2011; Hu et al. 2013) enforces xi \u2264 1 as an additional constraint, however, this constraint is not necessary in our framework.\nDefinition (Sentiment Matrix) For a set of m reviews, sentences or featureopinion pairs t1, t2 \u00b7 \u00b7 \u00b7 tm, the m\u00d7 r matrix X = [x1x2 \u00b7 \u00b7 \u00b7 xm]T is used to represent the sentiment orientations of them, where xi is the sentiment vector for ti, and s(X) = [s(x1)s(x2) \u00b7 \u00b7 \u00b7 s(xm)]T is their overall sentiment orientation.\nDefinition (Review-Level Sentiment Classification) Given a review corpus T of m user reviews t1, t2 \u00b7 \u00b7 \u00b7 tm, a review-level sentiment classification algorithm C gives C(T ) = Xm\u00d7r = [x1x2 \u00b7 \u00b7 \u00b7 xm]T = [C(t1)C(t2) \u00b7 \u00b7 \u00b7 C(tm)]T , where xi = C(ti) is the sentiment vector for the i-th user review ti given by C.\nNote that, in real applications, the review-level sentiment classification algorithm C could be both supervised or unsupervised. In this work, we consider unsupervised algorithms primarily to avoid the expensive manual labelling process and make our framework domain-independent.\nDefinition (Sentiment Lexicon) A sentiment lexicon constituting n FO pairs FO1FO2 \u00b7 \u00b7 \u00b7FOn is defined as an n\u00d7 r sentiment matrix X = [x1x2 \u00b7 \u00b7 \u00b7 xn]T , where xi is the sentiment vector for the pair FOi.\nAs has stated before, the same opinion word may express different sentiment orientations when accompanied with different feature words, which makes a sentiment lexicon contextual. In this work, we use General Sentiment Lexicon and Contextual Sentiment Lexicon to indicate the two different kinds of sentiment lexicons. In the general sentiment lexicon, the sentiment vector of an FO pair is labelled according to its opinion word directly. For example, the opinion word excellent usually expresses a positive opinion regardless of the feature word, as a result, the sentiment vector for (service,excellent) will be labelled as [1, 0]T directly. If the sentiment orientation of an opinion word is unknown, then the corresponding sentiment vector will be labelled as\n[0, 0]T . This lexicon is of high precision but low coverage. In the contextual sentiment lexicon, however, the sentiment vectors of FO pairs are labelled on considering of both the feature words and opinion words.\nDefinition (Phrase-Level Sentiment Polarity Labelling) In the process of contextual sentiment lexicon construction, once the feature-opinion pairs have been extracted from the review corpus, an important task is to determine the sentiment polarity of each FO pair, which is referred to as phrase-level sentiment polarity labelling. More formally, the task is to determine the sentiment matrix Xn\u00d7r = [x1x2 \u00b7 \u00b7 \u00b7 xn]T , which is further transformed into the overall sentiment orientations s(X) = [s(x1)s(x2) \u00b7 \u00b7 \u00b7 s(xn)]T .\nIn this work, the only input of our framework is a user review corpus T and a general sentiment lexicon X0, and the expected output is a contextual sentiment lexicon X, as well as their overall sentiment orientations s(X). The general sentiment lexicon X0 is achieved using some publicly available opinion word sets1. These word sets contain simple and frequently used opinion words such as excellent, good, bad, etc., and they are commonly viewed as basic background knowledges in natural language processing tasks."}, {"heading": "4. The Framework", "text": "In general, the framework is two-stage. In the first stage, we use an unsupervised review-level sentiment classification algorithm C to get the sentiment matrix X\u0303 = C(T ) for the review corpus T ; in the second stage, we extract feature-opinion pairs from corpus T and leverage the sentiment matrix X\u0303 as well as the general sentiment lexicon X0 in a unified optimization framework to obtain the contextual sentiment lexicon X. After that, a pre-defined function s(X) is used to determine the overall sentiment orientation of each feature-opinion pair. We introduce the details of the framework in the following part of the section."}, {"heading": "4.1 Review-Level Sentiment Classification", "text": "The goal of this step is to present review-level overall sentiment polarities of user reviews by sentiment classification algorithms, with which to supervise the phrase-level sentiment polarity labelling process in the next stage.\nWe choose to use unsupervised review-level sentiment classification algorithms mainly because of three reasons. The first is to avoid the manual labelling process of review-level sentiment polarities, which makes our framework domain adaptable. The second is to keep our framework as general as possible to ensure its independence from specific data requirements. For example, the optimization framework in (Lu et al. 2011) takes advantage of numerical ratings given by users in online shopping or review service websites, to supervise the process of phrase-level sentiment polarity labelling. However, such numerical ratings might not exist in specific environments, for example, in forum discussions, emails and newsgroups (Liu and Zhang 2012). Finally, the relationship between sentiment orientations of reviews and user ratings is still open (Moghaddam and Ester 2013; Liu and Zhang 2012). In fact, we will point out in\n1 We choose the commonly used MPQA sentiment corpus for English reviews and use HowNet for Chinese reviews. They will be formally introduced in the following part.\nthe experiments of user rating analysis that, the numerical ratings do not necessarily indicate the sentiment orientation of textual reviews.\nClassifying the sentiment orientations of user reviews into neutral or no opinion is usually ignored in most work, as has been pointed out in (Liu and Zhang 2012) and (Moghaddam and Ester 2013), for it is extremely hard and might bring about negative effects to positive and negative sentiment classification. As a result, we choose two-class classification frameworks for review-level sentiment classification, namely, a review is classified as either positive or negative. More formally, the dimensionality of a sentiment vector is r = 2, with the dimensionalities S1 = positive and S2 = negative, correspondingly.\nTwo possible sentiment vector candidates are used in this stage. If a review is classified as positive by a sentiment classification algorithm C, then its sentient vector is assigned as x = [1, 0]T . Otherwise, the corresponding sentiment vector is set to be x = [0, 1]T . Based on the classification results, a sentiment matrix X\u0303 is constructed:\nX\u0303 = [x1x2 \u00b7 \u00b7 \u00b7 xm]T (1)\nwhere xi is the sentiment vector of the i-th review in corpus T . X\u0303 will be used as a constraint in the next stage.\nWe use the sentence orientation prediction approach in (Hu and Liu 2004) for English reviews. In this approach, a small amount (around 30) of seed opinion words are manually selected to construct the positive word set and negative word set. For example, the words such as great, fantastic, nice and cool are in the positive word set, and words like bad and dull are in the negative word set. After that, each of the two word sets are expanded by adding the synonyms of their own words and antonyms of the words from the other set, where the synonyms and antonyms are defined in WordNet. At last, the positive and negative word sets are used to aggregate the overall orientations of reviews.\nWe use the the automatic seed word selection scheme in (Zagibalov and Carroll 2008a) for sentiment classification of Chinese reviews. It runs on Chinese characters directly and does not require pre-segmentation. In this scheme, the positive and negative seed words are selected in an automatic framework by taking advantage of negation words, which are further used to aggregate the overall sentiment of a review.\nBoth of the two methods achieve pretty high sentiment classification accuracies of around 90%, especially on some specific domains of product reviews. For example, the precision in digital camera and mobile phone reviews could be up to 92% in English corpora and 93% in Chinese corpora, which are the state-of-the-art performance in sentiment classification of English and Chinese texts."}, {"heading": "4.2 Sentiment Lexicon Construction", "text": "In this stage, we construct the contextual sentiment lexicon. We generate the featureopinion pairs first, and label their polarities in a unified optimization framework."}, {"heading": "4.2.1 Generation of Feature-Opinion Pairs.", "text": "Each of the entries in the contextual sentiment lexicon is a feature-opinion pair. The feature word describes an aspect of a product, and the opinion word expresses the user\u2019s sentiment on the corresponding feature. In this stage, we first extract feature words from reviews, and then extract opinion words to pair with their corresponding\nfeature words.\nFeature words extraction We extract feature word candidates first, and then filter out the wrong words using a PMI-based filtering approach. We use the Stanford Parser (M. Marneffe 2006; Levy and Manning 2003; Chang et al. 2009) to conduct Part-of-Speech tagging, morphological analysis and grammatical analysis for both English and Chinese reviews. A sentence is converted into a dependency tree after parsing, which contains both the part of speech tagging results and grammatical relationships. The following example shows the dependency tree constructed for the review sentence \"Phone quality is perfect and the service is excellent.\"\n(ROOT (S (S (NP (JJ Phone) (NN quality)) (VP (VBZ is) (ADJP (JJ perfect))))\n(CC and) (S (NP (DT the) (NN service)) (VP (VBZ is) (ADJP (JJ excellent)))) (. .)))\nWe extract the Noun Phrases (NP) and retrain those whose frequency is greater than an experimentally set threshold. These phrases are treated as feature word candidates. E.g., the phases phone quality and the service could be extracted.\nThe filtering process is then performed on the candidates in a similar way to that in (Popescu and Etzioni 2005). We first compute the Pointwise Mutual Information (PMI) of these feature word candidates with some predefined discriminator phrases, (e.g. in the domain of cellphone the discriminator phrases are \"of phone\", \"phone has\", \"phone comes with\", etc). The PMI of two phrases p1 and p2 is defined as:\nPMI(p1, p2) = Freq(p1, p2)\nFreq(p1) \u00b7 Freq(p2) (2)\nwhere Freq(p) indicates the total term frequency of a phrase p in all the user reviews, and Freq(p1, p2) is the frequency that p1 and p2 co-occur in a subsentence. We use subsentences instead of sentences when computing PMI because it is often the case that a sentence covers different aspects in several subsentences, as stated in (Lu et al. 2011). A feature word candidate is retained if its average PMI across all discriminator phrases is greater than an experimentally set threshold.\nOpinion words extraction We attempt to extract opinion words from user reviews and assign them to appropriate feature words to construct feature-opinion pairs, which serve as the entries in the sentiment lexicon. For English reviews, we extract the Adjective Phrases (ADJP) that co-occur with a feature word in a subsentence as an opinion word candidate, to pair with the corresponding feature word, which forms a feature-opinion pair candidate. For example, the adjective phrase perfect could be extracted as an opinion word, as it\nco-occurs with the feature word phone quality in a subsentence. The Chinese reviews are processed in the same manner to extract opinion word candidates except that Verb Phrases (VP) are also taken into consideration besides adjective phrases.\nFor each of the feature-opinion pair candidates, we compute their Co-Occure Ratio (COR) in terms of the corresponding feature word. The co-occur ratio of a feature word f and an opinion word o is defined in the following way:\nCOR = Freq(f, o) Freq(f)\n(3)\nThe notations are the same with those in equation (2). Intuitionally, a high COR score means that an opinion word candidate is frequently used to modify the corresponding feature word, which indicates that they might be more likely to form a featureopinion pair.\nWe also use an experimentally set threshold of COR to filter the feature-opinion pair candidates, and the retained pairs constitute the entries in the lexicon. It is possible to employ other techniques to construct the lexicon entries, but we choose a relatively simple approach so as to focus on the next step of sentiment polarity labelling. We adopt the unified framework based on Finite State Matching Machine describe in (Tan et al. 2013) to locate the matched feature-opinion pairs in each review sentence."}, {"heading": "4.2.2 Constraints on Sentiment Polarity Labelling.", "text": "In this step, we attempt to assign a unique sentiment polarity (positive or negative) to each of the feature-opinion pairs in the sentiment lexicon, using a unified convex optimization framework. The framework attempts to learn the optimal assignment of sentiment polarities by searching for the minimum solution to a loss function, where each term of the loss function is capable of representing the intuition of a specific evidence from a specific information source. Besides, the loss function is expected to be convex so that we can design a fast and simple optimization algorithm to find the unique global optimal solution to the problem.\nMore formally, suppose the sentiment matrix of the n feature-opinion pairs extracted is represented by X \u2208 Rn\u00d7r, where r is the number of sentiment polarity labels used, then we attempt to learn an optimal X and calculate the overall sentiment polarity of each pair by the function s(X). The objective function of the learning process consists of the following constrains based on different information sources.\nConstraint on Review-level Sentiment Orientation Although an online user review might be either positive or negative in terms of overall sentiment polarity, it does not necessarily mean that users only discuss about positive or negative features in a single piece of review. A positive opinionated review about a product does not mean that the user has positive opinions on all aspects of the product. Likewise, a negative opinionated review does not mean that the user dislikes everything. As a result, the overall sentiment orientation of a review is the comprehensive effect of all the feature-opinion pairs contained in the review text.\nSuppose we have m user reviews in the review corpus T , then the sentiment matrix X\u0303 for the user reviews given by the review-level sentiment classification algorithm is an m\u00d7 r matrix, where each row of the matrix is a sentiment vector for the corresponding review.\nWe construct a matrix A with the dimension m\u00d7 n to indicate the frequency of each feature-opinion pair occurring in each review. Each row of the matrix represents a review, and each column represents a feature-opinion pair. The element for review i to pair j is defined as:\naij = I neg ij \u00b7 Freq(i, j)\u2211 k Freq(i, k)\n(4)\nwhere Freq(i, j) is the frequency of feature-opinion pair j in review i, and it would be 0 if review i does not contain pair j. Therefore, \u2211 k Freq(i, k) represents the total number of pairs that is contained in review i. The matrix Ineg is an indication matrix that allows us to take the \"negation rules\" into consideration. Inegij = \u22121 if the feature-opinion pair j is modified by a negation word in review i, e.g. \"no\", \"not\", \"hardly\", etc. Otherwise, Inegij = 1.\nAccording to our assumption that the overall sentiment of a text review is the comprehensive effect of all the feature opinion pairs it contains, we use a sentiment prediction function f(A,X) to estimate the sentiment orientations of the reviews, based on the review-pairs relationship matrix A and our contextual sentiment lexicon X. We expect to minimize the difference between our estimations and those given by the review-level sentiment classfication algorithm, which leads to the following objective function:\nR1 = \u2016f(A,X)\u2212 X\u0303\u20162F (5.1)\nIn this work, we choose a simple but frequently used sentiment prediction function, which is to predict the overall sentiment orientation of a review as the weighted average of all the feature-opinion pairs contained in it. This gives us the following objective function:\nR1 = \u2016AX\u2212 X\u0303\u20162F (5.2)\nAs the negation words have been represented by negative weights in A, multiplying A and X naturally incorporates the negation rule into consideration in (5.2).\nConstraint on General Sentiment Lexicon As stated in the definition of sentiment lexicon, some opinion words like excellent, good and bad have \"fixed\" polarities regardless of the feature word companioned. Therefore, we construct the general sentiment lexicon X0 by labelling the polarities of the feature-opinion pairs in X according to publicly available sentiment corpora directly.\nWe first construct a positive opinion word set and a negative opinion word set for English and Chinese reviews, respectively. The word sets for English is constructed from the MPQA opinion corpus2, which contains 2718 positive words and 4902 negative words, and the word sets for Chinese is constructed from HowNet3, with 3730 positive words and 3116 negative words. For the i-th feature-opinion pair (f, o), the\n2 http://mpqa.cs.pitt.edu/corpora/ 3 http://www.keenage.com/\ncorresponding sentiment vector xi in X0 is:\nxi =  [1, 0]T , if o is in positive word set [0, 1]T , if o is in negative word set\n[0, 0]T , otherwise\n(6)\nand finally, X0 = [x1x2 \u00b7 \u00b7 \u00b7 xn]T serves as the general sentiment lexicon. We expect the sentiment polarities of the fixed opinion words learnt in the contextual sentiment lexicon X to be close to those in the general sentiment lexicon X0, which leads to the following objective function:\nR2 = \u2016G(X\u2212 X0)\u20162F (7)\nwhere G is a diagonal matrix that indicates which feature-opinion pairs in X are \"fixed\" by the general sentiment lexicon X0. Namely, Gii = 1 if the i-th feature-opinion pair has fixed sentiment, and Gii = 0 otherwise.\nConstraint on Linguistic Heuristics An important and frequently adopted type of linguistic heuristic is the conjunctives in user reviews (Liu 2010; Hu et al. 2013). It is intuitional that feature-opinion pairs i and j that are frequently concatenated with \"and\" in the corpus might have similar sentiments, while those that are frequently connected by words like \"but\" tend to have opposite sentiments. For example, in the sentence \"the phone quality is perfect and the sound effect is clear\", if \"perfect\" is known to be positive, then it can be inferred that \"clear\" is also positive.\nTo formalize the intuition, we define two n\u00d7 n matrices Wa and Wb for the \"and\" and \"but\" linguistic heuristics, respectively, where W\u2217ij \u2208 [0, 1] indicates our confidence that pair i and j have the same or opposite sentiments. A simple but frequently used choice is to set Waij = W a ji = 1 if pair i andj are concatenated by \"and\" for a minimal number of times in all the subsentences in the corpus, otherwise, we set Waij = W a ji = 0. Similarly, Wbij = W b ji = 1 if pair i and pair j are linked by \"but\" for a minimal number of times in the corpus, and they are set to be 0 otherwise. To incorporate the \"and\" linguistic in our model, we propose to optimize the the following objective function:\nRa3 = 1\n2 n\u2211 i=1 n\u2211 j=1 \u2016Xi\u2217 \u2212 Xj\u2217\u20162FWaij = Tr(XTDaX)\u2212 Tr(XTWaX) (8)\nwhere Tr(\u00b7) is the trace of a matrix, and Xi\u2217 represents the i-th row of X, which is also the sentiment vector for pair i. Da \u2208 Rn\u00d7n is a diagonal matrix where Daii = \u2211n j=1 W a ij . The underlying intuition in (8) is that the sentiment vectors of pairs i and j should be similar to each other if they are frequently linked by \"and\", otherwise, a penalty would be introduced to the loss function.\nThe formalization of \"but\" linguistic is similar, except that we expect the sentiment vector for pair i to be close to the \"opposite\" of the sentiment vector for pair j. More intuitionally, if Xi\u2217 gains a high score in its first dimension, which implies that pair i tends to be positive, then Xj\u2217 should also gain a high score in its second dimension, which drives pair j to be negative, and vise verse. In order to model this intuition, we\nintroduce the following optimization term:\nRb3 = 1\n2 n\u2211 i=1 n\u2211 j=1 \u2016Xi\u2217 \u2212 Xj\u2217E\u20162FWbij = Tr(XTDbX)\u2212 Tr(XTWbXE) (9)\nwhere E = [ 0 11 0 ] is an anti-diagonal matrix that serves as a column permutation function to reverse the columns of X. Similarly, Db is a diagonal matrix where Dbii = \u2211n j=1 W b ij .\nFinally, the objective function regarding both \"and\" and \"but\" linguistic heuristic is:\nR3 = Ra3 +Rb3 = Tr(XTDaX)\u2212 Tr(XTWaX) + Tr(XTDbX)\u2212 Tr(XTWbXE)\n= Tr(XTDX)\u2212 Tr(XTWaX)\u2212 Tr(XTWbXE) (10)\nwhere D = Da + Db.\nConstraint on Sentential Sentiment Consistency The use of linguistic heuristic is extended in (Kanayama and Nasukawa 2006) by introducing sentential sentiment consistency (called coherency in (Kanayama and Nasukawa 2006)). The fundamental assumption of sentential sentiment consistency is that the same opinion orientation (positive or negative) is usually expressed in a few consecutive sentences, which is reported to be helpful in improving the accuracy of contextual sentiment polarity labelling.\nTo formalize the heuristic, a sentential similarity matrix Ws \u2208 Rn\u00d7n is introduced, which leverages the sentential distance between feature-opinion pairs in corpus to estimate their sentential similarities. For example, consider two pairs i and j, if they co-occur in the same piece of review in the corpus, then we calculate their sentential similarity in this review, and the final similarity between i and j is the average of all the intra-review similarities of their co-occurrences. More formally, suppose pair i and pair j co-occur in the same review for Nij times, and the k-th co-occurrence happens in review tik , then W s ij and W s ji are defined as:\nWsij = W s ji =  0, if Nij = 0 or Waij 6= 0 or Wbij 6= 0 1\nNij Nij\u2211 k=1 ( 1\u2212 dist(i, j) length(rik) ) , else\n(11)\nwhere the length of a review length(rik) is the number of words (punctuations excluded) in the review, and the distance of pair i and j in the review dist(i, j) is the number of words between the two feature words of the pair. Note that we do not consider two pairs if they have been constrained by \"and\" (Waij 6= 0) or \"but\" (Wbij 6= 0) linguistic heuristic. Besides, a pair might co-occur for more than one times in the same review, and we consider all the pairwise combinations in such cases.\nOnce the sentential similarity matrix is constructed, we incorporate sentential sentiment similarity constraint by taking into account the following objective function:\nR4 = 1\n2 n\u2211 i=1 n\u2211 j=1 \u2016Xi\u2217 \u2212 Xj\u2217\u20162FWsij = Tr(XTDsX)\u2212 Tr(XTWsX) (12)\nAlgorithm 1 Contextual Sentiment Polarity Labelling\nRequire: A, X\u0303,X0, \u03bb1, \u03bb2, \u03bb3, \u03bb4, N, \u03b4 Ensure: X\n1: Construct matrix G in Equation (7) 2: Construct matrix D,Wa and Wb in Equation (10) 3: Construct matrix Ds and Ws in Equation (12) 4: Initialize X\u2190 X0, X\u2032 \u2190 X0, n\u2190 0 5: repeat 6: n\u2190 n+ 1 7: X\u2032 \u2190 X 8: for each element Xij in X do\n9: Xij \u2190 Xij \u221a\n[\u03bb1AT X\u0303+\u03bb2GX0+\u03bb3WaX+\u03bb3WbXE+\u03bb4WsX]ij [\u03bb1AT AX+\u03bb2GX+\u03bb3DX+\u03bb4DsX]ij\n10: end for 11: until \u2016X\u2212 X\u2032\u20162F < \u03b4 or n > N 12: return X\nwhere Ds is also a diagonal matrix, and Dsii = \u2211n j=1 W s ij .\nThe underlying intuition of this subjective function is that, a large penalty would be introduced if the difference of the sentiment vectors of two near pairs is significant."}, {"heading": "4.2.3 The Unified Model for Polarity Labelling.", "text": "With the above constraints from different information and aspects, we have the following objective function for learning the contextual sentiment lexicon X:\nmin X\u22650 R = \u03bb1\u2016AX\u2212 X\u0303\u20162F + \u03bb2\u2016G(X\u2212 X0)\u20162F\n+ \u03bb3 ( Tr(XTDX)\u2212 Tr(XTWaX)\u2212 Tr(XTWbXE) ) + \u03bb4 ( Tr(XTDsX)\u2212 Tr(XTWsX) ) (13)\nwhere \u03bb1, \u03bb2, \u03bb3 and \u03bb4 are positive weighing parameters that control the contributions of each information source in the learning process.\nAn important property of the objective function (13) is its convexity, which makes it possible to search for the global optimal solution X\u2217 to the contextual sentiment polarity labelling problem. We give the updating rule for learning X\u2217 directly here, as shown in (14), and the proof of the updating rule as well as its convergence is given in the appendix.\nXij \u2190 Xij\n\u221a [\u03bb1AT X\u0303 + \u03bb2GX0 + \u03bb3WaX + \u03bb3WbXE + \u03bb4WsX]ij\n[\u03bb1ATAX + \u03bb2GX + \u03bb3DX + \u03bb4DsX]ij (14)\nThe algorithm for learning the contextual sentiment lexicon is shown in Algorithm 1. In this algorithm, we first initialize the indication matrices, Laplacian matrices and sentiment matrices through line 1 to line 4. The predefined parameter N is the number of maximum iterations to conduct. The contextual sentiment lexicon X is updated repeatedly until convergence or reaching the number of maximum iterations, where\nconvergence means that the `2-norm of the difference matrix between two consecutive iterations is less than a predefined residual error \u03b4."}, {"heading": "4.2.4 Overall Sentiment Polarity of the Pairs.", "text": "After the contextual sentiment lexicon X is constructed, we use the predefined function s(X) = [s(x1)s(x2) \u00b7 \u00b7 \u00b7 s(xn)]T to determine the sentiment polarities of the featureopinion pairs. In this work, we choose the function s(xi) = xi1 \u2212 xi2, where xi1 and xi2 are the values of positive and negative polarity labels in sentiment vector xi, respectively. Pair i is labeled as positive if s(xi) \u2265 0, and negative if s(xi) < 0. For simplicity, we leave out the polarity label of neutral like most of the existing work, as it is quite rare that xi1 = xi2."}, {"heading": "5. Experiments", "text": "In this section, we conduct extensive experiments to evaluate the proposed framework, and investigate the effect of different parameter settings in our framework. We will attempt to answer the following two research questions:\n1. Are the numerical star ratings always consistent with the overall sentiment orientations of textual user reviews? 2. How effective is our proposed framework compared with other polarity labelling methods?\nWe begin by introducing the experimental settings, and then investigate the relationship between numerical ratings and text reviews. Finally, we evaluate the proposed framework, and make some comparisons with other techniques."}, {"heading": "5.1 Experimental Setup", "text": "For the experimentation on English, we use the MP3 player reviews crawled from Amazon, which is publicly available4. For the experiment on Chinese language, we use the restaurant reviews crawled from DianPing.com5, which is a famous restaurant rating website in China, and we also made this dataset publicly available6. Each of the reviews of the two datasets consists of a piece of review text and an overall numerical rating raging from 1 to 5 stars. We choose these two datasets from both English and Chinese as these two languages are of quite different types in terms of Linguistics. We want to examine whether our framework works in different language environments. Some statistical information about these two datasets is shown in Table 1.\nTable 1 Some statistics of the two datasets.\nLanguage #Users #Items #Reviews MP3 Player English 26,113 796 55,740 Restaurant Chinese 11,857 89,462 510,551\n4 http://sifaka.cs.uiuc.edu/~wang296/Data/ 5 http://www.dianping.com/ 6 http://tv.thuir.org/data/\nAn important property of our restaurant review dataset is that, each review is accompanied with three sub-aspect ratings except for the overall rating. They are users\u2019 ratings made on the flavour, environment and service of restaurants, respectively. A user is required to make ratings on all these three aspects as well as the overall experience when writing reviews on the website, which makes it possible for us to conduct much detailed user rating analysis on this dataset. The range of the sub-aspect ratings are also from 1 to 5."}, {"heading": "5.2 User Rating Analysis", "text": "Previous work (Cui, Mittal, and Datar 2006; Lu et al. 2011; Wang, Lu, and Zhai 2010) labels a review text as positive if the corresponding overall rating is 4 or 5 stars, and negative if the overall rating is 1, 2 or 3 stars. However, the overall rating might not always be consistent with the sentient orientation of review texts. According to our observation, a substantial amount of users tend to make unaltered overall ratings although the sentiment orientation expressed in his or her review text might be quite different. Most interestingly, many users simply make 4 star ratings regardless of the review text he wrote. We analyze this phenomenon in the following part of this section."}, {"heading": "5.2.1 Overall and Sub-Aspect Ratings.", "text": "We begin by analyzing the difference in the rating distributions of the restaurant dataset. The ratings on three sub-aspects allow us to investigate a user\u2019s \"true\" feelings on more specific aspects of the restaurant beyond the overall rating. For the overall rating and each of the sub-aspect ratings, we calculate the percentages that each of the 5 star ratings take in the total number of ratings, as shown in Figure 2. The x-axis represents 1 star through 5 stars, and the y-axis is the percentage of each kind of star rating.\nWe see that user ratings tend to center around 4 stars on overall rating, while they tend to center around 2\u223c3 stars on the sub-aspect ratings. This implies that the overall rating might not serve as a real reflection of users\u2019 feelings, and users tend to \"tell the truth\" in much detailed sub-aspects. In order to examine the statistical significance, we calculate the average rating \u00b5 and coefficient of variation cv = \u03c3/\u00b5 for the overall rating and three kinds of sub-aspect ratings, where \u03c3 is the standard deviation. Table 2 shows\nthe results. We see that users tend to give higher scores on overall rating, and the scores on overall rating are more concentrated.\nMore intuitionally, we conduct per user analysis. For each user and each kind of rating (overall, flavour, environment and service), we calculate the percentage of 4+ stars (4 and 5 stars) that the user made. Then we sort these percentages of the users in descending order, which is shown in Figure 3.\nIt is clear that user rating behaviours on overall and sub-aspect ratings are different. More than a half of the users made 50% or more 4+ ratings in terms of overall rating, while less than 5% users did so on sub-aspect ratings.\nThis analysis partly shows that it might not be appropriate to use overall ratings as groundtruth to label the sentiment orientations of review texts, as users tend to act differently when making overall ratings and expressing their true feelings on detailed product aspects or features."}, {"heading": "5.2.2 Labelling Accuracy using Different Methods.", "text": "In fact, users might consider many other features except flavour, environment and service when giving the overall ratings, as a result, one may argue that the difference in distribution of ratings on different aspects is insufficient to imply that the overall rating is inappropriate in estimating the overall sentiment orientations of review texts. As a result, we evaluate the effect of sentiment orientation labelling of review texts using different kinds of numerical ratings.\nWe randomly sampled 1000 reviews out of the 510,551 reviews from the restaurant review dataset to be labeled manually by 3 annotators. An annotator is asked to give a single label to a review text, which could be positive or negative. The final sentiment of a review is assigned by using the majority label (more than two) of the three annotators. The inter-annotator agreement is 79.76%, which is comparable to the reports of existing\nwork in sentiment analysis (Pang and Lee 2008; Lu et al. 2011). Finally, the annotated reviews consist of 673 (67.3%) positive reviews and 327 (32.7%) negative reviews. We use four methods to estimate the overall sentiment orientation of the review texts automatically:\n1. Overall Rating: A review text is labeled as positive if the overall star rating is \u2265 4, and negative otherwise. The criterion of 4 stars is chosen to keep in accordance with previous work (Lu et al. 2011; Lu, Zhai, and Sundaresan 2009) for easier comparisons. 2. Normalized Overall Rating: We use r\u2032 = r \u2212 \u00b5i for labelling, where r is the overall rating, and \u00b5i is the average rating of the corresponding user i. A review is labeled as positive if r\u2032 \u2265 0 and negative if r\u2032 < 0. 3. Sub-Aspect Rating: We use r\u0304 = (rf + re + rs)/3 for labelling, where rf , re and rs are the ratings on flavour, environment and service, respectively. The criterion of 4 stars is also used here. 4. Sentiment Classification: We use the unsupervised review-level sentiment classification method described in section 4.1 for orientation labelling.\nWe use precision to evaluate the performance of each method, and the golden standard is our human annotations. The results are shown in Table 3, where \"Pos.Review\" and \"Neg.Review\" represent the precisions of labelling positive and negative reviews, respectively, and \"Overall\" is the overall performance of review-level orientation labelling.\nWe see that the orientation labelling performance by using sub-aspect ratings (3- Subaspect) overcomes the performance of using overall ratings directly (1-Overall). However, both of their performances are worse than that given by the sentiment classification method (4-Classify).\nIntuitionally, we expect to get better performance by conducting user-based normalization, as different users might have different rating scales. However, experimental results show that user-based normalization (2-Normalize) gives the worst performance. This is also because of the fact that many users make relatively high overall ratings consistently, regardless of the ratings they made on the sub-aspects. Suppose that a user made two ratings, of which one is 4 stars and the other is 5 stars. The ratings would be normalized to be -0.5 and 0.5, which leads to a negative label and a positive label. However, the 4-star review text and 5-star review text might both be positive in the golden standard.\nIn the following part of the experiments, we use the classification method primarily to label the review-level sentiment orientations, whose result is further used to supervise the process of contextual sentiment lexicon construction. Besides, we use the\nresults given by overall rating labelling and sub-aspect rating labelling for performance comparison."}, {"heading": "5.3 Feature-Opinion Pair Extraction", "text": "There are three experimentally set parameters in the process of feature-opinion pair extraction, which are 1) the minimum term frequency (denoted by freq) of NounPhrases to be selected as candidate feature words, 2) the minimum Pointwise Mutual Information (denoted by pmi) of a feature word candidate to be retained during the filtering process, and 3) the minimum Co-Occure Ratio (denoted by cor) of a pair to be selected as a final feature-opinion pair.\nPool Lexicon: In principle, we used relatively strict parameters in order to get high quality feature-opinion pairs, which allows us to focus on the core task of phraselevel polarity labelling in this work. After careful parameter selection, we set freq = 10, pmi = 0.005, cor = 0.05 on the mp3 player dataset, which leaves us with 1063 pairs, and freq = 20, pmi = 0.01, cor = 0.05 on the restaurant review dataset, which gives us 1329 pairs. These pairs are presented to the three annotators for polarity labelling (positive or negative), and the final polarity of a pair is assigned according to the majority of the labels. The average agreement among annotators in this task is 81.84%. The pool lexicon is used for evaluating the precision of polarity labelling.\nGolden Standard Lexicon: We then present the feature-opinion pair lists to human annotators to construct the golden standard lexicon. In this stage, each annotator is asked to select the feature-opinion pairs that describe an explicit aspect of an mp3 player or a restaurant. A pair is retained if it is selected by at least two annotators among the three, and the average agreement among annotators is 78.69%. The purpose of this stage is to further filter out the low quality pairs in the pool lexicon. For example, the pair (service is, good) could be discarded as the right feature word should be service, rather than service is, although this pair does express a positive sentiment. The final golden standard lexicons for mp3 player dataset and restaurant review dataset consist of 695 and 857 feature-opinion pairs, respectively, and it is used for the evaluation of recall.\nWe use different lexicons to evaluation precision and recall to avoid the problem of evaluation bias pointed out in (Lu et al. 2011)."}, {"heading": "5.4 Phrase-Level Polarity Labelling", "text": "In this section, we conduct automatic sentiment polarity labelling on the feature-opinion pairs in the pool lexicon, and report the evaluation results of our method and the methods for comparison."}, {"heading": "5.4.1 Evaluation Measures.", "text": "We choose the frequently used measures precision, recall and F-measure to evaluate the performance of polarity labelling, which are defined as follows:\nprecision = Np_agree Nlexicon , recall = Ng_agree Ngold , F-measure = 2\u00d7 precision\u00d7 recall precision+ recall\nwhere Nlexicon is the number of feature-opinion pairs in the automatically constructed sentiment lexicon, and Ngold is the number of pairs in the golden standard lexicon (695 on the mp3 player dataset and 857 on the restaurant review dataset). Np_agree and\nNg_agree are the number of pairs consistently labeled with the pool lexicon and golden standard lexicon, respectively."}, {"heading": "5.4.2 Polarity Labelling Results.", "text": "We adopted the following methods for phrase-level sentiment polarity labelling in the experiment:\n\u2022 General: Make predictions by querying the polarity of the opinion word in general sentiment opinion word sets. Also, we use MPQA for English and HowNet for Chinese, as in section 4.2.2. A pair is discarded if the polarity of its opinion word could not be determined. \u2022 Optimize: The optimization approach proposed in (Lu et al. 2011), which reduces the problem of sentiment polarity labelling to constrained linear programming. \u2022 Overall: Use our framework except that the review-level sentiment orientation is determined using the corresponding overall rating. \u2022 Subaspect: Use our framework except that sentiment orientations of reviews are determined by averaging the corresponding sub-aspect ratings. \u2022 Boost: Use our complete framework, where the sentiment classification on review text is conducted to boost phrase-level sentiment polarity labelling.\nWe set \u03b4 = 0.01 and N = 100 in algorithm 1 to ensure convergence, and use \u03bb1 = \u03bb2 = \u03bb3 = \u03bb4 = 1 in this set of experiment. Results on the two dataset using the above five methods are shown in Table 4, in which the bolded numbers are the best performance on the corresponding measure. We did not perform the \"Subaspect\" method on mp3 player reviews as the sub-aspect ratings are absent on this dataset.\nWe see that labelling the polarities by querying the general opinion word sets gives the best precision on both of the two datasets. However, the recall of this method is rather low. This implies that there are many \"context dependent\" opinion words which are absent from these word sets.\nThe \"Optimize\" method in (Lu et al. 2011) and our \"Overall\" method are similar in that both of them leverage overall numerical ratings as the groundtruth of reviewlevel sentiment orientations, and they make use of similar heuristics and constraints. Though the Optimize method achieves slightly better recall, their overall performance are comparable. Further more, by taking advantage of the sub-aspect ratings in the \"Subaspect\" method, both precision and recall are improved from \"Optimize\" and \"Overall\" methods, which implies that the detailed sub-aspect ratings could be more reliable than overall ratings.\nFinally, our \"Boost\" method achieves the best performance in terms of recall and Fmeasure, on both of the two datasets. Besides, it also achieves the best precision without regard to the \"General\" method. This further verifies the effect of leveraging reviewlevel sentiment classification in boosting the process of phrase-level polarity labelling."}, {"heading": "5.5 Parameter Analysis", "text": "In the previous sections, we set equal weights to the different kinds of constraints for general experimental purpose. In this subsection, we attempt to study the effect of different constraints in our framework by analyzing the four main parameters \u03bb1 \u223c \u03bb4 in objective function (13).\nWe first conduct \"Knock Out One Term\" experiment on these parameters, to see whether all these constraints contribute to the performance of phrase-level polarity labelling. We set one of the four parameters to 0 at a time, and evaluate the F-measure. The results are shown in Table 5.\nThe experimental result shows that knocking out any of the four parameters decreases the performance of polarity labelling. Besides, removing the constraint on review-Level sentiment orientation (\u03bb1) or the constraint on general sentiment lexicon (\u03bb2) decreases the performance to a great extent, which implies that these two information sources are of great importance in constructing the sentiment lexicon.\nWe further investigate the effect of different constraints by fixing three parameters to 1 and weighing the remaining parameter. The experimental results on restaurant dataset are shown in Figure 4, and the observations on mp3 player dataset are similar.\nThe experimental result shows that giving more weights to the constraints of review-level sentiment orientation and general sentiment lexicon could further improve the performance, which means that these two information sources might be more reliable. However, weighting the constraint on sentential sentiment consistency too much would decrease the performance, this implies that noise could be introduced by this heuristic and it is not as reliable as the linguistic heuristic of \"and\" and \"but\".\nWe tuned the parameters carefully to get the optimal performance. Finally, the optimal result on mp3 player dataset was achieved when using the parameters (4, 2, 1, 0.25), with an F-measure of 0.8237, and on restaurant review dataset (3, 2, 2, 0.5) is used, which gives the F-measure of 0.8584."}, {"heading": "6. Conclusions", "text": "Treating the numerical star rating as a sentiment indication of review text is a widely used assumption in previous phrase-level sentiment analysis algorithms (Lu et al. 2011; Lu, Zhai, and Sundaresan 2009; Dave, Lawrence, and Pennock 2003). In this paper, however, we investigated the inconsistency between the numerical ratings and textual sentiment orientations. Our observations on user rating analysis show that, users tend to make biased ratings regardless of the textual reviews they comment on a specific product. Besides, the evaluation results on labelling accuracy using different methods further verify the existence of such a bias, and indicate the effectiveness of leveraging review-level sentiment classification to recover the sentiment orientation of the reviews.\nThe biased assumption may hurt the performance of phrase-level sentiment polarity labelling to a large extent, because the numerical rating is usually incorporated as a kind of groundtruth to supervise the model learning process in previous work. In this paper, however, we attempt to bridge the gap between review-level and phraselevel sentiment analysis by leveraging review-level sentiment classification to boost the performance of phase-level sentiment polarity labelling.\nSpecifically, we formalized the phrase-level sentiment polarity labelling problem into a simple convex optimization framework, and incorporated four kinds of heuristics to supervise the polarity labelling process. We further designed iterative optimization algorithms for model learning, where the global optimal solution is guaranteed due to the convexity of the objective function. More over, except for the four kinds of heuristics investigated in this paper, the framework is also flexible to integrate various other information sources.\nWe conducted extensive experiments on two different language environments (English and Chinese) to investigate the performance of our framework, as well as its transportability across different language settings. The experimental results on both datasets show that our framework helps to improve the performance in contextual sentiment lexicon construction tasks. Besides, the experiment on parameter analysis shows that all of the four heuristics that we considered in this study contribute to the\nimprovement in the performance of polarity labelling, which is also in accordance with previous studies.\nIn the future we would like to further investigate the effect of incorporating other heuristics into the framework for polarity labelling. Besides, it would be interesting to further bridge the gap between review- and phrase-level sentiment analysis by integrating the two stages into a single unified framework through, for example, deep learning techniques. Except for the sentiment polarity labelling task investigated in this work, review-level analysis could also be promising to help extract feature or opinion words in phrase-level sentiment analysis, and the joint consideration of review- and phrase-level analysis may even lead to brand new sentiment analysis tasks."}, {"heading": "Acknowledgement", "text": "The authors thank Prof. Xiaoyan Zhu, Yang Liu and Maosong Sun for the fruitful discussions, as well as the anonymous reviewers for the constructive suggestions. The authors also sincerely thank the annotators Yunzhi Tan, Cheng Luo and Boya Wu for their careful annotations.\nAppendix\nIn the objective function (13), let L = \u03bb3D + \u03bb4Ds \u2212 \u03bb3Wa \u2212 \u03bb4Ws, let \u039b be the Lagrange multiplier for the constraint X \u2265 0, and let L(X) be the Lagrange function, then we have:\n\u2207XL(X) = 2\u03bb1ATAX\u2212 2\u03bb1AT X\u0303 + 2\u03bb2G(X\u2212 X0) + 2LX\u2212 2\u03bb3WbXE\u2212\u039b (15)\nBy setting\u2207XL(X) = 0, we have:\n\u039b = 2\u03bb1ATAX\u2212 2\u03bb1AT X\u0303 + 2\u03bb2G(X\u2212 X0) + 2LX\u2212 2\u03bb3WbXE (16)\nAccording to the Karush-Kuhn-Tucker (KKT) complementary condition (Boyd and Vandenberghe 2004) on the non-negativity constraint on X, we have \u039bij \u00b7 Xij = 0, namely:\n[\u03bb1ATAX\u2212 \u03bb1AT X\u0303 + \u03bb2G(X\u2212 X0) + LX\u2212 \u03bb3WbXE]ij \u00b7 Xij = 0 (17)\nEquation (17) can be further transformed into:\n[\u2212(\u03bb1AT X\u0303 + \u03bb2GX0 + \u03bb3WaX + \u03bb3WbXE + \u03bb4WsX)\n+ (\u03bb1ATAX + \u03bb2GX + \u03bb3DX + \u03bb4DsX)]ij \u00b7 Xij = 0 (18)\nwhich leads to the updating rule of X as follows:\nXij \u2190 Xij\n\u221a [\u03bb1AT X\u0303 + \u03bb2GX0 + \u03bb3WaX + \u03bb3WbXE + \u03bb4WsX]ij\n[\u03bb1ATAX + \u03bb2GX + \u03bb3DX + \u03bb4DsX]ij (19)\nThe correctness and convergence of the updating rule can be proved using the standard auxiliary function approach presented in (Lee and Seung 2001)."}], "references": [{"title": "A Hierarchical Classifier Applied to Multi-way Sentiment Detection", "author": ["Bickerstaffe", "Zukerman2010]Bickerstaffe", "Adrian", "Ingrid Zukerman"], "venue": "Proceedings of the 21st International Conference on Computational Linguistics (Coling),", "citeRegEx": "Bickerstaffe et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bickerstaffe et al\\.", "year": 2010}, {"title": "Discriminative Reordering with Chinese Grammatical Relations Features", "author": ["Chang et al.2009]Chang", "Pichuan", "Huihsin Tseng", "Dan Jurafsky", "Christopher D. Manning"], "venue": "Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation (SSST),", "citeRegEx": "al.2009.Chang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al.2009.Chang et al\\.", "year": 2009}, {"title": "Comparative Experiments on Sentiment Classification for Online Product Reviews", "author": ["Cui", "Mittal", "Datar2006]Cui", "Hang", "Vibhu Mittal", "Mayur Datar"], "venue": "Proceedings of the 21st national conference on Artificial intelligence (AAAI),", "citeRegEx": "Cui et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2006}, {"title": "Mine the Easy, Classify the Hard: A Semi-Supervised Approach to Automatic Sentiment Classification", "author": ["Dasgupta", "Ng2009]Dasgupta", "Sajib", "Vincent Ng"], "venue": "Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Dasgupta et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2009}, {"title": "Mining the Peanut Gallery: Opinion Extraction and Semantic Classification of Product Reviews", "author": ["Dave", "Lawrence", "Pennock2003]Dave", "Kushal", "Steve Lawrence", "David M. Pennock"], "venue": null, "citeRegEx": "Dave et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Dave et al\\.", "year": 2003}, {"title": "A Holistic Lexicon-Based Approach to Opinion Mining", "author": ["Ding", "Liu", "Yu2008]Ding", "Xiaowen", "Bing Liu", "Philip S. Yu"], "venue": "Proceedings of the 2008 International Conference on Web Search and Data Mining (WSDM),", "citeRegEx": "Ding et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2008}, {"title": "Seeing stars when there aren\u2019t many stars: Graph-based Semi-supervised Learning for Sentiment Categorization", "author": ["Goldberg", "Zhu2006]Goldberg", "Andrew B", "Xiaojin Zhu"], "venue": "Proceedings of the First Workshop on Graph Based Methods for Natural Language Processing,", "citeRegEx": "Goldberg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2006}, {"title": "Mining and Summarizing Customer Reviews", "author": ["Hu", "Liu2004]Hu", "Minqing", "Bing Liu"], "venue": "Proceedings of the 10th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD),", "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Unsupervised Sentiment Analysis with Emotional Signals", "author": ["Hu et al.2013]Hu", "Xia", "Jiliang Tang", "Huiji Gao", "Huan Liu"], "venue": null, "citeRegEx": "al.2013.Hu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al.2013.Hu et al\\.", "year": 2013}, {"title": "Micro-blogging as Online Word of Mouth Branding", "author": ["Jansen et al.2009]Jansen", "Bernard J", "Mimi Zhang", "Kate Sobel", "Abdur Chowdury"], "venue": "Proceedings of the 2009 International Conference on Human Factors in Computing Systems (CHI),", "citeRegEx": "al.2009.Jansen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al.2009.Jansen et al\\.", "year": 2009}, {"title": "Fully Automatic Lexicon Expansion for Domain-oriented Sentiment Analysis", "author": ["Kanayama", "Nasukawa2006]Kanayama", "Hiroshi", "Tetsuya Nasukawa"], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kanayama et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kanayama et al\\.", "year": 2006}, {"title": "Algorithms for Non-negative Matrix Factorization", "author": ["Lee", "Seung2001]Lee", "Daniel D", "H. Sebastian Seung"], "venue": "Proceedings of the Neural Information Processing Systems (NIPS),", "citeRegEx": "Lee et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2001}, {"title": "Is it harder to parse Chinese, or the Chinese Treebank", "author": ["Levy", "Manning2003]Levy", "Roger", "Christopher D. Manning"], "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Levy et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2003}, {"title": "Semi-Supervised Learning for Imbalanced Sentiment Classification", "author": ["Li et al.2011]Li", "Shoushan", "Zhongqing Wang", "Guodong Zhou", "Sophia Yat Mei Lee"], "venue": "Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "al.2011.Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al.2011.Li et al\\.", "year": 2011}, {"title": "A Comparative Study of Bayesian Models for Unsupervised Sentiment Detection", "author": ["Lin", "He", "Everson2010]Lin", "Chenghua", "Yulan He", "Richard Everson"], "venue": "Proceedings of the 14th Conference on Computational Natural Language Learning (CoNLL),", "citeRegEx": "Lin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2010}, {"title": "Sentiment Analysis and Subjectivity. Handbook of Natural Language Processing, Chapman and Hall/CRC, 2 edition", "author": ["Liu2010]Liu", "Bing"], "venue": null, "citeRegEx": "Liu2010.Liu and Bing.,? \\Q2010\\E", "shortCiteRegEx": "Liu2010.Liu and Bing.", "year": 2010}, {"title": "A Survey of Opinion Mining and Sentiment Analysis", "author": ["Liu", "Zhang2012]Liu", "Bing", "Lei Zhang"], "venue": "Mining Text Data,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Dialogue-Oriented Review Summary Generation for Spoken Dialogue Recommendation Systems", "author": ["Liu", "Seneff", "Zue2010]Liu", "Jingjing", "Stephanie Seneff", "Victor Zue"], "venue": "Proceedings of the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Automatic Construction of a Context-Aware Sentiment Lexicon: An Optimization Approach", "author": ["Lu et al.2011]Lu", "Yue", "Malu Castellanos", "Umeshwar Dayal", "ChengXiang Zhai"], "venue": null, "citeRegEx": "al.2011.Lu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al.2011.Lu et al\\.", "year": 2011}, {"title": "Generating Typed Dependency Parses from Phrase Structure Parses", "author": ["M. Marneffe2006]M. Marneffe", "B. Maccartney", "C. Manning"], "venue": "Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC),", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Learning Word Vectors for Sentiment Analysis", "author": ["Maas et al.2011]Maas", "Andrew L", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "al.2011.Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al.2011.Maas et al\\.", "year": 2011}, {"title": "Opinion Mining in Online Reviews: Recent Trends", "author": ["Moghaddam", "Ester2013]Moghaddam", "Samaneh", "Martin Ester"], "venue": "Tutorial on the 22th international conference on World Wide Web", "citeRegEx": "Moghaddam et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Moghaddam et al\\.", "year": 2013}, {"title": "Sentiment Analysis using Support Vector Machines with Diverse Information Sources", "author": ["Mullen", "Collier2004]Mullen", "Tony", "Nigel Collier"], "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Mullen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mullen et al\\.", "year": 2004}, {"title": "Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables", "author": ["Nakagawa", "Inui", "Kurohashi2010]Nakagawa", "Tetsuji", "Kentaro Inui", "Sadao Kurohashi"], "venue": "Proceedings of the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Nakagawa et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nakagawa et al\\.", "year": 2010}, {"title": "Performance and Trends in Recent Opinion Retrieval Techniques", "author": ["Orimaye", "Alhashmi", "Siew2013]Orimaye", "Sylvester O", "Saadat M. Alhashmi", "Eu Gene Siew"], "venue": "The Knowledge Engineering Review,", "citeRegEx": "Orimaye et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Orimaye et al\\.", "year": 2013}, {"title": "Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval, 2(1-2):1\u2013135", "author": ["Pang", "Lee2008]Pang", "Bo", "Lillian Lee"], "venue": null, "citeRegEx": "Pang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2008}, {"title": "Thumbs up? Sentiment Classification using Machine Learning Techniques", "author": ["Pang", "Lee", "Vaithyanathan2002]Pang", "Bo", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Extracting Product Features and Opinions from Reviews", "author": ["Popescu", "Etzioni2005]Popescu", "Ana Maria", "Oren Etzioni"], "venue": "Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Popescu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Popescu et al\\.", "year": 2005}, {"title": "SELC: A Self-Supervised Model for Sentiment Classification", "author": ["Qiu et al.2009]Qiu", "Likun", "Weishi Zhang", "Changjian Hu", "Kai Zhao"], "venue": "Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "al.2009.Qiu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al.2009.Qiu et al\\.", "year": 2009}, {"title": "Lexicon-Based Methods for Sentiment Analysis", "author": ["Taboada et al.2011]Taboada", "Maite", "Julian Brooke", "Milan Tofiloski", "Kimberly Voll", "Manfred Stede"], "venue": null, "citeRegEx": "al.2011.Taboada et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al.2011.Taboada et al\\.", "year": 2011}, {"title": "A Unified Framework for Emotional Elements Extraction Based on Finite State Matching Machine", "author": ["Tan et al.2013]Tan", "Yunzhi", "Yongfeng Zhang", "Min Zhang", "Yiqun Liu", "Shaoping Ma"], "venue": "Natural Language Processing and Chinese Computing (NLP&CC),", "citeRegEx": "al.2013.Tan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al.2013.Tan et al\\.", "year": 2013}, {"title": "Thumbs Up or Thumbs Down? Sentiment Orientation Applied to Unsupervised Classification of Reviews", "author": ["D. Peter"], "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Peter,? \\Q2002\\E", "shortCiteRegEx": "Peter", "year": 2002}, {"title": "Latent Aspect Rating Analysis on Review Text Data: A Rating Regression Approach", "author": ["Wang", "Lu", "Zhai2010]Wang", "Hongning", "Yue Lu", "Chengxiang Zhai"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Annotating Expressions of Opinions and Emotions in Language", "author": ["Wiebe", "Wilson", "Cardie2005]Wiebe", "Janyce", "Theresa Wilson", "Claire Cardie"], "venue": "Language Resources and Evaluation,", "citeRegEx": "Wiebe et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 2005}, {"title": "Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis", "author": ["Wilson", "Wiebe", "Hoffmann2005]Wilson", "Theresa", "Janyce Wiebe", "Paul Hoffmann"], "venue": "Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Wilson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "Multi-level Structured Models for Document-level Sentiment Classification", "author": ["Yessenalina", "Yue", "Cardie2010]Yessenalina", "Ainur", "Yisong Yue", "Claire Cardie"], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Yessenalina et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yessenalina et al\\.", "year": 2010}, {"title": "Automatic Seed Word Selection for Unsupervised Sentiment Classification of Chinese Text", "author": ["Zagibalov", "Carroll2008a]Zagibalov", "Taras", "John Carroll"], "venue": "Proceedings of the 19st International Conference on Computational Linguistics (Coling),", "citeRegEx": "Zagibalov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zagibalov et al\\.", "year": 2008}, {"title": "Unsupervised Classification of Sentiment and Objectivity in Chinese Text", "author": ["Zagibalov", "Carroll2008b]Zagibalov", "Taras", "John Carroll"], "venue": null, "citeRegEx": "Zagibalov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zagibalov et al\\.", "year": 2008}, {"title": "Incorporating Phrase-level Sentiment Analysis on Textual Reviews for Personalized Recommendation", "author": ["Zhang2015]Zhang", "Yongfeng"], "venue": "Proceedings of the 8th ACM international conference on Web Search and Data Mining (WSDM)", "citeRegEx": "Zhang2015.Zhang and Yongfeng.,? \\Q2015\\E", "shortCiteRegEx": "Zhang2015.Zhang and Yongfeng.", "year": 2015}, {"title": "Explicit Factor Models for Explainable Recommendation based on Phrase-level Sentiment Analysis", "author": ["Zhang et al.2014a]Zhang", "Yongfeng", "Guokun Lai", "Min Zhang", "Yi Zhang", "Yiqun Liu", "Shaoping Ma"], "venue": "Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval (SIGIR),", "citeRegEx": "al.2014a.Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al.2014a.Zhang et al\\.", "year": 2014}, {"title": "Do Users Rate or Review? Boost Phrase-level Sentiment Labeling with Review-level Sentiment Classification", "author": ["Zhang et al.2014b]Zhang", "Yongfeng", "Haochen Zhang", "Min Zhang", "Yiqun Liu", "Shaoping Ma"], "venue": "Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval (SIGIR),", "citeRegEx": "al.2014b.Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al.2014b.Zhang et al\\.", "year": 2014}, {"title": "Daily-Aware Personalized Recommendation based on Feature-Level Time Series Analysis", "author": ["Zhang et al.2015]Zhang", "Yongfeng", "Min Zhang", "Yi Zhang", "Guokun Lai", "Yiqun Liu", "Shaoping Ma"], "venue": "Proceedings of the 24nd international conference on World Wide Web (WWW)", "citeRegEx": "al.2015.Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al.2015.Zhang et al\\.", "year": 2015}, {"title": "Active Deep Networks for Semi-Supervised Sentiment Classification", "author": ["Zhou", "Chen", "Wang2010]Zhou", "Shusen", "Qingcai Chen", "Xiaolong Wang"], "venue": "Proceedings of the 21st International Conference on Computational Linguistics (Coling),", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [], "year": 2015, "abstractText": "Sentiment analysis on user reviews helps to keep track of user reactions towards products, and make advices to users about what to buy. State-of-the-art review-level sentiment classification techniques could give pretty good precisions of above 90%. However, current phrase-level sentiment analysis approaches might only give sentiment polarity labelling precisions of around 70% \u223c 80%, which is far from satisfaction and restricts its application in many practical tasks. In this paper, we focus on the problem of phrase-level sentiment polarity labelling and attempt to bridge the gap between phrase-level and review-level sentiment analysis. We investigate the inconsistency between the numerical star ratings and the sentiment orientation of textual user reviews. Although they have long been treated as identical, which serves as a basic assumption in previous work, we find that this assumption is not necessarily true. We further propose to leverage the results of review-level sentiment classification to boost the performance of phrase-level polarity labelling using a novel constrained convex optimization framework. Besides, the framework is capable of integrating various kinds of information sources and heuristics, while giving the global optimal solution due to its convexity. Experimental results on both English and Chinese reviews show that our framework achieves high labelling precisions of up to 89%, which is a significant improvement from current approaches.", "creator": "LaTeX with hyperref package"}}}