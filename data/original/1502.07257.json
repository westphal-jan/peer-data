{"id": "1502.07257", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2015", "title": "Breaking Sticks and Ambiguities with Adaptive Skip-gram", "abstract": "Recently proposed Skip-gram model is a powerful method for learning high-dimensional word representations that capture rich semantic relationships between words. However, Skip-gram as well as most prior work on learning word representations does not take into account word ambiguity and maintain only single representation per word. Although a number of Skip-gram modifications were proposed to overcome this limitation and learn multi-prototype word representations, they either require a known number of word meanings or learn them using greedy heuristic approaches. In this paper we propose the Adaptive Skip-gram model which is a nonparametric Bayesian extension of Skip-gram capable to automatically learn the required number of representations for all words at desired semantic resolution. We derive efficient online variational learning algorithm for the model and empirically demonstrate its efficiency on word-sense induction task.", "histories": [["v1", "Wed, 25 Feb 2015 17:15:56 GMT  (74kb)", "http://arxiv.org/abs/1502.07257v1", null], ["v2", "Sun, 15 Nov 2015 10:36:49 GMT  (133kb,D)", "http://arxiv.org/abs/1502.07257v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sergey bartunov", "dmitry kondrashkin", "anton osokin", "dmitry vetrov"], "accepted": false, "id": "1502.07257"}, "pdf": {"name": "1502.07257.pdf", "metadata": {"source": "META", "title": "Breaking Sticks and Ambiguities with Adaptive Skip-gram", "authors": ["Sergey Bartunov"], "emails": ["SBOS@SBOS.IN", "KONDRA2LP@GMAIL.COM", "ANTON.OSOKIN@INRIA.FR", "VETROVD@YANDEX.RU"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 2.\n07 25\n7v 1\n[ cs\n.C L\n] 2"}, {"heading": "1. Introduction", "text": "Continuous-valued word representations are very useful in many natural language processing applications. They could serve as input features for higher-level algorithms in text processing pipeline and help to overcome the word sparseness of natural texts. Moreover, they can explain on their own many semantic properties and relationships between concepts represented by words.\nRecently, with the success of the deep learning, new meth-\nods for learning word representations inspired by various neural architectures were introduced. Among many others the two particular models Continuous Bag of Words (CBOW) and Skip-gram (SG) proposed by Mikolov et al. (2013a) were used to obtain high-dimensional distributed representations that capture many semantic relationships and linguistic regularities (Mikolov et al., 2013b;c). In addition to high quality of learned representations these models are computationally very efficient and allow to process text data in online streaming setting.\nHowever, word ambiguity (which may appear as polysemy, homonymy, etc) the important property of a natural language is usually ignored in representation learning methods. For example, word \u201capple\u201d may refer to a fruit or to the Apple inc. depending on the context. Both CBOW and SG also fail to address this issue since they assume a unique representation for each word. As a consequence either the most frequent meaning of the word dominates the others or the meanings are mixed. Clearly both situations are not desirable for practical applications.\nWe address the problem of unsupervised learning of multiple representations that correspond to different meanings of a word, i.e. building multi-prototype word representations. This may be considered as specific case of word sense induction (WSI) problem which consists in automatic identification of the meanings of a word. In our case different meanings are distinguished by separate representations. We define meaning or sense as distinguishable interpretation of the spelled word which may be caused by any kind of ambiguity.\nWord-sense induction is closely related to the word-sense disambiguation (WSD) task where the goal is to choose\nwhich meaning of a word among provided in the sense inventory was used in the context. The sense inventory may be obtained by a WSI system or provided as external information.\nMany natural language processing (NLP) applications benefit from ability to deal with word ambiguity (Navigli & Crisafulli, 2010; Vickrey et al., 2005). Since word representations have been used as word features in dependency parsing (Chen & Manning, 2014), named-entity recognition (Turian et al., 2010) and sentiment analysis (Maas et al., 2011) among many other tasks, employing multi-prototype representations could increase the performance of such representation-based approaches.\nIn this paper we develop natural extension of the Skip-gram model which we call Adaptive Skip-gram (AdaGram). It retains all noticeable properties of SG such as fast online learning and high quality of representations while allowing to automatically learn the necessary number of prototypes per word at desired semantic resolution.\nThe rest of the paper is organized as follows: we start with reviewing original Skip-gram model (section 2), then we describe our extension called Adaptive Skip-gram (section 3). Then, we compare our model to existing approaches in section 4. In section 5 we evaluate our model qualitatively by considering neighborhoods of selected words in the learned latent space and by quantitative comparison against concurrent approaches. Finally, we conclude in section 6."}, {"heading": "2. Skip-gram model", "text": "The original Skip-gram model (Mikolov et al., 2013b) is formulated as a set of grouped word prediction tasks. Each task consists of prediction of a word v given a word w using correspondingly their output and input representations\np(v|w, \u03b8) = exp(in\u22bawoutv)\u2211V\nv\u2032=1 exp(in \u22ba woutv\u2032) , (1)\nwhere global parameter \u03b8 = {inv, outv}Vv=1 stands for both input and output representations for all words of the dictionary indexed with 1, . . . , V . Both input and output representations are real vectors of the dimensionality D.\nThese individual predictions are grouped in a way to simultaneously predict context words y of some input word x:\np(y|x, \u03b8) = \u220f\nj\np(yj |x, \u03b8).\nInput text o consisting of N words o1, o2, . . . , oN is then interpreted as a sequence of input words X = {xi}Ni=1 and their contexts Y = {yi}Ni=1. Here i-th training object (xi, yi) consists of word xi = oi and its contextyi = {ot}t\u2208c(i)\nwhere c(i) is a set of indices such that |t \u2212 i| \u2264 C/2 and t 6= i for all t \u2208 c(i)1.\nFinally, Skip-gram objective function is the likelihood of contexts given the corresponding input words:\np(Y |X, \u03b8) =\nN\u220f\ni=1\np(yi|xi, \u03b8) =\nN\u220f\ni=1\nC\u220f\nj=1\np(yij |xi, \u03b8). (2)\nNote that although contexts of adjacent words intersect, the model assumes the corresponding prediction problems independent.\nFor training the Skip-gram model it is common to ignore sentence and document boundaries and to interpret the input data as a stream of words. The objective (2) is then optimized in a stochastic fashion by sampling i-th word and its context, estimating gradients and updating parameters \u03b8. After the model is trained, Mikolov et al. (2013a;b) treated the input representations of the trained model as word features and showed that they captured semantic similarity between concepts represented by the words. Further we refer to the input representations as prototypes following (Reisinger & Mooney, 2010b).\nBoth evaluation and differentiation of (1) has linear complexity (in the dictionary size V ) which is too expensive for practical applications. Because of that the soft-max prediction model (1) is substituted by the hierarchical soft-max (Mnih & Hinton, 2008):\np(v|w, \u03b8) = \u220f\nn\u2208path(v)\n\u03c3(ch(n)in\u22bawoutn) (3)\nHere output representations are no longer associated with words, but rather with nodes in a binary tree where leaves are all possible words in the dictionary with unique paths from root to corresponding leaf. ch(n) assigns either 1 or \u22121 to each node in the path(v) depending on whether n is a left or right child of previous node in the path. Equation (3) is guaranteed to sum to 1 i.e. be a distribution w.r.t. v as \u03c3(x) = 1/(1 + exp(\u2212x)) = 1 \u2212 \u03c3(\u2212x). For computational efficiency Skip-gram uses Huffman tree to construct hierarchical soft-max."}, {"heading": "3. Adaptive Skip-gram", "text": "The original Skip-gram model maintains only one prototype per word. It would be unrealistic to assume that single representation may capture the semantics of all possible word meanings. At the same time it is non-trivial to specify exactly the right number of prototypes required for handling meanings of a particular word. Hence, an adaptive\n1For notational simplicity we will further assume that size of the context is always equal to C which is true for all non-boundary words.\napproach for allocation of additional prototypes for ambiguous words is required. Further we describe our Adaptive Skip-gram (AdaGram) model which extends the original Skip-gram and may automatically learn the required number of prototypes for each word using Bayesian nonparametric approach.\nFirst, assume that each word has K meanings each associated with its own prototype. That means that we have to modify (3) to account for particular choice of the meaning. For this reason we introduce latent variable z that encodes the index of active meaning and extend (3) to\np(v|z = k, w, \u03b8) = \u220f\nn\u2208path(v)\n\u03c3(ch(n)in\u22bawkoutn) (4)\nNote that we bring even more asymmetry between input and output representations comparing to (3) since now only prototypes depend on the particular word meaning. While it is possible to make context words be also meaning-aware this would make the training process much more complicated. Our experiments show that this word prediction model is enough to capture word ambiguity. Equation (4) could be viewed as prediction of context words using meanings of the input words.\nHowever, setting the number of prototypes for all words equal is not a very realistic assumption. Moreover, it is desirable that the number of prototypes for a particular word would be determined by the training text corpus. We approach this problem by employing Bayesian nonparametrics into Skip-gram model, i.e. we use the constructive definition of Dirichlet process (Ferguson, 1973) for automatic determination of the required number of prototypes. Dirichlet process (DP) has been successfully used for infinite mixture modeling and other problems where the number of structure components (e.g. clusters, latent factors, etc.) is not known a priori (Shahbaba & Neal, 2009; Rasmussen, 2000) which is exactly our case.\nWe use the constructive definition of DP via the stickbreaking representation (Sethuraman, 1994) to define a prior over meanings of a word. The meaning probabilities are computed by dividing total probability mass into infinite number of diminishing pieces summing to 1. So the prior probability of k-th meaning of the word w is\np(z = k|w,\u03b2) = \u03b2wk\nk\u22121\u220f\nr=1\n(1 \u2212 \u03b2wr),\np(\u03b2wk|\u03b1) = Beta(\u03b2wk|1, \u03b1), k = 1, . . .\nThis assumes that infinite number of prototypes for each word may exist. However, as long as we consider finite amount of text data, the number of prototypes (those with non-zero prior probabilities) for word w will not exceed the number of occurrences of w in the text which we denote as\nnw. The hyperparameter \u03b1 controls the number of prototypes for a word allocated a priori. Asymptotically, the expected number of prototypes of word w is proportional to \u03b1 log(nw). Thus, larger values of \u03b1 produce more prototypes which lead to more granular and specific meanings captured by learned representations and the number of prototypes scales logarithmically with number of occurrences.\nAnother attractive property of DPs is their ability to increase the complexity of latent variables\u2019 space with more data arriving. In our model this will result to more distinctive meanings of words discovered on larger text corpus.\nCombining all parts together we may write the AdaGram model as follows:\np(Y, Z,\u03b2|X,\u03b1, \u03b8) =\nV\u220f\nw=1\n\u221e\u220f\nk=1\np(\u03b2wk|\u03b1)\nN\u220f\ni=1\n[ p(zi|xi,\u03b2)) C\u220f\nj=1\np(yij |zi, xi, \u03b8) ] .\nwhere Z = {zi}Ni=1 is a set of senses for all the words.\nSimilarly to Mikolov et al. (2013a) we do not consider any regularization (and so the informative prior) for representations and seek for point estimate of \u03b8."}, {"heading": "3.1. Learning representations", "text": "One way to train the AdaGram is to maximize the marginal likelihood of the model\nlog p(Y |X, \u03b8, \u03b1) = log\n\u222b \u2211\nZ\np(Y, Z,\u03b2|X,\u03b1, \u03b8)d\u03b2 (5)\nwith respect to representations \u03b8. One may see that the marginal likelihood is intractable because of the latent variables Z and \u03b2. Moreover, \u03b2 and \u03b8 are infinite-dimensional parameters. Thus unlike the original Skip-gram and other methods for learning multiple word representations, our model could not be straightforwardly trained by stochastic gradient ascent w.r.t. \u03b8.\nTo make this tractable we consider the variational lower bound on the marginal likelihood (5)\nL = Eq [log p(Y, Z,\u03b2|X,\u03b1, \u03b8)\u2212 log q(Z,\u03b2)]\nwhere\nq(Z,\u03b2) = q(Z)q(\u03b2) =\nN\u220f\ni=1\nq(zi)\nV\u220f\nw=1\nT\u220f\nk=1\nq(\u03b2wk)\nis the fully factorized variational approximation to the posterior p(Z,\u03b2|X,Y, \u03b1, \u03b8) with possible number of representations for each word truncated to T (Blei & Jordan, 2005). It may be shown that the maximization of the variational lower bound with respect to q(Z,\u03b2) is equivalent to\nthe minimization of Kullback-Leibler divergence between q(Z,\u03b2) and the true posterior (Jordan et al., 1999).\nWithin this approximation the variational lower bound L(q(Z), q(\u03b2), \u03b8) takes the following form:\nL(q(Z),q(\u03b2),\u03b8)=Eq\n[ V\u2211\nw=1\nT\u2211\nk=1\nlog p(\u03b2wk|\u03b1)\u2212log q(\u03b2wk)+\nN\u2211\ni=1\n( log p(zi|xi,\u03b2)\u2212log q(zi)+ C\u2211\nj=1\nlog p(yij |zi, xi, \u03b8) ) ] .\nSetting derivatives of L(q(Z), q(\u03b2), \u03b8) with respect to q(Z) and q(\u03b2) to zero yields standard update equations\nlog q(zi = k) = Eq(\u03b2) [log p(Y, Z,\u03b2|X,\u03b1, \u03b8)] + const\n= Eq(\u03b2) [ log \u03b2xi,k + k\u22121\u2211\nr=1\nlog(1\u2212 \u03b2xi,r)\n]\n+\nC\u2211\nj=1\nlog p(yij |k, xi, \u03b8) + const, (6)\nlog q(\u03b2) = Eq(Z) [log p(Y, Z,\u03b2|X,\u03b1, \u03b8)] + const\n= V\u2211\nw=1\nT\u2211\nk=1\nlog Beta(\u03b2wk|awk, bwk), (7)\nwhere (natural) parameters awk and bwk deterministically depend on the expected number of assignments to particular sense nwk = \u2211 i:xi=w\nq(zi = k) (Blei & Jordan, 2005):\nawk = 1 + nwk, bwk = \u03b1+\nT\u2211\nr=k+1\nnwr. (8)\nStochastic variational inference. Although variational updates given by (6) and (7)-(8) are tractable, they require the full pass over training data. In order to keep the efficiency of Skip-gram training procedure, we employ stochastic variational inference approach (Hoffman et al., 2013) and derive online optimization algorithm for the maximization of L.\nThere are two groups of parameters in our objective: {q(\u03b2vk)} and \u03b8 are global because they affect all the objects; {q(zi)} are local, i.e. affect only the corresponding object xi. After updating the local parameters according to (6) with the global parameters fixed and defining the obtained distribution as q\u2217(Z) we have a function of the global parameters\nL\u2217(q(\u03b2), \u03b8) = L(q\u2217(Z), q(\u03b2), \u03b8) \u2265 L(q(Z), q(\u03b2), \u03b8).\nAlgorithm 1 Training AdaGram model\nInput: training data {(xi,yi)}Ni=1, hyperparameter \u03b1 Output: parameters \u03b8, distributions q(\u03b2), q(z)\nInitialize parameters \u03b8, distributions q(\u03b2), q(z) for i = 1 to N do\nSelect word w = xi and its context yi LOCAL STEP: for k = 1 to T do \u03b3ik = Eq(\u03b2w)[log p(zi = k|\u03b2, xi)] for j = 1 to C do \u03b3ik \u2190 \u03b3ik + log p(yij |xi, k, \u03b8)\nend end \u03b3ik \u2190 exp(\u03b3ik)/ \u2211 \u2113 exp(\u03b3i\u2113) GLOBAL STEP: \u03c1t \u2190 0.025(1\u2212 i/N), \u03bbt \u2190 0.025(1\u2212 i/N) for k = 1 to T do\nUpdate nwk \u2190 (1\u2212 \u03bbt)nwk + \u03bbtnw\u03b3ik end Update \u03b8 \u2190 \u03b8 + \u03c1t\u2207\u03b8 \u2211 k \u2211 j \u03b3ik log p(yij |xi, k, \u03b8)\nend\nThe new lower bound L\u2217 is no longer a function of local parameters which are always kept updated to their optimal values. Thus L\u2217 allows for employing stochastic optimization framework and was shown to converge faster than L when using gradient methods (Hensman et al., 2012).\nFollowing Hoffman et al. (2013) we iteratively optimizeL\u2217 with respect to the global parameters using stochastic gradient estimated at a single object. Stochastic gradient w.r.t \u03b8 computed on the i-th object is computed as follows:\n\u2207\u0302\u03b8L \u2217 = N\nC\u2211\nj=1\nT\u2211\nk=1\nq\u2217(zi = k)\u2207\u03b8 log p(yij |k, xi, \u03b8)\nNow we describe how to optimize L\u2217 w.r.t global posterior approximation q(\u03b2) = \u220fD w=1 \u220fT k=1 q(\u03b2wk). The stochastic gradient with respect to natural parameters awk and bwk according to (Hoffman et al., 2013) can be estimated by computing intermediate values of natural parameters (a\u0302wk, b\u0302wk) on the i-th data point as if we estimated q(z) for all occurrences of xi = w equal to q(zi):\na\u0302wk = 1+nwq(zi = k), b\u0302wk = \u03b1+\nT\u2211\nr=k+1\nnwq(zi = r),\nwhere nw is the total number of occurrences of word w. The stochastic gradient estimate then can be expressed in the following simple form:\n\u2207\u0302awkL \u2217 = a\u0302wk \u2212 awk, \u2207\u0302bwkL \u2217 = b\u0302wk \u2212 bwk.\nOne may see that making such gradient update is equivalent to updating counts nwk since they are sufficient statistics of q(\u03b2wk).\nWe use conservative initialization strategy for q(\u03b2) starting with only one allocated meaning for each word, i.e. nw1 = nw and nwk = 0, k > 1. Representations are initialized with random values drawn from Uniform(\u22120.5/D, 0.5/D). In our experiments we updated both learning rates \u03c1 and \u03bb using the same linear schedule from 0.025 to 0.\nThe resulting learning algorithm 1 may be also interpreted as an instance of stochastic variational EM algorithm. It has linear computational complexity in the length of text o similarly to Skip-gram learning procedure. The overhead of maintaining variational distributions is negligible comparing to dealing with representations and thus training of AdaGram is T times slower than Skip-gram."}, {"heading": "3.2. Disambiguation and prediction", "text": "After model is trained on data D = {(xi,yi)}Ni=1, it can be used to infer the meanings of an input word x given its context y. The predictive probability of a meaning can be computed as:\np(z = k|x,D, \u03b8, \u03b1) \u221d \u222b p(z = k|\u03b2, x)q(\u03b2)d\u03b2, (9)\nwhere q(\u03b2) can serve as an approximation of p(\u03b2|D, \u03b8, \u03b1). Since q(\u03b2) has the form of independent Beta distributions whose parameters are given in (8) the integral can be taken analytically. The number of learned prototypes for a word w may be computed as\nT\u2211\nk=1\n1[p(z = k|w,D, \u03b8, \u03b1) > \u01eb],\nwhere \u01eb is a threshold e.g. 10\u22123.\nThe probability of each meaning of x given context y is thus given by:\np(z = k|x,y, \u03b8) \u221d p(y|x, k, \u03b8) \u222b p(k|\u03b2, x)q(\u03b2)d\u03b2\n(10)\nNow the posterior predictive over context words y given input word x may be expressed as:\np(y|x,D, \u03b8, \u03b1) =\n\u222b T\u2211\nz=1\np(y|x, z, \u03b8)p(z|\u03b2, x)q(\u03b2)d\u03b2.\n(11)"}, {"heading": "4. Related work", "text": "Literature on learning continuous-space representations of embeddings for words is vast, therefore we concentrate on\nworks that are most relevant to our approach.\nHuang et al. (2012); Reisinger & Mooney (2010a;b) propose the neural network-based methods for learning multiprototype representations. Both methods include clustering contexts for all words as prepossessing or intermediate step. While this allows to learn multiple prototypes per word, clustering large number of contexts brings serious computational overhead and limit these approaches to offline setting.\nRecently various modifications of Skip-gram were proposed to learn multi-prototype representations. Qiu et al. (2014) developed Proximity-Ambiguity Sensitive Skipgram which maintains individual representations for different parts of speech (POS) of the same word. While this may handle word ambiguity to some extent, clearly there could be many meanings even for the same part of speech of some word remaining not discovered by this approach.\nWork of Tian et al. (2014) can be considered as a parametric form of our model with number of meanings for each word fixed. Their model also provides improvement over original Skip-gram, but it is not clear how to set the number of prototypes. Our approach not only allows to efficiently learn required number of prototypes for ambiguous words, but is able also to gradually increase the number of meanings when more data becomes available thus distinguishing between shades of same meaning.\nChen et al. (2014) incorporate external knowledge about word meanings into Skip-gram in the form of sense inventory. First, single-prototype representations are pre-trained with original Skip-gram. Afterwards, meanings provided by WordNet lexical database are used learn multi-prototype representations for ambiguous words. The dependency on the external high-quality linguistic resources such as WordNet makes this approach inapplicable to languages lacking such databases. In contrast, our model does not consider any form of supervision and learns the sense inventory automatically from the raw text.\nRecent work of Neelakantan et al. (2014) proposing Multisense Skip-gram (MSSG) and its nonparameteric (not in the sense of Bayesian nonparametrics) version (NP MSSG) is the closest to AdaGram prior art. While MSSG defines the number of prototypes apriori similarly to (Tian et al., 2014), NP MSSG features automatic discovery of multiple meanings for each word. In contrast to our approach, learning for NP MSSG is defined rather as ad-hoc greedy procedure that allocates new representation for a word if existing ones explain its context below some threshold. AdaGram instead follows more principled nonparametric Bayesian approach. Thus in our model the number of prototypes may not only increase during the training, but also decrease if this leads to a better model in terms of varia-\ntional lower bound. In our experimental evaluation we use models of Neelakantan et al. (2014) as baselines."}, {"heading": "5. Experiments", "text": "In this section we empirically evaluate our model in a number of different tests. First, we demonstrate learned multiprototype representations on several example words. Next we investigate how different values of \u03b1 affect the number of learned prototypes what we call a semantic resolution of a model. Finally, we evaluate our approach on the word sense induction task (WSI).\nIn order to evaluate our method we trained several models with different values of \u03b1 on April 2010 snapshot of English Wikipedia (Shaoul & Westbury, 2010). It contains nearly 2 million articles and 990 million tokens. We did not consider words which have less than 20 occurrences. The context width was set to C = 10 and the truncation level of Stick-breaking approximation (the maximum number of meanings) to T = 30. The dimensionality D of representations learned by our model was set to 300."}, {"heading": "5.1. Nearest neighbors", "text": "In Table 1 we present the meanings which were discovered by our model with parameter \u03b1 = 0.1 for words used in Neelakantan et al. (2014) and for a few other sample words. To distinguish the meanings we obtain their nearest neighbors by computing the cosine similarity between each meaning prototype and the prototypes of meanings of all other words. One may see that AdaGram model learns a reasonable number of prototypes which are meaningful and interpretable. The predictive probability of each meaning reflects how frequently it was used in the training corpus."}, {"heading": "5.2. Semantic resolution", "text": "As mentioned in section 3 hyperparameter \u03b1 of the AdaGram model indirectly controls the number of induced word meanings. Figure 1 shows the distribution of number of induced word meanings under different values of \u03b1. One may see that while for most words relatively small number of meanings is learned, larger values of \u03b1 lead to more meanings in general. This effect may be explained by the property of Dirichlet process to allocate number of prototypes that logarithmically depends on number of word occurrences. Since word occurrences are known to be distributed by Zipf\u2019s law, the majority of words is rather infrequent and thus our model discovers few meanings for them. Figure 2 quantitatively demonstrates this phenomenon.\nIn the table 2 we demonstrate how larger values of \u03b1 lead to more meanings on the example of the word \u201clight\u201d. The original Skip-gram discovered only the meaning related to a physical phenomenon, AdaGram with \u03b1 = 0.075 found\nthe second, military meaning, with further increase of \u03b1 value those meanings start splitting to submeanings, e.g. light tanks and light troops. In table 3 we demonstrate analogical result on the example of the word \u201ccore\u201d. The Skip-gram model discovered only one meaning which corresponds to microprocessor core, AdaGram with \u03b1 = 0.05 also found the meaning related to nuclear reactors, and several other meanings were obtained with the increase of \u03b1.\nFor most of the words \u03b1 = 0.1 results in most interpretable model. It seems that for values less than 0.1 for most words only one prototype is learned and for values greater than 0.1 the model becomes less interpretable as learned meanings are too specific sometimes duplicating."}, {"heading": "5.3. Word prediction", "text": "Since both Skip-gram and AdaGram are defined as models for predicting context of a word, it is essential to evaluate how well they explain test data by predictive likelihood. We use last 100 megabytes of December 2014 snapshot of English Wikipedia and 100 megabytes from One Billion Word benchmark dataset (Chelba et al., 2014) as test data for this experiment.\nSimilarly to the train procedure we consider this text as pairs of input words and contexts of size C = 10, that is, Dtest = {(xi,yi)}Ni=1 and compare AdaGram with original Skip-gram by average log-likelihood calculated by equation (11). The results are given in Table 4. Clearly, AdaGram models text data better than Skip-gram and achieves the significant improvement under \u03b1 = 0.1 which\ncorresponds to the most interpretable model.\nSince One Billion Word dataset consists of news articles which differ from encyclopedic articles on which the model was trained, it is not surprisingly that likelihood on Wikipedia contexts is consistently higher."}, {"heading": "5.4. Word-sense induction", "text": "The nonparametric learning of a multi-prototype representation model is closely related to the word-sense induction (WSI) task which aims at automatic discovery of different meanings for the words. Indeed, learned prototypes identify different word meanings and it is natural to assess how well are they aligned with human judgements.\nWe compare our AdaGram model with Nonparamet-\nric Multi-sense Skip-gram (NP-MSSG) proposed by Neelakantan et al. (2014) which is currently the only existing approach to learning multi-prototype word representations with Skip-gram. We also include in comparison the parametric form of NP-MSSG which has the number of meanings fixed to 3 for all words during the training. All models were trained on the same dataset which is the Wikipedia snapshot by Shaoul & Westbury (2010). For the comparison with MSSG and NP-MSSG we used source code and models released by the authors. Neelakantan et al. (2014) limited the number of words for which multi-prototype representations were learned (30000 and 6000 most frequent words) for these models. We use the following notation: 300D or 50D is the dimensionality of word representations, 6K or 30K is the number of multiprototype words (6000 and 30000 respectively) in case of MSSG and NP-MSSG models.\nThe evaluation is performed as follows. Dataset consisting of target word and context pairs is supplied to a model which uses the context to disambiguate target word into a meaning from its learned sense inventory. Then for each target word the model\u2019s labeling of contexts and ground truth one are compared as two different clusterings of the same set using appropriate metrics. The results are then averaged over all target words."}, {"heading": "5.4.1. SEMEVAL-2010 DATASET", "text": "The SemEval-2010 dataset was introduced for SemEval2010 Word Sense Induction & Disambiguation competition (Manandhar et al., 2010). This dataset consists of 100 target words and 8915 contexts in total. It is particularly suitable for evaluation of word representation learning methods as it consists of only single-term target words.\nWe do not consider SemEval-2013 dataset (Navigli & Vannella, 2013) for evaluation as it contains many multiple-term target words (like \u201cMortal Kombat\u201d) which can not be handled properly using representations of single words and consists only of 6400 contexts in total.\nManandhar et al. (2010) suggested two metrics for evaluation: V-Measure (VM) and F-Score (FS). Authors of the dataset pointed to the weakness of both VM and FS. VM favours large number of clusters and attains large values on unreasonable clusterings which assign each instance to its own cluster. At the same time, FS is biased towards clusterings consisting of small number of clusters e.g. assigning each instance to the same single cluster. Thus we consider another metric - adjusted Rand index (Hubert & Arabie, 1985) (ARI) which does not suffer from such drawbacks. Both examples of undesirable clusterings described above will get ARI of nearly zero which corresponds to human intuition. Thus we consider ARI as more reliable metric for WSI evaluation. We still report VM and FS values in order to make our results comparable to others obtained on the dataset.\nThe quantitative results on the dataset are provided in Table 5. Our AdaGram model with parameter \u03b1 = 0.2 is significantly better than all MSSG models by both adjusted Rand index and V-measure."}, {"heading": "5.4.2. SEMEVAL-2007 DATASET", "text": "Here we report the results on SemEval-2007 WSI dataset (Agirre & Soroa, 2007). This dataset contains 27232 contexts collected from Wall Street Journal (WSJ) corpus. We merged together train and test contexts and used them for model comparison. Since WSJ corpus differs from general-domain Wikipedia corpus, we retrained all considered models on \u00abOne billion word benchmark\u00bb dataset (Chelba et al., 2014) which consists of news articles and thus is more suitable for evaluation on WSJ data. Our results on word prediction task also suggest that difference in domain of training and test corpora affect the performance. The results are shown in table 6. Each model was supplied with contexts of the size that maximizes its ARI performance."}, {"heading": "5.4.3. NEW WWSI DATASET", "text": "In order to make the evaluation more comprehensive, we introduce the new Wikipedia Word-sense Induction (WWSI) dataset consisting of 188 target words and 36354 contexts. For the best of our knowledge it is currently the largest WSI dataset available. While SemEval datasets are prepared with hand effort of experts which mapped contexts into gold standard sense inventory, we collected WWSI using fully automatic approach from December 2014 snapshot of Wikipedia. More details on the dataset construction procedure are provided in the Appendix.\nWe use adjusted Rand index averaged over test words to compare AdaGram models trained with different values of \u03b1 and the models of Neelakantan et al. (2014). The results are provided in Table 7. Our model significantly outperforms both MSSG and NP-MSSG for all values of parameter \u03b1 except \u03b1 = 0.05. This demonstrates that not only large number of meanings can be learned by AdaGram, but they also are well aligned with Wikipedia sense inventory. However, while large values of \u03b1 result into higher number of learned prototypes, latter may become less interpretable to a human contradicting quantitative evaluation results. Such phenomenon is also common for topic modeling, see e.g. (Boyd-Graber et al., 2014)."}, {"heading": "6. Conclusion", "text": "In the paper we proposed AdaGram which is the Bayesian nonparametric extension of the well-known Skip-gram model. AdaGram uses different prototypes to represent a word depending on the context and thus may handle various forms of word ambiguity. Our experiments suggest that representations learned by our model correspond to different word meanings. Using resolution parameter \u03b1 we may control how many prototypes are extracted from the same text corpus. Too large values of \u03b1 lead to different prototypes that correspond to the same meaning which decreases model performance. The values \u03b1 = 0.1 \u2212 0.2 are generally good for practical purposes. For those values the truncation level T = 30 is enough and does not affect the number of discovered prototypes. AdaGram also features online variational learning algorithm which is very scalable and makes it possible to train our model just several times slower than extremely efficient Skip-gram model. Since the problem of learning multi-prototype word representation is closely related to word-sense induction, we evaluated AdaGram on several WSI datasets and contributed a new large one obtained automatically from Wikipedia disambiguation pages. The source code of our implementation, WWSI dataset and all trained models are available at http://bayesgroup.ru/adagram/."}, {"heading": "Appendix. WWSI Dataset construction details", "text": "Similarly to (Navigli & Vannella, 2013) we considered Wikipedia\u2019s disambiguation pages as a list of ambiguous words. From that list we have selected target single-term words which had occurred in the text at least 5000 times to ensure there is enough training contexts in Wikipedia to capture different meanings of a word (note, however, that all models were trained on earlier snapshot of Wikipedia). We also did not consider pages belonging to some categories such as \u201cLetternumber_combination_disambiguation_pages\u201d as they did not contain meaningful words. Then we prepared the sense inventory for each word in the list using Wikipedia pages with names matching to the pattern \u201cWORD_(*)\u201d which is used as convenient naming of specific word meanings. Again, we applied some automatic filtering to remove names of people and geographical places in order to obtain more coarse-grained meanings. Finally for each page selected on the previous step we find all occurrences of the target word on it and use its 5-word neighbourhood (5 words on the left and 5 words on the right) as a context. Such size of the context was chosen to minimize the intersection between adjacent contexts but still provide enough words for disambiguation. 10-word context results into average intersection of 1.115 words.\nThe list of the categories pages belonging to which were excluded during target word selection is following:\n\u2022 Place_name_disambiguation_pages\n\u2022 Disambiguation_pages_with_surname-holder_lists\n\u2022 Human_name_disambiguation_pages\n\u2022 Lists_of_ambiguous_numbers\n\u2022 Disambiguation_pages_with_given-nameholder_lists\n\u2022 Letter-number_combination_disambiguation_pages\n\u2022 Two-letter_disambiguation_pages\n\u2022 Transport_route_disambiguation_pages\n\u2022 Temple_name_disambiguation_pages\n\u2022 and also those from the categories which name contains one of the substrings: \u201ccleanup\u201d, \u201cpeople\u201d, \u201csurnames\u201d\nDuring the sense inventory collection we do not consider pages which name contains one of the following substrings: \u201ctv_\u201d, \u201cseries\u201d, \u201cmovie\u201d, \u201cfilm\u201d, \u201csong\u201d, \u201calbum\u201d, \u201cband\u201d, \u201csinger\u201d, \u201cmusical\", \u201ccomics\"; and also those from the\ncategories with names containing geography terms \u201ccountries\u201d, \u201cpeople\u201d, \u201cprovince\u201d, \u201cprovinces\u201d.\nIn table 8 we compare our WWSI dataset with SemEval2007 and SemEval-2010 datasets. Namely, we provide the total number of words and contexts as well as average number of contexts and senses per word. Our dataset has the largest number of words and contexts, however it consists of the words with less number of senses."}], "references": [{"title": "SemEval-2007 task 02: Evaluating word sense induction and discrimination systems", "author": ["E. Agirre", "A. Soroa"], "venue": "In International workshop on semantic evaluation (SemEval),", "citeRegEx": "Agirre and Soroa,? \\Q2007\\E", "shortCiteRegEx": "Agirre and Soroa", "year": 2007}, {"title": "Variational inference for Dirichlet process mixtures", "author": ["D.M. Blei", "M.I. Jordan"], "venue": "Bayesian Analysis,", "citeRegEx": "Blei and Jordan,? \\Q2005\\E", "shortCiteRegEx": "Blei and Jordan", "year": 2005}, {"title": "Care and Feeding of Topic Models: Problems, Diagnostics, and Improvements", "author": ["J. Boyd-Graber", "D. Mimno", "D. Newman"], "venue": "CRC Handbooks of Modern Statistical Methods. CRC Press, Boca Raton, Florida,", "citeRegEx": "Boyd.Graber et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Boyd.Graber et al\\.", "year": 2014}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["D. Chen", "C. Manning"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen and Manning,? \\Q2014\\E", "shortCiteRegEx": "Chen and Manning", "year": 2014}, {"title": "A unified model for word sense representation and disambiguation", "author": ["X. Chen", "Z. Liu", "M. Sun"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "A Bayesian analysis of some nonparametric problems", "author": ["T.S. Ferguson"], "venue": "The Annals of Statistics,", "citeRegEx": "Ferguson,? \\Q1973\\E", "shortCiteRegEx": "Ferguson", "year": 1973}, {"title": "Fast variational inference in the conjugate exponential family", "author": ["J. Hensman", "M. Rattray", "N.D. Lawrence"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hensman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hensman et al\\.", "year": 2012}, {"title": "Stochastic variational inference", "author": ["M.D. Hoffman", "D.M. Blei", "C. Wang", "J. Paisley"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "In Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Learning word vectors for sentiment analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT),", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "SemEval-2010 task 14: Word sense induction & disambiguation", "author": ["S. Manandhar", "I.P. Klapaftis", "D. Dligach", "S.S. Pradhan"], "venue": "In International workshop on semantic evaluation (SemEval),", "citeRegEx": "Manandhar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Manandhar et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Technical Report 1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT)", "author": ["T. Mikolov", "W. Yih", "G. Zweig"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mnih and Hinton,? \\Q2008\\E", "shortCiteRegEx": "Mnih and Hinton", "year": 2008}, {"title": "Inducing word senses to improve web search result clustering", "author": ["R. Navigli", "G. Crisafulli"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Navigli and Crisafulli,? \\Q2010\\E", "shortCiteRegEx": "Navigli and Crisafulli", "year": 2010}, {"title": "SemEval-2013 task 11: Word sense induction and disambiguation within an end-user application", "author": ["R. Navigli", "D. Vannella"], "venue": "In International workshop on semantic evaluation (SemEval),", "citeRegEx": "Navigli and Vannella,? \\Q2013\\E", "shortCiteRegEx": "Navigli and Vannella", "year": 2013}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space", "author": ["A. Neelakantan", "J. Shankar", "A. Passos", "A. McCallum"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Neelakantan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Learning word representation considering proximity and ambiguity", "author": ["L. Qiu", "Y. Cao", "Z. Nie", "Y. Yu", "Y. Rui"], "venue": "In Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Qiu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "The infinite Gaussian mixture model", "author": ["C.E. Rasmussen"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Rasmussen,? \\Q2000\\E", "shortCiteRegEx": "Rasmussen", "year": 2000}, {"title": "A mixture model with sharing for lexical semantics", "author": ["J. Reisinger", "R. Mooney"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Reisinger and Mooney,? \\Q2010\\E", "shortCiteRegEx": "Reisinger and Mooney", "year": 2010}, {"title": "Multi-prototype vectorspace models of word meaning. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT)", "author": ["J. Reisinger", "R.J. Mooney"], "venue": null, "citeRegEx": "Reisinger and Mooney,? \\Q2010\\E", "shortCiteRegEx": "Reisinger and Mooney", "year": 2010}, {"title": "A constructive definition of Dirichlet priors", "author": ["J. Sethuraman"], "venue": "Statistica Sinica,", "citeRegEx": "Sethuraman,? \\Q1994\\E", "shortCiteRegEx": "Sethuraman", "year": 1994}, {"title": "Nonlinear models using Dirichlet process mixtures", "author": ["B. Shahbaba", "R. Neal"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Shahbaba and Neal,? \\Q2009\\E", "shortCiteRegEx": "Shahbaba and Neal", "year": 2009}, {"title": "A probabilistic model for learning multiprototype word embeddings", "author": ["F. Tian", "H. Dai", "J. Bian", "B. Gao", "R. Zhang", "E. Chen", "Liu", "T.-Y"], "venue": "In International Conference on Computational Linguistics (COLING),", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "In Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Word-sense disambiguation for machine translation", "author": ["D. Vickrey", "L. Biewald", "M. Teyssier", "D. Koller"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Vickrey et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Vickrey et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 12, "context": "Among many others the two particular models Continuous Bag of Words (CBOW) and Skip-gram (SG) proposed by Mikolov et al. (2013a) were used to obtain high-dimensional distributed representations that capture many semantic relationships and linguistic regularities (Mikolov et al.", "startOffset": 106, "endOffset": 129}, {"referenceID": 27, "context": "Many natural language processing (NLP) applications benefit from ability to deal with word ambiguity (Navigli & Crisafulli, 2010; Vickrey et al., 2005).", "startOffset": 101, "endOffset": 151}, {"referenceID": 26, "context": "Since word representations have been used as word features in dependency parsing (Chen & Manning, 2014), named-entity recognition (Turian et al., 2010) and sentiment analysis (Maas et al.", "startOffset": 130, "endOffset": 151}, {"referenceID": 10, "context": ", 2010) and sentiment analysis (Maas et al., 2011) among many other tasks, employing multi-prototype representations could increase the performance of such representation-based approaches.", "startOffset": 31, "endOffset": 50}, {"referenceID": 5, "context": "we use the constructive definition of Dirichlet process (Ferguson, 1973) for automatic determination of the required number of prototypes.", "startOffset": 56, "endOffset": 72}, {"referenceID": 20, "context": ") is not known a priori (Shahbaba & Neal, 2009; Rasmussen, 2000) which is exactly our case.", "startOffset": 24, "endOffset": 64}, {"referenceID": 23, "context": "We use the constructive definition of DP via the stickbreaking representation (Sethuraman, 1994) to define a prior over meanings of a word.", "startOffset": 78, "endOffset": 96}, {"referenceID": 12, "context": "Similarly to Mikolov et al. (2013a) we do not consider any regularization (and so the informative prior) for representations and seek for point estimate of \u03b8.", "startOffset": 13, "endOffset": 36}, {"referenceID": 9, "context": "the minimization of Kullback-Leibler divergence between q(Z,\u03b2) and the true posterior (Jordan et al., 1999).", "startOffset": 86, "endOffset": 107}, {"referenceID": 7, "context": "In order to keep the efficiency of Skip-gram training procedure, we employ stochastic variational inference approach (Hoffman et al., 2013) and derive online optimization algorithm for the maximization of L.", "startOffset": 117, "endOffset": 139}, {"referenceID": 6, "context": "Thus L allows for employing stochastic optimization framework and was shown to converge faster than L when using gradient methods (Hensman et al., 2012).", "startOffset": 130, "endOffset": 152}, {"referenceID": 6, "context": "Thus L allows for employing stochastic optimization framework and was shown to converge faster than L when using gradient methods (Hensman et al., 2012). Following Hoffman et al. (2013) we iteratively optimizeL with respect to the global parameters using stochastic gradient estimated at a single object.", "startOffset": 131, "endOffset": 186}, {"referenceID": 7, "context": "The stochastic gradient with respect to natural parameters awk and bwk according to (Hoffman et al., 2013) can be estimated by computing intermediate values of natural parameters (\u00e2wk, b\u0302wk) on the i-th data point as if we estimated q(z) for all occurrences of xi = w equal to q(zi):", "startOffset": 84, "endOffset": 106}, {"referenceID": 25, "context": "While MSSG defines the number of prototypes apriori similarly to (Tian et al., 2014), NP MSSG features automatic discovery of multiple meanings for each word.", "startOffset": 65, "endOffset": 84}, {"referenceID": 7, "context": "Huang et al. (2012); Reisinger & Mooney (2010a;b) propose the neural network-based methods for learning multiprototype representations.", "startOffset": 0, "endOffset": 20}, {"referenceID": 7, "context": "Huang et al. (2012); Reisinger & Mooney (2010a;b) propose the neural network-based methods for learning multiprototype representations. Both methods include clustering contexts for all words as prepossessing or intermediate step. While this allows to learn multiple prototypes per word, clustering large number of contexts brings serious computational overhead and limit these approaches to offline setting. Recently various modifications of Skip-gram were proposed to learn multi-prototype representations. Qiu et al. (2014) developed Proximity-Ambiguity Sensitive Skipgram which maintains individual representations for different parts of speech (POS) of the same word.", "startOffset": 0, "endOffset": 526}, {"referenceID": 7, "context": "Huang et al. (2012); Reisinger & Mooney (2010a;b) propose the neural network-based methods for learning multiprototype representations. Both methods include clustering contexts for all words as prepossessing or intermediate step. While this allows to learn multiple prototypes per word, clustering large number of contexts brings serious computational overhead and limit these approaches to offline setting. Recently various modifications of Skip-gram were proposed to learn multi-prototype representations. Qiu et al. (2014) developed Proximity-Ambiguity Sensitive Skipgram which maintains individual representations for different parts of speech (POS) of the same word. While this may handle word ambiguity to some extent, clearly there could be many meanings even for the same part of speech of some word remaining not discovered by this approach. Work of Tian et al. (2014) can be considered as a parametric form of our model with number of meanings for each word fixed.", "startOffset": 0, "endOffset": 878}, {"referenceID": 4, "context": "Chen et al. (2014) incorporate external knowledge about word meanings into Skip-gram in the form of sense inventory.", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "Chen et al. (2014) incorporate external knowledge about word meanings into Skip-gram in the form of sense inventory. First, single-prototype representations are pre-trained with original Skip-gram. Afterwards, meanings provided by WordNet lexical database are used learn multi-prototype representations for ambiguous words. The dependency on the external high-quality linguistic resources such as WordNet makes this approach inapplicable to languages lacking such databases. In contrast, our model does not consider any form of supervision and learns the sense inventory automatically from the raw text. Recent work of Neelakantan et al. (2014) proposing Multisense Skip-gram (MSSG) and its nonparameteric (not in the sense of Bayesian nonparametrics) version (NP MSSG) is the closest to AdaGram prior art.", "startOffset": 0, "endOffset": 645}, {"referenceID": 18, "context": "In our experimental evaluation we use models of Neelakantan et al. (2014) as baselines.", "startOffset": 48, "endOffset": 74}, {"referenceID": 18, "context": "1 for words used in Neelakantan et al. (2014) and for a few other sample words.", "startOffset": 20, "endOffset": 46}, {"referenceID": 18, "context": "ric Multi-sense Skip-gram (NP-MSSG) proposed by Neelakantan et al. (2014) which is currently the only existing approach to learning multi-prototype word representations with Skip-gram.", "startOffset": 48, "endOffset": 74}, {"referenceID": 18, "context": "ric Multi-sense Skip-gram (NP-MSSG) proposed by Neelakantan et al. (2014) which is currently the only existing approach to learning multi-prototype word representations with Skip-gram. We also include in comparison the parametric form of NP-MSSG which has the number of meanings fixed to 3 for all words during the training. All models were trained on the same dataset which is the Wikipedia snapshot by Shaoul & Westbury (2010). For the comparison with MSSG and NP-MSSG we used source code and models released by the authors.", "startOffset": 48, "endOffset": 429}, {"referenceID": 18, "context": "ric Multi-sense Skip-gram (NP-MSSG) proposed by Neelakantan et al. (2014) which is currently the only existing approach to learning multi-prototype word representations with Skip-gram. We also include in comparison the parametric form of NP-MSSG which has the number of meanings fixed to 3 for all words during the training. All models were trained on the same dataset which is the Wikipedia snapshot by Shaoul & Westbury (2010). For the comparison with MSSG and NP-MSSG we used source code and models released by the authors. Neelakantan et al. (2014) limited the number of words for which multi-prototype representations were learned (30000 and 6000 most frequent words) for these models.", "startOffset": 48, "endOffset": 553}, {"referenceID": 11, "context": "The SemEval-2010 dataset was introduced for SemEval2010 Word Sense Induction & Disambiguation competition (Manandhar et al., 2010).", "startOffset": 106, "endOffset": 130}, {"referenceID": 2, "context": "(Boyd-Graber et al., 2014).", "startOffset": 0, "endOffset": 26}, {"referenceID": 17, "context": "We use adjusted Rand index averaged over test words to compare AdaGram models trained with different values of \u03b1 and the models of Neelakantan et al. (2014). The results are provided in Table 7.", "startOffset": 131, "endOffset": 157}], "year": 2015, "abstractText": "Recently proposed Skip-gram model is a powerful method for learning high-dimensional word representations that capture rich semantic relationships between words. However, Skip-gram as well as most prior work on learning word representations does not take into account word ambiguity and maintain only single representation per word. Although a number of Skip-gram modifications were proposed to overcome this limitation and learn multi-prototype word representations, they either require a known number of word meanings or learn them using greedy heuristic approaches. In this paper we propose the Adaptive Skip-gram model which is a nonparametric Bayesian extension of Skip-gram capable to automatically learn the required number of representations for all words at desired semantic resolution. We derive efficient online variational learning algorithm for the model and empirically demonstrate its efficiency on wordsense induction task.", "creator": "LaTeX with hyperref package"}}}