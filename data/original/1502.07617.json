{"id": "1502.07617", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2015", "title": "Online Learning with Feedback Graphs: Beyond Bandits", "abstract": "We study a general class of online learning problems where the feedback is specified by a graph. This class includes online prediction with expert advice and the multi-armed bandit problem, but also several learning problems where the online player does not necessarily observe his own loss. We analyze how the structure of the feedback graph controls the inherent difficulty of the induced $T$-round learning problem. Specifically, we show that any feedback graph belongs to one of three classes: strongly observable graphs, weakly observable graphs, and unobservable graphs. We prove that the first class induces learning problems with $\\widetilde\\Theta(\\alpha^{1/2} T^{1/2})$ minimax regret, where $\\alpha$ is the independence number of the underlying graph; the second class induces problems with $\\widetilde\\Theta(\\delta^{1/3}T^{2/3})$ minimax regret, where $\\delta$ is the domination number of a certain portion of the graph; and the third class induces problems with linear minimax regret. Our results subsume much of the previous work on learning with feedback graphs and reveal new connections to partial monitoring games. We also show how the regret is affected if the graphs are allowed to vary with time.", "histories": [["v1", "Thu, 26 Feb 2015 16:18:53 GMT  (33kb)", "http://arxiv.org/abs/1502.07617v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["noga alon", "nicol\\`o cesa-bianchi", "ofer dekel", "tomer koren"], "accepted": false, "id": "1502.07617"}, "pdf": {"name": "1502.07617.pdf", "metadata": {"source": "CRF", "title": "Online Learning with Feedback Graphs: Beyond Bandits", "authors": ["Noga Alon", "Nicol\u00f2 Cesa-Bianchi", "Ofer Dekel", "Tomer Koren"], "emails": ["nogaa@post.tau.ac.il.", "bianchi@unimi.it.", "oferd@microsoft.com.", "tomerk@technion.ac.il."], "sections": [{"heading": null, "text": "ar X\niv :1\n50 2.\n07 61\n7v 1\n[ cs\n.L G\n] 2\n6 Fe\n\u2217Tel Aviv University, Tel Aviv, Israel, and Microsoft Research, Herzliya, Israel, nogaa@post.tau.ac.il. \u2020Dipartimento di Informatica, Universita\u0300 degli Studi di Milano, Milan, Italy, nicolo.cesabianchi@unimi.it. Parts of this work were done while the author was at Microsoft Research, Redmond. \u2021Microsoft Research, Redmond, Washington; oferd@microsoft.com. \u00a7Technion\u2014Israel Institute of Technology, Haifa, Israel, and Microsoft Research, Herzliya, Israel, tomerk@technion.ac.il. Parts of this work were done while the author was at Microsoft Research, Redmond."}, {"heading": "1 Introduction", "text": "Online learning can be formulated as a repeated game between a randomized player and an arbitrary, possibly adversarial, environment (see, e.g., Cesa-Bianchi and Lugosi, 2006; Shalev-Shwartz, 2011). We focus on the version of the game where, on each round, the player chooses one ofK actions and incurs a corresponding loss. The loss associated with each action on each round is a number between 0 and 1, assigned in advance by the environment. The player\u2019s performance is measured using the game-theoretic notion of regret, which is the difference between his cumulative loss and the cumulative loss of the best fixed action in hindsight. We say that the player is learning if his regret after T rounds is o(T ).\nAfter choosing an action, the player observes some feedback, which enables him to learn and improve his choices on subsequent rounds. A variety of different feedback models are discussed in online learning. The most common is full feedback, where the player gets to see the loss of all the actions at the end of each round. This feedback model is often called prediction with expert advice (Cesa-Bianchi et al., 1997; Littlestone and Warmuth, 1994; Vovk, 1990). For example, imagine a single-minded stock market investor who invests all of his wealth in one of K stocks on each day. At the end of the day, the investor incurs the loss associated with the stock he chose, but he also observes the loss of all the other stocks.\nAnother common feedback model is bandit feedback (Auer et al., 2002), where the player only observes the loss of the action that he chose. In this model, the player\u2019s choices influence the feedback that he receives, so he has to balance an exploration-exploitation trade-off. On one hand, the player wants to exploit what he has learned from the previous rounds by choosing an action that is expected to have a small loss; on the other hand, he wants to explore by choosing an action that will give him the most informative feedback. The canonical example of online learning with bandit feedback is online advertising. Say that we operate an Internet website and we present one of K ads to each user that views the site. Our goal is to maximize the number of clicked ads and therefore we incur a unit loss whenever a user doesn\u2019t click on an ad. We know whether or not the user clicked on the ad we presented, but we don\u2019t know whether he would have clicked on any of the other ads.\nFull feedback and bandit feedback are special cases of a general framework introduced by Mannor and Shamir (2011), where the feedback model is specified by a feedback graph. A feedback graph is a directed graph whose nodes correspond to the player\u2019s K actions. A directed edge from action i to action j (when i = j this edge is called a self-loop) indicates that whenever the player chooses action i he gets to observe the loss associated with action j. The full feedback model is obtained by setting the feedback graph to be the directed clique (including all self-loops, see Fig. 1a). The bandit feedback model is obtained by the graph that only includes the self-loops (see Fig. 1b). Feedback graphs can describe many other interesting online learning scenarios, as discussed below.\nOur main goal is to understand how the structure of the feedback graph controls the inherent difficulty of the induced online learning problem. While regret measures the performance of a specific player or algorithm, the inherent difficulty of the game itself is measured by the minimax regret, which is the regret incurred by an optimal player that plays against\nthe worst-case environment. Freund and Schapire (1997) proves that the minimax regret of the full feedback game is \u0398( \u221a T lnK) while Auer et al. (2002) proves that the minimax re-\ngret of the bandit feedback game is \u0398\u0303( \u221a KT ). Both of these settings correspond to feedback graphs where all of the vertices have self-loops \u2014we say that the player in these settings is self-aware: he observes his own loss value on each round. The minimax regret rates induced by self-aware feedback graphs were extensively studied in Alon et al. (2014). In this paper, we focus on the intriguing situation that occurs when the feedback graph is missing some self-loops, namely, when the player does not always observe his own loss. He is still accountable for the loss on each round, but he does not always know how much loss he incurred. As revealed by our analysis, the absence of self-loops can have a significant impact on the minimax regret of the induced game.\nAn example of a concrete setting where the player is not always self-aware is the apple tasting problem (Helmbold et al., 2000). In this problem, the player examines a sequence of apples, some of which may be rotten. For each apple, he has two possible actions: he can either discard the apple (action 1) or he can ship the apple to the market (action 2). The player incurs a unit loss whenever he discards a good apple and whenever he sends a rotten apple to the market. However, the feedback is asymmetric: whenever the player chooses to discard an apple, he first tastes the apple and obtains full feedback; on the other hand, whenever he chooses to send the apple to the market, he doesn\u2019t taste it and receives no feedback at all. The feedback graph that describes the apple tasting problem is shown in Fig. 1d. Another problem that is closely related to apple tasting is the revealing action or label efficient problem (Cesa-Bianchi and Lugosi, 2006, Example 6.4). In this problem, one action is a special action, called the revealing action, which incurs a constant unit loss. Whenever the player chooses the revealing action, he receives full feedback. Whenever the player chooses any other action, he observes no feedback at all (see Fig. 1e).\nYet another interesting example where the player is not self-aware is obtained by setting the feedback graph to be the loopless clique (the directed clique minus the self-loops, see Fig. 1c). This problem is the complement to the bandit problem: when the player chooses an action, he observes the loss of all the other actions, but he does not observe his own loss. To motivate this, imagine a police officer who wants to prevent crime. On each day, the officer chooses to stand in one of K possible locations. Criminals then show up at some of these locations: if a criminal sees the officer, he runs away before being noticed and the crime is prevented; otherwise, he goes ahead with the crime. The officer gets a unit reward for each crime he prevents,1 and at the end of each day he receives a report of all the crimes that occurred that day. By construction, the officer does not know if his presence prevented a planned crime, or if no crime was planned for that location. In other words, the officer observes everything but his own reward.\nOur main result is a full characterization of the minimax regret of online learning problems defined by feedback graphs. Specifically, we categorize the set of all feedback graphs into three distinct sets. The first is the set of strongly observable feedback graphs, which induce\n1It is easier to describe this example in terms of maximizing rewards, rather than minimizing losses. In our formulation of the problem, a reward of r is mathematically equivalent to a loss of 1\u2212 r.\nonline learning problems whose minimax regret is \u0398\u0303(\u03b11/2T 1/2), where \u03b1 is the independence number of the feedback graph. This slow-growing minimax regret rate implies that the problems in this category are easy to learn. The set of strongly observable feedback graphs includes the set of self-aware graphs, so this result extends the characterization given in Alon et al. (2014). The second category is the set of weakly observable feedback graphs, which induce learning problems whose minimax regret is \u0398\u0303(\u03b41/3T 2/3), where \u03b4 is a new graph-dependent quantity called the weak domination number of the feedback graph. The minimax regret of these problems grows at a faster rate of T 2/3 with the number of rounds, which implies that the induced problems are hard to learn. The third category is the set of unobservable graphs, which induce unlearnable \u0398(T ) online problems.\nOur characterization bears some surprising implications. For example, the minimax regret for the loopless clique is the same, up to constant factors, as the \u0398( \u221a T lnK) minimax regret for the full feedback graph. However, if we start with the full feedback graph (the directed clique with self-loops) and remove a self-loop and an incoming edge from any node (see Fig. 1f), we are left with a weakly observable feedback graph, and the minimax regret jumps to order T 2/3. Another interesting property of our characterization is how the two learnable categories of feedback graphs depend on completely different graph-theoretic quantities: the independence number \u03b1 and the weak domination number \u03b4.\nThe setting of online learning with feedback graphs is closely related to the more general setting of partial monitoring (see, e.g., Cesa-Bianchi and Lugosi, 2006, Section 6.4), where the player\u2019s feedback is specified by a feedback matrix, rather than a feedback graph. Partial monitoring games have also been categorized into three classes: easy problems with T 1/2 regret, hard problems with T 2/3 regret, and unlearnable problems with linear regret (Barto\u0301k et al., 2014, Theorem 2). If the loss values are chosen from a finite set (say {0, 1}), then bandit feedback, apple tasting feedback, and the revealing action feedback models are all known to be special cases of partial monitoring. In fact, in Appendix D we show that any problem in our setting (with binary losses) can be reduced to the partial monitoring setting. Nevertheless, the characterization presented in this paper has several clear advantages over the more general characterization of partial monitoring games. First, our regret bounds are minimax optimal not only with respect to T , but also with respect to the other relevant problem parameters. Second, we obtain our upper bounds with a simple and efficient algorithm. Third, our characterization is stated in terms of simple and intuitive combinatorial properties of the problem.\nThe paper is organized as follows. In Section 2 we define the problem setting and state our main results. In Section 3 we describe our player algorithm and prove upper bounds on the minimax regret. In Section 4 we prove matching lower bounds on the minimax regret. Finally, in Section 5 we extend our analysis to the case where the feedback graph is neither fixed nor known in advance."}, {"heading": "2 Problem Setting and Main Results", "text": "Let G = (V,E) be a directed feedback graph over the set of actions V = {1, . . . , K}. For each i \u2208 V , let N in(i) = {j \u2208 V : (j, i) \u2208 E} be the in-neighborhood of i in G, and let Nout(i) = {j \u2208 V : (i, j) \u2208 E} be the out-neighborhood of i in G. If i has a self-loop, that is (i, i) \u2208 E, then i \u2208 N in(i) and i \u2208 Nout(i).\nBefore the game begins, the environment privately selects a sequence of loss functions \u21131, \u21132. . . . , where \u2113t : V 7\u2192 [0, 1] for each t \u2265 1. On each round t = 1, 2, . . . , the player randomly chooses an action It \u2208 V and incurs the loss \u2113t(It). At the end of round t, the player receives the feedback { ( j, \u2113t(j) ) : j \u2208 Nout(It)}. In words, the player observes the loss associated with each vertex in the out-neighborhood of the chosen action It. In particular, if It has no self-loop, then the player\u2019s loss \u2113t(It) remains unknown, and if the out-neighborhood of It is empty, then the player does not observe any feedback on that round. The player\u2019s expected regret against a specific loss sequence \u21131, . . . , \u2113T is defined as E [\u2211T t=1 \u2113t(It) ] \u2212 mini\u2208V \u2211T t=1 \u2113t(i). The inherent difficulty of the T -round online learning problem induced by the feedback graph G is measured by the minimax regret, denoted by R(G, T ) and defined as the minimum over all randomized player strategies, of the maximum over all loss sequences, of the player\u2019s expected regret."}, {"heading": "2.1 Main Results", "text": "The main result of this paper is a complete characterization of the minimax regret when the feedback graph G is fixed and known to the player. Our characterization relies on various properties of G, which we define below.\nDefinition (Observability). In a directed graph G = (V,E) a vertex i \u2208 V is observable if N in(i) 6= \u2205. A vertex is strongly observable if either {i} \u2286 N in(i), or V \\ {i} \u2286 N in(i), or both. A vertex is weakly observable if it is observable but not strongly. A graph G is observable if all its vertices are observable and it is strongly observable if all its vertices are strongly observable. A graph is weakly observable if it is observable but not strongly.\nIn words, a vertex is observable if it has at least one incoming edge (possibly a selfloop), and it is strongly observable if it has either a self-loop or incoming edges from all other vertices. Note that a graph with all of the self-loops is necessarily strongly observable. However, a graph that is missing some of its self-loops may or may not be observable or strongly observable.\nDefinition (Weak Domination). In a directed graph G = (V,E) with a set of weakly observable vertices W \u2286 V , a weakly dominating set D \u2286 V is a set of vertices that dominates W . Namely, for any w \u2208 W there exists d \u2208 D such that w \u2208 Nout(d). The weak domination number of G, denoted by \u03b4(G), is the size of the smallest weakly dominating set.\nOur characterization also relies on a more standard graph-theoretic quantity. An independent set S \u2286 V is a set of vertices that are not connected by any edges. Namely, for any u, v \u2208 S, u 6= v it holds that (u, v) 6\u2208 E. The independence number \u03b1(G) of G is the size of its largest independent set. Our characterization of the minimax regret rates is given by the following theorem.\nTheorem 1. Let G = (V,E) be a feedback graph with |V | \u2265 2, fixed and known in advance. Let \u03b1 = \u03b1(G) denote its independence number and let \u03b4 = \u03b4(G) denote its weak domination number. Then the minimax regret of the T -round online learning problem induced by G, where T \u2265 |V |3, is (i) R(G, T ) = \u0398\u0303(\u03b11/2 T 1/2) if G is strongly observable; (ii) R(G, T ) = \u0398\u0303(\u03b41/3 T 2/3) if G is weakly observable; (iii) R(G, T ) = \u0398(T ) if G is not observable.\nAs mentioned above, this characterization has some interesting consequences. Any strongly observable graph can be turned into a weakly observable graph by removing at most two edges. Doing so will cause the minimax regret rate to jump from order \u221a T to order T 2/3. Even more remarkably, removing these edges will cause the minimax regret to switch from depending on the independence number to depending on the weak domination number. A striking example of this abrupt change is the loopy star graph, which is the union of the directed star (Fig. 1e) and all of the self-loops (Fig. 1b). In other words, this example is a multi-armed bandit problem with a revealing action. The independence number of this\nAlgorithm 1: Exp3.G: online learning with a feedback graph\nParameters: Feedback graph G = (V,E), learning rate \u03b7 > 0, exploration set U \u2286 V , exploration rate \u03b3 \u2208 [0, 1]\nLet u be the uniform distribution over U ; Initialize q1 to the uniform distribution over V ; For round t = 1, 2, . . .\nCompute pt = (1\u2212 \u03b3)qt + \u03b3u; Draw It \u223c pt, play It and incur loss \u2113t(It); Observe {(i, \u2113t(i)) : i \u2208 Nout(It)}; Update\n\u2200 i \u2208 V \u2113\u0302t(i) = \u2113t(i) Pt(i) I { i \u2208 Nout(It) } , with Pt(i) = \u2211\nj\u2208N in(i)\npt(j) ; (1)\n\u2200 i \u2208 V qt+1(i) = qt(i) exp(\u2212\u03b7\u2113\u0302t(i))\u2211\nj\u2208V qt(j) exp(\u2212\u03b7\u2113\u0302t(j)) ; (2)\ngraph is K \u2212 1, while its weak domination number is 1. Since the loopy star is strongly observable, it induces a game with minimax regret \u0398\u0303( \u221a TK). However, removing a single loop from the feedback graph turns it into a weakly observable graph, and its minimax regret rate changes to \u0398\u0303(T 2/3) (with no polynomial dependence on K)."}, {"heading": "3 The Exp3.G Algorithm", "text": "The upper bounds for weakly and strongly observable graphs in Theorem 1 are both achieved by an algorithm we introduce, called Exp3.G (see Algorithm 1), which is a variant of the Exp3-SET algorithm for undirected feedback graphs (Alon et al., 2013).\nSimilarly to Exp3 and Exp3.SET, our algorithm uses importance sampling to construct unbiased loss estimates with controlled variance. Indeed, notice that Pt(i) = P(i \u2208 Nout(It)) is simply the probability of observing the loss \u2113t(i) upon playing It \u223c pt. Hence, \u2113\u0302t(i) is an unbiased estimate of the true loss \u2113t(i), and for all t and i \u2208 V we have\nEt[\u2113\u0302t(i)] = \u2113t(i) and Et[\u2113\u0302t(i) 2] =\n\u2113t(i) 2 Pt(i) . (3)\nThe purpose of the exploration distribution u is to control the variance of the loss estimates by providing a lower bound on Pt(i) for those i \u2208 V in the support of u; this ingredient will turn out to be essential to our analysis.\nWe now state the upper bounds on the regret achieved by Algorithm 1.\nTheorem 2. Let G = (V,E) be a feedback graph with K = |V |, independence number \u03b1 = \u03b1(G) and weakly dominating number \u03b4 = \u03b4(G). Let D be a weakly dominating set such\nthat |D| = \u03b4. The expected regret of Algorithm 1 on the online learning problem induced by G satisfies the following:\n(i) if G is strongly observable, then for U = V , \u03b3 = min {(\n1 \u03b1T )1/2 , 1 2 } and \u03b7 = 2\u03b3, the\nexpected regret against any loss sequence is O(\u03b11/2T 1/2 ln(KT )); (ii) if G is weakly observable and T \u2265 K3 ln(K)/\u03b42, then for U = D, \u03b3 = min {( \u03b4 lnK T )1/3 , 1 2 }\nand \u03b7 = \u03b3 2 \u03b4 , the expected regret against any loss sequence is O\n( (\u03b4 lnK)1/3T 2/3 ) .\nIn the previously studied self-aware case (i.e., strongly observable with self-loops), our result matches the bounds of Alon et al. (2014); Koca\u0301k et al. (2014). The tightness of our bounds in all cases is discussed in Section 4 below."}, {"heading": "3.1 A Tight Bound for the Loopless Clique", "text": "One of the simplest examples of a feedback graph that is not self-aware is the loopless clique (Fig. 1c). This graph is strongly observable with an independence number of 1, so Theorem 2 guarantees that the regret of Algorithm 1 in the induced game is O( \u221a T ln(KT )). However, in this case we can do better than Theorem 2 and prove (see Appendix C) that the regret of the same algorithm is actually O( \u221a T lnK), which is the same as the regret rate of the full feedback game (Fig. 1a). In other words, if we start with full feedback and then hide the player\u2019s own loss, the regret rate remains the same (up to constants).\nTheorem 3. For any sequence of loss functions \u21131, . . . , \u2113T , where \u2113t : V 7\u2192 [0, 1], the regret of Algorithm 1, with the loopless clique feedback graph and with parameters \u03b7 = \u221a (lnK)/(2T ) and \u03b3 = 2\u03b7, is upper-bounded by 5 \u221a T lnK."}, {"heading": "3.2 Refined Second-order Bound for Hedge", "text": "Our analysis of Exp3.G builds on a new second-order regret bound for the classic Hedge algorithm.2 Recall that Hedge (Freund and Schapire, 1997) operates in the full feedback setting (see Fig. 1a), where at time t the player has access to losses \u2113s(i) for all s < t and i \u2208 V . Hedge draws action It from the distribution pt defined by\n\u2200 i \u2208 V , qt(i) = exp\n( \u2212 \u03b7\u2211t\u22121s=1 \u2113s(i) ) \u2211\nj\u2208V exp ( \u2212 \u03b7\u2211t\u22121s=1 \u2113s(j) ) , (4)\nwhere \u03b7 is a positive learning rate. The following novel regret bound is key to proving that our algorithm achieves tight bounds over the regret (to within logarithmic factors).\nLemma 4. Let q1, . . . , qT be the probability vectors defined by Eq. (4) for a sequence of loss functions \u21131, . . . , \u2113T such that \u2113t(i) \u2265 0 for all t = 1, . . . , T and i \u2208 V . For each t, let St be\n2A second-order regret bound controls the regret with an expression that depends on a quantity akin to the second moment of the losses.\na subset of V such that \u2113t(i) \u2264 1/\u03b7 for all i \u2208 St. Then, for any i\u22c6 \u2208 V it holds that T\u2211\nt=1\n\u2211\ni\u2208V\nqt(i)\u2113t(i)\u2212 T\u2211\nt=1\n\u2113t(i \u22c6) \u2264 lnK\n\u03b7 + \u03b7\nT\u2211\nt=1\n( \u2211\ni\u2208St\nqt(i) ( 1\u2212 qt(i) ) \u2113t(i) 2 + \u2211\ni/\u2208St\nqt(i)\u2113t(i) 2 ) .\nSee Appendix A for a proof of this result. The standard second-order regret bound of Hedge (see, e.g., Cesa-Bianchi et al., 2007) is obtained by setting St = \u2205 for all t. Therefore, our bound features a slightly improved dependence (i.e., the 1 \u2212 qt(i) factors) on actions whose losses do not exceed 1/\u03b7. Indeed, in the analysis of Exp3.G, we apply the above lemma to the loss estimates \u2113\u0302t(i), and include in the sets St all strongly observable vertices i that do not have a self-loop. This allows us to gain a finer control on the variances \u2113t(i) 2 / Pt(i) of such vertices."}, {"heading": "3.3 Proof of Theorem 2", "text": "We now turn to prove Theorem 2. For the proof, we need the following graph-theoretic result, which is a variant of Alon et al. (2014, Lemma 16); for completeness, we include a proof in Appendix A.\nLemma 5. Let G = (V,E) be a directed graph with |V | = K, in which each node i \u2208 V is assigned a positive weight wi. Assume that \u2211 i\u2208V wi \u2264 1, and that wi \u2265 \u01eb for all i \u2208 V for some constant 0 < \u01eb < 1 2 . Then\n\u2211\ni\u2208V\nwi wi + \u2211 j\u2208N in(i) wj \u2264 4\u03b1 ln 4K \u03b1\u01eb\nwhere \u03b1 = \u03b1(G) is the independence number of G.\nProof of Theorem 2. Without loss of generality, we may assume that K \u2265 2. The proof proceeds by applying Lemma 4 and upper bounding the second-order terms it introduces. Indeed, since the distributions q1, q2, . . . generated by Algorithm 1 via Eq. (2) are of the form given by Eq. (4), with the losses \u2113t replaced by the nonnegative loss estimates \u2113\u0302t, we may apply Lemma 4 to these distributions and loss estimates. The way we apply the lemma differs between the strongly observable and weakly observable cases, and we treat each separately.\nFirst, assume that G is strongly observable, implying that the exploration distribution u is uniform on V . Notice that for any i \u2208 V without a self-loop, namely with i /\u2208 N in(i), we have j \u2208 N in(i) for all j 6= i, and so Pt(i) = 1 \u2212 pt(i). On the other hand, by the definition of pt and since \u03b7 = 2\u03b3 and K \u2265 2, we have pt(i) = (1\u2212 \u03b3)qt(i) + \u03b3K \u2264 1\u2212 \u03b3 + \u03b3 2 = 1\u2212 \u03b7, so that Pt(i) \u2265 \u03b7. Thus, we can apply Lemma 4 with St = S = {i : i /\u2208 N in(i)} to the vectors \u2113\u03021, . . . , \u2113\u0302T and take expectations, and obtain that\nE\n[ T\u2211\nt=1\n\u2211\ni\u2208V\nqt(i)Et[\u2113\u0302t(i)]\u2212 T\u2211\nt=1\nEt[\u2113\u0302t(i \u22c6)] ] \u2264 lnK\n\u03b7\n+ \u03b7\nT\u2211\nt=1\nE\n[ \u2211\ni\u2208S\nqt(i)(1\u2212 qt(i))Et[\u2113\u0302t(i)2] + \u2211\ni/\u2208S\nqt(i)Et[\u2113\u0302t(i) 2]\n]\nfor any fixed i\u22c6 \u2208 V . Recalling Eq. (3) and Pt(i) = 1\u2212 pt(i) for all i \u2208 S, we get\nE\n[ T\u2211\nt=1\n\u2211\ni\u2208V\nqt(i)\u2113t(i)\n] \u2212 T\u2211\nt=1\n\u2113t(i \u22c6) \u2264 lnK\n\u03b7 + \u03b7\nT\u2211\nt=1\nE\n[ \u2211\ni\u2208S\nqt(i) 1\u2212 qt(i) 1\u2212 pt(i) + \u2211\ni/\u2208S\nqt(i) Pt(i)\n] .\nThe sum over i \u2208 S on the right-hand side is bounded as follows: T\u2211\nt=1\n\u2211\ni\u2208S\nqt(i) 1\u2212 qt(i) 1\u2212 pt(i) \u2264 2 T\u2211\nt=1\n\u2211\ni\u2208S\nqt(i) \u2264 2T .\nFor the second sum, recall that any i /\u2208 S has a self-loop in the feedback graph, and also that pt(i) \u2265 \u03b3K as a result of mixing in the uniform distribution over V . Hence, we can use pt(i) \u2265 (1\u2212 \u03b3)qt(i) \u2265 12qt(i) and apply Lemma 5 with \u01eb = \u03b3 K that yields\n\u2211\ni/\u2208S\nqt(i) Pt(i) \u2264 2\n\u2211\ni/\u2208S\npt(i) Pt(i) \u2264 8\u03b1 ln K\n2\n4\u03b3 .\nPutting everything together, and using the fact that pt(i) \u2264 qt(i) + \u03b3u(i) to obtain \u2211\ni\u2208V\npt(i)\u2113t(i) \u2264 \u2211\ni\u2208V\nqt(i)\u2113t(i) + \u03b3 , (5)\nresults with the regret bound\nE\n[ T\u2211\nt=1\n\u2211\ni\u2208V\npt(i)\u2113t(i)\n] \u2212 T\u2211\nt=1\n\u2113t(i \u22c6) \u2264 \u03b3T + lnK\n\u03b7 + 2\u03b7T\n( 1 + 4\u03b1 ln K2\n4\u03b3\n) .\nSubstituting the chosen values of \u03b7 and \u03b3 gives the first claim of the theorem. Next, assume that G is only weakly observable. Let D \u2286 V be a weakly dominating set supporting the exploration distribution u, with |D| = \u03b4. Similarly to the strongly observable case, we apply Lemma 4 to the vectors \u2113\u03021, . . . , \u2113\u0302T , but in this case we set St = \u2205 for all t. Using Eqs. (3) and (5) and proceeding exactly as the strongly observable case, we obtain\nE\n[ T\u2211\nt=1\n\u2211\ni\u2208V\npt(i)\u2113t(i)\n] \u2212 T\u2211\nt=1\n\u2113t(i \u22c6) \u2264 \u03b3T + lnK\n\u03b7 + \u03b7\nT\u2211\nt=1\nE\n[ \u2211\ni\u2208V\nqt(i) Pt(i)\n]\nfor any fixed i\u22c6 \u2208 V . In order to bound the expectation in the right-hand side, consider again the set S = {i : i /\u2208 N in(i)} of vertices without a self-loop, and observe that Pt(i) =\u2211\nj\u2208N in(i)pt(j) \u2265 \u03b3\u03b4 for all i \u2208 S. Indeed, if i is weakly observable then there exists some k \u2208 D such that k \u2208 N in(i) and pt(k) \u2265 \u03b3\u03b4 because the exploration distribution u is uniform over D; if i is strongly observable then the same holds since i does not have a self-loop and thus must be dominated by all other vertices in the graph. Hence,\n\u2211\ni\u2208V\nqt(i) Pt(i) =\n\u2211\ni\u2208S\nqt(i) Pt(i) + \u2211\ni/\u2208S\nqt(i) Pt(i) \u2264 \u03b4 \u03b3 + 2K ,\nwhere we used Pt(i) \u2265 pt(i) \u2265 (1\u2212\u03b3)qt(i) \u2265 12qt(i) to bound the sum over the vertices having a self-loop. Therefore, we may write\nE\n[ T\u2211\nt=1\n\u2211\ni\u2208V\npt(i)\u2113t(i)\n] \u2212 T\u2211\nt=1\n\u2113t(i \u22c6) \u2264 \u03b3T + lnK\n\u03b7 +\n\u03b7\u03b4\n\u03b3 T + 2\u03b7KT .\nSubstituting our choices of \u03b7 and \u03b3, we obtain the second claim of the theorem."}, {"heading": "4 Lower Bounds", "text": "In this section we prove lower bounds on the minimax regret for non-observable and weakly observable graphs. Together with Theorem 2 and the known lower bound of \u2126 (\u221a \u03b1(G)T ) for strongly observable graphs (Alon et al., 2014, Theorem 5),3 these results complete the proof of Theorem 1. We remark that their lower bound applies when T \u2265 \u03b1(G)3, which includes our regime of interest. We begin with a simple lower bound for non-observable feedback graphs.\nTheorem 6. If G = (V,E) is not observable and |V | \u2265 2, then for any player algorithm there exists a sequence of loss functions \u21131, \u21132, . . . : V 7\u2192 [0, 1] such that the player\u2019s expected regret is at least 1\n4 T .\nThe proof is straightforward: if G is not observable, then it is possible to find a vertex of G with no incoming edges; the environment can then set the loss of this vertex to be either 0 or 1 on all rounds of the game, and the player has no way of knowing which is the case. For the formal proof, refer to Appendix B. Next, we prove a lower bound for weakly observable feedback graphs.\nTheorem 7. If G = (V,E) is weakly observable with K = |V | \u2265 2 and weak domination number \u03b4 = \u03b4(G), then for any randomized player algorithm and for any time horizon T there exists a sequence of loss functions \u21131, . . . , \u2113T : V 7\u2192 [0, 1] such that the player\u2019s expected regret is at least 1\n150\n( \u03b4/ ln2K )1/3 T 2/3.\nThe proof relies on the following graph-theoretic result, relating the notions of domination and independence in directed graphs.\nLemma 8. Let G = (V,E) be a directed graph over |V | = n vertices, and let W \u2286 V be a set of vertices whose minimal dominating set is of size k. Then, W contains an independent set U of size at least 1\n50 k/ lnn, with the property that any vertex of G dominates at most lnn\nvertices of U .\n3While Alon et al. (2014) only consider the special case of graphs that have self-loops at all vertices, their lower bound applies to any strongly observable graph: we can simply add any missing self-loops to the graph, without changing its independence number \u03b1. The resulting learning problem, whose minimax regret is \u2126 (\u221a \u03b1T ) , is only easier for the player who may ignore the additional feedback.\nProof. If k < 50 lnn the statement is vacuous; hence, in what follows we assume k \u2265 50 lnn. Let \u03b2 = (2 lnn)/k < 1. Our first step is to prove thatW contains a non-empty setR such that each vertex of G dominates at most \u03b2 fraction of R, namely such that |Nout(v) \u2229R| \u2264 \u03b2|R| for all v \u2208 V . To prove this, consider the following iterative process: initialize R = W , and as long as there exists a vertex v \u2208 V such that |Nout(v)\u2229R| > \u03b2|R|, remove all the vertices v dominates from R. Notice that the process cannot continue for k (or more) iterations, since each step the size of R decreases at least by a factor of 1\u2212 \u03b2, so after k \u2212 1 steps we have |R| \u2264 n(1 \u2212 \u03b2)k\u22121 < ne\u2212\u03b2k/2 = 1. On the other hand, the process cannot end with R = \u2205, as in that case the vertices v found along the way form a dominating set of W whose size is less than k, which is a contradiction to our assumption. Hence, the set R at the end of process must be non-empty and satisfy |Nout(v) \u2229 R| \u2264 \u03b2|R| for all v \u2208 V , as claimed.\nNext, consider a random set S \u2286 R formed by picking a multiset S\u0303 of m = \u230a 1 10\u03b2 \u230b elements from R independently and uniformly at random (with replacement), and discarding any repeating elements. Notice that m \u2264 1\n10 |R|, as |R| \u2265 1 \u03b2 |Nout(v) \u2229 R| for any v \u2208 V , and for\nsome v the right-hand side is non-zero. The proof proceeds via the probabilistic method: we will show that with positive probability, S contains an independence set as required, which would give the theorem.\nWe first observe the following properties of the set S.\nClaim. With probability at least 3 4 , it holds that |S| \u2265 1 10 m.\nTo see this, note that each element from R is not included in S\u0303 with probability (1\u2212 1 r )m \u2264 e\u2212m/r with r = |R|. Since m \u2264 1 10 r, the expected size of S is at least r(1 \u2212 e\u2212m/r) = re\u2212m/r(em/r \u2212 1) \u2265 me\u2212m/r \u2265 9 10 m, where both inequality use ex \u2265 x + 1. Since always |S| \u2264 m, Markov\u2019s inequality shows that |S| \u2265 1 10 m with probability at least 3 4 ; otherwise,\nwe would have E [ |S| ] \u2264 1\n10 m+mP\n( |S| \u2265 1 10 m ) < 9 10 m.\nClaim. With probability at least 3 4 , we have |Nout(v) \u2229 S| \u2264 lnn for all v \u2208 V .\nIndeed, fix some v \u2208 V and recall that v dominates at most a \u03b2 fraction of the vertices in R, so each element of S\u0303 (that was chosen uniformly at random from R) is dominated by v with probability at most \u03b2. Hence, the random variable X\u0303v = |Nout(v) \u2229 S\u0303| has a binomial distribution Bin(m, p) with p \u2264 \u03b2. By a standard binomial tail bound,\nP(X\u0303v \u2265 lnn) \u2264 ( m\nlnn\n) \u03b2 lnn \u2264 (m\u03b2)lnn \u2264 e\u22122 lnn = 1\nn2 .\nThe same bound holds also for the random variable Xv = |Nout(v) \u2229 S|, that can only be smaller than X\u0303v. Our claim now follows from a union bound over all v \u2208 V . Claim. With probability at least 3\n4 , we have 1 |S| \u2211 v\u2208S |Nout(v) \u2229 S| \u2264 12 .\nTo obtain this, we note that for each v \u2208 V the random variable Xv = |Nout(v) \u2229 S| defined above has E[Xv] \u2264 E[X\u0303v] \u2264 m\u03b2 \u2264 110 , and therefore E [ 1 |S| \u2211 v\u2208S Xv ] \u2264 1 10 . By Markov\u2019s inequality we then have 1 |S| \u2211 v\u2208S Xv > 1 2 with probability less than 1 5 , which gives the claim.\nThe three claims together imply that there exists a set S \u2286 W of size at least 1 10 m, such that any v \u2208 V dominates at most lnn vertices of S, and the average degree of the induced undirected graph over S is at most 1. Hence, by Tura\u0301n\u2019s Theorem,4 S contains an independent set U of size 1\n20 m \u2265 1 50 k/ lnn. This concludes the proof, as each v \u2208 V\ndominates at most lnn vertices of U .\nGiven Lemma 8, the idea of the proof is quite intuitive; here we only give a sketch of the proof, and defer the formal details to Appendix B.\nProof of Theorem 7 (sketch). First, we use the lemma to find an independent set U of weakly observable vertices of size \u2126\u0303(\u03b4), with the crucial property that each vertex in the entire graph dominates at most O\u0303(1) vertices of U . Then, we embed in the set U a hard instance of the stochastic multiarmed bandit problem, in which the optimal action has expected loss smaller by \u01eb than the expected loss of the other actions in U . To all other vertices of the graph, we assign the maximal loss of 1. Hence, unless the player is able to detect the optimal action, his regret cannot be better than \u2126(\u01ebT ).\nThe main observation is that, due to the properties of the set U , in order to obtain accurate estimates of the losses of all actions in U the player has to use \u2126\u0303(\u03b4) different actions outside of U and pick each for \u2126(1/\u01eb2) times. Since each such action entails a constant instantaneous regret, the player has to pay an \u2126(\u03b4/\u01eb2) penalty in his cumulative regret for exploration. The overall regret is thus of order \u2126 ( min{\u01ebT, \u03b4/\u01eb2} ) , which is maximized at \u01eb = (\u03b4/T )1/3 and gives the stated lower bound."}, {"heading": "5 Time-Varying Feedback Graphs", "text": "The setting discussed above can be generalized by allowing the feedback graphs to change arbitrarily from round to round (see Mannor and Shamir (2011); Alon et al. (2013); Koca\u0301k et al. (2014)). Namely, the environment chooses a sequence of feedback graphs G1, . . . , GT along with the sequence of loss functions. We consider two different variants of this setting: in the informed model, the player observes Gt at the beginning of round t, before drawing the action It. In the harder uninformed model, the player observes Gt at the end of round t, after drawing It. In this section, we discuss how our algorithm can be modified to handle time-varying feedback graphs, and whether this generalization increases the minimax regret of the induced online learning problem.\nStrongly Observable. If G1, . . . , GT are all strongly observable, Algorithm 1 and its analysis can be adapted to the time-varying setting (both informed and uninformed) with only a few cosmetic modifications. Specifically, we replace G with Gt, to define time-dependent neighborhoods, Noutt and N in t , in Eq. (1) of the algorithm. This modification holds in both the informed and uninformed models because the structure of the feedback graph is only used\n4Tura\u0301n\u2019s Theorem (e.g., Alon and Spencer, 2008) states that in any undirected graph whose average degree is d, there is an independent set of size n/(d+ 1).\nto update qt+1, which takes place after the action It is chosen. Moreover, the upper-bound in Theorem 2 can be adapted to the time-varying model by replacing \u03b1 with 1\nT \u2211T t=1 \u03b1t, where\neach \u03b1t is the independence number of the corresponding Gt (e.g., using a doubling trick, or an adaptive learning rate as in Koca\u0301k et al. (2014)).\nWeakly Observable, Informed. If G1, . . . , GT are all weakly observable, Algorithm 1 can again be adapted to the informed time-varying model, but the required modification is more substantial than before, and in particular, relies on the fact that Gt is known before the prediction on round t is made. The exploration set U must change from round to round, according to the feedback graph. Specifically, we choose the exploration set on round t to be Dt, the smallest weakly dominating set in Gt. We then define ut to be the uniform distribution over this set, and pt = (1 \u2212 \u03b3)qt + \u03b3ut. Again, via standard techniques, the upper-bound in Theorem 2 can be adapted to this setting by replacing \u03b4 with 1\nT \u2211T t=1 \u03b4t,\nwhere \u03b4t = |Dt|.\nWeakly Observable, Uninformed. So far, we discussed cases where the minimax regret rates of our problem do not increase when we allow the feedback graphs to change from round to round. However, if G1, . . . , GT are all weakly observable and they are revealed according to the uninformed model, then the minimax regret can strictly increase. Recall that Theorem 1 states that the minimax regret for a constant weakly observable graph is \u0398\u0303(\u03b41/3 T 2/3), where \u03b4 is the size of the smallest weakly dominating set. We now show that the minimax regret in the analogous uninformed setting is \u0398\u0303(K1/3 T 2/3), where K is the number of actions. The O\u0303(K1/3 T 2/3) upper bound is obtained by running Algorithm 1 with uniform exploration over the entire set of actions (namely, U = V ). To show that this bound is tight, we state the following matching lower bound.\nTheorem 9. For any randomized player strategy in the uninformed feedback model, there exists a sequence of weakly observable graphs G1, . . . , GT over a set V of K \u2265 4 actions with \u03b4(Gt) = \u03b1(Gt) = 1 for all t, and a sequence of loss functions \u21131, . . . , \u2113T : V 7\u2192 [0, 1], such that the player\u2019s expected regret is at least 1\n16 K1/3T 2/3.\nWe sketch the proof below, and present it in full detail in Appendix B.\nProof (sketch). For each t = 1, . . . , T , construct the graph Gt as follows: start with the complete graph over K vertices (that includes all self-loops), and then remove the self-loop and all edges incoming to i = 1 except of a single edge incoming from some vertex jt 6= 1 chosen arbitrarily. Notice that the resulting graph is weakly observable (each vertex is observable, but i = 1 is only weakly observable), has \u03b4(Gt) = 1 since jt dominates the entire graph, and \u03b1(Gt) = 1 as each two vertices are connected by at least one edge. However, for observing the loss of i = 1 the player has to \u201cguess\u201d the revealing action jt, that might change arbitrarily from round to round. This random guessing of one out of \u2126(K) actions introduces the K1/3 factor in the resulting bound."}, {"heading": "Acknowledgements", "text": "We thank Se\u0301bastien Bubeck for helpful discussions during various stages of this work, and Ga\u0301bor Barto\u0301k for clarifying the connections to observability in partial monitoring."}, {"heading": "A Additional Proofs", "text": "A.1 Proof of Lemma 4\nIn order to prove our new regret bound for Hedge, we first state and prove the standard second-order regret bound for this algorithm.\nLemma 10. For any \u03b7 > 0 and for any sequence \u21131, . . . , \u2113T of loss functions such that \u2113t(i) \u2265 \u22121/\u03b7 for all t and i, the probability vectors q1, . . . , qT of Eq. (4) satisfy\nT\u2211\nt=1\n\u2211\ni\u2208V\nqt(i)\u2113t(i)\u2212min k\u2208V\nT\u2211\nt=1\n\u2113t(k) \u2264 lnK\n\u03b7 + \u03b7\nT\u2211\nt=1\n\u2211\ni\u2208V\nqt(i)\u2113t(i) 2 .\nProof. The proof follows the standard analysis of exponential weighting schemes: let wt(i) = exp ( \u2212\u03b7\u2211t\u22121s=1 \u2113s(i) ) and let Wt = \u2211 i\u2208V wt(i). Then qt(i) = wt(i)/Wt and we can write\nWt+1 Wt\n= \u2211\ni\u2208V\nwt+1(i)\nWt\n= \u2211\ni\u2208V\nwt(i) exp ( \u2212\u03b7 \u2113t(i) )\nWt\n= \u2211\ni\u2208V\nqt(i) exp ( \u2212\u03b7 \u2113t(i) )\n\u2264 \u2211\ni\u2208V\nqt(i) ( 1\u2212 \u03b7\u2113t(i) + \u03b72\u2113t(i)2 ) (using ex \u2264 1 + x+ x2 for all x \u2264 1)\n\u2264 1\u2212 \u03b7 \u2211\ni\u2208V\nqt(i)\u2113t(i) + \u03b7 2 \u2211\ni\u2208V\nqt(i)\u2113t(i) 2 .\nTaking logs, using ln(1\u2212 x) \u2264 \u2212x for all x \u2265 0, and summing over t = 1, . . . , T yields\nln WT+1 W1\n\u2264 T\u2211\nt=1\n\u2211\ni\u2208V\n( \u2212\u03b7 qt(i)\u2113t(i) + \u03b72 qt(i)\u2113t(i)2 ) .\nMoreover, for any fixed action k, we also have\nln WT+1 W1 \u2265 ln wT+1(k) W1\n= \u2212 \u03b7 T\u2211\nt=1\n\u2113t(k)\u2212 lnK .\nPutting together and rearranging gives the result.\nWe can now prove Lemma 4, restated here for the convenience of the reader.\nLemma 4 (restated). Let q1, . . . , qT be the probability vectors defined by Eq. (4) for a sequence of loss functions \u21131, . . . , \u2113T such that \u2113t(i) \u2265 0 for all t = 1, . . . , T and i \u2208 V . For each t, let St be a subset of V such that \u2113t(i) \u2264 1/\u03b7 for all i \u2208 St. Then, it holds that\nT\u2211\nt=1\n\u2211\ni\u2208V\nqt(i)\u2113t(i)\u2212min k\u2208V\nT\u2211\nt=1\n\u2113t(k) \u2264 lnK\n\u03b7 + \u03b7\nT\u2211\nt=1\n( \u2211\ni\u2208St\nqt(i) ( 1\u2212 qt(i) ) \u2113t(i) 2 + \u2211\ni/\u2208St\nqt(i)\u2113t(i) 2 ) .\nProof. For all t, let \u2113\u0304t = \u2211 i\u2208St pt(i)\u2113t(i) for which \u2113\u0304t \u2264 1/\u03b7 by construction. Notice that executing Hedge on the loss vectors \u21131, . . . , \u2113T is equivalent to executing in on vectors \u2113\u20321, . . . , \u2113 \u2032 T with \u2113 \u2032 t(i) = \u2113t(i)\u2212 \u2113\u0304t for all i. Applying Lemma 10 for the latter case (notice that \u2113\u2032t(i) \u2265 \u22121/\u03b7 for all t and i), we obtain T\u2211\nt=1\n\u2211\ni\u2208V\npt(i)\u2113t(i)\u2212min k\u2208V\nT\u2211\nt=1\n\u2113t(k) = T\u2211\nt=1\n\u2211\ni\u2208V\npt(i)\u2113 \u2032 t(i)\u2212min\nk\u2208V\nT\u2211\nt=1\n\u2113\u2032t(k)\n\u2264 lnK \u03b7\n+ \u03b7 T\u2211\nt=1\n\u2211\ni\u2208V\npt(i)\u2113 \u2032 t(i) 2\n= lnK\n\u03b7 + \u03b7\nT\u2211\nt=1\n\u2211\ni\u2208V\npt(i)(\u2113t(i)\u2212 \u2113\u0304t)2 .\nOn the other hand, for all t,\n\u2211\ni\u2208St\npt(i)(\u2113t(i)\u2212 \u2113\u0304t)2 = \u2211\ni\u2208St\npt(i)\u2113t(i) 2 \u2212\n(\u2211\ni\u2208St\npt(i)\u2113t(i) )2\n\u2264 \u2211\ni\u2208St\npt(i)\u2113t(i) 2 \u2212\n\u2211\ni\u2208St\npt(i) 2\u2113t(i) 2\n= \u2211\ni\u2208St\npt(i)(1\u2212 pt(i))\u2113t(i)2\nwhere the inequality follows from the non-negativity of the losses \u2113t(i). Also, since \u2113t(i) > 1/\u03b7 \u2265 \u2113\u0304t for all i /\u2208 St, we also have\n\u2211\ni/\u2208St\npt(i)(\u2113t(i)\u2212 \u2113\u0304t)2 \u2264 \u2211\ni/\u2208St\npt(i)\u2113t(i) 2 .\nCombining the inequalities gives the lemma.\nA.2 Proof of Lemma 5\nLemma 5 (restated). Let G = (V,E) be a directed graph with |V | = K, in which each node i \u2208 V is assigned a positive weight wi. Assume that \u2211 i\u2208V wi \u2264 1, and that wi \u2265 \u01eb for all i \u2208 V for some constant 0 < \u01eb < 1 2 . Then\n\u2211\ni\u2208V\nwi wi + \u2211 j\u2208N in(i) wj \u2264 4\u03b1 ln 4K \u03b1\u01eb ,\nwhere \u03b1 = \u03b1(G) is the independence number of G.\nProof. Following the proof idea of Alon et al. (2013), let M = \u23082K/\u01eb\u2309 and introduce a discretization of the values w1, . . . , wT such that (mi \u2212 1)/M \u2264 wi \u2264 mi/M for positive integers m1, . . . , mT . Since each wi \u2265 \u01eb, we have mi \u2265 Mwi \u2265 2K\u01eb \u00b7 \u01eb = 2K. Hence, we obtain\n\u2211\ni\u2208V\nwi wi + \u2211 j\u2208N in(i) wj\n= \u2211\ni\u2208V\nmi mi + \u2211 j\u2208N in(i) mj \u2212K\n\u2264 2 \u2211\ni\u2208V\nmi mi + \u2211 j\u2208N in(i)mj , (6)\nwhere the final inequality is true since K \u2264 1 2 mi \u2264 12 ( mi + \u2211 j\u2208N in(i) mj ) .\nNow, consider a graph G\u2032 = (V \u2032, E \u2032) created from G by replacing each node i \u2208 V with a clique Ci over mi vertices, and connecting each vertex of Ci to each vertex of Cj if and only if the edge (i, j) is present in G. Then, the right-hand side of Eq. (6) equals \u2211 i\u2208V \u2032 1 1+di\n, where di is the in-degree of the vertex i \u2208 V \u2032 in the graph G\u2032. Applying Lemma 13 of Alon et al. (2013) to the graph G\u2032, we can show that\n\u2211\ni\u2208V\nmi mi + \u2211 j\u2208N in(i)mj\n\u2264 2\u03b1 ln ( 1 + \u2211 i\u2208V mi\n\u03b1\n) \u2264 2\u03b1 ln ( 1 + M +K\n\u03b1\n) \u2264 2\u03b1 ln 4K\n\u03b1\u01eb ,\nand the lemma follows."}, {"heading": "B Proofs of Lower Bounds", "text": "B.1 Non-observable Feedback Graphs\nWe first prove Theorem 6.\nTheorem 6 (restated). If G = (V,E) is not observable and |V | \u2265 2, then for any player algorithm there exists a sequence of loss functions \u21131, \u21132, . . . : V 7\u2192 [0, 1] such that the player\u2019s expected regret is at least 1\n4 T .\nProof. Since G is not observable, there exists a node with no incoming edges, say node i = 1. Consider the following randomized construction of loss functions L1, L2, . . . : V 7\u2192 [0, 1]: draw \u03c7 \u2208 {0, 1} uniformly at random and set\nLt(i) = { \u03c7 if i = 1, 1 2 if i 6= 1 t = 1, 2, . . .\nNow fix some strategy of the player (which, without loss of generality, we may assume to be deterministic) and denote by M the random number of times it chooses action i = 1. Notice that the player\u2019s actions, and consequently M , are independent of the random variable \u03c7 since the player never observes the loss value assigned to action i = 1. Letting RT denote the player\u2019s regret after T rounds, it holds that E[RT ] (where expectation is taken with respect to the randomization of the loss functions) satisfies\nE[RT ] = 1 2 E [ 1 2 M | \u03c7 = 1 ] + 1 2 E [ 1 2 (T \u2212M) | \u03c7 = 0 ]\n= 1 2 E [ 1 2 M + 1 2 (T \u2212M) ] = 1 4 T .\nThis implies that there exists a realization \u21131, . . . , \u2113T of the random functions for which the regret is at least 1\n8 T , as claimed.\nB.2 Weakly observable Feedback Graphs\nWe now turn to prove our main lower bound for weakly observable graphs, stated in Theorem 7.\nTheorem 7 (restated). If G = (V,E) is weakly observable with K = |V | \u2265 2 and weak domination number \u03b4 = \u03b4(G), then for any randomized player algorithm and for any time horizon T there exists a sequence of loss functions \u21131, . . . , \u2113T : V 7\u2192 [0, 1] such that the player\u2019s expected regret is at least 1\n150 (\u03b4/ ln2K)1/3T 2/3.\nBefore proving the theorem, we recall the key combinatorial lemma it relies upon.\nLemma 8 (restated). Let G = (V,E) be a directed graph over |V | = n vertices, and let W \u2286 V be a set of vertices whose minimal dominating set is of size k. Then, W contains an independent set U of size at least 1\n50 (k/ lnn), with the property that any vertex of G\ndominates at most lnn vertices of U .\nProof of Theorem 7. As the minimal dominating set of the weakly observable part of G is of size \u03b4, Lemma 8 says that G must contain an independent set U of m \u2265 \u03b4/(50 lnK) weakly observable vertices, such that any v \u2208 V dominates at most lnK vertices of U . For simplicity, we shall assume that \u03b4 \u2265 100 lnK which ensures that the set U consists of at least m \u2265 2 vertices; a proof of the theorem for the (less interesting) case where \u03b4 < 100 lnK is given after the current proof.\nConsider the following randomized construction of loss functions L1, . . . , LT : V 7\u2192 [0, 1]: fix \u01eb = m1/3(32T lnK)\u22121/3, choose \u03c7 \u2208 U uniformly at random and for all t and i, and let the loss Lt(i) \u223c Ber(\u00b5i) be a Bernoulli random variable with parameter\n\u2200 i \u2208 V , \u00b5i =    1 2 \u2212 \u01eb if i = \u03c7, 1 2\nif i \u2208 U, i 6= \u03c7, 1 i /\u2208 U .\nWe refer to actions in U as \u201cgood\u201d actions (whose expected instantaneous regret is at most \u01eb), and to actions in V \\U as \u201cbad\u201d actions (with expected instantaneous regret larger than 1 2 ). Notice that N in(i) \u2286 V \\ U for all good actions i \u2208 U , since U is an independent set of weakly observable vertices (that do not have self-loops). In other words, in order to observe the loss of a good action in a given round, the player has to pick a bad action on that round.\nFix some strategy of the player which we assume to be deterministic (again, this is without loss of generality). Up to a constant factor in the resulting regret lower bound, we may also assume that the strategy chooses bad actions at most \u01ebT times with probability one (i.e., over any realization of the stochastic loss functions). Indeed, we can ensure this is the case by simply halting the player\u2019s algorithm once it chooses bad actions for more than \u01ebT times, and picking an arbitrary good action in the remaining rounds; since the instantaneous regret of a good action is at most \u01eb, the regret of the modified algorithm is at most 3 times larger than the regret of the original algorithm (the latter regret is at least 1\n2 \u01ebT , while the\nmodification results in an increase of at most \u01ebT in the regret). Denote by I1, . . . , IT the sequence of actions played by the player\u2019s strategy throughout the game, in response to the loss functions L1, . . . , LT . For all t, let Yt be the vector of loss values observed by the player on round t; we think about Yt as being a full K-vector, with the unobserved values replaced by \u22121. For all i \u2208 U , let Mi be the number of times the player picks the good action i, and Ni be the number of times the player picks a bad action from N in(i). Also, let M be the total number of times the player picks a good action, and N be the number of times he picks a bad action. Notice that \u2211 i\u2208U Ni \u2264 N lnK as each vertex in V \\ U dominates at most lnK vertices of U by construction. This, together with our assumption that N \u2264 \u01ebT with probability one (i.e., that the player picks bad actions for at most \u01ebT times), implies that\n\u2211\ni\u2208U\nNi \u2264 \u01ebT lnK . (7)\nIn order to analyze the amount of information on the value of \u03c7 the player obtains by observing the Yt\u2019s, we let F be the \u03c3-algebra generated by the observed variables Y1, . . . , YT , and define the conditional probability functions Qi(\u00b7) = P( \u00b7 | \u03c7 = i) over F , for all i \u2208 U . Notice that under Qi, action i is the optimal action. For technical purposes, we also let Q0(\u00b7) denote the fictitious probability function induced by picking \u03c7 = 0; under this distribution, all good actions in U have an expected loss equal to 1\n2 . For two probability functions Q,Q\u2032\nover F , we denote by\nDTV(Q,Q\u2032) = sup A\u2208F |Q(A)\u2212Q\u2032(A)|\nthe total variation distance between Q and Q\u2032 with respect to F . Then, we can bound the total variation distance between Q0 and each of the Qi\u2019s in terms of the random variables Ni, as follows. Lemma. For each i \u2208 U , we have DTV(Q0,Qi) \u2264 \u01eb \u221a 2EQ0 [Ni].\nProof. As an intermediate step, we first upper bound the KL-divergence between Qi and Q0 in terms of the random variable Ni. Let Qjt = Qj( \u00b7 | Y1, . . . , Yt\u22121) for all j. Notice that Qit and Q0t are identical unless the player picked an action from N in(i) on round t. In this latter case, DKL(Q0t ,Qit) equals the KL-divergence between two Bernoulli random variables with biases 1\n2 and 1 2 \u2212 \u01eb, which is upper bounded by 4\u01eb2 for \u01eb \u2264 1 4 .5 Thus, using the chain rule for\nrelative entropy we may write\nDKL(Q0,Qi) = T\u2211\nt=1\nDKL(Q0t ,Qit)\n=\nT\u2211\nt=1\nQ0 ( It \u2208 N in(i) ) \u00b7 DKL(Ber(12),Ber(12 \u2212 \u01eb))\n\u2264 4\u01eb2 T\u2211\nt=1\nQ0 ( It \u2208 N in(i) ) = 4\u01eb2 EQ0 [Ni] .\nBy Pinsker\u2019s inequality we have DTV(Q0,Qi) \u2264 \u221a 1 2 DKL(Q0,Qi), which gives the lemma.\nAveraging the lemma\u2019s inequality over i \u2208 U , using the concavity of the square-root and recalling Eq. (7), we obtain\n1\nm\n\u2211\ni\u2208U\nDTV(Q0,Qi) \u2264 \u221a\u221a\u221a\u221a2\u01eb2 m EQ0 [ \u2211\ni\u2208U\nNi\n] \u2264 \u221a 2\u01eb3\nm T lnK =\n1 4 , (8)\nwhere the final equality follows from our choice of \u01eb. We now turn to lower bound the player\u2019s expected regret. Since the player incurs (at least) \u01eb regret each time he picks an action different from \u03c7, his overall regret is lower bounded by \u01eb(T \u2212M\u03c7), whence\nE[RT ] \u2265 1\nm\n\u2211\ni\u2208U\nE [ \u01eb(T \u2212M\u03c7) | \u03c7 = i ] = \u01ebT \u2212 \u01eb\nm\n\u2211\ni\u2208U\nEQi[Mi] . (9)\nIn order to bound the sum on the right-hand side, note that\nEQi [Mi]\u2212 EQ0 [Mi] = T\u2211\nt=1\n( Qi(It = i)\u2212Q0(It = i) ) \u2264 T \u00b7 DTV(Q0,Qi) ,\nand average over i \u2208 U to obtain\n1\nm\n\u2211\ni\u2208U\nEQi [Mi] \u2264 T\nm\n\u2211\ni\u2208U\nDTV(Q0,Qi) + 1\nm EQ0\n[ \u2211\ni\u2208U\nMi\n] \u2264 1\n4 T +\n1 m T \u2264 3 4 T ,\n5This KL-divergence equals 1 2 ln 1/2 1/2\u2212\u01eb + 1 2 ln 1/2 1/2+\u01eb = 1 2 ln ( 1 + 4\u01eb 2 1\u22124\u01eb2 ) \u2264 1 2 \u00b7 4\u01eb2 1\u22124\u01eb2 \u2264 4\u01eb2, where the last\nstep is valid for \u01eb \u2264 1 4 .\nwhere the last inequality is due to m \u2265 2. Combining this with Eq. (9) yields E[RT ] \u2265 14\u01ebT , and plugging in our choice of \u01eb gives\nE[RT ] \u2265 1\n4 ( m 32 lnK ) 1/3T 2/3 \u2265 \u03b4 1/3T 2/3 50 ln2/3K ,\nwhich concludes the proof (recall the additional 1 3 -factor stemming from our simplifying assumption made earlier).\nThe claim of the theorem for the case \u03b4 < 100 lnK, that remained unaddressed in the proof above, follows from a simpler lower bound that applies to weakly observable graphs of any size.\nTheorem 11. If G = (V,E) is weakly observable and |V | \u2265 2, then for any player algorithm and for any time horizon T there exists a sequence of loss functions \u21131, . . . , \u2113T : V 7\u2192 [0, 1] such that the player\u2019s expected regret is at least 1\n8 T 2/3.\nProof. First, we observe that any graph over less than 3 vertices is either non-observable or strongly observable; in other words, any weakly observable graph has at least 3 vertices, so |V | \u2265 3. Now, if G is weakly observable, then there is a node of G, say i = 1, without a self-loop and without an incoming edge from (at least) one of the other nodes of the graph, say from j = 2. Since |V | \u2265 3 and the graph is observable, i = 1 has at least one incoming edge from a third node of the graph.\nConsider the following randomized construction of loss functions L1, . . . , LT : V 7\u2192 [0, 1]: fix \u01eb = 1\n2 T\u22121/3, choose \u03c7 \u2208 {\u22121,+1} uniformly at random and for all t and i, let the loss\nLt(i) \u223c Ber(\u00b5i) be a Bernoulli random variable with parameter\n\u00b5i =    1 2 \u2212 \u01eb \u03c7 if i = 1, 1 2 if i = 2,\n1 otherwise.\nHere, the \u201cgood\u201d actions (whose expected instantaneous regret is at most \u01eb) are i = 1 and i = 2, and all other actions are \u201cbad\u201d actions (with expected instantaneous regret larger than 1\n2 ).\nNow, fix a deterministic strategy of the player and let the random variable N1 be the number of times the player chooses a bad action from N in(1). Define the conditional probability functions Q1(\u00b7) = P( \u00b7 | \u03c7 = +1) and Q2(\u00b7) = P( \u00b7 | \u03c7 = \u22121) where under Qi action i is the optimal action. Also, define Q0 to be the fictitious distribution induced by setting \u03c7 = 0, under which the actions i = 1 and i = 2 both have an expected loss of 1\n2 . Then,\nexactly as in the proof of Theorem 7, we can show that\nDTV(Q0,Qi) \u2264 \u01eb \u221a 2EQi[N1] , i = 1, 2 .\nAveraging the two inequalities and using the concavity of the square root, we obtain\n1 2 DTV(Q0,Q1) + 12DTV(Q0,Q2) \u2264 \u01eb \u221a EQ1 [N1] + EQ2 [N1] = \u01eb \u221a 2E[N1] , (10)\nwhere we have used the fact that P(\u00b7) = 1 2 Q1(\u00b7) + 1 2 Q2(\u00b7).\nWe can now analyze the player\u2019s expected regret, again denoted by E[RT ]. Notice that if E[N1] > 1 32 \u01eb\u22122, we have E[RT ] \u2265 E[12N1] > 164\u01eb\u22122 = 18T 2/3 (since each action that reveals the loss of i = 1 is a bad action whose instantaneous regret is at least 1 2 ), which gives the required lower bound. Hence, we may assume that E[N1] \u2264 132\u01eb\u22122, in which case the right-hand side of Eq. (10) is bounded by 1\n4 . This yields an analogue of Eq. (8), from which we can proceed\nexactly as in the proof of Theorem 7 to obtain that E[RT ] \u2265 14\u01ebT . Using our choice of \u01eb gives the theorem.\nB.3 Separation Between the Informed and Uninformed Models\nFinally, we prove our separation result for weakly observable time-varying graphs, which shows that the uninformed model is harder than the informed model (in terms of the dependence on the feedback structure) for weakly observable feedback graphs.\nTheorem 9 (restated). For any randomized player strategy in the uninformed feedback model, there exists a sequence of weakly observable graphs G1, . . . , GT over a set V of K \u2265 4 actions with \u03b4(Gt) = \u03b1(Gt) = 1 for all t, and a sequence of loss functions \u21131, . . . , \u2113T : V 7\u2192 [0, 1], such that the player\u2019s expected regret is at least 1\n16 K1/3T 2/3.\nProof. As before, it is enough to demonstrate a randomized construction of weakly observable graphs G1, . . . , GT and loss functions L1, . . . , LT such that the expected regret of any deterministic algorithm is \u2126(K1/3T 2/3).\nThe random loss functions L1, . . . , LT are constructed almost identically to those used in the proof of Theorem 11; the only change is in the value of \u01eb, which is now fixed to \u01eb = 1\n4 (K/T )1/3. In order to construct the random sequence of weakly observable graphs\nG1, . . . , GT , first pick nodes J1, . . . , JT independently and uniformly at random from V \u2032 = {3, . . . , K}. Then, for each t, form the graph Gt by taking the complete graph over V (that includes all directed edges and self-loops) and removing all edges incoming to node i = 1 (including its self-loop), except for the edge incoming from Jt. In other words, the only way to observe the loss Lt(1) of node 1 on round t is by picking the action Jt on that round. Notice that Gt is weakly observable, as each of its nodes has at least one incoming edge, but there is a node (node 1) which is not strongly observable. Also, we have \u03b4(Gt) = 1 since Jt dominates the entire graph, and \u03b1(Gt) = 1 as any pair of vertices is connected by at least one directed edge.\nWe now turn to analyze the expected regret of any player on our construction; the analysis is very similar to that of Theorem 11, and we only describe the required modifications. Fix any deterministic algorithm, and define the random variables I1, . . . , IT and N1 exactly as in the proof of Theorem 11. In addition, define the distributions Q0, Q1, and Q2 as in that proof, for which we proved (recall Eq. (10)) that\n1 2 DTV(Q0,Q1) + 12DTV(Q0,Q2) \u2264 \u01eb \u221a 2E[N1] . (11)\nNow, define another random variable N to be the number of times the player picked an action from V \u2032 throughout the game. Notice that in case E[N ] > 1\n4 K1/3T 2/3, we have\nE[RT ] \u2265 E[12N ] > 18K1/3T 2/3 which implies the stated lower-bound on the expected regret. Hence, in what follows we assume that E[N ] \u2264 1\n4 K1/3T 2/3. Notice that for the graphs we\nconstructed, Q(It = Jt) \u2264 2KQ(It \u2208 V \u2032) since Jt is picked uniformly at random from V \u2032 (and independently from It because in the uninformed model Gt is not known when It is drawn) and since K \u2265 4. Summing this over t = 1, . . . , T , we obtain that E[N1] \u2264 2KE[N ] \u2264 1 2 (T/K)2/3, and with our choice of \u01eb this shows that the right-hand size of Eq. (11) is upper bounded by 1 4 . Again, continuing exactly as in the proof of Theorem 7, we finally get that E[RT ] \u2265 14\u01ebT , and with our choice of \u01eb this concludes the proof."}, {"heading": "C Tight Bounds for the Loopless Clique", "text": "We restate and prove Theorem 3.\nTheorem 3 (restated). For any sequence of loss functions \u21131, . . . , \u2113T , where \u2113t : V 7\u2192 [0, 1], the expected regret of Algorithm 1, with the loopless clique feedback graph and with parameters \u03b7 = \u221a (lnK)/(2T ) and \u03b3 = 2\u03b7, is upper-bounded by 5 \u221a T lnK.\nProof. Since G is strongly observable, the exploration distribution u is uniform on V . Fix any i\u22c6 \u2208 V . Notice that for any i \u2208 V we have j \u2208 N in(i) for all j 6= i, and so Pt(i) = 1 \u2212 pt(i). On the other hand, by the definition of pt and since \u03b7 = 2\u03b3 and K \u2265 2, we have pt(i) = (1 \u2212 \u03b3)qt(i) + \u03b3K \u2264 1 \u2212 \u03b3 + \u03b3 2\n= 1 \u2212 \u03b7, so that Pt(i) \u2265 \u03b7. Thus, we can apply Lemma 4 with St = V to the vectors \u2113\u03021, . . . , \u2113\u0302T and take expectations,\nE\n[ T\u2211\nt=1\n( K\u2211\ni=1\nqt(i)Et[\u2113\u0302t(i)]\u2212 Et[\u2113\u0302t(i\u22c6)] )]\n\u2264 lnK \u03b7 + \u03b7\nT\u2211\nt=1\nE\n[ \u2211\ni\u2208V\nqt(i)(1\u2212 qt(i))Et[\u2113\u0302t(i)2] ] .\nRecalling Eq. (3) and Pt(i) = 1\u2212 pt(i), we get\nE\n[ T\u2211\nt=1\nk\u2211\ni=1\nqt(i)\u2113t(i)\n] \u2212 T\u2211\nt=1\n\u2113t(i \u22c6) \u2264 lnK\n\u03b7 + \u03b7\nT\u2211\nt=1\nE\n[ \u2211\ni\u2208S\nqt(i) 1\u2212 qt(i) 1\u2212 pt(i)\n] .\nFinally, for the distributions pt and qt generated by the algorithm we note that\n1\u2212 pt(i) \u2265 ( 1\u2212 \u03b3\nK\n)( 1\u2212 qt(i) ) \u2265 1\n2\n( 1\u2212 qt(i) )\nwhere the last inequality holds since K \u2265 2. Hence, T\u2211\nt=1\n\u2211\ni\u2208V\nqt(i) 1\u2212 qt(i) 1\u2212 pt(i) \u2264 2 T\u2211\nt=1\n\u2211\ni\u2208V\nqt(i) \u2264 2T .\nCombining this with Eq. (5) gives\nE\n[ T\u2211\nt=1\n\u2211\ni\u2208V\npt(i)\u2113t(i)\u2212 T\u2211\nt=1\n\u2113t(i \u22c6) ] \u2264 \u03b3T + lnK\n\u03b7 + 2\u03b7T =\nlnK\n\u03b7 + 4\u03b7T ,\nwhere we substituted our choice \u03b3 = 2\u03b7. Picking \u03b7 = \u221a (lnK)/2T proves the theorem."}, {"heading": "D Connections to Partial Monitoring", "text": "In online learning with partial monitoring the player is given a loss matrix L over [0, 1] and a feedback matrix H over a finite alphabet \u03a3. The matrices L and H are both of size K \u00d7M , where K is the number of player\u2019s actions and M is the number of environment\u2019s actions. The environment preliminarily fixes a sequence y1, y2, . . . of actions (i.e., matrix column indices) hidden from the player.6 At each round t = 1, 2, . . . , the loss \u2113t(It) of the player choosing action It (i.e., a matrix row index) is given by the matrix entry L(It, yt) \u2208 [0, 1]. The only feedback that the player observes is the symbol H(It, yt) \u2208 \u03a3; in particular, the column index yt and the loss value L(It, yt) remain both unknown. The player\u2019s goal is to control a notion of regret analogous to ours, where the minimization over V is replaced by a minimization over the set of row indices, corresponding to the player\u2019s K actions.\nWe now introduce a reduction from our online setting to partial monitoring for the special case of {0, 1}-valued loss functions (note that our lower bounds still hold under this restriction, and so does our characterization of Theorem 1). Given a feedback graph G, we create a partial monitoring game in which the environment has a distinct action for each binary assignment of losses to vertices in V . Hence, L and H have K rows and M = 2K columns, where the union of columns in L is the set {0, 1}K. The entries of H encode G using any alphabet \u03a3 such that, for any row i \u2208 V and for any two columns y 6= y\u2032,\nH(i, y) = H(i, y\u2032) \u21d4 {( k, L(k, y) ) : k \u2208 Nout(i) } \u2261 {( k, L(k, y\u2032) ) : k \u2208 Nout(i) } . (12)\nNote that this is a bona fide reduction: given a partial monitoring algorithm A, we can define an algorithm A\u2032 for solving any online learning problem with known feedback graph G = (V,E) and {0, 1}-valued loss functions. The algorithm A\u2032 pre-computes a mapping from{(\nk, L(k, y) ) : k \u2208 Nout(i) } for each i \u2208 V and for each y = 1, . . . ,M to the alphabet \u03a3 such that Eq. (12) is satisfied. Then, at each round t, A\u2032 asks A to draw a row (i.e., a vertex of V ) It and obtains the feedback {( k, L(k, yt) ) : k \u2208 Nout(It) } from the environment. Finally, A\u2032 uses the pre-computed mapping to obtain the symbol \u03c3t \u2208 \u03a3 which is fed to A. The minimax regret of partial monitoring games is determined by a set of observability conditions on the pair (L,H). These conditions are expressed in terms of a canonical representation of H as the set of matrices Si for i \u2208 V . Si has a row for each distinct symbol \u03c3 \u2208 \u03a3 in the i-th row of H , and Si(\u03c3, y) = I{H(i, y) = \u03c3} for y = 1, . . . ,M . When cast to the class of pairs (L,H) obtained from feedback graphs G through the above encoding, the partial monitoring observability conditions of Barto\u0301k et al. (2014, Definitions 5 and 6) can be expressed as follows. Let L(i, \u00b7) be the column vector denoting the i-th row of L. Let also rowsp be the rowspace of a matrix and \u2295 be the cartesian product between linear spaces. Then\n6 The standard definition of partial monitoring (see, e.g., Cesa-Bianchi and Lugosi, 2006, Section 6.4) assumes a harder adaptive environment, where each action yt is allowed to depend on all of past player\u2019s actions I1, . . . , It\u22121. However, the partial monitoring lower bounds of Antos et al. (2013, Theorem 13) and Barto\u0301k et al. (2014, Theorem 3) hold for our weaker notion of environment as well.\n\u2022 (L,H) is globally observable if for all pairs i, j \u2208 V of actions,\nL(i, \u00b7)\u2212 L(j, \u00b7) \u2208 \u2295\nk=1,...,K\nrowsp(Sk)\n\u2022 (L,H) is locally observable if for all pairs i, j \u2208 V of actions,\nL(i, \u00b7)\u2212 L(j, \u00b7) \u2208 rowsp(Si \u2295 rowsp(Sj) .\nThe characterization result for partial monitoring of Barto\u0301k et al. (2014, Theorem 2) states that the minimax regret is of order \u221a T for locally observable games and of order T 2/3 for globally observable games. We now prove that the above encoding of feedback graphs G as instances (L,H) of partial monitoring games preserves the observability conditions. Namely, our encoding maps weakly (resp., strongly) observable graphs G to globally (resp., locally) observable instances of partial monitoring. Combining this with our characterization result (Theorem 1) and the partial monitoring characterization result (Barto\u0301k et al., 2014, Theorem 2), we conclude that the minimax rates are preserved by our reduction.\nClaim 12. If j \u2208 Nout(i) then there exists a subset \u03a30 of rows of Si such that\nL(j, \u00b7) = \u2211\n\u03c3\u2208\u03a30\nSi(\u03c3, \u00b7) .\nProof. Let \u03a30 to be the union of rows Si(\u03c3y, \u00b7) such that H(i, y) = \u03c3y and L(j, y) = 1 for some y. Each such row has a 1 in position y because Si(\u03c3y, y) = 1 holds by definition. Moreover, no such row has a 1 in a position y\u2032 where L(j, y\u2032) = 0. Indeed, combining j \u2208 Nout(i) with Eq. (12), we get that L(j, y\u2032) = 0 implies H(i, y\u2032) 6= \u03c3y, which in turn implies Si(\u03c3y, y \u2032) = 0.\nTheorem 13. Any feedback graph G can be encoded as a partial monitoring problem (L,H) such that the observability conditions are preserved.\nProof. If G is weakly observable, then for every j \u2208 V there is some i \u2208 V such that j \u2208 Nout(i). By Claim 12, L(j, \u00b7) \u2208 rowsp(Si) and the global observability condition follows. If G is strongly observable, then for any distinct i, j \u2208 V the subgraph G\u2032 of G restricted to the pair of vertices i, j is weakly observable. By the previous argument, this implies that L(i, \u00b7)\u2212 L(j, \u00b7) \u2208 rowsp(Si)\u2295 rowsp(Sj) and the proof is concluded."}], "references": [{"title": "The Probabilistic Method", "author": ["N. Alon", "J.H. Spencer"], "venue": null, "citeRegEx": "Alon and Spencer.,? \\Q2008\\E", "shortCiteRegEx": "Alon and Spencer.", "year": 2008}, {"title": "From bandits to experts: A tale of domination and independence", "author": ["N. Alon", "N. Cesa-Bianchi", "C. Gentile", "Y. Mansour"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Alon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2013}, {"title": "Nonstochastic multi-armed bandits with graph-structured feedback", "author": ["N. Alon", "N. Cesa-Bianchi", "C. Gentile", "S. Mannor", "Y. Mansour", "O. Shamir"], "venue": "CoRR, abs/1409.8428,", "citeRegEx": "Alon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2014}, {"title": "Toward a classification of finite partialmonitoring games", "author": ["A. Antos", "G. Bart\u00f3k", "D. P\u00e1l", "C. Szepesv\u00e1ri"], "venue": "Theoretical Computer Science,", "citeRegEx": "Antos et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Antos et al\\.", "year": 2013}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Partial monitoring\u2014 classification, regret bounds, and algorithms", "author": ["G. Bart\u00f3k", "D.P. Foster", "D. P\u00e1l", "A. Rakhlin", "C. Szepesv\u00e1ri"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bart\u00f3k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bart\u00f3k et al\\.", "year": 2014}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "How to use expert advice", "author": ["N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D. Helmbold", "R. Schapire", "M. Warmuth"], "venue": "Journal of the ACM,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1997}, {"title": "Improved second-order bounds for prediction with expert advice", "author": ["N. Cesa-Bianchi", "Y. Mansour", "G. Stoltz"], "venue": "Machine Learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2007}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Efficient learning by implicit exploration in bandit problems with side observations", "author": ["T. Koc\u00e1k", "G. Neu", "M. Valko", "R. Munos"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Koc\u00e1k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koc\u00e1k et al\\.", "year": 2014}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "From bandits to experts: On the value of side-observations", "author": ["S. Mannor", "O. Shamir"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mannor and Shamir.,? \\Q2011\\E", "shortCiteRegEx": "Mannor and Shamir.", "year": 2011}, {"title": "Online learning and online convex optimization", "author": ["S. Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2011}, {"title": "Aggregating strategies", "author": ["V. Vovk"], "venue": "In Proceedings of the 3rd Annual Workshop on Computational Learning Theory,", "citeRegEx": "Vovk.,? \\Q1990\\E", "shortCiteRegEx": "Vovk.", "year": 1990}, {"title": "Following the proof idea of Alon et al. (2013), let M = \u23082K/\u01eb\u2309 and introduce a discretization of the values", "author": ["G. Proof"], "venue": null, "citeRegEx": "Proof.,? \\Q2013\\E", "shortCiteRegEx": "Proof.", "year": 2013}, {"title": "Definitions 5 and 6) can be expressed as follows. Let L(i, \u00b7) be the column vector denoting the i-th row of L. Let also rowsp be the rowspace of a matrix and \u2295 be the cartesian product between linear spaces", "author": ["Bart\u00f3k"], "venue": null, "citeRegEx": "Bart\u00f3k,? \\Q2014\\E", "shortCiteRegEx": "Bart\u00f3k", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Online learning can be formulated as a repeated game between a randomized player and an arbitrary, possibly adversarial, environment (see, e.g., Cesa-Bianchi and Lugosi, 2006; Shalev-Shwartz, 2011).", "startOffset": 133, "endOffset": 197}, {"referenceID": 7, "context": "This feedback model is often called prediction with expert advice (Cesa-Bianchi et al., 1997; Littlestone and Warmuth, 1994; Vovk, 1990).", "startOffset": 66, "endOffset": 136}, {"referenceID": 11, "context": "This feedback model is often called prediction with expert advice (Cesa-Bianchi et al., 1997; Littlestone and Warmuth, 1994; Vovk, 1990).", "startOffset": 66, "endOffset": 136}, {"referenceID": 14, "context": "This feedback model is often called prediction with expert advice (Cesa-Bianchi et al., 1997; Littlestone and Warmuth, 1994; Vovk, 1990).", "startOffset": 66, "endOffset": 136}, {"referenceID": 4, "context": "Another common feedback model is bandit feedback (Auer et al., 2002), where the player only observes the loss of the action that he chose.", "startOffset": 49, "endOffset": 68}, {"referenceID": 4, "context": "Another common feedback model is bandit feedback (Auer et al., 2002), where the player only observes the loss of the action that he chose. In this model, the player\u2019s choices influence the feedback that he receives, so he has to balance an exploration-exploitation trade-off. On one hand, the player wants to exploit what he has learned from the previous rounds by choosing an action that is expected to have a small loss; on the other hand, he wants to explore by choosing an action that will give him the most informative feedback. The canonical example of online learning with bandit feedback is online advertising. Say that we operate an Internet website and we present one of K ads to each user that views the site. Our goal is to maximize the number of clicked ads and therefore we incur a unit loss whenever a user doesn\u2019t click on an ad. We know whether or not the user clicked on the ad we presented, but we don\u2019t know whether he would have clicked on any of the other ads. Full feedback and bandit feedback are special cases of a general framework introduced by Mannor and Shamir (2011), where the feedback model is specified by a feedback graph.", "startOffset": 50, "endOffset": 1097}, {"referenceID": 5, "context": "Freund and Schapire (1997) proves that the minimax regret of the full feedback game is \u0398( \u221a T lnK) while Auer et al.", "startOffset": 0, "endOffset": 27}, {"referenceID": 2, "context": "Freund and Schapire (1997) proves that the minimax regret of the full feedback game is \u0398( \u221a T lnK) while Auer et al. (2002) proves that the minimax regret of the bandit feedback game is \u0398\u0303( \u221a KT ).", "startOffset": 105, "endOffset": 124}, {"referenceID": 1, "context": "The minimax regret rates induced by self-aware feedback graphs were extensively studied in Alon et al. (2014). In this paper, we focus on the intriguing situation that occurs when the feedback graph is missing some self-loops, namely, when the player does not always observe his own loss.", "startOffset": 91, "endOffset": 110}, {"referenceID": 1, "context": "The set of strongly observable feedback graphs includes the set of self-aware graphs, so this result extends the characterization given in Alon et al. (2014). The second category is the set of weakly observable feedback graphs, which induce learning problems whose minimax regret is \u0398\u0303(\u03b4T ), where \u03b4 is a new graph-dependent quantity called the weak domination number of the feedback graph.", "startOffset": 139, "endOffset": 158}, {"referenceID": 1, "context": "G (see Algorithm 1), which is a variant of the Exp3-SET algorithm for undirected feedback graphs (Alon et al., 2013).", "startOffset": 97, "endOffset": 116}, {"referenceID": 1, "context": ", strongly observable with self-loops), our result matches the bounds of Alon et al. (2014); Koc\u00e1k et al.", "startOffset": 73, "endOffset": 92}, {"referenceID": 1, "context": ", strongly observable with self-loops), our result matches the bounds of Alon et al. (2014); Koc\u00e1k et al. (2014). The tightness of our bounds in all cases is discussed in Section 4 below.", "startOffset": 73, "endOffset": 113}, {"referenceID": 9, "context": "Recall that Hedge (Freund and Schapire, 1997) operates in the full feedback setting (see Fig.", "startOffset": 18, "endOffset": 45}, {"referenceID": 1, "context": "While Alon et al. (2014) only consider the special case of graphs that have self-loops at all vertices, their lower bound applies to any strongly observable graph: we can simply add any missing self-loops to the graph, without changing its independence number \u03b1.", "startOffset": 6, "endOffset": 25}, {"referenceID": 9, "context": "The setting discussed above can be generalized by allowing the feedback graphs to change arbitrarily from round to round (see Mannor and Shamir (2011); Alon et al.", "startOffset": 126, "endOffset": 151}, {"referenceID": 1, "context": "The setting discussed above can be generalized by allowing the feedback graphs to change arbitrarily from round to round (see Mannor and Shamir (2011); Alon et al. (2013); Koc\u00e1k et al.", "startOffset": 152, "endOffset": 171}, {"referenceID": 1, "context": "The setting discussed above can be generalized by allowing the feedback graphs to change arbitrarily from round to round (see Mannor and Shamir (2011); Alon et al. (2013); Koc\u00e1k et al. (2014)).", "startOffset": 152, "endOffset": 192}, {"referenceID": 10, "context": ", using a doubling trick, or an adaptive learning rate as in Koc\u00e1k et al. (2014)).", "startOffset": 61, "endOffset": 81}], "year": 2015, "abstractText": "We study a general class of online learning problems where the feedback is specified by a graph. This class includes online prediction with expert advice and the multiarmed bandit problem, but also several learning problems where the online player does not necessarily observe his own loss. We analyze how the structure of the feedback graph controls the inherent difficulty of the induced T -round learning problem. Specifically, we show that any feedback graph belongs to one of three classes: strongly observable graphs, weakly observable graphs, and unobservable graphs. We prove that the first class induces learning problems with \u0398\u0303(\u03b11/2T 1/2) minimax regret, where \u03b1 is the independence number of the underlying graph; the second class induces problems with \u0398\u0303(\u03b41/3T 2/3) minimax regret, where \u03b4 is the domination number of a certain portion of the graph; and the third class induces problems with linear minimax regret. Our results subsume much of the previous work on learning with feedback graphs and reveal new connections to partial monitoring games. We also show how the regret is affected if the graphs are allowed to vary with time. Tel Aviv University, Tel Aviv, Israel, and Microsoft Research, Herzliya, Israel, nogaa@post.tau.ac.il. Dipartimento di Informatica, Universit\u00e0 degli Studi di Milano, Milan, Italy, nicolo.cesabianchi@unimi.it. Parts of this work were done while the author was at Microsoft Research, Redmond. Microsoft Research, Redmond, Washington; oferd@microsoft.com. Technion\u2014Israel Institute of Technology, Haifa, Israel, and Microsoft Research, Herzliya, Israel, tomerk@technion.ac.il. Parts of this work were done while the author was at Microsoft Research, Redmond.", "creator": "LaTeX with hyperref package"}}}