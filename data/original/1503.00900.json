{"id": "1503.00900", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2015", "title": "Normalization based K means Clustering Algorithm", "abstract": "K-means is an effective clustering technique used to separate similar data into groups based on initial centroids of clusters. In this paper, Normalization based K-means clustering algorithm(N-K means) is proposed. Proposed N-K means clustering algorithm applies normalization prior to clustering on the available data as well as the proposed approach calculates initial centroids based on weights. Experimental results prove the betterment of proposed N-K means clustering algorithm over existing K-means clustering algorithm in terms of complexity and overall performance.", "histories": [["v1", "Tue, 3 Mar 2015 11:26:27 GMT  (360kb)", "http://arxiv.org/abs/1503.00900v1", "5 pages, 4 figures in International Journal of Advanced Engineering Research and Science (IJAERS)-Feb 2015"]], "COMMENTS": "5 pages, 4 figures in International Journal of Advanced Engineering Research and Science (IJAERS)-Feb 2015", "reviews": [], "SUBJECTS": "cs.LG cs.DB", "authors": ["deepali virmani", "shweta taneja", "geetika malhotra"], "accepted": false, "id": "1503.00900"}, "pdf": {"name": "1503.00900.pdf", "metadata": {"source": "CRF", "title": "Normalization based K means Clustering Algorithm", "authors": ["Deepali Virmani", "Shweta Taneja"], "emails": ["deepalivirmani@gmail.com", "shweta_taneja08@yahoo.co.in", "geets002@gmail.com"], "sections": [{"heading": null, "text": "technique used to separate similar data into groups based on initial centroids of clusters. In this paper, Normalization based K-means clustering algorithm(N-K means) is proposed. Proposed N-K means clustering algorithm applies normalization prior to clustering on the available data as well as the proposed approach calculates initial centroids based on weights. Experimental results prove the betterment of proposed N-K means clustering algorithm over existing K-means clustering algorithm in terms of complexity and overall performance.\nKeywords- Clustering, Data mining, K means, Normalization, Weighted Average\nI. INTRODUCTION\nData mining[7][11]or knowledge discovery is a process of analysing large amounts of data and extracting useful information. It is an important technology which is used by industries as a novel approach to mine data. Data mining tools and techniques are used to generate effective results which was earlier difficult and time consuming.Data mining is widely used in various areas like financial data analysis, retail and telecommunication industry, biological data analysis, fraud detection, spatial data analysis and other scientific applications.\nClustering is a technique of data mining in which similar objects are grouped into clusters. Clustering techniques are widely used in various domains like information retrieval, image processing,etc[1][2].There are two types of approaches in clustering: hierarchical and\npartitioning. In hierarchical clustering, the clusters are combined based on their proximity or how close they are. This combination is prevented when further process leads to undesirable clusters. In partition clustering approach, one dataset is separated into definite number of small sets in a single iteration[10]. The accuracy and quality of clustering results depends how the algorithms are implemented and their ability to find hidden knowledge.\nThere are various clustering algorithms based on the nature of generated clusters and techniques. Few of them are BIRCH(Balanced iterative reducing and clustering using hierarchies),CURE(Clustering using representatives),K-means, genetic K-means, Clara, Dbscan,Clarans etc[6]. The most widely used clustering algorithm is the K- means algorithm. This algorithm is used in many practical applications.It works by selecting the initial number of clusters and initial centroids[7][13]. We have chosen K-means algorithm over other clustering algorithms as it very efficient in processing large data sets. It often terminates at a local optimum and generates tighter clusters than hierarchical clustering, especially if clusters are globular. It is a popular algorithm because of its observable speed and simplicity.But K-means has a major disadvantage that it does not work well with clusters of different size and different density. Moreover initial centroids are chosen randomly due to which clusters produced vary from one run to another. Also various datapoints exist on which Kmeans takes superpolynomial time[8][5].\nDifferent researchers have put forward various methods to improve the efficiency and time of K-\nmeans algorithm. K-means uses the concept of Euclidean distance to calculate the centroids of the clusters. This method is less effective when new data sets are added and have no effect on the measured distance between various data objects. The computational complexity of k means algorithm is also very high[1][9].Also, K-means is unable to handle noisy data and missing values.Data preprocessing techniques are often applied to the datasets to make them more clean, consistent and noise free. Normalization is used to eliminate redundant data and ensures that good quality clusters are generated which can improve the efficiency of clustering algorithms.So it becomes an essential step before clustering as Euclidean distance is very sensitive to the changes in the differences[3].\nThis paper is organized as follows:in Section II, a description of the literature survey is done in which we have covered the work done by various authors to improve K-means clustering algorithm. Then in Section III, our proposed N-K means algorithm is described stepwise followed by experimental results in Section IV, where a comparison is shown between traditional K-means clustering algorithm and N-K means clustering algorithm. Lastly, the conclusions are addressed in Section V.\nII. LITERATURE SURVEY\nA lot of methods and techniques have been\nproposed over the past few years to improve the accuracy of the algorithm and there is a need to optimize it to have good results.This section discusses the various approaches proposed by researchers to find better initial centroids in k means algorithm .\nAuthors[3], have proposed data preprocessing techniques like cleaning and normalization to produce optimum quality clusters. In normalization the data to be analyzed is scaled to a specific range. A modified k means algorithm is proposed which provides a solution for automatic initialization of centroids and performance is enhanced using normalization. This techqnique overcomes many drawbacks of naive k means algorithm.\nSome authors have proposed their methods to identify initial centroids. In the following work[1], authors proposed a novel method to find better initial centroids as well as more accurate clusters with less computational time. This method was adopted to find weighted average score of dataset by averaging the value of attribute of each data point to generate initial centroids.\nIn another work, Authors[8] proposed a new k means clustering method with improved initial centre. In this method, initial cluster centres are selected and the centres are used as input to the k means. The user is not required to give the number of clusters as input.\nIn another research as done by authors[4], a data clustering approach is proposed which works by partitioning the space into different segments and calculating the frequency of data point in each segment and the segment which shows maximum frequency of data point has maximum chances to contain the centroid of the cluster.The authors have introduced concept of threshold distance for each cluster\u2019s centroid for comparing the distance between data point and cluster\u2019s centroid and using this method, efforts to calculate the distance between data point and cluster\u2019s centroid is minimized. This algorithm effectively decreases the complexity and makes calculations easier.\nIII. PROPOSED N- K MEANS ALGORITHM\nK-means algorithm can generate better results after the modification of the databases. We apply the modified algorithm with calculation of initial centroids based on weighted average score of dataset. Next, we preprocess and normalize dataset before we apply the N-K means algorithm. This proposed method works in three stages.During the first stage,data preprocessing technique is adopted that transforms raw data into understandable format. During the second stage, normalization is performed to standardize the data objects into specific range. During the third stage we apply the N-K means algorithm to generate clusters.\n3.1 DATA PRE - PROCESSING It is a very important step and should be adopted in clustering as this method uses concepts like constant, average, minimum, maximum, standard deviation to calculates missing values in the tuples[7]. These missing values need to be avoided for accurate results. Preprocessing involves steps like data cleaning, data integration, data transformation, data reduction and data discretization.\n3.2 NORMALIZATION Data Mining can generate effective results if normalization is applied to the dataset. It is a\nprocess used to standardize all the attributes of the dataset and give them equal weight so that redundant or noisy objects can be eliminated and there is valid and reliable data which enhances the accuracy of the result. K-Means algorithm uses Euclidean distance that is highly prone to irregularities in the size of various features[3]. There are various data normalization methods like Min-Max, Z-Score and Decimal Scaling.The best normalization method depends on the data to be normalized. Here, we have used Min-Max normalization technique in our algorithm because our dataset is limited and has not much variability between minimum and maximum. Min-Max normalization technique performs a linear transformation on the data. In this method, we fit the data in a predefined boundary or in a predefined interval.\n3.3 INITIAL CENTROIDS CALCULATION\nWe use a uniform method to find score by taking the average of the attribute of each data point which will generate initial centroids that follow the data distribution of the given set. A sorting algorithm is applied to the score of each data point and then divided into k subsets where k is the number of clusters. Finally the nearest value of mean from each subset is taken as initial centroid. In this method we have introduced a weight with each attribute, which makes the method advantageous as it can cause enhancement of any feature of the dataset by increasing the weight related to that attribute.\nThe algorithm is given in Fig 1:\nIV. EXPERIMENTAL RESULTS\nOur experiment was conducted on Iris data set [12] from UCI Machine Learning Repository for evaluating the performance of N-K means clustering algorithm. In this section, we represent a comparative analysis of traditional K-means clustering algorithm with N-K means algorithm. Both the algorithms are run for different values of\nk. From the comparisons we can make out that N-K means algorithm outperforms the traditional Kmeans algorithm in terms of parameters namely execution time and speed. Hence the algorithm computationally runs faster as it executes in less number of iterations and the complexity is reduced. The results are depicted in Table 1.\nFigure 3: shows comparison between K-means and N-K means on the basis of time\nV. CONCLUSION\nThe K-means clustering algorithm is widely used for clustering huge data sets. But traditional k means algorithm does not always generate good quality results as automatic initialization of centroids affects final clusters.This paper presents an efficient algorithm where we have first preprocessed our dataset based on normalization technique and then generated effective clusters. This is done by assigning weights to each attribute value to achieve standardization. Our algorithm has proved to be better than traditional K-means algorithm in terms of execution time and speed.\nFigure 4: shows comparison between K-means and N-K means on the basis of speed\nREFERENCES\n[1] Md Sohrab Mahmud, Md. Mostafizer Rahman, Md. Nasim Akhtar, \u201c Improvement of K-means Clustering algorithm with better initial centroids based on weighted average\u201d, 7th International Conference on Electrical and Computer Engineering, 2012, pp. 647-650. [2] Madhu Yedla,Srinivasa Rao Pathakota,TM Srinivasa, \u201cEnhancing K-means Clustering Algorithm with Improved Initial Center\u201d, International Journal of Computer Science and Information Technologies(IJCSIT),Vol.1(2),2010,pp. 121-125.\n[3]Vaishali Rajeev Patel, Rupa G. Mehta, \u201cPerformance Analysis of MK-means Clustering Algorithm with Normalization Approach\u201d, World Congress on Information and Communication Technologies, 2011, pp. 974-979. [4] Ran Vijay Singh, M.P.S Bhatia, \u201cData Clustering With Modified K means Algorithm\u201d, IEEE-International Conference on Recent Trends in Information Technology, ICRTIT 2011, pp. 717-721. [5] David Arthur & Sergei Vassilvitskii , \"How Slow is the k means Method?\", Proceedings of the 22nd Symposium on Computational Geometry (SoCG),2006, pp. 144-153. [6] J. Han and M.Kamber, \u201cData Mining Concepts and Techniques\u201d, Morgan Kaufmann Publishers,SanDiego, 2001. [7]Vaishali R. Patel, Rupa G. Mehta, \u201cImpact of Outlier Removal and Normalization Approach in Modified k-Means Clustering Algorithm\u201d, IJCSI International Journal of Computer Science Issues, Vol. 8, Issue 5, No 2, 2011, pp. 331-336. [8]Zhang Chen, Xia Shixiong, \u201cK-means Clustering Algorithm with improved Initial\nCenter\u201d, Second International Workshop on Knowledge Discovery and Data Mining,2009, pp. 790-792. [9] K. A.Abdul Nazeer, M.P.Sebastian ,\u201dImproving the Accuracy and Efficiency of the k-means Clustering Algorithm\u201d, Proceedings of the World Congress On Engineering 2009 Vol I, WCE 2009, pp. 308-312. [10] Margaret H.Dunham, \u201cData MiningIntroductory and Advanced Concepts\u201d, Pearson Education,2006. [11] R. Agrawal, T. Imielinksi and A. Swami, \u201cMining association rules between sets of items in large database\u201d, The ACM SIGMOD Conference, Washington DC, USA, 1993, pp. 207-216. [12] UCI Repository of Machine Learning Databases,\nAvailable: archive.ics.uci.edu/ml/\n[13] McQueen J, \u201cSome methods for classification and analysis of ultivariate observations,\u201d Proc. 5th Berkeley Symp. Math. Statist. Prob., Vol. 1, 1967, pp. 281\u2013297."}], "references": [{"title": "Enhancing K-means Clustering Algorithm with Improved Initial Center\u201d, International Journal of Computer Science and Information Technologies(IJCSIT),Vol.1(2),2010,pp", "author": ["Madhu Yedla", "Srinivasa Rao Pathakota", "TM Srinivasa"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Performance Analysis of MK-means Clustering Algorithm with Normalization Approach", "author": ["Vaishali Rajeev Patel", "Rupa G. Mehta"], "venue": "World Congress on Information and Communication Technologies,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Data Clustering With Modified K means Algorithm", "author": ["Ran Vijay Singh", "M.P.S Bhatia"], "venue": "IEEE-International Conference on Recent Trends in Information Technology,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Impact of Outlier Removal and Normalization Approach in Modified k-Means Clustering Algorithm", "author": ["Vaishali R. Patel", "Rupa G. Mehta"], "venue": "IJCSI International Journal of Computer Science Issues,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "K-means Clustering Algorithm with improved Initial Center", "author": ["Zhang Chen", "Xia Shixiong"], "venue": "Second International Workshop on Knowledge Discovery and Data", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "\u201dImproving the Accuracy and Efficiency of the k-means Clustering Algorithm", "author": ["K.A.Abdul Nazeer", "M.P.Sebastian"], "venue": "Proceedings of the World Congress On Engineering", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Mining association rules between sets of items in large database", "author": ["R. Agrawal", "T. Imielinksi", "A. Swami"], "venue": "The ACM SIGMOD Conference, Washington DC,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "Some methods for classification and analysis of ultivariate observations", "author": ["J McQueen"], "venue": "Proc. 5th Berkeley Symp. Math. Statist. Prob., Vol. 1, 1967, pp. 281\u2013297.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1967}], "referenceMentions": [{"referenceID": 3, "context": "Data mining[7][11]or knowledge discovery is a process of analysing large amounts of data and extracting useful information.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "Data mining[7][11]or knowledge discovery is a process of analysing large amounts of data and extracting useful information.", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "Clustering techniques are widely used in various domains like information retrieval, image processing,etc[1][2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "It works by selecting the initial number of clusters and initial centroids[7][13].", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "It works by selecting the initial number of clusters and initial centroids[7][13].", "startOffset": 77, "endOffset": 81}, {"referenceID": 4, "context": "means takes superpolynomial time[8][5].", "startOffset": 32, "endOffset": 35}, {"referenceID": 5, "context": "The computational complexity of k means algorithm is also very high[1][9].", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "So it becomes an essential step before clustering as Euclidean distance is very sensitive to the changes in the differences[3].", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "Authors[3], have proposed data preprocessing techniques like cleaning and normalization to produce optimum quality clusters.", "startOffset": 7, "endOffset": 10}, {"referenceID": 4, "context": "In another work, Authors[8] proposed a new k means clustering method with improved initial centre.", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": "In another research as done by authors[4], a data clustering approach is proposed which works by partitioning the space into different segments and calculating the frequency of data point in each segment and the segment which shows maximum frequency of data point has maximum chances to contain the centroid of the cluster.", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "1 DATA PRE - PROCESSING It is a very important step and should be adopted in clustering as this method uses concepts like constant, average, minimum, maximum, standard deviation to calculates missing values in the tuples[7].", "startOffset": 220, "endOffset": 223}, {"referenceID": 1, "context": "K-Means algorithm uses Euclidean distance that is highly prone to irregularities in the size of various features[3].", "startOffset": 112, "endOffset": 115}], "year": 2015, "abstractText": "K-means is an effective clustering technique used to separate similar data into groups based on initial centroids of clusters. In this paper, Normalization based K-means clustering algorithm(N-K means) is proposed. Proposed N-K means clustering algorithm applies normalization prior to clustering on the available data as well as the proposed approach calculates initial centroids based on weights. Experimental results prove the betterment of proposed N-K means clustering algorithm over existing K-means clustering algorithm in terms of complexity and overall performance. KeywordsClustering, Data mining, K means, Normalization, Weighted Average", "creator": "Microsoft\u00ae Office Word 2007"}}}