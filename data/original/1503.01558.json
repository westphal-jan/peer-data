{"id": "1503.01558", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2015", "title": "What's Cookin'? Interpreting Cooking Videos using Text, Speech and Vision", "abstract": "We present a novel method for aligning a sequence of instructions to a video of someone carrying out a task. In particular, we focus on the cooking domain, where the instructions correspond to the recipe. Our technique relies on an HMM to align the recipe steps to the (automatically generated) speech transcript. We then refine this alignment using a state-of-the-art visual food detector, based on a deep convolutional neural network. We show that our technique outperforms simpler techniques based on keyword spotting. It also enables interesting applications, such as automatically illustrating recipes with keyframes, and searching within a video for events of interest.", "histories": [["v1", "Thu, 5 Mar 2015 07:07:48 GMT  (1602kb,D)", "https://arxiv.org/abs/1503.01558v1", null], ["v2", "Sun, 8 Mar 2015 04:11:49 GMT  (1602kb,D)", "http://arxiv.org/abs/1503.01558v2", "To appear in NAACL 2015"], ["v3", "Fri, 13 Mar 2015 18:55:22 GMT  (1602kb,D)", "http://arxiv.org/abs/1503.01558v3", "To appear in NAACL 2015"]], "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.IR", "authors": ["jonathan malmaud", "jonathan huang", "vivek rathod", "nicholas johnston", "andrew rabinovich", "kevin murphy 0002"], "accepted": true, "id": "1503.01558"}, "pdf": {"name": "1503.01558.pdf", "metadata": {"source": "CRF", "title": "What\u2019s Cookin\u2019? Interpreting Cooking Videos using Text, Speech and Vision", "authors": ["Jonathan Malmaud", "Jonathan Huang", "Vivek Rathod", "Nick Johnston", "Andrew Rabinovich"], "emails": ["malmaud@mit.edu", "kpmurphy}@google.com"], "sections": [{"heading": "1 Introduction", "text": "In recent years, there have been many successful attempts to build large \u201cknowledge bases\u201d (KBs), such as NELL (Carlson et al., 2010), KnowItAll (Etzioni et al., 2011), YAGO (Suchanek et al., 2007), and Google\u2019s Knowledge Graph/ Vault (Dong et al., 2014). These KBs mostly focus on declarative facts, such as \u201cBarack Obama was born in Hawaii\u201d. But human knowledge also encompasses procedural information not yet within the scope of such declarative KBs \u2013 instructions and demonstrations of how to dance the tango, for example, or how to change a tire on your car. A KB for organizing and retrieving such procedural knowledge could be a valuable resource for helping people (and potentially even robots \u2013 e.g., (Saxena et al., 2014; Yang et al., 2015)) learn to perform various tasks.\nIn contrast to declarative information, procedural knowledge tends to be inherently multimodal. In particular, both language and perceptual information are typically used to parsimoniously describe procedures, as evidenced by the large number of \u201chowto\u201d videos and illustrated guides on the open web. To automatically construct a multimodal database of procedural knowledge, we thus need tools for extracting information from both textual and visual sources. Crucially, we also need to figure out how these various kinds of information, which often complement and overlap each other, fit together to a form a structured knowledge base of procedures.\nAs a small step toward the broader goal of aligning language and perception, we focus in this paper on the problem of aligning video depictions of procedures to steps in an accompanying text that corresponds to the procedure. We focus on the cooking domain due to the prevalence of cooking videos on the web and the relative ease of interpreting their recipes as linear sequences of canonical actions. In this domain, the textual source is a user-uploaded recipe attached to the video showing the recipe\u2019s execution. The individual steps of procedures are cooking actions like \u201cpeel an onion\u201d, \u201cslice an onion\u201d, etc. However, our techniques can be applied to any domain that has textual instructions and corresponding videos, including videos at sites such as youtube.com, howcast.com, howdini.com or videojug.com.\nThe approach we take in this paper leverages the fact that the speech signal in instructional videos is often closely related to the actions that the person is performing (which is not true in more general\nar X\niv :1\n50 3.\n01 55\n8v 3\n[ cs\n.C L\n] 1\n3 M\nar 2\nvideos). Thus we first align the instructional steps to the speech signal using an HMM, and then refine this alignment by using a state of the art computer vision system.\nIn summary, our contributions are as follows. First, we propose a novel system that combines text, speech and vision to perform an alignment between textual instructions and instructional videos. Second, we use our system to create a large corpus of 180k aligned recipe-video pairs, and an even larger corpus of 1.4M short video clips, each labeled with a cooking action and a noun phrase. We evaluate the quality of our corpus using human raters. Third, we show how we can use our methods to support applications such as within-video search and recipe auto-illustration."}, {"heading": "2 Data and pre-processing", "text": "We first describe how we collected our corpus of recipes and videos, and the pre-processing steps that we run before applying our alignment model. The corpus of recipes, as well as the results of the alignment model, will be made available for download at github.com/malmaud/whats_cookin."}, {"heading": "2.1 Collecting a large corpus of cooking videos with recipes", "text": "We first searched Youtube for videos which have been automatically tagged with the Freebase mids /m/01mtb (Cooking) and /m/0p57p (recipe), and which have (automatically produced) Englishlanguage speech transcripts, which yielded a collection of 7.4M videos. Of these videos, we kept the videos that also had accompanying descriptive text, leaving 6.2M videos.\nSometimes the recipe for a video is included in this text description, but sometimes it is stored on an external site. For example, a video\u2019s text description might say \u201cClick here for the recipe\u201d. To find the recipe in such cases, we look for sentences in the video description with any of the following keywords: \u201crecipe\u201d, \u201csteps\u201d, \u201ccook\u201d, \u201cprocedure\u201d, \u201cpreparation\u201d, \u201cmethod\u201d. If we find any such tokens, we find any URLs that are mentioned in the same sentence, and extract the corresponding document, giving us an additional 206k documents. We then combine the original descriptive text with any\nadditional text that we retrieve in this way. Finally, in order to extract the recipe from the text description of a video, we trained a classifier that classifies each sentence into 1 of 3 classes: recipe step, recipe ingredient, or background. We keep only the videos which have at least one ingredient sentence and at least one recipe sentence. This last step leaves us with 180,000 videos.\nTo train the recipe classifier, we need labeled examples, which we obtain by exploiting the fact that many text webpages containing recipes use the machine-readable markup defined at http: //schema.org/Recipe. From this we extract 500k examples of recipe sentences, and 500k examples of ingredient sentences. We also sample 500k sentences at random from webpages to represent the non-recipe class. Finally, we train a 3-class na\u0131\u0308ve Bayes model on this data using simple bag-of-words feature vectors. The performance of this model on a separate test set is shown in Table 1."}, {"heading": "2.2 Parsing the recipe text", "text": "For each recipe, we apply a suite of in-house NLP tools, similar to the Stanford Core NLP pipeline. In particular, we perform POS tagging, entity chunking, and constituency parsing (based on a reimplementation of (Petrov et al., 2006)).1 Following (Druck and Pang, 2012), we use the parse tree structure to partition each sentence into \u201cmicro steps\u201d. In particular, we split at any token categorized by the parser as a conjunction only if that token\u2019s parent in the sentence\u2019s constituency parse is a verb phrase. Any recipe step that is missing a verb is considered noise and discarded.\nWe then label each recipe step with an optional action and a list of 0 or more noun chunks. The ac-\n1 Sometimes the parser performs poorly, because the language used in recipes is often full of imperative sentences, such as \u201cMix the flour\u201d, whereas the parser is trained on newswire text. As a simple heuristic for overcoming this, we classify any token at the beginning of a sentence as a verb if it lexically matches a manually-defined list of cooking-related verbs.\ntion label is the lemmatized version of the head verb of the recipe step. We look at all chunked noun entities in the step which are the direct object of the action (either directly or via the preposition \u201cof\u201d, as in \u201cAdd a cup of flour\u201d).\nWe canonicalize these entities by computing their similarity to the list of ingredients associated with this recipe. If an ingredient is sufficiently similar, that ingredient is added to this step\u2019s entity list. Otherwise, the stemmed entity is used. For example, consider the step \u201cMix tomato sauce and pasta\u201d; if the recipe has a known ingredient called \u201cspaghetti\u201d, we would label the action as \u201cmix\u201d and the entities as \u201ctomato sauce\u201d and \u201cspaghetti\u201d, because of its high semantic similarity to \u201cpasta\u201d. (Semantic similarity is estimated based on Euclidean distance between word embedding vectors computed using the method of (Mikolov et al., 2013) trained on general web text.)\nIn many cases, the direct object of a transitive verb is elided (not explicitly stated); this is known as the \u201czero anaphora\u201d problem. For example, the text may say \u201cAdd eggs and flour to the bowl. Mix well.\u201d. The object of the verb \u201cmix\u201d is clearly the stuff that was just added to the bowl (namely the eggs and flour), although this is not explicitly stated. To handle this, we use a simple recency heuristic, and insert the entities from the previous step to the current step."}, {"heading": "2.3 Processing the speech transcript", "text": "The output of Youtube\u2019s ASR system is a sequence of time-stamped tokens, produced by a standard Viterbi decoding system. We concatenate these tokens into a single long document, and then apply our NLP pipeline to it. Note that, in addition to errors introduced by the ASR system2, the NLP system can introduce additional errors, because it does not work well on text that may be ungrammatical and which is entirely devoid of punctuation and sentence boundary markers.\nTo assess the impact of these combined sources\n2 According to (Liao et al., 2013), the Youtube ASR system we used, based on using Gaussian mixture models for the acoustic model, has a word error rate of about 52% (averaged over all English-language videos; some genres, such as news, had lower error rates). The newer system, which uses deep neural nets for the acoustic model, has an average WER of 44%; however, this was not available to us at the time we did our experiments.\nof error, we also collected a much smaller set of 480 cooking videos (with corresponding recipe text) for which the video creator had uploaded a manually curated speech transcript; this has no transcription errors, it contains sentence boundary markers, and it also aligns whole phrases with the video (instead of just single tokens). We applied the same NLP pipeline to these manual transcripts. In the results section, we will see that the accuracy of our end-toend system is indeed higher when the speech transcript is error-free and well-formed. However, we can still get good results using noisier, automatically produced transcripts."}, {"heading": "3 Methods", "text": "In this section, we describe our system for aligning instructional text and video."}, {"heading": "3.1 HMM to align recipe with ASR transcript", "text": "We align each step of the recipe to a corresponding sequence of words in the ASR transcript by using the input-output HMM shown in Figure 1. Here X(1 : K) represents the textual recipe steps (obtained using the process described in Section 2.2); Y (1 : T ) represent the ASR tokens (spoken words); R(t) \u2208 {1, . . . ,K} is the recipe step number for frame t; and B(t) \u2208 {0, 1} represents whether timestep t is generated by the background (B = 1) or foreground model (B = 0). This background variable is needed since sometimes sequences of spoken words are unrelated to the content of the recipe, especially at the beginning and end of a video.\nThe conditional probability distributions (CPDs) for the Markov chain is as follows:\np(R(t) = r|R(t\u2212 1) = r\u2032) =  \u03b1 if r = r\u2032+1 1\u2212 \u03b1 if r = r\u2032 0.0 otherwise\np(B(t) = b|B(t\u2212 1) = b) = \u03b3.\nThis encodes our assumption that the video follows the same ordering as the recipe and that background/foreground tokens tend to cluster together. Obviously these assumptions do not always hold, but they are a reasonable approximation.\nFor each recipe, we set \u03b1 = K/T , the ratio of recipe steps to transcript tokens. This setting corresponds to an a priori belief that each recipe step is aligned with the same number of transcript tokens. The parameter \u03b3 in our experiments is set by crossvalidation to 0.7 based on a small set of manuallylabeled recipes.\nFor the foreground observation model, we generate the observed word from the corresponding recipe step via:\nlog p(Y (t) = y|R(t) = k,X(1 : K), B(t) = 0) \u221d max({WordSimilarity(y, x) : x \u2208 X(k)}),\nwhere X(k) is the set of words in the k\u2019th recipe step, and WordSimilarity(s, t) is a measure of similarity between words s and t, based on word vector distance.\nIf this frame is aligned to the background, we generate it from the empirical distribution of words, which is estimated based on pooling all the data:\np(Y (t) = y|R(t) = k,B(t) = 1) = p\u0302(y).\nFinally, the prior for p(B(t)) is uniform, and p(R(1)) is set to a delta function on R(1) = 1 (i.e., we assume videos start at step 1 of the recipe).\nHaving defined the model, we \u201cflatten\u201d it to a standard HMM (by taking the cross product of Rt and Bt), then estimate the MAP sequence using the Viterbi algorithm. See Figure 2 for an example.\nFinally, we label each segment of the video as follows: use the segmentation induced by the alignment, and extract the action and object from the corresponding recipe step as described in Section 2.2. If the segment was labeled as background by the HMM, we do not apply any label to it."}, {"heading": "3.2 Keyword spotting", "text": "A simpler approach to labeling video segments is to just search for verbs in the ASR transcript, and then to extract a fixed-sized window around the timestamp where the keyword occurred. We call this approach \u201ckeyword spotting\u201d. A similar method from\n(Yu et al., 2014) filters ASR transcripts by part-ofspeech tag and finds tokens that match a small vocabulary to create a corpus of video clips (extracted from instructional videos), each labeled with an action/object pair.\nIn more detail, we manually define a whitelist of \u223c200 actions (all transitive verbs) of interest, such as \u201cadd\u201d, \u201cchop\u201d, \u201cfry\u201d, etc. We then identify when these words are spoken (relying on the POS tags to filter out non-verbs), and extract an 8 second video clip around this timestamp. (Using 2 seconds prior to the action being mentioned, and 6 seconds following.) To extract the object, we take all tokens tagged as \u201cnoun\u201d within 5 tokens after the action."}, {"heading": "3.3 Hybrid HMM + keyword spotting", "text": "We cannot use keyword spotting if the goal is to align instructional text to videos. However, if our goal is just to create a labeled corpus of video clips, keyword spotting is a reasonable approach. Unfortunately, we noticed that the quality of the labels (especially the object labels) generated by keyword spotting was not very high, due to errors in the ASR. On the other hand, we also noticed that the recall of the HMM approach was about 5 times lower than using keyword spotting, and furthermore, that the temporal localization accuracy was sometimes worse.\nTo get the best of both worlds, we employ the following hybrid technique. We perform keyword spotting for the action in the ASR transcript as before, but use the HMM alignment to infer the corresponding object. To avoid false positives, we only use the output of the HMM for this video if at least half of the recipe steps are aligned by it to the speech transcript; otherwise we back off to the baseline approach of extracting the noun phrase from the ASR transcript in the window after the verb."}, {"heading": "3.4 Temporal refinement using vision", "text": "In our experiments, we noticed that sometimes the narrator describes an action before actually performing it (this was also noted in (Yu et al., 2014)). To partially combat this problem, we used computer vision to refine candidate video segments as follows. We first trained visual detectors for a large collection of food items (described below). Then, given a candidate video segment annotated with an action/object pair (coming from any of the previous\nthree methods), we find a translation of the window (of up to 3 seconds in either direction) for which the average detector score corresponding to the object is maximized. The intuition is that by detecting when the object in question is visually present in the scene, it is more likely that the corresponding action is actually being performed.\nTraining visual food detectors. We trained a deep convolutional neural network (CNN) classifier (specifically, the 16 layer VGG model from (Simonyan and Zisserman, 2014)) on the FoodFood101 dataset of (Bossard et al., 2014), using the Caffe open source software (Jia et al., 2014). The Food101 dataset contains 1000 images for 101 different kinds of food. To compensate for the small training set, we pretrained the CNN on the ImageNet dataset (Russakovsky et al., 2014), which has 1.2M images, and then fine-tuned on Food-101. After a few hours of fine tuning (using a single GPU), we obtained 79% classification accuracy (assuming all 101 labels are mutually exclusive) on the test set, which is consistent with the state of the art results.3\n3 In particular, the website https://www.metamind. io/vision/food (accessed on 2/25/15) claims they also got 79% on this dataset. This is much better than the 56.4% for a CNN reported in (Bossard et al., 2014). We believe the main reason for the improved performance is the use of pre-training on ImageNet.\nWe then trained our model on an internal, proprietary dataset of 220 million images harvested from Google Images and Flickr. About 20% of these images contain food, the rest are used to train the background class. In this set, there are 2809 classes of food, including 1005 raw ingredients, such as avocado or beef, and 1804 dishes, such as ratatouille or cheeseburger with bacon. We use the model trained on this much larger dataset in the current paper, due to its increased coverage. (Unfortunately, we cannot report quantitative results, since the dataset is very noisy (sometimes half of the labels are wrong), so we have no ground truth. Nevertheless, qualitative behavior is reasonable, and the model does well on Food-101, as we discussed above.)\nVisual refinement pipeline. For storage and time efficiency, we downsample each video temporally to 5 frames per second and each frame to 224 \u00d7 224 before applying the CNN. Running the food detector on each video then produces a vector of scores (one entry for each of 2809 classes) per timeframe.\nThere is not a perfect map from the names of ingredients to the names of the detector outputs. For example, an omelette recipe may say \u201cegg\u201d, but there are two kinds of visual detectors, one for \u201cscrambled egg\u201d and one for \u201craw egg\u201d. We therefore decided to define the match score between an ingredient and a frame by taking the maximum\nscore for that frame over all detectors whose names matched any of the ingredient tokens (after lemmatization and stopword filtering).\nFinally, the match score of a video segment to an object is computed by taking the average score of all frames within that segment. By then scoring and maximizing over all translations of the candidate segment (of up to three seconds away), we produce a final \u201crefined\u201d segment."}, {"heading": "3.5 Quantifying confidence via vision and affordances", "text": "The output of the keyword spotting and/or HMM systems is an (action, object) label assigned to certain video clips. In order to estimate how much confidence we have in that label (so that we can trade off precision and recall), we use a linear combination of two quantities: (1) the final match score produced by the visual refinement pipeline, which measures the visibility of the object in the given video segment, and (2) an affordance probability, measuring the probability that o appears as a direct object of a.\nThe affordance model allows us to, for example, prioritize a segment labeled as (peel, garlic) over a segment labeled as (peel, sugar). The probabilities P (object = o|action = a) are estimated by first forming an inverse document frequency matrix capturing action/object co-occurrences (treating actions as documents). To generalize across actions and objects we form a low-rank approximation to this IDF matrix using a singular value decomposition and set affordance probabilities to be proportional to exponentiated entries of the resulting matrix. Figure 3 visualizes these affordance probabilities for a selected subset of frequently used action/object pairs."}, {"heading": "4 Evaluation and applications", "text": "In this section, we experimentally evaluate how well our methods work. We then briefly demonstrate some prototype applications."}, {"heading": "4.1 Evaluating the clip database", "text": "One of the main outcomes of our process is a set of video clips, each of which is labeled with a verb (action) and a noun (object). We generated 3 such labeled corpora, using 3 different methods: keyword spotting (\u201cKW\u201d), the hybrid HMM + keyword spotting (\u201cHybrid\u201d), and the hybrid system with visual\nfood detector (\u201cvisual refinement\u201d). The total number of clips produced by each method is very similar, approximately 1.4 million. The coverage of the clips is approximately 260k unique (action, noun phrase) pairs.\nTo evaluate the quality of these methods, we created a random subset of 900 clips from each corpus using stratified sampling. That is, we picked an action uniformly at random, and then picked a corresponding object for that action from its support set uniformly at random, and finally picked a clip with that (action, object) label uniformly at random from the clip corpuses produced in Section 3; this ensures\nthe test set is not dominated by frequent actions or objects.\nWe then performed a Mechanical Turk experiment on each test set. Each clip was shown to 3 raters, and each rater was asked the question \u201cHow well does this clip show the given action/object?\u201d. Raters then had to answer on a 3-point scale: 0 means \u201cnot at all\u201d, 1 means \u201csomewhat\u201d, and 2 means \u201cvery well\u201d.\nThe results are shown in Figure 4. We see that the quality of the hybrid method is significantly better than the baseline keyword spotting method, for both actions and objects.4 While a manually curated\n4 Inter-rater agreement, measured via Fleiss\u2019s kappa by aggregating across all judgment tasks, is .41, which is statistically significant at a p < .05 level.\nspeech transcript indeed yields better results (see the bars labeled \u2018manual\u2019), we observe that automatically generated transcripts allow us to perform almost as well, especially using our alignment model with visual refinement.\nComparing accuracy on actions against that on objects in the same figure, we see that keyword spotting is far more accurate for actions than it is for objects (by over 30%). This disparity is not surprising since keyword spotting searches only for action keywords and relies on a rough heuristic to recover objects. We also see that using alignment (which extracts the object from the \u201cclean\u201d recipe text) and visual refinement (which is trained explicitly to detect ingredients) both help to increase the relative accuracy of objects \u2014 under the hybrid method, for example, the accuracy for actions is only 8% better than that of objects.\nNote that clips from the HMM and hybrid methods varied in length between 2 and 10 seconds (mean 4.2 seconds), while clips from the keyword spotting method were always exactly 8 seconds. Thus clip length is potentially a confounding factor in the evaluation when comparing the hybrid method to the keyword-spotting method; however, if there is a bias to assign higher ratings to longer clips (which are a priori more likely to contain a depiction of a given action than shorter clips), it would benefit the keyword spoting method.\nSegment confidence scores (from Section 3.5) can be used to filter out low confidence segments, thus improving the precision of clip retrieval at the cost of recall. Figure 5 visualizes this trade-off as we vary our confidence threshold, showing that indeed, segments with higher confidences tend to have the highest quality as judged by our human raters. Moreover, the top 167,000 segments as ranked by our confidence measure have an average rating exceeding 1.75.\nWe additionally sought to evaluate how well recipe steps from the recipe body could serve as captions for video clips in comparison to the often noisy ASR transript, which serves as a rough proxy for evaluating the quality of the alignment model as well as demonstration a potential application of our method for \u201ccleaning up\u201d noisy ASR captions into complete grammatical sentences. To that end, we randomly selected 200 clips from our corpus that\nboth have an associated action keyword from the transcript as well as an aligned recipe step selected by the HMM alignment model. For each clip, three raters on Mechanical Turk were shown the clip, the text from the recipe step, and a fragment of the ASR transcript (the keyword, plus 5 tokens to the left and right of the keyword). Raters then indicated which description they preferred: 2 indicates a strong preference for the recipe step, 1 a weak preference, 0 indifference, -1 a weak preference for the transcript fragment, and -2 a strong preference. Results are shown in Figure 6. Excluding raters who indicated indiffierence, 67% of raters preferred the recipe step as the clip\u2019s description.\nA potential confound for using this analysis as a proxy for the quality of the alignment model is that the ASR transcript is generally an ungrammatical sentence fragment as opposed to the grammatical recipe steps, which is likely to reduce the raters\u2019 approval of ASR captions in the case when both accurately describe the scene. However, if users still on average prefer an ASR sentence fragment which describes the clip correctly versus a full recipe step which is unrelated to the scene, then this experiment still provides evidence of the quality of the alignment model."}, {"heading": "4.2 Automatically illustrating a recipe", "text": "One useful byproduct of our alignment method is that each recipe step is associated with a segment of the corresponding video.5 We use a standard keyframe selection algorithm to pick the best frame from each segment. We can then associate this frame with the corresponding recipe step, thus automatically illustrating the recipe steps. An illustration of this process is shown in Figure 7."}, {"heading": "4.3 Search within a video", "text": "Another application which our methods enable is search within a video. For example, if a user would like to find a clip illustrating how to knead dough, we can simply search our corpus of labeled clips,\n5 The HMM may assign multiple non-consecutive regions of the video to the same recipe step (since the background state can turn on and off). In such cases, we just take the \u201cconvex hull\u201d of the regions as the interval which corresponds to that step. It is also possible for the HMM not to assign a given step to any interval of the video.\nand return a list of matches (ranked by confidence). Since each clip has a corresponding \u201cprovenance\u201d, we can return the results to the user as a set of videos in which we have automatically \u201cfast forwarded\u201d to the relevant section of the video (see Figure 8 for an example). This stands in contrast to standard video search on Youtube, which returns the whole video, but does not (in general) indicate where within the video the user\u2019s search query occurs."}, {"heading": "5 Related work", "text": "There are several pieces of related work. (Yu et al., 2014) performs keyword spotting in the speech transcript in order to label clips extracted from instructional videos. However, our hybrid approach performs better; the gain is especially significant on automatically generated speech transcripts, as shown in Figure 4.\nThe idea of using an HMM to align instructional steps to a video was also explored in (Naim et al., 2014). However, their conditional model has to generate images, whereas ours just has to generate ASR words, which is an easier task. Furthermore, they only consider 6 videos collected in a controlled lab setting, whereas we consider over 180k videos collected \u201cin the wild\u201d.\nAnother paper that uses HMMs to process recipe text is (Druck and Pang, 2012). They use the HMM to align the steps of a recipe to the comments made by users in an online forum, whereas we align the steps of a recipe to the speech transcript. Also, we use video information, which was not considered in this earlier work.\n(Joshi et al., 2006) describes a system to automatically illustrate a text document, however they only generate one image, not a sequence, and their techniques are very different.\nThere is also a large body of other work on connecting language and vision; we only have space to\nbriefly mention a few key papers. (Rohrbach et al., 2012b) describes the MPII Cooking Composite Activities dataset, which consists of 212 videos collected in the lab of people performing various cooking activities. (This extends the dataset described in their earlier work, (Rohrbach et al., 2012a).) They also describe a method to recognize objects and actions using standard vision features. However, they do not leverage the speech signal, and their dataset is significantly smaller than ours.\n(Guadarrama et al., 2013) describes a method for generating subject-verb-object triples given a short video clip, using standard object and action detectors. The technique was extended in (Thomason et al., 2014) to also predict the location/ place. Furthermore, they use a linear-chain CRF to combine the visual scores with a simple (s,v,o,p) language model (similar to our affordance model). They applied their technique to the dataset in (Chen and Dolan, 2011), which consists of 2000 short video clips, each described with 1-3 sentences. By contrast, we focus on aligning instructional text to the video, and our corpus is significantly larger.\n(Yu and Siskind, 2013) describes a technique for estimating the compatibility between a video clip and a sentence, based on relative motion of the objects (which are tracked using HMMs). Their method is tested on 159 video clips, created under carefully controlled conditions. By contrast, we focus on aligning instructional text to the video, and our corpus is significantly larger."}, {"heading": "6 Discussion and future work", "text": "In this paper, we have presented a novel method for aligning instructional text to videos, leveraging both speech recognition and visual object detection. We\nhave used this to align 180k recipe-video pairs, from which we have extracted a corpus of 1.4M labeled video clips \u2013 a small but crucial step toward building a multimodal procedural knowlege base. In the future, we hope to use this labeled corpus to train visual action detectors, which can then be combined with the existing visual object detectors to interpret novel videos. Additionally, we believe that combining visual and linguistic cues may help overcome longstanding challenges to language understanding, such as anaphora resolution and word sense disambiguation.\nAcknowledgments. We would like to thank Alex Gorban and Anoop Korattikara for helping with some of the experiments, and Nancy Chang for feedback on the paper."}], "references": [{"title": "Food-101 \u2013 mining discriminative components with random forests", "author": ["Bossard et al", "L. 2014] Bossard", "M. Guillaumin", "L. Van Gool"], "venue": "In Proc. European Conf. on Computer Vision", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Toward an architecture for never-ending language learning", "author": ["Carlson et al", "A. 2010] Carlson", "J. Betteridge", "B. Kisiel", "B. Settles", "E.H. Jr.", "T. Mitchell"], "venue": "In Procs. AAAI", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["Chen", "Dolan", "D.L. 2011] Chen", "W.B. Dolan"], "venue": "In Proc. ACL,", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Knowledge vault: A webscale approach to probabilistic knowledge fusion", "author": ["Dong et al", "X. 2014] Dong", "E. Gabrilovich", "G. Heitz", "W. Horn", "N. Lao", "K. Murphy", "T. Strohmann", "S. Sun", "W. Zhang"], "venue": "In Proc. of the Int\u2019l Conf. on Knowledge Discovery", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Spice it up?: Mining refinements to online instructions from user generated content", "author": ["Druck", "Pang", "G. 2012] Druck", "B. Pang"], "venue": "In Proc. ACL,", "citeRegEx": "Druck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Druck et al\\.", "year": 2012}, {"title": "Open Information Extraction: the Second Generation", "author": ["Etzioni et al", "O. 2011] Etzioni", "A. Fader", "J. Christensen", "S. Soderland", "Mausam"], "venue": "In Intl. Joint Conf. on AI", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "YouTube2Text: Recognizing and describing arbitrary activities using semantic hierarchies and Zero-Shot recognition", "author": ["Guadarrama et al", "S. 2013] Guadarrama", "N. Krishnamoorthy", "G. Malkarnenkar", "S. Venugopalan", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia et al", "Y. 2014] Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "The story picturing Engine-A system for automatic text illustration", "author": ["Joshi et al", "D. 2006] Joshi", "J.Z. Wang", "J. Li"], "venue": "ACM Trans. Multimedia Comp., Comm. and Appl.,", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Large scale deep neural network acoustic modeling with semi-supervised training data for YouTube video transcription", "author": ["Liao et al", "H. 2013] Liao", "E. McDermott", "A. Senior"], "venue": "In ASRU (IEEE Automatic Speech Recognition and Understanding Work-", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space. http://arxiv.org/abs/1301.3781", "author": ["Mikolov et al", "T. 2013] Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Unsupervised alignment of natural language instructions with video segments", "author": ["Naim et al", "I. 2014] Naim", "Y.C. Song", "Q. Liu", "H. Kautz", "J. Luo", "D. Gildea"], "venue": "In Procs. of AAAI", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Petrov et al", "S. 2006] Petrov", "L. Barrett", "R. Thibaux", "D. Klein"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Associ-", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "A database for fine grained activity detection of cooking activities", "author": ["Rohrbach et al", "M. 2012a] Rohrbach", "S. Amin", "M. Andriluka", "B. Schiele"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Script data for Attribute-Based recognition of composite activities", "author": ["Rohrbach et al", "M. 2012b] Rohrbach", "M. Regneri", "M. Andriluka", "S. Amin", "M. Pinkal", "B. Schiele"], "venue": "In Proc. European Conf. on Computer Vision,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Russakovsky et al", "O. 2014] Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "RoboBrain: Large-Scale knowledge engine for robots. http://arxiv.org/pdf/1412.0691.pdf", "author": ["Saxena et al", "A. 2014] Saxena", "A. Jain", "O. Sener", "A. Jami", "D.K. Misra", "H.S. Koppula"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for Large-Scale image recognition. http://arxiv.org/abs/1409.1556", "author": ["Simonyan", "Zisserman", "K. 2014] Simonyan", "A. Zisserman"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "YAGO: A Large Ontology from Wikipedia and WordNet", "author": ["Suchanek et al", "F.M. 2007] Suchanek", "G. Kasneci", "G. Weikum"], "venue": "J. Web Semantics,", "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "Integrating language and vision to generate natural language descriptions of videos in the wild", "author": ["Thomason et al", "J. 2014] Thomason", "S. Venugopalan", "S. Guadarrama", "K. Saenko", "R. Mooney"], "venue": "In Intl. Conf. on Comp. Linguistics", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Robot learning manipulation action plans by watching unconstrained videos from the world wide web", "author": ["Yang et al", "Y. 2015] Yang", "Y. Li", "C. Ferm\u00fcller", "Y. Aloimonos"], "venue": "In The Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15)", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Grounded language learning from video described with sentences", "author": ["Yu", "Siskind", "H. 2013] Yu", "J. Siskind"], "venue": "In Proc. ACL", "citeRegEx": "Yu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2013}, {"title": "Instructional videos for unsupervised harvesting and learning of action examples", "author": ["Yu et al", "2014] Yu", "S.-I", "L. Jiang", "A. Hauptmann"], "venue": "In Intl. Conf. Multimedia,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}], "referenceMentions": [], "year": 2015, "abstractText": "We present a novel method for aligning a sequence of instructions to a video of someone carrying out a task. In particular, we focus on the cooking domain, where the instructions correspond to the recipe. Our technique relies on an HMM to align the recipe steps to the (automatically generated) speech transcript. We then refine this alignment using a state-of-the-art visual food detector, based on a deep convolutional neural network. We show that our technique outperforms simpler techniques based on keyword spotting. It also enables interesting applications, such as automatically illustrating recipes with keyframes, and searching within a video for events of interest.", "creator": "LaTeX with hyperref package"}}}