{"id": "1503.05938", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2015", "title": "On Invariance and Selectivity in Representation Learning", "abstract": "We discuss data representation which can be learned automatically from data, are invariant to transformations, and at the same time selective, in the sense that two points have the same representation only if they are one the transformation of the other. The mathematical results here sharpen some of the key claims of i-theory -- a recent theory of feedforward processing in sensory cortex.", "histories": [["v1", "Thu, 19 Mar 2015 20:30:46 GMT  (35kb,D)", "http://arxiv.org/abs/1503.05938v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["fabio anselmi", "lorenzo rosasco", "tomaso poggio"], "accepted": false, "id": "1503.05938"}, "pdf": {"name": "1503.05938.pdf", "metadata": {"source": "CRF", "title": "On Invariance and Selectivity in Representation Learning", "authors": ["Fabio Anselmi", "Lorenzo"], "emails": [], "sections": [{"heading": null, "text": "Keywords:Invariance, Machine Learning The paper is submitted to Information and Inference Journal."}, {"heading": "1 Introduction", "text": "This paper considers the problem of learning \u201dgood\u201d data representation which can lower the need of labeled data (sample complexity) in machine learning (ML). Indeed, while current ML systems have achieved impressive results in a variety of tasks, an obvious bottleneck appears to be the huge amount of labeled data needed. This paper builds on the idea that data representation, which are learned in an unsupervised manner, can be key to solve the problem. Classical statistical learning theory focuses on supervised learning and postulates that a suitable hypothesis space is given. In turn, under very general conditions, the latter can be seen to be equivalent to a data representation. In other words, data representation and how to select and learn it, is classically not considered to be part of the learning problem, but rather as a prior information. In practice ad hoc solutions are often empirically found for each problem.\nThe study in this paper is a step towards developing a theory of learning data representation. Our starting point is the intuition that, since many learning\n1Center for Brains Minds and Machines, Massachusetts Institute of Technology, Cambridge, MA 02139\n2Laboratory for Computational Learning, Istituto Italiano di Tecnologia and Massachusetts Institute of Technology\n3DIBRIS, Universita\u0301 degli studi di Genova, Italy, 16146\nar X\niv :1\n50 3.\n05 93\n8v 1\n[ cs\n.L G\n] 1\n9 M\ntasks are invariant to transformations of the data, learning invariant representation from \u201cunsupervised\u201d experiences can significantly lower the \u201dsize\u201d of the problem, effectively decreasing the need of labeled data. In the following, we formalize the above idea and discuss how such invariant representations can be learned. Crucial to our reasoning is the requirement for invariant representations to satisfy a form of selectivity, broadly referred to as the property of distinguishing images which are not one the transformation of the other. Indeed, it is this latter requirement that informs the design of non trivial invariant representations. Our work is motivated by a theory of cortex and in particular visual cortex [5].\nData representation is a classical concept in harmonic analysis and signal processing. Here representations are typically designed on the basis of prior information assumed to be available. More recently, there has been an effort to automatically learn adaptive representation on the basis of data samples. Examples in this class of methods include so called dictionary learning [30], autoencoders [6] and metric learning techniques (see e.g. [33]). The idea of deriving invariant data representation has been considered before. For example in the analysis of shapes [19] and more generally in computational topology [10], or in the design of positive definite functions associated to reproducing kernel Hilbert spaces [12]. However, in these lines of study the selectivity properties of the representations have hardly been considered. The ideas in [22, 28] are close in spirit to the study in this paper. In particular, the results in [22] develop a different invariant and stable representation within a signal processing framework. In [28] an information theoretic perspective is considered to formalize the problem of learning invariant/selective representations.\nIn this work we develop a machine learning perspective closely following computational neuroscience models of the information processing in the visual cortex [15, 16, 26]. Our first and main result shows that, for compact groups, representation defined by nonlinear group averages can be shown to be invariant, as well as selective, to the action of the group. While invariance follows from the properties of the Haar measure associated to the group, selectivity is shown using probabilistic results that characterize a probability measure in terms of one dimensional projections. This set of ideas, which form the core of the paper, is then extended to local transformations, and multilayer architectures. These results bear some understanding to the nature of certain deep architecture, in particular neural networks of the convolution type.\nThe rest of the paper is organized as follows. We describe the concept of invariance and selective representation in Section 2 and their role for learning in Section 3. We discuss a family of invariant/selective representation for transformations which belong to compact groups in Section 4 that we further develop in Sections 5 and 6. Finally we conclude in Section 7 with some final comments."}, {"heading": "2 Invariant and Selective Data Representations", "text": "We next formalize and discuss the notion of invariant and selective data representation, which is the main focus of the rest of the paper.\nWe model the data space as a (real separable) Hilbert space I and denote by \u3008\u00b7, \u00b7\u3009 and \u2016\u00b7\u2016 the inner product and norm, respectively. Example of data spaces are one dimensional signals (as in audio data), where we could let I \u2282 L2(R), or two dimensional signals (such as images), where we could let I \u2282 L2(R2). After discretization, data can often be seen as vectors in high-dimensional Euclidean spaces, e.g. I = Rd. The case of (digital) images serves as a main example throughout the paper.\nA data representation is a map from the data space in a suitable representation space, that is\n\u00b5 : I \u2192 F .\nIndeed, the above concept appears under different names in various branch of pure and applied sciences, e.g. it is called an encoding (information theory), a feature map (learning theory), a transform (harmonic analysis/signal processing) or an embedding (computational geometry).\nIn this paper, we are interested in representations which are invariant (see below) to suitable sets of transformations. The latter can be seen as a set of maps\nG \u2282 {g | g : I \u2192 I}.\nMany interesting examples of transformations have a group structure. Recall that a group is a set endowed with a well defined composition/multiplication operation satisfying four basic properties,\n\u2022 closure: gg\u2032 \u2208 G, for all g, g\u2032 \u2208 G\n\u2022 associativity: (gg\u2032)g\u2032\u2032 = g(g\u2032g\u2032\u2032), for all g, g\u2032, g\u2032\u2032 \u2208 G\n\u2022 identity: there exists Id \u2208 G such that Idg = gId = g, for all g \u2208 G.\n\u2022 invertibility: for all g \u2208 G there exists g\u22121 \u2208 G such that (gg\u22121) = Id.\nThere are different kind of groups. In particular, \u201csmall\u201d groups such as compact (or locally compact, i.e. a group that admits a locally compact Hausdorff topology such that the group operations of composition and inversion are continuous.) groups, or \u201clarge\u201d groups which are not locally compact. In the case of images, examples of locally compact groups include affine transformations (e.g. scaling, translations, rotations and their combinations) which can be thought of as suitable viewpoint changes. Examples of non locally compact groups are diffeomorphisms, which can be thought of as various kind of local or global deformations.\nExample 1. Let I \u2208 L2(R). A basic example of group transformation is given by the translation group, which can be represented as a family of linear operators\nT\u03c4 : L 2(R)\u2192 L2(R), T\u03c4I(p) = I(p\u2212 \u03c4), \u2200p \u2208 R, I \u2208 I,\nfor \u03c4 \u2208 R. Other basic examples of locally compact groups include scaling (the multiplication group) and affine transformations (affine group). Given a smooth map d : R\u2192 R a diffeomorphism can also be seen as a linear operator given by\nDd : L 2(R)\u2192 L2(R), DdI(p) = I(d(p)), \u2200p \u2208 R, I \u2208 I.\nNote that also in this case the representation is linear.\nClearly, not all transformations have a group structure\u2013 think for example of images obtained from three dimensional rotations of an object. Given the above premise, we next discuss, properties of data representation with respect to transformations. We first add one remark about the notation.\nRemark 1 (Notation: Group Action and Representation). If G is a group and I a set, the group action is the map (g, x) 7\u2192 g.x \u2208 I. In the following, with an abuse of notation we will denote by gx the group action. Indeed, when I is a linear space, we also often denote by g both a group element and its representation, so that g can be identified with a linear operator. Throughout the article we assume the group representation to be unitary [25].\nTo introduce the notion of invariant representation, we recall that an orbit associated to an element I \u2208 I is the set OI \u2282 I given by OI = {I \u2032 \u2208 I | I \u2032 = gI, g \u2208 G}. Orbits form a partition of I in equivalence classes, with respect to the equivalence relation,"}, {"heading": "I \u223c I \u2032 \u21d4 \u2203 g \u2208 G such that gI = I \u2032,", "text": "for all I, I \u2032 \u2208 I. We have the following definition.\nDefinition 1 (Invariant Representation). We say that a representation \u00b5 is invariant with respect to G if\nI \u223c I \u2032 \u21d2 \u00b5(I) = \u00b5(I \u2032),\nfor all I, I \u2032 \u2208 I.\nIn words, the above definition states that if two data points are one the transformation of the other, than they will have the same representation. Indeed, if a representation \u00b5 is invariant\n\u00b5(I) = \u00b5(gI)\nfor all I \u2208 I, g \u2208 G. Clearly, trivial invariant representations can be defined, e.g. the constant function. This motivates a second requirement, namely selectivity.\nDefinition 2 (Selective Representation). We say that a representation \u00b5 is selective with respect to G if\n\u00b5(I) = \u00b5(I \u2032)\u21d2 I \u223c I \u2032,\nfor all I, I \u2032 \u2208 I.\nTogether with invariance, selectivity asserts that two points have the same representation if and only if they are one a transformation of the other. Several comments are in order. First, the requirement of exact invariance as in Definition 1, seems desirable for (locally) compact groups, but not for non locally compact group such as diffeomorphisms. In this case, requiring a form of stability to small transformations seems to be natural, as it is more generally to require stability to small perturbations, e.g. noise (see [22]). Second, the concept of selectivity is natural and requires that no two orbits are mapped in the same representation. It corresponds to an injectivity property of a representation on the quotient space I/ \u223c. Assuming F to be endowed with a metric dF , a stronger requirement would be to characterize the metric embedding induced by \u00b5, that is to control the ratio (or the deviation) of the distance of two representation and the distance of two orbits. Indeed, the problem of finding invariant and selective representation, is tightly related to the problem of finding an injective embedding of the quotient space I/ \u223c.\nWe next provide a discussion of the potential impact of invariant representations on the solution of subsequent learning tasks."}, {"heading": "3 From Invariance to Low Sample Complexity", "text": "In this section we first recall how the concepts of data representation and hypothesis space are closely related, and how the sample complexity of a supervised problem can be characterized by the covering numbers of the hypothesis space. Then, we discuss how invariant representations can lower the sample complexity of a supervised learning problem.\nSupervised learning amounts to finding an input-output relationship on the basis of a training set of input-output pairs. Outputs can be scalar or vector valued, as in regression, or categorical, as in multi-category or multi-label classification, binary classification being a basic example. The bulk of statistical learning theory is devoted to study conditions under which learning problems can be solved, approximately and up to a certain confidence, provided a suitable hypothesis space is given. A hypotheses space is a subset\nH \u2282 {f | f : I \u2192 Y},\nof the set of all possible input output relations. As we comment below, under very general assumptions hypothesis spaces and data representations are equivalent concepts."}, {"heading": "3.1 Data Representation and Hypothesis Space", "text": "Indeed, practically useful hypothesis spaces are typically endowed with a Hilbert space structure, since it is in this setting that most computational solutions can be developed. A further natural requirement is for evaluation functions to be well defined and continuous. This latter property allows to give a well defined\nmeaning of the evaluation of a function at every points, a property which is arguably natural since we are interested in making predictions. The requirements of 1) being a Hilbert space of of functions and 2) have continuous evaluation functionals, define so called reproducing kernel Hilbert spaces [24]. Among other properties, these spaces of functions are characterized by the existence of a feature map \u00b5 : I \u2192 F , which is a map from the data space into a feature space which is itself a Hilbert space. Roughly speaking, functions in a RKHS H with an associated feature map \u00b5 can be seen as hyperplanes in the feature space, in the sense that \u2200f \u2208 H, there exists w \u2208 F such that\nf(I) = \u3008w, \u00b5(I)\u3009F , \u2200I \u2208 I.\nThe above discussion illustrates how, under mild assumptions, the choice of a hypothesis space is equivalent to the choice of a data representation (a feature map). In the next section, we recall how hypothesis spaces, hence data representation, are usually assumed to be given in statistical learning theory and are characterized in terms of sample complexity."}, {"heading": "3.2 Sample Complexity in Supervised Learning", "text": "Supervised statistical learning theory characterizes the difficulty of a learning problem in terms of the \u201dsize\u201d of the considered hypothesis space, as measured by suitable capacity measures. More precisely, given a measurable loss function V : Y \u00d7 Y \u2192 [0,\u221e), for any measurable function f : I \u2192 Y the expected error is defined as\nE(f) = \u222b V (f(I), y)d\u03c1(I, y)\nwhere \u03c1 is a probability measure on I\u00d7Y. Given a training set Sn = {(I1, y1), . . . , (In, yn)} of input-output pairs sampled identically and independently with respect to \u03c1, and a hypothesis space H, the goal of learning is to find an approximate solution fn = fSn \u2208 H to the problem\ninf f\u2208H E(f)\nThe difficulty of a learning problem is captured by the following definition.\nDefinition 3 (Learnability and Sample Complexity). A hypothesis space H is said to be learnable if, for all \u2208 [0,\u221e), \u03b4 \u2208 [0, 1], there exists n( , \u03b4,H) \u2208 N such that\ninf fn sup \u03c1\nP ( E(fn)\u2212 inf\nf\u2208H E(f) \u2265\n) \u2264 \u03b4. (1)\nThe quantity n( , \u03b4,H) is called the sample complexity of the problem.\nThe above definition characterizes the complexity of the learning problem associated to a hypothesis space H, in terms of the existence of an algorithm that, provided with at least n( , \u03b4,H) training set points, can approximately solve the learning problem on H with accuracy and confidence \u03b4.\nThe sample complexity associated to a hypothesis space H can be derived from suitable notions of covering numbers, and related quantities, that characterize the size of H. Recall that, roughly speaking, the covering number N associated to a (metric) space is defined as the minimal number of balls needed to cover the space. The sample complexity can be shown [31, 9] to be proportional to the logarithm of the covering number, i.e.\nn( , \u03b4,H) \u221d 1 2 log N \u03b4 .\nAs a basic example, consider I to be d-dimensional and a hypothesis space of linear functions\nf(I) = \u3008w, I\u3009 , \u2200I \u2208 I, w \u2208 I,\nso that the data representation is simply the identity. Then the -covering number of the set of linear functions with \u2016w\u2016 \u2264 1 is given by\nN \u223c \u2212d.\nIf the input data lie in a subspace of dimension s \u2264 d then the covering number of the space of linear functions becomes N \u223c \u2212s. In the next section, we further comment on the above example and provide an argument to illustrate the potential benefits of invariant representations."}, {"heading": "3.3 Sample Complexity of the Invariance Oracle", "text": "Consider the simple example of a set of images of p \u00d7 p pixels each containing an object within a (square) window of k\u00d7k pixels and surrounded by a uniform background. Imagine the object positions to be possibly anywhere in the image. Then it is easy to see that as soon as objects are translated so that they not overlap we get an orthogonal subspace. Then, we see that there are r2 = (p/k)2 possible subspaces of dimension k2, that is the set of translated images can be seen as a distribution of vectors supported within a ball in d = p2 dimensions. Following the discussion in the previous section the best algorithm based on a linear hypothesis space will incur in a sample complexity proportional to d. Assume now to have access to an oracle that can \u201dregister\u201d each image so that each object occupies the centered position. In this case, the distribution of images is effectively supported within a ball in s = k2 dimensions and the sample complexity is proportional to s rather than d. In other words a linear learning algorithm would need\nr2 = d/s\nless examples to achieve the same accuracy. The idea is that invariant representations can act as an invariance oracle, and have the same impact on the sample complexity. We add a few comments. First, while the above reasoning is developed for linear hypothesis space, a similar conclusion holds if non linear hypothesis spaces are considered. Second, one can see that the set of images obtained by translation is a low dimensional manifold, embedded in a very high\ndimensional space. Other transformations, such as small deformation, while being more complex, would have a much milder effect on the dimensionality of the embedded space. Finally, the natural question is how invariant representations can be learned, a topic we address next."}, {"heading": "4 Compact Group Invariant Representations", "text": "Consider a set of transformations G which is a locally compact group. Recall that each locally compact groups has a finite measure naturally associated to it, the so called Haar measure. The key feature of the Haar measure is its invariance to the group action, and in particular for all measurable functions f : G \u2192 R, and g\u2032 \u2208 G, it holds\u222b\ndgf(g) = \u222b dgf(g\u2032g).\nThe above equation is reminding of the invariance to translation of Lebesgue integrals and indeed, the Lebesgue measure can be shown to be the Haar measure associated to the translation group. The invariance property of the Haar measure associated to a locally compact group, is key to our development of invariant representation, as we describe next."}, {"heading": "4.1 Invariance via Group Averaging", "text": "The starting point for deriving invariant representations is the following direct application of the invariance property of the Haar measure.\nTheorem 1. Let \u03c8 : I \u2192 R be a, possibly non linear, functional on I. Then, the functional defined by\n\u00b5 : I \u2192 R, \u00b5(I) = \u222b dg\u03c8(gI), I \u2208 I (2)\nis invariant in the sense of Definition 1.\nThe functionals \u03c8, \u00b5 can be thought to be measurements, or features, of the data. In the following we are interested in measurements of the form\n\u03c8 : I \u2192 R, \u03c8(I) = \u03b7(\u3008gI, t\u3009), I \u2208 I, g \u2208 G (3)\nwhere t \u2208 T \u2286 I the set of unit vectors in I and \u03b7 : R \u2192 R is a possibly non linear function. As discussed in [4], the main motivation for considering measurements of the above form is their interpretation in terms of biological or artificial neural networks, see the following remarks.\nRemark 2 (Hubel and Wiesel Simple and Complex Cells [14]). A measurement as in (3) can be interpreted as the output of a neuron which computes a possibly high-dimensional inner product with a template t \u2208 T . In this interpretation,\n\u03b7 can be seen as a, so called, activation function, for which natural choices are sigmoidal functions, such as the hyperbolic tangent or rectifying functions such as the hinge. The functional \u00b5, obtained plugging (3) in (2) can be seen as the output of a second neuron which aggregates the output of other neurons by a simple averaging operation. Neurons of the former kind are similar to simple cells, whereas neurons of the second kind are similar to complex cells in the visual cortex.\nRemark 3 (Convolutional Neural Networks [20]). The computation of a measurement obtained plugging (3) in (2) can also be seen as the output of a so called convolutional neural network where each neuron, \u03c8 is performing the inner product operation between the input, I, and its synaptic weights, t, followed by a pointwise nonlinearity \u03b7 and a pooling layer.\nA second, reason to consider measurements of the form (3) is computational and, as shown later, have direct implications for learning. Indeed, to compute an invariant feature, according to (2) it is necessary to be able to compute the action of any element I \u2208 I for which we wish to compute the invariant measurement. However, a simple observation suggests an alternative strategy. Indeed, since the group representation is unitary, then\n\u3008gI, I \u2032\u3009 = \u2329 I, g\u22121I \u2032 \u232a , \u2200I, I \u2032 \u2208 I\nso that in particular we can compute \u03c8 by considering\n\u03c8(I) = \u222b dg\u03b7(\u3008I, gt\u3009), \u2200I \u2208 I, (4)\nwhere we used the invariance of the Haar measure. The above reasoning implies that an invariant feature can be computed for any point provided that for t \u2208 T , the sequence gt, g \u2208 G is available. This observation has the following interpretation: if we view a sequence gt, g \u2208 G, as a \u201dmovie\u201d of an object undergoing a family of transformations, then the idea is that invariant features can be computed for any new image provided that a movie of the template is available.\nWhile group averaging provides a natural way to tackle the problem of invariant representation, it is not clear how a family of invariant measurements can be ensured to be selective. Indeed, in the case of compact groups selectivity can be provably characterized using a probabilistic argument summarized in the following three steps:\n1. A unique probability distribution can be naturally associated to each orbit.\n2. Each such probability distributions can be characterized in terms of onedimensional projections.\n3. One dimensional probability distributions are easy to characterize, e.g. in terms of their cumulative distribution or their moments.\nWe note in passing that the above development, which we describe in detail next, naturally provides as a byproduct indications on how the non linearity in (3) needs to be chosen and thus gives insights on the nature of the pooling operation."}, {"heading": "4.2 A Probabilistic Approach to Selectivity", "text": "Let I = Rd, and P(I) the space of probability measures on I. Recall that for any compact group, the Haar measure is finite, so that, if appropriately normalized, it correspond to a probability measure.\nAssumption 1. In the following we assume G to be Abelian and compact and the corresponding Haar measure to be normalized.\nThe first step in our reasoning is the following definition.\nDefinition 4 (Representation via Orbit Probability). For all I \u2208 I, define the random variable\nZI : (G, dg)\u2192 I, ZI(g) = gI, \u2200g \u2208 G,\nwith law\n\u03c1I(A) = \u222b Z\u22121I (A) dg,\nfor all measurable sets A \u2282 I. Let\nP : I \u2192 P(I), P (I) = \u03c1I , \u2200I \u2208 I.\nThe map P associates to each point a corresponding probability distribution. From the above definition we see that we are essentially viewing an orbit as a distribution of points, and mapping each point in one such distribution. Then we have the following result.\nTheorem 2. For all I, I \u2032 \u2208 I\nI \u223c I \u2032 \u21d4 P (I) = P (I \u2032). (5)\nProof. We first prove that I \u223c I \u2032 \u21d2 \u03c1I = \u03c1I\u2032 . Recalling that if Cc(I) is the set of continuous functions on I with compact support, \u03c1I can be alternatively defined as the unique probability distribution such that\u222b\nf(z)d\u03c1I(z) = \u222b f(ZI(g))dg, \u2200f \u2208 Cc(I). (6)\nTherefore \u03c1I = \u03c1I\u2032 if and only if for any f \u2208 Cc(I), we have \u222b G f(ZI(g))dg =\u222b\nG f(ZI\u2032(g))dg which follows immediately by a change of variable and invariance of the Haar measure:\u222b\nG f(ZI(g))dg = \u222b G f(gI)dg = \u222b G f(gI \u2032)dg = \u222b G f(gg\u0303I)dg = \u222b G f(g\u0302I)dg\u0302\nTo prove that \u03c1I = \u03c1I\u2032 \u21d2 I \u223c I \u2032, note that \u03c1I(A) \u2212 \u03c1I\u2032(A) = 0 for all measurable sets A \u2286 I implies in particular that the support of the probability distributions of I has non null intersection on a set of non zero measure. Since the support of the distributions \u03c1I , \u03c1I\u2032 are exactly the orbits associated to I, I \u2032 respectively, then the orbits coincide, that is I \u223c I \u2032.\nThe above result shows that an invariant representation can be defined considering the probability distribution naturally associated to each orbit, however its computational realization would require dealing with high-dimensional distributions. Indeed, we next show that the above representation can be further developed to consider only probability distributions on the real line."}, {"heading": "4.2.1 Tomographic Probabilistic Representations", "text": "We need to introduce some notation and definitions. Let T = S, the unit sphere in I, and let P(R) denote the set of probability measures on the real line. For each t \u2208 T , let \u03c0t : I \u2192 R, \u03c0t(I) = \u3008I, t\u3009 , \u2200I \u2208 I. If \u03c1 \u2208 P(I), for all t \u2208 T we denote by \u03c1t \u2208 P(R) the random variable with law given by\n\u03c1t(B) = \u222b \u03c0\u22121t (B) d\u03c1,\nfor all measurable sets B \u2282 R.\nDefinition 5 (Radon Embedding). Let P(R)T = {h | h : T \u2192 P(R)} and define\nR : P(I)\u2192 P(R)T , R(\u03c1)(t) = \u03c1t, \u2200I \u2208 I.\nThe above map associates to each probability distribution a (continuous) family of probability distributions on the real line defined by one dimensional projections (tomographies). Interestingly, R can be shown to be a generalization of the Radon Transform to probability distributions [17]. We are going to use it to define the following data representation.\nDefinition 6 (TP Representation). We define the Tomographic Probabilistic (TP) representation as\n\u03a8 : I \u2192 P(R)T , \u03a8 = R \u25e6 P,\nwith P and R as in Definitions 4, 5, respectively.\nThe TP representation is obtained by first mapping each point in the distribution supported on its orbit and then in a (continuous) family of corresponding one dimensional distributions. The following result characterizes the invariance/selectivity property of the TP representation.\nTheorem 3. Let \u03a8 be the TP representation in Definition 6, then for all I, I \u2032 \u2208 I\nI \u223c I \u2032 \u21d4 \u03a8(I) = \u03a8(I \u2032). (7)\nThe proof of the above result is obtained combining Theorem 2 with the following well known result, characterizing probability distributions in terms of their one dimensional projections.\nTheorem 4 (Cramer-Wold [8]). For any \u03c1, \u03b3 \u2208 P(I), it holds\n\u03c1 = \u03b3 \u21d4 \u03c1t = \u03b3t, \u2200t \u2208 S. (8)\nThrough the TP representation, the problem of finding invariant/selective representations reduces to the study of one dimensional distributions, as we discuss next."}, {"heading": "4.2.2 CDF Representation", "text": "A natural way to describe a one-dimensional probability distribution is to consider the associated cumulative distribution function (CDF). Recall that if \u03be : (\u2126, p) \u2192 R is a random variable with law q \u2208 P(R), then the associated CDF is given by\nfq(b) = q((\u221e, b]) = \u222b dp(a)H(b\u2212 \u03be(a)), b \u2208 R, (9)\nwhere where H is the Heaviside step function. Also recall that the CDF uniquely defines a probability distribution since, by the Fundamental Theorem of Calculus, we have\nd\ndb fq(b) =\nd\ndb\n\u222b dp(a)H(b\u2212 \u03be(a)) = d\ndb \u222b b \u2212\u221e dp(a) = p(b).\nWe consider the following map.\nDefinition 7 (CDF Vector Map). Let F(R) = {h | h : R\u2192 [0, 1]}, and\nF(R)T = {h | h : T \u2192 F(R)}.\nDefine F : P(R)T \u2192 F(R)T , F (\u03b3)(t) = f\u03b3t\nfor \u03b3 \u2208 P(R)T and where we let \u03b3t = \u03b3(t) for all t \u2208 T .\nThe above map associates to a family of probability distributions on the real line their corresponding CDFs. We can then define the following representation.\nDefinition 8 (CDF Representation). Let\n\u00b5 : I \u2192 F(R)T , \u00b5 = F \u25e6R \u25e6 P,\nwith F ,P and R as in Definitions 7, 4, 5, respectively.\nThen, the following result holds.\nTheorem 5. For all I \u2208 I and t \u2208 T\n\u00b5t(I)(b) = \u222b dg\u03b7b(\u3008I, gt\u3009), b \u2208 R, (10)\nwhere we let \u00b5t(I) = \u00b5(I)(t) and, for all b \u2208 R, \u03b7b : R \u2192 R, is given by \u03b7b(a) = H(b\u2212 a), a \u2208 R. Moreover, for all I, I \u2032 \u2208 I\nI \u223c I \u2032 \u21d4 \u00b5(I) = \u00b5(I \u2032).\nProof. The proof follows noting that \u00b5 is the composition of the one to one maps F,R and a map P that is one to one w.r.t. the equivalence classes induced by the group of transformations G. Therefore \u00b5 is one to one w.r.t. the equivalence classes i.e. I \u223c I \u2032 \u21d4 \u00b5(I) = \u00b5(I \u2032).\nWe note that, from a direct comparison, one can see that (10) is of the form (4). Different measurements correspond to different choices of the threshold b.\nRemark 4. [Pooling Functions: from CDF to Moments and Beyond] The above reasoning suggests that a principled choice for the non linearity in (4) is a step function, which in practice could be replaced by a smooth approximation such a sigmoidal function. Interestingly, other choices of non linearities could be considered. For example, considering different powers would yield information on the moments of the distributions (more general non linear function than powers would yield generalized moments). This latter point of view is discussed in some detail in Appendix A."}, {"heading": "4.3 Templates Sampling and Metric Embedings", "text": "We next discuss what happens if only a finite number of (possibly random) templates are available. In this case, while invariance can be ensured, in general we cannot expect selectivity to be preserved. However, it is possible to show that the representation is almost selective (see below) if a sufficiently large number number of templates is available.\nTowards this end we introduce a metric structure on the representation space. Recall that if \u03c1, \u03c1\u2032 \u2208 P(R) are two probability distributions on the real line and f\u03c1, f\u03c1\u2032 their cumulative distributions functions, then the uniform Kolmogorov-Smirnov (KS) metric is induced by the uniform norm of the cumulative distributions that is\nd\u221e(f\u03c1, f\u03c1\u2032) = sup s\u2208R |f\u03c1(s)\u2212 f\u03c1\u2032(s)|,\nand takes values in [0, 1]. Then, if \u00b5 is the representation in (10) we can consider the metric\nd(I, I \u2032) = \u222b du(t)d\u221e(\u00b5 t(I), \u00b5t(I \u2032)) (11)\nwhere u is the (normalized) uniform measure on the sphere S. We note that, theorems 4 and 5 ensure that (11) is a well defined metric on the quotient space induced by the group transformations, in particular\nd(I, I \u2032) = 0\u21d4 I \u223c I \u2032.\nIf we consider the case in which only a finite set Tk = {t1, . . . , tk} \u2282 S of k templates is available, each point is mapped in a finite sequence of probability distributions or CDFs and (11) is replaced by\nd\u0302(I, I \u2032) = 1\nk k\u2211 i=1 d\u221e(\u00b5 ti(I), \u00b5ti(I \u2032)) (12)\nClearly, in this case we cannot expect to be able to discriminate every pair of points, however we have the following result.\nTheorem 6. Consider n images In in I. Let k \u2265 2c 2 log n \u03b4 , where c is a constant. Then with probability 1\u2212 \u03b42,\n|d(I, I \u2032)\u2212 d\u0302(I, I \u2032)| \u2264 . (13)\nfor all I, I \u2032 \u2208 In.\nProof. The proof follows from a direct application of Ho\u0308effding\u2019s inequality and a union bound. Fix I, I \u2032 \u2208 In. Define the real random variable Z : S \u2192 [0, 1],\nZ(ti) = d\u221e(\u00b5 ti(I), \u00b5ti(I \u2032)), i = 1, . . . , k.\nFrom the definitions it follows that \u2016Z\u2016 \u2264 1 and E(Z) = d(I, I \u2032). Then, Ho\u0308effding inequality implies\n|d(I, I \u2032)\u2212 d\u0302(I, I \u2032)| = |1 k k\u2211 i=1 E(Z)\u2212 Z(ti)| \u2265 ,\nwith probability at most 2e\u2212 2k. A union bound implies that the result holds uniformly on In with probability at least n22e\u2212 2k. The proof is concluded setting this probability to \u03b42 and taking k \u2265 2c 2 log n \u03b4 .\nWe note that, while we considered the KS metric for convenience, other metrics over probability distributions can be considered. Also, we note that a natural further question is how discretization/sampling of the group affects the representation. The above reasoning could be extended to yield results in this latter case. Finally, we note that, when compared to classical results on distance preserving embedding, such as Johnson Linderstrauss Lemma [18], Theorem 12 only ensures distance preservation up to a given accuracy which increases with a larger number of projections. This is hardly surprising, since the problem of finding suitable embedding for probability spaces is known to be considerably harder than the analogue problem for vector spaces [2]. The question of how devise strategies to define distance preserving embedding is an interesting open problem."}, {"heading": "5 Locally Invariant and Covariant Representa-", "text": "tions\nWe consider the case where a representation is given by collection of \u201dlocal\u201d group averages, and refer to this situation as the partially observable group (POG) case. Roughly speaking, the idea is that this kind of measurements can be invariant to sufficiently small transformations, i.e. be locally invariant. Moreover, representations given by collections of POG averages can be shown to be covariant (see section 5.2 for a definition)."}, {"heading": "5.1 Partially Observable Group Averages", "text": "For a subset G0 \u2282 G consider a POG measurement of the form\n\u03c8(I) = \u222b G0 dg\u03b7(\u3008I, gt\u3009). (14)\nThe above quantity can be interpreted as the \u201dresponse\u201d of a cell that can perceive visual stimuli within a \u201dwindow\u201d (receptive field) of size G0. A POG measurement corresponds to a local group average restricted to a subset of transformations G0. Clearly, such a measurement will not in general be invariant. Consider a POG measurement on a transformed point\u222b\nG0 dg\u03b7(\u3008g\u0303I, gt\u3009) = \u222b G0 dg\u03b7( \u2329 I, g\u0303\u22121gt \u232a ) = \u222b g\u0303G0 dg\u03b7(\u3008I, gt\u3009).\nIf we compare the POG measurements on the same point with and without a transformation, we have\n| \u222b G0 dg\u03b7(\u3008I, gt\u3009)\u2212 \u222b g\u0303G0 dg\u03b7(\u3008I, gt\u3009)|. (15)\nWhile there are several situations in which the above difference can be zero, the intuition from the vision interpretation is that the same response should be obtained if a sufficiently small object does not move (transform) too much with respect to the receptive field size. This latter situation can be described by the assumption that the function\nh : G \u2192 R, h(g) = \u03b7(\u3008I, gt\u3009)\nis zero outside of the intersection of g\u0303G0 \u2229 G0. Indeed, for all g\u0303 \u2208 G satisfying this latter assumption, the difference in (15) would clearly be zero. The above reasoning results in the following theorem.\nTheorem 7. Given I \u2208 I and t \u2208 T , assume that there exists a set G\u0303 \u2282 G such that, for all g\u0303 \u2208 G\u0303, \u03b7(\u3008I, gt\u3009) = 0 \u2200g /\u2208 g\u0303G0 \u2229 G0. (16) Then for g\u0303 \u2208 G\u0303\n\u03c8(I) = \u03c8(g\u0303I),\nwith \u03c8 as in (14).\nWe add a few comments. First, we note that condition (16) can be weakened requiring only \u03b7(\u3008I, gt\u3009) = 0 for all g \u2208 g\u0303G0\u2206G0, where we denote by \u2206 the symmetric difference of two sets (A\u2206B = (A \u222a B)/(A \u2229 B) with A,B sets). Second, we note that if the non linearity \u03b7 is zero only in zero, then we can rewrite condition (16) as\n\u3008I, gt\u3009 = 0, \u2200g \u2208 g\u0303G0\u2206G0.\nFinally, we note that the latter expression has a simple interpretation in the case of the translation group. In fact, we can interpret (16) as a spatial localization condition on the image I and the template t (assumed to be positive valued functions), see Figure 1. We conclude with the following remark.\nRemark 5 (Localization Condition and V1). Regarding the localization condition discussed above, as we comment elsewhere [3], the fact that a template needs to be localized could have implications from a biological modeling standpoint. More precisely, it could provides a theoretical foundation of the Gabor like shape of the responses observed in V1 cells in the visual cortex [23, 3, 5].\nRemark 6 (More on the Localization Condition). From a more mathematical point of view, an interesting question is about conditions under which whether the localization condition (16) is also necessary rather than only sufficient."}, {"heading": "5.2 POG Representation", "text": "For all g\u0304 \u2208 G, let g\u0304G0 = {g \u2208 G | g = g\u0304g\u2032, g\u2032 \u2208 G0}, the collection of \u201dlocal\u201d subsets of the group obtained from the subset G0. Moreover, let\nV = \u222b G0 dg.\nClearly, by the invariance of the measure, we have \u222b g\u0304G0 dg = V , for all g\u0304 \u2208 G. Then, for all I \u2208 I, g\u0304 \u2208 G, define the random variables\nZI,g\u0304 : g\u0304G0 \u2192 I, ZI,g\u0304(g) = gI, g \u2208 g\u0304G0, (17)\nwith laws\n\u03c1I,g\u0304(A) = 1\nV \u222b Z\u22121I,g\u0304(A) dg, t\nfor all measurable sets A \u2282 I. For each I \u2208 I, g\u0304 \u2208 G, the measure \u03c1I,g\u0304 corresponds to the distribution on the fraction of the orbit corresponding to the observable group subset g\u0304G0. Then we can represent each point with a collection of POG distributions.\nDefinition 9 (Representation via POG Probabilities). Let P(I)G = {h | h : G \u2192 P(I)} and define\nP\u0304 : I \u2192 P(I)G , P\u0304 (I)(g) = \u03c1I,g \u2200I \u2208 I, g \u2208 G\nEach point is mapped in the collection of distributions obtained considering all possible fractions of the orbit corresponding to g\u0304G0, g\u0304 \u2208 G. Note that, the action of an element g\u0303 \u2208 G of the group on the POG probability representation is given by\ng\u0303P\u0304 (I)(g) = P\u0304 (I)(g\u0303g)\nfor all g \u2208 G. The following result holds.\nTheorem 8. Let P\u0304 as in Definition (9). Then for all I, I \u2032 \u2208 I if\nI \u223c I \u2032 \u21d2 \u2203g\u0303 \u2208 G such that P\u0304 (I \u2032) = g\u0303P\u0304 (I). (18)\nEquivalently, for all I, I \u2032 \u2208 I if I \u2032 = g\u0303I\nthen P\u0304 (I \u2032)(g) = P\u0304 (I)(gg\u0303), \u2200g \u2208 G. (19)\ni.e. P\u0304 is covariant.\nProof. The proof follows noting that \u03c1I\u2032,g\u0304 = \u03c1I,g\u0304g\u0303 holds since, using the same characterization of \u03c1 as in (6),we have that for any f \u2208 Cc(I)\u222b\ng\u0304G0 f(ZI\u2032,g\u0304(g))dg = \u222b g\u0304G0 f(gI \u2032)dg = \u222b g\u0304G0 f(gg\u0303I)dg = \u222b g\u0304G0g\u0303 f(gI)dg\nwhere we used the invariance of the measure.\nFollowing the reasoning in the previous sections and recalling Definition 5, we consider the mapping given by one dimensional projections (tomographies) and corresponding representations.\nDefinition 10 (TP-POG Representation). Let P(R)G\u00d7T = {h | h : G \u00d7 T \u2192 P(R)} and define\nR\u0304 : P(I)G \u2192 P(R)G\u00d7T , R\u0304(h)(g, t) = R(h(g))(t) = ht(g),\nfor all h \u2208 P(I)G , g \u2208 G, t \u2208 T . Moreover, we define the Tomographic Probabilistic POG representation as\n\u03a8\u0304 : I \u2192 P(R)G\u00d7T , \u03a8\u0304 = R\u0304 \u25e6 P\u0304 ,\nwith P\u0304 as in Definition 9.\nWe have the following result:\nTheorem 9. The representation \u03a8\u0304 defined in 10 is covariant, i.e. \u03a8\u0304(g\u0303I)(g) = \u03a8\u0304(I)(g\u0303g).\nProof. The map \u03a8\u0304 = R\u0304 \u25e6 P\u0304 is covariant if both R\u0304 and P\u0304 are covariant. The map P\u0304 was proven to be covariant in Theorem 8. We then need to prove the covariance of R\u0304 i.e. g\u0303R\u0304(h)(g, t) = R\u0304(h)(g\u0303g, t) for all h \u2208 P(I)G . This follows from\nR\u0304(g\u0303h)(g, t) = R(g\u0303h(g))(t) = R(h(g\u0303g))(t) = R(h)(g\u0303g, t).\nThe TP-POG representation is obtained by first mapping each point I in the family of distributions \u03c1I,g, g \u2208 G supported on the orbit fragments corresponding to POG and then in a (continuous) family of corresponding one dimensional distributions \u03c1tI,g, g \u2208 G, t \u2208 T . Finally, we can consider the representation obtained representing each distribution via the corresponding CDF.\nDefinition 11 (CDF-POG Representation). Let F(R)G\u00d7T = {h | h : G \u00d7T \u2192 F(R)} and define\nF\u0304 : P(I)G\u00d7T \u2192 P(R)G\u00d7T , F\u0304 (h)(g, t) = F (h(g, t)) = fh(g,t),\nfor all h \u2208 P(I)G\u00d7T and g \u2208 G, t \u2208 T . Moreover, define the CDF-POG representation as \u00b5\u0304 : I \u2192 F(R)G\u00d7T , \u00b5\u0304 = F\u0304 \u25e6 R\u0304 \u25e6 P\u0304 , with P\u0304 ,F\u0304 as in Definition 9, 10, respectively.\nIt is easy to show that\n\u00b5g\u0304,t(I)(b) = \u222b g\u0304G0 \u03b7b(\u3008I, gt\u3009)dg. (20)\nwhere we let \u00b5g\u0304,t(I) = \u00b5(I)(g\u0304, t)."}, {"heading": "6 Further Developments: Hierarchical Repre-", "text": "sentation\nIn this section we discuss some further developments of the framework presented in the previous section. In particular, we sketch how multi-layer (deep) representations can be obtained abstracting and iterating the basic ideas introduced before.\nHierarchical representations, based on multiple layers of computations, have naturally arisen from models of information processing in the brain [11, 26]. They have also been critically important in recent machine learning successes in a variety of engineering applications, see e.g. [27]. In this section we address the question of how to generalize the framework previously introduced to consider multi-layer representations.\nRecall that the basic idea for building invariant/selective representation is to consider local (or global) measurements of the form\u222b\nG0 \u03b7(\u3008I, gt\u3009)dg, (21)\nwith G0 \u2286 G. A main difficulty to iterate this idea is that, following the development in previous sections, the representation (11)-(20), induced by collection of (local) group averages, maps the data space I in the space P(R)G\u00d7T . The latter space lacks an inner product as well as natural linear structure needed to define the measurements in (21). One possibility to overcome this problem is to consider an embedding in a suitable Hilbert space. The first step in this direction is to consider an embedding of the probability space P(R) in a (real separable) Hilbert space H. Interestingly, this can be achieved considering a variety of reproducing kernels over probability distributions, as we describe in Appendix B. Here we note that if \u03a6 : P(R)\u2192 H is one such embeddings, then we could consider a corresponding embedding of P(R)G\u00d7T in the space\nL2(G \u00d7 T ,H) = {h : G \u00d7 T \u2192 H | \u222b \u2016h(g, t)\u20162 dgdu(t)}\nwhere \u2016\u00b7\u2016H is the norm induced by the inner product \u3008\u00b7, \u00b7\u3009H in H and u is the uniform measure on the sphere S \u2282 I. The space L2(G\u00d7T ,H) is endowed with the inner product\n\u3008h, h\u2032\u3009H = \u222b \u3008h(g, t), h\u2032(g, t)\u30092H dgdu(t),\nfor all h, h\u2032 \u2208 L2(G \u00d7 T ,H), so that the corresponding norm is exactly \u2016h\u20162H = \u222b \u2016h(g, t)\u20162 dgdu(t).\nThe embedding of P(R)G\u00d7T in L2(G \u00d7 T ,H) is simply given by\nJ\u03a6 : P(R)G\u00d7T \u2192 L2(G \u00d7 T ,H), J\u03a6(\u03c1)(g, t) = \u03a6(\u03c1(g, t)) i.e.\nfor all \u03c1 \u2208 P(R)G\u00d7T . Provided with above notation we have the following result.\nTheorem 10. The representation defined by\nQ\u0304 : I \u2192 L2(G \u00d7 T ,H), Q\u0304 = J\u03a6 \u25e6 \u03a8\u0304. (22)\nwith \u03a8\u0304 as in Definition 10, is covariant, in the sense that,\nQ\u0304(gI) = gQ\u0304(I)\nfor all I \u2208 I, g \u2208 G.\nProof. The proof follows checking that by definition both R\u0304 and J\u03a6 are covariant and using Theorem 8. The fact that R\u0304 is covariant was proven in Th. 9. The covariance of J\u03a6, i.e. g\u0303J\u03a6(h)(g, t) = J\u03a6(h)(g\u0303g, t) for all h \u2208 P(R)G\u00d7T , follows from\nJ\u03a6(g\u0303h)(g, t) = \u03a6(g\u0303h(g, t)) = \u03a6(h(g\u0303g, t)) = J\u03a6(h)(g\u0303g, t).\nNow since P\u0304 was already proven covariant in Th. 8 we have that, being Q\u0304 = J\u03a6 \u25e6 R\u0304 \u25e6 P\u0304 composition of covariant representations, Q\u0304 is covariant i.e. g\u0303Q\u0304(I) = Q\u0304(g\u0303I).\nUsing the above definitions a second layer invariant measurement can be defined considering,\nv : I \u2192 R, v(I) = \u222b G0 \u03b7( \u2329 Q\u0304(x), g\u03c4 \u232a 2 )dg (23)\nwhere \u03c4 \u2208 L2(G \u00d7 T ,H) has unit norm. We add several comments. First, following the analysis in the previous sections Equation (23) can be used to define invariant (or locally invariant) measurements and hence representations defined by collections of measurements. Second, the construction can be further iterated to consider multi-layer representations, where at each layer an intermediate representation is obtained considering \u201ddistributions of distributions\u201d. Third, considering multiple layers naturally begs the question of how the number and properties of each layer affect the properties of the representation. Preliminary answers to these questions are described in [3, 4, 21, 23]. A full mathematical treatment is beyond the scope of the current paper which however provides a formal framework to tackle them in future work."}, {"heading": "7 Discussion", "text": "Motivated by the goal of characterizing good data representation that can be learned, this paper studies the mathematics of an approach to learn data representation that are invariant and selective to suitable transformations. While invariance can be proved rather directly from the invariance of the Haar measure associated with the group, characterizing selectivity requires a novel probabilistic argument developed in the previous sections.\nSeveral extensions of the theory are natural and have been sketched with preliminary results in [3, 4, 21, 23]. The main directions that need a rigorous theory extending the results of this paper are:\n\u2022 Hierarchical architectures. We described how the theory can be used to analyze local invariance properties, in particular for locally compact groups. We described covariance properties. Covariant layers can integrate representations that are locally invariant into representations that are more globally invariant.\n\u2022 Approximate invariance for transformations that are not groups. The same basic algorithm analyzed in this paper is used to yield approximate invariance, provided the templates transforms as the image, which requires the templates to be tuned to specific object classes.\nWe conclude with a few general remarks connecting our paper with this special issue on deep learning and especially with an eventual theory of such networks. Hierarchical architectures of simple and complex units. Feedforward architecture with n layers, consisting of dot products and nonlinear pooling functions, are quite general computing devices, basically equivalent to Turing machines running for n time points (for example the layers of the HMAX architecture in [26] can be described as AND operations (dot products) followed by OR operations (pooling), i.e. as disjunctions of conjunctions.). Given a very large set of labeled examples it is not too surprising that greedy algorithms such as stochastic gradient descent can find satisfactory parameters in such an architecture, as shown by the recent successes of Deep Convolutional Networks. Supervised learning with millions of examples, however, is not, in general, biologically plausible. Our theory can be seen as proving that a form of unsupervised learning in convolutional architectures is possible and effective, because it provides invariant representations with small sample complexity. Two stages: group and non-group transformations. The core of the theory applies to compact groups such as rotations of the image in the image plane. Exact invariance for each module is equivalent to a localization condition which could be interpreted as a form of sparsity [3]. If the condition is relaxed to hold approximately it becomes a sparsity condition for the class of images w.r.t. the dictionary tk under the group G when restricted to a subclass of similar images. This property, which is similar to compressive sensing \u201cincoherence\u201d (but in a group context), requires that I and tk have a representation with rather sharply peaked autocorrelation (and correlation) and guarantees approximate invariance for transformations which do not have group structure, see [21]. Robustness of pooling. It is interesting that the theory is robust with respect to the pooling nonlinearity. Indeed, as discussed, very general class of nonlinearities will work, see Appendix A. Any nonlinearity will provide invariance, if the nonlinearity does not change with time and is the same for all the simple cells pooled by the same complex cells. A sufficient number of different nonlinearities, each corresponding to a complex cell, can provide selectivity [3]. Biological predictions and biophysics, including dimensionality reduction and\nPCAs. There are at least two possible biophysical models for the theory. The first is the original Hubel and Wiesel model of simple cells feeding into a complex cell. The theory proposes the \u201dideal\u201d computation of a CDF, in which case the nonlinearity at the output of the simple cells is a threshold. A complex cell, summating the outputs of a set of simple cells, would then represent a bin of the histogram; a different complex cell in the same position pooling a set of similar simple cells with a different threshold would represent another bin of the histogram. The second biophysical model for the HW module that implements the computation required by i-theory consists of a single cell where dendritic branches play the role of simple cells (each branch containing a set of synapses with weights providing, for instance, Gabor-like tuning of the dendritic branch) with inputs from the LGN; active properties of the dendritic membrane distal to the soma provide separate threshold-like nonlinearities for each branch separately, while the soma summates the contributions for all the branches. This model would solve the puzzle that so far there seems to be no morphological difference between pyramidal cells classified as simple vs complex by physiologists. Further if the synapses are Hebbian it can be proved that Hebb\u2019s rule, appropriately modified with a normalization factor, is an online algorithm to compute the eigenvectors of the input covariance matrix, therefore tuning the dendritic branches weights to principal components and thus providing an efficient dimensionality reduction. (n\u2192 1).The present phase of Machine Learning is characterized by supervised learning algorithms relying on large sets of labeled examples (n\u2192\u221e). The next phase is likely to focus on algorithms capable of learning from very few labeled examples (n\u2192 1), like humans seem able to do. We propose and analyze a possible approach to this problem based on the unsupervised, automatic learning of a good representation for supervised learning, characterized by small sample complexity (n). In this view we take a step towards a major challenge in learning theory beyond the supervised learning, that is the problem of representation learning, formulated here as the unsupervised learning of invariant representations that significantly reduce the sample complexity of the supervised learning stage."}, {"heading": "Acknowledgment", "text": "We would like to thank the McGovern Institute for Brain Research for their support. This research was sponsored by grants from the National Science Foundation, AFSOR-THRL (FA8650-05-C-7262). Additional support was provided by the Eugene McDermott Foundation."}, {"heading": "A Representation Via Moments", "text": "In Section 4.2.2 we have discussed the derivation of invariant selective representation considering the CDFs of suitable one dimensional probability distributions. As we commented in Remark 4 alternative representations are possible, for example by considering moments. Here we discuss this point of view in some more detail.\nRecall that if \u03be : (\u2126, p)\u2192 R is a random variables with law q \u2208 P(R), then the associated moment vector is given is given by\nmrq = E|\u03be|r = \u222b dq|\u03be|r, r \u2208 N. (24)\nIn this case we have the following definitions and results.\nDefinition 12 (Moments Vector Map). Let M(R) = {h | h : N\u2192 R}, and\nM(R)T = {h | h : T \u2192M(R)}.\nDefine M : P(R)T \u2192M(R)T , M(\u00b5)(t) = m\u00b5t\nfor \u00b5 \u2208 P(R) and where we let \u00b5(t) = \u00b5t, for all t \u2208 T .\nThe above mapping associates to each one dimensional distribution the corresponding vector of moments. Recall that this association uniquely determines the probability distribution if the so called Carleman\u2019s condition is satisfied:\n\u221e\u2211 r=1 m \u2212 12r 2r = +\u221e\nwhere mr is the set of moments of the distribution. We can then define the following representation.\nDefinition 13 (Moments Representation). Let\n\u00b5 : I \u2192M(R)T , \u00b5 = M \u25e6R \u25e6 P,\nwith M ,P and R as in Definitions 12, 4, 5, respectively.\nThen, the following result holds.\nTheorem 11. For all I \u2208 I and t \u2208 T\n\u00b5t(I)(r) = \u222b dg| \u3008I, gt\u3009 |r, r \u2208 N,\nwhere we let \u00b5(I)(t) = \u00b5t(I). Moreover, for all I, I \u2032 \u2208 I\nI \u223c I \u2032 \u21d4 \u00b5(I) = \u00b5(I \u2032).\nProof. \u00b5 = M \u25e6R\u25e6P is a composition of a one to one map R, a map P that is one to one w.r.t. the equivalence classes induced by the group of transformations G and a map M that is one to one since Carleman\u2019s condition is satisfied. Indeed, we have,\n\u221e\u2211 r=1 (\u222b dg \u3008I, gt\u30092r )\u2212 12r \u2264 \u221e\u2211 r=1 (\u222b dg| \u3008I, gt\u3009 | )\u2212 12r 2r = \u221e\u2211 r=1 1 C = +\u221e\nwhere C = \u222b\ndg| \u3008I, gt\u3009 |. Therefore \u00b5 is one to one w.r.t. the equivalence classes i.e. I \u223c I \u2032 \u21d4 \u00b5(I) = \u00b5(I \u2032).\nWe add one remark regarding possible developments of the above result.\nRemark 7. Note that the above result essentially depends on the characterization of the moment problem of probability distributions on the real line. In this view, it could be further developed to consider for example the truncated case when only a finite number of moments is considered or the generalized moments problem, where families of (nonlinear) continuous functions, more general than powers, are considered (see e.g. [1])."}, {"heading": "B Kernels on probability distributions", "text": "To consider multi-layers within the framework proposed in the paper we need to embed probability spaces in Hilbert spaces. A natural way to do so is by considering appropriate positive definite (PD) kernels, that is symmetric functions K : X \u00d7X \u2192 R such that\nn\u2211 i,j=1 K(\u03c1i, \u03c1j)\u03b1i\u03b1j \u2265 0\nfor all \u2200\u03c11, . . . , \u03c1n \u2208 X,\u03b11, . . . , \u03b1n \u2208 R and where X is any set, e.g. X = R or X = P(R). Indeed, PD kernels are known to define a unique reproducing kernel Hilbert space (RKHS) HK for which they correspond to reproducing kernels, in the sense that if HK is the RKHS defined by K, then Kx = K(x, \u00b7) \u2208 HK for all x \u2208 X and\n\u3008f,Kx\u3009K = f(x), \u2200f \u2208 HK , x \u2208 X, (25)\nwhere \u3008\u00b7, \u00b7\u3009K is the inner product in HK (see for example [7] for an introduction to RKHS). Many examples of kernels on distributions are known and have been studied. For example [13, 32] discuss a variety of kernels of the form\nK(\u03c1, \u03c1\u2032) = \u222b \u222b d\u03b3(x)\u03ba(p\u03c1(x), p\u03c1\u2032(x))\nwhere p\u03c1, p\u03c1\u2032 are the densities of the measures \u03c1, \u03c1 \u2032 with respect to a dominating measure \u03b3 (which is assumed to exist) and \u03ba : R+0 \u00d7R + 0 \u2192 R is a PD kernel. Recalling that a PD kernel defines a pseudo-metric via the equation\ndK(\u03c1, \u03c1 \u2032)2 = K(\u03c1, \u03c1) +K(\u03c1\u2032, \u03c1)\u2212 2K(\u03c1, \u03c1\u2032).\nit is shown in [13, 32] how different classic metric on probability distributions can be recovered by suitable choices of the kernel \u03ba. For example,\n\u03ba(x, x\u2032) = \u221a xx\u2032,\ncorresponds to the Hellinger\u2019s distance, see [13, 32] for other examples. A different approach is based on defining kernels of the form\nK(\u03c1, \u03c1\u2032) = \u222b \u222b d\u03c1(x)d\u03c1\u2032(x\u2032)k(x, x\u2032), (26)\nwhere k : R\u00d7R \u2192 R is a PD kernel. Using the reproducing property of k we can write\nK(\u03c1, \u03c1\u2032) = \u2329\u222b d\u03c1(x\u2032)kx, \u222b d\u03c1(x)kx\u2032 \u232a k = \u3008\u03a6(\u03c1),\u03a6(\u03c1\u2032)\u3009\nwhere \u03a6 : P(R)\u2192 H is the embedding \u03a6(x) = \u222b d\u03c1(x\u2032)kx mapping each distribution in a corresponding kernel mean, see e.g. [7]. Condition on the kernel k, hence on K, ensuring that the corresponding function dK is a metric have been studied in detail, see e.g. [29]."}], "references": [{"title": "The classical moment problem: and some related questions in analysis, University mathematical monographs", "author": ["N. Akhiezer"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1965}, {"title": "Efficient Sketches for Earth-Mover Distance, with Applications", "author": ["A. Andoni", "K.D. Ba", "P. Indyk", "D.P. Woodruff"], "venue": "in FOCS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Unsupervised Learning of Invariant Representations in Hierarchical Architectures. arXiv preprint 1311.4158", "author": ["F. Anselmi", "J.Z. Leibo", "L. Rosasco", "J. Mutch", "A. Tacchetti", "T. Poggio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Representation Learning in Sensory Cortex: a theory. CBMM memo n 26", "author": ["F. Anselmi", "T. Poggio"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Magic Materials: a theory of deep hierarchical architectures for learning sensory representations", "author": ["T.T.A. P"], "venue": "CBCL paper", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Learning Deep Architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Reproducing kernel Hilbert spaces in probability and statistics", "author": ["A. Berlinet", "C. Thomas-Agnan"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Some theorems on distribution functions", "author": ["H. Cramer", "H. Wold"], "venue": "J. London Math. Soc.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1936}, {"title": "On the mathematical foundations of learning", "author": ["F. Cucker", "S. Smale"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Computational Topology, An Introduction", "author": ["H. Edelsbrunner", "J.L. Harer"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological Cybernetics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1980}, {"title": "Invariant Kernel Functions for Pattern Analysis and Machine Learning", "author": ["B. Haasdonk", "H. Burkhardt"], "venue": "Mach. Learn.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Hilbertian Metrics and Positive Definite Kernels on Probability Measures", "author": ["M. Hein", "O. Bousquet"], "venue": "AISTATS", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex", "author": ["D. Hubel", "T. Wiesel"], "venue": "The Journal of Physiology,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1962}, {"title": "Receptive fields and functional architecture in two nonstriate visual areas (18 and 19) of the cat", "author": ["D. Hubel", "T. Wiesel"], "venue": "Journal of Neurophysiology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1965}, {"title": "Receptive fields and functional architecture of monkey striate cortex", "author": ["D. Hubel", "T. Wiesel"], "venue": "The Journal of Physiology,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1968}, {"title": "Support Theorems for the Radon Transform and Cramr-Wold Theorems", "author": ["F.L. Jan Boman"], "venue": "Journal of Theoretical Probability,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["Johnson", "J.W.B. Lindenstrauss"], "venue": "Contemporary Mathematics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1984}, {"title": "Rotation Invariant Spherical Harmonic Representation of 3D Shape Descriptors", "author": ["M. Kazhdan", "T. Funkhouser", "S. Rusinkiewicz"], "venue": "Proceedings of the 2003 Eurographics/ACM SIGGRAPH Symposium on Geometry Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "The invariance hypothesis implies domain-specific regions in visual cortex. http://dx.doi.org/10.1101/004473", "author": ["J.Z. Leibo", "Q. Liao", "F. Anselmi", "T. Poggio"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Group Invariant Scattering", "author": ["S. Mallat"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Does invariant recognition predict tuning of neurons in sensory cortex?. MIT-CSAIL-TR-2013-019, CBCL-313", "author": ["T. Poggio", "J. Mutch", "F. Anselmi", "A. Tacchetti", "L. Rosasco", "J.Z. Leibo"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "On the theory of reproducing kernel hilbert spaces", "author": ["A.G. Ramm"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Methods of modern mathematical physics. II. , Fourier Analysis, Self-Adjointness", "author": ["M. Reed", "B. Simon"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1978}, {"title": "Hierarchical models of object recognition in cortex", "author": ["M. Riesenhuber", "T. Poggio"], "venue": "Nature Neuroscience,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "in International Conference on Learning Representations (ICLR2014). CBLS", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Actionable information in vision", "author": ["S. Soatto"], "venue": "Computer Vision,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Dictionary learning: What is the right representation for my signal", "author": ["I. Tosic", "P. Frossard"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Estimation of dependencies based on empirical data", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1982}, {"title": "Efficient Additive Kernels via Explicit Feature Maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "Pattern Analysis and Machine Intellingence,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "[3, 4, 5].", "startOffset": 0, "endOffset": 9}, {"referenceID": 3, "context": "[3, 4, 5].", "startOffset": 0, "endOffset": 9}, {"referenceID": 4, "context": "[3, 4, 5].", "startOffset": 0, "endOffset": 9}, {"referenceID": 4, "context": "Our work is motivated by a theory of cortex and in particular visual cortex [5].", "startOffset": 76, "endOffset": 79}, {"referenceID": 28, "context": "Examples in this class of methods include so called dictionary learning [30], autoencoders [6] and metric learning techniques (see e.", "startOffset": 72, "endOffset": 76}, {"referenceID": 5, "context": "Examples in this class of methods include so called dictionary learning [30], autoencoders [6] and metric learning techniques (see e.", "startOffset": 91, "endOffset": 94}, {"referenceID": 18, "context": "For example in the analysis of shapes [19] and more generally in computational topology [10], or in the design of positive definite functions associated to reproducing kernel Hilbert spaces [12].", "startOffset": 38, "endOffset": 42}, {"referenceID": 9, "context": "For example in the analysis of shapes [19] and more generally in computational topology [10], or in the design of positive definite functions associated to reproducing kernel Hilbert spaces [12].", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": "For example in the analysis of shapes [19] and more generally in computational topology [10], or in the design of positive definite functions associated to reproducing kernel Hilbert spaces [12].", "startOffset": 190, "endOffset": 194}, {"referenceID": 21, "context": "The ideas in [22, 28] are close in spirit to the study in this paper.", "startOffset": 13, "endOffset": 21}, {"referenceID": 27, "context": "The ideas in [22, 28] are close in spirit to the study in this paper.", "startOffset": 13, "endOffset": 21}, {"referenceID": 21, "context": "In particular, the results in [22] develop a different invariant and stable representation within a signal processing framework.", "startOffset": 30, "endOffset": 34}, {"referenceID": 27, "context": "In [28] an information theoretic perspective is considered to formalize the problem of learning invariant/selective representations.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "In this work we develop a machine learning perspective closely following computational neuroscience models of the information processing in the visual cortex [15, 16, 26].", "startOffset": 158, "endOffset": 170}, {"referenceID": 15, "context": "In this work we develop a machine learning perspective closely following computational neuroscience models of the information processing in the visual cortex [15, 16, 26].", "startOffset": 158, "endOffset": 170}, {"referenceID": 25, "context": "In this work we develop a machine learning perspective closely following computational neuroscience models of the information processing in the visual cortex [15, 16, 26].", "startOffset": 158, "endOffset": 170}, {"referenceID": 24, "context": "Throughout the article we assume the group representation to be unitary [25].", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "noise (see [22]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 23, "context": "The requirements of 1) being a Hilbert space of of functions and 2) have continuous evaluation functionals, define so called reproducing kernel Hilbert spaces [24].", "startOffset": 159, "endOffset": 163}, {"referenceID": 0, "context": "A hypothesis space H is said to be learnable if, for all \u2208 [0,\u221e), \u03b4 \u2208 [0, 1], there exists n( , \u03b4,H) \u2208 N such that inf fn sup \u03c1 P ( E(fn)\u2212 inf f\u2208H E(f) \u2265 ) \u2264 \u03b4.", "startOffset": 70, "endOffset": 76}, {"referenceID": 29, "context": "The sample complexity can be shown [31, 9] to be proportional to the logarithm of the covering number, i.", "startOffset": 35, "endOffset": 42}, {"referenceID": 8, "context": "The sample complexity can be shown [31, 9] to be proportional to the logarithm of the covering number, i.", "startOffset": 35, "endOffset": 42}, {"referenceID": 3, "context": "As discussed in [4], the main motivation for considering measurements of the above form is their interpretation in terms of biological or artificial neural networks, see the following remarks.", "startOffset": 16, "endOffset": 19}, {"referenceID": 13, "context": "Remark 2 (Hubel and Wiesel Simple and Complex Cells [14]).", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "Remark 3 (Convolutional Neural Networks [20]).", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "Interestingly, R can be shown to be a generalization of the Radon Transform to probability distributions [17].", "startOffset": 105, "endOffset": 109}, {"referenceID": 7, "context": "Theorem 4 (Cramer-Wold [8]).", "startOffset": 23, "endOffset": 26}, {"referenceID": 0, "context": "Let F(R) = {h | h : R\u2192 [0, 1]}, and F(R)T = {h | h : T \u2192 F(R)}.", "startOffset": 23, "endOffset": 29}, {"referenceID": 0, "context": "and takes values in [0, 1].", "startOffset": 20, "endOffset": 26}, {"referenceID": 0, "context": "Define the real random variable Z : S \u2192 [0, 1], Z(ti) = d\u221e(\u03bc i(I), \u03bci(I \u2032)), i = 1, .", "startOffset": 40, "endOffset": 46}, {"referenceID": 17, "context": "Finally, we note that, when compared to classical results on distance preserving embedding, such as Johnson Linderstrauss Lemma [18], Theorem 12 only ensures distance preservation up to a given accuracy which increases with a larger number of projections.", "startOffset": 128, "endOffset": 132}, {"referenceID": 1, "context": "This is hardly surprising, since the problem of finding suitable embedding for probability spaces is known to be considerably harder than the analogue problem for vector spaces [2].", "startOffset": 177, "endOffset": 180}, {"referenceID": 2, "context": "Regarding the localization condition discussed above, as we comment elsewhere [3], the fact that a template needs to be localized could have implications from a biological modeling standpoint.", "startOffset": 78, "endOffset": 81}, {"referenceID": 22, "context": "More precisely, it could provides a theoretical foundation of the Gabor like shape of the responses observed in V1 cells in the visual cortex [23, 3, 5].", "startOffset": 142, "endOffset": 152}, {"referenceID": 2, "context": "More precisely, it could provides a theoretical foundation of the Gabor like shape of the responses observed in V1 cells in the visual cortex [23, 3, 5].", "startOffset": 142, "endOffset": 152}, {"referenceID": 4, "context": "More precisely, it could provides a theoretical foundation of the Gabor like shape of the responses observed in V1 cells in the visual cortex [23, 3, 5].", "startOffset": 142, "endOffset": 152}, {"referenceID": 10, "context": "Hierarchical representations, based on multiple layers of computations, have naturally arisen from models of information processing in the brain [11, 26].", "startOffset": 145, "endOffset": 153}, {"referenceID": 25, "context": "Hierarchical representations, based on multiple layers of computations, have naturally arisen from models of information processing in the brain [11, 26].", "startOffset": 145, "endOffset": 153}, {"referenceID": 26, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Preliminary answers to these questions are described in [3, 4, 21, 23].", "startOffset": 56, "endOffset": 70}, {"referenceID": 3, "context": "Preliminary answers to these questions are described in [3, 4, 21, 23].", "startOffset": 56, "endOffset": 70}, {"referenceID": 20, "context": "Preliminary answers to these questions are described in [3, 4, 21, 23].", "startOffset": 56, "endOffset": 70}, {"referenceID": 22, "context": "Preliminary answers to these questions are described in [3, 4, 21, 23].", "startOffset": 56, "endOffset": 70}, {"referenceID": 2, "context": "Several extensions of the theory are natural and have been sketched with preliminary results in [3, 4, 21, 23].", "startOffset": 96, "endOffset": 110}, {"referenceID": 3, "context": "Several extensions of the theory are natural and have been sketched with preliminary results in [3, 4, 21, 23].", "startOffset": 96, "endOffset": 110}, {"referenceID": 20, "context": "Several extensions of the theory are natural and have been sketched with preliminary results in [3, 4, 21, 23].", "startOffset": 96, "endOffset": 110}, {"referenceID": 22, "context": "Several extensions of the theory are natural and have been sketched with preliminary results in [3, 4, 21, 23].", "startOffset": 96, "endOffset": 110}, {"referenceID": 25, "context": "Feedforward architecture with n layers, consisting of dot products and nonlinear pooling functions, are quite general computing devices, basically equivalent to Turing machines running for n time points (for example the layers of the HMAX architecture in [26] can be described as AND operations (dot products) followed by OR operations (pooling), i.", "startOffset": 255, "endOffset": 259}, {"referenceID": 2, "context": "Exact invariance for each module is equivalent to a localization condition which could be interpreted as a form of sparsity [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 20, "context": "This property, which is similar to compressive sensing \u201cincoherence\u201d (but in a group context), requires that I and t have a representation with rather sharply peaked autocorrelation (and correlation) and guarantees approximate invariance for transformations which do not have group structure, see [21].", "startOffset": 297, "endOffset": 301}, {"referenceID": 2, "context": "A sufficient number of different nonlinearities, each corresponding to a complex cell, can provide selectivity [3].", "startOffset": 111, "endOffset": 114}], "year": 2015, "abstractText": "We discuss data representation which can be learned automatically from data, are invariant to transformations, and at the same time selective, in the sense that two points have the same representation only if they are one the transformation of the other. The mathematical results here sharpen some of the key claims of i-theory \u2013 a recent theory of feedforward processing in sensory cortex. [3, 4, 5].", "creator": "LaTeX with hyperref package"}}}