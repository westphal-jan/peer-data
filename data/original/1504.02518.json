{"id": "1504.02518", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Apr-2015", "title": "Unsupervised Feature Learning from Temporal Data", "abstract": "Current state-of-the-art classification and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to define a more temporally and semantically coherent metric.", "histories": [["v1", "Thu, 9 Apr 2015 23:26:26 GMT  (4472kb,D)", "https://arxiv.org/abs/1504.02518v1", "arXiv admin note: substantial text overlap witharXiv:1412.6056"], ["v2", "Wed, 15 Apr 2015 23:08:30 GMT  (4471kb,D)", "http://arxiv.org/abs/1504.02518v2", "arXiv admin note: substantial text overlap witharXiv:1412.6056"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1412.6056", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["ross goroshin", "joan bruna", "jonathan tompson", "david eigen", "yann lecun"], "accepted": true, "id": "1504.02518"}, "pdf": {"name": "1504.02518.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ross Goroshin", "Joan Bruna", "Jonathan Tompson", "David Eigen", "Yann LeCun"], "emails": ["goroshin@cims.nyu.edu", "bruna@cims.nyu.edu", "tompson@cims.nyu.edu", "deigen@cims.nyu.edu", "yann@cims.nyu.edu"], "sections": [{"heading": "UNSUPERVISED FEATURE LEARNING FROM TEMPORAL DATA", "text": "Ross Goroshin1, Joan Bruna1,2, Jonathan Tompson1, David Eigen1, Yann LeCun1,2 1Courant Institute of Mathematical Science 719 Broadway 12th Floor, New York, NY 10003 2Facebook AI Research, 770 Broadway, New York, NY 10003 {goroshin,bruna,tompson,deigen,yann}@cims.nyu.edu\nCurrent state-of-the-art classification and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to define a more temporally and semantically coherent metric.\nOur main assumption is that data samples that are temporal neighbors are also likely to be neighbors in the latent space. For example, adjacent frames in a video sequence are more likely to be semantically similar than non-adjacent frames. This assumption naturally leads to the slowness prior on features which was introduced in SFA (Wiskott & Sejnowski (2002)).\nTemporal coherence can be exploited by assuming a prior on the features extracted from the temporal data sequence. One such prior is that the features should vary slowly with respect to time. In the discrete time setting this prior corresponds to minimizing an Lp norm of the difference of feature vectors for temporally adjacent inputs. Consider a video sequence with T frames, if zt represents the feature vector extracted from the frame at time t then the slowness prior corresponds to minimizing\u2211T t=1 \u2016zt \u2212 zt\u22121\u2016p. To avoid the degenerate solution zt = z0 for t = 1...T , a second term is introduced which encourages data samples that are not temporal neighbors to be separated by at least a distance ofm-units in feature space, wherem is known as the margin. In the temporal setting this corresponds to minimizing max(0,m \u2212 \u2016zt \u2212 zt\u2032\u2016p), where |t \u2212 t\u2032| > 1. Together the two terms form the loss function introduced in Hadsell et al. (2006) as a dimension reduction and data visualization algorithm known as DrLIM. Assume that there is a differentiable mapping from input space to feature space which operates on individual temporal samples. Denote this mapping by G and assume it is parametrized by a set of trainable coefficients denoted byW . That is, zt = GW (xt). The per-sample loss function can be written as:\nL(xt, xt\u2032 ,W ) = { \u2016GW (xt)\u2212GW (xt\u2032)\u2016p, if |t\u2212 t\u2032| = 1 max(0,m\u2212 \u2016GW (xt)\u2212GW (xt\u2032)\u2016p) if |t\u2212 t\u2032| > 1 (1)\nThe second contrastive term in Equation 1 only acts to avoid the degenerate solution in whichGW is a constant mapping, it does not guarantee that the resulting feature space is informative with respect to the input. This discriminative criteria only depends on pairwise distances in the representation space which is a geometrically weak notion in high dimensions. We propose to replace this contrastive term with a term that penalizes the reconstruction error of both data samples. Introducing a reconstruction terms not only prevents the constant solution but also acts to explicitly preserve information about the input. This is a useful property of features which are obtained using unsupervised learning; since the task to which these features will be applied is not known a priori, we would like to preserve as much information about the input as possible.\nWhat is the optimal architecture of GW for extracting slow features? Slow features are invariant to temporal changes by definition. In natural video and on small spatial scales these changes mainly correspond to local translations and deformations. Invariances to such changes can be achieved using appropriate pooling operators Bruna & Mallat (2013); LeCun et al. (1998). Such operators are at the heart of deep convolutional networks (ConvNets), currently the most successful supervised feature learning architectures Krizhevsky et al. (2012). Inspired by these observations, let GWe be a two stage encoder comprised of a learned, generally over-complete, linear map (We) and rectifying nonlinearity f(\u00b7), followed by a local pooling. Let the N hidden activations, h = f(Wex), be sub-\nar X\niv :1\n50 4.\n02 51\n8v 2\n[ cs\n.C V\n] 1\n5 A\npr 2\n01 5\ndivided into K potentially overlapping neighborhoods denoted by Pi. Note that biases are absorbed by expressing the input x in homogeneous coordinates. Feature zi produced by the encoder for the\ninput at time t can be expressed as GiWe(t) = \u2016ht\u2016 Pi p = (\u2211 j\u2208Pi h p tj ) 1 p\n. Training through a local pooling operator enforces a local topology on the hidden activations, inducing units that are pooled together to learn complimentary features. In the following experiments we will use p = 2. Although it has recently been shown that it is possible to recover the input when We is sufficiently redundant, reconstructing from these coefficients corresponds to solving a phase recovery problem Bruna et al. (2014) which is not possible with a simple inverse mapping, such as a linear map Wd. Instead of reconstructing from z we reconstruct from the hidden representation h. This is the same approach taken when training group-sparse auto-encoders Kavukcuoglu et al. (2009). In order to promote sparse activations in the case of over-complete bases we additionally add a sparsifying L1 penalty on the hidden activations. Including the rectifying nonlinearity becomes critical for learning sparse inference in a hugely redundant dictionary, e.g. convolutional dictionaries Gregor & LeCun (2010). The complete loss functional is:\nL(xt, xt\u2032 ,W ) = \u2211\n\u03c4={t,t\u2032}\n( \u2016Wdh\u03c4 \u2212 x\u03c4\u20162 + \u03b1|h\u03c4 | ) + \u03b2 K\u2211 i=1 \u2223\u2223\u2016ht\u2016Pi \u2212 \u2016ht\u2032\u2016Pi\u2223\u2223 (2) Figure 2 shows a convolutional version of the proposed architecture and loss. By replacing all linear operators in our model with convolutional filter banks and including spatial pooling, translation invariance need not be learned LeCun et al. (1998). In all other respects the convolutional model is conceptually identical to the fully connected model described in the previous section."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron C", "Vincent", "Pascal"], "venue": "Technical report, University of Montreal,", "citeRegEx": "Bengio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2012}, {"title": "Signature verification using a siamese time delay neural network", "author": ["Bromley", "Jane", "Bentz", "James W", "Bottou", "L\u00e9on", "Guyon", "Isabelle", "LeCun", "Yann", "Moore", "Cliff", "S\u00e4ckinger", "Eduard", "Shah", "Roopak"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Bromley et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "Invariant scattering convolution networks. Pattern Analysis and Machine Intelligence", "author": ["Bruna", "Joan", "Mallat", "St\u00e9phane"], "venue": "IEEE Transactions on,", "citeRegEx": "Bruna et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2013}, {"title": "Signal recovery from pooling representations", "author": ["Bruna", "Joan", "Szlam", "Arthur", "LeCun", "Yann"], "venue": "In ICML,", "citeRegEx": "Bruna et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2014}, {"title": "Learning intermediate-level representations of form and motion from natural movies", "author": ["Cadieu", "Charles F", "Olshausen", "Bruno A"], "venue": "Neural Computation,", "citeRegEx": "Cadieu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cadieu et al\\.", "year": 2012}, {"title": "Saturating auto-encoders", "author": ["Goroshin", "Rostislav", "LeCun", "Yann"], "venue": "In ICLR,", "citeRegEx": "Goroshin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goroshin et al\\.", "year": 2013}, {"title": "Learning fast approximations of sparse coding", "author": ["Gregor", "Karol", "LeCun", "Yann"], "venue": "In ICML\u20192010,", "citeRegEx": "Gregor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2010}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["Hadsell", "Raia", "Chopra", "Soumit", "LeCun", "Yann"], "venue": "In CVPR,", "citeRegEx": "Hadsell et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hadsell et al\\.", "year": 2006}, {"title": "Independent component analysis, volume 46", "author": ["Hyv\u00e4rinen", "Aapo", "Karhunen", "Juha", "Oja", "Erkki"], "venue": null, "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2004}, {"title": "Bubbles: a unifying framework for low-level statistical properties of natural image sequences", "author": ["Hyv\u00e4rinen", "Aapo", "Hurri", "Jarmo", "V\u00e4yrynen", "Jaakko"], "venue": "JOSA A,", "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2003}, {"title": "Learning invariant features through topographic filter maps", "author": ["Kavukcuoglu", "Koray", "Ranzato", "MarcAurelio", "Fergus", "Rob", "LeCun", "Yann"], "venue": "In CVPR,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2009}, {"title": "Extracting slow subspaces from natural videos leads to complex cells", "author": ["Kayser", "Christoph", "Einhauser", "Wolfgang", "Dummer", "Olaf", "Konig", "Peter", "Kding", "Konrad"], "venue": "In ICANN\u20192001,", "citeRegEx": "Kayser et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kayser et al\\.", "year": 2001}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex"], "venue": "Master\u2019s thesis,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Slowness and sparseness have diverging effects on complex cell learning", "author": ["Lies", "Jorn-Philipp", "Hafner", "Ralf M", "Bethge", "Matthias"], "venue": null, "citeRegEx": "Lies et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lies et al\\.", "year": 2014}, {"title": "Deep learning from temporal coherence in video", "author": ["Mobahi", "Hossein", "Collobert", "Ronan", "Weston", "Jason"], "venue": "In ICML,", "citeRegEx": "Mobahi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mobahi et al\\.", "year": 2009}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["Rifai", "Salah", "Vincent", "Pascal", "Muller", "Xavier", "Galrot", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "Technical report, University of Montreal,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Slow feature analysis: Unsupervised learning of invariances", "author": ["Wiskott", "Laurenz", "Sejnowski", "Terrence J"], "venue": "Neural Computation,", "citeRegEx": "Wiskott et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Wiskott et al\\.", "year": 2002}, {"title": "Deep learning of invariant features via simulated fixations in video", "author": ["Zou", "Will", "Zhu", "Shenghuo", "Yu", "Kai", "Ng", "Andrew Y"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "Together the two terms form the loss function introduced in Hadsell et al. (2006) as a dimension reduction and data visualization algorithm known as DrLIM.", "startOffset": 60, "endOffset": 82}, {"referenceID": 13, "context": "Invariances to such changes can be achieved using appropriate pooling operators Bruna & Mallat (2013); LeCun et al. (1998). Such operators are at the heart of deep convolutional networks (ConvNets), currently the most successful supervised feature learning architectures Krizhevsky et al.", "startOffset": 103, "endOffset": 123}, {"referenceID": 13, "context": "Such operators are at the heart of deep convolutional networks (ConvNets), currently the most successful supervised feature learning architectures Krizhevsky et al. (2012). Inspired by these observations, let GWe be a two stage encoder comprised of a learned, generally over-complete, linear map (We) and rectifying nonlinearity f(\u00b7), followed by a local pooling.", "startOffset": 147, "endOffset": 172}, {"referenceID": 2, "context": "Although it has recently been shown that it is possible to recover the input when We is sufficiently redundant, reconstructing from these coefficients corresponds to solving a phase recovery problem Bruna et al. (2014) which is not possible with a simple inverse mapping, such as a linear map Wd.", "startOffset": 199, "endOffset": 219}, {"referenceID": 2, "context": "Although it has recently been shown that it is possible to recover the input when We is sufficiently redundant, reconstructing from these coefficients corresponds to solving a phase recovery problem Bruna et al. (2014) which is not possible with a simple inverse mapping, such as a linear map Wd. Instead of reconstructing from z we reconstruct from the hidden representation h. This is the same approach taken when training group-sparse auto-encoders Kavukcuoglu et al. (2009). In order to promote sparse activations in the case of over-complete bases we additionally add a sparsifying L1 penalty on the hidden activations.", "startOffset": 199, "endOffset": 478}, {"referenceID": 2, "context": "Although it has recently been shown that it is possible to recover the input when We is sufficiently redundant, reconstructing from these coefficients corresponds to solving a phase recovery problem Bruna et al. (2014) which is not possible with a simple inverse mapping, such as a linear map Wd. Instead of reconstructing from z we reconstruct from the hidden representation h. This is the same approach taken when training group-sparse auto-encoders Kavukcuoglu et al. (2009). In order to promote sparse activations in the case of over-complete bases we additionally add a sparsifying L1 penalty on the hidden activations. Including the rectifying nonlinearity becomes critical for learning sparse inference in a hugely redundant dictionary, e.g. convolutional dictionaries Gregor & LeCun (2010). The complete loss functional is:", "startOffset": 199, "endOffset": 798}], "year": 2015, "abstractText": "Current state-of-the-art classification and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to define a more temporally and semantically coherent metric.", "creator": "LaTeX with hyperref package"}}}