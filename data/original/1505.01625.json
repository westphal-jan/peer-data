{"id": "1505.01625", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2015", "title": "Context-Aware Mobility Management in HetNets: A Reinforcement Learning Approach", "abstract": "The use of small cell deployments in heterogeneous network (HetNet) environments is expected to be a key feature of 4G networks and beyond, and essential for providing higher user throughput and cell-edge coverage. However, due to different coverage sizes of macro and pico base stations (BSs), such a paradigm shift introduces additional requirements and challenges in dense networks. Among these challenges is the handover performance of user equipment (UEs), which will be impacted especially when high velocity UEs traverse picocells. In this paper, we propose a coordination-based and context-aware mobility management (MM) procedure for small cell networks using tools from reinforcement learning. Here, macro and pico BSs jointly learn their long-term traffic loads and optimal cell range expansion, and schedule their UEs based on their velocities and historical rates (exchanged among tiers). The proposed approach is shown to not only outperform the classical MM in terms of UE throughput, but also to enable better fairness. In average, a gain of up to 80\\% is achieved for UE throughput, while the handover failure probability is reduced up to a factor of three by the proposed learning based MM approaches.", "histories": [["v1", "Thu, 7 May 2015 08:45:45 GMT  (462kb)", "http://arxiv.org/abs/1505.01625v1", null]], "reviews": [], "SUBJECTS": "cs.NI cs.LG", "authors": ["meryem simsek", "mehdi bennis", "ismail g\\\"uvenc"], "accepted": false, "id": "1505.01625"}, "pdf": {"name": "1505.01625.pdf", "metadata": {"source": "CRF", "title": "Context-Aware Mobility Management in HetNets: A Reinforcement Learning Approach", "authors": ["Meryem Simsek", "Mehdi Bennis", "\u0130smail G\u00fcven\u00e7"], "emails": ["meryem.simsek@tu-dresden.de,", "bennis@ee.oulu.fi,", "iguvenc@fiu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 5.\n01 62\n5v 1\n[ cs\n.N I]\n7 M\nay 2\nIndex Terms\u2014Cell range expansion, HetNets, load balancing, mobility management, reinforcement learning, context-aware scheduling.\nI. INTRODUCTION\nThe deployment of Long Term Evolution (LTE) heterogeneous networks (HetNets) is a promising approach to meet the ever-increasing wireless broadband capacity challenge [1], [2]. However, deploying HetNets entails a number of challenges in terms of capacity, coverage, mobility management (MM), and mobility load balancing (MLB) across multiple network tiers [3]. Mobility management is essential to ensure a continuous connectivity to mobile user equipment (UEs) while maintaining quality of service (QoS).\nThe mobility framework for LTE was originally developed and analyzed by the 3rd generation partnership project (3GPP) for macro-only networks, and was therefore not explicitly optimized for HetNets. In LTE Rel. 11, mobility enhancements in HetNets have been investigated through a dedicated study item [3]. 3GPP has defined key performance indicators (KPIs) for mobility measurements, i.e., the handover failure (HOF) due to a degraded signal-to-interference-plus-noiseratio (SINR), the radio link failure (RLF), as well as the\nThis research was supported in part by the SHARING project under the Finland grant 128010 and by the U.S. National Science Foundation under the grants CNS-1406968 and AST-1443999.\nprobability of unnecessary handovers, typically referred to as ping-pong (PP) events.\nPoor MM approaches may increase the HOFs, RLFs, and PPs, and result in unbalanced load among cells. This entails a low resource utilization efficiency and hence deterioration of the user experience. In order to solve this problem, while minimizing PPs, mobility parameters in each cell need to be carefully and dynamically optimized according to cell traffic loads. It is essential to optimize handover parameters such as time to trigger (TTT), range expansion bias (REB), and hysteresis margin in order to answer the question: \u201cwhen to handover which UE to which cell?\u201d\nMobility management techniques for HetNets have been recently investigated in the literature, e.g., in [4]\u2013[7]. In [4], the authors evaluate the effect of different combinations of MM parameter settings for HetNets. The main result is that mobility performance strongly depends on the cell size and UE speed. The simulations in [4] consider that all UEs have the same velocity in each simulation setup. In [5], the authors evaluate the mobility performance of HetNets considering almost blank subframes in the presence of cell range expansion and propose a mobility based intercell interference coordination (ICIC) scheme. Hereby, picocells configure coordinated resources by muting certain subframes so that macrocells can schedule their high velocity UEs in these resources without co-channel interference from picocells. However, the proposed approach only considers three broad classes of UE velocities: low, medium, and high. Moreover, no adaptation of the REB has been taken into account. In [6], a handover-aware ICIC approach based on reinforcement learning is proposed. Hereby, the authors model the ICIC approach as a sub-band selection problem for mobility robustness optimization in a small cell only network. In [7], the cell selection problem in HetNets is formulated as a network wide proportional fairness optimization problem by jointly considering the long-term channel condition and load balance in a HetNet. While the proposed method enhances the cell-edge UE performance, no results related to mobility parameters are presented.\nTo the best of our knowledge there is no previous work related to learning based mobility management in HetNets by jointly considering load balancing and UE scheduling. In this paper, we propose a joint MM and context-aware UE scheduling approach by using tools from reinforcement learning. Hereby, each base station (BS) individually optimizes\n2 Classical MM: W/o picocell coverage optimization\nmacro UE pico UE\nMacro/pico coordination: Average rate of UE is exchanged in case of handover Learning based MM:\nMacro-/picocell coverage optimization\nmacro UE pico UE\na)\nb)\nFig. 1: a) Classical MM framework w/o picocell coverage optimization, b) Proposed learning based MM framework considering velocity and history (average rate) based scheduling.\nits own strategy (REB, UE scheduling) based on limited coordination among tiers. Both macro- and picocells learn how to optimize their traffic load in the long-term and the UE association process in the short-term by performing history and velocity based scheduling. We propose multi armed bandit (MAB) and satisfaction based MM learning approaches aiming at improving the overall system performance and reducing the HOF and PP probabilities.\nTo illustrate the differences between the classical MM and our proposed approach, we depict in Fig. 1 a) and Fig. 1 b) the basic idea of the classical MM and proposed MM approaches, respectively. In the classical MM approach, there is no information exchange among tiers in case of UE handover and traffic offloading might be achieved by picocell range expansion. In the proposed MM approaches, instead, each cell individually optimizes its own MM strategy based on limited coordination among tiers. The major difference between MAB and satisfaction based learning is that MAB aims at maximizing the overall capacity while satisfaction based learning aims at satisfying the network in terms of capacity. In both cases, macro and pico BSs learn on the long-term how to optimize their REB, which results in loadbalancing. On the short-term, based on these optimized REB values, each cell carries out user scheduling by considering each UE\u2019s velocity and average rate, through coordinated effort among the tiers. Our contributions are as follows:\n\u2022 In the proposed MM approaches, we focus on both short-term and long-term solutions. In the long-term, a traffic load balancing procedure in a HetNet scenario is proposed, while in the short-term the UE association process is solved. \u2022 To implement the long-term load balancing method, we propose two learning based MM approaches by using reinforcement learning techniques: a MAB based and a satisfaction based MM approach.\n\u2022 The short-term UE association process is based on a proposed context-aware scheduler considering a UE\u2019s throughput history and velocity to enable fair scheduling and enhanced cell association.\nThe rest of the paper is organized as follows. Section II describes the system model, the problem formulation for MM, and the context-aware scheduler. In Section III, we introduce the learning based MM approaches. Section IV presents system level simulation results, and finally, Section V concludes the paper."}, {"heading": "II. SYSTEM MODEL", "text": "We focus on the downlink transmission of a 2-layer HetNet, where layer 1 is modeled as macrocells and layer 2 as picocells. The HetNet consists of a set of BSs K = {1, . . . ,K} with a set M = {1, . . . ,M} of macrocells underlaid by a set P = {1, ..., P} of picocells, where K = M \u222a P . Macro BSs are dropped following a hexagonal layout including three sectors. Within each macro sector m, p \u2208 P picocells are randomly positioned, and a set U = {1, ..., U} of UEs which are randomly dropped within a circle around each picocell p (hotspot). The UEs associated to macrocells are referred as macro UEs U(m) = {1(m), . . . , U(m)} \u2208 U and the UEs served by picocells are referred as pico UEs U(p) = {1(p), . . . , U(p)} \u2208 U , where U(p) 6= U(m). Each UE i(k) with k \u2208 {m, p} has a randomly selected velocity vi(k) \u2208 V km/h and a random direction of movement within an angle of [0; 2\u03c0]. A co-channel deployment is considered, in which picocells and macrocells operate in a system with a bandwidth B consisting of r = {1, . . . , R} resource blocks (RBs). At every time instant tn = nTs with n = [1, . . . , N ] and Ts = 1 ms, each BS k decides how to expand its coverage area by learning its REB \u03b2k = {\u03b2m, \u03b2p} with \u03b2m = {0; 3; 6} dB and \u03b2p = {0; 3; 6; 9; 12; 15; 18} dB1. Both macro and pico BSs select their REB to decide which UE i(k) to schedule on which RB based on the UE\u2019s context parameters. These context parameters are defined as the UE\u2019s velocity vi(k), its instantaneous rate \u03c6i(k)(tn) when associated to BS k and its average rate \u03c6i(k)(tn) defined as \u03c6i(k)(tn) = 1 T \u2211N n=1 \u03c6i(k)(tn), whereby T = NTs is a time window. The instantaneous rate \u03c6i(k)(tn) is given by:\n\u03c6i(k)(tn) = Bi(k) \u00b7 log ( 1 + \u03b3i(k)(tn) ) , (1)\nwith \u03b3i(k)(tn) being the SINR of UE i(k) at time tn, which is defined as:\n\u03b3i(k)(tn) = pk \u00b7 gi(k),k(tn) \u2211\nj\u2208K j 6=k\npj \u00b7 gi(k),j(tn) + \u03c32 , (2)\nwith pk being the transmit power of BS k, and gi(k),k(tn) being the channel gain from cell k to UE i(k) associated to BS k. The bandwidth Bi(k) in equation (1) is the bandwidth which is allocated to UE i(k) by BS k at time tn.\n1We consider lower REB values for macro BSs to avoid overloaded macrocells due to their large transmission power.\n3"}, {"heading": "A. Handover Procedure", "text": "According to the 3GPP standard, the handover mechanism is based on RSRP measurements, the filtering of measured RSRP samples, Handover Hysteresis Margin, and TTT mechanisms [3]. A handover is executed if the target cell\u2019s (biased) RSRP (plus hysteresis margin) is larger than the source cell\u2019s (biased) RSRP. In summary, the handover condition for a UE i(k) to BS k is defined as:\nPl(i(l)) + \u03b2l < Pk(i(k)) + \u03b2k +mhist, (3)\nwith {l, k} \u2208 K, mhist is the UE- or cell-specific hysteresis margin, \u03b2k(\u03b2l) is the REB of BS k(l), and Pk(i(k)) (or Pl(i(l))) [dBm] is the i(k)-th ( or i(l)-th) UE\u2019s RSRP from BS k(l) after TTT."}, {"heading": "B. Problem Formulation", "text": "Our optimization approach aims at maximizing the total rate of the network. Hereby, we consider long-term and shortterm processes. The long-term load balancing optimization approach is solved by the proposed learning based MM approaches presented in Section III-A and Section III-B, which result in REB \u03b2k value optimization and in load balancing \u03c6k,tot(tn). Based on the estimated instantaneous load, the context-aware scheduler selects, in the short-term, for each RB a UE by considering its history and velocity as described in Section II-C. This results in each UE\u2019s instantaneous rate \u03c6i(k)(tn) and the RB allocation vector \u03b1i(k)(tn) = [ \u03b1i(k),1, ..., \u03b1i(k),R ]\ncontaining binary variables \u03b1i(k),r , and indicating whether UE i(k) of BS k is allocated at RB r or not. At each time instant tn, each BS k performs the following optimization:\nmax \u03b1i(k)(tn)\n\u03b2k\nN \u2211\nn=1\n\u2211\ni(k)\u2208Uk\nR \u2211\nr=1\n\u03b1i(k),r(tn) \u00b7 \u03c6i(k),r(tn) (4)\nsubject to:\n\u03b1i(k),r(tn) \u2208 {0, 1} (5) \u2211\ni(k)\u2208Uk\n\u03b1i(k),r = 1 \u2200r, \u2200k, (6)\npk \u2264 p max k (7) \u03c6i(k)(tn) \u2265 \u03c6k,min, (8)\nwhere \u03c6i(k),r(tn) is the instantaneous rate of UE i(k) at RB r. The condition in (7) implies that the total transmitted power over all RBs does not exceed the maximum transmission power pmaxk of BS k."}, {"heading": "C. Context-Aware Scheduler", "text": "The proposed MM approach does not only optimize the load according to Section II-B, but considers also contextaware and fairness based UE scheduling. At each RB r, a UE i(k) is selected to be served by BS k on RB r according to the following scheduling criterion:\ni(k)r \u2217 = sort min (vi(k))\n(\narg max i(k)\u2208Uk\n\u03c6i(k),r(tn)\n\u03c6i(tn)\n)\n, (9)\nwhere sortmin(vi(k)) sorts the candidate UEs according to their velocity starting with the slowest UE, i.e. if more than one UE can be selected for RB r, the UE with minimum velocity is selected. The rationale behind introducing the sorting/ranking function for candidate UEs according to their velocity is that high-velocity UEs will not be favored over slow moving UEs.\nA scheduler according to (9) will allocate many (or even all) resources to a newly handed over UE since its average rate \u03c6i(tn) in the target cell is zero, i.e. in the classical Proportional Fair scheduler, \u03c6i(tn) = \u03c6i(k)(tn) = 0 when a UE is handed over to cell k, whereas we redefine it according to (10). To avoid this and enable a fair resource allocation among all UEs in a cell, we propose a history based scheduling approach. We define the average rate \u03c6i(tn) according to (10) incorporating the following idea: Via the X2-interface macro- and picocells coordinate, so that once a macro UE i(m) is handed over to picocell p its rate history at time instant tn is provided to picocell p in terms of average rate \u03c6i(m)(tn), such that the UE\u2019s (which is named as i(p) after the handover) average rate at picocell p becomes:\n\u03c6i(p)(tn + Ts) = T \u00b7 \u03c6i(m)(tn) + \u03c6i(p)(tn + Ts)\nT + Ts . (10)\nIn (10), a moving average rate is considered from macrocell to picocell, whereas in the classical MM approaches a UE\u2019s history is not considered and is equal to zero. In other words, the proposed MM approach considers the historical rate when UE i(m) was associated to the macrocell m in the past."}, {"heading": "III. LEARNING BASED MOBILITY MANAGEMENT ALGORITHM", "text": "To solve the optimization approach defined in Section II-B, we rely on the self organizing capabilities of HetNets and propose an autonomous solution for load balancing by using tools from reinforcement learning [8]. Hereby, each cell develops its own MM strategy to perform optimal load balancing based on the proposed learning based approaches presented in Section III-A and Section III-B. To realize this, we consider the game G = {K, {Ak}k\u2208K, {uk}k\u2208K}. Hereby, the set K = {M\u222aP} represents the set of players (i.e., BSs), and for all k \u2208 K, the set Ak = {\u03b2k} represents the set of actions player k can adopt. For all k \u2208 K, the function uk(tn) is the utility function of player k. The players learn at each time instant tn to optimize the load in long-term and to perform context aware scheduling in short-term based on the algorithms presented in Section III-A and III-B by the following steps:\n1) Action ak \u2208 Ak is selected based on the obtained utility uk(tn) = \u03c6k,tot(tn) with \u03c6k,tot(tn) being the total rate of player k at time tn as defined in equation (11). 2) The action selection strategy is updated based on the selected learning algorithm presented in Section III-A and Section III-B. 3) UE of BS k is allocated at RB r based on its velocity, its instantaneous rate, and its average rate according to (9).\n4"}, {"heading": "A. Multi-Armed Bandit Based Learning Approach", "text": "The objective of the MAB approach is to maximize the overall system performance. MAB is a machine learning technique based on an analogy with the traditional slot machine (one armed bandit) [9]. When pulled at time tn, each machine/player provides a reward. The objective is to maximize the collected reward through iterative pulls, i.e. learning iterations. The player selects its actions based on a decision function reflecting the well-known explorationexploitation trade-off in learning algorithms.\nThe set of players, actions and the utility function for our MAB based MM approach is defined as follows:\n\u2022 Players: Macro BSs M = {1, . . . ,M} and pico BSs P = {1, . . . , P}. \u2022 Actions: Ak = {\u03b2k} with \u03b2m = [0, 3, 6] dB and \u03b2p = [0, 3, 6, 9, 12, 15, 18] dB being the CRE bias. We consider higher bias values for picocells due to their low transmit power. The considered bias values rely partially on the assumptions in [10] and at the same time extensive simulation results.\n\u2022 Strategy:\n1) Every BS learns its optimum CRE bias value on a long-term basis considering its load:\n\u03c6k,tot(tn) = \u2211\ni(k)\u2208Uk\nR \u2211\nr=1\n\u03b1i(k),r(tn) \u00b7 \u03c6i(k),r(tn).\n(11)\nThis is inter-related with the handover triggering by defining the cell border of each cell, 2) A UE is handed over to BS k if it fulfills the condition (3). 3) RB based scheduling is performed based on equation (9).\n\u2022 Utility Function: The utility function in MAB learning is a decision function composed by an exploitation term represented by player k\u2019s total rate and exploration part considering the number of times an action has been selected so far. Player k selects its action aj(k)(tn) \u2208 Ak at time tn through maximizing a decision function dk,aj(k) (tn), which is defined as:\ndk,aj(k) (tn) = uk,aj(k)(tn)+\n\u221a \u221a \u221a \u221a 2 log ( \u2211|Ak| i=1 nk,ai(k)(tn) )\nnk,aj(k) (tn) ,\n(12) whereby uk,aj(k)(tn) is the mean reward of player k at time tn for action aj(k), nk,aj(k)(tn) is the number of times action aj(k) has been selected by player k until time tn, and | \u00b7 | represents the cardinality.\nDuring the first tn = |Ak| \u00b7 Ts player k selects each action once in a random order to initialize the learning process by receiving a reward for each action. For the following iterations tn > |Ak| \u00b7 Ts action selection is performed according to Algorithm 1. In each learning iteration the action a\u2217j(k) that maximizes the decision function in (12) is selected. Then the parameters are updated, whereby the following notation is used: sk,aj(k)(tn) is the cumulated reward of player k after\nAlgorithm 1 MAB based mobility management algorithm.\n1: for tn do 2: for i = 1 : |Ak| do 3: Select action a\u2217\nj(k) according: 4: a\u2217j(k) = argmaxaj(k)\u2208|Ak| ( dk,aj(k) (tn) ) 5: Update parameters according to: 6: Update the cumulated reward when player k selects action aj(k) 7: sk,aj(k) (tn + Ts) = sk,aj(k) (tn) + 1i=j \u00b7 \u03c6k,tot(tn) 8: nk,aj(k)(tn + Ts) = nk,aj(k) (tn) + 1i=j 9: uk,aj(k)(tn + Ts) = sk,aj(k) (tn+Ts)\nnk,aj(k) (tn+Ts)\n10: end for 11: tn = tn + Ts 12: end for\nplaying action aj(k) and 1i=j is equal to 1 if i = j and zero otherwise."}, {"heading": "B. Satisfaction Based Learning Approach", "text": "Satisfaction based learning approaches guarantee to satisfy the players in a system [11]. Here, we consider the player to be satisfied if its cell reaches a certain minimum level of total rate and if at least 90% of the UEs in the cell obtain a certain average rate. The rationale behind considering these satisfaction conditions is to guarantee each single UE\u2019s minimum rate while at the same time improving the total rate of the cell.\nTo enable a fair comparison, the set of players and the corresponding set of actions in the proposed satisfaction based MM approach are the same as in the MAB based MM approach. The utility function of player k at time tn is defined as the load according to equation (11). In the satisfaction based learning approach, the actions are selected according to a probability distribution \u03c0k(tn) = [\u03c0k,1(tn), . . . , \u03c0k,|Ak|(tn)]. Hereby, \u03c0k,j(tn) is the probability with which BS k chooses its action aj(k)(tn) at time tn. The following learning steps are performed in each learning iteration:\n1) In the first learning iteration tn = 1 the probability of each action is equal and an action is selected randomly. 2) In the following learning iterations tn > 1, the player changes its action selection strategy only if the received utility does not satisfy the cell, i.e. if the satisfaction condition is not fulfilled. 3) If the satisfaction condition is not fulfilled, the player k selects its action aj(k)(tn) according to the probability distribution \u03c0k(tn). 4) Each player k receives a reward \u03c6k,tot(tn) based on the selected actions. 5) The probability \u03c0k,j(tn) of action aj(k)(tn) is updated according to the linear reward-inaction scheme: \u03c0k,j(tn) = \u03c0k,j(tn \u2212 Ts) + \u03bb \u00b7 bk(tn)\u00b7\n(\n1aj(k)(tn)=ai(k)(tn) \u2212 \u03c0k,j(tn \u2212 Ts)\n)\n, (13)\nwhereby 1aj(k)(tn)=ai(k)(tn) = 1 for the selected action and zero for the non-selected actions and bk(tn) is defined as follows:\n5 0 0.5 1 1.5 2 2.5 3 3.5 4 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nThroughput per UE [Mbps]\nC D\nF\n0 0.1 0.2 0.3 0\n0.02\n0.04\nCell\u2212edge UE throughput\nClassical MAB Satisfaction"}, {"heading": "IV. SIMULATION RESULTS", "text": "The scenario used in the system-level simulations is based on configuration #4b HetNet scenario in [12]. Simulations are performed with the picocell deployment based modified version of the system level simulator presented in [13]. We consider a macrocell consisting of three sectors, an inter-side distance of 500 m, and P = {1, 2, 3} pico BSs per macro sector, randomly distributed within the macrocellular environment. In each macro sector, U = 30 mobile UEs are randomly dropped within a 60 m radius of each pico BS. The rationale behind dropping all UEs around pico BSs is to obtain a large number of handover within a short time in order to avoid large computation times due to the complexity of our system level simulations. Each UE i(k) has a randomly selected velocity vi(k) of V = {3; 30; 60; 120} km/h and a random direction of movement within [0; 2\u03c0], so that both macro-to-pico and pico-to-macro handover may occur. We consider fast-fading and shadowing effects in our simulations that are based on 3GPP assumptions [12]. To compare our results with other approaches we consider a baseline MM approach as defined in [3]. The UE performs RSRP measurements over one subframe every 40 ms and reports this value. The Layer 1 filtering averages the reported RSRP values every 200 ms to filter out fast fading effects. This value is further averaged through afirst-order ifinite impulse response (IIR-)filter which is known as a Layer 3 filter. A handover is then triggered if the Layer 3 filtered handover measurement meets the handover event entry condition in (3). A UE is handed over to its target cell after TTT. For the baseline MM approach, we consider proportional fair based scheduling, with no information exchange between macro and pico BSs. This baseline approach is referred to as classical HO approach.\n1 2 3 0\n50\n100\n150\n200\n250\nNumber of PBSs per macrocell\nSu m\n\u2212 ra\nte [\nM bp\ns]\nClassical MAB Satisfaction 76%\n71%\n80%\nFig. 3: Sum-rate vs. number of pico BSs per macrocell and TTT = 480 ms.\nFig. 2 depicts the cumulative distribution function (CDF) of the UE throughput for the classical, MAB and satisfaction based MM approaches. Compared to the classical approach, MAB and satisfaction based approaches lead to an improvement of 43% and 75% in average (50-th %), respectively. Hence, the satisfaction based approach outperforms the other MM approaches in terms of average UE throughput. In case of the cell-center UE throughput, which is defined as the 95-th % throughput, the opposite behavior is obtained. In this case an improvement of 124% and 80% is achieved for the MAB and satisfaction based approaches, respectively. The reason is that the satisfaction based MM approach only aims at satisfying the network in terms of rate and does not update its learning strategy once satisfaction is achieved. The MAB based approach on the other hand aims at maximizing the network performance, which is reflected in the improved cell-center UE throughput. The gains of the proposed MM approaches are also reflected in the cell-edge UE throughput, which is zoomed in Fig. 2. Here, the MAB and satisfaction based approaches yield 39% and 80% improvement compared to the classical approach.\nTo compare the performance of the proposed approaches for different number of picocells per macrocell, Fig. 3 plots the sum-rate versus number of pico BSs per macrocell. For different number of pico BSs the proposed MM approaches yield gains of around 70%-80 % for TTT = 480 ms. In Fig. 4, the performance of the sum-rate versus UE density per macrocell is depicted for TTT = 40 ms and TTT = 480 ms. In both cases, the classical approach yields very low rates, while the proposed approaches lead to significant improvement of up to 81 % for TTT = 40 ms and 85 % for TTT = 480 ms and converge to a significantly larger sum-rate than the classical approach.\nBesides the gains in terms of rate, our proposed learning based approaches yield also improvements in terms of HOF probability as depicted in Fig. 5. For the HOF performance evaluation, we modify our simulation settings by setting the same velocity for each UE. Compared to the classical MM approach, the proposed methods yield the same HOF\n6 10 15 20 25 30 35 40 45 50 50 75 100 125 150\nNumber of UEs per macrocell\nSu m\n\u2212 ra\nte [\nM pb\ns]\nClassical (TTT = 480 ms) MAB (TTT = 480 ms) Satisfaction (TTT = 480 ms) Classical (TTT = 40 ms) MAB (TTT = 40 ms) Satisfaction (TTT = 40 ms)\nFig. 4: Sum-rate vs. number of UEs per macrocell with 1 pico BS and TTT = 40 ms and TTT = 480 ms.\n0 20 40 60 80 100 120 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nVelocity [km/h]\nH O\nF pr\nob ab\nili ty\nClassical MAB SatisfactionTTT = 480 ms\nTTT = 40 ms\nFig. 5: HOF and ping pong probability for 30 UEs and 1 pico BS per macrocell and TTT = 40 ms and TTT = 480 ms.\nprobability for UEs at 3 km/h speed. For higher velocities in which more HOF is expected, the HOF probability obtained by the proposed approaches is significantly lower than in case of classical MM.\nThe PP probability is depicted in Fig. 6. For TTT = 40 ms, all MM methods yield very similar PP probabilities for lower velocities while this probability is decreased for higher velocities. This slope is aligned with the results presented in [3]. However, for high velocity UEs, the PP probability of the proposed MM approaches is half of the PP probability obtained for the classical MM approach which shows a significant improvement. The rationale behind this is that both tiers perform CRE for load balancing, i.e. if one cell tries to extend its coverage/handover a UE the other cell may prevent this handover by extending its coverage, too. In case of TTT = 480 ms almost no PPs are observed."}, {"heading": "V. CONCLUSION", "text": "We propose two learning based MM approaches and a history based context-aware scheduling method for HetNets. The first learning approach is based on MAB methods and aims\n0 20 40 60 80 100 120 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nVelocity [km/h]\nPP p\nro ba\nbi lit\ny\nClassical MAB Satisfaction\nTTT = 480 ms\nTTT = 40 ms\nFig. 6: PP probability for 30 UEs and 1 pico BS per macrocell and TTT = 40 ms and TTT = 480 ms.\nat system performance maximization. The second learning method aims at satisfying each cell and each UE of a cell based on satisfaction based learning. System level simulations demonstrate the performance enhancement of the proposed approaches compared to the classical MM method. While up to 80% gains are achieved in average for UE throughput, the HOF probability is reduced up to a factor of three by the proposed learning based MM approaches."}], "references": [{"title": "A Survey on 3GPP Heterogeneous Networks", "author": ["A. Damnjanovic"], "venue": "IEEE Wirel. Comm., vol.18,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Evolved Universal Terrestrial Radio Access (E- UTRA); Mobility Enhancements in Heterogeneous Networks", "author": ["3GPP TR 36.839"], "venue": "V11.1.0, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Mobility Performance of LTE Co-Channel Deployment of Macro and Pico Cells", "author": ["S. Barbera", "P.H. Michaelsen", "M. S\u00c3d\u2019ily", "K. Pedersen"], "venue": "Proc. IEEE Wireless Comm. and Networking Conf. (WCNC), France, Apr. 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Mobility Management Challenges in 3GPP Heterogeneous Networks", "author": ["D. Lopez-Perez", "I. Guvenc", "X. Chu"], "venue": "IEEE Comm. Mag., vol. 50, no. 12, Dec. 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Handover aware interference management in LTE small cells networks", "author": ["A. Feki", "V. Capdevielle", "L. Roullet", "A.G.Sanches"], "venue": "Proc. IEEE 11th International Symposium on Modeling & Optimization in Mobile, Ad Hoc & Wireless Networks (WiOpt), May 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimized Fairness Cell Selection for 3GPP LTE-A Macro-Pico HetNets, ", "author": ["J. Wang", "J. Liu", "D. Wang", "J. Pang", "G. Shen"], "venue": "in Proc. IEEE Vehic. Technol. Conf. (VTC),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Reinforcement Learning: A Tutorial", "author": ["M.E. Harmon", "S.S. Harmon"], "venue": "2000. Available: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.33.2480&rep=rep1&type=pdf.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Finite time Analysis for the Multiarmed Bandit Problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Performance Study on ABS with Reduced Macro Power", "author": ["3GPP R1-113806"], "venue": "3GPP TSG-RAN Meeting #67, Nov. 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Satisfaction Equilibrium: Achieving Cooperation in Incomplete Information Games", "author": ["S. Ross", "B. Chaib-draa"], "venue": "19th Canadian Conf. on Artificial Intelligence, 2006.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Evolved Universal Terrestrial Radio Access (EU- TRA); Further advancements for E-UTRA Physical Layer Aspects", "author": ["3GPP TR 36.814"], "venue": "V9.0.0, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "An LTE-Femtocell Dynamic System Level Simulator", "author": ["M. Simsek", "T. Akbudak", "B. Zhao", "A. Czylwik"], "venue": "International ITG Workshop on Smart Antennas (WSA), 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "The deployment of Long Term Evolution (LTE) heterogeneous networks (HetNets) is a promising approach to meet the ever-increasing wireless broadband capacity challenge [1], [2].", "startOffset": 172, "endOffset": 175}, {"referenceID": 1, "context": "However, deploying HetNets entails a number of challenges in terms of capacity, coverage, mobility management (MM), and mobility load balancing (MLB) across multiple network tiers [3].", "startOffset": 180, "endOffset": 183}, {"referenceID": 1, "context": "11, mobility enhancements in HetNets have been investigated through a dedicated study item [3].", "startOffset": 91, "endOffset": 94}, {"referenceID": 2, "context": ", in [4]\u2013[7].", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": ", in [4]\u2013[7].", "startOffset": 9, "endOffset": 12}, {"referenceID": 2, "context": "In [4], the authors evaluate the effect of different combinations of MM parameter settings for HetNets.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "The simulations in [4] consider that all UEs have the same velocity in each simulation setup.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "In [5], the authors evaluate the mobility performance of HetNets considering almost blank subframes in the presence of cell range expansion and propose a mobility based intercell interference coordination (ICIC) scheme.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "In [6], a handover-aware ICIC approach based on reinforcement learning is proposed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "In [7], the cell selection problem in HetNets is formulated as a network wide proportional fairness optimization problem by jointly considering the long-term channel condition and load balance in a HetNet.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "According to the 3GPP standard, the handover mechanism is based on RSRP measurements, the filtering of measured RSRP samples, Handover Hysteresis Margin, and TTT mechanisms [3].", "startOffset": 173, "endOffset": 176}, {"referenceID": 6, "context": "To solve the optimization approach defined in Section II-B, we rely on the self organizing capabilities of HetNets and propose an autonomous solution for load balancing by using tools from reinforcement learning [8].", "startOffset": 212, "endOffset": 215}, {"referenceID": 7, "context": "MAB is a machine learning technique based on an analogy with the traditional slot machine (one armed bandit) [9].", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "\u2022 Actions: Ak = {\u03b2k} with \u03b2m = [0, 3, 6] dB and \u03b2p = [0, 3, 6, 9, 12, 15, 18] dB being the CRE bias.", "startOffset": 31, "endOffset": 40}, {"referenceID": 4, "context": "\u2022 Actions: Ak = {\u03b2k} with \u03b2m = [0, 3, 6] dB and \u03b2p = [0, 3, 6, 9, 12, 15, 18] dB being the CRE bias.", "startOffset": 31, "endOffset": 40}, {"referenceID": 1, "context": "\u2022 Actions: Ak = {\u03b2k} with \u03b2m = [0, 3, 6] dB and \u03b2p = [0, 3, 6, 9, 12, 15, 18] dB being the CRE bias.", "startOffset": 53, "endOffset": 77}, {"referenceID": 4, "context": "\u2022 Actions: Ak = {\u03b2k} with \u03b2m = [0, 3, 6] dB and \u03b2p = [0, 3, 6, 9, 12, 15, 18] dB being the CRE bias.", "startOffset": 53, "endOffset": 77}, {"referenceID": 7, "context": "\u2022 Actions: Ak = {\u03b2k} with \u03b2m = [0, 3, 6] dB and \u03b2p = [0, 3, 6, 9, 12, 15, 18] dB being the CRE bias.", "startOffset": 53, "endOffset": 77}, {"referenceID": 10, "context": "\u2022 Actions: Ak = {\u03b2k} with \u03b2m = [0, 3, 6] dB and \u03b2p = [0, 3, 6, 9, 12, 15, 18] dB being the CRE bias.", "startOffset": 53, "endOffset": 77}, {"referenceID": 8, "context": "The considered bias values rely partially on the assumptions in [10] and at the same time extensive simulation results.", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "Satisfaction based learning approaches guarantee to satisfy the players in a system [11].", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "The scenario used in the system-level simulations is based on configuration #4b HetNet scenario in [12].", "startOffset": 99, "endOffset": 103}, {"referenceID": 11, "context": "Simulations are performed with the picocell deployment based modified version of the system level simulator presented in [13].", "startOffset": 121, "endOffset": 125}, {"referenceID": 10, "context": "We consider fast-fading and shadowing effects in our simulations that are based on 3GPP assumptions [12].", "startOffset": 100, "endOffset": 104}, {"referenceID": 1, "context": "To compare our results with other approaches we consider a baseline MM approach as defined in [3].", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "This slope is aligned with the results presented in [3].", "startOffset": 52, "endOffset": 55}], "year": 2015, "abstractText": "The use of small cell deployments in heterogeneous network (HetNet) environments is expected to be a key feature of 4G networks and beyond, and essential for providing higher user throughput and cell-edge coverage. However, due to different coverage sizes of macro and pico base stations (BSs), such a paradigm shift introduces additional requirements and challenges in dense networks. Among these challenges is the handover performance of user equipment (UEs), which will be impacted especially when high velocity UEs traverse picocells. In this paper, we propose a coordination-based and context-aware mobility management (MM) procedure for small cell networks using tools from reinforcement learning. Here, macro and pico BSs jointly learn their long-term traffic loads and optimal cell range expansion, and schedule their UEs based on their velocities and historical rates (exchanged among tiers). The proposed approach is shown to not only outperform the classical MM in terms of UE throughput, but also to enable better fairness. In average, a gain of up to 80% is achieved for UE throughput, while the handover failure probability is reduced up to a factor of three by the proposed learning based MM approaches.", "creator": "LaTeX with hyperref package"}}}